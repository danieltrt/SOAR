file_path,api_count,code
DecAtt/main_mnli.py,35,"b'from __future__ import division\n\nfrom datetime import datetime\nfrom torch.autograd import Variable\nfrom model import *\nimport argparse\nimport sys\nfrom util import *\nimport time\nimport torch\nimport random\nimport logging\nimport numpy as np\nimport cPickle as pickle\nfrom datetime import timedelta\nfrom os.path import expanduser\nfrom torch.autograd import Variable\nfrom torchtext.vocab import load_word_vectors\n\ndef prepare_data(pairs):\n\tbatch_list = []\n\tbatch_index = 0\n\twhile batch_index < len(pairs):\n\t\ttry:\n\t\t\tsubset = pairs[batch_index:batch_index + batch_size]\n\t\texcept:\n\t\t\tsubset = pairs[batch_index:]\n\t\ttmp_a = np.array([len(item[0]) for item in subset])\n\t\ttmp_b = np.array([len(item[1]) for item in subset])\n\t\tbatch_index2 = batch_index + min(len(np.where(tmp_a == tmp_a[0])[0]), len(np.where(tmp_b == tmp_b[0])[0]))\n\t\tbatch_list.append([batch_index, batch_index2])\n\t\tbatch_index = batch_index2\n\treturn batch_list\n\ndef create_batch(data,from_index, to_index):\n\tif to_index>len(data):\n\t\tto_index=len(data)\n\tlsize=0\n\trsize=0\n\tlsize_list=[]\n\trsize_list=[]\n\tfor i in range(from_index, to_index):\n\t\tlength=len(data[i][0])+2\n\t\tlsize_list.append(length)\n\t\tif length>lsize:\n\t\t\tlsize=length\n\t\tlength=len(data[i][1])+2\n\t\trsize_list.append(length)\n\t\tif length>rsize:\n\t\t\trsize=length\n\t#lsize+=1\n\t#rsize+=1\n\tlsent = data[from_index][0]\n\tlsent = [\'bos\']+lsent + [\'oov\' for k in range(lsize -1 - len(lsent))]\n\t#print(lsent)\n\tleft_sents = torch.cat((dict[word].view(1, -1) for word in lsent))\n\tleft_sents = torch.unsqueeze(left_sents,0)\n\n\trsent = data[from_index][1]\n\trsent = [\'bos\']+rsent + [\'oov\' for k in range(rsize -1 - len(rsent))]\n\t#print(rsent)\n\tright_sents = torch.cat((dict[word].view(1, -1) for word in rsent))\n\tright_sents = torch.unsqueeze(right_sents,0)\n\n\tlabels=[data[from_index][2]]\n\n\tfor i in range(from_index+1, to_index):\n\n\t\tlsent=data[i][0]\n\t\tlsent=[\'bos\']+lsent+[\'oov\' for k in range(lsize -1 - len(lsent))]\n\t\t#print(lsent)\n\t\tleft_sent = torch.cat((dict[word].view(1,-1) for word in lsent))\n\t\tleft_sent = torch.unsqueeze(left_sent, 0)\n\t\tleft_sents = torch.cat([left_sents, left_sent])\n\n\t\trsent=data[i][1]\n\t\trsent=[\'bos\']+rsent+[\'oov\' for k in range(rsize -1 - len(rsent))]\n\t\t#print(rsent)\n\t\tright_sent = torch.cat((dict[word].view(1,-1) for word in rsent))\n\t\tright_sent = torch.unsqueeze(right_sent, 0)\n\t\tright_sents = torch.cat((right_sents, right_sent))\n\n\t\tlabels.append(data[i][2])\n\n\tleft_sents=Variable(left_sents)\n\tright_sents=Variable(right_sents)\n\tif task==\'sts\':\n\t\tlabels=Variable(torch.Tensor(labels))\n\telse:\n\t\tlabels=Variable(torch.LongTensor(labels))\n\tlsize_list=Variable(torch.LongTensor(lsize_list))\n\trsize_list = Variable(torch.LongTensor(rsize_list))\n\n\tif torch.cuda.is_available():\n\t\tleft_sents=left_sents.cuda()\n\t\tright_sents=right_sents.cuda()\n\t\tlabels=labels.cuda()\n\t\tlsize_list=lsize_list.cuda()\n\t\trsize_list=rsize_list.cuda()\n\t#print(left_sents)\n\t#print(right_sents)\n\t#print(labels)\n\treturn left_sents, right_sents, labels, lsize_list, rsize_list\n\nif __name__ == \'__main__\':\n\ttask=\'mnli\'\n\tprint(\'task: \'+task)\n\tprint(\'model: DecAtt\')\n\ttorch.manual_seed(123456)\n\tEMBEDDING_DIM = 300\n\tPROJECTED_EMBEDDING_DIM = 300\n\n\tparser = argparse.ArgumentParser(description=__doc__)\n\tparser.add_argument(\'--e\', dest=\'num_epochs\', default=5000, type=int, help=\'Number of epochs\')\n\tparser.add_argument(\'--b\', dest=\'batch_size\', default=32, help=\'Batch size\', type=int)\n\tparser.add_argument(\'--u\', dest=\'num_units\', help=\'Number of hidden units\', default=100, type=int)\n\tparser.add_argument(\'--r\', help=\'Learning rate\', type=float, default=0.05, dest=\'rate\')\n\tparser.add_argument(\'--lower\', help=\'Lowercase the corpus\', default=True, action=\'store_true\')\n\tparser.add_argument(\'--model\', help=\'Model selection\', default=\'DecAtt\', type=str)\n\tparser.add_argument(\'--optim\', help=\'Optimizer algorithm\', default=\'adagrad\', choices=[\'adagrad\', \'adadelta\', \'adam\'])\n\tparser.add_argument(\'--max_grad_norm\', help=\'If the norm of the gradient vector exceeds this renormalize it\\\n\t\t\t\t\t\t\t\t\t   to have the norm equal to max_grad_norm\', type=float, default=5)\n\tif task==\'snli\':\n\t\tnum_class=3\n\t\tparser.add_argument(\'--train\', help=\'JSONL or TSV file with training corpus\', default=\'/data/snli_1.0_train.jsonl\')\n\t\tparser.add_argument(\'--dev\', help=\'JSONL or TSV file with development corpus\', default=\'/data/snli_1.0_dev.jsonl\')\n\t\tparser.add_argument(\'--test\', help=\'JSONL or TSV file with testing corpus\', default=\'/data/snli_1.0_test.jsonl\')\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DecAtt/data/snli\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DecAtt/data/snli\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\ttrain_pairs = pickle.load(open(basepath + ""/train_pairs.p"", ""rb""))\n\t\tdev_pairs = pickle.load(open(basepath + ""/dev_pairs.p"", ""rb""))\n\t\ttest_pairs = pickle.load(open(basepath + ""/test_pairs.p"", ""rb""))\n\telif task==\'mnli\':\n\t\tnum_class = 3\n\t\tparser.add_argument(\'--train\', help=\'JSONL or TSV file with training corpus\', default=\'/data/mnli/multinli_1.0_train.jsonl\')\n\t\tparser.add_argument(\'--dev_m\', help=\'JSONL or TSV file with development corpus\', default=\'/data/mnli/multinli_1.0_dev_matched.jsonl\')\n\t\tparser.add_argument(\'--dev_um\', help=\'JSONL or TSV file with development corpus\',\n\t\t                   default=\'/data/mnli/multinli_1.0_dev_mismatched.jsonl\')\n\t\tparser.add_argument(\'--test_m\', help=\'JSONL or TSV file with testing corpus\', default=\'/data/mnli/multinli_1.0_test_matched.jsonl\')\n\t\tparser.add_argument(\'--test_um\', help=\'JSONL or TSV file with testing corpus\',\n\t\t                    default=\'/data/mnli/multinli_1.0_test_mismatched.jsonl\')\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DecAtt/data/mnli\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DecAtt/data/mnli\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\t\'\'\'\n\t\ttrain_pairs = util.read_corpus(expanduser(""~"")+\'/Documents/research/pytorch/DeepPairWiseWord\' + args.train, True)\n\t\tdev_pairs_m = util.read_corpus(expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord\' + args.dev_m, True)\n\t\tdev_pairs_um = util.read_corpus(expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord\' + args.dev_um, True)\n\t\ttest_pairs_m = util.read_corpus(expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord\' + args.test_m, True)\n\t\ttest_pairs_um = util.read_corpus(expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord\' + args.test_um, True)\n\t\tpickle.dump(train_pairs, open(""data/mnli/train_pairs.p"", ""wb""))\n\t\tpickle.dump(dev_pairs_m, open(""data/mnli/dev_pairs_m.p"", ""wb""))\n\t\tpickle.dump(dev_pairs_um, open(""data/mnli/dev_pairs_um.p"", ""wb""))\n\t\tpickle.dump(test_pairs_m, open(""data/mnli/test_pairs_m.p"", ""wb""))\n\t\tpickle.dump(test_pairs_um, open(""data/mnli/test_pairs_um.p"", ""wb""))\n\t\t\'\'\'\n\t\ttrain_pairs = pickle.load(open(basepath + ""/train_pairs.p"", ""rb""))\n\t\tdev_pairs_m = pickle.load(open(basepath + ""/dev_pairs_m.p"", ""rb""))\n\t\tdev_pairs_um = pickle.load(open(basepath + ""/dev_pairs_um.p"", ""rb""))\n\t\ttest_pairs_m = pickle.load(open(basepath + ""/test_pairs_m.p"", ""rb""))\n\t\ttest_pairs_um = pickle.load(open(basepath + ""/test_pairs_um.p"", ""rb""))\n\telif task==\'quora\':\n\t\tnum_class = 2\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/quora\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/quora\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\ttrain_pairs = readQuoradata(basepath+\'/train/\')\n\t\tdev_pairs=readQuoradata(basepath+\'/dev/\')\n\t\ttest_pairs=readQuoradata(basepath+\'/test/\')\n\telif task==\'url\':\n\t\tnum_class = 2\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/url\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/url\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\ttrain_pairs = readQuoradata(basepath + \'/train/\')\n\t\tdev_pairs = None#readQuoradata(basepath + \'/dev/\')\n\t\ttest_pairs = readQuoradata(basepath + \'/test_9324/\')\n\t\tif dev_pairs==None:\n\t\t\tdev_pairs=test_pairs\n\telif task==\'pit\':\n\t\tnum_class = 2\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/pit\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/pit\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\ttrain_pairs = readQuoradata(basepath + \'/train/\')\n\t\ttest_pairs = readQuoradata(basepath + \'/test/\')\n\t\tdev_pairs=test_pairs\n\telif task==\'sts\':\n\t\tnum_class = 6\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/sts\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/sts\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\ttrain_pairs = readSTSdata(basepath + \'/train/\')\n\t\ttest_pairs = readSTSdata(basepath + \'/test/\')\n\t\tdev_pairs=test_pairs\n\telif task==\'wikiqa\':\n\t\tnum_class=2\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/wikiqa\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/wikiqa\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\t\ttrain_pairs = readQuoradata(basepath + \'/train/\')\n\t\t\tdev_pairs = readQuoradata(basepath + \'/dev/\')\n\t\t\ttest_pairs = readQuoradata(basepath + \'/test/\')\n\telif task==\'trecqa\':\n\t\tnum_class = 2\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/trecqa\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/trecqa\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\t\ttrain_pairs = readQuoradata(basepath + \'/train-all/\')\n\t\t\tdev_pairs = readQuoradata(basepath + \'/raw-dev/\')\n\t\t\ttest_pairs = readQuoradata(basepath + \'/raw-test/\')\n\t#sys.exit()\n\targs = parser.parse_args()\n\t#print(\'Model: %s\' % args.model)\n\tprint(\'Read data ...\')\n\tprint(\'Number of training pairs: %d\' % len(train_pairs))\n\tprint(\'Number of development m pairs: %d\' % len(dev_pairs_m))\n\tprint(\'Number of development um pairs: %d\' % len(dev_pairs_um))\n\tprint(\'Number of testing m pairs: %d\' % len(test_pairs_m))\n\tprint(\'Number of testing um pairs: %d\' % len(test_pairs_um))\n\tbatch_size = args.batch_size\n\tnum_epochs = args.num_epochs\n\ttokens = []\n\tdict={}\n\tword2id={}\n\tvocab=set()\n\tfor pair in train_pairs:\n\t\tleft=pair[0]\n\t\tright=pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in dev_pairs_m:\n\t\tleft=pair[0]\n\t\tright=pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in dev_pairs_um:\n\t\tleft=pair[0]\n\t\tright=pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in test_pairs_m:\n\t\tleft=pair[0]\n\t\tright=pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in test_pairs_um:\n\t\tleft=pair[0]\n\t\tright=pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\ttokens=list(vocab)\n\t#for line in open(basepath + \'/vocab.txt\'):\n\t#\ttokens.append(line.strip().decode(\'utf-8\'))\n\twv_dict, wv_arr, wv_size = load_word_vectors(embedding_path, \'glove.840B\', EMBEDDING_DIM)\n\t#embedding = []\n\ttokens.append(\'oov\')\n\ttokens.append(\'bos\')\n\t#embedding.append(dict[word].numpy())\n\t#print(len(embedding))\n\t#np.save(\'embedding\',np.array(embedding))\n\t#sys.exit()\n\tpretrained_emb = np.zeros(shape=(len(tokens), EMBEDDING_DIM))\n\toov={}\n\tfor id in range(100):\n\t\toov[id]=torch.normal(torch.zeros(EMBEDDING_DIM),std=1)\n\tid=0\n\tfor word in tokens:\n\t\ttry:\n\t\t\tdict[word] = wv_arr[wv_dict[word]]/torch.norm(wv_arr[wv_dict[word]])\n\t\texcept:\n\t\t\t#if args.model==\'DecAtt\':\n\t\t\t#\tdict[word]=oov[np.random.randint(100)]\n\t\t\t#else:\n\t\t\tdict[word] = torch.normal(torch.zeros(EMBEDDING_DIM),std=1)\n\t\tword2id[word]=id\n\t\tpretrained_emb[id] = dict[word].numpy()\n\t\tid+=1\n\n\tif task==\'sts\':\n\t\tcriterion = nn.KLDivLoss()\n\telse:\n\t\tcriterion = torch.nn.NLLLoss(size_average=True)\n\t#criterion = torch.nn.CrossEntropyLoss()\n\tmodel=DecAtt(200,num_class,len(tokens),EMBEDDING_DIM, PROJECTED_EMBEDDING_DIM, pretrained_emb)\n\tif torch.cuda.is_available():\n\t\tmodel = model.cuda()\n\t\tcriterion = criterion.cuda()\n\toptimizer = torch.optim.Adagrad(model.parameters(), lr=0.05, weight_decay=5e-5)\n\n\tprint(\'Start training...\')\n\tbatch_counter = 0\n\tbest_dev_acc_m = 0\n\tbest_dev_acc_um = 0\n\taccumulated_loss=0\n\treport_interval = 1000\n\tmodel.train()\n\ttrain_pairs=np.array(train_pairs)\n\trand_idx = np.random.permutation(len(train_pairs))\n\ttrain_pairs = train_pairs[rand_idx]\n\tboth_lengths = np.array([(len(train_pairs[i][0]), len(train_pairs[i][1])) for i in range(len(train_pairs))],\n\t                        dtype={\'names\': [\'x\', \'y\'], \'formats\': [\'i4\', \'i4\']})\n\tsorted_lengths = np.argsort(both_lengths, order=(\'x\', \'y\'))\n\ttrain_pairs = train_pairs[sorted_lengths]\n\ttrain_batch_list=prepare_data(train_pairs)\n\n\tbatch_index=0\n\tfor epoch in range(num_epochs):\n\t\tbatch_counter=0\n\t\taccumulated_loss = 0\n\t\tmodel.train()\n\t\tprint(\'--\' * 20)\n\t\tstart_time = time.time()\n\t\ttrain_rand_i=np.random.permutation(len(train_batch_list))\n\t\ttrain_batch_i=0\n\t\ttrain_sents_scaned=0\n\t\ttrain_num_correct=0\n\t\twhile train_batch_i<len(train_batch_list):\n\t\t\tbatch_index, batch_index2=train_batch_list[train_rand_i[train_batch_i]]\n\t\t\ttrain_batch_i+=1\n\t\t\t#batch_index2=batch_index+batch_size\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(train_pairs, batch_index, batch_index2)\n\t\t\ttrain_sents_scaned+=len(labels)\n\t\t\t#print(lsize_list)\n\t\t\t#print(rsize_list)\n\t\t\t#batch_index=batch_index2\n\t\t\toptimizer.zero_grad()\n\n\t\t\toutput = model(left_sents, right_sents, lsize_list, rsize_list)\n\t\t\tresult = output.data.cpu().numpy()\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = labels.data.cpu().numpy()\n\t\t\ttrain_num_correct += np.sum(a == b)\n\t\t\t#print(\'forward finished\'+str(datetime.now()))\n\t\t\t#print(output)\n\t\t\t#print(labels)\n\t\t\t#sys.exit()\n\t\t\tloss = criterion(output, labels)\n\t\t\tloss.backward()\n\n\t\t\t\'\'\'\'\'\'\n\t\t\tgrad_norm = 0.\n\t\t\t#para_norm = 0.\n\n\t\t\tfor m in model.modules():\n\t\t\t\tif isinstance(m, nn.Linear):\n\t\t\t\t\t# print(m)\n\t\t\t\t\tgrad_norm += m.weight.grad.data.norm() ** 2\n\t\t\t\t\t#para_norm += m.weight.data.norm() ** 2\n\t\t\t\t\tif m.bias is not None:\n\t\t\t\t\t\tgrad_norm += m.bias.grad.data.norm() ** 2\n\t\t\t\t\t\t#para_norm += m.bias.data.norm() ** 2\n\n\t\t\tgrad_norm ** 0.5\n\t\t\t#para_norm ** 0.5\n\n\t\t\ttry:\n\t\t\t\tshrinkage = args.max_grad_norm / grad_norm\n\t\t\texcept:\n\t\t\t\tpass\n\t\t\tif shrinkage < 1:\n\t\t\t\tfor m in model.modules():\n\t\t\t\t\t# print m\n\t\t\t\t\tif isinstance(m, nn.Linear):\n\t\t\t\t\t\tm.weight.grad.data = m.weight.grad.data * shrinkage\n\t\t\t\t\t\tif m.bias is not None:\n\t\t\t\t\t\t\tm.bias.grad.data = m.bias.grad.data * shrinkage\n\t\t\t\'\'\'\'\'\'\n\t\t\toptimizer.step()\n\t\t\t#print(\'backword finished\' + str(datetime.now()))\n\t\t\tbatch_counter += 1\n\t\t\t#print(batch_counter, loss.data[0])\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\tif batch_counter % report_interval ==0:\n\t\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\t\tmsg += \'\\t train batch loss: %f\' % (accumulated_loss/train_sents_scaned)\n\t\t\t\tmsg += \'\\t train accuracy: %f\' % (train_num_correct/train_sents_scaned)\n\t\t\t\tprint(msg)\n\t\t# valid after each epoch\n\t\tmodel.eval()\n\t\tdev_batch_index=0\n\t\tdev_num_correct=0\n\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\taccumulated_loss=0\n\t\tpred=[]\n\t\twhile dev_batch_index<len(dev_pairs_m):\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(dev_pairs_m, dev_batch_index, dev_batch_index+1)\n\t\t\tdev_batch_index += 1\n\t\t\toutput = model(left_sents, right_sents, lsize_list, rsize_list)\n\t\t\tresult=np.exp(output.data.cpu().numpy())\n\t\t\tloss = criterion(output, labels)\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = labels.data.cpu().numpy()\n\t\t\tdev_num_correct+=np.sum(a == b)\n\t\t\tpred.extend(result)\n\t\tmsg += \'\\t dev_m loss: %f\' % accumulated_loss\n\t\tdev_acc=dev_num_correct/len(dev_pairs_m)\n\t\tmsg += \'\\t dev_m accuracy: %f\' % dev_acc\n\t\tprint(msg)\n\t\tif dev_acc > best_dev_acc_m:\n\t\t\tbest_dev_acc_m=dev_acc\n\t\t\twith open(basepath + \'/prob_DecAtt_\' + task+\'_m\', \'w\') as f:\n\t\t\t\tfor item in pred:\n\t\t\t\t\tf.writelines(str(item[0]) + \'\\t\' + str(item[1]) + \'\\t\' + str(item[2]) + \'\\n\')\n\n\t\t\ttest_batch_index = 0\n\t\t\tpred=[]\n\t\t\twhile test_batch_index < len(test_pairs_m):\n\t\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(test_pairs_m, test_batch_index,\n\t\t\t\t                                                                       test_batch_index+1)\n\t\t\t\ttest_batch_index+=1\n\t\t\t\toutput = model(left_sents, right_sents, lsize_list, rsize_list)\n\t\t\t\tresult = output.data.cpu().numpy()\n\t\t\t\ta = np.argmax(result, axis=1)\n\t\t\t\tpred.extend(a)\n\t\t\twith open(basepath + \'/sub_m.csv\', \'w+\') as f:\n\t\t\t\tindex = [\'neutral\',\'contradiction\',\'entailment\']\n\t\t\t\tf.write(""pairID,gold_label\\n"")\n\t\t\t\tfor i, k in enumerate(pred):\n\t\t\t\t\tf.write(str(i + 9847) + "","" + index[k] + ""\\n"")\n\t\t\t#torch.save(model, basepath + \'/model_DecAtt_m\' + \'.pkl\')\n\n\t\tdev_batch_index=0\n\t\tdev_num_correct=0\n\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\taccumulated_loss=0\n\t\tpred=[]\n\t\twhile dev_batch_index<len(dev_pairs_um):\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(dev_pairs_um, dev_batch_index, dev_batch_index+1)\n\t\t\tdev_batch_index+=1\n\t\t\t#left_sents, right_sents, labels = create_batch(dev_pairs, 0, len(dev_pairs))\n\t\t\toutput = model(left_sents, right_sents, lsize_list, rsize_list)\n\t\t\tresult=np.exp(output.data.cpu().numpy())\n\t\t\tloss = criterion(output, labels)\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = labels.data.cpu().numpy()\n\t\t\tdev_num_correct+=np.sum(a == b)\n\t\t\tpred.extend(result)\n\t\tmsg += \'\\t dev_um loss: %f\' % accumulated_loss\n\t\tdev_acc=dev_num_correct/len(dev_pairs_um)\n\t\tmsg += \'\\t dev_um accuracy: %f\' % dev_acc\n\t\tprint(msg)\n\t\tif dev_acc > best_dev_acc_um:\n\t\t\tbest_dev_acc_um=dev_acc\n\t\t\twith open(basepath + \'/prob_DecAtt_\' + task+\'_um\', \'w\') as f:\n\t\t\t\tfor item in pred:\n\t\t\t\t\tf.writelines(str(item[0]) + \'\\t\' + str(item[1]) + \'\\t\' + str(item[2]) + \'\\n\')\n\n\t\t\ttest_batch_index = 0\n\t\t\tpred=[]\n\t\t\twhile test_batch_index < len(test_pairs_um):\n\t\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(test_pairs_um, test_batch_index,\n\t\t\t\t                                                                       test_batch_index+1)\n\t\t\t\ttest_batch_index+=1\n\t\t\t\toutput = model(left_sents, right_sents, lsize_list, rsize_list)\n\t\t\t\tresult = output.data.cpu().numpy()\n\t\t\t\ta = np.argmax(result, axis=1)\n\t\t\t\tpred.extend(a)\n\t\t\twith open(basepath + \'/sub_um.csv\', \'w+\') as f:\n\t\t\t\tindex = [\'neutral\',\'contradiction\',\'entailment\']\n\t\t\t\tf.write(""pairID,gold_label\\n"")\n\t\t\t\tfor i, k in enumerate(pred):\n\t\t\t\t\tf.write(str(i) + "","" + index[k] + ""\\n"")\n\t\t\t#torch.save(model, basepath + \'/model_DecAtt_um\' + \'.pkl\')\n\t\telapsed_time = time.time() - start_time\n\t\tprint(\'Epoch \' + str(epoch) + \' finished within \' + str(timedelta(seconds=elapsed_time)))\n'"
DecAtt/main_quora.py,32,"b'from __future__ import division\n\nfrom datetime import datetime\nfrom torch.autograd import Variable\nfrom model import *\nimport argparse\nimport sys\nfrom util import *\nimport time\nimport torch\nimport random\nimport logging\nimport numpy as np\nimport cPickle as pickle\nfrom datetime import timedelta\nfrom os.path import expanduser\nfrom torch.autograd import Variable\nfrom torchtext.vocab import load_word_vectors\n\ndef prepare_data(pairs):\n\tbatch_list = []\n\tbatch_index = 0\n\twhile batch_index < len(pairs):\n\t\ttry:\n\t\t\tsubset = pairs[batch_index:batch_index + batch_size]\n\t\texcept:\n\t\t\tsubset = pairs[batch_index:]\n\t\ttmp_a = np.array([len(item[0]) for item in subset])\n\t\ttmp_b = np.array([len(item[1]) for item in subset])\n\t\tbatch_index2 = batch_index + min(len(np.where(tmp_a == tmp_a[0])[0]), len(np.where(tmp_b == tmp_b[0])[0]))\n\t\tbatch_list.append([batch_index, batch_index2])\n\t\tbatch_index = batch_index2\n\treturn batch_list\n\ndef create_batch(data,from_index, to_index):\n\tif to_index>len(data):\n\t\tto_index=len(data)\n\tlsize=0\n\trsize=0\n\tlsize_list=[]\n\trsize_list=[]\n\tfor i in range(from_index, to_index):\n\t\tlength=len(data[i][0])+2\n\t\tlsize_list.append(length)\n\t\tif length>lsize:\n\t\t\tlsize=length\n\t\tlength=len(data[i][1])+2\n\t\trsize_list.append(length)\n\t\tif length>rsize:\n\t\t\trsize=length\n\t#lsize+=1\n\t#rsize+=1\n\tlsent = data[from_index][0]\n\tlsent = [\'bos\']+lsent + [\'oov\' for k in range(lsize -1 - len(lsent))]\n\t#print(lsent)\n\tleft_sents = torch.cat((dict[word].view(1, -1) for word in lsent))\n\tleft_sents = torch.unsqueeze(left_sents,0)\n\n\trsent = data[from_index][1]\n\trsent = [\'bos\']+rsent + [\'oov\' for k in range(rsize -1 - len(rsent))]\n\t#print(rsent)\n\tright_sents = torch.cat((dict[word].view(1, -1) for word in rsent))\n\tright_sents = torch.unsqueeze(right_sents,0)\n\n\tlabels=[data[from_index][2]]\n\n\tfor i in range(from_index+1, to_index):\n\n\t\tlsent=data[i][0]\n\t\tlsent=[\'bos\']+lsent+[\'oov\' for k in range(lsize -1 - len(lsent))]\n\t\t#print(lsent)\n\t\tleft_sent = torch.cat((dict[word].view(1,-1) for word in lsent))\n\t\tleft_sent = torch.unsqueeze(left_sent, 0)\n\t\tleft_sents = torch.cat([left_sents, left_sent])\n\n\t\trsent=data[i][1]\n\t\trsent=[\'bos\']+rsent+[\'oov\' for k in range(rsize -1 - len(rsent))]\n\t\t#print(rsent)\n\t\tright_sent = torch.cat((dict[word].view(1,-1) for word in rsent))\n\t\tright_sent = torch.unsqueeze(right_sent, 0)\n\t\tright_sents = torch.cat((right_sents, right_sent))\n\n\t\tlabels.append(data[i][2])\n\n\tleft_sents=Variable(left_sents)\n\tright_sents=Variable(right_sents)\n\tif task==\'sts\':\n\t\tlabels=Variable(torch.Tensor(labels))\n\telse:\n\t\tlabels=Variable(torch.LongTensor(labels))\n\tlsize_list=Variable(torch.LongTensor(lsize_list))\n\trsize_list = Variable(torch.LongTensor(rsize_list))\n\n\tif torch.cuda.is_available():\n\t\tleft_sents=left_sents.cuda()\n\t\tright_sents=right_sents.cuda()\n\t\tlabels=labels.cuda()\n\t\tlsize_list=lsize_list.cuda()\n\t\trsize_list=rsize_list.cuda()\n\t#print(left_sents)\n\t#print(right_sents)\n\t#print(labels)\n\treturn left_sents, right_sents, labels, lsize_list, rsize_list\n\nif __name__ == \'__main__\':\n\ttask=\'quora\'\n\tprint(\'task: \'+task)\n\tprint(\'model: DecAtt\')\n\tEMBEDDING_DIM = 300\n\tPROJECTED_EMBEDDING_DIM = 300\n\n\tparser = argparse.ArgumentParser(description=__doc__)\n\tparser.add_argument(\'--e\', dest=\'num_epochs\', default=5000, type=int, help=\'Number of epochs\')\n\tparser.add_argument(\'--b\', dest=\'batch_size\', default=32, help=\'Batch size\', type=int)\n\tparser.add_argument(\'--u\', dest=\'num_units\', help=\'Number of hidden units\', default=100, type=int)\n\tparser.add_argument(\'--r\', help=\'Learning rate\', type=float, default=0.05, dest=\'rate\')\n\tparser.add_argument(\'--lower\', help=\'Lowercase the corpus\', default=True, action=\'store_true\')\n\tparser.add_argument(\'--model\', help=\'Model selection\', default=\'DecAtt\', type=str)\n\tparser.add_argument(\'--optim\', help=\'Optimizer algorithm\', default=\'adagrad\', choices=[\'adagrad\', \'adadelta\', \'adam\'])\n\tparser.add_argument(\'--max_grad_norm\', help=\'If the norm of the gradient vector exceeds this renormalize it\\\n\t\t\t\t\t\t\t\t\t   to have the norm equal to max_grad_norm\', type=float, default=5)\n\tif task==\'snli\':\n\t\tnum_class=3\n\t\tparser.add_argument(\'--train\', help=\'JSONL or TSV file with training corpus\', default=\'/data/snli_1.0_train.jsonl\')\n\t\tparser.add_argument(\'--dev\', help=\'JSONL or TSV file with development corpus\', default=\'/data/snli_1.0_dev.jsonl\')\n\t\tparser.add_argument(\'--test\', help=\'JSONL or TSV file with testing corpus\', default=\'/data/snli_1.0_test.jsonl\')\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DecAtt/data/snli\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DecAtt/data/snli\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\ttrain_pairs = pickle.load(open(basepath + ""/train_pairs.p"", ""rb""))\n\t\tdev_pairs = pickle.load(open(basepath + ""/dev_pairs.p"", ""rb""))\n\t\ttest_pairs = pickle.load(open(basepath + ""/test_pairs.p"", ""rb""))\n\telif task==\'mnli\':\n\t\tnum_class = 3\n\t\tparser.add_argument(\'--train\', help=\'JSONL or TSV file with training corpus\', default=\'/data/mnli/multinli_1.0_train.jsonl\')\n\t\tparser.add_argument(\'--dev_m\', help=\'JSONL or TSV file with development corpus\', default=\'/data/mnli/multinli_1.0_dev_matched.jsonl\')\n\t\tparser.add_argument(\'--dev_um\', help=\'JSONL or TSV file with development corpus\',\n\t\t                   default=\'/data/mnli/multinli_1.0_dev_mismatched.jsonl\')\n\t\tparser.add_argument(\'--test_m\', help=\'JSONL or TSV file with testing corpus\', default=\'/data/mnli/multinli_1.0_test_matched.jsonl\')\n\t\tparser.add_argument(\'--test_um\', help=\'JSONL or TSV file with testing corpus\',\n\t\t                    default=\'/data/mnli/multinli_1.0_test_mismatched.jsonl\')\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DecAtt/data/mnli\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DecAtt/data/mnli\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\t\'\'\'\n\t\ttrain_pairs = util.read_corpus(expanduser(""~"")+\'/Documents/research/pytorch/DeepPairWiseWord\' + args.train, True)\n\t\tdev_pairs_m = util.read_corpus(expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord\' + args.dev_m, True)\n\t\tdev_pairs_um = util.read_corpus(expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord\' + args.dev_um, True)\n\t\ttest_pairs_m = util.read_corpus(expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord\' + args.test_m, True)\n\t\ttest_pairs_um = util.read_corpus(expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord\' + args.test_um, True)\n\t\tpickle.dump(train_pairs, open(""data/mnli/train_pairs.p"", ""wb""))\n\t\tpickle.dump(dev_pairs_m, open(""data/mnli/dev_pairs_m.p"", ""wb""))\n\t\tpickle.dump(dev_pairs_um, open(""data/mnli/dev_pairs_um.p"", ""wb""))\n\t\tpickle.dump(test_pairs_m, open(""data/mnli/test_pairs_m.p"", ""wb""))\n\t\tpickle.dump(test_pairs_um, open(""data/mnli/test_pairs_um.p"", ""wb""))\n\t\t\'\'\'\n\t\ttrain_pairs = pickle.load(open(basepath + ""/train_pairs.p"", ""rb""))\n\t\tdev_pairs_m = pickle.load(open(basepath + ""/dev_pairs_m.p"", ""rb""))\n\t\tdev_pairs_um = pickle.load(open(basepath + ""/dev_pairs_um.p"", ""rb""))\n\t\ttest_pairs_m = pickle.load(open(basepath + ""/test_pairs_m.p"", ""rb""))\n\t\ttest_pairs_um = pickle.load(open(basepath + ""/test_pairs_um.p"", ""rb""))\n\telif task==\'quora\':\n\t\tnum_class = 2\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/quora\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/quora\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\ttrain_pairs = readQuoradata(basepath+\'/train/\')\n\t\tdev_pairs=readQuoradata(basepath+\'/dev/\')\n\t\ttest_pairs=readQuoradata(basepath+\'/test/\')\n\telif task==\'url\':\n\t\tnum_class = 2\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/url\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/url\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\ttrain_pairs = readQuoradata(basepath + \'/train/\')\n\t\tdev_pairs = None#readQuoradata(basepath + \'/dev/\')\n\t\ttest_pairs = readQuoradata(basepath + \'/test_9324/\')\n\t\tif dev_pairs==None:\n\t\t\tdev_pairs=test_pairs\n\telif task==\'pit\':\n\t\tnum_class = 2\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/pit\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/pit\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\ttrain_pairs = readQuoradata(basepath + \'/train/\')\n\t\ttest_pairs = readQuoradata(basepath + \'/test/\')\n\t\tdev_pairs=test_pairs\n\telif task==\'sts\':\n\t\tnum_class = 6\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/sts\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/sts\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\ttrain_pairs = readSTSdata(basepath + \'/train/\')\n\t\ttest_pairs = readSTSdata(basepath + \'/test/\')\n\t\tdev_pairs=test_pairs\n\telif task==\'wikiqa\':\n\t\tnum_class=2\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/wikiqa\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/wikiqa\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\t\ttrain_pairs = readQuoradata(basepath + \'/train/\')\n\t\t\tdev_pairs = readQuoradata(basepath + \'/dev/\')\n\t\t\ttest_pairs = readQuoradata(basepath + \'/test/\')\n\telif task==\'trecqa\':\n\t\tnum_class = 2\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/trecqa\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/trecqa\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\t\ttrain_pairs = readQuoradata(basepath + \'/train-all/\')\n\t\t\tdev_pairs = readQuoradata(basepath + \'/raw-dev/\')\n\t\t\ttest_pairs = readQuoradata(basepath + \'/raw-test/\')\n\t#sys.exit()\n\targs = parser.parse_args()\n\t#print(\'Model: %s\' % args.model)\n\tprint(\'Read data ...\')\n\tprint(\'Number of training pairs: %d\' % len(train_pairs))\n\tprint(\'Number of development pairs: %d\' % len(dev_pairs))\n\tprint(\'Number of testing pairs: %d\' % len(test_pairs))\n\tbatch_size = args.batch_size\n\tnum_epochs = args.num_epochs\n\ttokens = []\n\tdict={}\n\tword2id={}\n\tvocab = set()\n\tfor pair in train_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in dev_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in test_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\ttokens=list(vocab)\n\t#for line in open(basepath + \'/vocab.txt\'):\n\t#\ttokens.append(line.strip().decode(\'utf-8\'))\n\twv_dict, wv_arr, wv_size = load_word_vectors(embedding_path, \'glove.840B\', EMBEDDING_DIM)\n\t#embedding = []\n\ttokens.append(\'oov\')\n\ttokens.append(\'bos\')\n\t#embedding.append(dict[word].numpy())\n\t#print(len(embedding))\n\t#np.save(\'embedding\',np.array(embedding))\n\t#sys.exit()\n\tpretrained_emb = np.zeros(shape=(len(tokens), EMBEDDING_DIM))\n\toov={}\n\tfor id in range(100):\n\t\toov[id]=torch.normal(torch.zeros(EMBEDDING_DIM),std=1)\n\tid=0\n\tfor word in tokens:\n\t\ttry:\n\t\t\tdict[word] = wv_arr[wv_dict[word]]/torch.norm(wv_arr[wv_dict[word]])\n\t\texcept:\n\t\t\t#if args.model==\'DecAtt\':\n\t\t\t#\tdict[word]=oov[np.random.randint(100)]\n\t\t\t#else:\n\t\t\tdict[word] = torch.normal(torch.zeros(EMBEDDING_DIM),std=1)\n\t\tword2id[word]=id\n\t\tpretrained_emb[id] = dict[word].numpy()\n\t\tid+=1\n\n\tif task==\'sts\':\n\t\tcriterion = nn.KLDivLoss()\n\telse:\n\t\tcriterion = torch.nn.NLLLoss(size_average=True)\n\t#criterion = torch.nn.CrossEntropyLoss()\n\tmodel=DecAtt(200,num_class,len(tokens),EMBEDDING_DIM, PROJECTED_EMBEDDING_DIM, pretrained_emb)\n\tif torch.cuda.is_available():\n\t\tmodel = model.cuda()\n\t\tcriterion = criterion.cuda()\n\toptimizer = torch.optim.Adagrad(model.parameters(), lr=0.05, weight_decay=5e-5)\n\n\tprint(\'Start training...\')\n\tbatch_counter = 0\n\tbest_dev_loss=10e10\n\tbest_dev_loss_m=10e10\n\tbest_dev_loss_um=10e10\n\taccumulated_loss=0\n\treport_interval = 1000\n\tmodel.train()\n\ttrain_pairs=np.array(train_pairs)\n\trand_idx = np.random.permutation(len(train_pairs))\n\ttrain_pairs = train_pairs[rand_idx]\n\tboth_lengths = np.array([(len(train_pairs[i][0]), len(train_pairs[i][1])) for i in range(len(train_pairs))],\n\t                        dtype={\'names\': [\'x\', \'y\'], \'formats\': [\'i4\', \'i4\']})\n\tsorted_lengths = np.argsort(both_lengths, order=(\'x\', \'y\'))\n\ttrain_pairs = train_pairs[sorted_lengths]\n\ttrain_batch_list=prepare_data(train_pairs)\n\n\n\tdev_pairs=np.array(dev_pairs)\n\tboth_lengths = np.array([(len(dev_pairs[i][0]), len(dev_pairs[i][1])) for i in range(len(dev_pairs))],\n\t                        dtype={\'names\': [\'x\', \'y\'], \'formats\': [\'i4\', \'i4\']})\n\tsorted_lengths = np.argsort(both_lengths, order=(\'x\', \'y\'))\n\tdev_pairs = dev_pairs[sorted_lengths]\n\tdev_batch_list=prepare_data(dev_pairs)\n\n\ttest_pairs=np.array(test_pairs)\n\tboth_lengths = np.array([(len(test_pairs[i][0]), len(test_pairs[i][1])) for i in range(len(test_pairs))],\n\t                        dtype={\'names\': [\'x\', \'y\'], \'formats\': [\'i4\', \'i4\']})\n\tsorted_lengths = np.argsort(both_lengths, order=(\'x\', \'y\'))\n\ttest_pairs = test_pairs[sorted_lengths]\n\ttest_batch_list=prepare_data(test_pairs)\n\tbatch_index=0\n\tfor epoch in range(num_epochs):\n\t\tbatch_counter=0\n\t\taccumulated_loss = 0\n\t\tmodel.train()\n\t\tprint(\'--\' * 20)\n\t\tstart_time = time.time()\n\t\ttrain_rand_i=np.random.permutation(len(train_batch_list))\n\t\ttrain_batch_i=0\n\t\ttrain_sents_scaned=0\n\t\ttrain_num_correct=0\n\t\twhile train_batch_i<len(train_batch_list):\n\t\t\tbatch_index, batch_index2=train_batch_list[train_rand_i[train_batch_i]]\n\t\t\ttrain_batch_i+=1\n\t\t\t#batch_index2=batch_index+batch_size\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(train_pairs, batch_index, batch_index2)\n\t\t\ttrain_sents_scaned+=len(labels)\n\t\t\t#print(lsize_list)\n\t\t\t#print(rsize_list)\n\t\t\t#batch_index=batch_index2\n\t\t\toptimizer.zero_grad()\n\n\t\t\toutput = model(left_sents, right_sents, lsize_list, rsize_list)\n\t\t\tresult = output.data.cpu().numpy()\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = labels.data.cpu().numpy()\n\t\t\ttrain_num_correct += np.sum(a == b)\n\t\t\t#print(\'forward finished\'+str(datetime.now()))\n\t\t\t#print(output)\n\t\t\t#print(labels)\n\t\t\t#sys.exit()\n\t\t\tloss = criterion(output, labels)\n\t\t\tloss.backward()\n\n\t\t\t\'\'\'\'\'\'\n\t\t\tgrad_norm = 0.\n\t\t\t#para_norm = 0.\n\n\t\t\tfor m in model.modules():\n\t\t\t\tif isinstance(m, nn.Linear):\n\t\t\t\t\t# print(m)\n\t\t\t\t\tgrad_norm += m.weight.grad.data.norm() ** 2\n\t\t\t\t\t#para_norm += m.weight.data.norm() ** 2\n\t\t\t\t\tif m.bias is not None:\n\t\t\t\t\t\tgrad_norm += m.bias.grad.data.norm() ** 2\n\t\t\t\t\t\t#para_norm += m.bias.data.norm() ** 2\n\n\t\t\tgrad_norm ** 0.5\n\t\t\t#para_norm ** 0.5\n\n\t\t\ttry:\n\t\t\t\tshrinkage = args.max_grad_norm / grad_norm\n\t\t\texcept:\n\t\t\t\tpass\n\t\t\tif shrinkage < 1:\n\t\t\t\tfor m in model.modules():\n\t\t\t\t\t# print m\n\t\t\t\t\tif isinstance(m, nn.Linear):\n\t\t\t\t\t\tm.weight.grad.data = m.weight.grad.data * shrinkage\n\t\t\t\t\t\tif m.bias is not None:\n\t\t\t\t\t\t\tm.bias.grad.data = m.bias.grad.data * shrinkage\n\t\t\t\'\'\'\'\'\'\n\t\t\toptimizer.step()\n\t\t\t#print(\'backword finished\' + str(datetime.now()))\n\t\t\tbatch_counter += 1\n\t\t\t#print(batch_counter, loss.data[0])\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\tif batch_counter % report_interval ==0:\n\t\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\t\tmsg += \'\\t train batch loss: %f\' % (accumulated_loss/train_sents_scaned)\n\t\t\t\tmsg += \'\\t train accuracy: %f\' % (train_num_correct/train_sents_scaned)\n\t\t\t\tprint(msg)\n\t\t# valid after each epoch\n\t\tmodel.eval()\n\t\tdev_batch_index = 0\n\t\tdev_num_correct = 0\n\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\taccumulated_loss = 0\n\t\tdev_batch_i = 0\n\t\tpred=[]\n\t\tgold=[]\n\t\twhile dev_batch_i < len(dev_batch_list):\n\t\t\tdev_batch_index, dev_batch_index2 = dev_batch_list[dev_batch_i]\n\t\t\tdev_batch_i += 1\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(dev_pairs, dev_batch_index,\n\t\t\t                                                                       dev_batch_index2)\n\t\t\t# left_sents, right_sents, labels = create_batch(dev_pairs, 0, len(dev_pairs))\n\t\t\toutput = model(left_sents, right_sents, lsize_list, rsize_list)\n\t\t\tresult = output.data.cpu().numpy()\n\t\t\tloss = criterion(output, labels)\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = labels.data.cpu().numpy()\n\t\t\tdev_num_correct += np.sum(a == b)\n\t\t\tif task==\'pit\' or task==\'url\' or task==\'wikiqa\' or task==\'trecqa\':\n\t\t\t\tpred.extend(result[:,1])\n\t\t\t\tgold.extend(b)\n\t\t\tif task==\'sts\':\n\t\t\t\tpred.extend(0*result[:,0]+1*result[:,1]+2*result[:,2]+3*result[:,3]+4*result[:,4]+5*result[:,5])\n\t\t\t\tgold.extend(0*b[:,0]+1*b[:,1]+2*b[:,2]+3*b[:,3]+4*b[:,4]+5*b[:,5])\n\t\tmsg += \'\\t dev loss: %f\' % accumulated_loss\n\t\tdev_acc = dev_num_correct / len(dev_pairs)\n\t\tmsg += \'\\t dev accuracy: %f\' % dev_acc\n\t\tprint(msg)\n\t\tif task==\'pit\' or task==\'url\'or task==\'wikiqa\' or task==\'trecqa\':\n\t\t\tURL_maxF1_eval(pred, gold)\n\t\telif task==\'sts\':\n\t\t\tprint(\'pearson: \'+str(pearson(pred,gold)))\n\t\t# test after each epoch\n\t\ttest_batch_index = 0\n\t\ttest_num_correct = 0\n\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\taccumulated_loss = 0\n\t\ttest_batch_i = 0\n\t\tpred = []\n\t\tgold = []\n\t\twhile test_batch_i < len(test_batch_list):\n\t\t\ttest_batch_index, test_batch_index2 = test_batch_list[test_batch_i]\n\t\t\ttest_batch_i += 1\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(test_pairs, test_batch_index, test_batch_index2)\n\t\t\toutput = model(left_sents, right_sents, lsize_list, rsize_list)\n\t\t\tresult = np.exp(output.data.cpu().numpy())\n\t\t\tloss = criterion(output, labels)\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = labels.data.cpu().numpy()\n\t\t\ttest_num_correct += np.sum(a == b)\n\t\t\tif task == \'pit\' or task == \'url\'or task==\'wikiqa\' or task==\'trecqa\':\n\t\t\t\tpred.extend(result[:, 1])\n\t\t\t\tgold.extend(b)\n\t\t\tif task == \'sts\':\n\t\t\t\tpred.extend(0 * result[:, 0] + 1 * result[:, 1] + 2 * result[:, 2] + 3 * result[:, 3] + 4 * result[:,4] + 5 * result[:, 5])\n\t\t\t\tgold.extend(0 * b[:, 0] + 1 * b[:, 1] + 2 * b[:, 2] + 3 * b[:, 3] + 4 * b[:, 4] + 5 * b[:, 5])\n\t\tmsg += \'\\t test loss: %f\' % accumulated_loss\n\t\ttest_acc = test_num_correct / len(test_pairs)\n\t\tmsg += \'\\t test accuracy: %f\' % test_acc\n\t\tprint(msg)\n\t\tif task == \'pit\' or task == \'url\'or task==\'wikiqa\' or task==\'trecqa\':\n\t\t\tURL_maxF1_eval(pred, gold)\n\t\t\tif task == \'wikiqa\' or task == \'trecqa\':\n\t\t\t\tlist1 = []\n\t\t\t\tfor line in open(basepath+\'/test.qrel\'):\n\t\t\t\t\tlist1.append(line.strip().split())\n\t\t\t\twith open(basepath + \'/result_\' + task, \'w\') as f:\n\t\t\t\t\tfor i in range(len(pred)):\n\t\t\t\t\t\tf.writelines(str(list1[i][0]) + \'\\t\' + str(list1[i][1]) + \'\\t\' + str(\n\t\t\t\t\t\t\tlist1[i][2]) + \'\\t\' + \'*\\t\' + str(pred[i]) + \'\\t\' + \'*\\n\')\n\t\telif task == \'sts\':\n\t\t\tprint(\'pearson: \' + str(pearson(pred, gold)))\n\t\telapsed_time = time.time() - start_time\n\t\tprint(\'Epoch \' + str(epoch) + \' finished within \' + str(timedelta(seconds=elapsed_time)))\n'"
DecAtt/main_snli.py,31,"b'from __future__ import division\n\nfrom model import *\nimport argparse\nimport sys\nfrom util import *\nimport time\nimport torch\nimport random\nimport logging\nimport numpy as np\nimport cPickle as pickle\nfrom datetime import timedelta\nfrom os.path import expanduser\nfrom torch.autograd import Variable\nfrom torchtext.vocab import load_word_vectors\n\ndef prepare_data(pairs, batch_size=32):\n\tbatch_list = []\n\tbatch_index = 0\n\twhile batch_index < len(pairs):\n\t\ttry:\n\t\t\tsubset = pairs[batch_index:batch_index + batch_size]\n\t\texcept:\n\t\t\tsubset = pairs[batch_index:]\n\t\ttmp_a = np.array([len(item[0]) for item in subset])\n\t\ttmp_b = np.array([len(item[1]) for item in subset])\n\t\tbatch_index2 = batch_index + min(len(np.where(tmp_a == tmp_a[0])[0]), len(np.where(tmp_b == tmp_b[0])[0]))\n\t\tbatch_list.append([batch_index, batch_index2])\n\t\tbatch_index = batch_index2\n\treturn batch_list\n\ndef create_batch(data,from_index, to_index):\n\tif to_index>len(data):\n\t\tto_index=len(data)\n\tlsize=0\n\trsize=0\n\tlsize_list=[]\n\trsize_list=[]\n\tfor i in range(from_index, to_index):\n\t\tlength=len(data[i][0])+2\n\t\tlsize_list.append(length)\n\t\tif length>lsize:\n\t\t\tlsize=length\n\t\tlength=len(data[i][1])+2\n\t\trsize_list.append(length)\n\t\tif length>rsize:\n\t\t\trsize=length\n\t#lsize+=1\n\t#rsize+=1\n\tlsent = data[from_index][0]\n\tlsent = [\'bos\']+lsent + [\'oov\' for k in range(lsize -1 - len(lsent))]\n\t#print(lsent)\n\tleft_sents = torch.cat((dict[word].view(1, -1) for word in lsent))\n\tleft_sents = torch.unsqueeze(left_sents,0)\n\n\trsent = data[from_index][1]\n\trsent = [\'bos\']+rsent + [\'oov\' for k in range(rsize -1 - len(rsent))]\n\t#print(rsent)\n\tright_sents = torch.cat((dict[word].view(1, -1) for word in rsent))\n\tright_sents = torch.unsqueeze(right_sents,0)\n\n\tlabels=[data[from_index][2]]\n\n\tfor i in range(from_index+1, to_index):\n\n\t\tlsent=data[i][0]\n\t\tlsent=[\'bos\']+lsent+[\'oov\' for k in range(lsize -1 - len(lsent))]\n\t\t#print(lsent)\n\t\tleft_sent = torch.cat((dict[word].view(1,-1) for word in lsent))\n\t\tleft_sent = torch.unsqueeze(left_sent, 0)\n\t\tleft_sents = torch.cat([left_sents, left_sent])\n\n\t\trsent=data[i][1]\n\t\trsent=[\'bos\']+rsent+[\'oov\' for k in range(rsize -1 - len(rsent))]\n\t\t#print(rsent)\n\t\tright_sent = torch.cat((dict[word].view(1,-1) for word in rsent))\n\t\tright_sent = torch.unsqueeze(right_sent, 0)\n\t\tright_sents = torch.cat((right_sents, right_sent))\n\n\t\tlabels.append(data[i][2])\n\n\tleft_sents=Variable(left_sents)\n\tright_sents=Variable(right_sents)\n\tif task==\'sts\':\n\t\tlabels=Variable(torch.Tensor(labels))\n\telse:\n\t\tlabels=Variable(torch.LongTensor(labels))\n\tlsize_list=Variable(torch.LongTensor(lsize_list))\n\trsize_list = Variable(torch.LongTensor(rsize_list))\n\n\tif torch.cuda.is_available():\n\t\tleft_sents=left_sents.cuda()\n\t\tright_sents=right_sents.cuda()\n\t\tlabels=labels.cuda()\n\t\tlsize_list=lsize_list.cuda()\n\t\trsize_list=rsize_list.cuda()\n\t#print(left_sents)\n\t#print(right_sents)\n\t#print(labels)\n\treturn left_sents, right_sents, labels, lsize_list, rsize_list\n\nif __name__ == \'__main__\':\n\ttask=\'snli\'\n\tprint(\'task: \'+task)\n\tprint(\'model: DecAtt\')\n\tEMBEDDING_DIM = 300\n\tPROJECTED_EMBEDDING_DIM = 300\n\n\tparser = argparse.ArgumentParser(description=__doc__)\n\tparser.add_argument(\'--e\', dest=\'num_epochs\', default=5000, type=int, help=\'Number of epochs\')\n\tparser.add_argument(\'--b\', dest=\'batch_size\', default=32, help=\'Batch size\', type=int)\n\tparser.add_argument(\'--u\', dest=\'num_units\', help=\'Number of hidden units\', default=100, type=int)\n\tparser.add_argument(\'--r\', help=\'Learning rate\', type=float, default=0.05, dest=\'rate\')\n\tparser.add_argument(\'--lower\', help=\'Lowercase the corpus\', default=True, action=\'store_true\')\n\tparser.add_argument(\'--model\', help=\'Model selection\', default=\'DecAtt\', type=str)\n\tparser.add_argument(\'--optim\', help=\'Optimizer algorithm\', default=\'adagrad\', choices=[\'adagrad\', \'adadelta\', \'adam\'])\n\tparser.add_argument(\'--max_grad_norm\', help=\'If the norm of the gradient vector exceeds this renormalize it\\\n\t\t\t\t\t\t\t\t\t   to have the norm equal to max_grad_norm\', type=float, default=5)\n\tif task==\'snli\':\n\t\tnum_class=3\n\t\tparser.add_argument(\'--train\', help=\'JSONL or TSV file with training corpus\', default=\'/data/snli_1.0_train.jsonl\')\n\t\tparser.add_argument(\'--dev\', help=\'JSONL or TSV file with development corpus\', default=\'/data/snli_1.0_dev.jsonl\')\n\t\tparser.add_argument(\'--test\', help=\'JSONL or TSV file with testing corpus\', default=\'/data/snli_1.0_test.jsonl\')\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DecAtt/data/snli\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DecAtt/data/snli\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\ttrain_pairs = pickle.load(open(basepath + ""/train_pairs.p"", ""rb""))\n\t\tdev_pairs = pickle.load(open(basepath + ""/dev_pairs.p"", ""rb""))\n\t\ttest_pairs = pickle.load(open(basepath + ""/test_pairs.p"", ""rb""))\n\telif task==\'mnli\':\n\t\tnum_class = 3\n\t\tparser.add_argument(\'--train\', help=\'JSONL or TSV file with training corpus\', default=\'/data/mnli/multinli_1.0_train.jsonl\')\n\t\tparser.add_argument(\'--dev_m\', help=\'JSONL or TSV file with development corpus\', default=\'/data/mnli/multinli_1.0_dev_matched.jsonl\')\n\t\tparser.add_argument(\'--dev_um\', help=\'JSONL or TSV file with development corpus\',\n\t\t                   default=\'/data/mnli/multinli_1.0_dev_mismatched.jsonl\')\n\t\tparser.add_argument(\'--test_m\', help=\'JSONL or TSV file with testing corpus\', default=\'/data/mnli/multinli_1.0_test_matched.jsonl\')\n\t\tparser.add_argument(\'--test_um\', help=\'JSONL or TSV file with testing corpus\',\n\t\t                    default=\'/data/mnli/multinli_1.0_test_mismatched.jsonl\')\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DecAtt/data/mnli\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DecAtt/data/mnli\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\t\'\'\'\n\t\ttrain_pairs = util.read_corpus(expanduser(""~"")+\'/Documents/research/pytorch/DeepPairWiseWord\' + args.train, True)\n\t\tdev_pairs_m = util.read_corpus(expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord\' + args.dev_m, True)\n\t\tdev_pairs_um = util.read_corpus(expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord\' + args.dev_um, True)\n\t\ttest_pairs_m = util.read_corpus(expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord\' + args.test_m, True)\n\t\ttest_pairs_um = util.read_corpus(expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord\' + args.test_um, True)\n\t\tpickle.dump(train_pairs, open(""data/mnli/train_pairs.p"", ""wb""))\n\t\tpickle.dump(dev_pairs_m, open(""data/mnli/dev_pairs_m.p"", ""wb""))\n\t\tpickle.dump(dev_pairs_um, open(""data/mnli/dev_pairs_um.p"", ""wb""))\n\t\tpickle.dump(test_pairs_m, open(""data/mnli/test_pairs_m.p"", ""wb""))\n\t\tpickle.dump(test_pairs_um, open(""data/mnli/test_pairs_um.p"", ""wb""))\n\t\t\'\'\'\n\t\ttrain_pairs = pickle.load(open(basepath + ""/train_pairs.p"", ""rb""))\n\t\tdev_pairs_m = pickle.load(open(basepath + ""/dev_pairs_m.p"", ""rb""))\n\t\tdev_pairs_um = pickle.load(open(basepath + ""/dev_pairs_um.p"", ""rb""))\n\t\ttest_pairs_m = pickle.load(open(basepath + ""/test_pairs_m.p"", ""rb""))\n\t\ttest_pairs_um = pickle.load(open(basepath + ""/test_pairs_um.p"", ""rb""))\n\telif task==\'quora\':\n\t\tnum_class = 2\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/quora\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/quora\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\ttrain_pairs = readQuoradata(basepath+\'/train/\')\n\t\tdev_pairs=readQuoradata(basepath+\'/dev/\')\n\t\ttest_pairs=readQuoradata(basepath+\'/test/\')\n\telif task==\'url\':\n\t\tnum_class = 2\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/url\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/url\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\ttrain_pairs = readQuoradata(basepath + \'/train/\')\n\t\tdev_pairs = None#readQuoradata(basepath + \'/dev/\')\n\t\ttest_pairs = readQuoradata(basepath + \'/test_9324/\')\n\t\tif dev_pairs==None:\n\t\t\tdev_pairs=test_pairs\n\telif task==\'pit\':\n\t\tnum_class = 2\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/pit\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/pit\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\ttrain_pairs = readQuoradata(basepath + \'/train/\')\n\t\ttest_pairs = readQuoradata(basepath + \'/test/\')\n\t\tdev_pairs=test_pairs\n\telif task==\'sts\':\n\t\tnum_class = 6\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/sts\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/sts\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\ttrain_pairs = readSTSdata(basepath + \'/train/\')\n\t\ttest_pairs = readSTSdata(basepath + \'/test/\')\n\t\tdev_pairs=test_pairs\n\telif task==\'wikiqa\':\n\t\tnum_class=2\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/wikiqa\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/wikiqa\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\t\ttrain_pairs = readQuoradata(basepath + \'/train/\')\n\t\t\tdev_pairs = readQuoradata(basepath + \'/dev/\')\n\t\t\ttest_pairs = readQuoradata(basepath + \'/test/\')\n\telif task==\'trecqa\':\n\t\tnum_class = 2\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/trecqa\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/trecqa\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\t\ttrain_pairs = readQuoradata(basepath + \'/train-all/\')\n\t\t\tdev_pairs = readQuoradata(basepath + \'/raw-dev/\')\n\t\t\ttest_pairs = readQuoradata(basepath + \'/raw-test/\')\n\t#sys.exit()\n\targs = parser.parse_args()\n\tprint(\'Read data ...\')\n\tprint(\'Number of training pairs: %d\' % len(train_pairs))\n\tprint(\'Number of development pairs: %d\' % len(dev_pairs))\n\tprint(\'Number of testing pairs: %d\' % len(test_pairs))\n\tbatch_size = args.batch_size\n\tnum_epochs = args.num_epochs\n\t\'\'\'\n\tvocab=set()\n\tfor pair in train_pairs:\n\t\tleft=pair[0]\n\t\tright=pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in dev_pairs_m:\n\t\tleft=pair[0]\n\t\tright=pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in dev_pairs_um:\n\t\tleft=pair[0]\n\t\tright=pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in test_pairs_m:\n\t\tleft=pair[0]\n\t\tright=pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in test_pairs_um:\n\t\tleft=pair[0]\n\t\tright=pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\twith open(\'vocab.txt\', \'w\') as f:\n\t\tfor word in vocab:\n\t\t\tf.write(word.encode(\'utf-8\')+\'\\n\')\n\tsys.exit()\n\t\'\'\'\n\ttokens = []\n\tdict={}\n\tword2id={}\n\tvocab = set()\n\tfor pair in train_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in dev_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in test_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\ttokens=list(vocab)\n\t#for line in open(basepath + \'/vocab.txt\'):\n\t#\ttokens.append(line.strip().decode(\'utf-8\'))\n\twv_dict, wv_arr, wv_size = load_word_vectors(embedding_path, \'glove.840B\', EMBEDDING_DIM)\n\t#embedding = []\n\ttokens.append(\'oov\')\n\ttokens.append(\'bos\')\n\t#embedding.append(dict[word].numpy())\n\t#print(len(embedding))\n\t#np.save(\'embedding\',np.array(embedding))\n\t#sys.exit()\n\tpretrained_emb = np.zeros(shape=(len(tokens), EMBEDDING_DIM))\n\toov={}\n\tfor id in range(100):\n\t\toov[id]=torch.normal(torch.zeros(EMBEDDING_DIM),std=1)\n\tid=0\n\tfor word in tokens:\n\t\ttry:\n\t\t\tdict[word] = wv_arr[wv_dict[word]]/torch.norm(wv_arr[wv_dict[word]])\n\t\texcept:\n\t\t\t#if args.model==\'DecAtt\':\n\t\t\t#\tdict[word]=oov[np.random.randint(100)]\n\t\t\t#else:\n\t\t\tdict[word] = torch.normal(torch.zeros(EMBEDDING_DIM),std=1)\n\t\tword2id[word]=id\n\t\tpretrained_emb[id] = dict[word].numpy()\n\t\tid+=1\n\n\tif task==\'sts\':\n\t\tcriterion = nn.KLDivLoss()\n\telse:\n\t\tcriterion = torch.nn.NLLLoss(size_average=True)\n\t#criterion = torch.nn.CrossEntropyLoss()\n\tmodel=DecAtt(200,num_class,len(tokens),EMBEDDING_DIM, PROJECTED_EMBEDDING_DIM, pretrained_emb)\n\tif torch.cuda.is_available():\n\t\tmodel = model.cuda()\n\t\tcriterion = criterion.cuda()\n\toptimizer = torch.optim.Adagrad(model.parameters(), lr=0.05, weight_decay=5e-5)\n\n\tprint(\'Start training...\')\n\tbatch_counter = 0\n\tbest_dev_loss=10e10\n\tbest_dev_loss_m=10e10\n\tbest_dev_loss_um=10e10\n\tbest_result=0\n\taccumulated_loss=0\n\treport_interval = 1000\n\tmodel.train()\n\t\'\'\'\n\ttrain_pairs = np.array(train_pairs)\n\tboth_lengths = np.array([(len(train_pairs[i][0]), len(train_pairs[i][1])) for i in range(len(train_pairs))],\n\t                        dtype={\'names\': [\'x\', \'y\'], \'formats\': [\'i4\', \'i4\']})\n\tfor epoch in range(num_epochs):\n\t\trand_idx = np.random.permutation(len(train_pairs))\n\t\ttrain_pairs = train_pairs[rand_idx]\n\t\tboth_lengths = both_lengths[rand_idx]\n\t\tsorted_lengths = np.argsort(both_lengths, order=(\'x\', \'y\'))\n\t\ttrain_pairs=train_pairs[sorted_lengths]\n\t\tboth_lengths=both_lengths[sorted_lengths]\n\t\tprint(\'--\' * 20)\n\t\tbatch_index = 0\n\t\tbatch_counter=0\n\t\twhile batch_index < len(train_pairs):\n\t\t\tbatch_index2=batch_index+batch_size\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(train_pairs, batch_index,\n\t\t\t                                                                       batch_index2)\n\t\t\tbatch_index = batch_index2\n\t\t\toptimizer.zero_grad()\n\t\t\toutput = model(left_sents, right_sents, lsize_list, rsize_list)\n\t\t\t# print(\'forward finished\'+str(datetime.now()))\n\t\t\tloss = criterion(output, labels)\n\t\t\tloss.backward()\n\t\t\toptimizer.step()\n\t\t\t# print(\'backword finished\' + str(datetime.now()))\n\t\t\tbatch_counter += 1\n\t\t\t# print(batch_counter, loss.data[0])\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\tif batch_counter % report_interval == 0:\n\t\t\t\tmodel.eval()\n\t\t\t\tdev_batch_index = 0\n\t\t\t\tdev_num_correct = 0\n\t\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\t\tmsg += \'\\t training batch loss: %f\' % accumulated_loss\n\t\t\t\taccumulated_loss = 0\n\t\t\t\twhile dev_batch_index<len(test_pairs):\n\t\t\t\t\tdev_batch_index2=dev_batch_index+1\n\t\t\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(test_pairs, dev_batch_index,\n\t\t\t\t\t                                                                       dev_batch_index2)\n\t\t\t\t\tdev_batch_index=dev_batch_index2\n\t\t\t\t\toutput = model(left_sents, right_sents, lsize_list, rsize_list)\n\t\t\t\t\tresult = output.data.cpu().numpy()\n\t\t\t\t\tloss = criterion(output, labels)\n\t\t\t\t\taccumulated_loss += loss.data[0]\n\t\t\t\t\ta = np.argmax(result, axis=1)\n\t\t\t\t\tb = labels.data.cpu().numpy()\n\t\t\t\t\tdev_num_correct += np.sum(a == b)\n\t\t\t\tmsg += \'\\t dev loss: %f\' % accumulated_loss\n\t\t\t\tdev_acc = dev_num_correct / len(test_pairs)\n\t\t\t\tmsg += \'\\t dev accuracy: %f\' % dev_acc\n\t\t\t\tprint(msg)\n\t\t\t\taccumulated_loss = 0\n\t\t\tmodel.train()\n\t\'\'\'\n\ttrain_pairs=np.array(train_pairs)\n\trand_idx = np.random.permutation(len(train_pairs))\n\ttrain_pairs = train_pairs[rand_idx]\n\tboth_lengths = np.array([(len(train_pairs[i][0]), len(train_pairs[i][1])) for i in range(len(train_pairs))],\n\t                        dtype={\'names\': [\'x\', \'y\'], \'formats\': [\'i4\', \'i4\']})\n\tsorted_lengths = np.argsort(both_lengths, order=(\'x\', \'y\'))\n\ttrain_pairs = train_pairs[sorted_lengths]\n\ttrain_batch_list=prepare_data(train_pairs)\n\n\n\tdev_pairs=np.array(dev_pairs)\n\tboth_lengths = np.array([(len(dev_pairs[i][0]), len(dev_pairs[i][1])) for i in range(len(dev_pairs))],\n\t                        dtype={\'names\': [\'x\', \'y\'], \'formats\': [\'i4\', \'i4\']})\n\tsorted_lengths = np.argsort(both_lengths, order=(\'x\', \'y\'))\n\tdev_pairs = dev_pairs[sorted_lengths]\n\tdev_batch_list=prepare_data(dev_pairs)\n\n\ttest_batch_list=prepare_data(test_pairs, batch_size=1)\n\t\'\'\'\n\n\tdev_pairs_m = np.array(dev_pairs_m)\n\tboth_lengths = np.array([(len(dev_pairs_m[i][0]), len(dev_pairs_m[i][1])) for i in range(len(dev_pairs_m))],\n\t                        dtype={\'names\': [\'x\', \'y\'], \'formats\': [\'i4\', \'i4\']})\n\tsorted_lengths = np.argsort(both_lengths, order=(\'x\', \'y\'))\n\tdev_pairs_m = dev_pairs_m[sorted_lengths]\n\tdev_batch_list_m = prepare_data(dev_pairs_m)\n\n\tdev_pairs_um = np.array(dev_pairs_um)\n\tboth_lengths = np.array([(len(dev_pairs_um[i][0]), len(dev_pairs_um[i][1])) for i in range(len(dev_pairs_um))],\n\t                        dtype={\'names\': [\'x\', \'y\'], \'formats\': [\'i4\', \'i4\']})\n\tsorted_lengths = np.argsort(both_lengths, order=(\'x\', \'y\'))\n\tdev_pairs_um = dev_pairs_um[sorted_lengths]\n\tdev_batch_list_um = prepare_data(dev_pairs_um)\n\t\'\'\'\n\tbatch_index=0\n\ttrain_rand_i = np.random.permutation(len(train_batch_list))\n\tfor epoch in range(num_epochs):\n\t\tbatch_counter=0\n\t\taccumulated_loss = 0\n\t\tmodel.train()\n\t\tprint(\'--\' * 20)\n\t\tstart_time = time.time()\n\t\ttrain_batch_i=0\n\t\ttrain_sents_scaned=0\n\t\ttrain_num_correct=0\n\t\twhile train_batch_i<len(train_batch_list):\n\t\t\tbatch_index, batch_index2=train_batch_list[train_rand_i[train_batch_i]]\n\t\t\ttrain_batch_i+=1\n\t\t\t#batch_index2=batch_index+batch_size\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(train_pairs, batch_index, batch_index2)\n\t\t\ttrain_sents_scaned+=len(labels)\n\t\t\t#print(lsize_list)\n\t\t\t#print(rsize_list)\n\t\t\t#batch_index=batch_index2\n\t\t\toptimizer.zero_grad()\n\n\t\t\toutput = model(left_sents, right_sents, lsize_list, rsize_list)\n\t\t\tresult = output.data.cpu().numpy()\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = labels.data.cpu().numpy()\n\t\t\ttrain_num_correct += np.sum(a == b)\n\t\t\t#print(\'forward finished\'+str(datetime.now()))\n\t\t\t#print(output)\n\t\t\t#print(labels)\n\t\t\t#sys.exit()\n\t\t\tloss = criterion(output, labels)\n\t\t\tloss.backward()\n\n\t\t\t\'\'\'\'\'\'\n\t\t\tgrad_norm = 0.\n\t\t\t#para_norm = 0.\n\n\t\t\tfor m in model.modules():\n\t\t\t\tif isinstance(m, nn.Linear):\n\t\t\t\t\t# print(m)\n\t\t\t\t\tgrad_norm += m.weight.grad.data.norm() ** 2\n\t\t\t\t\t#para_norm += m.weight.data.norm() ** 2\n\t\t\t\t\tif m.bias is not None:\n\t\t\t\t\t\tgrad_norm += m.bias.grad.data.norm() ** 2\n\t\t\t\t\t\t#para_norm += m.bias.data.norm() ** 2\n\n\t\t\tgrad_norm ** 0.5\n\t\t\t#para_norm ** 0.5\n\n\t\t\ttry:\n\t\t\t\tshrinkage = args.max_grad_norm / grad_norm\n\t\t\texcept:\n\t\t\t\tpass\n\t\t\tif shrinkage < 1:\n\t\t\t\tfor m in model.modules():\n\t\t\t\t\t# print m\n\t\t\t\t\tif isinstance(m, nn.Linear):\n\t\t\t\t\t\tm.weight.grad.data = m.weight.grad.data * shrinkage\n\t\t\t\t\t\tif m.bias is not None:\n\t\t\t\t\t\t\tm.bias.grad.data = m.bias.grad.data * shrinkage\n\t\t\t\'\'\'\'\'\'\n\t\t\toptimizer.step()\n\t\t\t#print(\'backword finished\' + str(datetime.now()))\n\t\t\tbatch_counter += 1\n\t\t\t#print(batch_counter, loss.data[0])\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\tif batch_counter % report_interval ==0:\n\t\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\t\tmsg += \'\\t train batch loss: %f\' % (accumulated_loss/train_sents_scaned)\n\t\t\t\tmsg += \'\\t train accuracy: %f\' % (train_num_correct/train_sents_scaned)\n\t\t\t\tprint(msg)\n\t\t\t#if batch_counter>(int(len(train_batch_list)/2)):\n\t\t\t#\tbreak\n\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\tmsg += \'\\t train batch loss: %f\' % (accumulated_loss / train_sents_scaned)\n\t\tmsg += \'\\t train accuracy: %f\' % (train_num_correct / train_sents_scaned)\n\t\tprint(msg)\n\t\t# valid after each epoch\n\t\tmodel.eval()\n\t\tdev_batch_index = 0\n\t\tdev_num_correct = 0\n\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\taccumulated_loss = 0\n\t\tdev_batch_i = 0\n\t\tpred=[]\n\t\tgold=[]\n\t\twhile dev_batch_i < len(dev_batch_list):\n\t\t\tdev_batch_index, dev_batch_index2 = dev_batch_list[dev_batch_i]\n\t\t\tdev_batch_i += 1\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(dev_pairs, dev_batch_index,\n\t\t\t                                                                       dev_batch_index2)\n\t\t\t# left_sents, right_sents, labels = create_batch(dev_pairs, 0, len(dev_pairs))\n\t\t\toutput = model(left_sents, right_sents, lsize_list, rsize_list)\n\t\t\tresult = output.data.cpu().numpy()\n\t\t\tloss = criterion(output, labels)\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = labels.data.cpu().numpy()\n\t\t\tdev_num_correct += np.sum(a == b)\n\t\t\tif task==\'pit\' or task==\'url\' or task==\'wikiqa\' or task==\'trecqa\':\n\t\t\t\tpred.extend(result[:,1])\n\t\t\t\tgold.extend(b)\n\t\t\tif task==\'sts\':\n\t\t\t\tpred.extend(0*result[:,0]+1*result[:,1]+2*result[:,2]+3*result[:,3]+4*result[:,4]+5*result[:,5])\n\t\t\t\tgold.extend(0*b[:,0]+1*b[:,1]+2*b[:,2]+3*b[:,3]+4*b[:,4]+5*b[:,5])\n\t\tmsg += \'\\t dev loss: %f\' % accumulated_loss\n\t\tdev_acc = dev_num_correct / len(dev_pairs)\n\t\tmsg += \'\\t dev accuracy: %f\' % dev_acc\n\t\tprint(msg)\n\t\tif task==\'pit\' or task==\'url\'or task==\'wikiqa\' or task==\'trecqa\':\n\t\t\tURL_maxF1_eval(pred, gold)\n\t\telif task==\'sts\':\n\t\t\tprint(\'pearson: \'+str(pearson(pred,gold)))\n\t\t# test after each epoch\n\t\ttest_batch_index = 0\n\t\ttest_num_correct = 0\n\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\taccumulated_loss = 0\n\t\ttest_batch_i = 0\n\t\tpred = []\n\t\tgold = []\n\t\twhile test_batch_i < len(test_batch_list):\n\t\t\ttest_batch_index, test_batch_index2 = test_batch_list[test_batch_i]\n\t\t\ttest_batch_i += 1\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(test_pairs, test_batch_index, test_batch_index2)\n\t\t\toutput = model(left_sents, right_sents, lsize_list, rsize_list)\n\t\t\tresult = np.exp(output.data.cpu().numpy())\n\t\t\tloss = criterion(output, labels)\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = labels.data.cpu().numpy()\n\t\t\ttest_num_correct += np.sum(a == b)\n\t\t\tpred.extend(result)\n\t\t\tgold.extend(b)\n\t\tmsg += \'\\t test loss: %f\' % accumulated_loss\n\t\ttest_acc = test_num_correct / len(test_pairs)\n\t\tmsg += \'\\t test accuracy: %f\' % test_acc\n\t\tprint(msg)\n\t\tif test_acc>best_result:\n\t\t\tbest_result=test_acc\n\t\t\twith open(basepath + \'/DecAtt_snli_prob.txt\', \'w\') as f:\n\t\t\t\tfor item in pred:\n\t\t\t\t\tf.writelines(str(item[0])+\'\\t\'+str(item[1])+\'\\t\'+str(item[2])+\'\\n\')\n\t\telapsed_time = time.time() - start_time\n\t\tprint(\'Epoch \' + str(epoch) + \' finished within \' + str(timedelta(seconds=elapsed_time)))\n'"
DecAtt/main_sts.py,34,"b'from __future__ import division\n\nfrom datetime import datetime\nfrom torch.autograd import Variable\nfrom model import *\nimport argparse\nimport sys\nfrom util import *\nimport time\nimport torch\nimport random\nimport logging\nimport numpy as np\nimport cPickle as pickle\nfrom datetime import timedelta\nfrom os.path import expanduser\nfrom torch.autograd import Variable\nfrom torchtext.vocab import load_word_vectors\n\ndef prepare_data(pairs):\n\tbatch_list = []\n\tbatch_index = 0\n\twhile batch_index < len(pairs):\n\t\ttry:\n\t\t\tsubset = pairs[batch_index:batch_index + batch_size]\n\t\texcept:\n\t\t\tsubset = pairs[batch_index:]\n\t\ttmp_a = np.array([len(item[0]) for item in subset])\n\t\ttmp_b = np.array([len(item[1]) for item in subset])\n\t\tbatch_index2 = batch_index + min(len(np.where(tmp_a == tmp_a[0])[0]), len(np.where(tmp_b == tmp_b[0])[0]))\n\t\tbatch_list.append([batch_index, batch_index2])\n\t\tbatch_index = batch_index2\n\treturn batch_list\n\ndef create_batch(data,from_index, to_index):\n\tif to_index>len(data):\n\t\tto_index=len(data)\n\tlsize=0\n\trsize=0\n\tlsize_list=[]\n\trsize_list=[]\n\tfor i in range(from_index, to_index):\n\t\tlength=len(data[i][0])+2\n\t\tlsize_list.append(length)\n\t\tif length>lsize:\n\t\t\tlsize=length\n\t\tlength=len(data[i][1])+2\n\t\trsize_list.append(length)\n\t\tif length>rsize:\n\t\t\trsize=length\n\t#lsize+=1\n\t#rsize+=1\n\tlsent = data[from_index][0]\n\tlsent = [\'bos\']+lsent + [\'oov\' for k in range(lsize -1 - len(lsent))]\n\t#print(lsent)\n\tleft_sents = torch.cat((dict[word].view(1, -1) for word in lsent))\n\tleft_sents = torch.unsqueeze(left_sents,0)\n\n\trsent = data[from_index][1]\n\trsent = [\'bos\']+rsent + [\'oov\' for k in range(rsize -1 - len(rsent))]\n\t#print(rsent)\n\tright_sents = torch.cat((dict[word].view(1, -1) for word in rsent))\n\tright_sents = torch.unsqueeze(right_sents,0)\n\n\tlabels=[data[from_index][2]]\n\n\tfor i in range(from_index+1, to_index):\n\n\t\tlsent=data[i][0]\n\t\tlsent=[\'bos\']+lsent+[\'oov\' for k in range(lsize -1 - len(lsent))]\n\t\t#print(lsent)\n\t\tleft_sent = torch.cat((dict[word].view(1,-1) for word in lsent))\n\t\tleft_sent = torch.unsqueeze(left_sent, 0)\n\t\tleft_sents = torch.cat([left_sents, left_sent])\n\n\t\trsent=data[i][1]\n\t\trsent=[\'bos\']+rsent+[\'oov\' for k in range(rsize -1 - len(rsent))]\n\t\t#print(rsent)\n\t\tright_sent = torch.cat((dict[word].view(1,-1) for word in rsent))\n\t\tright_sent = torch.unsqueeze(right_sent, 0)\n\t\tright_sents = torch.cat((right_sents, right_sent))\n\n\t\tlabels.append(data[i][2])\n\n\tleft_sents=Variable(left_sents)\n\tright_sents=Variable(right_sents)\n\tif task==\'sts\':\n\t\tlabels=Variable(torch.Tensor(labels))\n\telse:\n\t\tlabels=Variable(torch.LongTensor(labels))\n\tlsize_list=Variable(torch.LongTensor(lsize_list))\n\trsize_list = Variable(torch.LongTensor(rsize_list))\n\n\tif torch.cuda.is_available():\n\t\tleft_sents=left_sents.cuda()\n\t\tright_sents=right_sents.cuda()\n\t\tlabels=labels.cuda()\n\t\tlsize_list=lsize_list.cuda()\n\t\trsize_list=rsize_list.cuda()\n\t#print(left_sents)\n\t#print(right_sents)\n\t#print(labels)\n\treturn left_sents, right_sents, labels, lsize_list, rsize_list\n\nif __name__ == \'__main__\':\n\ttask=\'sts\'\n\tprint(\'task: \'+task)\n\tEMBEDDING_DIM = 300\n\tPROJECTED_EMBEDDING_DIM = 300\n\n\tparser = argparse.ArgumentParser(description=__doc__)\n\tparser.add_argument(\'--e\', dest=\'num_epochs\', default=5000, type=int, help=\'Number of epochs\')\n\tparser.add_argument(\'--b\', dest=\'batch_size\', default=32, help=\'Batch size\', type=int)\n\tparser.add_argument(\'--u\', dest=\'num_units\', help=\'Number of hidden units\', default=100, type=int)\n\tparser.add_argument(\'--r\', help=\'Learning rate\', type=float, default=0.05, dest=\'rate\')\n\tparser.add_argument(\'--lower\', help=\'Lowercase the corpus\', default=True, action=\'store_true\')\n\tparser.add_argument(\'--model\', help=\'Model selection\', default=\'DecAtt\', type=str)\n\tparser.add_argument(\'--optim\', help=\'Optimizer algorithm\', default=\'adagrad\', choices=[\'adagrad\', \'adadelta\', \'adam\'])\n\tparser.add_argument(\'--max_grad_norm\', help=\'If the norm of the gradient vector exceeds this renormalize it\\\n\t\t\t\t\t\t\t\t\t   to have the norm equal to max_grad_norm\', type=float, default=5)\n\tif task==\'snli\':\n\t\tnum_class=3\n\t\tparser.add_argument(\'--train\', help=\'JSONL or TSV file with training corpus\', default=\'/data/snli_1.0_train.jsonl\')\n\t\tparser.add_argument(\'--dev\', help=\'JSONL or TSV file with development corpus\', default=\'/data/snli_1.0_dev.jsonl\')\n\t\tparser.add_argument(\'--test\', help=\'JSONL or TSV file with testing corpus\', default=\'/data/snli_1.0_test.jsonl\')\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DecAtt/data/snli\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DecAtt/data/snli\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\ttrain_pairs = pickle.load(open(basepath + ""/train_pairs.p"", ""rb""))\n\t\tdev_pairs = pickle.load(open(basepath + ""/dev_pairs.p"", ""rb""))\n\t\ttest_pairs = pickle.load(open(basepath + ""/test_pairs.p"", ""rb""))\n\telif task==\'mnli\':\n\t\tnum_class = 3\n\t\tparser.add_argument(\'--train\', help=\'JSONL or TSV file with training corpus\', default=\'/data/mnli/multinli_1.0_train.jsonl\')\n\t\tparser.add_argument(\'--dev_m\', help=\'JSONL or TSV file with development corpus\', default=\'/data/mnli/multinli_1.0_dev_matched.jsonl\')\n\t\tparser.add_argument(\'--dev_um\', help=\'JSONL or TSV file with development corpus\',\n\t\t                   default=\'/data/mnli/multinli_1.0_dev_mismatched.jsonl\')\n\t\tparser.add_argument(\'--test_m\', help=\'JSONL or TSV file with testing corpus\', default=\'/data/mnli/multinli_1.0_test_matched.jsonl\')\n\t\tparser.add_argument(\'--test_um\', help=\'JSONL or TSV file with testing corpus\',\n\t\t                    default=\'/data/mnli/multinli_1.0_test_mismatched.jsonl\')\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DecAtt/data/mnli\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DecAtt/data/mnli\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\t\'\'\'\n\t\ttrain_pairs = util.read_corpus(expanduser(""~"")+\'/Documents/research/pytorch/DeepPairWiseWord\' + args.train, True)\n\t\tdev_pairs_m = util.read_corpus(expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord\' + args.dev_m, True)\n\t\tdev_pairs_um = util.read_corpus(expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord\' + args.dev_um, True)\n\t\ttest_pairs_m = util.read_corpus(expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord\' + args.test_m, True)\n\t\ttest_pairs_um = util.read_corpus(expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord\' + args.test_um, True)\n\t\tpickle.dump(train_pairs, open(""data/mnli/train_pairs.p"", ""wb""))\n\t\tpickle.dump(dev_pairs_m, open(""data/mnli/dev_pairs_m.p"", ""wb""))\n\t\tpickle.dump(dev_pairs_um, open(""data/mnli/dev_pairs_um.p"", ""wb""))\n\t\tpickle.dump(test_pairs_m, open(""data/mnli/test_pairs_m.p"", ""wb""))\n\t\tpickle.dump(test_pairs_um, open(""data/mnli/test_pairs_um.p"", ""wb""))\n\t\t\'\'\'\n\t\ttrain_pairs = pickle.load(open(basepath + ""/train_pairs.p"", ""rb""))\n\t\tdev_pairs_m = pickle.load(open(basepath + ""/dev_pairs_m.p"", ""rb""))\n\t\tdev_pairs_um = pickle.load(open(basepath + ""/dev_pairs_um.p"", ""rb""))\n\t\ttest_pairs_m = pickle.load(open(basepath + ""/test_pairs_m.p"", ""rb""))\n\t\ttest_pairs_um = pickle.load(open(basepath + ""/test_pairs_um.p"", ""rb""))\n\telif task==\'quora\':\n\t\tnum_class = 2\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/quora\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/quora\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\ttrain_pairs = readQuoradata(basepath+\'/train/\')\n\t\tdev_pairs=readQuoradata(basepath+\'/dev/\')\n\t\ttest_pairs=readQuoradata(basepath+\'/test/\')\n\telif task==\'url\':\n\t\tnum_class = 2\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/url\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/url\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\ttrain_pairs = readQuoradata(basepath + \'/train/\')\n\t\tdev_pairs = None#readQuoradata(basepath + \'/dev/\')\n\t\ttest_pairs = readQuoradata(basepath + \'/test_9324/\')\n\t\tif dev_pairs==None:\n\t\t\tdev_pairs=test_pairs\n\telif task==\'pit\':\n\t\tnum_class = 2\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/pit\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/pit\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\ttrain_pairs = readQuoradata(basepath + \'/train/\')\n\t\ttest_pairs = readQuoradata(basepath + \'/test/\')\n\t\tdev_pairs=test_pairs\n\telif task==\'sts\':\n\t\tnum_class = 6\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/sts\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/sts\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\ttrain_pairs = readSTSdata(basepath + \'/train/\')\n\t\ttest_pairs = readSTSdata(basepath + \'/test/\')\n\t\tdev_pairs=test_pairs\n\telif task==\'wikiqa\':\n\t\tnum_class=2\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/wikiqa\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/wikiqa\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\t\ttrain_pairs = readQuoradata(basepath + \'/train/\')\n\t\t\tdev_pairs = readQuoradata(basepath + \'/dev/\')\n\t\t\ttest_pairs = readQuoradata(basepath + \'/test/\')\n\telif task==\'trecqa\':\n\t\tnum_class = 2\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/trecqa\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/trecqa\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\t\ttrain_pairs = readQuoradata(basepath + \'/train-all/\')\n\t\t\tdev_pairs = readQuoradata(basepath + \'/raw-dev/\')\n\t\t\ttest_pairs = readQuoradata(basepath + \'/raw-test/\')\n\t#sys.exit()\n\targs = parser.parse_args()\n\tprint(\'Model: %s\' % args.model)\n\tprint(\'Read data ...\')\n\tprint(\'Number of training pairs: %d\' % len(train_pairs))\n\t#print(\'Number of development pairs: %d\' % len(dev_pairs))\n\t#print(\'Number of testing pairs: %d\' % len(test_pairs))\n\tbatch_size = args.batch_size\n\tnum_epochs = args.num_epochs\n\t\'\'\'\n\tvocab=set()\n\tfor pair in train_pairs:\n\t\tleft=pair[0]\n\t\tright=pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in dev_pairs_m:\n\t\tleft=pair[0]\n\t\tright=pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in dev_pairs_um:\n\t\tleft=pair[0]\n\t\tright=pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in test_pairs_m:\n\t\tleft=pair[0]\n\t\tright=pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in test_pairs_um:\n\t\tleft=pair[0]\n\t\tright=pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\twith open(\'vocab.txt\', \'w\') as f:\n\t\tfor word in vocab:\n\t\t\tf.write(word.encode(\'utf-8\')+\'\\n\')\n\tsys.exit()\n\t\'\'\'\n\ttokens = []\n\tdict={}\n\tword2id={}\n\tvocab = set()\n\tfor pair in train_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in dev_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in test_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\ttokens=list(vocab)\n\t#for line in open(basepath + \'/vocab.txt\'):\n\t#\ttokens.append(line.strip().decode(\'utf-8\'))\n\twv_dict, wv_arr, wv_size = load_word_vectors(embedding_path, \'glove.840B\', EMBEDDING_DIM)\n\t#embedding = []\n\ttokens.append(\'oov\')\n\ttokens.append(\'bos\')\n\t#embedding.append(dict[word].numpy())\n\t#print(len(embedding))\n\t#np.save(\'embedding\',np.array(embedding))\n\t#sys.exit()\n\tpretrained_emb = np.zeros(shape=(len(tokens), EMBEDDING_DIM))\n\toov={}\n\tfor id in range(100):\n\t\toov[id]=torch.normal(torch.zeros(EMBEDDING_DIM),std=1)\n\tid=0\n\tfor word in tokens:\n\t\ttry:\n\t\t\tdict[word] = wv_arr[wv_dict[word]]/torch.norm(wv_arr[wv_dict[word]])\n\t\texcept:\n\t\t\t#if args.model==\'DecAtt\':\n\t\t\t#\tdict[word]=oov[np.random.randint(100)]\n\t\t\t#else:\n\t\t\tdict[word] = torch.normal(torch.zeros(EMBEDDING_DIM),std=1)\n\t\tword2id[word]=id\n\t\tpretrained_emb[id] = dict[word].numpy()\n\t\tid+=1\n\n\tif task==\'sts\':\n\t\tcriterion = nn.KLDivLoss()\n\telse:\n\t\tcriterion = torch.nn.NLLLoss(size_average=True)\n\t#criterion = torch.nn.CrossEntropyLoss()\n\tmodel=DecAtt(200,num_class,len(tokens),EMBEDDING_DIM, PROJECTED_EMBEDDING_DIM, pretrained_emb)\n\tif torch.cuda.is_available():\n\t\tmodel = model.cuda()\n\t\tcriterion = criterion.cuda()\n\toptimizer = torch.optim.Adagrad(model.parameters(), lr=0.05, weight_decay=5e-5)\n\n\tprint(\'Start training...\')\n\tbatch_counter = 0\n\tbest_dev_loss=10e10\n\tbest_dev_loss_m=10e10\n\tbest_dev_loss_um=10e10\n\taccumulated_loss=0\n\treport_interval = 1000\n\tmodel.train()\n\t\'\'\'\n\ttrain_pairs = np.array(train_pairs)\n\tboth_lengths = np.array([(len(train_pairs[i][0]), len(train_pairs[i][1])) for i in range(len(train_pairs))],\n\t                        dtype={\'names\': [\'x\', \'y\'], \'formats\': [\'i4\', \'i4\']})\n\tfor epoch in range(num_epochs):\n\t\trand_idx = np.random.permutation(len(train_pairs))\n\t\ttrain_pairs = train_pairs[rand_idx]\n\t\tboth_lengths = both_lengths[rand_idx]\n\t\tsorted_lengths = np.argsort(both_lengths, order=(\'x\', \'y\'))\n\t\ttrain_pairs=train_pairs[sorted_lengths]\n\t\tboth_lengths=both_lengths[sorted_lengths]\n\t\tprint(\'--\' * 20)\n\t\tbatch_index = 0\n\t\tbatch_counter=0\n\t\twhile batch_index < len(train_pairs):\n\t\t\tbatch_index2=batch_index+batch_size\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(train_pairs, batch_index,\n\t\t\t                                                                       batch_index2)\n\t\t\tbatch_index = batch_index2\n\t\t\toptimizer.zero_grad()\n\t\t\toutput = model(left_sents, right_sents, lsize_list, rsize_list)\n\t\t\t# print(\'forward finished\'+str(datetime.now()))\n\t\t\tloss = criterion(output, labels)\n\t\t\tloss.backward()\n\t\t\toptimizer.step()\n\t\t\t# print(\'backword finished\' + str(datetime.now()))\n\t\t\tbatch_counter += 1\n\t\t\t# print(batch_counter, loss.data[0])\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\tif batch_counter % report_interval == 0:\n\t\t\t\tmodel.eval()\n\t\t\t\tdev_batch_index = 0\n\t\t\t\tdev_num_correct = 0\n\t\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\t\tmsg += \'\\t training batch loss: %f\' % accumulated_loss\n\t\t\t\taccumulated_loss = 0\n\t\t\t\twhile dev_batch_index<len(test_pairs):\n\t\t\t\t\tdev_batch_index2=dev_batch_index+1\n\t\t\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(test_pairs, dev_batch_index,\n\t\t\t\t\t                                                                       dev_batch_index2)\n\t\t\t\t\tdev_batch_index=dev_batch_index2\n\t\t\t\t\toutput = model(left_sents, right_sents, lsize_list, rsize_list)\n\t\t\t\t\tresult = output.data.cpu().numpy()\n\t\t\t\t\tloss = criterion(output, labels)\n\t\t\t\t\taccumulated_loss += loss.data[0]\n\t\t\t\t\ta = np.argmax(result, axis=1)\n\t\t\t\t\tb = labels.data.cpu().numpy()\n\t\t\t\t\tdev_num_correct += np.sum(a == b)\n\t\t\t\tmsg += \'\\t dev loss: %f\' % accumulated_loss\n\t\t\t\tdev_acc = dev_num_correct / len(test_pairs)\n\t\t\t\tmsg += \'\\t dev accuracy: %f\' % dev_acc\n\t\t\t\tprint(msg)\n\t\t\t\taccumulated_loss = 0\n\t\t\tmodel.train()\n\t\'\'\'\n\ttrain_pairs=np.array(train_pairs)\n\trand_idx = np.random.permutation(len(train_pairs))\n\ttrain_pairs = train_pairs[rand_idx]\n\tboth_lengths = np.array([(len(train_pairs[i][0]), len(train_pairs[i][1])) for i in range(len(train_pairs))],\n\t                        dtype={\'names\': [\'x\', \'y\'], \'formats\': [\'i4\', \'i4\']})\n\tsorted_lengths = np.argsort(both_lengths, order=(\'x\', \'y\'))\n\ttrain_pairs = train_pairs[sorted_lengths]\n\ttrain_batch_list=prepare_data(train_pairs)\n\n\n\t#dev_pairs=np.array(dev_pairs)\n\t#both_lengths = np.array([(len(dev_pairs[i][0]), len(dev_pairs[i][1])) for i in range(len(dev_pairs))],\n\t#                        dtype={\'names\': [\'x\', \'y\'], \'formats\': [\'i4\', \'i4\']})\n\t#sorted_lengths = np.argsort(both_lengths, order=(\'x\', \'y\'))\n\t#dev_pairs = dev_pairs[sorted_lengths]\n\tdev_batch_list=prepare_data(dev_pairs)\n\n\t#test_pairs=np.array(test_pairs)\n\t#both_lengths = np.array([(len(test_pairs[i][0]), len(test_pairs[i][1])) for i in range(len(test_pairs))],\n\t#                        dtype={\'names\': [\'x\', \'y\'], \'formats\': [\'i4\', \'i4\']})\n\t#sorted_lengths = np.argsort(both_lengths, order=(\'x\', \'y\'))\n\t#test_pairs = test_pairs[sorted_lengths]\n\ttest_batch_list=prepare_data(test_pairs)\n\t\'\'\'\n\n\tdev_pairs_m = np.array(dev_pairs_m)\n\tboth_lengths = np.array([(len(dev_pairs_m[i][0]), len(dev_pairs_m[i][1])) for i in range(len(dev_pairs_m))],\n\t                        dtype={\'names\': [\'x\', \'y\'], \'formats\': [\'i4\', \'i4\']})\n\tsorted_lengths = np.argsort(both_lengths, order=(\'x\', \'y\'))\n\tdev_pairs_m = dev_pairs_m[sorted_lengths]\n\tdev_batch_list_m = prepare_data(dev_pairs_m)\n\n\tdev_pairs_um = np.array(dev_pairs_um)\n\tboth_lengths = np.array([(len(dev_pairs_um[i][0]), len(dev_pairs_um[i][1])) for i in range(len(dev_pairs_um))],\n\t                        dtype={\'names\': [\'x\', \'y\'], \'formats\': [\'i4\', \'i4\']})\n\tsorted_lengths = np.argsort(both_lengths, order=(\'x\', \'y\'))\n\tdev_pairs_um = dev_pairs_um[sorted_lengths]\n\tdev_batch_list_um = prepare_data(dev_pairs_um)\n\t\'\'\'\n\tbatch_index=0\n\tfor epoch in range(num_epochs):\n\t\tbatch_counter=0\n\t\taccumulated_loss = 0\n\t\tmodel.train()\n\t\tprint(\'--\' * 20)\n\t\tstart_time = time.time()\n\t\ttrain_rand_i=np.random.permutation(len(train_batch_list))\n\t\ttrain_batch_i=0\n\t\ttrain_sents_scaned=0\n\t\ttrain_num_correct=0\n\t\twhile train_batch_i<len(train_batch_list):\n\t\t\tbatch_index, batch_index2=train_batch_list[train_rand_i[train_batch_i]]\n\t\t\ttrain_batch_i+=1\n\t\t\t#batch_index2=batch_index+batch_size\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(train_pairs, batch_index, batch_index2)\n\t\t\ttrain_sents_scaned+=len(labels)\n\t\t\t#print(lsize_list)\n\t\t\t#print(rsize_list)\n\t\t\t#batch_index=batch_index2\n\t\t\toptimizer.zero_grad()\n\n\t\t\toutput = model(left_sents, right_sents, lsize_list, rsize_list)\n\t\t\tresult = output.data.cpu().numpy()\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = labels.data.cpu().numpy()\n\t\t\ttrain_num_correct += np.sum(a == b)\n\t\t\t#print(\'forward finished\'+str(datetime.now()))\n\t\t\t#print(output)\n\t\t\t#print(labels)\n\t\t\t#sys.exit()\n\t\t\tloss = criterion(output, labels)\n\t\t\tloss.backward()\n\n\t\t\t\'\'\'\n\t\t\tgrad_norm = 0.\n\t\t\t#para_norm = 0.\n\n\t\t\tfor m in model.modules():\n\t\t\t\tif isinstance(m, nn.Linear):\n\t\t\t\t\t# print(m)\n\t\t\t\t\tgrad_norm += m.weight.grad.data.norm() ** 2\n\t\t\t\t\t#para_norm += m.weight.data.norm() ** 2\n\t\t\t\t\tif m.bias is not None:\n\t\t\t\t\t\tgrad_norm += m.bias.grad.data.norm() ** 2\n\t\t\t\t\t\t#para_norm += m.bias.data.norm() ** 2\n\n\t\t\tgrad_norm ** 0.5\n\t\t\t#para_norm ** 0.5\n\n\t\t\ttry:\n\t\t\t\tshrinkage = args.max_grad_norm / grad_norm\n\t\t\texcept:\n\t\t\t\tpass\n\t\t\tif shrinkage < 1:\n\t\t\t\tfor m in model.modules():\n\t\t\t\t\t# print m\n\t\t\t\t\tif isinstance(m, nn.Linear):\n\t\t\t\t\t\tm.weight.grad.data = m.weight.grad.data * shrinkage\n\t\t\t\t\t\tif m.bias is not None:\n\t\t\t\t\t\t\tm.bias.grad.data = m.bias.grad.data * shrinkage\n\t\t\t\'\'\'\n\t\t\toptimizer.step()\n\t\t\t#print(\'backword finished\' + str(datetime.now()))\n\t\t\tbatch_counter += 1\n\t\t\t#print(batch_counter, loss.data[0])\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\tif batch_counter % report_interval ==0:\n\t\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\t\tmsg += \'\\t train batch loss: %f\' % (accumulated_loss/train_sents_scaned)\n\t\t\t\t#msg += \'\\t train accuracy: %f\' % (train_num_correct/train_sents_scaned)\n\t\t\t\tprint(msg)\n\t\t\'\'\'\n\t\t# valid after each epoch\n\t\tmodel.eval()\n\t\tdev_batch_index=0\n\t\tdev_num_correct=0\n\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\taccumulated_loss=0\n\t\tdev_batch_i = 0\n\t\twhile dev_batch_i<len(dev_batch_list_m):\n\t\t\tdev_batch_index, dev_batch_index2 = dev_batch_list_m[dev_batch_i]\n\t\t\tdev_batch_i += 1\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(dev_pairs_m, dev_batch_index, dev_batch_index2)\n\t\t\t#left_sents, right_sents, labels = create_batch(dev_pairs, 0, len(dev_pairs))\n\t\t\toutput = model(left_sents, right_sents, lsize_list, rsize_list)\n\t\t\tresult=output.data.cpu().numpy()\n\t\t\tloss = criterion(output, labels)\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = labels.data.cpu().numpy()\n\t\t\tdev_num_correct+=np.sum(a == b)\n\t\tmsg += \'\\t dev_m loss: %f\' % accumulated_loss\n\t\tdev_acc=dev_num_correct/len(dev_pairs_m)\n\t\tmsg += \'\\t dev_m accuracy: %f\' % dev_acc\n\t\tprint(msg)\n\t\tif accumulated_loss < best_dev_loss_m:\n\t\t\tbest_dev_loss_m=accumulated_loss\n\t\t\ttest_batch_index = 0\n\t\t\tpred=[]\n\t\t\twhile test_batch_index < len(test_pairs_m):\n\t\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(test_pairs_m, test_batch_index,\n\t\t\t\t                                                                       test_batch_index+1)\n\t\t\t\ttest_batch_index+=1\n\t\t\t\toutput = model(left_sents, right_sents, lsize_list, rsize_list)\n\t\t\t\tresult = output.data.cpu().numpy()\n\t\t\t\ta = np.argmax(result, axis=1)\n\t\t\t\tpred.extend(a)\n\t\t\twith open(basepath + \'/sub_m.csv\', \'w+\') as f:\n\t\t\t\tindex = [\'neutral\',\'contradiction\',\'entailment\']\n\t\t\t\tf.write(""pairID,gold_label\\n"")\n\t\t\t\tfor i, k in enumerate(pred):\n\t\t\t\t\tf.write(str(i + 9847) + "","" + index[k] + ""\\n"")\n\t\t\ttorch.save(model, basepath + \'/model_DecAtt_m\' + \'.pkl\')\n\n\t\tdev_batch_index=0\n\t\tdev_num_correct=0\n\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\taccumulated_loss=0\n\t\tdev_batch_i = 0\n\t\twhile dev_batch_i<len(dev_batch_list_um):\n\t\t\tdev_batch_index, dev_batch_index2 = dev_batch_list_um[dev_batch_i]\n\t\t\tdev_batch_i += 1\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(dev_pairs_um, dev_batch_index, dev_batch_index2)\n\t\t\t#left_sents, right_sents, labels = create_batch(dev_pairs, 0, len(dev_pairs))\n\t\t\toutput = model(left_sents, right_sents, lsize_list, rsize_list)\n\t\t\tresult=output.data.cpu().numpy()\n\t\t\tloss = criterion(output, labels)\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = labels.data.cpu().numpy()\n\t\t\tdev_num_correct+=np.sum(a == b)\n\t\tmsg += \'\\t dev_um loss: %f\' % accumulated_loss\n\t\tdev_acc=dev_num_correct/len(dev_pairs_um)\n\t\tmsg += \'\\t dev_um accuracy: %f\' % dev_acc\n\t\tprint(msg)\n\t\tif accumulated_loss < best_dev_loss_um:\n\t\t\tbest_dev_loss_um=accumulated_loss\n\t\t\ttest_batch_index = 0\n\t\t\tpred=[]\n\t\t\twhile test_batch_index < len(test_pairs_um):\n\t\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(test_pairs_um, test_batch_index,\n\t\t\t\t                                                                       test_batch_index+1)\n\t\t\t\ttest_batch_index+=1\n\t\t\t\toutput = model(left_sents, right_sents, lsize_list, rsize_list)\n\t\t\t\tresult = output.data.cpu().numpy()\n\t\t\t\ta = np.argmax(result, axis=1)\n\t\t\t\tpred.extend(a)\n\t\t\twith open(basepath + \'/sub_um.csv\', \'w+\') as f:\n\t\t\t\tindex = [\'neutral\',\'contradiction\',\'entailment\']\n\t\t\t\tf.write(""pairID,gold_label\\n"")\n\t\t\t\tfor i, k in enumerate(pred):\n\t\t\t\t\tf.write(str(i) + "","" + index[k] + ""\\n"")\n\t\t\ttorch.save(model, basepath + \'/model_DecAtt_um\' + \'.pkl\')\n\t\telapsed_time = time.time() - start_time\n\t\tprint(\'Epoch \' + str(epoch) + \' finished within \' + str(timedelta(seconds=elapsed_time)))\n\t\t\'\'\'\n\t\t# valid after each epoch\n\t\tmodel.eval()\n\t\tdev_batch_index = 0\n\t\tdev_num_correct = 0\n\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\taccumulated_loss = 0\n\t\tdev_batch_i = 0\n\t\tpred=[]\n\t\tgold=[]\n\t\twhile dev_batch_i < len(dev_batch_list):\n\t\t\tdev_batch_index, dev_batch_index2 = dev_batch_list[dev_batch_i]\n\t\t\tdev_batch_i += 1\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(dev_pairs, dev_batch_index,\n\t\t\t                                                                       dev_batch_index2)\n\t\t\t# left_sents, right_sents, labels = create_batch(dev_pairs, 0, len(dev_pairs))\n\t\t\toutput = model(left_sents, right_sents, lsize_list, rsize_list)\n\t\t\tresult = np.exp(output.data.cpu().numpy())\n\t\t\tloss = criterion(output, labels)\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = labels.data.cpu().numpy()\n\t\t\tdev_num_correct += np.sum(a == b)\n\t\t\tif task==\'pit\' or task==\'url\' or task==\'wikiqa\' or task==\'trecqa\':\n\t\t\t\tpred.extend(result[:,1])\n\t\t\t\tgold.extend(b)\n\t\t\tif task==\'sts\':\n\t\t\t\tpred.extend(0*result[:,0]+1*result[:,1]+2*result[:,2]+3*result[:,3]+4*result[:,4]+5*result[:,5])\n\t\t\t\tgold.extend(0*b[:,0]+1*b[:,1]+2*b[:,2]+3*b[:,3]+4*b[:,4]+5*b[:,5])\n\t\tmsg += \'\\t dev loss: %f\' % accumulated_loss\n\t\tdev_acc = dev_num_correct / len(dev_pairs)\n\t\t#msg += \'\\t dev accuracy: %f\' % dev_acc\n\t\tprint(msg)\n\t\tresult1 = pearson(pred[0:450], gold[0:450])\n\t\tresult2 = pearson(pred[450:750], gold[450:750])\n\t\tresult3 = pearson(pred[750:1500], gold[750:1500])\n\t\tresult4 = pearson(pred[1500:2250], gold[1500:2250])\n\t\tresult5 = pearson(pred[2250:3000], gold[2250:3000])\n\t\tresult6 = pearson(pred[3000:3750], gold[3000:3750])\n\t\twt_mean = 0.12 * result1 + 0.08 * result2 + 0.2 * result3 + 0.2 * result4 + 0.2 * result5 + 0.2 * result6\n\t\tprint(\'weighted pearson mean: %.6f\' % wt_mean)\n\t\telapsed_time = time.time() - start_time\n\t\tprint(\'Epoch \' + str(epoch) + \' finished within \' + str(timedelta(seconds=elapsed_time)))\n'"
DecAtt/main_trecqa.py,25,"b'from __future__ import division\n\nfrom datetime import datetime\nfrom torch.autograd import Variable\nfrom model import *\nimport argparse\nimport sys\nimport os\nfrom util import *\nimport time\nimport torch\nimport random\nimport logging\nimport numpy as np\nimport cPickle as pickle\nfrom datetime import timedelta\nfrom os.path import expanduser\nfrom torch.autograd import Variable\nfrom torchtext.vocab import load_word_vectors\n\ndef prepare_data(pairs, batch_size=32):\n\tbatch_list = []\n\tbatch_index = 0\n\twhile batch_index < len(pairs):\n\t\ttry:\n\t\t\tsubset = pairs[batch_index:batch_index + batch_size]\n\t\texcept:\n\t\t\tsubset = pairs[batch_index:]\n\t\ttmp_a = np.array([len(item[0]) for item in subset])\n\t\ttmp_b = np.array([len(item[1]) for item in subset])\n\t\tbatch_index2 = batch_index + min(len(np.where(tmp_a == tmp_a[0])[0]), len(np.where(tmp_b == tmp_b[0])[0]))\n\t\tbatch_list.append([batch_index, batch_index2])\n\t\tbatch_index = batch_index2\n\treturn batch_list\n\ndef create_batch(data,from_index, to_index):\n\tif to_index>len(data):\n\t\tto_index=len(data)\n\tlsize=0\n\trsize=0\n\tlsize_list=[]\n\trsize_list=[]\n\tfor i in range(from_index, to_index):\n\t\tlength=len(data[i][0])+2\n\t\tlsize_list.append(length)\n\t\tif length>lsize:\n\t\t\tlsize=length\n\t\tlength=len(data[i][1])+2\n\t\trsize_list.append(length)\n\t\tif length>rsize:\n\t\t\trsize=length\n\t#lsize+=1\n\t#rsize+=1\n\tlsent = data[from_index][0]\n\tlsent = [\'bos\']+lsent + [\'oov\' for k in range(lsize -1 - len(lsent))]\n\t#print(lsent)\n\tleft_sents = torch.cat((dict[word].view(1, -1) for word in lsent))\n\tleft_sents = torch.unsqueeze(left_sents,0)\n\n\trsent = data[from_index][1]\n\trsent = [\'bos\']+rsent + [\'oov\' for k in range(rsize -1 - len(rsent))]\n\t#print(rsent)\n\tright_sents = torch.cat((dict[word].view(1, -1) for word in rsent))\n\tright_sents = torch.unsqueeze(right_sents,0)\n\n\tlabels=[data[from_index][2]]\n\n\tfor i in range(from_index+1, to_index):\n\n\t\tlsent=data[i][0]\n\t\tlsent=[\'bos\']+lsent+[\'oov\' for k in range(lsize -1 - len(lsent))]\n\t\t#print(lsent)\n\t\tleft_sent = torch.cat((dict[word].view(1,-1) for word in lsent))\n\t\tleft_sent = torch.unsqueeze(left_sent, 0)\n\t\tleft_sents = torch.cat([left_sents, left_sent])\n\n\t\trsent=data[i][1]\n\t\trsent=[\'bos\']+rsent+[\'oov\' for k in range(rsize -1 - len(rsent))]\n\t\t#print(rsent)\n\t\tright_sent = torch.cat((dict[word].view(1,-1) for word in rsent))\n\t\tright_sent = torch.unsqueeze(right_sent, 0)\n\t\tright_sents = torch.cat((right_sents, right_sent))\n\n\t\tlabels.append(data[i][2])\n\n\tleft_sents=Variable(left_sents)\n\tright_sents=Variable(right_sents)\n\tif task==\'sts\':\n\t\tlabels=Variable(torch.Tensor(labels))\n\telse:\n\t\tlabels=Variable(torch.LongTensor(labels))\n\tlsize_list=Variable(torch.LongTensor(lsize_list))\n\trsize_list = Variable(torch.LongTensor(rsize_list))\n\n\tif torch.cuda.is_available():\n\t\tleft_sents=left_sents.cuda()\n\t\tright_sents=right_sents.cuda()\n\t\tlabels=labels.cuda()\n\t\tlsize_list=lsize_list.cuda()\n\t\trsize_list=rsize_list.cuda()\n\t#print(left_sents)\n\t#print(right_sents)\n\t#print(labels)\n\treturn left_sents, right_sents, labels, lsize_list, rsize_list\n\nif __name__ == \'__main__\':\n\ttask=\'trecqa\'\n\tprint(\'task: \'+task)\n\tEMBEDDING_DIM = 300\n\tPROJECTED_EMBEDDING_DIM = 300\n\n\tparser = argparse.ArgumentParser(description=__doc__)\n\tparser.add_argument(\'--e\', dest=\'num_epochs\', default=5000, type=int, help=\'Number of epochs\')\n\tparser.add_argument(\'--b\', dest=\'batch_size\', default=32, help=\'Batch size\', type=int)\n\tparser.add_argument(\'--u\', dest=\'num_units\', help=\'Number of hidden units\', default=100, type=int)\n\tparser.add_argument(\'--r\', help=\'Learning rate\', type=float, default=0.05, dest=\'rate\')\n\tparser.add_argument(\'--lower\', help=\'Lowercase the corpus\', default=True, action=\'store_true\')\n\tparser.add_argument(\'--model\', help=\'Model selection\', default=\'DecAtt\', type=str)\n\tparser.add_argument(\'--optim\', help=\'Optimizer algorithm\', default=\'adagrad\', choices=[\'adagrad\', \'adadelta\', \'adam\'])\n\tparser.add_argument(\'--max_grad_norm\', help=\'If the norm of the gradient vector exceeds this renormalize it\\\n\t\t\t\t\t\t\t\t\t   to have the norm equal to max_grad_norm\', type=float, default=5)\n\tnum_class = 2\n\tif torch.cuda.is_available():\n\t\tprint(\'CUDA is available!\')\n\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/trecqa\'\n\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\tcastorini_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/castorini/trec_eval.9.0/trec_eval\'\n\t\ttrain_pairs = readQuoradata(basepath + \'/train/\')\n\t\tdev_pairs = readQuoradata(basepath + \'/dev/\')\n\t\ttest_pairs = readQuoradata(basepath + \'/test/\')\n\telse:\n\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/trecqa\'\n\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\tcastorini_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/castorini/trec_eval.9.0/trec_eval\'\n\t\ttrain_pairs = readQuoradata(basepath + \'/train/\')\n\t\tdev_pairs = readQuoradata(basepath + \'/dev/\')\n\t\ttest_pairs = readQuoradata(basepath + \'/test/\')\n\n\tcmd = (\'%s -m map %s %s\' % (castorini_path, basepath + \'/test.qrel\', basepath + \'/result_DecAtt_\' + task))\n\tos.system(cmd)\n\tcmd = (\'%s -m recip_rank %s %s\' % (castorini_path, basepath + \'/test.qrel\', basepath + \'/result_DecAtt_\' + task))\n\tos.system(cmd)\n\n\t#sys.exit()\n\targs = parser.parse_args()\n\tprint(\'Model: %s\' % args.model)\n\tprint(\'Read data ...\')\n\tprint(\'Number of training pairs: %d\' % len(train_pairs))\n\t#print(\'Number of development pairs: %d\' % len(dev_pairs))\n\t#print(\'Number of testing pairs: %d\' % len(test_pairs))\n\tbatch_size = args.batch_size\n\tnum_epochs = args.num_epochs\n\ttokens = []\n\tdict={}\n\tword2id={}\n\tvocab = set()\n\tfor pair in train_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in dev_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in test_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\ttokens=list(vocab)\n\t#for line in open(basepath + \'/vocab.txt\'):\n\t#\ttokens.append(line.strip().decode(\'utf-8\'))\n\twv_dict, wv_arr, wv_size = load_word_vectors(embedding_path, \'glove.840B\', EMBEDDING_DIM)\n\t#embedding = []\n\ttokens.append(\'oov\')\n\ttokens.append(\'bos\')\n\t#embedding.append(dict[word].numpy())\n\t#print(len(embedding))\n\t#np.save(\'embedding\',np.array(embedding))\n\t#sys.exit()\n\tpretrained_emb = np.zeros(shape=(len(tokens), EMBEDDING_DIM))\n\toov={}\n\tfor id in range(100):\n\t\toov[id]=torch.normal(torch.zeros(EMBEDDING_DIM),std=1)\n\tid=0\n\tfor word in tokens:\n\t\ttry:\n\t\t\tdict[word] = wv_arr[wv_dict[word]]/torch.norm(wv_arr[wv_dict[word]])\n\t\texcept:\n\t\t\t#if args.model==\'DecAtt\':\n\t\t\t#\tdict[word]=oov[np.random.randint(100)]\n\t\t\t#else:\n\t\t\tdict[word] = torch.normal(torch.zeros(EMBEDDING_DIM),std=1)\n\t\tword2id[word]=id\n\t\tpretrained_emb[id] = dict[word].numpy()\n\t\tid+=1\n\n\tif task==\'sts\':\n\t\tcriterion = nn.KLDivLoss()\n\telse:\n\t\tcriterion = torch.nn.NLLLoss(size_average=True)\n\t#criterion = torch.nn.CrossEntropyLoss()\n\tmodel=DecAtt(200,num_class,len(tokens),EMBEDDING_DIM, PROJECTED_EMBEDDING_DIM, pretrained_emb)\n\tif torch.cuda.is_available():\n\t\tmodel = model.cuda()\n\t\tcriterion = criterion.cuda()\n\toptimizer = torch.optim.Adagrad(model.parameters(), lr=0.05, weight_decay=5e-5)\n\n\tprint(\'Start training...\')\n\tbatch_counter = 0\n\tbest_dev_loss=10e10\n\tbest_dev_loss_m=10e10\n\tbest_dev_loss_um=10e10\n\taccumulated_loss=0\n\treport_interval = 1000\n\tmodel.train()\n\ttrain_pairs=np.array(train_pairs)\n\trand_idx = np.random.permutation(len(train_pairs))\n\ttrain_pairs = train_pairs[rand_idx]\n\tboth_lengths = np.array([(len(train_pairs[i][0]), len(train_pairs[i][1])) for i in range(len(train_pairs))],\n\t                        dtype={\'names\': [\'x\', \'y\'], \'formats\': [\'i4\', \'i4\']})\n\tsorted_lengths = np.argsort(both_lengths, order=(\'x\', \'y\'))\n\ttrain_pairs = train_pairs[sorted_lengths]\n\ttrain_batch_list=prepare_data(train_pairs, batch_size=32)\n\n\tdev_batch_list=prepare_data(dev_pairs, batch_size=1)\n\n\ttest_batch_list=prepare_data(test_pairs, batch_size=1)\n\n\tbatch_index=0\n\tfor epoch in range(num_epochs):\n\t\tbatch_counter=0\n\t\taccumulated_loss = 0\n\t\tmodel.train()\n\t\tprint(\'--\' * 20)\n\t\tstart_time = time.time()\n\t\ttrain_rand_i=np.random.permutation(len(train_batch_list))\n\t\ttrain_batch_i=0\n\t\ttrain_sents_scaned=0\n\t\ttrain_num_correct=0\n\t\twhile train_batch_i<len(train_batch_list):\n\t\t\tbatch_index, batch_index2=train_batch_list[train_rand_i[train_batch_i]]\n\t\t\ttrain_batch_i+=1\n\t\t\t#batch_index2=batch_index+batch_size\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(train_pairs, batch_index, batch_index2)\n\t\t\ttrain_sents_scaned+=len(labels)\n\t\t\t#print(lsize_list)\n\t\t\t#print(rsize_list)\n\t\t\t#batch_index=batch_index2\n\t\t\toptimizer.zero_grad()\n\n\t\t\toutput = model(left_sents, right_sents, lsize_list, rsize_list)\n\t\t\tresult = output.data.cpu().numpy()\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = labels.data.cpu().numpy()\n\t\t\ttrain_num_correct += np.sum(a == b)\n\t\t\t#print(\'forward finished\'+str(datetime.now()))\n\t\t\t#print(output)\n\t\t\t#print(labels)\n\t\t\t#sys.exit()\n\t\t\tloss = criterion(output, labels)\n\t\t\tloss.backward()\n\n\t\t\t\'\'\'\'\'\'\n\t\t\tgrad_norm = 0.\n\t\t\t#para_norm = 0.\n\n\t\t\tfor m in model.modules():\n\t\t\t\tif isinstance(m, nn.Linear):\n\t\t\t\t\t# print(m)\n\t\t\t\t\tgrad_norm += m.weight.grad.data.norm() ** 2\n\t\t\t\t\t#para_norm += m.weight.data.norm() ** 2\n\t\t\t\t\tif m.bias is not None:\n\t\t\t\t\t\tgrad_norm += m.bias.grad.data.norm() ** 2\n\t\t\t\t\t\t#para_norm += m.bias.data.norm() ** 2\n\n\t\t\tgrad_norm ** 0.5\n\t\t\t#para_norm ** 0.5\n\n\t\t\ttry:\n\t\t\t\tshrinkage = args.max_grad_norm / grad_norm\n\t\t\texcept:\n\t\t\t\tpass\n\t\t\tif shrinkage < 1:\n\t\t\t\tfor m in model.modules():\n\t\t\t\t\t# print m\n\t\t\t\t\tif isinstance(m, nn.Linear):\n\t\t\t\t\t\tm.weight.grad.data = m.weight.grad.data * shrinkage\n\t\t\t\t\t\tif m.bias is not None:\n\t\t\t\t\t\t\tm.bias.grad.data = m.bias.grad.data * shrinkage\n\t\t\t\'\'\'\'\'\'\n\t\t\toptimizer.step()\n\t\t\t#print(\'backword finished\' + str(datetime.now()))\n\t\t\tbatch_counter += 1\n\t\t\t#print(batch_counter, loss.data[0])\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\tif batch_counter % report_interval ==0:\n\t\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\t\tmsg += \'\\t train batch loss: %f\' % (accumulated_loss/train_sents_scaned)\n\t\t\t\tmsg += \'\\t train accuracy: %f\' % (train_num_correct/train_sents_scaned)\n\t\t\t\tprint(msg)\n\t\t# valid after each epoch\n\t\tmodel.eval()\n\t\tdev_batch_index = 0\n\t\tdev_num_correct = 0\n\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\taccumulated_loss = 0\n\t\tdev_batch_i = 0\n\t\tpred=[]\n\t\tgold=[]\n\t\twhile dev_batch_i < len(dev_batch_list):\n\t\t\tdev_batch_index, dev_batch_index2 = dev_batch_list[dev_batch_i]\n\t\t\tdev_batch_i += 1\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(dev_pairs, dev_batch_index,\n\t\t\t                                                                       dev_batch_index2)\n\t\t\t# left_sents, right_sents, labels = create_batch(dev_pairs, 0, len(dev_pairs))\n\t\t\toutput = model(left_sents, right_sents, lsize_list, rsize_list)\n\t\t\tresult = np.exp(output.data.cpu().numpy())\n\t\t\tloss = criterion(output, labels)\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = labels.data.cpu().numpy()\n\t\t\tdev_num_correct += np.sum(a == b)\n\t\t\tif task==\'pit\' or task==\'url\' or task==\'wikiqa\' or task==\'trecqa\':\n\t\t\t\tpred.extend(result[:,1])\n\t\t\t\tgold.extend(b)\n\t\t\tif task==\'sts\':\n\t\t\t\tpred.extend(0*result[:,0]+1*result[:,1]+2*result[:,2]+3*result[:,3]+4*result[:,4]+5*result[:,5])\n\t\t\t\tgold.extend(0*b[:,0]+1*b[:,1]+2*b[:,2]+3*b[:,3]+4*b[:,4]+5*b[:,5])\n\t\tmsg += \'\\t dev loss: %f\' % accumulated_loss\n\t\tdev_acc = dev_num_correct / len(dev_pairs)\n\t\t#msg += \'\\t dev accuracy: %f\' % dev_acc\n\t\tprint(msg)\n\t\tif task==\'pit\' or task==\'url\'or task==\'wikiqa\' or task==\'trecqa\':\n\t\t\tURL_maxF1_eval(pred, gold)\n\t\t#Test after each epoch\n\t\ttest_batch_index = 0\n\t\ttest_num_correct = 0\n\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\taccumulated_loss = 0\n\t\ttest_batch_i = 0\n\t\tpred = []\n\t\tgold = []\n\t\twhile test_batch_i < len(test_batch_list):\n\t\t\ttest_batch_index, test_batch_index2 = test_batch_list[test_batch_i]\n\t\t\ttest_batch_i += 1\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(test_pairs, test_batch_index, test_batch_index2)\n\t\t\toutput = model(left_sents, right_sents, lsize_list, rsize_list)\n\t\t\tresult = np.exp(output.data.cpu().numpy())\n\t\t\tloss = criterion(output, labels)\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = labels.data.cpu().numpy()\n\t\t\ttest_num_correct += np.sum(a == b)\n\t\t\tif task == \'pit\' or task == \'url\'or task==\'wikiqa\' or task==\'trecqa\':\n\t\t\t\tpred.extend(result[:, 1])\n\t\t\t\tgold.extend(b)\n\t\t\tif task == \'sts\':\n\t\t\t\tpred.extend(0 * result[:, 0] + 1 * result[:, 1] + 2 * result[:, 2] + 3 * result[:, 3] + 4 * result[:,4] + 5 * result[:, 5])\n\t\t\t\tgold.extend(0 * b[:, 0] + 1 * b[:, 1] + 2 * b[:, 2] + 3 * b[:, 3] + 4 * b[:, 4] + 5 * b[:, 5])\n\t\tmsg += \'\\t test loss: %f\' % accumulated_loss\n\t\ttest_acc = test_num_correct / len(test_pairs)\n\t\t#msg += \'\\t test accuracy: %f\' % test_acc\n\t\tprint(msg)\n\t\tif task == \'pit\' or task == \'url\'or task==\'wikiqa\' or task==\'trecqa\':\n\t\t\tURL_maxF1_eval(pred, gold)\n\t\t\twith open(basepath + \'/prob_DecAtt_\'+task,\'w\') as f:\n\t\t\t\tfor i in range(len(pred)):\n\t\t\t\t\tf.writelines(str(pred[i])+\'\\n\')\n\t\t\tif task == \'wikiqa\' or task == \'trecqa\':\n\t\t\t\tlist1 = []\n\t\t\t\tfor line in open(basepath+\'/test.qrel\'):\n\t\t\t\t\tlist1.append(line.strip().split())\n\t\t\t\twith open(basepath + \'/result_DecAtt_\' + task, \'w\') as f:\n\t\t\t\t\tfor i in range(len(pred)):\n\t\t\t\t\t\tf.writelines(str(list1[i][0]) + \'\\t\' + str(list1[i][1]) + \'\\t\' + str(\n\t\t\t\t\t\t\tlist1[i][2]) + \'\\t\' + \'*\\t\' + str(pred[i]) + \'\\t\' + \'*\\n\')\n\t\t\tcmd = (\'%s -m map %s %s\' % (castorini_path, basepath + \'/test.qrel\', basepath + \'/result_DecAtt_\' + task))\n\t\t\tos.system(cmd)\n\t\t\tcmd = (\'%s -m recip_rank %s %s\' % (castorini_path, basepath + \'/test.qrel\', basepath + \'/result_DecAtt_\' + task))\n\t\t\tos.system(cmd)\n\t\telapsed_time = time.time() - start_time\n\t\tprint(\'Epoch \' + str(epoch) + \' finished within \' + str(timedelta(seconds=elapsed_time)))\n'"
DecAtt/main_url.py,33,"b'from __future__ import division\n\nfrom datetime import datetime\nfrom torch.autograd import Variable\nfrom model import *\nimport argparse\nimport sys\nimport os\nfrom util import *\nimport time\nimport torch\nimport random\nimport logging\nimport itertools\nimport numpy as np\nimport cPickle as pickle\nfrom datetime import timedelta\nfrom os.path import expanduser\nfrom torch.autograd import Variable\nfrom torchtext.vocab import load_word_vectors\n\ndef get_n_params(model):\n    pp=0\n    parameters = itertools.ifilter(lambda p: p.requires_grad, model.parameters())\n    for p in list(parameters):\n        nn=1\n        for s in list(p.size()):\n            nn = nn*s\n        pp += nn\n    return pp\n\ndef prepare_data(pairs, batch_size=32):\n\tbatch_list = []\n\tbatch_index = 0\n\twhile batch_index < len(pairs):\n\t\ttry:\n\t\t\tsubset = pairs[batch_index:batch_index + batch_size]\n\t\texcept:\n\t\t\tsubset = pairs[batch_index:]\n\t\ttmp_a = np.array([len(item[0]) for item in subset])\n\t\ttmp_b = np.array([len(item[1]) for item in subset])\n\t\tbatch_index2 = batch_index + min(len(np.where(tmp_a == tmp_a[0])[0]), len(np.where(tmp_b == tmp_b[0])[0]))\n\t\tbatch_list.append([batch_index, batch_index2])\n\t\tbatch_index = batch_index2\n\treturn batch_list\n\ndef create_batch(data,from_index, to_index):\n\tif to_index>len(data):\n\t\tto_index=len(data)\n\tlsize=0\n\trsize=0\n\tlsize_list=[]\n\trsize_list=[]\n\tfor i in range(from_index, to_index):\n\t\tlength=len(data[i][0])+2\n\t\tlsize_list.append(length)\n\t\tif length>lsize:\n\t\t\tlsize=length\n\t\tlength=len(data[i][1])+2\n\t\trsize_list.append(length)\n\t\tif length>rsize:\n\t\t\trsize=length\n\t#lsize+=1\n\t#rsize+=1\n\tlsent = data[from_index][0]\n\tlsent = [\'bos\']+lsent + [\'oov\' for k in range(lsize -1 - len(lsent))]\n\t#print(lsent)\n\t#left_sents = torch.cat((dict[word].view(1, -1) if word in dict.keys() else dict[\'oov\'].view(1,-1) for word in lsent))\n\ttmp=[]\n\tfor word in lsent:\n\t\ttry:\n\t\t\ttmp.append(dict[word].view(1, -1))\n\t\texcept:\n\t\t\ttmp.append(dict[\'oov\'].view(1,-1))\n\tleft_sents = torch.cat(tmp)\n\tleft_sents = torch.unsqueeze(left_sents,0)\n\n\trsent = data[from_index][1]\n\trsent = [\'bos\']+rsent + [\'oov\' for k in range(rsize -1 - len(rsent))]\n\t#print(rsent)\n\t#right_sents = torch.cat((dict[word].view(1, -1) if word in dict.keys() else dict[\'oov\'].view(1,-1) for word in rsent))\n\ttmp = []\n\tfor word in rsent:\n\t\ttry:\n\t\t\ttmp.append(dict[word].view(1, -1))\n\t\texcept:\n\t\t\ttmp.append(dict[\'oov\'].view(1, -1))\n\tright_sents = torch.cat(tmp)\n\tright_sents = torch.unsqueeze(right_sents,0)\n\n\tlabels=[data[from_index][2]]\n\n\tfor i in range(from_index+1, to_index):\n\n\t\tlsent=data[i][0]\n\t\tlsent=[\'bos\']+lsent+[\'oov\' for k in range(lsize -1 - len(lsent))]\n\t\t#print(lsent)\n\t\t#left_sent = torch.cat((dict[word].view(1,-1) if word in dict.keys() else dict[\'oov\'].view(1,-1) for word in lsent))\n\t\ttmp = []\n\t\tfor word in lsent:\n\t\t\ttry:\n\t\t\t\ttmp.append(dict[word].view(1, -1))\n\t\t\texcept:\n\t\t\t\ttmp.append(dict[\'oov\'].view(1, -1))\n\t\tleft_sent = torch.cat(tmp)\n\t\tleft_sent = torch.unsqueeze(left_sent, 0)\n\t\tleft_sents = torch.cat([left_sents, left_sent])\n\n\t\trsent=data[i][1]\n\t\trsent=[\'bos\']+rsent+[\'oov\' for k in range(rsize -1 - len(rsent))]\n\t\t#print(rsent)\n\t\t#right_sent = torch.cat((dict[word].view(1,-1) if word in dict.keys() else dict[\'oov\'].view(1,-1) for word in rsent))\n\t\ttmp = []\n\t\tfor word in rsent:\n\t\t\ttry:\n\t\t\t\ttmp.append(dict[word].view(1, -1))\n\t\t\texcept:\n\t\t\t\ttmp.append(dict[\'oov\'].view(1, -1))\n\t\tright_sent = torch.cat(tmp)\n\t\tright_sent = torch.unsqueeze(right_sent, 0)\n\t\tright_sents = torch.cat((right_sents, right_sent))\n\n\t\tlabels.append(data[i][2])\n\n\tleft_sents=Variable(left_sents)\n\tright_sents=Variable(right_sents)\n\tif task==\'sts\':\n\t\tlabels=Variable(torch.Tensor(labels))\n\telse:\n\t\tlabels=Variable(torch.LongTensor(labels))\n\tlsize_list=Variable(torch.LongTensor(lsize_list))\n\trsize_list = Variable(torch.LongTensor(rsize_list))\n\n\tif torch.cuda.is_available():\n\t\tleft_sents=left_sents.cuda()\n\t\tright_sents=right_sents.cuda()\n\t\tlabels=labels.cuda()\n\t\tlsize_list=lsize_list.cuda()\n\t\trsize_list=rsize_list.cuda()\n\t#print(left_sents)\n\t#print(right_sents)\n\t#print(labels)\n\treturn left_sents, right_sents, labels, lsize_list, rsize_list\n\nif __name__ == \'__main__\':\n\ttask=\'url\'\n\tprint(\'task: \'+task)\n\tprint(\'model: DecAtt\')\n\tEMBEDDING_DIM = 300\n\tPROJECTED_EMBEDDING_DIM = 300\n\n\tparser = argparse.ArgumentParser(description=__doc__)\n\tparser.add_argument(\'--e\', dest=\'num_epochs\', default=5000, type=int, help=\'Number of epochs\')\n\tparser.add_argument(\'--b\', dest=\'batch_size\', default=32, help=\'Batch size\', type=int)\n\tparser.add_argument(\'--u\', dest=\'num_units\', help=\'Number of hidden units\', default=100, type=int)\n\tparser.add_argument(\'--r\', help=\'Learning rate\', type=float, default=0.05, dest=\'rate\')\n\tparser.add_argument(\'--lower\', help=\'Lowercase the corpus\', default=True, action=\'store_true\')\n\tparser.add_argument(\'--model\', help=\'Model selection\', default=\'DecAtt\', type=str)\n\tparser.add_argument(\'--optim\', help=\'Optimizer algorithm\', default=\'adagrad\', choices=[\'adagrad\', \'adadelta\', \'adam\'])\n\tparser.add_argument(\'--max_grad_norm\', help=\'If the norm of the gradient vector exceeds this renormalize it\\\n\t\t\t\t\t\t\t\t\t   to have the norm equal to max_grad_norm\', type=float, default=5)\n\tif task==\'snli\':\n\t\tnum_class=3\n\t\tparser.add_argument(\'--train\', help=\'JSONL or TSV file with training corpus\', default=\'/data/snli_1.0_train.jsonl\')\n\t\tparser.add_argument(\'--dev\', help=\'JSONL or TSV file with development corpus\', default=\'/data/snli_1.0_dev.jsonl\')\n\t\tparser.add_argument(\'--test\', help=\'JSONL or TSV file with testing corpus\', default=\'/data/snli_1.0_test.jsonl\')\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DecAtt/data/snli\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DecAtt/data/snli\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\ttrain_pairs = pickle.load(open(basepath + ""/train_pairs.p"", ""rb""))\n\t\tdev_pairs = pickle.load(open(basepath + ""/dev_pairs.p"", ""rb""))\n\t\ttest_pairs = pickle.load(open(basepath + ""/test_pairs.p"", ""rb""))\n\telif task==\'mnli\':\n\t\tnum_class = 3\n\t\tparser.add_argument(\'--train\', help=\'JSONL or TSV file with training corpus\', default=\'/data/mnli/multinli_1.0_train.jsonl\')\n\t\tparser.add_argument(\'--dev_m\', help=\'JSONL or TSV file with development corpus\', default=\'/data/mnli/multinli_1.0_dev_matched.jsonl\')\n\t\tparser.add_argument(\'--dev_um\', help=\'JSONL or TSV file with development corpus\',\n\t\t                   default=\'/data/mnli/multinli_1.0_dev_mismatched.jsonl\')\n\t\tparser.add_argument(\'--test_m\', help=\'JSONL or TSV file with testing corpus\', default=\'/data/mnli/multinli_1.0_test_matched.jsonl\')\n\t\tparser.add_argument(\'--test_um\', help=\'JSONL or TSV file with testing corpus\',\n\t\t                    default=\'/data/mnli/multinli_1.0_test_mismatched.jsonl\')\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DecAtt/data/mnli\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DecAtt/data/mnli\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\t\'\'\'\n\t\ttrain_pairs = util.read_corpus(expanduser(""~"")+\'/Documents/research/pytorch/DeepPairWiseWord\' + args.train, True)\n\t\tdev_pairs_m = util.read_corpus(expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord\' + args.dev_m, True)\n\t\tdev_pairs_um = util.read_corpus(expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord\' + args.dev_um, True)\n\t\ttest_pairs_m = util.read_corpus(expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord\' + args.test_m, True)\n\t\ttest_pairs_um = util.read_corpus(expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord\' + args.test_um, True)\n\t\tpickle.dump(train_pairs, open(""data/mnli/train_pairs.p"", ""wb""))\n\t\tpickle.dump(dev_pairs_m, open(""data/mnli/dev_pairs_m.p"", ""wb""))\n\t\tpickle.dump(dev_pairs_um, open(""data/mnli/dev_pairs_um.p"", ""wb""))\n\t\tpickle.dump(test_pairs_m, open(""data/mnli/test_pairs_m.p"", ""wb""))\n\t\tpickle.dump(test_pairs_um, open(""data/mnli/test_pairs_um.p"", ""wb""))\n\t\t\'\'\'\n\t\ttrain_pairs = pickle.load(open(basepath + ""/train_pairs.p"", ""rb""))\n\t\tdev_pairs_m = pickle.load(open(basepath + ""/dev_pairs_m.p"", ""rb""))\n\t\tdev_pairs_um = pickle.load(open(basepath + ""/dev_pairs_um.p"", ""rb""))\n\t\ttest_pairs_m = pickle.load(open(basepath + ""/test_pairs_m.p"", ""rb""))\n\t\ttest_pairs_um = pickle.load(open(basepath + ""/test_pairs_um.p"", ""rb""))\n\telif task==\'quora\':\n\t\tnum_class = 2\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/quora\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\t\tcastorini_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/castorini/trec_eval.9.0/trec_eval\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/quora\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\t\tcastorini_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/castorini/trec_eval.9.0/trec_eval\'\n\t\ttrain_pairs = readQuoradata(basepath+\'/train/\')\n\t\tdev_pairs=readQuoradata(basepath+\'/dev/\')\n\t\ttest_pairs=readQuoradata(basepath+\'/test/\')\n\telif task==\'url\':\n\t\tnum_class = 2\n\t\tif torch.cuda.is_available():\n\t\t\tprint(\'CUDA is available!\')\n\t\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/url\'\n\t\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\t\tcastorini_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/castorini/trec_eval.9.0/trec_eval\'\n\t\telse:\n\t\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/url\'\n\t\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\t\tcastorini_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/castorini/trec_eval.9.0/trec_eval\'\n\t\ttrain_pairs = readQuoradata(basepath + \'/train/\')\n\t\tdev_pairs = None#readQuoradata(basepath + \'/dev/\')\n\t\ttest_pairs = readQuoradata(basepath + \'/test_9324/\')\n\t\tif dev_pairs==None:\n\t\t\tdev_pairs=test_pairs\n\t#sys.exit()\n\targs = parser.parse_args()\n\t#print(\'Model: %s\' % args.model)\n\tprint(\'Read data ...\')\n\tprint(\'Number of training pairs: %d\' % len(train_pairs))\n\tprint(\'Number of development pairs: %d\' % len(dev_pairs))\n\tprint(\'Number of testing pairs: %d\' % len(test_pairs))\n\tbatch_size = args.batch_size\n\tnum_epochs = args.num_epochs\n\ttokens = []\n\tdict={}\n\tword2id={}\n\tvocab = set()\n\tfor pair in train_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in dev_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in test_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\ttokens=list(vocab)\n\t#for line in open(basepath + \'/vocab.txt\'):\n\t#\ttokens.append(line.strip().decode(\'utf-8\'))\n\twv_dict, wv_arr, wv_size = load_word_vectors(embedding_path, \'glove.840B\', EMBEDDING_DIM)\n\t#embedding = []\n\ttokens.append(\'oov\')\n\ttokens.append(\'bos\')\n\t#embedding.append(dict[word].numpy())\n\t#print(len(embedding))\n\t#np.save(\'embedding\',np.array(embedding))\n\t#sys.exit()\n\tpretrained_emb = np.zeros(shape=(len(tokens), EMBEDDING_DIM))\n\toov={}\n\tfor id in range(100):\n\t\toov[id]=torch.normal(torch.zeros(EMBEDDING_DIM),std=1)\n\tid=0\n\tfor word in tokens:\n\t\ttry:\n\t\t\tdict[word] = wv_arr[wv_dict[word]]/torch.norm(wv_arr[wv_dict[word]])\n\t\texcept:\n\t\t\t#if args.model==\'DecAtt\':\n\t\t\t#\tdict[word]=oov[np.random.randint(100)]\n\t\t\t#else:\n\t\t\tdict[word] = torch.normal(torch.zeros(EMBEDDING_DIM),std=1)\n\t\tword2id[word]=id\n\t\tpretrained_emb[id] = dict[word].numpy()\n\t\tid+=1\n\n\tif task==\'sts\':\n\t\tcriterion = nn.KLDivLoss()\n\telse:\n\t\tcriterion = torch.nn.NLLLoss(size_average=True)\n\t#criterion = torch.nn.CrossEntropyLoss()\n\tmodel=DecAtt(200,num_class,len(tokens),EMBEDDING_DIM, PROJECTED_EMBEDDING_DIM, pretrained_emb)\n\tif torch.cuda.is_available():\n\t\tmodel = model.cuda()\n\t\tcriterion = criterion.cuda()\n\toptimizer = torch.optim.Adagrad(model.parameters(), lr=0.05, weight_decay=5e-5)\n\n\tprint(model)\n\tmodel.bias_embedding.weight.requires_grad = False\n\tmodel.word_embedding.weight.requires_grad = False\n\tprint(get_n_params(model))\n\tsys.exit()\n\n\tprint(\'Start training...\')\n\tbatch_counter = 0\n\tbest_dev_loss=10e10\n\tbest_dev_loss_m=10e10\n\tbest_dev_loss_um=10e10\n\tbest_result=0\n\taccumulated_loss=0\n\treport_interval = 1000\n\tmodel.train()\n\ttrain_pairs=np.array(train_pairs)\n\trand_idx = np.random.permutation(len(train_pairs))\n\ttrain_pairs = train_pairs[rand_idx]\n\tboth_lengths = np.array([(len(train_pairs[i][0]), len(train_pairs[i][1])) for i in range(len(train_pairs))],\n\t                        dtype={\'names\': [\'x\', \'y\'], \'formats\': [\'i4\', \'i4\']})\n\tsorted_lengths = np.argsort(both_lengths, order=(\'x\', \'y\'))\n\ttrain_pairs = train_pairs[sorted_lengths]\n\ttrain_batch_list=prepare_data(train_pairs)\n\n\n\tdev_pairs=np.array(dev_pairs)\n\tboth_lengths = np.array([(len(dev_pairs[i][0]), len(dev_pairs[i][1])) for i in range(len(dev_pairs))],\n\t                        dtype={\'names\': [\'x\', \'y\'], \'formats\': [\'i4\', \'i4\']})\n\tsorted_lengths = np.argsort(both_lengths, order=(\'x\', \'y\'))\n\tdev_pairs = dev_pairs[sorted_lengths]\n\tdev_batch_list=prepare_data(dev_pairs)\n\n\tbatch_index=0\n\tfor epoch in range(num_epochs):\n\t\tbatch_counter=0\n\t\taccumulated_loss = 0\n\t\tmodel.train()\n\t\tprint(\'--\' * 20)\n\t\tstart_time = time.time()\n\t\ttrain_rand_i=np.random.permutation(len(train_batch_list))\n\t\ttrain_batch_i=0\n\t\ttrain_sents_scaned=0\n\t\ttrain_num_correct=0\n\t\twhile train_batch_i<len(train_batch_list):\n\t\t\tbatch_index, batch_index2=train_batch_list[train_rand_i[train_batch_i]]\n\t\t\ttrain_batch_i+=1\n\t\t\t#batch_index2=batch_index+batch_size\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(train_pairs, batch_index, batch_index2)\n\t\t\ttrain_sents_scaned+=len(labels)\n\t\t\t#print(lsize_list)\n\t\t\t#print(rsize_list)\n\t\t\t#batch_index=batch_index2\n\t\t\toptimizer.zero_grad()\n\n\t\t\toutput = model(left_sents, right_sents, lsize_list, rsize_list)\n\t\t\tresult = output.data.cpu().numpy()\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = labels.data.cpu().numpy()\n\t\t\ttrain_num_correct += np.sum(a == b)\n\t\t\t#print(\'forward finished\'+str(datetime.now()))\n\t\t\t#print(output)\n\t\t\t#print(labels)\n\t\t\t#sys.exit()\n\t\t\tloss = criterion(output, labels)\n\t\t\tloss.backward()\n\t\t\t\'\'\'\'\'\'\n\t\t\tgrad_norm = 0.\n\t\t\t#para_norm = 0.\n\n\t\t\tfor m in model.modules():\n\t\t\t\tif isinstance(m, nn.Linear):\n\t\t\t\t\t# print(m)\n\t\t\t\t\tgrad_norm += m.weight.grad.data.norm() ** 2\n\t\t\t\t\t#para_norm += m.weight.data.norm() ** 2\n\t\t\t\t\tif m.bias is not None:\n\t\t\t\t\t\tgrad_norm += m.bias.grad.data.norm() ** 2\n\t\t\t\t\t\t#para_norm += m.bias.data.norm() ** 2\n\n\t\t\tgrad_norm ** 0.5\n\t\t\t#para_norm ** 0.5\n\n\t\t\ttry:\n\t\t\t\tshrinkage = args.max_grad_norm / grad_norm\n\t\t\texcept:\n\t\t\t\tpass\n\t\t\tif shrinkage < 1:\n\t\t\t\tfor m in model.modules():\n\t\t\t\t\t# print m\n\t\t\t\t\tif isinstance(m, nn.Linear):\n\t\t\t\t\t\tm.weight.grad.data = m.weight.grad.data * shrinkage\n\t\t\t\t\t\tif m.bias is not None:\n\t\t\t\t\t\t\tm.bias.grad.data = m.bias.grad.data * shrinkage\n\t\t\t\'\'\'\'\'\'\n\t\t\toptimizer.step()\n\t\t\t#print(\'backword finished\' + str(datetime.now()))\n\t\t\tbatch_counter += 1\n\t\t\t#print(batch_counter, loss.data[0])\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\tif batch_counter % report_interval ==0:\n\t\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\t\tmsg += \'\\t train batch loss: %f\' % (accumulated_loss/train_sents_scaned)\n\t\t\t\tmsg += \'\\t train accuracy: %f\' % (train_num_correct/train_sents_scaned)\n\t\t\t\tprint(msg)\n\t\t# valid after each epoch\n\t\tmodel.eval()\n\t\t# test on URL dataset\n\t\tprint(\'testing on URL dataset:\')\n\t\turl_basepath = basepath.replace(\'url\', \'url\')\n\t\ttest_pairs = readQuoradata(url_basepath + \'/test_9324/\')\n\t\ttest_batch_list = prepare_data(test_pairs, batch_size=1)\n\t\ttest_batch_index = 0\n\t\ttest_num_correct = 0\n\t\taccumulated_loss = 0\n\t\ttest_batch_i = 0\n\t\tpred = []\n\t\tgold = []\n\t\twhile test_batch_i < len(test_batch_list):\n\t\t\ttest_batch_index, test_batch_index2 = test_batch_list[test_batch_i]\n\t\t\ttest_batch_i += 1\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(test_pairs, test_batch_index,\n\t\t\t                                                                       test_batch_index2)\n\t\t\toutput = model(left_sents, right_sents, lsize_list, rsize_list)\n\t\t\tresult = np.exp(output.data.cpu().numpy())\n\t\t\tloss = criterion(output, labels)\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = labels.data.cpu().numpy()\n\t\t\ttest_num_correct += np.sum(a == b)\n\t\t\tif task == \'pit\' or task == \'url\' or task == \'wikiqa\' or task == \'trecqa\' or task == \'quora\':\n\t\t\t\tpred.extend(result[:, 1])\n\t\t\t\tgold.extend(b)\n\t\t_,result=URL_maxF1_eval(pred, gold)\n\t\tif result>best_result:\n\t\t\tbest_result=result\n\t\t\twith open(basepath + \'/DecAtt_domain_adaptation_train_on_url_test_on_url_prob.txt\', \'w\') as f:\n\t\t\t\tfor i in range(len(pred)):\n\t\t\t\t\tf.writelines(str(1 - pred[i]) + \'\\t\' + str(pred[i]) + \'\\n\')\n\t\t\ttorch.save(model, basepath + \'/model_DecAtt_domain_adaptation_\' + task + \'.pkl\')\n\t\t\t# test on Quora dataset\n\t\t\tprint(\'testing on Quora dataset:\')\n\t\t\tquora_basepath = basepath.replace(\'url\', \'quora\')\n\t\t\ttest_pairs = readQuoradata(quora_basepath + \'/test/\')\n\t\t\ttest_batch_list = prepare_data(test_pairs, batch_size=1)\n\t\t\ttest_batch_index = 0\n\t\t\ttest_num_correct = 0\n\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\taccumulated_loss = 0\n\t\t\ttest_batch_i = 0\n\t\t\tpred = []\n\t\t\tgold = []\n\t\t\twhile test_batch_i < len(test_batch_list):\n\t\t\t\ttest_batch_index, test_batch_index2 = test_batch_list[test_batch_i]\n\t\t\t\ttest_batch_i += 1\n\t\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(test_pairs,\n\t\t\t\t                                                                       test_batch_index,\n\t\t\t\t                                                                       test_batch_index2)\n\t\t\t\toutput = model(left_sents, right_sents, lsize_list, rsize_list)\n\t\t\t\tresult = np.exp(output.data.cpu().numpy())\n\t\t\t\tloss = criterion(output, labels)\n\t\t\t\taccumulated_loss += loss.data[0]\n\t\t\t\ta = np.argmax(result, axis=1)\n\t\t\t\tb = labels.data.cpu().numpy()\n\t\t\t\ttest_num_correct += np.sum(a == b)\n\t\t\t\tif task == \'pit\' or task == \'url\' or task == \'wikiqa\' or task == \'trecqa\' or task == \'quora\':\n\t\t\t\t\tpred.extend(result[:, 1])\n\t\t\t\t\tgold.extend(b)\n\t\t\tmsg += \'\\t test loss: %f\' % accumulated_loss\n\t\t\ttest_acc = test_num_correct / len(test_pairs)\n\t\t\tmsg += \'\\t test accuracy: %f\' % test_acc\n\t\t\tprint(msg)\n\t\t\twith open(basepath + \'/DecAtt_domain_adaptation_train_on_url_test_on_quora_prob.txt\', \'w\') as f:\n\t\t\t\tfor i in range(len(pred)):\n\t\t\t\t\tf.writelines(str(1 - pred[i]) + \'\\t\' + str(pred[i]) + \'\\n\')\n\t\t\t# test on PIT dataset\n\t\t\tprint(\'testing on PIT-2015 dataset:\')\n\t\t\turl_basepath = basepath.replace(\'url\', \'pit\')\n\t\t\ttest_pairs = readQuoradata(url_basepath + \'/test/\')\n\t\t\ttest_batch_list = prepare_data(test_pairs, batch_size=1)\n\t\t\ttest_batch_index = 0\n\t\t\ttest_num_correct = 0\n\t\t\taccumulated_loss = 0\n\t\t\ttest_batch_i = 0\n\t\t\tpred = []\n\t\t\tgold = []\n\t\t\twhile test_batch_i < len(test_batch_list):\n\t\t\t\ttest_batch_index, test_batch_index2 = test_batch_list[test_batch_i]\n\t\t\t\ttest_batch_i += 1\n\t\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(test_pairs,\n\t\t\t\t                                                                       test_batch_index,\n\t\t\t\t                                                                       test_batch_index2)\n\t\t\t\toutput = model(left_sents, right_sents, lsize_list, rsize_list)\n\t\t\t\tresult = np.exp(output.data.cpu().numpy())\n\t\t\t\tloss = criterion(output, labels)\n\t\t\t\taccumulated_loss += loss.data[0]\n\t\t\t\ta = np.argmax(result, axis=1)\n\t\t\t\tb = labels.data.cpu().numpy()\n\t\t\t\ttest_num_correct += np.sum(a == b)\n\t\t\t\tif task == \'pit\' or task == \'url\' or task == \'wikiqa\' or task == \'trecqa\' or task == \'quora\':\n\t\t\t\t\tpred.extend(result[:, 1])\n\t\t\t\t\tgold.extend(b)\n\t\t\tURL_maxF1_eval(pred, gold)\n\t\t\twith open(basepath + \'/DecAtt_domain_adaptation_train_on_url_test_on_pit_prob.txt\', \'w\') as f:\n\t\t\t\tfor i in range(len(pred)):\n\t\t\t\t\tf.writelines(str(1 - pred[i]) + \'\\t\' + str(pred[i]) + \'\\n\')\n\t\t\t# test on wikiqa dataset\n\t\t\tprint(\'testing on WikiQA dataset:\')\n\t\t\twikiqa_basepath = basepath.replace(\'url\', \'wikiqa\')\n\t\t\ttest_pairs = readQuoradata(wikiqa_basepath + \'/test/\')\n\t\t\ttest_batch_list = prepare_data(test_pairs, batch_size=1)\n\t\t\ttest_batch_index = 0\n\t\t\ttest_num_correct = 0\n\t\t\taccumulated_loss = 0\n\t\t\ttest_batch_i = 0\n\t\t\tpred = []\n\t\t\tgold = []\n\t\t\twhile test_batch_i < len(test_batch_list):\n\t\t\t\ttest_batch_index, test_batch_index2 = test_batch_list[test_batch_i]\n\t\t\t\ttest_batch_i += 1\n\t\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(test_pairs,\n\t\t\t\t                                                                       test_batch_index,\n\t\t\t\t                                                                       test_batch_index2)\n\t\t\t\toutput = model(left_sents, right_sents, lsize_list, rsize_list)\n\t\t\t\tresult = np.exp(output.data.cpu().numpy())\n\t\t\t\tloss = criterion(output, labels)\n\t\t\t\taccumulated_loss += loss.data[0]\n\t\t\t\ta = np.argmax(result, axis=1)\n\t\t\t\tb = labels.data.cpu().numpy()\n\t\t\t\ttest_num_correct += np.sum(a == b)\n\t\t\t\tif task == \'pit\' or task == \'url\' or task == \'wikiqa\' or task == \'trecqa\' or task == \'quora\':\n\t\t\t\t\tpred.extend(result[:, 1])\n\t\t\t\t\tgold.extend(b)\n\t\t\tlist1 = []\n\t\t\tfor line in open(wikiqa_basepath + \'/test.qrel\'):\n\t\t\t\tlist1.append(line.strip().split())\n\t\t\twith open(basepath + \'/DecAtt_domain_adaptation_train_on_url_test_on_wikiqa\', \'w\') as f:\n\t\t\t\tfor i in range(len(pred)):\n\t\t\t\t\tf.writelines(str(list1[i][0]) + \'\\t\' + str(list1[i][1]) + \'\\t\' + str(\n\t\t\t\t\t\tlist1[i][2]) + \'\\t\' + \'*\\t\' + str(pred[i]) + \'\\t\' + \'*\\n\')\n\t\t\twith open(basepath + \'/DecAtt_domain_adaptation_train_on_url_test_on_wikiqa_prob.txt\', \'w\') as f:\n\t\t\t\tfor i in range(len(pred)):\n\t\t\t\t\tf.writelines(str(1 - pred[i]) + \'\\t\' + str(pred[i]) + \'\\n\')\n\t\t\tcmd = (\'%s -m map %s %s\' % (castorini_path, wikiqa_basepath + \'/test.qrel\',\n\t\t\t                            basepath + \'/DecAtt_domain_adaptation_train_on_url_test_on_wikiqa\'))\n\t\t\tos.system(cmd)\n\t\t\tcmd = (\'%s -m recip_rank %s %s\' % (castorini_path, wikiqa_basepath + \'/test.qrel\',\n\t\t\t                                   basepath + \'/DecAtt_domain_adaptation_train_on_url_test_on_wikiqa\'))\n\t\t\tos.system(cmd)\n\t\t\t# test on trecqa dataset\n\t\t\tprint(\'testing on TrecQA dataset:\')\n\t\t\twikiqa_basepath = basepath.replace(\'url\', \'trecqa\')\n\t\t\ttest_pairs = readQuoradata(wikiqa_basepath + \'/test/\')\n\t\t\ttest_batch_list = prepare_data(test_pairs, batch_size=1)\n\t\t\ttest_batch_index = 0\n\t\t\ttest_num_correct = 0\n\t\t\taccumulated_loss = 0\n\t\t\ttest_batch_i = 0\n\t\t\tpred = []\n\t\t\tgold = []\n\t\t\twhile test_batch_i < len(test_batch_list):\n\t\t\t\ttest_batch_index, test_batch_index2 = test_batch_list[test_batch_i]\n\t\t\t\ttest_batch_i += 1\n\t\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(test_pairs,\n\t\t\t\t                                                                       test_batch_index,\n\t\t\t\t                                                                       test_batch_index2)\n\t\t\t\toutput = model(left_sents, right_sents, lsize_list, rsize_list)\n\t\t\t\tresult = np.exp(output.data.cpu().numpy())\n\t\t\t\tloss = criterion(output, labels)\n\t\t\t\taccumulated_loss += loss.data[0]\n\t\t\t\ta = np.argmax(result, axis=1)\n\t\t\t\tb = labels.data.cpu().numpy()\n\t\t\t\ttest_num_correct += np.sum(a == b)\n\t\t\t\tif task == \'pit\' or task == \'url\' or task == \'wikiqa\' or task == \'trecqa\' or task == \'quora\':\n\t\t\t\t\tpred.extend(result[:, 1])\n\t\t\t\t\tgold.extend(b)\n\t\t\tlist1 = []\n\t\t\tfor line in open(wikiqa_basepath + \'/test.qrel\'):\n\t\t\t\tlist1.append(line.strip().split())\n\t\t\twith open(basepath + \'/DecAtt_domain_adaptation_train_on_url_test_on_trecqa\', \'w\') as f:\n\t\t\t\tfor i in range(len(pred)):\n\t\t\t\t\tf.writelines(str(list1[i][0]) + \'\\t\' + str(list1[i][1]) + \'\\t\' + str(\n\t\t\t\t\t\tlist1[i][2]) + \'\\t\' + \'*\\t\' + str(pred[i]) + \'\\t\' + \'*\\n\')\n\t\t\twith open(basepath + \'/DecAtt_domain_adaptation_train_on_url_test_on_trecqa_prob.txt\', \'w\') as f:\n\t\t\t\tfor i in range(len(pred)):\n\t\t\t\t\tf.writelines(str(1 - pred[i]) + \'\\t\' + str(pred[i]) + \'\\n\')\n\t\t\tcmd = (\'%s -m map %s %s\' % (castorini_path, wikiqa_basepath + \'/test.qrel\',\n\t\t\t                            basepath + \'/DecAtt_domain_adaptation_train_on_url_test_on_trecqa\'))\n\t\t\tos.system(cmd)\n\t\t\tcmd = (\'%s -m recip_rank %s %s\' % (castorini_path, wikiqa_basepath + \'/test.qrel\',\n\t\t\t                                   basepath + \'/DecAtt_domain_adaptation_train_on_url_test_on_trecqa\'))\n\t\t\tos.system(cmd)\n\t\telapsed_time = time.time() - start_time\n\t\tprint(\'Epoch \' + str(epoch) + \' finished within \' + str(timedelta(seconds=elapsed_time)))\n'"
DecAtt/main_wikiqa.py,25,"b'from __future__ import division\n\nfrom datetime import datetime\nfrom torch.autograd import Variable\nfrom model import *\nimport argparse\nimport sys\nimport os\nfrom util import *\nimport time\nimport torch\nimport random\nimport logging\nimport numpy as np\nimport cPickle as pickle\nimport subprocess\nfrom datetime import timedelta\nfrom os.path import expanduser\nfrom torch.autograd import Variable\nfrom torchtext.vocab import load_word_vectors\n\ndef prepare_data(pairs, batch_size=32):\n\tbatch_list = []\n\tbatch_index = 0\n\twhile batch_index < len(pairs):\n\t\ttry:\n\t\t\tsubset = pairs[batch_index:batch_index + batch_size]\n\t\texcept:\n\t\t\tsubset = pairs[batch_index:]\n\t\ttmp_a = np.array([len(item[0]) for item in subset])\n\t\ttmp_b = np.array([len(item[1]) for item in subset])\n\t\tbatch_index2 = batch_index + min(len(np.where(tmp_a == tmp_a[0])[0]), len(np.where(tmp_b == tmp_b[0])[0]))\n\t\tbatch_list.append([batch_index, batch_index2])\n\t\tbatch_index = batch_index2\n\treturn batch_list\n\ndef create_batch(data,from_index, to_index):\n\tif to_index>len(data):\n\t\tto_index=len(data)\n\tlsize=0\n\trsize=0\n\tlsize_list=[]\n\trsize_list=[]\n\tfor i in range(from_index, to_index):\n\t\tlength=len(data[i][0])+2\n\t\tlsize_list.append(length)\n\t\tif length>lsize:\n\t\t\tlsize=length\n\t\tlength=len(data[i][1])+2\n\t\trsize_list.append(length)\n\t\tif length>rsize:\n\t\t\trsize=length\n\t#lsize+=1\n\t#rsize+=1\n\tlsent = data[from_index][0]\n\tlsent = [\'bos\']+lsent + [\'oov\' for k in range(lsize -1 - len(lsent))]\n\t#print(lsent)\n\tleft_sents = torch.cat((dict[word].view(1, -1) for word in lsent))\n\tleft_sents = torch.unsqueeze(left_sents,0)\n\n\trsent = data[from_index][1]\n\trsent = [\'bos\']+rsent + [\'oov\' for k in range(rsize -1 - len(rsent))]\n\t#print(rsent)\n\tright_sents = torch.cat((dict[word].view(1, -1) for word in rsent))\n\tright_sents = torch.unsqueeze(right_sents,0)\n\n\tlabels=[data[from_index][2]]\n\n\tfor i in range(from_index+1, to_index):\n\n\t\tlsent=data[i][0]\n\t\tlsent=[\'bos\']+lsent+[\'oov\' for k in range(lsize -1 - len(lsent))]\n\t\t#print(lsent)\n\t\tleft_sent = torch.cat((dict[word].view(1,-1) for word in lsent))\n\t\tleft_sent = torch.unsqueeze(left_sent, 0)\n\t\tleft_sents = torch.cat([left_sents, left_sent])\n\n\t\trsent=data[i][1]\n\t\trsent=[\'bos\']+rsent+[\'oov\' for k in range(rsize -1 - len(rsent))]\n\t\t#print(rsent)\n\t\tright_sent = torch.cat((dict[word].view(1,-1) for word in rsent))\n\t\tright_sent = torch.unsqueeze(right_sent, 0)\n\t\tright_sents = torch.cat((right_sents, right_sent))\n\n\t\tlabels.append(data[i][2])\n\n\tleft_sents=Variable(left_sents)\n\tright_sents=Variable(right_sents)\n\tif task==\'sts\':\n\t\tlabels=Variable(torch.Tensor(labels))\n\telse:\n\t\tlabels=Variable(torch.LongTensor(labels))\n\tlsize_list=Variable(torch.LongTensor(lsize_list))\n\trsize_list = Variable(torch.LongTensor(rsize_list))\n\n\tif torch.cuda.is_available():\n\t\tleft_sents=left_sents.cuda()\n\t\tright_sents=right_sents.cuda()\n\t\tlabels=labels.cuda()\n\t\tlsize_list=lsize_list.cuda()\n\t\trsize_list=rsize_list.cuda()\n\t#print(left_sents)\n\t#print(right_sents)\n\t#print(labels)\n\treturn left_sents, right_sents, labels, lsize_list, rsize_list\n\nif __name__ == \'__main__\':\n\ttask=\'wikiqa\'\n\tprint(\'task: \'+task)\n\tEMBEDDING_DIM = 300\n\tPROJECTED_EMBEDDING_DIM = 300\n\n\tparser = argparse.ArgumentParser(description=__doc__)\n\tparser.add_argument(\'--e\', dest=\'num_epochs\', default=5000, type=int, help=\'Number of epochs\')\n\tparser.add_argument(\'--b\', dest=\'batch_size\', default=32, help=\'Batch size\', type=int)\n\tparser.add_argument(\'--u\', dest=\'num_units\', help=\'Number of hidden units\', default=100, type=int)\n\tparser.add_argument(\'--r\', help=\'Learning rate\', type=float, default=0.05, dest=\'rate\')\n\tparser.add_argument(\'--lower\', help=\'Lowercase the corpus\', default=True, action=\'store_true\')\n\tparser.add_argument(\'--model\', help=\'Model selection\', default=\'DecAtt\', type=str)\n\tparser.add_argument(\'--optim\', help=\'Optimizer algorithm\', default=\'adagrad\', choices=[\'adagrad\', \'adadelta\', \'adam\'])\n\tparser.add_argument(\'--max_grad_norm\', help=\'If the norm of the gradient vector exceeds this renormalize it\\\n\t\t\t\t\t\t\t\t\t   to have the norm equal to max_grad_norm\', type=float, default=5)\n\tnum_class=2\n\tif torch.cuda.is_available():\n\t\tprint(\'CUDA is available!\')\n\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/wikiqa\'\n\t\tcastorini_path=expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/castorini/trec_eval.9.0/trec_eval\'\n\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\ttrain_pairs = readQuoradata(basepath + \'/train/\')\n\t\tdev_pairs = readQuoradata(basepath + \'/dev/\')\n\t\ttest_pairs = readQuoradata(basepath + \'/test/\')\n\telse:\n\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/wikiqa\'\n\t\tcastorini_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/castorini/trec_eval.9.0/trec_eval\'\n\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\ttrain_pairs = readQuoradata(basepath + \'/train/\')\n\t\tdev_pairs = readQuoradata(basepath + \'/dev/\')\n\t\ttest_pairs = readQuoradata(basepath + \'/test/\')\n\t#sys.exit()\n\tcmd = (\'%s -m map %s %s\' % (castorini_path, basepath + \'/test.qrel\', basepath + \'/result_DecAtt_\' + task))\n\tos.system(cmd)\n\tcmd = (\'%s -m recip_rank %s %s\' % (castorini_path, basepath + \'/test.qrel\', basepath + \'/result_DecAtt_\' + task))\n\tos.system(cmd)\n\n\targs = parser.parse_args()\n\tprint(\'Model: %s\' % args.model)\n\tprint(\'Read data ...\')\n\tprint(\'Number of training pairs: %d\' % len(train_pairs))\n\t#print(\'Number of development pairs: %d\' % len(dev_pairs))\n\t#print(\'Number of testing pairs: %d\' % len(test_pairs))\n\tbatch_size = args.batch_size\n\tnum_epochs = args.num_epochs\n\ttokens = []\n\tdict={}\n\tword2id={}\n\tvocab = set()\n\tfor pair in train_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in dev_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in test_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\ttokens=list(vocab)\n\t#for line in open(basepath + \'/vocab.txt\'):\n\t#\ttokens.append(line.strip().decode(\'utf-8\'))\n\twv_dict, wv_arr, wv_size = load_word_vectors(embedding_path, \'glove.840B\', EMBEDDING_DIM)\n\t#embedding = []\n\ttokens.append(\'oov\')\n\ttokens.append(\'bos\')\n\t#embedding.append(dict[word].numpy())\n\t#print(len(embedding))\n\t#np.save(\'embedding\',np.array(embedding))\n\t#sys.exit()\n\tpretrained_emb = np.zeros(shape=(len(tokens), EMBEDDING_DIM))\n\toov={}\n\tfor id in range(100):\n\t\toov[id]=torch.normal(torch.zeros(EMBEDDING_DIM),std=1)\n\tid=0\n\tfor word in tokens:\n\t\ttry:\n\t\t\tdict[word] = wv_arr[wv_dict[word]]/torch.norm(wv_arr[wv_dict[word]])\n\t\texcept:\n\t\t\t#if args.model==\'DecAtt\':\n\t\t\t#\tdict[word]=oov[np.random.randint(100)]\n\t\t\t#else:\n\t\t\tdict[word] = torch.normal(torch.zeros(EMBEDDING_DIM),std=1)\n\t\tword2id[word]=id\n\t\tpretrained_emb[id] = dict[word].numpy()\n\t\tid+=1\n\n\tif task==\'sts\':\n\t\tcriterion = nn.KLDivLoss()\n\telse:\n\t\tcriterion = torch.nn.NLLLoss(size_average=True)\n\t#criterion = torch.nn.CrossEntropyLoss()\n\tmodel=DecAtt(200,num_class,len(tokens),EMBEDDING_DIM, PROJECTED_EMBEDDING_DIM, pretrained_emb)\n\tif torch.cuda.is_available():\n\t\tmodel = model.cuda()\n\t\tcriterion = criterion.cuda()\n\toptimizer = torch.optim.Adagrad(model.parameters(), lr=0.05, weight_decay=5e-5)\n\n\tprint(\'Start training...\')\n\tbatch_counter = 0\n\tbest_dev_loss=10e10\n\tbest_dev_loss_m=10e10\n\tbest_dev_loss_um=10e10\n\taccumulated_loss=0\n\treport_interval = 1000\n\tmodel.train()\n\n\ttrain_pairs=np.array(train_pairs)\n\trand_idx = np.random.permutation(len(train_pairs))\n\ttrain_pairs = train_pairs[rand_idx]\n\tboth_lengths = np.array([(len(train_pairs[i][0]), len(train_pairs[i][1])) for i in range(len(train_pairs))],\n\t                        dtype={\'names\': [\'x\', \'y\'], \'formats\': [\'i4\', \'i4\']})\n\tsorted_lengths = np.argsort(both_lengths, order=(\'x\', \'y\'))\n\ttrain_pairs = train_pairs[sorted_lengths]\n\ttrain_batch_list=prepare_data(train_pairs, batch_size=32)\n\n\t#dev_pairs=np.array(dev_pairs)\n\t#both_lengths = np.array([(len(dev_pairs[i][0]), len(dev_pairs[i][1])) for i in range(len(dev_pairs))],\n\t#                        dtype={\'names\': [\'x\', \'y\'], \'formats\': [\'i4\', \'i4\']})\n\t#sorted_lengths = np.argsort(both_lengths, order=(\'x\', \'y\'))\n\t#dev_pairs = dev_pairs[sorted_lengths]\n\tdev_batch_list=prepare_data(dev_pairs, batch_size=1)\n\t#test_pairs=np.array(test_pairs)\n\t#both_lengths = np.array([(len(test_pairs[i][0]), len(test_pairs[i][1])) for i in range(len(test_pairs))],\n\t#                        dtype={\'names\': [\'x\', \'y\'], \'formats\': [\'i4\', \'i4\']})\n\t#sorted_lengths = np.argsort(both_lengths, order=(\'x\', \'y\'))\n\t#test_pairs = test_pairs[sorted_lengths]\n\ttest_batch_list=prepare_data(test_pairs, batch_size=1)\n\n\tbatch_index=0\n\tfor epoch in range(num_epochs):\n\t\tbatch_counter=0\n\t\taccumulated_loss = 0\n\t\tmodel.train()\n\t\tprint(\'--\' * 20)\n\t\tstart_time = time.time()\n\t\ttrain_rand_i=np.random.permutation(len(train_batch_list))\n\t\ttrain_batch_i=0\n\t\ttrain_sents_scaned=0\n\t\ttrain_num_correct=0\n\t\twhile train_batch_i<len(train_batch_list):\n\t\t\tbatch_index, batch_index2=train_batch_list[train_rand_i[train_batch_i]]\n\t\t\ttrain_batch_i+=1\n\t\t\t#batch_index2=batch_index+batch_size\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(train_pairs, batch_index, batch_index2)\n\t\t\ttrain_sents_scaned+=len(labels)\n\t\t\t#print(lsize_list)\n\t\t\t#print(rsize_list)\n\t\t\t#batch_index=batch_index2\n\t\t\toptimizer.zero_grad()\n\n\t\t\toutput = model(left_sents, right_sents, lsize_list, rsize_list)\n\t\t\tresult = output.data.cpu().numpy()\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = labels.data.cpu().numpy()\n\t\t\ttrain_num_correct += np.sum(a == b)\n\t\t\t#print(\'forward finished\'+str(datetime.now()))\n\t\t\t#print(output)\n\t\t\t#print(labels)\n\t\t\t#sys.exit()\n\t\t\tloss = criterion(output, labels)\n\t\t\tloss.backward()\n\n\t\t\t\'\'\'\'\'\'\n\t\t\tgrad_norm = 0.\n\t\t\t#para_norm = 0.\n\n\t\t\tfor m in model.modules():\n\t\t\t\tif isinstance(m, nn.Linear):\n\t\t\t\t\t# print(m)\n\t\t\t\t\tgrad_norm += m.weight.grad.data.norm() ** 2\n\t\t\t\t\t#para_norm += m.weight.data.norm() ** 2\n\t\t\t\t\tif m.bias is not None:\n\t\t\t\t\t\tgrad_norm += m.bias.grad.data.norm() ** 2\n\t\t\t\t\t\t#para_norm += m.bias.data.norm() ** 2\n\n\t\t\tgrad_norm ** 0.5\n\t\t\t#para_norm ** 0.5\n\n\t\t\ttry:\n\t\t\t\tshrinkage = args.max_grad_norm / grad_norm\n\t\t\texcept:\n\t\t\t\tpass\n\t\t\tif shrinkage < 1:\n\t\t\t\tfor m in model.modules():\n\t\t\t\t\t# print m\n\t\t\t\t\tif isinstance(m, nn.Linear):\n\t\t\t\t\t\tm.weight.grad.data = m.weight.grad.data * shrinkage\n\t\t\t\t\t\tif m.bias is not None:\n\t\t\t\t\t\t\tm.bias.grad.data = m.bias.grad.data * shrinkage\n\t\t\t\'\'\'\'\'\'\n\t\t\toptimizer.step()\n\t\t\t#print(\'backword finished\' + str(datetime.now()))\n\t\t\tbatch_counter += 1\n\t\t\t#print(batch_counter, loss.data[0])\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\tif batch_counter % report_interval ==0:\n\t\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\t\tmsg += \'\\t train batch loss: %f\' % (accumulated_loss/train_sents_scaned)\n\t\t\t\t#msg += \'\\t train accuracy: %f\' % (train_num_correct/train_sents_scaned)\n\t\t\t\tprint(msg)\n\t\t# valid after each epoch\n\t\tmodel.eval()\n\t\tdev_batch_index = 0\n\t\tdev_num_correct = 0\n\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\taccumulated_loss = 0\n\t\tdev_batch_i = 0\n\t\tpred=[]\n\t\tgold=[]\n\t\twhile dev_batch_i < len(dev_batch_list):\n\t\t\tdev_batch_index, dev_batch_index2 = dev_batch_list[dev_batch_i]\n\t\t\tdev_batch_i += 1\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(dev_pairs, dev_batch_index,\n\t\t\t                                                                       dev_batch_index2)\n\t\t\t# left_sents, right_sents, labels = create_batch(dev_pairs, 0, len(dev_pairs))\n\t\t\toutput = model(left_sents, right_sents, lsize_list, rsize_list)\n\t\t\tresult = np.exp(output.data.cpu().numpy())\n\t\t\tloss = criterion(output, labels)\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = labels.data.cpu().numpy()\n\t\t\tdev_num_correct += np.sum(a == b)\n\t\t\tif task==\'pit\' or task==\'url\' or task==\'wikiqa\' or task==\'trecqa\':\n\t\t\t\tpred.extend(result[:,1])\n\t\t\t\tgold.extend(b)\n\t\t\tif task==\'sts\':\n\t\t\t\tpred.extend(0*result[:,0]+1*result[:,1]+2*result[:,2]+3*result[:,3]+4*result[:,4]+5*result[:,5])\n\t\t\t\tgold.extend(0*b[:,0]+1*b[:,1]+2*b[:,2]+3*b[:,3]+4*b[:,4]+5*b[:,5])\n\t\tmsg += \'\\t dev loss: %f\' % accumulated_loss\n\t\tdev_acc = dev_num_correct / len(dev_pairs)\n\t\t#msg += \'\\t dev accuracy: %f\' % dev_acc\n\t\tprint(msg)\n\t\tif task==\'pit\' or task==\'url\'or task==\'wikiqa\' or task==\'trecqa\':\n\t\t\tURL_maxF1_eval(pred, gold)\n\t\t#Test after each epoch\n\t\ttest_batch_index = 0\n\t\ttest_num_correct = 0\n\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\taccumulated_loss = 0\n\t\ttest_batch_i = 0\n\t\tpred = []\n\t\tgold = []\n\t\twhile test_batch_i < len(test_batch_list):\n\t\t\ttest_batch_index, test_batch_index2 = test_batch_list[test_batch_i]\n\t\t\ttest_batch_i += 1\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(test_pairs, test_batch_index, test_batch_index2)\n\t\t\toutput = model(left_sents, right_sents, lsize_list, rsize_list)\n\t\t\tresult = np.exp(output.data.cpu().numpy())\n\t\t\tloss = criterion(output, labels)\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = labels.data.cpu().numpy()\n\t\t\ttest_num_correct += np.sum(a == b)\n\t\t\tif task == \'pit\' or task == \'url\'or task==\'wikiqa\' or task==\'trecqa\':\n\t\t\t\tpred.extend(result[:, 1])\n\t\t\t\tgold.extend(b)\n\t\tmsg += \'\\t test loss: %f\' % accumulated_loss\n\t\ttest_acc = test_num_correct / len(test_pairs)\n\t\t#msg += \'\\t test accuracy: %f\' % test_acc\n\t\tprint(msg)\n\t\tif task == \'pit\' or task == \'url\'or task==\'wikiqa\' or task==\'trecqa\':\n\t\t\tURL_maxF1_eval(pred, gold)\n\t\t\twith open(basepath + \'/prob_DecAtt_\'+task,\'w\') as f:\n\t\t\t\tfor i in range(len(pred)):\n\t\t\t\t\tf.writelines(str(pred[i])+\'\\n\')\n\t\t\tif task == \'wikiqa\' or task == \'trecqa\':\n\t\t\t\tlist1 = []\n\t\t\t\tfor line in open(basepath+\'/test.qrel\'):\n\t\t\t\t\tlist1.append(line.strip().split())\n\t\t\t\twith open(basepath + \'/result_DecAtt_\' + task, \'w\') as f:\n\t\t\t\t\tfor i in range(len(pred)):\n\t\t\t\t\t\tf.writelines(str(list1[i][0]) + \'\\t\' + str(list1[i][1]) + \'\\t\' + str(\n\t\t\t\t\t\t\tlist1[i][2]) + \'\\t\' + \'*\\t\' + str(pred[i]) + \'\\t\' + \'*\\n\')\n\t\t\tcmd = (\'%s -m map %s %s\' % (castorini_path, basepath + \'/test.qrel\', basepath + \'/result_DecAtt_\' + task))\n\t\t\tos.system(cmd)\n\t\t\tcmd = (\'%s -m recip_rank %s %s\' % (castorini_path, basepath + \'/test.qrel\', basepath + \'/result_DecAtt_\' + task))\n\t\t\tos.system(cmd)\n\t\telapsed_time = time.time() - start_time\n\t\tprint(\'Epoch \' + str(epoch) + \' finished within \' + str(timedelta(seconds=elapsed_time)))\n'"
DecAtt/model.py,31,"b'import sys\nimport math\nimport torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom datetime import datetime\n\n\ndef generate_mask_2(values, sent_sizes):\n\tmask_matrix = np.zeros((len(sent_sizes), max(sent_sizes), values.size(2)))\n\tfor i in range(len(sent_sizes)):\n\t\tmask_matrix[i][:sent_sizes[i]][:]=1\n\tif torch.cuda.is_available():\n\t\tmask_matrix = torch.Tensor(mask_matrix).cuda()\n\telse:\n\t\tmask_matrix = torch.Tensor(mask_matrix)\n\treturn values*Variable(mask_matrix)\n\ndef generate_mask(lsent_sizes, rsent_sizes):\n\tmask_matrix=np.zeros((len(lsent_sizes),max(lsent_sizes),max(rsent_sizes)))\n\tfor i in range(len(lsent_sizes)):\n\t\tmask_matrix[i][:lsent_sizes[i]][:rsent_sizes[i]]=1\n\tif torch.cuda.is_available():\n\t\tmask_matrix = torch.Tensor(mask_matrix).cuda()\n\telse:\n\t\tmask_matrix = torch.Tensor(mask_matrix)\n\treturn Variable(mask_matrix)\n\nclass DecAtt(nn.Module):\n\t""""""\n\t    Implementation of the multi feed forward network model described in\n\t    the paper ""A Decomposable Attention Model for Natural Language\n\t    Inference"" by Parikh et al., 2016.\n\n\t    It applies feedforward MLPs to combinations of parts of the two sentences,\n\t    without any recurrent structure.\n\t""""""\n\tdef __init__(self, num_units, num_classes, vocab_size, embedding_size,\n\t             pretrained_emb, training=True, project_input=True,\n                 use_intra_attention=False, distance_biases=10, max_sentence_length=30):\n\t\t""""""\n\t\tCreate the model based on MLP networks.\n\n\t\t:param num_units: size of the networks\n\t\t:param num_classes: number of classes in the problem\n\t\t:param vocab_size: size of the vocabulary\n\t\t:param embedding_size: size of each word embedding\n\t\t:param use_intra_attention: whether to use intra-attention model\n\t\t:param training: whether to create training tensors (optimizer)\n\t\t:param project_input: whether to project input embeddings to a\n\t\t    different dimensionality\n\t\t:param distance_biases: number of different distances with biases used\n\t\t    in the intra-attention model\n\t\t""""""\n\t\tsuper(DecAtt, self).__init__()\n\t\tself.vocab_size=vocab_size\n\t\tself.num_units = num_units\n\t\tself.num_classes = num_classes\n\t\tself.project_input = project_input\n\t\tself.embedding_size=embedding_size\n\t\tself.distance_biases=distance_biases\n\t\tself.intra_attention=False\n\t\tself.max_sentence_length=max_sentence_length\n\t\tself.pretrained_emb=pretrained_emb\n\n\t\tself.bias_embedding=nn.Embedding(max_sentence_length,1)\n\t\tself.word_embedding=nn.Embedding(vocab_size,embedding_size)\n\t\t#self.linear_layer_project = Variable(torch.Tensor(embedding_size, projected_embedding_size))\n\t\t#if torch.cuda.is_available():\n\t\t#\tself.linear_layer_project=self.linear_layer_project.cuda()\n\t\tself.linear_layer_project = nn.Linear(embedding_size, num_units, bias=False)\n\t\t#self.linear_layer_intra = nn.Sequential(nn.Linear(num_units, num_units), nn.ReLU(), nn.Linear(num_units, num_units), nn.ReLU())\n\n\t\tself.linear_layer_attend = nn.Sequential(nn.Dropout(p=0.2), nn.Linear(num_units, num_units), nn.ReLU(),\n\t\t                                         nn.Dropout(p=0.2), nn.Linear(num_units, num_units), nn.ReLU())\n\n\t\tself.linear_layer_compare = nn.Sequential(nn.Dropout(p=0.2), nn.Linear(num_units*2, num_units), nn.ReLU(),\n\t\t                                          nn.Dropout(p=0.2), nn.Linear(num_units, num_units), nn.ReLU())\n\n\t\tself.linear_layer_aggregate = nn.Sequential(nn.Dropout(p=0.2), nn.Linear(num_units*2, num_units), nn.ReLU(),\n\t\t                                            nn.Dropout(p=0.2), nn.Linear(num_units, num_units), nn.ReLU(),\n\t\t                                            nn.Linear(num_units, num_classes), nn.LogSoftmax())\n\t\tself.init_weight()\n\n\tdef init_weight(self):\n\t\t#print(self.linear_layer_attend[3])\n\t\tself.linear_layer_project.weight.data.normal_(0, 0.01)\n\t\tself.linear_layer_attend[1].weight.data.normal_(0, 0.01)\n\t\tself.linear_layer_attend[1].bias.data.fill_(0)\n\t\tself.linear_layer_attend[4].weight.data.normal_(0, 0.01)\n\t\tself.linear_layer_attend[4].bias.data.fill_(0)\n\t\tself.linear_layer_compare[1].weight.data.normal_(0, 0.01)\n\t\tself.linear_layer_compare[1].bias.data.fill_(0)\n\t\tself.linear_layer_compare[4].weight.data.normal_(0, 0.01)\n\t\tself.linear_layer_compare[4].bias.data.fill_(0)\n\t\tself.linear_layer_aggregate[1].weight.data.normal_(0, 0.01)\n\t\tself.linear_layer_aggregate[1].bias.data.fill_(0)\n\t\tself.linear_layer_aggregate[4].weight.data.normal_(0, 0.01)\n\t\tself.linear_layer_aggregate[4].bias.data.fill_(0)\n\t\t#self.word_embedding.weight.data.copy_(torch.from_numpy(self.pretrained_emb))\n\n\tdef attention_softmax3d(self,raw_attentions):\n\t\treshaped_attentions = raw_attentions.view(-1, raw_attentions.size(2))\n\t\tout=nn.functional.softmax(reshaped_attentions, dim=1)\n\t\treturn out.view(raw_attentions.size(0),raw_attentions.size(1),raw_attentions.size(2))\n\n\tdef _transformation_input(self,embed_sent):\n\t\t#embed_sent=torch.matmul(embed_sent,self.linear_layer_project)\n\t\t#embed_sent=self.word_embedding(embed_sent)\n\t\tembed_sent=self.linear_layer_project(embed_sent)\n\t\tresult=embed_sent\n\t\t#result, (state, _) = self.lstm_intra(embed_sent)\n\t\tif self.intra_attention:\n\t\t\tf_intra = self.linear_layer_intra(embed_sent)\n\t\t\tf_intra_t = torch.transpose(f_intra, 1, 2)\n\t\t\traw_attentions = torch.matmul(f_intra, f_intra_t)\n\t\t\ttime_steps=embed_sent.size(1)\n\t\t\tr = torch.arange(0, time_steps)\n\t\t\tr_matrix=r.view(1,-1).expand(time_steps,time_steps)\n\t\t\traw_index=r_matrix-r.view(-1,1)\n\t\t\tclipped_index=torch.clamp(raw_index,0,self.distance_biases-1)\n\t\t\tclipped_index=Variable(clipped_index.long())\n\t\t\tif torch.cuda.is_available():\n\t\t\t\tclipped_index=clipped_index.cuda()\n\t\t\tbias=self.bias_embedding(clipped_index)\n\t\t\tbias=torch.squeeze(bias)\n\t\t\traw_attentions += bias\n\t\t\tattentions = self.attention_softmax3d(raw_attentions)\n\t\t\tattended = torch.matmul(attentions, embed_sent)\n\t\t\tresult =torch.cat([embed_sent,attended],2)\n\t\treturn result\n\n\tdef attend(self,sent1,sent2, lsize_list, rsize_list):\n\t\t""""""\n\t\tCompute inter-sentence attention. This is step 1 (attend) in the paper\n\n\t\t:param sent1: tensor in shape (batch, time_steps, num_units),\n\t\t    the projected sentence 1\n\t\t:param sent2: tensor in shape (batch, time_steps, num_units)\n\t\t:return: a tuple of 3-d tensors, alfa and beta.\n\t\t""""""\n\t\trepr1=self.linear_layer_attend(sent1)\n\t\trepr2=self.linear_layer_attend(sent2)\n\t\trepr2=torch.transpose(repr2,1,2)\n\t\traw_attentions = torch.matmul(repr1, repr2)\n\n\t\t#self.mask = generate_mask(lsize_list, rsize_list)\n\t\t# masked = mask(self.raw_attentions, rsize_list)\n\t\t#masked = raw_attentions * self.mask\n\t\tatt_sent1 = self.attention_softmax3d(raw_attentions)\n\t\tbeta=torch.matmul(att_sent1,sent2) #input2_soft\n\n\t\traw_attentions_t=torch.transpose(raw_attentions,1,2).contiguous()\n\t\t#self.mask_t = torch.transpose(self.mask, 1, 2).contiguous()\n\t\t# masked = mask(raw_attentions_t, lsize_list)\n\t\t#masked = raw_attentions_t * self.mask_t\n\t\tatt_sent2 = self.attention_softmax3d(raw_attentions_t)\n\t\talpha=torch.matmul(att_sent2,sent1) #input1_soft\n\n\t\treturn alpha, beta\n\n\tdef compare(self,sentence,soft_alignment):\n\t\t""""""\n\t\tApply a feed forward network to compare o   ne sentence to its\n\t\tsoft alignment with the other.\n\n\t\t:param sentence: embedded and projected sentence,\n\t\t    shape (batch, time_steps, num_units)\n\t\t:param soft_alignment: tensor with shape (batch, time_steps, num_units)\n\t\t:return: a tensor (batch, time_steps, num_units)\n\t\t""""""\n\t\tsent_alignment=torch.cat([sentence, soft_alignment],2)\n\t\tout = self.linear_layer_compare(sent_alignment)\n\t\t#out, (state, _) = self.lstm_compare(out)\n\t\treturn out\n\n\tdef aggregate(self,v1, v2):\n\t\t""""""\n\t\tAggregate the representations induced from both sentences and their\n\t\trepresentations\n\n\t\t:param v1: tensor with shape (batch, time_steps, num_units)\n\t\t:param v2: tensor with shape (batch, time_steps, num_units)\n\t\t:return: logits over classes, shape (batch, num_classes)\n\t\t""""""\n\t\tv1_sum=torch.sum(v1,1)\n\t\tv2_sum=torch.sum(v2,1)\n\t\tout=self.linear_layer_aggregate(torch.cat([v1_sum,v2_sum],1))\n\t\treturn out\n\n\tdef forward(self,sent1, sent2, lsize_list, rsize_list):\n\t\tsent1=self._transformation_input(sent1)\n\t\tsent2=self._transformation_input(sent2)\n\t\talpha, beta = self.attend(sent1, sent2, lsize_list, rsize_list)\n\t\tv1=self.compare(sent1,beta)\n\t\tv2=self.compare(sent2,alpha)\n\t\t#v1 = generate_mask_2(v1, lsize_list)\n\t\t#v2 = generate_mask_2(v2, rsize_list)\n\t\tlogits=self.aggregate(v1,v2)\n\t\treturn logits\n\n'"
DecAtt/model_0.851.py,30,"b'import sys\nimport math\nimport torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom datetime import datetime\n\n\ndef generate_mask_2(values, sent_sizes):\n\tmask_matrix = np.zeros((len(sent_sizes), max(sent_sizes), values.size(2)))\n\tfor i in range(len(sent_sizes)):\n\t\tmask_matrix[i][:sent_sizes[i]][:]=1\n\tif torch.cuda.is_available():\n\t\tmask_matrix = torch.Tensor(mask_matrix).cuda()\n\telse:\n\t\tmask_matrix = torch.Tensor(mask_matrix)\n\treturn values*Variable(mask_matrix)\n\ndef generate_mask(lsent_sizes, rsent_sizes):\n\tmask_matrix=np.zeros((len(lsent_sizes),max(lsent_sizes),max(rsent_sizes)))\n\tfor i in range(len(lsent_sizes)):\n\t\tmask_matrix[i][:lsent_sizes[i]][:rsent_sizes[i]]=1\n\tif torch.cuda.is_available():\n\t\tmask_matrix = torch.Tensor(mask_matrix).cuda()\n\telse:\n\t\tmask_matrix = torch.Tensor(mask_matrix)\n\treturn Variable(mask_matrix)\n\nclass DecAtt(nn.Module):\n\t""""""\n\t    Implementation of the multi feed forward network model described in\n\t    the paper ""A Decomposable Attention Model for Natural Language\n\t    Inference"" by Parikh et al., 2016.\n\n\t    It applies feedforward MLPs to combinations of parts of the two sentences,\n\t    without any recurrent structure.\n\t""""""\n\tdef __init__(self, num_units, num_classes, vocab_size, embedding_size,\n\t             pretrained_emb, training=True, project_input=True,\n                 use_intra_attention=False, distance_biases=10, max_sentence_length=30):\n\t\t""""""\n\t\tCreate the model based on MLP networks.\n\n\t\t:param num_units: size of the networks\n\t\t:param num_classes: number of classes in the problem\n\t\t:param vocab_size: size of the vocabulary\n\t\t:param embedding_size: size of each word embedding\n\t\t:param use_intra_attention: whether to use intra-attention model\n\t\t:param training: whether to create training tensors (optimizer)\n\t\t:param project_input: whether to project input embeddings to a\n\t\t    different dimensionality\n\t\t:param distance_biases: number of different distances with biases used\n\t\t    in the intra-attention model\n\t\t""""""\n\t\tsuper(DecAtt, self).__init__()\n\t\tself.vocab_size=vocab_size\n\t\tself.num_units = num_units\n\t\tself.num_classes = num_classes\n\t\tself.project_input = project_input\n\t\tself.embedding_size=embedding_size\n\t\tself.distance_biases=distance_biases\n\t\tself.intra_attention=False\n\t\tself.max_sentence_length=max_sentence_length\n\t\tself.pretrained_emb=pretrained_emb\n\n\t\tself.bias_embedding=nn.Embedding(max_sentence_length,1)\n\t\tself.word_embedding=nn.Embedding(vocab_size,embedding_size)\n\t\t#self.linear_layer_project = Variable(torch.Tensor(embedding_size, projected_embedding_size))\n\t\t#if torch.cuda.is_available():\n\t\t#\tself.linear_layer_project=self.linear_layer_project.cuda()\n\t\tself.linear_layer_project = nn.Linear(embedding_size, num_units, bias=False)\n\n\t\tself.linear_layer_attend = nn.Sequential(nn.Dropout(p=0.2), nn.Linear(num_units, num_units), nn.ReLU(),\n\t\t                                         nn.Dropout(p=0.2), nn.Linear(num_units, num_units), nn.ReLU())\n\n\t\tself.linear_layer_compare = nn.Sequential(nn.Dropout(p=0.2), nn.Linear(num_units*2, num_units), nn.ReLU(),\n\t\t                                          nn.Dropout(p=0.2), nn.Linear(num_units, num_units), nn.ReLU())\n\n\t\tself.linear_layer_aggregate = nn.Sequential(nn.Dropout(p=0.2), nn.Linear(num_units*2, num_units), nn.ReLU(),\n\t\t                                            nn.Dropout(p=0.2), nn.Linear(num_units, num_units), nn.ReLU(),\n\t\t                                            nn.Linear(num_units, num_classes), nn.LogSoftmax())\n\t\tself.init_weight()\n\n\tdef init_weight(self):\n\t\t#print(self.linear_layer_attend[3])\n\t\tself.linear_layer_project.weight.data.normal_(0, 0.01)\n\t\tself.linear_layer_attend[1].weight.data.normal_(0, 0.01)\n\t\tself.linear_layer_attend[1].bias.data.fill_(0)\n\t\tself.linear_layer_attend[4].weight.data.normal_(0, 0.01)\n\t\tself.linear_layer_attend[4].bias.data.fill_(0)\n\t\tself.linear_layer_compare[1].weight.data.normal_(0, 0.01)\n\t\tself.linear_layer_compare[1].bias.data.fill_(0)\n\t\tself.linear_layer_compare[4].weight.data.normal_(0, 0.01)\n\t\tself.linear_layer_compare[4].bias.data.fill_(0)\n\t\tself.linear_layer_aggregate[1].weight.data.normal_(0, 0.01)\n\t\tself.linear_layer_aggregate[1].bias.data.fill_(0)\n\t\tself.linear_layer_aggregate[4].weight.data.normal_(0, 0.01)\n\t\tself.linear_layer_aggregate[4].bias.data.fill_(0)\n\t\tself.word_embedding.weight.data.copy_(self.pretrained_emb)\n\n\tdef attention_softmax3d(self,raw_attentions):\n\t\treshaped_attentions = raw_attentions.view(-1, raw_attentions.size(2))\n\t\tout=nn.functional.softmax(reshaped_attentions, dim=1)\n\t\treturn out.view(raw_attentions.size(0),raw_attentions.size(1),raw_attentions.size(2))\n\n\tdef _transformation_input(self,embed_sent):\n\t\t#embed_sent=torch.matmul(embed_sent,self.linear_layer_project)\n\t\tembed_sent=self.word_embedding(embed_sent)\n\t\tembed_sent=self.linear_layer_project(embed_sent)\n\t\tresult=embed_sent\n\t\t#result, (state, _) = self.lstm_intra(embed_sent)\n\t\tif self.intra_attention:\n\t\t\tf_intra = self.linear_layer_intra(embed_sent)\n\t\t\tf_intra_t = torch.transpose(f_intra, 1, 2)\n\t\t\traw_attentions = torch.matmul(f_intra, f_intra_t)\n\t\t\ttime_steps=embed_sent.size(1)\n\t\t\tr = torch.arange(0, time_steps)\n\t\t\tr_matrix=r.view(1,-1).expand(time_steps,time_steps)\n\t\t\traw_index=r_matrix-r.view(-1,1)\n\t\t\tclipped_index=torch.clamp(raw_index,0,self.distance_biases-1)\n\t\t\tclipped_index=Variable(clipped_index.long())\n\t\t\tif torch.cuda.is_available():\n\t\t\t\tclipped_index=clipped_index.cuda()\n\t\t\tbias=self.bias_embedding(clipped_index)\n\t\t\tbias=torch.squeeze(bias)\n\t\t\traw_attentions += bias\n\t\t\tattentions = self.attention_softmax3d(raw_attentions)\n\t\t\tattended = torch.matmul(attentions, embed_sent)\n\t\t\tresult =torch.cat([embed_sent,attended],2)\n\t\treturn result\n\n\tdef attend(self,sent1,sent2, lsize_list, rsize_list):\n\t\t""""""\n\t\tCompute inter-sentence attention. This is step 1 (attend) in the paper\n\n\t\t:param sent1: tensor in shape (batch, time_steps, num_units),\n\t\t    the projected sentence 1\n\t\t:param sent2: tensor in shape (batch, time_steps, num_units)\n\t\t:return: a tuple of 3-d tensors, alfa and beta.\n\t\t""""""\n\t\trepr1=self.linear_layer_attend(sent1)\n\t\trepr2=self.linear_layer_attend(sent2)\n\t\trepr2=torch.transpose(repr2,1,2)\n\t\traw_attentions = torch.matmul(repr1, repr2)\n\n\t\t#self.mask = generate_mask(lsize_list, rsize_list)\n\t\t# masked = mask(self.raw_attentions, rsize_list)\n\t\t#masked = raw_attentions * self.mask\n\t\tatt_sent1 = self.attention_softmax3d(raw_attentions)\n\t\tbeta=torch.matmul(att_sent1,sent2) #input2_soft\n\n\t\traw_attentions_t=torch.transpose(raw_attentions,1,2).contiguous()\n\t\t#self.mask_t = torch.transpose(self.mask, 1, 2).contiguous()\n\t\t# masked = mask(raw_attentions_t, lsize_list)\n\t\t#masked = raw_attentions_t * self.mask_t\n\t\tatt_sent2 = self.attention_softmax3d(raw_attentions_t)\n\t\talpha=torch.matmul(att_sent2,sent1) #input1_soft\n\n\t\treturn alpha, beta\n\n\tdef compare(self,sentence,soft_alignment):\n\t\t""""""\n\t\tApply a feed forward network to compare o   ne sentence to its\n\t\tsoft alignment with the other.\n\n\t\t:param sentence: embedded and projected sentence,\n\t\t    shape (batch, time_steps, num_units)\n\t\t:param soft_alignment: tensor with shape (batch, time_steps, num_units)\n\t\t:return: a tensor (batch, time_steps, num_units)\n\t\t""""""\n\t\tsent_alignment=torch.cat([sentence, soft_alignment],2)\n\t\tout = self.linear_layer_compare(sent_alignment)\n\t\t#out, (state, _) = self.lstm_compare(out)\n\t\treturn out\n\n\tdef aggregate(self,v1, v2):\n\t\t""""""\n\t\tAggregate the representations induced from both sentences and their\n\t\trepresentations\n\n\t\t:param v1: tensor with shape (batch, time_steps, num_units)\n\t\t:param v2: tensor with shape (batch, time_steps, num_units)\n\t\t:return: logits over classes, shape (batch, num_classes)\n\t\t""""""\n\t\tv1_sum=torch.sum(v1,1)\n\t\tv2_sum=torch.sum(v2,1)\n\t\tout=self.linear_layer_aggregate(torch.cat([v1_sum,v2_sum],1))\n\t\treturn out\n\n\tdef forward(self,sent1, sent2, lsize_list=None, rsize_list=None):\n\t\tsent1=self._transformation_input(sent1)\n\t\tsent2=self._transformation_input(sent2)\n\t\talpha, beta = self.attend(sent1, sent2, lsize_list, rsize_list)\n\t\tv1=self.compare(sent1,beta)\n\t\tv2=self.compare(sent2,alpha)\n\t\t#v1 = generate_mask_2(v1, lsize_list)\n\t\t#v2 = generate_mask_2(v2, rsize_list)\n\t\tlogits=self.aggregate(v1,v2)\n\t\treturn logits\n\n'"
DecAtt/preprocess.py,1,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""Create the data for sentence pair classification\n""""""\n\nimport os\nimport sys\nimport argparse\nimport numpy as np\nimport h5py\nimport torch\nimport itertools\nfrom collections import defaultdict\nfrom torch.autograd import Variable\n\nclass Indexer:\n\tdef __init__(self, symbols = [""<blank>"",""<unk>"",""<s>"",""</s>""]):\n\t\tself.vocab = defaultdict(int)\n\t\tself.PAD = symbols[0]\n\t\tself.UNK = symbols[1]\n\t\tself.BOS = symbols[2]\n\t\tself.EOS = symbols[3]\n\t\tself.d = {self.PAD: 1, self.UNK: 2, self.BOS: 3, self.EOS: 4}\n\n\tdef add_w(self, ws):\n\t\tfor w in ws:\n\t\t\tif w not in self.d:\n\t\t\t\tself.d[w] = len(self.d) + 1\n\n\tdef convert(self, w):\n\t\treturn self.d[w] if w in self.d else self.d[\'<oov\' + str(np.random.randint(1,100)) + \'>\']\n\n\tdef convert_sequence(self, ls):\n\t\treturn [self.convert(l) for l in ls]\n\n\tdef clean(self, s):\n\t\ts = s.replace(self.PAD, """")\n\t\ts = s.replace(self.BOS, """")\n\t\ts = s.replace(self.EOS, """")\n\t\treturn s\n\n\tdef write(self, outfile):\n\t\tout = open(outfile, ""w"")\n\t\titems = [(v, k) for k, v in self.d.iteritems()]\n\t\titems.sort()\n\t\tfor v, k in items:\n\t\t\tprint >>out, k, v\n\t\tout.close()\n\n\tdef prune_vocab(self, k, cnt=False):\n\t\tvocab_list = [(word, count) for word, count in self.vocab.iteritems()]\n\t\tif cnt:\n\t\t\tself.pruned_vocab = {pair[0]:pair[1] for pair in vocab_list if pair[1] > k}\n\t\telse:\n\t\t\tvocab_list.sort(key = lambda x: x[1], reverse=True)\n\t\t\tk = min(k, len(vocab_list))\n\t\t\tself.pruned_vocab = {pair[0]:pair[1] for pair in vocab_list[:k]}\n\t\tfor word in self.pruned_vocab:\n\t\t\tif word not in self.d:\n\t\t\t\tself.d[word] = len(self.d) + 1\n\n\tdef load_vocab(self, vocab_file):\n\t\tself.d = {}\n\t\tfor line in open(vocab_file, \'r\'):\n\t\t\tv, k = line.strip().split()\n\t\t\tself.d[v] = int(k)\n\ndef pad(ls, length, symbol, pad_back = True):\n\tif len(ls) >= length:\n\t\treturn ls[:length]\n\tif pad_back:\n\t\treturn ls + [symbol] * (length -len(ls))\n\telse:\n\t\treturn [symbol] * (length -len(ls)) + ls\n\ndef get_glove_words(f):\n\tglove_words = set()\n\tfor line in open(f, ""r""):\n\t\tword = line.split()[0].strip()\n\t\tglove_words.add(word)\n\treturn glove_words\n\ndef get_data(args):\n\tword_indexer = Indexer([""<blank>"",""<unk>"",""<s>"",""</s>""])\n\tlabel_indexer = Indexer([""<blank>"",""<unk>"",""<s>"",""</s>""])\n\tlabel_indexer.d = {}\n\tglove_vocab = get_glove_words(args.glove)\n\tfor i in range(1,101): #hash oov words to one of 100 random embeddings, per Parikh et al. 2016\n\t\toov_word = \'<oov\'+ str(i) + \'>\'\n\t\tword_indexer.vocab[oov_word] += 1\n\tdef make_vocab(srcfile, targetfile, labelfile, seqlength):\n\t\tnum_sents = 0\n\t\tfor _, (src_orig, targ_orig, label_orig) in \\\n\t\t\t\tenumerate(itertools.izip(open(srcfile,\'r\'),\n\t\t\t\t\t\t\t\t\t\t open(targetfile,\'r\'), open(labelfile, \'r\'))):\n\t\t\tsrc_orig = word_indexer.clean(src_orig.strip())\n\t\t\ttarg_orig = word_indexer.clean(targ_orig.strip())\n\t\t\ttarg = targ_orig.strip().split()\n\t\t\tsrc = src_orig.strip().split()\n\t\t\tlabel = label_orig.strip().split()\n\t\t\tif len(targ) > seqlength or len(src) > seqlength or len(targ) < 1 or len(src) < 1:\n\t\t\t\tcontinue\n\t\t\tnum_sents += 1\n\t\t\tfor word in targ:\n\t\t\t\tif word in glove_vocab:\n\t\t\t\t\tword_indexer.vocab[word] += 1\n\n\t\t\tfor word in src:\n\t\t\t\tif word in glove_vocab:\n\t\t\t\t\tword_indexer.vocab[word] += 1\n\n\t\t\tfor word in label:\n\t\t\t\tlabel_indexer.vocab[word] += 1\n\n\t\treturn num_sents\n\n\tdef convert(srcfile, targetfile, labelfile, batchsize, seqlength, outfile, num_sents,\n\t\t\t\tmax_sent_l=0, shuffle=0):\n\n\t\tnewseqlength = seqlength + 1 #add 1 for BOS\n\t\ttargets = np.zeros((num_sents, newseqlength), dtype=int)\n\t\tsources = np.zeros((num_sents, newseqlength), dtype=int)\n\t\tlabels = np.zeros((num_sents,), dtype =int)\n\t\tsource_lengths = np.zeros((num_sents,), dtype=int)\n\t\ttarget_lengths = np.zeros((num_sents,), dtype=int)\n\t\tboth_lengths = np.zeros(num_sents, dtype = {\'names\': [\'x\',\'y\'], \'formats\': [\'i4\', \'i4\']})\n\t\tdropped = 0\n\t\tsent_id = 0\n\t\tfor _, (src_orig, targ_orig, label_orig) in \\\n\t\t\t\tenumerate(itertools.izip(open(srcfile,\'r\'), open(targetfile,\'r\')\n\t\t\t\t\t\t\t\t\t\t ,open(labelfile,\'r\'))):\n\t\t\tsrc_orig = word_indexer.clean(src_orig.strip())\n\t\t\ttarg_orig = word_indexer.clean(targ_orig.strip())\n\t\t\ttarg =  [word_indexer.BOS] + targ_orig.strip().split()\n\t\t\tsrc =  [word_indexer.BOS] + src_orig.strip().split()\n\t\t\tlabel = label_orig.strip().split()\n\t\t\tmax_sent_l = max(len(targ), len(src), max_sent_l)\n\t\t\tif len(targ) > newseqlength or len(src) > newseqlength or len(targ) < 2 or len(src) < 2:\n\t\t\t\tdropped += 1\n\t\t\t\tcontinue\n\t\t\ttarg = pad(targ, newseqlength, word_indexer.PAD)\n\t\t\ttarg = word_indexer.convert_sequence(targ)\n\t\t\ttarg = np.array(targ, dtype=int)\n\n\t\t\tsrc = pad(src, newseqlength, word_indexer.PAD)\n\t\t\tsrc = word_indexer.convert_sequence(src)\n\t\t\tsrc = np.array(src, dtype=int)\n\n\t\t\ttargets[sent_id] = np.array(targ,dtype=int)\n\t\t\ttarget_lengths[sent_id] = (targets[sent_id] != 1).sum()\n\t\t\tsources[sent_id] = np.array(src, dtype=int)\n\t\t\tsource_lengths[sent_id] = (sources[sent_id] != 1).sum()\n\t\t\tlabels[sent_id] = label_indexer.d[label[0]]\n\t\t\tboth_lengths[sent_id] = (source_lengths[sent_id], target_lengths[sent_id])\n\t\t\tsent_id += 1\n\t\t\tif sent_id % 100000 == 0:\n\t\t\t\tprint(""{}/{} sentences processed"".format(sent_id, num_sents))\n\n\t\tprint(sent_id, num_sents)\n\t\tif shuffle == 1:\n\t\t\trand_idx = np.random.permutation(sent_id)\n\t\t\ttargets = targets[rand_idx]\n\t\t\tsources = sources[rand_idx]\n\t\t\tsource_lengths = source_lengths[rand_idx]\n\t\t\ttarget_lengths = target_lengths[rand_idx]\n\t\t\tlabels = labels[rand_idx]\n\t\t\tboth_lengths = both_lengths[rand_idx]\n\n\t\t#break up batches based on source/target lengths\n\n\n\t\tsource_lengths = source_lengths[:sent_id]\n\t\tsource_sort = np.argsort(source_lengths)\n\n\t\tboth_lengths = both_lengths[:sent_id]\n\t\tsorted_lengths = np.argsort(both_lengths, order = (\'x\', \'y\'))\n\t\tsources = sources[sorted_lengths]\n\t\ttargets = targets[sorted_lengths]\n\t\tlabels = labels[sorted_lengths]\n\t\ttarget_l = target_lengths[sorted_lengths]\n\t\tsource_l = source_lengths[sorted_lengths]\n\n\t\tcurr_l_src = 0\n\t\tcurr_l_targ = 0\n\t\tl_location = [] #idx where sent length changes\n\n\t\tfor j,i in enumerate(sorted_lengths):\n\t\t\tif source_lengths[i] > curr_l_src or target_lengths[i] > curr_l_targ:\n\t\t\t\tcurr_l_src = source_lengths[i]\n\t\t\t\tcurr_l_targ = target_lengths[i]\n\t\t\t\tl_location.append(j+1)\n\t\tl_location.append(len(sources))\n\n\t\t#get batch sizes\n\t\tcurr_idx = 1\n\t\tbatch_idx = [1]\n\t\tbatch_l = []\n\t\ttarget_l_new = []\n\t\tsource_l_new = []\n\t\tfor i in range(len(l_location)-1):\n\t\t\twhile curr_idx < l_location[i+1]:\n\t\t\t\tcurr_idx = min(curr_idx + batchsize, l_location[i+1])\n\t\t\t\tbatch_idx.append(curr_idx)\n\t\tfor i in range(len(batch_idx)-1):\n\t\t\tbatch_l.append(batch_idx[i+1] - batch_idx[i])\n\t\t\tsource_l_new.append(source_l[batch_idx[i]-1])\n\t\t\ttarget_l_new.append(target_l[batch_idx[i]-1])\n\t\t# Write output\n\t\tf = h5py.File(outfile, ""w"")\n\t\tf[""source""] = sources\n\t\tf[""target""] = targets\n\t\tf[""target_l""] = np.array(target_l_new, dtype=int)\n\t\tf[""source_l""] = np.array(source_l_new, dtype=int)\n\t\tf[""label""] = np.array(labels, dtype=int)\n\t\tf[""label_size""] = np.array([len(np.unique(np.array(labels, dtype=int)))])\n\t\tf[""batch_l""] = np.array(batch_l, dtype=int)\n\t\tf[""batch_idx""] = np.array(batch_idx[:-1], dtype=int)\n\t\tf[""source_size""] = np.array([len(word_indexer.d)])\n\t\tf[""target_size""] = np.array([len(word_indexer.d)])\n\t\tprint(""Saved {} sentences (dropped {} due to length/unk filter)"".format(\n\t\t\tlen(f[""source""]), dropped))\n\t\tf.close()\n\t\treturn max_sent_l\n\n\tprint(""First pass through data to get vocab..."")\n\tnum_sents_train = make_vocab(args.srcfile, args.targetfile, args.labelfile,\n\t\t\t\t\t\t\t\t\t\t\t args.seqlength)\n\tprint(""Number of sentences in training: {}"".format(num_sents_train))\n\tnum_sents_valid = make_vocab(args.srcvalfile, args.targetvalfile, args.labelvalfile,\n\t\t\t\t\t\t\t\t\t\t\t args.seqlength)\n\tprint(""Number of sentences in valid: {}"".format(num_sents_valid))\n\tnum_sents_test = make_vocab(args.srctestfile, args.targettestfile, args.labeltestfile,\n\t\t\t\t\t\t\t\t\t\t\t args.seqlength)\n\tprint(""Number of sentences in test: {}"".format(num_sents_test))\n\n\t#prune and write vocab\n\tword_indexer.prune_vocab(0, True)\n\tlabel_indexer.prune_vocab(1000)\n\tif args.vocabfile != \'\':\n\t\tprint(\'Loading pre-specified source vocab from \' + args.vocabfile)\n\t\tword_indexer.load_vocab(args.vocabfile)\n\tword_indexer.write(args.outputfile + "".word.dict"")\n\tlabel_indexer.write(args.outputfile + "".label.dict"")\n\tprint(""Source vocab size: Original = {}, Pruned = {}"".format(len(word_indexer.vocab),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t  len(word_indexer.d)))\n\tprint(""Target vocab size: Original = {}, Pruned = {}"".format(len(word_indexer.vocab),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t  len(word_indexer.d)))\n\n\tmax_sent_l = 0\n\tmax_sent_l = convert(args.srcvalfile, args.targetvalfile, args.labelvalfile,\n\t\t\t\t\t\t args.batchsize, args.seqlength,\n\t\t\t\t\t\t args.outputfile + ""-val.hdf5"", num_sents_valid,\n\t\t\t\t\t\t max_sent_l, args.shuffle)\n\tmax_sent_l = convert(args.srcfile, args.targetfile, args.labelfile,\n\t\t\t\t\t\t args.batchsize, args.seqlength,\n\t\t\t\t\t\t args.outputfile + ""-train.hdf5"", num_sents_train,\n\t\t\t\t\t\t max_sent_l, args.shuffle)\n\tmax_sent_l = convert(args.srctestfile, args.targettestfile, args.labeltestfile,\n\t\t\t\t\t\t args.batchsize, args.seqlength,\n\t\t\t\t\t\t args.outputfile + ""-test.hdf5"", num_sents_test,\n\t\t\t\t\t\t max_sent_l, args.shuffle)\n\tprint(""Max sent length (before dropping): {}"".format(max_sent_l))\n\n\ndef main(arguments):\n\tparser = argparse.ArgumentParser(\n\t\tdescription=__doc__,\n\t\tformatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\tparser.add_argument(\'--vocabsize\', help=""Size of source vocabulary, constructed ""\n\t\t\t\t\t\t\t\t\t\t\t\t""by taking the top X most frequent words. ""\n\t\t\t\t\t\t\t\t\t\t\t\t"" Rest are replaced with special UNK tokens."",\n\t\t\t\t\t\t\t\t\t\t\t\ttype=int, default=50000)\n\tparser.add_argument(\'--srcfile\', help=""Path to sent1 training data."",\n\t\t\t\t\t\tdefault = ""data/entail/src-train.txt"")\n\tparser.add_argument(\'--targetfile\', help=""Path to sent2 training data."",\n\t\t\t\t\t\tdefault = ""data/entail/targ-train.txt"")\n\tparser.add_argument(\'--labelfile\', help=""Path to label data, ""\n\t\t\t\t\t\t\t\t\t\t   ""where each line represents a single ""\n\t\t\t\t\t\t\t\t\t\t   ""label for the sentence pair."",\n\t\t\t\t\t\tdefault = ""data/entail/label-train.txt"")\n\tparser.add_argument(\'--srcvalfile\', help=""Path to sent1 validation data."",\n\t\t\t\t\t\tdefault = ""data/entail/src-dev.txt"")\n\tparser.add_argument(\'--targetvalfile\', help=""Path to sent2 validation data."",\n\t\t\t\t\t\tdefault = ""data/entail/targ-dev.txt"")\n\tparser.add_argument(\'--labelvalfile\', help=""Path to label validation data."",\n\t\t\t\t\t\tdefault = ""data/entail/label-dev.txt"")\n\tparser.add_argument(\'--srctestfile\', help=""Path to sent1 test data."",\n\t\t\t\t\t\tdefault = ""data/entail/src-test.txt"")\n\tparser.add_argument(\'--targettestfile\', help=""Path to sent2 test data."",\n\t\t\t\t\t\tdefault = ""data/entail/targ-test.txt"")\n\tparser.add_argument(\'--labeltestfile\', help=""Path to label test data."",\n\t\t\t\t\t\tdefault = ""data/entail/label-test.txt"")\n\n\tparser.add_argument(\'--batchsize\', help=""Size of each minibatch."", type=int, default=32)\n\tparser.add_argument(\'--seqlength\', help=""Maximum sequence length. Sequences longer ""\n\t\t\t\t\t\t\t\t\t\t\t   ""than this are dropped."", type=int, default=100)\n\tparser.add_argument(\'--outputfile\', help=""Prefix of the output file names. "",\n\t\t\t\t\t\ttype=str, default = ""data/entail"")\n\tparser.add_argument(\'--vocabfile\', help=""If working with a preset vocab, ""\n\t\t\t\t\t\t\t\t\t\t  ""then including this will ignore vocabsize and use the""\n\t\t\t\t\t\t\t\t\t\t  ""vocab provided here."",\n\t\t\t\t\t\t\t\t\t\t  type = str, default=\'\')\n\tparser.add_argument(\'--shuffle\', help=""If = 1, shuffle sentences before sorting (based on  ""\n\t\t\t\t\t\t\t\t\t\t   ""source length)."", type = int, default = 1)\n\tparser.add_argument(\'--glove\', type = str, default = \'\')\n\targs = parser.parse_args(arguments)\n\tget_data(args)\n\nif __name__ == \'__main__\':\n\tsys.exit(main(sys.argv[1:]))\n'"
DecAtt/util.py,0,"b'from __future__ import division\nimport json\n#import nltk\nimport sys\nimport torch\nimport math\nimport logging\nimport numpy as np\nfrom os.path import expanduser\nfrom numpy import linalg as LA\n\n#tokenizer = nltk.tokenize.TreebankWordTokenizer()\n\n\ndef pearson(x,y):\n\tx=np.array(x)\n\ty=np.array(y)\n\tx=x-np.mean(x)\n\ty=y-np.mean(y)\n\treturn x.dot(y)/(LA.norm(x)*LA.norm(y))\n\ndef URL_maxF1_eval(predict_result,test_data_label):\n\ttest_data_label=[item>=1 for item in test_data_label]\n\tcounter = 0\n\ttp = 0.0\n\tfp = 0.0\n\tfn = 0.0\n\ttn = 0.0\n\n\tfor i, t in enumerate(predict_result):\n\n\t\tif t>0.5:\n\t\t\tguess=True\n\t\telse:\n\t\t\tguess=False\n\t\tlabel = test_data_label[i]\n\t\t#print guess, label\n\t\tif guess == True and label == False:\n\t\t\tfp += 1.0\n\t\telif guess == False and label == True:\n\t\t\tfn += 1.0\n\t\telif guess == True and label == True:\n\t\t\ttp += 1.0\n\t\telif guess == False and label == False:\n\t\t\ttn += 1.0\n\t\tif label == guess:\n\t\t\tcounter += 1.0\n\t\t#else:\n\t\t\t#print label+\'--\'*20\n\t\t\t# if guess:\n\t\t\t# print ""GOLD-"" + str(label) + ""\\t"" + ""SYS-"" + str(guess) + ""\\t"" + sent1 + ""\\t"" + sent2\n\n\ttry:\n\t\tP = tp / (tp + fp)\n\t\tR = tp / (tp + fn)\n\t\tF = 2 * P * R / (P + R)\n\texcept:\n\t\tP=0\n\t\tR=0\n\t\tF=0\n\n\t#print ""PRECISION: %s, RECALL: %s, F1: %s"" % (P, R, F)\n\t#print ""ACCURACY: %s"" % (counter/len(predict_result))\n\taccuracy=counter/len(predict_result)\n\n\t#print ""# true pos:"", tp\n\t#print ""# false pos:"", fp\n\t#print ""# false neg:"", fn\n\t#print ""# true neg:"", tn\n\tmaxF1=0\n\tP_maxF1=0\n\tR_maxF1=0\n\tprobs = predict_result\n\tsortedindex = sorted(range(len(probs)), key=probs.__getitem__)\n\tsortedindex.reverse()\n\n\ttruepos=0\n\tfalsepos=0\n\tfor sortedi in sortedindex:\n\t\tif test_data_label[sortedi]==True:\n\t\t\ttruepos+=1\n\t\telif test_data_label[sortedi]==False:\n\t\t\tfalsepos+=1\n\t\tprecision=0\n\t\tif truepos+falsepos>0:\n\t\t\tprecision=truepos/(truepos+falsepos)\n\n\t\trecall=truepos/(tp+fn)\n\t\tf1=0\n\t\tif precision+recall>0:\n\t\t\tf1=2*precision*recall/(precision+recall)\n\t\t\tif f1>maxF1:\n\t\t\t\t#print probs[sortedi]\n\t\t\t\tmaxF1=f1\n\t\t\t\tP_maxF1=precision\n\t\t\t\tR_maxF1=recall\n\tprint ""PRECISION: %s, RECALL: %s, max_F1: %s"" % (P_maxF1, R_maxF1, maxF1)\n\treturn (accuracy, maxF1)\n\ndef tokenize(text):\n\t""""""\n\tTokenize a piece of text using the Treebank tokenizer\n\n\t:return: a list of strings\n\t""""""\n\treturn tokenizer.tokenize(text)\n\t#return text.split()\n\ndef shuffle_arrays(*arrays):\n\t""""""\n\tShuffle all given arrays with the same RNG state.\n\n\tAll shuffling is in-place, i.e., this function returns None.\n\t""""""\n\trng_state = np.random.get_state()\n\tfor array in arrays:\n\t\tnp.random.shuffle(array)\n\t\tnp.random.set_state(rng_state)\n\ndef readSTSdata(dir):\n\t#print(len(dict))\n\t#print(dict[\'bmxs\'])\n\tlsents=[]\n\trsents=[]\n\tlabels=[]\n\tfor line in open(dir+\'a.toks\'):\n\t\tline = line.decode(\'utf-8\')\n\t\tpieces=line.strip().split()\n\t\tlsents.append(pieces)\n\tfor line in open(dir+\'b.toks\'):\n\t\tline = line.decode(\'utf-8\')\n\t\tpieces = line.strip().split()\n\t\trsents.append(pieces)\n\tfor line in open(dir + \'sim.txt\'):\n\t\tsim = float(line.strip())\n\t\tceil = int(math.ceil(sim))\n\t\tfloor = int(math.floor(sim))\n\t\ttmp = [0, 0, 0, 0, 0, 0]\n\t\tif floor != ceil:\n\t\t\ttmp[ceil] = sim - floor\n\t\t\ttmp[floor] = ceil - sim\n\t\telse:\n\t\t\ttmp[floor] = 1\n\t\tlabels.append(tmp)\n\t#data=(lsents,rsents,labels)\n\tif not len(lsents)==len(rsents)==len(labels):\n\t\tprint(\'error!\')\n\t\tsys.exit()\n\tclean_data = []\n\tfor i in range(len(lsents)):\n\t\tclean_data.append((lsents[i], rsents[i], labels[i]))\n\treturn clean_data\n\ndef readQuoradata(dir):\n\tlsents = []\n\trsents = []\n\tlabels = []\n\tfor line in open(dir+\'a.toks\'):\n\t\tline = line.decode(\'utf-8\')\n\t\tpieces=line.strip().split()\n\t\tlsents.append(pieces)\n\tfor line in open(dir+\'b.toks\'):\n\t\tline = line.decode(\'utf-8\')\n\t\tpieces = line.strip().split()\n\t\trsents.append(pieces)\n\tfor line in open(dir + \'sim.txt\'):\n\t\tlabels.append(int(line.strip()))\n\tclean_data = []\n\tfor i in range(len(lsents)):\n\t\tclean_data.append((lsents[i],rsents[i],labels[i]))\n\treturn clean_data\n\ndef readSNLIdata(dir):\n\t#print(len(dict))\n\t#print(dict[\'bmxs\'])\n\tlsents=[]\n\trsents=[]\n\tlabels=[]\n\tfor line in open(dir+\'a.toks\'):\n\t\tpieces=line.strip().split()\n\t\tlsents.append(pieces)\n\tfor line in open(dir+\'b.toks\'):\n\t\tpieces = line.strip().split()\n\t\trsents.append(pieces)\n\tfor line in open(dir + \'sim.txt\'):\n\t\tsim = line.strip()\n\t\tif sim==\'neutral\':\n\t\t\tlabels.append([0,1,0])\n\t\telif sim==\'entailment\':\n\t\t\tlabels.append([1,0,0])\n\t\telif sim==\'contradiction\':\n\t\t\tlabels.append([0,0,1])\n\t\telse:\n\t\t\tlabels.append([0, 0, 0])\n\tclean_data = []\n\tfor i in range(len(lsents)):\n\t\tif labels[i]!=[0,0,0]:\n\t\t\tclean_data.append((lsents[i],rsents[i],labels[i].index(max(labels[i]))))\n\treturn clean_data\n\ndef read_corpus(filename, lowercase):\n\t""""""\n\tRead a JSONL or TSV file with the SNLI corpus\n\n\t:param filename: path to the file\n\t:param lowercase: whether to convert content to lower case\n\t:return: a list of tuples (first_sent, second_sent, label)\n\t""""""\n\tlogging.info(\'Reading data from %s\' % filename)\n\t# we are only interested in the actual sentences + gold label\n\t# the corpus files has a few more things\n\tuseful_data = []\n\n\t# the SNLI corpus has one JSON object per line\n\twith open(filename, \'rb\') as f:\n\n\t\tif filename.endswith(\'.tsv\') or filename.endswith(\'.txt\'):\n\n\t\t\tfor line in f:\n\t\t\t\tline = line.decode(\'utf-8\').strip()\n\t\t\t\tif lowercase:\n\t\t\t\t\tline = line.lower()\n\t\t\t\tif len(line.split(\'\\t\')) == 10:\n\t\t\t\t\t(label, _, _, _, _, sent1, sent2, _, _, _) = line.split(\'\\t\')\n\t\t\t\telif len(line.split(\'\\t\')) == 14:\n\t\t\t\t\t(label, _, _, _, _, sent1, sent2, _, _, _, _, _, _, _) = line.split(\'\\t\')\n\t\t\t\t#sent1, sent2, label = line.split(\'\\t\')\n\t\t\t\tif label not in (\'contradiction\',\'neutral\',\'entailment\'):\n\t\t\t\t\tcontinue\n\t\t\t\ttokens1 = tokenize(sent1)\n\t\t\t\ttokens2 = tokenize(sent2)\n\t\t\t\tuseful_data.append((tokens1, tokens2, (\'contradiction\',\'neutral\',\'entailment\').index(label)))\n\t\telse:\n\t\t\tfor line in f:\n\t\t\t\tline = line.decode(\'utf-8\')\n\t\t\t\tif lowercase:\n\t\t\t\t\tline = line.lower()\n\t\t\t\tdata = json.loads(line)\n\t\t\t\tif data[\'gold_label\'] == \'-\':\n\t\t\t\t\t# ignore items without a gold label\n\t\t\t\t\tcontinue\n\n\t\t\t\tsentence1_parse = data[\'sentence1_parse\']\n\t\t\t\tsentence2_parse = data[\'sentence2_parse\']\n\t\t\t\tlabel = data[\'gold_label\']\n\n\t\t\t\ttree1 = nltk.Tree.fromstring(sentence1_parse)\n\t\t\t\ttree2 = nltk.Tree.fromstring(sentence2_parse)\n\t\t\t\ttokens1 = tree1.leaves()\n\t\t\t\ttokens2 = tree2.leaves()\n\t\t\t\tt = (tokens1, tokens2, (\'neutral\',\'contradiction\',\'entailment\', \'hidden\').index(label))\n\t\t\t\tuseful_data.append(t)\n\n\treturn useful_data\n'"
ESIM/data_iterator.py,0,"b'import cPickle as pkl\nimport gzip\nimport numpy\nimport random\nimport math\n\ndef fopen(filename, mode=\'r\'):\n    if filename.endswith(\'.gz\'):\n        return gzip.open(filename, mode)\n    return open(filename, mode)\n\nclass TextIterator:\n    """"""Simple Bitext iterator.""""""\n    def __init__(self, source, target, label,\n                 dict,\n                 batch_size=128,\n                 n_words=-1,\n                 shuffle=True):\n        self.source = fopen(source, \'r\')\n        self.target = fopen(target, \'r\')\n        self.label = fopen(label, \'r\')\n        with open(dict, \'rb\') as f:\n            self.dict = pkl.load(f)\n        self.batch_size = batch_size\n        self.n_words = n_words\n        self.shuffle = shuffle\n        self.end_of_data = False\n\n        self.source_buffer = []\n        self.target_buffer = []\n        self.label_buffer = []\n        self.k = batch_size\n\n    def __iter__(self):\n        return self\n\n    def reset(self):\n        self.source.seek(0)\n        self.target.seek(0)\n        self.label.seek(0)\n\n    def next(self):\n        if self.end_of_data:\n            self.end_of_data = False\n            self.reset()\n            raise StopIteration\n\n        source = []\n        target = []\n        label = []\n\n        # fill buffer, if it\'s empty\n        assert len(self.source_buffer) == len(self.target_buffer), \'Buffer size mismatch!\'\n        assert len(self.source_buffer) == len(self.label_buffer), \'Buffer size mismatch!\'\n\n        if len(self.source_buffer) == 0:\n            for k_ in xrange(self.k):\n                ss = self.source.readline()\n                if ss == """":\n                    break\n                tt = self.target.readline()\n                if tt == """":\n                    break\n                ll = self.label.readline()\n                if ll == """":\n                    break\n\n                self.source_buffer.append(ss.strip().split())\n                self.target_buffer.append(tt.strip().split())\n                self.label_buffer.append(ll.strip())\n\n            if self.shuffle:\n                # sort by target buffer\n                tlen = numpy.array([len(t) for t in self.target_buffer])\n                tidx = tlen.argsort()\n                # shuffle mini-batch\n                tindex = []\n                small_index = range(int(math.ceil(len(tidx)*1./self.batch_size)))\n                random.shuffle(small_index)\n                for i in small_index:\n                    if (i+1)*self.batch_size > len(tidx):\n                        tindex.extend(tidx[i*self.batch_size:])\n                    else:\n                        tindex.extend(tidx[i*self.batch_size:(i+1)*self.batch_size])\n\n                tidx = tindex\n\n                _sbuf = [self.source_buffer[i] for i in tidx]\n                _tbuf = [self.target_buffer[i] for i in tidx]\n                _lbuf = [self.label_buffer[i] for i in tidx]\n\n                self.source_buffer = _sbuf\n                self.target_buffer = _tbuf\n                self.label_buffer = _lbuf\n\n        if len(self.source_buffer) == 0 or len(self.target_buffer) == 0 or len(self.label_buffer) == 0:\n            self.end_of_data = False\n            self.reset()\n            raise StopIteration\n\n        try:\n\n            # actual work here\n            while True:\n\n                # read from source file and map to word index\n                try:\n                    ss = self.source_buffer.pop(0)\n                except IndexError:\n                    break\n\n                ss.insert(0, \'_BOS_\')\n                ss.append(\'_EOS_\')\n                ss = [self.dict[w] if w in self.dict else 1\n                      for w in ss]\n                if self.n_words > 0:\n                    ss = [w if w < self.n_words else 1 for w in ss]\n\n                # read from source file and map to word index\n                tt = self.target_buffer.pop(0)\n                tt.insert(0, \'_BOS_\')\n                tt.append(\'_EOS_\')\n                tt = [self.dict[w] if w in self.dict else 1\n                      for w in tt]\n                if self.n_words > 0:\n                    tt = [w if w < self.n_words else 1 for w in tt]\n\n                # read label \n                ll = self.label_buffer.pop(0)\n\n                source.append(ss)\n                target.append(tt)\n                label.append(ll)\n\n                if len(source) >= self.batch_size or \\\n                        len(target) >= self.batch_size or \\\n                        len(label) >= self.batch_size:\n                    break\n        except IOError:\n            self.end_of_data = True\n\n        if len(source) <= 0 or len(target) <= 0 or len(label) <= 0:\n            self.end_of_data = False\n            self.reset()\n            raise StopIteration\n\n        return source, target, label\n'"
ESIM/main_batch_mnli.py,19,"b'from __future__ import division\nimport sys\nfrom data_iterator import TextIterator\nimport cPickle as pkl\nfrom os.path import expanduser\nimport torch\nimport numpy\nfrom model_batch import *\nimport time\nfrom datetime import timedelta\nfrom torchtext.vocab import load_word_vectors\n\n# batch preparation\n# batch preparation\ndef prepare_data(seqs_x, seqs_y, labels, maxlen=None):\n\tlengths_x = [len(s) for s in seqs_x]\n\tlengths_y = [len(s) for s in seqs_y]\n\n\tif maxlen is not None:\n\t\tnew_seqs_x = []\n\t\tnew_seqs_y = []\n\t\tnew_lengths_x = []\n\t\tnew_lengths_y = []\n\t\tnew_labels = []\n\t\tfor l_x, s_x, l_y, s_y, l in zip(lengths_x, seqs_x, lengths_y, seqs_y, labels):\n\t\t\tif l_x < maxlen and l_y < maxlen:\n\t\t\t\tnew_seqs_x.append(s_x)\n\t\t\t\tnew_lengths_x.append(l_x)\n\t\t\t\tnew_seqs_y.append(s_y)\n\t\t\t\tnew_lengths_y.append(l_y)\n\t\t\t\tnew_labels.append(l)\n\t\tlengths_x = new_lengths_x\n\t\tseqs_x = new_seqs_x\n\t\tlengths_y = new_lengths_y\n\t\tseqs_y = new_seqs_y\n\t\tlabels = new_labels\n\n\t\tif len(lengths_x) < 1 or len(lengths_y) < 1:\n\t\t\treturn None, None, None, None, None\n\n\tn_samples = len(seqs_x)\n\tmaxlen_x = numpy.max(lengths_x)\n\tmaxlen_y = numpy.max(lengths_y)\n\n\tx = numpy.zeros((maxlen_x, n_samples)).astype(\'int64\')\n\ty = numpy.zeros((maxlen_y, n_samples)).astype(\'int64\')\n\tx_mask = numpy.zeros((maxlen_x, n_samples)).astype(\'float32\')\n\ty_mask = numpy.zeros((maxlen_y, n_samples)).astype(\'float32\')\n\tl = numpy.zeros((n_samples,)).astype(\'int64\')\n\tfor idx, [s_x, s_y, ll] in enumerate(zip(seqs_x, seqs_y, labels)):\n\t\tx[:lengths_x[idx], idx] = s_x\n\t\tx_mask[:lengths_x[idx], idx] = 1.\n\t\ty[:lengths_y[idx], idx] = s_y\n\t\ty_mask[:lengths_y[idx], idx] = 1.\n\t\tl[idx] = ll\n\n\tif torch.cuda.is_available():\n\t\tx=Variable(torch.LongTensor(x)).cuda()\n\t\tx_mask=Variable(torch.Tensor(x_mask)).cuda()\n\t\ty=Variable(torch.LongTensor(y)).cuda()\n\t\ty_mask=Variable(torch.Tensor(y_mask)).cuda()\n\t\tl=Variable(torch.LongTensor(l)).cuda()\n\telse:\n\t\tx = Variable(torch.LongTensor(x))\n\t\tx_mask = Variable(torch.FloatTensor(x_mask))\n\t\ty = Variable(torch.LongTensor(y))\n\t\ty_mask = Variable(torch.FloatTensor(y_mask))\n\t\tl = Variable(torch.LongTensor(l))\n\treturn x, x_mask, y, y_mask, l\n\n# some utilities\ndef ortho_weight(ndim):\n\t""""""\n\tRandom orthogonal weights\n\n\tUsed by norm_weights(below), in which case, we\n\tare ensuring that the rows are orthogonal\n\t(i.e W = U \\Sigma V, U has the same\n\t# of rows, V has the same # of cols)\n\t""""""\n\tW = numpy.random.randn(ndim, ndim)\n\tu, s, v = numpy.linalg.svd(W)\n\treturn u.astype(\'float32\')\n\n\ndef norm_weight(nin, nout=None, scale=0.01, ortho=True):\n\t""""""\n\tRandom weights drawn from a Gaussian\n\t""""""\n\tif nout is None:\n\t\tnout = nin\n\tif nout == nin and ortho:\n\t\tW = ortho_weight(nin)\n\telse:\n\t\tW = scale * numpy.random.randn(nin, nout)\n\treturn W.astype(\'float32\')\n\nif torch.cuda.is_available():\n\tprint(\'CUDA is available!\')\n\tbase_path = expanduser(""~"") + \'/pytorch/ESIM\'\n\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\nelse:\n\tbase_path = expanduser(""~"") + \'/Documents/research/pytorch/ESIM\'\n\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\ntask=\'mnli\'\nprint(\'task: \'+task)\nif task==\'snli\':\n\tdictionary=base_path+\'/data/word_sequence/snli_vocab_cased.pkl\'\n\tdatasets         = [base_path+\'/data/word_sequence/premise_snli_1.0_train.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/hypothesis_snli_1.0_train.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/label_snli_1.0_train.txt\']\n\tvalid_datasets   = [base_path+\'/data/word_sequence/premise_snli_1.0_dev.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/hypothesis_snli_1.0_dev.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/label_snli_1.0_dev.txt\']\n\ttest_datasets    = [base_path+\'/data/word_sequence/premise_snli_1.0_test.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/hypothesis_snli_1.0_test.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/label_snli_1.0_test.txt\']\nelif task==\'mnli\':\n\tdic = {\'entailment\': \'0\', \'neutral\': \'1\', \'contradiction\': \'2\'}\n\tdictionary = base_path + \'/data/word_sequence/mnli_vocab_cased.pkl\'\n\tdatasets = [base_path + \'/data/word_sequence/premise_multinli_1.0_train.txt\',\n\t\t\t\tbase_path + \'/data/word_sequence/hypothesis_multinli_1.0_train.txt\',\n\t\t\t\tbase_path + \'/data/word_sequence/label_multinli_1.0_train.txt\']\n\tvalid_datasets_m = [base_path + \'/data/word_sequence/premise_multinli_1.0_dev_matched.txt\',\n\t\t\t\t\t  base_path + \'/data/word_sequence/hypothesis_multinli_1.0_dev_matched.txt\',\n\t\t\t\t\t  base_path + \'/data/word_sequence/label_multinli_1.0_dev_matched.txt\']\n\tvalid_datasets_um = [base_path + \'/data/word_sequence/premise_multinli_1.0_dev_mismatched.txt\',\n\t\t\t\t\t\tbase_path + \'/data/word_sequence/hypothesis_multinli_1.0_dev_mismatched.txt\',\n\t\t\t\t\t\tbase_path + \'/data/word_sequence/label_multinli_1.0_dev_mismatched.txt\']\n\ttest_datasets_m = [base_path + \'/data/word_sequence/premise_multinli_1.0_test_matched.txt\',\n\t\t\t\t\t  base_path + \'/data/word_sequence/hypothesis_multinli_1.0_test_matched.txt\',\n\t\t\t\t\t  base_path + \'/data/word_sequence/label_multinli_1.0_test_matched.txt\']\n\ttest_datasets_um = [base_path + \'/data/word_sequence/premise_multinli_1.0_test_mismatched.txt\',\n\t\t\t\t\t\tbase_path + \'/data/word_sequence/hypothesis_multinli_1.0_test_mismatched.txt\',\n\t\t\t\t\t\tbase_path + \'/data/word_sequence/label_multinli_1.0_test_mismatched.txt\']\n#n_words=42394\ndim_word=300\nbatch_size=32\nnum_epochs=500\nvalid_batch_size=32\nprint \'Loading data\'\nwith open(dictionary, \'rb\') as f:\n\tworddicts = pkl.load(f)\nn_words=len(worddicts)\nwv_dict, wv_arr, wv_size = load_word_vectors(embedding_path, \'glove.840B\', dim_word)\npretrained_emb=norm_weight(n_words, dim_word)\nfor word in worddicts.keys():\n\ttry:\n\t\tpretrained_emb[worddicts[word]]=wv_arr[wv_dict[word]].numpy()\n\texcept:\n\t\tpretrained_emb[worddicts[word]] = torch.normal(torch.zeros(dim_word),std=1).numpy()\ntrain = TextIterator(datasets[0], datasets[1], datasets[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=batch_size)\n\'\'\'\ntrain_valid = TextIterator(datasets[0], datasets[1], datasets[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=valid_batch_size,\n\t\t\t\t\t shuffle=False)\n\'\'\'\n\nvalid_m = TextIterator(valid_datasets_m[0], valid_datasets_m[1], valid_datasets_m[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=valid_batch_size,\n\t\t\t\t\t shuffle=False)\nvalid_um = TextIterator(valid_datasets_um[0], valid_datasets_um[1], valid_datasets_um[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=valid_batch_size,\n\t\t\t\t\t shuffle=False)\ntest_m = TextIterator(test_datasets_m[0], test_datasets_m[1], test_datasets_m[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=valid_batch_size,\n\t\t\t\t\t shuffle=False)\ntest_um = TextIterator(test_datasets_um[0], test_datasets_um[1], test_datasets_um[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=valid_batch_size,\n\t\t\t\t\t shuffle=False)\n\'\'\'\nvalid = TextIterator(valid_datasets[0],valid_datasets[1],valid_datasets[2],\n                     dictionary,\n                     n_words=n_words,\n                     batch_size=valid_batch_size,\n                     shuffle=False)\ntest = TextIterator(test_datasets[0], test_datasets[1], test_datasets[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=valid_batch_size,\n\t\t\t\t\t shuffle=False)\n\'\'\'\ncriterion = torch.nn.CrossEntropyLoss()\nmodel = ESIM(dim_word, 3, n_words, dim_word, pretrained_emb)\nif torch.cuda.is_available():\n\tmodel = model.cuda()\n\tcriterion = criterion.cuda()\noptimizer = torch.optim.Adam(model.parameters(),lr=0.0004)\nprint(\'start training...\')\naccumulated_loss=0\nbatch_counter=0\nreport_interval = 1000\nbest_dev_loss=10e10\nbest_dev_loss2=10e10\nclip_c=10\nmax_len=100\nmax_result=0\nmax_result_um=0\nmodel.train()\nfor epoch in range(num_epochs):\n\taccumulated_loss = 0\n\tmodel.train()\n\tprint(\'--\' * 20)\n\tstart_time = time.time()\n\ttrain_sents_scaned = 0\n\ttrain_num_correct = 0\n\tbatch_counter=0\n\tfor x1, x2, y in train:\n\t\tx1, x1_mask, x2, x2_mask, y = prepare_data(x1, x2, y, maxlen=max_len)\n\t\ttrain_sents_scaned += len(y)\n\t\toptimizer.zero_grad()\n\t\toutput = model(x1, x1_mask, x2, x2_mask)\n\t\tresult = output.data.cpu().numpy()\n\t\ta = np.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\ttrain_num_correct += np.sum(a == b)\n\t\tloss = criterion(output, y)\n\t\tloss.backward()\n\n\t\t\'\'\'\'\'\'\n\t\tgrad_norm = 0.\n\n\t\tfor m in list(model.parameters()):\n\t\t\tgrad_norm+=m.grad.data.norm() ** 2\n\n\t\tfor m in list(model.parameters()):\n\t\t\tif grad_norm>clip_c**2:\n\t\t\t\ttry:\n\t\t\t\t\tm.grad.data= m.grad.data / torch.sqrt(grad_norm) * clip_c\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\t\t\'\'\'\'\'\'\n\n\t\toptimizer.step()\n\t\taccumulated_loss += loss.data[0]\n\t\tbatch_counter += 1\n\t\tif batch_counter % report_interval == 0:\n\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\tmsg += \'\\t training batch loss: %f\' % (accumulated_loss / train_sents_scaned)\n\t\t\tmsg += \'\\t train accuracy: %f\' % (train_num_correct / train_sents_scaned)\n\t\t\tprint(msg)\n\t# valid_m after each epoch\n\tmodel.eval()\n\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\taccumulated_loss = 0\n\tdev_num_correct = 0\n\tn_done=0\n\tpred=[]\n\tfor dev_x1, dev_x2, dev_y in valid_m:\n\t\tn_done += len(dev_x1)\n\t\tx1, x1_mask, x2, x2_mask, y = prepare_data(dev_x1, dev_x2, dev_y, maxlen=max_len)\n\t\twith torch.no_grad():\n\t\t\toutput = F.softmax(model(x1, x1_mask, x2, x2_mask))\n\t\tresult = output.data.cpu().numpy()\n\t\tloss = criterion(output, y)\n\t\taccumulated_loss += loss.data[0]\n\t\ta = numpy.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\tdev_num_correct += numpy.sum(a == b)\n\t\tpred.extend(result)\n\tmsg += \'\\t dev_m loss: %f\' % (accumulated_loss/n_done)\n\tdev_acc = dev_num_correct / n_done\n\tmsg += \'\\t dev_m accuracy: %f\' % dev_acc\n\tprint(msg)\n\tif dev_acc>max_result:\n\t\tmax_result=dev_acc\n\t\twith open(base_path+\'/prob_ESIM_\' + task+\'_m\',\'w\') as f:\n\t\t\tfor item in pred:\n\t\t\t\tf.writelines(str(item[0])+\'\\t\'+str(item[1])+\'\\t\'+str(item[2])+\'\\n\')\n\t# valid_um after each epoch\n\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\taccumulated_loss = 0\n\tdev_num_correct = 0\n\tn_done = 0\n\tpred=[]\n\tfor dev_x1, dev_x2, dev_y in valid_um:\n\t\tn_done += len(dev_x1)\n\t\tx1, x1_mask, x2, x2_mask, y = prepare_data(dev_x1, dev_x2, dev_y, maxlen=max_len)\n\t\twith torch.no_grad():\n\t\t\toutput = F.softmax(model(x1, x1_mask, x2, x2_mask))\n\t\tresult = output.data.cpu().numpy()\n\t\tloss = criterion(output, y)\n\t\taccumulated_loss += loss.data[0]\n\t\ta = numpy.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\tdev_num_correct += numpy.sum(a == b)\n\t\tpred.extend(result)\n\tmsg += \'\\t dev_um loss: %f\' % (accumulated_loss / n_done)\n\tdev_acc = dev_num_correct / n_done\n\tmsg += \'\\t dev_um accuracy: %f\' % dev_acc\n\tprint(msg)\n\tif dev_acc > max_result_um:\n\t\tmax_result_um = dev_acc\n\t\twith open(base_path + \'/prob_ESIM_\' + task + \'_um\', \'w\') as f:\n\t\t\tfor item in pred:\n\t\t\t\tf.writelines(str(item[0]) + \'\\t\' + str(item[1]) + \'\\t\' + str(item[2]) + \'\\n\')\n\telapsed_time = time.time() - start_time\n\tprint(\'Epoch \' + str(epoch) + \' finished within \' + str(timedelta(seconds=elapsed_time)))'"
ESIM/main_batch_pit.py,20,"b'from __future__ import division\nimport sys\nfrom data_iterator import TextIterator\nimport cPickle as pkl\nfrom os.path import expanduser\nimport torch\nimport numpy\nfrom model_batch import *\nimport time\nfrom datetime import timedelta\nfrom torchtext.vocab import load_word_vectors\n\n# batch preparation\n# batch preparation\ndef prepare_data(seqs_x, seqs_y, labels, maxlen=None):\n\tlengths_x = [len(s) for s in seqs_x]\n\tlengths_y = [len(s) for s in seqs_y]\n\n\tif maxlen is not None:\n\t\tnew_seqs_x = []\n\t\tnew_seqs_y = []\n\t\tnew_lengths_x = []\n\t\tnew_lengths_y = []\n\t\tnew_labels = []\n\t\tfor l_x, s_x, l_y, s_y, l in zip(lengths_x, seqs_x, lengths_y, seqs_y, labels):\n\t\t\tif l_x < maxlen and l_y < maxlen:\n\t\t\t\tnew_seqs_x.append(s_x)\n\t\t\t\tnew_lengths_x.append(l_x)\n\t\t\t\tnew_seqs_y.append(s_y)\n\t\t\t\tnew_lengths_y.append(l_y)\n\t\t\t\tnew_labels.append(l)\n\t\tlengths_x = new_lengths_x\n\t\tseqs_x = new_seqs_x\n\t\tlengths_y = new_lengths_y\n\t\tseqs_y = new_seqs_y\n\t\tlabels = new_labels\n\n\t\tif len(lengths_x) < 1 or len(lengths_y) < 1:\n\t\t\treturn None, None, None, None, None\n\n\tn_samples = len(seqs_x)\n\tmaxlen_x = numpy.max(lengths_x)\n\tmaxlen_y = numpy.max(lengths_y)\n\n\tx = numpy.zeros((maxlen_x, n_samples)).astype(\'int64\')\n\ty = numpy.zeros((maxlen_y, n_samples)).astype(\'int64\')\n\tx_mask = numpy.zeros((maxlen_x, n_samples)).astype(\'float32\')\n\ty_mask = numpy.zeros((maxlen_y, n_samples)).astype(\'float32\')\n\tl = numpy.zeros((n_samples,)).astype(\'int64\')\n\tfor idx, [s_x, s_y, ll] in enumerate(zip(seqs_x, seqs_y, labels)):\n\t\tx[:lengths_x[idx], idx] = s_x\n\t\tx_mask[:lengths_x[idx], idx] = 1.\n\t\ty[:lengths_y[idx], idx] = s_y\n\t\ty_mask[:lengths_y[idx], idx] = 1.\n\t\tl[idx] = ll\n\n\tif torch.cuda.is_available():\n\t\tx=Variable(torch.LongTensor(x)).cuda()\n\t\tx_mask=Variable(torch.Tensor(x_mask)).cuda()\n\t\ty=Variable(torch.LongTensor(y)).cuda()\n\t\ty_mask=Variable(torch.Tensor(y_mask)).cuda()\n\t\tl=Variable(torch.LongTensor(l)).cuda()\n\telse:\n\t\tx = Variable(torch.LongTensor(x))\n\t\tx_mask = Variable(torch.FloatTensor(x_mask))\n\t\ty = Variable(torch.LongTensor(y))\n\t\ty_mask = Variable(torch.FloatTensor(y_mask))\n\t\tl = Variable(torch.LongTensor(l))\n\treturn x, x_mask, y, y_mask, l\n\n# some utilities\ndef ortho_weight(ndim):\n\t""""""\n\tRandom orthogonal weights\n\n\tUsed by norm_weights(below), in which case, we\n\tare ensuring that the rows are orthogonal\n\t(i.e W = U \\Sigma V, U has the same\n\t# of rows, V has the same # of cols)\n\t""""""\n\tW = numpy.random.randn(ndim, ndim)\n\tu, s, v = numpy.linalg.svd(W)\n\treturn u.astype(\'float32\')\n\n\ndef norm_weight(nin, nout=None, scale=0.01, ortho=True):\n\t""""""\n\tRandom weights drawn from a Gaussian\n\t""""""\n\tif nout is None:\n\t\tnout = nin\n\tif nout == nin and ortho:\n\t\tW = ortho_weight(nin)\n\telse:\n\t\tW = scale * numpy.random.randn(nin, nout)\n\treturn W.astype(\'float32\')\n\nif torch.cuda.is_available():\n\tprint(\'CUDA is available!\')\n\tbase_path = expanduser(""~"") + \'/pytorch/ESIM\'\n\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\nelse:\n\tbase_path = expanduser(""~"") + \'/Documents/research/pytorch/ESIM\'\n\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\ntask=\'pit\'\nprint(\'task: \'+task)\nif task==\'snli\':\n\tdictionary=base_path+\'/data/word_sequence/snli_vocab_cased.pkl\'\n\tdatasets         = [base_path+\'/data/word_sequence/premise_snli_1.0_train.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/hypothesis_snli_1.0_train.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/label_snli_1.0_train.txt\']\n\tvalid_datasets   = [base_path+\'/data/word_sequence/premise_snli_1.0_dev.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/hypothesis_snli_1.0_dev.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/label_snli_1.0_dev.txt\']\n\ttest_datasets    = [base_path+\'/data/word_sequence/premise_snli_1.0_test.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/hypothesis_snli_1.0_test.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/label_snli_1.0_test.txt\']\nelif task==\'mnli\':\n\tdic = {\'entailment\': \'0\', \'neutral\': \'1\', \'contradiction\': \'2\'}\n\tdictionary = base_path + \'/data/word_sequence/mnli_vocab_cased.pkl\'\n\tdatasets = [base_path + \'/data/word_sequence/premise_multinli_1.0_train.txt\',\n\t\t\t\tbase_path + \'/data/word_sequence/hypothesis_multinli_1.0_train.txt\',\n\t\t\t\tbase_path + \'/data/word_sequence/label_multinli_1.0_train.txt\']\n\tvalid_datasets_m = [base_path + \'/data/word_sequence/premise_multinli_1.0_dev_matched.txt\',\n\t\t\t\t\t  base_path + \'/data/word_sequence/hypothesis_multinli_1.0_dev_matched.txt\',\n\t\t\t\t\t  base_path + \'/data/word_sequence/label_multinli_1.0_dev_matched.txt\']\n\tvalid_datasets_um = [base_path + \'/data/word_sequence/premise_multinli_1.0_dev_mismatched.txt\',\n\t\t\t\t\t\tbase_path + \'/data/word_sequence/hypothesis_multinli_1.0_dev_mismatched.txt\',\n\t\t\t\t\t\tbase_path + \'/data/word_sequence/label_multinli_1.0_dev_mismatched.txt\']\n\ttest_datasets_m = [base_path + \'/data/word_sequence/premise_multinli_1.0_test_matched.txt\',\n\t\t\t\t\t  base_path + \'/data/word_sequence/hypothesis_multinli_1.0_test_matched.txt\',\n\t\t\t\t\t  base_path + \'/data/word_sequence/label_multinli_1.0_test_matched.txt\']\n\ttest_datasets_um = [base_path + \'/data/word_sequence/premise_multinli_1.0_test_mismatched.txt\',\n\t\t\t\t\t\tbase_path + \'/data/word_sequence/hypothesis_multinli_1.0_test_mismatched.txt\',\n\t\t\t\t\t\tbase_path + \'/data/word_sequence/label_multinli_1.0_test_mismatched.txt\']\nelif task==\'pit\':\n\tif torch.cuda.is_available():\n\t\tprint(\'CUDA is available!\')\n\t\tbase_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/pit\'\n\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\telse:\n\t\tbase_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/pit\'\n\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\tdictionary=base_path+\'/pit_vocab_cased.pkl\'\n\tdatasets=[base_path+\'/train/a.toks\', base_path+\'/train/b.toks\', base_path+\'/train/sim.txt\']\n\ttest_datasets=[base_path+\'/test/a.toks\', base_path+\'/test/b.toks\', base_path+\'/test/sim.txt\']\n\n#n_words=42394\ndim_word=300\nbatch_size=32\nnum_epochs=500\nvalid_batch_size=32\nprint \'Loading data\'\nwith open(dictionary, \'rb\') as f:\n\tworddicts = pkl.load(f)\nn_words=len(worddicts)\nwv_dict, wv_arr, wv_size = load_word_vectors(embedding_path, \'glove.840B\', dim_word)\npretrained_emb=norm_weight(n_words, dim_word)\nfor word in worddicts.keys():\n\ttry:\n\t\tpretrained_emb[worddicts[word]]=wv_arr[wv_dict[word]].numpy()\n\texcept:\n\t\tpretrained_emb[worddicts[word]] = torch.normal(torch.zeros(dim_word),std=1).numpy()\ntrain = TextIterator(datasets[0], datasets[1], datasets[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=batch_size)\ntest = TextIterator(test_datasets[0], test_datasets[1], test_datasets[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=valid_batch_size,\n\t\t\t\t\t shuffle=False)\n\ncriterion = torch.nn.CrossEntropyLoss()\nmodel = ESIM(dim_word, 2, n_words, dim_word, pretrained_emb)\nif torch.cuda.is_available():\n\tmodel = model.cuda()\n\tcriterion = criterion.cuda()\noptimizer = torch.optim.Adam(model.parameters(),lr=0.0004)\nprint(\'start training...\')\naccumulated_loss=0\nbatch_counter=0\nreport_interval = 1000\nbest_dev_loss=10e10\nbest_dev_loss2=10e10\nclip_c=10\nmax_len=100\nmax_result=0\nbest_result = 0\n\nmodel.train()\nfor epoch in range(num_epochs):\n\taccumulated_loss = 0\n\tmodel.train()\n\tprint(\'--\' * 20)\n\tstart_time = time.time()\n\ttrain_sents_scaned = 0\n\ttrain_num_correct = 0\n\tbatch_counter=0\n\tfor x1, x2, y in train:\n\t\tx1, x1_mask, x2, x2_mask, y = prepare_data(x1, x2, y, maxlen=max_len)\n\t\ttrain_sents_scaned += len(y)\n\t\toptimizer.zero_grad()\n\t\toutput = model(x1, x1_mask, x2, x2_mask)\n\t\tresult = output.data.cpu().numpy()\n\t\ta = np.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\ttrain_num_correct += np.sum(a == b)\n\t\tloss = criterion(output, y)\n\t\tloss.backward()\n\n\t\t\'\'\'\'\'\'\n\t\tgrad_norm = 0.\n\n\t\tfor m in list(model.parameters()):\n\t\t\tgrad_norm+=m.grad.data.norm() ** 2\n\n\t\tfor m in list(model.parameters()):\n\t\t\tif grad_norm>clip_c**2:\n\t\t\t\ttry:\n\t\t\t\t\tm.grad.data= m.grad.data / torch.sqrt(grad_norm) * clip_c\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\t\t\'\'\'\'\'\'\n\n\t\toptimizer.step()\n\t\taccumulated_loss += loss.data[0]\n\t\tbatch_counter += 1\n\t\tif batch_counter % report_interval == 0:\n\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\tmsg += \'\\t training batch loss: %f\' % (accumulated_loss / train_sents_scaned)\n\t\t\t#msg += \'\\t train accuracy: %f\' % (train_num_correct / train_sents_scaned)\n\t\t\tprint(msg)\n\t# test after each epoch\n\tmodel.eval()\n\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\taccumulated_loss = 0\n\tdev_num_correct = 0\n\tn_done = 0\n\tpred=[]\n\tgold = []\n\tfor test_x1, test_x2, test_y in test:\n\t\tn_done+=len(test_y)\n\t\tx1, x1_mask, x2, x2_mask, y = prepare_data(test_x1, test_x2, test_y, maxlen=max_len)\n\t\twith torch.no_grad():\n\t\t\toutput = F.softmax(model(x1, x1_mask, x2, x2_mask))\n\t\tresult = output.data.cpu().numpy()\n\t\tloss = criterion(output, y)\n\t\taccumulated_loss += loss.data[0]\n\t\ta = numpy.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\tdev_num_correct += numpy.sum(a == b)\n\t\tpred.extend(result[:, 1])\n\t\tgold.extend(b)\n\tmsg += \'\\t test loss: %f\' % (accumulated_loss / n_done)\n\ttest_acc = dev_num_correct / n_done\n\t#msg += \'\\t test accuracy: %f\' % test_acc\n\tprint(msg)\n\t_, my_result = URL_maxF1_eval(pred, gold)\n\tif my_result > best_result:\n\t\tbest_result=my_result\n\t\ttorch.save(model, base_path + \'/model_ESIM_batch_\' + task + \'.pkl\')\n\t\twith open(base_path+\'/result_ESIM_batch_\'+task+\'_prob.txt\',\'w\') as f:\n\t\t\tfor i in range(len(pred)):\n\t\t\t\tf.writelines(str(1 - pred[i]) + \'\\t\' + str(pred[i]) + \'\\n\')\n\telapsed_time = time.time() - start_time\n\tprint(\'Epoch \' + str(epoch) + \' finished within \' + str(timedelta(seconds=elapsed_time)))'"
ESIM/main_batch_quora.py,22,"b'from __future__ import division\nimport sys\nfrom data_iterator import TextIterator\nimport cPickle as pkl\nfrom os.path import expanduser\nimport torch\nimport numpy\nfrom model_batch import *\nimport time\nfrom datetime import timedelta\nfrom torchtext.vocab import load_word_vectors\n\n# batch preparation\ndef prepare_data(seqs_x, seqs_y, labels, maxlen=None):\n\tlengths_x = [len(s) for s in seqs_x]\n\tlengths_y = [len(s) for s in seqs_y]\n\n\tif maxlen is not None:\n\t\tnew_seqs_x = []\n\t\tnew_seqs_y = []\n\t\tnew_lengths_x = []\n\t\tnew_lengths_y = []\n\t\tnew_labels = []\n\t\tfor l_x, s_x, l_y, s_y, l in zip(lengths_x, seqs_x, lengths_y, seqs_y, labels):\n\t\t\tif l_x < maxlen and l_y < maxlen:\n\t\t\t\tnew_seqs_x.append(s_x)\n\t\t\t\tnew_lengths_x.append(l_x)\n\t\t\t\tnew_seqs_y.append(s_y)\n\t\t\t\tnew_lengths_y.append(l_y)\n\t\t\t\tnew_labels.append(l)\n\t\tlengths_x = new_lengths_x\n\t\tseqs_x = new_seqs_x\n\t\tlengths_y = new_lengths_y\n\t\tseqs_y = new_seqs_y\n\t\tlabels = new_labels\n\n\t\tif len(lengths_x) < 1 or len(lengths_y) < 1:\n\t\t\treturn None, None, None, None, None\n\n\tn_samples = len(seqs_x)\n\tmaxlen_x = numpy.max(lengths_x)\n\tmaxlen_y = numpy.max(lengths_y)\n\n\tx = numpy.zeros((maxlen_x, n_samples)).astype(\'int64\')\n\ty = numpy.zeros((maxlen_y, n_samples)).astype(\'int64\')\n\tx_mask = numpy.zeros((maxlen_x, n_samples)).astype(\'float32\')\n\ty_mask = numpy.zeros((maxlen_y, n_samples)).astype(\'float32\')\n\tl = numpy.zeros((n_samples,)).astype(\'int64\')\n\tfor idx, [s_x, s_y, ll] in enumerate(zip(seqs_x, seqs_y, labels)):\n\t\tx[:lengths_x[idx], idx] = s_x\n\t\tx_mask[:lengths_x[idx], idx] = 1.\n\t\ty[:lengths_y[idx], idx] = s_y\n\t\ty_mask[:lengths_y[idx], idx] = 1.\n\t\tl[idx] = ll\n\n\tif torch.cuda.is_available():\n\t\tx=Variable(torch.LongTensor(x)).cuda()\n\t\tx_mask=Variable(torch.Tensor(x_mask)).cuda()\n\t\ty=Variable(torch.LongTensor(y)).cuda()\n\t\ty_mask=Variable(torch.Tensor(y_mask)).cuda()\n\t\tl=Variable(torch.LongTensor(l)).cuda()\n\telse:\n\t\tx = Variable(torch.LongTensor(x))\n\t\tx_mask = Variable(torch.FloatTensor(x_mask))\n\t\ty = Variable(torch.LongTensor(y))\n\t\ty_mask = Variable(torch.FloatTensor(y_mask))\n\t\tl = Variable(torch.LongTensor(l))\n\treturn x, x_mask, y, y_mask, l\n\n# some utilities\ndef ortho_weight(ndim):\n\t""""""\n\tRandom orthogonal weights\n\n\tUsed by norm_weights(below), in which case, we\n\tare ensuring that the rows are orthogonal\n\t(i.e W = U \\Sigma V, U has the same\n\t# of rows, V has the same # of cols)\n\t""""""\n\tW = numpy.random.randn(ndim, ndim)\n\tu, s, v = numpy.linalg.svd(W)\n\treturn u.astype(\'float32\')\n\n\ndef norm_weight(nin, nout=None, scale=0.01, ortho=True):\n\t""""""\n\tRandom weights drawn from a Gaussian\n\t""""""\n\tif nout is None:\n\t\tnout = nin\n\tif nout == nin and ortho:\n\t\tW = ortho_weight(nin)\n\telse:\n\t\tW = scale * numpy.random.randn(nin, nout)\n\treturn W.astype(\'float32\')\n\nif torch.cuda.is_available():\n\tprint(\'CUDA is available!\')\n\tbase_path = expanduser(""~"") + \'/pytorch/ESIM\'\n\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\nelse:\n\tbase_path = expanduser(""~"") + \'/Documents/research/pytorch/ESIM\'\n\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\ntask=\'quora\'\nprint(\'task: \'+task)\nif task==\'snli\':\n\tdictionary=base_path+\'/data/word_sequence/snli_vocab_cased.pkl\'\n\tdatasets         = [base_path+\'/data/word_sequence/premise_snli_1.0_train.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/hypothesis_snli_1.0_train.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/label_snli_1.0_train.txt\']\n\tvalid_datasets   = [base_path+\'/data/word_sequence/premise_snli_1.0_dev.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/hypothesis_snli_1.0_dev.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/label_snli_1.0_dev.txt\']\n\ttest_datasets    = [base_path+\'/data/word_sequence/premise_snli_1.0_test.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/hypothesis_snli_1.0_test.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/label_snli_1.0_test.txt\']\nelif task==\'mnli\':\n\tdic = {\'entailment\': \'0\', \'neutral\': \'1\', \'contradiction\': \'2\'}\n\tdictionary = base_path + \'/data/word_sequence/mnli_vocab_cased.pkl\'\n\tdatasets = [base_path + \'/data/word_sequence/premise_multinli_1.0_train.txt\',\n\t\t\t\tbase_path + \'/data/word_sequence/hypothesis_multinli_1.0_train.txt\',\n\t\t\t\tbase_path + \'/data/word_sequence/label_multinli_1.0_train.txt\']\n\tvalid_datasets_m = [base_path + \'/data/word_sequence/premise_multinli_1.0_dev_matched.txt\',\n\t\t\t\t\t  base_path + \'/data/word_sequence/hypothesis_multinli_1.0_dev_matched.txt\',\n\t\t\t\t\t  base_path + \'/data/word_sequence/label_multinli_1.0_dev_matched.txt\']\n\tvalid_datasets_um = [base_path + \'/data/word_sequence/premise_multinli_1.0_dev_mismatched.txt\',\n\t\t\t\t\t\tbase_path + \'/data/word_sequence/hypothesis_multinli_1.0_dev_mismatched.txt\',\n\t\t\t\t\t\tbase_path + \'/data/word_sequence/label_multinli_1.0_dev_mismatched.txt\']\n\ttest_datasets_m = [base_path + \'/data/word_sequence/premise_multinli_1.0_test_matched.txt\',\n\t\t\t\t\t  base_path + \'/data/word_sequence/hypothesis_multinli_1.0_test_matched.txt\',\n\t\t\t\t\t  base_path + \'/data/word_sequence/label_multinli_1.0_test_matched.txt\']\n\ttest_datasets_um = [base_path + \'/data/word_sequence/premise_multinli_1.0_test_mismatched.txt\',\n\t\t\t\t\t\tbase_path + \'/data/word_sequence/hypothesis_multinli_1.0_test_mismatched.txt\',\n\t\t\t\t\t\tbase_path + \'/data/word_sequence/label_multinli_1.0_test_mismatched.txt\']\nelif task==\'quora\':\n\tif torch.cuda.is_available():\n\t\tprint(\'CUDA is available!\')\n\t\tbase_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/quora\'\n\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\telse:\n\t\tbase_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/quora\'\n\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\tdictionary=base_path+\'/quora_vocab_cased.pkl\'\n\tdatasets=[base_path+\'/train/a.toks\', base_path+\'/train/b.toks\', base_path+\'/train/sim.txt\']\n\ttest_datasets=[base_path+\'/test/a.toks\', base_path+\'/test/b.toks\', base_path+\'/test/sim.txt\']\n\n#n_words=42394\ndim_word=300\nbatch_size=32\nnum_epochs=500\nvalid_batch_size=32\nprint \'Loading data\'\nwith open(dictionary, \'rb\') as f:\n\tworddicts = pkl.load(f)\nn_words=len(worddicts)\nwv_dict, wv_arr, wv_size = load_word_vectors(embedding_path, \'glove.840B\', dim_word)\npretrained_emb=norm_weight(n_words, dim_word)\nfor word in worddicts.keys():\n\ttry:\n\t\tpretrained_emb[worddicts[word]]=wv_arr[wv_dict[word]].numpy()\n\texcept:\n\t\tpretrained_emb[worddicts[word]] = torch.normal(torch.zeros(dim_word),std=1).numpy()\ntrain = TextIterator(datasets[0], datasets[1], datasets[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=batch_size)\ntest = TextIterator(test_datasets[0], test_datasets[1], test_datasets[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=valid_batch_size,\n\t\t\t\t\t shuffle=False)\n\ncriterion = torch.nn.CrossEntropyLoss()\nmodel = ESIM(dim_word, 2, n_words, dim_word, pretrained_emb)\nif torch.cuda.is_available():\n\tmodel = model.cuda()\n\tcriterion = criterion.cuda()\noptimizer = torch.optim.Adam(model.parameters(),lr=0.0004)\nprint(\'start training...\')\naccumulated_loss=0\nbatch_counter=0\nreport_interval = 1000\nbest_dev_loss=10e10\nbest_dev_loss2=10e10\nclip_c=10\nmax_len=100\nmax_result=0\nbest_result = 0\n\nmodel.train()\nfor epoch in range(num_epochs):\n\taccumulated_loss = 0\n\tmodel.train()\n\tprint(\'--\' * 20)\n\tstart_time = time.time()\n\ttrain_sents_scaned = 0\n\ttrain_num_correct = 0\n\tbatch_counter=0\n\tfor x1, x2, y in train:\n\t\tx1, x1_mask, x2, x2_mask, y = prepare_data(x1, x2, y, maxlen=max_len)\n\t\ttrain_sents_scaned += len(y)\n\t\toptimizer.zero_grad()\n\t\toutput = model(x1, x1_mask, x2, x2_mask)\n\t\tresult = output.data.cpu().numpy()\n\t\ta = np.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\ttrain_num_correct += np.sum(a == b)\n\t\tloss = criterion(output, y)\n\t\tloss.backward()\n\n\t\t\'\'\'\'\'\'\n\t\tgrad_norm = 0.\n\n\t\tfor m in list(model.parameters()):\n\t\t\tgrad_norm+=m.grad.data.norm() ** 2\n\n\t\tfor m in list(model.parameters()):\n\t\t\tif grad_norm>clip_c**2:\n\t\t\t\ttry:\n\t\t\t\t\tm.grad.data= m.grad.data / torch.sqrt(grad_norm) * clip_c\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\t\t\'\'\'\'\'\'\n\n\t\toptimizer.step()\n\t\taccumulated_loss += loss.data[0]\n\t\tbatch_counter += 1\n\t\tif batch_counter % report_interval == 0:\n\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\tmsg += \'\\t training batch loss: %f\' % (accumulated_loss / train_sents_scaned)\n\t\t\t#msg += \'\\t train accuracy: %f\' % (train_num_correct / train_sents_scaned)\n\t\t\tprint(msg)\n\t# test after each epoch\n\tmodel.eval()\n\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\taccumulated_loss = 0\n\tdev_num_correct = 0\n\tn_done = 0\n\tpred=[]\n\tgold = []\n\tprint(\'testing on URL dataset:\')\n\tpit_base_path = base_path.replace(\'quora\', \'url\')\n\ttest_datasets = [base_path + \'/test_9324/a.toks\', base_path + \'/test_9324/b.toks\', base_path + \'/test_9324/sim.txt\']\n\ttest = TextIterator(test_datasets[0], test_datasets[1], test_datasets[2],\n\t                    dictionary,\n\t                    n_words=n_words,\n\t                    batch_size=valid_batch_size,\n\t                    shuffle=False)\n\tfor test_x1, test_x2, test_y in test:\n\t\tn_done+=len(test_y)\n\t\tx1, x1_mask, x2, x2_mask, y = prepare_data(test_x1, test_x2, test_y)\n\t\twith torch.no_grad():\n\t\t\toutput = F.softmax(model(x1, x1_mask, x2, x2_mask))\n\t\tresult = output.data.cpu().numpy()\n\t\tloss = criterion(output, y)\n\t\taccumulated_loss += loss.data[0]\n\t\ta = numpy.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\tdev_num_correct += numpy.sum(a == b)\n\t\tpred.extend(result[:, 1])\n\t\tgold.extend(b)\n\tmsg += \'\\t test loss: %f\' % (accumulated_loss / n_done)\n\t#test_acc = dev_num_correct / n_done\n\t#msg += \'\\t test accuracy: %f\' % test_acc\n\tprint(msg)\n\tURL_maxF1_eval(pred, gold)\n\tprint(\'testing on PIT dataset:\')\n\tpred = []\n\tgold = []\n\taccumulated_loss=0\n\tpit_base_path=base_path.replace(\'quora\',\'pit\')\n\ttest_datasets = [pit_base_path + \'/test/a.toks\', pit_base_path + \'/test/b.toks\', pit_base_path + \'/test/sim.txt\']\n\ttest = TextIterator(test_datasets[0], test_datasets[1], test_datasets[2],\n\t                    dictionary,\n\t                    n_words=n_words,\n\t                    batch_size=valid_batch_size,\n\t                    shuffle=False)\n\tfor test_x1, test_x2, test_y in test:\n\t\tn_done+=len(test_y)\n\t\tx1, x1_mask, x2, x2_mask, y = prepare_data(test_x1, test_x2, test_y)\n\t\twith torch.no_grad():\n\t\t\toutput = F.softmax(model(x1, x1_mask, x2, x2_mask))\n\t\tresult = output.data.cpu().numpy()\n\t\tloss = criterion(output, y)\n\t\taccumulated_loss += loss.data[0]\n\t\ta = numpy.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\tdev_num_correct += numpy.sum(a == b)\n\t\tpred.extend(result[:, 1])\n\t\tgold.extend(b)\n\t#msg += \'\\t test loss: %f\' % (accumulated_loss / n_done)\n\t#msg += \'\\t test accuracy: %f\' % test_acc\n\t#print(msg)\n\tURL_maxF1_eval(pred, gold)\n\tprint(\'testing on Quora dataset:\')\n\tdev_num_correct=0\n\tn_done=0\n\tquora_base_path=base_path.replace(\'quora\',\'quora\')\n\ttest_datasets = [quora_base_path+\'/test/a.toks\', quora_base_path+\'/test/b.toks\', quora_base_path+\'/test/sim.txt\']\n\ttest = TextIterator(test_datasets[0], test_datasets[1], test_datasets[2],\n\t                    dictionary,\n\t                    n_words=n_words,\n\t                    batch_size=valid_batch_size,\n\t                    shuffle=False)\n\tfor test_x1, test_x2, test_y in test:\n\t\tn_done+=len(test_y)\n\t\tx1, x1_mask, x2, x2_mask, y = prepare_data(test_x1, test_x2, test_y)\n\t\twith torch.no_grad():\n\t\t\toutput = F.softmax(model(x1, x1_mask, x2, x2_mask))\n\t\tresult = output.data.cpu().numpy()\n\t\tloss = criterion(output, y)\n\t\taccumulated_loss += loss.data[0]\n\t\ta = numpy.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\tdev_num_correct += numpy.sum(a == b)\n\ttest_acc=dev_num_correct/n_done\n\tmsg = \'\\t test accuracy: %f\' % test_acc\n\tprint(msg)\n\t#if my_result > best_result:\n\t#\tbest_result=my_result\n\t#\ttorch.save(model, base_path + \'/model_ESIM_batch_\' + task + \'.pkl\')\n\t#\twith open(base_path+\'/result_ESIM_batch_\'+task+\'_prob.txt\',\'w\') as f:\n\t#\t\tfor i in range(len(pred)):\n\t#\t\t\tf.writelines(str(1 - pred[i]) + \'\\t\' + str(pred[i]) + \'\\n\')\n\telapsed_time = time.time() - start_time\n\tprint(\'Epoch \' + str(epoch) + \' finished within \' + str(timedelta(seconds=elapsed_time)))'"
ESIM/main_batch_snli.py,20,"b'from __future__ import division\nimport sys\nfrom data_iterator import TextIterator\nimport cPickle as pkl\nfrom os.path import expanduser\nimport torch\nimport numpy\nfrom model_batch import *\nimport time\nfrom datetime import timedelta\nfrom torchtext.vocab import load_word_vectors\n\n# batch preparation\n# batch preparation\ndef prepare_data(seqs_x, seqs_y, labels, maxlen=None):\n\tlengths_x = [len(s) for s in seqs_x]\n\tlengths_y = [len(s) for s in seqs_y]\n\n\tif maxlen is not None:\n\t\tnew_seqs_x = []\n\t\tnew_seqs_y = []\n\t\tnew_lengths_x = []\n\t\tnew_lengths_y = []\n\t\tnew_labels = []\n\t\tfor l_x, s_x, l_y, s_y, l in zip(lengths_x, seqs_x, lengths_y, seqs_y, labels):\n\t\t\tif l_x < maxlen and l_y < maxlen:\n\t\t\t\tnew_seqs_x.append(s_x)\n\t\t\t\tnew_lengths_x.append(l_x)\n\t\t\t\tnew_seqs_y.append(s_y)\n\t\t\t\tnew_lengths_y.append(l_y)\n\t\t\t\tnew_labels.append(l)\n\t\tlengths_x = new_lengths_x\n\t\tseqs_x = new_seqs_x\n\t\tlengths_y = new_lengths_y\n\t\tseqs_y = new_seqs_y\n\t\tlabels = new_labels\n\n\t\tif len(lengths_x) < 1 or len(lengths_y) < 1:\n\t\t\treturn None, None, None, None, None\n\n\tn_samples = len(seqs_x)\n\tmaxlen_x = numpy.max(lengths_x)\n\tmaxlen_y = numpy.max(lengths_y)\n\n\tx = numpy.zeros((maxlen_x, n_samples)).astype(\'int64\')\n\ty = numpy.zeros((maxlen_y, n_samples)).astype(\'int64\')\n\tx_mask = numpy.zeros((maxlen_x, n_samples)).astype(\'float32\')\n\ty_mask = numpy.zeros((maxlen_y, n_samples)).astype(\'float32\')\n\tl = numpy.zeros((n_samples,)).astype(\'int64\')\n\tfor idx, [s_x, s_y, ll] in enumerate(zip(seqs_x, seqs_y, labels)):\n\t\tx[:lengths_x[idx], idx] = s_x\n\t\tx_mask[:lengths_x[idx], idx] = 1.\n\t\ty[:lengths_y[idx], idx] = s_y\n\t\ty_mask[:lengths_y[idx], idx] = 1.\n\t\tl[idx] = ll\n\n\tif torch.cuda.is_available():\n\t\tx=Variable(torch.LongTensor(x)).cuda()\n\t\tx_mask=Variable(torch.Tensor(x_mask)).cuda()\n\t\ty=Variable(torch.LongTensor(y)).cuda()\n\t\ty_mask=Variable(torch.Tensor(y_mask)).cuda()\n\t\tl=Variable(torch.LongTensor(l)).cuda()\n\telse:\n\t\tx = Variable(torch.LongTensor(x))\n\t\tx_mask = Variable(torch.FloatTensor(x_mask))\n\t\ty = Variable(torch.LongTensor(y))\n\t\ty_mask = Variable(torch.FloatTensor(y_mask))\n\t\tl = Variable(torch.LongTensor(l))\n\treturn x, x_mask, y, y_mask, l\n\n# some utilities\ndef ortho_weight(ndim):\n\t""""""\n\tRandom orthogonal weights\n\n\tUsed by norm_weights(below), in which case, we\n\tare ensuring that the rows are orthogonal\n\t(i.e W = U \\Sigma V, U has the same\n\t# of rows, V has the same # of cols)\n\t""""""\n\tW = numpy.random.randn(ndim, ndim)\n\tu, s, v = numpy.linalg.svd(W)\n\treturn u.astype(\'float32\')\n\n\ndef norm_weight(nin, nout=None, scale=0.01, ortho=True):\n\t""""""\n\tRandom weights drawn from a Gaussian\n\t""""""\n\tif nout is None:\n\t\tnout = nin\n\tif nout == nin and ortho:\n\t\tW = ortho_weight(nin)\n\telse:\n\t\tW = scale * numpy.random.randn(nin, nout)\n\treturn W.astype(\'float32\')\n\nif torch.cuda.is_available():\n\tprint(\'CUDA is available!\')\n\tbase_path = expanduser(""~"") + \'/pytorch/ESIM\'\n\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\nelse:\n\tbase_path = expanduser(""~"") + \'/Documents/research/pytorch/ESIM\'\n\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\ntask=\'snli\'\nprint(\'task: \'+task)\nif task==\'snli\':\n\tdictionary=base_path+\'/data/word_sequence/snli_vocab_cased.pkl\'\n\tdatasets         = [base_path+\'/data/word_sequence/premise_snli_1.0_train.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/hypothesis_snli_1.0_train.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/label_snli_1.0_train.txt\']\n\tvalid_datasets   = [base_path+\'/data/word_sequence/premise_snli_1.0_dev.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/hypothesis_snli_1.0_dev.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/label_snli_1.0_dev.txt\']\n\ttest_datasets    = [base_path+\'/data/word_sequence/premise_snli_1.0_test.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/hypothesis_snli_1.0_test.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/label_snli_1.0_test.txt\']\nelif task==\'mnli\':\n\tdic = {\'entailment\': \'0\', \'neutral\': \'1\', \'contradiction\': \'2\'}\n\tdictionary = base_path + \'/data/word_sequence/mnli_vocab_cased.pkl\'\n\tdatasets = [base_path + \'/data/word_sequence/premise_multinli_1.0_train.txt\',\n\t\t\t\tbase_path + \'/data/word_sequence/hypothesis_multinli_1.0_train.txt\',\n\t\t\t\tbase_path + \'/data/word_sequence/label_multinli_1.0_train.txt\']\n\tvalid_datasets_m = [base_path + \'/data/word_sequence/premise_multinli_1.0_dev_matched.txt\',\n\t\t\t\t\t  base_path + \'/data/word_sequence/hypothesis_multinli_1.0_dev_matched.txt\',\n\t\t\t\t\t  base_path + \'/data/word_sequence/label_multinli_1.0_dev_matched.txt\']\n\tvalid_datasets_um = [base_path + \'/data/word_sequence/premise_multinli_1.0_dev_mismatched.txt\',\n\t\t\t\t\t\tbase_path + \'/data/word_sequence/hypothesis_multinli_1.0_dev_mismatched.txt\',\n\t\t\t\t\t\tbase_path + \'/data/word_sequence/label_multinli_1.0_dev_mismatched.txt\']\n\ttest_datasets_m = [base_path + \'/data/word_sequence/premise_multinli_1.0_test_matched.txt\',\n\t\t\t\t\t  base_path + \'/data/word_sequence/hypothesis_multinli_1.0_test_matched.txt\',\n\t\t\t\t\t  base_path + \'/data/word_sequence/label_multinli_1.0_test_matched.txt\']\n\ttest_datasets_um = [base_path + \'/data/word_sequence/premise_multinli_1.0_test_mismatched.txt\',\n\t\t\t\t\t\tbase_path + \'/data/word_sequence/hypothesis_multinli_1.0_test_mismatched.txt\',\n\t\t\t\t\t\tbase_path + \'/data/word_sequence/label_multinli_1.0_test_mismatched.txt\']\n#n_words=42394\ndim_word=300\nbatch_size=32\nnum_epochs=500\nvalid_batch_size=1\nprint \'Loading data\'\nwith open(dictionary, \'rb\') as f:\n\tworddicts = pkl.load(f)\nn_words=len(worddicts)\nwv_dict, wv_arr, wv_size = load_word_vectors(embedding_path, \'glove.840B\', dim_word)\npretrained_emb=norm_weight(n_words, dim_word)\nfor word in worddicts.keys():\n\ttry:\n\t\tpretrained_emb[worddicts[word]]=wv_arr[wv_dict[word]].numpy()\n\texcept:\n\t\tpretrained_emb[worddicts[word]] = torch.normal(torch.zeros(dim_word),std=1).numpy()\ntrain = TextIterator(datasets[0], datasets[1], datasets[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=batch_size)\n\'\'\'\ntrain_valid = TextIterator(datasets[0], datasets[1], datasets[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=valid_batch_size,\n\t\t\t\t\t shuffle=False)\n\'\'\'\n\'\'\'\nvalid_m = TextIterator(valid_datasets_m[0], valid_datasets_m[1], valid_datasets_m[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=valid_batch_size,\n\t\t\t\t\t shuffle=False)\nvalid_um = TextIterator(valid_datasets_um[0], valid_datasets_um[1], valid_datasets_um[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=valid_batch_size,\n\t\t\t\t\t shuffle=False)\ntest_m = TextIterator(test_datasets_m[0], test_datasets_m[1], test_datasets_m[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=valid_batch_size,\n\t\t\t\t\t shuffle=False)\ntest_um = TextIterator(test_datasets_um[0], test_datasets_um[1], test_datasets_um[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=valid_batch_size,\n\t\t\t\t\t shuffle=False)\n\'\'\'\nvalid = TextIterator(valid_datasets[0],valid_datasets[1],valid_datasets[2],\n                     dictionary,\n                     n_words=n_words,\n                     batch_size=valid_batch_size,\n                     shuffle=False)\ntest = TextIterator(test_datasets[0], test_datasets[1], test_datasets[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=valid_batch_size,\n\t\t\t\t\t shuffle=False)\n\ncriterion = torch.nn.CrossEntropyLoss()\nmodel = ESIM(dim_word, 3, n_words, dim_word, pretrained_emb)\nif torch.cuda.is_available():\n\tmodel = model.cuda()\n\tcriterion = criterion.cuda()\noptimizer = torch.optim.Adam(model.parameters(),lr=0.0004)\nprint(\'start training...\')\naccumulated_loss=0\nbatch_counter=0\nreport_interval = 1000\nbest_dev_loss=10e10\nbest_dev_loss2=10e10\nclip_c=10\nmax_len=500\nmax_result=0\nmodel.train()\nfor epoch in range(num_epochs):\n\taccumulated_loss = 0\n\tmodel.train()\n\tprint(\'--\' * 20)\n\tstart_time = time.time()\n\ttrain_sents_scaned = 0\n\ttrain_num_correct = 0\n\tbatch_counter=0\n\ttrain = TextIterator(datasets[0], datasets[1], datasets[2],\n\t                     dictionary,\n\t                     n_words=n_words,\n\t                     batch_size=batch_size)\n\tfor x1, x2, y in train:\n\t\tx1, x1_mask, x2, x2_mask, y = prepare_data(x1, x2, y, maxlen=max_len)\n\t\ttrain_sents_scaned += len(y)\n\t\toptimizer.zero_grad()\n\t\toutput = model(x1, x1_mask, x2, x2_mask)\n\t\tresult = output.data.cpu().numpy()\n\t\ta = np.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\ttrain_num_correct += np.sum(a == b)\n\t\tloss = criterion(output, y)\n\t\tloss.backward()\n\n\t\t\'\'\'\'\'\'\n\t\tgrad_norm = 0.\n\n\t\tfor m in list(model.parameters()):\n\t\t\tgrad_norm+=m.grad.data.norm() ** 2\n\n\t\tfor m in list(model.parameters()):\n\t\t\tif grad_norm>clip_c**2:\n\t\t\t\ttry:\n\t\t\t\t\tm.grad.data= m.grad.data / torch.sqrt(grad_norm) * clip_c\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\t\t\'\'\'\'\'\'\n\n\t\toptimizer.step()\n\t\taccumulated_loss += loss.data[0]\n\t\tbatch_counter += 1\n\t\t#print(batch_counter)\n\t\tif batch_counter % report_interval == 0:\n\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\tmsg += \'\\t training batch loss: %f\' % (accumulated_loss / train_sents_scaned)\n\t\t\tmsg += \'\\t train accuracy: %f\' % (train_num_correct / train_sents_scaned)\n\t\t\tprint(msg)\n\t\t#if batch_counter>(int(17168/64)):\n\t\t#\tbreak\n\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\tmsg += \'\\t training batch loss: %f\' % (accumulated_loss / train_sents_scaned)\n\tmsg += \'\\t train accuracy: %f\' % (train_num_correct / train_sents_scaned)\n\tprint(msg)\n\t# valid after each epoch\n\tmodel.eval()\n\t\'\'\'\'\'\'\n\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\taccumulated_loss = 0\n\tdev_num_correct = 0\n\tn_done=0\n\tfor dev_x1, dev_x2, dev_y in valid:\n\t\tn_done += len(dev_x1)\n\t\tx1, x1_mask, x2, x2_mask, y = prepare_data(dev_x1, dev_x2, dev_y, maxlen=max_len)\n\t\twith torch.no_grad():\n\t\t\toutput = model(x1, x1_mask, x2, x2_mask)\n\t\tresult = output.data.cpu().numpy()\n\t\tloss = criterion(output, y)\n\t\taccumulated_loss += loss.data[0]\n\t\ta = numpy.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\tdev_num_correct += numpy.sum(a == b)\n\tmsg += \'\\t dev loss: %f\' % (accumulated_loss/n_done)\n\tdev_acc = dev_num_correct / n_done\n\tmsg += \'\\t dev accuracy: %f\' % dev_acc\n\tprint(msg)\n\t\'\'\'\'\'\'\n\tif dev_acc>max_result:\n\t\tmax_result=dev_acc\n\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\taccumulated_loss = 0\n\t\tdev_num_correct = 0\n\t\tn_done = 0\n\t\tpred=[]\n\t\terror_analysis = []\n\t\tfor test_x1, test_x2, test_y in test:\n\t\t\tn_done+=len(test_y)\n\t\t\tx1, x1_mask, x2, x2_mask, y = prepare_data(test_x1, test_x2, test_y, maxlen=max_len)\n\t\t\twith torch.no_grad():\n\t\t\t\toutput = model(x1, x1_mask, x2, x2_mask)\n\t\t\tresult = output.data.cpu().numpy()\n\t\t\tloss = criterion(output, y)\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\ta = numpy.argmax(result, axis=1)\n\t\t\tb = y.data.cpu().numpy()\n\t\t\tdev_num_correct += numpy.sum(a == b)\n\t\t\tpred.extend(result)\n\n\t\t\tif a != b:\n\t\t\t\tmy_dict = {}\n\t\t\t\ts1 = \'\'\n\t\t\t\tfor word in x1:\n\t\t\t\t\ts1 += worddicts.keys()[word.data[0]] + \' \'\n\t\t\t\t# print(s1)\n\t\t\t\tmy_dict[\'s1\'] = s1\n\t\t\t\ts2 = \'\'\n\t\t\t\tfor word in x2:\n\t\t\t\t\ts2 += worddicts.keys()[word.data[0]] + \' \'\n\t\t\t\t# print(s2)\n\t\t\t\tmy_dict[\'s2\'] = s2\n\t\t\t\t# print(model.alpha[:,:,0].data.cpu().numpy())\n\t\t\t\t# print(model.beta[:,:,0].data.cpu().numpy())\n\t\t\t\tmy_dict[\'alpha\'] = model.alpha[:, :, 0].data.cpu().numpy()\n\t\t\t\tmy_dict[\'beta\'] = model.beta[:, :, 0].data.cpu().numpy()\n\t\t\t\tmy_dict[\'pred_label\']=a[0]\n\t\t\t\tmy_dict[\'true_label\']=b[0]\n\t\t\t\terror_analysis.append(my_dict)\n\t\tpkl.dump(error_analysis, open(base_path + \'/ESIM_snli_error_analysis.pkl\', \'wb\'))\n\t\tmsg += \'\\t test loss: %f\' % (accumulated_loss / n_done)\n\t\ttest_acc = dev_num_correct / n_done\n\t\tmsg += \'\\t test accuracy: %f\' % test_acc\n\t\tprint(msg)\n\t\ttorch.save(model, base_path + \'/model_ESIM_snli_visualize\' + \'.pkl\')\n\t\t#\tmax_result=test_acc\n\t\t#\twith open(base_path+\'/result_ESIM_prob.txt\',\'w\') as f:\n\t\t#\t\tfor item in pred:\n\t\t#\t\t\tf.writelines(str(item[0])+\'\\t\'+str(item[1])+\'\\t\'+str(item[2])+\'\\n\')\n\telapsed_time = time.time() - start_time\n\tprint(\'Epoch \' + str(epoch) + \' finished within \' + str(timedelta(seconds=elapsed_time)))'"
ESIM/main_batch_trecqa.py,20,"b'from __future__ import division\nimport sys\nfrom data_iterator import TextIterator\nimport cPickle as pkl\nfrom os.path import expanduser\nimport torch\nimport numpy\nfrom model_batch import *\nimport time\nimport os\nfrom datetime import timedelta\nfrom torchtext.vocab import load_word_vectors\n\n# batch preparation\n# batch preparation\ndef prepare_data(seqs_x, seqs_y, labels, maxlen=None):\n\tlengths_x = [len(s) for s in seqs_x]\n\tlengths_y = [len(s) for s in seqs_y]\n\n\tif maxlen is not None:\n\t\tnew_seqs_x = []\n\t\tnew_seqs_y = []\n\t\tnew_lengths_x = []\n\t\tnew_lengths_y = []\n\t\tnew_labels = []\n\t\tfor l_x, s_x, l_y, s_y, l in zip(lengths_x, seqs_x, lengths_y, seqs_y, labels):\n\t\t\tif l_x < maxlen and l_y < maxlen:\n\t\t\t\tnew_seqs_x.append(s_x)\n\t\t\t\tnew_lengths_x.append(l_x)\n\t\t\t\tnew_seqs_y.append(s_y)\n\t\t\t\tnew_lengths_y.append(l_y)\n\t\t\t\tnew_labels.append(l)\n\t\tlengths_x = new_lengths_x\n\t\tseqs_x = new_seqs_x\n\t\tlengths_y = new_lengths_y\n\t\tseqs_y = new_seqs_y\n\t\tlabels = new_labels\n\n\t\tif len(lengths_x) < 1 or len(lengths_y) < 1:\n\t\t\treturn None, None, None, None, None\n\n\tn_samples = len(seqs_x)\n\tmaxlen_x = numpy.max(lengths_x)\n\tmaxlen_y = numpy.max(lengths_y)\n\n\tx = numpy.zeros((maxlen_x, n_samples)).astype(\'int64\')\n\ty = numpy.zeros((maxlen_y, n_samples)).astype(\'int64\')\n\tx_mask = numpy.zeros((maxlen_x, n_samples)).astype(\'float32\')\n\ty_mask = numpy.zeros((maxlen_y, n_samples)).astype(\'float32\')\n\tl = numpy.zeros((n_samples,)).astype(\'int64\')\n\tfor idx, [s_x, s_y, ll] in enumerate(zip(seqs_x, seqs_y, labels)):\n\t\tx[:lengths_x[idx], idx] = s_x\n\t\tx_mask[:lengths_x[idx], idx] = 1.\n\t\ty[:lengths_y[idx], idx] = s_y\n\t\ty_mask[:lengths_y[idx], idx] = 1.\n\t\tl[idx] = ll\n\n\tif torch.cuda.is_available():\n\t\tx=Variable(torch.LongTensor(x)).cuda()\n\t\tx_mask=Variable(torch.Tensor(x_mask)).cuda()\n\t\ty=Variable(torch.LongTensor(y)).cuda()\n\t\ty_mask=Variable(torch.Tensor(y_mask)).cuda()\n\t\tl=Variable(torch.LongTensor(l)).cuda()\n\telse:\n\t\tx = Variable(torch.LongTensor(x))\n\t\tx_mask = Variable(torch.FloatTensor(x_mask))\n\t\ty = Variable(torch.LongTensor(y))\n\t\ty_mask = Variable(torch.FloatTensor(y_mask))\n\t\tl = Variable(torch.LongTensor(l))\n\treturn x, x_mask, y, y_mask, l\n\n# some utilities\ndef ortho_weight(ndim):\n\t""""""\n\tRandom orthogonal weights\n\n\tUsed by norm_weights(below), in which case, we\n\tare ensuring that the rows are orthogonal\n\t(i.e W = U \\Sigma V, U has the same\n\t# of rows, V has the same # of cols)\n\t""""""\n\tW = numpy.random.randn(ndim, ndim)\n\tu, s, v = numpy.linalg.svd(W)\n\treturn u.astype(\'float32\')\n\n\ndef norm_weight(nin, nout=None, scale=0.01, ortho=True):\n\t""""""\n\tRandom weights drawn from a Gaussian\n\t""""""\n\tif nout is None:\n\t\tnout = nin\n\tif nout == nin and ortho:\n\t\tW = ortho_weight(nin)\n\telse:\n\t\tW = scale * numpy.random.randn(nin, nout)\n\treturn W.astype(\'float32\')\n\nif torch.cuda.is_available():\n\tprint(\'CUDA is available!\')\n\tbase_path = expanduser(""~"") + \'/pytorch/ESIM\'\n\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\nelse:\n\tbase_path = expanduser(""~"") + \'/Documents/research/pytorch/ESIM\'\n\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\ntask=\'trecqa\'\nprint(\'task: \'+task)\nif task==\'snli\':\n\tdictionary=base_path+\'/data/word_sequence/snli_vocab_cased.pkl\'\n\tdatasets         = [base_path+\'/data/word_sequence/premise_snli_1.0_train.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/hypothesis_snli_1.0_train.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/label_snli_1.0_train.txt\']\n\tvalid_datasets   = [base_path+\'/data/word_sequence/premise_snli_1.0_dev.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/hypothesis_snli_1.0_dev.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/label_snli_1.0_dev.txt\']\n\ttest_datasets    = [base_path+\'/data/word_sequence/premise_snli_1.0_test.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/hypothesis_snli_1.0_test.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/label_snli_1.0_test.txt\']\nelif task==\'mnli\':\n\tdic = {\'entailment\': \'0\', \'neutral\': \'1\', \'contradiction\': \'2\'}\n\tdictionary = base_path + \'/data/word_sequence/mnli_vocab_cased.pkl\'\n\tdatasets = [base_path + \'/data/word_sequence/premise_multinli_1.0_train.txt\',\n\t\t\t\tbase_path + \'/data/word_sequence/hypothesis_multinli_1.0_train.txt\',\n\t\t\t\tbase_path + \'/data/word_sequence/label_multinli_1.0_train.txt\']\n\tvalid_datasets_m = [base_path + \'/data/word_sequence/premise_multinli_1.0_dev_matched.txt\',\n\t\t\t\t\t  base_path + \'/data/word_sequence/hypothesis_multinli_1.0_dev_matched.txt\',\n\t\t\t\t\t  base_path + \'/data/word_sequence/label_multinli_1.0_dev_matched.txt\']\n\tvalid_datasets_um = [base_path + \'/data/word_sequence/premise_multinli_1.0_dev_mismatched.txt\',\n\t\t\t\t\t\tbase_path + \'/data/word_sequence/hypothesis_multinli_1.0_dev_mismatched.txt\',\n\t\t\t\t\t\tbase_path + \'/data/word_sequence/label_multinli_1.0_dev_mismatched.txt\']\n\ttest_datasets_m = [base_path + \'/data/word_sequence/premise_multinli_1.0_test_matched.txt\',\n\t\t\t\t\t  base_path + \'/data/word_sequence/hypothesis_multinli_1.0_test_matched.txt\',\n\t\t\t\t\t  base_path + \'/data/word_sequence/label_multinli_1.0_test_matched.txt\']\n\ttest_datasets_um = [base_path + \'/data/word_sequence/premise_multinli_1.0_test_mismatched.txt\',\n\t\t\t\t\t\tbase_path + \'/data/word_sequence/hypothesis_multinli_1.0_test_mismatched.txt\',\n\t\t\t\t\t\tbase_path + \'/data/word_sequence/label_multinli_1.0_test_mismatched.txt\']\nelif task==\'trecqa\':\n\tif torch.cuda.is_available():\n\t\tprint(\'CUDA is available!\')\n\t\tbase_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/trecqa\'\n\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\tcastorini_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/castorini/trec_eval.9.0/trec_eval\'\n\telse:\n\t\tbase_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/trecqa\'\n\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\tcastorini_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/castorini/trec_eval.9.0/trec_eval\'\n\tdictionary=base_path+\'/trecqa_vocab_cased.pkl\'\n\tdatasets=[base_path+\'/train/a.toks\', base_path+\'/train/b.toks\', base_path+\'/train/sim.txt\']\n\ttest_datasets=[base_path+\'/test/a.toks\', base_path+\'/test/b.toks\', base_path+\'/test/sim.txt\']\n\ncmd = (\'%s -m map %s %s\' % (castorini_path, base_path + \'/test.qrel\', base_path + \'/result_ESIM_\' + task))\nos.system(cmd)\ncmd = (\'%s -m recip_rank %s %s\' % (castorini_path, base_path + \'/test.qrel\', base_path + \'/result_ESIM_\' + task))\nos.system(cmd)\n\n#n_words=42394\ndim_word=300\nbatch_size=32\nnum_epochs=500\nvalid_batch_size=32\nprint \'Loading data\'\nwith open(dictionary, \'rb\') as f:\n\tworddicts = pkl.load(f)\nn_words=len(worddicts)\nwv_dict, wv_arr, wv_size = load_word_vectors(embedding_path, \'glove.840B\', dim_word)\npretrained_emb=norm_weight(n_words, dim_word)\nfor word in worddicts.keys():\n\ttry:\n\t\tpretrained_emb[worddicts[word]]=wv_arr[wv_dict[word]].numpy()\n\texcept:\n\t\tpretrained_emb[worddicts[word]] = torch.normal(torch.zeros(dim_word),std=1).numpy()\ntrain = TextIterator(datasets[0], datasets[1], datasets[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=batch_size)\ntest = TextIterator(test_datasets[0], test_datasets[1], test_datasets[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=valid_batch_size,\n\t\t\t\t\t shuffle=False)\n\ncriterion = torch.nn.CrossEntropyLoss()\nmodel = ESIM(dim_word, 2, n_words, dim_word, pretrained_emb)\nif torch.cuda.is_available():\n\tmodel = model.cuda()\n\tcriterion = criterion.cuda()\noptimizer = torch.optim.Adam(model.parameters(),lr=0.0004)\nprint(\'start training...\')\naccumulated_loss=0\nbatch_counter=0\nreport_interval = 1000\nbest_dev_loss=10e10\nbest_dev_loss2=10e10\nclip_c=10\nmax_len=100\nmax_result=0\nbest_result = 0\n\nmodel.train()\nfor epoch in range(num_epochs):\n\taccumulated_loss = 0\n\tmodel.train()\n\tprint(\'--\' * 20)\n\tstart_time = time.time()\n\ttrain_sents_scaned = 0\n\ttrain_num_correct = 0\n\tbatch_counter=0\n\tfor x1, x2, y in train:\n\t\tx1, x1_mask, x2, x2_mask, y = prepare_data(x1, x2, y, maxlen=max_len)\n\t\ttrain_sents_scaned += len(y)\n\t\toptimizer.zero_grad()\n\t\toutput = model(x1, x1_mask, x2, x2_mask)\n\t\tresult = output.data.cpu().numpy()\n\t\ta = np.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\ttrain_num_correct += np.sum(a == b)\n\t\tloss = criterion(output, y)\n\t\tloss.backward()\n\n\t\t\'\'\'\'\'\'\n\t\tgrad_norm = 0.\n\n\t\tfor m in list(model.parameters()):\n\t\t\tgrad_norm+=m.grad.data.norm() ** 2\n\n\t\tfor m in list(model.parameters()):\n\t\t\tif grad_norm>clip_c**2:\n\t\t\t\ttry:\n\t\t\t\t\tm.grad.data= m.grad.data / torch.sqrt(grad_norm) * clip_c\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\t\t\'\'\'\'\'\'\n\n\t\toptimizer.step()\n\t\taccumulated_loss += loss.data[0]\n\t\tbatch_counter += 1\n\t\tif batch_counter % report_interval == 0:\n\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\tmsg += \'\\t training batch loss: %f\' % (accumulated_loss / train_sents_scaned)\n\t\t\t#msg += \'\\t train accuracy: %f\' % (train_num_correct / train_sents_scaned)\n\t\t\tprint(msg)\n\t# test after each epoch\n\tmodel.eval()\n\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\taccumulated_loss = 0\n\tdev_num_correct = 0\n\tn_done = 0\n\tpred=[]\n\tgold = []\n\tfor test_x1, test_x2, test_y in test:\n\t\tn_done+=len(test_y)\n\t\tx1, x1_mask, x2, x2_mask, y = prepare_data(test_x1, test_x2, test_y, maxlen=max_len)\n\t\twith torch.no_grad():\n\t\t\toutput = F.softmax(model(x1, x1_mask, x2, x2_mask))\n\t\tresult = output.data.cpu().numpy()\n\t\tloss = criterion(output, y)\n\t\taccumulated_loss += loss.data[0]\n\t\ta = numpy.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\tdev_num_correct += numpy.sum(a == b)\n\t\tpred.extend(result[:, 1])\n\t\tgold.extend(b)\n\tmsg += \'\\t test loss: %f\' % (accumulated_loss / n_done)\n\ttest_acc = dev_num_correct / n_done\n\t#msg += \'\\t test accuracy: %f\' % test_acc\n\tprint(msg)\n\t_, my_result = URL_maxF1_eval(pred, gold)\n\tif my_result > best_result:\n\t\tbest_result=my_result\n\t\ttorch.save(model, base_path + \'/model_ESIM_batch_\' + task + \'.pkl\')\n\t\twith open(base_path+\'/result_ESIM_batch_\'+task+\'_prob.txt\',\'w\') as f:\n\t\t\tfor i in range(len(pred)):\n\t\t\t\tf.writelines(str(1 - pred[i]) + \'\\t\' + str(pred[i]) + \'\\n\')\n\t\tif task == \'wikiqa\' or task == \'trecqa\':\n\t\t\tlist1 = []\n\t\t\tfor line in open(base_path + \'/test.qrel\'):\n\t\t\t\tlist1.append(line.strip().split())\n\t\t\twith open(base_path + \'/result_ESIM_\' + task, \'w\') as f:\n\t\t\t\tfor i in range(len(pred)):\n\t\t\t\t\tf.writelines(str(list1[i][0]) + \'\\t\' + str(list1[i][1]) + \'\\t\' + str(\n\t\t\t\t\t\tlist1[i][2]) + \'\\t\' + \'*\\t\' + str(pred[i]) + \'\\t\' + \'*\\n\')\n\t\t\tcmd = (\'%s -m map %s %s\' % (castorini_path, base_path + \'/test.qrel\', base_path + \'/result_ESIM_\' + task))\n\t\t\tos.system(cmd)\n\t\t\tcmd = (\'%s -m recip_rank %s %s\' % (castorini_path, base_path + \'/test.qrel\', base_path + \'/result_ESIM_\' + task))\n\t\t\tos.system(cmd)\n\telapsed_time = time.time() - start_time\n\tprint(\'Epoch \' + str(epoch) + \' finished within \' + str(timedelta(seconds=elapsed_time)))'"
ESIM/main_batch_url.py,22,"b'from __future__ import division\nimport sys\nfrom data_iterator import TextIterator\nimport cPickle as pkl\nfrom os.path import expanduser\nimport torch\nimport numpy\nfrom model_batch import *\nimport time\nfrom datetime import timedelta\nfrom torchtext.vocab import load_word_vectors\n\n# batch preparation\ndef prepare_data(seqs_x, seqs_y, labels, maxlen=None):\n\tlengths_x = [len(s) for s in seqs_x]\n\tlengths_y = [len(s) for s in seqs_y]\n\n\tif maxlen is not None:\n\t\tnew_seqs_x = []\n\t\tnew_seqs_y = []\n\t\tnew_lengths_x = []\n\t\tnew_lengths_y = []\n\t\tnew_labels = []\n\t\tfor l_x, s_x, l_y, s_y, l in zip(lengths_x, seqs_x, lengths_y, seqs_y, labels):\n\t\t\tif l_x < maxlen and l_y < maxlen:\n\t\t\t\tnew_seqs_x.append(s_x)\n\t\t\t\tnew_lengths_x.append(l_x)\n\t\t\t\tnew_seqs_y.append(s_y)\n\t\t\t\tnew_lengths_y.append(l_y)\n\t\t\t\tnew_labels.append(l)\n\t\tlengths_x = new_lengths_x\n\t\tseqs_x = new_seqs_x\n\t\tlengths_y = new_lengths_y\n\t\tseqs_y = new_seqs_y\n\t\tlabels = new_labels\n\n\t\tif len(lengths_x) < 1 or len(lengths_y) < 1:\n\t\t\treturn None, None, None, None, None\n\n\tn_samples = len(seqs_x)\n\tmaxlen_x = numpy.max(lengths_x)\n\tmaxlen_y = numpy.max(lengths_y)\n\n\tx = numpy.zeros((maxlen_x, n_samples)).astype(\'int64\')\n\ty = numpy.zeros((maxlen_y, n_samples)).astype(\'int64\')\n\tx_mask = numpy.zeros((maxlen_x, n_samples)).astype(\'float32\')\n\ty_mask = numpy.zeros((maxlen_y, n_samples)).astype(\'float32\')\n\tl = numpy.zeros((n_samples,)).astype(\'int64\')\n\tfor idx, [s_x, s_y, ll] in enumerate(zip(seqs_x, seqs_y, labels)):\n\t\tx[:lengths_x[idx], idx] = s_x\n\t\tx_mask[:lengths_x[idx], idx] = 1.\n\t\ty[:lengths_y[idx], idx] = s_y\n\t\ty_mask[:lengths_y[idx], idx] = 1.\n\t\tl[idx] = ll\n\n\tif torch.cuda.is_available():\n\t\tx=Variable(torch.LongTensor(x)).cuda()\n\t\tx_mask=Variable(torch.Tensor(x_mask)).cuda()\n\t\ty=Variable(torch.LongTensor(y)).cuda()\n\t\ty_mask=Variable(torch.Tensor(y_mask)).cuda()\n\t\tl=Variable(torch.LongTensor(l)).cuda()\n\telse:\n\t\tx = Variable(torch.LongTensor(x))\n\t\tx_mask = Variable(torch.FloatTensor(x_mask))\n\t\ty = Variable(torch.LongTensor(y))\n\t\ty_mask = Variable(torch.FloatTensor(y_mask))\n\t\tl = Variable(torch.LongTensor(l))\n\treturn x, x_mask, y, y_mask, l\n\n# some utilities\ndef ortho_weight(ndim):\n\t""""""\n\tRandom orthogonal weights\n\n\tUsed by norm_weights(below), in which case, we\n\tare ensuring that the rows are orthogonal\n\t(i.e W = U \\Sigma V, U has the same\n\t# of rows, V has the same # of cols)\n\t""""""\n\tW = numpy.random.randn(ndim, ndim)\n\tu, s, v = numpy.linalg.svd(W)\n\treturn u.astype(\'float32\')\n\n\ndef norm_weight(nin, nout=None, scale=0.01, ortho=True):\n\t""""""\n\tRandom weights drawn from a Gaussian\n\t""""""\n\tif nout is None:\n\t\tnout = nin\n\tif nout == nin and ortho:\n\t\tW = ortho_weight(nin)\n\telse:\n\t\tW = scale * numpy.random.randn(nin, nout)\n\treturn W.astype(\'float32\')\n\nif torch.cuda.is_available():\n\tprint(\'CUDA is available!\')\n\tbase_path = expanduser(""~"") + \'/pytorch/ESIM\'\n\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\nelse:\n\tbase_path = expanduser(""~"") + \'/Documents/research/pytorch/ESIM\'\n\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\ntask=\'url\'\nprint(\'task: \'+task)\nif task==\'snli\':\n\tdictionary=base_path+\'/data/word_sequence/snli_vocab_cased.pkl\'\n\tdatasets         = [base_path+\'/data/word_sequence/premise_snli_1.0_train.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/hypothesis_snli_1.0_train.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/label_snli_1.0_train.txt\']\n\tvalid_datasets   = [base_path+\'/data/word_sequence/premise_snli_1.0_dev.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/hypothesis_snli_1.0_dev.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/label_snli_1.0_dev.txt\']\n\ttest_datasets    = [base_path+\'/data/word_sequence/premise_snli_1.0_test.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/hypothesis_snli_1.0_test.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/label_snli_1.0_test.txt\']\nelif task==\'mnli\':\n\tdic = {\'entailment\': \'0\', \'neutral\': \'1\', \'contradiction\': \'2\'}\n\tdictionary = base_path + \'/data/word_sequence/mnli_vocab_cased.pkl\'\n\tdatasets = [base_path + \'/data/word_sequence/premise_multinli_1.0_train.txt\',\n\t\t\t\tbase_path + \'/data/word_sequence/hypothesis_multinli_1.0_train.txt\',\n\t\t\t\tbase_path + \'/data/word_sequence/label_multinli_1.0_train.txt\']\n\tvalid_datasets_m = [base_path + \'/data/word_sequence/premise_multinli_1.0_dev_matched.txt\',\n\t\t\t\t\t  base_path + \'/data/word_sequence/hypothesis_multinli_1.0_dev_matched.txt\',\n\t\t\t\t\t  base_path + \'/data/word_sequence/label_multinli_1.0_dev_matched.txt\']\n\tvalid_datasets_um = [base_path + \'/data/word_sequence/premise_multinli_1.0_dev_mismatched.txt\',\n\t\t\t\t\t\tbase_path + \'/data/word_sequence/hypothesis_multinli_1.0_dev_mismatched.txt\',\n\t\t\t\t\t\tbase_path + \'/data/word_sequence/label_multinli_1.0_dev_mismatched.txt\']\n\ttest_datasets_m = [base_path + \'/data/word_sequence/premise_multinli_1.0_test_matched.txt\',\n\t\t\t\t\t  base_path + \'/data/word_sequence/hypothesis_multinli_1.0_test_matched.txt\',\n\t\t\t\t\t  base_path + \'/data/word_sequence/label_multinli_1.0_test_matched.txt\']\n\ttest_datasets_um = [base_path + \'/data/word_sequence/premise_multinli_1.0_test_mismatched.txt\',\n\t\t\t\t\t\tbase_path + \'/data/word_sequence/hypothesis_multinli_1.0_test_mismatched.txt\',\n\t\t\t\t\t\tbase_path + \'/data/word_sequence/label_multinli_1.0_test_mismatched.txt\']\nelif task==\'url\':\n\tif torch.cuda.is_available():\n\t\tprint(\'CUDA is available!\')\n\t\tbase_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/url\'\n\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\telse:\n\t\tbase_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/url\'\n\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\tdictionary=base_path+\'/url_vocab_cased.pkl\'\n\tdatasets=[base_path+\'/train/a.toks\', base_path+\'/train/b.toks\', base_path+\'/train/sim.txt\']\n\ttest_datasets=[base_path+\'/test_9324/a.toks\', base_path+\'/test_9324/b.toks\', base_path+\'/test_9324/sim.txt\']\n\n#n_words=42394\ndim_word=300\nbatch_size=1\nnum_epochs=500\nvalid_batch_size=1\nprint \'Loading data\'\nwith open(dictionary, \'rb\') as f:\n\tworddicts = pkl.load(f)\nn_words=len(worddicts)\nwv_dict, wv_arr, wv_size = load_word_vectors(embedding_path, \'glove.840B\', dim_word)\npretrained_emb=norm_weight(n_words, dim_word)\nfor word in worddicts.keys():\n\ttry:\n\t\tpretrained_emb[worddicts[word]]=wv_arr[wv_dict[word]].numpy()\n\texcept:\n\t\tpretrained_emb[worddicts[word]] = torch.normal(torch.zeros(dim_word),std=1).numpy()\ntrain = TextIterator(datasets[0], datasets[1], datasets[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=batch_size)\ntest = TextIterator(test_datasets[0], test_datasets[1], test_datasets[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=valid_batch_size,\n\t\t\t\t\t shuffle=False)\n\ncriterion = torch.nn.CrossEntropyLoss()\nmodel = ESIM(dim_word, 2, n_words, dim_word, pretrained_emb)\nif torch.cuda.is_available():\n\tmodel = model.cuda()\n\tcriterion = criterion.cuda()\noptimizer = torch.optim.Adam(model.parameters(),lr=0.0004)\nprint(\'start training...\')\naccumulated_loss=0\nbatch_counter=0\nreport_interval = 10000\nbest_dev_loss=10e10\nbest_dev_loss2=10e10\nclip_c=10\nmax_len=500\nmax_result=0\nbest_result = 0\n\nmodel.train()\nfor epoch in range(num_epochs):\n\taccumulated_loss = 0\n\tmodel.train()\n\tprint(\'--\' * 20)\n\tstart_time = time.time()\n\ttrain_sents_scaned = 0\n\ttrain_num_correct = 0\n\tbatch_counter=0\n\tfor x1, x2, y in train:\n\t\tx1, x1_mask, x2, x2_mask, y = prepare_data(x1, x2, y, maxlen=max_len)\n\t\ttrain_sents_scaned += len(y)\n\t\toptimizer.zero_grad()\n\t\toutput = model(x1, x1_mask, x2, x2_mask)\n\t\tresult = output.data.cpu().numpy()\n\t\ta = np.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\ttrain_num_correct += np.sum(a == b)\n\t\tloss = criterion(output, y)\n\t\tloss.backward()\n\n\t\t\'\'\'\'\'\'\n\t\tgrad_norm = 0.\n\n\t\tfor m in list(model.parameters()):\n\t\t\tgrad_norm+=m.grad.data.norm() ** 2\n\n\t\tfor m in list(model.parameters()):\n\t\t\tif grad_norm>clip_c**2:\n\t\t\t\ttry:\n\t\t\t\t\tm.grad.data= m.grad.data / torch.sqrt(grad_norm) * clip_c\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\t\t\'\'\'\'\'\'\n\n\t\toptimizer.step()\n\t\taccumulated_loss += loss.data[0]\n\t\tbatch_counter += 1\n\t\tprint(\'batch_counter: \' + str(batch_counter))\n\t\tif batch_counter % report_interval == 0:\n\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\tmsg += \'\\t training batch loss: %f\' % (accumulated_loss / train_sents_scaned)\n\t\t\t#msg += \'\\t train accuracy: %f\' % (train_num_correct / train_sents_scaned)\n\t\t\tprint(msg)\n\t# test after each epoch\n\tmodel.eval()\n\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\taccumulated_loss = 0\n\tdev_num_correct = 0\n\tn_done = 0\n\tpred=[]\n\tgold = []\n\tprint(\'testing on URL dataset:\')\n\ttest_datasets = [base_path + \'/test_9324/a.toks\', base_path + \'/test_9324/b.toks\', base_path + \'/test_9324/sim.txt\']\n\ttest = TextIterator(test_datasets[0], test_datasets[1], test_datasets[2],\n\t                    dictionary,\n\t                    n_words=n_words,\n\t                    batch_size=valid_batch_size,\n\t                    shuffle=False)\n\tfor test_x1, test_x2, test_y in test:\n\t\tn_done+=len(test_y)\n\t\tx1, x1_mask, x2, x2_mask, y = prepare_data(test_x1, test_x2, test_y)\n\t\twith torch.no_grad():\n\t\t\toutput = F.softmax(model(x1, x1_mask, x2, x2_mask))\n\t\tresult = output.data.cpu().numpy()\n\t\tloss = criterion(output, y)\n\t\taccumulated_loss += loss.data[0]\n\t\ta = numpy.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\tdev_num_correct += numpy.sum(a == b)\n\t\tpred.extend(result[:, 1])\n\t\tgold.extend(b)\n\tmsg += \'\\t test loss: %f\' % (accumulated_loss / n_done)\n\t#test_acc = dev_num_correct / n_done\n\t#msg += \'\\t test accuracy: %f\' % test_acc\n\tprint(msg)\n\tURL_maxF1_eval(pred, gold)\n\t\'\'\'\n\tprint(\'testing on PIT dataset:\')\n\tpred = []\n\tgold = []\n\taccumulated_loss=0\n\tpit_base_path=base_path.replace(\'url\',\'pit\')\n\ttest_datasets = [pit_base_path + \'/test/a.toks\', pit_base_path + \'/test/b.toks\', pit_base_path + \'/test/sim.txt\']\n\ttest = TextIterator(test_datasets[0], test_datasets[1], test_datasets[2],\n\t                    dictionary,\n\t                    n_words=n_words,\n\t                    batch_size=valid_batch_size,\n\t                    shuffle=False)\n\tfor test_x1, test_x2, test_y in test:\n\t\tn_done+=len(test_y)\n\t\tx1, x1_mask, x2, x2_mask, y = prepare_data(test_x1, test_x2, test_y)\n\t\twith torch.no_grad():\n\t\t\toutput = F.softmax(model(x1, x1_mask, x2, x2_mask))\n\t\tresult = output.data.cpu().numpy()\n\t\tloss = criterion(output, y)\n\t\taccumulated_loss += loss.data[0]\n\t\ta = numpy.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\tdev_num_correct += numpy.sum(a == b)\n\t\tpred.extend(result[:, 1])\n\t\tgold.extend(b)\n\t#msg += \'\\t test loss: %f\' % (accumulated_loss / n_done)\n\t#msg += \'\\t test accuracy: %f\' % test_acc\n\t#print(msg)\n\tURL_maxF1_eval(pred, gold)\n\tprint(\'testing on Quora dataset:\')\n\tdev_num_correct=0\n\tn_done=0\n\tquora_base_path=base_path.replace(\'url\',\'quora\')\n\ttest_datasets = [quora_base_path+\'/test/a.toks\', quora_base_path+\'/test/b.toks\', quora_base_path+\'/test/sim.txt\']\n\ttest = TextIterator(test_datasets[0], test_datasets[1], test_datasets[2],\n\t                    dictionary,\n\t                    n_words=n_words,\n\t                    batch_size=valid_batch_size,\n\t                    shuffle=False)\n\tfor test_x1, test_x2, test_y in test:\n\t\tn_done+=len(test_y)\n\t\tx1, x1_mask, x2, x2_mask, y = prepare_data(test_x1, test_x2, test_y)\n\t\twith torch.no_grad():\n\t\t\toutput = F.softmax(model(x1, x1_mask, x2, x2_mask))\n\t\tresult = output.data.cpu().numpy()\n\t\tloss = criterion(output, y)\n\t\taccumulated_loss += loss.data[0]\n\t\ta = numpy.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\tdev_num_correct += numpy.sum(a == b)\n\ttest_acc=dev_num_correct/n_done\n\tmsg = \'\\t test accuracy: %f\' % test_acc\n\tprint(msg)\n\t\'\'\'\n\t#if my_result > best_result:\n\t#\tbest_result=my_result\n\t#\ttorch.save(model, base_path + \'/model_ESIM_batch_\' + task + \'.pkl\')\n\t#\twith open(base_path+\'/result_ESIM_batch_\'+task+\'_prob.txt\',\'w\') as f:\n\t#\t\tfor i in range(len(pred)):\n\t#\t\t\tf.writelines(str(1 - pred[i]) + \'\\t\' + str(pred[i]) + \'\\n\')\n\telapsed_time = time.time() - start_time\n\tprint(\'Epoch \' + str(epoch) + \' finished within \' + str(timedelta(seconds=elapsed_time)))'"
ESIM/main_batch_wikiqa.py,20,"b'from __future__ import division\nimport sys\nfrom data_iterator import TextIterator\nimport cPickle as pkl\nfrom os.path import expanduser\nimport torch\nimport numpy\nfrom model_batch import *\nimport time\nimport os\nfrom datetime import timedelta\nfrom torchtext.vocab import load_word_vectors\n\n# batch preparation\n# batch preparation\ndef prepare_data(seqs_x, seqs_y, labels, maxlen=None):\n\tlengths_x = [len(s) for s in seqs_x]\n\tlengths_y = [len(s) for s in seqs_y]\n\n\tif maxlen is not None:\n\t\tnew_seqs_x = []\n\t\tnew_seqs_y = []\n\t\tnew_lengths_x = []\n\t\tnew_lengths_y = []\n\t\tnew_labels = []\n\t\tfor l_x, s_x, l_y, s_y, l in zip(lengths_x, seqs_x, lengths_y, seqs_y, labels):\n\t\t\tif l_x < maxlen and l_y < maxlen:\n\t\t\t\tnew_seqs_x.append(s_x)\n\t\t\t\tnew_lengths_x.append(l_x)\n\t\t\t\tnew_seqs_y.append(s_y)\n\t\t\t\tnew_lengths_y.append(l_y)\n\t\t\t\tnew_labels.append(l)\n\t\tlengths_x = new_lengths_x\n\t\tseqs_x = new_seqs_x\n\t\tlengths_y = new_lengths_y\n\t\tseqs_y = new_seqs_y\n\t\tlabels = new_labels\n\n\t\tif len(lengths_x) < 1 or len(lengths_y) < 1:\n\t\t\treturn None, None, None, None, None\n\n\tn_samples = len(seqs_x)\n\tmaxlen_x = numpy.max(lengths_x)\n\tmaxlen_y = numpy.max(lengths_y)\n\n\tx = numpy.zeros((maxlen_x, n_samples)).astype(\'int64\')\n\ty = numpy.zeros((maxlen_y, n_samples)).astype(\'int64\')\n\tx_mask = numpy.zeros((maxlen_x, n_samples)).astype(\'float32\')\n\ty_mask = numpy.zeros((maxlen_y, n_samples)).astype(\'float32\')\n\tl = numpy.zeros((n_samples,)).astype(\'int64\')\n\tfor idx, [s_x, s_y, ll] in enumerate(zip(seqs_x, seqs_y, labels)):\n\t\tx[:lengths_x[idx], idx] = s_x\n\t\tx_mask[:lengths_x[idx], idx] = 1.\n\t\ty[:lengths_y[idx], idx] = s_y\n\t\ty_mask[:lengths_y[idx], idx] = 1.\n\t\tl[idx] = ll\n\n\tif torch.cuda.is_available():\n\t\tx=Variable(torch.LongTensor(x)).cuda()\n\t\tx_mask=Variable(torch.Tensor(x_mask)).cuda()\n\t\ty=Variable(torch.LongTensor(y)).cuda()\n\t\ty_mask=Variable(torch.Tensor(y_mask)).cuda()\n\t\tl=Variable(torch.LongTensor(l)).cuda()\n\telse:\n\t\tx = Variable(torch.LongTensor(x))\n\t\tx_mask = Variable(torch.FloatTensor(x_mask))\n\t\ty = Variable(torch.LongTensor(y))\n\t\ty_mask = Variable(torch.FloatTensor(y_mask))\n\t\tl = Variable(torch.LongTensor(l))\n\treturn x, x_mask, y, y_mask, l\n\n# some utilities\ndef ortho_weight(ndim):\n\t""""""\n\tRandom orthogonal weights\n\n\tUsed by norm_weights(below), in which case, we\n\tare ensuring that the rows are orthogonal\n\t(i.e W = U \\Sigma V, U has the same\n\t# of rows, V has the same # of cols)\n\t""""""\n\tW = numpy.random.randn(ndim, ndim)\n\tu, s, v = numpy.linalg.svd(W)\n\treturn u.astype(\'float32\')\n\n\ndef norm_weight(nin, nout=None, scale=0.01, ortho=True):\n\t""""""\n\tRandom weights drawn from a Gaussian\n\t""""""\n\tif nout is None:\n\t\tnout = nin\n\tif nout == nin and ortho:\n\t\tW = ortho_weight(nin)\n\telse:\n\t\tW = scale * numpy.random.randn(nin, nout)\n\treturn W.astype(\'float32\')\n\nif torch.cuda.is_available():\n\tprint(\'CUDA is available!\')\n\tbase_path = expanduser(""~"") + \'/pytorch/ESIM\'\n\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\nelse:\n\tbase_path = expanduser(""~"") + \'/Documents/research/pytorch/ESIM\'\n\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\ntask=\'wikiqa\'\nprint(\'task: \'+task)\nif task==\'snli\':\n\tdictionary=base_path+\'/data/word_sequence/snli_vocab_cased.pkl\'\n\tdatasets         = [base_path+\'/data/word_sequence/premise_snli_1.0_train.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/hypothesis_snli_1.0_train.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/label_snli_1.0_train.txt\']\n\tvalid_datasets   = [base_path+\'/data/word_sequence/premise_snli_1.0_dev.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/hypothesis_snli_1.0_dev.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/label_snli_1.0_dev.txt\']\n\ttest_datasets    = [base_path+\'/data/word_sequence/premise_snli_1.0_test.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/hypothesis_snli_1.0_test.txt\',\n\t\t\t\t\t\t\tbase_path+\'/data/word_sequence/label_snli_1.0_test.txt\']\nelif task==\'mnli\':\n\tdic = {\'entailment\': \'0\', \'neutral\': \'1\', \'contradiction\': \'2\'}\n\tdictionary = base_path + \'/data/word_sequence/mnli_vocab_cased.pkl\'\n\tdatasets = [base_path + \'/data/word_sequence/premise_multinli_1.0_train.txt\',\n\t\t\t\tbase_path + \'/data/word_sequence/hypothesis_multinli_1.0_train.txt\',\n\t\t\t\tbase_path + \'/data/word_sequence/label_multinli_1.0_train.txt\']\n\tvalid_datasets_m = [base_path + \'/data/word_sequence/premise_multinli_1.0_dev_matched.txt\',\n\t\t\t\t\t  base_path + \'/data/word_sequence/hypothesis_multinli_1.0_dev_matched.txt\',\n\t\t\t\t\t  base_path + \'/data/word_sequence/label_multinli_1.0_dev_matched.txt\']\n\tvalid_datasets_um = [base_path + \'/data/word_sequence/premise_multinli_1.0_dev_mismatched.txt\',\n\t\t\t\t\t\tbase_path + \'/data/word_sequence/hypothesis_multinli_1.0_dev_mismatched.txt\',\n\t\t\t\t\t\tbase_path + \'/data/word_sequence/label_multinli_1.0_dev_mismatched.txt\']\n\ttest_datasets_m = [base_path + \'/data/word_sequence/premise_multinli_1.0_test_matched.txt\',\n\t\t\t\t\t  base_path + \'/data/word_sequence/hypothesis_multinli_1.0_test_matched.txt\',\n\t\t\t\t\t  base_path + \'/data/word_sequence/label_multinli_1.0_test_matched.txt\']\n\ttest_datasets_um = [base_path + \'/data/word_sequence/premise_multinli_1.0_test_mismatched.txt\',\n\t\t\t\t\t\tbase_path + \'/data/word_sequence/hypothesis_multinli_1.0_test_mismatched.txt\',\n\t\t\t\t\t\tbase_path + \'/data/word_sequence/label_multinli_1.0_test_mismatched.txt\']\nelif task==\'wikiqa\':\n\tif torch.cuda.is_available():\n\t\tprint(\'CUDA is available!\')\n\t\tbase_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/wikiqa\'\n\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\tcastorini_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/castorini/trec_eval.9.0/trec_eval\'\n\telse:\n\t\tbase_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/wikiqa\'\n\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\tcastorini_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/castorini/trec_eval.9.0/trec_eval\'\n\tdictionary=base_path+\'/wikiqa_vocab_cased.pkl\'\n\tdatasets=[base_path+\'/train/a.toks\', base_path+\'/train/b.toks\', base_path+\'/train/sim.txt\']\n\ttest_datasets=[base_path+\'/test/a.toks\', base_path+\'/test/b.toks\', base_path+\'/test/sim.txt\']\n\ncmd = (\'%s -m map %s %s\' % (castorini_path, base_path + \'/test.qrel\', base_path + \'/result_ESIM_\' + task))\nos.system(cmd)\ncmd = (\'%s -m recip_rank %s %s\' % (castorini_path, base_path + \'/test.qrel\', base_path + \'/result_ESIM_\' + task))\nos.system(cmd)\n\n#n_words=42394\ndim_word=300\nbatch_size=32\nnum_epochs=500\nvalid_batch_size=32\nprint \'Loading data\'\nwith open(dictionary, \'rb\') as f:\n\tworddicts = pkl.load(f)\nn_words=len(worddicts)\nwv_dict, wv_arr, wv_size = load_word_vectors(embedding_path, \'glove.840B\', dim_word)\npretrained_emb=norm_weight(n_words, dim_word)\nfor word in worddicts.keys():\n\ttry:\n\t\tpretrained_emb[worddicts[word]]=wv_arr[wv_dict[word]].numpy()\n\texcept:\n\t\tpretrained_emb[worddicts[word]] = torch.normal(torch.zeros(dim_word),std=1).numpy()\ntrain = TextIterator(datasets[0], datasets[1], datasets[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=batch_size)\ntest = TextIterator(test_datasets[0], test_datasets[1], test_datasets[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=valid_batch_size,\n\t\t\t\t\t shuffle=False)\n\ncriterion = torch.nn.CrossEntropyLoss()\nmodel = ESIM(dim_word, 2, n_words, dim_word, pretrained_emb)\nif torch.cuda.is_available():\n\tmodel = model.cuda()\n\tcriterion = criterion.cuda()\noptimizer = torch.optim.Adam(model.parameters(),lr=0.0004)\nprint(\'start training...\')\naccumulated_loss=0\nbatch_counter=0\nreport_interval = 1000\nbest_dev_loss=10e10\nbest_dev_loss2=10e10\nclip_c=10\nmax_len=100\nmax_result=0\nbest_result = 0\n\nmodel.train()\nfor epoch in range(num_epochs):\n\taccumulated_loss = 0\n\tmodel.train()\n\tprint(\'--\' * 20)\n\tstart_time = time.time()\n\ttrain_sents_scaned = 0\n\ttrain_num_correct = 0\n\tbatch_counter=0\n\tfor x1, x2, y in train:\n\t\tx1, x1_mask, x2, x2_mask, y = prepare_data(x1, x2, y, maxlen=max_len)\n\t\ttrain_sents_scaned += len(y)\n\t\toptimizer.zero_grad()\n\t\toutput = model(x1, x1_mask, x2, x2_mask)\n\t\tresult = output.data.cpu().numpy()\n\t\ta = np.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\ttrain_num_correct += np.sum(a == b)\n\t\tloss = criterion(output, y)\n\t\tloss.backward()\n\n\t\t\'\'\'\'\'\'\n\t\tgrad_norm = 0.\n\n\t\tfor m in list(model.parameters()):\n\t\t\tgrad_norm+=m.grad.data.norm() ** 2\n\n\t\tfor m in list(model.parameters()):\n\t\t\tif grad_norm>clip_c**2:\n\t\t\t\ttry:\n\t\t\t\t\tm.grad.data= m.grad.data / torch.sqrt(grad_norm) * clip_c\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\t\t\'\'\'\'\'\'\n\n\t\toptimizer.step()\n\t\taccumulated_loss += loss.data[0]\n\t\tbatch_counter += 1\n\t\tif batch_counter % report_interval == 0:\n\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\tmsg += \'\\t training batch loss: %f\' % (accumulated_loss / train_sents_scaned)\n\t\t\t#msg += \'\\t train accuracy: %f\' % (train_num_correct / train_sents_scaned)\n\t\t\tprint(msg)\n\t# test after each epoch\n\tmodel.eval()\n\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\taccumulated_loss = 0\n\tdev_num_correct = 0\n\tn_done = 0\n\tpred=[]\n\tgold = []\n\tfor test_x1, test_x2, test_y in test:\n\t\tn_done+=len(test_y)\n\t\tx1, x1_mask, x2, x2_mask, y = prepare_data(test_x1, test_x2, test_y, maxlen=max_len)\n\t\twith torch.no_grad():\n\t\t\toutput = F.softmax(model(x1, x1_mask, x2, x2_mask))\n\t\tresult = output.data.cpu().numpy()\n\t\tloss = criterion(output, y)\n\t\taccumulated_loss += loss.data[0]\n\t\ta = numpy.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\tdev_num_correct += numpy.sum(a == b)\n\t\tpred.extend(result[:, 1])\n\t\tgold.extend(b)\n\tmsg += \'\\t test loss: %f\' % (accumulated_loss / n_done)\n\ttest_acc = dev_num_correct / n_done\n\t#msg += \'\\t test accuracy: %f\' % test_acc\n\tprint(msg)\n\t_, my_result = URL_maxF1_eval(pred, gold)\n\tif my_result > best_result:\n\t\tbest_result=my_result\n\t\ttorch.save(model, base_path + \'/model_ESIM_batch_\' + task + \'.pkl\')\n\t\twith open(base_path+\'/result_ESIM_batch_\'+task+\'_prob.txt\',\'w\') as f:\n\t\t\tfor i in range(len(pred)):\n\t\t\t\tf.writelines(str(1 - pred[i]) + \'\\t\' + str(pred[i]) + \'\\n\')\n\t\tif task == \'wikiqa\' or task == \'trecqa\':\n\t\t\tlist1 = []\n\t\t\tfor line in open(base_path + \'/test.qrel\'):\n\t\t\t\tlist1.append(line.strip().split())\n\t\t\twith open(base_path + \'/result_ESIM_\' + task, \'w\') as f:\n\t\t\t\tfor i in range(len(pred)):\n\t\t\t\t\tf.writelines(str(list1[i][0]) + \'\\t\' + str(list1[i][1]) + \'\\t\' + str(\n\t\t\t\t\t\tlist1[i][2]) + \'\\t\' + \'*\\t\' + str(pred[i]) + \'\\t\' + \'*\\n\')\n\t\t\tcmd = (\'%s -m map %s %s\' % (castorini_path, base_path + \'/test.qrel\', base_path + \'/result_ESIM_\' + task))\n\t\t\tos.system(cmd)\n\t\t\tcmd = (\'%s -m recip_rank %s %s\' % (castorini_path, base_path + \'/test.qrel\', base_path + \'/result_ESIM_\' + task))\n\t\t\tos.system(cmd)\n\telapsed_time = time.time() - start_time\n\tprint(\'Epoch \' + str(epoch) + \' finished within \' + str(timedelta(seconds=elapsed_time)))'"
ESIM/model_batch.py,93,"b'import sys\nimport math\nimport torch\nimport numpy as np\nimport torch.nn as nn\nfrom util import *\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom datetime import datetime\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\ndef ortho_weight(ndim):\n\t""""""\n\tRandom orthogonal weights\n\n\tUsed by norm_weights(below), in which case, we\n\tare ensuring that the rows are orthogonal\n\t(i.e W = U \\Sigma V, U has the same\n\t# of rows, V has the same # of cols)\n\t""""""\n\tW = np.random.randn(ndim, ndim)\n\tu, s, v = np.linalg.svd(W)\n\treturn u.astype(\'float32\')\n\ndef norm_weight(nin, nout=None, scale=0.01, ortho=True):\n\t""""""\n\tRandom weights drawn from a Gaussian\n\t""""""\n\tif nout is None:\n\t\tnout = nin\n\tif nout == nin and ortho:\n\t\tW = ortho_weight(nin)\n\telse:\n\t\tW = scale * np.random.randn(nin, nout)\n\treturn W.astype(\'float32\')\n\ndef generate_mask_2(values, sent_sizes):\n\tmask_matrix = np.zeros((len(sent_sizes), max(sent_sizes), values.size(2)))\n\tfor i in range(len(sent_sizes)):\n\t\tmask_matrix[i][:sent_sizes[i]][:]=1\n\tif torch.cuda.is_available():\n\t\tmask_matrix = torch.Tensor(mask_matrix).cuda()\n\telse:\n\t\tmask_matrix = torch.Tensor(mask_matrix)\n\treturn values*Variable(mask_matrix)\n\ndef generate_mask(lsent_sizes, rsent_sizes):\n\tmask_matrix=np.zeros((len(lsent_sizes),max(lsent_sizes),max(rsent_sizes)))\n\tfor i in range(len(lsent_sizes)):\n\t\tmask_matrix[i][:lsent_sizes[i]][:rsent_sizes[i]]=1\n\tif torch.cuda.is_available():\n\t\tmask_matrix = torch.Tensor(mask_matrix).cuda()\n\telse:\n\t\tmask_matrix = torch.Tensor(mask_matrix)\n\treturn Variable(mask_matrix)\n\nclass LSTM_Cell(nn.Module):\n\n\tdef __init__(self, cuda, in_dim, mem_dim):\n\t\tsuper(LSTM_Cell, self).__init__()\n\t\tself.cudaFlag = cuda\n\t\tself.in_dim = in_dim\n\t\tself.mem_dim = mem_dim\n\n\t\tdef new_gate():\n\t\t\th = nn.Linear(self.mem_dim, self.mem_dim, bias=False)\n\t\t\th.weight.data.copy_(torch.from_numpy(ortho_weight(self.mem_dim)))\n\t\t\treturn h\n\n\t\tdef new_W():\n\t\t\tw = nn.Linear(self.in_dim, self.mem_dim)\n\t\t\tw.weight.data.copy_(torch.from_numpy(ortho_weight(self.mem_dim)))\n\t\t\treturn w\n\n\t\tself.ih = new_gate()\n\t\tself.fh = new_gate()\n\t\tself.oh = new_gate()\n\t\tself.ch = new_gate()\n\n\t\tself.cx = new_W()\n\t\tself.ox = new_W()\n\t\tself.fx = new_W()\n\t\tself.ix = new_W()\n\n\n\tdef forward(self, input, h, c):\n\t\tu = F.tanh(self.cx(input) + self.ch(h))\n\t\ti = F.sigmoid(self.ix(input) + self.ih(h))\n\t\tf = F.sigmoid(self.fx(input) + self.fh(h))\n\t\tc = i*u + f*c\n\t\to = F.sigmoid(self.ox(input) + self.oh(h))\n\t\th = o * F.tanh(c)\n\t\treturn c, h\n\nclass LSTM(nn.Module):\n\tdef __init__(self, cuda, in_dim, mem_dim):\n\t\tsuper(LSTM, self).__init__()\n\t\tself.cudaFlag = cuda\n\t\tself.in_dim = in_dim\n\t\tself.mem_dim = mem_dim\n\n\t\tself.TreeCell = LSTM_Cell(cuda, in_dim, mem_dim)\n\t\tself.output_module = None\n\n\tdef forward(self, x, x_mask):\n\t\t""""""\n\t\t:param x: #step x #sample x dim_emb\n\t\t:param x_mask: #step x #sample\n\t\t:param x_left_mask: #step x #sample x #step\n\t\t:param x_right_mask: #step x #sample x #step\n\t\t:return:\n\t\t""""""\n\t\th = Variable(torch.zeros(x.size(1), x.size(2)))\n\t\tc = Variable(torch.zeros(x.size(1), x.size(2)))\n\t\tif torch.cuda.is_available():\n\t\t\th=h.cuda()\n\t\t\tc=c.cuda()\n\t\tall_hidden=[]\n\t\tfor step in range(x.size(0)):\n\t\t\tinput=x[step] # #sample x dim_emb\n\t\t\tstep_c, step_h=self.TreeCell(input, h, c)\n\t\t\th=x_mask[step][:,None] * step_h + (1. - x_mask[step])[:,None] * h\n\t\t\tc = x_mask[step][:, None] * step_c + (1. - x_mask[step])[:, None] * c\n\t\t\tall_hidden.append(torch.unsqueeze(h,0))\n\t\treturn torch.cat(all_hidden,0)\n\nclass ESIM(nn.Module):\n\t""""""\n\t\tImplementation of the multi feed forward network model described in\n\t\tthe paper ""A Decomposable Attention Model for Natural Language\n\t\tInference"" by Parikh et al., 2016.\n\n\t\tIt applies feedforward MLPs to combinations of parts of the two sentences,\n\t\twithout any recurrent structure.\n\t""""""\n\tdef __init__(self, num_units, num_classes, vocab_size, embedding_size, pretrained_emb,\n\t\t\t\t training=True, project_input=True,\n\t\t\t\t use_intra_attention=False, distance_biases=10, max_sentence_length=30):\n\t\t""""""\n\t\tCreate the model based on MLP networks.\n\n\t\t:param num_units: size of the networks\n\t\t:param num_classes: number of classes in the problem\n\t\t:param vocab_size: size of the vocabulary\n\t\t:param embedding_size: size of each word embedding\n\t\t:param use_intra_attention: whether to use intra-attention model\n\t\t:param training: whether to create training tensors (optimizer)\n\t\t:param project_input: whether to project input embeddings to a\n\t\t\tdifferent dimensionality\n\t\t:param distance_biases: number of different distances with biases used\n\t\t\tin the intra-attention model\n\t\t""""""\n\t\tsuper(ESIM, self).__init__()\n\t\tself.vocab_size=vocab_size\n\t\tself.num_units = num_units\n\t\tself.num_classes = num_classes\n\t\tself.project_input = project_input\n\t\tself.embedding_size=embedding_size\n\t\tself.distance_biases=distance_biases\n\t\tself.max_sentence_length=max_sentence_length\n\t\tself.pretrained_emb=pretrained_emb\n\n\t\tself.dropout = nn.Dropout(p=0.5)\n\t\tself.word_embedding=nn.Embedding(vocab_size,embedding_size)\n\n\t\tself.lstm_intra=LSTM(torch.cuda.is_available(),embedding_size, num_units)\n\n\t\tself.linear_layer_compare = nn.Sequential(nn.Linear(4*num_units*2, num_units), nn.ReLU(), nn.Dropout(p=0.5))\n\t\t#                                          nn.Dropout(p=0.2), nn.Linear(num_units, num_units), nn.ReLU())\n\n\t\tself.lstm_compare=LSTM(torch.cuda.is_available(), embedding_size, num_units)\n\n\t\tself.linear_layer_aggregate = nn.Sequential(nn.Dropout(p=0.5), nn.Linear(4*num_units*2, num_units), nn.ReLU(),\n\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Dropout(p=0.5), nn.Linear(num_units, num_classes))\n\n\t\tself.init_weight()\n\n\tdef ortho_weight(self):\n\t\t""""""\n\t\tRandom orthogonal weights\n\t\tUsed by norm_weights(below), in which case, we\n\t\tare ensuring that the rows are orthogonal\n\t\t(i.e W = U \\Sigma V, U has the same\n\t\t# of rows, V has the same # of cols)\n\t\t""""""\n\t\tndim=self.num_units\n\t\tW = np.random.randn(ndim, ndim)\n\t\tu, s, v = np.linalg.svd(W)\n\t\treturn u.astype(\'float32\')\n\n\tdef initialize_lstm(self):\n\t\tif torch.cuda.is_available():\n\t\t\tinit=torch.Tensor(np.concatenate([self.ortho_weight(),self.ortho_weight(),self.ortho_weight(),self.ortho_weight()], 0)).cuda()\n\t\telse:\n\t\t\tinit = torch.Tensor(\n\t\t\t\tnp.concatenate([self.ortho_weight(), self.ortho_weight(), self.ortho_weight(), self.ortho_weight()], 0))\n\t\treturn init\n\n\tdef init_weight(self):\n\t\t#nn.init.normal(self.linear_layer_project,mean=0,std=0.1)\n\t\t#print(self.linear_layer_attend[3])\n\t\t#self.linear_layer_attend[1].weight.data.normal_(0, 0.01)\n\t\t#self.linear_layer_attend[1].bias.data.fill_(0)\n\t\t#self.linear_layer_attend[4].weight.data.normal_(0, 0.01)\n\t\t#self.linear_layer_attend[4].bias.data.fill_(0)\n\t\tself.linear_layer_compare[0].weight.data.normal_(0, 0.01)\n\t\tself.linear_layer_compare[0].bias.data.fill_(0)\n\t\t#self.linear_layer_compare[4].weight.data.normal_(0, 0.01)\n\t\t#self.linear_layer_compare[4].bias.data.fill_(0)\n\t\tself.linear_layer_aggregate[1].weight.data.normal_(0, 0.01)\n\t\tself.linear_layer_aggregate[1].bias.data.fill_(0)\n\t\tself.linear_layer_aggregate[4].weight.data.normal_(0, 0.01)\n\t\tself.linear_layer_aggregate[4].bias.data.fill_(0)\n\t\tself.word_embedding.weight.data.copy_(torch.from_numpy(self.pretrained_emb))\n\n\tdef attention_softmax3d(self,raw_attentions):\n\t\treshaped_attentions = raw_attentions.view(-1, raw_attentions.size(2))\n\t\tout=nn.functional.softmax(reshaped_attentions, dim=1)\n\t\treturn out.view(raw_attentions.size(0),raw_attentions.size(1),raw_attentions.size(2))\n\n\tdef _transformation_input(self,embed_sent, x1_mask):\n\t\tembed_sent = self.word_embedding(embed_sent)\n\t\tembed_sent = self.dropout(embed_sent)\n\t\thidden=self.lstm_intra(embed_sent, x1_mask)\n\t\treturn hidden\n\n\n\tdef aggregate(self,v1, v2):\n\t\t""""""\n\t\tAggregate the representations induced from both sentences and their\n\t\trepresentations\n\n\t\t:param v1: tensor with shape (batch, time_steps, num_units)\n\t\t:param v2: tensor with shape (batch, time_steps, num_units)\n\t\t:return: logits over classes, shape (batch, num_classes)\n\t\t""""""\n\t\tv1_mean = torch.mean(v1, 0)\n\t\tv2_mean = torch.mean(v2, 0)\n\t\tv1_max, _ = torch.max(v1, 0)\n\t\tv2_max, _ = torch.max(v2, 0)\n\t\tout = self.linear_layer_aggregate(torch.cat((v1_mean, v1_max, v2_mean, v2_max), 1))\n\n\t\t#v1_sum=torch.sum(v1,1)\n\t\t#v2_sum=torch.sum(v2,1)\n\t\t#out=self.linear_layer_aggregate(torch.cat([v1_sum,v2_sum],1))\n\n\t\treturn out\n\n\tdef cosine_interaction(self, tensor1, tensor2):\n\t\t""""""\n\t\t:param tensor1: #step1 * dim\n\t\t:param tensor2: #step2 * dim\n\t\t:return: #step1 * #step2\n\t\t""""""\n\t\tsimCube_0=tensor1[0].view(1,-1)\n\t\tsimCube_1=tensor2[0].view(1,-1)\n\t\tfor i in range(tensor1.size(0)):\n\t\t\tfor j in range(tensor2.size(0)):\n\t\t\t\tif not(i==0 and j==0):\n\t\t\t\t\tsimCube_0=torch.cat((simCube_0, tensor1[i].view(1,-1)))\n\t\t\t\t\tsimCube_1=torch.cat((simCube_1, tensor2[j].view(1,-1)))\n\t\tsimCube=F.cosine_similarity(simCube_0, simCube_1)\n\t\treturn simCube.view(tensor1.size(0),tensor2.size(0))\n\n\tdef forward_old(self, x1, x1_mask, x2, x2_mask):\n\t\t# idx = [i for i in range(embed_sent.size(1) - 1, -1, -1)]\n\t\t# if torch.cuda.is_available():\n\t\t#\tidx = torch.cuda.LongTensor(idx)\n\t\t# else:\n\t\t#\tidx = torch.LongTensor(idx)\n\n\t\tx1 = self.word_embedding(x1)\n\t\tx1 = self.dropout(x1)\n\t\tx2 = self.word_embedding(x2)\n\t\tx2 = self.dropout(x2)\n\n\t\tidx_1 = [i for i in range(x1.size(0) - 1, -1, -1)]\n\t\tif torch.cuda.is_available():\n\t\t\tidx_1 = Variable(torch.cuda.LongTensor(idx_1))\n\t\telse:\n\t\t\tidx_1 = Variable(torch.LongTensor(idx_1))\n\t\tx1_r=torch.index_select(x1,0,idx_1)\n\t\tx1_mask_r=torch.index_select(x1_mask,0,idx_1)\n\t\tidx_2=[i for i in range(x2.size(0) -1, -1, -1)]\n\t\tif torch.cuda.is_available():\n\t\t\tidx_2 = Variable(torch.cuda.LongTensor(idx_2))\n\t\telse:\n\t\t\tidx_2 = Variable(torch.LongTensor(idx_2))\n\t\tx2_r=torch.index_select(x2,0,idx_2)\n\t\tx2_mask_r=torch.index_select(x2_mask, 0, idx_2)\n\n\t\tproj1=self.lstm_intra(x1, x1_mask)\n\t\tproj1_r=self.lstm_intra(x1_r, x1_mask_r)\n\t\tproj2=self.lstm_intra(x2, x2_mask)\n\t\tproj2_r=self.lstm_intra(x2_r, x2_mask_r)\n\n\t\tctx1=torch.cat((proj1, torch.index_select(proj1_r,0,idx_1)),2)\n\t\tctx2=torch.cat((proj2, torch.index_select(proj2_r, 0, idx_2)),2)\n\t\t# ctx1: #step1 x #sample x #dimctx\n\t\t# ctx2: #step2 x #sample x #dimctx\n\t\tctx1 = ctx1 * x1_mask[:, :, None]\n\t\tctx2 = ctx2 * x2_mask[:, :, None]\n\n\t\t# weight_matrix: #sample x #step1 x #step2\n\t\tweight_matrix = torch.matmul(ctx1.permute(1, 0, 2), ctx2.permute(1, 2, 0))\n\t\tweight_matrix_1 = torch.exp(weight_matrix - weight_matrix.max(1, keepdim=True)[0]).permute(1, 2, 0)\n\t\tweight_matrix_2 = torch.exp(weight_matrix - weight_matrix.max(2, keepdim=True)[0]).permute(1, 2, 0)\n\n\t\t# weight_matrix_1: #step1 x #step2 x #sample\n\t\tweight_matrix_1 = weight_matrix_1 * x1_mask[:, None, :]\n\t\tweight_matrix_2 = weight_matrix_2 * x2_mask[None, :, :]\n\n\t\talpha = weight_matrix_1 / weight_matrix_1.sum(0, keepdim=True)\n\t\tbeta = weight_matrix_2 / weight_matrix_2.sum(1, keepdim=True)\n\n\t\tctx2_ = (torch.unsqueeze(ctx1,1) * torch.unsqueeze(alpha,3)).sum(0)\n\t\tctx1_ = (torch.unsqueeze(ctx2, 0) * torch.unsqueeze(beta,3)).sum(1)\n\n\t\tinp1 = torch.cat([ctx1, ctx1_, ctx1 * ctx1_, ctx1 - ctx1_], 2)\n\t\tinp2 = torch.cat([ctx2, ctx2_, ctx2 * ctx2_, ctx2 - ctx2_], 2)\n\t\tinp1=self.dropout(self.linear_layer_compare(inp1))\n\t\tinp2=self.dropout(self.linear_layer_compare(inp2))\n\t\tinp1_r=torch.index_select(inp1, 0, idx_1)\n\t\tinp2_r=torch.index_select(inp2, 0, idx_2)\n\n\t\tv1=self.lstm_compare(inp1, x1_mask)\n\t\tv2=self.lstm_compare(inp2, x2_mask)\n\t\tv1_r = self.lstm_compare(inp1_r, x1_mask)\n\t\tv2_r = self.lstm_compare(inp2_r, x2_mask)\n\t\tv1=torch.cat((v1, torch.index_select(v1_r, 0, idx_1)),2)\n\t\tv2=torch.cat((v2, torch.index_select(v2_r, 0, idx_2)),2)\n\t\tlogits = self.aggregate(v1, v2)\n\t\treturn logits\n\n\tdef forward(self, x1, x1_mask, x2, x2_mask):\n\t\t# idx = [i for i in range(embed_sent.size(1) - 1, -1, -1)]\n\t\t# if torch.cuda.is_available():\n\t\t#\tidx = torch.cuda.LongTensor(idx)\n\t\t# else:\n\t\t#\tidx = torch.LongTensor(idx)\n\n\t\tx1 = self.word_embedding(x1)\n\t\tx1 = self.dropout(x1)\n\t\tx2 = self.word_embedding(x2)\n\t\tx2 = self.dropout(x2)\n\n\t\tidx_1 = [i for i in range(x1.size(0) - 1, -1, -1)]\n\t\tif torch.cuda.is_available():\n\t\t\tidx_1 = Variable(torch.cuda.LongTensor(idx_1))\n\t\telse:\n\t\t\tidx_1 = Variable(torch.LongTensor(idx_1))\n\t\tx1_r=torch.index_select(x1,0,idx_1)\n\t\tx1_mask_r=torch.index_select(x1_mask,0,idx_1)\n\t\tidx_2=[i for i in range(x2.size(0) -1, -1, -1)]\n\t\tif torch.cuda.is_available():\n\t\t\tidx_2 = Variable(torch.cuda.LongTensor(idx_2))\n\t\telse:\n\t\t\tidx_2 = Variable(torch.LongTensor(idx_2))\n\t\tx2_r=torch.index_select(x2,0,idx_2)\n\t\tx2_mask_r=torch.index_select(x2_mask, 0, idx_2)\n\n\t\tproj1=self.lstm_intra(x1, x1_mask)\n\t\tproj1_r=self.lstm_intra(x1_r, x1_mask_r)\n\t\tproj2=self.lstm_intra(x2, x2_mask)\n\t\tproj2_r=self.lstm_intra(x2_r, x2_mask_r)\n\n\t\tctx1=torch.cat((proj1, torch.index_select(proj1_r,0,idx_1)),2)\n\t\tctx2=torch.cat((proj2, torch.index_select(proj2_r, 0, idx_2)),2)\n\t\t# ctx1: #step1 x #sample x #dimctx\n\t\t# ctx2: #step2 x #sample x #dimctx\n\t\tctx1 = ctx1 * x1_mask[:, :, None]\n\t\tctx2 = ctx2 * x2_mask[:, :, None]\n\n\t\t# weight_matrix: #sample x #step1 x #step2\n\t\tweight_matrix = torch.matmul(ctx1.permute(1, 0, 2), ctx2.permute(1, 2, 0))\n\t\tweight_matrix_1 = torch.exp(weight_matrix - weight_matrix.max(1, keepdim=True)[0]).permute(1, 2, 0)\n\t\tweight_matrix_2 = torch.exp(weight_matrix - weight_matrix.max(2, keepdim=True)[0]).permute(1, 2, 0)\n\n\t\t# weight_matrix_1: #step1 x #step2 x #sample\n\t\tweight_matrix_1 = weight_matrix_1 * x1_mask[:, None, :]\n\t\tweight_matrix_2 = weight_matrix_2 * x2_mask[None, :, :]\n\n\t\talpha = weight_matrix_1 / weight_matrix_1.sum(0, keepdim=True)\n\t\tbeta = weight_matrix_2 / weight_matrix_2.sum(1, keepdim=True)\n\n\t\tself.alpha=alpha\n\t\tself.beta=beta\n\n\t\tctx2_ = (torch.unsqueeze(ctx1,1) * torch.unsqueeze(alpha,3)).sum(0)\n\t\tctx1_ = (torch.unsqueeze(ctx2, 0) * torch.unsqueeze(beta,3)).sum(1)\n\n\t\t# cosine distance and Euclidean distance\n\t\t\'\'\'\n\t\ttmp_result=[]\n\t\tfor batch_i in range(ctx1.size(1)):\n\t\t\ttmp_result.append(torch.unsqueeze(self.cosine_interaction(ctx1[:,batch_i,:], ctx2[:,batch_i,:]), 0))\n\t\tweight_matrix=torch.cat(tmp_result)\n\t\tweight_matrix_1 = torch.exp(weight_matrix - weight_matrix.max(1, keepdim=True)[0]).permute(1, 2, 0)\n\t\tweight_matrix_2 = torch.exp(weight_matrix - weight_matrix.max(2, keepdim=True)[0]).permute(1, 2, 0)\n\n\t\t# weight_matrix_1: #step1 x #step2 x #sample\n\t\tweight_matrix_1 = weight_matrix_1 * x1_mask[:, None, :]\n\t\tweight_matrix_2 = weight_matrix_2 * x2_mask[None, :, :]\n\n\t\talpha = weight_matrix_1 / weight_matrix_1.sum(0, keepdim=True)\n\t\tbeta = weight_matrix_2 / weight_matrix_2.sum(1, keepdim=True)\n\n\t\tctx2_cos_ = (torch.unsqueeze(ctx1, 1) * torch.unsqueeze(alpha, 3)).sum(0)\n\t\tctx1_cos_ = (torch.unsqueeze(ctx2, 0) * torch.unsqueeze(beta, 3)).sum(1)\n\t\t\'\'\'\n\n\t\tinp1 = torch.cat([ctx1, ctx1_, ctx1 * ctx1_, ctx1 - ctx1_], 2)\n\t\tinp2 = torch.cat([ctx2, ctx2_, ctx2 * ctx2_, ctx2 - ctx2_], 2)\n\t\t#inp1 = torch.cat([ctx1, ctx1_, ctx1_cos_, ctx1 * ctx1_, ctx1 * ctx1_cos_, ctx1 - ctx1_, ctx1 - ctx1_cos_], 2)\n\t\t#inp2 = torch.cat([ctx2, ctx2_, ctx2_cos_, ctx2 * ctx2_, ctx2 * ctx2_cos_, ctx2 - ctx2_, ctx2 - ctx2_cos_], 2)\n\t\tinp1=self.dropout(self.linear_layer_compare(inp1))\n\t\tinp2=self.dropout(self.linear_layer_compare(inp2))\n\t\tinp1_r=torch.index_select(inp1, 0, idx_1)\n\t\tinp2_r=torch.index_select(inp2, 0, idx_2)\n\n\t\tv1=self.lstm_compare(inp1, x1_mask)\n\t\tv2=self.lstm_compare(inp2, x2_mask)\n\t\tv1_r = self.lstm_compare(inp1_r, x1_mask)\n\t\tv2_r = self.lstm_compare(inp2_r, x2_mask)\n\t\tv1=torch.cat((v1, torch.index_select(v1_r, 0, idx_1)),2)\n\t\tv2=torch.cat((v2, torch.index_select(v2_r, 0, idx_2)),2)\n\t\tlogits = self.aggregate(v1, v2)\n\t\treturn logits\n'"
ESIM/util.py,0,"b'from __future__ import division\nimport json\n#import nltk\nimport sys\nimport torch\nimport math\nimport logging\nimport numpy as np\nfrom os.path import expanduser\nfrom numpy import linalg as LA\n\n#tokenizer = nltk.tokenize.TreebankWordTokenizer()\n\ndef pearson(x,y):\n\tx=np.array(x)\n\ty=np.array(y)\n\tx=x-np.mean(x)\n\ty=y-np.mean(y)\n\treturn x.dot(y)/(LA.norm(x)*LA.norm(y))\n\ndef URL_maxF1_eval(predict_result,test_data_label):\n\ttest_data_label=[item>=1 for item in test_data_label]\n\tcounter = 0\n\ttp = 0.0\n\tfp = 0.0\n\tfn = 0.0\n\ttn = 0.0\n\n\tfor i, t in enumerate(predict_result):\n\n\t\tif t>0.5:\n\t\t\tguess=True\n\t\telse:\n\t\t\tguess=False\n\t\tlabel = test_data_label[i]\n\t\t#print guess, label\n\t\tif guess == True and label == False:\n\t\t\tfp += 1.0\n\t\telif guess == False and label == True:\n\t\t\tfn += 1.0\n\t\telif guess == True and label == True:\n\t\t\ttp += 1.0\n\t\telif guess == False and label == False:\n\t\t\ttn += 1.0\n\t\tif label == guess:\n\t\t\tcounter += 1.0\n\t\t#else:\n\t\t\t#print label+\'--\'*20\n\t\t\t# if guess:\n\t\t\t# print ""GOLD-"" + str(label) + ""\\t"" + ""SYS-"" + str(guess) + ""\\t"" + sent1 + ""\\t"" + sent2\n\n\ttry:\n\t\tP = tp / (tp + fp)\n\t\tR = tp / (tp + fn)\n\t\tF = 2 * P * R / (P + R)\n\texcept:\n\t\tP=0\n\t\tR=0\n\t\tF=0\n\n\t#print ""PRECISION: %s, RECALL: %s, F1: %s"" % (P, R, F)\n\t#print ""ACCURACY: %s"" % (counter/len(predict_result))\n\taccuracy=counter/len(predict_result)\n\n\t#print ""# true pos:"", tp\n\t#print ""# false pos:"", fp\n\t#print ""# false neg:"", fn\n\t#print ""# true neg:"", tn\n\tmaxF1=0\n\tP_maxF1=0\n\tR_maxF1=0\n\tprobs = predict_result\n\tsortedindex = sorted(range(len(probs)), key=probs.__getitem__)\n\tsortedindex.reverse()\n\n\ttruepos=0\n\tfalsepos=0\n\tfor sortedi in sortedindex:\n\t\tif test_data_label[sortedi]==True:\n\t\t\ttruepos+=1\n\t\telif test_data_label[sortedi]==False:\n\t\t\tfalsepos+=1\n\t\tprecision=0\n\t\tif truepos+falsepos>0:\n\t\t\tprecision=truepos/(truepos+falsepos)\n\n\t\trecall=truepos/(tp+fn)\n\t\tf1=0\n\t\tif precision+recall>0:\n\t\t\tf1=2*precision*recall/(precision+recall)\n\t\t\tif f1>maxF1:\n\t\t\t\t#print probs[sortedi]\n\t\t\t\tmaxF1=f1\n\t\t\t\tP_maxF1=precision\n\t\t\t\tR_maxF1=recall\n\tprint ""PRECISION: %s, RECALL: %s, max_F1: %s"" % (P_maxF1, R_maxF1, maxF1)\n\treturn (accuracy, maxF1)\n\ndef tokenize(text):\n\t""""""\n\tTokenize a piece of text using the Treebank tokenizer\n\n\t:return: a list of strings\n\t""""""\n\treturn tokenizer.tokenize(text)\n\t#return text.split()\n\ndef shuffle_arrays(*arrays):\n\t""""""\n\tShuffle all given arrays with the same RNG state.\n\n\tAll shuffling is in-place, i.e., this function returns None.\n\t""""""\n\trng_state = np.random.get_state()\n\tfor array in arrays:\n\t\tnp.random.shuffle(array)\n\t\tnp.random.set_state(rng_state)\n\ndef readSTSdata(dir):\n\t#print(len(dict))\n\t#print(dict[\'bmxs\'])\n\tlsents=[]\n\trsents=[]\n\tlabels=[]\n\tfor line in open(dir+\'a.toks\'):\n\t\tline = line.decode(\'utf-8\')\n\t\tpieces=line.strip().split()\n\t\tlsents.append(pieces)\n\tfor line in open(dir+\'b.toks\'):\n\t\tline = line.decode(\'utf-8\')\n\t\tpieces = line.strip().split()\n\t\trsents.append(pieces)\n\tfor line in open(dir + \'sim.txt\'):\n\t\tsim = float(line.strip())\n\t\tceil = int(math.ceil(sim))\n\t\tfloor = int(math.floor(sim))\n\t\ttmp = [0, 0, 0, 0, 0, 0]\n\t\tif floor != ceil:\n\t\t\ttmp[ceil] = sim - math.floor(sim)\n\t\t\ttmp[floor] = math.ceil(sim) - sim\n\t\telse:\n\t\t\ttmp[floor] = 1\n\t\tlabels.append(tmp)\n\t#data=(lsents,rsents,labels)\n\tif not len(lsents)==len(rsents)==len(labels):\n\t\tprint(\'error!\')\n\t\tsys.exit()\n\tclean_data = []\n\tfor i in range(len(lsents)):\n\t\tclean_data.append((lsents[i], rsents[i], labels[i]))\n\treturn clean_data\n\ndef readQuoradata(dir):\n\tlsents = []\n\trsents = []\n\tlabels = []\n\tfor line in open(dir+\'a.toks\'):\n\t\tline = line.decode(\'utf-8\')\n\t\tpieces=line.strip().split()\n\t\tlsents.append(pieces)\n\tfor line in open(dir+\'b.toks\'):\n\t\tline = line.decode(\'utf-8\')\n\t\tpieces = line.strip().split()\n\t\trsents.append(pieces)\n\tfor line in open(dir + \'sim.txt\'):\n\t\tlabels.append(int(line.strip()))\n\tclean_data = []\n\tfor i in range(len(lsents)):\n\t\tclean_data.append((lsents[i],rsents[i],labels[i]))\n\treturn clean_data\n\ndef readSNLIdata(dir):\n\t#print(len(dict))\n\t#print(dict[\'bmxs\'])\n\tlsents=[]\n\trsents=[]\n\tlabels=[]\n\tfor line in open(dir+\'a.toks\'):\n\t\tpieces=line.strip().split()\n\t\tlsents.append(pieces)\n\tfor line in open(dir+\'b.toks\'):\n\t\tpieces = line.strip().split()\n\t\trsents.append(pieces)\n\tfor line in open(dir + \'sim.txt\'):\n\t\tsim = line.strip()\n\t\tif sim==\'neutral\':\n\t\t\tlabels.append([0,1,0])\n\t\telif sim==\'entailment\':\n\t\t\tlabels.append([1,0,0])\n\t\telif sim==\'contradiction\':\n\t\t\tlabels.append([0,0,1])\n\t\telse:\n\t\t\tlabels.append([0, 0, 0])\n\tclean_data = []\n\tfor i in range(len(lsents)):\n\t\tif labels[i]!=[0,0,0]:\n\t\t\tclean_data.append((lsents[i],rsents[i],labels[i].index(max(labels[i]))))\n\treturn clean_data\n\ndef read_corpus(filename, lowercase):\n\t""""""\n\tRead a JSONL or TSV file with the SNLI corpus\n\n\t:param filename: path to the file\n\t:param lowercase: whether to convert content to lower case\n\t:return: a list of tuples (first_sent, second_sent, label)\n\t""""""\n\tlogging.info(\'Reading data from %s\' % filename)\n\t# we are only interested in the actual sentences + gold label\n\t# the corpus files has a few more things\n\tuseful_data = []\n\n\t# the SNLI corpus has one JSON object per line\n\twith open(filename, \'rb\') as f:\n\n\t\tif filename.endswith(\'.tsv\') or filename.endswith(\'.txt\'):\n\n\t\t\tfor line in f:\n\t\t\t\tline = line.decode(\'utf-8\').strip()\n\t\t\t\tif lowercase:\n\t\t\t\t\tline = line.lower()\n\t\t\t\tif len(line.split(\'\\t\')) == 10:\n\t\t\t\t\t(label, _, _, _, _, sent1, sent2, _, _, _) = line.split(\'\\t\')\n\t\t\t\telif len(line.split(\'\\t\')) == 14:\n\t\t\t\t\t(label, _, _, _, _, sent1, sent2, _, _, _, _, _, _, _) = line.split(\'\\t\')\n\t\t\t\t#sent1, sent2, label = line.split(\'\\t\')\n\t\t\t\tif label not in (\'contradiction\',\'neutral\',\'entailment\'):\n\t\t\t\t\tcontinue\n\t\t\t\ttokens1 = tokenize(sent1)\n\t\t\t\ttokens2 = tokenize(sent2)\n\t\t\t\tuseful_data.append((tokens1, tokens2, (\'contradiction\',\'neutral\',\'entailment\').index(label)))\n\t\telse:\n\t\t\tfor line in f:\n\t\t\t\tline = line.decode(\'utf-8\')\n\t\t\t\tif lowercase:\n\t\t\t\t\tline = line.lower()\n\t\t\t\tdata = json.loads(line)\n\t\t\t\tif data[\'gold_label\'] == \'-\':\n\t\t\t\t\t# ignore items without a gold label\n\t\t\t\t\tcontinue\n\n\t\t\t\tsentence1_parse = data[\'sentence1_parse\']\n\t\t\t\tsentence2_parse = data[\'sentence2_parse\']\n\t\t\t\tlabel = data[\'gold_label\']\n\n\t\t\t\ttree1 = nltk.Tree.fromstring(sentence1_parse)\n\t\t\t\ttree2 = nltk.Tree.fromstring(sentence2_parse)\n\t\t\t\ttokens1 = tree1.leaves()\n\t\t\t\ttokens2 = tree2.leaves()\n\t\t\t\tt = (tokens1, tokens2, (\'neutral\',\'contradiction\',\'entailment\', \'hidden\').index(label))\n\t\t\t\tuseful_data.append(t)\n\n\treturn useful_data\nif __name__ == \'__main__\':\n\twikiqa_map_mrr(None)'"
PWIM/main.py,16,"b'#Author: Wuwei Lan\n\nfrom __future__ import division\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nfrom torchtext.vocab import load_word_vectors\nimport random\nimport time\nfrom datetime import datetime\nfrom datetime import timedelta\nimport pickle\nimport numpy\nfrom model import DeepPairWiseWord\nfrom util import *\nimport argparse\nimport os\n\ndef main(args):\n\t#torch.manual_seed(123)\n\tEMBEDDING_DIM = 200\n\tHIDDEN_DIM = 250\n\tnum_epochs = 20\n\ttask=args.task\n\tgranularity=args.granularity\n\tdict={}\n\tdict_char_ngram={}\n\tword_freq={}\n\tfake_dict={}\n\toov=[]\n\tfeature_maps = [50, 100, 150, 200, 200, 200, 200]\n\tkernels = [1, 2, 3, 4, 5, 6, 7]\n\tcharcnn_embedding_size = 15\n\tmax_word_length = 20\n\tc2w_mode = False\n\tcharacter_ngrams = 3\n\tcharacter_ngrams_2 = None\n\tcharacter_ngrams_overlap = False\n\tglove_mode = None\n\tupdate_inv_mode = None\n\tupdate_oov_mode = None\n\tcombine_mode=None\n\tlm_mode=None\n\tword_mode = (glove_mode, update_inv_mode, update_oov_mode)\n\n\tbasepath= os.path.dirname(os.path.abspath(__file__))\n\n\tif task==\'url\':\n\t\tnum_class = 2\n\t\ttrainset = readURLdata(basepath+\'/data/url/train/\',granularity)\n\t\ttestset = readURLdata(basepath + \'/data/url/test/\', granularity)\n\telif task==\'msrp\':\n\t\tnum_class = 2\n\t\ttrainset = readURLdata(basepath+\'/data/msrp/train/\', granularity)\n\t\ttestset = readURLdata(basepath + \'/data/msrp/test/\', granularity)\n\telif task==\'pit\':\n\t\tnum_class = 2\n\t\ttrainset = readPITdata(basepath+\'/data/pit/train/\', granularity)\n\t\ttestset = readPITdata(basepath+\'/data/pit/test/\', granularity)\n\telse:\n\t\tprint(\'wrong input for the first argument!\')\n\t\tsys.exit()\n\n\tif granularity==\'char\':\n\t\t# charcnn parameters\n\t\tfeature_maps = [50, 100, 150, 200, 200, 200, 200]\n\t\tkernels = [1, 2, 3, 4, 5, 6, 7]\n\t\tcharcnn_embedding_size = 15\n\t\tmax_word_length = 20\n\n\t\t# c2w parameters\n\t\tif args.language_model:\n\t\t\tlm_mode = True\n\t\telse:\n\t\t\tlm_mode = False\n\t\tif args.char_assemble==\'c2w\':\n\t\t\tc2w_mode = True\n\t\telse:\n\t\t\tc2w_mode = False\n\t\tcharacter_ngrams = args.char_ngram\n\t\tcharacter_ngrams_overlap = False\n\n\t\t#tokens = []\n\t\t#for line in open(basepath + \'/data/\' + task + \'/vocab.txt\'):\n\t\t#\ttokens.append(line.strip())\n\t\ttokens=set()\n\t\tlsents, rsents, labels = trainset\n\t\tfor sent in lsents:\n\t\t\tfor word in sent:\n\t\t\t\ttokens.add(word)\n\t\tfor sent in rsents:\n\t\t\tfor word in sent:\n\t\t\t\ttokens.add(word)\n\t\tlsents, rsents, labels = testset\n\t\tfor sent in lsents:\n\t\t\tfor word in sent:\n\t\t\t\ttokens.add(word)\n\t\tfor sent in rsents:\n\t\t\tfor word in sent:\n\t\t\t\ttokens.add(word)\n\t\ttokens=list(tokens)\n\t\torg_tokens = tokens[:]\n\t\ttokens.append(\'<s>\')\n\t\ttokens.append(\'</s>\')\n\t\ttokens.append(\'oov\')\n\t\t# word_freq = pickle.load(open(basepath + \'/data/\' + task + \'/word_freq.p\', ""rb""))\n\t\tword_freq = {}\n\t\tfiles = [\'/train/a.toks\', \'/train/b.toks\', \'/test/a.toks\', \'/test/b.toks\']\n\t\tfor filename in files:\n\t\t\tfor line in open(basepath + \'/data/\' + task + filename):\n\t\t\t\tline = line.strip()\n\t\t\t\tfor word in line.split():\n\t\t\t\t\t# if word not in oov:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tword_freq[word] += 1\n\t\t\t\t\texcept:\n\t\t\t\t\t\tword_freq[word] = 1\n\t\tif c2w_mode:\n\t\t\tEMBEDDING_DIM = 200\n\t\telse:\n\t\t\tEMBEDDING_DIM = 1100\n\t\tif character_ngrams == 1:\n\t\t\t# dict_char_ngram = pickle.load(open(base_path+ \'/char_dict.p\', ""rb""))\n\t\t\tdict_char_ngram = set()\n\t\t\tfor word in tokens:\n\t\t\t\tfor i in range(len(word)):\n\t\t\t\t\tdict_char_ngram.add(word[i])\n\t\t\tngrams_list = list(dict_char_ngram)\n\t\t\tdict_char_ngram = {}\n\t\t\tcount = 0\n\t\t\tfor unit in ngrams_list:\n\t\t\t\tdict_char_ngram[unit] = count\n\t\t\t\tcount += 1\n\t\t\tpickle.dump(dict_char_ngram, open( ""./saved_dir/dict_char_unigram.p"", ""wb"" ) )\n\t\telif character_ngrams == 2 and character_ngrams_overlap:\n\t\t\t# dict_char_ngram = pickle.load(open(base_path+ \'/bigram_dict.p\', ""rb""))\n\t\t\tdict_char_ngram = set()\n\t\t\tfor word in tokens:\n\t\t\t\tif len(word) <= 2:\n\t\t\t\t\tdict_char_ngram.add(word)\n\t\t\t\telse:\n\t\t\t\t\tfor i in range(len(word) - 1):\n\t\t\t\t\t\tdict_char_ngram.add(word[i:i + 2])\n\t\t\tngrams_list = list(dict_char_ngram)\n\t\t\tdict_char_ngram = {}\n\t\t\tcount = 0\n\t\t\tfor unit in ngrams_list:\n\t\t\t\tdict_char_ngram[unit] = count\n\t\t\t\tcount += 1\n\t\telif character_ngrams == 2 and not character_ngrams_overlap:\n\t\t\t# dict_char_ngram = pickle.load(open(base_path+ \'/bigram_dict_no_overlap.p\', ""rb""))\n\t\t\tdict_char_ngram = set()\n\t\t\tfor word in tokens:\n\t\t\t\tif len(word) <= 2:\n\t\t\t\t\tdict_char_ngram.add(word)\n\t\t\t\telse:\n\t\t\t\t\tfor i in range(0, len(word) - 1, 2):\n\t\t\t\t\t\tdict_char_ngram.add(word[i:i + 2])\n\t\t\t\t\tif len(word) % 2 == 1:\n\t\t\t\t\t\tdict_char_ngram.add(word[len(word) - 1])\n\t\t\tngrams_list = list(dict_char_ngram)\n\t\t\tdict_char_ngram = {}\n\t\t\tcount = 0\n\t\t\tfor unit in ngrams_list:\n\t\t\t\tdict_char_ngram[unit] = count\n\t\t\t\tcount += 1\n\t\telif character_ngrams == 3 and character_ngrams_overlap:\n\t\t\t# dict_char_ngram = pickle.load(open(base_path+ \'/trigram_dict.p\', ""rb""))\n\t\t\tdict_char_ngram = set()\n\t\t\tfor word in tokens:\n\t\t\t\tif len(word) <= 3:\n\t\t\t\t\tdict_char_ngram.add(word)\n\t\t\t\telse:\n\t\t\t\t\tfor i in range(len(word) - 2):\n\t\t\t\t\t\tdict_char_ngram.add(word[i:i + 3])\n\t\t\tngrams_list = list(dict_char_ngram)\n\t\t\tdict_char_ngram = {}\n\t\t\tcount = 0\n\t\t\tfor unit in ngrams_list:\n\t\t\t\tdict_char_ngram[unit] = count\n\t\t\t\tcount += 1\n\t\telif character_ngrams == 3 and not character_ngrams_overlap:\n\t\t\t# dict_char_ngram = pickle.load(open(base_path+ \'/trigram_dict_no_overlap.p\', ""rb""))\n\t\t\tdict_char_ngram = set()\n\t\t\tfor word in tokens:\n\t\t\t\tif len(word) <= 3:\n\t\t\t\t\tdict_char_ngram.add(word)\n\t\t\t\telse:\n\t\t\t\t\tfor i in range(0, len(word) - 2, 3):\n\t\t\t\t\t\tdict_char_ngram.add(word[i:i + 3])\n\t\t\t\t\tif len(word) % 3 == 1:\n\t\t\t\t\t\tdict_char_ngram.add(word[len(word) - 1])\n\t\t\t\t\telif len(word) % 3 == 2:\n\t\t\t\t\t\tdict_char_ngram.add(word[len(word) - 2:])\n\t\t\tngrams_list = list(dict_char_ngram)\n\t\t\tdict_char_ngram = {}\n\t\t\tcount = 0\n\t\t\tfor unit in ngrams_list:\n\t\t\t\tdict_char_ngram[unit] = count\n\t\t\t\tcount += 1\n\t\tdict_char_ngram[\' \'] = len(dict_char_ngram)\n\t\tprint(\'current task: \' + task + \', lm mode: \' + str(lm_mode) + \', c2w mode: \' + str(c2w_mode) + \', n = \' + str(\n\t\t\tcharacter_ngrams) + \', overlap = \' + str(character_ngrams_overlap) + \'.\')\n\telif granularity == \'word\':\n\t\ttokens = []\n\t\tcount = 0\n\t\tnum_inv = 0\n\t\tnum_oov = 0\n\t\tif args.pretrained:\n\t\t\tglove_mode = True\n\t\telse:\n\t\t\tglove_mode = False\n\t\tupdate_inv_mode = False\n\t\tupdate_oov_mode = False\n\t\tword_mode = (glove_mode, update_inv_mode, update_oov_mode)\n\t\tif task == \'msrp\':\n\t\t\t#for line in open(basepath + \'/data/\' + task + \'/vocab.txt\'):\n\t\t\t#\ttokens.append(line.strip())\n\t\t\ttokens=set()\n\t\t\tlsents, rsents, labels = trainset\n\t\t\tfor sent in lsents:\n\t\t\t\tfor word in sent:\n\t\t\t\t\ttokens.add(word)\n\t\t\tfor sent in rsents:\n\t\t\t\tfor word in sent:\n\t\t\t\t\ttokens.add(word)\n\t\t\tlsents, rsents, labels = testset\n\t\t\tfor sent in lsents:\n\t\t\t\tfor word in sent:\n\t\t\t\t\ttokens.add(word)\n\t\t\tfor sent in rsents:\n\t\t\t\tfor word in sent:\n\t\t\t\t\ttokens.add(word)\n\t\t\ttokens=list(tokens)\n\t\t\ttokens.append(\'oov\')\n\t\t\tdict = {}\n\t\t\tEMBEDDING_DIM = 300\n\t\t\t# wv_dict, wv_arr, wv_size = load_word_vectors(basepath + \'/VDPWI-NN-Torch/data/glove\', \'glove.twitter.27B\', EMBEDDING_DIM)\n\t\t\twv_dict, wv_arr, wv_size = load_word_vectors(expanduser(""~"")+\'/Documents/research/pytorch/DeepPairWiseWord\' + \'/VDPWI-NN-Torch/data/glove\', \'glove.840B\',\n\t\t\t                                             EMBEDDING_DIM)\n\t\t\t# wv_dict, wv_arr, wv_size = load_word_vectors(basepath + \'/data/paragram/paragram_300_sl999/\', \'paragram\', EMBEDDING_DIM)\n\t\t\t# wv_dict={}\n\t\t\t# wv_arr={}\n\t\t\tfor word in tokens:\n\t\t\t\tfake_dict[word] = torch.Tensor([random.uniform(-0.05, 0.05) for i in range(EMBEDDING_DIM)])\n\t\t\t\ttry:\n\t\t\t\t\tdict[word] = wv_arr[wv_dict[word]]\n\t\t\t\t\tnum_inv += 1\n\t\t\t\texcept:\n\t\t\t\t\tnum_oov += 1\n\t\t\t\t\t# print(word)\n\t\t\t\t\toov.append(word)\n\t\t\t\t\tdict[word] = torch.Tensor([random.uniform(-0.05, 0.05) for i in range(EMBEDDING_DIM)])\n\t\telif task == \'url\' or task == \'pit\':\n\t\t\tfor line in open(basepath + \'/data/\' + task + \'/vocab.txt\'):\n\t\t\t\ttokens.append(line.strip())\n\t\t\t# print(len(tokens))\n\t\t\ttokens.append(\'oov\')\n\t\t\tdict = {}\n\t\t\tEMBEDDING_DIM = 200\n\t\t\twv_dict, wv_arr, wv_size = load_word_vectors(basepath + \'/VDPWI-NN-Torch/data/glove\',\n\t\t\t                                             \'glove.twitter.27B\', EMBEDDING_DIM)\n\t\t\tnum_oov = 0\n\t\t\tnum_inv = 0\n\t\t\tfor word in tokens:\n\t\t\t\tfake_dict[word] = torch.Tensor([random.uniform(-0.05, 0.05) for i in range(EMBEDDING_DIM)])\n\t\t\t\ttry:\n\t\t\t\t\tdict[word] = wv_arr[wv_dict[word]]\n\t\t\t\t\tnum_inv += 1\n\t\t\t\texcept:\n\t\t\t\t\tnum_oov += 1\n\t\t\t\t\toov.append(word)\n\t\t\t\t\tdict[word] = torch.Tensor([random.uniform(-0.05, 0.05) for i in range(EMBEDDING_DIM)])\n\t\t\tdict_char_ngram = pickle.load(open(basepath + \'/data/\' + task + \'/char_dict.p\', ""rb""))\n\t\t\tword_freq = pickle.load(open(basepath + \'/data/\' + task + \'/word_freq.p\', ""rb""))\n\t\tprint(\'finished loading word vector, there are \' + str(num_inv) + \' INV words and \' + str(\n\t\t\tnum_oov) + \' OOV words.\')\n\t\tprint(\'current task: \' + task + \', glove mode = \' + str(glove_mode) + \', update_inv_mode = \' + str(\n\t\t\tupdate_inv_mode) + \', update_oov_mode = \' + str(update_oov_mode))\n\t\tsaved_file = \'current task: \' + task + \', glove mode = \' + str(glove_mode) + \', update_inv_mode = \' + str(\n\t\t\tupdate_inv_mode) + \', update_oov_mode = \' + str(update_oov_mode) + \'.txt\'\n\telse:\n\t\tprint(\'wrong input for the second argument!\')\n\t\tsys.exit()\n\n\tmodel=DeepPairWiseWord(EMBEDDING_DIM,HIDDEN_DIM,1,task,granularity,num_class,dict,fake_dict, dict_char_ngram, oov,tokens, word_freq,\n\t                       feature_maps,kernels,charcnn_embedding_size,max_word_length,character_ngrams,c2w_mode,character_ngrams_overlap, word_mode,\n\t                       combine_mode, lm_mode, args.deep_CNN)#, corpus)\n\tif torch.cuda.is_available():\n\t\tmodel=model.cuda()\n\tlsents, rsents, labels = trainset\n\tcriterion = nn.MultiMarginLoss(p=1, margin=1.0, weight=None, size_average=True)\n\tif torch.cuda.is_available():\n\t\tcriterion = criterion.cuda()\n\toptimizer = torch.optim.RMSprop(model.parameters(), lr=0.0001)#, momentum=0.1, weight_decay=0.05)#,momentum=0.9,weight_decay=0.95)\n\t#optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n\t# Train the Model\n\t#print(oov)\n\tprint(\'start training\')\n\tmax_result=-1\n\tbatch_size=32\n\treport_interval=50000\n\tfor epoch in range(num_epochs):\n\t\tprint(\'--\'*20)\n\t\tmodel.train()\n\t\toptimizer.zero_grad()\n\t\tstart_time = time.time()\n\t\tdata_loss = 0\n\t\tindices = torch.randperm(len(lsents))\n\t\ttrain_correct=0\n\t\t#print(len(indices))\n\t\tfor index, i in enumerate(indices):\n\t\t\t#print(index)\n\t\t\t#start_time = time.time()\n\t\t\tsentA = lsents[i]\n\t\t\tsentB = rsents[i]\n\t\t\tif task==\'sick\' or task==\'sts\' or task==\'snli\' or task==\'wiki\':\n\t\t\t\tlabel=Variable(torch.Tensor(labels[i]))\n\t\t\telse:\n\t\t\t\tlabel = Variable(torch.LongTensor(labels[i]))#.cuda()\n\t\t\tif torch.cuda.is_available():\n\t\t\t\tlabel=label.cuda()\n\t\t\toutput,extra_loss = model(sentA, sentB, index)\n\t\t\t#tmp_output = np.exp(output.data[0].cpu().numpy())\n\t\t\t#print index, \'gold: \', labels[i][0], \'predict: \', np.argmax(tmp_output)\n\t\t\t#print(extra_loss)\n\t\t\tloss = criterion(output, label)+extra_loss\n\t\t\tloss.backward()\n\t\t\tdata_loss += loss.data[0]\n\t\t\toutput = np.exp(output.data[0].cpu().numpy())\n\t\t\tif labels[i][0] == np.argmax(output):\n\t\t\t\ttrain_correct += 1\n\t\t\t#print(loss-extra_loss)\n\t\t\t#print(\'*\'*20)\n\t\t\tif (index+1) % batch_size == 0:\n\t\t\t\toptimizer.step()\n\t\t\t\toptimizer.zero_grad()\n\n\t\t\tif (index+1) % report_interval == 0:\n\t\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, index+1)\n\t\t\t\tmsg += \'\\t train batch loss: %f\' % (data_loss / (index+1))\n\t\t\t\ttrain_acc=train_correct/(index+1)\n\t\t\t\tprint(msg)\n\n\t\t\tif (index + 1) % (int(len(lsents)/2)) == 0:\n\t\t\t\tmodel.eval()\n\t\t\t\t# test on URL dataset\n\t\t\t\t#print(\'testing on URL dataset:\')\n\t\t\t\t#testset = readURLdata(basepath + \'/data/url/test_9324/\', granularity)\n\t\t\t\ttest_lsents, test_rsents, test_labels = testset\n\t\t\t\tpredicted = []\n\t\t\t\tgold = []\n\t\t\t\tcorrect = 0\n\t\t\t\tfor test_i in range(len(test_lsents)):\n\t\t\t\t\tsentA = test_lsents[test_i]\n\t\t\t\t\tsentB = test_rsents[test_i]\n\t\t\t\t\toutput, _ = model(sentA, sentB, index)\n\t\t\t\t\toutput = np.exp(output.data[0].cpu().numpy())\n\t\t\t\t\tif test_labels[test_i][0] == np.argmax(output):\n\t\t\t\t\t\tcorrect += 1\n\t\t\t\t\tpredicted.append(output[1])\n\t\t\t\t\tgold.append(test_labels[test_i][0])\n\t\t\t\t_,result=URL_maxF1_eval(predict_result=predicted, test_data_label=gold)\n\t\t\t\tif result > max_result:\n\t\t\t\t\tmax_result = result\n\t\t\t\t\ttorch.save(model,\'./saved_dir/char_CNN_unigram.pkl\')\n\t\t\t\telapsed_time = time.time() - start_time\n\t\t\t\tprint(\'Epoch \' + str(epoch + 1) + \' finished within \' + str(timedelta(seconds=elapsed_time))+\', and current time:\'+ str(datetime.now()))\n\t\t\t\tprint(\'Best result until now: %.6f\' % max_result)\n\t\t\t\tmodel.train()\n\n\nif __name__ == \'__main__\':\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\'--task\', type=str, default=\'msrp\',\n\t\t\t\t\t\thelp=\'Currently supported tasks: pit, url, msrp\')\n\tparser.add_argument(\'--granularity\', type=str, default=\'word\',\n\t\t\t\t\t\thelp=\'Currently supported granularities: char and word.\')\n\tparser.add_argument(\'--pretrained\', type=bool, default=True,\n\t                    help=\'Use pretrained word embedding or not\')\n\tparser.add_argument(\'--char_ngram\', type=int, default=1,\n\t                    help=\'unigram (1), bigram (2) or trigram (3)\')\n\tparser.add_argument(\'--char_assemble\', type=str, default=\'cnn\',\n\t                    help=\'Assemble char embedding into word embedding: c2w or cnn\')\n\tparser.add_argument(\'--language_model\', type=bool, default=False,\n\t                    help=\'Use multi task language model or not\')\n\tparser.add_argument(\'--deep_CNN\', type=bool, default=True,\n\t                    help=\'use 19 layer CNN or not\')\n\targs = parser.parse_args()\n\tprint(args)\n\tmain(args)\n'"
PWIM/main_sts.py,66,"b'#Author: Wuwei Lan\n#This model is for this paper: Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement, by Hua He and Jimmy Lin\n\nfrom __future__ import division\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nfrom torchtext.vocab import load_word_vectors\nimport random\nimport time\nfrom datetime import datetime\nfrom datetime import timedelta\nimport pickle\nimport numpy\nfrom model import DeepPairWiseWord\nfrom util import *\nimport argparse\nfrom os.path import expanduser\nfrom gensim.models.keyedvectors import KeyedVectors\nimport cPickle\n\ndef main(args):\n\t#torch.manual_seed(123)\n\tEMBEDDING_DIM = 200\n\tHIDDEN_DIM = 250\n\tnum_epochs = 20\n\ttask=args.task\n\tgranularity=args.granularity\n\tdict={}\n\tdict_char_ngram={}\n\tword_freq={}\n\tfake_dict={}\n\toov=[]\n\tfeature_maps = [50, 100, 150, 200, 200, 200, 200]\n\tkernels = [1, 2, 3, 4, 5, 6, 7]\n\tcharcnn_embedding_size = 15\n\tmax_word_length = 20\n\tc2w_mode = False\n\tcharacter_ngrams = 3\n\tcharacter_ngrams_2 = None\n\tcharacter_ngrams_overlap = False\n\tglove_mode = None\n\tupdate_inv_mode = None\n\tupdate_oov_mode = None\n\tcombine_mode=None\n\tlm_mode=None\n\tword_mode = (glove_mode, update_inv_mode, update_oov_mode)\n\n\tif torch.cuda.is_available():\n\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord\'\n\telse:\n\t\tbasepath=expanduser(""~"")+\'/Documents/research/pytorch/DeepPairWiseWord\'\n\n\tif task==\'url\':\n\t\tnum_class = 2\n\t\ttrainset = readURLdata(basepath+\'/data/url/train/\',granularity)\n\t\ttestset = readURLdata(basepath + \'/data/url/test_9324/\', granularity)\n\telif task==\'quora\':\n\t\tnum_class = 2\n\t\ttrainset = readURLdata(basepath+\'/data/quora/train/\', granularity)\n\t\ttestset = readURLdata(basepath + \'/data/quora/test/\', granularity)\n\telif task==\'msrp\':\n\t\tnum_class = 2\n\t\ttrainset = readURLdata(basepath+\'/data/msrp/train/\', granularity)\n\t\ttestset = readURLdata(basepath + \'/data/msrp/test/\', granularity)\n\telif task==\'sick\':\n\t\tnum_class = 5\n\t\ttrainset = readSICKdata(basepath+\'/data/sick/train/\',granularity)\n\t\tdevset = readSICKdata(basepath+\'/data/sick/dev/\',granularity)\n\t\ttestset = readSICKdata(basepath+\'/data/sick/test/\',granularity)\n\telif task==\'pit\':\n\t\tnum_class = 2\n\t\ttrainset = readPITdata(basepath+\'/data/pit/train/\', granularity)\n\t\t#devset = readPITdata(basepath+\'/data/pit/dev/\',granularity)\n\t\ttestset = readPITdata(basepath+\'/data/pit/test/\', granularity)\n\telif task==\'hindi\':\n\t\tnum_class=2\n\t\ttrainset = read_Hindi_data(basepath+\'/data/hindi/train/\', granularity)\n\t\ttestset = read_Hindi_data(basepath + \'/data/hindi/test/\', granularity)\n\telif task==\'sts\':\n\t\tnum_class = 6\n\t\ttrainset = readSTSdata(basepath+\'/data/sts/train/\',granularity)\n\t\ttestset = readSTSdata(basepath+\'/data/sts/test/\',granularity)\n\telif task==\'snli\':\n\t\tnum_class = 3\n\t\ttrainset = readSNLIdata(basepath+\'/data/snli/train/\',granularity)\n\t\ttestset = readSNLIdata(basepath+\'/data/snli/test/\', granularity)\n\telif task==\'mnli\':\n\t\tnum_class = 3\n\t\ttrainset = readMNLIdata(basepath+\'/data/mnli/train/\',granularity)\n\t\tdevset_m = readMNLIdata(basepath+\'/data/mnli/dev_m/\', granularity)\n\t\tdevset_um = readMNLIdata(basepath + \'/data/mnli/dev_um/\', granularity)\n\t\ttestset_m = readMNLIdata(basepath + \'/data/mnli/test_m/\', granularity)\n\t\ttestset_um = readMNLIdata(basepath + \'/data/mnli/test_um/\', granularity)\n\telif task==\'wiki\':\n\t\t\'\'\'\n\t\t_name_to_id = {\n        \'counter-vandalism\': 0,\n        \'fact-update\': 1,\n        \'refactoring\': 2,\n        \'copy-editing\': 3,\n        \'other\': 4,\n        \'wikification\': 5,\n        \'vandalism\': 6,\n        \'simplification\': 7,\n        \'elaboration\': 8,\n        \'verifiability\': 9,\n        \'process\': 10,\n        \'clarification\': 11,\n        \'disambiguation\': 12,\n        \'point-of-view\': 13\n    }\n\t\t\'\'\'\n\t\tnum_class = 14\n\t\tdata = pickle.load(open(basepath+""/data/wiki/data.cpickle"", ""rb""))\n\t\tleft=[]\n\t\tright=[]\n\t\tlabel=[]\n\t\tid=[]\n\t\tfor i in range(2976):\n\t\t\tid.append(data[i][0])\n\t\t\tlabel.append([int(item) for item in data[i][3][0]])\n\t\t\tleft_sent=[item.encode(\'utf-8\') for item in data[i][1][0]]\n\t\t\tright_sent=[item.encode(\'utf-8\') for item in data[i][2][0]]\n\t\t\tshared=[]\n\t\t\tfor item in left_sent:\n\t\t\t\tif item in right_sent:\n\t\t\t\t\tshared.append(item)\n\t\t\tfor item in shared:\n\t\t\t\tif item in left_sent and item in right_sent:\n\t\t\t\t\tleft_sent.remove(item)\n\t\t\t\t\tright_sent.remove(item)\n\t\t\tif len(left_sent) == 0:\n\t\t\t\tleft_sent=[\'<EMPTY-EDIT>\']\n\t\t\tif len(right_sent) == 0:\n\t\t\t\tright_sent=[\'<EMPTY-EDIT>\']\n\t\t\tleft.append(left_sent)\n\t\t\tright.append(right_sent)\n\t\t\t#print(left_sent)\n\t\t\t#print(right_sent)\n\t\t\t#print(id[0])\n\t\t\t#print(\'*\'*20)\n\t\ttrainset=(left, right, label)\n\t\t#sys.exit()\n\t\tleft = []\n\t\tright = []\n\t\tlabel = []\n\t\tfor i in range(2376, 2976):\n\t\t\tid.append(data[i][0])\n\t\t\tlabel.append([int(item) for item in data[i][3][0]])\n\t\t\tleft_sent = [item.encode(\'utf-8\') for item in data[i][1][0]]\n\t\t\tright_sent = [item.encode(\'utf-8\') for item in data[i][2][0]]\n\t\t\tshared = []\n\t\t\tfor item in left_sent:\n\t\t\t\tif item in right_sent:\n\t\t\t\t\tshared.append(item)\n\t\t\tfor item in shared:\n\t\t\t\tif item in left_sent and item in right_sent:\n\t\t\t\t\tleft_sent.remove(item)\n\t\t\t\t\tright_sent.remove(item)\n\t\t\tif len(left_sent) == 0:\n\t\t\t\tleft_sent = [\'<EMPTY-EDIT>\']\n\t\t\tif len(right_sent) == 0:\n\t\t\t\tright_sent = [\'<EMPTY-EDIT>\']\n\t\t\tleft.append(left_sent)\n\t\t\tright.append(right_sent)\n\t\ttestset = (left, right, label)\n\telif task==\'wikiqa\':\n\t\tnum_class=2\n\t\ttrainset=readURLdata(basepath+\'/data/wikiqa/train/\',granularity)\n\t\ttestset=readURLdata(basepath+\'/data/wikiqa/test/\',granularity)\n\telif task==\'trecqa\':\n\t\tnum_class=2\n\t\ttrainset=readURLdata(basepath+\'/data/trecqa/train-all/\',granularity)\n\t\ttestset=readURLdata(basepath+\'/data/trecqa/raw-test/\',granularity)\n\telse:\n\t\tprint(\'wrong input for the first argument!\')\n\t\tsys.exit()\n\n\tif granularity==\'word\':\n\t\ttokens = []\n\t\tcount=0\n\t\tnum_inv=0\n\t\tnum_oov=0\n\t\tglove_mode = True\n\t\tupdate_inv_mode = True\n\t\tupdate_oov_mode = True\n\t\tword_mode=(glove_mode,update_inv_mode,update_oov_mode)\n\t\tif task == \'sick\' or task==\'quora\' or task==\'msrp\':\n\t\t\tfor line in open(basepath+\'/data/\'+task+\'/vocab.txt\'):\n\t\t\t\ttokens.append(line.strip())\n\t\t\tdict = {}\n\t\t\tEMBEDDING_DIM = 300\n\t\t\t#wv_dict, wv_arr, wv_size = load_word_vectors(basepath + \'/VDPWI-NN-Torch/data/glove\', \'glove.twitter.27B\', EMBEDDING_DIM)\n\t\t\twv_dict, wv_arr, wv_size = load_word_vectors(basepath+\'/VDPWI-NN-Torch/data/glove\', \'glove.840B\', EMBEDDING_DIM)\n\t\t\t#wv_dict, wv_arr, wv_size = load_word_vectors(basepath + \'/data/paragram/paragram_300_sl999/\', \'paragram\', EMBEDDING_DIM)\n\t\t\t#wv_dict={}\n\t\t\t#wv_arr={}\n\t\t\tfor word in tokens:\n\t\t\t\tfake_dict[word] = torch.Tensor([random.uniform(-0.05, 0.05) for i in range(EMBEDDING_DIM)])\n\t\t\t\ttry:\n\t\t\t\t\tdict[word] = wv_arr[wv_dict[word]]\n\t\t\t\t\tnum_inv += 1\n\t\t\t\texcept:\n\t\t\t\t\tnum_oov += 1\n\t\t\t\t\t#print(word)\n\t\t\t\t\toov.append(word)\n\t\t\t\t\tdict[word] = torch.Tensor([random.uniform(-0.05, 0.05) for i in range(EMBEDDING_DIM)])\n\t\telif task==\'sts\':\n\t\t\tfor line in open(basepath+\'/data/\'+task+\'/vocab.txt\'):\n\t\t\t\ttokens.append(line.strip())\n\t\t\tdict = {}\n\t\t\t#EMBEDDING_DIM = 200\n\t\t\t#wv_dict, wv_arr, wv_size = load_word_vectors(basepath + \'/VDPWI-NN-Torch/data/glove\', \'glove.twitter.27B\', EMBEDDING_DIM)\n\t\t\t#EMBEDDING_DIM = 300\n\t\t\t#wv_dict, wv_arr, wv_size = load_word_vectors(basepath + \'/VDPWI-NN-Torch/data/glove\', \'glove.840B\', EMBEDDING_DIM)\n\t\t\tEMBEDDING_DIM = 300\n\t\t\twv_dict, wv_arr, wv_size = load_word_vectors(basepath+\'/data/paragram/paragram_300_sl999/\', \'paragram\', EMBEDDING_DIM)\n\t\t\t#wv_dict={}\n\t\t\t#wv_arr={}\n\t\t\t#oov = []\n\t\t\t#for line in open(basepath + \'/data/\' + task + \'/oov.txt\'):\n\t\t\t#\tline = line.strip()\n\t\t\t#\toov.append(line)\n\t\t\t#inv = []\n\t\t\t#for line in open(basepath + \'/data/\' + task + \'/inv_14000.txt\'):\n\t\t\t#\tline = line.strip()\n\t\t\t#\tinv.append(line)\n\t\t\t# count=len(oov)+len(inv)\n\t\t\t#inv = tokens\n\t\t\tnum_oov = 0\n\t\t\tnum_inv = 0\n\t\t\tfor word in tokens:\n\t\t\t\tfake_dict[word] = torch.Tensor([random.uniform(-0.05, 0.05) for i in range(EMBEDDING_DIM)])\n\t\t\t\ttry:\n\t\t\t\t\tdict[word] = wv_arr[wv_dict[word]]\n\t\t\t\t\tnum_inv += 1\n\t\t\t\texcept:\n\t\t\t\t\tnum_oov += 1\n\t\t\t\t\toov.append(word)\n\t\t\t\t\tdict[word] = torch.Tensor([random.uniform(-0.05, 0.05) for i in range(EMBEDDING_DIM)])\n\t\t\tdict_char_ngram = pickle.load(open(basepath + \'/data/\' + task + \'/char_dict.p\', ""rb""))\n\t\t\tword_freq = pickle.load(open(basepath + \'/data/\' + task + \'/word_freq.p\', ""rb""))\n\t\telif task == \'snli\' or task==\'wikiqa\' or task==\'trecqa\' or task==\'mnli\':\n\t\t\tfor line in open(basepath+\'/data/\'+task+\'/vocab.txt\'):\n\t\t\t\ttokens.append(line.strip())\n\t\t\tdict = {}\n\t\t\t#EMBEDDING_DIM = 200\n\t\t\t#wv_dict, wv_arr, wv_size = load_word_vectors(basepath + \'/VDPWI-NN-Torch/data/glove\', \'glove.twitter.27B\', EMBEDDING_DIM)\n\t\t\tEMBEDDING_DIM = 300\n\t\t\twv_dict, wv_arr, wv_size = load_word_vectors(basepath + \'/VDPWI-NN-Torch/data/glove\', \'glove.840B\', EMBEDDING_DIM)\n\t\t\t#EMBEDDING_DIM = 300\n\t\t\t#wv_dict, wv_arr, wv_size = load_word_vectors(basepath+\'/data/paragram/paragram_300_sl999/\', \'paragram\', EMBEDDING_DIM)\n\t\t\tnum_oov = 0\n\t\t\tnum_inv = 0\n\t\t\tfor word in tokens:\n\t\t\t\tfake_dict[word] = torch.Tensor([random.uniform(-0.05, 0.05) for i in range(EMBEDDING_DIM)])\n\t\t\t\ttry:\n\t\t\t\t\tdict[word] = wv_arr[wv_dict[word]]\n\t\t\t\t\tnum_inv += 1\n\t\t\t\texcept:\n\t\t\t\t\tnum_oov += 1\n\t\t\t\t\toov.append(word)\n\t\t\t\t\tdict[word] = torch.Tensor([random.uniform(-0.05, 0.05) for i in range(EMBEDDING_DIM)])\n\t\t\t#dict_char_ngram = pickle.load(open(basepath + \'/data/\' + task + \'/char_dict.p\', ""rb""))\n\t\t\t#word_freq = pickle.load(open(basepath + \'/data/\' + task + \'/word_freq.p\', ""rb""))\n\t\t\tdict_char_ngram = {}\n\t\t\tword_freq= {}\n\t\telif task == \'hindi\':\n\t\t\t#words, embeddings = pickle.load(open(basepath+\'/data/hindi/polyglot-hi.pkl\', \'rb\'))\n\t\t\t#print(""Emebddings shape is {}"".format(embeddings.shape))\n\t\t\t#print words[777], embeddings[777]\n\t\t\tembeddings_file_bin = basepath+\'/data/hindi/hi/hi.bin\'\n\t\t\tmodel_bin = KeyedVectors.load(embeddings_file_bin)\n\t\t\t#print(words[777], model_bin[words[777]])\n\t\t\t#sys.exit()\n\t\t\tfor line in open(basepath+\'/data/\'+task+\'/vocab.txt\'):\n\t\t\t\ttokens.append(line.strip().decode(\'utf-8\'))\n\t\t\tdict = {}\n\t\t\tEMBEDDING_DIM = 300\n\t\t\tfor word in tokens:\n\t\t\t\tfake_dict[word] = torch.Tensor([random.uniform(-0.05, 0.05) for i in range(EMBEDDING_DIM)])\n\t\t\t\ttry:\n\t\t\t\t\tdict[word] = model_bin[word]\n\t\t\t\t\tnum_inv += 1\n\t\t\t\texcept:\n\t\t\t\t\tnum_oov += 1\n\t\t\t\t\toov.append(word)\n\t\t\t\t\tdict[word] = torch.Tensor([random.uniform(-0.05, 0.05) for i in range(EMBEDDING_DIM)])\n\t\telif task == \'url\' or task==\'pit\':\n\t\t\tfor line in open(basepath+\'/data/\'+task+\'/vocab.txt\'):\n\t\t\t\ttokens.append(line.strip())\n\t\t\t# print(len(tokens))\n\t\t\tdict = {}\n\t\t\tEMBEDDING_DIM = 200\n\t\t\twv_dict, wv_arr, wv_size = load_word_vectors(basepath+\'/VDPWI-NN-Torch/data/glove\', \'glove.twitter.27B\', EMBEDDING_DIM)\n\t\t\t#EMBEDDING_DIM = 300\n\t\t\t#wv_dict, wv_arr, wv_size = load_word_vectors(basepath + \'/data/paragram/paragram_300_sl999/\', \'paragram\', EMBEDDING_DIM)\n\t\t\t#wv_dict={}\n\t\t\t#wv_arr={}\n\t\t\t# print(len(wv_dict))\n\t\t\t#oov = []\n\t\t\t#for line in open(basepath+\'/data/\'+task+\'/oov.txt\'):\n\t\t\t#\tline = line.strip()\n\t\t\t#\toov.append(line)\n\t\t\t#inv=[]\n\t\t\t#for line in open(basepath+\'/data/\'+task+\'/inv_4000.txt\'):\n\t\t\t#\tline = line.strip()\n\t\t\t#\tinv.append(line)\n\t\t\t#count=len(oov)+len(inv)\n\t\t\t#inv = tokens\n\t\t\tnum_oov=0\n\t\t\tnum_inv=0\n\t\t\tfor word in tokens:\n\t\t\t\tfake_dict[word] = torch.Tensor([random.uniform(-0.05, 0.05) for i in range(EMBEDDING_DIM)])\n\t\t\t\ttry:\n\t\t\t\t\tdict[word] = wv_arr[wv_dict[word]]\n\t\t\t\t\tnum_inv += 1\n\t\t\t\texcept:\n\t\t\t\t\tnum_oov += 1\n\t\t\t\t\toov.append(word)\n\t\t\t\t\tdict[word] = torch.Tensor([random.uniform(-0.05, 0.05) for i in range(EMBEDDING_DIM)])\n\t\t\tdict_char_ngram = pickle.load(open(basepath + \'/data/\' + task + \'/char_dict.p\', ""rb""))\n\t\t\tword_freq = pickle.load(open(basepath + \'/data/\' + task + \'/word_freq.p\', ""rb""))\n\t\tprint(\'finished loading word vector, there are \'+str(num_inv)+\' INV words and \'+str(num_oov)+\' OOV words.\')\n\t\tprint(\'current task: \'+task+\', glove mode = \' + str(glove_mode) + \', update_inv_mode = \' + str(update_inv_mode) + \', update_oov_mode = \' + str(update_oov_mode))\n\t\tsaved_file=\'current task: \'+task+\', glove mode = \' + str(glove_mode) + \', update_inv_mode = \' + str(update_inv_mode) + \', update_oov_mode = \' + str(update_oov_mode)+\'.txt\'\n\t#subprocess.call([\'echo\',\'finished loading word vector, there are \',str(num_inv),\' INV words and \',str(len(oov)),\' OOV words.\'])\n\telif granularity==\'char\':\n\t\t# charcnn parameters\n\t\tfeature_maps=[50,100,150,200,200,200,200]\n\t\tkernels=[1,2,3,4,5,6,7]\n\t\tcharcnn_embedding_size=15\n\t\tmax_word_length=20\n\n\t\t#c2w parameters\n\t\tlm_mode = False\n\t\tc2w_mode = False\n\t\tcharacter_ngrams=1\n\t\tcharacter_ngrams_overlap=True\n\n\t\ttokens=[]\n\t\tif task!=\'wiki\':\n\t\t\tif task==\'hindi\':\n\t\t\t\tfor line in open(basepath+\'/data/\' + task +\'/vocab.txt\'):\n\t\t\t\t\ttokens.append(line.strip().decode(\'utf-8\'))\n\t\t\t\ttokens.append(\'<s>\'.decode())\n\t\t\t\ttokens.append(\'</s>\'.decode())\n\t\t\t\ttokens.append(\'oov\'.decode())\n\t\t\telse:\n\t\t\t\tfor line in open(basepath+\'/data/\' + task + \'/vocab.txt\'):\n\t\t\t\t\ttokens.append(line.strip())\n\t\t\t\torg_tokens=tokens[:]\n\t\t\t\ttokens.append(\'<s>\')\n\t\t\t\ttokens.append(\'</s>\')\n\t\t\t\ttokens.append(\'oov\')\n\t\t\tword_freq = pickle.load(open(basepath + \'/data/\' + task + \'/word_freq.p\', ""rb""))\n\t\tif c2w_mode:\n\t\t\tEMBEDDING_DIM = 200\n\t\telse:\n\t\t\tEMBEDDING_DIM = 1100\n\t\tif character_ngrams==1:\n\t\t\tdict_char_ngram = pickle.load(open(basepath+\'/data/\' + task + \'/char_dict.p\', ""rb""))\n\t\telif character_ngrams==2 and character_ngrams_overlap:\n\t\t\tdict_char_ngram = pickle.load(open(basepath + \'/data/\' + task + \'/bigram_dict.p\', ""rb""))\n\t\telif character_ngrams==2 and not character_ngrams_overlap:\n\t\t\tdict_char_ngram = pickle.load(open(basepath + \'/data/\' + task + \'/bigram_dict_no_overlap.p\', ""rb""))\n\t\telif character_ngrams==3 and character_ngrams_overlap:\n\t\t\tdict_char_ngram = pickle.load(open(basepath + \'/data/\' + task + \'/trigram_dict.p\', ""rb""))\n\t\telif character_ngrams==3 and not character_ngrams_overlap:\n\t\t\tdict_char_ngram = pickle.load(open(basepath + \'/data/\' + task + \'/trigram_dict_no_overlap.p\', ""rb""))\n\t\tprint(\'current task: \'+task+\', lm mode: \'+str(lm_mode)+\', c2w mode: \'+str(c2w_mode)+\', n = \'+str(character_ngrams)+\', overlap = \'+str(character_ngrams_overlap)+\'.\')\n\t\tsaved_file=\'current task: \'+task+\', lm mode: \'+str(lm_mode)+\', c2w mode: \'+str(c2w_mode)+\', n = \'+str(character_ngrams)+\', overlap = \'+str(character_ngrams_overlap)+\'.txt\'\n\telif granularity==\'mix\':\n\t\ttokens = []\n\t\tnum_oov=0\n\t\tnum_inv=0\n\t\tfor line in open(basepath+\'/data/\'+task+\'/vocab.txt\'):\n\t\t\ttokens.append(line.strip())\n\t\ttokens.append(\'<s>\')\n\t\ttokens.append(\'</s>\')\n\t\ttokens.append(\'oov\')\n\t\t# print(len(tokens))\n\t\tdict = {}\n\t\t#oov=[]\n\t\tif task==\'sts\':\n\t\t\tEMBEDDING_DIM = 300\n\t\t\twv_dict, wv_arr, wv_size = load_word_vectors(basepath + \'/data/paragram/paragram_300_sl999/\', \'paragram\', EMBEDDING_DIM)\n\t\t\t#wv_dict, wv_arr, wv_size = load_word_vectors(basepath + \'/VDPWI-NN-Torch/data/glove\', \'glove.840B\', EMBEDDING_DIM)\n\t\telse:\n\t\t\tEMBEDDING_DIM = 200\n\t\t\twv_dict, wv_arr, wv_size = load_word_vectors(basepath+\'/VDPWI-NN-Torch/data/glove\', \'glove.twitter.27B\', EMBEDDING_DIM)\n\t\t\'\'\'\n\t\tEMBEDDING_DIM = 300\n\t\twv_dict, wv_arr, wv_size = load_word_vectors(basepath + \'/data/paragram/paragram_300_sl999/\', \'paragram\', EMBEDDING_DIM)\n\t\t\'\'\'\n\t\toov=[]\n\t\tfor word in tokens:\n\t\t\t\'\'\'\n\t\t\tif word in oov or word in inv:\n\t\t\t\tcount+=1\n\t\t\t\tdict[word] = torch.Tensor([0 for i in range(EMBEDDING_DIM)])\n\t\t\telse:\n\t\t\t\tdict[word] = wv_arr[wv_dict[word]]\n\t\t\t\tnum_inv+=1\n\t\t\t\'\'\'\n\t\t\ttry:\n\t\t\t\tdict[word] = wv_arr[wv_dict[word]]\n\t\t\t\tnum_inv+=1\n\t\t\texcept:\n\t\t\t\tnum_oov+=1\n\t\t\t\toov.append(word)\n\t\t\t\t# print(word)\n\t\t\t\tdict[word] = torch.Tensor([random.uniform(-0.05, 0.05) for i in range(EMBEDDING_DIM)])\n\t\t\t\t#dict[word] = torch.Tensor([0 for i in range(EMBEDDING_DIM)])\n\n\t\tlm_mode = False\n\t\tcombine_mode=\'g_0.75\' # \'concat\', \'g_0.25\', \'g_0.50\', \'g_0.75\', \'adaptive\', \'attention\', \'backoff\'\n\t\t# c2w parameters\n\t\tc2w_mode = False\n\t\tcharacter_ngrams = 1\n\t\t#character_ngrams_2 = 3\n\t\tcharacter_ngrams_overlap = False\n\t\tif character_ngrams == 1:\n\t\t\tdict_char_ngram = pickle.load(open(basepath + \'/data/\' + task + \'/char_dict.p\', ""rb""))\n\t\telif character_ngrams == 2 and character_ngrams_overlap:\n\t\t\tdict_char_ngram = pickle.load(open(basepath + \'/data/\' + task + \'/bigram_dict.p\', ""rb""))\n\t\telif character_ngrams == 2 and not character_ngrams_overlap:\n\t\t\tdict_char_ngram = pickle.load(open(basepath + \'/data/\' + task + \'/bigram_dict_no_overlap.p\', ""rb""))\n\t\telif character_ngrams == 3 and character_ngrams_overlap:\n\t\t\tdict_char_ngram = pickle.load(open(basepath + \'/data/\' + task + \'/trigram_dict.p\', ""rb""))\n\t\telif character_ngrams == 3 and not character_ngrams_overlap:\n\t\t\tdict_char_ngram = pickle.load(open(basepath + \'/data/\' + task + \'/trigram_dict_no_overlap.p\', ""rb""))\n\t\t\'\'\'\n\t\tif character_ngrams_2 == 1:\n\t\t\tdict_char_ngram_2 = pickle.load(open(basepath + \'/data/\' + task + \'/char_dict.p\', ""rb""))\n\t\telif character_ngrams_2 == 2 and character_ngrams_overlap:\n\t\t\tdict_char_ngram_2 = pickle.load(open(basepath + \'/data/\' + task + \'/bigram_dict.p\', ""rb""))\n\t\telif character_ngrams_2 == 2 and not character_ngrams_overlap:\n\t\t\tdict_char_ngram_2 = pickle.load(open(basepath + \'/data/\' + task + \'/bigram_dict_no_overlap.p\', ""rb""))\n\t\telif character_ngrams_2 == 3 and character_ngrams_overlap:\n\t\t\tdict_char_ngram_2 = pickle.load(open(basepath + \'/data/\' + task + \'/trigram_dict.p\', ""rb""))\n\t\telif character_ngrams_2 == 3 and not character_ngrams_overlap:\n\t\t\tdict_char_ngram_2 = pickle.load(open(basepath + \'/data/\' + task + \'/trigram_dict_no_overlap.p\', ""rb""))\n\t\t\'\'\'\n\t\tword_freq = pickle.load(open(basepath + \'/data/\' + task + \'/word_freq.p\', ""rb""))\n\t\tprint(\'current task: \'+task+\', lm mode: \'+str(lm_mode)+\', combination mode: \'+combine_mode+\', c2w mode: \'+str(c2w_mode)+\', n = \'+str(character_ngrams)+\', overlap = \'+str(character_ngrams_overlap)+\'.\')\n\t\tprint(\'finished loading word & char table, there are \'+str(num_inv)+\' INV words and \'+str(num_oov)+\' OOV words.\')\n\telif granularity==\'cross\':\n\t\toov=[]\n\t\tdict_char=[]\n\t\ttokens=[]\n\t\tword_freq=[]\n\t\toverlap=True\n\t\tif overlap:\n\t\t\tdict_ngram = pickle.load(open(basepath + \'/data/\' + task + \'/cross_trigram_dict.p\', ""rb""))\n\t\telse:\n\t\t\tdict_ngram = pickle.load(open(basepath + \'/data/\' + task + \'/cross_trigram_dict_no_overlap.p\', ""rb""))\n\telse:\n\t\tprint(\'wrong input for the second argument!\')\n\t\tsys.exit()\n\n\tmodel=DeepPairWiseWord(EMBEDDING_DIM,HIDDEN_DIM,1,task,granularity,num_class,dict,fake_dict, dict_char_ngram, oov,tokens, word_freq,\n\t                       feature_maps,kernels,charcnn_embedding_size,max_word_length,character_ngrams,c2w_mode,character_ngrams_overlap, word_mode,\n\t                       combine_mode, lm_mode)#, corpus)\n\t#print(get_n_params(model))\n\t#sys.exit()\n\t#print(model.lm_train_data)\n\t#sys.exit()\n\t#premodel=DeepPairWiseWord(EMBEDDING_DIM,HIDDEN_DIM,1,task,granularity,num_class,dict,dict_char,oov)\n\t#premodel.load_state_dict(torch.load(\'model_char_only.pkl\'))\n\t#premodel=torch.load(\'model_char_only.pkl\')\n\t#model.embedding=premodel.embedding\n\t#model.lstm_c2w=premodel.lstm_c2w\n\t#model.df=premodel.df\n\t#model.db=premodel.db\n\t#model.bias=premodel.bias\n\tif torch.cuda.is_available():\n\t\tmodel=model.cuda()\n\tlsents, rsents, labels = trainset\n\t#print(len(lsents))\n\t#threshold=40000\n\t#lsents = lsents[:threshold]\n\t#rsents = rsents[:threshold]\n\t#labels = labels[:threshold]\n\t# Loss and Optimizer\n\tif task==\'sick\' or task==\'sts\' or task==\'snli\':\n\t\tindices = torch.randperm(len(lsents))\n\t\tprint(\'indices:\')\n\t\tprint(indices[:10])\n\t\t#for line in open(\'./data/sick/order.txt\'):\n\t\t#\tindices.append(int(line.strip()) - 1)\n\t\tcriterion = nn.KLDivLoss()\n\t\tif torch.cuda.is_available():\n\t\t\tcriterion=criterion.cuda()\n\telif task==\'url\' or task==\'pit\' or task==\'hindi\' or task==\'quora\' or task==\'msrp\' or task==\'wikiqa\' or task==\'trecqa\' or task==\'mnli\':\n\t\t\'\'\'\n\t\tindices = torch.randperm(len(trainset[0]))\n\t\twith open(\'./data/\'+task+\'/order.txt\',\'w\') as f:\n\t\t\tfor item in indices:\n\t\t\t\tf.writelines(str(item)+\'\\n\')\n\t\t\'\'\'\n\t\t#indices = []\n\t\t#for line in open(\'./data/\'+task+\'/order.txt\'):\n\t\t#\tindices.append(int(line.strip()))\n\t\tindices = torch.randperm(len(lsents))\n\t\t#print(\'indices:\')\n\t\t#print(indices[:10])\n\t\tcriterion = nn.MultiMarginLoss(p=1, margin=1.0, weight=None, size_average=True)\n\t\tif torch.cuda.is_available():\n\t\t\tcriterion = criterion.cuda()\n\telif task==\'wiki\':\n\t\tindices = torch.randperm(len(lsents))\n\t\tprint(\'indices:\')\n\t\tprint(indices[:10])\n\t\tcriterion = nn.MultiLabelSoftMarginLoss()\n\t\tif torch.cuda.is_available():\n\t\t\tcriterion = criterion.cuda()\n\toptimizer = torch.optim.RMSprop(model.parameters(), lr=0.0001)#, momentum=0.1, weight_decay=0.05)#,momentum=0.9,weight_decay=0.95)\n\t#optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n\t# Train the Model\n\t#print(oov)\n\tprint(\'start training\')\n\t#subprocess.call([\'echo\',\'start training\'])\n\tgold=[]\n\tgold_um=[]\n\tif task==\'url\':\n\t\tfor line in open(basepath+\'/data/\' + task + \'/test_9324/sim.txt\'):\n\t\t\tgold.append(int(line.strip()))\n\telif task==\'snli\':\n\t\tfor line in open(basepath+\'/data/\' + task + \'/test/sim.txt\'):\n\t\t\tgold.append(line.strip())\n\telif task==\'trecqa\':\n\t\tfor line in open(basepath+\'/data/\' + task + \'/raw-test/sim.txt\'):\n\t\t\tgold.append(float(line.strip()))\n\telif task==\'mnli\':\n\t\tpass\n\t\t\'\'\'\n\t\tfor line in open(basepath+\'/data/\' + task + \'/dev_m/sim.txt\'):\n\t\t\tgold.append(float([\'neutral\', \'entailment\',\'contradiction\'].index(line.strip())))\n\t\tfor line in open(basepath+\'/data/\' + task + \'/dev_um/sim.txt\'):\n\t\t\tgold_um.append(float([\'neutral\', \'entailment\',\'contradiction\'].index(line.strip())))\n\t\t\'\'\'\n\telse:\n\t\tfor line in open(basepath+\'/data/\' + task + \'/test/sim.txt\'):\n\t\t\tgold.append(float(line.strip()))\n\tmax_result=-1\n\tmax_result_um=-1\n\tbatch_size=32\n\treport_interval=50000\n\tfor epoch in range(num_epochs):\n\t\tprint(\'--\'*20)\n\t\tmodel.train()\n\t\toptimizer.zero_grad()\n\t\tstart_time = time.time()\n\t\tdata_loss = 0\n\t\tindices = torch.randperm(len(lsents))\n\t\ttrain_correct=0\n\t\t#print(len(indices))\n\t\tfor index, i in enumerate(indices):\n\t\t\t#print(index)\n\t\t\t#start_time = time.time()\n\t\t\tif granularity==\'word\':\n\t\t\t\tsentA = lsents[i]\n\t\t\t\tsentB = rsents[i]\n\t\t\t\t\'\'\'\n\t\t\t\t#print(lsents[i])\n\t\t\t\ttry:\n\t\t\t\t\tsentA = torch.cat((dict[word].view(1, EMBEDDING_DIM) for word in lsents[i]), 0)\n\t\t\t\t\tsentA = Variable(sentA)#.cuda()\n\t\t\t\t\t#print(lsents[i])\n\t\t\t\t\t#print(sentA)\n\t\t\t\t\t#print(rsents[i])\n\t\t\t\t\tsentB = torch.cat((dict[word].view(1, EMBEDDING_DIM) for word in rsents[i]), 0)\n\t\t\t\t\tsentB = Variable(sentB)#.cuda()\n\t\t\t\texcept:\n\t\t\t\t\tprint(lsents[i])\n\t\t\t\t\tprint(rsents[i])\n\t\t\t\t\tsys.exit()\n\t\t\t\t#print(rsents[i])\n\t\t\t\t#print(sentB)\n\t\t\t\t#sys.exit()\n\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\tsentA=sentA.cuda()\n\t\t\t\t\tsentB=sentB.cuda()\n\t\t\t\tsentA = torch.unsqueeze(sentA, 0).view(-1, 1, EMBEDDING_DIM)\n\t\t\t\tsentB = torch.unsqueeze(sentB, 0).view(-1, 1, EMBEDDING_DIM)\n\t\t\t\t# label=torch.unsqueeze(label,0)\n\t\t\t\t\'\'\'\n\t\t\telif granularity==\'char\' or granularity==\'mix\' or granularity==\'cross\':\n\t\t\t\t#sentA=[]\n\t\t\t\t#sentB=[]\n\t\t\t\t#for word in lsents[i]:\n\t\t\t\t#\tsentA.append([dict[char] for char in word])\n\t\t\t\t#for word in rsents[i]:\n\t\t\t\t#\tsentB.append([dict[char] for char in word])\n\t\t\t\t#print(i)\n\t\t\t\tsentA=lsents[i]\n\t\t\t\tsentB=rsents[i]\n\t\t\tif task==\'sick\' or task==\'sts\' or task==\'snli\' or task==\'wiki\':\n\t\t\t\tlabel=Variable(torch.Tensor(labels[i]))\n\t\t\telse:\n\t\t\t\tlabel = Variable(torch.LongTensor(labels[i]))#.cuda()\n\t\t\tif torch.cuda.is_available():\n\t\t\t\tlabel=label.cuda()\n\t\t\t# Forward + Backward + Optimize\n\t\t\t#elapsed_time = time.time() - start_time\n\t\t\t#print(\'data preparation time: \'+str(timedelta(seconds=elapsed_time)))\n\t\t\t#print(sentA)\n\t\t\t#print(sentB)\n\t\t\t#print(id[i])\n\t\t\t#print(\'*\'*20)\n\t\t\toutput,extra_loss = model(sentA, sentB, index)\n\t\t\t#tmp_output = np.exp(output.data[0].cpu().numpy())\n\t\t\t#print index, \'gold: \', labels[i][0], \'predict: \', np.argmax(tmp_output)\n\t\t\t#print(extra_loss)\n\t\t\tloss = criterion(output, label)+extra_loss\n\t\t\tloss.backward()\n\t\t\tdata_loss += loss.data[0]\n\t\t\toutput = np.exp(output.data[0].cpu().numpy())\n\t\t\tif labels[i][0] == np.argmax(output):\n\t\t\t\ttrain_correct += 1\n\t\t\t#print(loss-extra_loss)\n\t\t\t#print(\'*\'*20)\n\t\t\tif (index+1) % batch_size == 0:\n\t\t\t\toptimizer.step()\n\t\t\t\toptimizer.zero_grad()\n\n\t\t\tif (index+1) % report_interval == 0:\n\t\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, index+1)\n\t\t\t\tmsg += \'\\t train batch loss: %f\' % (data_loss / (index+1))\n\t\t\t\ttrain_acc=train_correct/(index+1)\n\t\t\t\tmsg += \'\\t train accuracy: %f\' % train_acc\n\t\t\t\tprint(msg)\n\n\t\t\tif (index + 1) % (int(len(lsents)/2)) == 0:\n\t\t\t\t#print (\'Epoch [%d/%d], Iter [%d/%d] Loss: %.6f\'\n\t\t\t\t#\t   % (epoch + 1, num_epochs, index + 1, len(lsents) // 1, data_loss))#loss.data[0]))\n\t\t\t\t#subprocess.call([\'echo\',\'Epoch \',str(epoch+1),\'Loss: \',str(data_loss)])\n\t\t\t\t#break\n\t\t\t\t#data_loss = 0\n\t\t\t\t#torch.save(model.state_dict(), \'model.pkl\')\n\t\t\t\t#model.load_state_dict(torch.load(\'model_char_only.pkl\'))\n\n\t\t\t\tif task==\'sick\' or task==\'sts\' or task==\'snli\' or task==\'wiki\':\n\t\t\t\t\tmodel.eval()\n\t\t\t\t\ttest_lsents, test_rsents, test_labels = testset\n\t\t\t\t\tpredicted=[]\n\t\t\t\t\ttmp_result=0\n\t\t\t\t\t#gold=[]\n\t\t\t\t\t#for line in open(\'./data/sick/test/sim.txt\'):\n\t\t\t\t\t#\tgold.append(float(line.strip()))\n\t\t\t\t\tfor test_i in range(len(test_lsents)):\n\t\t\t\t\t\tif granularity == \'word\':\n\t\t\t\t\t\t\t\'\'\'\n\t\t\t\t\t\t\tsentA = torch.cat((dict[word].view(1, EMBEDDING_DIM) for word in test_lsents[test_i]), 0)\n\t\t\t\t\t\t\tsentA = Variable(sentA)\n\t\t\t\t\t\t\t# print(sentA)\n\t\t\t\t\t\t\tsentB = torch.cat((dict[word].view(1, EMBEDDING_DIM) for word in test_rsents[test_i]), 0)\n\t\t\t\t\t\t\tsentB = Variable(sentB)\n\t\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\t\tsentA = sentA.cuda()\n\t\t\t\t\t\t\t\tsentB = sentB.cuda()\n\t\t\t\t\t\t\t#label = torch.unsqueeze(label, 0)\n\t\t\t\t\t\t\tsentA = torch.unsqueeze(sentA, 0).view(-1, 1, EMBEDDING_DIM)\n\t\t\t\t\t\t\tsentB = torch.unsqueeze(sentB, 0).view(-1, 1, EMBEDDING_DIM)\n\t\t\t\t\t\t\t\'\'\'\n\t\t\t\t\t\t\tsentA = test_lsents[test_i]\n\t\t\t\t\t\t\tsentB = test_rsents[test_i]\n\t\t\t\t\t\telif granularity == \'char\' or granularity==\'mix\':\n\t\t\t\t\t\t\tsentA = test_lsents[test_i]\n\t\t\t\t\t\t\tsentB = test_rsents[test_i]\n\t\t\t\t\t\traw_output,_ = model(sentA, sentB, index)\n\t\t\t\t\t\t#print(output)\n\t\t\t\t\t\tif task==\'sick\':\n\t\t\t\t\t\t\toutput=raw_output\n\t\t\t\t\t\t\toutput = np.exp(output.data[0].cpu().numpy())\n\t\t\t\t\t\t\tpredicted.append(1*output[0]+2*output[1]+3*output[2]+4*output[3]+5*output[4])\n\t\t\t\t\t\telif task==\'snli\':\n\t\t\t\t\t\t\toutput = raw_output\n\t\t\t\t\t\t\toutput = np.exp(output.data[0].cpu().numpy())\n\t\t\t\t\t\t\toutput=[output[0],output[1],output[2]]\n\t\t\t\t\t\t\ttmp_output=output.index(max(output))\n\t\t\t\t\t\t\tpredicted.append(tmp_output)\n\t\t\t\t\t\t\tif test_labels[test_i].index(max(test_labels[test_i]))==tmp_output:\n\t\t\t\t\t\t\t\ttmp_result+=1\n\t\t\t\t\t\telif task==\'wiki\':\n\t\t\t\t\t\t\toutput = torch.sigmoid(raw_output).data > 0.5\n\t\t\t\t\t\t\toutput = output.cpu()\n\t\t\t\t\t\t\tpredicted = list(output.numpy()[0])\n\t\t\t\t\t\t\tif predicted==test_labels[test_i]:\n\t\t\t\t\t\t\t\ttmp_result+=1\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\toutput = raw_output\n\t\t\t\t\t\t\toutput = np.exp(output.data[0].cpu().numpy())\n\t\t\t\t\t\t\tpredicted.append(0 * output[0] + 1 * output[1] + 2 * output[2] + 3 * output[3] + 4 * output[4] + 5*output[5])\n\t\t\t\t\t#print(predicted)\n\t\t\t\t\t#print(gold)\n\t\t\t\t\tif task==\'sick\':\n\t\t\t\t\t\tresult = pearson(predicted, gold)\n\t\t\t\t\t\tprint(\'Test Correlation: %.6f\' % result)\n\t\t\t\t\t\tif result>max_result:\n\t\t\t\t\t\t\tmax_result=result\n\t\t\t\t\telif task==\'snli\' or task==\'wiki\':\n\t\t\t\t\t\tresult=tmp_result/len(test_lsents)\n\t\t\t\t\t\tprint(\'Test Accuracy: %.6f\' % result)\n\t\t\t\t\t\tif result>max_result:\n\t\t\t\t\t\t\tmax_result=result\n\t\t\t\t\telse:\n\t\t\t\t\t\tresult1=pearson(predicted[0:450],gold[0:450])\n\t\t\t\t\t\tresult2=pearson(predicted[450:750],gold[450:750])\n\t\t\t\t\t\tresult3=pearson(predicted[750:1500],gold[750:1500])\n\t\t\t\t\t\tresult4=pearson(predicted[1500:2250],gold[1500:2250])\n\t\t\t\t\t\tresult5=pearson(predicted[2250:3000],gold[2250:3000])\n\t\t\t\t\t\tresult6=pearson(predicted[3000:3750],gold[3000:3750])\n\t\t\t\t\t\tprint(\'deft-forum: %.6f, deft-news: %.6f, headlines: %.6f, images: %.6f, OnWN: %.6f, tweet-news: %.6f\' %(result1, result2, result3, result4, result5, result6))\n\t\t\t\t\t\twt_mean=0.12*result1+0.08*result2+0.2*result3+0.2*result4+0.2*result5+0.2*result6\n\t\t\t\t\t\tprint(\'weighted mean: %.6f\' % wt_mean)\n\t\t\t\t\t\tif wt_mean>max_result:\n\t\t\t\t\t\t\tmax_result=wt_mean\n\t\t\t\t\t\tif task==\'sts\':\n\t\t\t\t\t\t\twith open(basepath+\'/data/sts/sts_PWIM_prob.txt\', \'w\') as f:\n\t\t\t\t\t\t\t\tfor item in predicted:\n\t\t\t\t\t\t\t\t\tf.writelines(str(item)+\'\\n\')\n\t\t\t\t\t\t#else:\n\t\t\t\t\t\t#\twith open(\'SICK_with_paragram_result.txt\', \'w\') as f:\n\t\t\t\t\t\t#\t\tfor item in predicted:\n\t\t\t\t\t\t#\t\t\tf.writelines(str(item)+\'\\n\')\n\t\t\t\telse:\n\t\t\t\t\tmodel.eval()\n\t\t\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, index+1)\n\t\t\t\t\tif task==\'mnli\':\n\t\t\t\t\t\ttest_lsents, test_rsents, test_labels = devset_m\n\t\t\t\t\telse:\n\t\t\t\t\t\ttest_lsents, test_rsents, test_labels = testset\n\t\t\t\t\tpredicted = []\n\t\t\t\t\tcorrect=0\n\t\t\t\t\t#gold=gold[:3000]\n\t\t\t\t\t#print(len(gold))\n\t\t\t\t\tfor test_i in range(len(test_lsents)):\n\t\t\t\t\t\t# start_time = time.time()\n\t\t\t\t\t\tif granularity == \'word\':\n\t\t\t\t\t\t\tsentA = test_lsents[test_i]\n\t\t\t\t\t\t\tsentB = test_rsents[test_i]\n\t\t\t\t\t\t\t\'\'\'\n\t\t\t\t\t\t\tsentA = torch.cat((dict[word].view(1, EMBEDDING_DIM) for word in test_lsents[test_i]), 0)\n\t\t\t\t\t\t\tsentA = Variable(sentA)#.cuda()\n\t\t\t\t\t\t\t# print(sentA)\n\t\t\t\t\t\t\tsentB = torch.cat((dict[word].view(1, EMBEDDING_DIM) for word in test_rsents[test_i]), 0)\n\t\t\t\t\t\t\tsentB = Variable(sentB)#.cuda()\n\t\t\t\t\t\t\t# print(sentB)\n\t\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\t\tsentA=sentA.cuda()\n\t\t\t\t\t\t\t\tsentB=sentB.cuda()\n\t\t\t\t\t\t\tsentA = torch.unsqueeze(sentA, 0).view(-1, 1, EMBEDDING_DIM)\n\t\t\t\t\t\t\tsentB = torch.unsqueeze(sentB, 0).view(-1, 1, EMBEDDING_DIM)\n\t\t\t\t\t\t# label=torch.unsqueeze(label,0)\n\t\t\t\t\t\t\t\'\'\'\n\t\t\t\t\t\telif granularity == \'char\' or granularity==\'mix\':\n\t\t\t\t\t\t\tsentA = test_lsents[test_i]\n\t\t\t\t\t\t\tsentB = test_rsents[test_i]\n\t\t\t\t\t\toutput, _ = model(sentA, sentB, index)\n\t\t\t\t\t\t#print(output)\n\t\t\t\t\t\toutput=np.exp(output.data[0].cpu().numpy())\n\t\t\t\t\t\tif test_labels[test_i][0]==np.argmax(output):\n\t\t\t\t\t\t\tcorrect+=1\n\t\t\t\t\t\tpredicted.append(output[1])\n\t\t\t\t\t#result=float(correct)/len(test_lsents)\n\t\t\t\t\t#print(\'Test Accuracy: %.4f\'% result)\n\t\t\t\t\t#result_acc, result_f1=URL_maxF1_eval(predict_result=predicted,test_data_label=gold)\n\t\t\t\t\tresult = correct / len(test_lsents)\n\t\t\t\t\tmsg += \'\\t dev m accuracy: %f\' % result\n\t\t\t\t\tprint(msg)\n\t\t\t\t\tif result>max_result:\n\t\t\t\t\t\tmax_result=result\n\t\t\t\t\t\ttest_lsents, test_rsents, test_labels = testset_m\n\t\t\t\t\t\tpredicted=[]\n\t\t\t\t\t\tfor test_i in range(len(test_lsents)):\n\t\t\t\t\t\t\t# start_time = time.time()\n\t\t\t\t\t\t\tif granularity == \'word\':\n\t\t\t\t\t\t\t\tsentA = test_lsents[test_i]\n\t\t\t\t\t\t\t\tsentB = test_rsents[test_i]\n\t\t\t\t\t\t\toutput, _ = model(sentA, sentB, index)\n\t\t\t\t\t\t\toutput = np.exp(output.data[0].cpu().numpy())\n\t\t\t\t\t\t\tpredicted.append(np.argmax(output))\n\t\t\t\t\t\twith open(basepath + \'/sub_m.csv\', \'w+\') as f:\n\t\t\t\t\t\t\tlabel_dict = [ \'neutral\', \'entailment\',\'contradiction\']\n\t\t\t\t\t\t\tf.write(""pairID,gold_label\\n"")\n\t\t\t\t\t\t\tfor i, k in enumerate(predicted):\n\t\t\t\t\t\t\t\tf.write(str(i + 9847) + "","" + label_dict[k] + ""\\n"")\n\t\t\t\t\t\t#with open(basepath+\'/PWIM_prob_result_\'+task, \'w\') as f:\n\t\t\t\t\t\t#\tfor item in predicted:\n\t\t\t\t\t\t#\t\tf.writelines(str(item)+\'\\n\')\n\t\t\t\t\tif task==\'mnli\':\n\t\t\t\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, index + 1)\n\t\t\t\t\t\ttest_lsents, test_rsents, test_labels = devset_um\n\t\t\t\t\t\tpredicted = []\n\t\t\t\t\t\tcorrect = 0\n\t\t\t\t\t\tfor test_i in range(len(test_lsents)):\n\t\t\t\t\t\t\t# start_time = time.time()\n\t\t\t\t\t\t\tif granularity == \'word\':\n\t\t\t\t\t\t\t\tsentA = test_lsents[test_i]\n\t\t\t\t\t\t\t\tsentB = test_rsents[test_i]\n\t\t\t\t\t\t\toutput, _ = model(sentA, sentB, index)\n\t\t\t\t\t\t\t# print(output)\n\t\t\t\t\t\t\toutput = np.exp(output.data[0].cpu().numpy())\n\t\t\t\t\t\t\tif test_labels[test_i][0] == np.argmax(output):\n\t\t\t\t\t\t\t\tcorrect += 1\n\t\t\t\t\t\t\tpredicted.append(output[1])\n\t\t\t\t\t\t#result_acc, result_f1 = URL_maxF1_eval(predict_result=predicted, test_data_label=gold_um)\n\t\t\t\t\t\tresult_acc = correct / len(test_lsents)\n\t\t\t\t\t\tmsg += \'\\t dev um accuracy: %f\' % result_acc\n\t\t\t\t\t\tprint(msg)\n\t\t\t\t\t\tif result_acc > max_result_um:\n\t\t\t\t\t\t\tmax_result_um = result_acc\n\t\t\t\t\t\t\ttest_lsents, test_rsents, test_labels = testset_um\n\t\t\t\t\t\t\tpredicted = []\n\t\t\t\t\t\t\tfor test_i in range(len(test_lsents)):\n\t\t\t\t\t\t\t\t# start_time = time.time()\n\t\t\t\t\t\t\t\tif granularity == \'word\':\n\t\t\t\t\t\t\t\t\tsentA = test_lsents[test_i]\n\t\t\t\t\t\t\t\t\tsentB = test_rsents[test_i]\n\t\t\t\t\t\t\t\toutput, _ = model(sentA, sentB, index)\n\t\t\t\t\t\t\t\toutput = np.exp(output.data[0].cpu().numpy())\n\t\t\t\t\t\t\t\tpredicted.append(np.argmax(output))\n\t\t\t\t\t\t\twith open(basepath + \'/sub_um.csv\', \'w+\') as f:\n\t\t\t\t\t\t\t\tlabel_dict = [\'neutral\', \'entailment\',\'contradiction\']\n\t\t\t\t\t\t\t\tf.write(""pairID,gold_label\\n"")\n\t\t\t\t\t\t\t\tfor i, k in enumerate(predicted):\n\t\t\t\t\t\t\t\t\tf.write(str(i) + "","" + label_dict[k] + ""\\n"")\n\t\t\t\t\t\t#with open(\'current task: \'+task+\', lm mode: \'+str(lm_mode)+\', combination mode: \'+combine_mode+\', c2w mode: \'+str(c2w_mode)+\', n = \'+str(character_ngrams)+\', overlap = \'+str(character_ngrams_overlap)+\'.txt\',\'w\') as f:\n\t\t\t\t\t\t#\tfor item in predicted:\n\t\t\t\t\t\t#\t\tf.writelines(str(item)+\'\\n\')\n\t\t\t\t\t\t#torch.save(model, \'model_URL_unigram_CNN.pkl\')\n\t\t\t\t\t\t#torch.save(model, \'model_word_inv_18k.pkl\')\n\t\t\t\t\t\t#torch.save(model, \'model_word_inv_3k.pkl\')\n\t\t\t\t\t\t#torch.save(model, \'model_char_only.pkl\')\n\t\t\t\t\t\t#torch.save(model, \'model_word_only_pit.pkl\')\n\t\t\t\t\t\t#torch.save(model, \'model_word_char_backoff.pkl\')\n\t\t\t\t\t\t#torch.save(model, \'model_word_char_g_0.5.pkl\')\n\t\t\t\t\t\t#torch.save(model, \'model_word_char_adaptive.pkl\')\n\t\t\t\t\t\t#torch.save(model, \'model_word_char_attention.pkl\')\n\t\t\t\t\t\t#with open(\'model_word_inv_0k_result.txt\', \'w\') as f:\n\t\t\t\t\t\t#with open(\'sts_model_word_only_inv_17k_result.txt\', \'w\') as f:\n\t\t\t\t\t\t#with open(\'model_word_inv_3k_result.txt\', \'w\') as f:\n\t\t\t\t\t\t#with open(\'model_char_only_result.txt\', \'w\') as f:\n\t\t\t\t\t\t#with open(\'model_word_only_result_pit.txt\', \'w\') as f:\n\t\t\t\t\t\t#with open(\'model_word_char_g_0.5_result.txt\', \'w\') as f:\n\t\t\t\t\t\t#with open(\'model_word_char_backoff_result.txt\', \'w\') as f:\n\t\t\t\t\t\t#with open(\'model_word_char_adaptive.txt\', \'w\') as f:\n\t\t\t\t\t\t#with open(\'model_word_char_attention_result.txt\',\'w\') as f:\n\t\t\t\t\t\t#\tfor item in predicted:\n\t\t\t\t\t\t#\t\tf.writelines(str(item)+\'\\n\')\n\t\t\t\t\t\t\'\'\'\n\t\t\t\t\t\th = Variable(torch.zeros(2, 1, model.embedding_dim))  # 2 for bidirection\n\t\t\t\t\t\tc = Variable(torch.zeros(2, 1, model.embedding_dim))\n\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\th = h.cuda()\n\t\t\t\t\t\t\tc = c.cuda()\n\t\t\t\t\t\tsubword_embedding={}\n\t\t\t\t\t\tfor word in org_tokens:\n\t\t\t\t\t\t\ttmp_indices = model.generate_word_indices(word)\n\t\t\t\t\t\t\tif not model.c2w_mode:\n\t\t\t\t\t\t\t\tif len(tmp_indices) < 20:\n\t\t\t\t\t\t\t\t\ttmp_indices = tmp_indices + [0 for i in range(model.charcnn_max_word_length - len(tmp_indices))]\n\t\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\t\ttmp_indices = tmp_indices[0:20]\n\t\t\t\t\t\t\tif model.c2w_mode:\n\t\t\t\t\t\t\t\toutput = model.c2w_cell([tmp_indices], h, c)\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\toutput = model.charCNN_cell([tmp_indices])\n\t\t\t\t\t\t\tsubword_embedding[word]=output.data[0].cpu().numpy()\n\t\t\t\t\t\tpickle.dump(subword_embedding, open(\'URL_subword_lm_embedding.p\', ""wb""))\n\t\t\t\t\t\t\'\'\'\n\t\t\t\telapsed_time = time.time() - start_time\n\t\t\t\tprint(\'Epoch \' + str(epoch + 1) + \' finished within \' + str(timedelta(seconds=elapsed_time))+\', and current time:\'+ str(datetime.now()))\n\t\t\t\tprint(\'Best result until now: %.6f\' % max_result)\n\t\t\t\tprint(\'Best um result until now: %.6f\' % max_result_um)\n\t\t\t\t#subprocess.call([\'echo\',\'Epoch \' , str(epoch + 1) , \' finished within \' , str(timedelta(seconds=elapsed_time)),\', and current time:\', str(datetime.now())])\n\t\t\t\t#subprocess.call([\'echo\',\'Best result until now: \',str(max_result)])\n\t\t\t\tmodel.train()\n\n\nif __name__ == \'__main__\':\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\'--task\', type=str, default=\'sts\',\n\t\t\t\t\t\thelp=\'Currently supported tasks: sick, pit, sts, url, snli, mnli, hindi, quora, msrp, wiki wikiqa and trecqa\')\n\tparser.add_argument(\'--granularity\', type=str, default=\'word\',\n\t\t\t\t\t\thelp=\'Currently supported granularities: char, word, mix and cross.\')\n\targs = parser.parse_args()\n\tprint(args)\n\tmain(args)\n'"
PWIM/model.py,218,"b""import sys\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torch\nimport math\nimport torch.nn as nn\nfrom util import *\nimport numpy\nimport random\n\n\nclass DeepPairWiseWord(nn.Module):\n\tdef __init__(self, embedding_dim, hidden_dim, num_layers,task,granularity,num_class,dict,fake_dict, dict_char_ngram, oov,tokens, word_freq,\n\t             feature_maps, kernels, charcnn_embedding_size, charcnn_max_word_length, character_ngrams,c2w_mode,character_ngrams_overlap,word_mode,\n\t             combine_mode,lm_mode, deep_CNN):#,corpus):\n\t\tsuper(DeepPairWiseWord, self).__init__()\n\t\tself.task = task\n\t\tif task=='pit':\n\t\t\tself.limit = 32\n\t\telse:\n\t\t\tself.limit = 48\n\t\t# Language model parameters--------------------------\n\t\tif lm_mode:\n\t\t\tself.lm_loss=nn.NLLLoss()\n\t\t\tself.lm_softmax=nn.LogSoftmax()\n\t\t\tself.lm_tanh=nn.Tanh()\n\t\t\tself.lm_lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, bidirectional=True)\n\t\t\tself.lm_Wm_forward=Variable(torch.rand(hidden_dim,hidden_dim))\n\t\t\tself.lm_Wm_backward = Variable(torch.rand(hidden_dim, hidden_dim))\n\t\t\tself.lm_Wq_forward=Variable(torch.rand(hidden_dim,len(tokens)))\n\t\t\tself.lm_Wq_backword=Variable(torch.rand(hidden_dim, len(tokens)))\n\n\t\t# end of LM parameters-------------------------------\n\t\tself.granularity=granularity\n\t\tself.num_class=num_class\n\t\tself.hidden_dim = hidden_dim\n\t\tself.num_layers = num_layers\n\t\tself.embedding_dim=embedding_dim\n\t\tself.dict=dict\n\t\tself.fake_dict=fake_dict\n\t\tself.dict_char_ngram=dict_char_ngram\n\t\tself.word_freq=word_freq\n\t\tself.oov=oov\n\t\tself.tokens=tokens\n\t\tword2id={}\n\t\tindex=0\n\t\tfor word in tokens:\n\t\t\tword2id[word]=index\n\t\t\tindex+=1\n\t\tself.word2id=word2id\n\t\tself.feature_maps=feature_maps\n\t\tself.kernels=kernels\n\t\tself.charcnn_max_word_length=charcnn_max_word_length\n\t\tself.character_ngrams=character_ngrams\n\t\tself.c2w_mode=c2w_mode\n\t\tself.character_ngrams_overlap=character_ngrams_overlap\n\t\tself.word_mode=word_mode\n\t\tself.combine_mode=combine_mode\n\t\tself.lm_mode=lm_mode\n\t\tself.deep_CNN=deep_CNN\n\t\tif granularity=='char':\n\t\t\tself.df = Variable(torch.rand(embedding_dim, embedding_dim))\n\t\t\tself.db = Variable(torch.rand(embedding_dim, embedding_dim))\n\t\t\tself.bias = Variable(torch.rand(embedding_dim))\n\t\t\tself.Wx=Variable(torch.rand(embedding_dim, embedding_dim))\n\t\t\tself.W1=Variable(torch.rand(embedding_dim, embedding_dim))\n\t\t\tself.W2=Variable(torch.rand(embedding_dim, embedding_dim))\n\t\t\tself.W3=Variable(torch.rand(embedding_dim, embedding_dim))\n\t\t\tself.vg = Variable(torch.rand(embedding_dim, 1))\n\t\t\tself.bg = Variable(torch.rand(1, 1))\n\t\t\t#--self.string_sim_weight=Variable(torch.rand(13,48,48))\n\t\t\t#--self.string_sim_bias = Variable(torch.rand(13,48, 48))\n\t\t\tif torch.cuda.is_available():\n\t\t\t\tself.df = self.df.cuda()\n\t\t\t\tself.db = self.db.cuda()\n\t\t\t\tself.bias = self.bias.cuda()\n\t\t\t\tself.Wx=self.Wx.cuda()\n\t\t\t\tself.W1=self.W1.cuda()\n\t\t\t\tself.W2=self.W2.cuda()\n\t\t\t\tself.W3=self.W3.cuda()\n\t\t\t\tself.vg=self.vg.cuda()\n\t\t\t\tself.bg=self.bg.cuda()\n\t\t\t\t#--self.string_sim_weight=self.string_sim_weight.cuda()\n\t\t\t\t#--self.string_sim_bias=self.string_sim_bias.cuda()\n\t\t\t\tif lm_mode:\n\t\t\t\t\tself.lm_Wm_forward=self.lm_Wm_forward.cuda()\n\t\t\t\t\tself.lm_Wm_backward=self.lm_Wm_backward.cuda()\n\t\t\t\t\tself.lm_Wq_forward=self.lm_Wq_forward.cuda()\n\t\t\t\t\tself.lm_Wq_backword=self.lm_Wq_backword.cuda()\n\t\t\t\tpass\n\t\t\tself.c2w_embedding=nn.Embedding(len(dict_char_ngram),50)\n\t\t\tself.char_cnn_embedding=nn.Embedding(len(dict_char_ngram),charcnn_embedding_size)\n\t\t\tself.lstm_c2w = nn.LSTM(50, embedding_dim, 1, bidirectional=True)\n\t\t\tself.charCNN_filter1 = nn.Sequential(\n\t\t\t\tnn.Conv2d(1, feature_maps[0], (kernels[0], charcnn_embedding_size)),\n\t\t\t\tnn.Tanh(),\n\t\t\t\tnn.MaxPool2d((charcnn_max_word_length - kernels[0] + 1, 1), stride=1)\n\t\t\t)\n\t\t\tself.charCNN_filter2 = nn.Sequential(\n\t\t\t\tnn.Conv2d(1, feature_maps[1], (kernels[1], charcnn_embedding_size)),\n\t\t\t\tnn.Tanh(),\n\t\t\t\tnn.MaxPool2d((charcnn_max_word_length - kernels[1] + 1, 1), stride=1)\n\t\t\t)\n\t\t\tself.charCNN_filter3 = nn.Sequential(\n\t\t\t\tnn.Conv2d(1, feature_maps[2], (kernels[2], charcnn_embedding_size)),\n\t\t\t\tnn.Tanh(),\n\t\t\t\tnn.MaxPool2d((charcnn_max_word_length - kernels[2] + 1, 1), stride=1)\n\t\t\t)\n\t\t\tself.charCNN_filter4 = nn.Sequential(\n\t\t\t\tnn.Conv2d(1, feature_maps[3], (kernels[3], charcnn_embedding_size)),\n\t\t\t\tnn.Tanh(),\n\t\t\t\tnn.MaxPool2d((charcnn_max_word_length - kernels[3] + 1, 1), stride=1)\n\t\t\t)\n\t\t\tself.charCNN_filter5 = nn.Sequential(\n\t\t\t\tnn.Conv2d(1, feature_maps[4], (kernels[4], charcnn_embedding_size)),\n\t\t\t\tnn.Tanh(),\n\t\t\t\tnn.MaxPool2d((charcnn_max_word_length - kernels[4] + 1, 1), stride=1)\n\t\t\t)\n\t\t\tself.charCNN_filter6 = nn.Sequential(\n\t\t\t\tnn.Conv2d(1, feature_maps[5], (kernels[5], charcnn_embedding_size)),\n\t\t\t\tnn.Tanh(),\n\t\t\t\tnn.MaxPool2d((charcnn_max_word_length - kernels[5] + 1, 1), stride=1)\n\t\t\t)\n\t\t\tself.charCNN_filter7 = nn.Sequential(\n\t\t\t\tnn.Conv2d(1, feature_maps[6], (kernels[6], charcnn_embedding_size)),\n\t\t\t\tnn.Tanh(),\n\t\t\t\tnn.MaxPool2d((charcnn_max_word_length - kernels[6] + 1, 1), stride=1)\n\t\t\t)\n\t\t\tself.transform_gate = nn.Sequential(nn.Linear(1100, 1100), nn.Sigmoid())\n\t\t\tself.char_cnn_mlp = nn.Sequential(nn.Linear(1100, 1100), nn.Tanh())\n\t\t\t# --\n\t\t\tself.down_sampling_200 = nn.Linear(1100, 200)\n\t\t\tself.down_sampling_300 = nn.Linear(1100, 300)\n\t\telif granularity=='word':\n\t\t\t#pass\n\t\t\t''''''\n\t\t\tself.word_embedding=nn.Embedding(len(tokens),embedding_dim)\n\t\t\tself.copied_word_embedding=nn.Embedding(len(tokens),embedding_dim)\n\t\t\tpretrained_weight = numpy.zeros(shape=(len(self.tokens), self.embedding_dim))\n\t\t\tfor word in self.tokens:\n\t\t\t\tpretrained_weight[self.tokens.index(word)] = self.dict[word].numpy()\n\t\t\tself.copied_word_embedding.weight.data.copy_(torch.from_numpy(pretrained_weight))\n\t\t\t''''''\n\t\t#self.lstm = nn.LSTM(embedding_dim*2, hidden_dim, num_layers, bidirectional=True)\n\t\tself.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, bidirectional=True)\n\t\tif not deep_CNN:\n\t\t\tself.mlp_layer = nn.Sequential(nn.Linear(self.limit*self.limit*13, 16),nn.Linear(16, num_class), nn.LogSoftmax())\n\t\telse:\n\t\t\tself.layer1 = nn.Sequential(\n\t\t\t\tnn.Conv2d(13, 128, kernel_size=3,stride=1, padding=1),\n\t\t\t\t#nn.Conv2d(26, 128, kernel_size=3, stride=1, padding=1),\n\t\t\t\tnn.ReLU(),\n\t\t\t\tnn.MaxPool2d(kernel_size=2,stride=2, ceil_mode=True))\n\t\t\tself.layer1_ = nn.Sequential(\n\t\t\t\t#nn.Conv2d(13, 128, kernel_size=3, stride=1, padding=1),\n\t\t\t\tnn.Conv2d(26, 128, kernel_size=3, stride=1, padding=1),\n\t\t\t\tnn.ReLU(),\n\t\t\t\tnn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True))\n\t\t\tself.layer2 = nn.Sequential(\n\t\t\t\tnn.Conv2d(128, 164, kernel_size=3, stride=1, padding=1),\n\t\t\t\tnn.ReLU(),\n\t\t\t\tnn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True))\n\t\t\tself.layer3 = nn.Sequential(\n\t\t\t\tnn.Conv2d(164, 192, kernel_size=3, stride=1, padding=1),\n\t\t\t\tnn.ReLU(),\n\t\t\t\tnn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True))\n\t\t\tself.layer4 = nn.Sequential(\n\t\t\t\tnn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1),\n\t\t\t\tnn.ReLU(),\n\t\t\t\tnn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True))\n\t\t\tself.layer5 = nn.Sequential(\n\t\t\t\tnn.Conv2d(192, 128, kernel_size=3, stride=1, padding=1),\n\t\t\t\tnn.ReLU(),\n\t\t\t\tnn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True))\n\t\t\tself.layer5_0 = nn.Sequential(\n\t\t\t\tnn.Conv2d(192, 128, kernel_size=3, stride=1, padding=1),\n\t\t\t\tnn.ReLU(),\n\t\t\t\tnn.MaxPool2d(kernel_size=3, stride=3, ceil_mode=True))\n\t\t\tself.fc1 = nn.Sequential(nn.Linear(128, 128), nn.ReLU(inplace=True))\n\t\t\tself.fc2 = nn.Sequential(nn.Linear(128, num_class), nn.LogSoftmax())\n\t\tself.init_weight()\n\n\tdef init_weight(self):\n\t\tif self.deep_CNN:\n\t\t\tself.layer1[0].weight.data.normal_(0, math.sqrt(2.0/(3*3*128)))\n\t\t\tself.layer1[0].bias.data.fill_(0)\n\t\t\tself.layer2[0].weight.data.normal_(0, math.sqrt(2.0/(3*3*164)))\n\t\t\tself.layer2[0].bias.data.fill_(0)\n\t\t\tself.layer3[0].weight.data.normal_(0, math.sqrt(2.0/(3*3*192)))\n\t\t\tself.layer3[0].bias.data.fill_(0)\n\t\t\tself.layer4[0].weight.data.normal_(0, math.sqrt(2.0/(3*3*192)))\n\t\t\tself.layer4[0].bias.data.fill_(0)\n\t\t\tself.layer5[0].weight.data.normal_(0, math.sqrt(2.0/(3*3*128)))\n\t\t\tself.layer5[0].bias.data.fill_(0)\n\t\t\tself.fc1[0].weight.data.uniform_(-0.1, 0.1)\n\t\t\tself.fc1[0].bias.data.fill_(0)\n\t\t\tself.fc2[0].weight.data.uniform_(-0.1, 0.1)\n\t\t\tself.fc2[0].bias.data.fill_(0)\n\t\tif self.granularity=='word':\n\t\t\tpretrained_weight=numpy.zeros(shape=(len(self.tokens),self.embedding_dim))\n\t\t\tfor word in self.tokens:\n\t\t\t\tpretrained_weight[self.tokens.index(word)]=self.dict[word].numpy()\n\t\t\tself.copied_word_embedding.weight.data.copy_(torch.from_numpy(pretrained_weight))\n\n\n\tdef unpack(self,bi_hidden, half_dim):\n\t\t#print(bi_hidden[0])\n\t\tfor i in range(bi_hidden.size(0)):\n\t\t\tvec=bi_hidden[i][:]\n\t\t\tif i==0:\n\t\t\t\th_fw=vec[:half_dim].view(1,-1)\n\t\t\t\th_bw=vec[half_dim:].view(1,-1)\n\t\t\telse:\n\t\t\t\th_fw_new = vec[:half_dim].view(1, -1)\n\t\t\t\th_bw_new = vec[half_dim:].view(1, -1)\n\t\t\t\th_fw=torch.cat((h_fw,h_fw_new),0)\n\t\t\t\th_bw=torch.cat((h_bw,h_bw_new),0)\n\t\t#print(h_fw.size())\n\t\t#print(h_fw[0])\n\t\t#print(h_bw[0])\n\t\t#sys.exit()\n\t\treturn (h_fw, h_bw)\n\n\tdef pairwise_word_interaction(self,out0,out1, target_A, target_B):\n\t\textra_loss=0\n\t\th_fw_0, h_bw_0 = self.unpack(out0.view(out0.size(0),out0.size(2)),half_dim=self.hidden_dim)\n\t\th_fw_1, h_bw_1 = self.unpack(out1.view(out1.size(0),out1.size(2)),half_dim=self.hidden_dim)\n\t\t#print(h_fw_0)\n\t\t#print(h_bw_0)\n\t\t#print(h_fw_1)\n\t\t#print(h_bw_1)\n\t\t#sys.exit()\n\t\th_bi_0 = out0.view(out0.size(0),out0.size(2))\n\t\th_bi_1 = out1.view(out1.size(0),out1.size(2))\n\t\th_sum_0 = h_fw_0 + h_bw_0\n\t\th_sum_1 = h_fw_1 + h_bw_1\n\t\tlen0 = h_fw_0.size(0)\n\t\tlen1 = h_fw_1.size(0)\n\t\ti=0\n\t\tj=0\n\t\t#simCube1 = torch.mm(h_fw_0[i].view(1, -1), h_fw_1[j].view(-1, 1))\n\t\t#simCube2 = torch.mm(h_bw_0[i].view(1, -1), h_bw_1[j].view(-1, 1))\n\t\t#simCube3 = torch.mm(h_bi_0[i].view(1, -1), h_bi_1[j].view(-1, 1))\n\t\t#simCube4 = torch.mm(h_sum_0[i].view(1, -1), h_sum_1[j].view(-1, 1))\n\t\t#simCube5 = F.pairwise_distance(h_fw_0[i].view(1, -1), h_fw_1[j].view(1, -1))\n\t\tsimCube5_0 = h_fw_0[i].view(1, -1)\n\t\tsimCube5_1 = h_fw_1[j].view(1, -1)\n\t\t#simCube6 = F.pairwise_distance(h_bw_0[i].view(1, -1), h_bw_1[j].view(1, -1))\n\t\tsimCube6_0 = h_bw_0[i].view(1, -1)\n\t\tsimCube6_1 = h_bw_1[j].view(1, -1)\n\t\t#simCube7 = F.pairwise_distance(h_bi_0[i].view(1, -1), h_bi_1[j].view(1, -1))\n\t\tsimCube7_0 = h_bi_0[i].view(1, -1)\n\t\tsimCube7_1 = h_bi_1[j].view(1, -1)\n\t\t#simCube8 = F.pairwise_distance(h_sum_0[i].view(1, -1), h_sum_1[j].view(1, -1))\n\t\tsimCube8_0 = h_sum_0[i].view(1,-1)\n\t\tsimCube8_1 = h_sum_1[j].view(1,-1)\n\t\t#simCube9 = F.cosine_similarity(h_fw_0[i].view(1, -1), h_fw_1[j].view(1, -1))\n\t\t#simCube10 = F.cosine_similarity(h_bw_0[i].view(1, -1), h_bw_1[j].view(1, -1))\n\t\t#simCube11 = F.cosine_similarity(h_bi_0[i].view(1, -1), h_bi_1[j].view(1, -1))\n\t\t#simCube12 = F.cosine_similarity(h_sum_0[i].view(1, -1), h_sum_1[j].view(1, -1))\n\t\tfor i in range(len0):\n\t\t\tfor j in range(len1):\n\t\t\t\tif not(i==0 and j==0):\n\t\t\t\t\tsimCube5_0 = torch.cat((simCube5_0, h_fw_0[i].view(1, -1)))\n\t\t\t\t\tsimCube5_1 = torch.cat((simCube5_1, h_fw_1[j].view(1, -1)))\n\t\t\t\t\tsimCube6_0 = torch.cat((simCube6_0, h_bw_0[i].view(1, -1)))\n\t\t\t\t\tsimCube6_1 = torch.cat((simCube6_1, h_bw_1[j].view(1, -1)))\n\t\t\t\t\tsimCube7_0 = torch.cat((simCube7_0, h_bi_0[i].view(1, -1)))\n\t\t\t\t\tsimCube7_1 = torch.cat((simCube7_1, h_bi_1[j].view(1, -1)))\n\t\t\t\t\tsimCube8_0 = torch.cat((simCube8_0, h_sum_0[i].view(1, -1)))\n\t\t\t\t\tsimCube8_1 = torch.cat((simCube8_1, h_sum_1[j].view(1, -1)))\n\t\tsimCube1 = torch.unsqueeze(torch.mm(h_fw_0, torch.transpose(h_fw_1, 0, 1)), 0)\n\t\tsimCube2 = torch.unsqueeze(torch.mm(h_bw_0, torch.transpose(h_bw_1, 0, 1)), 0)\n\t\tsimCube3 = torch.unsqueeze(torch.mm(h_bi_0, torch.transpose(h_bi_1, 0, 1)), 0)\n\t\tsimCube4 = torch.unsqueeze(torch.mm(h_sum_0, torch.transpose(h_sum_1, 0, 1)), 0)\n\t\tsimCube5 = torch.neg(F.pairwise_distance(simCube5_0, simCube5_1))\n\t\tsimCube5 = torch.unsqueeze(simCube5.view(len0, len1), 0)\n\t\tsimCube6 = torch.neg(F.pairwise_distance(simCube6_0, simCube6_1))\n\t\tsimCube6 = torch.unsqueeze(simCube6.view(len0, len1), 0)\n\t\tsimCube7 = torch.neg(F.pairwise_distance(simCube7_0, simCube7_1))\n\t\tsimCube7 = torch.unsqueeze(simCube7.view(len0, len1), 0)\n\t\tsimCube8 = torch.neg(F.pairwise_distance(simCube8_0,simCube8_1))\n\t\tsimCube8 = torch.unsqueeze(simCube8.view(len0, len1), 0)\n\n\t\tsimCube9 = F.cosine_similarity(simCube5_0,simCube5_1)\n\t\tsimCube9 = torch.unsqueeze(simCube9.view(len0,len1), 0)\n\t\tsimCube10 = F.cosine_similarity(simCube6_0, simCube6_1)\n\t\tsimCube10 = torch.unsqueeze(simCube10.view(len0, len1), 0)\n\t\tsimCube11 = F.cosine_similarity(simCube7_0, simCube7_1)\n\t\tsimCube11 = torch.unsqueeze(simCube11.view(len0, len1), 0)\n\t\tsimCube12 = F.cosine_similarity(simCube8_0, simCube8_1)\n\t\tsimCube12= torch.unsqueeze(simCube12.view(len0, len1), 0)\n\t\t''''''\n\t\tif torch.cuda.is_available():\n\t\t\tsimCube13 = torch.unsqueeze(Variable(torch.zeros(len0,len1)).cuda()+1,0)\n\t\telse:\n\t\t\tsimCube13 = torch.unsqueeze(Variable(torch.zeros(len0,len1))+1,0)\n\t\tsimCube=torch.cat((simCube9,simCube5,simCube1,simCube10,simCube6,simCube2,simCube12,simCube8,simCube4,simCube11,simCube7,simCube3,simCube13),0)\n\t\t#simCube=torch.unsqueeze(simCube,0)\n\t\t#simCube = F.pad(simCube, (0, self.limit - simCube.size(3), 0, self.limit - simCube.size(2)))[0]\n\t\t#print(simCube1)\n\t\t#print(simCube)\n\t\t#print(simCube8)\n\t\t#sys.exit()\n\t\treturn simCube, extra_loss\n\n\tdef similarity_focus(self,simCube):\n\t\tif torch.cuda.is_available():\n\t\t\tmask=torch.mul(torch.ones(simCube.size(0),simCube.size(1),simCube.size(2)).cuda(),0.1)\n\t\telse:\n\t\t\tmask=torch.mul(torch.ones(simCube.size(0),simCube.size(1),simCube.size(2)),0.1)\n\t\ts1tag=torch.zeros(simCube.size(1))\n\t\ts2tag=torch.zeros(simCube.size(2))\n\t\tsorted, indices=torch.sort(simCube[6].view(1,-1),descending=True)\n\t\trecord=[]\n\t\tfor indix in indices[0]:\n\t\t\tpos1=torch.div(indix,simCube.size(2)).data[0]\n\t\t\tpos2=(indix-simCube.size(2)*pos1).data[0]\n\t\t\tif s1tag[pos1]+s2tag[pos2]<=0:\n\t\t\t\ts1tag[pos1]=1\n\t\t\t\ts2tag[pos2]=1\n\t\t\t\trecord.append((pos1,pos2))\n\t\t\t\tmask[0][pos1][pos2] = mask[0][pos1][pos2] + 0.9\n\t\t\t\tmask[1][pos1][pos2] = mask[1][pos1][pos2] + 0.9\n\t\t\t\tmask[2][pos1][pos2] = mask[2][pos1][pos2] + 0.9\n\t\t\t\tmask[3][pos1][pos2] = mask[3][pos1][pos2] + 0.9\n\t\t\t\tmask[4][pos1][pos2] = mask[4][pos1][pos2] + 0.9\n\t\t\t\tmask[5][pos1][pos2] = mask[5][pos1][pos2] + 0.9\n\t\t\t\tmask[6][pos1][pos2] = mask[6][pos1][pos2] + 0.9\n\t\t\t\tmask[7][pos1][pos2] = mask[7][pos1][pos2] + 0.9\n\t\t\t\tmask[8][pos1][pos2] = mask[8][pos1][pos2] + 0.9\n\t\t\t\tmask[9][pos1][pos2] = mask[9][pos1][pos2] + 0.9\n\t\t\t\tmask[10][pos1][pos2] = mask[10][pos1][pos2] + 0.9\n\t\t\t\tmask[11][pos1][pos2] = mask[11][pos1][pos2] + 0.9\n\t\t\tmask[12][pos1][pos2] = mask[12][pos1][pos2] + 0.9\n\t\ts1tag = torch.zeros(simCube.size(1))\n\t\ts2tag = torch.zeros(simCube.size(2))\n\t\tsorted, indices = torch.sort(simCube[7].view(1, -1), descending=True)\n\t\tcounter=0\n\t\tfor indix in indices[0]:\n\t\t\tpos1 = torch.div(indix, simCube.size(2)).data[0]\n\t\t\tpos2 = (indix-simCube.size(2)*pos1).data[0]\n\t\t\tif s1tag[pos1] + s2tag[pos2] <= 0:\n\t\t\t\tcounter+=1\n\t\t\t\tif (pos1,pos2) in record:\n\t\t\t\t\tcontinue\n\t\t\t\telse:\n\t\t\t\t\ts1tag[pos1] = 1\n\t\t\t\t\ts2tag[pos2] = 1\n\t\t\t\t\t#record.append((pos1,pos2))\n\t\t\t\t\tmask[0][pos1][pos2] = mask[0][pos1][pos2] + 0.9\n\t\t\t\t\tmask[1][pos1][pos2] = mask[1][pos1][pos2] + 0.9\n\t\t\t\t\tmask[2][pos1][pos2] = mask[2][pos1][pos2] + 0.9\n\t\t\t\t\tmask[3][pos1][pos2] = mask[3][pos1][pos2] + 0.9\n\t\t\t\t\tmask[4][pos1][pos2] = mask[4][pos1][pos2] + 0.9\n\t\t\t\t\tmask[5][pos1][pos2] = mask[5][pos1][pos2] + 0.9\n\t\t\t\t\tmask[6][pos1][pos2] = mask[6][pos1][pos2] + 0.9\n\t\t\t\t\tmask[7][pos1][pos2] = mask[7][pos1][pos2] + 0.9\n\t\t\t\t\tmask[8][pos1][pos2] = mask[8][pos1][pos2] + 0.9\n\t\t\t\t\tmask[9][pos1][pos2] = mask[9][pos1][pos2] + 0.9\n\t\t\t\t\tmask[10][pos1][pos2] = mask[10][pos1][pos2] + 0.9\n\t\t\t\t\tmask[11][pos1][pos2] = mask[11][pos1][pos2] + 0.9\n\t\t\tif counter>=len(record):\n\t\t\t\tbreak\n\t\tfocusCube=torch.mul(simCube,Variable(mask))\n\t\treturn focusCube\n\n\tdef deep_cnn(self,focusCube):\n\t\tsimCube = torch.unsqueeze(focusCube, 0)\n\t\tfocusCube = F.pad(simCube, (0, self.limit - simCube.size(3), 0, self.limit - simCube.size(2)))[0]\n\t\tfocusCube=torch.unsqueeze(focusCube,0)\n\t\tout=self.layer1(focusCube)\n\t\tout=self.layer2(out)\n\t\tout=self.layer3(out)\n\t\tif self.limit==16:\n\t\t\tout=self.layer5(out)\n\t\telif self.limit==32:\n\t\t\tout = self.layer4(out)\n\t\t\tout=self.layer5(out)\n\t\telif self.limit==48:\n\t\t\tout = self.layer4(out)\n\t\t\tout=self.layer5_0(out)\n\t\t\t#out=self.layer5_1(out)\n\t\t#print('debug 6: (out size)')\n\t\t#print(out.size())\n\t\tout = out.view(out.size(0), -1)\n\t\tout = self.fc1(out)\n\t\tout = self.fc2(out)\n\t\t#print(out)\n\t\treturn out\n\n\tdef mlp(self,focusCube):\n\t\tsimCube = torch.unsqueeze(focusCube, 0)\n\t\tfocusCube = F.pad(simCube, (0, self.limit - simCube.size(3), 0, self.limit - simCube.size(2)))[0]\n\t\t#print(focusCube.view(-1))\n\t\tresult=self.mlp_layer(focusCube.view(-1))\n\t\t#sys.exit()\n\t\treturn result\n\n\tdef language_model(self,out0,out1, target_A, target_B):\n\t\textra_loss=0\n\t\th_fw_0, h_bw_0 = self.unpack(out0.view(out0.size(0),out0.size(2)),half_dim=self.hidden_dim)\n\t\th_fw_1, h_bw_1 = self.unpack(out1.view(out1.size(0),out1.size(2)),half_dim=self.hidden_dim)\n\t\t''''''\n\t\tm_fw_0=self.lm_tanh(torch.mm(h_fw_0, self.lm_Wm_forward))\n\t\tm_bw_0=self.lm_tanh(torch.mm(h_bw_0, self.lm_Wm_backward))\n\t\tm_fw_1=self.lm_tanh(torch.mm(h_fw_1, self.lm_Wm_forward))\n\t\tm_bw_1=self.lm_tanh(torch.mm(h_bw_1, self.lm_Wm_backward))\n\t\tq_fw_0=self.lm_softmax(torch.mm(m_fw_0, self.lm_Wq_forward))\n\t\tq_bw_0=self.lm_softmax(torch.mm(m_bw_0, self.lm_Wq_backword))\n\t\tq_fw_1 = self.lm_softmax(torch.mm(m_fw_1, self.lm_Wq_forward))\n\t\tq_bw_1 = self.lm_softmax(torch.mm(m_bw_1, self.lm_Wq_backword))\n\t\ttarget_fw_0=Variable(torch.LongTensor(target_A[1:]+[self.tokens.index('</s>')]))\n\t\ttarget_bw_0=Variable(torch.LongTensor([self.tokens.index('<s>')]+target_A[:-1]))\n\t\ttarget_fw_1=Variable(torch.LongTensor(target_B[1:]+[self.tokens.index('</s>')]))\n\t\ttarget_bw_1=Variable(torch.LongTensor([self.tokens.index('<s>')]+target_B[:-1]))\n\t\tif torch.cuda.is_available():\n\t\t\ttarget_fw_0=target_fw_0.cuda()\n\t\t\ttarget_bw_0=target_bw_0.cuda()\n\t\t\ttarget_fw_1=target_fw_1.cuda()\n\t\t\ttarget_bw_1=target_bw_1.cuda()\n\t\tloss1=self.lm_loss(q_fw_0, target_fw_0)\n\t\tloss2=self.lm_loss(q_bw_0, target_bw_0)\n\t\tloss3=self.lm_loss(q_fw_1, target_fw_1)\n\t\tloss4=self.lm_loss(q_bw_1, target_bw_1)\n\t\textra_loss=loss1+loss2+loss3+loss4\n\t\t''''''\n\t\treturn extra_loss\n\n\tdef word_layer(self, lsents, rsents):\n\t\tglove_mode=self.word_mode[0]\n\t\tupdate_inv_mode=self.word_mode[1]\n\t\tupdate_oov_mode=self.word_mode[2]\n\t\tif glove_mode==True and update_inv_mode==False and update_oov_mode==False:\n\t\t\ttry:\n\t\t\t\tsentA = torch.cat([self.dict[word].view(1, self.embedding_dim) for word in lsents], 0)\n\t\t\t\tsentA = Variable(sentA)  # .cuda()\n\t\t\t\tsentB = torch.cat([self.dict[word].view(1, self.embedding_dim) for word in rsents], 0)\n\t\t\t\tsentB = Variable(sentB)  # .cuda()\n\t\t\texcept:\n\t\t\t\tprint(lsents)\n\t\t\t\tprint(rsents)\n\t\t\t\tsys.exit()\n\t\t\tif torch.cuda.is_available():\n\t\t\t\tsentA = sentA.cuda()\n\t\t\t\tsentB = sentB.cuda()\n\t\telif glove_mode==True and update_inv_mode==False and update_oov_mode==True:\n\t\t\tfirstFlag=True\n\t\t\tfor word in lsents:\n\t\t\t\tif firstFlag:\n\t\t\t\t\tif word in self.oov:\n\t\t\t\t\t\tindice=Variable(torch.LongTensor([self.tokens.index(word)]))\n\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\tindice=indice.cuda()\n\t\t\t\t\t\toutput=self.word_embedding(indice)\n\t\t\t\t\t\tfirstFlag=False\n\t\t\t\t\telse:\n\t\t\t\t\t\toutput=Variable(self.dict[word].view(1, self.embedding_dim))\n\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\toutput=output.cuda()\n\t\t\t\t\t\tfirstFlag=False\n\t\t\t\telse:\n\t\t\t\t\tif word in self.oov:\n\t\t\t\t\t\tindice = Variable(torch.LongTensor([self.tokens.index(word)]))\n\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\tindice = indice.cuda()\n\t\t\t\t\t\toutput_new = self.word_embedding(indice)\n\t\t\t\t\t\toutput = torch.cat((output, output_new), 0)\n\t\t\t\t\telse:\n\t\t\t\t\t\toutput_new=Variable(self.dict[word].view(1, self.embedding_dim))\n\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\toutput_new=output_new.cuda()\n\t\t\t\t\t\toutput = torch.cat((output, output_new), 0)\n\t\t\tsentA=output\n\t\t\tfirstFlag = False\n\t\t\tfor word in rsents:\n\t\t\t\tif firstFlag:\n\t\t\t\t\tif word in self.oov:\n\t\t\t\t\t\tindice = Variable(torch.LongTensor([self.tokens.index(word)]))\n\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\tindice = indice.cuda()\n\t\t\t\t\t\toutput = self.word_embedding(indice)\n\t\t\t\t\t\tfirstFlag = False\n\t\t\t\t\telse:\n\t\t\t\t\t\toutput = Variable(self.dict[word].view(1, self.embedding_dim))\n\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\toutput = output.cuda()\n\t\t\t\t\t\tfirstFlag = False\n\t\t\t\telse:\n\t\t\t\t\tif word in self.oov:\n\t\t\t\t\t\tindice = Variable(torch.LongTensor([self.tokens.index(word)]))\n\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\tindice = indice.cuda()\n\t\t\t\t\t\toutput_new = self.word_embedding(indice)\n\t\t\t\t\t\toutput = torch.cat((output, output_new), 0)\n\t\t\t\t\telse:\n\t\t\t\t\t\toutput_new = Variable(self.dict[word].view(1, self.embedding_dim))\n\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\toutput_new = output_new.cuda()\n\t\t\t\t\t\toutput = torch.cat((output, output_new), 0)\n\t\t\tsentB = output\n\t\telif glove_mode==True and update_inv_mode==True and update_oov_mode==False:\n\t\t\tfirstFlag = True\n\t\t\tfor word in lsents:\n\t\t\t\tif firstFlag:\n\t\t\t\t\tif word in self.oov:\n\t\t\t\t\t\toutput = Variable(self.fake_dict[word].view(1, self.embedding_dim))\n\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\toutput = output.cuda()\n\t\t\t\t\t\toutput=output.view(1,-1)\n\t\t\t\t\t\tfirstFlag = False\n\t\t\t\t\telse:\n\t\t\t\t\t\tindice = Variable(torch.LongTensor([self.tokens.index(word)]))\n\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\tindice = indice.cuda()\n\t\t\t\t\t\toutput = self.copied_word_embedding(indice)\n\t\t\t\t\t\tfirstFlag = False\n\t\t\t\telse:\n\t\t\t\t\tif word in self.oov:\n\t\t\t\t\t\toutput_new=Variable(self.fake_dict[word].view(1, self.embedding_dim))\n\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\toutput_new = output_new.cuda()\n\t\t\t\t\t\toutput = torch.cat((output, output_new.view(1,-1)), 0)\n\t\t\t\t\telse:\n\t\t\t\t\t\tindice = Variable(torch.LongTensor([self.tokens.index(word)]))\n\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\tindice = indice.cuda()\n\t\t\t\t\t\toutput_new = self.copied_word_embedding(indice)\n\t\t\t\t\t\toutput = torch.cat((output, output_new), 0)\n\t\t\tsentA = output\n\t\t\tfirstFlag = True\n\t\t\tfor word in rsents:\n\t\t\t\tif firstFlag:\n\t\t\t\t\tif word in self.oov:\n\t\t\t\t\t\toutput = Variable(\n\t\t\t\t\t\t\ttorch.Tensor([random.uniform(-0.05, 0.05) for i in range(self.embedding_dim)]))\n\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\toutput = output.cuda()\n\t\t\t\t\t\toutput = output.view(1, -1)\n\t\t\t\t\t\tfirstFlag = False\n\t\t\t\t\telse:\n\t\t\t\t\t\tindice = Variable(torch.LongTensor([self.tokens.index(word)]))\n\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\tindice = indice.cuda()\n\t\t\t\t\t\toutput = self.copied_word_embedding(indice)\n\t\t\t\t\t\tfirstFlag = False\n\t\t\t\telse:\n\t\t\t\t\tif word in self.oov:\n\t\t\t\t\t\toutput_new = Variable(\n\t\t\t\t\t\t\ttorch.Tensor([random.uniform(-0.05, 0.05) for i in range(self.embedding_dim)]))\n\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\toutput_new = output_new.cuda()\n\t\t\t\t\t\toutput = torch.cat((output, output_new.view(1,-1)), 0)\n\t\t\t\t\telse:\n\t\t\t\t\t\tindice = Variable(torch.LongTensor([self.tokens.index(word)]))\n\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\tindice = indice.cuda()\n\t\t\t\t\t\toutput_new = self.copied_word_embedding(indice)\n\t\t\t\t\t\toutput = torch.cat((output, output_new), 0)\n\t\t\tsentB = output\n\t\telif glove_mode==True and update_inv_mode==True and update_oov_mode==True:\n\t\t\ttmp=[]\n\t\t\tfor word in lsents:\n\t\t\t\ttry:\n\t\t\t\t\ttmp.append(self.word2id[word])\n\t\t\t\texcept:\n\t\t\t\t\ttmp.append(self.word2id['oov'])\n\t\t\tindices = Variable(torch.LongTensor(tmp))\n\t\t\tif torch.cuda.is_available():\n\t\t\t\tindices = indices.cuda()\n\t\t\tsentA = self.copied_word_embedding(indices)\n\t\t\ttmp = []\n\t\t\tfor word in rsents:\n\t\t\t\ttry:\n\t\t\t\t\ttmp.append(self.word2id[word])\n\t\t\t\texcept:\n\t\t\t\t\ttmp.append(self.word2id['oov'])\n\t\t\tindices = Variable(torch.LongTensor(tmp))\n\t\t\tif torch.cuda.is_available():\n\t\t\t\tindices = indices.cuda()\n\t\t\tsentB = self.copied_word_embedding(indices)\n\t\telif glove_mode==False and update_inv_mode==False and update_oov_mode==False:\n\t\t\tfirstFlag = True\n\t\t\tfor word in lsents:\n\t\t\t\tif firstFlag:\n\t\t\t\t\toutput = Variable(self.fake_dict[word].view(1, self.embedding_dim))\n\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\toutput = output.cuda()\n\t\t\t\t\toutput = output.view(1, -1)\n\t\t\t\t\tfirstFlag = False\n\t\t\t\telse:\n\t\t\t\t\toutput_new = Variable(self.fake_dict[word].view(1, self.embedding_dim))\n\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\toutput_new = output_new.cuda()\n\t\t\t\t\toutput = torch.cat((output, output_new.view(1, -1)), 0)\n\t\t\tsentA = output\n\t\t\tfirstFlag = True\n\t\t\tfor word in rsents:\n\t\t\t\tif firstFlag:\n\t\t\t\t\toutput = Variable(self.fake_dict[word].view(1, self.embedding_dim))\n\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\toutput = output.cuda()\n\t\t\t\t\toutput = output.view(1, -1)\n\t\t\t\t\tfirstFlag = False\n\t\t\t\telse:\n\t\t\t\t\toutput_new = Variable(self.fake_dict[word].view(1, self.embedding_dim))\n\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\toutput_new = output_new.cuda()\n\t\t\t\t\toutput = torch.cat((output, output_new.view(1, -1)), 0)\n\t\t\tsentB = output\n\t\telif glove_mode==False and update_inv_mode==False and update_oov_mode==True:\n\t\t\tfirstFlag = True\n\t\t\tfor word in lsents:\n\t\t\t\tif firstFlag:\n\t\t\t\t\tif word in self.oov:\n\t\t\t\t\t\tindice = Variable(torch.LongTensor([self.tokens.index(word)]))\n\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\tindice = indice.cuda()\n\t\t\t\t\t\toutput = self.word_embedding(indice)\n\t\t\t\t\t\tfirstFlag = False\n\t\t\t\t\telse:\n\t\t\t\t\t\toutput = Variable(self.fake_dict[word].view(1, self.embedding_dim))\n\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\toutput = output.cuda()\n\t\t\t\t\t\toutput = output.view(1, -1)\n\t\t\t\t\t\tfirstFlag = False\n\t\t\t\telse:\n\t\t\t\t\tif word in self.oov:\n\t\t\t\t\t\tindice = Variable(torch.LongTensor([self.tokens.index(word)]))\n\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\tindice = indice.cuda()\n\t\t\t\t\t\toutput_new = self.word_embedding(indice)\n\t\t\t\t\t\toutput = torch.cat((output, output_new), 0)\n\t\t\t\t\telse:\n\t\t\t\t\t\toutput_new = Variable(self.fake_dict[word].view(1, self.embedding_dim))\n\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\toutput_new = output_new.cuda()\n\t\t\t\t\t\toutput_new = output_new.view(1,-1)\n\t\t\t\t\t\toutput = torch.cat((output, output_new), 0)\n\t\t\tsentA = output\n\t\t\tfirstFlag = True\n\t\t\tfor word in rsents:\n\t\t\t\tif firstFlag:\n\t\t\t\t\tif word in self.oov:\n\t\t\t\t\t\tindice = Variable(torch.LongTensor([self.tokens.index(word)]))\n\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\tindice = indice.cuda()\n\t\t\t\t\t\toutput = self.word_embedding(indice)\n\t\t\t\t\t\tfirstFlag = False\n\t\t\t\t\telse:\n\t\t\t\t\t\toutput = Variable(self.fake_dict[word].view(1, self.embedding_dim))\n\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\toutput = output.cuda()\n\t\t\t\t\t\toutput = output.view(1, -1)\n\t\t\t\t\t\tfirstFlag = False\n\t\t\t\telse:\n\t\t\t\t\tif word in self.oov:\n\t\t\t\t\t\tindice = Variable(torch.LongTensor([self.tokens.index(word)]))\n\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\tindice = indice.cuda()\n\t\t\t\t\t\toutput_new = self.word_embedding(indice)\n\t\t\t\t\t\toutput = torch.cat((output, output_new), 0)\n\t\t\t\t\telse:\n\t\t\t\t\t\toutput_new = Variable(self.fake_dict[word].view(1, self.embedding_dim))\n\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\toutput_new = output_new.cuda()\n\t\t\t\t\t\toutput_new = output_new.view(1, -1)\n\t\t\t\t\t\toutput = torch.cat((output, output_new), 0)\n\t\t\tsentB = output\n\t\telif glove_mode==False and update_inv_mode==True and update_oov_mode==False:\n\t\t\tfirstFlag = True\n\t\t\tfor word in lsents:\n\t\t\t\tif firstFlag:\n\t\t\t\t\tif word in self.oov:\n\t\t\t\t\t\toutput = Variable(self.dict[word].view(1, self.embedding_dim))\n\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\toutput = output.cuda()\n\t\t\t\t\t\toutput = output.view(1, -1)\n\t\t\t\t\t\tfirstFlag = False\n\t\t\t\t\telse:\n\t\t\t\t\t\tindice = Variable(torch.LongTensor([self.tokens.index(word)]))\n\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\tindice = indice.cuda()\n\t\t\t\t\t\toutput = self.word_embedding(indice)\n\t\t\t\t\t\tfirstFlag = False\n\t\t\t\telse:\n\t\t\t\t\tif word in self.oov:\n\t\t\t\t\t\toutput_new = Variable(torch.Tensor(self.dict[word].view(1, self.embedding_dim)))\n\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\toutput_new = output_new.cuda()\n\t\t\t\t\t\toutput = torch.cat((output, output_new.view(1, -1)), 0)\n\t\t\t\t\telse:\n\t\t\t\t\t\tindice = Variable(torch.LongTensor([self.tokens.index(word)]))\n\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\tindice = indice.cuda()\n\t\t\t\t\t\toutput_new = self.word_embedding(indice)\n\t\t\t\t\t\toutput = torch.cat((output, output_new.view(1, -1)), 0)\n\t\t\tsentA = output\n\t\t\tfirstFlag = True\n\t\t\tfor word in rsents:\n\t\t\t\tif firstFlag:\n\t\t\t\t\tif word in self.oov:\n\t\t\t\t\t\toutput = Variable(\n\t\t\t\t\t\t\ttorch.Tensor([random.uniform(-0.05, 0.05) for i in range(self.embedding_dim)]))\n\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\toutput = output.cuda()\n\t\t\t\t\t\toutput = output.view(1, -1)\n\t\t\t\t\t\tfirstFlag = False\n\t\t\t\t\telse:\n\t\t\t\t\t\tindice = Variable(torch.LongTensor([self.tokens.index(word)]))\n\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\tindice = indice.cuda()\n\t\t\t\t\t\toutput = self.word_embedding(indice)\n\t\t\t\t\t\tfirstFlag = False\n\t\t\t\telse:\n\t\t\t\t\tif word in self.oov:\n\t\t\t\t\t\toutput_new = Variable(\n\t\t\t\t\t\t\ttorch.Tensor([random.uniform(-0.05, 0.05) for i in range(self.embedding_dim)]))\n\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\toutput_new = output_new.cuda()\n\t\t\t\t\t\toutput = torch.cat((output, output_new.view(1, -1)), 0)\n\t\t\t\t\telse:\n\t\t\t\t\t\tindice = Variable(torch.LongTensor([self.tokens.index(word)]))\n\t\t\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\t\t\tindice = indice.cuda()\n\t\t\t\t\t\toutput_new = self.word_embedding(indice)\n\t\t\t\t\t\toutput = torch.cat((output, output_new.view(1, -1)), 0)\n\t\t\tsentB = output\n\t\telif glove_mode==False and update_inv_mode==True and update_oov_mode==True:\n\t\t\tindices=Variable(torch.LongTensor([self.tokens.index(word) for word in lsents]))\n\t\t\t#print(indices)\n\t\t\tif torch.cuda.is_available():\n\t\t\t\tindices=indices.cuda()\n\t\t\tsentA=self.word_embedding(indices)\n\t\t\tindices = Variable(torch.LongTensor([self.tokens.index(word) for word in rsents]))\n\t\t\t#print(indices)\n\t\t\tif torch.cuda.is_available():\n\t\t\t\tindices = indices.cuda()\n\t\t\tsentB = self.word_embedding(indices)\n\t\tsentA = torch.unsqueeze(sentA, 0).view(-1, 1, self.embedding_dim)\n\t\tsentB = torch.unsqueeze(sentB, 0).view(-1, 1, self.embedding_dim)\n\t\treturn (sentA, sentB)\n\n\tdef c2w_cell(self,indices,h,c):\n\t\tinput=Variable(torch.LongTensor(indices))\n\t\tif torch.cuda.is_available():\n\t\t\tinput = input.cuda()\n\t\tinput=self.c2w_embedding(input)\n\t\tinput=input.view(-1,1,50)\n\t\tout,(state,_)=self.lstm_c2w(input,(h,c))\n\t\toutput_char = torch.mm(self.df, state[0][0][:].view(-1, 1)) + torch.mm(self.db, state[1][0][:].view(-1, 1)) + self.bias.view(-1, 1)\n\t\toutput_char = output_char.view(1, -1)\n\t\treturn output_char\n\n\tdef charCNN_cell(self,indices):\n\t\tinput = Variable(torch.LongTensor(indices))\n\t\tif torch.cuda.is_available():\n\t\t\tinput = input.cuda()\n\t\tinput = self.char_cnn_embedding(input)\n\t\tinput=torch.unsqueeze(input,0)\n\t\tout1 = self.charCNN_filter1(input)\n\t\tout2 = self.charCNN_filter2(input)\n\t\tout3 = self.charCNN_filter3(input)\n\t\tout4 = self.charCNN_filter4(input)\n\t\tout5 = self.charCNN_filter5(input)\n\t\tout6 = self.charCNN_filter6(input)\n\t\tout7 = self.charCNN_filter7(input)\n\t\tfinal_output=torch.cat([torch.squeeze(out1),torch.squeeze(out2),torch.squeeze(out3),torch.squeeze(out4),torch.squeeze(out5),torch.squeeze(out6),torch.squeeze(out7)])\n\t\tfinal_output=final_output.view(1,-1)\n\t\ttransform_gate=self.transform_gate(final_output)\n\t\tfinal_output=transform_gate * self.char_cnn_mlp(final_output)+(1-transform_gate) * final_output\n\t\treturn final_output\n\n\tdef generate_word_indices(self,word):\n\t\tif self.task=='hindi':\n\t\t\t#char_gram = syllabifier.orthographic_syllabify(word, 'hi')\n\t\t\tchar_gram = list(splitclusters(word))\n\t\t\tindices=[]\n\t\t\tif self.character_ngrams == 1:\n\t\t\t\tindices=[self.dict_char_ngram[char] for char in char_gram]\n\t\t\telif self.character_ngrams == 2:\n\t\t\t\tif self.character_ngrams_overlap:\n\t\t\t\t\tif len(char_gram) <= 2:\n\t\t\t\t\t\tindices = [self.dict_char_ngram[word]]\n\t\t\t\t\telse:\n\t\t\t\t\t\tfor i in range(len(char_gram) - 1):\n\t\t\t\t\t\t\tindices.append(self.dict_char_ngram[char_gram[i]+char_gram[i+1]])\n\t\t\t\telse:\n\t\t\t\t\tif len(char_gram) <= 2:\n\t\t\t\t\t\tindices = [self.dict_char_ngram[word]]\n\t\t\t\t\telse:\n\t\t\t\t\t\tfor i in range(0, len(char_gram) - 1, 2):\n\t\t\t\t\t\t\tindices.append(self.dict_char_ngram[char_gram[i] + char_gram[i + 1]])\n\t\t\t\t\t\tif len(char_gram)%2==1:\n\t\t\t\t\t\t\tindices.append(self.dict_char_ngram[char_gram[len(char_gram)-1]])\n\t\telse:\n\t\t\tindices=[]\n\t\t\tif self.character_ngrams == 1:\n\t\t\t\tfor char in word:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tindices.append(self.dict_char_ngram[char])\n\t\t\t\t\texcept:\n\t\t\t\t\t\tcontinue\n\t\t\t\t#indices = [self.dict_char_ngram[char] for char in word]\n\t\t\telif self.character_ngrams == 2:\n\t\t\t\tif self.character_ngrams_overlap:\n\t\t\t\t\tif len(word) <= 2:\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tindices = [self.dict_char_ngram[word]]\n\t\t\t\t\t\texcept:\n\t\t\t\t\t\t\tindices = [self.dict_char_ngram[' ']]\n\t\t\t\t\telse:\n\t\t\t\t\t\tfor i in range(len(word) - 1):\n\t\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\t\tindices.append(self.dict_char_ngram[word[i:i + 2]])\n\t\t\t\t\t\t\texcept:\n\t\t\t\t\t\t\t\tindices.append(self.dict_char_ngram[' '])\n\t\t\t\telse:\n\t\t\t\t\tif len(word) <= 2:\n\t\t\t\t\t\tindices = [self.dict_char_ngram[word]]\n\t\t\t\t\telse:\n\t\t\t\t\t\tfor i in range(0, len(word) - 1, 2):\n\t\t\t\t\t\t\tindices.append(self.dict_char_ngram[word[i:i + 2]])\n\t\t\t\t\t\tif len(word) % 2 == 1:\n\t\t\t\t\t\t\tindices.append(self.dict_char_ngram[word[len(word) - 1]])\n\t\t\telif self.character_ngrams == 3:\n\t\t\t\tif self.character_ngrams_overlap:\n\t\t\t\t\tif len(word) <= 3:\n\t\t\t\t\t\tindices = [self.dict_char_ngram[word]]\n\t\t\t\t\telse:\n\t\t\t\t\t\tfor i in range(len(word) - 2):\n\t\t\t\t\t\t\tindices.append(self.dict_char_ngram[word[i:i + 3]])\n\t\t\t\telse:\n\t\t\t\t\tif len(word) <= 3:\n\t\t\t\t\t\tindices = [self.dict_char_ngram[word]]\n\t\t\t\t\telse:\n\t\t\t\t\t\tfor i in range(0, len(word) - 2, 3):\n\t\t\t\t\t\t\tindices.append(self.dict_char_ngram[word[i:i + 3]])\n\t\t\t\t\t\tif len(word) % 3 == 1:\n\t\t\t\t\t\t\tindices.append(self.dict_char_ngram[word[len(word) - 1]])\n\t\t\t\t\t\telif len(word) % 3 == 2:\n\t\t\t\t\t\t\tindices.append(self.dict_char_ngram[word[len(word) - 2:]])\n\t\treturn indices\n\n\tdef c2w_or_cnn_layer(self,lsents, rsents):\n\t\th = Variable(torch.zeros(2, 1, self.embedding_dim))  # 2 for bidirection\n\t\tc = Variable(torch.zeros(2, 1, self.embedding_dim))\n\t\tif torch.cuda.is_available():\n\t\t\th=h.cuda()\n\t\t\tc=c.cuda()\n\t\tfirstFlag=True\n\t\tfor word in lsents:\n\t\t\tindices=self.generate_word_indices(word)\n\t\t\tif not self.c2w_mode:\n\t\t\t\tif len(indices)<20:\n\t\t\t\t\tindices = indices + [0 for i in range(self.charcnn_max_word_length - len(indices))]\n\t\t\t\telse:\n\t\t\t\t\tindices = indices[0:20]\n\t\t\tif firstFlag:\n\t\t\t\tif self.c2w_mode:\n\t\t\t\t\toutput=self.c2w_cell([indices], h, c)\n\t\t\t\telse:\n\t\t\t\t\toutput=self.charCNN_cell([indices])\n\t\t\t\tfirstFlag=False\n\t\t\telse:\n\t\t\t\tif self.c2w_mode:\n\t\t\t\t\toutput_new=self.c2w_cell([indices], h, c)\n\t\t\t\telse:\n\t\t\t\t\toutput_new=self.charCNN_cell([indices])\n\t\t\t\toutput=torch.cat((output,output_new),0)\n\t\t#print(output)\n\t\t#sys.exit()\n\t\tsentA=output\n\t\tfirstFlag = True\n\t\tfor word in rsents:\n\t\t\t# print word\n\t\t\tindices = self.generate_word_indices(word)\n\t\t\tif not self.c2w_mode:\n\t\t\t\tif len(indices)<20:\n\t\t\t\t\tindices = indices + [0 for i in range(self.charcnn_max_word_length - len(indices))]\n\t\t\t\telse:\n\t\t\t\t\tindices = indices[0:20]\n\t\t\t# print(indices)\n\t\t\tif firstFlag:\n\t\t\t\tif self.c2w_mode:\n\t\t\t\t\toutput=self.c2w_cell([indices], h, c)\n\t\t\t\telse:\n\t\t\t\t\toutput=self.charCNN_cell([indices])\n\t\t\t\tfirstFlag = False\n\t\t\telse:\n\t\t\t\tif self.c2w_mode:\n\t\t\t\t\toutput_new=self.c2w_cell([indices], h, c)\n\t\t\t\telse:\n\t\t\t\t\toutput_new=self.charCNN_cell([indices])\n\t\t\t\toutput = torch.cat((output, output_new), 0)\n\t\t#print(output)\n\t\t#sys.exit()\n\t\tsentB=output\n\t\tsentA = torch.unsqueeze(sentA, 0).view(-1, 1, self.embedding_dim)\n\t\tsentB = torch.unsqueeze(sentB, 0).view(-1, 1, self.embedding_dim)\n\t\treturn (sentA,sentB)\n\n\tdef mix_cell(self,word, output_word, output_char):\n\t\tresult=None\n\t\textra_loss=0\n\t\tindices_reduce_dim = Variable(torch.LongTensor([i * 2 for i in range(self.embedding_dim)]))\n\t\tif torch.cuda.is_available():\n\t\t\tindices_reduce_dim=indices_reduce_dim.cuda()\n\t\tif self.combine_mode == 'concat':\n\t\t\tresult = torch.cat((output_word, output_char), 1)\n\t\t\tresult = torch.index_select(result, 1, indices_reduce_dim)\n\t\telif self.combine_mode == 'g_0.25':\n\t\t\tresult = 0.25 * output_word + 0.75 * output_char\n\t\telif self.combine_mode == 'g_0.50':\n\t\t\tresult = 0.5 * output_word + 0.5 * output_char\n\t\telif self.combine_mode == 'g_0.75':\n\t\t\tresult = 0.75 * output_word + 0.25 * output_char\n\t\telif self.combine_mode == 'adaptive':\n\t\t\tgate = self.sigmoid(torch.mm(output_word, self.vg) + self.bg)\n\t\t\tgate=gate.expand(1,self.embedding_dim)\n\t\t\tresult = (1-gate) * output_word + gate * output_char\n\t\telif self.combine_mode == 'attention':\n\t\t\tgate = self.sigmoid(torch.mm(self.tanh(torch.mm(output_word, self.W1) + torch.mm(output_char, self.W2)), self.W3))\n\t\t\tresult = gate*output_word+(1-gate)*output_char\n\t\t\tif word not in self.oov:\n\t\t\t\textra_loss+=(1-F.cosine_similarity(output_word,output_char))\n\t\telif self.combine_mode == 'backoff':\n\t\t\tif word in self.oov:\n\t\t\t\tresult=output_char\n\t\t\telse:\n\t\t\t\tresult=output_word\n\t\treturn (result, extra_loss)\n\n\tdef mix_layer(self,lsents,rsents):\n\t\th = Variable(torch.zeros(2, 1, self.embedding_dim))  # 2 for bidirection\n\t\tc = Variable(torch.zeros(2, 1, self.embedding_dim))\n\t\tif torch.cuda.is_available():\n\t\t\th=h.cuda()\n\t\t\tc=c.cuda()\n\t\tfirstFlag = True\n\t\t#if (index + 1) % (int(42200 / 4)) == 0:\n\t\t#\textra_loss=0\n\t\t#else:\n\t\t#\textra_loss=self.language_model(index, h,c)\n\t\textra_loss=0\n\t\t# print(lsents)\n\t\t# print(rsents)\n\t\t# sys.exit()\n\t\tfor word in lsents:\n\t\t\tindices = self.generate_word_indices(word)\n\t\t\tif self.c2w_mode:\n\t\t\t\toutput_char = self.c2w_cell([indices], h, c)\n\t\t\telse:\n\t\t\t\tif len(indices) < 20:\n\t\t\t\t\tindices = indices + [0 for i in range(self.charcnn_max_word_length - len(indices))]\n\t\t\t\telse:\n\t\t\t\t\tindices = indices[0:20]\n\t\t\t\toutput_char = self.charCNN_cell([indices])\n\t\t\t\tif self.task=='sts':\n\t\t\t\t\toutput_char=self.down_sampling_300(output_char)\n\t\t\t\telse:\n\t\t\t\t\toutput_char=self.down_sampling_200(output_char)\n\t\t\t#indice=Variable(torch.LongTensor([self.tokens.index(word)]))\n\t\t\t#if torch.cuda.is_available():\n\t\t\t#\tindice=indice.cuda()\n\t\t\t#output_word = self.copied_word_embedding(indice).view(1,-1)\n\t\t\toutput_word = Variable(torch.Tensor(self.dict[word])).view(1,-1)\n\t\t\tif torch.cuda.is_available():\n\t\t\t\toutput_word = output_word.cuda()\n\t\t\tif firstFlag:\n\t\t\t\toutput, extra_loss=self.mix_cell(word, output_word, output_char)\n\t\t\t\toutput2 = output_char\n\t\t\t\tfirstFlag = False\n\t\t\telse:\n\t\t\t\toutput_new, extra_loss = self.mix_cell(word, output_word, output_char)\n\t\t\t\toutput_new2 = output_char\n\t\t\t\toutput = torch.cat((output, output_new), 0)\n\t\t\t\toutput2 = torch.cat((output2, output_new2), 0)\n\t\tsentA = output\n\t\tsentA2 = output2\n\t\tfirstFlag=True\n\t\tfor word in rsents:\n\t\t\tindices = self.generate_word_indices(word)\n\t\t\tif self.c2w_mode:\n\t\t\t\toutput_char = self.c2w_cell([indices], h, c)\n\t\t\telse:\n\t\t\t\tif len(indices) < 20:\n\t\t\t\t\tindices = indices + [0 for i in range(self.charcnn_max_word_length - len(indices))]\n\t\t\t\telse:\n\t\t\t\t\tindices = indices[0:20]\n\t\t\t\toutput_char = self.charCNN_cell([indices])\n\t\t\t\tif self.task=='sts':\n\t\t\t\t\toutput_char=self.down_sampling_300(output_char)\n\t\t\t\telse:\n\t\t\t\t\toutput_char=self.down_sampling_200(output_char)\n\t\t\t#indice = Variable(torch.LongTensor([self.tokens.index(word)]))\n\t\t\t#if torch.cuda.is_available():\n\t\t\t#\tindice = indice.cuda()\n\t\t\t#output_word = self.copied_word_embedding(indice).view(1, -1)\n\t\t\toutput_word = Variable(torch.Tensor(self.dict[word])).view(1,-1)\n\t\t\tif torch.cuda.is_available():\n\t\t\t\toutput_word = output_word.cuda()\n\t\t\tif firstFlag:\n\t\t\t\toutput, extra_loss=self.mix_cell(word, output_word, output_char)\n\t\t\t\toutput2 = output_char\n\t\t\t\tfirstFlag = False\n\t\t\telse:\n\t\t\t\toutput_new, extra_loss = self.mix_cell(word, output_word, output_char)\n\t\t\t\toutput_new2 = output_char\n\t\t\t\toutput = torch.cat((output, output_new), 0)\n\t\t\t\toutput2 = torch.cat((output2, output_new2), 0)\n\t\tsentB = output\n\t\tsentB2 = output2\n\t\tsentA = torch.unsqueeze(sentA, 0).view(-1, 1, self.embedding_dim)#*2)\n\t\tsentB = torch.unsqueeze(sentB, 0).view(-1, 1, self.embedding_dim)#*2)\n\t\tsentA2 = torch.unsqueeze(sentA2, 0).view(-1, 1, self.embedding_dim)  # *2)\n\t\tsentB2 = torch.unsqueeze(sentB2, 0).view(-1, 1, self.embedding_dim)  # *2)\n\t\treturn (sentA, sentA2, sentB, sentB2, extra_loss)\n\n\tdef forward(self,input_A, input_B, index):\n\t\textra_loss1=0\n\t\textra_loss2=0\n\t\traw_input_A=input_A\n\t\traw_input_B=input_B\n\t\tif torch.cuda.is_available():\n\t\t\th0 = Variable(torch.zeros(self.num_layers * 2, 1, self.hidden_dim)).cuda() # 2 for bidirection\n\t\t\t#initial_h0 = Variable(torch.cat((input_A.data[0][0].view(1,300),input_A.data[-1][0].view(1,300)),0).view(2,1,300)).cuda()\n\t\t\t#initial_c0 = Variable(torch.cat((input_A.data[0][0].view(1,300),input_A.data[-1][0].view(1,300)),0).view(2,1,300)).cuda()\n\t\t\tc0 = Variable(torch.zeros(self.num_layers * 2, 1, self.hidden_dim)).cuda()\n\t\telse:\n\t\t\th0 = Variable(torch.zeros(self.num_layers * 2, 1, self.hidden_dim))\n\t\t\tc0 = Variable(torch.zeros(self.num_layers * 2, 1, self.hidden_dim))\n\t\tif self.granularity=='word':\n\t\t\tinput_A, input_B = self.word_layer(input_A, input_B)\n\t\telif self.granularity=='char':\n\t\t\tinput_A, input_B=self.c2w_or_cnn_layer(input_A,input_B)\n\t\t\tif self.lm_mode:\n\t\t\t\ttarget_A = []  # [self.tokens.index(word) for word in input_A]\n\t\t\t\tfor word in raw_input_A:\n\t\t\t\t\tif self.word_freq[word] >= 4:\n\t\t\t\t\t\ttarget_A.append(self.tokens.index(word))\n\t\t\t\t\telse:\n\t\t\t\t\t\ttarget_A.append(self.tokens.index('oov'))\n\t\t\t\ttarget_B = []  # [self.tokens.index(word) for word in input_B]\n\t\t\t\tfor word in raw_input_B:\n\t\t\t\t\tif self.word_freq[word] >= 4:\n\t\t\t\t\t\ttarget_B.append(self.tokens.index(word))\n\t\t\t\t\telse:\n\t\t\t\t\t\ttarget_B.append(self.tokens.index('oov'))\n\t\t\t\tlm_out0, _ = self.lm_lstm(input_A, (h0, c0))\n\t\t\t\tlm_out1, _ = self.lm_lstm(input_B, (h0, c0))\n\t\t\t\textra_loss2 = self.language_model(lm_out0, lm_out1, target_A, target_B)\n\t\telif self.granularity=='mix':\n\t\t\tinput_A, input_A2, input_B, input_B2, extra_loss1 = self.mix_layer(input_A, input_B)\n\t\t\tif self.lm_mode:\n\t\t\t\ttarget_A = []  # [self.tokens.index(word) for word in input_A]\n\t\t\t\tfor word in raw_input_A:\n\t\t\t\t\tif self.word_freq[word] >= 4:\n\t\t\t\t\t\ttarget_A.append(self.tokens.index(word))\n\t\t\t\t\telse:\n\t\t\t\t\t\ttarget_A.append(self.tokens.index('oov'))\n\t\t\t\ttarget_B = []  # [self.tokens.index(word) for word in input_B]\n\t\t\t\tfor word in raw_input_B:\n\t\t\t\t\tif self.word_freq[word] >= 4:\n\t\t\t\t\t\ttarget_B.append(self.tokens.index(word))\n\t\t\t\t\telse:\n\t\t\t\t\t\ttarget_B.append(self.tokens.index('oov'))\n\t\t\t\tlm_out0, _ = self.lm_lstm(input_A2, (h0, c0))\n\t\t\t\tlm_out1, _ = self.lm_lstm(input_B2, (h0, c0))\n\t\t\t\textra_loss2 = self.language_model(lm_out0, lm_out1, target_A, target_B)\n\n\t\tout0, (state0,_) = self.lstm(input_A, (h0, c0))\n\t\tout1, (state1,_) = self.lstm(input_B, (h0, c0))\n\t\tsimCube, _=self.pairwise_word_interaction(out0,out1, target_A=None, target_B=None)\n\t\tfocusCube=self.similarity_focus(simCube)\n\t\tif self.deep_CNN:\n\t\t\toutput = self.deep_cnn(focusCube)\n\t\telse:\n\t\t\toutput = self.mlp(focusCube)\n\t\toutput=output.view(1,2)\n\t\treturn (output,extra_loss1+ extra_loss2)\n\n"""
PWIM/util.py,0,"b'from __future__ import division\nimport sys\nimport numpy as np\nfrom numpy import linalg as LA\nimport math\nimport os\nimport torch\nimport unicodedata\nfrom os.path import expanduser\nfrom collections import *\n\ndef splitclusters(s):\n    """"""Generate the grapheme clusters for the string s. (Not the full\n    Unicode text segmentation algorithm, but probably good enough for\n    Devanagari.)\n\n    """"""\n    virama = u\'\\N{DEVANAGARI SIGN VIRAMA}\'\n    cluster = u\'\'\n    last = None\n    for c in s:\n        cat = unicodedata.category(c)[0]\n        if cat == \'M\' or cat == \'L\' and last == virama:\n            cluster += c\n        else:\n            if cluster:\n                yield cluster\n            cluster = c\n        last = c\n    if cluster:\n        yield cluster\n\ndef jaccard_index(set_1, set_2):\n    return len(set_1.intersection(set_2)) / float(len(set_1.union(set_2)))\n\ndef PINC(source,candidate,N=3):\n    tokenize_source=[char for char in source]\n    tokenize_candidate=[char for char in candidate]\n    if len(tokenize_candidate)<N or len(tokenize_source)<N:\n        return 1.0\n    sum=0\n    for i in range(N):\n        n_gram_s=[]\n        n_gram_c=[]\n        for k in range(len(tokenize_source)-i):\n            n_gram_s.append(tokenize_source[k:k+i+1])\n        for k in range(len(tokenize_candidate)-i):\n            n_gram_c.append(tokenize_candidate[k:k+i+1])\n        #print(n_gram_s)\n        #print(n_gram_c)\n        counter=0\n        for element in n_gram_s:\n            if element in n_gram_c:\n                counter+=1\n        sum+=float(counter)/len(n_gram_c)\n        #print(counter)\n        #print(sum/N)\n    return sum/N\n\ndef intersect(list1, list2):\n    cnt1 = Counter()\n    cnt2 = Counter()\n    for tk1 in list1:\n        cnt1[tk1] += 1\n    for tk2 in list2:\n        cnt2[tk2] += 1\n    inter = cnt1 & cnt2\n    return list(inter.elements())\n\ndef ngram_overlap_features(s_word, t_word):\n\ts1grams = [w for w in s_word]\n\tt1grams = [w for w in t_word]\n\ts2grams = []\n\tt2grams = []\n\ts3grams = []\n\tt3grams = []\n\tfor i in range(0, len(s1grams) - 1):\n\t\tif i < len(s1grams) - 1:\n\t\t\ts2gram = s1grams[i] + "" "" + s1grams[i + 1]\n\t\t\ts2grams.append(s2gram)\n\t\tif i < len(s1grams) - 2:\n\t\t\ts3gram = s1grams[i] + "" "" + s1grams[i + 1] + "" "" + s1grams[i + 2]\n\t\t\ts3grams.append(s3gram)\n\n\tfor i in range(0, len(t1grams) - 1):\n\t\tif i < len(t1grams) - 1:\n\t\t\tt2gram = t1grams[i] + "" "" + t1grams[i + 1]\n\t\t\tt2grams.append(t2gram)\n\t\tif i < len(t1grams) - 2:\n\t\t\tt3gram = t1grams[i] + "" "" + t1grams[i + 1] + "" "" + t1grams[i + 2]\n\t\t\tt3grams.append(t3gram)\n\n\tf1gram = 0\n\tprecision1gram = len(set(intersect(s1grams, t1grams))) / len(set(s1grams))\n\trecall1gram = len(set(intersect(s1grams, t1grams))) / len(set(t1grams))\n\tif (precision1gram + recall1gram) > 0:\n\t\tf1gram = 2 * precision1gram * recall1gram / (precision1gram + recall1gram)\n\tif len(set(s2grams)) > 0 and len(set(t2grams)) > 0:\n\t\tprecision2gram = len(set(intersect(s2grams, t2grams))) / len(set(s2grams))\n\t\trecall2gram = len(set(intersect(s2grams, t2grams))) / len(set(t2grams))\n\telse:\n\t\tprecision2gram = 0\n\t\trecall2gram = 0\n\tf2gram = 0\n\tif (precision2gram + recall2gram) > 0:\n\t\tf2gram = 2 * precision1gram * recall2gram / (precision2gram + recall2gram)\n\tif len(set(s3grams)) > 0 and len(set(t3grams)):\n\t\tprecision3gram = len(set(intersect(s3grams, t3grams))) / len(set(s3grams))\n\t\trecall3gram = len(set(intersect(s3grams, t3grams))) / len(set(t3grams))\n\telse:\n\t\tprecision3gram = 0\n\t\trecall3gram = 0\n\tf3gram = 0\n\tif (precision3gram + recall3gram) > 0:\n\t\tf3gram = 2 * precision3gram * recall3gram / (precision3gram + recall3gram)\n\tfeatures=[f1gram, precision1gram, recall1gram, f2gram, precision2gram, recall2gram, f3gram, precision3gram, recall3gram]\n\treturn features\n\ndef pearson(x,y):\n\tx=np.array(x)\n\ty=np.array(y)\n\tx=x-np.mean(x)\n\ty=y-np.mean(y)\n\treturn x.dot(y)/(LA.norm(x)*LA.norm(y))\n\ndef readURLdata(dir,granularity):\n\tlsents = []\n\trsents = []\n\tlabels = []\n\tif granularity==\'word\' or granularity ==\'mix\' or granularity == \'char\' or granularity==\'multi-task\':\n\t\tfor line in open(dir + \'a.toks\'):\n\t\t\tpieces = line.strip().split()\n\t\t\tlsents.append(pieces)\n\t\tfor line in open(dir + \'b.toks\'):\n\t\t\tpieces = line.strip().split()\n\t\t\trsents.append(pieces)\n\t\tfor line in open(dir + \'sim.txt\'):\n\t\t\tpieces = line.strip().split()\n\t\t\tlabels.append([int(item) for item in pieces])\n\t\'\'\'\n\telif granularity==\'char\':\n\t\tif \'train\' in dir:\n\t\t\tfilename=\'Twitter_URL_Corpus_train.txt\'\n\t\telse:\n\t\t\tfilename=\'Twitter_URL_Corpus_test.txt\'#\'new_testing_5136.txt\'\n\t\tfor line in open(dir+filename):\n\t\t\tif len(line.strip().split(\'\\t\')) == 4:\n\t\t\t\ta, b, sim, url = line.strip().split(\'\\t\')\n\t\t\t\tscore, _ = eval(sim)\n\t\t\t\tif score >= 4:\n\t\t\t\t\tscore = 1\n\t\t\t\telif score <= 2:\n\t\t\t\t\tscore = 0\n\t\t\t\tif int(score) != 3:\n\t\t\t\t\tlsents.append(a.split())\n\t\t\t\t\trsents.append(b.split())\n\t\t\t\t\tlabels.append([int(score)])\n\t\'\'\'\n\tdata = (lsents, rsents, labels)\n\tif not len(lsents) == len(rsents) == len(labels):\n\t\tprint(\'error!\')\n\t\tsys.exit()\n\treturn data\n\ndef URL_maxF1_eval(predict_result,test_data_label):\n\ttest_data_label=[item>=1 for item in test_data_label]\n\tcounter = 0\n\ttp = 0.0\n\tfp = 0.0\n\tfn = 0.0\n\ttn = 0.0\n\n\tfor i, t in enumerate(predict_result):\n\n\t\tif t>0.5:\n\t\t\tguess=True\n\t\telse:\n\t\t\tguess=False\n\t\tlabel = test_data_label[i]\n\t\t#print guess, label\n\t\tif guess == True and label == False:\n\t\t\tfp += 1.0\n\t\telif guess == False and label == True:\n\t\t\tfn += 1.0\n\t\telif guess == True and label == True:\n\t\t\ttp += 1.0\n\t\telif guess == False and label == False:\n\t\t\ttn += 1.0\n\t\tif label == guess:\n\t\t\tcounter += 1.0\n\t\t#else:\n\t\t\t#print label+\'--\'*20\n\t\t\t# if guess:\n\t\t\t# print ""GOLD-"" + str(label) + ""\\t"" + ""SYS-"" + str(guess) + ""\\t"" + sent1 + ""\\t"" + sent2\n\n\ttry:\n\t\tP = tp / (tp + fp)\n\t\tR = tp / (tp + fn)\n\t\tF = 2 * P * R / (P + R)\n\texcept:\n\t\tP=0\n\t\tR=0\n\t\tF=0\n\n\t#print ""PRECISION: %s, RECALL: %s, F1: %s"" % (P, R, F)\n\t#print ""ACCURACY: %s"" % (counter/len(predict_result))\n\taccuracy=counter/len(predict_result)\n\n\t#print ""# true pos:"", tp\n\t#print ""# false pos:"", fp\n\t#print ""# false neg:"", fn\n\t#print ""# true neg:"", tn\n\tmaxF1=0\n\tP_maxF1=0\n\tR_maxF1=0\n\tprobs = predict_result\n\tsortedindex = sorted(range(len(probs)), key=probs.__getitem__)\n\tsortedindex.reverse()\n\n\ttruepos=0\n\tfalsepos=0\n\tfor sortedi in sortedindex:\n\t\tif test_data_label[sortedi]==True:\n\t\t\ttruepos+=1\n\t\telif test_data_label[sortedi]==False:\n\t\t\tfalsepos+=1\n\t\tprecision=0\n\t\tif truepos+falsepos>0:\n\t\t\tprecision=truepos/(truepos+falsepos)\n\n\t\tif (tp+fn)>0:\n\t\t\trecall=truepos/(tp+fn)\n\t\telse:\n\t\t\trecall=0\n\t\tf1=0\n\t\tif precision+recall>0:\n\t\t\tf1=2*precision*recall/(precision+recall)\n\t\t\tif f1>maxF1:\n\t\t\t\t#print probs[sortedi]\n\t\t\t\tmaxF1=f1\n\t\t\t\tP_maxF1=precision\n\t\t\t\tR_maxF1=recall\n\tprint ""PRECISION: %s, RECALL: %s, max_F1: %s"" % (P_maxF1, R_maxF1, maxF1)\n\treturn (accuracy, maxF1)\n\ndef readPITdata(dir,granularity):\n\t# print(len(dict))\n\t# print(dict[\'bmxs\'])\n\tlsents = []\n\trsents = []\n\tlabels = []\n\tif granularity == \'word\' or granularity == \'mix\' or granularity==\'char\' or granularity==\'multi-task\':\n\t\tfor line in open(dir + \'a.toks\'):\n\t\t\tpieces = line.strip().split()\n\t\t\tlsents.append(pieces)\n\t\tfor line in open(dir + \'b.toks\'):\n\t\t\tpieces = line.strip().split()\n\t\t\trsents.append(pieces)\n\telif granularity==\'cross\':\n\t\tfor line in open(dir + \'a.toks\'):\n\t\t\tpieces = line.strip()\n\t\t\tlsents.append(pieces)\n\t\tfor line in open(dir + \'b.toks\'):\n\t\t\tpieces = line.strip()\n\t\t\trsents.append(pieces)\n\t\'\'\'\n\telif granularity == \'char\':\n\t\tfilename=\'\'\n\t\tif \'train\' in dir:\n\t\t\tfilename = \'train.data\'\n\t\telif \'dev\' in dir:\n\t\t\tfilename = \'dev.data\'\n\t\telif \'test\' in dir:\n\t\t\tfilename = \'test.data\'\n\t\tfor line in open(dir + filename):\n\t\t\tif filename==\'train.data\':\n\t\t\t\t(trendid, trendname, origsent, candsent, judge, origsenttag, candsenttag) = line.strip().split(\'\\t\')\n\t\t\telse:\n\t\t\t\t(_ , trendid, trendname, origsent, candsent, judge, tag) = line.strip().split(\'\\t\')\n\t\t\texpert_label = -1\n\t\t\tif judge[0] == \'(\':  # labelled by Amazon Mechanical Turk in format like ""(2,3)""\n\t\t\t\tnYes = eval(judge)[0]\n\t\t\t\tif nYes >= 3:\n\t\t\t\t\texpert_label = 1\n\t\t\t\telif nYes <= 1:\n\t\t\t\t\texpert_label = 0\n\t\t\telif judge[0].isdigit():  # labelled by expert in format like ""2""\n\t\t\t\tnYes = int(judge[0])\n\t\t\t\tif nYes >= 4:\n\t\t\t\t\texpert_label = 1\n\t\t\t\telif nYes <= 2:\n\t\t\t\t\texpert_label = 0\n\t\t\tif expert_label != -1:\n\t\t\t\tlsents.append(origsent.split())\n\t\t\t\trsents.append(candsent.split())\n\t\'\'\'\n\tfor line in open(dir + \'sim.txt\'):\n\t\tlabels.append([int(line.strip())])\n\tdata = (lsents, rsents, labels)\n\tif not len(lsents) == len(rsents) == len(labels):\n\t\tprint(\'error!\')\n\t\tsys.exit()\n\t# print(lsents[0])\n\t# print(rsents[0])\n\t# print(labels[0])\n\treturn data\n\ndef trim_list_pair(l1, l2):\n\tstart_index=None\n\tend_index=None\n\tfor i in range(len(l1)):\n\t\tif i < len(l2):\n\t\t\tif l1[i] == l2[i]:\n\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\tstart_index = i\n\t\t\t\tbreak\n\t\telse:\n\t\t\tstart_index=i\n\t\t\tbreak\n\tif start_index==None:\n\t\tstart_index=len(l1)\n\tfor i in range(len(l1)):\n\t\tif i < len(l2):\n\t\t\tif l1[-1 - i] == l2[-1 - i]:\n\t\t\t\tcontinue\n\t\t\telse:\n\t\t\t\tend_index = i\n\t\t\t\tbreak\n\t\telse:\n\t\t\tend_index=i\n\t\t\tbreak\n\tif end_index == None:\n\t\tend_index=len(l1)\n\tif end_index==0:\n\t\tend_index=None\n\t\treturn (l1[start_index:end_index], l2[start_index:end_index])\n\telse:\n\t\treturn (l1[start_index:-end_index], l2[start_index:-end_index])\n\ndef get_n_params(model):\n    pp=0\n    for p in list(model.parameters()):\n        nn=1\n        for s in list(p.size()):\n            nn = nn*s\n        pp += nn\n    return pp'"
SSE/data_loader.py,11,"b""import torch\nimport config\nfrom torchtext import data, vocab\nfrom torchtext import datasets\nfrom mnli import MNLI\nimport numpy as np\nimport itertools\nfrom torch.autograd import Variable\n\n\nclass RParsedTextLField(data.Field):\n\tdef __init__(self, eos_token='<pad>', lower=False, include_lengths=True):\n\t\tsuper(RParsedTextLField, self).__init__(\n\t\t\teos_token=eos_token, lower=lower, include_lengths=True, preprocessing=lambda parse: [\n\t\t\t\tt for t in parse if t not in ('(', ')')],\n\t\t\tpostprocessing=lambda parse, _, __: [\n\t\t\t\tlist(reversed(p)) for p in parse])\n\n\nclass ParsedTextLField(data.Field):\n\tdef __init__(self, eos_token='<pad>', lower=False, include_lengths=True):\n\t\tsuper(ParsedTextLField, self).__init__(\n\t\t\teos_token=eos_token, lower=lower, include_lengths=True, preprocessing=lambda parse: [\n\t\t\t\tt for t in parse if t not in ('(', ')')])\n\n\ndef load_data(data_root, embd_file, reseversed=True, batch_sizes=(32, 32, 32), device=-1):\n\tif reseversed:\n\t\ttestl_field = RParsedTextLField()\n\telse:\n\t\ttestl_field = ParsedTextLField()\n\n\ttransitions_field = datasets.snli.ShiftReduceField()\n\ty_field = data.Field(sequential=False)\n\n\ttrain, dev, test = datasets.SNLI.splits(testl_field, y_field, transitions_field, root=data_root)\n\ttestl_field.build_vocab(train, dev, test)\n\ty_field.build_vocab(train)\n\n\tif torch.cuda.is_available():\n\t\ttestl_field.vocab.vectors = torch.load(embd_file)\n\telse:\n\t\ttestl_field.vocab.vectors = torch.load(embd_file, map_location=lambda storage, loc: storage)\n\n\ttrain_iter, dev_iter, test_iter = data.Iterator.splits(\n\t\t(train, dev, test), batch_sizes=batch_sizes, device=device, shuffle=False)\n\n\treturn train_iter, dev_iter, test_iter, testl_field.vocab.vectors\n\n\ndef load_data_sm(data_root, embd_file, reseversed=True, batch_sizes=(32, 32, 32, 32, 32), device=-1):\n\tif reseversed:\n\t\ttestl_field = RParsedTextLField()\n\telse:\n\t\ttestl_field = ParsedTextLField()\n\n\ttransitions_field = datasets.snli.ShiftReduceField()\n\ty_field = data.Field(sequential=False)\n\tg_field = data.Field(sequential=False)\n\n\ttrain_size, dev_size, test_size, m_dev_size, m_test_size = batch_sizes\n\n\tsnli_train, snli_dev, snli_test = datasets.SNLI.splits(testl_field, y_field, transitions_field, root=data_root)\n\n\tmnli_train, mnli_dev_m, mnli_dev_um = MNLI.splits(testl_field, y_field, transitions_field, g_field, root=data_root,\n\t\t\t\t\t\t\t\t\t\t\t\t\t  train='train.jsonl',\n\t\t\t\t\t\t\t\t\t\t\t\t\t  validation='dev_matched.jsonl',\n\t\t\t\t\t\t\t\t\t\t\t\t\t  test='dev_mismatched.jsonl')\n\n\tmnli_test_m, mnli_test_um = MNLI.splits(testl_field, y_field, transitions_field, g_field, root=data_root,\n\t\t\t\t\t\t\t\t\t\t\ttrain=None,\n\t\t\t\t\t\t\t\t\t\t\tvalidation='test_matched_unlabeled.jsonl',\n\t\t\t\t\t\t\t\t\t\t\ttest='test_mismatched_unlabeled.jsonl')\n\n\ttestl_field.build_vocab(snli_train, snli_dev, snli_test,\n\t\t\t\t\t\t\tmnli_train, mnli_dev_m, mnli_dev_um, mnli_test_m, mnli_test_um)\n\n\tg_field.build_vocab(mnli_train, mnli_dev_m, mnli_dev_um, mnli_test_m, mnli_test_um)\n\ty_field.build_vocab(snli_train)\n\t#print('Important:', y_field.vocab.itos)\n\tif torch.cuda.is_available():\n\t\ttestl_field.vocab.vectors = torch.load(embd_file)\n\telse:\n\t\ttestl_field.vocab.vectors = torch.load(embd_file, map_location=lambda storage, loc: storage)\n\n\tsnli_train_iter, snli_dev_iter, snli_test_iter = data.Iterator.splits(\n\t\t(snli_train, snli_dev, snli_test), batch_sizes=batch_sizes, device=device, shuffle=False)\n\n\tmnli_train_iter, mnli_dev_m_iter, mnli_dev_um_iter, mnli_test_m_iter, mnli_test_um_iter = data.Iterator.splits(\n\t\t(mnli_train, mnli_dev_m, mnli_dev_um, mnli_test_m, mnli_test_um),\n\t\tbatch_sizes=(train_size, m_dev_size, m_test_size, m_dev_size, m_test_size),\n\t\tdevice=device, shuffle=False, sort=False)\n\n\t# if random_combined:\n\t#     snli_train.examples = list(np.random.choice(snli_train.examples, round(len(snli_train) * rate), replace=False)) + mnli_train.examples\n\t#     train = snli_train\n\t#     train_iter = data.Iterator.splits(train, batch_sizes=train_size, device=device, shuffle=False, sort=False)\n\t#     mnli_train_iter, snli_train_iter = train_iter, train_iter\n\n\treturn (snli_train_iter, snli_dev_iter, snli_test_iter), (mnli_train_iter, mnli_dev_m_iter, mnli_dev_um_iter, mnli_test_m_iter, mnli_test_um_iter), testl_field.vocab.vectors\n\n\ndef load_data_with_dict(data_root, embd_file, reseversed=True, batch_sizes=(32, 32, 32, 32, 32), device=-1):\n\tif reseversed:\n\t\ttestl_field = RParsedTextLField()\n\telse:\n\t\ttestl_field = ParsedTextLField()\n\n\ttransitions_field = datasets.snli.ShiftReduceField()\n\ty_field = data.Field(sequential=False)\n\tg_field = data.Field(sequential=False)\n\n\ttrain_size, dev_size, test_size, m_dev_size, m_test_size = batch_sizes\n\n\tsnli_train, snli_dev, snli_test = datasets.SNLI.splits(testl_field, y_field, transitions_field, root=data_root)\n\n\tmnli_train, mnli_dev_m, mnli_dev_um = MNLI.splits(testl_field, y_field, transitions_field, g_field, root=data_root,\n\t\t\t\t\t\t\t\t\t\t\t\t\t  train='train.jsonl',\n\t\t\t\t\t\t\t\t\t\t\t\t\t  validation='dev_matched.jsonl',\n\t\t\t\t\t\t\t\t\t\t\t\t\t  test='dev_mismatched.jsonl')\n\n\tmnli_test_m, mnli_test_um = MNLI.splits(testl_field, y_field, transitions_field, g_field, root=data_root,\n\t\t\t\t\t\t\t\t\t\t\ttrain=None,\n\t\t\t\t\t\t\t\t\t\t\tvalidation='test_matched_unlabeled.jsonl',\n\t\t\t\t\t\t\t\t\t\t\ttest='test_mismatched_unlabeled.jsonl')\n\n\ttestl_field.build_vocab(snli_train, snli_dev, snli_test,\n\t\t\t\t\t\t\tmnli_train, mnli_dev_m, mnli_dev_um, mnli_test_m, mnli_test_um)\n\n\tg_field.build_vocab(mnli_train, mnli_dev_m, mnli_dev_um, mnli_test_m, mnli_test_um)\n\ty_field.build_vocab(snli_train)\n\tprint('Important:', y_field.vocab.itos)\n\ttestl_field.vocab.vectors = torch.load(embd_file, map_location=lambda storage, loc: storage)\n\n\tsnli_train_iter, snli_dev_iter, snli_test_iter = data.Iterator.splits(\n\t\t(snli_train, snli_dev, snli_test), batch_sizes=batch_sizes, device=device, shuffle=False)\n\n\tmnli_train_iter, mnli_dev_m_iter, mnli_dev_um_iter, mnli_test_m_iter, mnli_test_um_iter = data.Iterator.splits(\n\t\t(mnli_train, mnli_dev_m, mnli_dev_um, mnli_test_m, mnli_test_um),\n\t\tbatch_sizes=(train_size, m_dev_size, m_test_size, m_dev_size, m_test_size),\n\t\tdevice=device, shuffle=False, sort=False)\n\n\treturn (snli_train_iter, snli_dev_iter, snli_test_iter), (mnli_train_iter, mnli_dev_m_iter, mnli_dev_um_iter, mnli_test_m_iter, mnli_test_um_iter), testl_field.vocab.vectors, testl_field.vocab\n\n\ndef raw_input(ws, dict, device=-1):\n\t# ws = ['I', 'like', 'research', '.']\n\tws_t = Variable(torch.from_numpy(np.asarray([[dict.stoi[w]] for w in ws], dtype=np.int64)))\n\twl_t = torch.LongTensor(1).zero_()\n\twl_t[0] = len(ws)\n\n\tif device != -1 and torch.cuda.is_available():\n\t\twl_t.cuda()\n\t\tws_t.cuda()\n\n\treturn ws_t, wl_t\n\n\ndef combine_two_set(set_1, set_2, rate=(1, 1), seed=0):\n\tnp.random.seed(seed)\n\tlen_1 = len(set_1)\n\tlen_2 = len(set_2)\n\t# print(len_1, len_2)\n\tp1, p2 = rate\n\tc_1 = np.random.choice([0, 1], len_1, p=[1 - p1, p1])\n\tc_2 = np.random.choice([0, 1], len_2, p=[1 - p2, p2])\n\titer_1 = itertools.compress(iter(set_1), c_1)\n\titer_2 = itertools.compress(iter(set_2), c_2)\n\tfor it in itertools.chain(iter_1, iter_2):\n\t\tyield it\n\n\nif __name__ == '__main__':\n\tpass\n  """
SSE/main_mnli.py,9,"b'from __future__ import division\nimport torch\nimport os\nfrom model import *\nfrom torch import optim\nimport torch.nn as nn\nfrom datetime import datetime\nfrom torch_util import *\nimport config\nimport tqdm\nimport data_loader\nimport sys\nimport time\nfrom datetime import timedelta\nfrom os.path import expanduser\n\ndef build_kaggle_submission_file(model_path):\n\tprint(\'start testing...\')\n\ttorch.manual_seed(6)\n\n\tsnli_d, mnli_d, embd = data_loader.load_data_sm(\n\t\tconfig.DATA_ROOT, config.EMBD_FILE, reseversed=False, batch_sizes=(32, 32, 32, 32, 32), device=0)\n\n\tm_train, m_dev_m, m_dev_um, m_test_m, m_test_um = mnli_d\n\n\tm_test_um.shuffle = False\n\tm_test_m.shuffle = False\n\tm_test_um.sort = False\n\tm_test_m.sort = False\n\n\tmodel = StackBiLSTMMaxout(h_size=[512, 1024, 2048], v_size=10, d=300, mlp_d=1600, dropout_r=0.1, max_l=60)\n\tmodel.Embd.weight.data = embd\n\t# model.display()\n\n\tif torch.cuda.is_available():\n\t\tembd.cuda()\n\t\tmodel.cuda()\n\n\tcriterion = nn.CrossEntropyLoss()\n\n\tmodel.load_state_dict(torch.load(model_path))\n\n\tm_pred = model_eval(model, m_test_m, criterion, pred=True)\n\tum_pred = model_eval(model, m_test_um, criterion, pred=True)\n\n\tmodel.max_l = 150\n\tprint(um_pred)\n\tprint(m_pred)\n\n\twith open(basepath+\'/sub_um.csv\', \'w+\') as f:\n\t\tindex = [\'entailment\', \'contradiction\', \'neutral\']\n\t\tf.write(""pairID,gold_label\\n"")\n\t\tfor i, k in enumerate(um_pred):\n\t\t\tf.write(str(i) + "","" + index[k] + ""\\n"")\n\n\twith open(basepath+\'/sub_m.csv\', \'w+\') as f:\n\t\tindex = [\'entailment\', \'contradiction\', \'neutral\']\n\t\tf.write(""pairID,gold_label\\n"")\n\t\tfor j, k in enumerate(m_pred):\n\t\t\tf.write(str(j + 9847) + "","" + index[k] + ""\\n"")\n\ndef train(combined_set=False):\n\ttorch.manual_seed(6)\n\n\tif torch.cuda.is_available():\n\t\tprint(\'CUDA is available!\')\n\t\tbase_path = expanduser(""~"") + \'/pytorch/SSE\'\n\telse:\n\t\tbase_path = expanduser(""~"") + \'/Documents/research/pytorch/SSE\'\n\n\tif torch.cuda.is_available():\n\t\tsnli_d, mnli_d, embd = data_loader.load_data_sm(\n\t\t\tconfig.DATA_ROOT, config.EMBD_FILE, reseversed=False, batch_sizes=(32, 200, 200, 30, 30), device=None)\n\telse:\n\t\tsnli_d, mnli_d, embd = data_loader.load_data_sm(\n\t\t\tconfig.DATA_ROOT, config.EMBD_FILE, reseversed=False, batch_sizes=(32, 200, 200, 30, 30), device=-1)\n\n\ts_train, s_dev, s_test = snli_d\n\tm_train, m_dev_m, m_dev_um, m_test_m, m_test_um = mnli_d\n\n\ts_train.repeat = False\n\tm_train.repeat = False\n\n\tmodel = StackBiLSTMMaxout(h_size=[512, 1024, 2048], v_size=10, d=300, mlp_d=1600, dropout_r=0.1, max_l=60)\n\tmodel.Embd.weight.data = embd\n\n\tif torch.cuda.is_available():\n\t\tembd.cuda()\n\t\tmodel.cuda()\n\n\tstart_lr = 2e-4\n\n\toptimizer = optim.Adam(model.parameters(), lr=start_lr)\n\tcriterion = nn.CrossEntropyLoss()\n\n\tbest_m_dev = -1\n\tbest_um_dev = -1\n\treport_interval = 1000\n\tthreshold=1\n\n\tprint(\'start training...\')\n\tfor epoch in range(20):\n\t\tprint(\'--\' * 20)\n\t\taccumulated_loss = 0\n\t\tbatch_counter = 0\n\t\ttrain_sents_scaned = 0\n\t\ts_train.init_epoch()\n\t\tm_train.init_epoch()\n\t\tstart_time = time.time()\n\n\t\tif not combined_set:\n\t\t\t#train_iter, dev_iter, test_iter = s_train, s_dev, s_test\n\t\t\ttrain_iter=m_train\n\t\t\ttrain_iter.repeat = False\n\t\t\tprint(len(train_iter))\n\t\telse:\n\t\t\ttrain_iter = data_loader.combine_two_set(s_train, m_train, rate=[0.15, 1], seed=epoch)\n\t\t\tdev_iter, test_iter = s_dev, s_test\n\n\t\t#start_perf = model_eval(model, dev_iter, criterion)\n\t\ti_decay = epoch / 2\n\t\tlr = start_lr / (2 ** i_decay)\n\t\tprint(lr)\n\t\tmodel.train()\n\t\ttrain_num_correct = 0\n\n\t\tfor batch_idx, batch in enumerate(train_iter):\n\n\t\t\ts1, s1_l = batch.premise\n\t\t\ts2, s2_l = batch.hypothesis\n\t\t\ty = batch.label - 1\n\n\t\t\ttrain_sents_scaned += len(y)\n\t\t\tout = model(s1, s1_l - 1, s2, s2_l - 1)\n\t\t\tloss = criterion(out, y)\n\t\t\tresult = out.data.cpu().numpy()\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = y.data.cpu().numpy()\n\t\t\ttrain_num_correct += np.sum(a == b)\n\t\t\toptimizer.zero_grad()\n\n\t\t\tfor pg in optimizer.param_groups:\n\t\t\t\tpg[\'lr\'] = lr\n\n\t\t\tloss.backward()\n\t\t\toptimizer.step()\n\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\tbatch_counter += 1\n\t\t\tif batch_counter % report_interval == 0:\n\t\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\t\tmsg += \'\\t training batch loss: %f\' % (accumulated_loss / train_sents_scaned)\n\t\t\t\tmsg += \'\\t train accuracy: %f\' % (train_num_correct / train_sents_scaned)\n\t\t\t\tprint(msg)\n\n\t\t\tif batch_counter > (len(train_iter) // threshold):\n\t\t\t\tbreak\n\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\tmsg += \'\\t training batch loss: %f\' % (accumulated_loss / train_sents_scaned)\n\t\tmsg += \'\\t train accuracy: %f\' % (train_num_correct / train_sents_scaned)\n\t\tprint(msg)\n\t\t# valid after each epoch\n\t\tmodel.max_l = 150\n\t\tmdm_score, mdm_loss, validm_prob = model_eval(model, m_dev_m, criterion)\n\t\tmdum_score, mdum_loss, validum_prob = model_eval(model, m_dev_um, criterion)\n\n\t\tprint(\' MNLI_M:{}/{}\'.format(mdm_score, mdm_loss))\n\t\tprint(\' MNLI_UM:{}/{}\'.format(mdum_score, mdum_loss))\n\t\tmodel.max_l = 60\n\n\t\tif mdm_score>best_m_dev:\n\t\t\tbest_m_dev = mdm_score\n\t\t\twith open(base_path + \'/prob_SSE_\' + task+\'_m\', \'w\') as f:\n\t\t\t\tfor item in validm_prob:\n\t\t\t\t\tf.writelines(str(item[0]) + \'\\t\' + str(item[1]) + \'\\t\' + str(item[2]) + \'\\n\')\n\n\t\tif mdum_score>best_um_dev:\n\t\t\tbest_um_dev = mdum_score\n\t\t\twith open(base_path + \'/prob_SSE_\' + task+\'_um\', \'w\') as f:\n\t\t\t\tfor item in validum_prob:\n\t\t\t\t\tf.writelines(str(item[0]) + \'\\t\' + str(item[1]) + \'\\t\' + str(item[2]) + \'\\n\')\n\n\t\telapsed_time = time.time() - start_time\n\t\tprint(\'Epoch \' + str(epoch) + \'Batch number \' + str(batch_counter) + \' finished within \' + str(timedelta(seconds=elapsed_time)))\n\nif __name__ == ""__main__"":\n\tprint(\'model: SSE\')\n\tprint(\'task: mnli\')\n\ttask=\'mnli\'\n\tif torch.cuda.is_available():\n\t\tprint(\'CUDA is available!\')\n\t\tbasepath = expanduser(""~"") + \'/pytorch/SSE\'\n\telse:\n\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/SSE\'\n\ttrain(False)\n\t#build_kaggle_submission_file(basepath+\'/saved_model/model_saved\')'"
SSE/main_pit.py,32,"b'from __future__ import division\nimport torch\nimport os\nfrom model import *\nfrom torch import optim\nimport torch.nn as nn\nfrom datetime import datetime\nfrom torch_util import *\nimport config\nimport tqdm\nimport data_loader\nimport sys\nimport time\nfrom datetime import timedelta\nfrom os.path import expanduser\nfrom torchtext.vocab import load_word_vectors\n\ndef create_batch(data,from_index, to_index):\n\tif to_index>len(data):\n\t\tto_index=len(data)\n\tlsize=0\n\trsize=0\n\tlsize_list=[]\n\trsize_list=[]\n\tfor i in range(from_index, to_index):\n\t\tlength=len(data[i][0])+2\n\t\tlsize_list.append(length)\n\t\tif length>lsize:\n\t\t\tlsize=length\n\t\tlength=len(data[i][1])+2\n\t\trsize_list.append(length)\n\t\tif length>rsize:\n\t\t\trsize=length\n\t#lsize+=1\n\t#rsize+=1\n\tlsent = data[from_index][0]\n\tlsent = [\'bos\']+lsent + [\'oov\' for k in range(lsize -1 - len(lsent))]\n\t#print(lsent)\n\tleft_sents = [[word2id[word] for word in lsent]]\n\t#left_sents = torch.cat((dict[word].view(1, -1) for word in lsent))\n\t#left_sents = torch.unsqueeze(left_sents,0)\n\n\trsent = data[from_index][1]\n\trsent = [\'bos\']+rsent + [\'oov\' for k in range(rsize -1 - len(rsent))]\n\t#print(rsent)\n\tright_sents = [[word2id[word] for word in rsent]]\n\t#right_sents = torch.cat((dict[word].view(1, -1) for word in rsent))\n\t#right_sents = torch.unsqueeze(right_sents,0)\n\n\tlabels=[data[from_index][2]]\n\n\tfor i in range(from_index+1, to_index):\n\n\t\tlsent=data[i][0]\n\t\tlsent=[\'bos\']+lsent+[\'oov\' for k in range(lsize -1 - len(lsent))]\n\t\t#print(lsent)\n\t\tleft_sents.append([word2id[word] for word in lsent])\n\t\t#left_sent = torch.cat((dict[word].view(1,-1) for word in lsent))\n\t\t#left_sent = torch.unsqueeze(left_sent, 0)\n\t\t#left_sents = torch.cat([left_sents, left_sent])\n\n\t\trsent=data[i][1]\n\t\trsent=[\'bos\']+rsent+[\'oov\' for k in range(rsize -1 - len(rsent))]\n\t\t#print(rsent)\n\t\tright_sents.append([word2id[word] for word in rsent])\n\t\t#right_sent = torch.cat((dict[word].view(1,-1) for word in rsent))\n\t\t#right_sent = torch.unsqueeze(right_sent, 0)\n\t\t#right_sents = torch.cat((right_sents, right_sent))\n\n\t\tlabels.append(data[i][2])\n\n\tleft_sents=Variable(torch.LongTensor(left_sents))\n\tright_sents=Variable(torch.LongTensor(right_sents))\n\tif task==\'sts\':\n\t\tlabels=Variable(torch.Tensor(labels))\n\telse:\n\t\tlabels=Variable(torch.LongTensor(labels))\n\tlsize_list=torch.LongTensor(lsize_list)\n\trsize_list =torch.LongTensor(rsize_list)\n\n\tif torch.cuda.is_available():\n\t\tleft_sents=left_sents.cuda()\n\t\tright_sents=right_sents.cuda()\n\t\tlabels=labels.cuda()\n\t\tlsize_list=lsize_list.cuda()\n\t\trsize_list=rsize_list.cuda()\n\t#print(left_sents)\n\t#print(right_sents)\n\treturn left_sents, right_sents, labels, lsize_list, rsize_list\n\nif __name__ == \'__main__\':\n\ttask=\'pit\'\n\tprint(\'task: \'+task)\n\ttorch.manual_seed(6)\n\n\tnum_class = 2\n\tif torch.cuda.is_available():\n\t\tprint(\'CUDA is available!\')\n\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/pit\'\n\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\telse:\n\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/pit\'\n\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\ttrain_pairs = readQuoradata(basepath + \'/train/\')\n\t#dev_pairs = readQuoradata(basepath + \'/dev/\')\n\ttest_pairs = readQuoradata(basepath + \'/test/\')\n\tdev_pairs=test_pairs\n\n\ttokens = []\n\tdict={}\n\tword2id={}\n\tvocab = set()\n\tfor pair in train_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in dev_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in test_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\ttokens=list(vocab)\n\t#for line in open(basepath + \'/vocab.txt\'):\n\t#\ttokens.append(line.strip().decode(\'utf-8\'))\n\twv_dict, wv_arr, wv_size = load_word_vectors(embedding_path, \'glove.840B\', 300)\n\t#embedding = []\n\ttokens.append(\'oov\')\n\ttokens.append(\'bos\')\n\tpretrained_emb = np.zeros(shape=(len(tokens), 300))\n\toov={}\n\tfor id in range(100):\n\t\toov[id]=torch.normal(torch.zeros(300),std=1)\n\tid=0\n\tfor word in tokens:\n\t\ttry:\n\t\t\tdict[word] = wv_arr[wv_dict[word]]/torch.norm(wv_arr[wv_dict[word]])\n\t\t\t#print(word)\n\t\texcept:\n\t\t\tdict[word] = torch.normal(torch.zeros(300),std=1)\n\t\tword2id[word]=id\n\t\tpretrained_emb[id] = dict[word].numpy()\n\t\tid+=1\n\n\tmodel = StackBiLSTMMaxout(h_size=[512, 1024, 2048], v_size=10, d=300, mlp_d=1600, dropout_r=0.1, max_l=60, num_class=num_class)\n\tif torch.cuda.is_available():\n\t\tpretrained_emb=torch.Tensor(pretrained_emb).cuda()\n\telse:\n\t\tpretrained_emb = torch.Tensor(pretrained_emb)\n\tmodel.Embd.weight.data = pretrained_emb\n\n\tif torch.cuda.is_available():\n\t\tmodel.cuda()\n\n\tstart_lr = 2e-4\n\tbatch_size=32\n\treport_interval=1000\n\toptimizer = optim.Adam(model.parameters(), lr=start_lr)\n\tcriterion = nn.CrossEntropyLoss()\n\n\titerations = 0\n\n\tbest_m_dev = -1\n\tbest_um_dev = -1\n\tbest_dev_loss=10e10\n\tbest_result=0\n\n\tprint(\'start training...\')\n\tfor epoch in range(20):\n\t\tbatch_counter = 0\n\t\taccumulated_loss = 0\n\t\tmodel.train()\n\t\tprint(\'--\' * 20)\n\t\tstart_time = time.time()\n\t\ti_decay = epoch / 2\n\t\tlr = start_lr / (2 ** i_decay)\n\t\tprint(lr)\n\t\ttrain_pairs = np.array(train_pairs)\n\t\trand_idx = np.random.permutation(len(train_pairs))\n\t\ttrain_pairs = train_pairs[rand_idx]\n\t\ttrain_batch_i = 0\n\t\ttrain_num_correct=0\n\t\ttrain_sents_scaned = 0\n\t\twhile train_batch_i < len(train_pairs):\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(train_pairs, train_batch_i, train_batch_i+batch_size)\n\t\t\ttrain_sents_scaned += len(labels)\n\t\t\ttrain_batch_i+=len(labels)\n\t\t\tleft_sents=torch.transpose(left_sents,0,1)\n\t\t\tright_sents=torch.transpose(right_sents,0,1)\n\t\t\toutput=model(left_sents,lsize_list,right_sents,rsize_list)\n\t\t\tresult = output.data.cpu().numpy()\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = labels.data.cpu().numpy()\n\t\t\ttrain_num_correct += np.sum(a == b)\n\t\t\tloss = criterion(output, labels)\n\t\t\toptimizer.zero_grad()\n\t\t\tfor pg in optimizer.param_groups:\n\t\t\t\tpg[\'lr\'] = lr\n\t\t\tloss.backward()\n\t\t\toptimizer.step()\n\t\t\tbatch_counter += 1\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\tif batch_counter % report_interval == 0:\n\t\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\t\tmsg += \'\\t train batch loss: %f\' % (accumulated_loss / train_sents_scaned)\n\t\t\t\tmsg += \'\\t train accuracy: %f\' % (train_num_correct / train_sents_scaned)\n\t\t\t\tprint(msg)\n\t\t# valid after each epoch\n\t\tmodel.eval()\n\t\tdev_batch_index = 0\n\t\tdev_num_correct = 0\n\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\taccumulated_loss = 0\n\t\tdev_batch_i = 0\n\t\tpred=[]\n\t\tgold=[]\n\t\twhile dev_batch_i < len(dev_pairs):\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(dev_pairs, dev_batch_i,\n\t\t\t                                                                       dev_batch_i+batch_size)\n\t\t\tdev_batch_i += len(labels)\n\t\t\tleft_sents = torch.transpose(left_sents, 0, 1)\n\t\t\tright_sents = torch.transpose(right_sents, 0, 1)\n\t\t\toutput = F.softmax(model(left_sents, lsize_list,right_sents, rsize_list))\n\t\t\tresult = output.data.cpu().numpy()\n\t\t\tloss = criterion(output, labels)\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = labels.data.cpu().numpy()\n\t\t\tdev_num_correct += np.sum(a == b)\n\t\t\tif task==\'pit\' or task==\'url\' or task==\'wikiqa\' or task==\'trecqa\':\n\t\t\t\tpred.extend(result[:,1])\n\t\t\t\tgold.extend(b)\n\t\t\tif task==\'sts\':\n\t\t\t\tpred.extend(0*result[:,0]+1*result[:,1]+2*result[:,2]+3*result[:,3]+4*result[:,4]+5*result[:,5])\n\t\t\t\tgold.extend(0*b[:,0]+1*b[:,1]+2*b[:,2]+3*b[:,3]+4*b[:,4]+5*b[:,5])\n\t\tmsg += \'\\t dev loss: %f\' % (accumulated_loss/len(dev_pairs))\n\t\tdev_acc = dev_num_correct / len(dev_pairs)\n\t\tmsg += \'\\t dev accuracy: %f\' % dev_acc\n\t\tprint(msg)\n\t\tif task==\'pit\' or task==\'url\'or task==\'wikiqa\' or task==\'trecqa\':\n\t\t\t_,my_result=URL_maxF1_eval(pred, gold)\n\t\telif task==\'sts\':\n\t\t\tprint(\'pearson: \'+str(pearson(pred,gold)))\n\t\tif my_result > best_result:\n\t\t\tbest_result=my_result\n\t\t\ttorch.save(model, basepath+\'/model_SSE_\'+task+\'.pkl\')\n\t\t\twith open(basepath + \'/result_prob_SSE_\' + task, \'w\') as f:\n\t\t\t\tfor i in range(len(pred)):\n\t\t\t\t\tf.writelines(str(1 - pred[i]) + \'\\t\' + str(pred[i]) + \'\\n\')\n\t\telapsed_time = time.time() - start_time\n\t\tprint(\'Epoch \' + str(epoch) + \' finished within \' + str(timedelta(seconds=elapsed_time)))\n'"
SSE/main_quora.py,34,"b'from __future__ import division\nimport torch\nimport os\nimport random\nfrom model import *\nfrom torch import optim\nimport torch.nn as nn\nfrom datetime import datetime\nfrom torch_util import *\nimport tqdm\nimport data_loader\nimport sys\nimport time\nfrom datetime import timedelta\nfrom os.path import expanduser\nfrom torchtext.vocab import load_word_vectors\n\ndef create_batch(data,from_index, to_index):\n\tif to_index>len(data):\n\t\tto_index=len(data)\n\tlsize=0\n\trsize=0\n\tlsize_list=[]\n\trsize_list=[]\n\tfor i in range(from_index, to_index):\n\t\tlength=len(data[i][0])+2\n\t\tlsize_list.append(length)\n\t\tif length>lsize:\n\t\t\tlsize=length\n\t\tlength=len(data[i][1])+2\n\t\trsize_list.append(length)\n\t\tif length>rsize:\n\t\t\trsize=length\n\t#lsize+=1\n\t#rsize+=1\n\tlsent = data[from_index][0]\n\tlsent = [\'bos\']+lsent + [\'oov\' for k in range(lsize -1 - len(lsent))]\n\t#print(lsent)\n\tleft_sents = [[word2id[word] for word in lsent]]\n\t#left_sents = torch.cat((dict[word].view(1, -1) for word in lsent))\n\t#left_sents = torch.unsqueeze(left_sents,0)\n\n\trsent = data[from_index][1]\n\trsent = [\'bos\']+rsent + [\'oov\' for k in range(rsize -1 - len(rsent))]\n\t#print(rsent)\n\tright_sents = [[word2id[word] for word in rsent]]\n\t#right_sents = torch.cat((dict[word].view(1, -1) for word in rsent))\n\t#right_sents = torch.unsqueeze(right_sents,0)\n\n\tlabels=[data[from_index][2]]\n\n\tfor i in range(from_index+1, to_index):\n\n\t\tlsent=data[i][0]\n\t\tlsent=[\'bos\']+lsent+[\'oov\' for k in range(lsize -1 - len(lsent))]\n\t\t#print(lsent)\n\t\tleft_sents.append([word2id[word] for word in lsent])\n\t\t#left_sent = torch.cat((dict[word].view(1,-1) for word in lsent))\n\t\t#left_sent = torch.unsqueeze(left_sent, 0)\n\t\t#left_sents = torch.cat([left_sents, left_sent])\n\n\t\trsent=data[i][1]\n\t\trsent=[\'bos\']+rsent+[\'oov\' for k in range(rsize -1 - len(rsent))]\n\t\t#print(rsent)\n\t\tright_sents.append([word2id[word] for word in rsent])\n\t\t#right_sent = torch.cat((dict[word].view(1,-1) for word in rsent))\n\t\t#right_sent = torch.unsqueeze(right_sent, 0)\n\t\t#right_sents = torch.cat((right_sents, right_sent))\n\n\t\tlabels.append(data[i][2])\n\n\tleft_sents=Variable(torch.LongTensor(left_sents))\n\tright_sents=Variable(torch.LongTensor(right_sents))\n\tlabels=Variable(torch.LongTensor(labels))\n\tlsize_list=torch.LongTensor(lsize_list)\n\trsize_list =torch.LongTensor(rsize_list)\n\n\tif torch.cuda.is_available():\n\t\tleft_sents=left_sents.cuda()\n\t\tright_sents=right_sents.cuda()\n\t\tlabels=labels.cuda()\n\t\tlsize_list=lsize_list.cuda()\n\t\trsize_list=rsize_list.cuda()\n\t#print(left_sents)\n\t#print(right_sents)\n\treturn left_sents, right_sents, labels, lsize_list, rsize_list\n\nif __name__ == \'__main__\':\n\ttask=\'quora\'\n\tprint(\'task: \'+task)\n\tprint(\'model: SSE\')\n\ttorch.manual_seed(6)\n\n\tnum_class = 2\n\tif torch.cuda.is_available():\n\t\tprint(\'CUDA is available!\')\n\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/quora\'\n\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\telse:\n\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/quora\'\n\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\ttrain_pairs = readQuoradata(basepath + \'/train/\')\n\tdev_pairs = readQuoradata(basepath + \'/dev/\')\n\ttest_pairs = readQuoradata(basepath + \'/test/\')\n\n\tprint(\'# of train pairs: %d\' % len(train_pairs))\n\tprint(\'# of dev pairs: %d\' % len(dev_pairs))\n\tprint(\'# of test pairs: %d\' % len(test_pairs))\n\n\ttokens = []\n\tdict={}\n\tword2id={}\n\tvocab = set()\n\tfor pair in train_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in dev_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in test_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\ttokens=list(vocab)\n\t#for line in open(basepath + \'/vocab.txt\'):\n\t#\ttokens.append(line.strip().decode(\'utf-8\'))\n\twv_dict, wv_arr, wv_size = load_word_vectors(embedding_path, \'glove.840B\', 300)\n\t#embedding = []\n\ttokens.append(\'oov\')\n\ttokens.append(\'bos\')\n\tpretrained_emb = np.zeros(shape=(len(tokens), 300))\n\tid=0\n\tfor word in tokens:\n\t\ttry:\n\t\t\tdict[word] = wv_arr[wv_dict[word]]\n\t\texcept:\n\t\t\tdict[word] = torch.Tensor([random.uniform(-0.05, 0.05) for i in range(300)])\n\t\tword2id[word]=id\n\t\tpretrained_emb[id] = dict[word].numpy()\n\t\tid+=1\n\n\tmodel = StackBiLSTMMaxout(h_size=[512, 1024, 2048], v_size=10, d=300, mlp_d=1600, dropout_r=0.1, max_l=60, num_class=num_class)\n\tif torch.cuda.is_available():\n\t\tpretrained_emb=torch.Tensor(pretrained_emb).cuda()\n\telse:\n\t\tpretrained_emb = torch.Tensor(pretrained_emb)\n\tmodel.Embd.weight.data = pretrained_emb\n\n\tif torch.cuda.is_available():\n\t\tmodel.cuda()\n\n\tstart_lr = 2e-4\n\tbatch_size=32\n\treport_interval=10000\n\toptimizer = optim.Adam(model.parameters(), lr=start_lr)\n\tcriterion = nn.CrossEntropyLoss()\n\n\titerations = 0\n\n\tbest_dev_loss = 10e10\n\n\tmodel=torch.load(basepath + \'/model_SSE_\' + task + \'.pkl\')\n\ttest_batch_index = 0\n\ttest_num_correct = 0\n\t#msg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\taccumulated_loss = 0\n\ttest_batch_i = 0\n\tpred = []\n\tgold = []\n\twhile test_batch_i < len(test_pairs):\n\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(test_pairs, test_batch_i,\n\t\t                                                                       test_batch_i + batch_size)\n\t\tleft_sents = torch.transpose(left_sents, 0, 1)\n\t\tright_sents = torch.transpose(right_sents, 0, 1)\n\t\ttest_batch_i += len(labels)\n\t\toutput = F.softmax(model(left_sents, lsize_list, right_sents, rsize_list))\n\t\tresult = output.data.cpu().numpy()\n\t\tloss = criterion(output, labels)\n\t\taccumulated_loss += loss.data[0]\n\t\ta = np.argmax(result, axis=1)\n\t\tb = labels.data.cpu().numpy()\n\t\ttest_num_correct += np.sum(a == b)\n\t\tpred.extend(result[:,1])\n\tmsg = \'\\t test loss: %f\' % accumulated_loss\n\ttest_acc = test_num_correct / len(test_pairs)\n\tmsg += \'\\t test accuracy: %f\' % test_acc\n\tprint(msg)\n\twith open(basepath+\'/quora_SSE_prob.txt\',\'w\') as f:\n\t\tfor item in pred:\n\t\t\tf.writelines(str(1 - item)+\'\\t\'+str(item)+\'\\n\')\n\tsys.exit()\n\tprint(\'start training...\')\n\tfor epoch in range(100):\n\t\tbatch_counter = 0\n\t\taccumulated_loss = 0\n\t\tprint(\'--\' * 20)\n\t\tstart_time = time.time()\n\t\ti_decay = epoch / 2\n\t\tlr = start_lr / (2 ** i_decay)\n\t\tprint(lr)\n\t\ttrain_pairs = np.array(train_pairs)\n\t\trand_idx = np.random.permutation(len(train_pairs))\n\t\ttrain_pairs = train_pairs[rand_idx]\n\t\ttrain_batch_i = 0\n\t\ttrain_num_correct=0\n\t\ttrain_sents_scaned = 0\n\t\twhile train_batch_i < len(train_pairs):\n\t\t\tmodel.train()\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(train_pairs, train_batch_i, train_batch_i+batch_size)\n\t\t\ttrain_sents_scaned += len(labels)\n\t\t\ttrain_batch_i+=len(labels)\n\t\t\tleft_sents=torch.transpose(left_sents,0,1)\n\t\t\tright_sents=torch.transpose(right_sents,0,1)\n\t\t\toutput=model(left_sents,lsize_list,right_sents,rsize_list)\n\t\t\tresult = output.data.cpu().numpy()\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = labels.data.cpu().numpy()\n\t\t\ttrain_num_correct += np.sum(a == b)\n\t\t\tloss = criterion(output, labels)\n\t\t\toptimizer.zero_grad()\n\t\t\tfor pg in optimizer.param_groups:\n\t\t\t\tpg[\'lr\'] = lr\n\t\t\tloss.backward()\n\t\t\toptimizer.step()\n\t\t\tbatch_counter += 1\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\tif batch_counter % report_interval == 0:\n\t\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\t\tmsg += \'\\t train batch loss: %f\' % accumulated_loss\n\t\t\t\tmsg += \'\\t train accuracy: %f\' % (train_num_correct / train_sents_scaned)\n\t\t\t\tprint(msg)\n\t\t\t\t# valid after each report interval\n\t\t\t\tmodel.eval()\n\t\t\t\tdev_batch_index = 0\n\t\t\t\tdev_num_correct = 0\n\t\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\t\taccumulated_loss = 0\n\t\t\t\tdev_batch_i = 0\n\t\t\t\tpred=[]\n\t\t\t\tgold=[]\n\t\t\t\twhile dev_batch_i < len(dev_pairs):\n\t\t\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(dev_pairs, dev_batch_i,\n\t\t\t\t\t                                                                       dev_batch_i+batch_size)\n\t\t\t\t\tdev_batch_i += len(labels)\n\t\t\t\t\tleft_sents = torch.transpose(left_sents, 0, 1)\n\t\t\t\t\tright_sents = torch.transpose(right_sents, 0, 1)\n\t\t\t\t\toutput = model(left_sents, lsize_list,right_sents, rsize_list)\n\t\t\t\t\tresult = output.data.cpu().numpy()\n\t\t\t\t\tloss = criterion(output, labels)\n\t\t\t\t\taccumulated_loss += loss.data[0]\n\t\t\t\t\ta = np.argmax(result, axis=1)\n\t\t\t\t\tb = labels.data.cpu().numpy()\n\t\t\t\t\tdev_num_correct += np.sum(a == b)\n\t\t\t\tmsg += \'\\t dev loss: %f\' % accumulated_loss\n\t\t\t\tdev_acc = dev_num_correct / len(dev_pairs)\n\t\t\t\tmsg += \'\\t dev accuracy: %f\' % dev_acc\n\t\t\t\tprint(msg)\n\t\t\t\tif accumulated_loss < best_dev_loss:\n\t\t\t\t\tbest_dev_loss=accumulated_loss\n\t\t\t\t\ttorch.save(model, basepath+\'/model_SSE_\'+task+\'.pkl\')\n\t\t\t\t\ttest_batch_index = 0\n\t\t\t\t\ttest_num_correct = 0\n\t\t\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\t\t\taccumulated_loss = 0\n\t\t\t\t\ttest_batch_i = 0\n\t\t\t\t\tpred = []\n\t\t\t\t\tgold = []\n\t\t\t\t\twhile test_batch_i < len(test_pairs):\n\t\t\t\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(test_pairs, test_batch_i, test_batch_i+batch_size)\n\t\t\t\t\t\tleft_sents = torch.transpose(left_sents, 0, 1)\n\t\t\t\t\t\tright_sents = torch.transpose(right_sents, 0, 1)\n\t\t\t\t\t\ttest_batch_i+=len(labels)\n\t\t\t\t\t\toutput = model(left_sents, lsize_list, right_sents, rsize_list)\n\t\t\t\t\t\tresult = output.data.cpu().numpy()\n\t\t\t\t\t\tloss = criterion(output, labels)\n\t\t\t\t\t\taccumulated_loss += loss.data[0]\n\t\t\t\t\t\ta = np.argmax(result, axis=1)\n\t\t\t\t\t\tb = labels.data.cpu().numpy()\n\t\t\t\t\t\ttest_num_correct += np.sum(a == b)\n\t\t\t\t\tmsg += \'\\t test loss: %f\' % accumulated_loss\n\t\t\t\t\ttest_acc = test_num_correct / len(test_pairs)\n\t\t\t\t\tmsg += \'\\t test accuracy: %f\' % test_acc\n\t\t\t\t\tprint(msg)\n\t\telapsed_time = time.time() - start_time\n\t\tprint(\'Epoch \' + str(epoch) + \' finished within \' + str(timedelta(seconds=elapsed_time)))\n'"
SSE/main_snli.py,10,"b'from __future__ import division\nimport torch\nimport os\nfrom model import *\nfrom torch import optim\nimport torch.nn as nn\nfrom datetime import datetime\nfrom torch_util import *\nimport config\nimport tqdm\nimport data_loader\nimport sys\nimport time\nfrom datetime import timedelta\nfrom os.path import expanduser\n\ndef build_kaggle_submission_file(model_path):\n\tprint(\'start testing...\')\n\ttorch.manual_seed(6)\n\n\tsnli_d, mnli_d, embd = data_loader.load_data_sm(\n\t\tconfig.DATA_ROOT, config.EMBD_FILE, reseversed=False, batch_sizes=(32, 32, 32, 32, 32), device=0)\n\n\tm_train, m_dev_m, m_dev_um, m_test_m, m_test_um = mnli_d\n\n\tm_test_um.shuffle = False\n\tm_test_m.shuffle = False\n\tm_test_um.sort = False\n\tm_test_m.sort = False\n\n\tmodel = StackBiLSTMMaxout(h_size=[512, 1024, 2048], v_size=10, d=300, mlp_d=1600, dropout_r=0.1, max_l=60)\n\tmodel.Embd.weight.data = embd\n\t# model.display()\n\n\tif torch.cuda.is_available():\n\t\tembd.cuda()\n\t\tmodel.cuda()\n\n\tcriterion = nn.CrossEntropyLoss()\n\n\tmodel.load_state_dict(torch.load(model_path))\n\n\tm_pred = model_eval(model, m_test_m, criterion, pred=True)\n\tum_pred = model_eval(model, m_test_um, criterion, pred=True)\n\n\tmodel.max_l = 150\n\tprint(um_pred)\n\tprint(m_pred)\n\n\twith open(basepath+\'/sub_um.csv\', \'w+\') as f:\n\t\tindex = [\'entailment\', \'contradiction\', \'neutral\']\n\t\tf.write(""pairID,gold_label\\n"")\n\t\tfor i, k in enumerate(um_pred):\n\t\t\tf.write(str(i) + "","" + index[k] + ""\\n"")\n\n\twith open(basepath+\'/sub_m.csv\', \'w+\') as f:\n\t\tindex = [\'entailment\', \'contradiction\', \'neutral\']\n\t\tf.write(""pairID,gold_label\\n"")\n\t\tfor j, k in enumerate(m_pred):\n\t\t\tf.write(str(j + 9847) + "","" + index[k] + ""\\n"")\n\ndef train(combined_set=False):\n\ttorch.manual_seed(6)\n\n\tif torch.cuda.is_available():\n\t\tprint(\'CUDA is available!\')\n\t\tbase_path = expanduser(""~"") + \'/pytorch/SSE\'\n\telse:\n\t\tbase_path = expanduser(""~"") + \'/Documents/research/pytorch/SSE\'\n\n\tif torch.cuda.is_available():\n\t\tsnli_d, mnli_d, embd = data_loader.load_data_sm(\n\t\t\tconfig.DATA_ROOT, config.EMBD_FILE, reseversed=False, batch_sizes=(32, 200, 200, 30, 30), device=None)\n\telse:\n\t\tsnli_d, mnli_d, embd = data_loader.load_data_sm(\n\t\t\tconfig.DATA_ROOT, config.EMBD_FILE, reseversed=False, batch_sizes=(32, 200, 200, 30, 30), device=-1)\n\n\ts_train, s_dev, s_test = snli_d\n\tm_train, m_dev_m, m_dev_um, m_test_m, m_test_um = mnli_d\n\n\ts_train.repeat = False\n\tm_train.repeat = False\n\n\tmodel = StackBiLSTMMaxout(h_size=[512, 1024, 2048], v_size=10, d=300, mlp_d=1600, dropout_r=0.1, max_l=60)\n\tmodel.Embd.weight.data = embd\n\t#model.display()\n\n\tif torch.cuda.is_available():\n\t\tembd.cuda()\n\t\tmodel.cuda()\n\n\tstart_lr = 2e-4\n\n\toptimizer = optim.Adam(model.parameters(), lr=start_lr)\n\tcriterion = nn.CrossEntropyLoss()\n\n\treport_interval=1000\n\tmax_epoch=100\n\tbest_result=0\n\tthreshold=1\n\n\tprint(\'start training...\')\n\tfor epoch in range(max_epoch):\n\t\taccumulated_loss = 0\n\t\tbatch_counter=0\n\t\ttrain_sents_scaned=0\n\t\ts_train.init_epoch()\n\t\tm_train.init_epoch()\n\t\tstart_time = time.time()\n\n\t\tif not combined_set:\n\t\t\ttrain_iter, dev_iter, test_iter = s_train, s_dev, s_test\n\t\telse:\n\t\t\ttrain_iter = data_loader.combine_two_set(s_train, m_train, rate=[0.15, 1], seed=epoch)\n\t\t\tdev_iter, test_iter = s_dev, s_test\n\n\t\ti_decay = epoch / 2\n\t\tlr = start_lr / (2 ** i_decay)\n\t\tmodel.train()\n\t\ttrain_num_correct=0\n\t\tfor batch_idx, batch in enumerate(train_iter):\n\t\t\ts1, s1_l = batch.premise\n\t\t\ts2, s2_l = batch.hypothesis\n\t\t\ty = batch.label - 1\n\t\t\ttrain_sents_scaned+=len(y)\n\t\t\toutput = model(s1, s1_l - 1, s2, s2_l - 1)\n\t\t\tloss = criterion(output, y)\n\t\t\tresult = output.data.cpu().numpy()\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = y.data.cpu().numpy()\n\t\t\ttrain_num_correct += np.sum(a == b)\n\t\t\toptimizer.zero_grad()\n\n\t\t\tfor pg in optimizer.param_groups:\n\t\t\t\tpg[\'lr\'] = lr\n\n\t\t\tloss.backward()\n\t\t\toptimizer.step()\n\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\tbatch_counter += 1\n\t\t\tif batch_counter % report_interval == 0:\n\t\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\t\tmsg += \'\\t training batch loss: %f\' % (accumulated_loss / train_sents_scaned)\n\t\t\t\tmsg += \'\\t train accuracy: %f\' % (train_num_correct / train_sents_scaned)\n\t\t\t\tprint(msg)\n\n\t\t\tif batch_counter>(len(train_iter)//threshold):\n\t\t\t\tbreak\n\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\tmsg += \'\\t training batch loss: %f\' % (accumulated_loss / train_sents_scaned)\n\t\tmsg += \'\\t train accuracy: %f\' % (train_num_correct / train_sents_scaned)\n\t\tprint(msg)\n\t\t#valid after each epoch\n\t\tmodel.max_l = 150\n\t\tvalid_score, valid_loss, valid_prob = model_eval(model, s_dev, criterion)\n\t\ttest_score, test_loss, test_prob = model_eval(model, s_test, criterion)\n\n\t\tprint(\'Dev score/loss: {}/{}\'.format(valid_score, valid_loss))\n\t\tprint(\'Test score/loss: {}/{}\'.format(test_score, test_loss))\n\t\tmodel.max_l = 60\n\n\t\tif test_score>best_result:\n\t\t\tbest_result=test_score\n\t\t\ttorch.save(model, base_path + \'/SNLI_model.pickle\')\n\t\t\twith open(base_path + \'/prob_SSE_\' + task + \'_\'+ str(threshold), \'w\') as f:\n\t\t\t\tfor item in test_prob:\n\t\t\t\t\tf.writelines(str(item[0])+\'\\t\'+str(item[1])+\'\\t\'+str(item[2])+\'\\n\')\n\n\t\telapsed_time = time.time() - start_time\n\t\tprint(\'Epoch \' + str(epoch) + \'Batch number \' + str(batch_counter) + \' finished within \' + str(timedelta(seconds=elapsed_time)))\n\nif __name__ == ""__main__"":\n\tprint(\'task: SNLI\')\n\tprint(\'model: SSE\')\n\ttask=\'snli\'\n\tif torch.cuda.is_available():\n\t\tprint(\'CUDA is available!\')\n\t\tbasepath = expanduser(""~"") + \'/pytorch/SSE\'\n\telse:\n\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/SSE\'\n\ttrain()'"
SSE/main_sts.py,31,"b'from __future__ import division\nimport torch\nimport os\nfrom model import *\nfrom torch import optim\nimport torch.nn as nn\nfrom datetime import datetime\nfrom torch_util import *\nimport config\nimport tqdm\nimport data_loader\nimport sys\nimport time\nfrom datetime import timedelta\nfrom os.path import expanduser\nfrom torchtext.vocab import load_word_vectors\n\ndef create_batch(data,from_index, to_index):\n\tif to_index>len(data):\n\t\tto_index=len(data)\n\tlsize=0\n\trsize=0\n\tlsize_list=[]\n\trsize_list=[]\n\tfor i in range(from_index, to_index):\n\t\tlength=len(data[i][0])+2\n\t\tlsize_list.append(length)\n\t\tif length>lsize:\n\t\t\tlsize=length\n\t\tlength=len(data[i][1])+2\n\t\trsize_list.append(length)\n\t\tif length>rsize:\n\t\t\trsize=length\n\t#lsize+=1\n\t#rsize+=1\n\tlsent = data[from_index][0]\n\tlsent = [\'bos\']+lsent + [\'oov\' for k in range(lsize -1 - len(lsent))]\n\t#print(lsent)\n\tleft_sents = [[word2id[word] for word in lsent]]\n\t#left_sents = torch.cat((dict[word].view(1, -1) for word in lsent))\n\t#left_sents = torch.unsqueeze(left_sents,0)\n\n\trsent = data[from_index][1]\n\trsent = [\'bos\']+rsent + [\'oov\' for k in range(rsize -1 - len(rsent))]\n\t#print(rsent)\n\tright_sents = [[word2id[word] for word in rsent]]\n\t#right_sents = torch.cat((dict[word].view(1, -1) for word in rsent))\n\t#right_sents = torch.unsqueeze(right_sents,0)\n\n\tlabels=[data[from_index][2]]\n\n\tfor i in range(from_index+1, to_index):\n\n\t\tlsent=data[i][0]\n\t\tlsent=[\'bos\']+lsent+[\'oov\' for k in range(lsize -1 - len(lsent))]\n\t\t#print(lsent)\n\t\tleft_sents.append([word2id[word] for word in lsent])\n\t\t#left_sent = torch.cat((dict[word].view(1,-1) for word in lsent))\n\t\t#left_sent = torch.unsqueeze(left_sent, 0)\n\t\t#left_sents = torch.cat([left_sents, left_sent])\n\n\t\trsent=data[i][1]\n\t\trsent=[\'bos\']+rsent+[\'oov\' for k in range(rsize -1 - len(rsent))]\n\t\t#print(rsent)\n\t\tright_sents.append([word2id[word] for word in rsent])\n\t\t#right_sent = torch.cat((dict[word].view(1,-1) for word in rsent))\n\t\t#right_sent = torch.unsqueeze(right_sent, 0)\n\t\t#right_sents = torch.cat((right_sents, right_sent))\n\n\t\tlabels.append(data[i][2])\n\n\tleft_sents=Variable(torch.LongTensor(left_sents))\n\tright_sents=Variable(torch.LongTensor(right_sents))\n\tif task==\'sts\':\n\t\tlabels=Variable(torch.Tensor(labels))\n\telse:\n\t\tlabels=Variable(torch.LongTensor(labels))\n\tlsize_list=torch.LongTensor(lsize_list)\n\trsize_list =torch.LongTensor(rsize_list)\n\n\tif torch.cuda.is_available():\n\t\tleft_sents=left_sents.cuda()\n\t\tright_sents=right_sents.cuda()\n\t\tlabels=labels.cuda()\n\t\tlsize_list=lsize_list.cuda()\n\t\trsize_list=rsize_list.cuda()\n\t#print(left_sents)\n\t#print(right_sents)\n\treturn left_sents, right_sents, labels, lsize_list, rsize_list\n\nif __name__ == \'__main__\':\n\ttask=\'sts\'\n\tprint(\'task: \'+task)\n\tprint(\'model: SSE\')\n\ttorch.manual_seed(6)\n\n\tnum_class = 6\n\tif torch.cuda.is_available():\n\t\tprint(\'CUDA is available!\')\n\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/sts\'\n\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\telse:\n\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/sts\'\n\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\ttrain_pairs = readSTSdata(basepath + \'/train/\')\n\t#dev_pairs = readQuoradata(basepath + \'/dev/\')\n\ttest_pairs = readSTSdata(basepath + \'/test/\')\n\tdev_pairs=test_pairs\n\n\ttokens = []\n\tdict={}\n\tword2id={}\n\tvocab = set()\n\tfor pair in train_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in dev_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in test_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\ttokens=list(vocab)\n\t#for line in open(basepath + \'/vocab.txt\'):\n\t#\ttokens.append(line.strip().decode(\'utf-8\'))\n\twv_dict, wv_arr, wv_size = load_word_vectors(embedding_path, \'glove.840B\', 300)\n\t#embedding = []\n\ttokens.append(\'oov\')\n\ttokens.append(\'bos\')\n\tpretrained_emb = np.zeros(shape=(len(tokens), 300))\n\toov={}\n\tfor id in range(100):\n\t\toov[id]=torch.normal(torch.zeros(300),std=1)\n\tid=0\n\tfor word in tokens:\n\t\ttry:\n\t\t\tdict[word] = wv_arr[wv_dict[word]]/torch.norm(wv_arr[wv_dict[word]])\n\t\t\t#print(word)\n\t\texcept:\n\t\t\tdict[word] = torch.normal(torch.zeros(300),std=1)\n\t\tword2id[word]=id\n\t\tpretrained_emb[id] = dict[word].numpy()\n\t\tid+=1\n\n\tmodel = StackBiLSTMMaxout(h_size=[512, 1024, 2048], v_size=10, d=300, mlp_d=1600, dropout_r=0.1, max_l=60, num_class=num_class)\n\tif torch.cuda.is_available():\n\t\tpretrained_emb=torch.Tensor(pretrained_emb).cuda()\n\telse:\n\t\tpretrained_emb = torch.Tensor(pretrained_emb)\n\tmodel.Embd.weight.data = pretrained_emb\n\n\tif torch.cuda.is_available():\n\t\tmodel.cuda()\n\n\tstart_lr = 2e-4\n\tbatch_size=32\n\treport_interval=1000\n\toptimizer = optim.Adam(model.parameters(), lr=start_lr)\n\tif task==\'sts\':\n\t\tcriterion=nn.KLDivLoss()\n\telse:\n\t\tcriterion = nn.CrossEntropyLoss()\n\n\titerations = 0\n\n\tbest_m_dev = -1\n\tbest_um_dev = -1\n\tbest_dev_loss=10e10\n\n\tprint(\'start training...\')\n\tfor epoch in range(20):\n\t\tbatch_counter = 0\n\t\taccumulated_loss = 0\n\t\tmodel.train()\n\t\tprint(\'--\' * 20)\n\t\tstart_time = time.time()\n\t\ti_decay = epoch / 2\n\t\tlr = start_lr / (2 ** i_decay)\n\t\tprint(lr)\n\t\ttrain_pairs = np.array(train_pairs)\n\t\trand_idx = np.random.permutation(len(train_pairs))\n\t\ttrain_pairs = train_pairs[rand_idx]\n\t\ttrain_batch_i = 0\n\t\ttrain_num_correct=0\n\t\ttrain_sents_scaned = 0\n\t\twhile train_batch_i < len(train_pairs):\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(train_pairs, train_batch_i, train_batch_i+batch_size)\n\t\t\ttrain_sents_scaned += len(labels)\n\t\t\ttrain_batch_i+=len(labels)\n\t\t\tleft_sents=torch.transpose(left_sents,0,1)\n\t\t\tright_sents=torch.transpose(right_sents,0,1)\n\t\t\toutput=model(left_sents,lsize_list,right_sents,rsize_list)\n\t\t\tresult = output.data.cpu().numpy()\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = labels.data.cpu().numpy()\n\t\t\ttrain_num_correct += np.sum(a == b)\n\t\t\tloss = criterion(F.log_softmax(output,1), labels)\n\t\t\toptimizer.zero_grad()\n\t\t\tfor pg in optimizer.param_groups:\n\t\t\t\tpg[\'lr\'] = lr\n\t\t\tloss.backward()\n\t\t\toptimizer.step()\n\t\t\tbatch_counter += 1\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\tif batch_counter % report_interval == 0:\n\t\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\t\tmsg += \'\\t train batch loss: %f\' % (accumulated_loss / train_sents_scaned)\n\t\t\t\t#msg += \'\\t train accuracy: %f\' % (train_num_correct / train_sents_scaned)\n\t\t\t\tprint(msg)\n\t\t# valid after each epoch\n\t\tmodel.eval()\n\t\tdev_batch_index = 0\n\t\tdev_num_correct = 0\n\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\taccumulated_loss = 0\n\t\tdev_batch_i = 0\n\t\tpred=[]\n\t\tgold=[]\n\t\twhile dev_batch_i < len(dev_pairs):\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(dev_pairs, dev_batch_i,\n\t\t\t                                                                       dev_batch_i+batch_size)\n\t\t\tdev_batch_i += len(labels)\n\t\t\tleft_sents = torch.transpose(left_sents, 0, 1)\n\t\t\tright_sents = torch.transpose(right_sents, 0, 1)\n\t\t\toutput = model(left_sents, lsize_list,right_sents, rsize_list)\n\t\t\tresult = output.data.cpu().numpy()\n\t\t\tloss = criterion(F.log_softmax(output,1), labels)\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = labels.data.cpu().numpy()\n\t\t\tdev_num_correct += np.sum(a == b)\n\t\t\tif task==\'pit\' or task==\'url\' or task==\'wikiqa\' or task==\'trecqa\':\n\t\t\t\tpred.extend(result[:,1])\n\t\t\t\tgold.extend(b)\n\t\t\tif task==\'sts\':\n\t\t\t\tpred.extend(0*result[:,0]+1*result[:,1]+2*result[:,2]+3*result[:,3]+4*result[:,4]+5*result[:,5])\n\t\t\t\tgold.extend(0*b[:,0]+1*b[:,1]+2*b[:,2]+3*b[:,3]+4*b[:,4]+5*b[:,5])\n\t\tmsg += \'\\t dev loss: %f\' % (accumulated_loss/len(dev_pairs))\n\t\tdev_acc = dev_num_correct / len(dev_pairs)\n\t\t#msg += \'\\t dev accuracy: %f\' % dev_acc\n\t\tprint(msg)\n\t\tif task==\'pit\' or task==\'url\'or task==\'wikiqa\' or task==\'trecqa\':\n\t\t\tURL_maxF1_eval(pred, gold)\n\t\telif task==\'sts\':\n\t\t\tprint(\'pearson: \'+str(pearson(pred,gold)))\n\t\telapsed_time = time.time() - start_time\n\t\tprint(\'Epoch \' + str(epoch) + \' finished within \' + str(timedelta(seconds=elapsed_time)))\n'"
SSE/main_trecqa.py,33,"b'from __future__ import division\nimport torch\nimport os\nfrom model import *\nfrom torch import optim\nimport torch.nn as nn\nfrom datetime import datetime\nfrom torch_util import *\nimport config\nimport tqdm\nimport data_loader\nimport sys\nimport time\nfrom datetime import timedelta\nfrom os.path import expanduser\nfrom torchtext.vocab import load_word_vectors\n\ndef create_batch(data,from_index, to_index):\n\tif to_index>len(data):\n\t\tto_index=len(data)\n\tlsize=0\n\trsize=0\n\tlsize_list=[]\n\trsize_list=[]\n\tfor i in range(from_index, to_index):\n\t\tlength=len(data[i][0])+2\n\t\tlsize_list.append(length)\n\t\tif length>lsize:\n\t\t\tlsize=length\n\t\tlength=len(data[i][1])+2\n\t\trsize_list.append(length)\n\t\tif length>rsize:\n\t\t\trsize=length\n\t#lsize+=1\n\t#rsize+=1\n\tlsent = data[from_index][0]\n\tlsent = [\'bos\']+lsent + [\'oov\' for k in range(lsize -1 - len(lsent))]\n\t#print(lsent)\n\tleft_sents = [[word2id[word] for word in lsent]]\n\t#left_sents = torch.cat((dict[word].view(1, -1) for word in lsent))\n\t#left_sents = torch.unsqueeze(left_sents,0)\n\n\trsent = data[from_index][1]\n\trsent = [\'bos\']+rsent + [\'oov\' for k in range(rsize -1 - len(rsent))]\n\t#print(rsent)\n\tright_sents = [[word2id[word] for word in rsent]]\n\t#right_sents = torch.cat((dict[word].view(1, -1) for word in rsent))\n\t#right_sents = torch.unsqueeze(right_sents,0)\n\n\tlabels=[data[from_index][2]]\n\n\tfor i in range(from_index+1, to_index):\n\n\t\tlsent=data[i][0]\n\t\tlsent=[\'bos\']+lsent+[\'oov\' for k in range(lsize -1 - len(lsent))]\n\t\t#print(lsent)\n\t\tleft_sents.append([word2id[word] for word in lsent])\n\t\t#left_sent = torch.cat((dict[word].view(1,-1) for word in lsent))\n\t\t#left_sent = torch.unsqueeze(left_sent, 0)\n\t\t#left_sents = torch.cat([left_sents, left_sent])\n\n\t\trsent=data[i][1]\n\t\trsent=[\'bos\']+rsent+[\'oov\' for k in range(rsize -1 - len(rsent))]\n\t\t#print(rsent)\n\t\tright_sents.append([word2id[word] for word in rsent])\n\t\t#right_sent = torch.cat((dict[word].view(1,-1) for word in rsent))\n\t\t#right_sent = torch.unsqueeze(right_sent, 0)\n\t\t#right_sents = torch.cat((right_sents, right_sent))\n\n\t\tlabels.append(data[i][2])\n\n\tleft_sents=Variable(torch.LongTensor(left_sents))\n\tright_sents=Variable(torch.LongTensor(right_sents))\n\tif task==\'sts\':\n\t\tlabels=Variable(torch.Tensor(labels))\n\telse:\n\t\tlabels=Variable(torch.LongTensor(labels))\n\tlsize_list=torch.LongTensor(lsize_list)\n\trsize_list =torch.LongTensor(rsize_list)\n\n\tif torch.cuda.is_available():\n\t\tleft_sents=left_sents.cuda()\n\t\tright_sents=right_sents.cuda()\n\t\tlabels=labels.cuda()\n\t\tlsize_list=lsize_list.cuda()\n\t\trsize_list=rsize_list.cuda()\n\t#print(left_sents)\n\t#print(right_sents)\n\treturn left_sents, right_sents, labels, lsize_list, rsize_list\n\nif __name__ == \'__main__\':\n\ttask=\'trecqa\'\n\tprint(\'task: \'+task)\n\ttorch.manual_seed(6)\n\n\tnum_class = 2\n\tif torch.cuda.is_available():\n\t\tprint(\'CUDA is available!\')\n\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/trecqa\'\n\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\tcastorini_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/castorini/trec_eval.9.0/trec_eval\'\n\telse:\n\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/trecqa\'\n\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\tcastorini_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/castorini/trec_eval.9.0/trec_eval\'\n\ttrain_pairs = readQuoradata(basepath + \'/train/\')\n\tdev_pairs = readQuoradata(basepath + \'/dev/\')\n\ttest_pairs = readQuoradata(basepath + \'/test/\')\n\t#dev_pairs=test_pairs\n\tcmd = (\'%s -m map %s %s\' % (castorini_path, basepath + \'/test.qrel\', basepath + \'/result_SSE_\' + task))\n\tos.system(cmd)\n\tcmd = (\'%s -m recip_rank %s %s\' % (castorini_path, basepath + \'/test.qrel\', basepath + \'/result_SSE_\' + task))\n\tos.system(cmd)\n\ttokens = []\n\tdict={}\n\tword2id={}\n\tvocab = set()\n\tfor pair in train_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in dev_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in test_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\ttokens=list(vocab)\n\t#for line in open(basepath + \'/vocab.txt\'):\n\t#\ttokens.append(line.strip().decode(\'utf-8\'))\n\twv_dict, wv_arr, wv_size = load_word_vectors(embedding_path, \'glove.840B\', 300)\n\t#embedding = []\n\ttokens.append(\'oov\')\n\ttokens.append(\'bos\')\n\tpretrained_emb = np.zeros(shape=(len(tokens), 300))\n\toov={}\n\tfor id in range(100):\n\t\toov[id]=torch.normal(torch.zeros(300),std=1)\n\tid=0\n\tfor word in tokens:\n\t\ttry:\n\t\t\tdict[word] = wv_arr[wv_dict[word]]/torch.norm(wv_arr[wv_dict[word]])\n\t\t\t#print(word)\n\t\texcept:\n\t\t\tdict[word] = torch.normal(torch.zeros(300),std=1)\n\t\tword2id[word]=id\n\t\tpretrained_emb[id] = dict[word].numpy()\n\t\tid+=1\n\n\tmodel = StackBiLSTMMaxout(h_size=[512, 1024, 2048], v_size=10, d=300, mlp_d=1600, dropout_r=0.1, max_l=60, num_class=num_class)\n\tif torch.cuda.is_available():\n\t\tpretrained_emb=torch.Tensor(pretrained_emb).cuda()\n\telse:\n\t\tpretrained_emb = torch.Tensor(pretrained_emb)\n\tmodel.Embd.weight.data = pretrained_emb\n\n\tif torch.cuda.is_available():\n\t\tmodel.cuda()\n\n\tstart_lr = 2e-4\n\tbatch_size=32\n\treport_interval=1000\n\toptimizer = optim.Adam(model.parameters(), lr=start_lr)\n\tif task==\'sts\':\n\t\tcriterion=nn.KLDivLoss()\n\telse:\n\t\tcriterion = nn.CrossEntropyLoss()\n\n\titerations = 0\n\n\tbest_m_dev = -1\n\tbest_um_dev = -1\n\tbest_dev_loss=10e10\n\n\tprint(\'start training...\')\n\tfor epoch in range(100):\n\t\tbatch_counter = 0\n\t\taccumulated_loss = 0\n\t\tmodel.train()\n\t\tprint(\'--\' * 20)\n\t\tstart_time = time.time()\n\t\ti_decay = epoch / 2\n\t\tlr = start_lr / (2 ** i_decay)\n\t\tprint(lr)\n\t\ttrain_pairs = np.array(train_pairs)\n\t\trand_idx = np.random.permutation(len(train_pairs))\n\t\ttrain_pairs = train_pairs[rand_idx]\n\t\ttrain_batch_i = 0\n\t\ttrain_num_correct=0\n\t\ttrain_sents_scaned = 0\n\t\twhile train_batch_i < len(train_pairs):\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(train_pairs, train_batch_i, train_batch_i+batch_size)\n\t\t\ttrain_sents_scaned += len(labels)\n\t\t\ttrain_batch_i+=len(labels)\n\t\t\tleft_sents=torch.transpose(left_sents,0,1)\n\t\t\tright_sents=torch.transpose(right_sents,0,1)\n\t\t\toutput=model(left_sents,lsize_list,right_sents,rsize_list)\n\t\t\tresult = output.data.cpu().numpy()\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = labels.data.cpu().numpy()\n\t\t\ttrain_num_correct += np.sum(a == b)\n\t\t\tloss = criterion(output, labels)\n\t\t\toptimizer.zero_grad()\n\t\t\tfor pg in optimizer.param_groups:\n\t\t\t\tpg[\'lr\'] = lr\n\t\t\tloss.backward()\n\t\t\toptimizer.step()\n\t\t\tbatch_counter += 1\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\tif batch_counter % report_interval == 0:\n\t\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\t\tmsg += \'\\t train batch loss: %f\' % (accumulated_loss / train_sents_scaned)\n\t\t\t\t#msg += \'\\t train accuracy: %f\' % (train_num_correct / train_sents_scaned)\n\t\t\t\tprint(msg)\n\t\t# valid after each epoch\n\t\tmodel.eval()\n\t\tdev_batch_index = 0\n\t\tdev_num_correct = 0\n\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\taccumulated_loss = 0\n\t\tdev_batch_i = 0\n\t\tpred=[]\n\t\tgold=[]\n\t\twhile dev_batch_i < len(dev_pairs):\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(dev_pairs, dev_batch_i,\n\t\t\t                                                                       dev_batch_i+batch_size)\n\t\t\tdev_batch_i += len(labels)\n\t\t\tleft_sents = torch.transpose(left_sents, 0, 1)\n\t\t\tright_sents = torch.transpose(right_sents, 0, 1)\n\t\t\toutput = F.softmax(model(left_sents, lsize_list,right_sents, rsize_list))\n\t\t\tresult = output.data.cpu().numpy()\n\t\t\tloss = criterion(output, labels)\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = labels.data.cpu().numpy()\n\t\t\tdev_num_correct += np.sum(a == b)\n\t\t\tif task==\'pit\' or task==\'url\' or task==\'wikiqa\' or task==\'trecqa\':\n\t\t\t\tpred.extend(result[:,1])\n\t\t\t\tgold.extend(b)\n\t\t\tif task==\'sts\':\n\t\t\t\tpred.extend(0*result[:,0]+1*result[:,1]+2*result[:,2]+3*result[:,3]+4*result[:,4]+5*result[:,5])\n\t\t\t\tgold.extend(0*b[:,0]+1*b[:,1]+2*b[:,2]+3*b[:,3]+4*b[:,4]+5*b[:,5])\n\t\tmsg += \'\\t dev loss: %f\' % accumulated_loss\n\t\t#dev_acc = dev_num_correct / len(dev_pairs)\n\t\t#msg += \'\\t dev accuracy: %f\' % dev_acc\n\t\tprint(msg)\n\t\tif task==\'pit\' or task==\'url\'or task==\'wikiqa\' or task==\'trecqa\':\n\t\t\tURL_maxF1_eval(pred, gold)\n\t\t# Test after each epoch\n\t\ttest_batch_index = 0\n\t\ttest_num_correct = 0\n\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\taccumulated_loss = 0\n\t\ttest_batch_i = 0\n\t\tpred = []\n\t\tgold = []\n\t\twhile test_batch_i < len(test_pairs):\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(test_pairs, test_batch_i, test_batch_i+batch_size)\n\t\t\tleft_sents = torch.transpose(left_sents, 0, 1)\n\t\t\tright_sents = torch.transpose(right_sents, 0, 1)\n\t\t\ttest_batch_i+=len(labels)\n\t\t\toutput = F.softmax(model(left_sents, lsize_list, right_sents, rsize_list))\n\t\t\tresult = output.data.cpu().numpy()\n\t\t\tloss = criterion(output, labels)\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = labels.data.cpu().numpy()\n\t\t\ttest_num_correct += np.sum(a == b)\n\t\t\tif task == \'pit\' or task == \'url\'or task==\'wikiqa\' or task==\'trecqa\':\n\t\t\t\tpred.extend(result[:, 1])\n\t\t\t\tgold.extend(b)\n\t\t\tif task == \'sts\':\n\t\t\t\tpred.extend(0 * result[:, 0] + 1 * result[:, 1] + 2 * result[:, 2] + 3 * result[:, 3] + 4 * result[:,4] + 5 * result[:, 5])\n\t\t\t\tgold.extend(0 * b[:, 0] + 1 * b[:, 1] + 2 * b[:, 2] + 3 * b[:, 3] + 4 * b[:, 4] + 5 * b[:, 5])\n\t\tmsg += \'\\t test loss: %f\' % accumulated_loss\n\t\ttest_acc = test_num_correct / len(test_pairs)\n\t\t#msg += \'\\t test accuracy: %f\' % test_acc\n\t\tprint(msg)\n\t\tif task == \'pit\' or task == \'url\'or task==\'wikiqa\' or task==\'trecqa\':\n\t\t\tURL_maxF1_eval(pred, gold)\n\t\t\twith open(basepath + \'/prob_SSE_\' + task, \'w\') as f:\n\t\t\t\tfor i in range(len(pred)):\n\t\t\t\t\tf.writelines(str(pred[i]) + \'\\n\')\n\t\t\tif task == \'wikiqa\' or task == \'trecqa\':\n\t\t\t\tlist1 = []\n\t\t\t\tfor line in open(basepath+\'/test.qrel\'):\n\t\t\t\t\tlist1.append(line.strip().split())\n\t\t\t\twith open(basepath + \'/result_SSE_\' + task, \'w\') as f:\n\t\t\t\t\tfor i in range(len(pred)):\n\t\t\t\t\t\tf.writelines(str(list1[i][0]) + \'\\t\' + str(list1[i][1]) + \'\\t\' + str(\n\t\t\t\t\t\t\tlist1[i][2]) + \'\\t\' + \'*\\t\' + str(pred[i]) + \'\\t\' + \'*\\n\')\n\t\t\tcmd = (\'%s -m map %s %s\' % (castorini_path, basepath + \'/test.qrel\', basepath + \'/result_SSE_\' + task))\n\t\t\tos.system(cmd)\n\t\t\tcmd = (\'%s -m recip_rank %s %s\' % (castorini_path, basepath + \'/test.qrel\', basepath + \'/result_SSE_\' + task))\n\t\t\tos.system(cmd)\n\t\telapsed_time = time.time() - start_time\n\t\tprint(\'Epoch \' + str(epoch) + \' finished within \' + str(timedelta(seconds=elapsed_time)))\n'"
SSE/main_url.py,33,"b'from __future__ import division\nimport torch\nimport os\nfrom model import *\nfrom torch import optim\nimport torch.nn as nn\nfrom datetime import datetime\nfrom torch_util import *\nimport config\nimport tqdm\nimport data_loader\nimport sys\nimport time\nimport itertools\nfrom datetime import timedelta\nfrom os.path import expanduser\nfrom torchtext.vocab import load_word_vectors\n\ndef get_n_params(model):\n    pp=0\n    parameters = itertools.ifilter(lambda p: p.requires_grad, model.parameters())\n    for p in list(parameters):\n        nn=1\n        for s in list(p.size()):\n            nn = nn*s\n        pp += nn\n    return pp\n\ndef create_batch(data,from_index, to_index):\n\tif to_index>len(data):\n\t\tto_index=len(data)\n\tlsize=0\n\trsize=0\n\tlsize_list=[]\n\trsize_list=[]\n\tfor i in range(from_index, to_index):\n\t\tlength=len(data[i][0])+2\n\t\tlsize_list.append(length)\n\t\tif length>lsize:\n\t\t\tlsize=length\n\t\tlength=len(data[i][1])+2\n\t\trsize_list.append(length)\n\t\tif length>rsize:\n\t\t\trsize=length\n\t#lsize+=1\n\t#rsize+=1\n\tlsent = data[from_index][0]\n\tlsent = [\'bos\']+lsent + [\'oov\' for k in range(lsize -1 - len(lsent))]\n\t#print(lsent)\n\tleft_sents = [[word2id[word] for word in lsent]]\n\t#left_sents = torch.cat((dict[word].view(1, -1) for word in lsent))\n\t#left_sents = torch.unsqueeze(left_sents,0)\n\n\trsent = data[from_index][1]\n\trsent = [\'bos\']+rsent + [\'oov\' for k in range(rsize -1 - len(rsent))]\n\t#print(rsent)\n\tright_sents = [[word2id[word] for word in rsent]]\n\t#right_sents = torch.cat((dict[word].view(1, -1) for word in rsent))\n\t#right_sents = torch.unsqueeze(right_sents,0)\n\n\tlabels=[data[from_index][2]]\n\n\tfor i in range(from_index+1, to_index):\n\n\t\tlsent=data[i][0]\n\t\tlsent=[\'bos\']+lsent+[\'oov\' for k in range(lsize -1 - len(lsent))]\n\t\t#print(lsent)\n\t\tleft_sents.append([word2id[word] for word in lsent])\n\t\t#left_sent = torch.cat((dict[word].view(1,-1) for word in lsent))\n\t\t#left_sent = torch.unsqueeze(left_sent, 0)\n\t\t#left_sents = torch.cat([left_sents, left_sent])\n\n\t\trsent=data[i][1]\n\t\trsent=[\'bos\']+rsent+[\'oov\' for k in range(rsize -1 - len(rsent))]\n\t\t#print(rsent)\n\t\tright_sents.append([word2id[word] for word in rsent])\n\t\t#right_sent = torch.cat((dict[word].view(1,-1) for word in rsent))\n\t\t#right_sent = torch.unsqueeze(right_sent, 0)\n\t\t#right_sents = torch.cat((right_sents, right_sent))\n\n\t\tlabels.append(data[i][2])\n\n\tleft_sents=Variable(torch.LongTensor(left_sents))\n\tright_sents=Variable(torch.LongTensor(right_sents))\n\tif task==\'sts\':\n\t\tlabels=Variable(torch.Tensor(labels))\n\telse:\n\t\tlabels=Variable(torch.LongTensor(labels))\n\tlsize_list=torch.LongTensor(lsize_list)\n\trsize_list =torch.LongTensor(rsize_list)\n\n\tif torch.cuda.is_available():\n\t\tleft_sents=left_sents.cuda()\n\t\tright_sents=right_sents.cuda()\n\t\tlabels=labels.cuda()\n\t\tlsize_list=lsize_list.cuda()\n\t\trsize_list=rsize_list.cuda()\n\t#print(left_sents)\n\t#print(right_sents)\n\treturn left_sents, right_sents, labels, lsize_list, rsize_list\n\nif __name__ == \'__main__\':\n\ttask=\'url\'\n\tprint(\'task: \'+task)\n\ttorch.manual_seed(6)\n\n\tnum_class = 2\n\tif torch.cuda.is_available():\n\t\tprint(\'CUDA is available!\')\n\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/url\'\n\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\telse:\n\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/url\'\n\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\ttrain_pairs = readQuoradata(basepath + \'/train/\')\n\ttest_pairs = readQuoradata(basepath + \'/test_9324/\')\n\tdev_pairs=test_pairs\n\n\ttokens = []\n\tdict={}\n\tword2id={}\n\tvocab = set()\n\tfor pair in train_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in test_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\ttokens=list(vocab)\n\t#for line in open(basepath + \'/vocab.txt\'):\n\t#\ttokens.append(line.strip().decode(\'utf-8\'))\n\twv_dict, wv_arr, wv_size = load_word_vectors(embedding_path, \'glove.840B\', 300)\n\t#embedding = []\n\ttokens.append(\'oov\')\n\ttokens.append(\'bos\')\n\tpretrained_emb = np.zeros(shape=(len(tokens), 300))\n\tid=0\n\tfor word in tokens:\n\t\ttry:\n\t\t\tdict[word] = wv_arr[wv_dict[word]]/torch.norm(wv_arr[wv_dict[word]])\n\t\t\t#print(word)\n\t\texcept:\n\t\t\tdict[word] = torch.normal(torch.zeros(300),std=1)\n\t\tword2id[word]=id\n\t\tpretrained_emb[id] = dict[word].numpy()\n\t\tid+=1\n\n\tmodel = StackBiLSTMMaxout(h_size=[512, 1024, 2048], v_size=10, d=300, mlp_d=1600, dropout_r=0.1, max_l=60, num_class=num_class)\n\tif torch.cuda.is_available():\n\t\tpretrained_emb=torch.Tensor(pretrained_emb).cuda()\n\telse:\n\t\tpretrained_emb = torch.Tensor(pretrained_emb)\n\tmodel.Embd.weight.data = pretrained_emb\n\tprint(model)\n\tmodel.Embd.weight.requires_grad = False\n\tprint(get_n_params(model))\n\tsys.exit()\n\n\tif torch.cuda.is_available():\n\t\tmodel.cuda()\n\n\tstart_lr = 2e-4\n\tbatch_size=32\n\treport_interval=1000\n\toptimizer = optim.Adam(model.parameters(), lr=start_lr)\n\tcriterion = nn.CrossEntropyLoss()\n\n\titerations = 0\n\n\tbest_dev_loss = 10e10\n\n\tprint(\'start training...\')\n\tfor epoch in range(20):\n\t\tbatch_counter = 0\n\t\taccumulated_loss = 0\n\t\tmodel.train()\n\t\tprint(\'--\' * 20)\n\t\tstart_time = time.time()\n\t\ti_decay = epoch / 2\n\t\tlr = start_lr / (2 ** i_decay)\n\t\tprint(lr)\n\t\ttrain_pairs = np.array(train_pairs)\n\t\trand_idx = np.random.permutation(len(train_pairs))\n\t\ttrain_pairs = train_pairs[rand_idx]\n\t\ttrain_batch_i = 0\n\t\ttrain_num_correct=0\n\t\ttrain_sents_scaned = 0\n\t\twhile train_batch_i < len(train_pairs):\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(train_pairs, train_batch_i, train_batch_i+batch_size)\n\t\t\ttrain_sents_scaned += len(labels)\n\t\t\ttrain_batch_i+=len(labels)\n\t\t\tleft_sents=torch.transpose(left_sents,0,1)\n\t\t\tright_sents=torch.transpose(right_sents,0,1)\n\t\t\toutput=model(left_sents,lsize_list,right_sents,rsize_list)\n\t\t\tresult = output.data.cpu().numpy()\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = labels.data.cpu().numpy()\n\t\t\ttrain_num_correct += np.sum(a == b)\n\t\t\tloss = criterion(output, labels)\n\t\t\toptimizer.zero_grad()\n\t\t\tfor pg in optimizer.param_groups:\n\t\t\t\tpg[\'lr\'] = lr\n\t\t\tloss.backward()\n\t\t\toptimizer.step()\n\t\t\tbatch_counter += 1\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\tif batch_counter % report_interval == 0:\n\t\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\t\tmsg += \'\\t train batch loss: %f\' % (accumulated_loss / train_sents_scaned)\n\t\t\t\tmsg += \'\\t train accuracy: %f\' % (train_num_correct / train_sents_scaned)\n\t\t\t\tprint(msg)\n\t\t# valid after each epoch\n\t\tmodel.eval()\n\t\tdev_batch_index = 0\n\t\tdev_num_correct = 0\n\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\taccumulated_loss = 0\n\t\tdev_batch_i = 0\n\t\tpred=[]\n\t\tgold=[]\n\t\twhile dev_batch_i < len(dev_pairs):\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(dev_pairs, dev_batch_i,\n\t\t\t                                                                       dev_batch_i+batch_size)\n\t\t\tdev_batch_i += len(labels)\n\t\t\tleft_sents = torch.transpose(left_sents, 0, 1)\n\t\t\tright_sents = torch.transpose(right_sents, 0, 1)\n\t\t\toutput = model(left_sents, lsize_list,right_sents, rsize_list)\n\t\t\tresult = output.data.cpu().numpy()\n\t\t\tloss = criterion(output, labels)\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = labels.data.cpu().numpy()\n\t\t\tdev_num_correct += np.sum(a == b)\n\t\t\tif task==\'pit\' or task==\'url\' or task==\'wikiqa\' or task==\'trecqa\':\n\t\t\t\tpred.extend(result[:,1])\n\t\t\t\tgold.extend(b)\n\t\t\tif task==\'sts\':\n\t\t\t\tpred.extend(0*result[:,0]+1*result[:,1]+2*result[:,2]+3*result[:,3]+4*result[:,4]+5*result[:,5])\n\t\t\t\tgold.extend(0*b[:,0]+1*b[:,1]+2*b[:,2]+3*b[:,3]+4*b[:,4]+5*b[:,5])\n\t\tmsg += \'\\t dev loss: %f\' % (accumulated_loss/len(dev_pairs))\n\t\tdev_acc = dev_num_correct / len(dev_pairs)\n\t\tmsg += \'\\t dev accuracy: %f\' % dev_acc\n\t\tprint(msg)\n\t\tif task==\'pit\' or task==\'url\'or task==\'wikiqa\' or task==\'trecqa\':\n\t\t\tURL_maxF1_eval(pred, gold)\n\t\telif task==\'sts\':\n\t\t\tprint(\'pearson: \'+str(pearson(pred,gold)))\n\t\tif accumulated_loss < best_dev_loss:\n\t\t\tbest_dev_loss=accumulated_loss\n\t\t\ttorch.save(model, basepath+\'/model_SSE_\'+task+\'.pkl\')\n\t\t\ttest_batch_index = 0\n\t\t\ttest_num_correct = 0\n\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\taccumulated_loss = 0\n\t\t\ttest_batch_i = 0\n\t\t\tpred = []\n\t\t\tgold = []\n\t\t\twhile test_batch_i < len(test_pairs):\n\t\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(test_pairs, test_batch_i, test_batch_i+batch_size)\n\t\t\t\tleft_sents = torch.transpose(left_sents, 0, 1)\n\t\t\t\tright_sents = torch.transpose(right_sents, 0, 1)\n\t\t\t\ttest_batch_i+=len(labels)\n\t\t\t\toutput = model(left_sents, lsize_list, right_sents, rsize_list)\n\t\t\t\tresult = output.data.cpu().numpy()\n\t\t\t\tloss = criterion(output, labels)\n\t\t\t\taccumulated_loss += loss.data[0]\n\t\t\t\ta = np.argmax(result, axis=1)\n\t\t\t\tb = labels.data.cpu().numpy()\n\t\t\t\ttest_num_correct += np.sum(a == b)\n\t\t\t\tif task == \'pit\' or task == \'url\'or task==\'wikiqa\' or task==\'trecqa\':\n\t\t\t\t\tpred.extend(result[:, 1])\n\t\t\t\t\tgold.extend(b)\n\t\t\t\tif task == \'sts\':\n\t\t\t\t\tpred.extend(0 * result[:, 0] + 1 * result[:, 1] + 2 * result[:, 2] + 3 * result[:, 3] + 4 * result[:,4] + 5 * result[:, 5])\n\t\t\t\t\tgold.extend(0 * b[:, 0] + 1 * b[:, 1] + 2 * b[:, 2] + 3 * b[:, 3] + 4 * b[:, 4] + 5 * b[:, 5])\n\t\t\tmsg += \'\\t test loss: %f\' % (accumulated_loss/len(test_pairs))\n\t\t\ttest_acc = test_num_correct / len(test_pairs)\n\t\t\tmsg += \'\\t test accuracy: %f\' % test_acc\n\t\t\tprint(msg)\n\t\t\tif task == \'pit\' or task == \'url\'or task==\'wikiqa\' or task==\'trecqa\':\n\t\t\t\tURL_maxF1_eval(pred, gold)\n\t\t\t\tif task == \'wikiqa\' or task == \'trecqa\':\n\t\t\t\t\tlist1 = []\n\t\t\t\t\tfor line in open(basepath+\'/test.qrel\'):\n\t\t\t\t\t\tlist1.append(line.strip().split())\n\t\t\t\t\twith open(basepath + \'/result_\' + task, \'w\') as f:\n\t\t\t\t\t\tfor i in range(len(pred)):\n\t\t\t\t\t\t\tf.writelines(str(list1[i][0]) + \'\\t\' + str(list1[i][1]) + \'\\t\' + str(\n\t\t\t\t\t\t\t\tlist1[i][2]) + \'\\t\' + \'*\\t\' + str(pred[i]) + \'\\t\' + \'*\\n\')\n\t\t\telif task == \'sts\':\n\t\t\t\tprint(\'pearson: \' + str(pearson(pred, gold)))\n\t\telapsed_time = time.time() - start_time\n\t\tprint(\'Epoch \' + str(epoch) + \' finished within \' + str(timedelta(seconds=elapsed_time)))\n'"
SSE/main_wikiqa.py,33,"b'from __future__ import division\nimport torch\nimport os\nfrom model import *\nfrom torch import optim\nimport torch.nn as nn\nfrom datetime import datetime\nfrom torch_util import *\nimport config\nimport tqdm\nimport data_loader\nimport sys\nimport time\nfrom datetime import timedelta\nfrom os.path import expanduser\nfrom torchtext.vocab import load_word_vectors\n\ndef create_batch(data,from_index, to_index):\n\tif to_index>len(data):\n\t\tto_index=len(data)\n\tlsize=0\n\trsize=0\n\tlsize_list=[]\n\trsize_list=[]\n\tfor i in range(from_index, to_index):\n\t\tlength=len(data[i][0])+2\n\t\tlsize_list.append(length)\n\t\tif length>lsize:\n\t\t\tlsize=length\n\t\tlength=len(data[i][1])+2\n\t\trsize_list.append(length)\n\t\tif length>rsize:\n\t\t\trsize=length\n\t#lsize+=1\n\t#rsize+=1\n\tlsent = data[from_index][0]\n\tlsent = [\'bos\']+lsent + [\'oov\' for k in range(lsize -1 - len(lsent))]\n\t#print(lsent)\n\tleft_sents = [[word2id[word] for word in lsent]]\n\t#left_sents = torch.cat((dict[word].view(1, -1) for word in lsent))\n\t#left_sents = torch.unsqueeze(left_sents,0)\n\n\trsent = data[from_index][1]\n\trsent = [\'bos\']+rsent + [\'oov\' for k in range(rsize -1 - len(rsent))]\n\t#print(rsent)\n\tright_sents = [[word2id[word] for word in rsent]]\n\t#right_sents = torch.cat((dict[word].view(1, -1) for word in rsent))\n\t#right_sents = torch.unsqueeze(right_sents,0)\n\n\tlabels=[data[from_index][2]]\n\n\tfor i in range(from_index+1, to_index):\n\n\t\tlsent=data[i][0]\n\t\tlsent=[\'bos\']+lsent+[\'oov\' for k in range(lsize -1 - len(lsent))]\n\t\t#print(lsent)\n\t\tleft_sents.append([word2id[word] for word in lsent])\n\t\t#left_sent = torch.cat((dict[word].view(1,-1) for word in lsent))\n\t\t#left_sent = torch.unsqueeze(left_sent, 0)\n\t\t#left_sents = torch.cat([left_sents, left_sent])\n\n\t\trsent=data[i][1]\n\t\trsent=[\'bos\']+rsent+[\'oov\' for k in range(rsize -1 - len(rsent))]\n\t\t#print(rsent)\n\t\tright_sents.append([word2id[word] for word in rsent])\n\t\t#right_sent = torch.cat((dict[word].view(1,-1) for word in rsent))\n\t\t#right_sent = torch.unsqueeze(right_sent, 0)\n\t\t#right_sents = torch.cat((right_sents, right_sent))\n\n\t\tlabels.append(data[i][2])\n\n\tleft_sents=Variable(torch.LongTensor(left_sents))\n\tright_sents=Variable(torch.LongTensor(right_sents))\n\tif task==\'sts\':\n\t\tlabels=Variable(torch.Tensor(labels))\n\telse:\n\t\tlabels=Variable(torch.LongTensor(labels))\n\tlsize_list=torch.LongTensor(lsize_list)\n\trsize_list =torch.LongTensor(rsize_list)\n\n\tif torch.cuda.is_available():\n\t\tleft_sents=left_sents.cuda()\n\t\tright_sents=right_sents.cuda()\n\t\tlabels=labels.cuda()\n\t\tlsize_list=lsize_list.cuda()\n\t\trsize_list=rsize_list.cuda()\n\t#print(left_sents)\n\t#print(right_sents)\n\treturn left_sents, right_sents, labels, lsize_list, rsize_list\n\nif __name__ == \'__main__\':\n\ttask=\'wikiqa\'\n\tprint(\'task: \'+task)\n\ttorch.manual_seed(6)\n\n\tnum_class = 2\n\tif torch.cuda.is_available():\n\t\tprint(\'CUDA is available!\')\n\t\tbasepath = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/wikiqa\'\n\t\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\tcastorini_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/castorini/trec_eval.9.0/trec_eval\'\n\telse:\n\t\tbasepath = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/wikiqa\'\n\t\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\t\tcastorini_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/castorini/trec_eval.9.0/trec_eval\'\n\ttrain_pairs = readQuoradata(basepath + \'/train/\')\n\tdev_pairs = readQuoradata(basepath + \'/dev/\')\n\ttest_pairs = readQuoradata(basepath + \'/test/\')\n\t#dev_pairs=test_pairs\n\tcmd = (\'%s -m map %s %s\' % (castorini_path, basepath + \'/test.qrel\', basepath + \'/result_SSE_\' + task))\n\tos.system(cmd)\n\tcmd = (\'%s -m recip_rank %s %s\' % (castorini_path, basepath + \'/test.qrel\', basepath + \'/result_SSE_\' + task))\n\tos.system(cmd)\n\ttokens = []\n\tdict={}\n\tword2id={}\n\tvocab = set()\n\tfor pair in train_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in dev_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\tfor pair in test_pairs:\n\t\tleft = pair[0]\n\t\tright = pair[1]\n\t\tvocab |= set(left)\n\t\tvocab |= set(right)\n\ttokens=list(vocab)\n\t#for line in open(basepath + \'/vocab.txt\'):\n\t#\ttokens.append(line.strip().decode(\'utf-8\'))\n\twv_dict, wv_arr, wv_size = load_word_vectors(embedding_path, \'glove.840B\', 300)\n\t#embedding = []\n\ttokens.append(\'oov\')\n\ttokens.append(\'bos\')\n\tpretrained_emb = np.zeros(shape=(len(tokens), 300))\n\toov={}\n\tfor id in range(100):\n\t\toov[id]=torch.normal(torch.zeros(300),std=1)\n\tid=0\n\tfor word in tokens:\n\t\ttry:\n\t\t\tdict[word] = wv_arr[wv_dict[word]]/torch.norm(wv_arr[wv_dict[word]])\n\t\t\t#print(word)\n\t\texcept:\n\t\t\tdict[word] = torch.normal(torch.zeros(300),std=1)\n\t\tword2id[word]=id\n\t\tpretrained_emb[id] = dict[word].numpy()\n\t\tid+=1\n\n\tmodel = StackBiLSTMMaxout(h_size=[512, 1024, 2048], v_size=10, d=300, mlp_d=1600, dropout_r=0.1, max_l=60, num_class=num_class)\n\tif torch.cuda.is_available():\n\t\tpretrained_emb=torch.Tensor(pretrained_emb).cuda()\n\telse:\n\t\tpretrained_emb = torch.Tensor(pretrained_emb)\n\tmodel.Embd.weight.data = pretrained_emb\n\n\tif torch.cuda.is_available():\n\t\tmodel.cuda()\n\n\tstart_lr = 2e-4\n\tbatch_size=32\n\treport_interval=1000\n\toptimizer = optim.Adam(model.parameters(), lr=start_lr)\n\tif task==\'sts\':\n\t\tcriterion=nn.KLDivLoss()\n\telse:\n\t\tcriterion = nn.CrossEntropyLoss()\n\n\titerations = 0\n\n\tbest_m_dev = -1\n\tbest_um_dev = -1\n\tbest_dev_loss=10e10\n\n\tprint(\'start training...\')\n\tfor epoch in range(100):\n\t\tbatch_counter = 0\n\t\taccumulated_loss = 0\n\t\tmodel.train()\n\t\tprint(\'--\' * 20)\n\t\tstart_time = time.time()\n\t\ti_decay = epoch / 2\n\t\tlr = start_lr / (2 ** i_decay)\n\t\tprint(lr)\n\t\ttrain_pairs = np.array(train_pairs)\n\t\trand_idx = np.random.permutation(len(train_pairs))\n\t\ttrain_pairs = train_pairs[rand_idx]\n\t\ttrain_batch_i = 0\n\t\ttrain_num_correct=0\n\t\ttrain_sents_scaned = 0\n\t\twhile train_batch_i < len(train_pairs):\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(train_pairs, train_batch_i, train_batch_i+batch_size)\n\t\t\ttrain_sents_scaned += len(labels)\n\t\t\ttrain_batch_i+=len(labels)\n\t\t\tleft_sents=torch.transpose(left_sents,0,1)\n\t\t\tright_sents=torch.transpose(right_sents,0,1)\n\t\t\toutput=model(left_sents,lsize_list,right_sents,rsize_list)\n\t\t\tresult = output.data.cpu().numpy()\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = labels.data.cpu().numpy()\n\t\t\ttrain_num_correct += np.sum(a == b)\n\t\t\tloss = criterion(output, labels)\n\t\t\toptimizer.zero_grad()\n\t\t\tfor pg in optimizer.param_groups:\n\t\t\t\tpg[\'lr\'] = lr\n\t\t\tloss.backward()\n\t\t\toptimizer.step()\n\t\t\tbatch_counter += 1\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\tif batch_counter % report_interval == 0:\n\t\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\t\tmsg += \'\\t train batch loss: %f\' % (accumulated_loss / train_sents_scaned)\n\t\t\t\t#msg += \'\\t train accuracy: %f\' % (train_num_correct / train_sents_scaned)\n\t\t\t\tprint(msg)\n\t\t# valid after each epoch\n\t\tmodel.eval()\n\t\tdev_batch_index = 0\n\t\tdev_num_correct = 0\n\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\taccumulated_loss = 0\n\t\tdev_batch_i = 0\n\t\tpred=[]\n\t\tgold=[]\n\t\twhile dev_batch_i < len(dev_pairs):\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(dev_pairs, dev_batch_i,\n\t\t\t                                                                       dev_batch_i+batch_size)\n\t\t\tdev_batch_i += len(labels)\n\t\t\tleft_sents = torch.transpose(left_sents, 0, 1)\n\t\t\tright_sents = torch.transpose(right_sents, 0, 1)\n\t\t\toutput = F.softmax(model(left_sents, lsize_list,right_sents, rsize_list))\n\t\t\tresult = output.data.cpu().numpy()\n\t\t\tloss = criterion(output, labels)\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = labels.data.cpu().numpy()\n\t\t\tdev_num_correct += np.sum(a == b)\n\t\t\tif task==\'pit\' or task==\'url\' or task==\'wikiqa\' or task==\'trecqa\':\n\t\t\t\tpred.extend(result[:,1])\n\t\t\t\tgold.extend(b)\n\t\t\tif task==\'sts\':\n\t\t\t\tpred.extend(0*result[:,0]+1*result[:,1]+2*result[:,2]+3*result[:,3]+4*result[:,4]+5*result[:,5])\n\t\t\t\tgold.extend(0*b[:,0]+1*b[:,1]+2*b[:,2]+3*b[:,3]+4*b[:,4]+5*b[:,5])\n\t\tmsg += \'\\t dev loss: %f\' % accumulated_loss\n\t\tdev_acc = dev_num_correct / len(dev_pairs)\n\t\t#msg += \'\\t dev accuracy: %f\' % dev_acc\n\t\tprint(msg)\n\t\tif task==\'pit\' or task==\'url\'or task==\'wikiqa\' or task==\'trecqa\':\n\t\t\tURL_maxF1_eval(pred, gold)\n\t\t# Test after each epoch\n\t\ttest_batch_index = 0\n\t\ttest_num_correct = 0\n\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\taccumulated_loss = 0\n\t\ttest_batch_i = 0\n\t\tpred = []\n\t\tgold = []\n\t\twhile test_batch_i < len(test_pairs):\n\t\t\tleft_sents, right_sents, labels, lsize_list, rsize_list = create_batch(test_pairs, test_batch_i, test_batch_i+batch_size)\n\t\t\tleft_sents = torch.transpose(left_sents, 0, 1)\n\t\t\tright_sents = torch.transpose(right_sents, 0, 1)\n\t\t\ttest_batch_i+=len(labels)\n\t\t\toutput = F.softmax(model(left_sents, lsize_list, right_sents, rsize_list))\n\t\t\tresult = output.data.cpu().numpy()\n\t\t\tloss = criterion(output, labels)\n\t\t\taccumulated_loss += loss.data[0]\n\t\t\ta = np.argmax(result, axis=1)\n\t\t\tb = labels.data.cpu().numpy()\n\t\t\ttest_num_correct += np.sum(a == b)\n\t\t\tif task == \'pit\' or task == \'url\'or task==\'wikiqa\' or task==\'trecqa\':\n\t\t\t\tpred.extend(result[:, 1])\n\t\t\t\tgold.extend(b)\n\t\t\tif task == \'sts\':\n\t\t\t\tpred.extend(0 * result[:, 0] + 1 * result[:, 1] + 2 * result[:, 2] + 3 * result[:, 3] + 4 * result[:,4] + 5 * result[:, 5])\n\t\t\t\tgold.extend(0 * b[:, 0] + 1 * b[:, 1] + 2 * b[:, 2] + 3 * b[:, 3] + 4 * b[:, 4] + 5 * b[:, 5])\n\t\tmsg += \'\\t test loss: %f\' % accumulated_loss\n\t\ttest_acc = test_num_correct / len(test_pairs)\n\t\t#msg += \'\\t test accuracy: %f\' % test_acc\n\t\tprint(msg)\n\t\tif task == \'pit\' or task == \'url\'or task==\'wikiqa\' or task==\'trecqa\':\n\t\t\tURL_maxF1_eval(pred, gold)\n\t\t\twith open(basepath + \'/prob_SSE_\' + task, \'w\') as f:\n\t\t\t\tfor i in range(len(pred)):\n\t\t\t\t\tf.writelines(str(pred[i]) + \'\\n\')\n\t\t\tif task == \'wikiqa\' or task == \'trecqa\':\n\t\t\t\tlist1 = []\n\t\t\t\tfor line in open(basepath+\'/test.qrel\'):\n\t\t\t\t\tlist1.append(line.strip().split())\n\t\t\t\twith open(basepath + \'/result_SSE_\' + task, \'w\') as f:\n\t\t\t\t\tfor i in range(len(pred)):\n\t\t\t\t\t\tf.writelines(str(list1[i][0]) + \'\\t\' + str(list1[i][1]) + \'\\t\' + str(\n\t\t\t\t\t\t\tlist1[i][2]) + \'\\t\' + \'*\\t\' + str(pred[i]) + \'\\t\' + \'*\\n\')\n\t\t\tcmd = (\'%s -m map %s %s\' % (castorini_path, basepath + \'/test.qrel\', basepath + \'/result_SSE_\' + task))\n\t\t\tos.system(cmd)\n\t\t\tcmd = (\'%s -m recip_rank %s %s\' % (castorini_path, basepath + \'/test.qrel\', basepath + \'/result_SSE_\' + task))\n\t\t\tos.system(cmd)\n\t\telapsed_time = time.time() - start_time\n\t\tprint(\'Epoch \' + str(epoch) + \' finished within \' + str(timedelta(seconds=elapsed_time)))\n'"
SSE/mnli.py,0,"b'import os\n# from util.data_loader import RParsedTextLField\n# from util.data_loader import ParsedTextLField\n\nfrom torchtext import data, vocab\nfrom torchtext import datasets\n\nimport config\nimport torch\n\n\nclass MNLI(data.ZipDataset, data.TabularDataset):\n    # url = \'http://nlp.stanford.edu/projects/snli/snli_1.0.zip\'\n    filename = \'multinli_0.9.zip\'\n    dirname = \'multinli_0.9\'\n\n    @staticmethod\n    def sort_key(ex):\n        return data.interleave_keys(\n            len(ex.premise), len(ex.hypothesis))\n\n    @classmethod\n    def splits(cls, text_field, label_field, parse_field=None, genre_field=None, root=\'.\',\n               train=None, validation=None, test=None):\n        """"""Create dataset objects for splits of the SNLI dataset.\n        This is the most flexible way to use the dataset.\n        Arguments:\n            text_field: The field that will be used for premise and hypothesis\n                data.\n            label_field: The field that will be used for label data.\n            parse_field: The field that will be used for shift-reduce parser\n                transitions, or None to not include them.\n            root: The root directory that the dataset\'s zip archive will be\n                expanded into; therefore the directory in whose snli_1.0\n                subdirectory the data files will be stored.\n            train: The filename of the train data. Default: \'train.jsonl\'.\n            validation: The filename of the validation data, or None to not\n                load the validation set. Default: \'dev.jsonl\'.\n            test: The filename of the test data, or None to not load the test\n                set. Default: \'test.jsonl\'.\n        """"""\n        path = cls.download_or_unzip(root)\n        if parse_field is None:\n            return super(MNLI, cls).splits(\n                os.path.join(path, \'multinli_0.9_\'), train, validation, test,\n                format=\'json\', fields={\'sentence1\': (\'premise\', text_field),\n                                       \'sentence2\': (\'hypothesis\', text_field),\n                                       \'gold_label\': (\'label\', label_field)},\n                filter_pred=lambda ex: ex.label != \'-\')\n        return super(MNLI, cls).splits(\n            os.path.join(path, \'multinli_0.9_\'), train, validation, test,\n            format=\'json\', fields={\'sentence1_binary_parse\':\n                                   [(\'premise\', text_field),\n                                    (\'premise_transitions\', parse_field)],\n                                   \'sentence2_binary_parse\':\n                                   [(\'hypothesis\', text_field),\n                                    (\'hypothesis_transitions\', parse_field)],\n                                   \'gold_label\': (\'label\', label_field),\n                                   \'genre\': (\'genre\', genre_field)},\n            filter_pred=lambda ex: ex.label != \'-\')\n\nif __name__ == ""__main__"":\n    pass'"
SSE/model.py,11,"b'import torch.nn as nn\nimport torch_util\nimport torch\nimport torch.nn.functional as F\n\nclass StackBiLSTMMaxout(nn.Module):\n\tdef __init__(self, h_size, v_size=10, d=300, mlp_d=1600, dropout_r=0.1, max_l=60, num_class=3):\n\t\tsuper(StackBiLSTMMaxout, self).__init__()\n\t\tself.Embd = nn.Embedding(v_size, d)\n\n\t\tself.lstm = nn.LSTM(input_size=d, hidden_size=h_size[0],\n\t\t\t\t\t\t\tnum_layers=1, bidirectional=True)\n\n\t\tself.lstm_1 = nn.LSTM(input_size=(d + h_size[0] * 2), hidden_size=h_size[1],\n\t\t\t\t\t\t\t  num_layers=1, bidirectional=True)\n\n\t\tself.lstm_2 = nn.LSTM(input_size=(d + (h_size[0] + h_size[1]) * 2), hidden_size=h_size[2],\n\t\t\t\t\t\t\t  num_layers=1, bidirectional=True)\n\n\t\tself.max_l = max_l\n\t\tself.h_size = h_size\n\n\t\tself.mlp_1 = nn.Linear(h_size[2] * 2 * 4, mlp_d)\n\t\tself.mlp_2 = nn.Linear(mlp_d, mlp_d)\n\t\tself.sm = nn.Linear(mlp_d, num_class)\n\n\t\tself.classifier = nn.Sequential(*[self.mlp_1, nn.ReLU(), nn.Dropout(dropout_r),\n\t\t\t\t\t\t\t\t\t\t  self.mlp_2, nn.ReLU(), nn.Dropout(dropout_r),\n\t\t\t\t\t\t\t\t\t\t  self.sm])\n\n\tdef display(self):\n\t\tfor param in self.parameters():\n\t\t\tprint(param.data.size())\n\n\tdef forward(self, s1, l1, s2, l2):\n\t\tif self.max_l:\n\t\t\tl1 = l1.clamp(max=self.max_l)\n\t\t\tl2 = l2.clamp(max=self.max_l)\n\t\t\tif s1.size(0) > self.max_l:\n\t\t\t\ts1 = s1[:self.max_l, :]\n\t\t\tif s2.size(0) > self.max_l:\n\t\t\t\ts2 = s2[:self.max_l, :]\n\n\t\tp_s1 = self.Embd(s1)\n\t\tp_s2 = self.Embd(s2)\n\n\t\ts1_layer1_out = torch_util.auto_rnn_bilstm(self.lstm, p_s1, l1)\n\t\ts2_layer1_out = torch_util.auto_rnn_bilstm(self.lstm, p_s2, l2)\n\n\t\t# Length truncate\n\t\tlen1 = s1_layer1_out.size(0)\n\t\tlen2 = s2_layer1_out.size(0)\n\t\tp_s1 = p_s1[:len1, :, :] # [T, B, D]\n\t\tp_s2 = p_s2[:len2, :, :] # [T, B, D]\n\n\t\t# Using residual connection\n\t\ts1_layer2_in = torch.cat([p_s1, s1_layer1_out], dim=2)\n\t\ts2_layer2_in = torch.cat([p_s2, s2_layer1_out], dim=2)\n\n\t\ts1_layer2_out = torch_util.auto_rnn_bilstm(self.lstm_1, s1_layer2_in, l1)\n\t\ts2_layer2_out = torch_util.auto_rnn_bilstm(self.lstm_1, s2_layer2_in, l2)\n\n\t\ts1_layer3_in = torch.cat([p_s1, s1_layer1_out, s1_layer2_out], dim=2)\n\t\ts2_layer3_in = torch.cat([p_s2, s2_layer1_out, s2_layer2_out], dim=2)\n\n\t\ts1_layer3_out = torch_util.auto_rnn_bilstm(self.lstm_2, s1_layer3_in, l1)\n\t\ts2_layer3_out = torch_util.auto_rnn_bilstm(self.lstm_2, s2_layer3_in, l2)\n\n\t\ts1_layer3_maxout = torch_util.max_along_time(s1_layer3_out, l1)\n\t\ts2_layer3_maxout = torch_util.max_along_time(s2_layer3_out, l2)\n\n\t\t# Only use the last layer\n\t\tfeatures = torch.cat([s1_layer3_maxout, s2_layer3_maxout,\n\t\t\t\t\t\t\t  torch.abs(s1_layer3_maxout - s2_layer3_maxout),\n\t\t\t\t\t\t\t  s1_layer3_maxout * s2_layer3_maxout],\n\t\t\t\t\t\t\t dim=1)\n\n\t\tout = self.classifier(features)\n\t\treturn out\n\ndef model_eval(model, data_iter, criterion, pred=False):\n\tmodel.eval()\n\tdata_iter.init_epoch()\n\tn_correct = loss = 0\n\ttotoal_size = 0\n\tprob=[]\n\tif not pred:\n\t\tfor batch_idx, batch in enumerate(data_iter):\n\n\t\t\ts1, s1_l = batch.premise\n\t\t\ts2, s2_l = batch.hypothesis\n\t\t\ty = batch.label.data - 1\n\n\t\t\tpred = model(s1, s1_l - 1, s2, s2_l - 1)\n\t\t\tn_correct += (torch.max(pred, 1)[1].view(batch.label.size()).data == y).sum()\n\n\t\t\tloss += criterion(pred, batch.label - 1).data[0] * batch.batch_size\n\t\t\ttotoal_size += batch.batch_size\n\n\t\t\toutput=F.softmax(pred)\n\t\t\tresult = output.data.cpu().numpy()\n\t\t\tprob.extend(result)\n\n\t\tavg_acc = 100. * n_correct / totoal_size\n\t\tavg_loss = loss / totoal_size\n\n\t\treturn avg_acc, avg_loss, prob\n\telse:\n\t\tpred_list = []\n\t\tfor batch_idx, batch in enumerate(data_iter):\n\n\t\t\ts1, s1_l = batch.premise\n\t\t\ts2, s2_l = batch.hypothesis\n\n\t\t\tpred = model(s1, s1_l - 1, s2, s2_l - 1)\n\t\t\tpred_list.append(torch.max(pred, 1)[1].view(batch.label.size()).data)\n\n\t\treturn torch.cat(pred_list, dim=0)'"
SSE/test_quora.py,13,"b""from __future__ import division\r\nimport os\r\nimport sys \r\nimport time\r\nimport torch\r\nimport random\r\nimport pickle\r\nfrom model import *\r\nimport torch.nn as nn\r\nfrom torch import optim\r\nfrom torch_util import *\r\nfrom datetime import datetime\r\nfrom datetime import timedelta\r\nfrom collections import Counter\r\nfrom torchtext.vocab import load_word_vectors\r\n\r\n\r\nimport pdb\r\n\r\n\r\ndef create_batch(data,from_index, to_index):\r\n  if to_index > len(data):\r\n    to_index = len(data)\r\n  lsize, rsize = 0, 0\r\n  lsize_list, rsize_list = [], []\r\n  for i in range(from_index, to_index):\r\n    length = len(data[i][0])\r\n    lsize_list.append(length)\r\n    if length > lsize:\r\n      lsize = length\r\n    length = len(data[i][1])\r\n    rsize_list.append(length)\r\n    if length > rsize:\r\n      rsize = length\r\n\r\n  left_sents, right_sents, labels = [], [], []\r\n  for i in range(from_index, to_index):\r\n    lsent = data[i][0]\r\n    lsent = lsent + ['<pad>' for k in range(lsize - len(lsent))]\r\n    left_sents.append([word2id[word] for word in lsent])\r\n    rsent = data[i][1]\r\n    rsent = rsent + ['<pad>' for k in range(rsize - len(rsent))]\r\n    right_sents.append([word2id[word] for word in rsent])\r\n    labels.append(data[i][2])\r\n\r\n  left_sents=Variable(torch.LongTensor(left_sents))\r\n  right_sents=Variable(torch.LongTensor(right_sents))\r\n  labels=Variable(torch.LongTensor(labels))\r\n  lsize_list=torch.LongTensor(lsize_list)\r\n  rsize_list =torch.LongTensor(rsize_list)\r\n\r\n  if torch.cuda.is_available():\r\n    left_sents=left_sents.cuda()\r\n    right_sents=right_sents.cuda()\r\n    labels=labels.cuda()\r\n    lsize_list=lsize_list.cuda()\r\n    rsize_list=rsize_list.cuda()\r\n  return left_sents, right_sents, labels, lsize_list, rsize_list\r\n\r\n\r\nif __name__ == '__main__':\r\n  task='quora'\r\n  print('task: '+task)\r\n  torch.manual_seed(6)\r\n\r\n  num_class = 2\r\n  if torch.cuda.is_available():\r\n    print('CUDA is available!')\r\n  \r\n  basepath = './data'\r\n  test_pairs = readQuoradata(basepath + '/test/', 1000)\r\n\r\n  with open(os.path.join('./results', 'vocab.pkl'), 'rb') as f:\r\n    tokens, word2id = pickle.load(f)\r\n\r\n  model = StackBiLSTMMaxout(h_size=[512, 1024, 2048], \r\n                            v_size=len(tokens), \r\n                            d=300, \r\n                            mlp_d=1600, \r\n                            dropout_r=0.1, \r\n                            max_l=60, \r\n                            num_class=num_class)\r\n\r\n\r\n  batch_size=32\r\n  criterion = nn.CrossEntropyLoss()\r\n\r\n  ckpt_path = os.path.join('./results', '%s_%d.pkl' % (task, 8))\r\n  model.load_state_dict(torch.load(ckpt_path))\r\n  model.eval()\r\n  if torch.cuda.is_available():\r\n    model.cuda()\r\n  \r\n  # Test\r\n  start_time = time.time()\r\n  test_batch_index = 0\r\n  test_num_correct = 0\r\n  msg = ''\r\n  accumulated_loss = 0\r\n  test_batch_i = 0\r\n  pred = []\r\n  gold = []\r\n  while test_batch_i < len(test_pairs):\r\n    left_sents, right_sents, labels, lsize_list, rsize_list = create_batch(\r\n        test_pairs, test_batch_i, test_batch_i+batch_size)\r\n    left_sents = torch.transpose(left_sents, 0, 1)\r\n    right_sents = torch.transpose(right_sents, 0, 1)\r\n    test_batch_i+=len(labels)\r\n    output = model(left_sents, lsize_list, right_sents, rsize_list)\r\n    result = output.data.cpu().numpy()\r\n    loss = criterion(output, labels)\r\n    # accumulated_loss += loss.data[0]\r\n    accumulated_loss += loss.item()\r\n    a = np.argmax(result, axis=1)\r\n    b = labels.data.cpu().numpy()\r\n    test_num_correct += np.sum(a == b)\r\n  \r\n  elapsed_time = time.time() - start_time\r\n  print('Test finished within ' + str(timedelta(seconds=elapsed_time)))\r\n  msg += '\\t test loss: %.4f' % accumulated_loss\r\n  test_acc = test_num_correct / len(test_pairs)\r\n  msg += '\\t test accuracy: %.4f' % test_acc\r\n  print(msg)\r\n\r\n"""
SSE/torch_util.py,23,"b'from __future__ import division\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nimport os\nimport sys\nimport math\nimport config\nfrom numpy import linalg as LA\n\ndef pearson(x,y):\n\tx=np.array(x)\n\ty=np.array(y)\n\tx=x-np.mean(x)\n\ty=y-np.mean(y)\n\treturn x.dot(y)/(LA.norm(x)*LA.norm(y))\n\ndef URL_maxF1_eval(predict_result,test_data_label):\n\ttest_data_label=[item>=1 for item in test_data_label]\n\tcounter = 0\n\ttp = 0.0\n\tfp = 0.0\n\tfn = 0.0\n\ttn = 0.0\n\n\tfor i, t in enumerate(predict_result):\n\n\t\tif t>0.5:\n\t\t\tguess=True\n\t\telse:\n\t\t\tguess=False\n\t\tlabel = test_data_label[i]\n\t\t#print guess, label\n\t\tif guess == True and label == False:\n\t\t\tfp += 1.0\n\t\telif guess == False and label == True:\n\t\t\tfn += 1.0\n\t\telif guess == True and label == True:\n\t\t\ttp += 1.0\n\t\telif guess == False and label == False:\n\t\t\ttn += 1.0\n\t\tif label == guess:\n\t\t\tcounter += 1.0\n\t\t#else:\n\t\t\t#print label+\'--\'*20\n\t\t\t# if guess:\n\t\t\t# print ""GOLD-"" + str(label) + ""\\t"" + ""SYS-"" + str(guess) + ""\\t"" + sent1 + ""\\t"" + sent2\n\n\ttry:\n\t\tP = tp / (tp + fp)\n\t\tR = tp / (tp + fn)\n\t\tF = 2 * P * R / (P + R)\n\texcept:\n\t\tP=0\n\t\tR=0\n\t\tF=0\n\n\t#print ""PRECISION: %s, RECALL: %s, F1: %s"" % (P, R, F)\n\t#print ""ACCURACY: %s"" % (counter/len(predict_result))\n\taccuracy=counter/len(predict_result)\n\n\t#print ""# true pos:"", tp\n\t#print ""# false pos:"", fp\n\t#print ""# false neg:"", fn\n\t#print ""# true neg:"", tn\n\tmaxF1=0\n\tP_maxF1=0\n\tR_maxF1=0\n\tprobs = predict_result\n\tsortedindex = sorted(range(len(probs)), key=probs.__getitem__)\n\tsortedindex.reverse()\n\n\ttruepos=0\n\tfalsepos=0\n\tfor sortedi in sortedindex:\n\t\tif test_data_label[sortedi]==True:\n\t\t\ttruepos+=1\n\t\telif test_data_label[sortedi]==False:\n\t\t\tfalsepos+=1\n\t\tprecision=0\n\t\tif truepos+falsepos>0:\n\t\t\tprecision=truepos/(truepos+falsepos)\n\n\t\trecall=truepos/(tp+fn)\n\t\tf1=0\n\t\tif precision+recall>0:\n\t\t\tf1=2*precision*recall/(precision+recall)\n\t\t\tif f1>maxF1:\n\t\t\t\t#print probs[sortedi]\n\t\t\t\tmaxF1=f1\n\t\t\t\tP_maxF1=precision\n\t\t\t\tR_maxF1=recall\n\tprint ""PRECISION: %s, RECALL: %s, max_F1: %s"" % (P_maxF1, R_maxF1, maxF1)\n\treturn (accuracy, maxF1)\n\ndef readSTSdata(dir):\n\t#print(len(dict))\n\t#print(dict[\'bmxs\'])\n\tlsents=[]\n\trsents=[]\n\tlabels=[]\n\tfor line in open(dir+\'a.toks\'):\n\t\tline = line.decode(\'utf-8\')\n\t\tpieces=line.strip().split()\n\t\tlsents.append(pieces)\n\tfor line in open(dir+\'b.toks\'):\n\t\tline = line.decode(\'utf-8\')\n\t\tpieces = line.strip().split()\n\t\trsents.append(pieces)\n\tfor line in open(dir + \'sim.txt\'):\n\t\tsim = float(line.strip())\n\t\tceil = int(math.ceil(sim))\n\t\tfloor = int(math.floor(sim))\n\t\ttmp = [0, 0, 0, 0, 0, 0]\n\t\tif floor != ceil:\n\t\t\ttmp[ceil] = sim - floor\n\t\t\ttmp[floor] = ceil - sim\n\t\telse:\n\t\t\ttmp[floor] = 1\n\t\tlabels.append(tmp)\n\t#data=(lsents,rsents,labels)\n\tif not len(lsents)==len(rsents)==len(labels):\n\t\tprint(\'error!\')\n\t\tsys.exit()\n\tclean_data = []\n\tfor i in range(len(lsents)):\n\t\tclean_data.append((lsents[i], rsents[i], labels[i]))\n\treturn clean_data\n\ndef readQuoradata(dir):\n\tlsents = []\n\trsents = []\n\tlabels = []\n\tfor line in open(dir+\'a.toks\'):\n\t\tline = line.decode(\'utf-8\')\n\t\tpieces=line.strip().split()\n\t\tlsents.append(pieces)\n\tfor line in open(dir+\'b.toks\'):\n\t\tline = line.decode(\'utf-8\')\n\t\tpieces = line.strip().split()\n\t\trsents.append(pieces)\n\tfor line in open(dir + \'sim.txt\'):\n\t\tlabels.append(int(line.strip()))\n\tclean_data = []\n\tfor i in range(len(lsents)):\n\t\tclean_data.append((lsents[i],rsents[i],labels[i]))\n\treturn clean_data\n\ndef pad(t, length):\n\tif length == t.size(0):\n\t\treturn t\n\telse:\n\t\treturn torch.cat([t, Variable(t.data.new(length - t.size(0), *t.size()[1:]).zero_())])\n\n\ndef pack_list_sequence(inputs, l):\n\tbatch_list = []\n\tmax_l = max(list(l))\n\tbatch_size = len(inputs)\n\n\tfor b_i in range(batch_size):\n\t\tbatch_list.append(pad(inputs[b_i], max_l))\n\tpack_batch_list = torch.stack(batch_list, dim=1)\n\treturn pack_batch_list\n\ndef pack_for_rnn_seq(inputs, lengths):\n\t""""""\n\t:param inputs: [T * B * D]\n\t:param lengths:  [B]\n\t:return:\n\t""""""\n\t_, sorted_indices = lengths.sort()\n\t\'\'\'\n\t\tReverse to decreasing order\n\t\'\'\'\n\tr_index = reversed(list(sorted_indices))\n\n\ts_inputs_list = []\n\tlengths_list = []\n\treverse_indices = np.zeros(lengths.size(0), dtype=np.int64)\n\n\tfor j, i in enumerate(r_index):\n\t\ts_inputs_list.append(inputs[:, i, :].unsqueeze(1))\n\t\tlengths_list.append(lengths[i])\n\t\treverse_indices[i] = j\n\n\treverse_indices = list(reverse_indices)\n\n\ts_inputs = torch.cat(s_inputs_list, 1)\n\tpacked_seq = nn.utils.rnn.pack_padded_sequence(s_inputs, lengths_list)\n\n\treturn packed_seq, reverse_indices\n\n\ndef unpack_from_rnn_seq(packed_seq, reverse_indices):\n\tunpacked_seq, _ = nn.utils.rnn.pad_packed_sequence(packed_seq)\n\ts_inputs_list = []\n\n\tfor i in reverse_indices:\n\t\ts_inputs_list.append(unpacked_seq[:, i, :].unsqueeze(1))\n\treturn torch.cat(s_inputs_list, 1)\n\n\ndef auto_rnn_bilstm(lstm= nn.LSTM, seqs=None, lengths=None):\n\tbatch_size = seqs.size(1)\n\n\tstate_shape = lstm.num_layers * 2, batch_size, lstm.hidden_size\n\n\th0 = c0 = Variable(seqs.data.new(*state_shape).zero_())\n\n\tpacked_pinputs, r_index = pack_for_rnn_seq(seqs, lengths)\n\n\toutput, (hn, cn) = lstm(packed_pinputs, (h0, c0))\n\n\toutput = unpack_from_rnn_seq(output, r_index)\n\n\treturn output\n\ndef auto_rnn_bigru(gru= nn.GRU, seqs=None, lengths=None):\n\n\tbatch_size = seqs.size(1)\n\n\tstate_shape = gru.num_layers * 2, batch_size, gru.hidden_size\n\n\th0 = Variable(seqs.data.new(*state_shape).zero_())\n\n\tpacked_pinputs, r_index = pack_for_rnn_seq(seqs, lengths)\n\n\toutput, hn = gru(packed_pinputs, h0)\n\n\toutput = unpack_from_rnn_seq(output, r_index)\n\n\treturn output\n\n\ndef select_last(inputs, lengths, hidden_size):\n\t""""""\n\t:param inputs: [T * B * D] D = 2 * hidden_size\n\t:param lengths: [B]\n\t:param hidden_size: dimension\n\t:return:  [B * D]\n\t""""""\n\tbatch_size = inputs.size(1)\n\tbatch_out_list = []\n\tfor b in range(batch_size):\n\t\tbatch_out_list.append(torch.cat((inputs[lengths[b] - 1, b, :hidden_size],\n\t\t\t\t\t\t\t\t\t\t inputs[0, b, hidden_size:])\n\t\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t  )\n\n\tout = torch.stack(batch_out_list)\n\treturn out\n\n\ndef channel_weighted_sum(s, w, l, sharpen=None):\n\tbatch_size = w.size(1)\n\tresult_list = []\n\tfor b_i in range(batch_size):\n\t\tif sharpen:\n\t\t\tb_w = w[:l[b_i], b_i, :] * sharpen\n\t\telse:\n\t\t\tb_w = w[:l[b_i], b_i, :]\n\t\tb_s = s[:l[b_i], b_i, :] # T, D\n\t\tsoft_b_w = F.softmax(b_w.transpose(0, 1)).transpose(0, 1)\n\t\t# print(soft_b_w)\n\t\t# print(\'soft:\', )\n\t\t# print(soft_b_w)\n\t\tresult_list.append(torch.sum(soft_b_w * b_s, dim=0)) # [T, D] -> [1, D]\n\treturn torch.cat(result_list, dim=0)\n\n\ndef pack_to_matching_matrix(s1, s2, cat_only=[False, False]):\n\tt1 = s1.size(0)\n\tt2 = s2.size(0)\n\tbatch_size = s1.size(1)\n\td = s1.size(2)\n\n\texpanded_p_s1 = s1.expand(t2, t1, batch_size, d)\n\n\texpanded_p_s2 = s2.view(t2, 1, batch_size, d)\n\texpanded_p_s2 = expanded_p_s2.expand(t2, t1, batch_size, d)\n\n\tif not cat_only[0] and not cat_only[1]:\n\t\tmatrix = torch.cat((expanded_p_s1, expanded_p_s2), dim=3)\n\telif not cat_only[0] and cat_only[1]:\n\t\tmatrix = torch.cat((expanded_p_s1, expanded_p_s2, expanded_p_s1 * expanded_p_s2), dim=3)\n\telse:\n\t\tmatrix = torch.cat((expanded_p_s1,\n\t\t\t\t\t\t\texpanded_p_s2,\n\t\t\t\t\t\t\ttorch.abs(expanded_p_s1 - expanded_p_s2),\n\t\t\t\t\t\t\texpanded_p_s1 * expanded_p_s2), dim=3)\n\n\t# matrix = torch.cat((expanded_p_s1,\n\t#                     expanded_p_s2), dim=3)\n\n\treturn matrix\n\ndef gen_prefix(name, date):\n\tfile_path = os.path.join(config.ROOT_DIR, \'saved_model\', \'_\'.join((date, name)))\n\treturn file_path\n\n\ndef logging2file(file_path, type, info, file_name=None):\n\tif not os.path.exists(file_path):\n\t\tos.mkdir(file_path)\n\tif type == \'message\':\n\t\twith open(os.path.join(file_path, \'message.txt\'), \'a+\') as f:\n\t\t\tf.write(info)\n\t\t\tf.flush()\n\telif type == \'log\':\n\t\twith open(os.path.join(file_path, \'log.txt\'), \'a+\') as f:\n\t\t\tf.write(info)\n\t\t\tf.flush()\n\telif type == \'code\':\n\t\twith open(os.path.join(file_path, \'code.pys\'), \'a+\') as f, open(file_name) as it:\n\t\t\tf.write(it.read())\n\t\t\tf.flush()\n\ndef max_along_time(inputs, lengths):\n\t""""""\n\t:param inputs: [T * B * D]\n\t:param lengths:  [B]\n\t:return: [B * D] max_along_time\n\t""""""\n\tls = list(lengths)\n\n\tb_seq_max_list = []\n\tfor i, l in enumerate(ls):\n\t\tseq_i = inputs[:l, i, :]\n\t\tseq_i_max, _ = seq_i.max(dim=0)\n\t\tseq_i_max = seq_i_max.squeeze()\n\t\tb_seq_max_list.append(seq_i_max)\n\n\treturn torch.stack(b_seq_max_list)\n\ndef text_conv1d(inputs, l1, conv_filter=nn.Linear, k_size=None, dropout=None, list_in=False,\n\t\t\t\tgate_way=True):\n\t""""""\n\t:param inputs: [T * B * D]\n\t:param l1:  [B]\n\t:param conv_filter:  [k * D_in, D_out * 2]\n\t:param k_size:\n\t:param dropout:\n\t:param padding:\n\t:param list_in:\n\t:return:\n\t""""""\n\tk = k_size\n\tbatch_size = l1.size(0)\n\td_in = inputs.size(2) if not list_in else inputs[0].size(1)\n\tunit_d = conv_filter.out_features // 2\n\tpad_n = (k - 1) // 2\n\n\tzeros_padding = Variable(inputs[0].data.new(pad_n, d_in).zero_())\n\n\tbatch_list = []\n\tinput_list = []\n\tfor b_i in range(batch_size):\n\t\tmasked_in = inputs[:l1[b_i], b_i, :] if not list_in else inputs[b_i]\n\t\tif gate_way:\n\t\t\tinput_list.append(masked_in)\n\n\t\tb_inputs = torch.cat([zeros_padding, masked_in, zeros_padding], dim=0)\n\t\tfor i in range(l1[b_i]):\n\t\t\t# print(b_inputs[i:i+k])\n\t\t\tbatch_list.append(b_inputs[i:i+k].view(k * d_in))\n\n\tbatch_in = torch.stack(batch_list, dim=0)\n\ta, b = torch.chunk(conv_filter(batch_in), 2, 1)\n\tout = a * F.sigmoid(b)\n\n\tout_list = []\n\tstart = 0\n\tfor b_i in range(batch_size):\n\t\tif gate_way:\n\t\t\tout_list.append(torch.cat((input_list[b_i], out[start:start + l1[b_i]]), dim=1))\n\t\telse:\n\t\t\tout_list.append(out[start:start + l1[b_i]])\n\n\t\tstart = start + l1[b_i]\n\n\t# max_out_list = []\n\t# for b_i in range(batch_size):\n\t#     max_out, _ = torch.max(out_list[b_i], dim=0)\n\t#     max_out_list.append(max_out)\n\t# max_out = torch.cat(max_out_list, 0)\n\t#\n\t# print(out_list)\n\n\treturn out_list'"
SSE/train_quora.py,24,"b""from __future__ import division\r\nimport os\r\nimport sys\r\nimport time\r\nimport torch\r\nimport random\r\nimport pickle\r\nfrom model import *\r\nimport torch.nn as nn\r\nfrom torch import optim\r\nfrom torch_util import *\r\nfrom datetime import datetime\r\nfrom datetime import timedelta\r\nfrom collections import Counter\r\nfrom torchtext.vocab import load_word_vectors\r\n\r\nimport pdb\r\n\r\n\r\ndef create_batch(data,from_index, to_index):\r\n  if to_index > len(data):\r\n    to_index = len(data)\r\n  lsize, rsize = 0, 0\r\n  lsize_list, rsize_list = [], []\r\n  for i in range(from_index, to_index):\r\n    length = len(data[i][0])\r\n    lsize_list.append(length)\r\n    if length > lsize:\r\n      lsize = length\r\n    length = len(data[i][1])\r\n    rsize_list.append(length)\r\n    if length > rsize:\r\n      rsize = length\r\n\r\n  left_sents, right_sents, labels = [], [], []\r\n  for i in range(from_index, to_index):\r\n    lsent = data[i][0]\r\n    lsent = lsent + ['<pad>' for k in range(lsize - len(lsent))]\r\n    left_sents.append([word2id[word] for word in lsent])\r\n    rsent = data[i][1]\r\n    rsent = rsent + ['<pad>' for k in range(rsize - len(rsent))]\r\n    right_sents.append([word2id[word] for word in rsent])\r\n    labels.append(data[i][2])\r\n\r\n  left_sents=Variable(torch.LongTensor(left_sents))\r\n  right_sents=Variable(torch.LongTensor(right_sents))\r\n  labels=Variable(torch.LongTensor(labels))\r\n  lsize_list=torch.LongTensor(lsize_list)\r\n  rsize_list =torch.LongTensor(rsize_list)\r\n\r\n  if torch.cuda.is_available():\r\n    left_sents=left_sents.cuda()\r\n    right_sents=right_sents.cuda()\r\n    labels=labels.cuda()\r\n    lsize_list=lsize_list.cuda()\r\n    rsize_list=rsize_list.cuda()\r\n  return left_sents, right_sents, labels, lsize_list, rsize_list\r\n\r\n\r\ndef make_vocab(train_pairs, dev_pairs, test_pairs):\r\n  vocab_counter = Counter()\r\n  for left, right, _ in train_pairs + dev_pairs + test_pairs:\r\n    vocab_counter.update(left)\r\n    vocab_counter.update(right)\r\n  \r\n  tokens = ['<pad>', '<unk>'] + [w for w, _ in vocab_counter.most_common()]\r\n  word2id={}\r\n  for _, word in enumerate(tokens):\r\n    word2id[word] = _\r\n\r\n  return tokens, word2id\r\n\r\n\r\nif __name__ == '__main__':\r\n  task='quora'\r\n  print('task: '+task)\r\n  torch.manual_seed(6)\r\n\r\n  num_class = 2\r\n  if torch.cuda.is_available():\r\n    print('CUDA is available!')\r\n  \r\n  basepath = './data'\r\n  embedding_path = '../data/glove'\r\n  train_pairs = readQuoradata(basepath + '/train/')\r\n  dev_pairs = readQuoradata(basepath + '/dev/')\r\n  test_pairs = readQuoradata(basepath + '/test/')\r\n\r\n  print('# of train pairs: %d' % len(train_pairs))\r\n  print('# of dev pairs: %d' % len(dev_pairs))\r\n  print('# of test pairs: %d' % len(test_pairs))\r\n\r\n  tokens, word2id = make_vocab(train_pairs, dev_pairs, test_pairs)\r\n  with open(os.path.join('./results', 'vocab.pkl'), 'wb') as f:\r\n    pickle.dump((tokens, word2id), f, protocol=pickle.HIGHEST_PROTOCOL)\r\n\r\n  wv_dict, wv_arr, wv_size = load_word_vectors(embedding_path, 'glove.840B', 300)\r\n  pretrained_emb = []\r\n  for _, word  in enumerate(tokens):\r\n    if word in wv_dict:\r\n      pretrained_emb.append(wv_arr[wv_dict[word]].numpy())\r\n    else:\r\n      pretrained_emb.append(np.random.uniform(-0.05, 0.05, size=[300]))\r\n  pretrained_emb = np.stack(pretrained_emb)\r\n  assert pretrained_emb.shape == (len(tokens), 300)\r\n\r\n\r\n  model = StackBiLSTMMaxout(h_size=[512, 1024, 2048], \r\n                            v_size=len(tokens), \r\n                            d=300, \r\n                            mlp_d=1600, \r\n                            dropout_r=0.1, \r\n                            max_l=60, \r\n                            num_class=num_class)\r\n\r\n  if torch.cuda.is_available():\r\n    pretrained_emb=torch.Tensor(pretrained_emb).cuda()\r\n  else:\r\n    pretrained_emb = torch.Tensor(pretrained_emb)\r\n  model.Embd.weight.data = pretrained_emb\r\n\r\n  if torch.cuda.is_available():\r\n    model.cuda()\r\n\r\n  start_lr = 2e-4\r\n  batch_size=32\r\n  optimizer = optim.Adam(model.parameters(), lr=start_lr)\r\n  criterion = nn.CrossEntropyLoss()\r\n\r\n  # ckpt_path = os.path.join('./results', '%s_%d.pkl' % (task, 0))\r\n  # #  model = torch.load(ckpt_path)\r\n  # model.load_state_dict(torch.load(ckpt_path))\r\n  # model.eval()\r\n  # \r\n  # # Test\r\n  # start_time = time.time()\r\n  # test_batch_index = 0\r\n  # test_num_correct = 0\r\n  # msg = ''\r\n  # accumulated_loss = 0\r\n  # test_batch_i = 0\r\n  # pred = []\r\n  # gold = []\r\n  # while test_batch_i < len(test_pairs):\r\n  #   left_sents, right_sents, labels, lsize_list, rsize_list = create_batch(\r\n  #       test_pairs, test_batch_i, test_batch_i+batch_size)\r\n  #   left_sents = torch.transpose(left_sents, 0, 1)\r\n  #   right_sents = torch.transpose(right_sents, 0, 1)\r\n  #   test_batch_i+=len(labels)\r\n  #   output = model(left_sents, lsize_list, right_sents, rsize_list)\r\n  #   result = output.data.cpu().numpy()\r\n  #   loss = criterion(output, labels)\r\n  #   # accumulated_loss += loss.data[0]\r\n  #   accumulated_loss += loss.item()\r\n  #   a = np.argmax(result, axis=1)\r\n  #   b = labels.data.cpu().numpy()\r\n  #   test_num_correct += np.sum(a == b)\r\n  # \r\n  # elapsed_time = time.time() - start_time\r\n  # print('Test finished within ' + str(timedelta(seconds=elapsed_time)))\r\n  # msg += '\\t test loss: %.4f' % accumulated_loss\r\n  # test_acc = test_num_correct / len(test_pairs)\r\n  # msg += '\\t test accuracy: %.4f' % test_acc\r\n  # print(msg)\r\n\r\n  # sys.exit()\r\n\r\n\r\n  \r\n  print('start training...')\r\n  best_dev_acc = 0.0\r\n  for epoch in range(100):\r\n    batch_counter = 0\r\n    accumulated_loss = 0\r\n    print('--' * 20)\r\n    start_time = time.time()\r\n    i_decay = epoch / 2\r\n    lr = start_lr / (2 ** i_decay)\r\n    print('lr: %f' % lr)\r\n    train_pairs = np.array(train_pairs)\r\n    rand_idx = np.random.permutation(len(train_pairs))\r\n    train_pairs = train_pairs[rand_idx]\r\n    train_batch_i = 0\r\n    train_num_correct=0\r\n    train_sents_scaned = 0\r\n    while train_batch_i < len(train_pairs):\r\n      model.train()\r\n      left_sents, right_sents, labels, lsize_list, rsize_list = create_batch(\r\n          train_pairs, train_batch_i, train_batch_i+batch_size)\r\n      train_sents_scaned += len(labels)\r\n      train_batch_i+=len(labels)\r\n      left_sents=torch.transpose(left_sents,0,1)\r\n      right_sents=torch.transpose(right_sents,0,1)\r\n      output=model(left_sents,lsize_list,right_sents,rsize_list)\r\n      result = output.data.cpu().numpy()\r\n      a = np.argmax(result, axis=1)\r\n      b = labels.data.cpu().numpy()\r\n      train_num_correct += np.sum(a == b)\r\n      loss = criterion(output, labels)\r\n      optimizer.zero_grad()\r\n      for pg in optimizer.param_groups:\r\n        pg['lr'] = lr\r\n      loss.backward()\r\n      optimizer.step()\r\n      batch_counter += 1\r\n      # accumulated_loss += loss.data[0]\r\n      accumulated_loss += loss.item()\r\n\r\n    elapsed_time = time.time() - start_time\r\n    print('Epoch ' + str(epoch) + ' finished within ' + str(timedelta(seconds=elapsed_time)))\r\n    msg = ''\r\n    msg += '\\t train loss: %.4f' % accumulated_loss\r\n    msg += '\\t train accuracy: %.4f' % (train_num_correct / train_sents_scaned)\r\n    print(msg)\r\n      \r\n    # Validation\r\n    start_time = time.time()\r\n    model.eval()\r\n    dev_batch_index = 0\r\n    dev_num_correct = 0\r\n    msg = ''\r\n    accumulated_loss = 0\r\n    dev_batch_i = 0\r\n    pred=[]\r\n    gold=[]\r\n    while dev_batch_i < len(dev_pairs):\r\n      left_sents, right_sents, labels, lsize_list, rsize_list = create_batch(\r\n          dev_pairs, dev_batch_i, dev_batch_i+batch_size)\r\n      dev_batch_i += len(labels)\r\n      left_sents = torch.transpose(left_sents, 0, 1)\r\n      right_sents = torch.transpose(right_sents, 0, 1)\r\n      output = model(left_sents, lsize_list,right_sents, rsize_list)\r\n      result = output.data.cpu().numpy()\r\n      loss = criterion(output, labels)\r\n      accumulated_loss += loss.item()\r\n      a = np.argmax(result, axis=1)\r\n      b = labels.data.cpu().numpy()\r\n      dev_num_correct += np.sum(a == b)\r\n    \r\n    elapsed_time = time.time() - start_time\r\n    print('Validation finished within ' + str(timedelta(seconds=elapsed_time)))\r\n    msg += '\\t dev loss: %.4f' % accumulated_loss\r\n    dev_acc = dev_num_correct / len(dev_pairs)\r\n    msg += '\\t dev accuracy: %.4f' % dev_acc\r\n    \r\n    # if accumulated_loss < best_dev_loss:\r\n    if dev_acc > best_dev_acc:\r\n      ckpt_path = os.path.join('./results', '%s_%d.pkl' % (task, epoch))\r\n      msg += '\\t | checkpoint: ' + ckpt_path\r\n      best_dev_acc = dev_acc\r\n      torch.save(model.state_dict(), ckpt_path)\r\n    print(msg)\r\n\r\n    # Test\r\n    start_time = time.time()\r\n    test_batch_index = 0\r\n    test_num_correct = 0\r\n    msg = ''\r\n    accumulated_loss = 0\r\n    test_batch_i = 0\r\n    pred = []\r\n    gold = []\r\n    while test_batch_i < len(test_pairs):\r\n      left_sents, right_sents, labels, lsize_list, rsize_list = create_batch(\r\n          test_pairs, test_batch_i, test_batch_i+batch_size)\r\n      left_sents = torch.transpose(left_sents, 0, 1)\r\n      right_sents = torch.transpose(right_sents, 0, 1)\r\n      test_batch_i+=len(labels)\r\n      output = model(left_sents, lsize_list, right_sents, rsize_list)\r\n      result = output.data.cpu().numpy()\r\n      loss = criterion(output, labels)\r\n      # accumulated_loss += loss.data[0]\r\n      accumulated_loss += loss.item()\r\n      a = np.argmax(result, axis=1)\r\n      b = labels.data.cpu().numpy()\r\n      test_num_correct += np.sum(a == b)\r\n    \r\n    elapsed_time = time.time() - start_time\r\n    print('Test finished within ' + str(timedelta(seconds=elapsed_time)))\r\n    msg += '\\t test loss: %.4f' % accumulated_loss\r\n    test_acc = test_num_correct / len(test_pairs)\r\n    msg += '\\t test accuracy: %.4f' % test_acc\r\n    print(msg)\r\n"""
ESIM/Tree_IM/Constants.py,0,"b""PAD = 0\nUNK = 1\nBOS = 2\nEOS = 3\n\nPAD_WORD = '<blank>'\nUNK_WORD = '<unk>'\nBOS_WORD = '<s>'\nEOS_WORD = '</s>'"""
ESIM/Tree_IM/binary_tree.py,0,"b'import numpy as np\n\n\nclass BinaryTree:\n\n    root = None\n    node_index = None\n\n    def __init__(self, index):\n        self.root = BinaryTreeNode(index)\n        self.node_index = {index: self.root}\n\n    def add_left_descendant(self, index, parent_index):\n        parent = self.node_index[parent_index]\n        new_node = BinaryTreeNode(index, parent)\n        self.node_index[index] = new_node\n        parent.add_left_descendant(new_node)\n\n    def has_left_descendant_at_node(self, index):\n        return self.node_index[index].has_left_descendant()\n\n    def add_right_descendant(self, index, parent_index):\n        parent = self.node_index[parent_index]\n        new_node = BinaryTreeNode(index, parent)\n        self.node_index[index] = new_node\n        parent.add_right_descendant(new_node)\n\n    def has_right_descendant_at_node(self, index):\n        return self.node_index[index].has_right_descendant()\n\n    def set_word(self, index, word):\n        self.node_index[index].set_word(word)\n\n    def print_tree(self):\n        self.root.recursive_print()\n\n    def get_sentence(self):\n        sentence = \' \'.join([n.word for n in self.node_index.values() if n.word != \'_PAD_\'])\n        return sentence\n\n    def get_words(self):\n        return [n.word for n in self.node_index.values()]\n\n    def convert_to_ptb_format(self):\n        return self.root.convert_to_ptb()\n\n    def convert_to_sequence_and_masks(self, head_node):\n        """"""\n        Convert a subtree into a sequence of words, and corresponding masks of the root\n        :param head_node: the node to treat as the root of the (sub)tree\n        :return words: list of words in tree order\n        :return left_mask, right_mask: masks denoting the tree structure\n        """"""\n        sequence = head_node.get_children_in_sequence()\n        sequence.reverse()\n        sequence_map = {}\n        for s_i, s in enumerate(sequence):\n            sequence_map[s] = s_i\n        # sequence_map = {s: s_i for s_i, s in enumerate(sequence)}\n        n_elements = len(sequence)\n        left_mask = np.zeros([n_elements, n_elements], dtype=np.float32)\n        right_mask = np.zeros([n_elements, n_elements], dtype=np.float32)\n        for s_i, n_i in enumerate(sequence):\n            node = self.node_index[n_i]\n            if node.has_left_descendant():\n                left_mask[s_i, sequence_map[node.left_descendant.index]] = 1.\n            if node.has_right_descendant():\n                right_mask[s_i, sequence_map[node.right_descendant.index]] = 1.\n        words = [self.node_index[n_i].word for n_i in sequence]\n        return words, left_mask, right_mask\n\n    def get_parents(self):\n        sequence = self.root.get_children_in_sequence()\n        words = [self.node_index[n_i].word for n_i in sequence]\n        parents = [self.node_index[n_i].parent.index + 1 if self.node_index[n_i].parent else 0 for n_i in sequence]\n        indexs = [self.node_index[n_i].index+1 for n_i in sequence]\n\n        return words, parents, indexs\n\n\n\n\nclass BinaryTreeNode:\n\n    word = None\n    index = None\n    parent = None\n    left_descendant = None\n    right_descendant = None\n\n    def __init__(self, index, parent=None):\n        self.index = index\n        self.word = \'_PAD_\'\n        if parent is not None:\n            self.parent = parent\n\n    def __str__(self):\n        return \'(%s %s)\' % (self.word, self.index)\n\n    def set_word(self, word):\n        self.word = word\n\n    def add_left_descendant(self, new_node):\n        self.left_descendant = new_node\n\n    def has_left_descendant(self):\n        return self.left_descendant is not None\n\n    def add_right_descendant(self, new_node):\n        self.right_descendant = new_node\n\n    def has_right_descendant(self):\n        return self.right_descendant is not None\n\n    def recursive_print(self, depth=0):\n        print \'  \'*depth, self.word, self.index\n        if self.left_descendant is not None:\n            self.left_descendant.recursive_print(depth+1)\n        if self.right_descendant is not None:\n            self.right_descendant.recursive_print(depth+1)\n\n    def convert_to_ptb(self):\n        ptb_string = \'(\' \n        if self.word != \'_PAD_\':\n            ptb_string += \' \' + self.word\n        if self.left_descendant is not None:\n            ptb_string += \' \' + self.left_descendant.convert_to_ptb()\n        if self.right_descendant is not None:\n            ptb_string += \' \' + self.right_descendant.convert_to_ptb()\n        ptb_string += \')\'\n        return ptb_string\n\n    def get_leaf_nodes(self):\n        leaves = []\n        if self.left_descendant is None and self.right_descendant is None:\n            leaves.append(self.word)\n        else:\n            if self.left_descendant is not None:\n                leaves.extend(self.left_descendant.get_leaf_nodes())\n            if self.right_descendant is not None:\n                leaves.extend(self.right_descendant.get_leaf_nodes())\n        return leaves\n\n    def get_children_in_sequence(self):\n        sequence = []\n        if self.left_descendant is None and self.right_descendant is None:\n            sequence.append(self.index)\n        else:\n            if self.left_descendant is not None:\n                sequence.extend(self.left_descendant.get_children_in_sequence())\n            if self.right_descendant is not None:\n                sequence.extend(self.right_descendant.get_children_in_sequence())\n            sequence = [self.index] + sequence\n        return sequence\n\n\n'"
ESIM/Tree_IM/data_iterator.py,0,"b'import cPickle as pkl\nimport gzip\nimport os\nimport re\nimport sys\nimport numpy\nimport math\nimport random\n\nfrom binary_tree import BinaryTree\n\ndef convert_ptb_to_tree(line):\n\tindex = 0\n\ttree = None\n\tline = line.rstrip()\n\n\tstack = []\n\tparts = line.split()\n\tfor p_i, p in enumerate(parts):\n\t\t# opening of a bracket, create a new node, take parent from top of stack\n\t\tif p == \'(\':\n\t\t\tif tree is None:\n\t\t\t\ttree = BinaryTree(index)\n\t\t\telse:\n\t\t\t\tadd_descendant(tree, index, stack[-1])\n\t\t\t# add the newly created node to the stack and increment the index\n\t\t\tstack.append(index)\n\t\t\tindex += 1\n\t\t# close of a bracket, pop node on top of the stack\n\t\telif p == \')\':\n\t\t\tstack.pop(-1)\n\t\t# otherwise, create a new node, take parent from top of stack, and set word\n\t\telse:\n\t\t\tadd_descendant(tree, index, stack[-1])\n\t\t\ttree.set_word(index, p)\n\t\t\tindex += 1\n\treturn tree\n\ndef add_descendant(tree, index, parent_index):\n\t# add to the left first if possible, then to the right\n\tif tree.has_left_descendant_at_node(parent_index):\n\t\tif tree.has_right_descendant_at_node(parent_index):\n\t\t\tsys.exit(""Node "" + str(parent_index) + "" already has two children"")\n\t\telse:\n\t\t\ttree.add_right_descendant(index, parent_index)\n\telse:\n\t\ttree.add_left_descendant(index, parent_index)\n\ndef fopen(filename, mode=\'r\'):\n\tif filename.endswith(\'.gz\'):\n\t\treturn gzip.open(filename, mode)\n\treturn open(filename, mode)\n\nclass TextIterator:\n\t""""""Simple Bitext iterator.""""""\n\tdef __init__(self, source, target, label,\n\t\t\t\t dict,\n\t\t\t\t batch_size=128,\n\t\t\t\t n_words=-1,\n\t\t\t\t maxlen=500,\n\t\t\t\t shuffle=True, task_type=\'classification\'):\n\t\tself.source = fopen(source, \'r\')\n\t\tself.target = fopen(target, \'r\')\n\t\tself.label = fopen(label, \'r\')\n\t\twith open(dict, \'rb\') as f:\n\t\t\tself.dict = pkl.load(f)\n\t\tself.batch_size = batch_size\n\t\tself.n_words = n_words\n\t\tself.maxlen = maxlen\n\t\tself.shuffle = shuffle\n\t\tself.end_of_data = False\n\t\tself.task_type=task_type\n\n\t\tself.source_buffer = []\n\t\tself.target_buffer = []\n\t\tself.label_buffer = []\n\t\tself.k = batch_size * 20\n\n\tdef __iter__(self):\n\t\treturn self\n\n\tdef reset(self):\n\t\tself.source.seek(0)\n\t\tself.target.seek(0)\n\t\tself.label.seek(0)\n\n\tdef next(self):\n\t\tif self.end_of_data:\n\t\t\tself.end_of_data = False\n\t\t\tself.reset()\n\t\t\traise StopIteration\n\n\t\tsource = []\n\t\ttarget = []\n\t\tlabel = []\n\n\t\t# fill buffer, if it\'s empty\n\t\tassert len(self.source_buffer) == len(self.target_buffer), \'Buffer size mismatch!\'\n\t\tassert len(self.source_buffer) == len(self.label_buffer), \'Buffer size mismatch!\'\n\n\t\tif len(self.source_buffer) == 0:\n\t\t\tfor k_ in xrange(self.k):\n\t\t\t\tss = self.source.readline()\n\t\t\t\tif ss == """":\n\t\t\t\t\tbreak\n\t\t\t\ttt = self.target.readline()\n\t\t\t\tif tt == """":\n\t\t\t\t\tbreak\n\t\t\t\tll = self.label.readline()\n\t\t\t\tif ll == """":\n\t\t\t\t\tbreak\n\t\t\t\tif self.task_type!=\'classification\':\n\t\t\t\t\ttry:\n\t\t\t\t\t\tsim = float(ll.strip())\n\t\t\t\t\texcept:\n\t\t\t\t\t\tprint(ss)\n\t\t\t\t\t\tsys.exit()\n\t\t\t\t\tceil = int(math.ceil(sim))\n\t\t\t\t\tfloor = int(math.floor(sim))\n\t\t\t\t\ttmp = [0, 0, 0, 0, 0, 0]\n\t\t\t\t\tif floor != ceil:\n\t\t\t\t\t\ttmp[ceil] = sim - floor\n\t\t\t\t\t\ttmp[floor] = ceil - sim\n\t\t\t\t\telse:\n\t\t\t\t\t\ttmp[floor] = 1\n\t\t\t\t\tll=tmp\n\n\t\t\t\ttry:\n\t\t\t\t\tss = convert_ptb_to_tree(ss)\n\t\t\t\texcept:\n\t\t\t\t\tprint(\'ss\')\n\t\t\t\t\tss = convert_ptb_to_tree(\'( null null )\')\n\t\t\t\t#ss.print_tree()\n\t\t\t\twords_ss, left_mask_ss, right_mask_ss = ss.convert_to_sequence_and_masks(ss.root)\n\t\t\t\twords_ss = [self.dict[w] if w in self.dict else 1\n\t\t\t\t\t  for w in words_ss]\n\t\t\t\tif self.n_words > 0:\n\t\t\t\t\twords_ss = [w if w < self.n_words else 1 for w in words_ss]\n\t\t\t\tss = (words_ss, left_mask_ss, right_mask_ss)\n\n\t\t\t\ttry:\n\t\t\t\t\ttt = convert_ptb_to_tree(tt)\n\t\t\t\texcept:\n\t\t\t\t\tprint(\'ss\')\n\t\t\t\t\ttt = convert_ptb_to_tree(\'( null null )\')\n\t\t\t\twords_tt, left_mask_tt, right_mask_tt = tt.convert_to_sequence_and_masks(tt.root)\n\t\t\t\twords_tt = [self.dict[w] if w in self.dict else 1\n\t\t\t\t\t  for w in words_tt]\n\t\t\t\tif self.n_words > 0:\n\t\t\t\t\twords_tt = [w if w < self.n_words else 1 for w in words_tt]\n\t\t\t\ttt = (words_tt, left_mask_tt, right_mask_tt)\n\n\t\t\t\tif len(words_ss) > self.maxlen or len(words_tt) > self.maxlen:\n\t\t\t\t\tcontinue\n\n\t\t\t\tself.source_buffer.append(ss)\n\t\t\t\tself.target_buffer.append(tt)\n\t\t\t\tif self.task_type!=\'classification\':\n\t\t\t\t\tself.label_buffer.append(ll)\n\t\t\t\telse:\n\t\t\t\t\tself.label_buffer.append(ll.strip())\n\n\t\t\tif self.shuffle:\n\t\t\t\t# sort by target buffer\n\t\t\t\ttlen = numpy.array([len(t[0]) for t in self.target_buffer])\n\t\t\t\ttidx = tlen.argsort()\n\t\t\t\t# shuffle mini-batch\n\t\t\t\ttindex = []\n\t\t\t\tsmall_index = range(int(math.ceil(len(tidx)*1./self.batch_size)))\n\t\t\t\trandom.shuffle(small_index)\n\t\t\t\tfor i in small_index:\n\t\t\t\t\tif (i+1)*self.batch_size > len(tidx):\n\t\t\t\t\t\ttindex.extend(tidx[i*self.batch_size:])\n\t\t\t\t\telse:\n\t\t\t\t\t\ttindex.extend(tidx[i*self.batch_size:(i+1)*self.batch_size])\n\n\t\t\t\ttidx = tindex\n\n\t\t\t\t_sbuf = [self.source_buffer[i] for i in tidx]\n\t\t\t\t_tbuf = [self.target_buffer[i] for i in tidx]\n\t\t\t\t_lbuf = [self.label_buffer[i] for i in tidx]\n\n\t\t\t\tself.source_buffer = _sbuf\n\t\t\t\tself.target_buffer = _tbuf\n\t\t\t\tself.label_buffer = _lbuf\n\n\t\tif len(self.source_buffer) == 0 or len(self.target_buffer) == 0 or len(self.label_buffer) == 0:\n\t\t\tself.end_of_data = False\n\t\t\tself.reset()\n\t\t\traise StopIteration\n\n\t\ttry:\n\n\t\t\t# actual work here\n\t\t\twhile True:\n\n\t\t\t\t# read from source file and map to word index\n\t\t\t\ttry:\n\t\t\t\t\tss = self.source_buffer.pop(0)\n\t\t\t\t\ttt = self.target_buffer.pop(0)\n\t\t\t\t\tll = self.label_buffer.pop(0)\n\t\t\t\texcept IndexError:\n\t\t\t\t\tbreak\n\n\t\t\t\tsource.append(ss)\n\t\t\t\ttarget.append(tt)\n\t\t\t\tlabel.append(ll)\n\n\t\t\t\tif len(source) >= self.batch_size or \\\n\t\t\t\t\t\tlen(target) >= self.batch_size or \\\n\t\t\t\t\t\tlen(label) >= self.batch_size:\n\t\t\t\t\tbreak\n\t\texcept IOError:\n\t\t\tself.end_of_data = True\n\n\t\tif len(source) <= 0 or len(target) <= 0 or len(label) <= 0:\n\t\t\tself.end_of_data = False\n\t\t\tself.reset()\n\t\t\traise StopIteration\n\n\t\treturn source, target, label\n'"
ESIM/Tree_IM/main.py,10,"b'from __future__ import division\nimport sys\nimport cPickle as pkl\nfrom os.path import expanduser\nimport torch\nimport numpy\nfrom model import *\nimport time\nimport gc\nfrom datetime import timedelta\nfrom vocab import Vocab\nfrom util import *\nfrom torchtext.vocab import load_word_vectors\n\n\ndef norm_weight(nin, nout=None, scale=0.01, ortho=True):\n\t""""""\n\tRandom weights drawn from a Gaussian\n\t""""""\n\tif nout is None:\n\t\tnout = nin\n\tif nout == nin and ortho:\n\t\tW = ortho_weight(nin)\n\telse:\n\t\tW = scale * numpy.random.randn(nin, nout)\n\treturn W.astype(\'float32\')\n\nif torch.cuda.is_available():\n\tprint(\'CUDA is available!\')\n\tbase_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/url/\'\n\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\nelse:\n\tbase_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/url/\'\n\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\ntask=\'url\'\nprint(\'task: \'+task)\nprint(\'model: TreeIM\')\nnum_classes=2\nvocab = Vocab(filename=base_path+\'vocab.txt\')\nvocab.add(\'__PAD__\')\nnum_words=vocab.size()\ntrain_dataset = Dataset(base_path+\'train/\', vocab, num_classes)\ntest_dataset = Dataset(base_path+\'test_9324/\', vocab, num_classes)\n#test_dataset = dev_dataset\ndim_word=300\nbatch_size=32\nnum_epochs=500\nvalid_batch_size=32\nprint \'Loading data\'\nn_words=vocab.size()\npretrained_emb=norm_weight(n_words, dim_word)\nwv_dict, wv_arr, wv_size = load_word_vectors(embedding_path, \'glove.840B\', dim_word)\nfor word in vocab.labelToIdx.keys():\n\ttry:\n\t\tpretrained_emb[vocab.labelToIdx[word]]=wv_arr[wv_dict[word]].numpy()\n\texcept:\n\t\tpretrained_emb[vocab.labelToIdx[word]]=np.random.uniform(-0.05,0.05,dim_word)\n\ncriterion = torch.nn.CrossEntropyLoss()\nmodel = ESIM(dim_word, num_classes, n_words, dim_word, pretrained_emb, num_words)\nif torch.cuda.is_available():\n\tmodel = model.cuda()\n\tcriterion = criterion.cuda()\noptimizer = torch.optim.Adam(model.parameters(),lr=0.0004)\nprint(\'start training...\')\naccumulated_loss=0\nbatch_counter=0\nreport_interval = 5000\nbest_dev_loss=10e10\nbest_dev_loss2=10e10\nclip_c=10\nmodel.train()\n\ngold=[]\nfor line in open(base_path+\'/test_9324/sim.txt\'):\n\tgold.append(int(line.strip()))\n\nfor epoch in range(num_epochs):\n\taccumulated_loss = 0\n\tmodel.train()\n\toptimizer.zero_grad()\n\tprint(\'--\' * 20)\n\tstart_time = time.time()\n\ttrain_sents_scaned = 0\n\ttrain_num_correct = 0\n\tbatch_counter=0\n\tindices = torch.randperm(len(train_dataset))\n\t#for idx in tqdm(xrange(len(train_dataset)), desc=\'Training epoch \' + str(epoch + 1) + \'\'):\n\tfor idx in range(len(train_dataset)):\n\t\t#print(idx)\n\t\ttrain_sents_scaned+=1\n\t\tltree, lsent, rtree, rsent, label = train_dataset[idx]#indices[idx]]\n\t\t#print(lsent)\n\t\t#for item in lsent:\n\t\t#\tprint(vocab.idxToLabel[item])\n\t\t#print_tree(ltree, 0)\n\t\t#sys.exit()\n\t\tlinput, rinput = Variable(lsent), Variable(rsent)\n\t\t#target = Variable(map_label_to_target(label, num_classes))\n\t\ttarget=Variable(torch.LongTensor([int(label)]))\n\t\tif torch.cuda.is_available():\n\t\t\tlinput, rinput = linput.cuda(), rinput.cuda()\n\t\t\t#ltree, rtree = ltree.cuda(), rtree.cuda()\n\t\t\ttarget = target.cuda()\n\t\toutput = model(linput, rinput, ltree, rtree)\n\t\tdel ltree, linput\n\t\tdel rtree, rinput\n\t\t#gc.collect()\n\t\tresult = output.data.cpu().numpy()\n\t\ta = np.argmax(result, axis=1)\n\t\tb = target.data.cpu().numpy()\n\t\ttrain_num_correct += np.sum(a == b)\n\t\tloss = criterion(output, target)\n\t\tloss.backward()\n\t\t#optimizer.step()\n\t\taccumulated_loss += loss.data[0]\n\t\t#print(loss.data[0])\n\t\tbatch_counter += 1\n\t\tif train_sents_scaned%batch_size==0:\n\t\t\toptimizer.step()\n\t\t\toptimizer.zero_grad()\n\t\tif batch_counter % report_interval == 0:\n\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\tmsg += \'\\t train batch loss: %f\' % (accumulated_loss / train_sents_scaned)\n\t\t\t#msg += \'\\t train accuracy: %f\' % (train_num_correct / train_sents_scaned)\n\t\t\tprint(msg)\n\t\t\tgc.collect()\n\t# valid after each epoch\n\tmodel.eval()\n\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\taccumulated_loss = 0\n\ttest_num_correct = 0\n\tn_done=0\n\tpred=[]\n\t#for idx in tqdm(xrange(len(test_dataset)), desc=\'Testing epoch \' + str(epoch + 1) + \'\'):\n\tfor idx in range(len(test_dataset)):\n\t\tltree, lsent, rtree, rsent, label = test_dataset[idx]\n\t\tlinput, rinput = Variable(lsent), Variable(rsent)\n\t\t#target = Variable(map_label_to_target(label, num_classes))\n\t\ttarget=Variable(torch.LongTensor([int(label)]))\n\t\tif torch.cuda.is_available():\n\t\t\tlinput, rinput = linput.cuda(), rinput.cuda()\n\t\t\t#ltree, rtree = ltree.cuda(), rtree.cuda()\n\t\t\ttarget = target.cuda()\n\t\twith torch.no_grad():\n\t\t\toutput = model(linput, rinput, ltree, rtree)\n\t\tloss = criterion(output, target)\n\t\taccumulated_loss+=loss\n\t\tresult = output.data.cpu().numpy()\n\t\ta = np.argmax(result, axis=1)\n\t\tb = target.data.cpu().numpy()\n\t\ttest_num_correct += np.sum(a == b)\n\t\tpred.append(output.data[0].cpu().numpy()[1])\n\t\tdel linput, rinput, ltree, rtree, target\n\tmsg += \'\\t test loss: %f\' % (accumulated_loss/len(test_dataset))\n\ttest_acc = test_num_correct / len(test_dataset)\n\t#msg += \'\\t test accuracy: %f\' % test_acc\n\tprint(msg)\n\tgc.collect()\n\tURL_maxF1_eval(predict_result=pred, test_data_label=gold)\n\twith open(base_path + \'/result_prob_TreeIM_\' + task, \'w\') as f:\n\t\tfor i in range(len(pred)):\n\t\t\tf.writelines(str(pred[i]) + \'\\n\')'"
ESIM/Tree_IM/main_batch.py,27,"b'from __future__ import division\nimport sys\n\nimport gc\nimport numpy\nimport torch\nimport time\nfrom data_iterator import TextIterator\nimport cPickle as pkl\nfrom model_batch import *\nfrom os.path import expanduser\nfrom datetime import timedelta\nfrom torchtext.vocab import load_word_vectors\n\n# batch preparation\ndef prepare_data(group_x, group_y, labels):\n\tlengths_x = [len(s[0]) for s in group_x]\n\tlengths_y = [len(s[0]) for s in group_y]\n\n\tn_samples = len(group_x)\n\tmaxlen_x = numpy.max(lengths_x)\n\tmaxlen_y = numpy.max(lengths_y)\n\n\tx_seq = numpy.zeros((maxlen_x, n_samples)).astype(\'int64\')\n\ty_seq = numpy.zeros((maxlen_y, n_samples)).astype(\'int64\')\n\tx_mask = numpy.zeros((maxlen_x, n_samples)).astype(\'float32\')\n\ty_mask = numpy.zeros((maxlen_y, n_samples)).astype(\'float32\')\n\tx_left_mask = numpy.zeros((maxlen_x, n_samples, maxlen_x)).astype(\'float32\')\n\tx_right_mask = numpy.zeros((maxlen_x, n_samples, maxlen_x)).astype(\'float32\')\n\ty_left_mask = numpy.zeros((maxlen_y, n_samples, maxlen_y)).astype(\'float32\')\n\ty_right_mask = numpy.zeros((maxlen_y, n_samples, maxlen_y)).astype(\'float32\')\n\tl = numpy.zeros((n_samples,)).astype(\'int64\')\n\n\tfor idx, [s_x, s_y, ll] in enumerate(zip(group_x, group_y, labels)):\n\t\tx_seq[-lengths_x[idx]:, idx] = s_x[0]\n\t\tx_mask[-lengths_x[idx]:, idx] = 1.\n\t\tx_left_mask[-lengths_x[idx]:, idx, -lengths_x[idx]:] = s_x[1]\n\t\tx_right_mask[-lengths_x[idx]:, idx, -lengths_x[idx]:] = s_x[2]\n\t\ty_seq[-lengths_y[idx]:, idx] = s_y[0]\n\t\ty_mask[-lengths_y[idx]:, idx] = 1.\n\t\ty_left_mask[-lengths_y[idx]:, idx, -lengths_y[idx]:] = s_y[1]\n\t\ty_right_mask[-lengths_y[idx]:, idx, -lengths_y[idx]:] = s_y[2]\n\t\tl[idx] = ll\n\n\tif torch.cuda.is_available():\n\t\tx_seq=Variable(torch.LongTensor(x_seq)).cuda()\n\t\ty_seq = Variable(torch.LongTensor(y_seq)).cuda()\n\t\tx_mask = Variable(torch.FloatTensor(x_mask)).cuda()\n\t\ty_mask = Variable(torch.FloatTensor(y_mask)).cuda()\n\t\tx_left_mask=Variable(torch.FloatTensor(x_left_mask)).cuda()\n\t\ty_left_mask=Variable(torch.FloatTensor(y_left_mask)).cuda()\n\t\tx_right_mask=Variable(torch.FloatTensor(x_right_mask)).cuda()\n\t\ty_right_mask=Variable(torch.FloatTensor(y_right_mask)).cuda()\n\t\tl=Variable(torch.LongTensor(l)).cuda()\n\telse:\n\t\tx_seq = Variable(torch.LongTensor(x_seq))\n\t\ty_seq = Variable(torch.LongTensor(y_seq))\n\t\tx_mask = Variable(torch.FloatTensor(x_mask))\n\t\ty_mask = Variable(torch.FloatTensor(y_mask))\n\t\tx_left_mask = Variable(torch.FloatTensor(x_left_mask))\n\t\ty_left_mask = Variable(torch.FloatTensor(y_left_mask))\n\t\tx_right_mask = Variable(torch.FloatTensor(x_right_mask))\n\t\ty_right_mask = Variable(torch.FloatTensor(y_right_mask))\n\t\tl = Variable(torch.LongTensor(l))\n\n\tx = (x_seq, x_mask, x_left_mask, x_right_mask)\n\ty = (y_seq, y_mask, y_left_mask, y_right_mask)\n\n\treturn x, y, l\n\nprint(\'task: SNLI\')\nprint(\'model: Tree_IM\')\nif torch.cuda.is_available():\n\tprint(\'CUDA is available!\')\n\tbase_path = expanduser(""~"") + \'/pytorch/ESIM\'\n\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\nelse:\n\tbase_path = expanduser(""~"") + \'/Documents/research/pytorch/ESIM\'\n\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\ndatasets = [base_path+\'/data/binary_tree/premise_snli_1.0_train.txt\',\n            base_path+\'/data/binary_tree/hypothesis_snli_1.0_train.txt\',\n            base_path+\'/data/binary_tree/label_snli_1.0_train.txt\']\nvalid_datasets = [base_path+\'/data/binary_tree/premise_snli_1.0_dev.txt\',\n                  base_path+\'/data/binary_tree/hypothesis_snli_1.0_dev.txt\',\n                  base_path+\'/data/binary_tree/label_snli_1.0_dev.txt\']\ntest_datasets = [base_path+\'/data/binary_tree/premise_snli_1.0_test.txt\',\n                 base_path+\'/data/binary_tree/hypothesis_snli_1.0_test.txt\',\n                 base_path+\'/data/binary_tree/label_snli_1.0_test.txt\']\ndictionary = base_path+\'/data/binary_tree/snli_vocab_cased.pkl\'\n\nmaxlen=150\nbatch_size=32\nmax_epochs=1000\ndim_word=300\nwith open(dictionary, \'rb\') as f:\n\tworddicts = pkl.load(f)\nn_words=len(worddicts)\nwv_dict, wv_arr, wv_size = load_word_vectors(embedding_path, \'glove.840B\', dim_word)\npretrained_emb=norm_weight(n_words, dim_word)\nfor word in worddicts.keys():\n\ttry:\n\t\tpretrained_emb[worddicts[word]]=wv_arr[wv_dict[word]].numpy()\n\texcept:\n\t\tpretrained_emb[worddicts[word]] = torch.normal(torch.zeros(dim_word),std=1).numpy()\nprint(\'load data...\')\ntrain = TextIterator(datasets[0], datasets[1], datasets[2],\n\t\t\t\t\t\t dictionary,\n\t\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t\t batch_size=batch_size,\n\t\t\t\t\t\t maxlen=maxlen, shuffle=True)\nvalid = TextIterator(valid_datasets[0], valid_datasets[1], valid_datasets[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=batch_size,\n\t\t\t\t\t shuffle=False)\ntest = TextIterator(test_datasets[0], test_datasets[1], test_datasets[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=batch_size,\n\t\t\t\t\t shuffle=False)\ncriterion = torch.nn.CrossEntropyLoss()\nmodel = ESIM(dim_word, 3, n_words, dim_word, pretrained_emb)\nif torch.cuda.is_available():\n\tmodel = model.cuda()\n\tcriterion = criterion.cuda()\noptimizer = torch.optim.Adam(model.parameters(),lr=0.0004)\nprint(\'start training...\')\nclip_c=10\nmax_result=0\nreport_interval=1000\nfor epoch in xrange(max_epochs):\n\tmodel.train()\n\tprint(\'--\' * 20)\n\tstart_time = time.time()\n\tbatch_counter=0\n\ttrain_batch_i = 0\n\ttrain_sents_scaned = 0\n\ttrain_num_correct = 0\n\taccumulated_loss=0\n\tfor x1, x2, y in train:\n\t\t#print(x1[0][0])\n\t\t#for item in x1[0][0]:\n\t\t#\tprint(worddicts.keys()[item])\n\t\tx1, x2, y = prepare_data(x1, x2, y)\n\t\ttrain_sents_scaned += len(y)\n\t\toptimizer.zero_grad()\n\t\t#if x1[0].size(0)>100:\n\t\t#\tprint(x1[0].size(0))\n\t\t#continue\n\t\toutput = model(x1[0], x1[1], x1[2], x1[3], x2[0], x2[1], x2[2], x2[3])\n\t\tresult = output.data.cpu().numpy()\n\t\ta = np.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\ttrain_num_correct += np.sum(a == b)\n\t\tloss = criterion(output, y)\n\t\tloss.backward()\n\t\tgrad_norm = 0.\n\n\t\tfor m in list(model.parameters()):\n\t\t\tgrad_norm+=m.grad.data.norm() ** 2\n\n\t\tfor m in list(model.parameters()):\n\t\t\tif grad_norm>clip_c**2:\n\t\t\t\ttry:\n\t\t\t\t\tm.grad.data= m.grad.data / torch.sqrt(grad_norm) * clip_c\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\n\t\toptimizer.step()\n\t\taccumulated_loss += loss.data[0]\n\t\tbatch_counter += 1\n\t\tdel x1,x2,y\n\t\tif batch_counter % report_interval == 0:\n\t\t\tgc.collect()\n\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\tmsg += \'\\t training batch loss: %f\' % (accumulated_loss / train_sents_scaned)\n\t\t\tmsg += \'\\t train accuracy: %f\' % (train_num_correct / train_sents_scaned)\n\t\t\tprint(msg)\n\t# valid after each epoch\n\tmodel.eval()\n\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\taccumulated_loss = 0\n\tdev_num_correct = 0\n\tn_done = 0\n\tfor dev_x1, dev_x2, dev_y in valid:\n\t\tx1, x2, y = prepare_data(dev_x1, dev_x2, dev_y)\n\t\twith torch.no_grad():\n\t\t\toutput = model(x1[0], x1[1], x1[2], x1[3], x2[0], x2[1], x2[2], x2[3])\n\t\tn_done += len(y)\n\t\tresult = output.data.cpu().numpy()\n\t\tloss = criterion(output, y)\n\t\taccumulated_loss += loss.data[0]\n\t\ta = numpy.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\tdev_num_correct += numpy.sum(a == b)\n\tmsg += \'\\t dev loss: %f\' % (accumulated_loss / n_done)\n\tdev_acc = dev_num_correct / n_done\n\tmsg += \'\\t dev accuracy: %f\' % dev_acc\n\t# test after each epoch\n\tprint(msg)\n\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\taccumulated_loss = 0\n\tdev_num_correct = 0\n\tn_done = 0\n\tpred=[]\n\tfor dev_x1, dev_x2, dev_y in test:\n\t\tx1, x2, y = prepare_data(dev_x1, dev_x2, dev_y)\n\t\twith torch.no_grad():\n\t\t\toutput = model(x1[0], x1[1], x1[2], x1[3], x2[0], x2[1], x2[2], x2[3])\n\t\tn_done += len(y)\n\t\tresult = output.data.cpu().numpy()\n\t\tloss = criterion(output, y)\n\t\taccumulated_loss += loss.data[0]\n\t\ta = numpy.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\tdev_num_correct += numpy.sum(a == b)\n\t\tpred.extend(result)\n\tmsg += \'\\t test loss: %f\' % (accumulated_loss / n_done)\n\tdev_acc = dev_num_correct / n_done\n\tmsg += \'\\t test accuracy: %f\' % dev_acc\n\tprint(msg)\n\tif dev_acc>max_result:\n\t\tmax_result=dev_acc\n\t\twith open(base_path+\'/result_Tree_IM_prob.txt\',\'w\') as f:\n\t\t\tfor item in pred:\n\t\t\t\tf.writelines(str(item[0])+\'\\t\'+str(item[1])+\'\\t\'+str(item[2])+\'\\n\')\n\telapsed_time = time.time() - start_time\n\tprint(\'Epoch \' + str(epoch) + \' finished within \' + str(timedelta(seconds=elapsed_time)))'"
ESIM/Tree_IM/main_mnli.py,29,"b'from __future__ import division\nimport sys\n\nimport gc\nimport numpy\nimport torch\nimport time\nfrom data_iterator import TextIterator\nimport cPickle as pkl\nfrom model_batch import *\nfrom os.path import expanduser\nfrom datetime import timedelta\nfrom torchtext.vocab import load_word_vectors\n\n# batch preparation\ndef prepare_data(group_x, group_y, labels):\n\tlengths_x = [len(s[0]) for s in group_x]\n\tlengths_y = [len(s[0]) for s in group_y]\n\n\tn_samples = len(group_x)\n\tmaxlen_x = numpy.max(lengths_x)\n\tmaxlen_y = numpy.max(lengths_y)\n\n\tx_seq = numpy.zeros((maxlen_x, n_samples)).astype(\'int64\')\n\ty_seq = numpy.zeros((maxlen_y, n_samples)).astype(\'int64\')\n\tx_mask = numpy.zeros((maxlen_x, n_samples)).astype(\'float32\')\n\ty_mask = numpy.zeros((maxlen_y, n_samples)).astype(\'float32\')\n\tx_left_mask = numpy.zeros((maxlen_x, n_samples, maxlen_x)).astype(\'float32\')\n\tx_right_mask = numpy.zeros((maxlen_x, n_samples, maxlen_x)).astype(\'float32\')\n\ty_left_mask = numpy.zeros((maxlen_y, n_samples, maxlen_y)).astype(\'float32\')\n\ty_right_mask = numpy.zeros((maxlen_y, n_samples, maxlen_y)).astype(\'float32\')\n\tl = numpy.zeros((n_samples,)).astype(\'int64\')\n\n\tfor idx, [s_x, s_y, ll] in enumerate(zip(group_x, group_y, labels)):\n\t\tx_seq[-lengths_x[idx]:, idx] = s_x[0]\n\t\tx_mask[-lengths_x[idx]:, idx] = 1.\n\t\tx_left_mask[-lengths_x[idx]:, idx, -lengths_x[idx]:] = s_x[1]\n\t\tx_right_mask[-lengths_x[idx]:, idx, -lengths_x[idx]:] = s_x[2]\n\t\ty_seq[-lengths_y[idx]:, idx] = s_y[0]\n\t\ty_mask[-lengths_y[idx]:, idx] = 1.\n\t\ty_left_mask[-lengths_y[idx]:, idx, -lengths_y[idx]:] = s_y[1]\n\t\ty_right_mask[-lengths_y[idx]:, idx, -lengths_y[idx]:] = s_y[2]\n\t\tl[idx] = ll\n\n\tif torch.cuda.is_available():\n\t\tx_seq=Variable(torch.LongTensor(x_seq)).cuda()\n\t\ty_seq = Variable(torch.LongTensor(y_seq)).cuda()\n\t\tx_mask = Variable(torch.FloatTensor(x_mask)).cuda()\n\t\ty_mask = Variable(torch.FloatTensor(y_mask)).cuda()\n\t\tx_left_mask=Variable(torch.FloatTensor(x_left_mask)).cuda()\n\t\ty_left_mask=Variable(torch.FloatTensor(y_left_mask)).cuda()\n\t\tx_right_mask=Variable(torch.FloatTensor(x_right_mask)).cuda()\n\t\ty_right_mask=Variable(torch.FloatTensor(y_right_mask)).cuda()\n\t\tl=Variable(torch.LongTensor(l)).cuda()\n\telse:\n\t\tx_seq = Variable(torch.LongTensor(x_seq))\n\t\ty_seq = Variable(torch.LongTensor(y_seq))\n\t\tx_mask = Variable(torch.FloatTensor(x_mask))\n\t\ty_mask = Variable(torch.FloatTensor(y_mask))\n\t\tx_left_mask = Variable(torch.FloatTensor(x_left_mask))\n\t\ty_left_mask = Variable(torch.FloatTensor(y_left_mask))\n\t\tx_right_mask = Variable(torch.FloatTensor(x_right_mask))\n\t\ty_right_mask = Variable(torch.FloatTensor(y_right_mask))\n\t\tl = Variable(torch.LongTensor(l))\n\n\tx = (x_seq, x_mask, x_left_mask, x_right_mask)\n\ty = (y_seq, y_mask, y_left_mask, y_right_mask)\n\n\treturn x, y, l\n\nprint(\'task: MNLI\')\nprint(\'model: Tree_IM\')\nif torch.cuda.is_available():\n\tprint(\'CUDA is available!\')\n\tbase_path = expanduser(""~"") + \'/pytorch/ESIM\'\n\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\nelse:\n\tbase_path = expanduser(""~"") + \'/Documents/research/pytorch/ESIM\'\n\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\ndatasets = [base_path+\'/data/binary_tree/premise_multinli_1.0_train.txt\',\n            base_path+\'/data/binary_tree/hypothesis_multinli_1.0_train.txt\',\n            base_path+\'/data/binary_tree/label_multinli_1.0_train.txt\']\nvalid_datasets_m = [base_path+\'/data/binary_tree/premise_multinli_1.0_dev_matched.txt\',\n                  base_path+\'/data/binary_tree/hypothesis_multinli_1.0_dev_matched.txt\',\n                  base_path+\'/data/binary_tree/label_multinli_1.0_dev_matched.txt\']\nvalid_datasets_um = [base_path+\'/data/binary_tree/premise_multinli_1.0_dev_mismatched.txt\',\n                  base_path+\'/data/binary_tree/hypothesis_multinli_1.0_dev_mismatched.txt\',\n                  base_path+\'/data/binary_tree/label_multinli_1.0_dev_mismatched.txt\']\ntest_datasets_m = [base_path+\'/data/binary_tree/premise_multinli_1.0_test_matched.txt\',\n                 base_path+\'/data/binary_tree/hypothesis_multinli_1.0_test_matched.txt\',\n                 base_path+\'/data/binary_tree/label_multinli_1.0_test_matched.txt\']\ntest_datasets_um = [base_path+\'/data/binary_tree/premise_multinli_1.0_test_mismatched.txt\',\n                 base_path+\'/data/binary_tree/hypothesis_multinli_1.0_test_mismatched.txt\',\n                 base_path+\'/data/binary_tree/label_multinli_1.0_test_mismatched.txt\']\ndictionary = base_path+\'/data/binary_tree/mnli_vocab_cased.pkl\'\n\nmaxlen=150\nbatch_size=32\nmax_epochs=1000\ndim_word=300\nwith open(dictionary, \'rb\') as f:\n\tworddicts = pkl.load(f)\nn_words=len(worddicts)\nwv_dict, wv_arr, wv_size = load_word_vectors(embedding_path, \'glove.840B\', dim_word)\npretrained_emb=norm_weight(n_words, dim_word)\nfor word in worddicts.keys():\n\ttry:\n\t\tpretrained_emb[worddicts[word]]=wv_arr[wv_dict[word]].numpy()\n\texcept:\n\t\tpretrained_emb[worddicts[word]] = torch.normal(torch.zeros(dim_word),std=1).numpy()\nprint(\'load data...\')\ntrain = TextIterator(datasets[0], datasets[1], datasets[2],\n\t\t\t\t\t\t dictionary,\n\t\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t\t batch_size=batch_size,\n\t\t\t\t\t\t maxlen=maxlen, shuffle=True)\nvalid_m = TextIterator(valid_datasets_m[0], valid_datasets_m[1], valid_datasets_m[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=batch_size,\n\t\t\t\t\t shuffle=False)\nvalid_um = TextIterator(valid_datasets_um[0], valid_datasets_um[1], valid_datasets_um[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=batch_size,\n\t\t\t\t\t shuffle=False)\ntest_m = TextIterator(test_datasets_m[0], test_datasets_m[1], test_datasets_m[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=batch_size,\n\t\t\t\t\t shuffle=False)\ntest_um = TextIterator(test_datasets_um[0], test_datasets_um[1], test_datasets_um[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=batch_size,\n\t\t\t\t\t shuffle=False)\ncriterion = torch.nn.CrossEntropyLoss()\nmodel = ESIM(dim_word, 3, n_words, dim_word, pretrained_emb)\nif torch.cuda.is_available():\n\tmodel = model.cuda()\n\tcriterion = criterion.cuda()\noptimizer = torch.optim.Adam(model.parameters(),lr=0.0004)\nprint(\'start training...\')\nclip_c=10\nmax_result=10e10\nmax_result_um=10e10\nreport_interval=1000\nfor epoch in xrange(max_epochs):\n\tmodel.train()\n\tprint(\'--\' * 20)\n\tstart_time = time.time()\n\tbatch_counter=0\n\ttrain_batch_i = 0\n\ttrain_sents_scaned = 0\n\ttrain_num_correct = 0\n\taccumulated_loss=0\n\tfor x1, x2, y in train:\n\t\t#print(x1[0][0])\n\t\t#for item in x1[0][0]:\n\t\t#\tprint(worddicts.keys()[item])\n\t\tx1, x2, y = prepare_data(x1, x2, y)\n\t\ttrain_sents_scaned += len(y)\n\t\toptimizer.zero_grad()\n\t\t#if x1[0].size(0)>100:\n\t\t#\tprint(x1[0].size(0))\n\t\t#continue\n\t\toutput = model(x1[0], x1[1], x1[2], x1[3], x2[0], x2[1], x2[2], x2[3])\n\t\tresult = output.data.cpu().numpy()\n\t\ta = np.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\ttrain_num_correct += np.sum(a == b)\n\t\tloss = criterion(output, y)\n\t\tloss.backward()\n\t\tgrad_norm = 0.\n\n\t\tfor m in list(model.parameters()):\n\t\t\tgrad_norm+=m.grad.data.norm() ** 2\n\n\t\tfor m in list(model.parameters()):\n\t\t\tif grad_norm>clip_c**2:\n\t\t\t\ttry:\n\t\t\t\t\tm.grad.data= m.grad.data / torch.sqrt(grad_norm) * clip_c\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\n\t\toptimizer.step()\n\t\taccumulated_loss += loss.data[0]\n\t\tbatch_counter += 1\n\t\tdel x1,x2,y\n\t\tif batch_counter % report_interval == 0:\n\t\t\tgc.collect()\n\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\tmsg += \'\\t training batch loss: %f\' % (accumulated_loss / train_sents_scaned)\n\t\t\tmsg += \'\\t train accuracy: %f\' % (train_num_correct / train_sents_scaned)\n\t\t\tprint(msg)\n\t# m valid after each epoch\n\tmodel.eval()\n\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\taccumulated_loss = 0\n\tdev_num_correct = 0\n\tn_done = 0\n\tfor dev_x1, dev_x2, dev_y in valid_m:\n\t\tx1, x2, y = prepare_data(dev_x1, dev_x2, dev_y)\n\t\twith torch.no_grad():\n\t\t\toutput = model(x1[0], x1[1], x1[2], x1[3], x2[0], x2[1], x2[2], x2[3])\n\t\tn_done += len(y)\n\t\tresult = output.data.cpu().numpy()\n\t\tloss = criterion(output, y)\n\t\taccumulated_loss += loss.data[0]\n\t\ta = numpy.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\tdev_num_correct += numpy.sum(a == b)\n\tmsg += \'\\t dev m loss: %f\' % (accumulated_loss / n_done)\n\tdev_acc = dev_num_correct / n_done\n\tmsg += \'\\t dev m accuracy: %f\' % dev_acc\n\tprint(msg)\n\t# test after each epoch\n\tif accumulated_loss < max_result:\n\t\tmax_result=accumulated_loss\n\t\tpred = []\n\t\tpred_all=[]\n\t\tfor dev_x1, dev_x2, dev_y in test_m:\n\t\t\tx1, x2, y = prepare_data(dev_x1, dev_x2, dev_y)\n\t\t\twith torch.no_grad():\n\t\t\t\toutput = model(x1[0], x1[1], x1[2], x1[3], x2[0], x2[1], x2[2], x2[3])\n\t\t\tresult = output.data.cpu().numpy()\n\t\t\ta = numpy.argmax(result, axis=1)\n\t\t\tpred.extend(a)\n\t\t\tpred_all.extend(result)\n\t\twith open(base_path+\'/mnli_m_Tree_IM_prob.txt\',\'w\') as f:\n\t\t\tfor item in pred_all:\n\t\t\t\tf.writelines(str(item[0])+\'\\t\'+str(item[1])+\'\\t\'+str(item[2])+\'\\n\')\n\t\twith open(base_path + \'/sub_m.csv\', \'w+\') as f:\n\t\t\tindex = [\'entailment\', \'neutral\', \'contradiction\']\n\t\t\tf.write(""pairID,gold_label\\n"")\n\t\t\tfor i, k in enumerate(pred):\n\t\t\t\tf.write(str(i + 9847) + "","" + index[k] + ""\\n"")\n\t# um valid after each epoch\n\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\taccumulated_loss = 0\n\tdev_num_correct = 0\n\tn_done = 0\n\tfor dev_x1, dev_x2, dev_y in valid_um:\n\t\tx1, x2, y = prepare_data(dev_x1, dev_x2, dev_y)\n\t\twith torch.no_grad():\n\t\t\toutput = model(x1[0], x1[1], x1[2], x1[3], x2[0], x2[1], x2[2], x2[3])\n\t\tn_done += len(y)\n\t\tresult = output.data.cpu().numpy()\n\t\tloss = criterion(output, y)\n\t\taccumulated_loss += loss.data[0]\n\t\ta = numpy.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\tdev_num_correct += numpy.sum(a == b)\n\tmsg += \'\\t dev um loss: %f\' % (accumulated_loss / n_done)\n\tdev_acc = dev_num_correct / n_done\n\tmsg += \'\\t dev um accuracy: %f\' % dev_acc\n\tprint(msg)\n\t# test after each epoch\n\tif accumulated_loss < max_result_um:\n\t\tmax_result_um=accumulated_loss\n\t\tpred = []\n\t\tpred_all=[]\n\t\tfor dev_x1, dev_x2, dev_y in test_um:\n\t\t\tx1, x2, y = prepare_data(dev_x1, dev_x2, dev_y)\n\t\t\twith torch.no_grad():\n\t\t\t\toutput = model(x1[0], x1[1], x1[2], x1[3], x2[0], x2[1], x2[2], x2[3])\n\t\t\tresult = output.data.cpu().numpy()\n\t\t\ta = numpy.argmax(result, axis=1)\n\t\t\tpred.extend(a)\n\t\t\tpred_all.extend(result)\n\t\twith open(base_path+\'/mnli_um_Tree_IM_prob.txt\',\'w\') as f:\n\t\t\tfor item in pred_all:\n\t\t\t\tf.writelines(str(item[0])+\'\\t\'+str(item[1])+\'\\t\'+str(item[2])+\'\\n\')\n\t\twith open(base_path + \'/sub_um.csv\', \'w+\') as f:\n\t\t\tindex = [\'entailment\', \'neutral\', \'contradiction\']\n\t\t\tf.write(""pairID,gold_label\\n"")\n\t\t\tfor i, k in enumerate(pred):\n\t\t\t\tf.write(str(i) + "","" + index[k] + ""\\n"")'"
ESIM/Tree_IM/main_pit.py,26,"b'from __future__ import division\nimport sys\n\nimport gc\nimport numpy\nimport torch\nimport time\nfrom data_iterator import TextIterator\nimport cPickle as pkl\nfrom model_batch import *\nfrom os.path import expanduser\nfrom datetime import timedelta\nfrom torchtext.vocab import load_word_vectors\n\n# batch preparation\ndef prepare_data(group_x, group_y, labels):\n\tlengths_x = [len(s[0]) for s in group_x]\n\tlengths_y = [len(s[0]) for s in group_y]\n\n\tn_samples = len(group_x)\n\tmaxlen_x = numpy.max(lengths_x)\n\tmaxlen_y = numpy.max(lengths_y)\n\n\tx_seq = numpy.zeros((maxlen_x, n_samples)).astype(\'int64\')\n\ty_seq = numpy.zeros((maxlen_y, n_samples)).astype(\'int64\')\n\tx_mask = numpy.zeros((maxlen_x, n_samples)).astype(\'float32\')\n\ty_mask = numpy.zeros((maxlen_y, n_samples)).astype(\'float32\')\n\tx_left_mask = numpy.zeros((maxlen_x, n_samples, maxlen_x)).astype(\'float32\')\n\tx_right_mask = numpy.zeros((maxlen_x, n_samples, maxlen_x)).astype(\'float32\')\n\ty_left_mask = numpy.zeros((maxlen_y, n_samples, maxlen_y)).astype(\'float32\')\n\ty_right_mask = numpy.zeros((maxlen_y, n_samples, maxlen_y)).astype(\'float32\')\n\tl = numpy.zeros((n_samples,)).astype(\'int64\')\n\n\tfor idx, [s_x, s_y, ll] in enumerate(zip(group_x, group_y, labels)):\n\t\tx_seq[-lengths_x[idx]:, idx] = s_x[0]\n\t\tx_mask[-lengths_x[idx]:, idx] = 1.\n\t\tx_left_mask[-lengths_x[idx]:, idx, -lengths_x[idx]:] = s_x[1]\n\t\tx_right_mask[-lengths_x[idx]:, idx, -lengths_x[idx]:] = s_x[2]\n\t\ty_seq[-lengths_y[idx]:, idx] = s_y[0]\n\t\ty_mask[-lengths_y[idx]:, idx] = 1.\n\t\ty_left_mask[-lengths_y[idx]:, idx, -lengths_y[idx]:] = s_y[1]\n\t\ty_right_mask[-lengths_y[idx]:, idx, -lengths_y[idx]:] = s_y[2]\n\t\tl[idx] = ll\n\n\tif torch.cuda.is_available():\n\t\tx_seq=Variable(torch.LongTensor(x_seq)).cuda()\n\t\ty_seq = Variable(torch.LongTensor(y_seq)).cuda()\n\t\tx_mask = Variable(torch.FloatTensor(x_mask)).cuda()\n\t\ty_mask = Variable(torch.FloatTensor(y_mask)).cuda()\n\t\tx_left_mask=Variable(torch.FloatTensor(x_left_mask)).cuda()\n\t\ty_left_mask=Variable(torch.FloatTensor(y_left_mask)).cuda()\n\t\tx_right_mask=Variable(torch.FloatTensor(x_right_mask)).cuda()\n\t\ty_right_mask=Variable(torch.FloatTensor(y_right_mask)).cuda()\n\t\tl=Variable(torch.LongTensor(l)).cuda()\n\telse:\n\t\tx_seq = Variable(torch.LongTensor(x_seq))\n\t\ty_seq = Variable(torch.LongTensor(y_seq))\n\t\tx_mask = Variable(torch.FloatTensor(x_mask))\n\t\ty_mask = Variable(torch.FloatTensor(y_mask))\n\t\tx_left_mask = Variable(torch.FloatTensor(x_left_mask))\n\t\ty_left_mask = Variable(torch.FloatTensor(y_left_mask))\n\t\tx_right_mask = Variable(torch.FloatTensor(x_right_mask))\n\t\ty_right_mask = Variable(torch.FloatTensor(y_right_mask))\n\t\tl = Variable(torch.LongTensor(l))\n\n\tx = (x_seq, x_mask, x_left_mask, x_right_mask)\n\ty = (y_seq, y_mask, y_left_mask, y_right_mask)\n\n\treturn x, y, l\n\nprint(\'task: pit\')\nprint(\'model: Tree_IM\')\nif torch.cuda.is_available():\n\tprint(\'CUDA is available!\')\n\tbase_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/pit\'\n\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\nelse:\n\tbase_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/pit\'\n\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\ndatasets = [base_path+\'/train/a.btree\',\n            base_path+\'/train/b.btree\',\n            base_path+\'/train/sim.txt\']\ntest_datasets = [base_path+\'/test/a.btree\',\n                 base_path+\'/test/b.btree\',\n                 base_path+\'/test/sim.txt\']\ndictionary = base_path+\'/pit_vocab_cased.pkl\'\n\nmaxlen=150\nbatch_size=32\nmax_epochs=1000\ndim_word=300\nwith open(dictionary, \'rb\') as f:\n\tworddicts = pkl.load(f)\nn_words=len(worddicts)\nwv_dict, wv_arr, wv_size = load_word_vectors(embedding_path, \'glove.840B\', dim_word)\npretrained_emb=norm_weight(n_words, dim_word)\nfor word in worddicts.keys():\n\ttry:\n\t\tpretrained_emb[worddicts[word]]=wv_arr[wv_dict[word]].numpy()\n\texcept:\n\t\tpretrained_emb[worddicts[word]] = torch.normal(torch.zeros(dim_word),std=1).numpy()\nprint(\'load data...\')\ntrain = TextIterator(datasets[0], datasets[1], datasets[2],\n\t\t\t\t\t\t dictionary,\n\t\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t\t batch_size=batch_size,\n\t\t\t\t\t\t maxlen=maxlen, shuffle=True)\ntest = TextIterator(test_datasets[0], test_datasets[1], test_datasets[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=batch_size,\n\t\t\t\t\t shuffle=False)\ncriterion = torch.nn.CrossEntropyLoss()\nmodel = ESIM(dim_word, 2, n_words, dim_word, pretrained_emb)\nif torch.cuda.is_available():\n\tmodel = model.cuda()\n\tcriterion = criterion.cuda()\noptimizer = torch.optim.Adam(model.parameters(),lr=0.0004)\nprint(\'start training...\')\nclip_c=10\nmax_result=0\nreport_interval=1000\nfor epoch in xrange(max_epochs):\n\tmodel.train()\n\tprint(\'--\' * 20)\n\tstart_time = time.time()\n\tbatch_counter=0\n\ttrain_batch_i = 0\n\ttrain_sents_scaned = 0\n\ttrain_num_correct = 0\n\taccumulated_loss=0\n\tfor x1, x2, y in train:\n\t\t#print(x1[0][0])\n\t\t#for item in x1[0][0]:\n\t\t#\tprint(worddicts.keys()[item])\n\t\tx1, x2, y = prepare_data(x1, x2, y)\n\t\ttrain_sents_scaned += len(y)\n\t\toptimizer.zero_grad()\n\t\t#if x1[0].size(0)>100:\n\t\t#\tprint(x1[0].size(0))\n\t\t#continue\n\t\toutput = model(x1[0], x1[1], x1[2], x1[3], x2[0], x2[1], x2[2], x2[3])\n\t\tresult = output.data.cpu().numpy()\n\t\ta = np.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\ttrain_num_correct += np.sum(a == b)\n\t\tloss = criterion(output, y)\n\t\tloss.backward()\n\t\tgrad_norm = 0.\n\t\t\'\'\'\'\'\'\n\t\tfor m in list(model.parameters()):\n\t\t\tgrad_norm+=m.grad.data.norm() ** 2\n\n\t\tfor m in list(model.parameters()):\n\t\t\tif grad_norm>clip_c**2:\n\t\t\t\ttry:\n\t\t\t\t\tm.grad.data= m.grad.data / torch.sqrt(grad_norm) * clip_c\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\t\t\'\'\'\'\'\'\n\t\toptimizer.step()\n\t\taccumulated_loss += loss.data[0]\n\t\tbatch_counter += 1\n\t\tdel x1,x2,y\n\t\tif batch_counter % report_interval == 0:\n\t\t\tgc.collect()\n\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\tmsg += \'\\t training batch loss: %f\' % (accumulated_loss / train_sents_scaned)\n\t\t\tmsg += \'\\t train accuracy: %f\' % (train_num_correct / train_sents_scaned)\n\t\t\tprint(msg)\n\t# test after each epoch\n\tmodel.eval()\n\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\taccumulated_loss = 0\n\tdev_num_correct = 0\n\tn_done = 0\n\tpred=[]\n\tpred_prob = []\n\tgold = []\n\tfor dev_x1, dev_x2, dev_y in test:\n\t\tx1, x2, y = prepare_data(dev_x1, dev_x2, dev_y)\n\t\twith torch.no_grad():\n\t\t\toutput = F.softmax(model(x1[0], x1[1], x1[2], x1[3], x2[0], x2[1], x2[2], x2[3]))\n\t\tn_done += len(y)\n\t\tresult = output.data.cpu().numpy()\n\t\tloss = criterion(output, y)\n\t\taccumulated_loss += loss.data[0]\n\t\ta = numpy.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\tdev_num_correct += numpy.sum(a == b)\n\t\tpred.extend(result)\n\t\tpred_prob.extend(result[:, 1])\n\t\tb = y.data.cpu().numpy()\n\t\tgold.extend(b)\n\tmsg += \'\\t test loss: %f\' % (accumulated_loss / n_done)\n\tdev_acc = dev_num_correct / n_done\n\tmsg += \'\\t test accuracy: %f\' % dev_acc\n\tprint(msg)\n\t_,maxF1=URL_maxF1_eval(pred_prob, gold)\n\tif maxF1>max_result:\n\t\tmax_result=maxF1\n\t\twith open(base_path+\'/pit_Tree_IM_prob.txt\',\'w\') as f:\n\t\t\tfor item in pred:\n\t\t\t\tf.writelines(str(item[0])+\'\\t\'+str(item[1])+\'\\n\')\n\telapsed_time = time.time() - start_time\n\tprint(\'Epoch \' + str(epoch) + \' finished within \' + str(timedelta(seconds=elapsed_time)))'"
ESIM/Tree_IM/main_quora.py,27,"b'from __future__ import division\nimport sys\n\nimport gc\nimport numpy\nimport torch\nimport time\nfrom data_iterator import TextIterator\nimport cPickle as pkl\nfrom model_batch import *\nfrom os.path import expanduser\nfrom datetime import timedelta\nfrom torchtext.vocab import load_word_vectors\n\n# batch preparation\ndef prepare_data(group_x, group_y, labels):\n\tlengths_x = [len(s[0]) for s in group_x]\n\tlengths_y = [len(s[0]) for s in group_y]\n\n\tn_samples = len(group_x)\n\tmaxlen_x = numpy.max(lengths_x)\n\tmaxlen_y = numpy.max(lengths_y)\n\n\tx_seq = numpy.zeros((maxlen_x, n_samples)).astype(\'int64\')\n\ty_seq = numpy.zeros((maxlen_y, n_samples)).astype(\'int64\')\n\tx_mask = numpy.zeros((maxlen_x, n_samples)).astype(\'float32\')\n\ty_mask = numpy.zeros((maxlen_y, n_samples)).astype(\'float32\')\n\tx_left_mask = numpy.zeros((maxlen_x, n_samples, maxlen_x)).astype(\'float32\')\n\tx_right_mask = numpy.zeros((maxlen_x, n_samples, maxlen_x)).astype(\'float32\')\n\ty_left_mask = numpy.zeros((maxlen_y, n_samples, maxlen_y)).astype(\'float32\')\n\ty_right_mask = numpy.zeros((maxlen_y, n_samples, maxlen_y)).astype(\'float32\')\n\tl = numpy.zeros((n_samples,)).astype(\'int64\')\n\n\tfor idx, [s_x, s_y, ll] in enumerate(zip(group_x, group_y, labels)):\n\t\tx_seq[-lengths_x[idx]:, idx] = s_x[0]\n\t\tx_mask[-lengths_x[idx]:, idx] = 1.\n\t\tx_left_mask[-lengths_x[idx]:, idx, -lengths_x[idx]:] = s_x[1]\n\t\tx_right_mask[-lengths_x[idx]:, idx, -lengths_x[idx]:] = s_x[2]\n\t\ty_seq[-lengths_y[idx]:, idx] = s_y[0]\n\t\ty_mask[-lengths_y[idx]:, idx] = 1.\n\t\ty_left_mask[-lengths_y[idx]:, idx, -lengths_y[idx]:] = s_y[1]\n\t\ty_right_mask[-lengths_y[idx]:, idx, -lengths_y[idx]:] = s_y[2]\n\t\tl[idx] = ll\n\n\tif torch.cuda.is_available():\n\t\tx_seq=Variable(torch.LongTensor(x_seq)).cuda()\n\t\ty_seq = Variable(torch.LongTensor(y_seq)).cuda()\n\t\tx_mask = Variable(torch.FloatTensor(x_mask)).cuda()\n\t\ty_mask = Variable(torch.FloatTensor(y_mask)).cuda()\n\t\tx_left_mask=Variable(torch.FloatTensor(x_left_mask)).cuda()\n\t\ty_left_mask=Variable(torch.FloatTensor(y_left_mask)).cuda()\n\t\tx_right_mask=Variable(torch.FloatTensor(x_right_mask)).cuda()\n\t\ty_right_mask=Variable(torch.FloatTensor(y_right_mask)).cuda()\n\t\tl=Variable(torch.LongTensor(l)).cuda()\n\telse:\n\t\tx_seq = Variable(torch.LongTensor(x_seq))\n\t\ty_seq = Variable(torch.LongTensor(y_seq))\n\t\tx_mask = Variable(torch.FloatTensor(x_mask))\n\t\ty_mask = Variable(torch.FloatTensor(y_mask))\n\t\tx_left_mask = Variable(torch.FloatTensor(x_left_mask))\n\t\ty_left_mask = Variable(torch.FloatTensor(y_left_mask))\n\t\tx_right_mask = Variable(torch.FloatTensor(x_right_mask))\n\t\ty_right_mask = Variable(torch.FloatTensor(y_right_mask))\n\t\tl = Variable(torch.LongTensor(l))\n\n\tx = (x_seq, x_mask, x_left_mask, x_right_mask)\n\ty = (y_seq, y_mask, y_left_mask, y_right_mask)\n\n\treturn x, y, l\n\nprint(\'task: quora\')\nprint(\'model: Tree_IM\')\nif torch.cuda.is_available():\n\tprint(\'CUDA is available!\')\n\tbase_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord\'\n\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\nelse:\n\tbase_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord\'\n\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\ndatasets = [base_path+\'/data/quora/train/a.btree\',\n            base_path+\'/data/quora/train/b.btree\',\n            base_path+\'/data/quora/train/sim.txt\']\nvalid_datasets = [base_path+\'/data/quora/dev/a.btree\',\n                  base_path+\'/data/quora/dev/b.btree\',\n                  base_path+\'/data/quora/dev/sim.txt\']\ntest_datasets = [base_path+\'/data/quora/test/a.btree\',\n                 base_path+\'/data/quora/test/b.btree\',\n                 base_path+\'/data/quora/test/sim.txt\']\ndictionary = base_path+\'/data/quora/quora_vocab_cased.pkl\'\n\nmaxlen=150\nbatch_size=32\nmax_epochs=1000\ndim_word=300\nwith open(dictionary, \'rb\') as f:\n\tworddicts = pkl.load(f)\nn_words=len(worddicts)\nwv_dict, wv_arr, wv_size = load_word_vectors(embedding_path, \'glove.840B\', dim_word)\npretrained_emb=norm_weight(n_words, dim_word)\nfor word in worddicts.keys():\n\ttry:\n\t\tpretrained_emb[worddicts[word]]=wv_arr[wv_dict[word]].numpy()\n\texcept:\n\t\tpretrained_emb[worddicts[word]] = torch.normal(torch.zeros(dim_word),std=1).numpy()\nprint(\'load data...\')\ntrain = TextIterator(datasets[0], datasets[1], datasets[2],\n\t\t\t\t\t\t dictionary,\n\t\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t\t batch_size=batch_size,\n\t\t\t\t\t\t maxlen=maxlen, shuffle=True)\nvalid = TextIterator(valid_datasets[0], valid_datasets[1], valid_datasets[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=batch_size,\n\t\t\t\t\t shuffle=False)\ntest = TextIterator(test_datasets[0], test_datasets[1], test_datasets[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=batch_size,\n\t\t\t\t\t shuffle=False)\ncriterion = torch.nn.CrossEntropyLoss()\nmodel = ESIM(dim_word, 2, n_words, dim_word, pretrained_emb)\nif torch.cuda.is_available():\n\tmodel = model.cuda()\n\tcriterion = criterion.cuda()\noptimizer = torch.optim.Adam(model.parameters(),lr=0.0004)\nprint(\'start training...\')\nclip_c=10\nmax_result=0\nreport_interval=1000\nfor epoch in xrange(max_epochs):\n\tmodel.train()\n\tprint(\'--\' * 20)\n\tstart_time = time.time()\n\tbatch_counter=0\n\ttrain_batch_i = 0\n\ttrain_sents_scaned = 0\n\ttrain_num_correct = 0\n\taccumulated_loss=0\n\tfor x1, x2, y in train:\n\t\t#print(x1[0][0])\n\t\t#for item in x1[0][0]:\n\t\t#\tprint(worddicts.keys()[item])\n\t\tx1, x2, y = prepare_data(x1, x2, y)\n\t\ttrain_sents_scaned += len(y)\n\t\toptimizer.zero_grad()\n\t\t#if x1[0].size(0)>100:\n\t\t#\tprint(x1[0].size(0))\n\t\t#continue\n\t\toutput = model(x1[0], x1[1], x1[2], x1[3], x2[0], x2[1], x2[2], x2[3])\n\t\tresult = output.data.cpu().numpy()\n\t\ta = np.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\ttrain_num_correct += np.sum(a == b)\n\t\tloss = criterion(output, y)\n\t\tloss.backward()\n\t\tgrad_norm = 0.\n\n\t\tfor m in list(model.parameters()):\n\t\t\tgrad_norm+=m.grad.data.norm() ** 2\n\n\t\tfor m in list(model.parameters()):\n\t\t\tif grad_norm>clip_c**2:\n\t\t\t\ttry:\n\t\t\t\t\tm.grad.data= m.grad.data / torch.sqrt(grad_norm) * clip_c\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\n\t\toptimizer.step()\n\t\taccumulated_loss += loss.data[0]\n\t\tbatch_counter += 1\n\t\tdel x1,x2,y\n\t\tif batch_counter % report_interval == 0:\n\t\t\tgc.collect()\n\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\tmsg += \'\\t training batch loss: %f\' % (accumulated_loss / train_sents_scaned)\n\t\t\tmsg += \'\\t train accuracy: %f\' % (train_num_correct / train_sents_scaned)\n\t\t\tprint(msg)\n\t\t\t# valid after each report interval\n\t\t\tmodel.eval()\n\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\taccumulated_loss = 0\n\t\t\tdev_num_correct = 0\n\t\t\tn_done = 0\n\t\t\tfor dev_x1, dev_x2, dev_y in valid:\n\t\t\t\tx1, x2, y = prepare_data(dev_x1, dev_x2, dev_y)\n\t\t\t\twith torch.no_grad():\n\t\t\t\t\toutput = F.softmax(model(x1[0], x1[1], x1[2], x1[3], x2[0], x2[1], x2[2], x2[3]))\n\t\t\t\tn_done += len(y)\n\t\t\t\tresult = output.data.cpu().numpy()\n\t\t\t\tloss = criterion(output, y)\n\t\t\t\taccumulated_loss += loss.data[0]\n\t\t\t\ta = numpy.argmax(result, axis=1)\n\t\t\t\tb = y.data.cpu().numpy()\n\t\t\t\tdev_num_correct += numpy.sum(a == b)\n\t\t\tmsg += \'\\t dev loss: %f\' % (accumulated_loss / n_done)\n\t\t\tdev_acc = dev_num_correct / n_done\n\t\t\tmsg += \'\\t dev accuracy: %f\' % dev_acc\n\t\t\t# test after each epoch\n\t\t\tprint(msg)\n\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\taccumulated_loss = 0\n\t\t\tdev_num_correct = 0\n\t\t\tn_done = 0\n\t\t\tpred=[]\n\t\t\tfor dev_x1, dev_x2, dev_y in test:\n\t\t\t\tx1, x2, y = prepare_data(dev_x1, dev_x2, dev_y)\n\t\t\t\twith torch.no_grad():\n\t\t\t\t\toutput = F.softmax(model(x1[0], x1[1], x1[2], x1[3], x2[0], x2[1], x2[2], x2[3]))\n\t\t\t\tn_done += len(y)\n\t\t\t\tresult = output.data.cpu().numpy()\n\t\t\t\tloss = criterion(output, y)\n\t\t\t\taccumulated_loss += loss.data[0]\n\t\t\t\ta = numpy.argmax(result, axis=1)\n\t\t\t\tb = y.data.cpu().numpy()\n\t\t\t\tdev_num_correct += numpy.sum(a == b)\n\t\t\t\tpred.extend(result)\n\t\t\tmsg += \'\\t test loss: %f\' % (accumulated_loss / n_done)\n\t\t\tdev_acc = dev_num_correct / n_done\n\t\t\tmsg += \'\\t test accuracy: %f\' % dev_acc\n\t\t\tprint(msg)\n\t\t\tif dev_acc>max_result:\n\t\t\t\tmax_result=dev_acc\n\t\t\t\twith open(base_path+\'/quora_Tree_IM_prob.txt\',\'w\') as f:\n\t\t\t\t\tfor item in pred:\n\t\t\t\t\t\tf.writelines(str(item[0])+\'\\t\'+str(item[1])+\'\\n\')\n\t\t\tmodel.train()\n\telapsed_time = time.time() - start_time\n\tprint(\'Epoch \' + str(epoch) + \' finished within \' + str(timedelta(seconds=elapsed_time)))'"
ESIM/Tree_IM/main_snli.py,27,"b'from __future__ import division\nimport sys\n\nimport gc\nimport numpy\nimport torch\nimport time\nfrom data_iterator import TextIterator\nimport cPickle as pkl\nfrom model_batch import *\nfrom os.path import expanduser\nfrom datetime import timedelta\nfrom torchtext.vocab import load_word_vectors\n\n# batch preparation\ndef prepare_data(group_x, group_y, labels):\n\tlengths_x = [len(s[0]) for s in group_x]\n\tlengths_y = [len(s[0]) for s in group_y]\n\n\tn_samples = len(group_x)\n\tmaxlen_x = numpy.max(lengths_x)\n\tmaxlen_y = numpy.max(lengths_y)\n\n\tx_seq = numpy.zeros((maxlen_x, n_samples)).astype(\'int64\')\n\ty_seq = numpy.zeros((maxlen_y, n_samples)).astype(\'int64\')\n\tx_mask = numpy.zeros((maxlen_x, n_samples)).astype(\'float32\')\n\ty_mask = numpy.zeros((maxlen_y, n_samples)).astype(\'float32\')\n\tx_left_mask = numpy.zeros((maxlen_x, n_samples, maxlen_x)).astype(\'float32\')\n\tx_right_mask = numpy.zeros((maxlen_x, n_samples, maxlen_x)).astype(\'float32\')\n\ty_left_mask = numpy.zeros((maxlen_y, n_samples, maxlen_y)).astype(\'float32\')\n\ty_right_mask = numpy.zeros((maxlen_y, n_samples, maxlen_y)).astype(\'float32\')\n\tl = numpy.zeros((n_samples,)).astype(\'int64\')\n\n\tfor idx, [s_x, s_y, ll] in enumerate(zip(group_x, group_y, labels)):\n\t\tx_seq[-lengths_x[idx]:, idx] = s_x[0]\n\t\tx_mask[-lengths_x[idx]:, idx] = 1.\n\t\tx_left_mask[-lengths_x[idx]:, idx, -lengths_x[idx]:] = s_x[1]\n\t\tx_right_mask[-lengths_x[idx]:, idx, -lengths_x[idx]:] = s_x[2]\n\t\ty_seq[-lengths_y[idx]:, idx] = s_y[0]\n\t\ty_mask[-lengths_y[idx]:, idx] = 1.\n\t\ty_left_mask[-lengths_y[idx]:, idx, -lengths_y[idx]:] = s_y[1]\n\t\ty_right_mask[-lengths_y[idx]:, idx, -lengths_y[idx]:] = s_y[2]\n\t\tl[idx] = ll\n\n\tif torch.cuda.is_available():\n\t\tx_seq=Variable(torch.LongTensor(x_seq)).cuda()\n\t\ty_seq = Variable(torch.LongTensor(y_seq)).cuda()\n\t\tx_mask = Variable(torch.FloatTensor(x_mask)).cuda()\n\t\ty_mask = Variable(torch.FloatTensor(y_mask)).cuda()\n\t\tx_left_mask=Variable(torch.FloatTensor(x_left_mask)).cuda()\n\t\ty_left_mask=Variable(torch.FloatTensor(y_left_mask)).cuda()\n\t\tx_right_mask=Variable(torch.FloatTensor(x_right_mask)).cuda()\n\t\ty_right_mask=Variable(torch.FloatTensor(y_right_mask)).cuda()\n\t\tl=Variable(torch.LongTensor(l)).cuda()\n\telse:\n\t\tx_seq = Variable(torch.LongTensor(x_seq))\n\t\ty_seq = Variable(torch.LongTensor(y_seq))\n\t\tx_mask = Variable(torch.FloatTensor(x_mask))\n\t\ty_mask = Variable(torch.FloatTensor(y_mask))\n\t\tx_left_mask = Variable(torch.FloatTensor(x_left_mask))\n\t\ty_left_mask = Variable(torch.FloatTensor(y_left_mask))\n\t\tx_right_mask = Variable(torch.FloatTensor(x_right_mask))\n\t\ty_right_mask = Variable(torch.FloatTensor(y_right_mask))\n\t\tl = Variable(torch.LongTensor(l))\n\n\tx = (x_seq, x_mask, x_left_mask, x_right_mask)\n\ty = (y_seq, y_mask, y_left_mask, y_right_mask)\n\n\treturn x, y, l\n\nprint(\'task: SNLI\')\nprint(\'model: Tree_IM\')\nif torch.cuda.is_available():\n\tprint(\'CUDA is available!\')\n\tbase_path = expanduser(""~"") + \'/pytorch/ESIM\'\n\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\nelse:\n\tbase_path = expanduser(""~"") + \'/Documents/research/pytorch/ESIM\'\n\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\ndatasets = [base_path+\'/data/binary_tree/premise_snli_1.0_train.txt\',\n            base_path+\'/data/binary_tree/hypothesis_snli_1.0_train.txt\',\n            base_path+\'/data/binary_tree/label_snli_1.0_train.txt\']\nvalid_datasets = [base_path+\'/data/binary_tree/premise_snli_1.0_dev.txt\',\n                  base_path+\'/data/binary_tree/hypothesis_snli_1.0_dev.txt\',\n                  base_path+\'/data/binary_tree/label_snli_1.0_dev.txt\']\ntest_datasets = [base_path+\'/data/binary_tree/premise_snli_1.0_test.txt\',\n                 base_path+\'/data/binary_tree/hypothesis_snli_1.0_test.txt\',\n                 base_path+\'/data/binary_tree/label_snli_1.0_test.txt\']\ndictionary = base_path+\'/data/binary_tree/snli_vocab_cased.pkl\'\n\nmaxlen=150\nbatch_size=32\nmax_epochs=1000\ndim_word=300\nwith open(dictionary, \'rb\') as f:\n\tworddicts = pkl.load(f)\nn_words=len(worddicts)\nwv_dict, wv_arr, wv_size = load_word_vectors(embedding_path, \'glove.840B\', dim_word)\npretrained_emb=norm_weight(n_words, dim_word)\nfor word in worddicts.keys():\n\ttry:\n\t\tpretrained_emb[worddicts[word]]=wv_arr[wv_dict[word]].numpy()\n\texcept:\n\t\tpretrained_emb[worddicts[word]] = torch.normal(torch.zeros(dim_word),std=1).numpy()\nprint(\'load data...\')\ntrain = TextIterator(datasets[0], datasets[1], datasets[2],\n\t\t\t\t\t\t dictionary,\n\t\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t\t batch_size=batch_size,\n\t\t\t\t\t\t maxlen=maxlen, shuffle=True)\nvalid = TextIterator(valid_datasets[0], valid_datasets[1], valid_datasets[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=batch_size,\n\t\t\t\t\t shuffle=False)\ntest = TextIterator(test_datasets[0], test_datasets[1], test_datasets[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=batch_size,\n\t\t\t\t\t shuffle=False)\ncriterion = torch.nn.CrossEntropyLoss()\nmodel = ESIM(dim_word, 3, n_words, dim_word, pretrained_emb)\nif torch.cuda.is_available():\n\tmodel = model.cuda()\n\tcriterion = criterion.cuda()\noptimizer = torch.optim.Adam(model.parameters(),lr=0.0004)\nprint(\'start training...\')\nclip_c=10\nmax_result=0\nreport_interval=1000\nfor epoch in xrange(max_epochs):\n\tmodel.train()\n\tprint(\'--\' * 20)\n\tstart_time = time.time()\n\tbatch_counter=0\n\ttrain_batch_i = 0\n\ttrain_sents_scaned = 0\n\ttrain_num_correct = 0\n\taccumulated_loss=0\n\tfor x1, x2, y in train:\n\t\t#print(x1[0][0])\n\t\t#for item in x1[0][0]:\n\t\t#\tprint(worddicts.keys()[item])\n\t\tx1, x2, y = prepare_data(x1, x2, y)\n\t\ttrain_sents_scaned += len(y)\n\t\toptimizer.zero_grad()\n\t\t#if x1[0].size(0)>100:\n\t\t#\tprint(x1[0].size(0))\n\t\t#continue\n\t\toutput = model(x1[0], x1[1], x1[2], x1[3], x2[0], x2[1], x2[2], x2[3])\n\t\tresult = output.data.cpu().numpy()\n\t\ta = np.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\ttrain_num_correct += np.sum(a == b)\n\t\tloss = criterion(output, y)\n\t\tloss.backward()\n\t\tgrad_norm = 0.\n\n\t\tfor m in list(model.parameters()):\n\t\t\tgrad_norm+=m.grad.data.norm() ** 2\n\n\t\tfor m in list(model.parameters()):\n\t\t\tif grad_norm>clip_c**2:\n\t\t\t\ttry:\n\t\t\t\t\tm.grad.data= m.grad.data / torch.sqrt(grad_norm) * clip_c\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\n\t\toptimizer.step()\n\t\taccumulated_loss += loss.data[0]\n\t\tbatch_counter += 1\n\t\tdel x1,x2,y\n\t\tif batch_counter % report_interval == 0:\n\t\t\tgc.collect()\n\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\tmsg += \'\\t training batch loss: %f\' % (accumulated_loss / train_sents_scaned)\n\t\t\tmsg += \'\\t train accuracy: %f\' % (train_num_correct / train_sents_scaned)\n\t\t\tprint(msg)\n\t# valid after each epoch\n\tmodel.eval()\n\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\taccumulated_loss = 0\n\tdev_num_correct = 0\n\tn_done = 0\n\tfor dev_x1, dev_x2, dev_y in valid:\n\t\tx1, x2, y = prepare_data(dev_x1, dev_x2, dev_y)\n\t\twith torch.no_grad():\n\t\t\toutput = F.softmax(model(x1[0], x1[1], x1[2], x1[3], x2[0], x2[1], x2[2], x2[3]))\n\t\tn_done += len(y)\n\t\tresult = output.data.cpu().numpy()\n\t\tloss = criterion(output, y)\n\t\taccumulated_loss += loss.data[0]\n\t\ta = numpy.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\tdev_num_correct += numpy.sum(a == b)\n\tmsg += \'\\t dev loss: %f\' % (accumulated_loss / n_done)\n\tdev_acc = dev_num_correct / n_done\n\tmsg += \'\\t dev accuracy: %f\' % dev_acc\n\t# test after each epoch\n\tprint(msg)\n\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\taccumulated_loss = 0\n\tdev_num_correct = 0\n\tn_done = 0\n\tpred=[]\n\tfor dev_x1, dev_x2, dev_y in test:\n\t\tx1, x2, y = prepare_data(dev_x1, dev_x2, dev_y)\n\t\twith torch.no_grad():\n\t\t\toutput = F.softmax(model(x1[0], x1[1], x1[2], x1[3], x2[0], x2[1], x2[2], x2[3]))\n\t\tn_done += len(y)\n\t\tresult = output.data.cpu().numpy()\n\t\tloss = criterion(output, y)\n\t\taccumulated_loss += loss.data[0]\n\t\ta = numpy.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\tdev_num_correct += numpy.sum(a == b)\n\t\tpred.extend(result)\n\tmsg += \'\\t test loss: %f\' % (accumulated_loss / n_done)\n\tdev_acc = dev_num_correct / n_done\n\tmsg += \'\\t test accuracy: %f\' % dev_acc\n\tprint(msg)\n\tif dev_acc>max_result:\n\t\tmax_result=dev_acc\n\t\twith open(base_path+\'/snli_Tree_IM_prob.txt\',\'w\') as f:\n\t\t\tfor item in pred:\n\t\t\t\tf.writelines(str(item[0])+\'\\t\'+str(item[1])+\'\\t\'+str(item[2])+\'\\n\')\n\telapsed_time = time.time() - start_time\n\tprint(\'Epoch \' + str(epoch) + \' finished within \' + str(timedelta(seconds=elapsed_time)))'"
ESIM/Tree_IM/main_sts.py,27,"b'from __future__ import division\nimport sys\n\nimport gc\nimport numpy\nimport torch\nimport time\nfrom data_iterator import TextIterator\nimport cPickle as pkl\nfrom model_batch import *\nfrom os.path import expanduser\nfrom datetime import timedelta\nfrom numpy import linalg as LA\nfrom torchtext.vocab import load_word_vectors\n\ndef pearson(x,y):\n\tx=np.array(x)\n\ty=np.array(y)\n\tx=x-np.mean(x)\n\ty=y-np.mean(y)\n\treturn x.dot(y)/(LA.norm(x)*LA.norm(y))\n\n# batch preparation\ndef prepare_data(group_x, group_y, labels):\n\tlengths_x = [len(s[0]) for s in group_x]\n\tlengths_y = [len(s[0]) for s in group_y]\n\n\tn_samples = len(group_x)\n\tmaxlen_x = numpy.max(lengths_x)\n\tmaxlen_y = numpy.max(lengths_y)\n\n\tx_seq = numpy.zeros((maxlen_x, n_samples)).astype(\'int64\')\n\ty_seq = numpy.zeros((maxlen_y, n_samples)).astype(\'int64\')\n\tx_mask = numpy.zeros((maxlen_x, n_samples)).astype(\'float32\')\n\ty_mask = numpy.zeros((maxlen_y, n_samples)).astype(\'float32\')\n\tx_left_mask = numpy.zeros((maxlen_x, n_samples, maxlen_x)).astype(\'float32\')\n\tx_right_mask = numpy.zeros((maxlen_x, n_samples, maxlen_x)).astype(\'float32\')\n\ty_left_mask = numpy.zeros((maxlen_y, n_samples, maxlen_y)).astype(\'float32\')\n\ty_right_mask = numpy.zeros((maxlen_y, n_samples, maxlen_y)).astype(\'float32\')\n\tl = numpy.zeros((n_samples, 6)).astype(\'float32\')\n\n\tfor idx, [s_x, s_y, ll] in enumerate(zip(group_x, group_y, labels)):\n\t\tx_seq[-lengths_x[idx]:, idx] = s_x[0]\n\t\tx_mask[-lengths_x[idx]:, idx] = 1.\n\t\tx_left_mask[-lengths_x[idx]:, idx, -lengths_x[idx]:] = s_x[1]\n\t\tx_right_mask[-lengths_x[idx]:, idx, -lengths_x[idx]:] = s_x[2]\n\t\ty_seq[-lengths_y[idx]:, idx] = s_y[0]\n\t\ty_mask[-lengths_y[idx]:, idx] = 1.\n\t\ty_left_mask[-lengths_y[idx]:, idx, -lengths_y[idx]:] = s_y[1]\n\t\ty_right_mask[-lengths_y[idx]:, idx, -lengths_y[idx]:] = s_y[2]\n\t\tl[idx] = ll\n\n\tif torch.cuda.is_available():\n\t\tx_seq=Variable(torch.LongTensor(x_seq)).cuda()\n\t\ty_seq = Variable(torch.LongTensor(y_seq)).cuda()\n\t\tx_mask = Variable(torch.FloatTensor(x_mask)).cuda()\n\t\ty_mask = Variable(torch.FloatTensor(y_mask)).cuda()\n\t\tx_left_mask=Variable(torch.FloatTensor(x_left_mask)).cuda()\n\t\ty_left_mask=Variable(torch.FloatTensor(y_left_mask)).cuda()\n\t\tx_right_mask=Variable(torch.FloatTensor(x_right_mask)).cuda()\n\t\ty_right_mask=Variable(torch.FloatTensor(y_right_mask)).cuda()\n\t\tl=Variable(torch.FloatTensor(l)).cuda()\n\telse:\n\t\tx_seq = Variable(torch.LongTensor(x_seq))\n\t\ty_seq = Variable(torch.LongTensor(y_seq))\n\t\tx_mask = Variable(torch.FloatTensor(x_mask))\n\t\ty_mask = Variable(torch.FloatTensor(y_mask))\n\t\tx_left_mask = Variable(torch.FloatTensor(x_left_mask))\n\t\ty_left_mask = Variable(torch.FloatTensor(y_left_mask))\n\t\tx_right_mask = Variable(torch.FloatTensor(x_right_mask))\n\t\ty_right_mask = Variable(torch.FloatTensor(y_right_mask))\n\t\tl = Variable(torch.FloatTensor(l))\n\n\tx = (x_seq, x_mask, x_left_mask, x_right_mask)\n\ty = (y_seq, y_mask, y_left_mask, y_right_mask)\n\n\treturn x, y, l\n\n#label conversion for STS dataset\ndef label_convertion(y):\n\tlabels = []\n\tfor sim in y:\n\t\tsim=float(sim)\n\t\tceil = int(math.ceil(sim))\n\t\tfloor = int(math.floor(sim))\n\t\ttmp = [0, 0, 0, 0, 0, 0]\n\t\tif floor != ceil:\n\t\t\ttmp[ceil] = sim - math.floor(sim)\n\t\t\ttmp[floor] = math.ceil(sim) - sim\n\t\telse:\n\t\t\ttmp[floor] = 1\n\t\tlabels.append(tmp)\n\treturn labels\n\nprint(\'task: sts\')\nprint(\'model: Tree_IM\')\nif torch.cuda.is_available():\n\tprint(\'CUDA is available!\')\n\tbase_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/sts\'\n\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\nelse:\n\tbase_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/sts\'\n\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\ndatasets = [base_path+\'/train/a.btree\',\n            base_path+\'/train/b.btree\',\n            base_path+\'/train/sim.txt\']\ntest_datasets = [base_path+\'/test/a.btree\',\n                 base_path+\'/test/b.btree\',\n                 base_path+\'/test/sim.txt\']\ndictionary = base_path+\'/sts_vocab_cased.pkl\'\n\nmaxlen=100\nbatch_size=32\nmax_epochs=1000\ndim_word=300\nwith open(dictionary, \'rb\') as f:\n\tworddicts = pkl.load(f)\nn_words=len(worddicts)\nwv_dict, wv_arr, wv_size = load_word_vectors(embedding_path, \'glove.840B\', dim_word)\npretrained_emb=norm_weight(n_words, dim_word)\nfor word in worddicts.keys():\n\ttry:\n\t\tpretrained_emb[worddicts[word]]=wv_arr[wv_dict[word]].numpy()\n\texcept:\n\t\tpretrained_emb[worddicts[word]] = torch.normal(torch.zeros(dim_word),std=1).numpy()\nprint(\'load data...\')\ntrain = TextIterator(datasets[0], datasets[1], datasets[2],\n\t\t\t\t\t\t dictionary,\n\t\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t\t batch_size=batch_size,\n\t\t\t\t\t\t maxlen=maxlen, shuffle=True)\ntest = TextIterator(test_datasets[0], test_datasets[1], test_datasets[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=batch_size,\n\t\t\t\t\t shuffle=False)\ncriterion=torch.nn.KLDivLoss()\n#criterion = torch.nn.CrossEntropyLoss()\nmodel = ESIM(dim_word, 6, n_words, dim_word, pretrained_emb)\nif torch.cuda.is_available():\n\tmodel = model.cuda()\n\tcriterion = criterion.cuda()\noptimizer = torch.optim.Adam(model.parameters(),lr=0.0004)\nprint(\'start training...\')\nclip_c=10\nmax_result=0\nreport_interval=1000\nfor epoch in xrange(max_epochs):\n\tmodel.train()\n\tprint(\'--\' * 20)\n\tstart_time = time.time()\n\tbatch_counter=0\n\ttrain_batch_i = 0\n\ttrain_sents_scaned = 0\n\ttrain_num_correct = 0\n\taccumulated_loss=0\n\tfor x1, x2, y in train:\n\t\t#print(x1[0][0])\n\t\t#for item in x1[0][0]:\n\t\t#\tprint(worddicts.keys()[item])\n\t\ty = label_convertion(y)\n\t\tx1, x2, y = prepare_data(x1, x2, y)\n\t\ttrain_sents_scaned += len(y)\n\t\toptimizer.zero_grad()\n\t\t#if x1[0].size(0)>100:\n\t\t#\tprint(x1[0].size(0))\n\t\t#continue\n\t\toutput = F.log_softmax(model(x1[0], x1[1], x1[2], x1[3], x2[0], x2[1], x2[2], x2[3]))\n\t\t#result = output.data.cpu().numpy()\n\t\t#a = np.argmax(result, axis=1)\n\t\t#b = y.data.cpu().numpy()\n\t\t#train_num_correct += np.sum(a == b)\n\t\tloss = criterion(output, y)\n\t\tloss.backward()\n\t\tgrad_norm = 0.\n\t\t\'\'\'\'\'\'\n\t\tfor m in list(model.parameters()):\n\t\t\tgrad_norm+=m.grad.data.norm() ** 2\n\n\t\tfor m in list(model.parameters()):\n\t\t\tif grad_norm>clip_c**2:\n\t\t\t\ttry:\n\t\t\t\t\tm.grad.data= m.grad.data / torch.sqrt(grad_norm) * clip_c\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\t\t\'\'\'\'\'\'\n\t\toptimizer.step()\n\t\taccumulated_loss += loss.data[0]\n\t\tbatch_counter += 1\n\t\tdel x1,x2,y\n\t\tif batch_counter % report_interval == 0:\n\t\t\tgc.collect()\n\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\tmsg += \'\\t training batch loss: %f\' % (accumulated_loss / train_sents_scaned)\n\t\t\t#msg += \'\\t train accuracy: %f\' % (train_num_correct / train_sents_scaned)\n\t\t\tprint(msg)\n\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\tmsg += \'\\t training batch loss: %f\' % (accumulated_loss / train_sents_scaned)\n\t# msg += \'\\t train accuracy: %f\' % (train_num_correct / train_sents_scaned)\n\tprint(msg)\n\t# test after each epoch\n\tmodel.eval()\n\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\taccumulated_loss = 0\n\tdev_num_correct = 0\n\tn_done = 0\n\tpred=[]\n\tgold = []\n\tfor dev_x1, dev_x2, dev_y in test:\n\t\ty = label_convertion(dev_y)\n\t\tx1, x2, y = prepare_data(dev_x1, dev_x2, y)\n\t\t#if x1[0].size(0)>100:\n\t\t#\tprint(x1[0].size(0))\n\t\t#continue\n\t\twith torch.no_grad():\n\t\t\toutput = F.softmax(model(x1[0], x1[1], x1[2], x1[3], x2[0], x2[1], x2[2], x2[3]))\n\t\tn_done += len(y)\n\t\tresult = output.data.cpu().numpy()\n\t\tloss = criterion(F.log_softmax(output), y)\n\t\taccumulated_loss += loss.data[0]\n\t\ta = numpy.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\tpred.extend(0 * result[:, 0] + 1 * result[:, 1] + 2 * result[:, 2] + 3 * result[:, 3] + 4 * result[:, 4] + 5 * result[:,5])\n\t\tgold.extend(0 * b[:, 0] + 1 * b[:, 1] + 2 * b[:, 2] + 3 * b[:, 3] + 4 * b[:, 4] + 5 * b[:, 5])\n\tmsg += \'\\t test loss: %f\' % (accumulated_loss / n_done)\n\tdev_acc = dev_num_correct / n_done\n\t#msg += \'\\t test accuracy: %f\' % dev_acc\n\tprint(msg)\n\tresult1 = pearson(pred[0:450], gold[0:450])\n\tresult2 = pearson(pred[450:750], gold[450:750])\n\tresult3 = pearson(pred[750:1500], gold[750:1500])\n\tresult4 = pearson(pred[1500:2250], gold[1500:2250])\n\tresult5 = pearson(pred[2250:3000], gold[2250:3000])\n\tresult6 = pearson(pred[3000:3750], gold[3000:3750])\n\twt_mean = 0.12 * result1 + 0.08 * result2 + 0.2 * result3 + 0.2 * result4 + 0.2 * result5 + 0.2 * result6\n\tprint(\'weighted pearson mean: %.6f\' % wt_mean)\n\tif wt_mean>max_result:\n\t\tmax_result=wt_mean\n\t\twith open(base_path+\'/sts_Tree_IM_prob.txt\',\'w\') as f:\n\t\t\tfor item in pred:\n\t\t\t\tf.writelines(str(item)+\'\\n\')\n\telapsed_time = time.time() - start_time\n\tprint(\'Epoch \' + str(epoch) + \' finished within \' + str(timedelta(seconds=elapsed_time)))'"
ESIM/Tree_IM/main_trecqa.py,27,"b'from __future__ import division\nimport sys\n\nimport gc\nimport numpy\nimport torch\nimport time\nfrom data_iterator import TextIterator\nimport cPickle as pkl\nfrom model_batch import *\nfrom os.path import expanduser\nfrom datetime import timedelta\nfrom torchtext.vocab import load_word_vectors\n\n# batch preparation\ndef prepare_data(group_x, group_y, labels):\n\tlengths_x = [len(s[0]) for s in group_x]\n\tlengths_y = [len(s[0]) for s in group_y]\n\n\tn_samples = len(group_x)\n\tmaxlen_x = numpy.max(lengths_x)\n\tmaxlen_y = numpy.max(lengths_y)\n\n\tx_seq = numpy.zeros((maxlen_x, n_samples)).astype(\'int64\')\n\ty_seq = numpy.zeros((maxlen_y, n_samples)).astype(\'int64\')\n\tx_mask = numpy.zeros((maxlen_x, n_samples)).astype(\'float32\')\n\ty_mask = numpy.zeros((maxlen_y, n_samples)).astype(\'float32\')\n\tx_left_mask = numpy.zeros((maxlen_x, n_samples, maxlen_x)).astype(\'float32\')\n\tx_right_mask = numpy.zeros((maxlen_x, n_samples, maxlen_x)).astype(\'float32\')\n\ty_left_mask = numpy.zeros((maxlen_y, n_samples, maxlen_y)).astype(\'float32\')\n\ty_right_mask = numpy.zeros((maxlen_y, n_samples, maxlen_y)).astype(\'float32\')\n\tl = numpy.zeros((n_samples,)).astype(\'int64\')\n\n\tfor idx, [s_x, s_y, ll] in enumerate(zip(group_x, group_y, labels)):\n\t\tx_seq[-lengths_x[idx]:, idx] = s_x[0]\n\t\tx_mask[-lengths_x[idx]:, idx] = 1.\n\t\tx_left_mask[-lengths_x[idx]:, idx, -lengths_x[idx]:] = s_x[1]\n\t\tx_right_mask[-lengths_x[idx]:, idx, -lengths_x[idx]:] = s_x[2]\n\t\ty_seq[-lengths_y[idx]:, idx] = s_y[0]\n\t\ty_mask[-lengths_y[idx]:, idx] = 1.\n\t\ty_left_mask[-lengths_y[idx]:, idx, -lengths_y[idx]:] = s_y[1]\n\t\ty_right_mask[-lengths_y[idx]:, idx, -lengths_y[idx]:] = s_y[2]\n\t\tl[idx] = ll\n\n\tif torch.cuda.is_available():\n\t\tx_seq=Variable(torch.LongTensor(x_seq)).cuda()\n\t\ty_seq = Variable(torch.LongTensor(y_seq)).cuda()\n\t\tx_mask = Variable(torch.FloatTensor(x_mask)).cuda()\n\t\ty_mask = Variable(torch.FloatTensor(y_mask)).cuda()\n\t\tx_left_mask=Variable(torch.FloatTensor(x_left_mask)).cuda()\n\t\ty_left_mask=Variable(torch.FloatTensor(y_left_mask)).cuda()\n\t\tx_right_mask=Variable(torch.FloatTensor(x_right_mask)).cuda()\n\t\ty_right_mask=Variable(torch.FloatTensor(y_right_mask)).cuda()\n\t\tl=Variable(torch.LongTensor(l)).cuda()\n\telse:\n\t\tx_seq = Variable(torch.LongTensor(x_seq))\n\t\ty_seq = Variable(torch.LongTensor(y_seq))\n\t\tx_mask = Variable(torch.FloatTensor(x_mask))\n\t\ty_mask = Variable(torch.FloatTensor(y_mask))\n\t\tx_left_mask = Variable(torch.FloatTensor(x_left_mask))\n\t\ty_left_mask = Variable(torch.FloatTensor(y_left_mask))\n\t\tx_right_mask = Variable(torch.FloatTensor(x_right_mask))\n\t\ty_right_mask = Variable(torch.FloatTensor(y_right_mask))\n\t\tl = Variable(torch.LongTensor(l))\n\n\tx = (x_seq, x_mask, x_left_mask, x_right_mask)\n\ty = (y_seq, y_mask, y_left_mask, y_right_mask)\n\n\treturn x, y, l\n\nprint(\'task: trecqa\')\nprint(\'model: Tree_IM\')\nif torch.cuda.is_available():\n\tprint(\'CUDA is available!\')\n\tbase_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord\'\n\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\nelse:\n\tbase_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord\'\n\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\ndatasets = [base_path+\'/data/trecqa/train/a.btree\',\n            base_path+\'/data/trecqa/train/b.btree\',\n            base_path+\'/data/trecqa/train/sim.txt\']\nvalid_datasets = [base_path+\'/data/trecqa/dev/a.btree\',\n            base_path+\'/data/trecqa/dev/b.btree\',\n            base_path+\'/data/trecqa/dev/sim.txt\']\ntest_datasets = [base_path+\'/data/trecqa/test/a.btree\',\n                 base_path+\'/data/trecqa/test/b.btree\',\n                 base_path+\'/data/trecqa/test/sim.txt\']\ndictionary = base_path+\'/data/trecqa/trecqa_vocab_cased.pkl\'\n\nmaxlen=200\nbatch_size=32\nmax_epochs=1000\ndim_word=300\nwith open(dictionary, \'rb\') as f:\n\tworddicts = pkl.load(f)\nn_words=len(worddicts)\nwv_dict, wv_arr, wv_size = load_word_vectors(embedding_path, \'glove.840B\', dim_word)\npretrained_emb=norm_weight(n_words, dim_word)\nfor word in worddicts.keys():\n\ttry:\n\t\tpretrained_emb[worddicts[word]]=wv_arr[wv_dict[word]].numpy()\n\texcept:\n\t\tpretrained_emb[worddicts[word]] = torch.normal(torch.zeros(dim_word),std=1).numpy()\nprint(\'load data...\')\ntrain = TextIterator(datasets[0], datasets[1], datasets[2],\n\t\t\t\t\t\t dictionary,\n\t\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t\t batch_size=batch_size,\n\t\t\t\t\t\t maxlen=maxlen, shuffle=True)\nvalid = TextIterator(valid_datasets[0], valid_datasets[1], valid_datasets[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=batch_size,\n\t\t\t\t\t shuffle=False)\ntest = TextIterator(test_datasets[0], test_datasets[1], test_datasets[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=batch_size,\n\t\t\t\t\t shuffle=False)\ncriterion = torch.nn.CrossEntropyLoss()\nmodel = ESIM(dim_word, 2, n_words, dim_word, pretrained_emb)\nif torch.cuda.is_available():\n\tmodel = model.cuda()\n\tcriterion = criterion.cuda()\noptimizer = torch.optim.Adam(model.parameters(),lr=0.0004)\nprint(\'start training...\')\nclip_c=10\nmax_result=0\nreport_interval=1000\nfor epoch in xrange(max_epochs):\n\tmodel.train()\n\tprint(\'--\' * 20)\n\tstart_time = time.time()\n\tbatch_counter=0\n\ttrain_batch_i = 0\n\ttrain_sents_scaned = 0\n\ttrain_num_correct = 0\n\taccumulated_loss=0\n\tfor x1, x2, y in train:\n\t\t#print(x1[0][0])\n\t\t#for item in x1[0][0]:\n\t\t#\tprint(worddicts.keys()[item])\n\t\tx1, x2, y = prepare_data(x1, x2, y)\n\t\ttrain_sents_scaned += len(y)\n\t\toptimizer.zero_grad()\n\t\t#if x1[0].size(0)>100:\n\t\t#\tprint(x1[0].size(0))\n\t\t#continue\n\t\toutput = model(x1[0], x1[1], x1[2], x1[3], x2[0], x2[1], x2[2], x2[3])\n\t\tresult = output.data.cpu().numpy()\n\t\ta = np.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\ttrain_num_correct += np.sum(a == b)\n\t\tloss = criterion(output, y)\n\t\tloss.backward()\n\t\tgrad_norm = 0.\n\n\t\tfor m in list(model.parameters()):\n\t\t\tgrad_norm+=m.grad.data.norm() ** 2\n\n\t\tfor m in list(model.parameters()):\n\t\t\tif grad_norm>clip_c**2:\n\t\t\t\ttry:\n\t\t\t\t\tm.grad.data= m.grad.data / torch.sqrt(grad_norm) * clip_c\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\n\t\toptimizer.step()\n\t\taccumulated_loss += loss.data[0]\n\t\tbatch_counter += 1\n\t\tdel x1,x2,y\n\t\tif batch_counter % report_interval == 0:\n\t\t\tgc.collect()\n\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\tmsg += \'\\t training batch loss: %f\' % (accumulated_loss / train_sents_scaned)\n\t\t\tmsg += \'\\t train accuracy: %f\' % (train_num_correct / train_sents_scaned)\n\t\t\tprint(msg)\n\t# test after each epoch\n\tmodel.eval()\n\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\taccumulated_loss = 0\n\tdev_num_correct = 0\n\tn_done = 0\n\tpred = []\n\tpred_prob = []\n\tgold = []\n\tfor dev_x1, dev_x2, dev_y in valid:\n\t\tx1, x2, y = prepare_data(dev_x1, dev_x2, dev_y)\n\t\twith torch.no_grad():\n\t\t\toutput = F.softmax(model(x1[0], x1[1], x1[2], x1[3], x2[0], x2[1], x2[2], x2[3]))\n\t\tn_done += len(y)\n\t\tresult = output.data.cpu().numpy()\n\t\tloss = criterion(output, y)\n\t\taccumulated_loss += loss.data[0]\n\t\ta = numpy.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\tdev_num_correct += numpy.sum(a == b)\n\t\tpred.extend(result)\n\t\tpred_prob.extend(result[:, 1])\n\t\tb = y.data.cpu().numpy()\n\t\tgold.extend(b)\n\tmsg += \'\\t dev loss: %f\' % (accumulated_loss / n_done)\n\tdev_acc = dev_num_correct / n_done\n\tmsg += \'\\t dev accuracy: %f\' % dev_acc\n\tprint(msg)\n\tURL_maxF1_eval(pred_prob, gold)\n\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\taccumulated_loss = 0\n\tdev_num_correct = 0\n\tn_done = 0\n\tpred=[]\n\tpred_prob = []\n\tgold = []\n\tfor dev_x1, dev_x2, dev_y in test:\n\t\tx1, x2, y = prepare_data(dev_x1, dev_x2, dev_y)\n\t\twith torch.no_grad():\n\t\t\toutput = F.softmax(model(x1[0], x1[1], x1[2], x1[3], x2[0], x2[1], x2[2], x2[3]))\n\t\tn_done += len(y)\n\t\tresult = output.data.cpu().numpy()\n\t\tloss = criterion(output, y)\n\t\taccumulated_loss += loss.data[0]\n\t\ta = numpy.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\tdev_num_correct += numpy.sum(a == b)\n\t\tpred.extend(result)\n\t\tpred_prob.extend(result[:, 1])\n\t\tb = y.data.cpu().numpy()\n\t\tgold.extend(b)\n\tmsg += \'\\t test loss: %f\' % (accumulated_loss / n_done)\n\tdev_acc = dev_num_correct / n_done\n\tmsg += \'\\t test accuracy: %f\' % dev_acc\n\tprint(msg)\n\t_,maxF1=URL_maxF1_eval(pred_prob, gold)\n\tif maxF1>max_result:\n\t\tmax_result=maxF1\n\t\twith open(base_path+\'/trecqa_Tree_IM_prob.txt\',\'w\') as f:\n\t\t\tfor item in pred:\n\t\t\t\tf.writelines(str(item[0])+\'\\t\'+str(item[1])+\'\\n\')\n\t\tlist1 = []\n\t\tfor line in open(base_path + \'/data/trecqa/test.qrel\'):\n\t\t\tlist1.append(line.strip().split())\n\t\twith open(base_path + \'/trecqa_Tree_IM_for_eval.txt\', \'w\') as f:\n\t\t\tfor i in range(len(pred_prob)):\n\t\t\t\tf.writelines(str(list1[i][0]) + \'\\t\' + str(list1[i][1]) + \'\\t\' + str(\n\t\t\t\t\tlist1[i][2]) + \'\\t\' + \'*\\t\' + str(pred_prob[i]) + \'\\t\' + \'*\\n\')\n\telapsed_time = time.time() - start_time\n\tprint(\'Epoch \' + str(epoch) + \' finished within \' + str(timedelta(seconds=elapsed_time)))'"
ESIM/Tree_IM/main_url.py,26,"b'from __future__ import division\nimport sys\n\nimport gc\nimport numpy\nimport torch\nimport time\nfrom data_iterator import TextIterator\nimport cPickle as pkl\nfrom model_batch import *\nfrom os.path import expanduser\nfrom datetime import timedelta\nfrom torchtext.vocab import load_word_vectors\n\n# batch preparation\ndef prepare_data(group_x, group_y, labels):\n\tlengths_x = [len(s[0]) for s in group_x]\n\tlengths_y = [len(s[0]) for s in group_y]\n\n\tn_samples = len(group_x)\n\tmaxlen_x = numpy.max(lengths_x)\n\tmaxlen_y = numpy.max(lengths_y)\n\n\tx_seq = numpy.zeros((maxlen_x, n_samples)).astype(\'int64\')\n\ty_seq = numpy.zeros((maxlen_y, n_samples)).astype(\'int64\')\n\tx_mask = numpy.zeros((maxlen_x, n_samples)).astype(\'float32\')\n\ty_mask = numpy.zeros((maxlen_y, n_samples)).astype(\'float32\')\n\tx_left_mask = numpy.zeros((maxlen_x, n_samples, maxlen_x)).astype(\'float32\')\n\tx_right_mask = numpy.zeros((maxlen_x, n_samples, maxlen_x)).astype(\'float32\')\n\ty_left_mask = numpy.zeros((maxlen_y, n_samples, maxlen_y)).astype(\'float32\')\n\ty_right_mask = numpy.zeros((maxlen_y, n_samples, maxlen_y)).astype(\'float32\')\n\tl = numpy.zeros((n_samples,)).astype(\'int64\')\n\n\tfor idx, [s_x, s_y, ll] in enumerate(zip(group_x, group_y, labels)):\n\t\tx_seq[-lengths_x[idx]:, idx] = s_x[0]\n\t\tx_mask[-lengths_x[idx]:, idx] = 1.\n\t\tx_left_mask[-lengths_x[idx]:, idx, -lengths_x[idx]:] = s_x[1]\n\t\tx_right_mask[-lengths_x[idx]:, idx, -lengths_x[idx]:] = s_x[2]\n\t\ty_seq[-lengths_y[idx]:, idx] = s_y[0]\n\t\ty_mask[-lengths_y[idx]:, idx] = 1.\n\t\ty_left_mask[-lengths_y[idx]:, idx, -lengths_y[idx]:] = s_y[1]\n\t\ty_right_mask[-lengths_y[idx]:, idx, -lengths_y[idx]:] = s_y[2]\n\t\tl[idx] = ll\n\n\tif torch.cuda.is_available():\n\t\tx_seq=Variable(torch.LongTensor(x_seq)).cuda()\n\t\ty_seq = Variable(torch.LongTensor(y_seq)).cuda()\n\t\tx_mask = Variable(torch.FloatTensor(x_mask)).cuda()\n\t\ty_mask = Variable(torch.FloatTensor(y_mask)).cuda()\n\t\tx_left_mask=Variable(torch.FloatTensor(x_left_mask)).cuda()\n\t\ty_left_mask=Variable(torch.FloatTensor(y_left_mask)).cuda()\n\t\tx_right_mask=Variable(torch.FloatTensor(x_right_mask)).cuda()\n\t\ty_right_mask=Variable(torch.FloatTensor(y_right_mask)).cuda()\n\t\tl=Variable(torch.LongTensor(l)).cuda()\n\telse:\n\t\tx_seq = Variable(torch.LongTensor(x_seq))\n\t\ty_seq = Variable(torch.LongTensor(y_seq))\n\t\tx_mask = Variable(torch.FloatTensor(x_mask))\n\t\ty_mask = Variable(torch.FloatTensor(y_mask))\n\t\tx_left_mask = Variable(torch.FloatTensor(x_left_mask))\n\t\ty_left_mask = Variable(torch.FloatTensor(y_left_mask))\n\t\tx_right_mask = Variable(torch.FloatTensor(x_right_mask))\n\t\ty_right_mask = Variable(torch.FloatTensor(y_right_mask))\n\t\tl = Variable(torch.LongTensor(l))\n\n\tx = (x_seq, x_mask, x_left_mask, x_right_mask)\n\ty = (y_seq, y_mask, y_left_mask, y_right_mask)\n\n\treturn x, y, l\n\nprint(\'task: url\')\nprint(\'model: Tree_IM\')\nif torch.cuda.is_available():\n\tprint(\'CUDA is available!\')\n\tbase_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord\'\n\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\nelse:\n\tbase_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord\'\n\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\ndatasets = [base_path+\'/data/url/train/a.btree\',\n            base_path+\'/data/url/train/b.btree\',\n            base_path+\'/data/url/train/sim.txt\']\ntest_datasets = [base_path+\'/data/url/test_9324/a.btree\',\n                 base_path+\'/data/url/test_9324/b.btree\',\n                 base_path+\'/data/url/test_9324/sim.txt\']\ndictionary = base_path+\'/data/url/url_vocab_cased.pkl\'\n\nmaxlen=200\nbatch_size=32\nmax_epochs=1000\ndim_word=300\nwith open(dictionary, \'rb\') as f:\n\tworddicts = pkl.load(f)\nn_words=len(worddicts)\nwv_dict, wv_arr, wv_size = load_word_vectors(embedding_path, \'glove.840B\', dim_word)\npretrained_emb=norm_weight(n_words, dim_word)\nfor word in worddicts.keys():\n\ttry:\n\t\tpretrained_emb[worddicts[word]]=wv_arr[wv_dict[word]].numpy()\n\texcept:\n\t\tpretrained_emb[worddicts[word]] = torch.normal(torch.zeros(dim_word),std=1).numpy()\nprint(\'load data...\')\ntrain = TextIterator(datasets[0], datasets[1], datasets[2],\n\t\t\t\t\t\t dictionary,\n\t\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t\t batch_size=batch_size,\n\t\t\t\t\t\t maxlen=maxlen, shuffle=True)\ntest = TextIterator(test_datasets[0], test_datasets[1], test_datasets[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=batch_size,\n\t\t\t\t\t shuffle=False)\ncriterion = torch.nn.CrossEntropyLoss()\nmodel = ESIM(dim_word, 2, n_words, dim_word, pretrained_emb)\nif torch.cuda.is_available():\n\tmodel = model.cuda()\n\tcriterion = criterion.cuda()\noptimizer = torch.optim.Adam(model.parameters(),lr=0.0004)\nprint(\'start training...\')\nclip_c=10\nmax_result=0\nreport_interval=1000\nfor epoch in xrange(max_epochs):\n\tmodel.train()\n\tprint(\'--\' * 20)\n\tstart_time = time.time()\n\tbatch_counter=0\n\ttrain_batch_i = 0\n\ttrain_sents_scaned = 0\n\ttrain_num_correct = 0\n\taccumulated_loss=0\n\tfor x1, x2, y in train:\n\t\t#print(x1[0][0])\n\t\t#for item in x1[0][0]:\n\t\t#\tprint(worddicts.keys()[item])\n\t\tx1, x2, y = prepare_data(x1, x2, y)\n\t\ttrain_sents_scaned += len(y)\n\t\toptimizer.zero_grad()\n\t\t#if x1[0].size(0)>100:\n\t\t#\tprint(x1[0].size(0))\n\t\t#continue\n\t\toutput = model(x1[0], x1[1], x1[2], x1[3], x2[0], x2[1], x2[2], x2[3])\n\t\tresult = output.data.cpu().numpy()\n\t\ta = np.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\ttrain_num_correct += np.sum(a == b)\n\t\tloss = criterion(output, y)\n\t\tloss.backward()\n\t\tgrad_norm = 0.\n\n\t\tfor m in list(model.parameters()):\n\t\t\tgrad_norm+=m.grad.data.norm() ** 2\n\n\t\tfor m in list(model.parameters()):\n\t\t\tif grad_norm>clip_c**2:\n\t\t\t\ttry:\n\t\t\t\t\tm.grad.data= m.grad.data / torch.sqrt(grad_norm) * clip_c\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\n\t\toptimizer.step()\n\t\taccumulated_loss += loss.data[0]\n\t\tbatch_counter += 1\n\t\tdel x1,x2,y\n\t\tif batch_counter % report_interval == 0:\n\t\t\tgc.collect()\n\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\tmsg += \'\\t training batch loss: %f\' % (accumulated_loss / train_sents_scaned)\n\t\t\tmsg += \'\\t train accuracy: %f\' % (train_num_correct / train_sents_scaned)\n\t\t\tprint(msg)\n\t# test after each epoch\n\tmodel.eval()\n\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\taccumulated_loss = 0\n\tdev_num_correct = 0\n\tn_done = 0\n\tpred=[]\n\tpred_prob=[]\n\tgold=[]\n\tfor dev_x1, dev_x2, dev_y in test:\n\t\tx1, x2, y = prepare_data(dev_x1, dev_x2, dev_y)\n\t\twith torch.no_grad():\n\t\t\toutput = F.softmax(model(x1[0], x1[1], x1[2], x1[3], x2[0], x2[1], x2[2], x2[3]))\n\t\tn_done += len(y)\n\t\tresult = output.data.cpu().numpy()\n\t\tloss = criterion(output, y)\n\t\taccumulated_loss += loss.data[0]\n\t\ta = numpy.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\tdev_num_correct += numpy.sum(a == b)\n\t\tpred.extend(result)\n\t\tpred_prob.extend(result[:, 1])\n\t\tb = y.data.cpu().numpy()\n\t\tgold.extend(b)\n\tmsg += \'\\t test loss: %f\' % (accumulated_loss / n_done)\n\tdev_acc = dev_num_correct / n_done\n\tmsg += \'\\t test accuracy: %f\' % dev_acc\n\tprint(msg)\n\t_,maxF1=URL_maxF1_eval(pred_prob, gold)\n\tif maxF1>max_result:\n\t\tmax_result=maxF1\n\t\twith open(base_path+\'/url_Tree_IM_prob.txt\',\'w\') as f:\n\t\t\tfor item in pred:\n\t\t\t\tf.writelines(str(item[0])+\'\\t\'+str(item[1])+\'\\n\')\n\telapsed_time = time.time() - start_time\n\tprint(\'Epoch \' + str(epoch) + \' finished within \' + str(timedelta(seconds=elapsed_time)))'"
ESIM/Tree_IM/main_wikiqa.py,26,"b'from __future__ import division\nimport sys\n\nimport gc\nimport numpy\nimport torch\nimport time\nfrom data_iterator import TextIterator\nimport cPickle as pkl\nfrom model_batch import *\nfrom os.path import expanduser\nfrom datetime import timedelta\nfrom torchtext.vocab import load_word_vectors\n\n# batch preparation\ndef prepare_data(group_x, group_y, labels):\n\tlengths_x = [len(s[0]) for s in group_x]\n\tlengths_y = [len(s[0]) for s in group_y]\n\n\tn_samples = len(group_x)\n\tmaxlen_x = numpy.max(lengths_x)\n\tmaxlen_y = numpy.max(lengths_y)\n\n\tx_seq = numpy.zeros((maxlen_x, n_samples)).astype(\'int64\')\n\ty_seq = numpy.zeros((maxlen_y, n_samples)).astype(\'int64\')\n\tx_mask = numpy.zeros((maxlen_x, n_samples)).astype(\'float32\')\n\ty_mask = numpy.zeros((maxlen_y, n_samples)).astype(\'float32\')\n\tx_left_mask = numpy.zeros((maxlen_x, n_samples, maxlen_x)).astype(\'float32\')\n\tx_right_mask = numpy.zeros((maxlen_x, n_samples, maxlen_x)).astype(\'float32\')\n\ty_left_mask = numpy.zeros((maxlen_y, n_samples, maxlen_y)).astype(\'float32\')\n\ty_right_mask = numpy.zeros((maxlen_y, n_samples, maxlen_y)).astype(\'float32\')\n\tl = numpy.zeros((n_samples,)).astype(\'int64\')\n\n\tfor idx, [s_x, s_y, ll] in enumerate(zip(group_x, group_y, labels)):\n\t\tx_seq[-lengths_x[idx]:, idx] = s_x[0]\n\t\tx_mask[-lengths_x[idx]:, idx] = 1.\n\t\tx_left_mask[-lengths_x[idx]:, idx, -lengths_x[idx]:] = s_x[1]\n\t\tx_right_mask[-lengths_x[idx]:, idx, -lengths_x[idx]:] = s_x[2]\n\t\ty_seq[-lengths_y[idx]:, idx] = s_y[0]\n\t\ty_mask[-lengths_y[idx]:, idx] = 1.\n\t\ty_left_mask[-lengths_y[idx]:, idx, -lengths_y[idx]:] = s_y[1]\n\t\ty_right_mask[-lengths_y[idx]:, idx, -lengths_y[idx]:] = s_y[2]\n\t\tl[idx] = ll\n\n\tif torch.cuda.is_available():\n\t\tx_seq=Variable(torch.LongTensor(x_seq)).cuda()\n\t\ty_seq = Variable(torch.LongTensor(y_seq)).cuda()\n\t\tx_mask = Variable(torch.FloatTensor(x_mask)).cuda()\n\t\ty_mask = Variable(torch.FloatTensor(y_mask)).cuda()\n\t\tx_left_mask=Variable(torch.FloatTensor(x_left_mask)).cuda()\n\t\ty_left_mask=Variable(torch.FloatTensor(y_left_mask)).cuda()\n\t\tx_right_mask=Variable(torch.FloatTensor(x_right_mask)).cuda()\n\t\ty_right_mask=Variable(torch.FloatTensor(y_right_mask)).cuda()\n\t\tl=Variable(torch.LongTensor(l)).cuda()\n\telse:\n\t\tx_seq = Variable(torch.LongTensor(x_seq))\n\t\ty_seq = Variable(torch.LongTensor(y_seq))\n\t\tx_mask = Variable(torch.FloatTensor(x_mask))\n\t\ty_mask = Variable(torch.FloatTensor(y_mask))\n\t\tx_left_mask = Variable(torch.FloatTensor(x_left_mask))\n\t\ty_left_mask = Variable(torch.FloatTensor(y_left_mask))\n\t\tx_right_mask = Variable(torch.FloatTensor(x_right_mask))\n\t\ty_right_mask = Variable(torch.FloatTensor(y_right_mask))\n\t\tl = Variable(torch.LongTensor(l))\n\n\tx = (x_seq, x_mask, x_left_mask, x_right_mask)\n\ty = (y_seq, y_mask, y_left_mask, y_right_mask)\n\n\treturn x, y, l\n\nprint(\'task: wikiqa\')\nprint(\'model: Tree_IM\')\nif torch.cuda.is_available():\n\tprint(\'CUDA is available!\')\n\tbase_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/wikiqa\'\n\tembedding_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\tcastorini_path = expanduser(""~"") + \'/pytorch/DeepPairWiseWord/data/castorini/trec_eval.9.0/trec_eval\'\nelse:\n\tbase_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/wikiqa\'\n\tembedding_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/VDPWI-NN-Torch/data/glove\'\n\tcastorini_path = expanduser(""~"") + \'/Documents/research/pytorch/DeepPairWiseWord/data/castorini/trec_eval.9.0/trec_eval\'\n\ndatasets = [base_path+\'/train/a.btree\',\n            base_path+\'/train/b.btree\',\n            base_path+\'/train/sim.txt\']\nvalid_datasets = [base_path+\'/dev/a.btree\',\n            base_path+\'/dev/b.btree\',\n            base_path+\'/dev/sim.txt\']\ntest_datasets = [base_path+\'/test/a.btree\',\n                 base_path+\'/test/b.btree\',\n                 base_path+\'/test/sim.txt\']\ndictionary = base_path+\'/wikiqa_vocab_cased.pkl\'\n\nmaxlen=200\nbatch_size=32\nmax_epochs=1000\ndim_word=300\nwith open(dictionary, \'rb\') as f:\n\tworddicts = pkl.load(f)\nn_words=len(worddicts)\nwv_dict, wv_arr, wv_size = load_word_vectors(embedding_path, \'glove.840B\', dim_word)\npretrained_emb=norm_weight(n_words, dim_word)\nfor word in worddicts.keys():\n\ttry:\n\t\tpretrained_emb[worddicts[word]]=wv_arr[wv_dict[word]].numpy()\n\texcept:\n\t\tpretrained_emb[worddicts[word]] = torch.normal(torch.zeros(dim_word),std=1).numpy()\nprint(\'load data...\')\ntrain = TextIterator(datasets[0], datasets[1], datasets[2],\n\t\t\t\t\t\t dictionary,\n\t\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t\t batch_size=batch_size,\n\t\t\t\t\t\t maxlen=maxlen, shuffle=True)\nvalid = TextIterator(valid_datasets[0], valid_datasets[1], valid_datasets[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=batch_size,\n\t\t\t\t\t shuffle=False)\ntest = TextIterator(test_datasets[0], test_datasets[1], test_datasets[2],\n\t\t\t\t\t dictionary,\n\t\t\t\t\t n_words=n_words,\n\t\t\t\t\t batch_size=batch_size,\n\t\t\t\t\t shuffle=False)\ncriterion = torch.nn.CrossEntropyLoss()\nmodel = ESIM(dim_word, 2, n_words, dim_word, pretrained_emb)\nif torch.cuda.is_available():\n\tmodel = model.cuda()\n\tcriterion = criterion.cuda()\noptimizer = torch.optim.Adam(model.parameters(),lr=0.0004)\nprint(\'start training...\')\nclip_c=10\nmax_result=0\nreport_interval=1000\nfor epoch in xrange(max_epochs):\n\tmodel.train()\n\tprint(\'--\' * 20)\n\tstart_time = time.time()\n\tbatch_counter=0\n\ttrain_batch_i = 0\n\ttrain_sents_scaned = 0\n\ttrain_num_correct = 0\n\taccumulated_loss=0\n\tfor x1, x2, y in train:\n\t\t#print(x1[0][0])\n\t\t#for item in x1[0][0]:\n\t\t#\tprint(worddicts.keys()[item])\n\t\tx1, x2, y = prepare_data(x1, x2, y)\n\t\ttrain_sents_scaned += len(y)\n\t\toptimizer.zero_grad()\n\t\t#if x1[0].size(0)>100:\n\t\t#\tprint(x1[0].size(0))\n\t\t#continue\n\t\toutput = model(x1[0], x1[1], x1[2], x1[3], x2[0], x2[1], x2[2], x2[3])\n\t\tresult = output.data.cpu().numpy()\n\t\ta = np.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\ttrain_num_correct += np.sum(a == b)\n\t\tloss = criterion(output, y)\n\t\tloss.backward()\n\t\tgrad_norm = 0.\n\n\t\tfor m in list(model.parameters()):\n\t\t\tgrad_norm+=m.grad.data.norm() ** 2\n\n\t\tfor m in list(model.parameters()):\n\t\t\tif grad_norm>clip_c**2:\n\t\t\t\ttry:\n\t\t\t\t\tm.grad.data= m.grad.data / torch.sqrt(grad_norm) * clip_c\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\n\t\toptimizer.step()\n\t\taccumulated_loss += loss.data[0]\n\t\tbatch_counter += 1\n\t\tdel x1,x2,y\n\t\tif batch_counter % report_interval == 0:\n\t\t\tgc.collect()\n\t\t\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\t\t\tmsg += \'\\t training batch loss: %f\' % (accumulated_loss / train_sents_scaned)\n\t\t\tmsg += \'\\t train accuracy: %f\' % (train_num_correct / train_sents_scaned)\n\t\t\tprint(msg)\n\t# test after each epoch\n\tmodel.eval()\n\tmsg = \'%d completed epochs, %d batches\' % (epoch, batch_counter)\n\taccumulated_loss = 0\n\tdev_num_correct = 0\n\tn_done = 0\n\tpred=[]\n\tpred_prob = []\n\tgold = []\n\tfor dev_x1, dev_x2, dev_y in test:\n\t\tx1, x2, y = prepare_data(dev_x1, dev_x2, dev_y)\n\t\twith torch.no_grad():\n\t\t\toutput = F.softmax(model(x1[0], x1[1], x1[2], x1[3], x2[0], x2[1], x2[2], x2[3]))\n\t\tn_done += len(y)\n\t\tresult = output.data.cpu().numpy()\n\t\tloss = criterion(output, y)\n\t\taccumulated_loss += loss.data[0]\n\t\ta = numpy.argmax(result, axis=1)\n\t\tb = y.data.cpu().numpy()\n\t\tdev_num_correct += numpy.sum(a == b)\n\t\tpred.extend(result)\n\t\tpred_prob.extend(result[:, 1])\n\t\tb = y.data.cpu().numpy()\n\t\tgold.extend(b)\n\tmsg += \'\\t test loss: %f\' % (accumulated_loss / n_done)\n\tdev_acc = dev_num_correct / n_done\n\tmsg += \'\\t test accuracy: %f\' % dev_acc\n\tprint(msg)\n\t_,maxF1=URL_maxF1_eval(pred_prob, gold)\n\n\tlist1 = []\n\tfor line in open(base_path + \'/test.qrel\'):\n\t\tlist1.append(line.strip().split())\n\twith open(base_path + \'/wikiqa_Tree_IM_for_eval_tmp.txt\', \'w\') as f:\n\t\tfor i in range(len(pred_prob)):\n\t\t\tf.writelines(str(list1[i][0]) + \'\\t\' + str(list1[i][1]) + \'\\t\' + str(\n\t\t\t\tlist1[i][2]) + \'\\t\' + \'*\\t\' + str(pred_prob[i]) + \'\\t\' + \'*\\n\')\n\tcmd = (\'%s -m map %s %s\' % (castorini_path, base_path + \'/test.qrel\', base_path + \'/wikiqa_Tree_IM_for_eval_tmp.txt\'))\n\tos.system(cmd)\n\tcmd = (\'%s -m recip_rank %s %s\' % (castorini_path, base_path + \'/test.qrel\', base_path + \'/wikiqa_Tree_IM_for_eval_tmp.txt\'))\n\tos.system(cmd)\n\n\tif maxF1>max_result:\n\t\tmax_result=maxF1\n\t\twith open(base_path+\'/wikiqa_Tree_IM_prob.txt\',\'w\') as f:\n\t\t\tfor item in pred:\n\t\t\t\tf.writelines(str(item[0])+\'\\t\'+str(item[1])+\'\\n\')\n\t\tlist1 = []\n\t\tfor line in open(base_path + \'/test.qrel\'):\n\t\t\tlist1.append(line.strip().split())\n\t\twith open(base_path + \'/wikiqa_Tree_IM_for_eval.txt\', \'w\') as f:\n\t\t\tfor i in range(len(pred_prob)):\n\t\t\t\tf.writelines(str(list1[i][0]) + \'\\t\' + str(list1[i][1]) + \'\\t\' + str(\n\t\t\t\t\tlist1[i][2]) + \'\\t\' + \'*\\t\' + str(pred_prob[i]) + \'\\t\' + \'*\\n\')\n\telapsed_time = time.time() - start_time\n\tprint(\'Epoch \' + str(epoch) + \' finished within \' + str(timedelta(seconds=elapsed_time)))'"
ESIM/Tree_IM/model.py,38,"b'import sys\nimport math\nimport torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom datetime import datetime\nimport gc\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\n\ndef ortho_weight(ndim):\n\t""""""\n\tRandom orthogonal weights\n\tUsed by norm_weights(below), in which case, we\n\tare ensuring that the rows are orthogonal\n\t(i.e W = U \\Sigma V, U has the same\n\t# of rows, V has the same # of cols)\n\t""""""\n\tW = np.random.randn(ndim, ndim)\n\tu, s, v = np.linalg.svd(W)\n\treturn u.astype(\'float32\')\n\n# mapping from scalar to vector\ndef map_label_to_target(label,num_classes):\n\ttarget = torch.Tensor(1,num_classes)\n\tceil = int(math.ceil(label))\n\tfloor = int(math.floor(label))\n\tif ceil==floor:\n\t\ttarget[0][floor-1] = 1\n\telse:\n\t\ttarget[0][floor-1] = ceil - label\n\t\ttarget[0][ceil-1] = label - floor\n\treturn target\n\ndef map_label_to_target_sentiment(label, num_classes = 0 ,fine_grain = False):\n\t# num_classes not use yet\n\ttarget = torch.LongTensor(1)\n\ttarget[0] = int(label) # nothing to do here as we preprocess data\n\treturn target\n\nclass BinaryTreeLeafModule(nn.Module):\n\n\tdef __init__(self, cuda, in_dim, mem_dim):\n\t\tsuper(BinaryTreeLeafModule, self).__init__()\n\t\tself.cudaFlag = cuda\n\t\tself.in_dim = in_dim\n\t\tself.mem_dim = mem_dim\n\n\t\tself.cx = nn.Linear(self.in_dim, self.mem_dim)\n\t\tself.ox = nn.Linear(self.in_dim, self.mem_dim)\n\t\tif self.cudaFlag:\n\t\t\tself.cx = self.cx.cuda()\n\t\t\tself.ox = self.ox.cuda()\n\n\tdef forward(self, input):\n\t\tc = self.cx(input)\n\t\to = F.sigmoid(self.ox(input))\n\t\th = o * F.tanh(c)\n\t\treturn c, h\n\nclass BinaryTreeComposer(nn.Module):\n\n\tdef __init__(self, cuda, in_dim, mem_dim):\n\t\tsuper(BinaryTreeComposer, self).__init__()\n\t\tself.cudaFlag = cuda\n\t\tself.in_dim = in_dim\n\t\tself.mem_dim = mem_dim\n\n\t\tdef new_gate():\n\t\t\tlh = nn.Linear(self.mem_dim, self.mem_dim, bias=False)\n\t\t\trh = nn.Linear(self.mem_dim, self.mem_dim, bias=False)\n\t\t\tlh.weight.data.copy_(torch.from_numpy(ortho_weight(self.mem_dim)))\n\t\t\trh.weight.data.copy_(torch.from_numpy(ortho_weight(self.mem_dim)))\n\t\t\treturn lh, rh\n\n\t\tdef new_W():\n\t\t\tw = nn.Linear(self.in_dim, self.mem_dim)\n\t\t\tw.weight.data.copy_(torch.from_numpy(ortho_weight(self.mem_dim)))\n\t\t\treturn w\n\n\t\tself.ilh, self.irh = new_gate()\n\t\tself.lflh, self.lfrh = new_gate()\n\t\tself.rflh, self.rfrh = new_gate()\n\t\tself.ulh, self.urh = new_gate()\n\t\tself.olh, self.orh = new_gate()\n\n\t\tself.cx = new_W()\n\t\tself.ox = new_W()\n\t\tself.fx = new_W()\n\t\tself.ix = new_W()\n\n\t\tif self.cudaFlag:\n\t\t\tself.ilh = self.ilh.cuda()\n\t\t\tself.irh = self.irh.cuda()\n\t\t\tself.lflh = self.lflh.cuda()\n\t\t\tself.lfrh = self.lfrh.cuda()\n\t\t\tself.rflh = self.rflh.cuda()\n\t\t\tself.rfrh = self.rfrh.cuda()\n\t\t\tself.ulh = self.ulh.cuda()\n\t\t\tself.urh = self.urh.cuda()\n\t\t\tself.olh = self.olh.cuda()\n\t\t\tself.orh = self.orh.cuda()\n\n\tdef forward(self, input, lc, lh , rc, rh):\n\t\tu = F.tanh(self.cx(input) + self.ulh(lh) + self.urh(rh))\n\t\ti = F.sigmoid(self.ix(input) + self.ilh(lh) + self.irh(rh))\n\t\tlf = F.sigmoid(self.fx(input) + self.lflh(lh) + self.lfrh(rh))\n\t\trf = F.sigmoid(self.fx(input) + self.rflh(lh) + self.rfrh(rh))\n\t\tc =  i* u + lf*lc + rf*rc\n\t\to = F.sigmoid(self.ox(input) + self.olh(lh) + self.orh(rh))\n\t\th = o * F.tanh(c)\n\t\treturn c, h\n\nclass BinaryTreeLSTM(nn.Module):\n\tdef __init__(self, cuda, in_dim, mem_dim, word_embedding, num_words):\n\t\tsuper(BinaryTreeLSTM, self).__init__()\n\t\tself.cudaFlag = cuda\n\t\tself.in_dim = in_dim\n\t\tself.mem_dim = mem_dim\n\t\tself.word_embedding=word_embedding\n\t\tself.num_words=num_words\n\n\t\t#self.leaf_module = BinaryTreeLeafModule(cuda,in_dim, mem_dim)\n\t\tself.composer = BinaryTreeComposer(cuda, in_dim, mem_dim)\n\t\tself.output_module = None\n\t\tself.all_ststes=[]\n\t\tself.all_words=[]\n\n\tdef set_output_module(self, output_module):\n\t\tself.output_module = output_module\n\n\tdef getParameters(self):\n\t\t""""""\n\t\tGet flatParameters\n\t\tnote that getParameters and parameters is not equal in this case\n\t\tgetParameters do not get parameters of output module\n\t\t:return: 1d tensor\n\t\t""""""\n\t\tparams = []\n\t\tfor m in [self.ix, self.ih, self.fx, self.fh, self.ox, self.oh, self.ux, self.uh]:\n\t\t\t# we do not get param of output module\n\t\t\tl = list(m.parameters())\n\t\t\tparams.extend(l)\n\n\t\tone_dim = [p.view(p.numel()) for p in params]\n\t\tparams = F.torch.cat(one_dim)\n\t\treturn params\n\n\tdef forward(self, tree, embs, PAD):\n\n\t\tif tree.num_children == 0:\n\t\t\tlc = Variable(torch.zeros(1, self.mem_dim))\n\t\t\tlh = Variable(torch.zeros(1, self.mem_dim))\n\t\t\trc = Variable(torch.zeros(1, self.mem_dim))\n\t\t\trh = Variable(torch.zeros(1, self.mem_dim))\n\t\t\tif torch.cuda.is_available():\n\t\t\t\tlc = lc.cuda()\n\t\t\t\tlh = lh.cuda()\n\t\t\t\trc = rc.cuda()\n\t\t\t\trh = rh.cuda()\n\t\t\ttree.state = self.composer.forward(embs[tree.idx-1], lc, lh, rc, rh)\n\t\t\tself.all_ststes.append(tree.state[1].view(1, self.mem_dim))\n\t\t\t#self.all_words.append(embs[tree.idx-1])\n\t\telse:\n\t\t\tfor idx in xrange(tree.num_children):\n\t\t\t\t_ = self.forward(tree.children[idx], embs, PAD)\n\n\t\t\tlc, lh, rc, rh = self.get_child_state(tree)\n\t\t\tif PAD:\n\t\t\t\tindex = Variable(torch.LongTensor([self.num_words-1]))\n\t\t\t\tif torch.cuda.is_available():\n\t\t\t\t\tindex=index.cuda()\n\t\t\t\ttree.state = self.composer.forward(self.word_embedding(index),lc, lh, rc, rh)\n\t\t\telse:\n\t\t\t\ttree.state = self.composer.forward(embs[tree.idx - 1], lc, lh, rc, rh)\n\t\t\tself.all_ststes.append(tree.state[1].view(1,self.mem_dim))\n\t\t\t#self.all_words.append(self.word_embedding[self.num_words-1])\n\n\t\treturn tree.state#, loss\n\n\tdef get_child_state(self, tree):\n\t\tlc, lh = tree.children[0].state\n\t\trc, rh = tree.children[1].state\n\t\treturn lc, lh, rc, rh\n\nclass ESIM(nn.Module):\n\t""""""\n\t\tImplementation of the multi feed forward network model described in\n\t\tthe paper ""A Decomposable Attention Model for Natural Language\n\t\tInference"" by Parikh et al., 2016.\n\n\t\tIt applies feedforward MLPs to combinations of parts of the two sentences,\n\t\twithout any recurrent structure.\n\t""""""\n\tdef __init__(self, num_units, num_classes, vocab_size, embedding_size, pretrained_emb, num_words):\n\t\tsuper(ESIM, self).__init__()\n\t\tself.vocab_size=vocab_size\n\t\tself.num_units = num_units\n\t\tself.num_classes = num_classes\n\t\tself.embedding_size=embedding_size\n\t\tself.pretrained_emb=pretrained_emb\n\n\t\tself.dropout = nn.Dropout(p=0.5)\n\t\tself.word_embedding=nn.Embedding(vocab_size,embedding_size)\n\n\t\tself.tree_lstm_intra=BinaryTreeLSTM(torch.cuda.is_available(),embedding_size,num_units, self.word_embedding, num_words)\n\n\t\tself.linear_layer_compare = nn.Sequential(nn.Linear(4*num_units, num_units), nn.ReLU(), nn.Dropout(p=0.5))\n\t\t#                                          nn.Dropout(p=0.2), nn.Linear(num_units, num_units), nn.ReLU())\n\n\t\tself.tree_lstm_compare=BinaryTreeLSTM(torch.cuda.is_available(),embedding_size,num_units, self.word_embedding, num_words)\n\n\t\tself.linear_layer_aggregate = nn.Sequential(nn.Dropout(p=0.5), nn.Linear(4*num_units, num_units), nn.ReLU(),\n\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Dropout(p=0.5), nn.Linear(num_units, num_classes))\n\n\t\tself.init_weight()\n\n\tdef init_weight(self):\n\t\t#nn.init.normal(self.linear_layer_project,mean=0,std=0.1)\n\t\t#print(self.linear_layer_attend[3])\n\t\t#self.linear_layer_attend[1].weight.data.normal_(0, 0.01)\n\t\t#self.linear_layer_attend[1].bias.data.fill_(0)\n\t\t#self.linear_layer_attend[4].weight.data.normal_(0, 0.01)\n\t\t#self.linear_layer_attend[4].bias.data.fill_(0)\n\t\tself.linear_layer_compare[0].weight.data.normal_(0, 0.01)\n\t\tself.linear_layer_compare[0].bias.data.fill_(0)\n\t\t#self.linear_layer_compare[4].weight.data.normal_(0, 0.01)\n\t\t#self.linear_layer_compare[4].bias.data.fill_(0)\n\t\tself.linear_layer_aggregate[1].weight.data.normal_(0, 0.01)\n\t\tself.linear_layer_aggregate[1].bias.data.fill_(0)\n\t\tself.linear_layer_aggregate[4].weight.data.normal_(0, 0.01)\n\t\tself.linear_layer_aggregate[4].bias.data.fill_(0)\n\t\tself.word_embedding.weight.data.copy_(torch.from_numpy(self.pretrained_emb))\n\n\tdef attention_softmax3d(self,raw_attentions):\n\t\treshaped_attentions = raw_attentions.view(-1, raw_attentions.size(2))\n\t\tout=nn.functional.softmax(reshaped_attentions, dim=1)\n\t\treturn out.view(raw_attentions.size(0),raw_attentions.size(1),raw_attentions.size(2))\n\n\tdef _transformation_input(self,embed_sent, tree, PAD=True):\n\n\t\tembed_sent = self.word_embedding(embed_sent)\n\t\tembed_sent = self.dropout(embed_sent)\n\t\t#print(\'intra:\')\n\t\t#print(embed_sent)\n\t\t_=self.tree_lstm_intra(tree, embed_sent, PAD)\n\t\t#print(len(self.tree_lstm_intra.all_ststes))\n\t\toutput=torch.cat(self.tree_lstm_intra.all_ststes,0)\n\t\t#embed_sent=torch.cat(self.tree_lstm_intra.all_words,0)\n\t\tdel self.tree_lstm_intra.all_ststes[:]\n\t\t#del self.tree_lstm_intra.all_words[:]\n\t\t#gc.collect()\n\t\treturn output\n\n\tdef attend(self,sent1,sent2):\n\n\t\trepr2=torch.transpose(sent2,1,2)\n\t\tself.raw_attentions = torch.matmul(sent1, repr2)\n\t\tatt_sent1 = self.attention_softmax3d(self.raw_attentions)\n\t\tbeta = torch.matmul(att_sent1, sent2)\n\n\t\traw_attentions_t = torch.transpose(self.raw_attentions, 1, 2).contiguous()\n\t\tatt_sent2 = self.attention_softmax3d(raw_attentions_t)\n\t\talpha = torch.matmul(att_sent2, sent1)\n\n\t\treturn alpha, beta\n\n\tdef compare(self,sentence,soft_alignment, tree, PAD=False):\n\n\t\tsent_alignment=torch.cat([sentence, soft_alignment, sentence-soft_alignment, sentence * soft_alignment],2)\n\t\tsent_alignment = self.linear_layer_compare(sent_alignment)\n\t\tsent_alignment = self.dropout(sent_alignment)\n\n\t\t#print(\'compare:\')\n\t\t#print(sent_alignment)\n\t\tsent_alignment=sent_alignment[0]\n\t\t_=self.tree_lstm_compare(tree, sent_alignment,PAD)\n\t\toutput = torch.cat(self.tree_lstm_compare.all_ststes, 0)\n\t\tdel self.tree_lstm_compare.all_ststes[:]\n\t\t#gc.collect()\n\t\treturn output\n\n\tdef aggregate(self,v1, v2):\n\n\t\tv1_mean = torch.mean(v1, 1)\n\t\tv2_mean = torch.mean(v2, 1)\n\t\tv1_max, _ = torch.max(v1, 1)\n\t\tv2_max, _ = torch.max(v2, 1)\n\t\tout = self.linear_layer_aggregate(torch.cat((v1_mean, v1_max, v2_mean, v2_max), 1))\n\n\t\treturn out\n\n\tdef forward(self,sent1, sent2,tree1, tree2):\n\t\tsent1=self._transformation_input(sent1, tree1)\n\t\tsent2=self._transformation_input(sent2, tree2)\n\t\tsent1=torch.unsqueeze(sent1,0)\n\t\tsent2=torch.unsqueeze(sent2,0)\n\t\talpha, beta = self.attend(sent1, sent2)\n\t\tv1=self.compare(sent1,beta, tree1)\n\t\tv2=self.compare(sent2,alpha, tree2)\n\t\tv1 = torch.unsqueeze(v1, 0)\n\t\tv2 = torch.unsqueeze(v2, 0)\n\t\tlogits=self.aggregate(v1,v2)\n\t\treturn logits'"
ESIM/Tree_IM/model_batch.py,49,"b'import sys\nimport math\nimport torch\nimport numpy as np\nimport torch.nn as nn\nfrom util import *\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom datetime import datetime\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\ndef ortho_weight(ndim):\n\t""""""\n\tRandom orthogonal weights\n\n\tUsed by norm_weights(below), in which case, we\n\tare ensuring that the rows are orthogonal\n\t(i.e W = U \\Sigma V, U has the same\n\t# of rows, V has the same # of cols)\n\t""""""\n\tW = np.random.randn(ndim, ndim)\n\tu, s, v = np.linalg.svd(W)\n\treturn u.astype(\'float32\')\n\ndef norm_weight(nin, nout=None, scale=0.01, ortho=True):\n\t""""""\n\tRandom weights drawn from a Gaussian\n\t""""""\n\tif nout is None:\n\t\tnout = nin\n\tif nout == nin and ortho:\n\t\tW = ortho_weight(nin)\n\telse:\n\t\tW = scale * np.random.randn(nin, nout)\n\treturn W.astype(\'float32\')\n\ndef generate_mask_2(values, sent_sizes):\n\tmask_matrix = np.zeros((len(sent_sizes), max(sent_sizes), values.size(2)))\n\tfor i in range(len(sent_sizes)):\n\t\tmask_matrix[i][:sent_sizes[i]][:]=1\n\tif torch.cuda.is_available():\n\t\tmask_matrix = torch.Tensor(mask_matrix).cuda()\n\telse:\n\t\tmask_matrix = torch.Tensor(mask_matrix)\n\treturn values*Variable(mask_matrix)\n\ndef generate_mask(lsent_sizes, rsent_sizes):\n\tmask_matrix=np.zeros((len(lsent_sizes),max(lsent_sizes),max(rsent_sizes)))\n\tfor i in range(len(lsent_sizes)):\n\t\tmask_matrix[i][:lsent_sizes[i]][:rsent_sizes[i]]=1\n\tif torch.cuda.is_available():\n\t\tmask_matrix = torch.Tensor(mask_matrix).cuda()\n\telse:\n\t\tmask_matrix = torch.Tensor(mask_matrix)\n\treturn Variable(mask_matrix)\n\nclass BinaryTreeCell(nn.Module):\n\n\tdef __init__(self, cuda, in_dim, mem_dim):\n\t\tsuper(BinaryTreeCell, self).__init__()\n\t\tself.cudaFlag = cuda\n\t\tself.in_dim = in_dim\n\t\tself.mem_dim = mem_dim\n\n\t\tdef new_gate():\n\t\t\tlh = nn.Linear(self.mem_dim, self.mem_dim, bias=False)\n\t\t\trh = nn.Linear(self.mem_dim, self.mem_dim, bias=False)\n\t\t\tlh.weight.data.copy_(torch.from_numpy(ortho_weight(self.mem_dim)))\n\t\t\trh.weight.data.copy_(torch.from_numpy(ortho_weight(self.mem_dim)))\n\t\t\treturn lh, rh\n\n\t\tdef new_W():\n\t\t\tw = nn.Linear(self.in_dim, self.mem_dim)\n\t\t\tw.weight.data.copy_(torch.from_numpy(ortho_weight(self.mem_dim)))\n\t\t\treturn w\n\n\t\tself.ilh, self.irh = new_gate()\n\t\tself.lflh, self.lfrh = new_gate()\n\t\tself.rflh, self.rfrh = new_gate()\n\t\tself.ulh, self.urh = new_gate()\n\t\tself.olh, self.orh = new_gate()\n\n\t\tself.cx = new_W()\n\t\tself.ox = new_W()\n\t\tself.fx = new_W()\n\t\tself.ix = new_W()\n\n\n\tdef forward(self, input, lc, lh , rc, rh):\n\t\tu = F.tanh(self.cx(input) + self.ulh(lh) + self.urh(rh))\n\t\ti = F.sigmoid(self.ix(input) + self.ilh(lh) + self.irh(rh))\n\t\tlf = F.sigmoid(self.fx(input) + self.lflh(lh) + self.lfrh(rh))\n\t\trf = F.sigmoid(self.fx(input) + self.rflh(lh) + self.rfrh(rh))\n\t\tc =  i* u + lf*lc + rf*rc\n\t\to = F.sigmoid(self.ox(input) + self.olh(lh) + self.orh(rh))\n\t\th = o * F.tanh(c)\n\t\treturn c, h\n\nclass BinaryTreeLSTM(nn.Module):\n\tdef __init__(self, cuda, in_dim, mem_dim):\n\t\tsuper(BinaryTreeLSTM, self).__init__()\n\t\tself.cudaFlag = cuda\n\t\tself.in_dim = in_dim\n\t\tself.mem_dim = mem_dim\n\n\t\t#self.leaf_module = BinaryTreeLeafModule(cuda,in_dim, mem_dim)\n\t\tself.TreeCell = BinaryTreeCell(cuda, in_dim, mem_dim)\n\t\tself.output_module = None\n\t\tself.all_ststes=[]\n\t\tself.all_words=[]\n\n\tdef forward(self, x, x_mask, x_left_mask, x_right_mask):\n\t\t""""""\n\n\t\t:param x: #step x #sample x dim_emb\n\t\t:param x_mask: #step x #sample\n\t\t:param x_left_mask: #step x #sample x #step\n\t\t:param x_right_mask: #step x #sample x #step\n\t\t:return:\n\t\t""""""\n\t\th = Variable(torch.zeros(x.size(1), x.size(0), x.size(2)))\n\t\tc = Variable(torch.zeros(x.size(1), x.size(0), x.size(2)))\n\t\tif torch.cuda.is_available():\n\t\t\th=h.cuda()\n\t\t\tc=c.cuda()\n\t\tfor step in range(x.size(0)):\n\t\t\tinput=x[step] # #sample x dim_emb\n\t\t\tlh=torch.sum(x_left_mask[step][:,:,None]*h,1)\n\t\t\trh=torch.sum(x_right_mask[step][:,:,None]*h,1)\n\t\t\tlc=torch.sum(x_left_mask[step][:,:,None]*c,1)\n\t\t\trc=torch.sum(x_right_mask[step][:,:,None]*c,1)\n\t\t\tstep_c, step_h=self.TreeCell(input, lc, lh , rc, rh)\n\t\t\tif step==0:\n\t\t\t\tnew_h = torch.cat((torch.unsqueeze(step_h, 1), h[:,step + 1:, :]), 1)\n\t\t\t\tnew_c = torch.cat((torch.unsqueeze(step_c, 1), c[:,step + 1:, :]), 1)\n\t\t\telif step==(x.size(0)-1):\n\t\t\t\tnew_h = torch.cat((h[:, :step, :], torch.unsqueeze(step_h, 1)), 1)\n\t\t\t\tnew_c = torch.cat((c[:, :step, :], torch.unsqueeze(step_c, 1)), 1)\n\t\t\telse:\n\t\t\t\tnew_h=torch.cat((h[:,:step,:], torch.unsqueeze(step_h,1), h[:,step+1:,:]),1)\n\t\t\t\tnew_c=torch.cat((c[:,:step,:], torch.unsqueeze(step_c,1), c[:,step+1:,:]),1)\n\t\t\th=x_mask[step][:,None,None]*new_h + (1-x_mask[step][:,None,None])*h\n\t\t\tc=x_mask[step][:,None,None]*new_c + (1-x_mask[step][:,None,None])*c\n\t\treturn h\n\nclass ESIM(nn.Module):\n\t""""""\n\t\tImplementation of the multi feed forward network model described in\n\t\tthe paper ""A Decomposable Attention Model for Natural Language\n\t\tInference"" by Parikh et al., 2016.\n\n\t\tIt applies feedforward MLPs to combinations of parts of the two sentences,\n\t\twithout any recurrent structure.\n\t""""""\n\tdef __init__(self, num_units, num_classes, vocab_size, embedding_size, pretrained_emb,\n\t\t\t\t training=True, project_input=True,\n\t\t\t\t use_intra_attention=False, distance_biases=10, max_sentence_length=30):\n\t\t""""""\n\t\tCreate the model based on MLP networks.\n\n\t\t:param num_units: size of the networks\n\t\t:param num_classes: number of classes in the problem\n\t\t:param vocab_size: size of the vocabulary\n\t\t:param embedding_size: size of each word embedding\n\t\t:param use_intra_attention: whether to use intra-attention model\n\t\t:param training: whether to create training tensors (optimizer)\n\t\t:param project_input: whether to project input embeddings to a\n\t\t\tdifferent dimensionality\n\t\t:param distance_biases: number of different distances with biases used\n\t\t\tin the intra-attention model\n\t\t""""""\n\t\tsuper(ESIM, self).__init__()\n\t\tself.vocab_size=vocab_size\n\t\tself.num_units = num_units\n\t\tself.num_classes = num_classes\n\t\tself.project_input = project_input\n\t\tself.embedding_size=embedding_size\n\t\tself.distance_biases=distance_biases\n\t\tself.max_sentence_length=max_sentence_length\n\t\tself.pretrained_emb=pretrained_emb\n\n\t\tself.dropout = nn.Dropout(p=0.5)\n\t\tself.word_embedding=nn.Embedding(vocab_size,embedding_size)\n\n\t\tself.tree_lstm_intra=BinaryTreeLSTM(torch.cuda.is_available(),embedding_size, num_units)\n\n\t\t#self.lstm_intra = nn.LSTM(embedding_size, num_units, num_layers=1, batch_first=True)\n\t\t#self.lstm_intra_r = nn.LSTM(embedding_size, num_units, num_layers=1, batch_first=True)\n\n\t\tself.linear_layer_compare = nn.Sequential(nn.Linear(4*num_units, num_units), nn.ReLU(), nn.Dropout(p=0.5))\n\t\t#                                          nn.Dropout(p=0.2), nn.Linear(num_units, num_units), nn.ReLU())\n\n\t\t#self.lstm_compare = nn.LSTM(num_units, num_units, num_layers=1, batch_first=True)\n\t\t#self.lstm_compare_r = nn.LSTM(num_units, num_units, num_layers=1, batch_first=True)\n\t\tself.tree_lstm_compare=BinaryTreeLSTM(torch.cuda.is_available(), embedding_size, num_units)\n\n\t\tself.linear_layer_aggregate = nn.Sequential(nn.Dropout(p=0.5), nn.Linear(4*num_units, num_units), nn.ReLU(),\n\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Dropout(p=0.5), nn.Linear(num_units, num_classes))\n\n\t\tself.init_weight()\n\n\tdef ortho_weight(self):\n\t\t""""""\n\t\tRandom orthogonal weights\n\t\tUsed by norm_weights(below), in which case, we\n\t\tare ensuring that the rows are orthogonal\n\t\t(i.e W = U \\Sigma V, U has the same\n\t\t# of rows, V has the same # of cols)\n\t\t""""""\n\t\tndim=self.num_units\n\t\tW = np.random.randn(ndim, ndim)\n\t\tu, s, v = np.linalg.svd(W)\n\t\treturn u.astype(\'float32\')\n\n\tdef initialize_lstm(self):\n\t\tif torch.cuda.is_available():\n\t\t\tinit=torch.Tensor(np.concatenate([self.ortho_weight(),self.ortho_weight(),self.ortho_weight(),self.ortho_weight()], 0)).cuda()\n\t\telse:\n\t\t\tinit = torch.Tensor(\n\t\t\t\tnp.concatenate([self.ortho_weight(), self.ortho_weight(), self.ortho_weight(), self.ortho_weight()], 0))\n\t\treturn init\n\n\tdef init_weight(self):\n\t\t#nn.init.normal(self.linear_layer_project,mean=0,std=0.1)\n\t\t#print(self.linear_layer_attend[3])\n\t\t#self.linear_layer_attend[1].weight.data.normal_(0, 0.01)\n\t\t#self.linear_layer_attend[1].bias.data.fill_(0)\n\t\t#self.linear_layer_attend[4].weight.data.normal_(0, 0.01)\n\t\t#self.linear_layer_attend[4].bias.data.fill_(0)\n\t\tself.linear_layer_compare[0].weight.data.normal_(0, 0.01)\n\t\tself.linear_layer_compare[0].bias.data.fill_(0)\n\t\t#self.linear_layer_compare[4].weight.data.normal_(0, 0.01)\n\t\t#self.linear_layer_compare[4].bias.data.fill_(0)\n\t\tself.linear_layer_aggregate[1].weight.data.normal_(0, 0.01)\n\t\tself.linear_layer_aggregate[1].bias.data.fill_(0)\n\t\tself.linear_layer_aggregate[4].weight.data.normal_(0, 0.01)\n\t\tself.linear_layer_aggregate[4].bias.data.fill_(0)\n\t\tself.word_embedding.weight.data.copy_(torch.from_numpy(self.pretrained_emb))\n\n\tdef attention_softmax3d(self,raw_attentions):\n\t\treshaped_attentions = raw_attentions.view(-1, raw_attentions.size(2))\n\t\tout=nn.functional.softmax(reshaped_attentions, dim=1)\n\t\treturn out.view(raw_attentions.size(0),raw_attentions.size(1),raw_attentions.size(2))\n\n\tdef _transformation_input(self,embed_sent, x1_mask, x1_left_mask, x1_right_mask):\n\t\tembed_sent = self.word_embedding(embed_sent)\n\t\tembed_sent = self.dropout(embed_sent)\n\t\thidden=self.tree_lstm_intra(embed_sent, x1_mask, x1_left_mask, x1_right_mask)\n\t\treturn hidden\n\n\n\tdef aggregate(self,v1, v2):\n\t\t""""""\n\t\tAggregate the representations induced from both sentences and their\n\t\trepresentations\n\n\t\t:param v1: tensor with shape (batch, time_steps, num_units)\n\t\t:param v2: tensor with shape (batch, time_steps, num_units)\n\t\t:return: logits over classes, shape (batch, num_classes)\n\t\t""""""\n\t\tv1_mean = torch.mean(v1, 1)\n\t\tv2_mean = torch.mean(v2, 1)\n\t\tv1_max, _ = torch.max(v1, 1)\n\t\tv2_max, _ = torch.max(v2, 1)\n\t\tout = self.linear_layer_aggregate(torch.cat((v1_mean, v1_max, v2_mean, v2_max), 1))\n\n\t\t#v1_sum=torch.sum(v1,1)\n\t\t#v2_sum=torch.sum(v2,1)\n\t\t#out=self.linear_layer_aggregate(torch.cat([v1_sum,v2_sum],1))\n\n\t\treturn out\n\n\tdef forward(self, x1, x1_mask, x1_left_mask, x1_right_mask, x2, x2_mask, x2_left_mask, x2_right_mask):\n\t\tsent1=self._transformation_input(x1,x1_mask, x1_left_mask, x1_right_mask)\n\t\tsent2=self._transformation_input(x2,x2_mask, x2_left_mask, x2_right_mask)\n\n\t\tctx1=torch.transpose(sent1,0,1)\n\t\tctx2=torch.transpose(sent2,0,1)\n\t\t# ctx1: #step1 x #sample x #dimctx\n\t\t# ctx2: #step2 x #sample x #dimctx\n\t\tctx1 = ctx1 * x1_mask[:, :, None]\n\t\tctx2 = ctx2 * x2_mask[:, :, None]\n\n\t\t# weight_matrix: #sample x #step1 x #step2\n\t\tweight_matrix = torch.matmul(ctx1.permute(1, 0, 2), ctx2.permute(1, 2, 0))\n\t\tweight_matrix_1 = torch.exp(weight_matrix - weight_matrix.max(1, keepdim=True)[0]).permute(1, 2, 0)\n\t\tweight_matrix_2 = torch.exp(weight_matrix - weight_matrix.max(2, keepdim=True)[0]).permute(1, 2, 0)\n\n\t\t# weight_matrix_1: #step1 x #step2 x #sample\n\t\tweight_matrix_1 = weight_matrix_1 * x1_mask[:, None, :]\n\t\tweight_matrix_2 = weight_matrix_2 * x2_mask[None, :, :]\n\n\t\talpha = weight_matrix_1 / weight_matrix_1.sum(0, keepdim=True)\n\t\tbeta = weight_matrix_2 / weight_matrix_2.sum(1, keepdim=True)\n\n\t\tctx2_ = (torch.unsqueeze(ctx1, 1) * torch.unsqueeze(alpha,3)).sum(0)\n\t\tctx1_ = (torch.unsqueeze(ctx2, 0) * torch.unsqueeze(beta,3)).sum(1)\n\n\t\tinp1 = torch.cat([ctx1, ctx1_, ctx1 * ctx1_, ctx1 - ctx1_], 2)\n\t\tinp2 = torch.cat([ctx2, ctx2_, ctx2 * ctx2_, ctx2 - ctx2_], 2)\n\t\tinp1=self.dropout(self.linear_layer_compare(inp1))\n\t\tinp2=self.dropout(self.linear_layer_compare(inp2))\n\t\tv1=self.tree_lstm_compare(inp1, x1_mask, x1_left_mask, x1_right_mask)\n\t\tv2=self.tree_lstm_compare(inp2, x2_mask, x2_left_mask, x2_right_mask)\n\t\tlogits = self.aggregate(v1, v2)\n\t\treturn logits\n'"
ESIM/Tree_IM/tree.py,0,"b""# tree object from stanfordnlp/treelstm\nclass Tree(object):\n    def __init__(self):\n        self.parent = None\n        self.num_children = 0\n        self.children = list()\n        self.gold_label = None # node label for SST\n        self.output = None # output node for SST\n\n    def add_child(self,child):\n        child.parent = self\n        self.num_children += 1\n        self.children.append(child)\n\n    def size(self):\n        #if getattr(self,'_size'):\n        #    return self._size\n        count = 1\n        for i in xrange(self.num_children):\n            count += self.children[i].size()\n        self._size = count\n        return self._size\n\n    def depth(self):\n        if getattr(self,'_depth'):\n            return self._depth\n        count = 0\n        if self.num_children>0:\n            for i in xrange(self.num_children):\n                child_depth = self.children[i].depth()\n                if child_depth>count:\n                    count = child_depth\n            count += 1\n        self._depth = count\n        return self._depth\n"""
ESIM/Tree_IM/util.py,3,"b'from __future__ import division\n\nimport os\nfrom copy import deepcopy\nfrom tqdm import tqdm\nimport torch\nimport torch.utils.data as data\nfrom tree import Tree\nimport Constants\n\ndef print_tree(tree, level):\n\tindent = \'\'\n\tfor i in range(level):\n\t\tindent += \'| \'\n\tline = indent + str(tree.idx)\n\tprint (line)\n\tfor i in xrange(tree.num_children):\n\t\tprint_tree(tree.children[i], level+1)\ndef URL_maxF1_eval(predict_result,test_data_label):\n\ttest_data_label=[item>=1 for item in test_data_label]\n\tcounter = 0\n\ttp = 0.0\n\tfp = 0.0\n\tfn = 0.0\n\ttn = 0.0\n\n\tfor i, t in enumerate(predict_result):\n\n\t\tif t>0.5:\n\t\t\tguess=True\n\t\telse:\n\t\t\tguess=False\n\t\tlabel = test_data_label[i]\n\t\t#print guess, label\n\t\tif guess == True and label == False:\n\t\t\tfp += 1.0\n\t\telif guess == False and label == True:\n\t\t\tfn += 1.0\n\t\telif guess == True and label == True:\n\t\t\ttp += 1.0\n\t\telif guess == False and label == False:\n\t\t\ttn += 1.0\n\t\tif label == guess:\n\t\t\tcounter += 1.0\n\t\t#else:\n\t\t\t#print label+\'--\'*20\n\t\t\t# if guess:\n\t\t\t# print ""GOLD-"" + str(label) + ""\\t"" + ""SYS-"" + str(guess) + ""\\t"" + sent1 + ""\\t"" + sent2\n\n\ttry:\n\t\tP = tp / (tp + fp)\n\t\tR = tp / (tp + fn)\n\t\tF = 2 * P * R / (P + R)\n\texcept:\n\t\tP=0\n\t\tR=0\n\t\tF=0\n\n\t#print ""PRECISION: %s, RECALL: %s, F1: %s"" % (P, R, F)\n\t#print ""ACCURACY: %s"" % (counter/len(predict_result))\n\taccuracy=counter/len(predict_result)\n\n\t#print ""# true pos:"", tp\n\t#print ""# false pos:"", fp\n\t#print ""# false neg:"", fn\n\t#print ""# true neg:"", tn\n\tmaxF1=0\n\tP_maxF1=0\n\tR_maxF1=0\n\tprobs = predict_result\n\tsortedindex = sorted(range(len(probs)), key=probs.__getitem__)\n\tsortedindex.reverse()\n\n\ttruepos=0\n\tfalsepos=0\n\tfor sortedi in sortedindex:\n\t\tif test_data_label[sortedi]==True:\n\t\t\ttruepos+=1\n\t\telif test_data_label[sortedi]==False:\n\t\t\tfalsepos+=1\n\t\tprecision=0\n\t\tif truepos+falsepos>0:\n\t\t\tprecision=truepos/(truepos+falsepos)\n\n\t\trecall=truepos/(tp+fn)\n\t\tf1=0\n\t\tif precision+recall>0:\n\t\t\tf1=2*precision*recall/(precision+recall)\n\t\t\tif f1>maxF1:\n\t\t\t\t#print probs[sortedi]\n\t\t\t\tmaxF1=f1\n\t\t\t\tP_maxF1=precision\n\t\t\t\tR_maxF1=recall\n\tprint ""PRECISION: %s, RECALL: %s, max_F1: %s"" % (P_maxF1, R_maxF1, maxF1)\n\treturn (accuracy, maxF1)\n\nclass Dataset(data.Dataset):\n\tdef __init__(self, path, vocab, num_classes):\n\t\tsuper(Dataset, self).__init__()\n\t\tself.vocab = vocab\n\t\tself.num_classes = num_classes\n\n\t\tself.lsentences = self.read_sentences(os.path.join(path,\'a.toks\'))\n\t\tself.rsentences = self.read_sentences(os.path.join(path,\'b.toks\'))\n\n\t\tself.ltrees = self.read_trees(os.path.join(path,\'a.cparents\'))\n\t\tself.rtrees = self.read_trees(os.path.join(path,\'b.cparents\'))\n\n\t\tself.labels = self.read_labels(os.path.join(path,\'sim.txt\'))\n\n\t\tself.size = self.labels.size(0)\n\n\tdef __len__(self):\n\t\treturn self.size\n\n\tdef __getitem__(self, index):\n\t\tltree = deepcopy(self.ltrees[index])\n\t\trtree = deepcopy(self.rtrees[index])\n\t\tlsent = deepcopy(self.lsentences[index])\n\t\trsent = deepcopy(self.rsentences[index])\n\t\tlabel = deepcopy(self.labels[index])\n\t\treturn (ltree,lsent,rtree,rsent,label)\n\n\tdef read_sentences(self, filename):\n\t\twith open(filename,\'r\') as f:\n\t\t\tsentences = [self.read_sentence(line) for line in f.readlines()]\n\t\treturn sentences\n\n\tdef read_sentence(self, line):\n\t\tindices = self.vocab.convertToIdx(line.split(), Constants.UNK_WORD)\n\t\treturn torch.LongTensor(indices)\n\n\tdef read_trees(self, filename):\n\t\twith open(filename,\'r\') as f:\n\t\t\ttrees = [self.read_tree(line) for line in f.readlines()]\n\t\treturn trees\n\n\tdef read_tree(self, line):\n\t\tparents = map(int,line.split())\n\t\ttrees = dict()\n\t\troot = None\n\t\tfor i in xrange(1,len(parents)+1):\n\t\t\t#if not trees[i-1] and parents[i-1]!=-1:\n\t\t\tif i-1 not in trees.keys() and parents[i-1]!=-1:\n\t\t\t\tidx = i\n\t\t\t\tprev = None\n\t\t\t\twhile True:\n\t\t\t\t\tparent = parents[idx-1]\n\t\t\t\t\tif parent == -1:\n\t\t\t\t\t\tbreak\n\t\t\t\t\ttree = Tree()\n\t\t\t\t\tif prev is not None:\n\t\t\t\t\t\ttree.add_child(prev)\n\t\t\t\t\ttrees[idx-1] = tree\n\t\t\t\t\ttree.idx = idx-1\n\t\t\t\t\t#if trees[parent-1] is not None:\n\t\t\t\t\tif parent-1 in trees.keys():\n\t\t\t\t\t\ttrees[parent-1].add_child(tree)\n\t\t\t\t\t\tbreak\n\t\t\t\t\telif parent==0:\n\t\t\t\t\t\troot = tree\n\t\t\t\t\t\tbreak\n\t\t\t\t\telse:\n\t\t\t\t\t\tprev = tree\n\t\t\t\t\t\tidx = parent\n\t\treturn root\n\n\tdef read_labels(self, filename):\n\t\twith open(filename,\'r\') as f:\n\t\t\tlabels = map(lambda x: float(x), f.readlines())\n\t\t\tlabels = torch.Tensor(labels)\n\t\treturn labels'"
ESIM/Tree_IM/vocab.py,0,"b""# vocab object from harvardnlp/opennmt-py\nclass Vocab(object):\n    def __init__(self, filename=None, data=None, lower=False):\n        self.idxToLabel = {}\n        self.labelToIdx = {}\n        self.lower = lower\n\n        # Special entries will not be pruned.\n        self.special = []\n\n        if data is not None:\n            self.addSpecials(data)\n        if filename  is not None:\n            self.loadFile(filename)\n\n    def size(self):\n        return len(self.idxToLabel)\n\n    # Load entries from a file.\n    def loadFile(self, filename):\n        idx = 0\n        for line in open(filename):\n            token = line.rstrip('\\n')\n            self.add(token)\n            idx += 1\n\n    def getIndex(self, key, default=None):\n        if self.lower:\n            key = key.lower()\n        try:\n            return self.labelToIdx[key]\n        except KeyError:\n            return default\n\n    def getLabel(self, idx, default=None):\n        try:\n            return self.idxToLabel[idx]\n        except KeyError:\n            return default\n\n    # Mark this `label` and `idx` as special\n    def addSpecial(self, label, idx=None):\n        idx = self.add(label)\n        self.special += [idx]\n\n    # Mark all labels in `labels` as specials\n    def addSpecials(self, labels):\n        for label in labels:\n            self.addSpecial(label)\n\n    # Add `label` in the dictionary. Use `idx` as its index if given.\n    def add(self, label):\n        if self.lower:\n            label = label.lower()\n\n        if label in self.labelToIdx:\n            idx = self.labelToIdx[label]\n        else:\n            idx = len(self.idxToLabel)\n            self.idxToLabel[idx] = label\n            self.labelToIdx[label] = idx\n        return idx\n\n    # Convert `labels` to indices. Use `unkWord` if not found.\n    # Optionally insert `bosWord` at the beginning and `eosWord` at the .\n    def convertToIdx(self, labels, unkWord, bosWord=None, eosWord=None):\n        vec = []\n\n        if bosWord is not None:\n            vec += [self.getIndex(bosWord)]\n\n        unk = self.getIndex(unkWord)\n        vec += [self.getIndex(label, default=unk) for label in labels]\n\n        if eosWord is not None:\n            vec += [self.getIndex(eosWord)]\n\n        return vec\n\n    # Convert `idx` to labels. If index `stop` is reached, convert it and return.\n    def convertToLabels(self, idx, stop):\n        labels = []\n\n        for i in idx:\n            labels += [self.getLabel(i)]\n            if i == stop:\n                break\n\n        return labels\n"""
