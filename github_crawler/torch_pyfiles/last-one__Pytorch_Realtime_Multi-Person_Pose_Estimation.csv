file_path,api_count,code
CocoFolder.py,1,"b'import torch\nimport torch.utils.data as data\nimport numpy as np\nimport shutil\nimport time\nimport random\nimport os\nimport math\nimport json\nfrom PIL import Image\nimport cv2\nimport Mytransforms\n\ndef read_data_file(file_dir):\n\n    lists = []\n    with open(file_dir, \'r\') as fp:\n        line = fp.readline()\n        while line:\n            path = line.strip()\n            lists.append(path)\n            line = fp.readline()\n\n    return lists\n\ndef read_json_file(file_dir):\n    """"""\n        filename: JSON file\n\n        return: two list: key_points list and centers list\n    """"""\n    fp = open(file_dir)\n    data = json.load(fp)\n    kpts = []\n    centers = []\n    scales = []\n\n    for info in data:\n        kpt = []\n        center = []\n        scale = []\n        lists = info[\'info\']\n        for x in lists:\n           kpt.append(x[\'keypoints\'])\n           center.append(x[\'pos\'])\n           scale.append(x[\'scale\'])\n        kpts.append(kpt)\n        centers.append(center)\n        scales.append(scale)\n    fp.close()\n\n    return kpts, centers, scales\n\ndef generate_heatmap(heatmap, kpt, stride, sigma):\n\n    height, width, num_point = heatmap.shape\n    start = stride / 2.0 - 0.5\n\n    num = len(kpt)\n    length = len(kpt[0])\n    for i in range(num):\n        for j in range(length):\n            if kpt[i][j][2] > 1:\n                continue\n            x = kpt[i][j][0]\n            y = kpt[i][j][1]\n            for h in range(height):\n                for w in range(width):\n                    xx = start + w * stride\n                    yy = start + h * stride\n                    dis = ((xx - x) * (xx - x) + (yy - y) * (yy - y)) / 2.0 / sigma / sigma\n                    if dis > 4.6052:\n                        continue\n                    heatmap[h][w][j + 1] += math.exp(-dis)\n                    if heatmap[h][w][j + 1] > 1:\n                        heatmap[h][w][j + 1] = 1\n\n    return heatmap\n\ndef generate_vector(vector, cnt, kpts, vec_pair, stride, theta):\n\n    height, width, channel = cnt.shape\n    length = len(kpts)\n\n    for j in range(length):\n        for i in range(channel):\n            a = vec_pair[0][i]\n            b = vec_pair[1][i]\n            if kpts[j][a][2] > 1 or kpts[j][b][2] > 1:\n                continue\n            ax = kpts[j][a][0] * 1.0 / stride\n            ay = kpts[j][a][1] * 1.0 / stride\n            bx = kpts[j][b][0] * 1.0 / stride\n            by = kpts[j][b][1] * 1.0 / stride\n\n            bax = bx - ax\n            bay = by - ay\n            norm_ba = math.sqrt(1.0 * bax * bax + bay * bay) + 1e-9 # to aviod two points have same position.\n            bax /= norm_ba\n            bay /= norm_ba\n\n            min_w = max(int(round(min(ax, bx) - theta)), 0)\n            max_w = min(int(round(max(ax, bx) + theta)), width)\n            min_h = max(int(round(min(ay, by) - theta)), 0)\n            max_h = min(int(round(max(ay, by) + theta)), height)\n\n            for h in range(min_h, max_h):\n                for w in range(min_w, max_w):\n                    px = w - ax\n                    py = h - ay\n\n                    dis = abs(bay * px - bax * py)\n                    if dis <= theta:\n                        vector[h][w][2 * i] = (vector[h][w][2 * i] * cnt[h][w][i] + bax) / (cnt[h][w][i] + 1)\n                        vector[h][w][2 * i + 1] = (vector[h][w][2 * i + 1] * cnt[h][w][i] + bay) / (cnt[h][w][i] + 1)\n                        cnt[h][w][i] += 1\n\n    return vector\n\nclass CocoFolder(data.Dataset):\n\n    def __init__(self, file_dir, stride, transformer=None):\n\n        self.img_list = read_data_file(file_dir[0])\n        self.mask_list = read_data_file(file_dir[1])\n        self.kpt_list, self.center_list, self.scale_list = read_json_file(file_dir[2])\n        self.stride = stride\n        self.transformer = transformer\n        self.vec_pair = [[2,3,5,6,8,9, 11,12,0,1,1, 1,1,2, 5, 0, 0, 14,15],\n                         [3,4,6,7,9,10,12,13,1,8,11,2,5,16,17,14,15,16,17]] # different from openpose\n        self.theta = 1.0\n        self.sigma = 7.0\n\n    def __getitem__(self, index):\n\n        img_path = self.img_list[index]\n\n        img = np.array(cv2.imread(img_path), dtype=np.float32)\n        mask_path = self.mask_list[index]\n        mask = np.load(mask_path)\n        mask = np.array(mask, dtype=np.float32)\n\n        kpt = self.kpt_list[index]\n        center = self.center_list[index]\n        scale = self.scale_list[index]\n\n        img, mask, kpt, center = self.transformer(img, mask, kpt, center, scale)\n\n        height, width, _ = img.shape\n\n        mask = cv2.resize(mask, (width / self.stride, height / self.stride)).reshape((height / self.stride, width / self.stride, 1))\n\n        heatmap = np.zeros((height / self.stride, width / self.stride, len(kpt[0]) + 1), dtype=np.float32)\n        heatmap = generate_heatmap(heatmap, kpt, self.stride, self.sigma)\n        heatmap[:,:,0] = 1.0 - np.max(heatmap[:,:,1:], axis=2) # for background\n        heatmap = heatmap * mask\n\n        vecmap = np.zeros((height / self.stride, width / self.stride, len(self.vec_pair[0]) * 2), dtype=np.float32)\n        cnt = np.zeros((height / self.stride, width / self.stride, len(self.vec_pair[0])), dtype=np.int32)\n\n        vecmap = generate_vector(vecmap, cnt, kpt, self.vec_pair, self.stride, self.theta)\n        vecmap = vecmap * mask\n\n        img = Mytransforms.normalize(Mytransforms.to_tensor(img), [128.0, 128.0, 128.0], [256.0, 256.0, 256.0]) # mean, std\n        mask = Mytransforms.to_tensor(mask)\n        heatmap = Mytransforms.to_tensor(heatmap)\n        vecmap = Mytransforms.to_tensor(vecmap)\n\n        return img, heatmap, vecmap, mask\n\n    def __len__(self):\n\n        return len(self.img_list)\n'"
Mytransforms.py,3,"b'from __future__ import division\nimport torch\nimport math\nimport random\nimport numpy as np\nimport numbers\nimport types\nimport collections\nimport warnings\nimport cv2\n\ndef normalize(tensor, mean, std):\n    """"""Normalize a ``torch.tensor``\n\n    Args:\n        tensor (torch.tensor): tensor to be normalized.\n        mean: (list): the mean of BGR\n        std: (list): the std of BGR\n    \n    Returns:\n        Tensor: Normalized tensor.\n    """"""\n\n    for t, m, s in zip(tensor, mean, std):\n        t.sub_(m).div_(s)\n    return tensor\n\ndef to_tensor(pic):\n    """"""Convert a ``numpy.ndarray`` to tensor.\n\n    See ``ToTensor`` for more details.\n\n    Args:\n        pic (numpy.ndarray): Image to be converted to tensor.\n\n    Returns:\n        Tensor: Converted image.\n    """"""\n\n    img = torch.from_numpy(pic.transpose((2, 0, 1)))\n\n    return img.float()\n\ndef resize(img, mask, kpt, center, ratio):\n    """"""Resize the ``numpy.ndarray`` and points as ratio.\n\n    Args:\n        img    (numpy.ndarray):   Image to be resized.\n        mask   (numpy.ndarray):   Mask to be resized.\n        kpt    (list):            Keypoints to be resized.\n        center (list):            Center points to be resized.\n        ratio  (tuple or number): the ratio to resize.\n\n    Returns:\n        numpy.ndarray: Resized image.\n        numpy.ndarray: Resized mask.\n        lists:         Resized keypoints.\n        lists:         Resized center points.\n    """"""\n\n    if not (isinstance(ratio, numbers.Number) or (isinstance(ratio, collections.Iterable) and len(ratio) == 2)):\n        raise TypeError(\'Got inappropriate ratio arg: {}\'.format(ratio))\n    \n    h, w, _ = img.shape\n    if w < 64:\n        img = cv2.copyMakeBorder(img, 0, 0, 0, 64 - w, cv2.BORDER_CONSTANT, value=(128, 128, 128))\n        mask = cv2.copyMakeBorder(mask, 0, 0, 0, 64 - w, cv2.BORDER_CONSTANT, value=(1, 1, 1))\n        w = 64\n    \n    if isinstance(ratio, numbers.Number):\n\n        num = len(kpt)\n        length = len(kpt[0])\n        for i in range(num):\n            for j in range(length):\n                kpt[i][j][0] *= ratio\n                kpt[i][j][1] *= ratio\n            center[i][0] *= ratio\n            center[i][1] *= ratio\n\n        return cv2.resize(img, (0, 0), fx=ratio, fy=ratio), cv2.resize(mask, (0, 0), fx=ratio, fy=ratio), kpt, center\n\n    else:\n        num = len(kpt)\n        length = len(kpt[0])\n        for i in range(num):\n            for j in range(length):\n                kpt[i][j][0] *= ratio[0]\n                kpt[i][j][1] *= ratio[1]\n            center[i][0] *= ratio[0]\n            center[i][1] *= ratio[1]\n        return np.ascontiguousarray(cv2.resize(img, (0, 0), fx=ratio[0], fy=ratio[1])), np.ascontiguousarray(cv2.resize(mask, (0, 0), fx=ratio[0], fy=ratio[1])), kpt, center\n\nclass RandomResized(object):\n    """"""Resize the given numpy.ndarray to random size and aspect ratio.\n\n    Args:\n        scale_min: the min scale to resize.\n        scale_max: the max scale to resize.\n    """"""\n\n    def __init__(self, scale_min=0.5, scale_max=1.1):\n        self.scale_min = scale_min\n        self.scale_max = scale_max\n\n    @staticmethod\n    def get_params(img, scale_min, scale_max, scale):\n\n        height, width, _ = img.shape\n\n        ratio = random.uniform(scale_min, scale_max)\n        ratio = ratio * 0.6 / scale\n\n        return ratio\n\n    def __call__(self, img, mask, kpt, center, scale):\n        """"""\n        Args:\n            img     (numpy.ndarray): Image to be resized.\n            mask    (numpy.ndarray): Mask to be resized.\n            kpt     (list):          keypoints to be resized.\n            center: (list):          center points to be resized.\n\n        Returns:\n            numpy.ndarray: Randomly resize image.\n            numpy.ndarray: Randomly resize mask.\n            list:          Randomly resize keypoints.\n            list:          Randomly resize center points.\n        """"""\n        ratio = self.get_params(img, self.scale_min, self.scale_max, scale[0])\n\n        return resize(img, mask, kpt, center, ratio)\n\nclass TestResized(object):\n    """"""Resize the given numpy.ndarray to the size for test.\n\n    Args:\n        size: the size to resize.\n    """"""\n\n    def __init__(self, size):\n        assert (isinstance(size, int) or (isinstance(size, collections.Iterable) and len(size) == 2))\n        if isinstance(size, int):\n            self.size = (size, size)\n        else:\n            self.size = size\n\n    @staticmethod\n    def get_params(img, output_size):\n\n        height, width, _ = img.shape\n        \n        return (output_size[0] * 1.0 / width, output_size[1] * 1.0 / height)\n\n    def __call__(self, img, mask, kpt, center):\n        """"""\n        Args:\n            img     (numpy.ndarray): Image to be resized.\n            mask    (numpy.ndarray): Mask to be resized.\n            kpt     (list):          keypoints to be resized.\n            center: (list):          center points to be resized.\n\n        Returns:\n            numpy.ndarray: Randomly resize image.\n            numpy.ndarray: Randomly resize mask.\n            list:          Randomly resize keypoints.\n            list:          Randomly resize center points.\n        """"""\n        ratio = self.get_params(img, self.size)\n\n        return resize(img, mask, kpt, center, ratio)\n\ndef rotate(img, mask, kpt, center, degree):\n    """"""Rotate the ``numpy.ndarray`` and points as degree.\n\n    Args:\n        img    (numpy.ndarray): Image to be rotated.\n        mask   (numpy.ndarray): Mask to be rotated.\n        kpt    (list):          Keypoints to be rotated.\n        center (list):          Center points to be rotated.\n        degree (number):        the degree to rotate.\n\n    Returns:\n        numpy.ndarray: Resized image.\n        numpy.ndarray: Resized mask.\n        list:          Resized keypoints.\n        list:          Resized center points.\n    """"""\n\n    height, width, _ = img.shape\n    \n    img_center = (width / 2.0 , height / 2.0)\n    \n    rotateMat = cv2.getRotationMatrix2D(img_center, degree, 1.0)\n    cos_val = np.abs(rotateMat[0, 0])\n    sin_val = np.abs(rotateMat[0, 1])\n    new_width = int(height * sin_val + width * cos_val)\n    new_height = int(height * cos_val + width * sin_val)\n    rotateMat[0, 2] += (new_width / 2.) - img_center[0]\n    rotateMat[1, 2] += (new_height / 2.) - img_center[1]\n\n    img = cv2.warpAffine(img, rotateMat, (new_width, new_height), borderValue=(128, 128, 128))\n    mask = cv2.warpAffine(mask, rotateMat, (new_width, new_height), borderValue=(1, 1, 1))\n\n    num = len(kpt)\n    length = len(kpt[0])\n    for i in range(num):\n        for j in range(length):\n            x = kpt[i][j][0]\n            y = kpt[i][j][1]\n            p = np.array([x, y, 1])\n            p = rotateMat.dot(p)\n            kpt[i][j][0] = p[0]\n            kpt[i][j][1] = p[1]\n\n        x = center[i][0]\n        y = center[i][1]\n        p = np.array([x, y, 1])\n        p = rotateMat.dot(p)\n        center[i][0] = p[0]\n        center[i][1] = p[1]\n\n    return np.ascontiguousarray(img), np.ascontiguousarray(mask), kpt, center\n\nclass RandomRotate(object):\n    """"""Rotate the input numpy.ndarray and points to the given degree.\n\n    Args:\n        degree (number): Desired rotate degree.\n    """"""\n\n    def __init__(self, max_degree):\n        assert isinstance(max_degree, numbers.Number)\n        self.max_degree = max_degree\n\n    @staticmethod\n    def get_params(max_degree):\n        """"""Get parameters for ``rotate`` for a random rotate.\n\n        Returns:\n            number: degree to be passed to ``rotate`` for random rotate.\n        """"""\n        degree = random.uniform(-max_degree, max_degree)\n\n        return degree\n\n    def __call__(self, img, mask, kpt, center):\n        """"""\n        Args:\n            img    (numpy.ndarray): Image to be rotated.\n            mask   (numpy.ndarray): Mask to be rotated.\n            kpt    (list):          Keypoints to be rotated.\n            center (list):          Center points to be rotated.\n\n        Returns:\n            numpy.ndarray: Rotated image.\n            list:          Rotated key points.\n        """"""\n        degree = self.get_params(self.max_degree)\n\n        return rotate(img, mask, kpt, center, degree)\n\ndef crop(img, mask, kpt, center, offset_left, offset_up, w, h):\n\n    num = len(kpt)\n    length = len(kpt[0])\n\n    for x in range(num):\n        for y in range(length):\n            kpt[x][y][0] -= offset_left\n            kpt[x][y][1] -= offset_up\n        center[x][0] -= offset_left\n        center[x][1] -= offset_up\n\n    height, width, _ = img.shape\n    mask = mask.reshape((height, width))\n\n    new_img = np.empty((h, w, 3), dtype=np.float32)\n    new_img.fill(128)\n\n    new_mask = np.empty((h, w), dtype=np.float32)\n    new_mask.fill(1)\n\n    st_x = 0\n    ed_x = w\n    st_y = 0\n    ed_y = h\n    or_st_x = offset_left\n    or_ed_x = offset_left + w\n    or_st_y = offset_up\n    or_ed_y = offset_up + h\n\n    if offset_left < 0:\n        st_x = -offset_left\n        or_st_x = 0\n    if offset_left + w > width:\n        ed_x = width - offset_left\n        or_ed_x = width\n    if offset_up < 0:\n        st_y = -offset_up\n        or_st_y = 0\n    if offset_up + h > height:\n        ed_y = height - offset_up\n        or_ed_y = height\n\n    new_img[st_y: ed_y, st_x: ed_x, :] = img[or_st_y: or_ed_y, or_st_x: or_ed_x, :].copy()\n    new_mask[st_y: ed_y, st_x: ed_x] = mask[or_st_y: or_ed_y, or_st_x: or_ed_x].copy()\n\n    return np.ascontiguousarray(new_img), np.ascontiguousarray(new_mask), kpt, center\n\nclass RandomCrop(object):\n    """"""Crop the given numpy.ndarray and  at a random location.\n\n    Args:\n        size (int): Desired output size of the crop.\n    """"""\n\n    def __init__(self, size, center_perturb_max=40):\n        assert isinstance(size, numbers.Number)\n        self.size = (int(size), int(size)) # (w, h)\n        self.center_perturb_max = center_perturb_max\n\n    @staticmethod\n    def get_params(img, center, output_size, center_perturb_max):\n        """"""Get parameters for ``crop`` for a random crop.\n\n        Args:\n            img                (numpy.ndarray): Image to be cropped.\n            center             (list):          the center of main person.\n            output_size        (tuple):         Expected output size of the crop.\n            center_perturb_max (int):           the max perturb size.\n\n        Returns:\n            tuple: params (i, j, h, w) to be passed to ``crop`` for random crop.\n        """"""\n        ratio_x = random.uniform(0, 1)\n        ratio_y = random.uniform(0, 1)\n        x_offset = int((ratio_x - 0.5) * 2 * center_perturb_max)\n        y_offset = int((ratio_y - 0.5) * 2 * center_perturb_max)\n        center_x = center[0][0] + x_offset\n        center_y = center[0][1] + y_offset\n\n        return int(round(center_x - output_size[0] / 2)), int(round(center_y - output_size[1] / 2))\n\n    def __call__(self, img, mask, kpt, center):\n        """"""\n        Args:\n            img (numpy.ndarray): Image to be cropped.\n            mask (numpy.ndarray): Mask to be cropped.\n            kpt (list): keypoints to be cropped.\n            center (list): center points to be cropped.\n\n        Returns:\n            numpy.ndarray: Cropped image.\n            numpy.ndarray: Cropped mask.\n            list:          Cropped keypoints.\n            list:          Cropped center points.\n        """"""\n\n        offset_left, offset_up = self.get_params(img, center, self.size, self.center_perturb_max)\n\n        return crop(img, mask, kpt, center, offset_left, offset_up, self.size[0], self.size[1])\n\ndef hflip(img, mask, kpt, center):\n\n    height, width, _ = img.shape\n    mask = mask.reshape((height, width, 1))\n\n    img = img[:, ::-1, :]\n    mask = mask[:, ::-1, :]\n\n    num = len(kpt)\n    length = len(kpt[0])\n    for i in range(num):\n        for j in range(length):\n            if kpt[i][j][2] <= 1:\n                kpt[i][j][0] = width - 1 - kpt[i][j][0]\n        center[i][0] = width - 1 - center[i][0]\n\n    swap_pair = [[3, 6], [4, 7], [5, 8], [9, 12], [10, 13], [11, 14], [15, 16], [17, 18]]\n    for x in swap_pair:\n        for i in range(num):\n            temp_point = kpt[i][x[0] - 1]\n            kpt[i][x[0] - 1] = kpt[i][x[1] - 1]\n            kpt[i][x[1] - 1] = temp_point\n\n    return np.ascontiguousarray(img), np.ascontiguousarray(mask), kpt, center\n\nclass RandomHorizontalFlip(object):\n    """"""Random horizontal flip the image.\n\n    Args:\n        prob (number): the probability to flip.\n    """"""\n    \n    def __init__(self, prob=0.5):\n        self.prob = prob\n        \n    def __call__(self, img, mask, kpt, center):\n        """"""\n        Args:\n            img    (numpy.ndarray): Image to be flipped.\n            mask   (numpy.ndarray): Mask to be flipped.\n            kpt    (list):          Keypoints to be flipped.\n            center (list):          Center points to be flipped.\n\n        Returns:\n            numpy.ndarray: Randomly flipped image.\n            list: Randomly flipped points.\n        """"""\n        if random.random() < self.prob:\n            return hflip(img, mask, kpt, center)\n        return img, mask, kpt, center\n\nclass Compose(object):\n    """"""Composes several transforms together.\n\n    Args:\n        transforms (list of ``Transform`` objects): list of transforms to compose.\n\n    Example:\n        >>> Mytransforms.Compose([\n        >>>     Mytransforms.CenterCrop(10),\n        >>>     Mytransforms.ToTensor(),\n        >>> ])\n    """"""\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img, mask, kpt, center, scale=None):\n\n        for t in self.transforms:\n            if isinstance(t, RandomResized):\n                img, mask, kpt, center = t(img, mask, kpt, center, scale)\n            else:\n                img, mask, kpt, center = t(img, mask, kpt, center)\n\n        return img, mask, kpt, center\n'"
pose_estimation.py,6,"b""import torch\nimport torch.nn as nn\nimport os\nimport sys\nimport math\nimport torchvision.models as models\n\nclass Pose_Estimation(nn.Module):\n\n    def __init__(self, net_dict, batch_norm=False):\n\n        super(Pose_Estimation, self).__init__()\n\n        self.model0 = self._make_layer(net_dict[0], batch_norm, True)\n\n        self.model1_1 = self._make_layer(net_dict[1][0], batch_norm)\n        self.model1_2 = self._make_layer(net_dict[1][1], batch_norm)\n\n        self.model2_1 = self._make_layer(net_dict[2][0], batch_norm)\n        self.model2_2 = self._make_layer(net_dict[2][1], batch_norm)\n\n        self.model3_1 = self._make_layer(net_dict[3][0], batch_norm)\n        self.model3_2 = self._make_layer(net_dict[3][1], batch_norm)\n\n        self.model4_1 = self._make_layer(net_dict[4][0], batch_norm)\n        self.model4_2 = self._make_layer(net_dict[4][1], batch_norm)\n\n        self.model5_1 = self._make_layer(net_dict[5][0], batch_norm)\n        self.model5_2 = self._make_layer(net_dict[5][1], batch_norm)\n\n        self.model6_1 = self._make_layer(net_dict[6][0], batch_norm)\n        self.model6_2 = self._make_layer(net_dict[6][1], batch_norm)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                m.weight.data.normal_(0, 0.01)\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, net_dict, batch_norm=False, last_activity=False):\n\n        layers = []\n        length = len(net_dict) - 1\n        for i in range(length):\n            one_layer = net_dict[i]\n            key = one_layer.keys()[0]\n            v = one_layer[key]\n\n            if 'pool' in key:\n                layers += [nn.MaxPool2d(kernel_size=v[0], stride=v[1], padding=v[2])]\n            else:\n                conv2d = nn.Conv2d(in_channels=v[0], out_channels=v[1], kernel_size=v[2], stride=v[3], padding=v[4])\n                if batch_norm:\n                    layers += [conv2d, nn.BatchNorm2d(v[1]), nn.ReLU(inplace=True)]\n                else:\n                    layers += [conv2d, nn.ReLU(inplace=True)]\n\n        if last_activity:\n            one_layer = net_dict[-1]\n            key = one_layer.keys()[0]\n            v = one_layer[key]\n\n            conv2d = nn.Conv2d(in_channels=v[0], out_channels=v[1], kernel_size=v[2], stride=v[3], padding=v[4])\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(v[1]), nn.ReLU(inplace=True)]\n            else:\n                layers += [conv2d, nn.ReLU(inplace=True)]\n        else:\n            one_layer = net_dict[-1]\n            key = one_layer.keys()[0]\n            v = one_layer[key]\n\n            conv2d = nn.Conv2d(in_channels=v[0], out_channels=v[1], kernel_size=v[2], stride=v[3], padding=v[4])\n            layers += [conv2d]\n        return nn.Sequential(*layers)\n\n    def forward(self, x, mask):\n        out0 = self.model0(x)\n\n        out1_1 = self.model1_1(out0)\n        out1_2 = self.model1_2(out0)\n        out1 = torch.cat([out1_1, out1_2, out0], 1)\n        out1_vec_mask = out1_1 * mask\n        out1_heat_mask = out1_2 * mask\n\n        out2_1 = self.model2_1(out1)\n        out2_2 = self.model2_2(out1)\n        out2 = torch.cat([out2_1, out2_2, out0], 1)\n        out2_vec_mask = out2_1 * mask\n        out2_heat_mask = out2_2 * mask\n\n        out3_1 = self.model3_1(out2)\n        out3_2 = self.model3_2(out2)\n        out3 = torch.cat([out3_1, out3_2, out0], 1)\n        out3_vec_mask = out3_1 * mask\n        out3_heat_mask = out3_2 * mask\n\n        out4_1 = self.model4_1(out3)\n        out4_2 = self.model4_2(out3)\n        out4 = torch.cat([out4_1, out4_2, out0], 1)\n        out4_vec_mask = out4_1 * mask\n        out4_heat_mask = out4_2 * mask\n\n        out5_1 = self.model5_1(out4)\n        out5_2 = self.model5_2(out4)\n        out5 = torch.cat([out5_1, out5_2, out0], 1)\n        out5_vec_mask = out5_1 * mask\n        out5_heat_mask = out5_2 * mask\n\n        out6_1 = self.model6_1(out5)\n        out6_2 = self.model6_2(out5)\n        out6_vec_mask = out6_1 * mask\n        out6_heat_mask = out6_2 * mask\n\n        return out1_vec_mask, out1_heat_mask, out2_vec_mask, out2_heat_mask, out3_vec_mask, out3_heat_mask, out4_vec_mask, out4_heat_mask, out5_vec_mask, out5_heat_mask, out6_vec_mask, out6_heat_mask\n\ndef PoseModel(num_point, num_vector, num_stages=6, batch_norm=False, pretrained=False):\n\n    net_dict = []\n    block0 = [{'conv1_1': [3, 64, 3, 1, 1]}, {'conv1_2': [64, 64, 3, 1, 1]}, {'pool1': [2, 2, 0]},\n            {'conv2_1': [64, 128, 3, 1, 1]}, {'conv2_2': [128, 128, 3, 1, 1]}, {'pool2': [2, 2, 0]},\n            {'conv3_1': [128, 256, 3, 1, 1]}, {'conv3_2': [256, 256, 3, 1, 1]}, {'conv3_3': [256, 256, 3, 1, 1]}, {'conv3_4': [256, 256, 3, 1, 1]}, {'pool3': [2, 2, 0]},\n            {'conv4_1': [256, 512, 3, 1, 1]}, {'conv4_2': [512, 512, 3, 1, 1]}, {'conv4_3_cpm': [512, 256, 3, 1, 1]}, {'conv4_4_cpm': [256, 128, 3, 1, 1]}]\n    net_dict.append(block0)\n\n    block1 = [[], []]\n    in_vec = [0, 128, 128, 128, 128, 512, num_vector * 2]\n    in_heat = [0, 128, 128, 128, 128, 512, num_point]\n    for i in range(1, 6):\n        if i < 4:\n            block1[0].append({'conv{}_stage1_vec'.format(i) :[in_vec[i], in_vec[i + 1], 3, 1, 1]})\n            block1[1].append({'conv{}_stage1_heat'.format(i):[in_heat[i], in_heat[i + 1], 3, 1, 1]})\n        else:\n            block1[0].append({'conv{}_stage1_vec'.format(i):[in_vec[i], in_vec[i + 1], 1, 1, 0]})\n            block1[1].append({'conv{}_stage1_heat'.format(i):[in_heat[i], in_heat[i + 1], 1, 1, 0]})\n    net_dict.append(block1)\n\n    in_vec_1 = [0, 128 + num_point + num_vector * 2, 128, 128, 128, 128, 128, 128, num_vector * 2]\n    in_heat_1 = [0, 128 + num_point + num_vector * 2, 128, 128, 128, 128, 128, 128, num_point]\n    for j in range(2, num_stages + 1):\n        blocks = [[], []]\n        for i in range(1, 8):\n            if i < 6:\n                blocks[0].append({'conv{}_stage{}_vec'.format(i, j):[in_vec_1[i], in_vec_1[i + 1], 7, 1, 3]})\n                blocks[1].append({'conv{}_stage{}_heat'.format(i, j):[in_heat_1[i], in_heat_1[i + 1], 7, 1, 3]})\n            else:\n                blocks[0].append({'conv{}_stage{}_vec'.format(i, j):[in_vec_1[i], in_vec_1[i + 1], 1, 1, 0]})\n                blocks[1].append({'conv{}_stage{}_heat'.format(i, j):[in_heat_1[i], in_heat_1[i + 1], 1, 1, 0]})\n        net_dict.append(blocks)\n\n    model = Pose_Estimation(net_dict, batch_norm)\n\n    if pretrained:\n        parameter_num = 10\n        if batch_norm:\n            vgg19 = models.vgg19_bn(pretrained=True)\n            parameter_num *= 6\n        else:\n            vgg19 = models.vgg19(pretrained=True)\n            parameter_num *= 2\n        vgg19_state_dict = vgg19.state_dict()\n        vgg19_keys = vgg19_state_dict.keys()\n\n        model_dict = model.state_dict()\n        from collections import OrderedDict\n        weights_load = OrderedDict()\n        for i in range(parameter_num):\n            weights_load[model.state_dict().keys()[i]] = vgg19_state_dict[vgg19_keys[i]]\n        model_dict.update(weights_load)\n        model.load_state_dict(model_dict)\n\n    return model\n\nif __name__ == '__main__':\n\n    print PoseModel(19, 6, True, True)\n"""
utils.py,1,"b'import math\nimport torch\nimport shutil\nimport time\nimport os\nimport random\nfrom easydict import EasyDict as edict\nimport yaml\nimport numpy as np\n\nclass AverageMeter(object):\n    """""" Computes ans stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0.\n        self.avg = 0.\n        self.sum = 0.\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\ndef adjust_learning_rate(optimizer, iters, base_lr, policy_parameter, policy=\'step\', multiple=[1]):\n\n    if policy == \'fixed\':\n        lr = base_lr\n    elif policy == \'step\':\n        lr = base_lr * (policy_parameter[\'gamma\'] ** (iters // policy_parameter[\'step_size\']))\n    elif policy == \'exp\':\n        lr = base_lr * (policy_parameter[\'gamma\'] ** iters)\n    elif policy == \'inv\':\n        lr = base_lr * ((1 + policy_parameter[\'gamma\'] * iters) ** (-policy_parameter[\'power\']))\n    elif policy == \'multistep\':\n        lr = base_lr\n        for stepvalue in policy_parameter[\'stepvalue\']:\n            if iters >= stepvalue:\n                lr *= policy_parameter[\'gamma\']\n            else:\n                break\n    elif policy == \'poly\':\n        lr = base_lr * ((1 - iters * 1.0 / policy_parameter[\'max_iter\']) ** policy_parameter[\'power\'])\n    elif policy == \'sigmoid\':\n        lr = base_lr * (1.0 / (1 + math.exp(-policy_parameter[\'gamma\'] * (iters - policy_parameter[\'stepsize\']))))\n    elif policy == \'multistep-poly\':\n        lr = base_lr\n        stepstart = 0\n        stepend = policy_parameter[\'max_iter\']\n        for stepvalue in policy_parameter[\'stepvalue\']:\n            if iters >= stepvalue:\n                lr *= policy_parameter[\'gamma\']\n                stepstart = stepvalue\n            else:\n                stepend = stepvalue\n                break\n        lr = max(lr * policy_parameter[\'gamma\'], lr * (1 - (iters - stepstart) * 1.0 / (stepend - stepstart)) ** policy_parameter[\'power\'])\n\n    for i, param_group in enumerate(optimizer.param_groups):\n        param_group[\'lr\'] = lr * multiple[i]\n    return lr\n\ndef save_checkpoint(state, is_best, filename=\'checkpoint.pth.tar\'):\n\n    torch.save(state, filename + \'_latest.pth.tar\')\n    if is_best:\n        shutil.copyfile(filename + \'_latest.pth.tar\', filename + \'_best.pth.tar\')\n\ndef Config(filename):\n\n    with open(filename, \'r\') as f:\n        parser = edict(yaml.load(f))\n    for x in parser:\n        print \'{}: {}\'.format(x, parser[x])\n    return parser\n'"
caffe2pytorch/convert.py,2,"b""import caffe\nfrom caffe.proto import caffe_pb2\nimport torch\nimport os\nimport sys\nsys.path.append('..')\nimport pose_estimation\nfrom utils import save_checkpoint as save_checkpoint\n\ndef load_caffe_model(deploy_path, model_path):\n\n    caffe.set_mode_cpu()\n    net = caffe.Net(deploy_path, model_path, caffe.TEST)\n\n    return net\n\ndef load_pytorch_model():\n    \n    model = pose_estimation.PoseModel(num_point=19, num_vector=19, pretrained=True)\n\n    return model\n\ndef convert(caffe_net, pytorch_net):\n\n    caffe_keys = caffe_net.params.keys()\n    pytorch_keys = pytorch_net.state_dict().keys()\n\n    length_caffe = len(caffe_keys)\n    length_pytorch = len(pytorch_keys)\n    dic = {}\n    L1 = []\n    L2 = []\n    _1 = []\n    _2 = []\n    for i in range(length_caffe):\n        if 'L1' in caffe_keys[i]:\n            L1.append(caffe_keys[i])\n            if '_1' in pytorch_keys[2 * i]:\n                _1.append(pytorch_keys[2 * i][:-7])\n            else:\n                _2.append(pytorch_keys[2 * i][:-7])\n        elif 'L2' in caffe_keys[i]:\n            L2.append(caffe_keys[i])\n            if '_1' in pytorch_keys[2 * i]:\n                _1.append(pytorch_keys[2 * i][:-7])\n            else:\n                _2.append(pytorch_keys[2 * i][:-7])\n        else:\n            dic[caffe_keys[i]] = pytorch_keys[2 * i][:-7]\n\n    for info in zip(L1, _1):\n        dic[info[0]] = info[1]\n    for info in zip(L2, _2):\n        dic[info[0]] = info[1]\n\n    model_dict = pytorch_net.state_dict()\n    from collections import OrderedDict\n    weights_load = OrderedDict()\n    for key in dic:\n        caffe_key = key\n        pytorch_key = dic[key]\n        weights_load[pytorch_key + '.weight'] = torch.from_numpy(caffe_net.params[caffe_key][0].data)\n        weights_load[pytorch_key + '.bias'] = torch.from_numpy(caffe_net.params[caffe_key][1].data)\n    model_dict.update(weights_load)\n    pytorch_net.load_state_dict(model_dict)\n    save_checkpoint({\n        'iter': 0,\n        'state_dict': pytorch_net.state_dict(),\n        }, True, 'caffe_model_coco')\n\nif __name__ == '__main__':\n\n    caffe_net = load_caffe_model('../caffe_model/coco/pose_deploy.prototxt', '../caffe_model/coco/pose_iter_440000.caffemodel')\n    pytorch_net = load_pytorch_model()\n\n    convert(caffe_net, pytorch_net)\n"""
preprocessing/generate_json_mask.py,0,"b'import os\nimport sys\nimport math\nimport json\nimport numpy as np\nfrom pycocotools.coco import COCO\nimport argparse\n\nCOCO_TO_OURS = [0, 15, 14, 17, 16, 5, 2, 6, 3, 7, 4, 11, 8, 12, 9, 13, 10]\n\ndef parse():\n    """"""\n    ann_path is the path of COCO annotations.\n\n    all of the remainder parameters are the save path for these generated files.\n\n    json_path(.json) is the save_path for the generated json file, which contains the information required for training.\n\n    mask_dir is the save_path for the generated mask files(.npy). COCO has the information. If you use yourself dataset, you don\'t need mask files.\n\n    filelist_path(.txt) is the save_path for the generated filelist, which saves all of the absolute path of images.\n\n    masklist_path(.txt) is the save_path for the generated masklist, which saves all of the absolute path of the generated mask files.\n\n    """"""\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'--ann_path\', type=str,\n                        dest=\'ann_path\', help=\'the path of annotations\')\n    parser.add_argument(\'--json_path\', type=str,\n                        dest=\'json_path\', help=\'the save_path for the generated json file\')\n    parser.add_argument(\'--mask_dir\', type=str,\n                        dest=\'mask_dir\', help=\'the save_dir for the generated mask files\')\n    parser.add_argument(\'--filelist_path\', type=str,\n                        dest=\'filelist_path\', help=\'the save_path for the generated filelist\')\n    parser.add_argument(\'--masklist_path\', type=str,\n                        dest=\'masklist_path\', help=\'the save_path for the generated masklist\')\n\n    return parser.parse_args()\n\ndef processing(args):\n\n    ann_path = args.ann_path\n    json_path = args.json_path\n    mask_dir = args.mask_dir\n    \n    coco = COCO(ann_path)\n    ids = list(coco.imgs.keys())\n    lists = []\n    \n    flielist_fp = open(args.filelist_path, \'w\')\n    masklist_fp = open(args.masklist_path, \'w\')\n    \n    for i, img_id in enumerate(ids):\n        ann_ids = coco.getAnnIds(imgIds=img_id)\n        img_anns = coco.loadAnns(ann_ids)\n    \n        numPeople = len(img_anns)\n        name = coco.imgs[img_id][\'file_name\']\n        height = coco.imgs[img_id][\'height\']\n        width = coco.imgs[img_id][\'width\']\n    \n        persons = []\n        person_centers = []\n    \n        for p in range(numPeople):\n    \n            if img_anns[p][\'num_keypoints\'] < 5 or img_anns[p][\'area\'] < 32 * 32:\n                continue\n            kpt = img_anns[p][\'keypoints\']\n            dic = dict()\n    \n            # person center\n            person_center = [img_anns[p][\'bbox\'][0] + img_anns[p][\'bbox\'][2] / 2.0, img_anns[p][\'bbox\'][1] + img_anns[p][\'bbox\'][3] / 2.0]\n            scale = img_anns[p][\'bbox\'][3] / 368.0\n    \n            # skip this person if the distance to exiting person is too small\n            flag = 0\n            for pc in person_centers:\n                dis = math.sqrt((person_center[0] - pc[0]) * (person_center[0] - pc[0]) + (person_center[1] - pc[1]) * (person_center[1] - pc[1]))\n                if dis < pc[2] * 0.3:\n                    flag = 1;\n                    break\n            if flag == 1:\n                continue\n            dic[\'objpos\'] = person_center\n            dic[\'keypoints\'] = np.zeros((17, 3)).tolist()\n            dic[\'scale\'] = scale\n            for part in range(17):\n                dic[\'keypoints\'][part][0] = kpt[part * 3]\n                dic[\'keypoints\'][part][1] = kpt[part * 3 + 1]\n                # visiable is 1, unvisiable is 0 and not labeled is 2\n                if kpt[part * 3 + 2] == 2:\n                    dic[\'keypoints\'][part][2] = 1\n                elif kpt[part * 3 + 2] == 1:\n                    dic[\'keypoints\'][part][2] = 0\n                else:\n                    dic[\'keypoints\'][part][2] = 2\n    \n            persons.append(dic)\n            person_centers.append(np.append(person_center, max(img_anns[p][\'bbox\'][2], img_anns[p][\'bbox\'][3])))\n    \n        if len(persons) > 0:\n            filelist_fp.write(name + \'\\n\')\n            info = dict()\n            info[\'filename\'] = name\n            info[\'info\'] = []\n            cnt = 1\n            for person in persons:\n                dic = dict()\n                dic[\'pos\'] = person[\'objpos\']\n                dic[\'keypoints\'] = np.zeros((18,3)).tolist()\n                dic[\'scale\'] = person[\'scale\']\n                for i in range(17):\n                    dic[\'keypoints\'][COCO_TO_OURS[i]][0] = person[\'keypoints\'][i][0]\n                    dic[\'keypoints\'][COCO_TO_OURS[i]][1] = person[\'keypoints\'][i][1]\n                    dic[\'keypoints\'][COCO_TO_OURS[i]][2] = person[\'keypoints\'][i][2]\n                dic[\'keypoints\'][1][0] = (person[\'keypoints\'][5][0] + person[\'keypoints\'][6][0]) * 0.5\n                dic[\'keypoints\'][1][1] = (person[\'keypoints\'][5][1] + person[\'keypoints\'][6][1]) * 0.5\n                if person[\'keypoints\'][5][2] == person[\'keypoints\'][6][2]:\n                    dic[\'keypoints\'][1][2] = person[\'keypoints\'][5][2]\n                elif person[\'keypoints\'][5][2] == 2 or person[\'keypoints\'][6][2] == 2:\n                    dic[\'keypoints\'][1][2] = 2\n                else:\n                    dic[\'keypoints\'][1][2] = 0\n                info[\'info\'].append(dic)\n            lists.append(info)\n            \n            mask_all = np.zeros((height, width), dtype=np.uint8)\n            mask_miss = np.zeros((height, width), dtype=np.uint8)\n            flag = 0\n            for p in img_anns:\n                if p[\'iscrowd\'] == 1:\n                    mask_crowd = coco.annToMask(p)\n                    temp = np.bitwise_and(mask_all, mask_crowd)\n                    mask_crowd = mask_crowd - temp\n                    flag += 1\n                    continue\n                else:\n                    mask = coco.annToMask(p)\n        \n                mask_all = np.bitwise_or(mask, mask_all)\n            \n                if p[\'num_keypoints\'] <= 0:\n                    mask_miss = np.bitwise_or(mask, mask_miss)\n        \n            if flag < 1:\n                mask_miss = np.logical_not(mask_miss)\n            elif flag == 1:\n                mask_miss = np.logical_not(np.bitwise_or(mask_miss, mask_crowd))\n                mask_all = np.bitwise_or(mask_all, mask_crowd)\n            else:\n                raise Exception(\'crowd segments > 1\')\n            np.save(os.path.join(mask_dir, name.split(\'.\')[0] + \'.npy\'), mask_miss)\n            masklist_fp.write(os.path.join(mask_dir, name.split(\'.\')[0] + \'.npy\') + \'\\n\')\n        if i % 1000 == 0:\n            print ""Processed {} of {}"".format(i, len(ids))\n    \n    masklist_fp.close()\n    filelist_fp.close()\n    print \'write json file\'\n    \n    fp = open(json_path, \'w\')\n    fp.write(json.dumps(lists))\n    fp.close()\n    \n    print \'done!\'\n\nif __name__ == \'__main__\':\n\n    args = parse()\n    processing(args)\n'"
testing/test_pose.py,3,"b""import argparse\nimport os\nimport math\nimport time\nimport numpy as np\nfrom scipy.ndimage.filters import gaussian_filter\nimport sys\nsys.path.append('..')\nimport torch\nimport pose_estimation\nimport cv2\n\nlimbSeq = [[3,4], [4,5], [6,7], [7,8], [9,10], [10,11], [12,13], [13,14], [1,2], [2,9], [2,12], [2,3], [2,6], \\\n           [3,17],[6,18],[1,16],[1,15],[16,18],[15,17]]\n\nmapIdx = [[19,20],[21,22],[23,24],[25,26],[27,28],[29,30],[31,32],[33,34],[35,36],[37,38],[39,40], \\\n          [41,42],[43,44],[45,46],[47,48],[49,50],[51,52],[53,54],[55,56]]\n\ncolors = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0], \\\n          [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255], \\\n          [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85]]\n\nboxsize = 368\nscale_search = [0.5, 1.0, 1.5, 2.0]\nstride = 8\npadValue = 0.\nthre_point = 0.15\nthre_line = 0.05\nstickwidth = 4\n\ndef construct_model(args):\n\n    model = pose_estimation.PoseModel(num_point=19, num_vector=19)\n    state_dict = torch.load(args.model)['state_dict']\n    from collections import OrderedDict\n    new_state_dict = OrderedDict()\n    for k, v in state_dict.items():\n        name = k[7:]\n        new_state_dict[name] = v\n    state_dict = model.state_dict()\n    state_dict.update(new_state_dict)\n    model.load_state_dict(state_dict)\n    model = model.cuda()\n    model.eval()\n\n    return model\n\ndef padRightDownCorner(img, stride, padValue):\n\n    h = img.shape[0]\n    w = img.shape[1]\n\n    pad = 4 * [None]\n    pad[0] = 0 # up\n    pad[1] = 0 # left\n    pad[2] = 0 if (h%stride==0) else stride - (h % stride) # down\n    pad[3] = 0 if (w%stride==0) else stride - (w % stride) # right\n\n    img_padded = img\n    pad_up = np.tile(img_padded[0:1,:,:]*0 + padValue, (pad[0], 1, 1))\n    img_padded = np.concatenate((pad_up, img_padded), axis=0)\n    pad_left = np.tile(img_padded[:,0:1,:]*0 + padValue, (1, pad[1], 1))\n    img_padded = np.concatenate((pad_left, img_padded), axis=1)\n    pad_down = np.tile(img_padded[-2:-1,:,:]*0 + padValue, (pad[2], 1, 1))\n    img_padded = np.concatenate((img_padded, pad_down), axis=0)\n    pad_right = np.tile(img_padded[:,-2:-1,:]*0 + padValue, (1, pad[3], 1))\n    img_padded = np.concatenate((img_padded, pad_right), axis=1)\n\n    return img_padded, pad\n\ndef normalize(origin_img):\n\n\n    origin_img = np.array(origin_img, dtype=np.float32)\n    origin_img -= 128.0\n    origin_img /= 256.0\n\n    return origin_img\n\ndef process(model, input_path):\n\n    origin_img = cv2.imread(input_path)\n    normed_img = normalize(origin_img)\n\n    height, width, _ = normed_img.shape\n\n    multiplier = [x * boxsize / height for x in scale_search]\n\n    heatmap_avg = np.zeros((height, width, 19)) # num_point\n    paf_avg = np.zeros((height, width, 38))     # num_vector\n\n    for m in range(len(multiplier)):\n        scale = multiplier[m]\n\n        # preprocess\n        imgToTest = cv2.resize(normed_img, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC)\n        imgToTest_padded, pad = padRightDownCorner(imgToTest, stride, padValue)\n\n        input_img = np.transpose(imgToTest_padded[:,:,:,np.newaxis], (3, 2, 0, 1)) # required shape (1, c, h, w)\n        mask = np.ones((1, 1, input_img.shape[2] / stride, input_img.shape[3] / stride), dtype=np.float32)\n\n        input_var = torch.autograd.Variable(torch.from_numpy(input_img).cuda())\n        mask_var = torch.autograd.Variable(torch.from_numpy(mask).cuda())\n\n        # get the features\n        vec1, heat1, vec2, heat2, vec3, heat3, vec4, heat4, vec5, heat5, vec6, heat6 = model(input_var, mask_var)\n\n        # get the heatmap\n        heatmap = heat6.data.cpu().numpy()\n        heatmap = np.transpose(np.squeeze(heatmap), (1, 2, 0)) # (h, w, c)\n        heatmap = cv2.resize(heatmap, (0, 0), fx=stride, fy=stride, interpolation=cv2.INTER_CUBIC)\n        heatmap = heatmap[:imgToTest_padded.shape[0] - pad[2], :imgToTest_padded.shape[1] - pad[3], :]\n        heatmap = cv2.resize(heatmap, (width, height), interpolation=cv2.INTER_CUBIC)\n        heatmap_avg = heatmap_avg + heatmap / len(multiplier)\n\n        # get the paf\n        paf = vec6.data.cpu().numpy()\n        paf = np.transpose(np.squeeze(paf), (1, 2, 0)) # (h, w, c)\n        paf = cv2.resize(paf, (0, 0), fx=stride, fy=stride, interpolation=cv2.INTER_CUBIC)\n        paf = paf[:imgToTest_padded.shape[0] - pad[2], :imgToTest_padded.shape[1] - pad[3], :]\n        paf = cv2.resize(paf, (width, height), interpolation=cv2.INTER_CUBIC)\n        paf_avg = paf_avg + paf / len(multiplier)\n\n    all_peaks = []   # all of the possible points by classes.\n    peak_counter = 0\n\n    for part in range(1, 19):\n        map_ori = heatmap_avg[:, :, part]\n        map = gaussian_filter(map_ori, sigma=3)\n\n        map_left = np.zeros(map.shape)\n        map_left[:, 1:] = map[:, :-1]\n        map_right = np.zeros(map.shape)\n        map_right[:, :-1] = map[:, 1:]\n        map_up = np.zeros(map.shape)\n        map_up[1:, :] = map[:-1, :]\n        map_down = np.zeros(map.shape)\n        map_down[:-1, :] = map[1:, :]\n\n        # get the salient point and its score > thre_point\n        peaks_binary = np.logical_and.reduce(\n                (map >= map_left, map >= map_right, map >= map_up, map >= map_down, map > thre_point))\n        peaks = list(zip(np.nonzero(peaks_binary)[1], np.nonzero(peaks_binary)[0])) # (w, h)\n        \n        # a point format: (w, h, score, number)\n        peaks_with_score = [x + (map_ori[x[1], x[0]],) for x in peaks]\n        id = range(peak_counter, peak_counter + len(peaks))\n        peaks_with_score_and_id = [peaks_with_score[i] + (id[i], ) for i in range(len(id))]\n\n        all_peaks.append(peaks_with_score_and_id)\n        peak_counter += len(peaks)\n\n    connection_all = [] # save all of the possible lines by classes.\n    special_k = []      # save the lines, which haven't legal points.\n    mid_num = 10        # could adjust to accelerate (small) or improve accuracy(large).\n\n    for k in range(len(mapIdx)):\n\n        score_mid = paf_avg[:, :, [x - 19 for x in mapIdx[k]]]\n        candA = all_peaks[limbSeq[k][0] - 1]\n        candB = all_peaks[limbSeq[k][1] - 1]\n\n        lenA = len(candA)\n        lenB = len(candB)\n\n        if lenA != 0 and lenB != 0:\n            connection_candidate = []\n            for i in range(lenA):\n                for j in range(lenB):\n                    vec = np.subtract(candB[j][:2], candA[i][:2]) # the vector of BA\n                    norm = math.sqrt(vec[0] * vec[0] + vec[1] * vec[1])\n                    if norm == 0:\n                        continue\n                    vec = np.divide(vec, norm)\n\n                    startend = list(zip(np.linspace(candA[i][0], candB[j][0], num=mid_num), np.linspace(candA[i][1], candB[j][1], num=mid_num)))\n\n                    # get the vector between A and B.\n                    vec_x = np.array([score_mid[int(round(startend[I][1])), int(round(startend[I][0])), 0] for I in range(len(startend))])\n                    vec_y = np.array([score_mid[int(round(startend[I][1])), int(round(startend[I][0])), 1] for I in range(len(startend))])\n\n                    score_midpts = np.multiply(vec_x, vec[0]) + np.multiply(vec_y, vec[1])\n                    score_with_dist_prior = sum(score_midpts) / len(score_midpts) + min(0.5 * height / norm - 1, 0) # ???\n                    criterion1 = len(np.nonzero(score_midpts > thre_line)[0]) > 0.8 * len(score_midpts)\n                    criterion2 = score_with_dist_prior > 0\n                    if criterion1 and criterion2:\n                        connection_candidate.append([i, j, score_with_dist_prior, score_with_dist_prior + candA[i][2] + candB[j][2]])\n\n            # sort the possible line from large to small order.\n            connection_candidate = sorted(connection_candidate, key=lambda x: x[3], reverse=True) # different from openpose, I think there should be sorted by x[3]\n            connection = np.zeros((0, 5))\n\n            for c in range(len(connection_candidate)):\n                i, j, s = connection_candidate[c][0: 3]\n                if (i not in connection[:, 3] and j not in connection[:, 4]):\n                    # the number of A point, the number of B point, score, A point, B point\n                    connection = np.vstack([connection, [candA[i][3], candB[j][3], s, i, j]]) \n                    if len(connection) >= min(lenA, lenB):\n                        break\n            connection_all.append(connection)\n        else:\n            special_k.append(k)\n            connection_all.append([])\n\n    subset = -1 * np.ones((0, 20))\n    candidate = np.array([item for sublist in all_peaks for item in sublist])\n\n    for k in range(len(mapIdx)):\n        if k not in special_k:\n            partAs = connection_all[k][:, 0]\n            partBs = connection_all[k][:, 1]\n            indexA, indexB = np.array(limbSeq[k]) - 1\n\n            for i in range(len(connection_all[k])):\n                found = 0\n                flag = [False, False]\n                subset_idx = [-1, -1]\n                for j in range(len(subset)):\n                    # fix the bug, found == 2 and not joint will lead someone occur more than once.\n                    # if more than one, we choose the subset, which has a higher score.\n                    if subset[j][indexA] == partAs[i]:\n                        if flag[0] == False:\n                            flag[0] = found\n                            subset_idx[found] = j\n                            flag[0] = True\n                            found += 1\n                        else:\n                            ids = subset_idx[flag[0]]\n                            if subset[ids][-1] < subset[j][-1]:\n                                subset_idx[flag[0]] = j\n                    if subset[j][indexB] == partBs[i]:\n                        if flag[1] == False:\n                            flag[1] = found\n                            subset_idx[found] = j\n                            flag[1] = True\n                            found += 1\n                        else:\n                            ids = subset_idx[flag[1]]\n                            if subset[ids][-1] < subset[j][-1]:\n                                subset_idx[flag[1]] = j\n\n                if found == 1:\n                    j = subset_idx[0]\n                    if (subset[j][indexB] != partBs[i]):\n                        subset[j][indexB] = partBs[i]\n                        subset[j][-1] += 1\n                        subset[j][-2] += candidate[partBs[i].astype(int), 2] + connection_all[k][i][2]\n                elif found == 2: # if found equals to 2 and disjoint, merge them\n                    j1, j2 = subset_idx\n                    membership = ((subset[j1] >= 0).astype(int) + (subset[j2] >= 0).astype(int))[:-2]\n                    if len(np.nonzero(membership == 2)[0]) == 0: # merge\n                        subset[j1][:-2] += (subset[j2][:-2] + 1)\n                        subset[j1][-2:] += subset[j2][-2:]\n                        subset[j1][-2] += connection_all[k][i][2]\n                        subset = np.delete(subset, j2, 0)\n                    else: # as like found == 1\n                        subset[j1][indexB] = partBs[i]\n                        subset[j1][-1] += 1\n                        subset[j1][-2] += candidate[partBs[i].astype(int), 2] + connection_all[k][i][2]\n                elif not found and k < 17:\n                    row = -1 * np.ones(20)\n                    row[indexA] = partAs[i]\n                    row[indexB] = partBs[i]\n                    row[-1] = 2\n                    row[-2] = sum(candidate[connection_all[k][i, :2].astype(int), 2]) + connection_all[k][i][2]\n                    subset = np.vstack([subset, row])\n\n    # delete som rows of subset which has few parts occur\n    deleteIdx = []\n    for i in range(len(subset)):\n        if subset[i][-1] < 4 or subset[i][-2] / subset[i][-1] < 0.4:\n            deleteIdx.append(i)\n    subset = np.delete(subset, deleteIdx, axis=0)\n\n    # draw points\n    canvas = cv2.imread(input_path)\n    for i in range(18):\n        for j in range(len(all_peaks[i])):\n            cv2.circle(canvas, all_peaks[i][j][0:2], 4, colors[i], thickness=-1)\n\n    # draw lines\n    for i in range(17):\n        for n in range(len(subset)):\n            index = subset[n][np.array(limbSeq[i]) - 1]\n            if -1 in index:\n                continue\n            cur_canvas = canvas.copy()\n            Y = candidate[index.astype(int), 0]\n            X = candidate[index.astype(int), 1]\n            mX = np.mean(X)\n            mY = np.mean(Y)\n            length = ((X[0] - X[1]) ** 2 + (Y[0] - Y[1]) ** 2) ** 0.5\n            angle = math.degrees(math.atan2(X[0] - X[1], Y[0] - Y[1]))\n            polygon = cv2.ellipse2Poly((int(mY), int(mX)), (int(length / 2), stickwidth), int(angle), 0, 360, 1)\n            cv2.fillConvexPoly(cur_canvas, polygon, colors[i])\n            canvas = cv2.addWeighted(canvas, 0.4, cur_canvas, 0.6, 0)\n\n    return canvas\n\nif __name__ == '__main__':\n\n    os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--image', type=str, required=True, help='input image')\n    parser.add_argument('--output', type=str, default='result.png', help='output image')\n    parser.add_argument('--model', type=str, default='openpose_coco_best.pth.tar', help='path to the weights file')\n\n    args = parser.parse_args()\n    input_image = args.image\n    output = args.output\n\n\n    # load model\n    model = construct_model(args)\n\n    tic = time.time()\n    print('start processing...')\n\n    # generate image with body parts\n    canvas = process(model, input_image)\n\n    toc = time.time()\n    print ('processing time is %.5f' % (toc - tic))\n\n    cv2.imwrite(output, canvas)\n"""
training/train_pose.py,16,"b""import torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport os\nimport sys\nimport argparse\nimport time\nsys.path.append('..')\nimport CocoFolder\nimport Mytransforms \nfrom utils import adjust_learning_rate as adjust_learning_rate\nfrom utils import AverageMeter as AverageMeter\nfrom utils import save_checkpoint as save_checkpoint\nfrom utils import Config as Config\nimport pose_estimation\n\ndef parse():\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--config', type=str,\n                        dest='config', help='to set the parameters')\n    parser.add_argument('--gpu', default=[0], nargs='+', type=int,\n                        dest='gpu', help='the gpu used')\n    parser.add_argument('--pretrained', default=None,type=str,\n                        dest='pretrained', help='the path of pretrained model')\n    parser.add_argument('--root', default=None, type=str,\n                        dest='root', help='the root of images')\n    parser.add_argument('--train_dir', nargs='+', type=str,\n                        dest='train_dir', help='the path of train file')\n    parser.add_argument('--val_dir', default=None, nargs='+', type=str,\n                        dest='val_dir', help='the path of val file')\n    parser.add_argument('--num_classes', default=1000, type=int,\n                        dest='num_classes', help='num_classes (default: 1000)')\n\n    return parser.parse_args()\n\ndef construct_model(args):\n\n    model = pose_estimation.PoseModel(num_point=19, num_vector=19, pretrained=True)\n    # state_dict = torch.load(args.pretrained)['state_dict']\n    # from collections import OrderedDict\n    # new_state_dict = OrderedDict()\n    # for k, v in state_dict.items():\n        # name = k[7:]\n        # new_state_dict[name] = v\n    # model.load_state_dict(new_state_dict)\n    # model.fc = nn.Linear(2048, 80)\n    model = torch.nn.DataParallel(model, device_ids=args.gpu).cuda()\n\n    return model\n\ndef get_parameters(model, config, isdefault=True):\n\n    if isdefault:\n        return model.parameters(), [1.]\n    lr_1 = []\n    lr_2 = []\n    lr_4 = []\n    lr_8 = []\n    params_dict = dict(model.module.named_parameters())\n    for key, value in params_dict.items():\n        if ('model1_' not in key) and ('model0.' not in key):\n            if key[-4:] == 'bias':\n                lr_8.append(value)\n            else:\n                lr_4.append(value)\n        elif key[-4:] == 'bias':\n            lr_2.append(value)\n        else:\n            lr_1.append(value)\n    params = [{'params': lr_1, 'lr': config.base_lr},\n            {'params': lr_2, 'lr': config.base_lr * 2.},\n            {'params': lr_4, 'lr': config.base_lr * 4.},\n            {'params': lr_8, 'lr': config.base_lr * 8.}]\n\n    return params, [1., 2., 4., 8.]\n\ndef train_val(model, args):\n\n    traindir = args.train_dir\n    valdir = args.val_dir\n\n    config = Config(args.config)\n    cudnn.benchmark = True\n    \n    train_loader = torch.utils.data.DataLoader(\n            CocoFolder.CocoFolder(traindir, 8,\n                Mytransforms.Compose([Mytransforms.RandomResized(),\n                Mytransforms.RandomRotate(40),\n                Mytransforms.RandomCrop(368),\n                Mytransforms.RandomHorizontalFlip(),\n            ])),\n            batch_size=config.batch_size, shuffle=True,\n            num_workers=config.workers, pin_memory=True)\n\n    if config.test_interval != 0 and args.val_dir is not None:\n        val_loader = torch.utils.data.DataLoader(\n                CocoFolder.CocoFolder(valdir, 8,\n                    Mytransforms.Compose([Mytransforms.TestResized(368),\n                ])),\n                batch_size=config.batch_size, shuffle=False,\n                num_workers=config.workers, pin_memory=True)\n    \n    criterion = nn.MSELoss().cuda()\n\n    params, multiple = get_parameters(model, config, False)\n    \n    optimizer = torch.optim.SGD(params, config.base_lr, momentum=config.momentum,\n                                weight_decay=config.weight_decay)\n    \n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    losses_list = [AverageMeter() for i in range(12)]\n    top1 = AverageMeter()\n    topk = AverageMeter()\n    \n    end = time.time()\n    iters = config.start_iters\n    best_model = config.best_model\n    learning_rate = config.base_lr\n\n    model.train()\n\n    heat_weight = 46 * 46 * 19 / 2.0 # for convenient to compare with origin code\n    vec_weight = 46 * 46 * 38 / 2.0\n\n    while iters < config.max_iter:\n    \n        for i, (input, heatmap, vecmap, mask) in enumerate(train_loader):\n\n            learning_rate = adjust_learning_rate(optimizer, iters, config.base_lr, policy=config.lr_policy, policy_parameter=config.policy_parameter, multiple=multiple)\n            data_time.update(time.time() - end)\n\n            heatmap = heatmap.cuda(async=True)\n            vecmap = vecmap.cuda(async=True)\n            mask = mask.cuda(async=True)\n            input_var = torch.autograd.Variable(input)\n            heatmap_var = torch.autograd.Variable(heatmap)\n            vecmap_var = torch.autograd.Variable(vecmap)\n            mask_var = torch.autograd.Variable(mask)\n\n            vec1, heat1, vec2, heat2, vec3, heat3, vec4, heat4, vec5, heat5, vec6, heat6 = model(input_var, mask_var)\n            loss1_1 = criterion(vec1, vecmap_var) * vec_weight\n            loss1_2 = criterion(heat1, heatmap_var) * heat_weight\n            loss2_1 = criterion(vec2, vecmap_var) * vec_weight\n            loss2_2 = criterion(heat2, heatmap_var) * heat_weight\n            loss3_1 = criterion(vec3, vecmap_var) * vec_weight\n            loss3_2 = criterion(heat3, heatmap_var) * heat_weight\n            loss4_1 = criterion(vec4, vecmap_var) * vec_weight\n            loss4_2 = criterion(heat4, heatmap_var) * heat_weight\n            loss5_1 = criterion(vec5, vecmap_var) * vec_weight\n            loss5_2 = criterion(heat5, heatmap_var) * heat_weight\n            loss6_1 = criterion(vec6, vecmap_var) * vec_weight\n            loss6_2 = criterion(heat6, heatmap_var) * heat_weight\n            \n            loss = loss1_1 + loss1_2 + loss2_1 + loss2_2 + loss3_1 + loss3_2 + loss4_1 + loss4_2 + loss5_1 + loss5_2 + loss6_1 + loss6_2\n\n            losses.update(loss.data[0], input.size(0))\n            for cnt, l in enumerate([loss1_1, loss1_2, loss2_1, loss2_2, loss3_1, loss3_2, loss4_1, loss4_2, loss5_1, loss5_2, loss6_1, loss6_2]):\n                losses_list[cnt].update(l.data[0], input.size(0))\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n    \n            batch_time.update(time.time() - end)\n            end = time.time()\n    \n            iters += 1\n            if iters % config.display == 0:\n                print('Train Iteration: {0}\\t'\n                    'Time {batch_time.sum:.3f}s / {1}iters, ({batch_time.avg:.3f})\\t'\n                    'Data load {data_time.sum:.3f}s / {1}iters, ({data_time.avg:3f})\\n'\n                    'Learning rate = {2}\\n'\n                    'Loss = {loss.val:.8f} (ave = {loss.avg:.8f})\\n'.format(\n                    iters, config.display, learning_rate, batch_time=batch_time,\n                    data_time=data_time, loss=losses))\n                for cnt in range(0,12,2):\n                    print('Loss{0}_1 = {loss1.val:.8f} (ave = {loss1.avg:.8f})\\t'\n                        'Loss{1}_2 = {loss2.val:.8f} (ave = {loss2.avg:.8f})'.format(cnt / 2 + 1, cnt / 2 + 1, loss1=losses_list[cnt], loss2=losses_list[cnt + 1]))\n                print time.strftime('%Y-%m-%d %H:%M:%S -----------------------------------------------------------------------------------------------------------------\\n', time.localtime())\n\n                batch_time.reset()\n                data_time.reset()\n                losses.reset()\n                for cnt in range(12):\n                    losses_list[cnt].reset()\n    \n            if config.test_interval != 0 and args.val_dir is not None and iters % config.test_interval == 0:\n\n                model.eval()\n                for j, (input, heatmap, vecmap, mask) in enumerate(val_loader):\n\n                    heatmap = heatmap.cuda(async=True)\n                    vecmap = vecmap.cuda(async=True)\n                    mask = mask.cuda(async=True)\n                    input_var = torch.autograd.Variable(input, volatile=True)\n                    heatmap_var = torch.autograd.Variable(heatmap, volatile=True)\n                    vecmap_var = torch.autograd.Variable(vecmap, volatile=True)\n                    mask_var = torch.autograd.Variable(mask, volatile=True)\n\n                    vec1, heat1, vec2, heat2, vec3, heat3, vec4, heat4, vec5, heat5, vec6, heat6 = model(input_var, mask_var)\n                    loss1_1 = criterion(vec1, vecmap_var) * vec_weight\n                    loss1_2 = criterion(heat1, heatmap_var) * heat_weight\n                    loss2_1 = criterion(vec2, vecmap_var) * vec_weight\n                    loss2_2 = criterion(heat2, heatmap_var) * heat_weight\n                    loss3_1 = criterion(vec3, vecmap_var) * vec_weight\n                    loss3_2 = criterion(heat3, heatmap_var) * heat_weight\n                    loss4_1 = criterion(vec4, vecmap_var) * vec_weight\n                    loss4_2 = criterion(heat4, heatmap_var) * heat_weight\n                    loss5_1 = criterion(vec5, vecmap_var) * vec_weight\n                    loss5_2 = criterion(heat5, heatmap_var) * heat_weight\n                    loss6_1 = criterion(vec6, vecmap_var) * vec_weight\n                    loss6_2 = criterion(heat6, heatmap_var) * heat_weight\n                    \n                    loss = loss1_1 + loss1_2 + loss2_1 + loss2_2 + loss3_1 + loss3_2 + loss4_1 + loss4_2 + loss5_1 + loss5_2 + loss6_1 + loss6_2\n\n                    losses.update(loss.data[0], input.size(0))\n                    for cnt, l in enumerate([loss1_1, loss1_2, loss2_1, loss2_2, loss3_1, loss3_2, loss4_1, loss4_2, loss5_1, loss5_2, loss6_1, loss6_2]):\n                        losses_list[cnt].update(l.data[0], input.size(0))\n    \n                batch_time.update(time.time() - end)\n                end = time.time()\n                is_best = losses.avg < best_model\n                best_model = min(best_model, losses.avg)\n                save_checkpoint({\n                    'iter': iters,\n                    'state_dict': model.state_dict(),\n                    }, is_best, 'openpose_coco')\n    \n                print(\n                    'Test Time {batch_time.sum:.3f}s, ({batch_time.avg:.3f})\\t'\n                    'Loss {loss.avg:.8f}\\n'.format(\n                    batch_time=batch_time, loss=losses))\n                for cnt in range(0,12,2):\n                    print('Loss{0}_1 = {loss1.val:.8f} (ave = {loss1.avg:.8f})\\t'\n                        'Loss{1}_2 = {loss2.val:.8f} (ave = {loss2.avg:.8f})'.format(cnt / 2 + 1, cnt / 2 + 1, loss1=losses_list[cnt], loss2=losses_list[cnt + 1]))\n                print time.strftime('%Y-%m-%d %H:%M:%S -----------------------------------------------------------------------------------------------------------------\\n', time.localtime())\n    \n                batch_time.reset()\n                losses.reset()\n                for cnt in range(12):\n                    losses_list[cnt].reset()\n                \n                model.train()\n    \n            if iters == config.max_iter:\n                break\n\n\nif __name__ == '__main__':\n\n    os.environ['CUDA_VISIBLE_DEVICES'] = '2,3'\n    args = parse()\n    model = construct_model(args)\n    train_val(model, args)\n"""
