file_path,api_count,code
test.py,1,"b""import os\nimport numpy as np\nfrom collections import OrderedDict\nfrom options.test_options import TestOptions\nfrom data.data_loader import CreateDataLoader\nfrom models.models import create_model\nimport utils.util as util\nfrom utils.visualizer import Visualizer\nfrom utils import html\nfrom torch.autograd import Variable\n\nopt = TestOptions().parse(save=False)\nopt.nThreads = 1   \nopt.batchSize = 1  \nopt.serial_batches = True  # no shuffle\n\ndata_loader = CreateDataLoader(opt)\ndataset, _ = data_loader.load_data()\nmodel = create_model(opt,data_loader.dataset)\nvisualizer = Visualizer(opt)\n# create website\nweb_dir = os.path.join(opt.results_dir, opt.name, '%s_%s' % (opt.phase, opt.which_epoch))\nwebpage = html.HTML(web_dir, '%s: %s' % (opt.name, pt.which_epoch))\n# test\n\n\nlabel_trues, label_preds = [], []\n\nmodel.model.eval()\ntic = time.time()\n\naccs=[]\nfor i, data in enumerate(dataset):\n    if i >= opt.how_many and opt.how_many!=0:\n        break\n    seggt, segpred = model.forward(data,False)\n    print time.time() - tic\n    tic = time.time()\n\n    seggt = seggt.data.cpu().numpy()\n    segpred = segpred.data.cpu().numpy()\n\n    label_trues.append(seggt)\n    label_preds.append(segpred)\n\n    visuals = model.get_visuals(i)\n    img_path = data['imgpath']\n    print('process image... %s' % img_path)\n    visualizer.save_images(webpage, visuals, img_path)\n\nmetrics0 = util.label_accuracy_score(\n    label_trues, label_preds, n_class=opt.label_nc, returniu=True)\nmetrics = np.array(metrics0[:4])\nmetrics *= 100\nprint('''\\\n        Accuracy: {0}\n        Accuracy Class: {1}\n        Mean IU: {2}\n        FWAV Accuracy: {3}'''.format(*metrics))\n\nwebpage.save()\n"""
test_ops.py,15,"b'import torch\nimport torch.nn as nn\nfrom torch.nn.modules.utils import _single, _pair, _triple\nfrom torch.autograd import Variable\n\nfrom utils.gradcheck import gradcheck\nfrom models.ops.depthconv.functions.depthconv import DepthconvFunction\n\n\nN, inC, inH, inW = 4, 2, 8, 8\nkH, kW = 3, 3\npad, stride, dilation = 0, 1, 1\n\noffC = 1 * 2 * kH * kW\n\noutC = 1\noutH = (inH + 2 * pad - (dilation * (kH - 1) + 1)) // stride + 1\noutW = (inW + 2 * pad - (dilation * (kW - 1) + 1)) // stride + 1\n\nconv_offset2d = DepthconvFunction(\n        padding=(pad, pad),\n        stride=(stride, stride),\n        dilation=(dilation, dilation), bias=True)\nconv2d = F.ConvNd(_pair(stride), _pair(pad), _pair(dilation), False,\n               _pair(0), 1, torch.backends.cudnn.benchmark, torch.backends.cudnn.enabled)\noffset = Variable(\n        torch.ones(N, 1, inH, inW).cuda(),\n        requires_grad=False)\ninput = Variable(\n        torch.rand(N, inC, inH, inW).cuda(),\n        requires_grad=True)\ninput2 = Variable(input.data.clone(),\n        requires_grad=True)\nweight = Variable(\n        10*torch.rand(outC, inC, kH, kW).cuda(),\n        requires_grad=True)\nweight2 = Variable(weight.data.clone(),\n        requires_grad=True)\nbias = Variable(torch.rand(outC).cuda(),requires_grad=True)\nbias2 = Variable(bias.data.clone(),\n        requires_grad=True)\ngrad = Variable(\n        torch.rand(N, outC, 6, 6).cuda(),\n        requires_grad=True)\n\nprint bias\nout1 = conv_offset2d(input, offset, weight, bias)\nout2 = conv2d(input2, weight2, bias2)\nprint (out1-out2).sum()\n\nout1.backward(grad)\nout2.backward(grad)\n\n\nprint (weight.grad-weight2.grad).sum()\nprint (\'input.grad\',input.grad.sum())\nprint (\'input.grad\',input2.grad.sum())\nprint (input.grad-input2.grad).sum()\nprint (bias.grad-bias2.grad).sum()\n\n\n# print bias.data.cpu().numpy().dtype\n\n# print(""pass gradcheck: {}"".format(gradcheck(conv_offset2d, (input, offset, weight, bias))))\n# print(""pass gradcheck: {}"".format(gradcheck(conv2d, (input, weight,None))))\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom models.ops.depthavgpooling.functions.depthavgpooling import DepthavgpoolingFunction\nfrom models.ops.depthavgpooling.modules import Depthavgpooling\nfrom torch.autograd import Variable\n\ndepth = [[[1,0,1,10000],\n         [0,1,10000,1],\n         [1,0,1,0],\n         [0,1,0,1]],\n         ]\ndepth = np.zeros([40,40])\ninputarray = torch.Tensor(np.asarray(range(2*40*40)).reshape([1,2,40,40]))\ndepth = torch.Tensor(np.asarray(depth).reshape([1,1,40,40]))\n\nprint inputarray\nN, inC, inH, inW = 4, 512, 50, 65\ninput = Variable(\n        inputarray,\n        requires_grad=True).cuda()\ndepth = Variable(\n    depth,\n        requires_grad=True).cuda()\nkH, kW = 3, 3\npad, stride, dilation = 1, 1, 1\ndepthpooling = Depthavgpooling(kH,stride,pad)\npooling = nn.AvgPool2d(kernel_size=kH, stride=stride,padding=pad)\n\nout1 = depthpooling(input, depth)\nout2 = pooling(input)\n\ngrad = Variable(\n        torch.ones(N, 2, 40, 40).cuda(),\n        requires_grad=True)\nout1.backward(grad)\n\nprint out1-out2\n'"
train.py,1,"b""import time\nfrom tensorboardX import SummaryWriter\nfrom collections import OrderedDict\nfrom options.train_options import TrainOptions\nfrom data.data_loader import CreateDataLoader\nfrom models.models import create_model\nimport utils.util as util\nfrom utils.visualizer import Visualizer\nimport os\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\nimport time\n\nopt = TrainOptions().parse()\niter_path = os.path.join(opt.checkpoints_dir, opt.name, 'iter.txt')\nioupath_path = os.path.join(opt.checkpoints_dir, opt.name, 'MIoU.txt')\nif opt.continue_train:\n    try:\n        start_epoch, epoch_iter = np.loadtxt(iter_path, delimiter=',', dtype=int)\n    except:\n        start_epoch, epoch_iter = 1, 0\n        \n    try:\n        best_iou = np.loadtxt(ioupath_path, dtype=float)\n    except:\n        best_iou = 0.\n    print('Resuming from epoch %d at iteration %d, previous best IoU %f' % (start_epoch, epoch_iter, best_iou))\nelse:\n    start_epoch, epoch_iter = 1, 0\n    best_iou = 0.\n\ndata_loader = CreateDataLoader(opt)\ndataset, dataset_val = data_loader.load_data()\ndataset_size = len(dataset)\nprint('#training images = %d' % dataset_size)\n\nmodel = create_model(opt, dataset.dataset)\n# print (model)\nvisualizer = Visualizer(opt)\ntotal_steps = (start_epoch - 1) * dataset_size + epoch_iter\nfor epoch in range(start_epoch, opt.nepochs):\n    epoch_start_time = time.time()\n    if epoch != start_epoch:\n        epoch_iter = epoch_iter % dataset_size\n\n    model.model.train()\n    for i, data in enumerate(dataset, start=epoch_iter):\n        iter_start_time = time.time()\n        total_steps += opt.batchSize\n        epoch_iter += opt.batchSize\n\n        ############## Forward and Backward Pass ######################\n        model.forward(data)\n        model.backward(total_steps, opt.nepochs * dataset.__len__() * opt.batchSize + 1)\n\n        ############## update tensorboard and web images ######################\n        if total_steps % opt.display_freq == 0:\n            visuals = model.get_visuals(total_steps)\n            visualizer.display_current_results(visuals, epoch, total_steps)\n\n        ############## Save latest Model   ######################\n        if total_steps % opt.save_latest_freq == 0:\n            print('saving the latest model (epoch %d, total_steps %d)' % (epoch, total_steps))\n            model.save('latest')\n            np.savetxt(iter_path, (epoch, epoch_iter), delimiter=',', fmt='%d')\n        # print time.time()-iter_start_time\n\n    # end of epoch\n    model.model.eval()\n    if dataset_val!=None:\n        label_trues, label_preds = [], []\n        for i, data in enumerate(dataset_val):\n            seggt, segpred = model.forward(data,False)\n            seggt = seggt.data.cpu().numpy()\n            segpred = segpred.data.cpu().numpy()\n\n            label_trues.append(seggt)\n            label_preds.append(segpred)\n\n        metrics = util.label_accuracy_score(\n            label_trues, label_preds, n_class=opt.label_nc)\n        metrics = np.array(metrics)\n        metrics *= 100\n        print('''\\\n                Validation:\n                Accuracy: {0}\n                Accuracy Class: {1}\n                Mean IU: {2}\n                FWAV Accuracy: {3}'''.format(*metrics))\n        model.update_tensorboard(metrics,total_steps)\n    iter_end_time = time.time()\n\n    print('End of epoch %d / %d \\t Time Taken: %d sec' %\n          (epoch+1, opt.nepochs, time.time() - epoch_start_time))\n    if metrics[2]>best_iou:\n        best_iou = metrics[2]\n        print('saving the model at the end of epoch %d, iters %d, loss %f' % (epoch, total_steps, model.trainingavgloss))\n        model.save('best')\n\n    ### save model for this epoch\n    if epoch % opt.save_epoch_freq == 0:\n        print('saving the model at the end of epoch %d, iters %d, loss %f' % (epoch, total_steps, model.trainingavgloss))\n        model.save('latest')\n        model.save(epoch)\n        np.savetxt(iter_path, (epoch + 1, 0), delimiter=',', fmt='%d')\n\n"""
data/VOC_dataset.py,0,"b'import os.path\nimport numpy as np\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch\nimport h5py\nimport time\nfrom data.base_dataset import *\nfrom PIL import Image\nimport math, random\n\n\ndef make_dataset_fromlst(listfilename):\n    """"""\n    NYUlist format:\n    imagepath seglabelpath depthpath HHApath\n    """"""\n    images = []\n    segs = []\n\n    with open(listfilename) as f:\n        content = f.readlines()\n        for x in content:\n            imgname, segname = x.strip().split(\' \')\n            images += [imgname]\n            segs += [segname]\n\n    return {\'images\':images, \'segs\':segs}\n\nclass VOCDataset(BaseDataset):\n    def initialize(self, opt):\n        self.opt = opt\n        np.random.seed(8964)\n        self.paths_dict = make_dataset_fromlst(opt.list)\n        self.len = len(self.paths_dict[\'images\'])\n        self.datafile = \'VOC_dataset.py\'\n\n    def __getitem__(self, index):\n        #self.paths[\'images\'][index]\n        # print self.opt.scale,self.opt.flip,self.opt.crop,self.opt.colorjitter\n        img = np.asarray(Image.open(self.paths_dict[\'images\'][index]))\n        seg = np.asarray(Image.open(self.paths_dict[\'segs\'][index])).astype(np.uint8)\n        # print(np.unique(seg))\n\n        params = get_params(self.opt, seg.shape)\n        seg_tensor_tranformed = transform(seg, params, normalize=False,method=\'nearest\',istrain=self.opt.isTrain)\n        if self.opt.inputmode == \'bgr-mean\':\n            img_tensor_tranformed = transform(img, params, normalize=False, istrain=self.opt.isTrain, option=1)\n        else:\n            img_tensor_tranformed = transform(img, params, istrain=self.opt.isTrain, option=1)\n        return {\'image\':img_tensor_tranformed,\n                \'seg\': seg_tensor_tranformed,\n                \'imgpath\': self.paths_dict[\'segs\'][index]}\n\n    def __len__(self):\n        return self.len\n\n    def name(self):\n        return \'VOCDataset\'\n\nclass VOCDataset_val(BaseDataset):\n    def initialize(self, opt):\n        self.opt = opt\n        self.paths_dict = make_dataset_fromlst(opt.vallist)\n        self.len = len(self.paths_dict[\'images\'])\n\n    def __getitem__(self, index):\n        img = np.asarray(Image.open(self.paths_dict[\'images\'][index]))#.astype(np.uint8)\n        seg = np.asarray(Image.open(self.paths_dict[\'segs\'][index])).astype(np.uint8)\n\n        params = get_params(self.opt, seg.shape, test=True)\n        seg_tensor_tranformed = transform(seg, params, normalize=False,method=\'nearest\',istrain=self.opt.isTrain)\n        if self.opt.inputmode == \'bgr-mean\':\n            img_tensor_tranformed = transform(img, params, normalize=False, istrain=self.opt.isTrain, option=1)\n        else:\n            img_tensor_tranformed = transform(img, params, istrain=self.opt.isTrain, option=1)\n\n        return {\'image\':img_tensor_tranformed,\n                \'seg\': seg_tensor_tranformed,\n                \'imgpath\': self.paths_dict[\'segs\'][index]}\n\n    def __len__(self):\n        return self.len\n\n    def name(self):\n        return \'VOCDataset_val\'\n\n\n'"
data/__init__.py,0,b''
data/base_data_loader.py,0,"b'\nclass BaseDataLoader():\n    def __init__(self):\n        pass\n    \n    def initialize(self, opt):\n        self.opt = opt\n        pass\n\n    def load_data():\n        return None\n\n        \n        \n'"
data/base_dataset.py,3,"b""import torch.utils.data as data\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport numpy as np\nimport torch\nimport cv2\nimport random\n\nclass BaseDataset(data.Dataset):\n    def __init__(self):\n        super(BaseDataset, self).__init__()\n\n    def name(self):\n        return 'BaseDataset'\n\n    def initialize(self, opt):\n        pass\n\ndef get_params(opt, size, test=False):\n    h, w = size\n    if opt.scale and test==False:\n        scale = random.uniform(0.76, 1.75)\n        new_h = h * scale\n        new_w = (new_h * w // h)\n\n        new_h = int(round(new_h / 8) * 8)\n        new_w = int(round(new_w / 8) * 8)\n\n    else:\n        new_h = h\n        new_w = w\n        # new_h = int(round(h / 8) * 8)\n        # new_w = int(round(w / 8) * 8)\n\n    if opt.flip and test==False:\n        flip = random.random() > 0.5\n    else:\n        flip = False\n\n    crop = False\n    x1 = x2 = y1 = y2 = 0\n    if opt.crop and test==False:\n        # if new_h > 241 and new_w > 321: #424\n        if opt.batchSize > 1:\n            cropsizeh = 321\n            cropsizew = 421#(cropsizeh * new_w // new_h)\n        else:\n            cropscale = random.uniform(0.6,.9)\n            cropsizeh = int (new_h * cropscale)\n            cropsizew = int (new_w * cropscale)\n            # print cropsizeh,cropsizew,new_h,new_w\n        x1 = random.randint(0, np.maximum(0, new_w - cropsizew))\n        y1 = random.randint(0, np.maximum(0, new_h - cropsizeh))\n        x2 = x1 + cropsizew -1\n        y2 = y1 + cropsizeh -1\n        crop = True\n\n        # if opt.batchSize > 1:\n        #     print cropsizew,cropsizeh\n    if opt.colorjitter and test==False:\n        colorjitter = True\n    else:\n        colorjitter = False\n    return {'scale': (new_w, new_h),\n            'flip': flip,\n            'crop_pos': (x1, x2, y1, y2),\n            'crop': crop,\n            'colorjitter': colorjitter}\n\ndef get_params_sunrgbd(opt, size, test=False, maxcrop=0.8, maxscale=1.75):\n    h, w = size\n    if opt.scale and test==False:\n        scale = random.uniform(0.76, maxscale)\n        new_h = h * scale\n        new_w = (new_h * w // h)\n\n        new_h = int(round(new_h / 8) * 8)\n        new_w = int(round(new_w / 8) * 8)\n\n    else:\n        new_h = h\n        new_w = w\n        # new_h = int(round(h / 8) * 8)\n        # new_w = int(round(w / 8) * 8)\n\n    if opt.flip and test==False:\n        flip = random.random() > 0.5\n    else:\n        flip = False\n\n    crop = False\n    x1 = x2 = y1 = y2 = 0\n    if opt.crop and test==False:\n        # if new_h > 241 and new_w > 321: #424\n        if opt.batchSize > 1:\n            cropsizeh = 321\n            cropsizew = 421#(cropsizeh * new_w // new_h)\n        else:\n            cropscale = random.uniform(0.6,maxcrop)\n            cropsizeh = int (new_h * cropscale)\n            cropsizew = int (new_w * cropscale)\n            # print cropsizeh,cropsizew,new_h,new_w\n        x1 = random.randint(0, np.maximum(0, new_w - cropsizew))\n        y1 = random.randint(0, np.maximum(0, new_h - cropsizeh))\n        x2 = x1 + cropsizew -1\n        y2 = y1 + cropsizeh -1\n        crop = True\n\n        # if opt.batchSize > 1:\n        #     print cropsizew,cropsizeh\n    if opt.colorjitter and test==False:\n        colorjitter = True\n    else:\n        colorjitter = False\n    return {'scale': (new_w, new_h),\n            'flip': flip,\n            'crop_pos': (x1, x2, y1, y2),\n            'crop': crop,\n            'colorjitter': colorjitter}\n\ndef transform(numpyarray, params, normalize=True, method='linear', istrain=True, colorjitter=False, option=0):\n    # print params['crop'],params['colorjitter'],params['flip']\n    if method == 'linear':\n        numpyarray = cv2.resize(numpyarray, (params['scale'][0], params['scale'][1]), interpolation=cv2.INTER_LINEAR)\n    else:\n        numpyarray = cv2.resize(numpyarray, (params['scale'][0], params['scale'][1]), interpolation=cv2.INTER_NEAREST)\n\n    if istrain:\n        if params['crop']:\n            # print (numpyarray.shape,params['crop_pos'])\n            numpyarray = numpyarray[params['crop_pos'][2]:params['crop_pos'][3],\n                                    params['crop_pos'][0]:params['crop_pos'][1],\n                                    ...]\n        if params['flip']:\n            numpyarray = numpyarray[:,\n                                    ::-1,\n                                    ...]\n\n        if option==1:\n            if colorjitter and params['colorjitter'] and random.random() > 0.1:\n                # numpyarray += np.random.rand() * 30 - 15\n                # numpyarray[numpyarray > 255] = 255\n                # numpyarray[numpyarray < 0] = 0\n                hsv = cv2.cvtColor(numpyarray, cv2.COLOR_BGR2HSV)\n                hsv[:, :, 0] += np.random.rand() * 70 - 35\n                hsv[:, :, 1] += np.random.rand() * 0.3 - 0.15\n                hsv[:, :, 2] += np.random.rand() * 50 - 25\n                hsv[:, :, 0] = np.clip(hsv[:, :, 0], 0, 360.)\n                hsv[:, :, 1] = np.clip(hsv[:, :, 1], 0, 1.)\n                hsv[:, :, 2] = np.clip(hsv[:, :, 2], 0, 255.)\n                numpyarray = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n                # print numpyarray.shape\n\n    if option == 1:\n        if not normalize:\n            numpyarray = numpyarray - np.asarray([122.675,116.669,104.008])\n            numpyarray = numpyarray.transpose((2, 0, 1))[::-1,:,:].astype(np.float32)\n        else:\n            numpyarray = numpyarray.transpose((2, 0, 1)).astype(np.float32)/255.\n\n    if option == 2:\n        if not normalize:\n            numpyarray = numpyarray - np.asarray([132.431, 94.076, 118.477])\n            numpyarray = numpyarray.transpose((2, 0, 1))[::-1,:,:].astype(np.float32)\n        else:\n            numpyarray = numpyarray.transpose((2, 0, 1)).astype(np.float32)/255.\n\n    if len(numpyarray.shape) == 3:\n        torchtensor = torch.from_numpy(numpyarray.copy()).float()#.div(255)\n    else:\n        torchtensor = torch.from_numpy(np.expand_dims(numpyarray,axis=0).copy())\n\n    if normalize:\n        # torchtensor = torchtensor.div(255)\n        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                         std=[0.229, 0.224, 0.225])\n        torchtensor = normalize(torchtensor)\n\n    return torchtensor\n\n\n"""
data/custom_dataset_data_loader.py,3,"b'import torch.utils.data\nfrom .base_data_loader import BaseDataLoader\n\n\ndef CreateDataset(opt):\n    dataset = None\n    if opt.dataset_mode == \'nyuv2\':\n        # from data.nyuv2_dataset import NYUDataset\n        from data.nyuv2_dataset_crop import NYUDataset,NYUDataset_val\n        dataset = NYUDataset()\n        if opt.vallist!=\'\':\n            dataset_val = NYUDataset_val()\n        else:\n            dataset_val = None\n    elif opt.dataset_mode == \'voc\':\n        from data.VOC_dataset import VOCDataset,VOCDataset_val\n        dataset = VOCDataset()\n        if opt.vallist!=\'\':\n            dataset_val = VOCDataset_val()\n        else:\n            dataset_val = None\n\n    elif opt.dataset_mode == \'sunrgbd\':\n        from data.sunrgbd_dataset import SUNRGBDDataset,SUNRGBDDataset_val\n        dataset = SUNRGBDDataset()\n        if opt.vallist!=\'\':\n            dataset_val = SUNRGBDDataset_val()\n        else:\n            dataset_val = None\n\n    elif opt.dataset_mode == \'stanfordindoor\':\n        from data.stanfordindoor_dataset import StanfordIndoorDataset, StanfordIndoorDataset_val\n        dataset = StanfordIndoorDataset()\n        if opt.vallist!=\'\':\n            dataset_val = StanfordIndoorDataset_val()\n        else:\n            dataset_val = None\n\n    print(""dataset [%s] was created"" % (dataset.name()))\n    dataset.initialize(opt)\n    if dataset_val != None:\n        dataset_val.initialize(opt)\n    return dataset,dataset_val\n\nclass CustomDatasetDataLoader(BaseDataLoader):\n    def name(self):\n        return \'CustomDatasetDataLoader\'\n\n    def initialize(self, opt):\n        BaseDataLoader.initialize(self, opt)\n        self.dataset, self.dataset_val = CreateDataset(opt)\n        self.dataloader = torch.utils.data.DataLoader(\n            self.dataset,\n            batch_size=opt.batchSize,\n            shuffle=not opt.serial_batches,\n            num_workers=int(opt.nThreads))\n        if self.dataset_val != None:\n            self.dataloader_val = torch.utils.data.DataLoader(\n                self.dataset_val,\n                batch_size=1,\n                shuffle=False,\n                num_workers=int(opt.nThreads))\n        else:\n            self.dataloader_val = None\n\n\n    def load_data(self):\n        return self.dataloader, self.dataloader_val\n\n    def __len__(self):\n        return min(len(self.dataset), self.opt.max_dataset_size)\n'"
data/data_loader.py,0,b'\ndef CreateDataLoader(opt):\n    from data.custom_dataset_data_loader import CustomDatasetDataLoader\n    data_loader = CustomDatasetDataLoader()\n    print(data_loader.name())\n    data_loader.initialize(opt)\n    return data_loader\n\n'
data/nyuv2_dataset.py,5,"b'import os\nimport random\nimport numpy as np\nimport torch\nimport torch.utils.data as torchdata\nfrom torchvision import transforms\nfrom scipy.misc import imread, imresize\n\n\ndef make_dataset_fromlst(listfilename):\n    """"""\n    NYUlist format:\n    imagepath seglabelpath depthpath HHApath\n    """"""\n    images = []\n    segs = []\n    depths = []\n    HHAs = []\n\n    with open(listfilename) as f:\n        content = f.readlines()\n        for x in content:\n            imgname, segname, depthname, HHAname = x.strip().split(\' \')\n            images += [imgname]\n            segs += [segname]\n            depths += [depthname]\n            HHAs += [HHAname]\n\n    return {\'images\':images, \'segs\':segs, \'HHAs\':HHAs, \'depths\':depths}\n\nclass Dataset(torchdata.Dataset):\n    def __init__(self, txt, opt, max_sample=-1, is_train=1):\n        self.root_img = opt.root_img\n        self.root_seg = opt.root_seg\n        self.imgSize = opt.imgSize\n        self.segSize = opt.segSize\n        self.is_train = is_train\n\n        # mean and std\n        self.img_transform = transforms.Compose([\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])])\n\n        # self.list_sample = [x.rstrip() for x in open(txt, \'r\')]\n\n        self.list_sample = make_dataset_fromlst(txt)\n        # if self.is_train:\n        #     random.shuffle(self.list_sample)\n        # if max_sample > 0:\n        #     self.list_sample = self.list_sample[0:max_sample]\n        num_sample = len(self.list_sample[\'images\'])\n        assert num_sample > 0\n        print(\'# samples: {}\'.format(num_sample))\n\n    def _scale_and_crop(self, img, seg, cropSize, is_train):\n        h, w = img.shape[0], img.shape[1]\n\n        if is_train:\n            # random scale\n            scale = random.random() + 0.5     # 0.5-1.5\n            scale = max(scale, 1. * cropSize / (min(h, w) - 1))\n        else:\n            # scale to crop size\n            scale = 1. * cropSize / (min(h, w) - 1)\n\n        img_scale = imresize(img, scale, interp=\'bilinear\')\n        seg_scale = imresize(seg, scale, interp=\'nearest\')\n\n        h_s, w_s = img_scale.shape[0], img_scale.shape[1]\n        if is_train:\n            # random crop\n            x1 = random.randint(0, w_s - cropSize)\n            y1 = random.randint(0, h_s - cropSize)\n        else:\n            # center crop\n            x1 = (w_s - cropSize) // 2\n            y1 = (h_s - cropSize) // 2\n\n        img_crop = img_scale[y1: y1 + cropSize, x1: x1 + cropSize, :]\n        seg_crop = seg_scale[y1: y1 + cropSize, x1: x1 + cropSize]\n        return img_crop, seg_crop\n\n    def _flip(self, img, seg):\n        img_flip = img[:, ::-1, :]\n        seg_flip = seg[:, ::-1]\n        return img_flip, seg_flip\n\n    def __getitem__(self, index):\n        img_basename = self.list_sample[\'images\'][index]#self.list_sample[index]\n        path_img = img_basename#os.path.join(self.root_img, img_basename)\n        path_seg = self.list_sample[\'depths\'][index]#os.path.join(self.root_seg,self.paths_dict[\'depths\'][index])\n                                # img_basename.replace(\'.jpg\', \'.png\'))\n\n        assert os.path.exists(path_img), \'[{}] does not exist\'.format(path_img)\n        assert os.path.exists(path_seg), \'[{}] does not exist\'.format(path_seg)\n\n        # load image and label\n        try:\n            img = imread(path_img, mode=\'RGB\')\n            seg = imread(path_seg)\n            assert(img.ndim == 3)\n            assert(seg.ndim == 2)\n            assert(img.shape[0] == seg.shape[0])\n            assert(img.shape[1] == seg.shape[1])\n\n            # random scale, crop, flip\n            if self.imgSize > 0:\n                img, seg = self._scale_and_crop(img, seg,\n                                                self.imgSize, self.is_train)\n                if random.choice([-1, 1]) > 0:\n                    img, seg = self._flip(img, seg)\n\n            # image to float\n            img = img.astype(np.float32) / 255.\n            img = img.transpose((2, 0, 1))\n\n            if self.segSize > 0:\n                seg = imresize(seg, (self.segSize, self.segSize),\n                               interp=\'nearest\')\n\n            # label to int from -1 to 149\n            seg = seg.astype(np.int) - 1\n\n            # to torch tensor\n            image = torch.from_numpy(img)\n            segmentation = torch.from_numpy(seg)\n        except Exception as e:\n            print(\'Failed loading image/segmentation [{}]: {}\'\n                  .format(path_img, e))\n            # dummy data\n            image = torch.zeros(3, self.imgSize, self.imgSize)\n            segmentation = -1 * torch.ones(self.segSize, self.segSize).long()\n            return image, segmentation, img_basename\n\n        # substracted by mean and divided by std\n        image = self.img_transform(image)\n\n        return image, segmentation, img_basename\n\n    def __len__(self):\n        return len(self.list_sample[\'images\'])\n'"
data/nyuv2_dataset_crop.py,1,"b'import os.path\nimport numpy as np\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch\nimport h5py\nfrom data.base_dataset import *\nfrom PIL import Image\nimport math, random\nimport time\n\nlabel_weight = [0.005770029194127712, 0.012971614093310078, 0.03362765598112945, 0.1221253676849356, 0.06859890961300749, 0.15823995906267385, 0.09602253559800432, 0.12810205801896177, 0.1718342979655409, 0.2830090542974214, 0.06808788822945917, 0.28288925581409397, 0.30927228790865696, 0.6046432911319981, 0.7276073719428268, 0.6584037740058684, 1.6161287361233052, 0.4147706187681264, 0.8706942889933341, 0.8146644289372541, 0.8744887302745185, 0.25134887482271207, 0.3527236656093415, 1.9965490899244573, 3.453731279765878, 0.603116521402235, 1.6573996378194742, 21.603576890926714, 1.3738455233450662, 11.13489209800063, 7.110616094064334, 3.5123361407056404, 8.061760999036036, 1.5451820155073996, 0.9412019674579293, 9.351917523626016, 0.8485119225668366, 0.09619406694759904, 0.07387533823120886, 0.019189673545819297]\ndef make_dataset_fromlst(listfilename):\n    """"""\n    NYUlist format:\n    imagepath seglabelpath depthpath HHApath\n    """"""\n    images = []\n    segs = []\n    depths = []\n    HHAs = []\n\n    with open(listfilename) as f:\n        content = f.readlines()\n        for x in content:\n            imgname, segname, depthname, HHAname = x.strip().split(\' \')\n            images += [imgname]\n            segs += [segname]\n            depths += [depthname]\n            HHAs += [HHAname]\n\n    return {\'images\':images, \'segs\':segs, \'HHAs\':HHAs, \'depths\':depths}\n\nclass NYUDataset(BaseDataset):\n    def initialize(self, opt):\n        self.opt = opt\n        np.random.seed(int(time.time()))\n        self.paths_dict = make_dataset_fromlst(opt.list)\n        self.len = len(self.paths_dict[\'images\'])\n        self.label_weight = torch.Tensor(label_weight)\n        self.datafile = \'nyuv2_dataset_crop.py\'\n\n    def __getitem__(self, index):\n        #self.paths[\'images\'][index]\n        # print self.opt.scale,self.opt.flip,self.opt.crop,self.opt.colorjitter\n        img = np.asarray(Image.open(self.paths_dict[\'images\'][index]))#.astype(np.uint8)\n        depth = np.asarray(Image.open(self.paths_dict[\'depths\'][index])).astype(np.float32)/120. # 1/10 * depth\n\n        HHA = np.asarray(Image.open(self.paths_dict[\'HHAs\'][index]))\n        seg = np.asarray(Image.open(self.paths_dict[\'segs\'][index])).astype(np.uint8)\n\n\n        params = get_params(self.opt, seg.shape)\n        depth_tensor_tranformed = transform(depth, params, normalize=False,istrain=self.opt.isTrain)\n        seg_tensor_tranformed = transform(seg, params, normalize=False,method=\'nearest\',istrain=self.opt.isTrain)\n        if self.opt.inputmode == \'bgr-mean\':\n            img_tensor_tranformed = transform(img, params, normalize=False, istrain=self.opt.isTrain, option=1)\n            HHA_tensor_tranformed = transform(HHA, params, normalize=False, istrain=self.opt.isTrain, option=2)\n        else:\n            img_tensor_tranformed = transform(img, params, istrain=self.opt.isTrain, option=1)\n            HHA_tensor_tranformed = transform(HHA, params, istrain=self.opt.isTrain, option=2)\n\n\n        # print img_tensor_tranformed\n        # print(np.unique(depth_tensor_tranformed.numpy()).shape)\n        # print img_tensor_tranformed.size()\n        return {\'image\':img_tensor_tranformed,\n                \'depth\':depth_tensor_tranformed,\n                \'seg\': seg_tensor_tranformed,\n                \'HHA\': HHA_tensor_tranformed,\n                \'imgpath\': self.paths_dict[\'segs\'][index]}\n\n    def __len__(self):\n        return self.len\n\n    def name(self):\n        return \'NYUDataset\'\n\nclass NYUDataset_val(BaseDataset):\n    def initialize(self, opt):\n        self.opt = opt\n        np.random.seed(8964)\n        self.paths_dict = make_dataset_fromlst(opt.vallist)\n        self.len = len(self.paths_dict[\'images\'])\n\n    def __getitem__(self, index):\n        #self.paths[\'images\'][index]\n        img = np.asarray(Image.open(self.paths_dict[\'images\'][index]))#.astype(np.uint8)\n        # print (img)\n        depth = np.asarray(Image.open(self.paths_dict[\'depths\'][index])).astype(np.float32)/120. # 1/5 * depth\n        HHA = np.asarray(Image.open(self.paths_dict[\'HHAs\'][index]))\n        seg = np.asarray(Image.open(self.paths_dict[\'segs\'][index])).astype(np.uint8)\n\n        params = get_params(self.opt, seg.shape, test=True)\n        depth_tensor_tranformed = transform(depth, params, normalize=False,istrain=self.opt.isTrain)\n        seg_tensor_tranformed = transform(seg, params, normalize=False,method=\'nearest\',istrain=self.opt.isTrain)\n        # HHA_tensor_tranformed = transform(HHA, params,istrain=self.opt.isTrain)\n        if self.opt.inputmode == \'bgr-mean\':\n            img_tensor_tranformed = transform(img, params, normalize=False, istrain=self.opt.isTrain, option=1)\n            HHA_tensor_tranformed = transform(HHA, params, normalize=False, istrain=self.opt.isTrain, option=2)\n        else:\n            img_tensor_tranformed = transform(img, params, istrain=self.opt.isTrain, option=1)\n            HHA_tensor_tranformed = transform(HHA, params, istrain=self.opt.isTrain, option=2)\n\n        return {\'image\':img_tensor_tranformed,\n                \'depth\':depth_tensor_tranformed,\n                \'seg\': seg_tensor_tranformed,\n                \'HHA\': HHA_tensor_tranformed,\n                \'imgpath\': self.paths_dict[\'segs\'][index]}\n\n    def __len__(self):\n        return self.len\n\n    def name(self):\n        return \'NYUDataset\'\n\n\n'"
data/stanfordindoor_dataset.py,1,"b'import os.path\nimport numpy as np\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch\nimport h5py\nfrom data.base_dataset import *\nfrom PIL import Image\nimport math, random\nimport time\n\ndef make_dataset_fromlst(listfilename):\n    """"""\n    NYUlist format:\n    imagepath seglabelpath depthpath HHApath\n    """"""\n    images = []\n    segs = []\n    depths = []\n    with open(listfilename) as f:\n        content = f.readlines()\n        for x in content:\n            imgname, segname, depthname = x.strip().split(\' \')\n            images += [imgname]\n            segs += [segname]\n            depths += [depthname]\n    return {\'images\':images, \'segs\':segs, \'depths\':depths}\n\nclass StanfordIndoorDataset(BaseDataset):\n    def initialize(self, opt):\n        self.opt = opt\n        np.random.seed(int(time.time()))\n        self.paths_dict = make_dataset_fromlst(opt.list)\n        self.len = len(self.paths_dict[\'images\'])\n        # self.label_weight = torch.Tensor(label_weight)\n        self.datafile = \'stanfordindoor_dataset.py\'\n\n    def __getitem__(self, index):\n\n        img = np.asarray(Image.open(self.paths_dict[\'images\'][index]))#.astype(np.uint8)\n        depth = np.asarray(Image.open(self.paths_dict[\'depths\'][index])).astype(np.float32)/120. # 1/10 * depth\n        seg = np.asarray(Image.open(self.paths_dict[\'segs\'][index]))-1\n\n        params = get_params_sunrgbd(self.opt, seg.shape,maxcrop=0.7, maxscale=1.1)\n        depth_tensor_tranformed = transform(depth, params, normalize=False,istrain=self.opt.isTrain)\n        seg_tensor_tranformed = transform(seg, params, normalize=False,method=\'nearest\',istrain=self.opt.isTrain)\n        if self.opt.inputmode == \'bgr-mean\':\n            img_tensor_tranformed = transform(img, params, normalize=False, istrain=self.opt.isTrain, option=1)\n        else:\n            img_tensor_tranformed = transform(img, params, istrain=self.opt.isTrain, option=1)\n\n        return {\'image\':img_tensor_tranformed,\n                \'depth\':depth_tensor_tranformed,\n                \'seg\': seg_tensor_tranformed,\n                \'imgpath\': self.paths_dict[\'segs\'][index]}\n\n    def __len__(self):\n        return self.len\n\n    def name(self):\n        return \'stanfordindoor_dataset\'\n\nclass StanfordIndoorDataset_val(BaseDataset):\n    def initialize(self, opt):\n        self.opt = opt\n        np.random.seed(8964)\n        self.paths_dict = make_dataset_fromlst(opt.vallist)\n        self.len = len(self.paths_dict[\'images\'])\n\n    def __getitem__(self, index):\n\n        img = np.asarray(Image.open(self.paths_dict[\'images\'][index]))#.astype(np.uint8)\n        depth = np.asarray(Image.open(self.paths_dict[\'depths\'][index])).astype(np.float32)/120. # 1/10 * depth\n        seg = np.asarray(Image.open(self.paths_dict[\'segs\'][index]))-1\n\n        params = get_params_sunrgbd(self.opt, seg.shape, test=True)\n        depth_tensor_tranformed = transform(depth, params, normalize=False,istrain=self.opt.isTrain)\n        seg_tensor_tranformed = transform(seg, params, normalize=False,method=\'nearest\',istrain=self.opt.isTrain)\n\n        if self.opt.inputmode == \'bgr-mean\':\n            img_tensor_tranformed = transform(img, params, normalize=False, istrain=self.opt.isTrain, option=1)\n        else:\n            img_tensor_tranformed = transform(img, params, istrain=self.opt.isTrain, option=1)\n\n        return {\'image\':img_tensor_tranformed,\n                \'depth\':depth_tensor_tranformed,\n                \'seg\': seg_tensor_tranformed,\n                \'imgpath\': self.paths_dict[\'segs\'][index]}\n\n    def __len__(self):\n        return self.len\n\n    def name(self):\n        return \'stanfordindoor_dataset\'\n\n\n'"
data/sunrgbd_dataset.py,1,"b'import os.path\nimport numpy as np\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch\nimport h5py\nfrom data.base_dataset import *\nfrom PIL import Image\nimport math, random\nimport time\n\ndef make_dataset_fromlst(listfilename):\n    """"""\n    NYUlist format:\n    imagepath seglabelpath depthpath HHApath\n    """"""\n    images = []\n    segs = []\n    depths = []\n    HHAs = []\n    with open(listfilename) as f:\n        content = f.readlines()\n        for x in content:\n            imgname, segname, depthname, HHAname = x.strip().split(\' \')\n            images += [imgname]\n            segs += [segname]\n            depths += [depthname]\n            HHAs += [HHAname]\n    return {\'images\':images, \'segs\':segs, \'HHAs\':HHAs, \'depths\':depths}\n\nclass SUNRGBDDataset(BaseDataset):\n    def initialize(self, opt):\n        self.opt = opt\n        np.random.seed(int(time.time()))\n        self.paths_dict = make_dataset_fromlst(opt.list)\n        self.len = len(self.paths_dict[\'images\'])\n        # self.label_weight = torch.Tensor(label_weight)\n        self.datafile = \'sunrgbd_dataset.py\'\n\n    def __getitem__(self, index):\n        #self.paths[\'images\'][index]\n        # print self.opt.scale,self.opt.flip,self.opt.crop,self.opt.colorjitter\n        img = np.asarray(Image.open(self.paths_dict[\'images\'][index]))#.astype(np.uint8)\n        HHA = np.asarray(Image.open(self.paths_dict[\'HHAs\'][index]))[:,:,::-1]\n        seg = np.asarray(Image.open(self.paths_dict[\'segs\'][index])).astype(np.uint8)-1\n        depth = np.asarray(Image.open(self.paths_dict[\'depths\'][index])).astype(np.uint16)\n\n        assert (img.shape[0]==HHA.shape[0]==seg.shape[0]==depth.shape[0])\n        assert (img.shape[1]==HHA.shape[1]==seg.shape[1]==depth.shape[1])\n\n        depth = np.bitwise_or(np.right_shift(depth,3),np.left_shift(depth,16-3))\n        depth = depth.astype(np.float32)/120. # 1/5 * depth\n\n\n\n\n        params = get_params_sunrgbd(self.opt, seg.shape, maxcrop=.7)\n        depth_tensor_tranformed = transform(depth, params, normalize=False,istrain=self.opt.isTrain)\n        seg_tensor_tranformed = transform(seg, params, normalize=False,method=\'nearest\',istrain=self.opt.isTrain)\n        if self.opt.inputmode == \'bgr-mean\':\n            img_tensor_tranformed = transform(img, params, normalize=False, istrain=self.opt.isTrain, option=1)\n            HHA_tensor_tranformed = transform(HHA, params, normalize=False, istrain=self.opt.isTrain, option=2)\n        else:\n            img_tensor_tranformed = transform(img, params, istrain=self.opt.isTrain, option=1)\n            HHA_tensor_tranformed = transform(HHA, params, istrain=self.opt.isTrain, option=2)\n\n\n        # print img_tensor_tranformed\n        # print(np.unique(depth_tensor_tranformed.numpy()).shape)\n        # print img_tensor_tranformed.size()\n        return {\'image\':img_tensor_tranformed,\n                \'depth\':depth_tensor_tranformed,\n                \'seg\': seg_tensor_tranformed,\n                \'HHA\': HHA_tensor_tranformed,\n                \'imgpath\': self.paths_dict[\'segs\'][index]}\n\n    def __len__(self):\n        return self.len\n\n    def name(self):\n        return \'sunrgbd_dataset\'\n\nclass SUNRGBDDataset_val(BaseDataset):\n    def initialize(self, opt):\n        self.opt = opt\n        np.random.seed(8964)\n        self.paths_dict = make_dataset_fromlst(opt.vallist)\n        self.len = len(self.paths_dict[\'images\'])\n\n    def __getitem__(self, index):\n        #self.paths[\'images\'][index]\n        img = np.asarray(Image.open(self.paths_dict[\'images\'][index]))#.astype(np.uint8)\n        HHA = np.asarray(Image.open(self.paths_dict[\'HHAs\'][index]))[:,:,::-1]\n        seg = np.asarray(Image.open(self.paths_dict[\'segs\'][index])).astype(np.uint8)-1\n        depth = np.asarray(Image.open(self.paths_dict[\'depths\'][index])).astype(np.uint16)\n        depth = np.bitwise_or(np.right_shift(depth,3),np.left_shift(depth,16-3))\n        depth = depth.astype(np.float32)/120. # 1/5 * depth\n\n        assert (img.shape[0]==HHA.shape[0]==seg.shape[0]==depth.shape[0])\n        assert (img.shape[1]==HHA.shape[1]==seg.shape[1]==depth.shape[1])\n\n        params = get_params_sunrgbd(self.opt, seg.shape, test=True)\n        depth_tensor_tranformed = transform(depth, params, normalize=False,istrain=self.opt.isTrain)\n        seg_tensor_tranformed = transform(seg, params, normalize=False,method=\'nearest\',istrain=self.opt.isTrain)\n        # HHA_tensor_tranformed = transform(HHA, params,istrain=self.opt.isTrain)\n        if self.opt.inputmode == \'bgr-mean\':\n            img_tensor_tranformed = transform(img, params, normalize=False, istrain=self.opt.isTrain, option=1)\n            HHA_tensor_tranformed = transform(HHA, params, normalize=False, istrain=self.opt.isTrain, option=2)\n        else:\n            img_tensor_tranformed = transform(img, params, istrain=self.opt.isTrain, option=1)\n            HHA_tensor_tranformed = transform(HHA, params, istrain=self.opt.isTrain, option=2)\n\n        return {\'image\':img_tensor_tranformed,\n                \'depth\':depth_tensor_tranformed,\n                \'seg\': seg_tensor_tranformed,\n                \'HHA\': HHA_tensor_tranformed,\n                \'imgpath\': self.paths_dict[\'segs\'][index]}\n\n    def __len__(self):\n        return self.len\n\n    def name(self):\n        return \'sunrgbd_dataset_Val\'\n\n\n'"
models/Deeplab.py,11,"b'import torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\nimport torch\nfrom .base_model import BaseModel\nimport numpy as np\nfrom . import losses\nimport shutil\nfrom utils.util import *\nfrom torch.autograd import Variable\nfrom collections import OrderedDict\nfrom tensorboardX import SummaryWriter\nimport os\nimport VGG_Deeplab as VGG_Deeplab\n\n\nclass Deeplab_VGG(nn.Module):\n    def __init__(self, num_classes, depthconv=False):\n        super(Deeplab_VGG,self).__init__()\n        self.Scale = VGG_Deeplab.vgg16(num_classes=num_classes,depthconv=depthconv)\n\n    def forward(self,x, depth=None):\n        output = self.Scale(x,depth) # for original scale\n        return output\n\n#------------------------------------------------------#\n\nclass Deeplab_Solver(BaseModel):\n    def __init__(self, opt, dataset=None, encoder=\'VGG\'):\n        BaseModel.initialize(self, opt)\n        self.encoder = encoder\n        if encoder == \'VGG\':\n            self.model = Deeplab_VGG(self.opt.label_nc, self.opt.depthconv)\n\n        if self.opt.isTrain:\n            self.criterionSeg = torch.nn.CrossEntropyLoss(ignore_index=255).cuda()\n            # self.criterionSeg = torch.nn.CrossEntropyLoss(ignore_index=255).cuda()\n            # self.criterionSeg = nn.NLLLoss2d(ignore_index=255)#.cuda()\n\n            if encoder == \'VGG\':\n                self.optimizer = torch.optim.SGD([{\'params\': self.model.Scale.get_1x_lr_params_NOscale(), \'lr\': self.opt.lr},\n                                                 {\'params\': self.model.Scale.get_10x_lr_params(), \'lr\': self.opt.lr},\n                                                 {\'params\': self.model.Scale.get_2x_lr_params_NOscale(), \'lr\': self.opt.lr, \'weight_decay\': 0.},\n                                                 {\'params\': self.model.Scale.get_20x_lr_params(), \'lr\': self.opt.lr, \'weight_decay\': 0.}\n                                                  ],\n                                                 lr=self.opt.lr, momentum=self.opt.momentum, weight_decay=self.opt.wd)\n\n            # self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.opt.lr, momentum=self.opt.momentum, weight_decay=self.opt.wd)\n\n            self.old_lr = self.opt.lr\n            self.averageloss = []\n            # copy scripts\n            self.model_path = \'./models\' #os.path.dirname(os.path.realpath(__file__))\n            self.data_path = \'./data\' #os.path.dirname(os.path.realpath(__file__))\n            shutil.copyfile(os.path.join(self.model_path, \'Deeplab.py\'), os.path.join(self.model_dir, \'Deeplab.py\'))\n\n            if encoder == \'VGG\':\n                shutil.copyfile(os.path.join(self.model_path, \'VGG_Deeplab.py\'), os.path.join(self.model_dir, \'VGG_Deeplab.py\'))\n            shutil.copyfile(os.path.join(self.model_path, \'model_utils.py\'), os.path.join(self.model_dir, \'model_utils.py\'))\n            shutil.copyfile(os.path.join(self.data_path, dataset.datafile), os.path.join(self.model_dir, dataset.datafile))\n            shutil.copyfile(os.path.join(self.data_path, \'base_dataset.py\'), os.path.join(self.model_dir, \'base_dataset.py\'))\n\n            self.writer = SummaryWriter(self.tensorborad_dir)\n            self.counter = 0\n\n        if not self.isTrain or self.opt.continue_train:\n            if self.opt.pretrained_model!=\'\':\n                self.load_pretrained_network(self.model, self.opt.pretrained_model, self.opt.which_epoch, strict=False)\n                print(""Successfully loaded from pretrained model with given path!"")\n            else:\n                self.load()\n                print(""Successfully loaded model, continue training....!"")\n\n        self.model.cuda()\n        self.normweightgrad=0.\n        # if len(opt.gpu_ids):#opt.isTrain and\n        #     self.model = torch.nn.DataParallel(self.model, device_ids=opt.gpu_ids)\n\n    def forward(self, data, isTrain=True):\n        self.model.zero_grad()\n\n        self.image = Variable(data[\'image\'], volatile=not isTrain).cuda()\n        if \'depth\' in data.keys():\n            self.depth = Variable(data[\'depth\'], volatile=not isTrain).cuda()\n        else:\n            self.depth = None\n        if data[\'seg\'] is not None:\n            self.seggt = Variable(data[\'seg\'], volatile=not isTrain).cuda()\n        else:\n            self.seggt = None\n\n        input_size = self.image.size()\n\n        self.segpred = self.model(self.image,self.depth)\n        self.segpred = nn.functional.upsample(self.segpred, size=(input_size[2], input_size[3]), mode=\'bilinear\')\n        # self.segpred = nn.functional.log_softmax(nn.functional.upsample(self.segpred, size=(input_size[2], input_size[3]), mode=\'bilinear\'))\n\n        if self.opt.isTrain:\n            self.loss = self.criterionSeg(self.segpred, torch.squeeze(self.seggt,1).long())\n            self.averageloss += [self.loss.data[0]]\n\n        segpred = self.segpred.max(1, keepdim=True)[1]\n        return self.seggt, segpred\n\n\n    def backward(self, step, total_step):\n        self.loss.backward()\n        self.optimizer.step()\n        # print self.model.Scale.classifier.fc6_2.weight.grad.mean().data.cpu().numpy()\n        # self.normweightgrad +=self.model.Scale.classifier.norm.scale.grad.mean().data.cpu().numpy()\n        # print self.normweightgrad#self.model.Scale.classifier.norm.scale.grad.mean().data.cpu().numpy()\n        if step % self.opt.iterSize  == 0:\n            self.update_learning_rate(step, total_step)\n            trainingavgloss = np.mean(self.averageloss)\n            if self.opt.verbose:\n                print (\'  Iter: %d, Loss: %f\' % (step, trainingavgloss) )\n\n    def get_visuals(self, step):\n        ############## Display results and errors ############\n        if self.opt.isTrain:\n            self.trainingavgloss = np.mean(self.averageloss)\n            if self.opt.verbose:\n                print (\'  Iter: %d, Loss: %f\' % (step, self.trainingavgloss) )\n            self.writer.add_scalar(self.opt.name+\'/trainingloss/\', self.trainingavgloss, step)\n            self.averageloss = []\n\n        if self.depth is not None:\n            return OrderedDict([(\'image\', tensor2im(self.image.data[0], inputmode=self.opt.inputmode)),\n                                (\'depth\', tensor2im(self.depth.data[0], inputmode=\'divstd-mean\')),\n                                (\'segpred\', tensor2label(self.segpred.data[0], self.opt.label_nc)),\n                                (\'seggt\', tensor2label(self.seggt.data[0], self.opt.label_nc))])\n        else:\n            return OrderedDict([(\'image\', tensor2im(self.image.data[0], inputmode=self.opt.inputmode)),\n                                (\'segpred\', tensor2label(self.segpred.data[0], self.opt.label_nc)),\n                                (\'seggt\', tensor2label(self.seggt.data[0], self.opt.label_nc))])\n\n    def update_tensorboard(self, data, step):\n        if self.opt.isTrain:\n            self.writer.add_scalar(self.opt.name+\'/Accuracy/\', data[0], step)\n            self.writer.add_scalar(self.opt.name+\'/Accuracy_Class/\', data[1], step)\n            self.writer.add_scalar(self.opt.name+\'/Mean_IoU/\', data[2], step)\n            self.writer.add_scalar(self.opt.name+\'/FWAV_Accuracy/\', data[3], step)\n\n            self.trainingavgloss = np.mean(self.averageloss)\n            self.writer.add_scalars(self.opt.name+\'/loss\', {""train"": self.trainingavgloss,\n                                                             ""val"": np.mean(self.averageloss)}, step)\n\n            self.writer.add_scalars(\'trainingavgloss/\', {self.opt.name: self.trainingavgloss}, step)\n            self.writer.add_scalars(\'valloss/\', {self.opt.name: np.mean(self.averageloss)}, step)\n            self.writer.add_scalars(\'val_MeanIoU/\', {self.opt.name: data[2]}, step)\n\n            file_name = os.path.join(self.save_dir, \'MIoU.txt\')\n            with open(file_name, \'wt\') as opt_file:\n                opt_file.write(\'%f\\n\' % (data[2]))\n            # self.writer.add_scalars(\'losses/\'+self.opt.name, {""train"": self.trainingavgloss,\n            #                                                  ""val"": np.mean(self.averageloss)}, step)\n            self.averageloss = []\n\n    def save(self, which_epoch):\n        # self.save_network(self.netG, \'G\', which_epoch, self.gpu_ids)\n        self.save_network(self.model, \'net\', which_epoch, self.gpu_ids)\n\n    def load(self):\n        self.load_network(self.model, \'net\',self.opt.which_epoch)\n\n    def update_learning_rate(self, step, total_step):\n\n        lr = max(self.opt.lr * ((1 - float(step) / total_step) ** (self.opt.lr_power)), 1e-6)\n\n        # drop_ratio = (1. * float(total_step - step) / (total_step - step + 1)) ** self.opt.lr_power\n        # lr = self.old_lr * drop_ratio\n\n        self.writer.add_scalar(self.opt.name+\'/Learning_Rate/\', lr, step)\n\n\tself.optimizer.param_groups[0][\'lr\'] = lr\n\tself.optimizer.param_groups[1][\'lr\'] = lr\n\tself.optimizer.param_groups[2][\'lr\'] = lr\n\tself.optimizer.param_groups[3][\'lr\'] = lr\n\t# self.optimizer.param_groups[0][\'lr\'] = lr\n\t# self.optimizer.param_groups[1][\'lr\'] = lr*10\n\t# self.optimizer.param_groups[2][\'lr\'] = lr*2 #* 100\n\t# self.optimizer.param_groups[3][\'lr\'] = lr*20\n\t# self.optimizer.param_groups[4][\'lr\'] = lr*100\n\n\n        # torch.nn.utils.clip_grad_norm(self.model.Scale.get_1x_lr_params_NOscale(), 1.)\n        # torch.nn.utils.clip_grad_norm(self.model.Scale.get_10x_lr_params(), 1.)\n\n        if self.opt.verbose:\n            print(\'     update learning rate: %f -> %f\' % (self.old_lr, lr))\n        self.old_lr = lr\n\n\n'"
models/Deeplab_HHA.py,12,"b'import torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\nimport torch\nfrom .base_model import BaseModel\nimport numpy as np\nfrom . import losses\nimport shutil\nfrom utils.util import *\nfrom torch.autograd import Variable\nfrom collections import OrderedDict\nfrom tensorboardX import SummaryWriter\nimport os\nimport VGG_Deeplab as VGG_Deeplab\n\nclass Deeplab_VGG(nn.Module):\n    def __init__(self, num_classes, depthconv=False):\n        super(Deeplab_VGG,self).__init__()\n        self.Scale = VGG_Deeplab.vgg16(num_classes=num_classes,depthconv=depthconv)\n\n    def forward(self,x, depth=None):\n        output = self.Scale(x,depth) # for original scale\n        return output\n\nclass Deeplab_HHA_Solver(BaseModel):\n    def __init__(self, opt, dataset=None):\n        BaseModel.initialize(self, opt)\n        self.model_rgb = Deeplab_VGG(self.opt.label_nc,self.opt.depthconv)\n        self.model_HHA = Deeplab_VGG(self.opt.label_nc,self.opt.depthconv)\n\n        self.model = nn.Sequential(*[self.model_rgb,self.model_HHA])\n\n        if self.opt.isTrain:\n            self.criterionSeg = torch.nn.CrossEntropyLoss(ignore_index=255).cuda()\n            # self.optimizer = torch.optim.SGD(\n            #     [\n            #         {\'params\': self.model_rgb.Scale.get_1x_lr_params_NOscale(), \'lr\': self.opt.lr},\n            #         {\'params\': self.model_rgb.Scale.get_10x_lr_params(), \'lr\': 10 * self.opt.lr},\n            #         {\'params\': self.model_rgb.Scale.get_2x_lr_params_NOscale(), \'lr\': 2 * self.opt.lr,\n            #          \'weight_decay\': 0.},\n            #         {\'params\': self.model_rgb.Scale.get_20x_lr_params(), \'lr\': 20 * self.opt.lr, \'weight_decay\': 0.},\n            #         {\'params\': self.model_HHA.Scale.get_1x_lr_params_NOscale(), \'lr\': self.opt.lr},\n            #         {\'params\': self.model_HHA.Scale.get_10x_lr_params(), \'lr\': 10 * self.opt.lr},\n            #         {\'params\': self.model_HHA.Scale.get_2x_lr_params_NOscale(), \'lr\': 2 * self.opt.lr,\n            #          \'weight_decay\': 0.},\n            #         {\'params\': self.model_HHA.Scale.get_20x_lr_params(), \'lr\': 20 * self.opt.lr, \'weight_decay\': 0.}\n            #     ],\n            #     lr=self.opt.lr, momentum=self.opt.momentum, weight_decay=self.opt.wd)\n            params_rgb = list(self.model_rgb.Scale.parameters())\n            params_HHA = list(self.model_HHA.Scale.parameters())\n            self.optimizer = torch.optim.SGD(params_rgb+params_HHA, lr=self.opt.lr, momentum=self.opt.momentum, weight_decay=self.opt.wd)\n            #\n            # self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.opt.lr, momentum=self.opt.momentum, weight_decay=self.opt.wd)\n\n            self.old_lr = self.opt.lr\n            self.averageloss = []\n            # copy scripts\n            self.model_path = \'./models\' #os.path.dirname(os.path.realpath(__file__))\n            self.data_path = \'./data\' #os.path.dirname(os.path.realpath(__file__))\n            shutil.copyfile(os.path.join(self.model_path, \'Deeplab_HHA.py\'), os.path.join(self.model_dir, \'Deeplab.py\'))\n            shutil.copyfile(os.path.join(self.model_path, \'VGG_Deeplab.py\'), os.path.join(self.model_dir, \'VGG_Deeplab.py\'))\n            shutil.copyfile(os.path.join(self.model_path, \'model_utils.py\'), os.path.join(self.model_dir, \'model_utils.py\'))\n            shutil.copyfile(os.path.join(self.data_path, dataset.datafile), os.path.join(self.model_dir, dataset.datafile))\n            shutil.copyfile(os.path.join(self.data_path, \'base_dataset.py\'), os.path.join(self.model_dir, \'base_dataset.py\'))\n\n            self.writer = SummaryWriter(self.tensorborad_dir)\n            self.counter = 0\n\n        if not self.isTrain or self.opt.continue_train:\n            pretrained_path = \'\'# if not self.isTrain else opt.load_pretrain\n\n            if self.opt.pretrained_model!=\'\' or (self.opt.pretrained_model_HHA != \'\' and self.opt.pretrained_model_rgb != \'\'):\n                if self.opt.pretrained_model_HHA != \'\' and self.opt.pretrained_model_rgb != \'\':\n                    self.load_pretrained_network(self.model_rgb, self.opt.pretrained_model_rgb, self.opt.which_epoch_rgb, False)\n                    self.load_pretrained_network(self.model_HHA, self.opt.pretrained_model_HHA, self.opt.which_epoch_HHA, False)\n                else:\n                    self.load_pretrained_network(self.model_rgb, self.opt.pretrained_model, self.opt.which_epoch, False)\n                    self.load_pretrained_network(self.model_HHA, self.opt.pretrained_model, self.opt.which_epoch, False)\n                print(""successfully loaded from pretrained model with given path!"")\n            else:\n                self.load()\n                print(""successfully loaded from pretrained model 0!"")\n\n        self.model_rgb.cuda()\n        self.model_HHA.cuda()\n        self.normweightgrad=0.\n\n    def forward(self, data, isTrain=True):\n        self.model_rgb.zero_grad()\n        self.model_HHA.zero_grad()\n\n        # x, depth = None, label = None\n        self.image = Variable(data[\'image\']).cuda()\n        self.HHA = Variable(data[\'HHA\']).cuda()\n        self.depth = Variable(data[\'depth\']).cuda()\n        self.seggt = Variable(data[\'seg\']).cuda()\n\n        input_size = self.image.size()\n        self.segpred_rgb = self.model_rgb(self.image, self.depth)\n        self.segpred_HHA = self.model_HHA(self.HHA, self.depth)\n\n        self.segpred = 0.5*self.segpred_rgb +0.5*self.segpred_HHA#\n\n        self.segpred = nn.functional.upsample(self.segpred, size=(input_size[2], input_size[3]), mode=\'bilinear\')\n\n\n        if isTrain:\n            self.loss = self.criterionSeg(self.segpred, torch.squeeze(self.seggt,1).long())\n            self.averageloss += [self.loss.data[0]]\n\n        segpred = self.segpred.max(1, keepdim=True)[1]\n        return self.seggt, segpred\n\n\n    def backward(self, step, total_step):\n        self.loss.backward()\n        self.optimizer.step()\n        if step % self.opt.iterSize  == 0:\n            self.update_learning_rate(step, total_step)\n            trainingavgloss = np.mean(self.averageloss)\n            if self.opt.verbose:\n                print (\'  Iter: %d, Loss: %f\' % (step, trainingavgloss) )\n\n    def get_visuals(self, step):\n        ############## Display results and errors ############\n        if self.opt.isTrain:\n            self.trainingavgloss = np.mean(self.averageloss)\n            if self.opt.verbose:\n                print (\'  Iter: %d, Loss: %f\' % (step, self.trainingavgloss) )\n            self.writer.add_scalar(self.opt.name+\'/trainingloss/\', self.trainingavgloss, step)\n            self.averageloss = []\n\n        if self.depth is not None:\n            return OrderedDict([(\'image\', tensor2im(self.image.data[0], inputmode=self.opt.inputmode)),\n                                (\'depth\', tensor2im(self.depth.data[0], inputmode=\'divstd-mean\')),\n                                (\'segpred\', tensor2label(self.segpred.data[0], self.opt.label_nc)),\n                                (\'seggt\', tensor2label(self.seggt.data[0], self.opt.label_nc))])\n\n    def update_tensorboard(self, data, step):\n        if self.opt.isTrain:\n            self.writer.add_scalar(self.opt.name+\'/Accuracy/\', data[0], step)\n            self.writer.add_scalar(self.opt.name+\'/Accuracy_Class/\', data[1], step)\n            self.writer.add_scalar(self.opt.name+\'/Mean_IoU/\', data[2], step)\n            self.writer.add_scalar(self.opt.name+\'/FWAV_Accuracy/\', data[3], step)\n\n            self.writer.add_scalars(self.opt.name+\'/loss\', {""train"": self.trainingavgloss,\n                                                             ""val"": np.mean(self.averageloss)}, step)\n\n            self.writer.add_scalars(\'trainingavgloss/\', {self.opt.name: self.trainingavgloss}, step)\n            self.writer.add_scalars(\'valloss/\', {self.opt.name: np.mean(self.averageloss)}, step)\n            self.writer.add_scalars(\'val_MeanIoU/\', {self.opt.name: data[2]}, step)\n\n            file_name = os.path.join(self.save_dir, \'MIoU.txt\')\n            with open(file_name, \'wt\') as opt_file:\n                opt_file.write(\'%f\\n\' % (data[2]))\n            # self.writer.add_scalars(\'losses/\'+self.opt.name, {""train"": self.trainingavgloss,\n            #                                                  ""val"": np.mean(self.averageloss)}, step)\n            self.averageloss = []\n\n    def save(self, which_epoch):\n        self.save_network(self.model_rgb, \'rgb\', which_epoch, self.gpu_ids)\n        self.save_network(self.model_HHA, \'HHA\', which_epoch, self.gpu_ids)\n\n    def load(self):\n        self.load_network(self.model_rgb, \'rgb\',self.opt.which_epoch)\n        self.load_network(self.model_HHA, \'HHA\',self.opt.which_epoch)\n\n    def update_learning_rate(self, step, total_step):\n\n        lr = max(self.opt.lr * ((1 - float(step) / total_step) ** (self.opt.lr_power)), 1e-7)\n        self.writer.add_scalar(\'Learning_Rate/\', lr, step)\n\n        self.optimizer.param_groups[0][\'lr\'] = lr\n        # self.optimizer.param_groups[1][\'lr\'] = lr*10\n        # self.optimizer.param_groups[2][\'lr\'] = lr*2\n        # self.optimizer.param_groups[3][\'lr\'] = lr*20\n        # self.optimizer.param_groups[4][\'lr\'] = lr\n        # self.optimizer.param_groups[5][\'lr\'] = lr*10\n        # self.optimizer.param_groups[6][\'lr\'] = lr*2\n        # self.optimizer.param_groups[7][\'lr\'] = lr*20\n\n        # torch.nn.utils.clip_grad_norm(self.model_rgb.Scale.get_1x_lr_params_NOscale(), 10.)\n        # torch.nn.utils.clip_grad_norm(self.model_rgb.Scale.get_1x_lr_params_NOscale(), 10.)\n        # torch.nn.utils.clip_grad_norm(self.model_HHA.Scale.get_10x_lr_params(), 10.)\n        # torch.nn.utils.clip_grad_norm(self.model_HHA.Scale.get_10x_lr_params(), 10.)\n\n        if self.opt.verbose:\n            print(\'     update learning rate: %f -> %f\' % (self.old_lr, lr))\n\n        self.old_lr = lr'"
models/VGG_Deeplab.py,26,"b'from model_utils import *\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\nimport math\nfrom .ops.depthconv.modules import DepthConv\nfrom .ops.depthavgpooling.modules import Depthavgpooling\nimport torch\nimport torchvision\n\n__all__ = [\n    \'VGG\', \'vgg11\', \'vgg11_bn\', \'vgg13\', \'vgg13_bn\', \'vgg16\', \'vgg16_bn\',\n    \'vgg19_bn\', \'vgg19\',\n]\n\n\nmodel_urls = {\n    \'vgg11\': \'https://download.pytorch.org/models/vgg11-bbd30ac9.pth\',\n    \'vgg13\': \'https://download.pytorch.org/models/vgg13-c768596a.pth\',\n    \'vgg16\': \'https://download.pytorch.org/models/vgg16-397923af.pth\',\n    \'vgg19\': \'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\',\n    \'vgg11_bn\': \'https://download.pytorch.org/models/vgg11_bn-6002323d.pth\',\n    \'vgg13_bn\': \'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth\',\n    \'vgg16_bn\': \'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\',\n    \'vgg19_bn\': \'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth\',\n}\n\n\ncfg = {\n # name:c1_1 c1_2     c2_1 c2_2      c3_1 c3_2 c3_3      c4_1 c4_2 c4_3      c5_1 c5_2 c5_3\n # dilation:                                                                   2    2    2\n    \'D\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, \'M\', 512, 512, 512, \'M\', 512, 512, 512, \'M\'],\n}\n\ndepth_cfg = {\n    \'D\': [0,3,6,10,14],\n}\n\n\nclass ConvModule(nn.Module):\n\n    def __init__(self, inplanes, planes, kernel_size=3, stride=1, padding=1, dilation=1,\n                 bn=False,\n                 maxpool=False, pool_kernel=3, pool_stride=2, pool_pad=1):\n        super(ConvModule, self).__init__()\n        conv2d = nn.Conv2d(inplanes,planes,kernel_size=kernel_size,stride=stride,padding=padding,dilation=dilation)\n        layers = []\n        if bn:\n            layers += [nn.BatchNorm2d(planes), nn.ReLU(inplace=True)]\n        else:\n            layers += [nn.ReLU(inplace=True)]\n        if maxpool:\n            layers += [nn.MaxPool2d(kernel_size=pool_kernel, stride=pool_stride,padding=pool_pad)]\n\n        self.layers = nn.Sequential(*([conv2d]+layers))\n    def forward(self, x):\n        # x = self.conv2d(x)\n        x = self.layers(x)\n        return x\n\nclass DepthConvModule(nn.Module):\n\n    def __init__(self, inplanes, planes, kernel_size=3, stride=1, padding=1, dilation=1,bn=False):\n        super(DepthConvModule, self).__init__()\n\n        conv2d = DepthConv(inplanes,planes,kernel_size=kernel_size,stride=stride,padding=padding,dilation=dilation)\n        layers = []\n        if bn:\n            layers += [nn.BatchNorm2d(planes), nn.ReLU(inplace=True)]\n        else:\n            layers += [nn.ReLU(inplace=True)]\n        self.layers = nn.Sequential(*([conv2d]+layers))#(*layers)\n\n    def forward(self, x, depth):\n\n        for im,module in enumerate(self.layers._modules.values()):\n            if im==0:\n                x = module(x,depth)\n            else:\n                x = module(x)\n        # x = self.conv2d(x, depth)\n        # x = self.layers(x)\n        return x\n\n\nclass VGG_layer2(nn.Module):\n\n    def __init__(self, batch_norm=False, depthconv=False):\n        super(VGG_layer2, self).__init__()\n        in_channels = 3\n        self.depthconv = depthconv\n        # if self.depthconv:\n        #     self.conv1_1_depthconvweight = 1.#nn.Parameter(torch.ones(1))\n        #     self.conv1_1 = DepthConvModule(3, 64, bn=batch_norm)\n        # else:\n        self.conv1_1 = ConvModule(3, 64, bn=batch_norm)\n        self.conv1_2 = ConvModule(64, 64, bn=batch_norm, maxpool=True)\n\n        # if self.depthconv:\n        # self.conv2_1_depthconvweight = 1.#nn.Parameter(torch.ones(1))\n        self.downsample_depth2_1 = nn.AvgPool2d(3,padding=1,stride=2)\n        #     self.conv2_1 = DepthConvModule(64, 128, bn=batch_norm)\n        # else:\n        self.conv2_1 = ConvModule(64, 128, bn=batch_norm)\n        self.conv2_2 = ConvModule(128, 128, bn=batch_norm, maxpool=True)\n\n        # if self.depthconv:\n        #     self.conv3_1_depthconvweight = 1.#nn.Parameter(torch.ones(1))\n        self.downsample_depth3_1 = nn.AvgPool2d(3,padding=1,stride=2)\n        #     self.conv3_1 = DepthConvModule(128, 256, bn=batch_norm)\n        # else:\n        self.conv3_1 = ConvModule(128, 256, bn=batch_norm)\n        self.conv3_2 = ConvModule(256, 256, bn=batch_norm)\n        self.conv3_3 = ConvModule(256, 256, bn=batch_norm, maxpool=True)\n\n        if self.depthconv:\n            self.conv4_1_depthconvweight = 1.#nn.Parameter(torch.ones(1))\n            self.downsample_depth4_1 = nn.AvgPool2d(3,padding=1,stride=2)\n            self.conv4_1 = DepthConvModule(256, 512, bn=batch_norm)\n        else:\n            self.conv4_1 = ConvModule(256, 512, bn=batch_norm)\n        self.conv4_2 = ConvModule(512, 512, bn=batch_norm)\n        self.conv4_3 = ConvModule(512, 512, bn=batch_norm,\n                                  maxpool=True, pool_kernel=3, pool_stride=1, pool_pad=1)\n\n        if self.depthconv:\n            self.conv5_1_depthconvweight = 1.#nn.Parameter(torch.ones(1))\n            self.conv5_1 = DepthConvModule(512, 512, bn=batch_norm,dilation=2,padding=2)\n        else:\n            self.conv5_1 = ConvModule(512, 512, bn=batch_norm, dilation=2, padding=2)\n        self.conv5_2 = ConvModule(512, 512, bn=batch_norm, dilation=2, padding=2)\n        self.conv5_3 = ConvModule(512, 512, bn=batch_norm, dilation=2, padding=2,\n                                  maxpool=True, pool_kernel=3, pool_stride=1, pool_pad=1)\n        self.pool5a = nn.AvgPool2d(kernel_size=3, stride=1,padding=1)\n        # self.pool5a = nn.AvgPool2d(kernel_size=3, stride=1,padding=1)\n\n    def forward(self, x, depth=None):\n        # print x.size()\n        # if self.depthconv:\n        #     # print self.conv1_1_depthconvweight\n        #     x = self.conv1_1(x,self.conv1_1_depthconvweight * depth)\n        # else:\n        x = self.conv1_1(x)\n        x = self.conv1_2(x)\n        # if self.depthconv:\n        depth = self.downsample_depth2_1(depth)\n        #     x = self.conv2_1(x, self.conv2_1_depthconvweight * depth)\n        # else:\n        x = self.conv2_1(x)\n        x = self.conv2_2(x)\n        # if self.depthconv:\n        depth = self.downsample_depth3_1(depth)\n        #     x = self.conv3_1(x, self.conv3_1_depthconvweight * depth)\n        # else:\n        x = self.conv3_1(x)\n        x = self.conv3_2(x)\n        x = self.conv3_3(x)\n        if self.depthconv:\n            depth = self.downsample_depth4_1(depth)\n            x = self.conv4_1(x, self.conv4_1_depthconvweight * depth)\n        else:\n            x = self.conv4_1(x)\n        x = self.conv4_2(x)\n        x = self.conv4_3(x)\n        if self.depthconv:\n            x = self.conv5_1(x, self.conv5_1_depthconvweight * depth)\n        else:\n            x = self.conv5_1(x)\n        x = self.conv5_2(x)\n        x = self.conv5_3(x)\n        x = self.pool5a(x)\n        return x,depth\n\nclass VGG_layer(nn.Module):\n\n    def __init__(self, batch_norm=False, depthconv=False):\n        super(VGG_layer, self).__init__()\n        in_channels = 3\n        self.depthconv = depthconv\n        if self.depthconv:\n            self.conv1_1_depthconvweight = 1.#nn.Parameter(torch.ones(1))\n            self.conv1_1 = DepthConvModule(3, 64, bn=batch_norm)\n        else:\n            self.conv1_1 = ConvModule(3, 64, bn=batch_norm)\n        self.conv1_2 = ConvModule(64, 64, bn=batch_norm, maxpool=True)\n\n        if self.depthconv:\n            self.conv2_1_depthconvweight = 1.#nn.Parameter(torch.ones(1))\n            self.downsample_depth2_1 = nn.AvgPool2d(3,padding=1,stride=2)\n            self.conv2_1 = DepthConvModule(64, 128, bn=batch_norm)\n        else:\n            self.conv2_1 = ConvModule(64, 128, bn=batch_norm)\n        self.conv2_2 = ConvModule(128, 128, bn=batch_norm, maxpool=True)\n\n        if self.depthconv:\n            self.conv3_1_depthconvweight = 1.#nn.Parameter(torch.ones(1))\n            self.downsample_depth3_1 = nn.AvgPool2d(3,padding=1,stride=2)\n            self.conv3_1 = DepthConvModule(128, 256, bn=batch_norm)\n        else:\n            self.conv3_1 = ConvModule(128, 256, bn=batch_norm)\n        self.conv3_2 = ConvModule(256, 256, bn=batch_norm)\n        self.conv3_3 = ConvModule(256, 256, bn=batch_norm, maxpool=True)\n\n        if self.depthconv:\n            self.conv4_1_depthconvweight = 1.#nn.Parameter(torch.ones(1))\n            self.downsample_depth4_1 = nn.AvgPool2d(3,padding=1,stride=2)\n            self.conv4_1 = DepthConvModule(256, 512, bn=batch_norm)\n        else:\n            self.conv4_1 = ConvModule(256, 512, bn=batch_norm)\n        self.conv4_2 = ConvModule(512, 512, bn=batch_norm)\n        self.conv4_3 = ConvModule(512, 512, bn=batch_norm,\n                                  maxpool=True, pool_kernel=3, pool_stride=1, pool_pad=1)\n\n        if self.depthconv:\n            self.conv5_1_depthconvweight = 1.#nn.Parameter(torch.ones(1))\n            self.conv5_1 = DepthConvModule(512, 512, bn=batch_norm,dilation=2,padding=2)\n        else:\n            self.conv5_1 = ConvModule(512, 512, bn=batch_norm, dilation=2, padding=2)\n        self.conv5_2 = ConvModule(512, 512, bn=batch_norm, dilation=2, padding=2)\n        self.conv5_3 = ConvModule(512, 512, bn=batch_norm, dilation=2, padding=2,\n                                  maxpool=True, pool_kernel=3, pool_stride=1, pool_pad=1)\n        self.pool5a = nn.AvgPool2d(kernel_size=3, stride=1,padding=1)\n        self.pool5a_d = Depthavgpooling(kernel_size=3, stride=1,padding=1)\n\n    def forward(self, x, depth=None):\n        # print x.size()\n        if self.depthconv:\n            # print self.conv1_1_depthconvweight\n            x = self.conv1_1(x,self.conv1_1_depthconvweight * depth)\n        else:\n            x = self.conv1_1(x)\n        x = self.conv1_2(x)\n        if self.depthconv:\n            depth = self.downsample_depth2_1(depth)\n            x = self.conv2_1(x, self.conv2_1_depthconvweight * depth)\n        else:\n            x = self.conv2_1(x)\n        # print \'xxxxxx\',x.size()\n        x = self.conv2_2(x)\n        if self.depthconv:\n            depth = self.downsample_depth3_1(depth)\n            x = self.conv3_1(x, self.conv3_1_depthconvweight * depth)\n        else:\n            x = self.conv3_1(x)\n        x = self.conv3_2(x)\n        x = self.conv3_3(x)\n        if self.depthconv:\n            depth = self.downsample_depth4_1(depth)\n            # print (depth.mean(),depth.max(),depth.min())\n            # torchvision.utils.save_image(depth.data, \'depth.png\')\n            x = self.conv4_1(x, self.conv4_1_depthconvweight * depth)\n        else:\n            x = self.conv4_1(x)\n        x = self.conv4_2(x)\n        x = self.conv4_3(x)\n        if self.depthconv:\n            x = self.conv5_1(x, self.conv5_1_depthconvweight * depth)\n        else:\n            x = self.conv5_1(x)\n        x = self.conv5_2(x)\n        x = self.conv5_3(x)\n        # x = self.pool5a(x,depth)\n        if self.depthconv:\n            x = self.pool5a_d(x,depth)\n        else:\n            x = self.pool5a(x)\n\n        return x, depth\n\ndef make_layers(cfg, depth_cfg=[], batch_norm=False, depthconv=False):\n    layers = []\n    in_channels = 3\n    for iv, v in enumerate(cfg):\n        if v == \'M\':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        else:\n            if depthconv and iv in depth_cfg:\n                conv2d = DepthConv(in_channels, v, kernel_size=3, padding=1)\n            else:\n                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n            else:\n                layers += [conv2d, nn.ReLU(inplace=True)]\n            in_channels = v\n    return nn.Sequential(*layers)\n\nclass Classifier_Module(nn.Module):\n\n    def __init__(self, num_classes, inplanes, depthconv=False):\n        super(Classifier_Module, self).__init__()\n        # [6, 12, 18, 24]\n        self.depthconv = depthconv\n        if depthconv:\n            self.fc6_1_depthconvweight = 1.#nn.Parameter(torch.ones(1))\n            self.fc6_1 = DepthConv(inplanes, 1024, kernel_size=3, stride=1, padding=6, dilation=6)  # fc6\n        else:\n            self.fc6_1 = nn.Conv2d(inplanes, 1024, kernel_size=3, stride=1, padding=6, dilation=6)  # fc6\n\n        self.fc7_1 = nn.Sequential(\n            *[nn.ReLU(True), nn.Dropout(),\n              nn.Conv2d(1024, 1024, kernel_size=1, stride=1), nn.ReLU(True), nn.Dropout()])  # fc7\n        self.fc8_1 = nn.Conv2d(1024, num_classes, kernel_size=1, stride=1, bias=True)  # fc8\n\n        if depthconv:\n            self.fc6_2_depthconvweight = 1.#nn.Parameter(torch.ones(1))\n            self.fc6_2 = DepthConv(inplanes, 1024, kernel_size=3, stride=1, padding=12, dilation=12)  # fc6\n        else:\n            self.fc6_2 = nn.Conv2d(inplanes, 1024, kernel_size=3, stride=1, padding=12, dilation=12)  # fc6\n\n        self.fc7_2 = nn.Sequential(\n            *[nn.ReLU(True), nn.Dropout(),\n              nn.Conv2d(1024, 1024, kernel_size=1, stride=1), nn.ReLU(True), nn.Dropout()])  # fc7\n        self.fc8_2 = nn.Conv2d(1024, num_classes, kernel_size=1, stride=1, bias=True)  # fc8\n\n        if depthconv:\n            self.fc6_3_depthconvweight = 1.#nn.Parameter(torch.ones(1))\n            self.fc6_3 = DepthConv(inplanes, 1024, kernel_size=3, stride=1, padding=18, dilation=18)  # fc6\n        else:\n            self.fc6_3 = nn.Conv2d(inplanes, 1024, kernel_size=3, stride=1, padding=18, dilation=18)  # fc6\n\n        self.fc7_3 = nn.Sequential(\n            *[nn.ReLU(True), nn.Dropout(),\n              nn.Conv2d(1024, 1024, kernel_size=1, stride=1), nn.ReLU(True), nn.Dropout()])  # fc7\n        self.fc8_3 = nn.Conv2d(1024, num_classes, kernel_size=1, stride=1, bias=True)  # fc8\n\n        if depthconv:\n            self.fc6_4_depthconvweight = 1.#nn.Parameter(torch.ones(1))\n            self.fc6_4 = DepthConv(inplanes, 1024, kernel_size=3, stride=1, padding=24, dilation=24)  # fc6\n        else:\n            self.fc6_4 = nn.Conv2d(inplanes, 1024, kernel_size=3, stride=1, padding=24, dilation=24)  # fc6\n\n        self.fc7_4 = nn.Sequential(\n            *[nn.ReLU(True), nn.Dropout(),\n              nn.Conv2d(1024, 1024, kernel_size=1, stride=1), nn.ReLU(True), nn.Dropout()])  # fc7\n        self.fc8_4 = nn.Conv2d(1024, num_classes, kernel_size=1, stride=1, bias=True)  # fc8\n\n    def forward(self, x, depth=None):\n        if self.depthconv:\n            out1 = self.fc6_1(x, self.fc6_1_depthconvweight * depth)\n        else:\n            out1 = self.fc6_1(x)\n        out1 = self.fc7_1(out1)\n        out1 = self.fc8_1(out1)\n\n        if self.depthconv:\n            out2 = self.fc6_2(x, self.fc6_2_depthconvweight * depth)\n        else:\n            out2 = self.fc6_2(x)\n        out2 = self.fc7_2(out2)\n        out2 = self.fc8_2(out2)\n\n        if self.depthconv:\n            out3 = self.fc6_3(x, self.fc6_3_depthconvweight * depth)\n        else:\n            out3 = self.fc6_3(x)\n        out3 = self.fc7_3(out3)\n        out3 = self.fc8_3(out3)\n\n        if self.depthconv:\n            out4 = self.fc6_4(x, self.fc6_4_depthconvweight * depth)\n        else:\n            out4 = self.fc6_4(x)\n        out4 = self.fc7_4(out4)\n        out4 = self.fc8_4(out4)\n\n        return out1+out2+out3+out4\n\nclass Classifier_Module2(nn.Module):\n\n    def __init__(self, num_classes, inplanes, depthconv=False):\n        super(Classifier_Module2, self).__init__()\n        # [6, 12, 18, 24]\n        self.depthconv = depthconv\n        if depthconv:\n            self.fc6_2_depthconvweight = 1.#nn.Parameter(torch.ones(1))\n            self.fc6_2 = DepthConv(inplanes, 1024, kernel_size=3, stride=1, padding=12, dilation=12)\n            self.downsample_depth = None\n        else:\n            self.downsample_depth = nn.AvgPool2d(9,padding=1,stride=8)\n            self.fc6_2 = nn.Conv2d(inplanes, 1024, kernel_size=3, stride=1, padding=12, dilation=12)  # fc6\n\n        self.fc7_2 = nn.Sequential(\n            *[nn.ReLU(True), nn.Dropout(),\n              nn.Conv2d(1024, 1024, kernel_size=1, stride=1), nn.ReLU(True), nn.Dropout()])  # fc7\n\n        # self.globalpooling = DepthGlobalPool(1024,3)#\n        # self.fc8_2 = nn.Conv2d(1024+3, num_classes, kernel_size=1, stride=1, bias=True)  # fc8\n\n        self.globalpooling = nn.AdaptiveAvgPool2d((1, 1))#nn.AvgPool2d((54,71))#\n        self.dropout = nn.Dropout(0.3)\n        # self.norm = CaffeNormalize(1024)#LayerNorm(1024)#nn.InstanceNorm2d(1024).use_running_stats(mode=False)\n        self.fc8_2 = nn.Conv2d(2048, num_classes, kernel_size=1, stride=1, bias=True)  # fc8\n\n    def forward(self, x, depth=None):\n        if self.depthconv:\n            out2 = self.fc6_2(x, self.fc6_2_depthconvweight * depth)\n        else:\n            out2 = self.fc6_2(x)\n        out2 = self.fc7_2(out2)\n        out2_size = out2.size()\n\n        #global pooling\n        globalpool = self.globalpooling(out2)\n        # globalpool = self.dropout(self.norm(globalpool))\n        globalpool = self.dropout(globalpool)#self.norm(globalpool))\n        upsample = nn.Upsample((out2_size[2],out2_size[3]), mode=\'bilinear\')#scale_factor=8)\n        globalpool = upsample(globalpool)\n\n        #global pooling with depth\n        # globalpool = self.globalpooling(out2,depth)\n\n\n        # print globalpool.mean()\n        out2 = torch.cat([out2, globalpool], 1)\n        out2 = self.fc8_2(out2)\n        # print out2.size()\n        return out2\n\nclass VGG(nn.Module):\n\n    def __init__(self, num_classes=20, init_weights=True, depthconv=False,bn=False):\n        super(VGG, self).__init__()\n        self.features = VGG_layer(batch_norm=bn,depthconv=depthconv)\n        self.classifier = Classifier_Module2(num_classes,512,depthconv=depthconv)\n\n        if init_weights:\n            self._initialize_weights()\n\n    def forward(self, x, depth=None):\n        x,depth = self.features(x,depth)\n        x = self.classifier(x,depth)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n\n    def get_normalize_params(self):\n        b=[]\n        b.append(self.classifier.norm)\n        for i in b:\n            if isinstance(i, CaffeNormalize):\n                yield i.scale\n\n    def get_1x_lr_params_NOscale(self):\n        """"""\n        This generator returns all the parameters of the net except for\n        the last classification layer. Note that for each batchnorm layer,\n        requires_grad is set to False in deeplab_resnet.py, therefore this function does not return\n        any batchnorm parameter\n        """"""\n        b = []\n\n        b.append(self.features.conv1_1)\n        b.append(self.features.conv1_2)\n        b.append(self.features.conv2_1)\n        b.append(self.features.conv2_2)\n        b.append(self.features.conv3_1)\n        b.append(self.features.conv3_2)\n        b.append(self.features.conv3_3)\n        b.append(self.features.conv4_1)\n        b.append(self.features.conv4_2)\n        b.append(self.features.conv4_3)\n        b.append(self.features.conv5_1)\n        b.append(self.features.conv5_2)\n        b.append(self.features.conv5_3)\n        # b.append(self.classifier.fc6_1)\n        b.append(self.classifier.fc6_2)\n        # b.append(self.classifier.norm)\n        # b.append(self.classifier.fc6_3)\n        # b.append(self.classifier.fc6_4)\n        # b.append(self.classifier.fc7_1)\n        b.append(self.classifier.fc7_2)\n        # b.append(self.classifier.fc7_3)\n        # b.append(self.classifier.fc7_4)\n\n        for i in range(len(b)):\n            for j in b[i].modules():\n                if isinstance(j, nn.Conv2d):\n                    if j.weight.requires_grad:\n                        yield j.weight\n                elif isinstance(j, DepthConv):\n                    if j.weight.requires_grad:\n                        yield j.weight\n\n\n    def get_2x_lr_params_NOscale(self):\n        """"""\n        This generator returns all the parameters of the net except for\n        the last classification layer. Note that for each batchnorm layer,\n        requires_grad is set to False in deeplab_resnet.py, therefore this function does not return\n        any batchnorm parameter\n        """"""\n        b = []\n\n        b.append(self.features.conv1_1)\n        b.append(self.features.conv1_2)\n        b.append(self.features.conv2_1)\n        b.append(self.features.conv2_2)\n        b.append(self.features.conv3_1)\n        b.append(self.features.conv3_2)\n        b.append(self.features.conv3_3)\n        b.append(self.features.conv4_1)\n        b.append(self.features.conv4_2)\n        b.append(self.features.conv4_3)\n        b.append(self.features.conv5_1)\n        b.append(self.features.conv5_2)\n        b.append(self.features.conv5_3)\n        # b.append(self.classifier.fc6_1)\n        b.append(self.classifier.fc6_2)\n        # b.append(self.classifier.fc6_3)\n        # b.append(self.classifier.fc6_4)\n        # b.append(self.classifier.fc7_1)\n        b.append(self.classifier.fc7_2)\n        # b.append(self.classifier.globalpooling.model)\n        # b.append(self.classifier.fc7_3)\n        # b.append(self.classifier.fc7_4)\n\n        for i in range(len(b)):\n            for j in b[i].modules():\n                if isinstance(j, nn.Conv2d):\n                    if j.bias is not None:\n                        if j.bias.requires_grad:\n                            yield j.bias\n                elif isinstance(j, DepthConv):\n                    if j.bias is not None:\n                        if j.bias.requires_grad:\n                            yield j.bias\n\n\n    def get_10x_lr_params(self):\n        """"""\n        This generator returns all the parameters for the last layer of the net,\n        which does the classification of pixel into classes\n        """"""\n        b = []\n        # b.append(self.classifier.fc8_1.weight)\n        b.append(self.classifier.fc8_2.weight)\n        # b.append(self.classifier.globalpooling.model.weight)\n        # b.append(self.classifier.fc8_3.weight)\n        # b.append(self.classifier.fc8_4.weight)\n\n        for i in b:\n            yield i\n        # for j in range(len(b)):\n        #     for i in b[j]:\n        #         yield i\n\n    def get_20x_lr_params(self):\n        """"""\n        This generator returns all the parameters for the last layer of the net,\n        which does the classification of pixel into classes\n        """"""\n        b = []\n        # b.append(self.classifier.fc8_1.bias)\n        b.append(self.classifier.fc8_2.bias)\n        # b.append(self.classifier.globalpooling.model.bias)\n        # b.append(self.classifier.fc8_3.bias)\n        # b.append(self.classifier.fc8_4.bias)\n\n        for i in b:\n            yield i\n        # for j in range(len(b)):\n        #     for i in b[j]:\n        #         yield i\n\n    def get_100x_lr_params(self):\n        """"""\n        This generator returns all the parameters for the last layer of the net,\n        which does the classification of pixel into classes\n        """"""\n        b = []\n        b.append(self.features.conv1_1_depthconvweight)\n        b.append(self.features.conv2_1_depthconvweight)\n        b.append(self.features.conv3_1_depthconvweight)\n        b.append(self.features.conv4_1_depthconvweight)\n        b.append(self.features.conv5_1_depthconvweight)\n        b.append(self.classifier.fc6_1_depthconvweight)\n        b.append(self.classifier.fc6_2_depthconvweight)\n        b.append(self.classifier.fc6_3_depthconvweight)\n        b.append(self.classifier.fc6_4_depthconvweight)\n\n        for j in range(len(b)):\n            yield b[j]\n            # for i in b[j]:\n            #     yield i\n\n\n\ndef vgg16(pretrained=False, **kwargs):\n    """"""VGG 16-layer model (configuration ""D"")\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    if pretrained:\n        kwargs[\'init_weights\'] = False\n    model = VGG(bn=False,**kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg16\']))\n    return model\n\n\ndef vgg16_bn(pretrained=False, **kwargs):\n    """"""VGG 16-layer model (configuration ""D"") with batch normalization\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    if pretrained:\n        kwargs[\'init_weights\'] = False\n    model = VGG(bn=True,**kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'vgg16_bn\']))\n    return model\n\n'"
models/__init__.py,0,b''
models/base_model.py,9,"b'import os\nimport numpy as np\nfrom utils import util\nimport torch\n\ndef load_pretrained_model(net, state_dict, strict=True):\n    """"""Copies parameters and buffers from :attr:`state_dict` into\n    this module and its descendants. If :attr:`strict` is ``True`` then\n    the keys of :attr:`state_dict` must exactly match the keys returned\n    by this module\'s :func:`state_dict()` function.\n\n    Arguments:\n        state_dict (dict): A dict containing parameters and\n            persistent buffers.\n        strict (bool): Strictly enforce that the keys in :attr:`state_dict`\n            match the keys returned by this module\'s `:func:`state_dict()`\n            function.\n    """"""\n    own_state = net.state_dict()\n    # print state_dict.keys()\n    # print own_state.keys()\n    for name, param in state_dict.items():\n        if name in own_state:\n            # print name, np.mean(param.numpy())\n            if isinstance(param, torch.nn.Parameter):\n                # backwards compatibility for serialized parameters\n                param = param.data\n            if strict:\n                try:\n                    own_state[name].copy_(param)\n                except Exception:\n                    raise RuntimeError(\'While copying the parameter named {}, \'\n                                       \'whose dimensions in the model are {} and \'\n                                       \'whose dimensions in the checkpoint are {}.\'\n                                       .format(name, own_state[name].size(), param.size()))\n            else:\n                try:\n                    own_state[name].copy_(param)\n                except Exception:\n                    print(\'Ignoring Error: While copying the parameter named {}, \'\n                                       \'whose dimensions in the model are {} and \'\n                                       \'whose dimensions in the checkpoint are {}.\'\n                                       .format(name, own_state[name].size(), param.size()))\n\n        elif strict:\n            raise KeyError(\'unexpected key ""{}"" in state_dict\'\n                           .format(name))\n    if strict:\n        missing = set(own_state.keys()) - set(state_dict.keys())\n        if len(missing) > 0:\n            raise KeyError(\'missing keys in state_dict: ""{}""\'.format(missing))\n\n\nclass BaseModel():\n\n    def name(self):\n        return \'BaseModel\'\n\n    def initialize(self, opt):\n        self.opt = opt\n        self.training = opt.isTrain\n        self.gpu_ids = opt.gpu_ids\n        self.isTrain = opt.isTrain\n        self.num_classes = opt.label_nc\n        self.Tensor = torch.cuda.FloatTensor if self.gpu_ids else torch.Tensor\n        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)\n        self.tensorborad_dir = os.path.join(self.opt.checkpoints_dir, \'tensorboard\', opt.dataset_mode)\n        self.model_dir = os.path.join(self.opt.checkpoints_dir, self.opt.name, \'model\')\n        util.mkdirs([self.tensorborad_dir, self.model_dir])\n\n    def set_input(self, input):\n        self.input = input\n\n    def forward(self):\n        pass\n\n    # used in test time, no backprop\n    def test(self):\n        pass\n\n    def get_image_paths(self):\n        pass\n\n    def optimize_parameters(self):\n        pass\n\n    def get_current_visuals(self):\n        return self.input\n\n    def save(self, label):\n        pass\n\n    # helper saving function that can be used by subclasses\n    def save_network(self, network, network_label, epoch_label, gpu_ids):\n        save_filename = \'%s_net_%s.pth\' % (epoch_label, network_label)\n        save_path = os.path.join(self.model_dir, save_filename)\n        torch.save(network.cpu().state_dict(), save_path)\n        if len(gpu_ids) and torch.cuda.is_available():\n            network.cuda()\n\n    # helper loading function that can be used by subclasses\n    def load_network(self, network, network_label, epoch_label, save_dir=\'\'):\n        save_filename = \'%s_net_%s.pth\' % (epoch_label,network_label)\n        if not save_dir:\n            save_dir = self.model_dir\n        save_path = os.path.join(save_dir, save_filename)        \n        if not os.path.isfile(save_path):\n            print(\'%s not exists yet!\' % save_path)\n        else:\n            #network.load_state_dict(torch.load(save_path))\n            try:\n                # print torch.load(save_path).keys()\n                # print network.state_dict()[\'Scale.features.conv2_1_depthconvweight\']\n                network.load_state_dict(torch.load(save_path))\n            except:   \n                pretrained_dict = torch.load(save_path)                \n                model_dict = network.state_dict()\n                try:\n                    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}                    \n                    network.load_state_dict(pretrained_dict)\n                    print(\'Pretrained network has excessive layers; Only loading layers that are used\' )\n                except:\n                    print(\'Pretrained network has fewer layers; The following are not initialized:\' )\n                    # from sets import Set\n                    # not_initialized = Set()\n                    for k, v in pretrained_dict.items():                      \n                        if v.size() == model_dict[k].size():\n                            model_dict[k] = v\n                    not_initialized=[]\n                    # print(pretrained_dict.keys())\n                    # print(model_dict.keys())\n                    for k, v in model_dict.items():\n                        if k not in pretrained_dict or v.size() != pretrained_dict[k].size():\n                            not_initialized+=[k]#[k.split(\'.\')[0]]\n                    print(sorted(not_initialized))\n                    network.load_state_dict(model_dict)                  \n\n    def update_learning_rate():\n        pass\n\n\n    def load_pretrained_network(self, network, pretraineddir, epoch_label,strict=True):\n        save_filename = \'%s.pth\' % (epoch_label)\n        save_path = os.path.join(pretraineddir, save_filename)\n        load_dict = torch.load(save_path, map_location=lambda storage, loc: storage)\n        # print (load_dict.values().size())\n        load_pretrained_model(network,load_dict,strict)\n'"
models/losses.py,7,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom distutils.version import LooseVersion\n\nclass CrossEntropyLoss2d(nn.Module):\n    def __init__(self, weight=None, size_average=False, ignore_index=255):\n        super(CrossEntropyLoss2d, self).__init__()\n        self.nll_loss = nn.NLLLoss2d(weight, size_average, ignore_index)\n\n    def forward(self, inputs, targets):\n        return self.nll_loss(F.log_softmax(inputs), targets)\n\ndef cross_entropy2d(input, target, weight=None, size_average=True):\n    # input: (n, c, h, w), target: (n, h, w)\n    n, c, h, w = input.size()\n    # log_p: (n, c, h, w)\n    if LooseVersion(torch.__version__) < LooseVersion(\'0.3\'):\n        # ==0.2.X\n        log_p = F.log_softmax(input).cuda()\n    else:\n        # >=0.3\n        log_p = F.log_softmax(input, dim=1).cuda()\n    # log_p: (n*h*w, c)\n    log_p = log_p.transpose(1, 2).transpose(2, 3).contiguous()\n    log_p = log_p[target.view(n, h, w, 1).repeat(1, 1, 1, c) >= 0]\n    log_p = log_p.view(-1, c)\n    # target: (n*h*w,)\n    # mask = (target != 255)\n    # target = target[mask]\n    loss = F.nll_loss(log_p, target, weight=weight, size_average=False, ignore_index=255).cuda()\n    if size_average:\n        loss /= (n*h*w)\n    return loss\n\nclass FocalLoss2d(nn.Module):\n    def __init__(self, gamma=2., weight=None, size_average=True, ignore_index=255):\n        super(FocalLoss2d, self).__init__()\n        self.gamma = gamma\n        self.nll_loss = nn.NLLLoss2d(weight, size_average, ignore_index)\n\n    def forward(self, inputs, targets):\n        return self.nll_loss((1 - F.softmax(inputs)) ** self.gamma * F.log_softmax(inputs), targets)\n\n\nclass FocalLoss(nn.Module):\n    """"""\n        This criterion is a implemenation of Focal Loss, which is proposed in\n        Focal Loss for Dense Object Detection.\n\n            Loss(x, class) = - \\alpha (1-softmax(x)[class])^gamma \\log(softmax(x)[class])\n\n        The losses are averaged across observations for each minibatch.\n        Args:\n            alpha(1D Tensor, Variable) : the scalar factor for this criterion\n            gamma(float, double) : gamma > 0\n            size_average(bool): size_average(bool): By default, the losses are averaged over observations for each minibatch.\n                                However, if the field size_average is set to False, the losses are\n                                instead summed for each minibatch.\n    """"""\n\n    def __init__(self, class_num, alpha=None, gamma=2, size_average=True):\n        super(FocalLoss, self).__init__()\n        if alpha is None:\n            self.alpha = Variable(torch.ones(class_num+1))\n        else:\n            if isinstance(alpha, Variable):\n                self.alpha = alpha\n            else:\n                self.alpha = Variable(alpha)\n        self.gamma = gamma\n        self.class_num = class_num\n        self.size_average = size_average\n\n    def forward(self, inputs, targets):  # variables\n        P = F.softmax(inputs)\n\n        b,c,h,w = inputs.size()\n        class_mask = Variable(torch.zeros([b,c+1,h,w]).cuda())\n        class_mask.scatter_(1, targets.long(), 1.)\n        class_mask = class_mask[:,:-1,:,:]\n\n        if inputs.is_cuda and not self.alpha.is_cuda:\n            self.alpha = self.alpha.cuda()\n        # print(\'alpha\',self.alpha.size())\n        alpha = self.alpha[targets.data.view(-1)].view_as(targets)\n        # print (alpha.size(),class_mask.size(),P.size())\n        probs = (P * class_mask).sum(1)  # + 1e-6#.view(-1, 1)\n        log_p = probs.log()\n\n        batch_loss = -alpha * (torch.pow((1 - probs), self.gamma)) * log_p\n\n        if self.size_average:\n            loss = batch_loss.mean()\n        else:\n            loss = batch_loss.sum()\n        return loss\n'"
models/model_utils.py,8,"b""import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np\nimport torchvision\nimport time\n\nclass LayerNorm(nn.Module):\n\n    def __init__(self, features, eps=1e-6, gamma=1.,beta=0.,learnable=False):\n        super(LayerNorm,self).__init__()\n        if learnable:\n            self.gamma = nn.Parameter(torch.ones(features))\n            self.beta = nn.Parameter(torch.zeros(features))\n        else:\n            self.gamma = gamma\n            self.beta = beta\n\n        self.eps = eps\n\n    def forward(self, x):\n        x_size = x.size()\n        mean = x.view(x_size[0],x_size[1],x_size[2]*x_size[3]).mean(2)\\\n            .view(x_size[0],x_size[1],1,1).repeat(1, 1, x_size[2], x_size[3])\n        std = x.view(x_size[0],x_size[1],x_size[2]*x_size[3]).std(2)\\\n            .view(x_size[0],x_size[1],1,1).repeat(1, 1, x_size[2], x_size[3])\n        # print 'mean',mean.size(),'x',x_size\n        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n\nclass CaffeNormalize(nn.Module):\n\n    def __init__(self, features, eps=1e-7):\n        super(CaffeNormalize,self).__init__()\n        self.scale = nn.Parameter(10.*torch.ones(features))#, requires_grad=False)\n        self.eps = eps\n\n    def forward(self, x):\n        # print self.scale\n        x_size = x.size()\n        norm = x.norm(2,dim=1,keepdim=True)#.detach()\n        #print norm.data.cpu().numpy(),self.scale.mean().data.cpu().numpy()#,self.scale.grad.mean().data.cpu().numpy()\n        x = x.div(norm+self.eps)\n\n        return x.mul(self.scale.view(1, x_size[1], 1, 1))\n\n\nclass DepthGlobalPool(nn.Module):\n    def __init__(self, n_features, n_out):\n        super(DepthGlobalPool, self).__init__()\n        self.model = nn.Conv2d(n_features, n_out, kernel_size=1, padding=0)\n        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n\n        self.norm = CaffeNormalize(n_out)\n        self.dropout = nn.Dropout(0.3)\n\n        n = self.model.kernel_size[0] * self.model.kernel_size[1] * self.model.out_channels\n        self.model.weight.data.normal_(0, np.sqrt(2. / n))\n        if self.model.bias is not None:\n            self.model.bias.data.zero_()\n\n    def forward(self, features, depth, depthpool=False):\n        # features = self.pool(self.model(features))\n        out2_size = features.size()\n        features = self.model(features)\n\n        if isinstance(depth, Variable) and depthpool:\n            outfeatures = features.clone()\n            n_c = features.size()[1]\n\n            # depth-wise average pooling\n            # depthclone = depth.clone()\n            depth = depth.data.cpu().numpy()\n            _, depth_bin = np.histogram(depth)\n\n            bin_low = depth_bin[0]\n            for bin_high in depth_bin[1:]:\n                indices = ((depth <= bin_high) & (depth >= bin_low)).nonzero()\n                if indices[0].shape[0] != 0:\n                    for j in range(n_c):\n                        output_ins = features[indices[0], indices[1] + j, indices[2], indices[3]]\n                        mean_feat = torch.mean(output_ins).expand_as(output_ins)\n                        outfeatures[indices[0], indices[1] + j, indices[2], indices[3]] = mean_feat  # torch.mean(output_ins)\n                    bin_low = bin_high\n\n            # outfeatures = self.norm(outfeatures)\n            outfeatures = self.dropout(outfeatures)\n\n            # bin_low = depth_bin[0]\n            # for bin_high in depth_bin[1:]:\n            #     indices = ((depth <= bin_high) & (depth >= bin_low)).nonzero()\n            #     if indices[0].shape[0] != 0:\n            #         output_ins = features[indices[0], indices[1], indices[2], indices[3]]\n            #         mean_feat = torch.mean(output_ins).expand_as(output_ins)\n            #         depthclone[indices[0], indices[1], indices[2], indices[3]] = mean_feat\n            #         bin_low = bin_high\n            #\n            # upsample = nn.UpsamplingBilinear2d(scale_factor=8)\n            # torchvision.utils.save_image(upsample(depthclone).data, 'depth_feature1.png', normalize=True, range=(0, 1))\n            # outfeatures = self.dropout(outfeatures)\n        else:\n            features = self.pool(features)\n            # features = self.norm(features)\n            outfeatures = self.dropout(features)\n            self.upsample = nn.UpsamplingBilinear2d((out2_size[2],out2_size[3]))\n            outfeatures = self.upsample(outfeatures)\n\n        return outfeatures\n"""
models/models.py,0,"b'import torch\n\ndef create_model(opt, dataset=None):\n\n    if opt.model == \'DeeplabVGG\':\n        from .Deeplab import Deeplab_Solver\n        model = Deeplab_Solver(opt, dataset)\n    elif opt.model == \'DeeplabVGG_HHA\':\n        from .Deeplab_HHA import Deeplab_HHA_Solver\n        model = Deeplab_HHA_Solver(opt, dataset)\n    elif opt.model == \'DeeplabResnet\':\n        from .Deeplab import Deeplab_Solver\n        model = Deeplab_Solver(opt, dataset,\'Resnet\')\n\n    print(""model [%s] was created"" % (model.name()))\n\n    return model\n'"
options/__init__.py,0,b''
options/base_options.py,1,"b'### Copyright (C) 2017 NVIDIA Corporation. All rights reserved. \n### Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nimport argparse\nimport os\nfrom utils import util\nimport torch\n\nclass BaseOptions():\n    def __init__(self):\n        self.parser = argparse.ArgumentParser()\n        self.initialized = False\n\n    def initialize(self):\n        # experiment specifics\n        self.parser.add_argument(\'--name\', type=str, default=\'label2city\', help=\'name of the experiment. It decides where to store samples and models\')\n        self.parser.add_argument(\'--gpu_ids\', type=str, default=\'0\', help=\'gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU\')\n        self.parser.add_argument(\'--checkpoints_dir\', type=str, default=\'./checkpoints\', help=\'models are saved here\')\n        self.parser.add_argument(\'--model\', type=str, default=\'DeeplabVGG\', help=\'model: DeeplabVGG, DeeplabVGG_HHA\')\n        self.parser.add_argument(\'--encoder\', type=str, default=\'resnet50_dilated8\', help=\'pretrained_model\')\n        self.parser.add_argument(\'--decoder\', type=str, default=\'psp_bilinear\', help=\'pretrained_model\')\n        self.parser.add_argument(\'--depthconv\', action=\'store_true\', help=\'if specified, use depthconv\')\n        self.parser.add_argument(\'--depthglobalpool\', action=\'store_true\', help=\'if specified, use global pooling with depth\')\n        self.parser.add_argument(\'--pretrained_model\', type=str, default=\'\', help=\'pretrained_model\')\n        self.parser.add_argument(\'--which_epoch\', type=str, default=\'latest\', help=\'which epoch to load? set to latest to use latest cached model\')\n        self.parser.add_argument(\'--pretrained_model_HHA\', type=str, default=\'\', help=\'pretrained_model\')\n        self.parser.add_argument(\'--which_epoch_HHA\', type=str, default=\'latest\', help=\'which epoch to load? set to latest to use latest cached model\')\n        self.parser.add_argument(\'--pretrained_model_rgb\', type=str, default=\'\', help=\'pretrained_model\')\n        self.parser.add_argument(\'--which_epoch_rgb\', type=str, default=\'latest\', help=\'which epoch to load? set to latest to use latest cached model\')\n\n        # input/output sizes\n        self.parser.add_argument(\'--batchSize\', type=int, default=1, help=\'input batch size\')\n        self.parser.add_argument(\'--fineSize\', type=str, default=\'480,640\', help=\'then crop to this size\')\n        self.parser.add_argument(\'--label_nc\', type=int, default=40, help=\'# of input image channels\')\n\n        # for setting inputs\n        self.parser.add_argument(\'--dataroot\', type=str, default=\'\',\n                                 help=\'chooses how datasets are loaded. [nyuv2]\')\n        self.parser.add_argument(\'--dataset_mode\', type=str, default=\'nyuv2\',\n                                 help=\'chooses how datasets are loaded. [nyuv2]\')\n        self.parser.add_argument(\'--list\', type=str, default=\'\', help=\'image and seg mask list file\')\n        self.parser.add_argument(\'--vallist\', type=str, default=\'\', help=\'image and seg mask list file\')\n\n        # for data augmentation\n        self.parser.add_argument(\'--flip\', action=\'store_true\',help=\'if specified, flip the images for data argumentation\')\n        self.parser.add_argument(\'--scale\', action=\'store_true\',help=\'if specified, scale the images for data argumentation\')\n        self.parser.add_argument(\'--crop\', action=\'store_true\',help=\'if specified, crop the images for data argumentation\')\n        self.parser.add_argument(\'--colorjitter\', action=\'store_true\',help=\'if specified, crop the images for data argumentation\')\n        self.parser.add_argument(\'--inputmode\', default=\'bgr-mean\', type=str, help=\'input image normalize option: bgr-mean, divstd-mean\')\n\n        self.parser.add_argument(\'--serial_batches\', action=\'store_true\', help=\'if true, takes images in order to make batches, otherwise takes them randomly\')\n        self.parser.add_argument(\'--nThreads\', default=1, type=int, help=\'# threads for loading data\')\n        self.parser.add_argument(\'--max_dataset_size\', type=int, default=float(""inf""), help=\'Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.\')\n\n        # for displays\n        self.parser.add_argument(\'--display_winsize\', type=int, default=512,  help=\'display window size\')\n        self.parser.add_argument(\'--tf_log\', action=\'store_true\', help=\'if specified, use tensorboard logging. Requires tensorflow installed\')\n        self.parser.add_argument(\'--verbose\', action=\'store_true\', help=\'if specified, print loss while training\')\n\n\n    def parse(self, save=True):\n        if not self.initialized:\n            self.initialize()\n        self.opt = self.parser.parse_args()\n        self.opt.isTrain = self.isTrain   # train or test\n\n        str_ids = self.opt.gpu_ids.split(\',\')\n        self.opt.gpu_ids = []\n        for str_id in str_ids:\n            id = int(str_id)\n            if id >= 0:\n                self.opt.gpu_ids.append(id)\n\n        str_sizes = self.opt.fineSize.split(\',\')\n        self.opt.fineSize = []\n        for str_size in str_sizes:\n            size_ = int(str_size)\n            if size_ >= 0:\n                self.opt.fineSize.append(size_)\n\n        # set gpu ids\n        if len(self.opt.gpu_ids) > 0:\n            torch.cuda.set_device(self.opt.gpu_ids[0])\n\n        args = vars(self.opt)\n\n        print(\'------------ Options -------------\')\n        for k, v in sorted(args.items()):\n            print(\'%s: %s\' % (str(k), str(v)))\n        print(\'-------------- End ----------------\')\n\n        # save to the disk        \n        expr_dir = os.path.join(self.opt.checkpoints_dir, self.opt.name)\n        util.mkdirs(expr_dir)\n        if save:\n            file_name = os.path.join(expr_dir, \'opt.txt\')\n            with open(file_name, \'wt\') as opt_file:\n                opt_file.write(\'------------ Options -------------\\n\')\n                for k, v in sorted(args.items()):\n                    opt_file.write(\'%s: %s\\n\' % (str(k), str(v)))\n                opt_file.write(\'-------------- End ----------------\\n\')\n        return self.opt\n'"
options/test_options.py,0,"b'### Copyright (C) 2017 NVIDIA Corporation. All rights reserved. \n### Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nfrom .base_options import BaseOptions\n\nclass TestOptions(BaseOptions):\n    def initialize(self):\n        BaseOptions.initialize(self)\n        self.parser.add_argument(\'--ntest\', type=int, default=float(""inf""), help=\'# of test examples.\')\n        self.parser.add_argument(\'--results_dir\', type=str, default=\'./results/\', help=\'saves results here.\')\n        self.parser.add_argument(\'--aspect_ratio\', type=float, default=1.0, help=\'aspect ratio of result images\')\n        self.parser.add_argument(\'--phase\', type=str, default=\'test\', help=\'train, val, test, etc\')\n        self.parser.add_argument(\'--how_many\', type=int, default=20, help=\'how many test images to run\')\n        self.parser.add_argument(\'--cluster_path\', type=str, default=\'features_clustered_010.npy\', help=\'the path for clustered results of encoded features\')\n        self.isTrain = False\n'"
options/train_options.py,0,"b""### Copyright (C) 2017 NVIDIA Corporation. All rights reserved. \n### Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\nfrom .base_options import BaseOptions\n\nclass TrainOptions(BaseOptions):\n    def initialize(self):\n        BaseOptions.initialize(self)\n\n        # for displays\n        self.parser.add_argument('--display_freq', type=int, default=100, help='frequency of showing training results on screen')\n        self.parser.add_argument('--print_freq', type=int, default=100, help='frequency of showing training results on console')\n        self.parser.add_argument('--save_latest_freq', type=int, default=1000, help='frequency of saving the latest results')\n        self.parser.add_argument('--save_epoch_freq', type=int, default=10, help='frequency of saving checkpoints at the end of epochs')        \n        self.parser.add_argument('--no_html', action='store_true', help='do not save intermediate training results to [opt.checkpoints_dir]/[opt.name]/web/')\n        self.parser.add_argument('--debug', action='store_true', help='only do one epoch and displays at each iteration')\n\n        # for training\n        self.parser.add_argument('--loadfroms', action='store_true', help='continue training: load from 32s or 16s')\n        self.parser.add_argument('--continue_train', action='store_true', help='continue training: load the latest model')\n        self.parser.add_argument('--use_softmax', action='store_true', help='if specified use softmax loss, otherwise log-softmax')\n        self.parser.add_argument('--phase', type=str, default='train', help='train, val, test, etc')\n        self.parser.add_argument('--nepochs', type=int, default=100, help='# of iter at starting learning rate')\n        self.parser.add_argument('--iterSize', type=int, default=10, help='# of iter at starting learning rate')\n        self.parser.add_argument('--maxbatchsize', type=int, default=-1, help='# of iter at starting learning rate')\n        self.parser.add_argument('--warmup_iters', type=int, default=500, help='# of iter at starting learning rate')\n        self.parser.add_argument('--beta1', type=float, default=0.5, help='momentum term of adam')\n        self.parser.add_argument('--lr', type=float, default=0.00025, help='initial learning rate for adam')\n        self.parser.add_argument('--lr_power', type=float, default=0.9, help='power of learning rate policy')\n        self.parser.add_argument('--momentum', type=float, default=0.9, help='momentum for sgd')\n        self.parser.add_argument('--wd', type=float, default=0.0004, help='weight decay for sgd')\n\n        self.isTrain = True\n"""
utils/__init__.py,0,b''
utils/gradcheck.py,9,"b'import torch\nfrom torch.autograd import Variable\nfrom collections import Iterable\nimport numpy as np\n\n\ndef iter_variables(x):\n    if isinstance(x, Variable):\n        if x.requires_grad:\n            yield (x.grad.data, x.data) if x.grad is not None else (None, None)\n    elif isinstance(x, Iterable):\n        for elem in x:\n            for result in iter_variables(elem):\n                yield result\n\n\ndef zero_gradients(x):\n    if isinstance(x, Variable):\n        if x.grad is not None:\n            x.grad.detach_()\n            x.grad.data.zero_()\n    elif isinstance(x, Iterable):\n        for elem in x:\n            zero_gradients(elem)\n\n\ndef make_jacobian(input, num_out):\n    if isinstance(input, Variable) and not input.requires_grad:\n        return None\n    elif torch.is_tensor(input) or isinstance(input, Variable):\n        return torch.zeros(input.nelement(), num_out)\n    elif isinstance(input, Iterable):\n        jacobians = list(filter(\n            lambda x: x is not None, (make_jacobian(elem, num_out) for elem in input)))\n        if not jacobians:\n            return None\n        return type(input)(jacobians)\n    else:\n        return None\n\n\ndef iter_tensors(x, only_requiring_grad=False):\n    if torch.is_tensor(x):\n        yield x\n    elif isinstance(x, Variable):\n        if x.requires_grad or not only_requiring_grad:\n            yield x.data\n    elif isinstance(x, Iterable):\n        for elem in x:\n            for result in iter_tensors(elem, only_requiring_grad):\n                yield result\n\n\ndef contiguous(input):\n    if torch.is_tensor(input):\n        return input.contiguous()\n    elif isinstance(input, Variable):\n        return input.contiguous()\n    elif isinstance(input, Iterable):\n        return type(input)(contiguous(e) for e in input)\n    return input\n\n\ndef get_numerical_jacobian(fn, input, target, eps=1e-3):\n    # To be able to use .view(-1) input must be contiguous\n    input = contiguous(input)\n    output_size = fn(input).numel()\n    jacobian = make_jacobian(target, output_size)\n\n    # It\'s much easier to iterate over flattened lists of tensors.\n    # These are reference to the same objects in jacobian, so any changes\n    # will be reflected in it as well.\n    x_tensors = [t for t in iter_tensors(target, True)]\n    j_tensors = [t for t in iter_tensors(jacobian)]\n\n    outa = torch.DoubleTensor(output_size)\n    outb = torch.DoubleTensor(output_size)\n\n    # TODO: compare structure\n    for x_tensor, d_tensor in zip(x_tensors, j_tensors):\n        flat_tensor = x_tensor.view(-1)\n        for i in range(flat_tensor.nelement()):\n            orig = flat_tensor[i]\n            flat_tensor[i] = orig - eps\n            outa.copy_(fn(input), broadcast=False)\n            flat_tensor[i] = orig + eps\n            outb.copy_(fn(input), broadcast=False)\n            flat_tensor[i] = orig\n\n            outb.add_(-1, outa).div_(2 * eps)\n            d_tensor[i] = outb\n\n    return jacobian\n\n\ndef get_analytical_jacobian(input, output):\n    jacobian = make_jacobian(input, output.numel())\n    jacobian_reentrant = make_jacobian(input, output.numel())\n    grad_output = output.data.clone().zero_()\n    flat_grad_output = grad_output.view(-1)\n    reentrant = True\n    correct_grad_sizes = True\n\n    for i in range(flat_grad_output.numel()):\n        flat_grad_output.zero_()\n        flat_grad_output[i] = 1\n        for jacobian_c in (jacobian, jacobian_reentrant):\n            zero_gradients(input)\n            output.backward(grad_output, create_graph=True)\n            for jacobian_x, (d_x, x) in zip(jacobian_c, iter_variables(input)):\n                if d_x is None:\n                    jacobian_x[:, i].zero_()\n                else:\n                    if d_x.size() != x.size():\n                        correct_grad_sizes = False\n                    jacobian_x[:, i] = d_x.to_dense() if d_x.is_sparse else d_x\n\n    for jacobian_x, jacobian_reentrant_x in zip(jacobian, jacobian_reentrant):\n        if (jacobian_x - jacobian_reentrant_x).abs().max() != 0:\n            reentrant = False\n\n    return jacobian, reentrant, correct_grad_sizes\n\n\ndef _as_tuple(x):\n    if isinstance(x, tuple):\n        return x\n    elif isinstance(x, list):\n        return tuple(x)\n    else:\n        return x,\n\n\ndef gradcheck(func, inputs, eps=1e-6, atol=1e-5, rtol=1e-3):\n    """"""Check gradients computed via small finite differences\n       against analytical gradients\n\n    The check between numerical and analytical has the same behaviour as\n    numpy.allclose https://docs.scipy.org/doc/numpy/reference/generated/numpy.allclose.html\n    meaning it check that\n        absolute(a - n) <= (atol + rtol * absolute(n))\n    is true for all elements of analytical jacobian a and numerical jacobian n.\n\n    Args:\n        func: Python function that takes Variable inputs and returns\n            a tuple of Variables\n        inputs: tuple of Variables\n        eps: perturbation for finite differences\n        atol: absolute tolerance\n        rtol: relative tolerance\n\n    Returns:\n        True if all differences satisfy allclose condition\n    """"""\n    output = func(*inputs)\n    output = _as_tuple(output)\n\n    for i, o in enumerate(output):\n        if not o.requires_grad:\n            continue\n        print \'i:\',i,o\n\n        def fn(input):\n            return _as_tuple(func(*input))[i].data\n\n        analytical, reentrant, correct_grad_sizes = get_analytical_jacobian(_as_tuple(inputs), o)\n        numerical = get_numerical_jacobian(fn, inputs, inputs, eps)\n        # -------------------\n        for a in analytical:\n            an = a.numpy()\n        for n in numerical:\n            nn = n.numpy()\n\n        diff = []\n        for a, n in zip(analytical, numerical):\n            dif = (a - n).abs().numpy()\n            diff.append(np.max(dif))\n        diff_max = max(diff)\n        # print(diff_max)\n\n        for a, n in zip(analytical, numerical):\n            if not ((a - n).abs() <= (atol + rtol * n.abs())).all():\n                print a.sum(),n.sum()\n                print(\'1111\')\n                return False\n        # --------------------------\n        # different two times\n        if not reentrant:\n            print(\'not same for 2\')\n            return False\n\n        if not correct_grad_sizes:\n            print(\'not same size\')\n            return False\n\n    # check if the backward multiplies by grad_output\n    zero_gradients(inputs)\n    output = _as_tuple(func(*inputs))\n    torch.autograd.backward(output, [o.data.new(o.size()).zero_() for o in output])\n    var_inputs = list(filter(lambda i: isinstance(i, Variable), inputs))\n    if not var_inputs:\n        raise RuntimeError(""no Variables found in input"")\n    for i in var_inputs:\n        if i.grad is None:\n            continue\n        if not i.grad.data.eq(0).all():\n            print(\'not all zero\')\n            return False\n\n    return True\n\n\ndef gradgradcheck(func, inputs, grad_outputs, eps=1e-6, atol=1e-5, rtol=1e-3):\n    """"""Check gradients of gradients computed via small finite differences\n       against analytical gradients\n    This function checks that backpropagating through the gradients computed\n    to the given grad_outputs are correct.\n\n    The check between numerical and analytical has the same behaviour as\n    numpy.allclose https://docs.scipy.org/doc/numpy/reference/generated/numpy.allclose.html\n    meaning it check that\n        absolute(a - n) <= (atol + rtol * absolute(n))\n    is true for all elements of analytical gradient a and numerical gradient n.\n\n    Args:\n        func: Python function that takes Variable inputs and returns\n            a tuple of Variables\n        inputs: tuple of Variables\n        grad_outputs: tuple of Variables\n        eps: perturbation for finite differences\n        atol: absolute tolerance\n        rtol: relative tolerance\n\n    Returns:\n        True if all differences satisfy allclose condition\n    """"""\n\n    def new_func(*input_args):\n        input_args = input_args[:-len(grad_outputs)]\n        outputs = func(*input_args)\n        outputs = _as_tuple(outputs)\n        input_args = tuple(x for x in input_args if isinstance(x, Variable) and x.requires_grad)\n        grad_inputs = torch.autograd.grad(outputs, input_args, grad_outputs)\n        return grad_inputs\n\n    return gradcheck(new_func, inputs + grad_outputs, eps, atol, rtol)\n'"
utils/html.py,0,"b'import dominate\nfrom dominate.tags import *\nimport os\n\n\nclass HTML:\n    def __init__(self, web_dir, title, refresh=0):\n        self.title = title\n        self.web_dir = web_dir\n        self.img_dir = os.path.join(self.web_dir, \'images\')\n        if not os.path.exists(self.web_dir):\n            os.makedirs(self.web_dir)\n        if not os.path.exists(self.img_dir):\n            os.makedirs(self.img_dir)\n\n        self.doc = dominate.document(title=title)\n        if refresh > 0:\n            with self.doc.head:\n                meta(http_equiv=""refresh"", content=str(refresh))\n\n    def get_image_dir(self):\n        return self.img_dir\n\n    def add_header(self, str):\n        with self.doc:\n            h3(str)\n\n    def add_table(self, border=1):\n        self.t = table(border=border, style=""table-layout: fixed;"")\n        self.doc.add(self.t)\n\n    def add_images(self, ims, txts, links, width=512):\n        self.add_table()\n        with self.t:\n            with tr():\n                for im, txt, link in zip(ims, txts, links):\n                    with td(style=""word-wrap: break-word;"", halign=""center"", valign=""top""):\n                        with p():\n                            with a(href=os.path.join(\'images\', link)):\n                                img(style=""width:%dpx"" % (width), src=os.path.join(\'images\', im))\n                            br()\n                            p(txt)\n\n    def save(self):\n        html_file = \'%s/index.html\' % self.web_dir\n        f = open(html_file, \'wt\')\n        f.write(self.doc.render())\n        f.close()\n\n\nif __name__ == \'__main__\':\n    html = HTML(\'web/\', \'test_html\')\n    html.add_header(\'hello world\')\n\n    ims = []\n    txts = []\n    links = []\n    for n in range(4):\n        ims.append(\'image_%d.jpg\' % n)\n        txts.append(\'text_%d\' % n)\n        links.append(\'image_%d.jpg\' % n)\n    html.add_images(ims, txts, links)\n    html.save()\n'"
utils/util.py,2,"b'from __future__ import print_function\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport numpy as np\nimport os\n\n####evaluation metrics\n\ndef _fast_hist(label_true, label_pred, n_class):\n    mask = (label_true >= 0) & (label_true < n_class)\n    hist = np.bincount(\n        n_class * label_true[mask].astype(int) +\n        label_pred[mask], minlength=n_class ** 2).reshape(n_class, n_class)\n    return hist\n\n\ndef label_accuracy_score(label_trues, label_preds, n_class, returniu = False):\n    """"""Returns accuracy score evaluation result.\n      - overall accuracy\n      - mean accuracy\n      - mean IU\n      - fwavacc\n    """"""\n    hist = np.zeros((n_class, n_class))\n    for lt, lp in zip(label_trues, label_preds):\n        hist += _fast_hist(lt.flatten(), lp.flatten(), n_class)\n    acc = np.diag(hist).sum() / hist.sum()\n    acc_cls = np.diag(hist) / hist.sum(axis=1)\n    acc_cls = np.nanmean(acc_cls)\n    iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n    mean_iu = np.nanmean(iu)\n    freq = hist.sum(axis=1) / hist.sum()\n    fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n    if returniu:\n        return acc, acc_cls, mean_iu, fwavacc, iu[freq > 0]\n    else:\n        return acc, acc_cls, mean_iu, fwavacc\n\n###############################################################################\n# Code from\n# https://github.com/ycszen/pytorch-seg/blob/master/transform.py\n# Modified so it complies with the Citscape label map colors\n###############################################################################\ndef uint82bin(n, count=8):\n    """"""returns the binary of integer n, count refers to amount of bits""""""\n    return \'\'.join([str((n >> y) & 1) for y in range(count-1, -1, -1)])\n\ndef labelcolormap(N):\n    if N == 35: # cityscape\n        cmap = np.array([(  0,  0,  0), (  0,  0,  0), (  0,  0,  0), (  0,  0,  0), (  0,  0,  0), (111, 74,  0), ( 81,  0, 81),\n                     (128, 64,128), (244, 35,232), (250,170,160), (230,150,140), ( 70, 70, 70), (102,102,156), (190,153,153),\n                     (180,165,180), (150,100,100), (150,120, 90), (153,153,153), (153,153,153), (250,170, 30), (220,220,  0),\n                     (107,142, 35), (152,251,152), ( 70,130,180), (220, 20, 60), (255,  0,  0), (  0,  0,142), (  0,  0, 70),\n                     (  0, 60,100), (  0,  0, 90), (  0,  0,110), (  0, 80,100), (  0,  0,230), (119, 11, 32), (  0,  0,142)],\n                     dtype=np.uint8)\n    else:\n        cmap = np.zeros((N, 3), dtype=np.uint8)\n        for i in range(N):\n            r, g, b = 0, 0, 0\n            id = i\n            for j in range(7):\n                str_id = uint82bin(id)\n                r = r ^ (np.uint8(str_id[-1]) << (7-j))\n                g = g ^ (np.uint8(str_id[-2]) << (7-j))\n                b = b ^ (np.uint8(str_id[-3]) << (7-j))\n                id = id >> 3\n            cmap[i, 0] = r\n            cmap[i, 1] = g\n            cmap[i, 2] = b\n    return cmap\n\nclass Colorize(object):\n    def __init__(self, n=35):\n        n = 256\n        self.cmap = labelcolormap(n)\n        self.cmap = torch.from_numpy(self.cmap[:n])\n\n    def __call__(self, gray_image):\n        size = gray_image.size()\n        color_image = torch.ByteTensor(3, size[1], size[2]).fill_(0)\n\n        for label in range(0, len(self.cmap)):\n            mask = (label == gray_image[0]).cpu()\n            color_image[0][mask] = self.cmap[label][0]\n            color_image[1][mask] = self.cmap[label][1]\n            color_image[2][mask] = self.cmap[label][2]\n\n        return color_image\n# Converts a Tensor into a Numpy array\n# |imtype|: the desired type of the converted numpy array\ndef tensor2im(image_tensor, imtype=np.uint8, inputmode=\'\'):\n    if isinstance(image_tensor, list):\n        image_numpy = []\n        for i in range(len(image_tensor)):\n            image_numpy.append(tensor2im(image_tensor[i], imtype, inputmode))\n        return image_numpy\n    image_numpy = image_tensor.cpu().float().numpy()\n    if inputmode==\'div255-mean\':\n        image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n    elif inputmode==\'divstd-mean\':\n        i_max = np.max(image_numpy)\n        i_min = np.min(image_numpy)\n        image_numpy = np.transpose((image_numpy-i_min)/(i_max-i_min) * 255.0, (1, 2, 0))\n    elif inputmode==\'bgr-mean\':\n        image_numpy = np.transpose(image_numpy, (1, 2, 0))[:,:,::-1] + np.asarray([122.675,116.669,104.008])\n        # print(image_numpy.max(),image_numpy.min())\n    else:\n        # print(\'depth\')\n        image_numpy = np.transpose(image_numpy, (1, 2, 0))[:,:,::-1] * 255.0\n    # image_numpy = np.clip(image_numpy, 0, 255)\n    if image_numpy.shape[2] == 1:        \n        image_numpy = image_numpy[:,:,0]\n    return image_numpy.astype(imtype)\n\n# Converts a one-hot tensor into a colorful label map\ncolormap = Colorize(255)\ndef tensor2label(label_tensor, n_label, imtype=np.uint8, colorize = True):\n    if n_label == 0:\n        return tensor2im(label_tensor, imtype)\n    label_tensor = label_tensor.cpu().float()    \n    if label_tensor.size()[0] > 1:\n        label_tensor = label_tensor.max(0, keepdim=True)[1]\n    if colorize:\n        label_tensor = colormap(label_tensor)\n        label_numpy = np.transpose(label_tensor.numpy(), (1, 2, 0))\n    else:\n        label_numpy = np.squeeze(label_tensor.numpy())\n        # print (np.unique(label_numpy.astype(imtype)))\n    return label_numpy.astype(imtype)\n\ndef save_image(image_numpy, image_path, imagesize = None):\n    image_pil = Image.fromarray(image_numpy)\n    if imagesize is not None:\n        img_w, img_h = imagesize\n        image_pil = image_pil.resize((img_w, img_h), Image.NEAREST)\n    # print(np.unique(np.asarray(image_pil)))\n    image_pil.save(image_path)\n    # if len(image_numpy.shape)==2:\n    #     image_pil = Image.open(image_path)\n    #     print(image_path,np.unique(np.asarray(image_pil)))\n\ndef mkdirs(paths):\n    if isinstance(paths, list) and not isinstance(paths, str):\n        for path in paths:\n            mkdir(path)\n    else:\n        mkdir(paths)\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n'"
utils/visualizer.py,0,"b'import numpy as np\nimport os\nimport ntpath\nimport time\nfrom . import util\nfrom . import html\nimport scipy.misc\ntry:\n    from StringIO import StringIO  # Python 2.7\nexcept ImportError:\n    from io import BytesIO         # Python 3.x\n\nclass Visualizer():\n    def __init__(self, opt):\n        # self.opt = opt\n        self.tf_log = opt.tf_log\n        self.use_html = opt.isTrain and not opt.no_html\n        self.win_size = opt.display_winsize\n        self.name = opt.name\n        if self.tf_log:\n            import tensorflow as tf\n            self.tf = tf\n            self.log_dir = os.path.join(opt.checkpoints_dir, opt.name, \'logs\')\n            self.writer = tf.summary.FileWriter(self.log_dir)\n\n        if self.use_html:\n            self.web_dir = os.path.join(opt.checkpoints_dir, opt.name, \'web\')\n            self.img_dir = os.path.join(self.web_dir, \'images\')\n            print(\'create web directory %s...\' % self.web_dir)\n            util.mkdirs([self.web_dir, self.img_dir])\n\n    # |visuals|: dictionary of images to display or save\n    def display_current_results(self, visuals, epoch, step):\n        if self.tf_log: # show images in tensorboard output\n            img_summaries = []\n            for label, image_numpy in visuals.items():\n                # Write the image to a string\n                try:\n                    s = StringIO()\n                except:\n                    s = BytesIO()\n                scipy.misc.toimage(image_numpy).save(s, format=""jpeg"")\n                # Create an Image object\n                img_sum = self.tf.Summary.Image(encoded_image_string=s.getvalue(), height=image_numpy.shape[0], width=image_numpy.shape[1])\n                # Create a Summary value\n                img_summaries.append(self.tf.Summary.Value(tag=label, image=img_sum))\n\n            # Create and write Summary\n            summary = self.tf.Summary(value=img_summaries)\n            self.writer.add_summary(summary, step)\n\n        if self.use_html: # save images to a html file\n            for label, image_numpy in visuals.items():\n                if isinstance(image_numpy, list):\n                    for i in range(len(image_numpy)):\n                        img_path = os.path.join(self.img_dir, \'epoch%.3d_%s_%d.jpg\' % (epoch, label, i))\n                        util.save_image(image_numpy[i], img_path)\n                else:\n                    img_path = os.path.join(self.img_dir, \'epoch%.3d_%s.jpg\' % (epoch, label))\n                    util.save_image(image_numpy, img_path)\n\n            # update website\n            webpage = html.HTML(self.web_dir, self.name, refresh=5)\n            for n in range(epoch, 0, -1):\n                webpage.add_header(\'epoch [%d]\' % n)\n                ims = []\n                txts = []\n                links = []\n\n                for label, image_numpy in visuals.items():\n                    if isinstance(image_numpy, list):\n                        for i in range(len(image_numpy)):\n                            img_path = \'epoch%.3d_%s_%d.jpg\' % (n, label, i)\n                            ims.append(img_path)\n                            txts.append(label+str(i))\n                            links.append(img_path)\n                    else:\n                        img_path = \'epoch%.3d_%s.jpg\' % (n, label)\n                        ims.append(img_path)\n                        txts.append(label)\n                        links.append(img_path)\n                if len(ims) < 10:\n                    webpage.add_images(ims, txts, links, width=self.win_size)\n                else:\n                    num = int(round(len(ims)/2.0))\n                    webpage.add_images(ims[:num], txts[:num], links[:num], width=self.win_size)\n                    webpage.add_images(ims[num:], txts[num:], links[num:], width=self.win_size)\n            webpage.save()\n\n    # errors: dictionary of error labels and values\n    def plot_current_errors(self, errors, step):\n        if self.tf_log:\n            for tag, value in errors.items():\n                summary = self.tf.Summary(value=[self.tf.Summary.Value(tag=tag, simple_value=value)])\n                self.writer.add_summary(summary, step)\n\n    # errors: same format as |errors| of plotCurrentErrors\n    def print_current_errors(self, epoch, i, errors, t):\n        message = \'(epoch: %d, iters: %d, time: %.3f) \' % (epoch, i, t)\n        for k, v in errors.items():\n            if v != 0:\n                message += \'%s: %.3f \' % (k, v)\n\n        print(message)\n        with open(self.log_name, ""a"") as log_file:\n            log_file.write(\'%s\\n\' % message)\n\n    # save image to the disk\n    def save_images(self, webpage, visuals, image_path):\n        image_dir = webpage.get_image_dir()\n        short_path = ntpath.basename(image_path[0])\n        name = os.path.splitext(short_path)[0]\n\n        webpage.add_header(name)\n        ims = []\n        txts = []\n        links = []\n\n        for label, image_numpy in visuals.items():\n            image_name = \'%s_%s.jpg\' % (name, label)\n            save_path = os.path.join(image_dir, image_name)\n            util.save_image(image_numpy, save_path)\n\n            ims.append(image_name)\n            txts.append(label)\n            links.append(image_name)\n        webpage.add_images(ims, txts, links, width=self.win_size)\n'"
models/ops/depthavgpooling/build.py,3,"b""import os\nimport torch\nfrom torch.utils.ffi import create_extension\n\nthis_file = os.path.dirname(__file__)\n\nsources = ['src/depthavgpooling.c']\nheaders = ['src/depthavgpooling.h']\ndefines = []\nwith_cuda = False\n\nif torch.cuda.is_available():\n    print('Including CUDA code.')\n    sources += ['src/depthavgpooling_cuda.c']\n    headers += ['src/depthavgpooling_cuda.h']\n    defines += [('WITH_CUDA', None)]\n    with_cuda = True\n\nthis_file = os.path.dirname(os.path.realpath(__file__))\nprint(this_file)\nextra_objects = ['src/depthavgpooling_cuda_kernel.cu.o']\nextra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\n\nffi = create_extension(\n    '_ext.depthavgpooling',\n    headers=headers,\n    sources=sources,\n    define_macros=defines,\n    relative_to=__file__,\n    with_cuda=with_cuda,\n    extra_objects=extra_objects\n)\n\nif __name__ == '__main__':\n    assert torch.cuda.is_available(), 'Please install CUDA for GPU support.'\n    ffi.build()\n"""
models/ops/depthconv/__init__.py,0,b''
models/ops/depthconv/build.py,3,"b""import os\nimport torch\nfrom torch.utils.ffi import create_extension\n\nthis_file = os.path.dirname(__file__)\n\nsources = ['src/depthconv.c']\nheaders = ['src/depthconv.h']\ndefines = []\nwith_cuda = False\n\nif torch.cuda.is_available():\n    print('Including CUDA code.')\n    sources += ['src/depthconv_cuda.c']\n    headers += ['src/depthconv_cuda.h']\n    defines += [('WITH_CUDA', None)]\n    with_cuda = True\n\nthis_file = os.path.dirname(os.path.realpath(__file__))\nprint(this_file)\nextra_objects = ['src/depthconv_cuda_kernel.cu.o']\nextra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\n\nffi = create_extension(\n    '_ext.depthconv',\n    headers=headers,\n    sources=sources,\n    define_macros=defines,\n    relative_to=__file__,\n    with_cuda=with_cuda,\n    extra_objects=extra_objects\n)\n\nif __name__ == '__main__':\n    assert torch.cuda.is_available(), 'Please install CUDA for GPU support.'\n    ffi.build()\n"""
models/ops/depthavgpooling/functions/__init__.py,0,b'from .depthavgpooling import depth_avgpooling\n'
models/ops/depthavgpooling/functions/depthavgpooling.py,4,"b'import torch\nfrom torch.autograd import Function\nfrom torch.nn.modules.utils import _pair\nimport cffi\nfrom .._ext import depthavgpooling\n\n\ndef depth_avgpooling( input,\n                      depth,\n                      kernel_size=3,\n                      stride=1,\n                      padding=0):\n\n    if input is not None and input.dim() != 4:\n        raise ValueError(\n            ""Expected 4D tensor as input, got {}D tensor instead."".format(\n                input.dim()))\n\n    f = DepthavgpoolingFunction(_pair(kernel_size), _pair(stride), _pair(padding))\n    return f(input, depth)\n\n\n\nclass DepthavgpoolingFunction(Function):\n    def __init__(self, kernel_size, stride, padding):\n        super(DepthavgpoolingFunction, self).__init__()\n        self.stride = stride\n        self.kernel_size = kernel_size\n        self.padding = padding\n\n    def forward(self, input, depth):\n        self.save_for_backward(input, depth)\n        self.depth = depth\n        output = input.new(*self._output_size(input))\n\n        self.depthweightcount = input.new(*(depth.size())).zero_()\n\n        if not input.is_cuda:\n            raise NotImplementedError\n        else:\n            if not isinstance(input, torch.cuda.FloatTensor):\n                raise NotImplementedError\n            depthavgpooling.depthavgpooling_forward_cuda(\n                    input, depth, output, self.depthweightcount,\n                    self.kernel_size[1], self.kernel_size[0], self.stride[1], self.stride[0],\n                    self.padding[1], self.padding[0])\n\n        return output\n\n    def backward(self, grad_output):\n        input, depth, = self.saved_tensors\n        grad_input = None\n\n        if not grad_output.is_cuda:\n            raise NotImplementedError\n        else:\n            if not isinstance(grad_output, torch.cuda.FloatTensor):\n                raise NotImplementedError\n            if self.needs_input_grad[0]:\n                grad_input = input.new(*input.size()).zero_()\n                depthavgpooling.depthavgpooling_backward_input_cuda(\n                    input, depth, self.depthweightcount,grad_output, grad_input,\n                    self.kernel_size[1], self.kernel_size[0], self.stride[1], self.stride[0],\n                    self.padding[1], self.padding[0])\n        # print \'grad_input\',grad_input\n        # print \'depthweightcount\',self.depthweightcount\n        return grad_input, None\n\n    def _output_size(self, input):\n\n        output_size = (input.size(0), input.size(0))\n        for d in range(input.dim() - 2):\n            in_size = input.size(d + 2)\n            pad = self.padding[d]\n            kernel = self.kernel_size[d]\n            stride = self.stride[d]\n            output_size += ((in_size + (2 * pad) - kernel) // stride + 1, )\n        if not all(map(lambda s: s > 0, output_size)):\n            raise ValueError(\n                ""avgpooling input is too small (output would be {})"".format(\n                    \'x\'.join(map(str, output_size))))\n        return output_size\n'"
models/ops/depthavgpooling/modules/__init__.py,0,b'from .depthavgpooling import Depthavgpooling\n'
models/ops/depthavgpooling/modules/depthavgpooling.py,3,"b'import math\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.modules.module import Module\nfrom torch.nn.modules.utils import _pair\nfrom ..functions import depth_avgpooling\n\n\nclass Depthavgpooling(Module):\n    def __init__(self,\n                 kernel_size,\n                 stride=1,\n                 padding=0):\n        super(Depthavgpooling, self).__init__()\n        self.kernel_size = _pair(kernel_size)\n        self.stride = _pair(stride)\n        self.padding = _pair(padding)\n\n    def forward(self, input, depth):\n        return depth_avgpooling(input, depth, self.kernel_size, self.stride, self.padding)\n'"
models/ops/depthconv/functions/__init__.py,0,b'from .depthconv import depth_conv\n'
models/ops/depthconv/functions/depthconv.py,5,"b'import torch\nfrom torch.autograd import Function\nfrom torch.nn.modules.utils import _pair\nimport cffi\nfrom .._ext import depthconv\n\n\ndef depth_conv(input,\n                  depth,\n                  weight,\n                  bias,\n                  stride=1,\n                  padding=0,\n                  dilation=1):\n\n    if input is not None and input.dim() != 4:\n        raise ValueError(\n            ""Expected 4D tensor as input, got {}D tensor instead."".format(\n                input.dim()))\n\n    f = DepthconvFunction(\n        _pair(stride), _pair(padding), _pair(dilation))\n    # print bias\n    if isinstance(bias, torch.nn.Parameter):\n        return f(input, depth, weight, bias)\n    else:\n        return f(input, depth, weight)\n\n\n\nclass DepthconvFunction(Function):\n    def __init__(self, stride, padding, dilation, bias=True):\n        super(DepthconvFunction, self).__init__()\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n        ffi_=cffi.FFI()\n        self.null = ffi_.NULL\n        self.bias = bias\n\n    def forward(self, input, depth, weight, bias = None):\n        # print(\'forward\')\n        self.save_for_backward(input, depth, weight, bias)\n        if (not self.bias) or (bias is None):\n            # print bias, self.bias\n            bias = self.null\n        output_size = [int((input.size()[i + 2] + 2 * self.padding[i] - weight.size()[i + 2]) / self.stride[i] + 1)\n                       for i in range(2)]\n\n        output = input.new(*self._output_size(input, weight))\n\n        self.columns = input.new(weight.size(1) * weight.size(2) * weight.size(3),\n                                  output_size[0] * output_size[1]).zero_()\n        self.ones = input.new(output_size[0] * output_size[1]).zero_()\n\n\n        if not input.is_cuda:\n            raise NotImplementedError\n        else:\n            if not isinstance(input, torch.cuda.FloatTensor):\n                raise NotImplementedError\n            depthconv.depthconv_forward_cuda(\n                    input, depth, weight, bias,  output, self.columns,self.ones,\n                    weight.size(3), weight.size(2), self.stride[1], self.stride[0],\n                    self.padding[1], self.padding[0], self.dilation[1], self.dilation[0])\n\n        return output\n\n    def backward(self, grad_output):\n        # print(\'backward\')\n        input, depth, weight, bias = self.saved_tensors\n\n        grad_input = grad_weight = grad_bias = None\n\n        if not grad_output.is_cuda:\n            raise NotImplementedError\n        else:\n            if not isinstance(grad_output, torch.cuda.FloatTensor):\n                raise NotImplementedError\n            if self.needs_input_grad[0]:\n                grad_input = input.new(*input.size()).zero_()\n                depthconv.depthconv_backward_input_cuda(\n                    input, depth, grad_output, grad_input,\n                    weight, self.columns,\n                    weight.size(3),\n                    weight.size(2), self.stride[1], self.stride[0],\n                    self.padding[1], self.padding[0], self.dilation[1],\n                    self.dilation[0])\n\n            if self.needs_input_grad[2]:\n                grad_weight = weight.new(*weight.size()).zero_()\n                if len(self.needs_input_grad) == 4:\n                    if self.needs_input_grad[3]:\n                        grad_bias = weight.new(*bias.size()).zero_()\n                    else:\n                        grad_bias = self.null\n                else:\n                    grad_bias = self.null\n\n                depthconv.depthconv_backward_parameters_cuda(\n                    input, depth, grad_output, grad_weight, grad_bias, self.columns,\n                    self.ones,\n                    weight.size(3),\n                    weight.size(2), self.stride[1], self.stride[0],\n                    self.padding[1], self.padding[0], self.dilation[1],\n                    self.dilation[0], 1)\n\n                if len(self.needs_input_grad) == 4:\n                    if not self.needs_input_grad[3]:\n                        grad_bias = None\n                else:\n                    grad_bias = None\n\n        return grad_input, None, grad_weight, grad_bias\n\n    def _output_size(self, input, weight):\n        channels = weight.size(0)\n\n        output_size = (input.size(0), channels)\n        for d in range(input.dim() - 2):\n            in_size = input.size(d + 2)\n            pad = self.padding[d]\n            kernel = self.dilation[d] * (weight.size(d + 2) - 1) + 1\n            stride = self.stride[d]\n            output_size += ((in_size + (2 * pad) - kernel) // stride + 1, )\n        if not all(map(lambda s: s > 0, output_size)):\n            raise ValueError(\n                ""convolution input is too small (output would be {})"".format(\n                    \'x\'.join(map(str, output_size))))\n        return output_size\n'"
models/ops/depthconv/modules/__init__.py,0,b'from .depthconv import DepthConv\n'
models/ops/depthconv/modules/depthconv.py,5,"b""import math\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.modules.module import Module\nfrom torch.nn.modules.utils import _pair\nfrom ..functions import depth_conv\n\n\nclass DepthConv(Module):\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 bias=True):\n        super(DepthConv, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = _pair(kernel_size)\n        self.stride = _pair(stride)\n        self.padding = _pair(padding)\n        self.dilation = _pair(dilation)\n\n        self.weight = nn.Parameter(\n            torch.Tensor(out_channels, in_channels, *self.kernel_size))\n\n        if bias:\n            self.bias = nn.Parameter(torch.Tensor(out_channels))\n        else:\n            self.register_parameter('bias', None)\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        n = self.in_channels\n        for k in self.kernel_size:\n            n *= k\n        stdv = 1. / math.sqrt(n)\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.uniform_(-stdv, stdv)\n\n    def forward(self, input, depth):\n        return depth_conv(input, depth, self.weight, self.bias, self.stride,\n                             self.padding, self.dilation)\n"""
