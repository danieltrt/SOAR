file_path,api_count,code
demo.py,4,"b'import argparse\nimport scipy.io\nimport torch\nimport numpy as np\nimport os\nfrom torchvision import datasets\nimport matplotlib\nmatplotlib.use(\'agg\')\nimport matplotlib.pyplot as plt\n#######################################################################\n# Evaluate\nparser = argparse.ArgumentParser(description=\'Demo\')\nparser.add_argument(\'--query_index\', default=777, type=int, help=\'test_image_index\')\nparser.add_argument(\'--test_dir\',default=\'../Market/pytorch\',type=str, help=\'./test_data\')\nopts = parser.parse_args()\n\ndata_dir = opts.test_dir\nimage_datasets = {x: datasets.ImageFolder( os.path.join(data_dir,x) ) for x in [\'gallery\',\'query\']}\n\n#####################################################################\n#Show result\ndef imshow(path, title=None):\n    """"""Imshow for Tensor.""""""\n    im = plt.imread(path)\n    plt.imshow(im)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\n######################################################################\nresult = scipy.io.loadmat(\'pytorch_result.mat\')\nquery_feature = torch.FloatTensor(result[\'query_f\'])\nquery_cam = result[\'query_cam\'][0]\nquery_label = result[\'query_label\'][0]\ngallery_feature = torch.FloatTensor(result[\'gallery_f\'])\ngallery_cam = result[\'gallery_cam\'][0]\ngallery_label = result[\'gallery_label\'][0]\n\nmulti = os.path.isfile(\'multi_query.mat\')\n\nif multi:\n    m_result = scipy.io.loadmat(\'multi_query.mat\')\n    mquery_feature = torch.FloatTensor(m_result[\'mquery_f\'])\n    mquery_cam = m_result[\'mquery_cam\'][0]\n    mquery_label = m_result[\'mquery_label\'][0]\n    mquery_feature = mquery_feature.cuda()\n\nquery_feature = query_feature.cuda()\ngallery_feature = gallery_feature.cuda()\n\n#######################################################################\n# sort the images\ndef sort_img(qf, ql, qc, gf, gl, gc):\n    query = qf.view(-1,1)\n    # print(query.shape)\n    score = torch.mm(gf,query)\n    score = score.squeeze(1).cpu()\n    score = score.numpy()\n    # predict index\n    index = np.argsort(score)  #from small to large\n    index = index[::-1]\n    # index = index[0:2000]\n    # good index\n    query_index = np.argwhere(gl==ql)\n    #same camera\n    camera_index = np.argwhere(gc==qc)\n\n    #good_index = np.setdiff1d(query_index, camera_index, assume_unique=True)\n    junk_index1 = np.argwhere(gl==-1)\n    junk_index2 = np.intersect1d(query_index, camera_index)\n    junk_index = np.append(junk_index2, junk_index1) \n\n    mask = np.in1d(index, junk_index, invert=True)\n    index = index[mask]\n    return index\n\ni = opts.query_index\nindex = sort_img(query_feature[i],query_label[i],query_cam[i],gallery_feature,gallery_label,gallery_cam)\n\n########################################################################\n# Visualize the rank result\n\nquery_path, _ = image_datasets[\'query\'].imgs[i]\nquery_label = query_label[i]\nprint(query_path)\nprint(\'Top 10 images are as follow:\')\ntry: # Visualize Ranking Result \n    # Graphical User Interface is needed\n    fig = plt.figure(figsize=(16,4))\n    ax = plt.subplot(1,11,1)\n    ax.axis(\'off\')\n    imshow(query_path,\'query\')\n    for i in range(10):\n        ax = plt.subplot(1,11,i+2)\n        ax.axis(\'off\')\n        img_path, _ = image_datasets[\'gallery\'].imgs[index[i]]\n        label = gallery_label[index[i]]\n        imshow(img_path)\n        if label == query_label:\n            ax.set_title(\'%d\'%(i+1), color=\'green\')\n        else:\n            ax.set_title(\'%d\'%(i+1), color=\'red\')\n        print(img_path)\nexcept RuntimeError:\n    for i in range(10):\n        img_path = image_datasets.imgs[index[i]]\n        print(img_path[0])\n    print(\'If you want to see the visualization of the ranking result, graphical user interface is needed.\')\n\nfig.savefig(""show.png"")\n'"
evaluate.py,3,"b""import scipy.io\nimport torch\nimport numpy as np\n#import time\nimport os\n\n#######################################################################\n# Evaluate\ndef evaluate(qf,ql,qc,gf,gl,gc):\n    query = qf\n    score = np.dot(gf,query)\n    # predict index\n    index = np.argsort(score)  #from small to large\n    index = index[::-1]\n    #index = index[0:2000]\n    # good index\n    query_index = np.argwhere(gl==ql)\n    camera_index = np.argwhere(gc==qc)\n\n    good_index = np.setdiff1d(query_index, camera_index, assume_unique=True)\n    junk_index1 = np.argwhere(gl==-1)\n    junk_index2 = np.intersect1d(query_index, camera_index)\n    junk_index = np.append(junk_index2, junk_index1) #.flatten())\n    \n    CMC_tmp = compute_mAP(index, good_index, junk_index)\n    return CMC_tmp\n\n\ndef compute_mAP(index, good_index, junk_index):\n    ap = 0\n    cmc = torch.IntTensor(len(index)).zero_()\n    if good_index.size==0:   # if empty\n        cmc[0] = -1\n        return ap,cmc\n\n    # remove junk_index\n    mask = np.in1d(index, junk_index, invert=True)\n    index = index[mask]\n\n    # find good_index index\n    ngood = len(good_index)\n    mask = np.in1d(index, good_index)\n    rows_good = np.argwhere(mask==True)\n    rows_good = rows_good.flatten()\n    \n    cmc[rows_good[0]:] = 1\n    for i in range(ngood):\n        d_recall = 1.0/ngood\n        precision = (i+1)*1.0/(rows_good[i]+1)\n        if rows_good[i]!=0:\n            old_precision = i*1.0/rows_good[i]\n        else:\n            old_precision=1.0\n        ap = ap + d_recall*(old_precision + precision)/2\n\n    return ap, cmc\n\n######################################################################\nresult = scipy.io.loadmat('pytorch_result.mat')\nquery_feature = result['query_f']\nquery_cam = result['query_cam'][0]\nquery_label = result['query_label'][0]\ngallery_feature = result['gallery_f']\ngallery_cam = result['gallery_cam'][0]\ngallery_label = result['gallery_label'][0]\n\nmulti = os.path.isfile('multi_query.mat')\n\nif multi:\n    m_result = scipy.io.loadmat('multi_query.mat')\n    mquery_feature = m_result['mquery_f']\n    mquery_cam = m_result['mquery_cam'][0]\n    mquery_label = m_result['mquery_label'][0]\n    \nCMC = torch.IntTensor(len(gallery_label)).zero_()\nap = 0.0\n#print(query_label)\nfor i in range(len(query_label)):\n    ap_tmp, CMC_tmp = evaluate(query_feature[i],query_label[i],query_cam[i],gallery_feature,gallery_label,gallery_cam)\n    if CMC_tmp[0]==-1:\n        continue\n    CMC = CMC + CMC_tmp\n    ap += ap_tmp\n    print(i, CMC_tmp[0])\n\nCMC = CMC.float()\nCMC = CMC/len(query_label) #average CMC\nprint('Rank@1:%f Rank@5:%f Rank@10:%f mAP:%f'%(CMC[0],CMC[4],CMC[9],ap/len(query_label)))\n\n# multiple-query\nCMC = torch.IntTensor(len(gallery_label)).zero_()\nap = 0.0\nif multi:\n    for i in range(len(query_label)):\n        mquery_index1 = np.argwhere(mquery_label==query_label[i])\n        mquery_index2 = np.argwhere(mquery_cam==query_cam[i])\n        mquery_index =  np.intersect1d(mquery_index1, mquery_index2)\n        mq = np.mean(mquery_feature[mquery_index,:], axis=0)\n        ap_tmp, CMC_tmp = evaluate(mq,query_label[i],query_cam[i],gallery_feature,gallery_label,gallery_cam)\n        if CMC_tmp[0]==-1:\n            continue\n        CMC = CMC + CMC_tmp\n        ap += ap_tmp\n        #print(i, CMC_tmp[0])\n    CMC = CMC.float()\n    CMC = CMC/len(query_label) #average CMC\n    print('multi Rank@1:%f Rank@5:%f Rank@10:%f mAP:%f'%(CMC[0],CMC[4],CMC[9],ap/len(query_label)))\n"""
evaluate_gpu.py,8,"b""import scipy.io\nimport torch\nimport numpy as np\n#import time\nimport os\n\n#######################################################################\n# Evaluate\ndef evaluate(qf,ql,qc,gf,gl,gc):\n    query = qf.view(-1,1)\n    # print(query.shape)\n    score = torch.mm(gf,query)\n    score = score.squeeze(1).cpu()\n    score = score.numpy()\n    # predict index\n    index = np.argsort(score)  #from small to large\n    index = index[::-1]\n    # index = index[0:2000]\n    # good index\n    query_index = np.argwhere(gl==ql)\n    camera_index = np.argwhere(gc==qc)\n\n    good_index = np.setdiff1d(query_index, camera_index, assume_unique=True)\n    junk_index1 = np.argwhere(gl==-1)\n    junk_index2 = np.intersect1d(query_index, camera_index)\n    junk_index = np.append(junk_index2, junk_index1) #.flatten())\n    \n    CMC_tmp = compute_mAP(index, good_index, junk_index)\n    return CMC_tmp\n\n\ndef compute_mAP(index, good_index, junk_index):\n    ap = 0\n    cmc = torch.IntTensor(len(index)).zero_()\n    if good_index.size==0:   # if empty\n        cmc[0] = -1\n        return ap,cmc\n\n    # remove junk_index\n    mask = np.in1d(index, junk_index, invert=True)\n    index = index[mask]\n\n    # find good_index index\n    ngood = len(good_index)\n    mask = np.in1d(index, good_index)\n    rows_good = np.argwhere(mask==True)\n    rows_good = rows_good.flatten()\n    \n    cmc[rows_good[0]:] = 1\n    for i in range(ngood):\n        d_recall = 1.0/ngood\n        precision = (i+1)*1.0/(rows_good[i]+1)\n        if rows_good[i]!=0:\n            old_precision = i*1.0/rows_good[i]\n        else:\n            old_precision=1.0\n        ap = ap + d_recall*(old_precision + precision)/2\n\n    return ap, cmc\n\n######################################################################\nresult = scipy.io.loadmat('pytorch_result.mat')\nquery_feature = torch.FloatTensor(result['query_f'])\nquery_cam = result['query_cam'][0]\nquery_label = result['query_label'][0]\ngallery_feature = torch.FloatTensor(result['gallery_f'])\ngallery_cam = result['gallery_cam'][0]\ngallery_label = result['gallery_label'][0]\n\nmulti = os.path.isfile('multi_query.mat')\n\nif multi:\n    m_result = scipy.io.loadmat('multi_query.mat')\n    mquery_feature = torch.FloatTensor(m_result['mquery_f'])\n    mquery_cam = m_result['mquery_cam'][0]\n    mquery_label = m_result['mquery_label'][0]\n    mquery_feature = mquery_feature.cuda()\n\nquery_feature = query_feature.cuda()\ngallery_feature = gallery_feature.cuda()\n\nprint(query_feature.shape)\nCMC = torch.IntTensor(len(gallery_label)).zero_()\nap = 0.0\n#print(query_label)\nfor i in range(len(query_label)):\n    ap_tmp, CMC_tmp = evaluate(query_feature[i],query_label[i],query_cam[i],gallery_feature,gallery_label,gallery_cam)\n    if CMC_tmp[0]==-1:\n        continue\n    CMC = CMC + CMC_tmp\n    ap += ap_tmp\n    #print(i, CMC_tmp[0])\n\nCMC = CMC.float()\nCMC = CMC/len(query_label) #average CMC\nprint('Rank@1:%f Rank@5:%f Rank@10:%f mAP:%f'%(CMC[0],CMC[4],CMC[9],ap/len(query_label)))\n\n# multiple-query\nCMC = torch.IntTensor(len(gallery_label)).zero_()\nap = 0.0\nif multi:\n    for i in range(len(query_label)):\n        mquery_index1 = np.argwhere(mquery_label==query_label[i])\n        mquery_index2 = np.argwhere(mquery_cam==query_cam[i])\n        mquery_index =  np.intersect1d(mquery_index1, mquery_index2)\n        mq = torch.mean(mquery_feature[mquery_index,:], dim=0)\n        ap_tmp, CMC_tmp = evaluate(mq,query_label[i],query_cam[i],gallery_feature,gallery_label,gallery_cam)\n        if CMC_tmp[0]==-1:\n            continue\n        CMC = CMC + CMC_tmp\n        ap += ap_tmp\n        #print(i, CMC_tmp[0])\n    CMC = CMC.float()\n    CMC = CMC/len(query_label) #average CMC\n    print('multi Rank@1:%f Rank@5:%f Rank@10:%f mAP:%f'%(CMC[0],CMC[4],CMC[9],ap/len(query_label)))\n"""
evaluate_rerank.py,2,"b""import scipy.io\nimport torch\nimport numpy as np\nimport time\nfrom  re_ranking import re_ranking\n#######################################################################\n# Evaluate\ndef evaluate(score,ql,qc,gl,gc):\n    index = np.argsort(score)  #from small to large\n    #index = index[::-1]\n    # good index\n    query_index = np.argwhere(gl==ql)\n    camera_index = np.argwhere(gc==qc)\n\n    good_index = np.setdiff1d(query_index, camera_index, assume_unique=True)\n    junk_index1 = np.argwhere(gl==-1)\n    junk_index2 = np.intersect1d(query_index, camera_index)\n    junk_index = np.append(junk_index2, junk_index1) #.flatten())\n    \n    CMC_tmp = compute_mAP(index, good_index, junk_index)\n    return CMC_tmp\n\n\ndef compute_mAP(index, good_index, junk_index):\n    ap = 0\n    cmc = torch.IntTensor(len(index)).zero_()\n    if good_index.size==0:   # if empty\n        cmc[0] = -1\n        return ap,cmc\n\n    # remove junk_index\n    mask = np.in1d(index, junk_index, invert=True)\n    index = index[mask]\n\n    # find good_index index\n    ngood = len(good_index)\n    mask = np.in1d(index, good_index)\n    rows_good = np.argwhere(mask==True)\n    rows_good = rows_good.flatten()\n    \n    cmc[rows_good[0]:] = 1\n    for i in range(ngood):\n        d_recall = 1.0/ngood\n        precision = (i+1)*1.0/(rows_good[i]+1)\n        if rows_good[i]!=0:\n            old_precision = i*1.0/rows_good[i]\n        else:\n            old_precision=1.0\n        ap = ap + d_recall*(old_precision + precision)/2\n\n    return ap, cmc\n\n######################################################################\nresult = scipy.io.loadmat('pytorch_result.mat')\nquery_feature = result['query_f']\nquery_cam = result['query_cam'][0]\nquery_label = result['query_label'][0]\ngallery_feature = result['gallery_f']\ngallery_cam = result['gallery_cam'][0]\ngallery_label = result['gallery_label'][0]\n\nCMC = torch.IntTensor(len(gallery_label)).zero_()\nap = 0.0\n#re-ranking\nprint('calculate initial distance')\nq_g_dist = np.dot(query_feature, np.transpose(gallery_feature))\nq_q_dist = np.dot(query_feature, np.transpose(query_feature))\ng_g_dist = np.dot(gallery_feature, np.transpose(gallery_feature))\nsince = time.time()\nre_rank = re_ranking(q_g_dist, q_q_dist, g_g_dist)\ntime_elapsed = time.time() - since\nprint('Reranking complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed // 60, time_elapsed % 60))\nfor i in range(len(query_label)):\n    ap_tmp, CMC_tmp = evaluate(re_rank[i,:],query_label[i],query_cam[i],gallery_label,gallery_cam)\n    if CMC_tmp[0]==-1:\n        continue\n    CMC = CMC + CMC_tmp\n    ap += ap_tmp\n    #print(i, CMC_tmp[0])\n\nCMC = CMC.float()\nCMC = CMC/len(query_label) #average CMC\nprint('top1:%f top5:%f top10:%f mAP:%f'%(CMC[0],CMC[4],CMC[9],ap/len(query_label)))\n"""
model.py,6,"b'import torch\nimport torch.nn as nn\nfrom torch.nn import init\nfrom torchvision import models\nfrom torch.autograd import Variable\nimport pretrainedmodels\n\n######################################################################\ndef weights_init_kaiming(m):\n    classname = m.__class__.__name__\n    # print(classname)\n    if classname.find(\'Conv\') != -1:\n        init.kaiming_normal_(m.weight.data, a=0, mode=\'fan_in\') # For old pytorch, you may use kaiming_normal.\n    elif classname.find(\'Linear\') != -1:\n        init.kaiming_normal_(m.weight.data, a=0, mode=\'fan_out\')\n        init.constant_(m.bias.data, 0.0)\n    elif classname.find(\'BatchNorm1d\') != -1:\n        init.normal_(m.weight.data, 1.0, 0.02)\n        init.constant_(m.bias.data, 0.0)\n\ndef weights_init_classifier(m):\n    classname = m.__class__.__name__\n    if classname.find(\'Linear\') != -1:\n        init.normal_(m.weight.data, std=0.001)\n        init.constant_(m.bias.data, 0.0)\n\n# Defines the new fc layer and classification layer\n# |--Linear--|--bn--|--relu--|--Linear--|\nclass ClassBlock(nn.Module):\n    def __init__(self, input_dim, class_num, droprate, relu=False, bnorm=True, num_bottleneck=512, linear=True, return_f = False):\n        super(ClassBlock, self).__init__()\n        self.return_f = return_f\n        add_block = []\n        if linear:\n            add_block += [nn.Linear(input_dim, num_bottleneck)]\n        else:\n            num_bottleneck = input_dim\n        if bnorm:\n            add_block += [nn.BatchNorm1d(num_bottleneck)]\n        if relu:\n            add_block += [nn.LeakyReLU(0.1)]\n        if droprate>0:\n            add_block += [nn.Dropout(p=droprate)]\n        add_block = nn.Sequential(*add_block)\n        add_block.apply(weights_init_kaiming)\n\n        classifier = []\n        classifier += [nn.Linear(num_bottleneck, class_num)]\n        classifier = nn.Sequential(*classifier)\n        classifier.apply(weights_init_classifier)\n\n        self.add_block = add_block\n        self.classifier = classifier\n    def forward(self, x):\n        x = self.add_block(x)\n        if self.return_f:\n            f = x\n            x = self.classifier(x)\n            return x,f\n        else:\n            x = self.classifier(x)\n            return x\n\n# Define the ResNet50-based Model\nclass ft_net(nn.Module):\n\n    def __init__(self, class_num, droprate=0.5, stride=2):\n        super(ft_net, self).__init__()\n        model_ft = models.resnet50(pretrained=True)\n        # avg pooling to global pooling\n        if stride == 1:\n            model_ft.layer4[0].downsample[0].stride = (1,1)\n            model_ft.layer4[0].conv2.stride = (1,1)\n        model_ft.avgpool = nn.AdaptiveAvgPool2d((1,1))\n        self.model = model_ft\n        self.classifier = ClassBlock(2048, class_num, droprate)\n\n    def forward(self, x):\n        x = self.model.conv1(x)\n        x = self.model.bn1(x)\n        x = self.model.relu(x)\n        x = self.model.maxpool(x)\n        x = self.model.layer1(x)\n        x = self.model.layer2(x)\n        x = self.model.layer3(x)\n        x = self.model.layer4(x)\n        x = self.model.avgpool(x)\n        x = x.view(x.size(0), x.size(1))\n        x = self.classifier(x)\n        return x\n\n# Define the DenseNet121-based Model\nclass ft_net_dense(nn.Module):\n\n    def __init__(self, class_num, droprate=0.5):\n        super().__init__()\n        model_ft = models.densenet121(pretrained=True)\n        model_ft.features.avgpool = nn.AdaptiveAvgPool2d((1,1))\n        model_ft.fc = nn.Sequential()\n        self.model = model_ft\n        # For DenseNet, the feature dim is 1024 \n        self.classifier = ClassBlock(1024, class_num, droprate)\n\n    def forward(self, x):\n        x = self.model.features(x)\n        x = x.view(x.size(0), x.size(1))\n        x = self.classifier(x)\n        return x\n\n# Define the NAS-based Model\nclass ft_net_NAS(nn.Module):\n\n    def __init__(self, class_num, droprate=0.5):\n        super().__init__()  \n        model_name = \'nasnetalarge\' \n        # pip install pretrainedmodels\n        model_ft = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained=\'imagenet\')\n        model_ft.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n        model_ft.dropout = nn.Sequential()\n        model_ft.last_linear = nn.Sequential()\n        self.model = model_ft\n        # For DenseNet, the feature dim is 4032\n        self.classifier = ClassBlock(4032, class_num, droprate)\n\n    def forward(self, x):\n        x = self.model.features(x)\n        x = self.model.avg_pool(x)\n        x = x.view(x.size(0), x.size(1))\n        x = self.classifier(x)\n        return x\n    \n# Define the ResNet50-based Model (Middle-Concat)\n# In the spirit of ""The Devil is in the Middle: Exploiting Mid-level Representations for Cross-Domain Instance Matching."" Yu, Qian, et al. arXiv:1711.08106 (2017).\nclass ft_net_middle(nn.Module):\n\n    def __init__(self, class_num, droprate=0.5):\n        super(ft_net_middle, self).__init__()\n        model_ft = models.resnet50(pretrained=True)\n        # avg pooling to global pooling\n        model_ft.avgpool = nn.AdaptiveAvgPool2d((1,1))\n        self.model = model_ft\n        self.classifier = ClassBlock(2048+1024, class_num, droprate)\n\n    def forward(self, x):\n        x = self.model.conv1(x)\n        x = self.model.bn1(x)\n        x = self.model.relu(x)\n        x = self.model.maxpool(x)\n        x = self.model.layer1(x)\n        x = self.model.layer2(x)\n        x = self.model.layer3(x)\n        # x0  n*1024*1*1\n        x0 = self.model.avgpool(x)\n        x = self.model.layer4(x)\n        # x1  n*2048*1*1\n        x1 = self.model.avgpool(x)\n        x = torch.cat((x0,x1),1)\n        x = x.view(x.size(0), x.size(1))\n        x = self.classifier(x)\n        return x\n\n# Part Model proposed in Yifan Sun etal. (2018)\nclass PCB(nn.Module):\n    def __init__(self, class_num ):\n        super(PCB, self).__init__()\n\n        self.part = 6 # We cut the pool5 to 6 parts\n        model_ft = models.resnet50(pretrained=True)\n        self.model = model_ft\n        self.avgpool = nn.AdaptiveAvgPool2d((self.part,1))\n        self.dropout = nn.Dropout(p=0.5)\n        # remove the final downsample\n        self.model.layer4[0].downsample[0].stride = (1,1)\n        self.model.layer4[0].conv2.stride = (1,1)\n        # define 6 classifiers\n        for i in range(self.part):\n            name = \'classifier\'+str(i)\n            setattr(self, name, ClassBlock(2048, class_num, droprate=0.5, relu=False, bnorm=True, num_bottleneck=256))\n\n    def forward(self, x):\n        x = self.model.conv1(x)\n        x = self.model.bn1(x)\n        x = self.model.relu(x)\n        x = self.model.maxpool(x)\n        \n        x = self.model.layer1(x)\n        x = self.model.layer2(x)\n        x = self.model.layer3(x)\n        x = self.model.layer4(x)\n        x = self.avgpool(x)\n        x = self.dropout(x)\n        part = {}\n        predict = {}\n        # get six part feature batchsize*2048*6\n        for i in range(self.part):\n            part[i] = torch.squeeze(x[:,:,i])\n            name = \'classifier\'+str(i)\n            c = getattr(self,name)\n            predict[i] = c(part[i])\n\n        # sum prediction\n        #y = predict[0]\n        #for i in range(self.part-1):\n        #    y += predict[i+1]\n        y = []\n        for i in range(self.part):\n            y.append(predict[i])\n        return y\n\nclass PCB_test(nn.Module):\n    def __init__(self,model):\n        super(PCB_test,self).__init__()\n        self.part = 6\n        self.model = model.model\n        self.avgpool = nn.AdaptiveAvgPool2d((self.part,1))\n        # remove the final downsample\n        self.model.layer4[0].downsample[0].stride = (1,1)\n        self.model.layer4[0].conv2.stride = (1,1)\n\n    def forward(self, x):\n        x = self.model.conv1(x)\n        x = self.model.bn1(x)\n        x = self.model.relu(x)\n        x = self.model.maxpool(x)\n\n        x = self.model.layer1(x)\n        x = self.model.layer2(x)\n        x = self.model.layer3(x)\n        x = self.model.layer4(x)\n        x = self.avgpool(x)\n        y = x.view(x.size(0),x.size(1),x.size(2))\n        return y\n\'\'\'\n# debug model structure\n# Run this code with:\npython model.py\n\'\'\'\nif __name__ == \'__main__\':\n# Here I left a simple forward function.\n# Test the model, before you train it. \n    net = ft_net(751, stride=1)\n    net.classifier = nn.Sequential()\n    print(net)\n    input = Variable(torch.FloatTensor(8, 3, 256, 128))\n    output = net(input)\n    print(\'net output size:\')\n    print(output.shape)\n'"
prepare.py,0,"b""import os\nfrom shutil import copyfile\n\n# You only need to change this line to your dataset download path\ndownload_path = '../Market'\n\nif not os.path.isdir(download_path):\n    print('please change the download_path')\n\nsave_path = download_path + '/pytorch'\nif not os.path.isdir(save_path):\n    os.mkdir(save_path)\n#-----------------------------------------\n#query\nquery_path = download_path + '/query'\nquery_save_path = download_path + '/pytorch/query'\nif not os.path.isdir(query_save_path):\n    os.mkdir(query_save_path)\n\nfor root, dirs, files in os.walk(query_path, topdown=True):\n    for name in files:\n        if not name[-3:]=='jpg':\n            continue\n        ID  = name.split('_')\n        src_path = query_path + '/' + name\n        dst_path = query_save_path + '/' + ID[0] \n        if not os.path.isdir(dst_path):\n            os.mkdir(dst_path)\n        copyfile(src_path, dst_path + '/' + name)\n\n#-----------------------------------------\n#multi-query\nquery_path = download_path + '/gt_bbox'\n# for dukemtmc-reid, we do not need multi-query\nif os.path.isdir(query_path):\n    query_save_path = download_path + '/pytorch/multi-query'\n    if not os.path.isdir(query_save_path):\n        os.mkdir(query_save_path)\n\n    for root, dirs, files in os.walk(query_path, topdown=True):\n        for name in files:\n            if not name[-3:]=='jpg':\n                continue\n            ID  = name.split('_')\n            src_path = query_path + '/' + name\n            dst_path = query_save_path + '/' + ID[0]\n            if not os.path.isdir(dst_path):\n                os.mkdir(dst_path)\n            copyfile(src_path, dst_path + '/' + name)\n\n#-----------------------------------------\n#gallery\ngallery_path = download_path + '/bounding_box_test'\ngallery_save_path = download_path + '/pytorch/gallery'\nif not os.path.isdir(gallery_save_path):\n    os.mkdir(gallery_save_path)\n\nfor root, dirs, files in os.walk(gallery_path, topdown=True):\n    for name in files:\n        if not name[-3:]=='jpg':\n            continue\n        ID  = name.split('_')\n        src_path = gallery_path + '/' + name\n        dst_path = gallery_save_path + '/' + ID[0]\n        if not os.path.isdir(dst_path):\n            os.mkdir(dst_path)\n        copyfile(src_path, dst_path + '/' + name)\n\n#---------------------------------------\n#train_all\ntrain_path = download_path + '/bounding_box_train'\ntrain_save_path = download_path + '/pytorch/train_all'\nif not os.path.isdir(train_save_path):\n    os.mkdir(train_save_path)\n\nfor root, dirs, files in os.walk(train_path, topdown=True):\n    for name in files:\n        if not name[-3:]=='jpg':\n            continue\n        ID  = name.split('_')\n        src_path = train_path + '/' + name\n        dst_path = train_save_path + '/' + ID[0]\n        if not os.path.isdir(dst_path):\n            os.mkdir(dst_path)\n        copyfile(src_path, dst_path + '/' + name)\n\n\n#---------------------------------------\n#train_val\ntrain_path = download_path + '/bounding_box_train'\ntrain_save_path = download_path + '/pytorch/train'\nval_save_path = download_path + '/pytorch/val'\nif not os.path.isdir(train_save_path):\n    os.mkdir(train_save_path)\n    os.mkdir(val_save_path)\n\nfor root, dirs, files in os.walk(train_path, topdown=True):\n    for name in files:\n        if not name[-3:]=='jpg':\n            continue\n        ID  = name.split('_')\n        src_path = train_path + '/' + name\n        dst_path = train_save_path + '/' + ID[0]\n        if not os.path.isdir(dst_path):\n            os.mkdir(dst_path)\n            dst_path = val_save_path + '/' + ID[0]  #first image is used as val image\n            os.mkdir(dst_path)\n        copyfile(src_path, dst_path + '/' + name)\n"""
prepare_static.py,8,"b""# -*- coding: utf-8 -*-\n\nfrom __future__ import print_function, division\n\nimport argparse\nimport torch\nfrom torchvision import datasets, transforms\nimport time\nimport os\n\nversion =  torch.__version__\n\n######################################################################\n# Options\n# --------\nparser = argparse.ArgumentParser(description='Training')\nparser.add_argument('--data_dir',default='/home/zzd/Market/pytorch',type=str, help='training dir path')\nparser.add_argument('--train_all', action='store_true', help='use all training data' )\nparser.add_argument('--color_jitter', action='store_true', help='use color jitter in training' )\nparser.add_argument('--batchsize', default=128, type=int, help='batchsize')\nopt = parser.parse_args()\n\ndata_dir = opt.data_dir\n\n######################################################################\n# Load Data\n# ---------\n#\n\ntransform_train_list = [\n        #transforms.RandomResizedCrop(size=128, scale=(0.75,1.0), ratio=(0.75,1.3333), interpolation=3), #Image.BICUBIC)\n        transforms.Resize((288,144), interpolation=3),\n        #transforms.RandomCrop((256,128)),\n        #transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]\n\ntransform_val_list = [\n        transforms.Resize(size=(256,128),interpolation=3), #Image.BICUBIC\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]\n\n\nprint(transform_train_list)\ndata_transforms = {\n    'train': transforms.Compose( transform_train_list ),\n    'val': transforms.Compose(transform_val_list),\n}\n\n\ntrain_all = ''\nif opt.train_all:\n     train_all = '_all'\n\nimage_datasets = {}\nimage_datasets['train'] = datasets.ImageFolder(os.path.join(data_dir, 'train' + train_all),\n                                          data_transforms['train'])\nimage_datasets['val'] = datasets.ImageFolder(os.path.join(data_dir, 'val'),\n                                          data_transforms['val'])\n\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=opt.batchsize,\n                                             shuffle=True, num_workers=16)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].classes\n\nuse_gpu = torch.cuda.is_available()\n\n######################################################################\n# prepare_dataset\n# ------------------\n#\n# Now, let's write a general function to train a model. Here, we will\n# illustrate:\n#\n# -  Scheduling the learning rate\n# -  Saving the best model\n#\n# In the following, parameter ``scheduler`` is an LR scheduler object from\n# ``torch.optim.lr_scheduler``.\n\ndef prepare_model():\n    since = time.time()\n\n    num_epochs = 1\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train']:\n\n            mean = torch.zeros(3)\n            std = torch.zeros(3)\n            # Iterate over data.\n            for data in dataloaders[phase]:\n                # get the inputs\n                inputs, labels = data\n                now_batch_size,c,h,w = inputs.shape\n                mean += torch.sum(torch.mean(torch.mean(inputs,dim=3),dim=2),dim=0)\n                std += torch.sum(torch.std(inputs.view(now_batch_size,c,h*w),dim=2),dim=0)\n                \n            print(mean/dataset_sizes['train'])\n            print(std/dataset_sizes['train'])\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed // 60, time_elapsed % 60))\n    return \n\n\n\nprepare_model()\n"""
prepare_viper.py,0,"b""import os\nfrom shutil import copyfile\nimport numpy as np\n\n\n#http://users.soe.ucsc.edu/~manduchi/VIPeR.v1.0.zip\ndownload_path = '../VIPeR'\n\nif not os.path.isdir(download_path):\n    print('please change the download_path')\n\nsave_path = download_path + '/pytorch'\nif not os.path.isdir(save_path):\n    os.mkdir(save_path)\n\n\n#train_all\nID_list = []\ntrain_path = download_path + '/cam_a'\ntrain_save_path = download_path + '/pytorch/all'\nif not os.path.isdir(train_save_path):\n    os.mkdir(train_save_path)\n\nfor root, dirs, files in os.walk(train_path, topdown=True):\n    for name in files:\n        if not name[-3:]=='bmp':\n            continue\n        ID  = name.split('_')\n        src_path = train_path + '/' + name\n        dst_path = train_save_path + '/' + ID[0]\n        if not os.path.isdir(dst_path):\n            os.mkdir(dst_path)\n            ID_list.append(ID[0])\n        copyfile(src_path, dst_path + '/' + 'ca_'+name)\n\n\ntrain_path = download_path + '/cam_b'\ntrain_save_path = download_path + '/pytorch/all'\nif not os.path.isdir(train_save_path):\n    os.mkdir(train_save_path)\n\nfor root, dirs, files in os.walk(train_path, topdown=True):\n    for name in files:\n        if not name[-3:]=='bmp':\n            continue\n        ID  = name.split('_')\n        src_path = train_path + '/' + name\n        dst_path = train_save_path + '/' + ID[0]\n        if not os.path.isdir(dst_path):\n            os.mkdir(dst_path)\n        copyfile(src_path, dst_path + '/' + 'cb_'+name)\n\nnp.random.seed(0)\n\nfor ii in range(5):\n    index = np.random.permutation(632)\n    test_save_path = download_path + '/test%d'%ii\n    if not os.path.isdir(test_save_path):\n        os.mkdir(test_save_path)\n        os.mkdir(test_save_path + '/query')\n        os.mkdir(test_save_path + '/gallery')\n\n    for i in range(316):\n        dir_name = ID_list[i]\n        src_path_1 = train_save_path + '/' + dir_name + '/ca_*.bmp'\n        src_path_2 = train_save_path + '/' + dir_name + '/cb_*.bmp'\n        dst_path_1 = test_save_path+'/query/'\n        dst_path_2 = test_save_path+'/gallery/'\n        os.system('cp %s %s'%(src_path_1, dst_path_1))\n        os.system('cp %s %s'%(src_path_2, dst_path_2))\n\n"""
random_erasing.py,0,"b'from __future__ import absolute_import\n\nfrom torchvision.transforms import *\n\n#from PIL import Image\nimport random\nimport math\n#import numpy as np\n#import torch\n\nclass RandomErasing(object):\n    """""" Randomly selects a rectangle region in an image and erases its pixels.\n        \'Random Erasing Data Augmentation\' by Zhong et al.\n        See https://arxiv.org/pdf/1708.04896.pdf\n    Args:\n         probability: The probability that the Random Erasing operation will be performed.\n         sl: Minimum proportion of erased area against input image.\n         sh: Maximum proportion of erased area against input image.\n         r1: Minimum aspect ratio of erased area.\n         mean: Erasing value. \n    """"""\n    \n    def __init__(self, probability = 0.5, sl = 0.02, sh = 0.4, r1 = 0.3, mean=[0.4914, 0.4822, 0.4465]):\n        self.probability = probability\n        self.mean = mean\n        self.sl = sl\n        self.sh = sh\n        self.r1 = r1\n       \n    def __call__(self, img):\n\n        if random.uniform(0, 1) > self.probability:\n            return img\n\n        for attempt in range(100):\n            area = img.size()[1] * img.size()[2]\n       \n            target_area = random.uniform(self.sl, self.sh) * area\n            aspect_ratio = random.uniform(self.r1, 1/self.r1)\n\n            h = int(round(math.sqrt(target_area * aspect_ratio)))\n            w = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if w < img.size()[2] and h < img.size()[1]:\n                x1 = random.randint(0, img.size()[1] - h)\n                y1 = random.randint(0, img.size()[2] - w)\n                if img.size()[0] == 3:\n                    img[0, x1:x1+h, y1:y1+w] = self.mean[0]\n                    img[1, x1:x1+h, y1:y1+w] = self.mean[1]\n                    img[2, x1:x1+h, y1:y1+w] = self.mean[2]\n                else:\n                    img[0, x1:x1+h, y1:y1+w] = self.mean[0]\n                return img\n\n        return img\n'"
re_ranking.py,0,"b'#!/usr/bin/env python2/python3\n# -*- coding: utf-8 -*-\n""""""\nCreated on Mon Jun 26 14:46:56 2017\n@author: luohao\nModified by Houjing Huang, 2017-12-22. \n- This version accepts distance matrix instead of raw features. \n- The difference of `/` division between python 2 and 3 is handled.\n- numpy.float16 is replaced by numpy.float32 for numerical precision.\n\nModified by Zhedong Zheng, 2018-1-12.\n- replace sort with topK, which save about 30s.\n""""""\n\n""""""\nCVPR2017 paper:Zhong Z, Zheng L, Cao D, et al. Re-ranking Person Re-identification with k-reciprocal Encoding[J]. 2017.\nurl:http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhong_Re-Ranking_Person_Re-Identification_CVPR_2017_paper.pdf\nMatlab version: https://github.com/zhunzhong07/person-re-ranking\n""""""\n\n""""""\nAPI\nq_g_dist: query-gallery distance matrix, numpy array, shape [num_query, num_gallery]\nq_q_dist: query-query distance matrix, numpy array, shape [num_query, num_query]\ng_g_dist: gallery-gallery distance matrix, numpy array, shape [num_gallery, num_gallery]\nk1, k2, lambda_value: parameters, the original paper is (k1=20, k2=6, lambda_value=0.3)\nReturns:\n  final_dist: re-ranked distance, numpy array, shape [num_query, num_gallery]\n""""""\n\n\nimport numpy as np\n\ndef k_reciprocal_neigh( initial_rank, i, k1):\n    forward_k_neigh_index = initial_rank[i,:k1+1]\n    backward_k_neigh_index = initial_rank[forward_k_neigh_index,:k1+1]\n    fi = np.where(backward_k_neigh_index==i)[0]\n    return forward_k_neigh_index[fi]\n\ndef re_ranking(q_g_dist, q_q_dist, g_g_dist, k1=20, k2=6, lambda_value=0.3):\n    # The following naming, e.g. gallery_num, is different from outer scope.\n    # Don\'t care about it.\n    original_dist = np.concatenate(\n      [np.concatenate([q_q_dist, q_g_dist], axis=1),\n       np.concatenate([q_g_dist.T, g_g_dist], axis=1)],\n      axis=0)\n    original_dist = 2. - 2 * original_dist   # change the cosine similarity metric to euclidean similarity metric\n    original_dist = np.power(original_dist, 2).astype(np.float32)\n    original_dist = np.transpose(1. * original_dist/np.max(original_dist,axis = 0))\n    V = np.zeros_like(original_dist).astype(np.float32)\n    #initial_rank = np.argsort(original_dist).astype(np.int32)\n    # top K1+1\n    initial_rank = np.argpartition( original_dist, range(1,k1+1) )\n\n    query_num = q_g_dist.shape[0]\n    all_num = original_dist.shape[0]\n\n    for i in range(all_num):\n        # k-reciprocal neighbors\n        k_reciprocal_index = k_reciprocal_neigh( initial_rank, i, k1)\n        k_reciprocal_expansion_index = k_reciprocal_index\n        for j in range(len(k_reciprocal_index)):\n            candidate = k_reciprocal_index[j]\n            candidate_k_reciprocal_index = k_reciprocal_neigh( initial_rank, candidate, int(np.around(k1/2)))\n            if len(np.intersect1d(candidate_k_reciprocal_index,k_reciprocal_index))> 2./3*len(candidate_k_reciprocal_index):\n                k_reciprocal_expansion_index = np.append(k_reciprocal_expansion_index,candidate_k_reciprocal_index)\n\n        k_reciprocal_expansion_index = np.unique(k_reciprocal_expansion_index)\n        weight = np.exp(-original_dist[i,k_reciprocal_expansion_index])\n        V[i,k_reciprocal_expansion_index] = 1.*weight/np.sum(weight)\n\n    original_dist = original_dist[:query_num,]\n    if k2 != 1:\n        V_qe = np.zeros_like(V,dtype=np.float32)\n        for i in range(all_num):\n            V_qe[i,:] = np.mean(V[initial_rank[i,:k2],:],axis=0)\n        V = V_qe\n        del V_qe\n    del initial_rank\n    invIndex = []\n    for i in range(all_num):\n        invIndex.append(np.where(V[:,i] != 0)[0])\n\n    jaccard_dist = np.zeros_like(original_dist,dtype = np.float32)\n\n    for i in range(query_num):\n        temp_min = np.zeros(shape=[1,all_num],dtype=np.float32)\n        indNonZero = np.where(V[i,:] != 0)[0]\n        indImages = []\n        indImages = [invIndex[ind] for ind in indNonZero]\n        for j in range(len(indNonZero)):\n            temp_min[0,indImages[j]] = temp_min[0,indImages[j]]+ np.minimum(V[i,indNonZero[j]],V[indImages[j],indNonZero[j]])\n        jaccard_dist[i] = 1-temp_min/(2.-temp_min)\n\n    final_dist = jaccard_dist*(1-lambda_value) + original_dist*lambda_value\n    del original_dist\n    del V\n    del jaccard_dist\n    final_dist = final_dist[:query_num,query_num:]\n    return final_dist\n'"
test.py,21,"b""# -*- coding: utf-8 -*-\n\nfrom __future__ import print_function, division\n\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport time\nimport os\nimport scipy.io\nimport yaml\nimport math\nfrom model import ft_net, ft_net_dense, ft_net_NAS, PCB, PCB_test\n\n#fp16\ntry:\n    from apex.fp16_utils import *\nexcept ImportError: # will be 3.x series\n    print('This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0')\n######################################################################\n# Options\n# --------\n\nparser = argparse.ArgumentParser(description='Test')\nparser.add_argument('--gpu_ids',default='0', type=str,help='gpu_ids: e.g. 0  0,1,2  0,2')\nparser.add_argument('--which_epoch',default='last', type=str, help='0,1,2,3...or last')\nparser.add_argument('--test_dir',default='../Market/pytorch',type=str, help='./test_data')\nparser.add_argument('--name', default='ft_ResNet50', type=str, help='save model path')\nparser.add_argument('--batchsize', default=256, type=int, help='batchsize')\nparser.add_argument('--use_dense', action='store_true', help='use densenet121' )\nparser.add_argument('--PCB', action='store_true', help='use PCB' )\nparser.add_argument('--multi', action='store_true', help='use multiple query' )\nparser.add_argument('--fp16', action='store_true', help='use fp16.' )\nparser.add_argument('--ms',default='1', type=str,help='multiple_scale: e.g. 1 1,1.1  1,1.1,1.2')\n\nopt = parser.parse_args()\n###load config###\n# load the training config\nconfig_path = os.path.join('./model',opt.name,'opts.yaml')\nwith open(config_path, 'r') as stream:\n        config = yaml.load(stream)\nopt.fp16 = config['fp16'] \nopt.PCB = config['PCB']\nopt.use_dense = config['use_dense']\nopt.use_NAS = config['use_NAS']\nopt.stride = config['stride']\n\nif 'nclasses' in config: # tp compatible with old config files\n    opt.nclasses = config['nclasses']\nelse: \n    opt.nclasses = 751 \n\nstr_ids = opt.gpu_ids.split(',')\n#which_epoch = opt.which_epoch\nname = opt.name\ntest_dir = opt.test_dir\n\ngpu_ids = []\nfor str_id in str_ids:\n    id = int(str_id)\n    if id >=0:\n        gpu_ids.append(id)\n\nprint('We use the scale: %s'%opt.ms)\nstr_ms = opt.ms.split(',')\nms = []\nfor s in str_ms:\n    s_f = float(s)\n    ms.append(math.sqrt(s_f))\n\n# set gpu ids\nif len(gpu_ids)>0:\n    torch.cuda.set_device(gpu_ids[0])\n    cudnn.benchmark = True\n\n######################################################################\n# Load Data\n# ---------\n#\n# We will use torchvision and torch.utils.data packages for loading the\n# data.\n#\ndata_transforms = transforms.Compose([\n        transforms.Resize((256,128), interpolation=3),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n############### Ten Crop        \n        #transforms.TenCrop(224),\n        #transforms.Lambda(lambda crops: torch.stack(\n         #   [transforms.ToTensor()(crop) \n          #      for crop in crops]\n           # )),\n        #transforms.Lambda(lambda crops: torch.stack(\n         #   [transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(crop)\n          #       for crop in crops]\n          # ))\n])\n\nif opt.PCB:\n    data_transforms = transforms.Compose([\n        transforms.Resize((384,192), interpolation=3),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n    ])\n\n\ndata_dir = test_dir\n\nif opt.multi:\n    image_datasets = {x: datasets.ImageFolder( os.path.join(data_dir,x) ,data_transforms) for x in ['gallery','query','multi-query']}\n    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=opt.batchsize,\n                                             shuffle=False, num_workers=16) for x in ['gallery','query','multi-query']}\nelse:\n    image_datasets = {x: datasets.ImageFolder( os.path.join(data_dir,x) ,data_transforms) for x in ['gallery','query']}\n    dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=opt.batchsize,\n                                             shuffle=False, num_workers=16) for x in ['gallery','query']}\nclass_names = image_datasets['query'].classes\nuse_gpu = torch.cuda.is_available()\n\n######################################################################\n# Load model\n#---------------------------\ndef load_network(network):\n    save_path = os.path.join('./model',name,'net_%s.pth'%opt.which_epoch)\n    network.load_state_dict(torch.load(save_path))\n    return network\n\n\n######################################################################\n# Extract feature\n# ----------------------\n#\n# Extract feature from  a trained model.\n#\ndef fliplr(img):\n    '''flip horizontal'''\n    inv_idx = torch.arange(img.size(3)-1,-1,-1).long()  # N x C x H x W\n    img_flip = img.index_select(3,inv_idx)\n    return img_flip\n\ndef extract_feature(model,dataloaders):\n    features = torch.FloatTensor()\n    count = 0\n    for data in dataloaders:\n        img, label = data\n        n, c, h, w = img.size()\n        count += n\n        print(count)\n        ff = torch.FloatTensor(n,512).zero_().cuda()\n        if opt.PCB:\n            ff = torch.FloatTensor(n,2048,6).zero_().cuda() # we have six parts\n\n        for i in range(2):\n            if(i==1):\n                img = fliplr(img)\n            input_img = Variable(img.cuda())\n            for scale in ms:\n                if scale != 1:\n                    # bicubic is only  available in pytorch>= 1.1\n                    input_img = nn.functional.interpolate(input_img, scale_factor=scale, mode='bicubic', align_corners=False)\n                outputs = model(input_img) \n                ff += outputs\n        # norm feature\n        if opt.PCB:\n            # feature size (n,2048,6)\n            # 1. To treat every part equally, I calculate the norm for every 2048-dim part feature.\n            # 2. To keep the cosine score==1, sqrt(6) is added to norm the whole feature (2048*6).\n            fnorm = torch.norm(ff, p=2, dim=1, keepdim=True) * np.sqrt(6) \n            ff = ff.div(fnorm.expand_as(ff))\n            ff = ff.view(ff.size(0), -1)\n        else:\n            fnorm = torch.norm(ff, p=2, dim=1, keepdim=True)\n            ff = ff.div(fnorm.expand_as(ff))\n\n        features = torch.cat((features,ff.data.cpu()), 0)\n    return features\n\ndef get_id(img_path):\n    camera_id = []\n    labels = []\n    for path, v in img_path:\n        #filename = path.split('/')[-1]\n        filename = os.path.basename(path)\n        label = filename[0:4]\n        camera = filename.split('c')[1]\n        if label[0:2]=='-1':\n            labels.append(-1)\n        else:\n            labels.append(int(label))\n        camera_id.append(int(camera[0]))\n    return camera_id, labels\n\ngallery_path = image_datasets['gallery'].imgs\nquery_path = image_datasets['query'].imgs\n\ngallery_cam,gallery_label = get_id(gallery_path)\nquery_cam,query_label = get_id(query_path)\n\nif opt.multi:\n    mquery_path = image_datasets['multi-query'].imgs\n    mquery_cam,mquery_label = get_id(mquery_path)\n\n######################################################################\n# Load Collected data Trained model\nprint('-------test-----------')\nif opt.use_dense:\n    model_structure = ft_net_dense(opt.nclasses)\nelif opt.use_NAS:\n    model_structure = ft_net_NAS(opt.nclasses)\nelse:\n    model_structure = ft_net(opt.nclasses, stride = opt.stride)\n\nif opt.PCB:\n    model_structure = PCB(opt.nclasses)\n\n#if opt.fp16:\n#    model_structure = network_to_half(model_structure)\n\nmodel = load_network(model_structure)\n\n# Remove the final fc layer and classifier layer\nif opt.PCB:\n    #if opt.fp16:\n    #    model = PCB_test(model[1])\n    #else:\n        model = PCB_test(model)\nelse:\n    #if opt.fp16:\n        #model[1].model.fc = nn.Sequential()\n        #model[1].classifier = nn.Sequential()\n    #else:\n        model.classifier.classifier = nn.Sequential()\n\n# Change to test mode\nmodel = model.eval()\nif use_gpu:\n    model = model.cuda()\n\n# Extract feature\nwith torch.no_grad():\n    gallery_feature = extract_feature(model,dataloaders['gallery'])\n    query_feature = extract_feature(model,dataloaders['query'])\n    if opt.multi:\n        mquery_feature = extract_feature(model,dataloaders['multi-query'])\n    \n# Save to Matlab for check\nresult = {'gallery_f':gallery_feature.numpy(),'gallery_label':gallery_label,'gallery_cam':gallery_cam,'query_f':query_feature.numpy(),'query_label':query_label,'query_cam':query_cam}\nscipy.io.savemat('pytorch_result.mat',result)\n\nprint(opt.name)\nresult = './model/%s/result.txt'%opt.name\nos.system('python evaluate_gpu.py | tee -a %s'%result)\n\nif opt.multi:\n    result = {'mquery_f':mquery_feature.numpy(),'mquery_label':mquery_label,'mquery_cam':mquery_cam}\n    scipy.io.savemat('multi_query.mat',result)\n"""
train.py,16,"b'# -*- coding: utf-8 -*-\n\nfrom __future__ import print_function, division\n\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\nfrom torchvision import datasets, transforms\nimport torch.backends.cudnn as cudnn\nimport matplotlib\nmatplotlib.use(\'agg\')\nimport matplotlib.pyplot as plt\n#from PIL import Image\nimport time\nimport os\nfrom model import ft_net, ft_net_dense, ft_net_NAS, PCB\nfrom random_erasing import RandomErasing\nimport yaml\nimport math\nfrom shutil import copyfile\n\nversion =  torch.__version__\n#fp16\ntry:\n    from apex.fp16_utils import *\n    from apex import amp, optimizers\nexcept ImportError: # will be 3.x series\n    print(\'This is not an error. If you want to use low precision, i.e., fp16, please install the apex with cuda support (https://github.com/NVIDIA/apex) and update pytorch to 1.0\')\n######################################################################\n# Options\n# --------\nparser = argparse.ArgumentParser(description=\'Training\')\nparser.add_argument(\'--gpu_ids\',default=\'0\', type=str,help=\'gpu_ids: e.g. 0  0,1,2  0,2\')\nparser.add_argument(\'--name\',default=\'ft_ResNet50\', type=str, help=\'output model name\')\nparser.add_argument(\'--data_dir\',default=\'../Market/pytorch\',type=str, help=\'training dir path\')\nparser.add_argument(\'--train_all\', action=\'store_true\', help=\'use all training data\' )\nparser.add_argument(\'--color_jitter\', action=\'store_true\', help=\'use color jitter in training\' )\nparser.add_argument(\'--batchsize\', default=32, type=int, help=\'batchsize\')\nparser.add_argument(\'--stride\', default=2, type=int, help=\'stride\')\nparser.add_argument(\'--erasing_p\', default=0, type=float, help=\'Random Erasing probability, in [0,1]\')\nparser.add_argument(\'--use_dense\', action=\'store_true\', help=\'use densenet121\' )\nparser.add_argument(\'--use_NAS\', action=\'store_true\', help=\'use NAS\' )\nparser.add_argument(\'--warm_epoch\', default=0, type=int, help=\'the first K epoch that needs warm up\')\nparser.add_argument(\'--lr\', default=0.05, type=float, help=\'learning rate\')\nparser.add_argument(\'--droprate\', default=0.5, type=float, help=\'drop rate\')\nparser.add_argument(\'--PCB\', action=\'store_true\', help=\'use PCB+ResNet50\' )\nparser.add_argument(\'--fp16\', action=\'store_true\', help=\'use float16 instead of float32, which will save about 50% memory\' )\nopt = parser.parse_args()\n\nfp16 = opt.fp16\ndata_dir = opt.data_dir\nname = opt.name\nstr_ids = opt.gpu_ids.split(\',\')\ngpu_ids = []\nfor str_id in str_ids:\n    gid = int(str_id)\n    if gid >=0:\n        gpu_ids.append(gid)\n\n# set gpu ids\nif len(gpu_ids)>0:\n    torch.cuda.set_device(gpu_ids[0])\n    cudnn.benchmark = True\n######################################################################\n# Load Data\n# ---------\n#\n\ntransform_train_list = [\n        #transforms.RandomResizedCrop(size=128, scale=(0.75,1.0), ratio=(0.75,1.3333), interpolation=3), #Image.BICUBIC)\n        transforms.Resize((256,128), interpolation=3),\n        transforms.Pad(10),\n        transforms.RandomCrop((256,128)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]\n\ntransform_val_list = [\n        transforms.Resize(size=(256,128),interpolation=3), #Image.BICUBIC\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]\n\nif opt.PCB:\n    transform_train_list = [\n        transforms.Resize((384,192), interpolation=3),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]\n    transform_val_list = [\n        transforms.Resize(size=(384,192),interpolation=3), #Image.BICUBIC\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ]\n\nif opt.erasing_p>0:\n    transform_train_list = transform_train_list +  [RandomErasing(probability = opt.erasing_p, mean=[0.0, 0.0, 0.0])]\n\nif opt.color_jitter:\n    transform_train_list = [transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0)] + transform_train_list\n\nprint(transform_train_list)\ndata_transforms = {\n    \'train\': transforms.Compose( transform_train_list ),\n    \'val\': transforms.Compose(transform_val_list),\n}\n\n\ntrain_all = \'\'\nif opt.train_all:\n     train_all = \'_all\'\n\nimage_datasets = {}\nimage_datasets[\'train\'] = datasets.ImageFolder(os.path.join(data_dir, \'train\' + train_all),\n                                          data_transforms[\'train\'])\nimage_datasets[\'val\'] = datasets.ImageFolder(os.path.join(data_dir, \'val\'),\n                                          data_transforms[\'val\'])\n\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=opt.batchsize,\n                                             shuffle=True, num_workers=8, pin_memory=True) # 8 workers may work faster\n              for x in [\'train\', \'val\']}\ndataset_sizes = {x: len(image_datasets[x]) for x in [\'train\', \'val\']}\nclass_names = image_datasets[\'train\'].classes\n\nuse_gpu = torch.cuda.is_available()\n\nsince = time.time()\ninputs, classes = next(iter(dataloaders[\'train\']))\nprint(time.time()-since)\n######################################################################\n# Training the model\n# ------------------\n#\n# Now, let\'s write a general function to train a model. Here, we will\n# illustrate:\n#\n# -  Scheduling the learning rate\n# -  Saving the best model\n#\n# In the following, parameter ``scheduler`` is an LR scheduler object from\n# ``torch.optim.lr_scheduler``.\n\ny_loss = {} # loss history\ny_loss[\'train\'] = []\ny_loss[\'val\'] = []\ny_err = {}\ny_err[\'train\'] = []\ny_err[\'val\'] = []\n\ndef train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n    since = time.time()\n\n    #best_model_wts = model.state_dict()\n    #best_acc = 0.0\n    warm_up = 0.1 # We start from the 0.1*lrRate\n    warm_iteration = round(dataset_sizes[\'train\']/opt.batchsize)*opt.warm_epoch # first 5 epoch\n\n    for epoch in range(num_epochs):\n        print(\'Epoch {}/{}\'.format(epoch, num_epochs - 1))\n        print(\'-\' * 10)\n        \n        # Each epoch has a training and validation phase\n        for phase in [\'train\', \'val\']:\n            if phase == \'train\':\n                scheduler.step()\n                model.train(True)  # Set model to training mode\n            else:\n                model.train(False)  # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0.0\n            # Iterate over data.\n            for data in dataloaders[phase]:\n                # get the inputs\n                inputs, labels = data\n                now_batch_size,c,h,w = inputs.shape\n                if now_batch_size<opt.batchsize: # skip the last batch\n                    continue\n                #print(inputs.shape)\n                # wrap them in Variable\n                if use_gpu:\n                    inputs = Variable(inputs.cuda().detach())\n                    labels = Variable(labels.cuda().detach())\n                else:\n                    inputs, labels = Variable(inputs), Variable(labels)\n                # if we use low precision, input also need to be fp16\n                #if fp16:\n                #    inputs = inputs.half()\n \n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                if phase == \'val\':\n                    with torch.no_grad():\n                        outputs = model(inputs)\n                else:\n                    outputs = model(inputs)\n\n                if not opt.PCB:\n                    _, preds = torch.max(outputs.data, 1)\n                    loss = criterion(outputs, labels)\n                else:\n                    part = {}\n                    sm = nn.Softmax(dim=1)\n                    num_part = 6\n                    for i in range(num_part):\n                        part[i] = outputs[i]\n\n                    score = sm(part[0]) + sm(part[1]) +sm(part[2]) + sm(part[3]) +sm(part[4]) +sm(part[5])\n                    _, preds = torch.max(score.data, 1)\n\n                    loss = criterion(part[0], labels)\n                    for i in range(num_part-1):\n                        loss += criterion(part[i+1], labels)\n\n                # backward + optimize only if in training phase\n                if epoch<opt.warm_epoch and phase == \'train\': \n                    warm_up = min(1.0, warm_up + 0.9 / warm_iteration)\n                    loss *= warm_up\n\n                if phase == \'train\':\n                    if fp16: # we use optimier to backward loss\n                        with amp.scale_loss(loss, optimizer) as scaled_loss:\n                            scaled_loss.backward()\n                    else:\n                        loss.backward()\n                    optimizer.step()\n\n                # statistics\n                if int(version[0])>0 or int(version[2]) > 3: # for the new version like 0.4.0, 0.5.0 and 1.0.0\n                    running_loss += loss.item() * now_batch_size\n                else :  # for the old version like 0.3.0 and 0.3.1\n                    running_loss += loss.data[0] * now_batch_size\n                running_corrects += float(torch.sum(preds == labels.data))\n\n            epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects / dataset_sizes[phase]\n            \n            print(\'{} Loss: {:.4f} Acc: {:.4f}\'.format(\n                phase, epoch_loss, epoch_acc))\n            \n            y_loss[phase].append(epoch_loss)\n            y_err[phase].append(1.0-epoch_acc)            \n            # deep copy the model\n            if phase == \'val\':\n                last_model_wts = model.state_dict()\n                if epoch%10 == 9:\n                    save_network(model, epoch)\n                draw_curve(epoch)\n\n        time_elapsed = time.time() - since\n        print(\'Training complete in {:.0f}m {:.0f}s\'.format(\n            time_elapsed // 60, time_elapsed % 60))\n        print()\n\n    time_elapsed = time.time() - since\n    print(\'Training complete in {:.0f}m {:.0f}s\'.format(\n        time_elapsed // 60, time_elapsed % 60))\n    #print(\'Best val Acc: {:4f}\'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(last_model_wts)\n    save_network(model, \'last\')\n    return model\n\n\n######################################################################\n# Draw Curve\n#---------------------------\nx_epoch = []\nfig = plt.figure()\nax0 = fig.add_subplot(121, title=""loss"")\nax1 = fig.add_subplot(122, title=""top1err"")\ndef draw_curve(current_epoch):\n    x_epoch.append(current_epoch)\n    ax0.plot(x_epoch, y_loss[\'train\'], \'bo-\', label=\'train\')\n    ax0.plot(x_epoch, y_loss[\'val\'], \'ro-\', label=\'val\')\n    ax1.plot(x_epoch, y_err[\'train\'], \'bo-\', label=\'train\')\n    ax1.plot(x_epoch, y_err[\'val\'], \'ro-\', label=\'val\')\n    if current_epoch == 0:\n        ax0.legend()\n        ax1.legend()\n    fig.savefig( os.path.join(\'./model\',name,\'train.jpg\'))\n\n######################################################################\n# Save model\n#---------------------------\ndef save_network(network, epoch_label):\n    save_filename = \'net_%s.pth\'% epoch_label\n    save_path = os.path.join(\'./model\',name,save_filename)\n    torch.save(network.cpu().state_dict(), save_path)\n    if torch.cuda.is_available():\n        network.cuda(gpu_ids[0])\n\n\n######################################################################\n# Finetuning the convnet\n# ----------------------\n#\n# Load a pretrainied model and reset final fully connected layer.\n#\n\nif opt.use_dense:\n    model = ft_net_dense(len(class_names), opt.droprate)\nelif opt.use_NAS:\n    model = ft_net_NAS(len(class_names), opt.droprate)\nelse:\n    model = ft_net(len(class_names), opt.droprate, opt.stride)\n\nif opt.PCB:\n    model = PCB(len(class_names))\n\nopt.nclasses = len(class_names)\n\nprint(model)\n\nif not opt.PCB:\n    ignored_params = list(map(id, model.classifier.parameters() ))\n    base_params = filter(lambda p: id(p) not in ignored_params, model.parameters())\n    optimizer_ft = optim.SGD([\n             {\'params\': base_params, \'lr\': 0.1*opt.lr},\n             {\'params\': model.classifier.parameters(), \'lr\': opt.lr}\n         ], weight_decay=5e-4, momentum=0.9, nesterov=True)\nelse:\n    ignored_params = list(map(id, model.model.fc.parameters() ))\n    ignored_params += (list(map(id, model.classifier0.parameters() )) \n                     +list(map(id, model.classifier1.parameters() ))\n                     +list(map(id, model.classifier2.parameters() ))\n                     +list(map(id, model.classifier3.parameters() ))\n                     +list(map(id, model.classifier4.parameters() ))\n                     +list(map(id, model.classifier5.parameters() ))\n                     #+list(map(id, model.classifier6.parameters() ))\n                     #+list(map(id, model.classifier7.parameters() ))\n                      )\n    base_params = filter(lambda p: id(p) not in ignored_params, model.parameters())\n    optimizer_ft = optim.SGD([\n             {\'params\': base_params, \'lr\': 0.1*opt.lr},\n             {\'params\': model.model.fc.parameters(), \'lr\': opt.lr},\n             {\'params\': model.classifier0.parameters(), \'lr\': opt.lr},\n             {\'params\': model.classifier1.parameters(), \'lr\': opt.lr},\n             {\'params\': model.classifier2.parameters(), \'lr\': opt.lr},\n             {\'params\': model.classifier3.parameters(), \'lr\': opt.lr},\n             {\'params\': model.classifier4.parameters(), \'lr\': opt.lr},\n             {\'params\': model.classifier5.parameters(), \'lr\': opt.lr},\n             #{\'params\': model.classifier6.parameters(), \'lr\': 0.01},\n             #{\'params\': model.classifier7.parameters(), \'lr\': 0.01}\n         ], weight_decay=5e-4, momentum=0.9, nesterov=True)\n\n# Decay LR by a factor of 0.1 every 40 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=40, gamma=0.1)\n\n######################################################################\n# Train and evaluate\n# ^^^^^^^^^^^^^^^^^^\n#\n# It should take around 1-2 hours on GPU. \n#\ndir_name = os.path.join(\'./model\',name)\nif not os.path.isdir(dir_name):\n    os.mkdir(dir_name)\n#record every run\ncopyfile(\'./train.py\', dir_name+\'/train.py\')\ncopyfile(\'./model.py\', dir_name+\'/model.py\')\n\n# save opts\nwith open(\'%s/opts.yaml\'%dir_name,\'w\') as fp:\n    yaml.dump(vars(opt), fp, default_flow_style=False)\n\n# model to gpu\nmodel = model.cuda()\nif fp16:\n    #model = network_to_half(model)\n    #optimizer_ft = FP16_Optimizer(optimizer_ft, static_loss_scale = 128.0)\n    model, optimizer_ft = amp.initialize(model, optimizer_ft, opt_level = ""O1"")\n\ncriterion = nn.CrossEntropyLoss()\n\nmodel = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,\n                       num_epochs=60)\n\n'"
