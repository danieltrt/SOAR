file_path,api_count,code
attention_augmented_conv.py,32,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(""cuda"" if use_cuda else ""cpu"")\n\n\nclass AugmentedConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, dk, dv, Nh, shape=0, relative=False, stride=1):\n        super(AugmentedConv, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.dk = dk\n        self.dv = dv\n        self.Nh = Nh\n        self.shape = shape\n        self.relative = relative\n        self.stride = stride\n        self.padding = (self.kernel_size - 1) // 2\n\n        assert self.Nh != 0, ""integer division or modulo by zero, Nh >= 1""\n        assert self.dk % self.Nh == 0, ""dk should be divided by Nh. (example: out_channels: 20, dk: 40, Nh: 4)""\n        assert self.dv % self.Nh == 0, ""dv should be divided by Nh. (example: out_channels: 20, dv: 4, Nh: 4)""\n        assert stride in [1, 2], str(stride) + "" Up to 2 strides are allowed.""\n\n        self.conv_out = nn.Conv2d(self.in_channels, self.out_channels - self.dv, self.kernel_size, stride=stride, padding=self.padding)\n\n        self.qkv_conv = nn.Conv2d(self.in_channels, 2 * self.dk + self.dv, kernel_size=self.kernel_size, stride=stride, padding=self.padding)\n\n        self.attn_out = nn.Conv2d(self.dv, self.dv, kernel_size=1, stride=1)\n\n        if self.relative:\n            self.key_rel_w = nn.Parameter(torch.randn((2 * self.shape - 1, dk // Nh), requires_grad=True))\n            self.key_rel_h = nn.Parameter(torch.randn((2 * self.shape - 1, dk // Nh), requires_grad=True))\n\n    def forward(self, x):\n        # Input x\n        # (batch_size, channels, height, width)\n        # batch, _, height, width = x.size()\n\n        # conv_out\n        # (batch_size, out_channels, height, width)\n        conv_out = self.conv_out(x)\n        batch, _, height, width = conv_out.size()\n\n        # flat_q, flat_k, flat_v\n        # (batch_size, Nh, height * width, dvh or dkh)\n        # dvh = dv / Nh, dkh = dk / Nh\n        # q, k, v\n        # (batch_size, Nh, height, width, dv or dk)\n        flat_q, flat_k, flat_v, q, k, v = self.compute_flat_qkv(x, self.dk, self.dv, self.Nh)\n        logits = torch.matmul(flat_q.transpose(2, 3), flat_k)\n        if self.relative:\n            h_rel_logits, w_rel_logits = self.relative_logits(q)\n            logits += h_rel_logits\n            logits += w_rel_logits\n        weights = F.softmax(logits, dim=-1)\n\n        # attn_out\n        # (batch, Nh, height * width, dvh)\n        attn_out = torch.matmul(weights, flat_v.transpose(2, 3))\n        attn_out = torch.reshape(attn_out, (batch, self.Nh, self.dv // self.Nh, height, width))\n        # combine_heads_2d\n        # (batch, out_channels, height, width)\n        attn_out = self.combine_heads_2d(attn_out)\n        attn_out = self.attn_out(attn_out)\n        return torch.cat((conv_out, attn_out), dim=1)\n\n    def compute_flat_qkv(self, x, dk, dv, Nh):\n        qkv = self.qkv_conv(x)\n        N, _, H, W = qkv.size()\n        q, k, v = torch.split(qkv, [dk, dk, dv], dim=1)\n        q = self.split_heads_2d(q, Nh)\n        k = self.split_heads_2d(k, Nh)\n        v = self.split_heads_2d(v, Nh)\n\n        dkh = dk // Nh\n        q *= dkh ** -0.5\n        flat_q = torch.reshape(q, (N, Nh, dk // Nh, H * W))\n        flat_k = torch.reshape(k, (N, Nh, dk // Nh, H * W))\n        flat_v = torch.reshape(v, (N, Nh, dv // Nh, H * W))\n        return flat_q, flat_k, flat_v, q, k, v\n\n    def split_heads_2d(self, x, Nh):\n        batch, channels, height, width = x.size()\n        ret_shape = (batch, Nh, channels // Nh, height, width)\n        split = torch.reshape(x, ret_shape)\n        return split\n\n    def combine_heads_2d(self, x):\n        batch, Nh, dv, H, W = x.size()\n        ret_shape = (batch, Nh * dv, H, W)\n        return torch.reshape(x, ret_shape)\n\n    def relative_logits(self, q):\n        B, Nh, dk, H, W = q.size()\n        q = torch.transpose(q, 2, 4).transpose(2, 3)\n\n        rel_logits_w = self.relative_logits_1d(q, self.key_rel_w, H, W, Nh, ""w"")\n        rel_logits_h = self.relative_logits_1d(torch.transpose(q, 2, 3), self.key_rel_h, W, H, Nh, ""h"")\n\n        return rel_logits_h, rel_logits_w\n\n    def relative_logits_1d(self, q, rel_k, H, W, Nh, case):\n        rel_logits = torch.einsum(\'bhxyd,md->bhxym\', q, rel_k)\n        rel_logits = torch.reshape(rel_logits, (-1, Nh * H, W, 2 * W - 1))\n        rel_logits = self.rel_to_abs(rel_logits)\n\n        rel_logits = torch.reshape(rel_logits, (-1, Nh, H, W, W))\n        rel_logits = torch.unsqueeze(rel_logits, dim=3)\n        rel_logits = rel_logits.repeat((1, 1, 1, H, 1, 1))\n\n        if case == ""w"":\n            rel_logits = torch.transpose(rel_logits, 3, 4)\n        elif case == ""h"":\n            rel_logits = torch.transpose(rel_logits, 2, 4).transpose(4, 5).transpose(3, 5)\n        rel_logits = torch.reshape(rel_logits, (-1, Nh, H * W, H * W))\n        return rel_logits\n\n    def rel_to_abs(self, x):\n        B, Nh, L, _ = x.size()\n\n        col_pad = torch.zeros((B, Nh, L, 1)).to(x)\n        x = torch.cat((x, col_pad), dim=3)\n\n        flat_x = torch.reshape(x, (B, Nh, L * 2 * L))\n        flat_pad = torch.zeros((B, Nh, L - 1)).to(x)\n        flat_x_padded = torch.cat((flat_x, flat_pad), dim=2)\n\n        final_x = torch.reshape(flat_x_padded, (B, Nh, L + 1, 2 * L - 1))\n        final_x = final_x[:, :, :L, L - 1:]\n        return final_x\n\n\n# Example Code\n# tmp = torch.randn((16, 3, 32, 32)).to(device)\n# augmented_conv1 = AugmentedConv(in_channels=3, out_channels=20, kernel_size=3, dk=40, dv=4, Nh=4, relative=True, padding=1, stride=2, shape=16).to(device)\n# conv_out1 = augmented_conv1(tmp)\n# print(conv_out1.shape)\n#\n# for name, param in augmented_conv1.named_parameters():\n#     print(\'parameter name: \', name)\n#\n# augmented_conv2 = AugmentedConv(in_channels=3, out_channels=20, kernel_size=3, dk=40, dv=4, Nh=4, relative=True, padding=1, stride=1, shape=32).to(device)\n# conv_out2 = augmented_conv2(tmp)\n# print(conv_out2.shape)\n'"
AA-Wide-ResNet/attention_augmented_conv.py,32,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(""cuda"" if use_cuda else ""cpu"")\n\n\nclass AugmentedConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, dk, dv, Nh, shape=0, relative=False, stride=1):\n        super(AugmentedConv, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.dk = dk\n        self.dv = dv\n        self.Nh = Nh\n        self.shape = shape\n        self.relative = relative\n        self.stride = stride\n        self.padding = (self.kernel_size - 1) // 2\n\n        assert self.Nh != 0, ""integer division or modulo by zero, Nh >= 1""\n        assert self.dk % self.Nh == 0, ""dk should be divided by Nh. (example: out_channels: 20, dk: 40, Nh: 4)""\n        assert self.dv % self.Nh == 0, ""dv should be divided by Nh. (example: out_channels: 20, dv: 4, Nh: 4)""\n        assert stride in [1, 2], str(stride) + "" Up to 2 strides are allowed.""\n\n        self.conv_out = nn.Conv2d(self.in_channels, self.out_channels - self.dv, self.kernel_size, stride=stride, padding=self.padding)\n\n        self.qkv_conv = nn.Conv2d(self.in_channels, 2 * self.dk + self.dv, kernel_size=self.kernel_size, stride=stride, padding=self.padding)\n\n        self.attn_out = nn.Conv2d(self.dv, self.dv, kernel_size=1, stride=1)\n\n        if self.relative:\n            self.key_rel_w = nn.Parameter(torch.randn((2 * self.shape - 1, dk // Nh), requires_grad=True))\n            self.key_rel_h = nn.Parameter(torch.randn((2 * self.shape - 1, dk // Nh), requires_grad=True))\n\n    def forward(self, x):\n        # Input x\n        # (batch_size, channels, height, width)\n        # batch, _, height, width = x.size()\n\n        # conv_out\n        # (batch_size, out_channels, height, width)\n        conv_out = self.conv_out(x)\n        batch, _, height, width = conv_out.size()\n\n        # flat_q, flat_k, flat_v\n        # (batch_size, Nh, height * width, dvh or dkh)\n        # dvh = dv / Nh, dkh = dk / Nh\n        # q, k, v\n        # (batch_size, Nh, height, width, dv or dk)\n        flat_q, flat_k, flat_v, q, k, v = self.compute_flat_qkv(x, self.dk, self.dv, self.Nh)\n        logits = torch.matmul(flat_q.transpose(2, 3), flat_k)\n        if self.relative:\n            h_rel_logits, w_rel_logits = self.relative_logits(q)\n            logits += h_rel_logits\n            logits += w_rel_logits\n        weights = F.softmax(logits, dim=-1)\n\n        # attn_out\n        # (batch, Nh, height * width, dvh)\n        attn_out = torch.matmul(weights, flat_v.transpose(2, 3))\n        attn_out = torch.reshape(attn_out, (batch, self.Nh, self.dv // self.Nh, height, width))\n        # combine_heads_2d\n        # (batch, out_channels, height, width)\n        attn_out = self.combine_heads_2d(attn_out)\n        attn_out = self.attn_out(attn_out)\n        return torch.cat((conv_out, attn_out), dim=1)\n\n    def compute_flat_qkv(self, x, dk, dv, Nh):\n        qkv = self.qkv_conv(x)\n        N, _, H, W = qkv.size()\n        q, k, v = torch.split(qkv, [dk, dk, dv], dim=1)\n        q = self.split_heads_2d(q, Nh)\n        k = self.split_heads_2d(k, Nh)\n        v = self.split_heads_2d(v, Nh)\n\n        dkh = dk // Nh\n        q *= dkh ** -0.5\n        flat_q = torch.reshape(q, (N, Nh, dk // Nh, H * W))\n        flat_k = torch.reshape(k, (N, Nh, dk // Nh, H * W))\n        flat_v = torch.reshape(v, (N, Nh, dv // Nh, H * W))\n        return flat_q, flat_k, flat_v, q, k, v\n\n    def split_heads_2d(self, x, Nh):\n        batch, channels, height, width = x.size()\n        ret_shape = (batch, Nh, channels // Nh, height, width)\n        split = torch.reshape(x, ret_shape)\n        return split\n\n    def combine_heads_2d(self, x):\n        batch, Nh, dv, H, W = x.size()\n        ret_shape = (batch, Nh * dv, H, W)\n        return torch.reshape(x, ret_shape)\n\n    def relative_logits(self, q):\n        B, Nh, dk, H, W = q.size()\n        q = torch.transpose(q, 2, 4).transpose(2, 3)\n\n        rel_logits_w = self.relative_logits_1d(q, self.key_rel_w, H, W, Nh, ""w"")\n        rel_logits_h = self.relative_logits_1d(torch.transpose(q, 2, 3), self.key_rel_h, W, H, Nh, ""h"")\n\n        return rel_logits_h, rel_logits_w\n\n    def relative_logits_1d(self, q, rel_k, H, W, Nh, case):\n        rel_logits = torch.einsum(\'bhxyd,md->bhxym\', q, rel_k)\n        rel_logits = torch.reshape(rel_logits, (-1, Nh * H, W, 2 * W - 1))\n        rel_logits = self.rel_to_abs(rel_logits)\n\n        rel_logits = torch.reshape(rel_logits, (-1, Nh, H, W, W))\n        rel_logits = torch.unsqueeze(rel_logits, dim=3)\n        rel_logits = rel_logits.repeat((1, 1, 1, H, 1, 1))\n\n        if case == ""w"":\n            rel_logits = torch.transpose(rel_logits, 3, 4)\n        elif case == ""h"":\n            rel_logits = torch.transpose(rel_logits, 2, 4).transpose(4, 5).transpose(3, 5)\n        rel_logits = torch.reshape(rel_logits, (-1, Nh, H * W, H * W))\n        return rel_logits\n\n    def rel_to_abs(self, x):\n        B, Nh, L, _ = x.size()\n\n        col_pad = torch.zeros((B, Nh, L, 1)).to(x)\n        x = torch.cat((x, col_pad), dim=3)\n\n        flat_x = torch.reshape(x, (B, Nh, L * 2 * L))\n        flat_pad = torch.zeros((B, Nh, L - 1)).to(x)\n        flat_x_padded = torch.cat((flat_x, flat_pad), dim=2)\n\n        final_x = torch.reshape(flat_x_padded, (B, Nh, L + 1, 2 * L - 1))\n        final_x = final_x[:, :, :L, L - 1:]\n        return final_x\n\n\n# Example Code\n# tmp = torch.randn((16, 3, 32, 32)).to(device)\n# augmented_conv1 = AugmentedConv(in_channels=3, out_channels=20, kernel_size=3, dk=40, dv=4, Nh=4, relative=True, padding=1, stride=2, shape=16).to(device)\n# conv_out1 = augmented_conv1(tmp)\n# print(conv_out1.shape)\n#\n# for name, param in augmented_conv1.named_parameters():\n#     print(\'parameter name: \', name)\n#\n# augmented_conv2 = AugmentedConv(in_channels=3, out_channels=20, kernel_size=3, dk=40, dv=4, Nh=4, relative=True, padding=1, stride=1, shape=32).to(device)\n# conv_out2 = augmented_conv2(tmp)\n# print(conv_out2.shape)\n'"
AA-Wide-ResNet/attention_augmented_wide_resnet.py,6,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom attention_augmented_conv import AugmentedConv\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\ndef _weights_init(m):\n    if isinstance(m, nn.Conv2d):\n        torch.nn.init.xavier_uniform_(m.weight)\n        if m.bias is not None:\n            torch.nn.init.zeros_(m.bias)\n    elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1)\n        m.bias.data.zero_()\n    elif isinstance(m, nn.Linear):\n        n = m.weight.size(1)\n        m.weight.data.normal_(0, 0.01)\n        m.bias.data.zero_()\n\n\nclass wide_basic(nn.Module):\n    def __init__(self, in_planes, planes, dropout_rate, shape, stride=1, v=0.2, k=2, Nh=4):\n        super(wide_basic, self).__init__()\n        if stride == 2:\n            original_shape = shape * 2\n        else:\n            original_shape = shape\n\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = AugmentedConv(in_planes, planes, kernel_size=3, dk=k * planes, dv=int(v * planes), Nh=Nh, relative=True, shape=original_shape)\n        self.dropout = nn.Dropout(p=dropout_rate)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = AugmentedConv(planes, planes, kernel_size=3, dk=k * planes, dv=int(v * planes), Nh=Nh, stride=stride, relative=True, shape=shape)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != planes:\n            self.shortcut = nn.Sequential(\n                AugmentedConv(in_planes, planes, kernel_size=3, dk=k * planes, dv=int(v * planes), Nh=Nh, relative=True, stride=stride, shape=shape),\n            )\n\n    def forward(self, x):\n        out = self.dropout(self.conv1(F.relu(self.bn1(x))))\n        out = self.conv2(F.relu(self.bn2(out)))\n        short = self.shortcut(x)\n        out += short\n\n        return out\n\n\nclass Wide_ResNet(nn.Module):\n    def __init__(self, depth, widen_factor, dropout_rate, num_classes, shape):\n        super(Wide_ResNet, self).__init__()\n        self.in_planes = 20\n        self.shape = shape\n\n        assert ((depth-4) % 6 == 0), 'Wide-resnet depth should be 6n+4'\n        n = int((depth - 4) / 6)\n        k = widen_factor\n\n        dv_v = 0.2\n        dk_k = 2\n        Nh = 4\n\n        print('| Wide-Resnet %dx%d' % (depth, k))\n        n_Stages = [20, 20 * k, 40 * k, 60 * k]\n\n        self.conv1 = AugmentedConv(in_channels=3, out_channels=n_Stages[0], kernel_size=3, dk=dk_k * n_Stages[0], dv=int(dv_v * n_Stages[0]), shape=shape, Nh=Nh, relative=True)\n        self.layer1 = nn.Sequential(\n            self._wide_layer(wide_basic, n_Stages[1], n, dropout_rate, stride=1, shape=shape),\n        )\n        self.layer2 = nn.Sequential(\n            self._wide_layer(wide_basic, n_Stages[2], n, dropout_rate, stride=2, shape=shape // 2),\n        )\n        self.layer3 = nn.Sequential(\n            self._wide_layer(wide_basic, n_Stages[3], n, dropout_rate, stride=2, shape=shape // 4),\n        )\n        self.bn1 = nn.BatchNorm2d(n_Stages[3], momentum=0.9)\n        self.linear = nn.Linear(n_Stages[3], num_classes)\n\n        self.apply(_weights_init)\n\n    def _wide_layer(self, block, planes, num_blocks, dropout_rate, stride, shape):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, dropout_rate=dropout_rate, stride=stride, shape=shape))\n            self.in_planes = planes\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = F.relu(self.bn1(out))\n        out = F.avg_pool2d(out, 8)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n\n        return out\n\n# tmp = torch.randn((4, 3, 32, 32))\n# model = Wide_ResNet(28, 10, 0.3, num_classes=100, shape=32)\n# print(model(tmp).shape)\n#\n# for name, param in model.named_parameters():\n#     print('parameter name: ', name)\n"""
AA-Wide-ResNet/main.py,7,"b'import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom preprocess import load_data\nfrom attention_augmented_wide_resnet import Wide_ResNet\n\nimport argparse\nfrom tqdm import tqdm\nimport time\nimport os\n\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(""cuda"" if use_cuda else ""cpu"")\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(""parameters"")\n\n    parser.add_argument(""--dataset-mode"", type=str, default=""CIFAR100"", help=""(example: CIFAR10, CIFAR100, MNIST), (default: CIFAR100)"")\n    parser.add_argument(""--epochs"", type=int, default=100, help=""number of epochs, (default: 100)"")\n    parser.add_argument(""--batch-size"", type=int, default=10, help=""number of batch size, (default, 8)"")\n    parser.add_argument(""--learning-rate"", type=float, default=1e-3, help=""learning_rate, (default: 1e-1)"")\n    parser.add_argument(""--depth"", type=int, default=28, help=""wide-ResNet depth, (default: 28)"")\n    parser.add_argument(""--widen_factor"", type=int, default=10, help=""wide_ResNet widen factor, (default: 10)"")\n    parser.add_argument(""--dropout"", type=float, default=0.3, help=""dropout rate, (default: 0.3)"")\n    parser.add_argument(""--load-pretrained"", type=bool, default=False)\n\n    args = parser.parse_args()\n\n    return args\n\n\ndef adjust_learning_rate(optimizer, epoch, args):\n    """"""Sets the learning rate to the initial LR decayed by 10 every 30 epochs""""""\n    lr = args.learning_rate * (0.1 ** (epoch // 30))\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\n\ndef train(model, train_loader, optimizer, criterion, epoch, args):\n    model.train()\n    step = 0\n    train_loss = 0\n    train_acc = 0\n    for data, target in tqdm(train_loader, desc=""epoch "" + str(epoch), mininterval=1):\n        adjust_learning_rate(optimizer, epoch, args)\n        data, target = data.to(device), target.to(device)\n\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.data\n        y_pred = output.data.max(1)[1]\n\n        acc = float(y_pred.eq(target.data).sum()) / len(data) * 100.\n        train_acc += acc\n        step += 1\n        if step % 500 == 0:\n            print(""[Epoch {0:4d}] Loss: {1:2.3f} Acc: {2:.3f}%"".format(epoch, loss.data, acc), end=\'\')\n            for param_group in optimizer.param_groups:\n                print("",  Current learning rate is: {}"".format(param_group[\'lr\']))\n\n    length = len(train_loader.dataset) // args.batch_size\n    return train_loss / length, train_acc / length\n\n\ndef get_test(model, test_loader):\n    model.eval()\n    correct = 0\n    with torch.no_grad():\n        for data, target in tqdm(test_loader, desc=""evaluation"", mininterval=1):\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            prediction = output.data.max(1)[1]\n            correct += prediction.eq(target.data).sum()\n\n    acc = 100. * float(correct) / len(test_loader.dataset)\n    return acc\n\n\ndef main():\n    args = get_args()\n    train_loader, test_loader = load_data(args)\n\n    if args.dataset_mode is ""CIFAR10"":\n        num_classes = 10\n    elif args.dataset_mode is ""CIFAR100"":\n        num_classes = 100\n    elif args.dataset_mode is ""MNIST"":\n        num_classes = 10\n\n    if args.load_pretrained:\n        model = Wide_ResNet(args.depth, args.widen_factor, args.dropout, num_classes=num_classes, shape=32).to(device)\n        filename = ""best_model_""\n        checkpoint = torch.load(\'./checkpoint/\' + filename + \'ckpt.t7\')\n        model.load_state_dict(checkpoint[\'model\'])\n        epoch = checkpoint[\'epoch\']\n        acc = checkpoint[\'acc\']\n        max_test_acc = acc\n        print(""Load Model Accuracy: "", acc, ""Load Model end epoch: "", epoch)\n    else:\n        model = Wide_ResNet(args.depth, args.widen_factor, args.dropout, num_classes=num_classes, shape=32).to(device)\n        epoch = 1\n        max_test_acc = 0\n    # if device is ""cuda"":\n    #     model = nn.DataParallel(model)\n    optimizer = optim.SGD(model.parameters(), lr=args.learning_rate, weight_decay=5e-4, momentum=0.9)\n    criterion = nn.CrossEntropyLoss().to(device)\n\n    if not os.path.isdir(""reporting""):\n        os.mkdir(""reporting"")\n\n    start_time = time.time()\n    for epoch in range(epoch, args.epochs):\n        train(model, train_loader, optimizer, criterion, epoch, args)\n        test_acc = get_test(model, test_loader)\n        if max_test_acc < test_acc:\n            print(\'Saving..\')\n            state = {\n                \'model\': model.state_dict(),\n                \'acc\': test_acc,\n                \'epoch\': epoch,\n            }\n            if not os.path.isdir(\'checkpoint\'):\n                os.mkdir(\'checkpoint\')\n            filename = ""best_model_""\n            torch.save(state, \'./checkpoint/\' + filename + \'ckpt.t7\')\n            max_test_acc = test_acc\n\n        time_interval = time.time() - start_time\n        time_split = time.gmtime(time_interval)\n        print(""Training time: "", time_interval, ""Hour: "", time_split.tm_hour, ""Minute: "", time_split.tm_min, ""Second: "", time_split.tm_sec)\n        print(""Test acc:"", max_test_acc, ""time: "", time.time() - start_time)\n        with open(""./reporting/"" + ""best_model.txt"", ""w"") as f:\n            f.write(""Epoch: "" + str(epoch) + "" "" + ""Best acc: "" + str(max_test_acc) + ""\\n"")\n            f.write(""Training time: "" + str(time_interval) + ""Hour: "" + str(time_split.tm_hour) + ""Minute: "" + str(\n                time_split.tm_min) + ""Second: "" + str(time_split.tm_sec))\n            f.write(""\\n"")\n\n\nif __name__ == ""__main__"":\n    main()\n'"
AA-Wide-ResNet/preprocess.py,6,"b'import torch\n\nfrom torchvision import datasets, transforms\n\n\n# CIFAR-10,\n# mean, [0.4914, 0.4822, 0.4465]\n# std, [0.2470, 0.2435, 0.2616]\n# CIFAR-100,\n# mean, [0.5071, 0.4865, 0.4409]\n# std, [0.2673, 0.2564, 0.2762]\ndef load_data(args):\n    if args.dataset_mode is ""CIFAR100"":\n        transform_train = transforms.Compose([\n            transforms.RandomCrop(32, padding=4),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5071, 0.4865, 0.4409), (0.2673, 0.2564, 0.2762)),\n        ])\n\n        transform_test = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.5071, 0.4865, 0.4409), (0.2673, 0.2564, 0.2762)),\n        ])\n        train_loader = torch.utils.data.DataLoader(\n            datasets.CIFAR100(\'data\', train=True, download=True, transform=transform_train),\n            batch_size=args.batch_size,\n            shuffle=True,\n            num_workers=2\n        )\n\n        test_loader = torch.utils.data.DataLoader(\n            datasets.CIFAR100(\'data\', train=False, transform=transform_test),\n            batch_size=args.batch_size,\n            shuffle=False,\n            num_workers=2\n        )\n    elif args.dataset_mode is ""CIFAR10"":\n        transform_train = transforms.Compose([\n            transforms.RandomCrop(32, padding=4),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n        ])\n\n        transform_test = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n        ])\n        train_loader = torch.utils.data.DataLoader(\n            datasets.CIFAR100(\'data\', train=True, download=True, transform=transform_train),\n            batch_size=args.batch_size,\n            shuffle=True,\n            num_workers=2\n        )\n\n        test_loader = torch.utils.data.DataLoader(\n            datasets.CIFAR100(\'data\', train=False, transform=transform_test),\n            batch_size=args.batch_size,\n            shuffle=False,\n            num_workers=2\n        )\n    elif args.dataset_mode is ""MNIST"":\n        transform_train = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,)),\n        ])\n\n        transform_test = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,)),\n        ])\n        train_loader = torch.utils.data.DataLoader(\n            datasets.CIFAR100(\'data\', train=True, download=True, transform=transform_train),\n            batch_size=args.batch_size,\n            shuffle=True,\n            num_workers=2\n        )\n\n        test_loader = torch.utils.data.DataLoader(\n            datasets.CIFAR100(\'data\', train=False, transform=transform_test),\n            batch_size=args.batch_size,\n            shuffle=True,\n            num_workers=2\n        )\n\n    return train_loader, test_loader\n'"
in_paper_attention_augmented_conv/attention_augmented_conv.py,31,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(""cuda"" if use_cuda else ""cpu"")\n\n\nclass AugmentedConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, dk, dv, Nh, relative):\n        super(AugmentedConv, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.dk = dk\n        self.dv = dv\n        self.Nh = Nh\n        self.relative = relative\n\n        self.conv_out = nn.Conv2d(self.in_channels, self.out_channels - self.dv, self.kernel_size, padding=1)\n\n        self.qkv_conv = nn.Conv2d(self.in_channels, 2 * self.dk + self.dv, kernel_size=1)\n\n        self.attn_out = nn.Conv2d(self.dv, self.dv, 1)\n\n    def forward(self, x):\n        # Input x\n        # (batch_size, channels, height, width)\n        batch, _, height, width = x.size()\n\n        # conv_out\n        # (batch_size, out_channels, height, width)\n        conv_out = self.conv_out(x)\n\n        # flat_q, flat_k, flat_v\n        # (batch_size, Nh, height * width, dvh or dkh)\n        # dvh = dv / Nh, dkh = dk / Nh\n        # q, k, v\n        # (batch_size, Nh, height, width, dv or dk)\n        flat_q, flat_k, flat_v, q, k, v = self.compute_flat_qkv(x, self.dk, self.dv, self.Nh)\n        logits = torch.matmul(flat_q.transpose(2, 3), flat_k)\n\n        if self.relative:\n            h_rel_logits, w_rel_logits = self.relative_logits(q)\n            logits += h_rel_logits\n            logits += w_rel_logits\n        weights = F.softmax(logits, dim=-1)\n\n        # attn_out\n        # (batch, Nh, height * width, dvh)\n        attn_out = torch.matmul(weights, flat_v.transpose(2, 3))\n        attn_out = torch.reshape(attn_out, (batch, self.Nh, self.dv // self.Nh, height, width))\n        # combine_heads_2d\n        # (batch, out_channels, height, width)\n        attn_out = self.combine_heads_2d(attn_out)\n        attn_out = self.attn_out(attn_out)\n        return torch.cat((conv_out, attn_out), dim=1)\n\n    def compute_flat_qkv(self, x, dk, dv, Nh):\n        N, _, H, W = x.size()\n        qkv = self.qkv_conv(x)\n        q, k, v = torch.split(qkv, [dk, dk, dv], dim=1)\n        q = self.split_heads_2d(q, Nh)\n        k = self.split_heads_2d(k, Nh)\n        v = self.split_heads_2d(v, Nh)\n\n        dkh = dk // Nh\n        q *= dkh ** -0.5\n        flat_q = torch.reshape(q, (N, Nh, dk // Nh, H * W))\n        flat_k = torch.reshape(k, (N, Nh, dk // Nh, H * W))\n        flat_v = torch.reshape(v, (N, Nh, dv // Nh, H * W))\n        return flat_q, flat_k, flat_v, q, k, v\n\n    def split_heads_2d(self, x, Nh):\n        batch, channels, height, width = x.size()\n        ret_shape = (batch, Nh, channels // Nh, height, width)\n        split = torch.reshape(x, ret_shape)\n        return split\n\n    def combine_heads_2d(self, x):\n        batch, Nh, dv, H, W = x.size()\n        ret_shape = (batch, Nh * dv, H, W)\n        return torch.reshape(x, ret_shape)\n\n    def relative_logits(self, q):\n        B, Nh, dk, H, W = q.size()\n        q = torch.transpose(q, 2, 4).transpose(2, 3)\n\n        key_rel_w = nn.Parameter(torch.randn((2 * W - 1, dk), requires_grad=True)).to(device)\n        rel_logits_w = self.relative_logits_1d(q, key_rel_w, H, W, Nh, ""w"")\n\n        key_rel_h = nn.Parameter(torch.randn((2 * H - 1, dk), requires_grad=True)).to(device)\n        rel_logits_h = self.relative_logits_1d(torch.transpose(q, 2, 3), key_rel_h, W, H, Nh, ""h"")\n\n        return rel_logits_h, rel_logits_w\n\n    def relative_logits_1d(self, q, rel_k, H, W, Nh, case):\n        rel_logits = torch.einsum(\'bhxyd,md->bhxym\', q, rel_k)\n        rel_logits = torch.reshape(rel_logits, (-1, Nh * H, W, 2 * W - 1))\n        rel_logits = self.rel_to_abs(rel_logits)\n\n        rel_logits = torch.reshape(rel_logits, (-1, Nh, H, W, W))\n        rel_logits = torch.unsqueeze(rel_logits, dim=3)\n        rel_logits = rel_logits.repeat((1, 1, 1, H, 1, 1))\n\n        if case == ""w"":\n            rel_logits = torch.transpose(rel_logits, 3, 4)\n        elif case == ""h"":\n            rel_logits = torch.transpose(rel_logits, 2, 4).transpose(4, 5).transpose(3, 5)\n        rel_logits = torch.reshape(rel_logits, (-1, Nh, H * W, H * W))\n        return rel_logits\n\n    def rel_to_abs(self, x):\n        B, Nh, L, _ = x.size()\n\n        col_pad = torch.zeros((B, Nh, L, 1)).to(device)\n        x = torch.cat((x, col_pad), dim=3)\n\n        flat_x = torch.reshape(x, (B, Nh, L * 2 * L))\n        flat_pad = torch.zeros((B, Nh, L - 1)).to(device)\n        flat_x_padded = torch.cat((flat_x, flat_pad), dim=2)\n\n        final_x = torch.reshape(flat_x_padded, (B, Nh, L + 1, 2 * L - 1))\n        final_x = final_x[:, :, :L, L - 1:]\n        return final_x\n'"
