file_path,api_count,code
attention.py,16,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.init as init\n\nimport math\n\n\nclass AttentionConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, groups=1, bias=False):\n        super(AttentionConv, self).__init__()\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.groups = groups\n\n        assert self.out_channels % self.groups == 0, ""out_channels should be divided by groups. (example: out_channels: 40, groups: 4)""\n\n        self.rel_h = nn.Parameter(torch.randn(out_channels // 2, 1, 1, kernel_size, 1), requires_grad=True)\n        self.rel_w = nn.Parameter(torch.randn(out_channels // 2, 1, 1, 1, kernel_size), requires_grad=True)\n\n        self.key_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n        self.query_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n        self.value_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n\n        self.reset_parameters()\n\n    def forward(self, x):\n        batch, channels, height, width = x.size()\n\n        padded_x = F.pad(x, [self.padding, self.padding, self.padding, self.padding])\n        q_out = self.query_conv(x)\n        k_out = self.key_conv(padded_x)\n        v_out = self.value_conv(padded_x)\n\n        k_out = k_out.unfold(2, self.kernel_size, self.stride).unfold(3, self.kernel_size, self.stride)\n        v_out = v_out.unfold(2, self.kernel_size, self.stride).unfold(3, self.kernel_size, self.stride)\n\n        k_out_h, k_out_w = k_out.split(self.out_channels // 2, dim=1)\n        k_out = torch.cat((k_out_h + self.rel_h, k_out_w + self.rel_w), dim=1)\n\n        k_out = k_out.contiguous().view(batch, self.groups, self.out_channels // self.groups, height, width, -1)\n        v_out = v_out.contiguous().view(batch, self.groups, self.out_channels // self.groups, height, width, -1)\n\n        q_out = q_out.view(batch, self.groups, self.out_channels // self.groups, height, width, 1)\n\n        out = q_out * k_out\n        out = F.softmax(out, dim=-1)\n        out = torch.einsum(\'bnchwk,bnchwk -> bnchw\', out, v_out).view(batch, -1, height, width)\n\n        return out\n\n    def reset_parameters(self):\n        init.kaiming_normal_(self.key_conv.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n        init.kaiming_normal_(self.value_conv.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n        init.kaiming_normal_(self.query_conv.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n\n        init.normal_(self.rel_h, 0, 1)\n        init.normal_(self.rel_w, 0, 1)\n\n\nclass AttentionStem(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, groups=1, m=4, bias=False):\n        super(AttentionStem, self).__init__()\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.groups = groups\n        self.m = m\n\n        assert self.out_channels % self.groups == 0, ""out_channels should be divided by groups. (example: out_channels: 40, groups: 4)""\n\n        self.emb_a = nn.Parameter(torch.randn(out_channels // groups, kernel_size), requires_grad=True)\n        self.emb_b = nn.Parameter(torch.randn(out_channels // groups, kernel_size), requires_grad=True)\n        self.emb_mix = nn.Parameter(torch.randn(m, out_channels // groups), requires_grad=True)\n\n        self.key_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n        self.query_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n        self.value_conv = nn.ModuleList([nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias) for _ in range(m)])\n\n        self.reset_parameters()\n\n    def forward(self, x):\n        batch, channels, height, width = x.size()\n\n        padded_x = F.pad(x, [self.padding, self.padding, self.padding, self.padding])\n\n        q_out = self.query_conv(x)\n        k_out = self.key_conv(padded_x)\n        v_out = torch.stack([self.value_conv[_](padded_x) for _ in range(self.m)], dim=0)\n\n        k_out = k_out.unfold(2, self.kernel_size, self.stride).unfold(3, self.kernel_size, self.stride)\n        v_out = v_out.unfold(3, self.kernel_size, self.stride).unfold(4, self.kernel_size, self.stride)\n\n        k_out = k_out[:, :, :height, :width, :, :]\n        v_out = v_out[:, :, :, :height, :width, :, :]\n\n        emb_logit_a = torch.einsum(\'mc,ca->ma\', self.emb_mix, self.emb_a)\n        emb_logit_b = torch.einsum(\'mc,cb->mb\', self.emb_mix, self.emb_b)\n        emb = emb_logit_a.unsqueeze(2) + emb_logit_b.unsqueeze(1)\n        emb = F.softmax(emb.view(self.m, -1), dim=0).view(self.m, 1, 1, 1, 1, self.kernel_size, self.kernel_size)\n\n        v_out = emb * v_out\n\n        k_out = k_out.contiguous().view(batch, self.groups, self.out_channels // self.groups, height, width, -1)\n        v_out = v_out.contiguous().view(self.m, batch, self.groups, self.out_channels // self.groups, height, width, -1)\n        v_out = torch.sum(v_out, dim=0).view(batch, self.groups, self.out_channels // self.groups, height, width, -1)\n\n        q_out = q_out.view(batch, self.groups, self.out_channels // self.groups, height, width, 1)\n\n        out = q_out * k_out\n        out = F.softmax(out, dim=-1)\n        out = torch.einsum(\'bnchwk,bnchwk->bnchw\', out, v_out).view(batch, -1, height, width)\n\n        return out\n\n    def reset_parameters(self):\n        init.kaiming_normal_(self.key_conv.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n        init.kaiming_normal_(self.query_conv.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n        for _ in self.value_conv:\n            init.kaiming_normal_(_.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n\n        init.normal_(self.emb_a, 0, 1)\n        init.normal_(self.emb_b, 0, 1)\n        init.normal_(self.emb_mix, 0, 1)\n\n\n# temp = torch.randn((2, 3, 32, 32))\n# conv = AttentionConv(3, 16, kernel_size=3, padding=1)\n# print(conv(temp).size())\n'"
config.py,0,"b""import argparse\nimport os\nimport logging\nimport logging.handlers\n\n\n# DEBUG < INFO < WARNING < ERROR < CRITICAL\ndef get_logger(filename):\n    logger = logging.getLogger('logger')\n    logger.setLevel(logging.DEBUG)\n\n    formatter = logging.Formatter('[%(levelname)s | %(filename)s:%(lineno)s] %(asctime)s: %(message)s')\n\n    if not os.path.isdir('log'):\n        os.mkdir('log')\n\n    file_handler = logging.FileHandler('./log/' + filename + '.log')\n    stream_handler = logging.StreamHandler()\n\n    file_handler.setFormatter(formatter)\n    stream_handler.setFormatter(formatter)\n\n    logger.addHandler(file_handler)\n    logger.addHandler(stream_handler)\n\n    return logger\n\n\ndef get_args():\n    parser = argparse.ArgumentParser('parameters')\n\n    parser.add_argument('--dataset', type=str, default='CIFAR10', help='CIFAR10, CIFAR100, MNIST')\n    parser.add_argument('--model-name', type=str, default='ResNet26', help='ResNet26, ResNet38, ResNet50')\n    parser.add_argument('--img-size', type=int, default=32)\n    parser.add_argument('--batch-size', type=int, default=25)\n    parser.add_argument('--num-workers', type=int, default=0)\n    parser.add_argument('--epochs', type=int, default=100)\n    parser.add_argument('--lr', type=float, default=1e-1)\n    parser.add_argument('--momentum', type=float, default=0.9)\n    parser.add_argument('--weight-decay', type=float, default=1e-4)\n    parser.add_argument('--print-interval', type=int, default=100)\n    parser.add_argument('--cuda', type=bool, default=True)\n    parser.add_argument('--pretrained-model', type=bool, default=False)\n    parser.add_argument('--stem', type=bool, default=False, help='attention stem: True, conv: False')\n\n    parser.add_argument('--distributed', type=bool, default=False)\n    parser.add_argument('--gpu-devices', type=int, nargs='+', default=None)\n    parser.add_argument('--gpu', type=int, default=None)\n    parser.add_argument('--rank', type=int, default=0, help='current process number')\n    parser.add_argument('--world-size', type=int, default=1, help='Total number of processes to be used (number of gpus)')\n    parser.add_argument('--dist-backend', type=str, default='nccl')\n    parser.add_argument('--dist-url', default='tcp://127.0.0.1:3456', type=str)\n\n    args = parser.parse_args()\n\n    logger = get_logger('train')\n    logger.info(vars(args))\n\n    return args, logger\n"""
main.py,8,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport os\nfrom tqdm import tqdm\nimport shutil\n\nfrom config import get_args, get_logger\nfrom model import ResNet50, ResNet38, ResNet26\nfrom preprocess import load_data\n\n\ndef adjust_learning_rate(optimizer, epoch, args):\n    """"""Sets the learning rate to the initial LR decayed by 10 every 30 epochs""""""\n    lr = args.lr * (0.1 ** (epoch // 30))\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\n\ndef train(model, train_loader, optimizer, criterion, epoch, args, logger):\n    model.train()\n\n    train_acc = 0.0\n    step = 0\n    for data, target in train_loader:\n        adjust_learning_rate(optimizer, epoch, args)\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n        y_pred = output.data.max(1)[1]\n\n        acc = float(y_pred.eq(target.data).sum()) / len(data) * 100.\n        train_acc += acc\n        step += 1\n        if step % args.print_interval == 0:\n            # print(""[Epoch {0:4d}] Loss: {1:2.3f} Acc: {2:.3f}%"".format(epoch, loss.data, acc), end=\'\')\n            logger.info(""[Epoch {0:4d}] Loss: {1:2.3f} Acc: {2:.3f}%"".format(epoch, loss.data, acc))\n            for param_group in optimizer.param_groups:\n                # print("",  Current learning rate is: {}"".format(param_group[\'lr\']))\n                logger.info(""Current learning rate is: {}"".format(param_group[\'lr\']))\n\n\ndef eval(model, test_loader, args):\n    print(\'evaluation ...\')\n    model.eval()\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            if args.cuda:\n                data, target = data.cuda(), target.cuda()\n            output = model(data)\n            prediction = output.data.max(1)[1]\n            correct += prediction.eq(target.data).sum()\n\n    acc = 100. * float(correct) / len(test_loader.dataset)\n    print(\'Test acc: {0:.2f}\'.format(acc))\n    return acc\n\n\ndef get_model_parameters(model):\n    total_parameters = 0\n    for layer in list(model.parameters()):\n        layer_parameter = 1\n        for l in list(layer.size()):\n            layer_parameter *= l\n        total_parameters += layer_parameter\n    return total_parameters\n\n\ndef main(args, logger):\n    train_loader, test_loader = load_data(args)\n    if args.dataset == \'CIFAR10\':\n        num_classes = 10\n    elif args.dataset == \'CIFAR100\':\n        num_classes = 100\n    elif args.dataset == \'IMAGENET\':\n        num_classes = 1000\n\n    print(\'img_size: {}, num_classes: {}, stem: {}\'.format(args.img_size, num_classes, args.stem))\n    if args.model_name == \'ResNet26\':\n        print(\'Model Name: {0}\'.format(args.model_name))\n        model = ResNet26(num_classes=num_classes, stem=args.stem)\n    elif args.model_name == \'ResNet38\':\n        print(\'Model Name: {0}\'.format(args.model_name))\n        model = ResNet38(num_classes=num_classes, stem=args.stem)\n    elif args.model_name == \'ResNet50\':\n        print(\'Model Name: {0}\'.format(args.model_name))\n        model = ResNet50(num_classes=num_classes, stem=args.stem)\n\n    if args.pretrained_model:\n        filename = \'best_model_\' + str(args.dataset) + \'_\' + str(args.model_name) + \'_\' + str(args.stem) + \'_ckpt.tar\'\n        print(\'filename :: \', filename)\n        file_path = os.path.join(\'./checkpoint\', filename)\n        checkpoint = torch.load(file_path)\n\n        model.load_state_dict(checkpoint[\'state_dict\'])\n        start_epoch = checkpoint[\'epoch\']\n        best_acc = checkpoint[\'best_acc\']\n        model_parameters = checkpoint[\'parameters\']\n        print(\'Load model, Parameters: {0}, Start_epoch: {1}, Acc: {2}\'.format(model_parameters, start_epoch, best_acc))\n        logger.info(\'Load model, Parameters: {0}, Start_epoch: {1}, Acc: {2}\'.format(model_parameters, start_epoch, best_acc))\n    else:\n        start_epoch = 1\n        best_acc = 0.0\n\n    if args.cuda:\n        if torch.cuda.device_count() > 1:\n            model = nn.DataParallel(model)\n        model = model.cuda()\n\n    print(""Number of model parameters: "", get_model_parameters(model))\n    logger.info(""Number of model parameters: {0}"".format(get_model_parameters(model)))\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n\n    for epoch in range(start_epoch, args.epochs + 1):\n        train(model, train_loader, optimizer, criterion, epoch, args, logger)\n        eval_acc = eval(model, test_loader, args)\n\n        is_best = eval_acc > best_acc\n        best_acc = max(eval_acc, best_acc)\n\n        if not os.path.isdir(\'checkpoint\'):\n            os.mkdir(\'checkpoint\')\n        filename = \'model_\' + str(args.dataset) + \'_\' + str(args.model_name) + \'_\' + str(args.stem) + \'_ckpt.tar\'\n        print(\'filename :: \', filename)\n\n        parameters = get_model_parameters(model)\n\n        if torch.cuda.device_count() > 1:\n            save_checkpoint({\n                \'epoch\': epoch,\n                \'arch\': args.model_name,\n                \'state_dict\': model.module.state_dict(),\n                \'best_acc\': best_acc,\n                \'optimizer\': optimizer.state_dict(),\n                \'parameters\': parameters,\n            }, is_best, filename)\n        else:\n            save_checkpoint({\n                \'epoch\': epoch,\n                \'arch\': args.model_name,\n                \'state_dict\': model.state_dict(),\n                \'best_acc\': best_acc,\n                \'optimizer\': optimizer.state_dict(),\n                \'parameters\': parameters,\n            }, is_best, filename)\n\n\ndef save_checkpoint(state, is_best, filename):\n    file_path = os.path.join(\'./checkpoint\', filename)\n    torch.save(state, file_path)\n    best_file_path = os.path.join(\'./checkpoint\', \'best_\' + filename)\n    if is_best:\n        print(\'best Model Saving ...\')\n        shutil.copyfile(file_path, best_file_path)\n\n\nif __name__ == \'__main__\':\n    args, logger = get_args()\n    main(args, logger)\n'"
model.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom attention import AttentionConv, AttentionStem\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_channels, out_channels, stride=1, groups=1, base_width=64):\n        super(Bottleneck, self).__init__()\n        self.stride = stride\n        width = int(out_channels * (base_width / 64.)) * groups\n\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels, width, kernel_size=1, bias=False),\n            nn.BatchNorm2d(width),\n            nn.ReLU(),\n        )\n        self.conv2 = nn.Sequential(\n            AttentionConv(width, width, kernel_size=7, padding=3, groups=8),\n            nn.BatchNorm2d(width),\n            nn.ReLU(),\n        )\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(width, self.expansion * out_channels, kernel_size=1, bias=False),\n            nn.BatchNorm2d(self.expansion * out_channels),\n        )\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != self.expansion * out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion * out_channels)\n            )\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.conv2(out)\n        out = self.conv3(out)\n        if self.stride >= 2:\n            out = F.avg_pool2d(out, (self.stride, self.stride))\n\n        out += self.shortcut(x)\n        out = F.relu(out)\n\n        return out\n\n\nclass Model(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=1000, stem=False):\n        super(Model, self).__init__()\n        self.in_places = 64\n\n        if stem:\n            self.init = nn.Sequential(\n                # CIFAR10\n                AttentionStem(in_channels=3, out_channels=64, kernel_size=4, stride=1, padding=2, groups=1),\n                nn.BatchNorm2d(64),\n                nn.ReLU(),\n\n                # For ImageNet\n                # AttentionStem(in_channels=3, out_channels=64, kernel_size=4, stride=1, padding=2, groups=1),\n                # nn.BatchNorm2d(64),\n                # nn.ReLU(),\n                # nn.MaxPool2d(4, 4)\n            )\n        else:\n            self.init = nn.Sequential(\n                # CIFAR10\n                nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n                nn.BatchNorm2d(64),\n                nn.ReLU(),\n\n                # For ImageNet\n                # nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n                # nn.BatchNorm2d(64),\n                # nn.ReLU(),\n                # nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n            )\n\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.dense = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_places, planes, stride))\n            self.in_places = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.init(x)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.dense(out)\n\n        return out\n\n\ndef ResNet26(num_classes=1000, stem=False):\n    return Model(Bottleneck, [1, 2, 4, 1], num_classes=num_classes, stem=stem)\n\n\ndef ResNet38(num_classes=1000, stem=False):\n    return Model(Bottleneck, [2, 3, 5, 2], num_classes=num_classes, stem=stem)\n\n\ndef ResNet50(num_classes=1000, stem=False):\n    return Model(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, stem=stem)\n\n\ndef get_model_parameters(model):\n    total_parameters = 0\n    for layer in list(model.parameters()):\n        layer_parameter = 1\n        for l in list(layer.size()):\n            layer_parameter *= l\n        total_parameters += layer_parameter\n    return total_parameters\n\n\n# temp = torch.randn((2, 3, 224, 224))\n# model = ResNet38(num_classes=1000, stem=True)\n# print(get_model_parameters(model))\n'"
preprocess.py,6,"b""import torch\n\nfrom torchvision import datasets, transforms\n\n\ndef load_data(args):\n    print('Load Dataset :: {}'.format(args.dataset))\n    if args.dataset == 'CIFAR10':\n        transform_train = transforms.Compose([\n            transforms.RandomCrop(32, padding=4),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=(0.4914, 0.4822, 0.4465),\n                std=(0.2470, 0.2435, 0.2616)\n            )\n        ])\n\n        transform_test = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=(0.4914, 0.4822, 0.4465),\n                std=(0.2470, 0.2435, 0.2616)\n            )\n        ])\n\n        train_loader = torch.utils.data.DataLoader(\n            datasets.CIFAR10('data', train=True, download=True, transform=transform_train),\n            batch_size=args.batch_size,\n            shuffle=True,\n            num_workers=args.num_workers\n        )\n\n        test_loader = torch.utils.data.DataLoader(\n            datasets.CIFAR10('data', train=False, transform=transform_test),\n            batch_size=args.batch_size,\n            shuffle=False,\n            num_workers=args.num_workers\n        )\n\n    elif args.dataset == 'CIFAR100':\n        transform_train = transforms.Compose([\n            transforms.RandomCrop(32, padding=4),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=(0.5071, 0.4865, 0.4409),\n                std=(0.2673, 0.2564, 0.2762)\n            ),\n        ])\n        transform_test = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=(0.5071, 0.4865, 0.4409),\n                std=(0.2673, 0.2564, 0.2762)\n            ),\n        ])\n\n        train_loader = torch.utils.data.DataLoader(\n            datasets.CIFAR100('data', train=True, download=True, transform=transform_train),\n            batch_size=args.batch_size,\n            shuffle=True,\n            num_workers=args.num_workers\n        )\n\n        test_loader = torch.utils.data.DataLoader(\n            datasets.CIFAR100('data', train=False, transform=transform_test),\n            batch_size=args.batch_size,\n            shuffle=False,\n            num_workers=args.num_workers\n        )\n\n    elif args.dataset == 'MNIST':\n        transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=(0.1307,),\n                std=(0.3081,)\n            )\n        ])\n        train_loader = torch.utils.data.DataLoader(\n            datasets.MNIST('data', train=True, download=True, transform=transform),\n            batch_size=args.batch_size,\n            shuffle=True,\n            num_workers=args.num_workers\n        )\n\n        test_loader = torch.utils.data.DataLoader(\n            datasets.MNIST('data', train=False, transform=transform),\n            batch_size=args.batch_size,\n            shuffle=False,\n            num_workers=args.num_workers\n        )\n\n    elif args.dataset == 'IMAGENET':\n        pass\n\n    return train_loader, test_loader\n"""
