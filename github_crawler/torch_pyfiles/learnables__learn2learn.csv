file_path,api_count,code
setup.py,0,"b'#!/usr/bin/env python3\n\nimport sys\nimport re\n\nfrom distutils.core import setup\nfrom setuptools import (\n    setup as install,\n    find_packages,\n    Extension\n)\n\n# Parses version number: https://stackoverflow.com/a/7071358\nVERSIONFILE = \'learn2learn/_version.py\'\nverstrline = open(VERSIONFILE, ""rt"").read()\nVSRE = r""^__version__ = [\'\\""]([^\'\\""]*)[\'\\""]""\nmo = re.search(VSRE, verstrline, re.M)\nif mo:\n    VERSION = mo.group(1)\nelse:\n    raise RuntimeError(\'Unable to find version string in %s.\' % (VERSIONFILE,))\n\n# Compile with Cython\n# https://cython.readthedocs.io/en/latest/src/userguide/source_files_and_compilation.html\n# https://github.com/FedericoStra/cython-package-example/blob/master/setup.py\nextension_type = \'.c\'\ncmd_class = {}\nuse_cython = \'develop\' in sys.argv or \'build_ext\' in sys.argv\nif use_cython:\n    from Cython.Build import cythonize\n    from Cython.Distutils import build_ext\n    extension_type = \'.pyx\'\n    cmd_class = {\'build_ext\': build_ext}\n\nextensions = [\n    Extension(name=\'learn2learn.data.meta_dataset\',\n              sources=[\'learn2learn/data/meta_dataset\' + extension_type]), \n    Extension(name=\'learn2learn.data.task_dataset\',\n              sources=[\'learn2learn/data/task_dataset\' + extension_type]), \n    Extension(name=\'learn2learn.data.transforms\',\n              sources=[\'learn2learn/data/transforms\' + extension_type]), \n]\n\nif use_cython:\n    compiler_directives = {\'language_level\': 3,\n                           \'embedsignature\': True,\n    #                       \'profile\': True,\n    #                       \'binding\': True,\n    }\n    extensions = cythonize(extensions, compiler_directives=compiler_directives)\n\n# Installs the package\ninstall(\n    name=\'learn2learn\',\n    packages=find_packages(),\n    ext_modules=extensions,\n    cmdclass=cmd_class,\n    zip_safe=False,  # as per Cython docs\n    version=VERSION,\n    description=\'PyTorch Meta-Learning Framework for Researchers\',\n    long_description=open(\'README.md\').read(),\n    long_description_content_type=\'text/markdown\',\n    author=\'Debajyoti Datta, Ian bunner, Seb Arnold, Praateek Mahajan\',\n    author_email=\'smr.arnold@gmail.com, praateekm@gmail.com\',\n    url=\'https://github.com/learnables/learn2learn\',\n    download_url=\'https://github.com/learnables/learn2learn/archive/\' + str(VERSION) + \'.zip\',\n    license=\'MIT\',\n    classifiers=[],\n    scripts=[],\n    setup_requires=[\'cython>=0.28.5\',],\n    install_requires=[\n        \'numpy>=1.15.4\',\n        \'gym>=0.14.0\',\n        \'torch>=1.1.0\',\n        \'torchvision>=0.3.0\',\n        \'pandas\',\n        \'requests\',\n    ],\n)\n'"
examples/maml_toy.py,0,"b'#!/usr/bin/env python3\n\n""""""\nThis script demonstrates how to use the MAML implementation of L2L.\n\nEach task i consists of learning the parameters of a Normal distribution N(mu_i, sigma_i).\nThe parameters mu_i, sigma_i are themselves sampled from a distribution N(mu, sigma).\n""""""\n\nimport torch as th\nfrom torch import nn, optim, distributions as dist\n\nimport learn2learn as l2l\n\nDIM = 5\nTIMESTEPS = 1000\nTASKS_PER_STEP = 50\n\n\nclass Model(nn.Module):\n\n    def __init__(self):\n        super(Model, self).__init__()\n        self.mu = nn.Parameter(th.randn(DIM))\n        self.sigma = nn.Parameter(th.randn(DIM))\n\n    def forward(self, x=None):\n        return dist.Normal(self.mu, self.sigma)\n\n\ndef main():\n    task_dist = dist.Normal(th.zeros(2 * DIM), th.ones(2 * DIM))\n    model = Model()\n    maml = l2l.algorithms.MAML(model, lr=1e-2)\n    opt = optim.Adam(maml.parameters())\n\n    for i in range(TIMESTEPS):\n        step_loss = 0.0\n        for t in range(TASKS_PER_STEP):\n            # Sample a task\n            task_params = task_dist.sample()\n            mu_i, sigma_i = task_params[:DIM], task_params[DIM:]\n\n            # Adaptation: Instanciate a copy of model\n            learner = maml.clone()\n            proposal = learner()\n\n            # Adaptation: Compute and adapt to task loss\n            loss = (mu_i - proposal.mean).pow(2).sum() + (sigma_i - proposal.variance).pow(2).sum()\n            learner.adapt(loss)\n\n            # Adaptation: Evaluate the effectiveness of adaptation\n            adapt_loss = (mu_i - proposal.mean).pow(2).sum() + (sigma_i - proposal.variance).pow(2).sum()\n\n            # Accumulate the error over all tasks\n            step_loss += adapt_loss\n\n        # Meta-learning step: compute gradient through the adaptation step, automatically.\n        step_loss = step_loss / TASKS_PER_STEP\n        print(i, step_loss.item())\n        opt.zero_grad()\n        step_loss.backward()\n        opt.step()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
learn2learn/__init__.py,0,b'#!/usr/bin/env python3\n\nfrom . import algorithms\nfrom . import data\nfrom . import gym\nfrom . import text\nfrom . import vision\nfrom ._version import __version__\nfrom .utils import *\n'
learn2learn/_version.py,0,"b""__version__ = '0.1.1'\n"""
learn2learn/utils.py,5,"b'#!/usr/bin/env python3\n\nimport copy\n\nimport torch\n\n\ndef magic_box(x):\n    """"""\n\n    [[Source]](https://github.com/learnables/learn2learn/blob/master/learn2learn/utils.py)\n\n    **Description**\n\n    The magic box operator, which evaluates to 1 but whose gradient is \\\\(dx\\\\):\n\n    $$\\\\boxdot (x) = \\\\exp(x - \\\\bot(x))$$\n\n    where \\\\(\\\\bot\\\\) is the stop-gradient (or detach) operator.\n\n    This operator is useful when computing higher-order derivatives of stochastic graphs.\n    For more informations, please refer to the DiCE paper. (Reference 1)\n\n    **References**\n\n    1. Foerster et al. 2018. ""DiCE: The Infinitely Differentiable Monte-Carlo Estimator."" arXiv.\n\n    **Arguments**\n\n    * **x** (Variable) - Variable to transform.\n\n    **Return**\n\n    * (Variable) - Tensor of 1, but it\'s gradient is the gradient of x.\n\n    **Example**\n\n    ~~~python\n    loss = (magic_box(cum_log_probs) * advantages).mean()  # loss is the mean advantage\n    loss.backward()\n    ~~~\n    """"""\n    if isinstance(x, torch.Tensor):\n        return torch.exp(x - x.detach())\n    return x\n\n\ndef clone_parameters(param_list):\n    return [p.clone() for p in param_list]\n\n\ndef clone_module(module):\n    """"""\n\n    [[Source]](https://github.com/learnables/learn2learn/blob/master/learn2learn/utils.py)\n\n    **Description**\n\n    Creates a copy of a module, whose parameters/buffers/submodules\n    are created using PyTorch\'s torch.clone().\n\n    This implies that the computational graph is kept, and you can compute\n    the derivatives of the new modules\' parameters w.r.t the original\n    parameters.\n\n    **Arguments**\n\n    * **module** (Module) - Module to be cloned.\n\n    **Return**\n\n    * (Module) - The cloned module.\n\n    **Example**\n\n    ~~~python\n    net = nn.Sequential(Linear(20, 10), nn.ReLU(), nn.Linear(10, 2))\n    clone = clone_module(net)\n    error = loss(clone(X), y)\n    error.backward()  # Gradients are back-propagate all the way to net.\n    ~~~\n    """"""\n    # NOTE: This function might break in future versions of PyTorch.\n\n    # TODO: This function might require that module.forward()\n    #       was called in order to work properly, if forward() instanciates\n    #       new variables.\n    # TODO: We can probably get away with a shallowcopy.\n    #       However, since shallow copy does not recurse, we need to write a\n    #       recursive version of shallow copy.\n    # NOTE: This can probably be implemented more cleanly with\n    #       clone = recursive_shallow_copy(model)\n    #       clone._apply(lambda t: t.clone())\n\n    # First, create a copy of the module.\n    # Adapted from:\n    # https://github.com/pytorch/pytorch/blob/65bad41cbec096aa767b3752843eddebf845726f/torch/nn/modules/module.py#L1171\n    if not isinstance(module, torch.nn.Module):\n        return module\n    clone = module.__new__(type(module))\n    clone.__dict__ = module.__dict__.copy()\n    clone._parameters = clone._parameters.copy()\n    clone._buffers = clone._buffers.copy()\n    clone._modules = clone._modules.copy()\n\n    # Second, re-write all parameters\n    if hasattr(clone, \'_parameters\'):\n        for param_key in module._parameters:\n            if module._parameters[param_key] is not None:\n                cloned = module._parameters[param_key].clone()\n                clone._parameters[param_key] = cloned\n\n    # Third, handle the buffers if necessary\n    if hasattr(clone, \'_buffers\'):\n        for buffer_key in module._buffers:\n            if clone._buffers[buffer_key] is not None and \\\n                    clone._buffers[buffer_key].requires_grad:\n                clone._buffers[buffer_key] = module._buffers[buffer_key].clone()\n\n    # Then, recurse for each submodule\n    if hasattr(clone, \'_modules\'):\n        for module_key in clone._modules:\n            clone._modules[module_key] = clone_module(module._modules[module_key])\n\n    # Finally, rebuild the flattened parameters for RNNs\n    # See this issue for more details:\n    # https://github.com/learnables/learn2learn/issues/139\n    clone = clone._apply(lambda x: x)\n    return clone\n\n\ndef detach_module(module):\n    """"""\n\n    [[Source]](https://github.com/learnables/learn2learn/blob/master/learn2learn/utils.py)\n\n    **Description**\n\n    Detaches all parameters/buffers of a previously cloned module from its computational graph.\n\n    Note: detach works in-place, so it does not return a copy.\n\n    **Arguments**\n\n    * **module** (Module) - Module to be detached.\n\n    **Example**\n\n    ~~~python\n    net = nn.Sequential(Linear(20, 10), nn.ReLU(), nn.Linear(10, 2))\n    clone = clone_module(net)\n    detach_module(clone)\n    error = loss(clone(X), y)\n    error.backward()  # Gradients are back-propagate on clone, not net.\n    ~~~\n    """"""\n    if not isinstance(module, torch.nn.Module):\n        return\n    # First, re-write all parameters\n    for param_key in module._parameters:\n        if module._parameters[param_key] is not None:\n            detached = module._parameters[param_key].detach_()\n\n    # Second, handle the buffers if necessary\n    for buffer_key in module._buffers:\n        if module._buffers[buffer_key] is not None and \\\n                module._buffers[buffer_key].requires_grad:\n            module._buffers[buffer_key] = module._buffers[buffer_key].detach_()\n\n    # Then, recurse for each submodule\n    for module_key in module._modules:\n        detach_module(module._modules[module_key])\n\n\ndef clone_distribution(dist):\n    # TODO: This function was never tested.\n    clone = copy.deepcopy(dist)\n\n    for param_key in clone.__dict__:\n        item = clone.__dict__[param_key]\n        if isinstance(item, th.Tensor):\n            if item.requires_grad:\n                clone.__dict__[param_key] = dist.__dict__[param_key].clone()\n        elif isinstance(item, th.nn.Module):\n            clone.__dict__[param_key] = clone_module(dist.__dict__[param_key])\n        elif isinstance(item, th.Distribution):\n            clone.__dict__[param_key] = clone_distribution(dist.__dict__[param_key])\n\n    return clone\n\n\ndef detach_distribution(dist):\n    # TODO: This function was never tested.\n    for param_key in dist.__dict__:\n        item = dist.__dict__[param_key]\n        if isinstance(item, th.Tensor):\n            if item.requires_grad:\n                dist.__dict__[param_key] = dist.__dict__[param_key].detach()\n        elif isinstance(item, th.nn.Module):\n            dist.__dict__[param_key] = detach_module(dist.__dict__[param_key])\n        elif isinstance(item, th.Distribution):\n            dist.__dict__[param_key] = detach_distribution(dist.__dict__[param_key])\n    return dist\n'"
tests/__init__.py,0,b'#!/usr/bin/env python3\n'
examples/rl/__init__.py,0,b'#!/usr/bin/env python3\n'
examples/rl/dist_promp.py,5,"b'#!/usr/bin/env python3\n\n""""""\nTrains a 2-layer MLP with ProMP distributed across the 4 training processes.\n\nUsage:\n\nOMP_NUM_THREADS=1 \\\nMKL_NUM_THREADS=1 \\\npython -m torch.distributed.launch \\\n          --nproc_per_node=4 \\\n            examples/rl/dist_promp.py\n""""""\n\nimport random\nfrom copy import deepcopy\n\nimport cherry as ch\nimport gym\nimport numpy as np\nimport torch\nfrom cherry.algorithms import ppo, trpo\nfrom cherry.models.robotics import LinearValue\nfrom torch import optim, distributed as dist\nfrom torch.distributions.kl import kl_divergence\nfrom tqdm import tqdm\n\nimport learn2learn as l2l\nfrom policies import DiagNormalPolicy\n\nWORLD_SIZE = 4\n\n\ndef compute_advantages(baseline, tau, gamma, rewards, dones, states, next_states):\n    # Update baseline\n    returns = ch.td.discount(gamma, rewards, dones)\n    baseline.fit(states, returns)\n    values = baseline(states)\n    next_values = baseline(next_states)\n    bootstraps = values * (1.0 - dones) + next_values * dones\n    next_value = torch.zeros(1, device=values.device)\n    return ch.pg.generalized_advantage(tau=tau,\n                                       gamma=gamma,\n                                       rewards=rewards,\n                                       dones=dones,\n                                       values=bootstraps,\n                                       next_value=next_value)\n\n\ndef maml_a2c_loss(train_episodes, learner, baseline, gamma, tau):\n    # Update policy and baseline\n    states = train_episodes.state()\n    actions = train_episodes.action()\n    rewards = train_episodes.reward()\n    dones = train_episodes.done()\n    next_states = train_episodes.next_state()\n    log_probs = learner.log_prob(states, actions)\n    advantages = compute_advantages(baseline, tau, gamma, rewards,\n                                    dones, states, next_states)\n    advantages = ch.normalize(advantages).detach()\n    return a2c.policy_loss(log_probs, advantages)\n\n\ndef fast_adapt_a2c(clone, train_episodes, adapt_lr, baseline, gamma, tau, first_order=False):\n    loss = maml_a2c_loss(train_episodes, clone, baseline, gamma, tau)\n    clone.adapt(loss, first_order=first_order)\n    return clone\n\n\ndef precompute_quantities(states, actions, old_policy, new_policy):\n    old_density = old_policy.density(states)\n    old_log_probs = old_density.log_prob(actions).mean(dim=1, keepdim=True).detach()\n    new_density = new_policy.density(states)\n    new_log_probs = new_density.log_prob(actions).mean(dim=1, keepdim=True)\n    return old_density, new_density, old_log_probs, new_log_probs\n\n\ndef main(\n        env_name=\'Particles2D-v1\',\n        adapt_lr=0.1,\n        meta_lr=0.001,\n        adapt_steps=3,\n        num_iterations=300,\n        meta_bsz=20,\n        adapt_bsz=40,\n        ppo_clip=0.3,\n        ppo_steps=5,\n        tau=1.00,\n        gamma=0.99,\n        eta=0.0005,\n        adaptive_penalty=True,\n        kl_target=0.01,\n        num_workers=2,\n        seed=42,\n):\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--local_rank"", type=int)\n    args = parser.parse_args()\n    dist.init_process_group(\'gloo\',\n                            init_method=\'file://.dist_init_promp\',\n                            rank=args.local_rank,\n                            world_size=WORLD_SIZE)\n\n    rank = dist.get_rank()\n    meta_bsz /= WORLD_SIZE\n    seed += rank\n    torch.set_num_threads(1)\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\n    def make_env():\n        return gym.make(env_name)\n\n    env = l2l.gym.AsyncVectorEnv([make_env for _ in range(num_workers)])\n    env.seed(seed)\n    env = ch.envs.Torch(env)\n    policy = DiagNormalPolicy(input_size=env.state_size,\n                              output_size=env.action_size,\n                              hiddens=[64, 64])\n    meta_learner = l2l.algorithms.MAML(policy, lr=meta_lr)\n    baseline = LinearValue(env.state_size, env.action_size)\n    opt = optim.Adam(meta_learner.parameters(), lr=meta_lr)\n\n    all_rewards = []\n    for iteration in range(num_iterations):\n        iteration_reward = 0.0\n        iteration_replays = []\n        iteration_policies = []\n\n        # Sample Trajectories\n        for task_config in tqdm(env.sample_tasks(meta_bsz), leave=False, desc=\'Data\'):\n            clone = deepcopy(meta_learner)\n            env.set_task(task_config)\n            env.reset()\n            task = ch.envs.Runner(env)\n            task_replay = []\n            task_policies = []\n\n            # Fast Adapt\n            for step in range(adapt_steps):\n                for p in clone.parameters():\n                    p.detach_().requires_grad_()\n                task_policies.append(deepcopy(clone))\n                train_episodes = task.run(clone, episodes=adapt_bsz)\n                clone = fast_adapt_a2c(clone, train_episodes, adapt_lr,\n                                       baseline, gamma, tau, first_order=True)\n                task_replay.append(train_episodes)\n\n            # Compute Validation Loss\n            for p in clone.parameters():\n                p.detach_().requires_grad_()\n            task_policies.append(deepcopy(clone))\n            valid_episodes = task.run(clone, episodes=adapt_bsz)\n            task_replay.append(valid_episodes)\n            iteration_reward += valid_episodes.reward().sum().item() / adapt_bsz\n            iteration_replays.append(task_replay)\n            iteration_policies.append(task_policies)\n\n        # Print statistics\n        print(\'\\nIteration\', iteration)\n        adaptation_reward = iteration_reward / meta_bsz\n        all_rewards.append(adaptation_reward)\n        print(\'adaptation_reward\', adaptation_reward)\n\n        # ProMP meta-optimization\n        for ppo_step in tqdm(range(ppo_steps), leave=False, desc=\'Optim\'):\n            promp_loss = 0.0\n            kl_total = 0.0\n            for task_replays, old_policies in zip(iteration_replays, iteration_policies):\n                new_policy = meta_learner.clone()\n                states = task_replays[0].state()\n                actions = task_replays[0].action()\n                rewards = task_replays[0].reward()\n                dones = task_replays[0].done()\n                next_states = task_replays[0].next_state()\n                old_policy = old_policies[0]\n                (old_density,\n                 new_density,\n                 old_log_probs,\n                 new_log_probs) = precompute_quantities(states,\n                                                        actions,\n                                                        old_policy,\n                                                        new_policy)\n                for step in range(adapt_steps):\n                    # Compute KL penalty\n                    kl_pen = kl_divergence(old_density, new_density).mean()\n                    kl_total += kl_pen.item()\n\n                    # Update the clone\n                    advantages = compute_advantages(baseline, tau, gamma, rewards,\n                                                    dones, states, next_states)\n                    advantages = ch.normalize(advantages).detach()\n                    surr_loss = trpo.policy_loss(new_log_probs, old_log_probs, advantages)\n                    new_policy.adapt(surr_loss)\n\n                    # Move to next adaptation step\n                    states = task_replays[step + 1].state()\n                    actions = task_replays[step + 1].action()\n                    rewards = task_replays[step + 1].reward()\n                    dones = task_replays[step + 1].done()\n                    next_states = task_replays[step + 1].next_state()\n                    old_policy = old_policies[step + 1]\n                    (old_density,\n                     new_density,\n                     old_log_probs,\n                     new_log_probs) = precompute_quantities(states,\n                                                            actions,\n                                                            old_policy,\n                                                            new_policy)\n\n                    # Compute clip loss\n                    advantages = compute_advantages(baseline, tau, gamma, rewards,\n                                                    dones, states, next_states)\n                    advantages = ch.normalize(advantages).detach()\n                    clip_loss = ppo.policy_loss(new_log_probs,\n                                                old_log_probs,\n                                                advantages,\n                                                clip=ppo_clip)\n\n                    # Combine into ProMP loss\n                    promp_loss += clip_loss - eta * kl_pen\n\n            kl_total /= meta_bsz * adapt_steps\n            promp_loss /= meta_bsz * adapt_steps\n            opt.zero_grad()\n            promp_loss.backward(retain_graph=True)\n            opt.step()\n\n            # Adapt KL penalty based on desired target\n            if adaptive_penalty:\n                if kl_total < kl_target / 1.5:\n                    eta /= 2.0\n                elif kl_total > kl_target * 1.5:\n                    eta *= 2.0\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/rl/maml_dice.py,3,"b'#!/usr/bin/env python3\n\n""""""\nTrains a 2-layer MLP with MAML-VPG augmented with the DiCE objective.\n\nUsage:\n\npython examples/rl/maml_dice.py\n""""""\n\nimport random\n\nimport cherry as ch\nimport gym\nimport numpy as np\nimport torch\nfrom cherry.algorithms import a2c\nfrom cherry.models.robotics import LinearValue\nfrom torch import optim\nfrom tqdm import tqdm\n\nimport learn2learn as l2l\nfrom policies import DiagNormalPolicy\n\n\ndef weighted_cumsum(values, weights):\n    for i in range(values.size(0)):\n        values[i] += values[i - 1] * weights[i]\n    return values\n\n\ndef compute_advantages(baseline, tau, gamma, rewards, dones, states, next_states):\n    # Update baseline\n    returns = ch.td.discount(gamma, rewards, dones)\n    baseline.fit(states, returns)\n    values = baseline(states)\n    next_values = baseline(next_states)\n    bootstraps = values * (1.0 - dones) + next_values * dones\n    next_value = torch.zeros(1, device=values.device)\n    return ch.pg.generalized_advantage(tau=tau,\n                                       gamma=gamma,\n                                       rewards=rewards,\n                                       dones=dones,\n                                       values=bootstraps,\n                                       next_value=next_value)\n\n\ndef maml_a2c_loss(train_episodes, learner, baseline, gamma, tau):\n    # Update policy and baseline\n    states = train_episodes.state()\n    actions = train_episodes.action()\n    rewards = train_episodes.reward()\n    dones = train_episodes.done()\n    next_states = train_episodes.next_state()\n    log_probs = learner.log_prob(states, actions)\n    weights = torch.ones_like(dones)\n    weights[1:].add_(-1.0, dones[:-1])\n    weights /= dones.sum()\n    cum_log_probs = weighted_cumsum(log_probs, weights)\n    advantages = compute_advantages(baseline, tau, gamma, rewards,\n                                    dones, states, next_states)\n    return a2c.policy_loss(l2l.magic_box(cum_log_probs), advantages)\n\n\ndef main(\n        experiment=\'dev\',\n        env_name=\'Particles2D-v1\',\n        adapt_lr=0.1,\n        meta_lr=0.001,\n        adapt_steps=1,\n        num_iterations=200,\n        meta_bsz=20,\n        adapt_bsz=20,\n        tau=1.00,\n        gamma=0.99,\n        num_workers=2,\n        seed=42,\n):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\n    def make_env():\n        return gym.make(env_name)\n\n    env = l2l.gym.AsyncVectorEnv([make_env for _ in range(num_workers)])\n    env.seed(seed)\n    env = ch.envs.Torch(env)\n    policy = DiagNormalPolicy(env.state_size, env.action_size)\n    meta_learner = l2l.algorithms.MAML(policy, lr=meta_lr)\n    baseline = LinearValue(env.state_size, env.action_size)\n    opt = optim.Adam(policy.parameters(), lr=meta_lr)\n    all_rewards = []\n\n    for iteration in range(num_iterations):\n        iteration_loss = 0.0\n        iteration_reward = 0.0\n        for task_config in tqdm(env.sample_tasks(meta_bsz), leave=False, desc=\'Data\'):  # Samples a new config\n            learner = meta_learner.clone()\n            env.set_task(task_config)\n            env.reset()\n            task = ch.envs.Runner(env)\n\n            # Fast Adapt\n            for step in range(adapt_steps):\n                train_episodes = task.run(learner, episodes=adapt_bsz)\n                loss = maml_a2c_loss(train_episodes, learner, baseline, gamma, tau)\n                learner.adapt(loss)\n\n            # Compute Validation Loss\n            valid_episodes = task.run(learner, episodes=adapt_bsz)\n            loss = maml_a2c_loss(valid_episodes, learner, baseline, gamma, tau)\n            iteration_loss += loss\n            iteration_reward += valid_episodes.reward().sum().item() / adapt_bsz\n\n        # Print statistics\n        print(\'\\nIteration\', iteration)\n        adaptation_reward = iteration_reward / meta_bsz\n        print(\'adaptation_reward\', adaptation_reward)\n        all_rewards.append(adaptation_reward)\n\n        adaptation_loss = iteration_loss / meta_bsz\n        print(\'adaptation_loss\', adaptation_loss.item())\n\n        opt.zero_grad()\n        adaptation_loss.backward()\n        opt.step()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/rl/maml_trpo.py,8,"b'#!/usr/bin/env python3\n\n""""""\nTrains a 2-layer MLP with MAML-TRPO.\n\nUsage:\n\npython examples/rl/maml_trpo.py\n""""""\n\nimport random\nfrom copy import deepcopy\n\nimport cherry as ch\nimport gym\nimport numpy as np\nimport torch\nfrom cherry.algorithms import a2c, trpo\nfrom cherry.models.robotics import LinearValue\nfrom torch import autograd\nfrom torch.distributions.kl import kl_divergence\nfrom torch.nn.utils import parameters_to_vector, vector_to_parameters\nfrom tqdm import tqdm\n\nimport learn2learn as l2l\nfrom policies import DiagNormalPolicy\n\n\ndef compute_advantages(baseline, tau, gamma, rewards, dones, states, next_states):\n    # Update baseline\n    returns = ch.td.discount(gamma, rewards, dones)\n    baseline.fit(states, returns)\n    values = baseline(states)\n    next_values = baseline(next_states)\n    bootstraps = values * (1.0 - dones) + next_values * dones\n    next_value = torch.zeros(1, device=values.device)\n    return ch.pg.generalized_advantage(tau=tau,\n                                       gamma=gamma,\n                                       rewards=rewards,\n                                       dones=dones,\n                                       values=bootstraps,\n                                       next_value=next_value)\n\n\ndef maml_a2c_loss(train_episodes, learner, baseline, gamma, tau):\n    # Update policy and baseline\n    states = train_episodes.state()\n    actions = train_episodes.action()\n    rewards = train_episodes.reward()\n    dones = train_episodes.done()\n    next_states = train_episodes.next_state()\n    log_probs = learner.log_prob(states, actions)\n    advantages = compute_advantages(baseline, tau, gamma, rewards,\n                                    dones, states, next_states)\n    advantages = ch.normalize(advantages).detach()\n    return a2c.policy_loss(log_probs, advantages)\n\n\ndef fast_adapt_a2c(clone, train_episodes, adapt_lr, baseline, gamma, tau, first_order=False):\n    second_order = not first_order\n    loss = maml_a2c_loss(train_episodes, clone, baseline, gamma, tau)\n    gradients = autograd.grad(loss,\n                              clone.parameters(),\n                              retain_graph=second_order,\n                              create_graph=second_order)\n    return l2l.algorithms.maml.maml_update(clone, adapt_lr, gradients)\n\n\ndef meta_surrogate_loss(iteration_replays, iteration_policies, policy, baseline, tau, gamma, adapt_lr):\n    mean_loss = 0.0\n    mean_kl = 0.0\n    for task_replays, old_policy in tqdm(zip(iteration_replays, iteration_policies),\n                                         total=len(iteration_replays),\n                                         desc=\'Surrogate Loss\',\n                                         leave=False):\n        train_replays = task_replays[:-1]\n        valid_episodes = task_replays[-1]\n        new_policy = l2l.clone_module(policy)\n\n        # Fast Adapt\n        for train_episodes in train_replays:\n            new_policy = fast_adapt_a2c(new_policy, train_episodes, adapt_lr,\n                                        baseline, gamma, tau, first_order=False)\n\n        # Useful values\n        states = valid_episodes.state()\n        actions = valid_episodes.action()\n        next_states = valid_episodes.next_state()\n        rewards = valid_episodes.reward()\n        dones = valid_episodes.done()\n\n        # Compute KL\n        old_densities = old_policy.density(states)\n        new_densities = new_policy.density(states)\n        kl = kl_divergence(new_densities, old_densities).mean()\n        mean_kl += kl\n\n        # Compute Surrogate Loss\n        advantages = compute_advantages(baseline, tau, gamma, rewards, dones, states, next_states)\n        advantages = ch.normalize(advantages).detach()\n        old_log_probs = old_densities.log_prob(actions).mean(dim=1, keepdim=True).detach()\n        new_log_probs = new_densities.log_prob(actions).mean(dim=1, keepdim=True)\n        mean_loss += trpo.policy_loss(new_log_probs, old_log_probs, advantages)\n    mean_kl /= len(iteration_replays)\n    mean_loss /= len(iteration_replays)\n    return mean_loss, mean_kl\n\n\ndef main(\n        env_name=\'AntDirection-v1\',\n        adapt_lr=0.1,\n        meta_lr=1.0,\n        adapt_steps=1,\n        num_iterations=1000,\n        meta_bsz=40,\n        adapt_bsz=20,\n        tau=1.00,\n        gamma=0.99,\n        seed=42,\n        num_workers=2,\n        cuda=0,\n):\n    cuda = bool(cuda)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if cuda:\n        torch.cuda.manual_seed(seed)\n\n    def make_env():\n        env = gym.make(env_name)\n        env = ch.envs.ActionSpaceScaler(env)\n        return env\n\n    env = l2l.gym.AsyncVectorEnv([make_env for _ in range(num_workers)])\n    env.seed(seed)\n    env.set_task(env.sample_tasks(1)[0])\n    env = ch.envs.Torch(env)\n    policy = DiagNormalPolicy(env.state_size, env.action_size)\n    if cuda:\n        policy.to(\'cuda\')\n    baseline = LinearValue(env.state_size, env.action_size)\n\n    for iteration in range(num_iterations):\n        iteration_reward = 0.0\n        iteration_replays = []\n        iteration_policies = []\n\n        for task_config in tqdm(env.sample_tasks(meta_bsz), leave=False, desc=\'Data\'):  # Samples a new config\n            clone = deepcopy(policy)\n            env.set_task(task_config)\n            env.reset()\n            task = ch.envs.Runner(env)\n            task_replay = []\n\n            # Fast Adapt\n            for step in range(adapt_steps):\n                train_episodes = task.run(clone, episodes=adapt_bsz)\n                clone = fast_adapt_a2c(clone, train_episodes, adapt_lr,\n                                       baseline, gamma, tau, first_order=True)\n                task_replay.append(train_episodes)\n\n            # Compute Validation Loss\n            valid_episodes = task.run(clone, episodes=adapt_bsz)\n            task_replay.append(valid_episodes)\n            iteration_reward += valid_episodes.reward().sum().item() / adapt_bsz\n            iteration_replays.append(task_replay)\n            iteration_policies.append(clone)\n\n        # Print statistics\n        print(\'\\nIteration\', iteration)\n        adaptation_reward = iteration_reward / meta_bsz\n        print(\'adaptation_reward\', adaptation_reward)\n\n        # TRPO meta-optimization\n        backtrack_factor = 0.5\n        ls_max_steps = 15\n        max_kl = 0.01\n        if cuda:\n            policy.to(\'cuda\', non_blocking=True)\n            baseline.to(\'cuda\', non_blocking=True)\n            iteration_replays = [[r.to(\'cuda\', non_blocking=True) for r in task_replays] for task_replays in\n                                 iteration_replays]\n\n        # Compute CG step direction\n        old_loss, old_kl = meta_surrogate_loss(iteration_replays, iteration_policies, policy, baseline, tau, gamma,\n                                               adapt_lr)\n        grad = autograd.grad(old_loss,\n                             policy.parameters(),\n                             retain_graph=True)\n        grad = parameters_to_vector([g.detach() for g in grad])\n        Fvp = trpo.hessian_vector_product(old_kl, policy.parameters())\n        step = trpo.conjugate_gradient(Fvp, grad)\n        shs = 0.5 * torch.dot(step, Fvp(step))\n        lagrange_multiplier = torch.sqrt(shs / max_kl)\n        step = step / lagrange_multiplier\n        step_ = [torch.zeros_like(p.data) for p in policy.parameters()]\n        vector_to_parameters(step, step_)\n        step = step_\n        del old_kl, Fvp, grad\n        old_loss.detach_()\n\n        # Line-search\n        for ls_step in range(ls_max_steps):\n            stepsize = backtrack_factor ** ls_step * meta_lr\n            clone = deepcopy(policy)\n            for p, u in zip(clone.parameters(), step):\n                p.data.add_(-stepsize, u.data)\n            new_loss, kl = meta_surrogate_loss(iteration_replays, iteration_policies, clone, baseline, tau, gamma,\n                                               adapt_lr)\n            if new_loss < old_loss and kl < max_kl:\n                for p, u in zip(policy.parameters(), step):\n                    p.data.add_(-stepsize, u.data)\n                break\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/rl/metasgd_a2c.py,3,"b'#!/usr/bin/env python3\n\n""""""\nTrains a 2-layer MLP with MetaSGD-VPG.\n\nUsage:\n\npython examples/rl/maml_trpo.py\n""""""\n\nimport random\n\nimport cherry as ch\nimport gym\nimport numpy as np\nimport torch\nfrom cherry.algorithms import a2c\nfrom cherry.models.robotics import LinearValue\nfrom torch import optim\nfrom tqdm import tqdm\n\nimport learn2learn as l2l\nfrom policies import DiagNormalPolicy\n\n\ndef compute_advantages(baseline, tau, gamma, rewards, dones, states, next_states):\n    # Update baseline\n    returns = ch.td.discount(gamma, rewards, dones)\n    baseline.fit(states, returns)\n    values = baseline(states)\n    next_values = baseline(next_states)\n    bootstraps = values * (1.0 - dones) + next_values * dones\n    next_value = torch.zeros(1, device=values.device)\n    return ch.pg.generalized_advantage(tau=tau,\n                                       gamma=gamma,\n                                       rewards=rewards,\n                                       dones=dones,\n                                       values=bootstraps,\n                                       next_value=next_value)\n\n\ndef maml_a2c_loss(train_episodes, learner, baseline, gamma, tau):\n    # Update policy and baseline\n    states = train_episodes.state()\n    actions = train_episodes.action()\n    rewards = train_episodes.reward()\n    dones = train_episodes.done()\n    next_states = train_episodes.next_state()\n    log_probs = learner.log_prob(states, actions)\n    advantages = compute_advantages(baseline, tau, gamma, rewards,\n                                    dones, states, next_states)\n    advantages = ch.normalize(advantages).detach()\n    return a2c.policy_loss(log_probs, advantages)\n\n\ndef main(\n        experiment=\'dev\',\n        env_name=\'Particles2D-v1\',\n        adapt_lr=0.1,\n        meta_lr=0.01,\n        adapt_steps=1,\n        num_iterations=200,\n        meta_bsz=20,\n        adapt_bsz=20,\n        tau=1.00,\n        gamma=0.99,\n        num_workers=2,\n        seed=42,\n):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\n    def make_env():\n        return gym.make(env_name)\n\n    env = l2l.gym.AsyncVectorEnv([make_env for _ in range(num_workers)])\n    env.seed(seed)\n    env = ch.envs.Torch(env)\n    policy = DiagNormalPolicy(env.state_size, env.action_size)\n    meta_learner = l2l.algoritorch.MetaSGD(policy, lr=meta_lr)\n    baseline = LinearValue(env.state_size, env.action_size)\n    opt = optim.Adam(policy.parameters(), lr=meta_lr)\n    all_rewards = []\n\n    for iteration in range(num_iterations):\n        iteration_loss = 0.0\n        iteration_reward = 0.0\n        for task_config in tqdm(env.sample_tasks(meta_bsz)):  # Samples a new config\n            learner = meta_learner.clone()\n            env.set_task(task_config)\n            env.reset()\n            task = ch.envs.Runner(env)\n\n            # Fast Adapt\n            for step in range(adapt_steps):\n                train_episodes = task.run(learner, episodes=adapt_bsz)\n                loss = maml_a2c_loss(train_episodes, learner, baseline, gamma, tau)\n                learner.adapt(loss)\n\n            # Compute Validation Loss\n            valid_episodes = task.run(learner, episodes=adapt_bsz)\n            loss = maml_a2c_loss(valid_episodes, learner, baseline, gamma, tau)\n            iteration_loss += loss\n            iteration_reward += valid_episodes.reward().sum().item() / adapt_bsz\n\n        # Print statistics\n        print(\'\\nIteration\', iteration)\n        adaptation_reward = iteration_reward / meta_bsz\n        print(\'adaptation_reward\', adaptation_reward)\n        all_rewards.append(adaptation_reward)\n\n        adaptation_loss = iteration_loss / meta_bsz\n        print(\'adaptation_loss\', adaptation_loss.item())\n\n        opt.zero_grad()\n        adaptation_loss.backward()\n        opt.step()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/rl/policies.py,3,"b""#!/usr/bin/env python3\n\nimport math\n\nimport cherry as ch\nimport torch\nfrom torch import nn\nfrom torch.distributions import Normal, Categorical\n\nEPSILON = 1e-6\n\n\ndef linear_init(module):\n    if isinstance(module, nn.Linear):\n        nn.init.xavier_uniform_(module.weight)\n        module.bias.data.zero_()\n    return module\n\n\nclass DiagNormalPolicy(nn.Module):\n\n    def __init__(self, input_size, output_size, hiddens=None, activation='relu'):\n        super(DiagNormalPolicy, self).__init__()\n        if hiddens is None:\n            hiddens = [100, 100]\n        if activation == 'relu':\n            activation = nn.ReLU\n        elif activation == 'tanh':\n            activation = nn.Tanh\n        layers = [linear_init(nn.Linear(input_size, hiddens[0])), activation()]\n        for i, o in zip(hiddens[:-1], hiddens[1:]):\n            layers.append(linear_init(nn.Linear(i, o)))\n            layers.append(activation())\n        layers.append(linear_init(nn.Linear(hiddens[-1], output_size)))\n        self.mean = nn.Sequential(*layers)\n        self.sigma = nn.Parameter(torch.Tensor(output_size))\n        self.sigma.data.fill_(math.log(1))\n\n    def density(self, state):\n        loc = self.mean(state)\n        scale = torch.exp(torch.clamp(self.sigma, min=math.log(EPSILON)))\n        return Normal(loc=loc, scale=scale)\n\n    def log_prob(self, state, action):\n        density = self.density(state)\n        return density.log_prob(action).mean(dim=1, keepdim=True)\n\n    def forward(self, state):\n        density = self.density(state)\n        action = density.sample()\n        return action\n\n\nclass CategoricalPolicy(nn.Module):\n\n    def __init__(self, input_size, output_size, hiddens=None):\n        super(CategoricalPolicy, self).__init__()\n        if hiddens is None:\n            hiddens = [100, 100]\n        layers = [linear_init(nn.Linear(input_size, hiddens[0])), nn.ReLU()]\n        for i, o in zip(hiddens[:-1], hiddens[1:]):\n            layers.append(linear_init(nn.Linear(i, o)))\n            layers.append(nn.ReLU())\n        layers.append(linear_init(nn.Linear(hiddens[-1], output_size)))\n        self.mean = nn.Sequential(*layers)\n        self.input_size = input_size\n\n    def forward(self, state):\n        state = ch.onehot(state, dim=self.input_size)\n        loc = self.mean(state)\n        density = Categorical(logits=loc)\n        action = density.sample()\n        log_prob = density.log_prob(action).mean().view(-1, 1).detach()\n        return action, {'density': density, 'log_prob': log_prob}\n"""
examples/rl/promp.py,3,"b'#!/usr/bin/env python3\n\n""""""\nTrains a 2-layer MLP with ProMP.\n\nUsage:\n\npython examples/rl/promp.py\n""""""\n\nimport random\nfrom copy import deepcopy\n\nimport cherry as ch\nimport gym\nimport numpy as np\nimport torch\nfrom cherry.algorithms import a2c, ppo, trpo\nfrom cherry.models.robotics import LinearValue\nfrom torch import optim\nfrom torch.distributions.kl import kl_divergence\nfrom tqdm import tqdm\n\nimport learn2learn as l2l\nfrom policies import DiagNormalPolicy\n\n\ndef compute_advantages(baseline, tau, gamma, rewards, dones, states, next_states):\n    # Update baseline\n    returns = ch.td.discount(gamma, rewards, dones)\n    baseline.fit(states, returns)\n    values = baseline(states)\n    next_values = baseline(next_states)\n    bootstraps = values * (1.0 - dones) + next_values * dones\n    next_value = torch.zeros(1, device=values.device)\n    return ch.pg.generalized_advantage(tau=tau,\n                                       gamma=gamma,\n                                       rewards=rewards,\n                                       dones=dones,\n                                       values=bootstraps,\n                                       next_value=next_value)\n\n\ndef maml_a2c_loss(train_episodes, learner, baseline, gamma, tau):\n    # Update policy and baseline\n    states = train_episodes.state()\n    actions = train_episodes.action()\n    rewards = train_episodes.reward()\n    dones = train_episodes.done()\n    next_states = train_episodes.next_state()\n    log_probs = learner.log_prob(states, actions)\n    advantages = compute_advantages(baseline, tau, gamma, rewards,\n                                    dones, states, next_states)\n    advantages = ch.normalize(advantages).detach()\n    return a2c.policy_loss(log_probs, advantages)\n\n\ndef fast_adapt_a2c(clone, train_episodes, adapt_lr, baseline, gamma, tau, first_order=False):\n    loss = maml_a2c_loss(train_episodes, clone, baseline, gamma, tau)\n    clone.adapt(loss, first_order=first_order)\n    return clone\n\n\ndef precompute_quantities(states, actions, old_policy, new_policy):\n    old_density = old_policy.density(states)\n    old_log_probs = old_density.log_prob(actions).mean(dim=1, keepdim=True).detach()\n    new_density = new_policy.density(states)\n    new_log_probs = new_density.log_prob(actions).mean(dim=1, keepdim=True)\n    return old_density, new_density, old_log_probs, new_log_probs\n\n\ndef main(\n        env_name=\'AntDirection-v1\',\n        adapt_lr=0.1,\n        meta_lr=3e-4,\n        adapt_steps=3,\n        num_iterations=1000,\n        meta_bsz=40,\n        adapt_bsz=20,\n        ppo_clip=0.3,\n        ppo_steps=5,\n        tau=1.00,\n        gamma=0.99,\n        eta=0.0005,\n        adaptive_penalty=False,\n        kl_target=0.01,\n        num_workers=4,\n        seed=421,\n):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\n    def make_env():\n        env = gym.make(env_name)\n        env = ch.envs.ActionSpaceScaler(env)\n        return env\n\n    env = l2l.gym.AsyncVectorEnv([make_env for _ in range(num_workers)])\n    env.seed(seed)\n    env = ch.envs.ActionSpaceScaler(env)\n    env = ch.envs.Torch(env)\n    policy = DiagNormalPolicy(input_size=env.state_size,\n                              output_size=env.action_size,\n                              hiddens=[64, 64],\n                              activation=\'tanh\')\n    meta_learner = l2l.algorithms.MAML(policy, lr=meta_lr)\n    baseline = LinearValue(env.state_size, env.action_size)\n    opt = optim.Adam(meta_learner.parameters(), lr=meta_lr)\n\n    for iteration in range(num_iterations):\n        iteration_reward = 0.0\n        iteration_replays = []\n        iteration_policies = []\n\n        # Sample Trajectories\n        for task_config in tqdm(env.sample_tasks(meta_bsz), leave=False, desc=\'Data\'):\n            clone = deepcopy(meta_learner)\n            env.set_task(task_config)\n            env.reset()\n            task = ch.envs.Runner(env)\n            task_replay = []\n            task_policies = []\n\n            # Fast Adapt\n            for step in range(adapt_steps):\n                for p in clone.parameters():\n                    p.detach_().requires_grad_()\n                task_policies.append(deepcopy(clone))\n                train_episodes = task.run(clone, episodes=adapt_bsz)\n                clone = fast_adapt_a2c(clone, train_episodes, adapt_lr,\n                                       baseline, gamma, tau, first_order=True)\n                task_replay.append(train_episodes)\n\n            # Compute Validation Loss\n            for p in clone.parameters():\n                p.detach_().requires_grad_()\n            task_policies.append(deepcopy(clone))\n            valid_episodes = task.run(clone, episodes=adapt_bsz)\n            task_replay.append(valid_episodes)\n            iteration_reward += valid_episodes.reward().sum().item() / adapt_bsz\n            iteration_replays.append(task_replay)\n            iteration_policies.append(task_policies)\n\n        # Print statistics\n        print(\'\\nIteration\', iteration)\n        adaptation_reward = iteration_reward / meta_bsz\n        print(\'adaptation_reward\', adaptation_reward)\n\n        # ProMP meta-optimization\n        for ppo_step in tqdm(range(ppo_steps), leave=False, desc=\'Optim\'):\n            promp_loss = 0.0\n            kl_total = 0.0\n            for task_replays, old_policies in zip(iteration_replays, iteration_policies):\n                new_policy = meta_learner.clone()\n                states = task_replays[0].state()\n                actions = task_replays[0].action()\n                rewards = task_replays[0].reward()\n                dones = task_replays[0].done()\n                next_states = task_replays[0].next_state()\n                old_policy = old_policies[0]\n                (old_density,\n                 new_density,\n                 old_log_probs,\n                 new_log_probs) = precompute_quantities(states,\n                                                        actions,\n                                                        old_policy,\n                                                        new_policy)\n                advantages = compute_advantages(baseline, tau, gamma, rewards,\n                                                dones, states, next_states)\n                advantages = ch.normalize(advantages).detach()\n                for step in range(adapt_steps):\n                    # Compute KL penalty\n                    kl_pen = kl_divergence(old_density, new_density).mean()\n                    kl_total += kl_pen.item()\n\n                    # Update the clone\n                    surr_loss = trpo.policy_loss(new_log_probs, old_log_probs, advantages)\n                    new_policy.adapt(surr_loss)\n\n                    # Move to next adaptation step\n                    states = task_replays[step + 1].state()\n                    actions = task_replays[step + 1].action()\n                    rewards = task_replays[step + 1].reward()\n                    dones = task_replays[step + 1].done()\n                    next_states = task_replays[step + 1].next_state()\n                    old_policy = old_policies[step + 1]\n                    (old_density,\n                     new_density,\n                     old_log_probs,\n                     new_log_probs) = precompute_quantities(states,\n                                                            actions,\n                                                            old_policy,\n                                                            new_policy)\n\n                    # Compute clip loss\n                    advantages = compute_advantages(baseline, tau, gamma, rewards,\n                                                    dones, states, next_states)\n                    advantages = ch.normalize(advantages).detach()\n                    clip_loss = ppo.policy_loss(new_log_probs,\n                                                old_log_probs,\n                                                advantages,\n                                                clip=ppo_clip)\n\n                    # Combine into ProMP loss\n                    promp_loss += clip_loss + eta * kl_pen\n\n            kl_total /= meta_bsz * adapt_steps\n            promp_loss /= meta_bsz * adapt_steps\n            opt.zero_grad()\n            promp_loss.backward(retain_graph=True)\n            opt.step()\n\n            # Adapt KL penalty based on desired target\n            if adaptive_penalty:\n                if kl_total < kl_target / 1.5:\n                    eta /= 2.0\n                elif kl_total > kl_target * 1.5:\n                    eta *= 2.0\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/text/news_topic_classification.py,9,"b'#!/usr/bin/env python3\n\nimport argparse\nimport random\n\nimport torch\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom tqdm import tqdm\n\nimport learn2learn as l2l\n\n\nclass Net(nn.Module):\n    """"""Head for sentence-level classification tasks.""""""\n\n    def __init__(self, num_classes, input_dim=768, inner_dim=200, pooler_dropout=0.3):\n        super().__init__()\n        self.dense = nn.Linear(input_dim, inner_dim)\n        self.activation_fn = nn.ReLU()\n        self.dropout = nn.Dropout(p=pooler_dropout)\n        self.out_proj = nn.Linear(inner_dim, num_classes)\n\n    def forward(self, x, **kwargs):\n        x = self.dropout(x)\n        x = self.dense(x)\n        x = self.activation_fn(x)\n        x = self.dropout(x)\n        x = F.log_softmax(self.out_proj(x), dim=1)\n        return x\n\n\ndef accuracy(predictions, targets):\n    predictions = predictions.argmax(dim=1)\n    acc = (predictions == targets).sum().float()\n    acc /= len(targets)\n    return acc.item()\n\n\ndef collate_tokens(values, pad_idx, eos_idx=None, left_pad=False, move_eos_to_beginning=False):\n    """"""Convert a list of 1d tensors into a padded 2d tensor.""""""\n    size = max(v.size(0) for v in values)\n    res = values[0].new(len(values), size).fill_(pad_idx)\n\n    def copy_tensor(src, dst):\n        assert dst.numel() == src.numel()\n        if move_eos_to_beginning:\n            assert src[-1] == eos_idx\n            dst[0] = eos_idx\n            dst[1:] = src[:-1]\n        else:\n            dst.copy_(src)\n\n    for i, v in enumerate(values):\n        copy_tensor(v, res[i][size - len(v):] if left_pad else res[i][:len(v)])\n    return res\n\n\ndef compute_loss(task, roberta, device, learner, loss_func, batch=15):\n    loss = 0.0\n    acc = 0.0\n    for i, (x, y) in enumerate(torch.utils.data.DataLoader(\n            task, batch_size=batch, shuffle=True, num_workers=0)):\n        # RoBERTa ENCODING\n        x = collate_tokens([roberta.encode(sent) for sent in x], pad_idx=1)\n        with torch.no_grad():\n            x = roberta.extract_features(x)\n        x = x[:, 0, :]\n\n        # Moving to device\n        x, y = x.to(device), y.view(-1).to(device)\n\n        output = learner(x)\n        curr_loss = loss_func(output, y)\n        acc += accuracy(output, y)\n        loss += curr_loss / len(task)\n    loss /= len(task)\n    return loss, acc\n\n\ndef main(lr=0.005, maml_lr=0.01, iterations=1000, ways=5, shots=1, tps=32, fas=5, device=torch.device(""cpu""),\n         download_location=""/tmp/text""):\n    text_train = l2l.text.datasets.NewsClassification(root=download_location, download=True)\n    train_gen = l2l.text.datasets.TaskGenerator(text_train, ways=ways)\n\n    torch.hub.set_dir(download_location)\n    roberta = torch.hub.load(\'pytorch/fairseq\', \'roberta.base\')\n    roberta.eval()\n    roberta.to(device)\n    model = Net(num_classes=ways)\n    model.to(device)\n    meta_model = l2l.algorithms.MAML(model, lr=maml_lr)\n    opt = optim.Adam(meta_model.parameters(), lr=lr)\n    loss_func = nn.NLLLoss(reduction=""sum"")\n\n    tqdm_bar = tqdm(range(iterations))\n    for iteration in tqdm_bar:\n        iteration_error = 0.0\n        iteration_acc = 0.0\n        for _ in range(tps):\n            learner = meta_model.clone()\n            train_task = train_gen.sample(shots=shots)\n            valid_task = train_gen.sample(shots=shots, classes_to_sample=train_task.sampled_classes)\n\n            # Fast Adaptation\n            for step in range(fas):\n                train_error, _ = compute_loss(train_task, roberta, device, learner, loss_func, batch=shots * ways)\n                learner.adapt(train_error)\n\n            # Compute validation loss\n            valid_error, valid_acc = compute_loss(valid_task, roberta, device, learner, loss_func,\n                                                  batch=shots * ways)\n            iteration_error += valid_error\n            iteration_acc += valid_acc\n\n        iteration_error /= tps\n        iteration_acc /= tps\n        tqdm_bar.set_description(""Loss : {:.3f} Acc : {:.3f}"".format(iteration_error.item(), iteration_acc))\n\n        # Take the meta-learning step\n        opt.zero_grad()\n        iteration_error.backward()\n        opt.step()\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'Learn2Learn Text Classification Example\')\n\n    parser.add_argument(\'--ways\', type=int, default=5, metavar=\'N\',\n                        help=\'number of ways (default: 5)\')\n    parser.add_argument(\'--shots\', type=int, default=1, metavar=\'N\',\n                        help=\'number of shots (default: 1)\')\n    parser.add_argument(\'-tps\', \'--tasks-per-step\', type=int, default=32, metavar=\'N\',\n                        help=\'tasks per step (default: 32)\')\n    parser.add_argument(\'-fas\', \'--fast-adaption-steps\', type=int, default=5, metavar=\'N\',\n                        help=\'steps per fast adaption (default: 5)\')\n\n    parser.add_argument(\'--iterations\', type=int, default=1000, metavar=\'N\',\n                        help=\'number of iterations (default: 1000)\')\n\n    parser.add_argument(\'--lr\', type=float, default=0.005, metavar=\'LR\',\n                        help=\'learning rate (default: 0.005)\')\n    parser.add_argument(\'--maml-lr\', type=float, default=0.01, metavar=\'LR\',\n                        help=\'learning rate for MAML (default: 0.01)\')\n\n    parser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                        help=\'disables CUDA training\')\n\n    parser.add_argument(\'--seed\', type=int, default=1, metavar=\'S\',\n                        help=\'random seed (default: 1)\')\n\n    parser.add_argument(\'--download-location\', type=str, default=""/tmp/text"", metavar=\'S\',\n                        help=\'download location for train data and roberta(default : /tmp/text\')\n\n    args = parser.parse_args()\n\n    use_cuda = not args.no_cuda and torch.cuda.is_available()\n\n    torch.manual_seed(args.seed)\n    random.seed(args.seed)\n\n    device = torch.device(""cuda"" if use_cuda else ""cpu"")\n\n    main(lr=args.lr, maml_lr=args.maml_lr, iterations=args.iterations, ways=args.ways, shots=args.shots,\n         tps=args.tasks_per_step, fas=args.fast_adaption_steps, device=device,\n         download_location=args.download_location)\n'"
examples/vision/anil_fc100.py,10,"b'#!/usr/bin/env python3\n\n""""""\n""""""\n\nimport os\nimport random\n\nimport numpy as np\nimport torch\nfrom torch import nn\nimport torchvision as tv\n\nimport learn2learn as l2l\nfrom learn2learn.data.transforms import FusedNWaysKShots, LoadData, RemapLabels, ConsecutiveLabels\n\nfrom statistics import mean\nfrom copy import deepcopy\n\n\nclass Lambda(nn.Module):\n\n    def __init__(self, fn):\n        super(Lambda, self).__init__()\n        self.fn = fn\n\n    def forward(self, x):\n        return self.fn(x)\n\n\ndef accuracy(predictions, targets):\n    predictions = predictions.argmax(dim=1).view(targets.shape)\n    return (predictions == targets).sum().float() / targets.size(0)\n\n\ndef fast_adapt(batch,\n               learner,\n               features,\n               loss,\n               adaptation_steps,\n               shots,\n               ways,\n               device=None):\n\n    data, labels = batch\n    data, labels = data.to(device), labels.to(device)\n    data = features(data)\n\n    # Separate data into adaptation/evaluation sets\n    adaptation_indices = np.zeros(data.size(0), dtype=bool)\n    adaptation_indices[np.arange(shots*ways) * 2] = True\n    evaluation_indices = torch.from_numpy(~adaptation_indices)\n    adaptation_indices = torch.from_numpy(adaptation_indices)\n    adaptation_data, adaptation_labels = data[adaptation_indices], labels[adaptation_indices]\n    evaluation_data, evaluation_labels = data[evaluation_indices], labels[evaluation_indices]\n\n    for step in range(adaptation_steps):\n        train_error = loss(learner(adaptation_data), adaptation_labels)\n        learner.adapt(train_error)\n\n    predictions = learner(evaluation_data)\n    valid_error = loss(predictions, evaluation_labels)\n    valid_accuracy = accuracy(predictions, evaluation_labels)\n    return valid_error, valid_accuracy\n\n\ndef main(\n        ways=5,\n        shots=5,\n        meta_lr=0.001,\n        fast_lr=0.1,\n        adapt_steps=5,\n        meta_bsz=32,\n        iters=1000,\n        cuda=1,\n        seed=42,\n):\n\n    cuda = bool(cuda)\n\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    device = torch.device(\'cpu\')\n    if cuda and torch.cuda.device_count():\n        torch.cuda.manual_seed(seed)\n        device = torch.device(\'cuda\')\n\n    # Create Datasets\n    train_dataset = l2l.vision.datasets.FC100(root=\'~/data\',\n                                              transform=tv.transforms.ToTensor(),\n                                              mode=\'train\')\n    valid_dataset = l2l.vision.datasets.FC100(root=\'~/data\',\n                                              transform=tv.transforms.ToTensor(),\n                                              mode=\'validation\')\n    test_dataset = l2l.vision.datasets.FC100(root=\'~/data\',\n                                              transform=tv.transforms.ToTensor(),\n                                             mode=\'test\')\n    train_dataset = l2l.data.MetaDataset(train_dataset)\n    valid_dataset = l2l.data.MetaDataset(valid_dataset)\n    test_dataset = l2l.data.MetaDataset(test_dataset)\n\n    train_transforms = [\n        FusedNWaysKShots(train_dataset, n=ways, k=2*shots),\n        LoadData(train_dataset),\n        RemapLabels(train_dataset),\n        ConsecutiveLabels(train_dataset),\n    ]\n    train_tasks = l2l.data.TaskDataset(train_dataset,\n                                       task_transforms=train_transforms,\n                                       num_tasks=20000)\n\n    valid_transforms = [\n        FusedNWaysKShots(valid_dataset, n=ways, k=2*shots),\n        LoadData(valid_dataset),\n        ConsecutiveLabels(valid_dataset),\n        RemapLabels(valid_dataset),\n    ]\n    valid_tasks = l2l.data.TaskDataset(valid_dataset,\n                                       task_transforms=valid_transforms,\n                                       num_tasks=600)\n\n    test_transforms = [\n        FusedNWaysKShots(test_dataset, n=ways, k=2*shots),\n        LoadData(test_dataset),\n        RemapLabels(test_dataset),\n        ConsecutiveLabels(test_dataset),\n    ]\n    test_tasks = l2l.data.TaskDataset(test_dataset,\n                                      task_transforms=test_transforms,\n                                      num_tasks=600)\n\n\n    # Create model\n    features = l2l.vision.models.ConvBase(output_size=64, channels=3, max_pool=True)\n    features = torch.nn.Sequential(features, Lambda(lambda x: x.view(-1, 256)))\n    features.to(device)\n    head = torch.nn.Linear(256, ways)\n    head = l2l.algorithms.MAML(head, lr=fast_lr)\n    head.to(device)\n\n    # Setup optimization\n    all_parameters = list(features.parameters()) + list(head.parameters())\n    optimizer = torch.optim.Adam(all_parameters, lr=meta_lr)\n    loss = nn.CrossEntropyLoss(reduction=\'mean\')\n\n    for iteration in range(iters):\n        optimizer.zero_grad()\n        meta_train_error = 0.0\n        meta_train_accuracy = 0.0\n        meta_valid_error = 0.0\n        meta_valid_accuracy = 0.0\n        meta_test_error = 0.0\n        meta_test_accuracy = 0.0\n        for task in range(meta_bsz):\n            # Compute meta-training loss\n            learner = head.clone()\n            batch = train_tasks.sample()\n            evaluation_error, evaluation_accuracy = fast_adapt(batch,\n                                                               learner,\n                                                               features,\n                                                               loss,\n                                                               adapt_steps,\n                                                               shots,\n                                                               ways,\n                                                               device)\n            evaluation_error.backward()\n            meta_train_error += evaluation_error.item()\n            meta_train_accuracy += evaluation_accuracy.item()\n\n            # Compute meta-validation loss\n            learner = head.clone()\n            batch = valid_tasks.sample()\n            evaluation_error, evaluation_accuracy = fast_adapt(batch,\n                                                               learner,\n                                                               features,\n                                                               loss,\n                                                               adapt_steps,\n                                                               shots,\n                                                               ways,\n                                                               device)\n            meta_valid_error += evaluation_error.item()\n            meta_valid_accuracy += evaluation_accuracy.item()\n\n            # Compute meta-testing loss\n            learner = head.clone()\n            batch = test_tasks.sample()\n            evaluation_error, evaluation_accuracy = fast_adapt(batch,\n                                                               learner,\n                                                               features,\n                                                               loss,\n                                                               adapt_steps,\n                                                               shots,\n                                                               ways,\n                                                               device)\n            meta_test_error += evaluation_error.item()\n            meta_test_accuracy += evaluation_accuracy.item()\n\n        # Print some metrics\n        print(\'\\n\')\n        print(\'Iteration\', iteration)\n        print(\'Meta Train Error\', meta_train_error / meta_bsz)\n        print(\'Meta Train Accuracy\', meta_train_accuracy / meta_bsz)\n        print(\'Meta Valid Error\', meta_valid_error / meta_bsz)\n        print(\'Meta Valid Accuracy\', meta_valid_accuracy / meta_bsz)\n        print(\'Meta Test Error\', meta_test_error / meta_bsz)\n        print(\'Meta Test Accuracy\', meta_test_accuracy / meta_bsz)\n\n        # Average the accumulated gradients and optimize\n        for p in all_parameters:\n            p.grad.data.mul_(1.0 / meta_bsz)\n        optimizer.step()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/vision/maml_miniimagenet.py,7,"b'#!/usr/bin/env python3\n\n""""""\nDemonstrates how to:\n    * use the MAML wrapper for fast-adaptation,\n    * use the benchmark interface to load mini-ImageNet, and\n    * sample tasks and split them in adaptation and evaluation sets.\n\nTo contrast the use of the benchmark interface with directly instantiating mini-ImageNet datasets and tasks, compare with `protonet_miniimagenet.py`.\n""""""\n\nimport random\nimport numpy as np\n\nimport torch\nfrom torch import nn, optim\n\nimport learn2learn as l2l\nfrom learn2learn.data.transforms import (NWays,\n                                         KShots,\n                                         LoadData,\n                                         RemapLabels,\n                                         ConsecutiveLabels)\n\n\ndef accuracy(predictions, targets):\n    predictions = predictions.argmax(dim=1).view(targets.shape)\n    return (predictions == targets).sum().float() / targets.size(0)\n\n\ndef fast_adapt(batch, learner, loss, adaptation_steps, shots, ways, device):\n    data, labels = batch\n    data, labels = data.to(device), labels.to(device)\n\n    # Separate data into adaptation/evalutation sets\n    adaptation_indices = np.zeros(data.size(0), dtype=bool)\n    adaptation_indices[np.arange(shots*ways) * 2] = True\n    evaluation_indices = torch.from_numpy(~adaptation_indices)\n    adaptation_indices = torch.from_numpy(adaptation_indices)\n    adaptation_data, adaptation_labels = data[adaptation_indices], labels[adaptation_indices]\n    evaluation_data, evaluation_labels = data[evaluation_indices], labels[evaluation_indices]\n\n    # Adapt the model\n    for step in range(adaptation_steps):\n        train_error = loss(learner(adaptation_data), adaptation_labels)\n        train_error /= len(adaptation_data)\n        learner.adapt(train_error)\n\n    # Evaluate the adapted model\n    predictions = learner(evaluation_data)\n    valid_error = loss(predictions, evaluation_labels)\n    valid_error /= len(evaluation_data)\n    valid_accuracy = accuracy(predictions, evaluation_labels)\n    return valid_error, valid_accuracy\n\n\ndef main(\n        ways=5,\n        shots=5,\n        meta_lr=0.003,\n        fast_lr=0.5,\n        meta_batch_size=32,\n        adaptation_steps=1,\n        num_iterations=60000,\n        cuda=True,\n        seed=42,\n):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    device = torch.device(\'cpu\')\n    if cuda and torch.cuda.device_count():\n        torch.cuda.manual_seed(seed)\n        device = torch.device(\'cuda\')\n\n    # Create Tasksets using the benchmark interface\n    tasksets = l2l.vision.benchmarks.get_tasksets(\'mini-imagenet\',\n                                                  train_samples=2*shots,\n                                                  train_ways=ways,\n                                                  test_samples=2*shots,\n                                                  test_ways=ways,\n                                                  root=\'~/data\',\n    )\n\n    # Create model\n    model = l2l.vision.models.MiniImagenetCNN(ways)\n    model.to(device)\n    maml = l2l.algorithms.MAML(model, lr=fast_lr, first_order=False)\n    opt = optim.Adam(maml.parameters(), meta_lr)\n    loss = nn.CrossEntropyLoss(reduction=\'mean\')\n\n    for iteration in range(num_iterations):\n        opt.zero_grad()\n        meta_train_error = 0.0\n        meta_train_accuracy = 0.0\n        meta_valid_error = 0.0\n        meta_valid_accuracy = 0.0\n        for task in range(meta_batch_size):\n            # Compute meta-training loss\n            learner = maml.clone()\n            batch = tasksets.train.sample()\n            evaluation_error, evaluation_accuracy = fast_adapt(batch,\n                                                               learner,\n                                                               loss,\n                                                               adaptation_steps,\n                                                               shots,\n                                                               ways,\n                                                               device)\n            evaluation_error.backward()\n            meta_train_error += evaluation_error.item()\n            meta_train_accuracy += evaluation_accuracy.item()\n\n            # Compute meta-validation loss\n            learner = maml.clone()\n            batch = tasksets.validation.sample()\n            evaluation_error, evaluation_accuracy = fast_adapt(batch,\n                                                               learner,\n                                                               loss,\n                                                               adaptation_steps,\n                                                               shots,\n                                                               ways,\n                                                               device)\n            meta_valid_error += evaluation_error.item()\n            meta_valid_accuracy += evaluation_accuracy.item()\n\n        # Print some metrics\n        print(\'\\n\')\n        print(\'Iteration\', iteration)\n        print(\'Meta Train Error\', meta_train_error / meta_batch_size)\n        print(\'Meta Train Accuracy\', meta_train_accuracy / meta_batch_size)\n        print(\'Meta Valid Error\', meta_valid_error / meta_batch_size)\n        print(\'Meta Valid Accuracy\', meta_valid_accuracy / meta_batch_size)\n\n        # Average the accumulated gradients and optimize\n        for p in maml.parameters():\n            p.grad.data.mul_(1.0 / meta_batch_size)\n        opt.step()\n\n    meta_test_error = 0.0\n    meta_test_accuracy = 0.0\n    for task in range(meta_batch_size):\n        # Compute meta-testing loss\n        learner = maml.clone()\n        batch = tasksets.test.sample()\n        evaluation_error, evaluation_accuracy = fast_adapt(batch,\n                                                           learner,\n                                                           loss,\n                                                           adaptation_steps,\n                                                           shots,\n                                                           ways,\n                                                           device)\n        meta_test_error += evaluation_error.item()\n        meta_test_accuracy += evaluation_accuracy.item()\n    print(\'Meta Test Error\', meta_test_error / meta_batch_size)\n    print(\'Meta Test Accuracy\', meta_test_accuracy / meta_batch_size)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/vision/maml_omniglot.py,6,"b'#!/usr/bin/env python3\n\n""""""\nDemonstrates how to:\n    * use the MAML wrapper for fast-adaptation,\n    * use the benchmark interface to load Omniglot, and\n    * sample tasks and split them in adaptation and evaluation sets.\n""""""\n\nimport random\nimport numpy as np\nimport torch\nimport learn2learn as l2l\n\nfrom torch import nn, optim\n\n\n\ndef accuracy(predictions, targets):\n    predictions = predictions.argmax(dim=1).view(targets.shape)\n    return (predictions == targets).sum().float() / targets.size(0)\n\n\ndef fast_adapt(batch, learner, loss, adaptation_steps, shots, ways, device):\n    data, labels = batch\n    data, labels = data.to(device), labels.to(device)\n\n    # Separate data into adaptation/evalutation sets\n    adaptation_indices = np.zeros(data.size(0), dtype=bool)\n    adaptation_indices[np.arange(shots*ways) * 2] = True\n    evaluation_indices = torch.from_numpy(~adaptation_indices)\n    adaptation_indices = torch.from_numpy(adaptation_indices)\n    adaptation_data, adaptation_labels = data[adaptation_indices], labels[adaptation_indices]\n    evaluation_data, evaluation_labels = data[evaluation_indices], labels[evaluation_indices]\n\n    # Adapt the model\n    for step in range(adaptation_steps):\n        train_error = loss(learner(adaptation_data), adaptation_labels)\n        train_error /= len(adaptation_data)\n        learner.adapt(train_error)\n\n    # Evaluate the adapted model\n    predictions = learner(evaluation_data)\n    valid_error = loss(predictions, evaluation_labels)\n    valid_error /= len(evaluation_data)\n    valid_accuracy = accuracy(predictions, evaluation_labels)\n    return valid_error, valid_accuracy\n\n\ndef main(\n        ways=5,\n        shots=1,\n        meta_lr=0.003,\n        fast_lr=0.5,\n        meta_batch_size=32,\n        adaptation_steps=1,\n        num_iterations=60000,\n        cuda=True,\n        seed=42,\n):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    device = torch.device(\'cpu\')\n    if cuda:\n        torch.cuda.manual_seed(seed)\n        device = torch.device(\'cuda\')\n\n    # Load train/validation/test tasksets using the benchmark interface\n    tasksets = l2l.vision.benchmarks.get_tasksets(\'omniglot\',\n                                                  train_ways=ways,\n                                                  train_samples=2*shots,\n                                                  test_ways=ways,\n                                                  test_samples=2*shots,\n                                                  num_tasks=20000,\n                                                  root=\'~/data\',\n    )\n\n    # Create model\n    model = l2l.vision.models.OmniglotFC(28 ** 2, ways)\n    model.to(device)\n    maml = l2l.algorithms.MAML(model, lr=fast_lr, first_order=False)\n    opt = optim.Adam(maml.parameters(), meta_lr)\n    loss = nn.CrossEntropyLoss(reduction=\'mean\')\n\n    for iteration in range(num_iterations):\n        opt.zero_grad()\n        meta_train_error = 0.0\n        meta_train_accuracy = 0.0\n        meta_valid_error = 0.0\n        meta_valid_accuracy = 0.0\n        for task in range(meta_batch_size):\n            # Compute meta-training loss\n            learner = maml.clone()\n            batch = tasksets.train.sample()\n            evaluation_error, evaluation_accuracy = fast_adapt(batch,\n                                                               learner,\n                                                               loss,\n                                                               adaptation_steps,\n                                                               shots,\n                                                               ways,\n                                                               device)\n            evaluation_error.backward()\n            meta_train_error += evaluation_error.item()\n            meta_train_accuracy += evaluation_accuracy.item()\n\n            # Compute meta-validation loss\n            learner = maml.clone()\n            batch = tasksets.validation.sample()\n            evaluation_error, evaluation_accuracy = fast_adapt(batch,\n                                                               learner,\n                                                               loss,\n                                                               adaptation_steps,\n                                                               shots,\n                                                               ways,\n                                                               device)\n            meta_valid_error += evaluation_error.item()\n            meta_valid_accuracy += evaluation_accuracy.item()\n\n        # Print some metrics\n        print(\'\\n\')\n        print(\'Iteration\', iteration)\n        print(\'Meta Train Error\', meta_train_error / meta_batch_size)\n        print(\'Meta Train Accuracy\', meta_train_accuracy / meta_batch_size)\n        print(\'Meta Valid Error\', meta_valid_error / meta_batch_size)\n        print(\'Meta Valid Accuracy\', meta_valid_accuracy / meta_batch_size)\n\n        # Average the accumulated gradients and optimize\n        for p in maml.parameters():\n            p.grad.data.mul_(1.0 / meta_batch_size)\n        opt.step()\n\n    meta_test_error = 0.0\n    meta_test_accuracy = 0.0\n    for task in range(meta_batch_size):\n        # Compute meta-testing loss\n        learner = maml.clone()\n        batch = tasksets.test.sample()\n        evaluation_error, evaluation_accuracy = fast_adapt(batch,\n                                                           learner,\n                                                           loss,\n                                                           adaptation_steps,\n                                                           shots,\n                                                           ways,\n                                                           device)\n        meta_test_error += evaluation_error.item()\n        meta_test_accuracy += evaluation_accuracy.item()\n    print(\'Meta Test Error\', meta_test_error / meta_batch_size)\n    print(\'Meta Test Accuracy\', meta_test_accuracy / meta_batch_size)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/vision/meta_mnist.py,11,"b'#!/usr/bin/env python3\n\nimport argparse\nimport random\n\nimport numpy as np\nimport torch\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\n\nimport learn2learn as l2l\n\n\nclass Net(nn.Module):\n    def __init__(self, ways=3):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n        self.fc2 = nn.Linear(500, ways)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 4 * 4 * 50)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n\ndef accuracy(predictions, targets):\n    predictions = predictions.argmax(dim=1)\n    acc = (predictions == targets).sum().float()\n    acc /= len(targets)\n    return acc.item()\n\n\ndef main(lr=0.005, maml_lr=0.01, iterations=1000, ways=5, shots=1, tps=32, fas=5, device=torch.device(""cpu""),\n         download_location=\'~/data\'):\n    transformations = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,)),\n        lambda x: x.view(1, 28, 28),\n    ])\n\n    mnist_train = l2l.data.MetaDataset(MNIST(download_location,\n                                             train=True,\n                                             download=True,\n                                             transform=transformations))\n\n    train_tasks = l2l.data.TaskDataset(mnist_train,\n                                       task_transforms=[\n                                            l2l.data.transforms.NWays(mnist_train, ways),\n                                            l2l.data.transforms.KShots(mnist_train, 2*shots),\n                                            l2l.data.transforms.LoadData(mnist_train),\n                                            l2l.data.transforms.RemapLabels(mnist_train),\n                                            l2l.data.transforms.ConsecutiveLabels(mnist_train),\n                                       ],\n                                       num_tasks=1000)\n\n    model = Net(ways)\n    model.to(device)\n    meta_model = l2l.algorithms.MAML(model, lr=maml_lr)\n    opt = optim.Adam(meta_model.parameters(), lr=lr)\n    loss_func = nn.NLLLoss(reduction=\'mean\')\n\n    for iteration in range(iterations):\n        iteration_error = 0.0\n        iteration_acc = 0.0\n        for _ in range(tps):\n            learner = meta_model.clone()\n            train_task = train_tasks.sample()\n            data, labels = train_task\n            data = data.to(device)\n            labels = labels.to(device)\n\n            # Separate data into adaptation/evalutation sets\n            adaptation_indices = np.zeros(data.size(0), dtype=bool)\n            adaptation_indices[np.arange(shots*ways) * 2] = True\n            evaluation_indices = torch.from_numpy(~adaptation_indices)\n            adaptation_indices = torch.from_numpy(adaptation_indices)\n            adaptation_data, adaptation_labels = data[adaptation_indices], labels[adaptation_indices]\n            evaluation_data, evaluation_labels = data[evaluation_indices], labels[evaluation_indices]\n\n            # Fast Adaptation\n            for step in range(fas):\n                train_error = loss_func(learner(adaptation_data), adaptation_labels)\n                learner.adapt(train_error)\n\n            # Compute validation loss\n            predictions = learner(evaluation_data)\n            valid_error = loss_func(predictions, evaluation_labels)\n            valid_error /= len(evaluation_data)\n            valid_accuracy = accuracy(predictions, evaluation_labels)\n            iteration_error += valid_error\n            iteration_acc += valid_accuracy\n\n        iteration_error /= tps\n        iteration_acc /= tps\n        print(\'Loss : {:.3f} Acc : {:.3f}\'.format(iteration_error.item(), iteration_acc))\n\n        # Take the meta-learning step\n        opt.zero_grad()\n        iteration_error.backward()\n        opt.step()\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'Learn2Learn MNIST Example\')\n\n    parser.add_argument(\'--ways\', type=int, default=5, metavar=\'N\',\n                        help=\'number of ways (default: 5)\')\n    parser.add_argument(\'--shots\', type=int, default=1, metavar=\'N\',\n                        help=\'number of shots (default: 1)\')\n    parser.add_argument(\'-tps\', \'--tasks-per-step\', type=int, default=32, metavar=\'N\',\n                        help=\'tasks per step (default: 32)\')\n    parser.add_argument(\'-fas\', \'--fast-adaption-steps\', type=int, default=5, metavar=\'N\',\n                        help=\'steps per fast adaption (default: 5)\')\n\n    parser.add_argument(\'--iterations\', type=int, default=1000, metavar=\'N\',\n                        help=\'number of iterations (default: 1000)\')\n\n    parser.add_argument(\'--lr\', type=float, default=0.005, metavar=\'LR\',\n                        help=\'learning rate (default: 0.005)\')\n    parser.add_argument(\'--maml-lr\', type=float, default=0.01, metavar=\'LR\',\n                        help=\'learning rate for MAML (default: 0.01)\')\n\n    parser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                        help=\'disables CUDA training\')\n\n    parser.add_argument(\'--seed\', type=int, default=1, metavar=\'S\',\n                        help=\'random seed (default: 1)\')\n\n    parser.add_argument(\'--download-location\', type=str, default=""/tmp/mnist"", metavar=\'S\',\n                        help=\'download location for train data (default : /tmp/mnist\')\n\n    args = parser.parse_args()\n\n    use_cuda = not args.no_cuda and torch.cuda.is_available()\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if use_cuda:\n        torch.cuda.manual_seed(args.seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\n    device = torch.device(""cuda"" if use_cuda else ""cpu"")\n\n    main(lr=args.lr,\n         maml_lr=args.maml_lr,\n         iterations=args.iterations,\n         ways=args.ways,\n         shots=args.shots,\n         tps=args.tasks_per_step,\n         fas=args.fast_adaption_steps,\n         device=device,\n         download_location=args.download_location)\n'"
examples/vision/protonet_miniimagenet.py,12,"b'#!/usr/bin/env python3\n\nimport argparse\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\nimport learn2learn as l2l\nfrom learn2learn.data.transforms import NWays, KShots, LoadData, RemapLabels\n\n\ndef pairwise_distances_logits(a, b):\n    n = a.shape[0]\n    m = b.shape[0]\n    logits = -((a.unsqueeze(1).expand(n, m, -1) -\n                b.unsqueeze(0).expand(n, m, -1))**2).sum(dim=2)\n    return logits\n\n\ndef accuracy(predictions, targets):\n    predictions = predictions.argmax(dim=1).view(targets.shape)\n    return (predictions == targets).sum().float() / targets.size(0)\n\n\nclass Convnet(nn.Module):\n\n    def __init__(self, x_dim=3, hid_dim=64, z_dim=64):\n        super().__init__()\n        self.encoder = l2l.vision.models.ConvBase(output_size=z_dim,\n                                                  hidden=hid_dim,\n                                                  channels=x_dim)\n        self.out_channels = 1600\n\n    def forward(self, x):\n        x = self.encoder(x)\n        return x.view(x.size(0), -1)\n\n\ndef fast_adapt(model, batch, ways, shot, query_num, metric=None, device=None):\n    if metric is None:\n        metric = pairwise_distances_logits\n    if device is None:\n        device = model.device()\n    data, labels = batch\n    data = data.to(device)\n    labels = labels.to(device)\n    n_items = shot * ways\n\n    # Sort data samples by labels\n    # TODO: Can this be replaced by ConsecutiveLabels ?\n    sort = torch.sort(labels)\n    data = data.squeeze(0)[sort.indices].squeeze(0)\n    labels = labels.squeeze(0)[sort.indices].squeeze(0)\n\n    # Compute support and query embeddings\n    embeddings = model(data)\n    support_indices = np.zeros(data.size(0), dtype=bool)\n    selection = np.arange(ways) * (shot + query_num)\n    for offset in range(shot):\n        support_indices[selection + offset] = True\n    query_indices = torch.from_numpy(~support_indices)\n    support_indices = torch.from_numpy(support_indices)\n    support = embeddings[support_indices]\n    support = support.reshape(ways, shot, -1).mean(dim=1)\n    query = embeddings[query_indices]\n    labels = labels[query_indices].long()\n\n    logits = pairwise_distances_logits(query, support)\n    loss = F.cross_entropy(logits, labels)\n    acc = accuracy(logits, labels)\n    return loss, acc\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--max-epoch\', type=int, default=250)\n    parser.add_argument(\'--shot\', type=int, default=1)\n    parser.add_argument(\'--test-way\', type=int, default=5)\n    parser.add_argument(\'--test-shot\', type=int, default=1)\n    parser.add_argument(\'--test-query\', type=int, default=30)\n    parser.add_argument(\'--train-query\', type=int, default=15)\n    parser.add_argument(\'--train-way\', type=int, default=30)\n    parser.add_argument(\'--gpu\', default=0)\n    args = parser.parse_args()\n    print(args)\n\n    device = torch.device(\'cpu\')\n    if args.gpu and torch.cuda.device_count():\n        print(""Using gpu"")\n        torch.cuda.manual_seed(43)\n        device = torch.device(\'cuda\')\n\n    model = Convnet()\n    model.to(device)\n\n    path_data = \'~/data\'\n    train_dataset = l2l.vision.datasets.MiniImagenet(\n        root=path_data, mode=\'train\')\n    valid_dataset = l2l.vision.datasets.MiniImagenet(\n        root=path_data, mode=\'validation\')\n    test_dataset = l2l.vision.datasets.MiniImagenet(\n        root=path_data, mode=\'test\')\n\n    train_dataset = l2l.data.MetaDataset(train_dataset)\n    train_transforms = [\n        NWays(train_dataset, args.train_way),\n        KShots(train_dataset, args.train_query + args.shot),\n        LoadData(train_dataset),\n        RemapLabels(train_dataset),\n    ]\n    train_tasks = l2l.data.TaskDataset(train_dataset, task_transforms=train_transforms)\n    train_loader = DataLoader(train_tasks, pin_memory=True, shuffle=True)\n\n    valid_dataset = l2l.data.MetaDataset(valid_dataset)\n    valid_transforms = [\n        NWays(valid_dataset, args.test_way),\n        KShots(valid_dataset, args.test_query + args.test_shot),\n        LoadData(valid_dataset),\n        RemapLabels(valid_dataset),\n    ]\n    valid_tasks = l2l.data.TaskDataset(valid_dataset,\n                                       task_transforms=valid_transforms,\n                                       num_tasks=200)\n    valid_loader = DataLoader(valid_tasks, pin_memory=True, shuffle=True)\n\n    test_dataset = l2l.data.MetaDataset(test_dataset)\n    test_transforms = [\n        NWays(test_dataset, args.test_way),\n        KShots(test_dataset, args.test_query + args.test_shot),\n        LoadData(test_dataset),\n        RemapLabels(test_dataset),\n    ]\n    test_tasks = l2l.data.TaskDataset(test_dataset,\n                                      task_transforms=test_transforms,\n                                      num_tasks=2000)\n    test_loader = DataLoader(test_tasks, pin_memory=True, shuffle=True)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    lr_scheduler = torch.optim.lr_scheduler.StepLR(\n        optimizer, step_size=20, gamma=0.5)\n\n    for epoch in range(1, args.max_epoch + 1):\n        model.train()\n\n        loss_ctr = 0\n        n_loss = 0\n        n_acc = 0\n\n        for i in range(100):\n            batch = next(iter(train_loader))\n\n            loss, acc = fast_adapt(model,\n                                   batch,\n                                   args.train_way,\n                                   args.shot,\n                                   args.train_query,\n                                   metric=pairwise_distances_logits,\n                                   device=device)\n\n            loss_ctr += 1\n            n_loss += loss.item()\n            n_acc += acc\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        lr_scheduler.step()\n\n        print(\'epoch {}, train, loss={:.4f} acc={:.4f}\'.format(\n            epoch, n_loss/loss_ctr, n_acc/loss_ctr))\n\n        model.eval()\n\n        loss_ctr = 0\n        n_loss = 0\n        n_acc = 0\n        for i, batch in enumerate(valid_loader):\n            loss, acc = fast_adapt(model,\n                                   batch,\n                                   args.test_way,\n                                   args.test_shot,\n                                   args.test_query,\n                                   metric=pairwise_distances_logits,\n                                   device=device)\n\n            loss_ctr += 1\n            n_loss += loss.item()\n            n_acc += acc\n\n        print(\'epoch {}, val, loss={:.4f} acc={:.4f}\'.format(\n            epoch, n_loss/loss_ctr, n_acc/loss_ctr))\n\n    loss_ctr = 0\n    n_acc = 0\n\n    for i, batch in enumerate(test_loader, 1):\n        loss, acc = fast_adapt(model,\n                               batch,\n                               args.test_way,\n                               args.test_shot,\n                               args.test_query,\n                               metric=pairwise_distances_logits,\n                               device=device)\n        loss_ctr += 1\n        n_acc += acc\n        print(\'batch {}: {:.2f}({:.2f})\'.format(\n            i, n_acc/loss_ctr * 100, acc * 100))\n        \n'"
examples/vision/reptile_miniimagenet.py,10,"b'#!/usr/bin/env python3\n\n""""""\nRe-implementation of Reptile with L2L.\n\nRunning as-is should replicate the mini-ImageNet 5-ways, 5-shots results.\n""""""\n\nimport random\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch import autograd\n\nimport learn2learn as l2l\nfrom learn2learn.data.transforms import NWays, KShots, LoadData, RemapLabels, ConsecutiveLabels\n\nfrom statistics import mean\nfrom copy import deepcopy\n\n\ndef accuracy(predictions, targets):\n    predictions = predictions.argmax(dim=1).view(targets.shape)\n    return (predictions == targets).sum().float() / targets.size(0)\n\n\ndef fast_adapt(batch,\n               learner,\n               fast_lr,\n               loss,\n               adapt_steps,\n               batch_size,\n               shots,\n               ways,\n               opt,\n               device):\n    """"""\n    Only use the adaptation data to update parameters. (evaluation is only indicative.)\n    """"""\n    data, labels = batch\n    data, labels = data.to(device), labels.to(device)\n\n    # Separate data into adaptation/evalutation sets\n    adaptation_indices = np.zeros(data.size(0), dtype=bool)\n    adaptation_indices[np.arange(shots*ways) * 2] = True\n    evaluation_indices = torch.from_numpy(~adaptation_indices)\n    adaptation_indices = torch.from_numpy(adaptation_indices)\n    adaptation_data, adaptation_labels = data[adaptation_indices], labels[adaptation_indices]\n    evaluation_data, evaluation_labels = data[evaluation_indices], labels[evaluation_indices]\n\n    # Adaptation steps\n    adaptation_data = [d for d in adaptation_data]\n    for step in range(adapt_steps):\n        data = random.sample(adaptation_data, batch_size)\n        adapt_X = torch.cat([d[0].unsqueeze(0) for d in data], dim=0).to(device)\n        adapt_y = torch.cat([torch.tensor(d[1]).view(-1) for d in data], dim=0).to(device)\n        opt.zero_grad()\n        error = loss(learner(adapt_X), adapt_y)\n        error.backward()\n        opt.step()\n\n    predictions = learner(evaluation_data)\n    valid_error = loss(predictions, evaluation_labels)\n    valid_accuracy = accuracy(predictions, evaluation_labels)\n    return valid_error, valid_accuracy\n\n\ndef main(\n        ways=5,\n        train_shots=15,\n        test_shots=5,\n        meta_lr=1.0,\n        meta_mom=0.0,\n        meta_bsz=5,\n        fast_lr=0.001,\n        train_bsz=10,\n        test_bsz=15,\n        train_adapt_steps=8,\n        test_adapt_steps=50,\n        num_iterations=100000,\n        test_interval=100,\n        adam=0,  # Use adam or sgd for fast-adapt\n        meta_decay=1,  # Linearly decay the meta-lr or not\n        cuda=1,\n        seed=42,\n):\n\n    cuda = bool(cuda)\n    use_adam = bool(adam)\n    meta_decay = bool(meta_decay)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    device = torch.device(\'cpu\')\n    if cuda and torch.cuda.device_count():\n        torch.cuda.manual_seed(seed)\n        device = torch.device(\'cuda\')\n\n    # Create Datasets\n    train_dataset = l2l.vision.datasets.MiniImagenet(root=\'~/data\', mode=\'train\')\n    valid_dataset = l2l.vision.datasets.MiniImagenet(root=\'~/data\', mode=\'validation\')\n    test_dataset = l2l.vision.datasets.MiniImagenet(root=\'~/data\', mode=\'test\')\n    train_dataset = l2l.data.MetaDataset(train_dataset)\n    valid_dataset = l2l.data.MetaDataset(valid_dataset)\n    test_dataset = l2l.data.MetaDataset(test_dataset)\n\n    train_transforms = [\n        NWays(train_dataset, ways),\n        KShots(train_dataset, 2*train_shots),\n        LoadData(train_dataset),\n        RemapLabels(train_dataset),\n        ConsecutiveLabels(train_dataset),\n    ]\n    train_tasks = l2l.data.TaskDataset(train_dataset,\n                                       task_transforms=train_transforms,\n                                       num_tasks=20000)\n\n    valid_transforms = [\n        NWays(valid_dataset, ways),\n        KShots(valid_dataset, 2*test_shots),\n        LoadData(valid_dataset),\n        ConsecutiveLabels(valid_dataset),\n        RemapLabels(valid_dataset),\n    ]\n    valid_tasks = l2l.data.TaskDataset(valid_dataset,\n                                       task_transforms=valid_transforms,\n                                       num_tasks=600)\n\n    test_transforms = [\n        NWays(test_dataset, ways),\n        KShots(test_dataset, 2*test_shots),\n        LoadData(test_dataset),\n        RemapLabels(test_dataset),\n        ConsecutiveLabels(test_dataset),\n    ]\n    test_tasks = l2l.data.TaskDataset(test_dataset,\n                                      task_transforms=test_transforms,\n                                      num_tasks=600)\n\n\n    # Create model\n    model = l2l.vision.models.MiniImagenetCNN(ways)\n    model.to(device)\n    if use_adam:\n        opt = optim.Adam(model.parameters(), meta_lr, betas=(meta_mom, 0.999))\n    else:\n        opt = optim.SGD(model.parameters(), lr=meta_lr, momentum=meta_mom)\n    adapt_opt = optim.Adam(model.parameters(), lr=fast_lr, betas=(0, 0.999))\n    adapt_opt_state = adapt_opt.state_dict()\n    loss = nn.CrossEntropyLoss(reduction=\'mean\')\n\n    for iteration in range(num_iterations):\n        # anneal meta-lr\n        if meta_decay:\n            frac_done = float(iteration) / num_iterations\n            new_lr = frac_done * meta_lr + (1 - frac_done) * meta_lr\n            for pg in opt.param_groups:\n                pg[\'lr\'] = new_lr\n\n        # zero-grad the parameters\n        for p in model.parameters():\n            p.grad = torch.zeros_like(p.data)\n\n        meta_train_error = 0.0\n        meta_train_accuracy = 0.0\n        meta_valid_error = 0.0\n        meta_valid_accuracy = 0.0\n        meta_test_error = 0.0\n        meta_test_accuracy = 0.0\n        for task in range(meta_bsz):\n            # Compute meta-training loss\n            learner = deepcopy(model)\n            adapt_opt = optim.Adam(learner.parameters(),\n                                   lr=fast_lr,\n                                   betas=(0, 0.999))\n            adapt_opt.load_state_dict(adapt_opt_state)\n            batch = train_tasks.sample()\n            evaluation_error, evaluation_accuracy = fast_adapt(batch,\n                                                               learner,\n                                                               fast_lr,\n                                                               loss,\n                                                               adapt_steps=train_adapt_steps,\n                                                               batch_size=train_bsz,\n                                                               opt=adapt_opt,\n                                                               shots=train_shots,\n                                                               ways=ways,\n                                                               device=device)\n            adapt_opt_state = adapt_opt.state_dict()\n            for p, l in zip(model.parameters(), learner.parameters()):\n                p.grad.data.add_(-1.0, l.data)\n\n            meta_train_error += evaluation_error.item()\n            meta_train_accuracy += evaluation_accuracy.item()\n\n\n            if iteration % test_interval == 0:\n                # Compute meta-validation loss\n                learner = deepcopy(model)\n                adapt_opt = optim.Adam(learner.parameters(),\n                                       lr=fast_lr,\n                                       betas=(0, 0.999))\n                adapt_opt.load_state_dict(adapt_opt_state)\n                batch = valid_tasks.sample()\n                evaluation_error, evaluation_accuracy = fast_adapt(batch,\n                                                                   learner,\n                                                                   fast_lr,\n                                                                   loss,\n                                                                   adapt_steps=test_adapt_steps,\n                                                                   batch_size=test_bsz,\n                                                                   opt=adapt_opt,\n                                                                   shots=test_shots,\n                                                                   ways=ways,\n                                                                   device=device)\n                meta_valid_error += evaluation_error.item()\n                meta_valid_accuracy += evaluation_accuracy.item()\n\n                # Compute meta-testing loss\n                learner = deepcopy(model)\n                adapt_opt = optim.Adam(learner.parameters(),\n                                       lr=fast_lr,\n                                       betas=(0, 0.999))\n                adapt_opt.load_state_dict(adapt_opt_state)\n                batch = test_tasks.sample()\n                evaluation_error, evaluation_accuracy = fast_adapt(batch,\n                                                                   learner,\n                                                                   fast_lr,\n                                                                   loss,\n                                                                   adapt_steps=test_adapt_steps,\n                                                                   batch_size=test_bsz,\n                                                                   opt=adapt_opt,\n                                                                   shots=test_shots,\n                                                                   ways=ways,\n                                                                   device=device)\n                meta_test_error += evaluation_error.item()\n                meta_test_accuracy += evaluation_accuracy.item()\n\n        # Print some metrics\n        print(\'\\n\')\n        print(\'Iteration\', iteration)\n        print(\'Meta Train Error\', meta_train_error / meta_bsz)\n        print(\'Meta Train Accuracy\', meta_train_accuracy / meta_bsz)\n        if iteration % test_interval == 0:\n            print(\'Meta Valid Error\', meta_valid_error / meta_bsz)\n            print(\'Meta Valid Accuracy\', meta_valid_accuracy / meta_bsz)\n            print(\'Meta Test Error\', meta_test_error / meta_bsz)\n            print(\'Meta Test Accuracy\', meta_test_accuracy / meta_bsz)\n\n        # Average the accumulated gradients and optimize\n        for p in model.parameters():\n            p.grad.data.mul_(1.0 / meta_bsz).add_(p.data)\n        opt.step()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
learn2learn/algorithms/__init__.py,0,"b'#!/usr/bin/env python3\n\nfrom .maml import MAML, maml_update\nfrom .meta_sgd import MetaSGD, meta_sgd_update\n'"
learn2learn/algorithms/base_learner.py,0,"b""#!/usr/bin/env python3\n\nfrom torch import nn\n\n\nclass BaseLearner(nn.Module):\n\n    def __init__(self, module=None):\n        super(BaseLearner, self).__init__()\n        self.module = module\n\n    def __getattr__(self, attr):\n        try:\n            return super(BaseLearner, self).__getattr__(attr)\n        except AttributeError:\n            return getattr(self.__dict__['_modules']['module'], attr)\n\n    def forward(self, *args, **kwargs):\n        return self.module(*args, **kwargs)\n"""
learn2learn/algorithms/maml.py,2,"b'#!/usr/bin/env python3\n\nimport traceback\nfrom torch.autograd import grad\n\nfrom learn2learn.algorithms.base_learner import BaseLearner\nfrom learn2learn.utils import clone_module\n\n\ndef maml_update(model, lr, grads=None):\n    """"""\n    [[Source]](https://github.com/learnables/learn2learn/blob/master/learn2learn/algorithms/maml.py)\n\n    **Description**\n\n    Performs a MAML update on model using grads and lr.\n    The function re-routes the Python object, thus avoiding in-place\n    operations.\n\n    NOTE: The model itself is updated in-place (no deepcopy), but the\n          parameters\' tensors are not.\n\n    **Arguments**\n\n    * **model** (Module) - The model to update.\n    * **lr** (float) - The learning rate used to update the model.\n    * **grads** (list, *optional*, default=None) - A list of gradients for each parameter\n        of the model. If None, will use the gradients in .grad attributes.\n\n    **Example**\n    ~~~python\n    maml = l2l.algorithms.MAML(Model(), lr=0.1)\n    model = maml.clone() # The next two lines essentially implement model.adapt(loss)\n    grads = autograd.grad(loss, model.parameters(), create_graph=True)\n    maml_update(model, lr=0.1, grads)\n    ~~~\n    """"""\n    if grads is not None:\n        params = list(model.parameters())\n        if not len(grads) == len(list(params)):\n            msg = \'WARNING:maml_update(): Parameters and gradients have different length. (\'\n            msg += str(len(params)) + \' vs \' + str(len(grads)) + \')\'\n            print(msg)\n        for p, g in zip(params, grads):\n            p.grad = g\n\n    # Update the params\n    for param_key in model._parameters:\n        p = model._parameters[param_key]\n        if p is not None and p.grad is not None:\n            model._parameters[param_key] = p - lr * p.grad\n\n    # Second, handle the buffers if necessary\n    for buffer_key in model._buffers:\n        buff = model._buffers[buffer_key]\n        if buff is not None and buff.grad is not None:\n            model._buffers[buffer_key] = buff - lr * buff.grad\n\n    # Then, recurse for each submodule\n    for module_key in model._modules:\n        model._modules[module_key] = maml_update(model._modules[module_key],\n                                                 lr=lr,\n                                                 grads=None)\n\n    # Finally, rebuild the flattened parameters for RNNs\n    # See this issue for more details:\n    # https://github.com/learnables/learn2learn/issues/139\n    model._apply(lambda x: x)\n    return model\n\n\nclass MAML(BaseLearner):\n    """"""\n\n    [[Source]](https://github.com/learnables/learn2learn/blob/master/learn2learn/algorithms/maml.py)\n\n    **Description**\n\n    High-level implementation of *Model-Agnostic Meta-Learning*.\n\n    This class wraps an arbitrary nn.Module and augments it with `clone()` and `adapt()`\n    methods.\n\n    For the first-order version of MAML (i.e. FOMAML), set the `first_order` flag to `True`\n    upon initialization.\n\n    **Arguments**\n\n    * **model** (Module) - Module to be wrapped.\n    * **lr** (float) - Fast adaptation learning rate.\n    * **first_order** (bool, *optional*, default=False) - Whether to use the first-order\n        approximation of MAML. (FOMAML)\n    * **allow_unused** (bool, *optional*, default=None) - Whether to allow differentiation\n        of unused parameters. Defaults to `allow_nograd`.\n    * **allow_nograd** (bool, *optional*, default=False) - Whether to allow adaptation with\n        parameters that have `requires_grad = False`.\n\n    **References**\n\n    1. Finn et al. 2017. ""Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.""\n\n    **Example**\n\n    ~~~python\n    linear = l2l.algorithms.MAML(nn.Linear(20, 10), lr=0.01)\n    clone = linear.clone()\n    error = loss(clone(X), y)\n    clone.adapt(error)\n    error = loss(clone(X), y)\n    error.backward()\n    ~~~\n    """"""\n\n    def __init__(self,\n                 model,\n                 lr,\n                 first_order=False,\n                 allow_unused=None,\n                 allow_nograd=False):\n        super(MAML, self).__init__()\n        self.module = model\n        self.lr = lr\n        self.first_order = first_order\n        self.allow_nograd = allow_nograd\n        if allow_unused is None:\n            allow_unused = allow_nograd\n        self.allow_unused = allow_unused\n\n    def forward(self, *args, **kwargs):\n        return self.module(*args, **kwargs)\n\n    def adapt(self,\n              loss,\n              first_order=None,\n              allow_unused=None,\n              allow_nograd=None):\n        """"""\n        **Description**\n\n        Takes a gradient step on the loss and updates the cloned parameters in place.\n\n        **Arguments**\n\n        * **loss** (Tensor) - Loss to minimize upon update.\n        * **first_order** (bool, *optional*, default=None) - Whether to use first- or\n            second-order updates. Defaults to self.first_order.\n        * **allow_unused** (bool, *optional*, default=None) - Whether to allow differentiation\n            of unused parameters. Defaults to self.allow_unused.\n        * **allow_nograd** (bool, *optional*, default=None) - Whether to allow adaptation with\n            parameters that have `requires_grad = False`. Defaults to self.allow_nograd.\n\n        """"""\n        if first_order is None:\n            first_order = self.first_order\n        if allow_unused is None:\n            allow_unused = self.allow_unused\n        if allow_nograd is None:\n            allow_nograd = self.allow_nograd\n        second_order = not first_order\n\n        if allow_nograd:\n            # Compute relevant gradients\n            diff_params = [p for p in self.module.parameters() if p.requires_grad]\n            grad_params = grad(loss,\n                               diff_params,\n                               retain_graph=second_order,\n                               create_graph=second_order,\n                               allow_unused=allow_unused)\n            gradients = []\n            grad_counter = 0\n\n            # Handles gradients for non-differentiable parameters\n            for param in self.module.parameters():\n                if param.requires_grad:\n                    gradient = grad_params[grad_counter]\n                    grad_counter += 1\n                else:\n                    gradient = None\n                gradients.append(gradient)\n        else:\n            try:\n                gradients = grad(loss,\n                                 self.module.parameters(),\n                                 retain_graph=second_order,\n                                 create_graph=second_order,\n                                 allow_unused=allow_unused)\n            except RuntimeError:\n                traceback.print_exc()\n                print(\'learn2learn: Maybe try with allow_nograd=True and/or allow_unused=True ?\')\n\n        # Update the module\n        self.module = maml_update(self.module, self.lr, gradients)\n\n    def clone(self, first_order=None, allow_unused=None, allow_nograd=None):\n        """"""\n        **Description**\n\n        Returns a `MAML`-wrapped copy of the module whose parameters and buffers\n        are `torch.clone`d from the original module.\n\n        This implies that back-propagating losses on the cloned module will\n        populate the buffers of the original module.\n        For more information, refer to learn2learn.clone_module().\n\n        **Arguments**\n\n        * **first_order** (bool, *optional*, default=None) - Whether the clone uses first-\n            or second-order updates. Defaults to self.first_order.\n        * **allow_unused** (bool, *optional*, default=None) - Whether to allow differentiation\n        of unused parameters. Defaults to self.allow_unused.\n        * **allow_nograd** (bool, *optional*, default=False) - Whether to allow adaptation with\n            parameters that have `requires_grad = False`. Defaults to self.allow_nograd.\n\n        """"""\n        if first_order is None:\n            first_order = self.first_order\n        if allow_unused is None:\n            allow_unused = self.allow_unused\n        if allow_nograd is None:\n            allow_nograd = self.allow_nograd\n        return MAML(clone_module(self.module),\n                    lr=self.lr,\n                    first_order=first_order,\n                    allow_unused=allow_unused,\n                    allow_nograd=allow_nograd)\n'"
learn2learn/algorithms/meta_sgd.py,1,"b'#!/usr/bin/env python3\n\nimport torch as th\nfrom torch import nn\nfrom torch.autograd import grad\n\nfrom learn2learn.algorithms.base_learner import BaseLearner\nfrom learn2learn.utils import clone_module, clone_parameters\n\n\ndef meta_sgd_update(model, lrs=None, grads=None):\n    """"""\n\n    **Description**\n\n    Performs a MetaSGD update on model using grads and lrs.\n    The function re-routes the Python object, thus avoiding in-place\n    operations.\n\n    NOTE: The model itself is updated in-place (no deepcopy), but the\n          parameters\' tensors are not.\n\n    **Arguments**\n\n    * **model** (Module) - The model to update.\n    * **lrs** (list) - The meta-learned learning rates used to update the model.\n    * **grads** (list, *optional*, default=None) - A list of gradients for each parameter\n        of the model. If None, will use the gradients in .grad attributes.\n\n    **Example**\n    ~~~python\n    meta = l2l.algorithms.MetaSGD(Model(), lr=1.0)\n    lrs = [th.ones_like(p) for p in meta.model.parameters()]\n    model = meta.clone() # The next two lines essentially implement model.adapt(loss)\n    grads = autograd.grad(loss, model.parameters(), create_graph=True)\n    meta_sgd_update(model, lrs=lrs, grads)\n    ~~~\n    """"""\n    if grads is not None and lrs is not None:\n        for p, lr, g in zip(model.parameters(), lrs, grads):\n            p.grad = g\n            p._lr = lr\n\n    # Update the params\n    for param_key in model._parameters:\n        p = model._parameters[param_key]\n        if p is not None and p.grad is not None:\n            model._parameters[param_key] = p - p._lr * p.grad\n\n    # Second, handle the buffers if necessary\n    for buffer_key in model._buffers:\n        buff = model._buffers[buffer_key]\n        if buff is not None and buff.grad is not None and buff._lr is not None:\n            model._buffers[buffer_key] = buff - buff._lr * buff.grad\n\n    # Then, recurse for each submodule\n    for module_key in model._modules:\n        model._modules[module_key] = meta_sgd_update(model._modules[module_key])\n    return model\n\n\nclass MetaSGD(BaseLearner):\n    """"""\n\n    [[Source]](https://github.com/learnables/learn2learn/blob/master/learn2learn/algorithms/meta_sgd.py)\n\n    **Description**\n\n    High-level implementation of *Meta-SGD*.\n\n    This class wraps an arbitrary nn.Module and augments it with `clone()` and `adapt`\n    methods.\n    It behaves similarly to `MAML`, but in addition a set of per-parameters learning rates\n    are learned for fast-adaptation.\n\n    **Arguments**\n\n    * **model** (Module) - Module to be wrapped.\n    * **lr** (float) - Initialization value of the per-parameter fast adaptation learning rates.\n    * **first_order** (bool, *optional*, default=False) - Whether to use the first-order version.\n    * **lrs** (list of Parameters, *optional*, default=None) - If not None, overrides `lr`, and uses the list\n        as learning rates for fast-adaptation.\n\n    **References**\n\n    1. Li et al. 2017. \xe2\x80\x9cMeta-SGD: Learning to Learn Quickly for Few-Shot Learning.\xe2\x80\x9d arXiv.\n\n    **Example**\n\n    ~~~python\n    linear = l2l.algorithms.MetaSGD(nn.Linear(20, 10), lr=0.01)\n    clone = linear.clone()\n    error = loss(clone(X), y)\n    clone.adapt(error)\n    error = loss(clone(X), y)\n    error.backward()\n    ~~~\n    """"""\n\n    def __init__(self, model, lr=1.0, first_order=False, lrs=None):\n        super(MetaSGD, self).__init__()\n        self.module = model\n        if lrs is None:\n            lrs = [th.ones_like(p) * lr for p in model.parameters()]\n            lrs = nn.ParameterList([nn.Parameter(lr) for lr in lrs])\n        self.lrs = lrs\n        self.first_order = first_order\n\n    def forward(self, *args, **kwargs):\n        return self.module(*args, **kwargs)\n\n    def clone(self):\n        """"""\n        **Descritpion**\n\n        Akin to `MAML.clone()` but for MetaSGD: it includes a set of learnable fast-adaptation\n        learning rates.\n        """"""\n        return MetaSGD(clone_module(self.module),\n                       lrs=clone_parameters(self.lrs),\n                       first_order=self.first_order)\n\n    def adapt(self, loss, first_order=None):\n        """"""\n        **Descritpion**\n\n        Akin to `MAML.adapt()` but for MetaSGD: it updates the model with the learnable\n        per-parameter learning rates.\n        """"""\n        if first_order is None:\n            first_order = self.first_order\n        second_order = not first_order\n        gradients = grad(loss,\n                         self.module.parameters(),\n                         retain_graph=second_order,\n                         create_graph=second_order)\n        self.module = meta_sgd_update(self.module, self.lrs, gradients)\n\n\nif __name__ == \'__main__\':\n    linear = nn.Sequential(nn.Linear(10, 2), nn.Linear(5, 5))\n    msgd = MetaSGD(linear, lr=0.001)\n    learner = msgd.new()\n'"
learn2learn/data/__init__.py,0,"b'#!/usr/bin/env python3\n\nfrom . import transforms\nfrom .meta_dataset import MetaDataset\nfrom .task_dataset import TaskDataset, DataDescription\n'"
learn2learn/data/utils.py,0,"b'#!/usr/bin/env python3\n\nimport requests\n\n\ndef download_file(source, destination):\n    req = requests.get(source)\n    with open(destination, \'wb\') as archive:\n        for chunk in req.iter_content(chunk_size=32768):\n            if chunk:\n                archive.write(chunk)\n\n\ndef download_file_from_google_drive(id, destination):\n    URL = ""https://docs.google.com/uc?export=download""\n    session = requests.Session()\n    response = session.get(URL, params={\'id\': id}, stream=True)\n    token = get_confirm_token(response)\n    if token:\n        params = {\'id\': id, \'confirm\': token}\n        response = session.get(URL, params=params, stream=True)\n    save_response_content(response, destination)\n\n\ndef get_confirm_token(response):\n    for key, value in response.cookies.items():\n        if key.startswith(\'download_warning\'):\n            return value\n    return None\n\n\ndef save_response_content(response, destination):\n    CHUNK_SIZE = 32768\n    with open(destination, ""wb"") as f:\n        for chunk in response.iter_content(CHUNK_SIZE):\n            if chunk:  # filter out keep-alive new chunks\n                f.write(chunk)\n'"
learn2learn/gym/__init__.py,0,b'#!/usr/bin/env python3\n\nfrom . import envs\nfrom .envs.meta_env import MetaEnv\nfrom .async_vec_env import AsyncVectorEnv\n'
learn2learn/gym/async_vec_env.py,0,"b'#!/usr/bin/env python3\n\nimport multiprocessing as mp\n\nfrom .envs import SubprocVecEnv\n\n\nclass AsyncVectorEnv(SubprocVecEnv):\n    """"""\n    [[Source]](https://github.com/learnables/learn2learn/blob/master/learn2learn/gym/async_vec_env.py)\n\n    **Description**\n\n    Asynchronous vectorized environment for working with l2l MetaEnvs.\n    Allows multiple environments to be run as separate processes.\n\n    **Credit**\n\n    Adapted from OpenAI and Tristan Deleu\'s implementations.\n\n    """"""\n    def __init__(self, env_fns, env=None):\n        self.num_envs = len(env_fns)\n        self.queue = mp.Queue()\n        super(AsyncVectorEnv, self).__init__(env_fns, queue=self.queue)\n        if env is None:\n            env = env_fns[0]()\n        self._env = env\n        self.reset()\n\n    def set_task(self, task):\n        tasks = [task for _ in range(self.num_envs)]\n        reset = super(AsyncVectorEnv, self).set_task(tasks)\n        return all(reset)\n\n    def sample_tasks(self, num_tasks):\n        tasks = self._env.unwrapped.sample_tasks(num_tasks)\n        return tasks\n\n    def step(self, actions):\n        obs, rews, dones, ids, infos = super(AsyncVectorEnv, self).step(actions)\n        return obs, rews, dones, infos\n\n    def reset(self):\n        for i in range(self.num_envs):\n            self.queue.put(i)\n        for i in range(self.num_envs):\n            self.queue.put(None)\n        obs, ids = super(AsyncVectorEnv, self).reset()\n        return obs\n\n    def render(self, *args, **kwargs):\n        self._env.render(*args, **kwargs)\n'"
learn2learn/text/__init__.py,0,b'#!/usr/bin/env python3\n\nfrom . import datasets\n'
learn2learn/vision/__init__.py,0,b'#!/usr/bin/env python3\n\nfrom . import datasets\nfrom . import models\nfrom . import transforms\nfrom . import benchmarks\n'
learn2learn/vision/models.py,2,"b'#!/usr/bin/env python3\n\n""""""\n**Description**\n\nA set of commonly used models for meta-learning vision tasks.\n""""""\n\nimport torch\nfrom scipy.stats import truncnorm\nfrom torch import nn\n\n\ndef truncated_normal_(tensor, mean=0.0, std=1.0):\n    # PT doesn\'t have truncated normal.\n    # https://discuss.pytorch.org/t/implementing-truncated-normal-initializer/4778/18\n    values = truncnorm.rvs(-2, 2, size=tensor.shape)\n    values = mean + std * values\n    tensor.copy_(torch.from_numpy(values))\n    return tensor\n\n\ndef fc_init_(module):\n    if hasattr(module, \'weight\') and module.weight is not None:\n        truncated_normal_(module.weight.data, mean=0.0, std=0.01)\n    if hasattr(module, \'bias\') and module.bias is not None:\n        nn.init.constant_(module.bias.data, 0.0)\n    return module\n\n\ndef maml_init_(module):\n    nn.init.xavier_uniform_(module.weight.data, gain=1.0)\n    nn.init.constant_(module.bias.data, 0.0)\n    return module\n\n\nclass LinearBlock(nn.Module):\n\n    def __init__(self, input_size, output_size):\n        super(LinearBlock, self).__init__()\n        self.relu = nn.ReLU()\n        self.normalize = nn.BatchNorm1d(output_size,\n                                        affine=True,\n                                        momentum=0.999,\n                                        eps=1e-3,\n                                        track_running_stats=False,\n                                        )\n        self.linear = nn.Linear(input_size, output_size)\n        fc_init_(self.linear)\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = self.normalize(x)\n        x = self.relu(x)\n        return x\n\n\nclass ConvBlock(nn.Module):\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 max_pool=True,\n                 max_pool_factor=1.0):\n        super(ConvBlock, self).__init__()\n        stride = (int(2 * max_pool_factor), int(2 * max_pool_factor))\n        if max_pool:\n            self.max_pool = nn.MaxPool2d(kernel_size=stride,\n                                         stride=stride,\n                                         ceil_mode=False,\n                                         )\n            stride = (1, 1)\n        else:\n            self.max_pool = lambda x: x\n        self.normalize = nn.BatchNorm2d(out_channels,\n                                        affine=True,\n                                        # eps=1e-3,\n                                        # momentum=0.999,\n                                        # track_running_stats=False,\n                                        )\n        nn.init.uniform_(self.normalize.weight)\n        self.relu = nn.ReLU()\n\n        self.conv = nn.Conv2d(in_channels,\n                              out_channels,\n                              kernel_size,\n                              stride=stride,\n                              padding=1,\n                              bias=True)\n        maml_init_(self.conv)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.normalize(x)\n        x = self.relu(x)\n        x = self.max_pool(x)\n        return x\n\n\nclass ConvBase(nn.Sequential):\n\n    # NOTE:\n    #     Omniglot: hidden=64, channels=1, no max_pool\n    #     MiniImagenet: hidden=32, channels=3, max_pool\n\n    def __init__(self,\n                 output_size,\n                 hidden=64,\n                 channels=1,\n                 max_pool=False,\n                 layers=4,\n                 max_pool_factor=1.0):\n        core = [ConvBlock(channels,\n                          hidden,\n                          (3, 3),\n                          max_pool=max_pool,\n                          max_pool_factor=max_pool_factor),\n                ]\n        for _ in range(layers - 1):\n            core.append(ConvBlock(hidden,\n                                  hidden,\n                                  kernel_size=(3, 3),\n                                  max_pool=max_pool,\n                                  max_pool_factor=max_pool_factor))\n        super(ConvBase, self).__init__(*core)\n\n\nclass OmniglotFC(nn.Sequential):\n    """"""\n\n    [[Source]]()\n\n    **Description**\n\n    The fully-connected network used for Omniglot experiments, as described in Santoro et al, 2016.\n\n    **References**\n\n    1. Santoro et al. 2016. \xe2\x80\x9cMeta-Learning with Memory-Augmented Neural Networks.\xe2\x80\x9d ICML.\n\n    **Arguments**\n\n    * **input_size** (int) - The dimensionality of the input.\n    * **output_size** (int) - The dimensionality of the output.\n    * **sizes** (list, *optional*, default=None) - A list of hidden layer sizes.\n\n    **Example**\n    ~~~python\n    net = OmniglotFC(input_size=28**2,\n                     output_size=10,\n                     sizes=[64, 64, 64])\n    ~~~\n\n    """"""\n\n    def __init__(self, input_size, output_size, sizes=None):\n        if sizes is None:\n            sizes = [256, 128, 64, 64]\n        layers = [LinearBlock(input_size, sizes[0]), ]\n        for s_i, s_o in zip(sizes[:-1], sizes[1:]):\n            layers.append(LinearBlock(s_i, s_o))\n        layers.append(fc_init_(nn.Linear(sizes[-1], output_size)))\n        super(OmniglotFC, self).__init__(*layers)\n        self.input_size = input_size\n\n    def forward(self, x):\n        return super(OmniglotFC, self).forward(x.view(-1, self.input_size))\n\n\nclass OmniglotCNN(nn.Module):\n    """"""\n\n    [Source]()\n\n    **Description**\n\n    The convolutional network commonly used for Omniglot, as described by Finn et al, 2017.\n\n    This network assumes inputs of shapes (1, 28, 28).\n\n    **References**\n\n    1. Finn et al. 2017. \xe2\x80\x9cModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.\xe2\x80\x9d ICML.\n\n    **Arguments**\n\n    * **output_size** (int) - The dimensionality of the network\'s output.\n    * **hidden_size** (int, *optional*, default=64) - The dimensionality of the hidden representation.\n    * **layers** (int, *optional*, default=4) - The number of convolutional layers.\n\n    **Example**\n    ~~~python\n    model = OmniglotCNN(output_size=20, hidden_size=128, layers=3)\n    ~~~\n\n    """"""\n\n    def __init__(self, output_size=5, hidden_size=64, layers=4):\n        super(OmniglotCNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.base = ConvBase(output_size=hidden_size,\n                             hidden=hidden_size,\n                             channels=1,\n                             max_pool=False,\n                             layers=layers)\n        self.linear = nn.Linear(hidden_size, output_size, bias=True)\n        self.linear.weight.data.normal_()\n        self.linear.bias.data.mul_(0.0)\n\n    def forward(self, x):\n        x = self.base(x.view(-1, 1, 28, 28))\n        x = x.mean(dim=[2, 3])\n        x = self.linear(x)\n        return x\n\n\nclass MiniImagenetCNN(nn.Module):\n    """"""\n\n    [[Source]]()\n\n    **Description**\n\n    The convolutional network commonly used for MiniImagenet, as described by Ravi et Larochelle, 2017.\n\n    This network assumes inputs of shapes (3, 84, 84).\n\n    **References**\n\n    1. Ravi and Larochelle. 2017. \xe2\x80\x9cOptimization as a Model for Few-Shot Learning.\xe2\x80\x9d ICLR.\n\n    **Arguments**\n\n    * **output_size** (int) - The dimensionality of the network\'s output.\n    * **hidden_size** (int, *optional*, default=32) - The dimensionality of the hidden representation.\n    * **layers** (int, *optional*, default=4) - The number of convolutional layers.\n\n    **Example**\n    ~~~python\n    model = MiniImagenetCNN(output_size=20, hidden_size=128, layers=3)\n    ~~~\n    """"""\n\n    def __init__(self, output_size, hidden_size=32, layers=4):\n        super(MiniImagenetCNN, self).__init__()\n        self.base = ConvBase(output_size=hidden_size,\n                             hidden=hidden_size,\n                             channels=3,\n                             max_pool=True,\n                             layers=layers,\n                             max_pool_factor=4 // layers)\n        self.linear = nn.Linear(25 * hidden_size, output_size, bias=True)\n        maml_init_(self.linear)\n        self.hidden_size = hidden_size\n\n    def forward(self, x):\n        x = self.base(x)\n        x = self.linear(x.view(-1, 25 * self.hidden_size))\n        return x\n'"
learn2learn/vision/transforms.py,0,"b'#!/usr/bin/env python3\n\n""""""\n\n**Description**\n\nA set of transformations commonly used in meta-learning vision tasks.\n""""""\n\nimport random\nfrom torchvision import transforms\n\n\nclass RandomClassRotation(object):\n    """"""\n\n    [[Source]]()\n\n    **Description**\n\n    Samples rotations from a given list uniformly at random, and applies it to\n    all images from a given class.\n\n    **Arguments**\n\n    * **degrees** (list) - The rotations to be sampled.\n\n    **Example**\n    ~~~python\n    transform = RandomClassRotation([0, 90, 180, 270])\n    ~~~\n\n    """"""\n\n    def __init__(self, dataset, degrees):\n        self.degrees = degrees\n        self.dataset = dataset\n\n    def __call__(self, task_description):\n        rotations = {}\n        for data_description in task_description:\n            c = self.dataset.indices_to_labels[data_description.index]\n            if c not in rotations:\n                rot = random.choice(self.degrees)\n                try:\n                    rotations[c] = transforms.Compose([\n                        transforms.ToPILImage(),\n                        transforms.RandomRotation((rot, rot), fill=(0, )),\n                        transforms.ToTensor(),\n                    ])\n                except Exception:\n                    rotations[c] = transforms.Compose([\n                        transforms.ToPILImage(),\n                        transforms.RandomRotation((rot, rot)),\n                        transforms.ToTensor(),\n                    ])\n            rotation = rotations[c]\n            data_description.transforms.append(lambda x: (rotation(x[0]), x[1]))\n        return task_description\n'"
tests/integration/__init__.py,0,b''
tests/integration/maml_miniimagenet_test_notravis.py,7,"b""#!/usr/bin/env python3\n\nimport unittest\nimport random\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch import optim\n\nimport learn2learn as l2l\n\n\ndef accuracy(predictions, targets):\n    predictions = predictions.argmax(dim=1).view(targets.shape)\n    return (predictions == targets).sum().float() / targets.size(0)\n\n\ndef fast_adapt(batch, learner, loss, adaptation_steps, shots, ways, device):\n    data, labels = batch\n    data, labels = data.to(device), labels.to(device)\n\n    # Separate data into adaptation/evalutation sets\n    adaptation_indices = np.zeros(data.size(0), dtype=bool)\n    adaptation_indices[np.arange(shots*ways) * 2] = True\n    evaluation_indices = torch.from_numpy(~adaptation_indices)\n    adaptation_indices = torch.from_numpy(adaptation_indices)\n    adaptation_data, adaptation_labels = data[adaptation_indices], labels[adaptation_indices]\n    evaluation_data, evaluation_labels = data[evaluation_indices], labels[evaluation_indices]\n\n    # Adapt the model\n    for step in range(adaptation_steps):\n        train_error = loss(learner(adaptation_data), adaptation_labels)\n        train_error /= len(adaptation_data)\n        learner.adapt(train_error)\n\n    # Evaluate the adapted model\n    predictions = learner(evaluation_data)\n    valid_error = loss(predictions, evaluation_labels)\n    valid_error /= len(evaluation_data)\n    valid_accuracy = accuracy(predictions, evaluation_labels)\n    return valid_error, valid_accuracy\n\n\ndef main(\n        ways=5,\n        shots=5,\n        meta_lr=0.003,\n        fast_lr=0.5,\n        meta_batch_size=32,\n        adaptation_steps=1,\n        num_iterations=60000,\n        cuda=True,\n        seed=42,\n):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    device = torch.device('cpu')\n    if cuda and torch.cuda.device_count():\n        torch.cuda.manual_seed(seed)\n        device = torch.device('cuda')\n\n    # Create Datasets\n    train_dataset = l2l.vision.datasets.MiniImagenet(root='./data', mode='train')\n    valid_dataset = l2l.vision.datasets.MiniImagenet(root='./data', mode='validation')\n    test_dataset = l2l.vision.datasets.MiniImagenet(root='./data', mode='test')\n    train_dataset = l2l.data.MetaDataset(train_dataset)\n    valid_dataset = l2l.data.MetaDataset(valid_dataset)\n    test_dataset = l2l.data.MetaDataset(test_dataset)\n\n    train_transforms = [\n        l2l.data.transforms.NWays(train_dataset, ways),\n        l2l.data.transforms.KShots(train_dataset, 2*shots),\n        l2l.data.transforms.LoadData(train_dataset),\n        l2l.data.transforms.RemapLabels(train_dataset),\n        l2l.data.transforms.ConsecutiveLabels(train_dataset),\n    ]\n    train_tasks = l2l.data.TaskDataset(train_dataset,\n                                       task_transforms=train_transforms,\n                                       num_tasks=20000)\n\n    valid_transforms = [\n        l2l.data.transforms.NWays(valid_dataset, ways),\n        l2l.data.transforms.KShots(valid_dataset, 2*shots),\n        l2l.data.transforms.LoadData(valid_dataset),\n        l2l.data.transforms.ConsecutiveLabels(train_dataset),\n        l2l.data.transforms.RemapLabels(valid_dataset),\n    ]\n    valid_tasks = l2l.data.TaskDataset(valid_dataset,\n                                       task_transforms=valid_transforms,\n                                       num_tasks=600)\n\n    test_transforms = [\n        l2l.data.transforms.NWays(test_dataset, ways),\n        l2l.data.transforms.KShots(test_dataset, 2*shots),\n        l2l.data.transforms.LoadData(test_dataset),\n        l2l.data.transforms.RemapLabels(test_dataset),\n        l2l.data.transforms.ConsecutiveLabels(train_dataset),\n    ]\n    test_tasks = l2l.data.TaskDataset(test_dataset,\n                                      task_transforms=test_transforms,\n                                      num_tasks=600)\n\n    # Create model\n    model = l2l.vision.models.MiniImagenetCNN(ways)\n    model.to(device)\n    maml = l2l.algorithms.MAML(model, lr=fast_lr, first_order=False)\n    opt = optim.Adam(maml.parameters(), meta_lr)\n    loss = nn.CrossEntropyLoss(size_average=True, reduction='mean')\n\n    for iteration in range(num_iterations):\n        opt.zero_grad()\n        meta_train_error = 0.0\n        meta_train_accuracy = 0.0\n        meta_valid_error = 0.0\n        meta_valid_accuracy = 0.0\n        meta_test_error = 0.0\n        meta_test_accuracy = 0.0\n        for task in range(meta_batch_size):\n            # Compute meta-training loss\n            learner = maml.clone()\n            batch = train_tasks.sample()\n            evaluation_error, evaluation_accuracy = fast_adapt(batch,\n                                                               learner,\n                                                               loss,\n                                                               adaptation_steps,\n                                                               shots,\n                                                               ways,\n                                                               device)\n            evaluation_error.backward()\n            meta_train_error += evaluation_error.item()\n            meta_train_accuracy += evaluation_accuracy.item()\n\n            # Compute meta-validation loss\n            learner = maml.clone()\n            batch = valid_tasks.sample()\n            evaluation_error, evaluation_accuracy = fast_adapt(batch,\n                                                               learner,\n                                                               loss,\n                                                               adaptation_steps,\n                                                               shots,\n                                                               ways,\n                                                               device)\n            meta_valid_error += evaluation_error.item()\n            meta_valid_accuracy += evaluation_accuracy.item()\n\n            # Compute meta-testing loss\n            learner = maml.clone()\n            batch = test_tasks.sample()\n            evaluation_error, evaluation_accuracy = fast_adapt(batch,\n                                                               learner,\n                                                               loss,\n                                                               adaptation_steps,\n                                                               shots,\n                                                               ways,\n                                                               device)\n            meta_test_error += evaluation_error.item()\n            meta_test_accuracy += evaluation_accuracy.item()\n\n        # Print some metrics\n        print('\\n')\n        print('Iteration', iteration)\n        print('Meta Train Error', meta_train_error / meta_batch_size)\n        print('Meta Train Accuracy', meta_train_accuracy / meta_batch_size)\n        print('Meta Valid Error', meta_valid_error / meta_batch_size)\n        print('Meta Valid Accuracy', meta_valid_accuracy / meta_batch_size)\n        print('Meta Test Error', meta_test_error / meta_batch_size)\n        print('Meta Test Accuracy', meta_test_accuracy / meta_batch_size)\n\n        # Average the accumulated gradients and optimize\n        for p in maml.parameters():\n            p.grad.data.mul_(1.0 / meta_batch_size)\n        opt.step()\n    return meta_train_accuracy, meta_valid_accuracy, meta_test_accuracy\n\nclass MAMLMiniImagenetIntegrationTests(unittest.TestCase):\n\n    def setUp(self):\n        pass\n\n    def tearDown(self):\n        pass\n\n    def test_final_accuracy(self):\n        train_acc, valid_acc, test_acc = main(num_iterations=1)\n        self.assertTrue(train_acc >= 0.2)\n        self.assertTrue(valid_acc >= 0.2)\n        self.assertTrue(test_acc >= 0.2)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/integration/maml_omniglot_test.py,6,"b""#!/usr/bin/env python3\n\nimport unittest\nimport random\nimport numpy as np\n\nimport torch\nfrom torch import nn, optim\nfrom torchvision import transforms\n\nimport learn2learn as l2l\n\nfrom PIL.Image import LANCZOS\n\n\ndef accuracy(predictions, targets):\n    predictions = predictions.argmax(dim=1).view(targets.shape)\n    return (predictions == targets).sum().float() / targets.size(0)\n\n\ndef fast_adapt(batch, learner, loss, adaptation_steps, shots, ways, device):\n    data, labels = batch\n    data, labels = data.to(device), labels.to(device)\n\n    # Separate data into adaptation/evalutation sets\n    adaptation_indices = np.zeros(data.size(0), dtype=bool)\n    adaptation_indices[np.arange(shots*ways) * 2] = True\n    evaluation_indices = torch.from_numpy(~adaptation_indices)\n    adaptation_indices = torch.from_numpy(adaptation_indices)\n    adaptation_data, adaptation_labels = data[adaptation_indices], labels[adaptation_indices]\n    evaluation_data, evaluation_labels = data[evaluation_indices], labels[evaluation_indices]\n\n    # Adapt the model\n    for step in range(adaptation_steps):\n        train_error = loss(learner(adaptation_data), adaptation_labels)\n        train_error /= len(adaptation_data)\n        learner.adapt(train_error)\n\n    # Evaluate the adapted model\n    predictions = learner(evaluation_data)\n    valid_error = loss(predictions, evaluation_labels)\n    valid_error /= len(evaluation_data)\n    valid_accuracy = accuracy(predictions, evaluation_labels)\n    return valid_error, valid_accuracy\n\n\ndef main(\n        ways=5,\n        shots=1,\n        meta_lr=0.003,\n        fast_lr=0.5,\n        meta_batch_size=32,\n        adaptation_steps=1,\n        num_iterations=60000,\n        cuda=False,\n        seed=42,\n):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    device = torch.device('cpu')\n    if cuda:\n        torch.cuda.manual_seed(seed)\n        device = torch.device('cuda')\n\n    omniglot = l2l.vision.datasets.FullOmniglot(root='./data',\n                                                transform=transforms.Compose([\n                                                    transforms.Resize(28, interpolation=LANCZOS),\n                                                    transforms.ToTensor(),\n                                                    lambda x: 1.0 - x,\n                                                ]),\n                                                download=True)\n\n    dataset = l2l.data.MetaDataset(omniglot)\n    classes = list(range(1623))\n    random.shuffle(classes)\n\n    train_transforms = [\n        l2l.data.transforms.FusedNWaysKShots(dataset,\n                                             n=ways,\n                                             k=2*shots,\n                                             filter_labels=classes[:1100]),\n        l2l.data.transforms.LoadData(dataset),\n        l2l.data.transforms.RemapLabels(dataset),\n        l2l.data.transforms.ConsecutiveLabels(dataset),\n        l2l.vision.transforms.RandomClassRotation(dataset, [0.0, 90.0, 180.0, 270.0])\n    ]\n    train_tasks = l2l.data.TaskDataset(dataset,\n                                       task_transforms=train_transforms,\n                                       num_tasks=20000)\n\n    valid_transforms = [\n        l2l.data.transforms.FusedNWaysKShots(dataset,\n                                             n=ways,\n                                             k=2*shots,\n                                             filter_labels=classes[1100:1200]),\n        l2l.data.transforms.LoadData(dataset),\n        l2l.data.transforms.RemapLabels(dataset),\n        l2l.data.transforms.ConsecutiveLabels(dataset),\n        l2l.vision.transforms.RandomClassRotation(dataset, [0.0, 90.0, 180.0, 270.0])\n    ]\n    valid_tasks = l2l.data.TaskDataset(dataset,\n                                       task_transforms=valid_transforms,\n                                       num_tasks=1024)\n\n    test_transforms = [\n        l2l.data.transforms.FusedNWaysKShots(dataset,\n                                             n=ways,\n                                             k=2*shots,\n                                             filter_labels=classes[1200:]),\n        l2l.data.transforms.LoadData(dataset),\n        l2l.data.transforms.RemapLabels(dataset),\n        l2l.data.transforms.ConsecutiveLabels(dataset),\n        l2l.vision.transforms.RandomClassRotation(dataset, [0.0, 90.0, 180.0, 270.0])\n    ]\n    test_tasks = l2l.data.TaskDataset(dataset,\n                                      task_transforms=test_transforms,\n                                      num_tasks=1024)\n\n    # Create model\n    model = l2l.vision.models.OmniglotFC(28 ** 2, ways)\n    model.to(device)\n    maml = l2l.algorithms.MAML(model, lr=fast_lr, first_order=False)\n    opt = optim.Adam(maml.parameters(), meta_lr)\n    loss = nn.CrossEntropyLoss(reduction='mean')\n\n    for iteration in range(num_iterations):\n        opt.zero_grad()\n        meta_train_error = 0.0\n        meta_train_accuracy = 0.0\n        meta_valid_error = 0.0\n        meta_valid_accuracy = 0.0\n        meta_test_error = 0.0\n        meta_test_accuracy = 0.0\n        for task in range(meta_batch_size):\n            # Compute meta-training loss\n            learner = maml.clone()\n            batch = train_tasks.sample()\n            evaluation_error, evaluation_accuracy = fast_adapt(batch,\n                                                               learner,\n                                                               loss,\n                                                               adaptation_steps,\n                                                               shots,\n                                                               ways,\n                                                               device)\n            evaluation_error.backward()\n            meta_train_error += evaluation_error.item()\n            meta_train_accuracy += evaluation_accuracy.item()\n\n            # Compute meta-validation loss\n            learner = maml.clone()\n            batch = valid_tasks.sample()\n            evaluation_error, evaluation_accuracy = fast_adapt(batch,\n                                                               learner,\n                                                               loss,\n                                                               adaptation_steps,\n                                                               shots,\n                                                               ways,\n                                                               device)\n            meta_valid_error += evaluation_error.item()\n            meta_valid_accuracy += evaluation_accuracy.item()\n\n            # Compute meta-testing loss\n            learner = maml.clone()\n            batch = test_tasks.sample()\n            evaluation_error, evaluation_accuracy = fast_adapt(batch,\n                                                               learner,\n                                                               loss,\n                                                               adaptation_steps,\n                                                               shots,\n                                                               ways,\n                                                               device)\n            meta_test_error += evaluation_error.item()\n            meta_test_accuracy += evaluation_accuracy.item()\n\n        # Print some metrics\n#        print('\\n')\n#        print('Iteration', iteration)\n#        print('Meta Train Error', meta_train_error / meta_batch_size)\n        print(iteration, 'Meta Train Accuracy', meta_train_accuracy / meta_batch_size)\n#        print('Meta Valid Error', meta_valid_error / meta_batch_size)\n#        print('Meta Valid Accuracy', meta_valid_accuracy / meta_batch_size)\n#        print('Meta Test Error', meta_test_error / meta_batch_size)\n#        print('Meta Test Accuracy', meta_test_accuracy / meta_batch_size)\n\n\n        # Average the accumulated gradients and optimize\n        for p in maml.parameters():\n            p.grad.data.mul_(1.0 / meta_batch_size)\n        opt.step()\n    return meta_train_accuracy, meta_valid_accuracy, meta_test_accuracy\n\n\nclass MAMLOmniglotIntegrationTests(unittest.TestCase):\n\n    def setUp(self):\n        pass\n\n    def tearDown(self):\n        pass\n\n    def test_final_accuracy(self):\n        train_acc, valid_acc, test_acc = main(num_iterations=5)\n        self.assertTrue(train_acc > 0.35)\n        self.assertTrue(valid_acc > 0.35)\n        self.assertTrue(test_acc > 0.35)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/integration/protonets_miniimagenet_test_notravis.py,11,"b""#!/usr/bin/env python3\n\nimport unittest\nimport random\nimport numpy as np\n\nimport torch\nfrom torch import nn, optim\nfrom torchvision import transforms\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\nimport learn2learn as l2l\n\n\ndef pairwise_distances_logits(a, b):\n    n = a.shape[0]\n    m = b.shape[0]\n    logits = -((a.unsqueeze(1).expand(n, m, -1) -\n                b.unsqueeze(0).expand(n, m, -1))**2).sum(dim=2)\n    return logits\n\n\ndef conv_block(in_channels, out_channels):\n    bn = nn.BatchNorm2d(out_channels)\n    nn.init.uniform_(bn.weight)\n    return nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n        bn,\n        nn.ReLU(),\n        nn.MaxPool2d(2)\n    )\n\n\ndef accuracy(predictions, targets):\n    predictions = predictions.argmax(dim=1).view(targets.shape)\n    return (predictions == targets).sum().float() / targets.size(0)\n\n\nclass Convnet(nn.Module):\n\n    # TODO: Is this architecture better than the one we have\n    # in l2l.vision.models.ConvBase ?\n\n    def __init__(self, x_dim=3, hid_dim=64, z_dim=64):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            conv_block(x_dim, hid_dim),\n            conv_block(hid_dim, hid_dim),\n            conv_block(hid_dim, hid_dim),\n            conv_block(hid_dim, z_dim),\n        )\n        self.out_channels = 1600\n\n    def forward(self, x):\n        x = self.encoder(x)\n        return x.view(x.size(0), -1)\n\n\ndef fast_adapt(model, batch, ways, shot, query_num, metric=None, device=None):\n    if metric is None:\n        metric = pairwise_distances_logits\n    if device is None:\n        device = model.device()\n    data, labels = batch\n    data = data.to(device)\n    labels = labels.to(device)\n    n_items = shot * ways\n\n    # Sort data samples by labels\n    # TODO: Can this be replaced by ConsecutiveLabels ?\n    sort = torch.sort(labels)\n    data = data.squeeze(0)[sort.indices].squeeze(0)\n    labels = labels.squeeze(0)[sort.indices].squeeze(0)\n\n    # Compute support and query embeddings\n    embeddings = model(data)\n    support_indices = np.zeros(data.size(0), dtype=bool)\n    selection = np.arange(ways) * (shot + query_num)\n    for offset in range(shot):\n        support_indices[selection + offset] = True\n    query_indices = torch.from_numpy(~support_indices)\n    support_indices = torch.from_numpy(support_indices)\n    support = embeddings[support_indices]\n    support = support.reshape(ways, shot, -1).mean(dim=1)\n    query = embeddings[query_indices]\n    labels = labels[query_indices].long()\n\n    logits = pairwise_distances_logits(query, support)\n    loss = F.cross_entropy(logits, labels)\n    acc = accuracy(logits, labels)\n    return loss, acc\n\n\nclass Object:\n    pass\n\n\ndef main(num_iterations=250):\n    args = Object()\n    setattr(args, 'max_epoch', num_iterations)\n    setattr(args, 'shot', 1)\n    setattr(args, 'test_way', 5)\n    setattr(args, 'test_shot', 1)\n    setattr(args, 'test_query', 30)\n    setattr(args, 'train_query', 15)\n    setattr(args, 'train_way', 30)\n    setattr(args, 'gpu', 0)\n\n    device = torch.device('cpu')\n    if torch.cuda.device_count():\n        torch.cuda.manual_seed(43)\n        device = torch.device('cuda')\n\n    model = Convnet()\n    model.to(device)\n\n    path_data = './data'\n    train_dataset = l2l.vision.datasets.MiniImagenet(\n        root=path_data, mode='train')\n    valid_dataset = l2l.vision.datasets.MiniImagenet(\n        root=path_data, mode='validation')\n    test_dataset = l2l.vision.datasets.MiniImagenet(\n        root=path_data, mode='test')\n\n    train_dataset = l2l.data.MetaDataset(train_dataset)\n    train_transforms = [\n        l2l.data.transforms.NWays(train_dataset, args.train_way),\n        l2l.data.transforms.KShots(train_dataset, args.train_query + args.shot),\n        l2l.data.transforms.LoadData(train_dataset),\n        l2l.data.transforms.RemapLabels(train_dataset),\n    ]\n    train_tasks = l2l.data.TaskDataset(train_dataset, task_transforms=train_transforms)\n#    train_loader = DataLoader(train_tasks, pin_memory=True, shuffle=True)\n\n    valid_dataset = l2l.data.MetaDataset(valid_dataset)\n    valid_transforms = [\n        l2l.data.transforms.NWays(valid_dataset, args.test_way),\n        l2l.data.transforms.KShots(valid_dataset, args.test_query + args.test_shot),\n        l2l.data.transforms.LoadData(valid_dataset),\n        l2l.data.transforms.RemapLabels(valid_dataset),\n    ]\n    valid_tasks = l2l.data.TaskDataset(valid_dataset,\n                                       task_transforms=valid_transforms,\n                                       num_tasks=200)\n#    valid_loader = DataLoader(valid_tasks, pin_memory=True, shuffle=True)\n\n    test_dataset = l2l.data.MetaDataset(test_dataset)\n    test_transforms = [\n        l2l.data.transforms.NWays(test_dataset, args.test_way),\n        l2l.data.transforms.KShots(test_dataset, args.test_query + args.test_shot),\n        l2l.data.transforms.LoadData(test_dataset),\n        l2l.data.transforms.RemapLabels(test_dataset),\n    ]\n    test_tasks = l2l.data.TaskDataset(test_dataset,\n                                      task_transforms=test_transforms,\n                                      num_tasks=200)\n#    test_loader = DataLoader(test_tasks, pin_memory=True, shuffle=True)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n    lr_scheduler = torch.optim.lr_scheduler.StepLR(\n        optimizer, step_size=20, gamma=0.5)\n\n    for epoch in range(1, args.max_epoch + 1):\n        model.train()\n\n        loss_ctr = 0\n        n_loss = 0\n        n_acc = 0\n\n        for i in range(100):\n#            batch = next(iter(train_loader))\n            batch = train_tasks.sample()\n\n            loss, acc = fast_adapt(model,\n                                   batch,\n                                   args.train_way,\n                                   args.shot,\n                                   args.train_query,\n                                   metric=pairwise_distances_logits,\n                                   device=device)\n\n            loss_ctr += 1\n            n_loss += loss.item()\n            n_acc += acc\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        lr_scheduler.step()\n\n        print('epoch {}, train, loss={:.4f} acc={:.4f}'.format(\n            epoch, n_loss/loss_ctr, n_acc/loss_ctr))\n        train_accuracy = n_acc / loss_ctr\n\n        model.eval()\n\n        loss_ctr = 0\n        n_loss = 0\n        n_acc = 0\n        for i, batch in enumerate(valid_tasks):\n            loss, acc = fast_adapt(model,\n                                   batch,\n                                   args.test_way,\n                                   args.test_shot,\n                                   args.test_query,\n                                   metric=pairwise_distances_logits,\n                                   device=device)\n\n            loss_ctr += 1\n            n_loss += loss.item()\n            n_acc += acc\n\n        print('epoch {}, val, loss={:.4f} acc={:.4f}'.format(\n            epoch, n_loss/loss_ctr, n_acc/loss_ctr))\n        valid_accuracy = n_acc / loss_ctr\n\n    loss_ctr = 0\n    n_acc = 0\n\n    for i, batch in enumerate(test_tasks, 1):\n        loss, acc = fast_adapt(model,\n                               batch,\n                               args.test_way,\n                               args.test_shot,\n                               args.test_query,\n                               metric=pairwise_distances_logits,\n                               device=device)\n        loss_ctr += 1\n        n_acc += acc\n    print('batch {}: {:.2f}({:.2f})'.format(\n        i, n_acc/loss_ctr * 100, acc * 100))\n    test_accuracy = n_acc / loss_ctr\n    return train_accuracy, valid_accuracy, test_accuracy\n\n\nclass ProtoNetMiniImageNetIntegrationTests(unittest.TestCase):\n\n    def setUp(self):\n        pass\n\n    def tearDown(self):\n        pass\n\n    def test_final_accuracy(self):\n        train_acc, valid_acc, test_acc = main(num_iterations=1)\n        self.assertTrue(train_acc > 0.03)\n        self.assertTrue(valid_acc > 0.20)\n        self.assertTrue(test_acc > 0.20)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/unit/__init__.py,0,b''
tests/unit/utils_test.py,29,"b'#!/usr/bin/env python3\n\nimport unittest\nimport copy\nimport torch\nimport learn2learn as l2l\n\n\ndef ref_clone_module(module):\n    """"""\n    Note: This implementation does not work for RNNs.\n    It requires calling learner.rnn._apply(lambda x: x) before\n    each forward call.\n    See this issue for more details:\n    https://github.com/learnables/learn2learn/issues/139\n    """"""\n    # First, create a copy of the module.\n    clone = copy.deepcopy(module)\n\n    # Second, re-write all parameters\n    if hasattr(clone, \'_parameters\'):\n        for param_key in module._parameters:\n            if module._parameters[param_key] is not None:\n                cloned = module._parameters[param_key].clone()\n                clone._parameters[param_key] = cloned\n\n    # Third, handle the buffers if necessary\n    if hasattr(clone, \'_buffers\'):\n        for buffer_key in module._buffers:\n            if clone._buffers[buffer_key] is not None and \\\n                    clone._buffers[buffer_key].requires_grad:\n                clone._buffers[buffer_key] = module._buffers[buffer_key].clone()\n\n    # Then, recurse for each submodule\n    if hasattr(clone, \'_modules\'):\n        for module_key in clone._modules:\n            clone._modules[module_key] = ref_clone_module(module._modules[module_key])\n    return clone\n\n\nclass Model(torch.nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.model = torch.nn.Sequential(\n            torch.nn.Linear(4, 64),\n            torch.nn.Tanh(),\n            torch.nn.Linear(64, 2)\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n\nclass UtilTests(unittest.TestCase):\n\n    def setUp(self):\n        self.model = Model()\n        self.loss_func = torch.nn.MSELoss()\n        self.input = torch.tensor([[0., 1., 2., 3.]])\n\n    def tearDown(self):\n        pass\n\n    def optimizer_step(self, model, gradients):\n        for param, gradient in zip(model.parameters(), gradients):\n            param.data.sub_(0.01 * gradient)\n\n    def test_clone_module_basics(self):\n        original_output = self.model(self.input)\n        original_loss = self.loss_func(original_output, torch.tensor([[0., 0.]]))\n        original_gradients = torch.autograd.grad(original_loss,\n                                                 self.model.parameters(),\n                                                 retain_graph=True,\n                                                 create_graph=True)\n\n        cloned_model = l2l.clone_module(self.model)\n        self.optimizer_step(self.model, original_gradients)\n\n        cloned_output = cloned_model(self.input)\n        cloned_loss = self.loss_func(cloned_output, torch.tensor([[0., 0.]]))\n\n        cloned_gradients = torch.autograd.grad(cloned_loss,\n                                               cloned_model.parameters(),\n                                               retain_graph=True,\n                                               create_graph=True)\n\n        self.optimizer_step(cloned_model, cloned_gradients)\n\n        for a, b in zip(self.model.parameters(), cloned_model.parameters()):\n            self.assertTrue(torch.equal(a, b))\n\n    def test_clone_module_nomodule(self):\n        # Tests that we can clone non-module objects\n        class TrickyModule(torch.nn.Module):\n\n            def __init__(self):\n                super(TrickyModule, self).__init__()\n                self.tricky_modules = torch.nn.ModuleList([\n                    torch.nn.Linear(2, 1),\n                    None,\n                    torch.nn.Linear(1, 1),\n                ])\n\n        model = TrickyModule()\n        clone = l2l.clone_module(model)\n        for i, submodule in enumerate(clone.tricky_modules):\n            if i % 2 == 0:\n                self.assertTrue(submodule is not None)\n            else:\n                self.assertTrue(submodule is None)\n\n    def test_clone_module_models(self):\n        ref_models = [l2l.vision.models.OmniglotCNN(10),\n                  l2l.vision.models.MiniImagenetCNN(10)]\n        l2l_models = [copy.deepcopy(m) for m in ref_models]\n        inputs = [torch.randn(5, 1, 28, 28), torch.randn(5, 3, 84, 84)]\n\n\n        # Compute reference gradients\n        ref_grads = []\n        for model, X in zip(ref_models, inputs):\n            for iteration in range(10):\n                model.zero_grad()\n                clone = ref_clone_module(model)\n                out = clone(X)\n                out.norm(p=2).backward()\n                self.optimizer_step(model, [p.grad for p in model.parameters()])\n                ref_grads.append([p.grad.clone().detach() for p in model.parameters()])\n\n        # Compute cloned gradients\n        l2l_grads = []\n        for model, X in zip(l2l_models, inputs):\n            for iteration in range(10):\n                model.zero_grad()\n                clone = l2l.clone_module(model)\n                out = clone(X)\n                out.norm(p=2).backward()\n                self.optimizer_step(model, [p.grad for p in model.parameters()])\n                l2l_grads.append([p.grad.clone().detach() for p in model.parameters()])\n\n        # Compare gradients and model parameters\n        for ref_g, l2l_g in zip(ref_grads, l2l_grads):\n            for r_g, l_g in zip(ref_g, l2l_g):\n                self.assertTrue(torch.equal(r_g, l_g))\n        for ref_model, l2l_model in zip(ref_models, l2l_models):\n            for ref_p, l2l_p in zip(ref_model.parameters(), l2l_model.parameters()):\n                self.assertTrue(torch.equal(ref_p, l2l_p))\n\n    def test_rnn_clone(self):\n        # Tests: https://github.com/learnables/learn2learn/issues/139\n        # The test is mainly about whether we can clone and adapt RNNs.\n        # See issue for details.\n        N_STEPS = 3\n        for rnn_class in [\n            torch.nn.RNN,\n            torch.nn.LSTM,\n            torch.nn.GRU,\n        ]:\n            torch.manual_seed(1234)\n            model = rnn_class(2, 1)\n            maml = l2l.algorithms.MAML(model, lr=1e-3, allow_unused=False)\n            optim = torch.optim.SGD(maml.parameters(), lr=0.001)\n            data = torch.randn(30, 500, 2)\n            \n            # Adapt and measure loss\n            learner = maml.clone()\n            for step in range(N_STEPS):\n                pred, hidden = learner(data)\n                loss = pred.norm(p=2)\n                learner.adapt(loss)\n            pred, _ = learner(data)\n            first_loss = pred.norm(p=2)\n\n            # Take an optimization step\n            optim.zero_grad()\n            first_loss.backward()\n            optim.step()\n            first_loss = first_loss.item()\n\n            # Adapt a second time\n            learner = maml.clone()\n            for step in range(N_STEPS):\n                pred, hidden = learner(data)\n                loss = pred.norm(p=2)\n                learner.adapt(loss)\n            pred, _ = learner(data)\n            second_loss = pred.norm(p=2)\n            second_loss = second_loss.item()\n\n            # Ensure we did better\n            self.assertTrue(first_loss > second_loss)\n\n\n    def test_module_detach(self):\n        original_output = self.model(self.input)\n        original_loss = self.loss_func(original_output, torch.tensor([[0., 0.]]))\n\n        original_gradients = torch.autograd.grad(original_loss,\n                                                 self.model.parameters(),\n                                                 retain_graph=True,\n                                                 create_graph=True)\n\n        l2l.detach_module(self.model)\n        severed = self.model\n\n        self.optimizer_step(self.model, original_gradients)\n\n        severed_output = severed(self.input)\n        severed_loss = self.loss_func(severed_output, torch.tensor([[0., 0.]]))\n\n        fail = False\n        try:\n            severed_gradients = torch.autograd.grad(severed_loss,\n                                                    severed.parameters(),\n                                                    retain_graph=True,\n                                                    create_graph=True)\n        except Exception as e:\n            fail = True\n\n        finally:\n            assert fail == True\n\n    def test_distribution_clone(self):\n        pass\n\n    def test_distribution_detach(self):\n        pass\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
learn2learn/gym/envs/__init__.py,0,"b""#!/usr/bin/env python3\n\nfrom gym.envs.registration import register\n\nfrom .subproc_vec_env import SubprocVecEnv\n\n# 2D Navigation\n# ----------------------------------------\n\nregister(\n    'Particles2D-v1',\n    entry_point='learn2learn.gym.envs.particles.particles_2d:Particles2DEnv',\n    max_episode_steps=100\n)\n\n# Mujoco\n# ----------------------------------------\n\nregister(\n    'HalfCheetahForwardBackward-v1',\n    entry_point='learn2learn.gym.envs.mujoco.halfcheetah_forward_backward:HalfCheetahForwardBackwardEnv',\n    max_episode_steps=100,\n)\n\nregister(\n    'AntForwardBackward-v1',\n    entry_point='learn2learn.gym.envs.mujoco.ant_forward_backward:AntForwardBackwardEnv',\n    max_episode_steps=100,\n)\n\nregister(\n    'AntDirection-v1',\n    entry_point='learn2learn.gym.envs.mujoco.ant_direction:AntDirectionEnv',\n    max_episode_steps=100,\n)\n\nregister(\n    'HumanoidForwardBackward-v1',\n    entry_point='learn2learn.gym.envs.mujoco.humanoid_forward_backward:HumanoidForwardBackwardEnv',\n    max_episode_steps=200,\n)\n\nregister(\n    'HumanoidDirection-v1',\n    entry_point='learn2learn.gym.envs.mujoco.humanoid_direction:HumanoidDirectionEnv',\n    max_episode_steps=200,\n)\n"""
learn2learn/gym/envs/meta_env.py,0,"b'#!/usr/bin/env python3\n\nfrom gym.core import Env\n\n\nclass MetaEnv(Env):\n    """"""\n    [[Source]](https://github.com/learnables/learn2learn/blob/master/learn2learn/gym/envs/meta_env.py)\n\n    **Description**\n\n    Interface for l2l envs. Environments have a certain number of task specific parameters that uniquely\n    identify the environment. Tasks are then a dictionary with the names of these parameters as keys and the\n    values of these parameters as values. Environments must then implement functions to get, set and sample tasks.\n    The flow is then\n    ```\n    env = EnvClass()\n    tasks = env.sample_tasks(num_tasks)\n    for task in tasks:\n        env.set_task(task)\n        *training code here*\n        ...\n    ```\n\n    **Credit**\n\n    Adapted from Tristan Deleu and Jonas Rothfuss\' implementations.\n\n    """"""\n\n    def __init__(self, task=None):\n        super(Env, self).__init__()\n        if task is None:\n            task = self.sample_tasks(1)[0]\n        self.set_task(task)\n\n    def sample_tasks(self, num_tasks):\n        """"""\n        **Description**\n\n        Samples num_tasks tasks for training or evaluation.\n        How the tasks are sampled determines the task distribution.\n\n        **Arguments**\n\n        num_tasks (int) - number of tasks to sample\n\n        **Returns**\n\n        tasks ([dict]) - returns a list of num_tasks tasks. Tasks are\n        dictionaries of task specific parameters. A\n        minimal example for num_tasks = 1 is [{\'goal\': value}].\n        """"""\n        raise NotImplementedError\n\n    def set_task(self, task):\n        """"""\n        **Description**\n\n        Sets the task specific parameters defined in task.\n\n        **Arguments**\n\n        task (dict) - A dictionary of task specific parameters and\n        their values.\n\n        **Returns**\n\n        None.\n        """"""\n        self._task = task\n\n    def get_task(self):\n        """"""\n        **Description**\n\n        Returns the current task.\n\n        **Arguments**\n\n        None.\n\n        **Returns**\n\n        (task) - Dictionary of task specific parameters and their\n        current values.\n        """"""\n        return self._task\n'"
learn2learn/gym/envs/subproc_vec_env.py,0,"b""import multiprocessing as mp\nimport sys\n\nimport gym\nimport numpy as np\n\nis_py2 = (sys.version[0] == '2')\nif is_py2:\n    pass\nelse:\n    pass\n\n\nclass EnvWorker(mp.Process):\n    def __init__(self, remote, env_fn, queue, lock):\n        super(EnvWorker, self).__init__()\n        self.remote = remote\n        self.env = env_fn()\n        self.queue = queue\n        self.lock = lock\n        self.task_id = None\n        self.done = False\n\n    def empty_step(self):\n        observation = np.zeros(self.env.observation_space.shape,\n                               dtype=np.float32)\n        reward, done = 0.0, True\n        return observation, reward, done, {}\n\n    def try_reset(self):\n        observation = self.env.reset()\n        return observation\n\n    def run(self):\n        while True:\n            command, data = self.remote.recv()\n            if command == 'step':\n                observation, reward, done, info = self.env.step(data)\n                if done and (not self.done):\n                    observation = self.try_reset()\n                self.remote.send((observation, reward, done, self.task_id, info))\n            elif command == 'reset':\n                observation = self.try_reset()\n                self.remote.send((observation, self.task_id))\n            elif command == 'set_task':\n                self.env.unwrapped.set_task(data)\n                self.remote.send(True)\n            elif command == 'close':\n                self.remote.close()\n                break\n            elif command == 'get_spaces':\n                self.remote.send((self.env.observation_space,\n                                  self.env.action_space))\n            else:\n                raise NotImplementedError()\n\n\nclass SubprocVecEnv(gym.Env):\n    def __init__(self, env_factory, queue):\n        self.lock = mp.Lock()\n        self.remotes, self.work_remotes = zip(*[mp.Pipe() for _ in env_factory])\n        self.workers = [EnvWorker(remote, env_fn, queue, self.lock)\n                        for (remote, env_fn) in zip(self.work_remotes, env_factory)]\n        for worker in self.workers:\n            worker.daemon = True\n            worker.start()\n        for remote in self.work_remotes:\n            remote.close()\n        self.waiting = False\n        self.closed = False\n\n        self.remotes[0].send(('get_spaces', None))\n        observation_space, action_space = self.remotes[0].recv()\n        self.observation_space = observation_space\n        self.action_space = action_space\n\n    def step(self, actions):\n        self.step_async(actions)\n        return self.step_wait()\n\n    def step_async(self, actions):\n        for remote, action in zip(self.remotes, actions):\n            remote.send(('step', action))\n        self.waiting = True\n\n    def step_wait(self):\n        results = [remote.recv() for remote in self.remotes]\n        self.waiting = False\n        observations, rewards, dones, task_ids, infos = zip(*results)\n        return np.stack(observations), np.stack(rewards), np.stack(dones), task_ids, infos\n\n    def reset(self):\n        for remote in self.remotes:\n            remote.send(('reset', None))\n        results = [remote.recv() for remote in self.remotes]\n        observations, task_ids = zip(*results)\n        return np.stack(observations), task_ids\n\n    def set_task(self, tasks):\n        for remote, task in zip(self.remotes, tasks):\n            remote.send(('set_task', task))\n        return np.stack([remote.recv() for remote in self.remotes])\n\n    def close(self):\n        if self.closed:\n            return\n        if self.waiting:\n            for remote in self.remotes:\n                remote.recv()\n        for remote in self.remotes:\n            remote.send(('close', None))\n        for worker in self.workers:\n            worker.join()\n        self.closed = True\n"""
learn2learn/text/datasets/__init__.py,0,b'#!/usr/bin/env python3\n\nfrom .news_classification import NewsClassification\n'
learn2learn/text/datasets/news_classification.py,2,"b'import io\nimport os\nimport zipfile\n\nimport pandas as pd\nimport requests\nimport torch\nfrom torch.utils.data import Dataset\n\n\nclass NewsClassification(Dataset):\n    """"""\n\n    [[Source]]()\n\n    **Description**\n\n    **References**\n\n    * TODO: Cite ...\n\n    **Arguments**\n\n    **Example**\n\n    """"""\n\n    def __init__(self, root, train=True, transform=None, download=False):\n        self.labels_list = {\'QUEER VOICES\': 0, \'GREEN\': 1, \'STYLE\': 2, \'BUSINESS\': 3, \'CULTURE & ARTS\': 4,\n                            \'WEDDINGS\': 5, \'ARTS\': 6, \'HEALTHY LIVING\': 7,\n                            \'LATINO VOICES\': 8, \'ENVIRONMENT\': 9, \'FIFTY\': 10, \'COMEDY\': 11, \'BLACK VOICES\': 12,\n                            \'TRAVEL\': 13, \'ENTERTAINMENT\': 14, \'TASTE\': 15,\n                            \'CRIME\': 16, \'WOMEN\': 17, \'TECH\': 18, \'PARENTING\': 19, \'SCIENCE\': 20, \'WORLD NEWS\': 21,\n                            \'WORLDPOST\': 22, \'POLITICS\': 23,\n                            \'ARTS & CULTURE\': 24, \'RELIGION\': 25, \'IMPACT\': 26, \'MEDIA\': 27, \'STYLE & BEAUTY\': 28,\n                            \'SPORTS\': 29, \'WEIRD NEWS\': 30,\n                            \'HOME & LIVING\': 31, \'THE WORLDPOST\': 32, \'MONEY\': 33, \'EDUCATION\': 34, \'DIVORCE\': 35,\n                            \'PARENTS\': 36, \'GOOD NEWS\': 37,\n                            \'FOOD & DRINK\': 38, \'WELLNESS\': 39, \'COLLEGE\': 40}\n        if train:\n            self.path = os.path.join(root, \'train_sample.csv\')\n        else:\n            self.path = os.path.join(root, \'test_sample.csv\')\n        self.transform = transform\n        if transform == \'roberta\':\n            self.roberta = torch.hub.load(\'pytorch/fairseq\', \'roberta.large\')\n\n        if download:\n            download_file_url = \'https://www.dropbox.com/s/g8hwl9pxftl36ww/test_sample.csv.zip?dl=1\'\n            if train:\n                download_file_url = \'https://www.dropbox.com/s/o71z7fq7mydbznc/train_sample.csv.zip?dl=1\'\n\n            r = requests.get(download_file_url)\n            z = zipfile.ZipFile(io.BytesIO(r.content))\n            z.extractall(path=root)\n\n        if root:\n\n            if os.path.exists(self.path):\n                self.df_data = pd.read_csv(self.path)\n            else:\n                raise ValueError(""Please download the file first."")\n\n    def __len__(self):\n        return self.df_data.shape[0]\n\n    def __getitem__(self, idx):\n        if self.transform:\n            return self.roberta.encode(self.df_data[\'headline\'][idx]), \\\n                   self.labels_list[self.df_data[\'category\'][idx]]\n\n        return self.df_data[\'headline\'][idx], self.labels_list[self.df_data[\'category\'][idx]]\n'"
learn2learn/vision/benchmarks/__init__.py,0,"b'#!/usr/bin/env python3\n\n""""""\nThe benchmark modules provides a convenient interface to standardized benchmarks in the literature.\nIt provides train/validation/test TaskDatasets and TaskTransforms for pre-defined datasets.\n\nThis utility is useful for researchers to compare new algorithms against existing benchmarks.\nFor a more fine-grained control over tasks and data, we recommend directly using `l2l.data.TaskDataset` and `l2l.data.TaskTransforms`.\n""""""\n\nimport os\nimport learn2learn as l2l\n\nfrom collections import namedtuple\nfrom .omniglot_benchmark import omniglot_tasksets\nfrom .mini_imagenet_benchmark import mini_imagenet_tasksets\nfrom .tiered_imagenet_benchmark import tiered_imagenet_tasksets\nfrom .fc100_benchmark import fc100_tasksets\nfrom .cifarfs_benchmark import cifarfs_tasksets\n\n\n__all__ = [\'list_tasksets\', \'get_tasksets\']\n\n\nBenchmarkTasksets = namedtuple(\'BenchmarkTasksets\', (\'train\', \'validation\', \'test\'))\n\n_TASKSETS = {\n    \'omniglot\': omniglot_tasksets,\n    \'mini-imagenet\': mini_imagenet_tasksets,\n    \'tiered-imagenet\': tiered_imagenet_tasksets,\n    \'fc100\': fc100_tasksets,\n    \'cifarfs\': cifarfs_tasksets,\n}\n\n\ndef list_tasksets():\n    """"""\n    [[Source]](https://github.com/learnables/learn2learn/blob/master/learn2learn/vision/benchmarks/)\n\n    **Description**\n\n    Returns a list of all available benchmarks.\n\n    **Example**\n    ~~~python\n    for name in l2l.vision.benchmarks.list_tasksets():\n        print(name)\n        tasksets = l2l.vision.benchmarks.get_tasksets(name)\n    ~~~\n    """"""\n    return _TASKSETS.keys()\n\n\ndef get_tasksets(\n    name,\n    train_ways=5,\n    train_samples=10,\n    test_ways=5,\n    test_samples=10,\n    num_tasks=-1,\n    root=\'~/data\',\n    device=None,\n    **kwargs,\n):\n    """"""\n    [[Source]](https://github.com/learnables/learn2learn/blob/master/learn2learn/vision/benchmarks/)\n\n    **Description**\n\n    Returns the tasksets for a particular benchmark, using literature standard data and task transformations.\n\n    The returned object is a namedtuple with attributes `train`, `validation`, `test` which\n    correspond to their respective TaskDatasets.\n    See `examples/vision/maml_miniimagenet.py` for an example.\n\n    **Arguments**\n\n    * **name** (str) - The name of the benchmark. Full list in `list_tasksets()`.\n    * **train_ways** (int, *optional*, default=5) - The number of classes per train tasks.\n    * **train_samples** (int, *optional*, default=10) - The number of samples per train tasks.\n    * **test_ways** (int, *optional*, default=5) - The number of classes per test tasks. Also used for validation tasks.\n    * **test_samples** (int, *optional*, default=10) - The number of samples per test tasks. Also used for validation tasks.\n    * **num_tasks** (int, *optional*, default=-1) - The number of tasks in each TaskDataset.\n    * **root** (str, *optional*, default=\'~/data\') - Where the data is stored.\n\n    **Example**\n    ~~~python\n    train_tasks, validation_tasks, test_tasks = l2l.vision.benchmarks.get_tasksets(\'omniglot\')\n    batch = train_tasks.sample()\n\n    or:\n\n    tasksets = l2l.vision.benchmarks.get_tasksets(\'omniglot\')\n    batch = tasksets.train.sample()\n    ~~~\n    """"""\n    root = os.path.expanduser(root)\n\n    if device is not None:\n        raise NotImplementedError(\'Device other than None not implemented. (yet)\')\n\n    # Load task-specific data and transforms\n    datasets, transforms = _TASKSETS[name](train_ways=train_ways,\n                                           train_samples=train_samples,\n                                           test_ways=test_ways,\n                                           test_samples=test_samples,\n                                           root=root,\n                                           **kwargs)\n    train_dataset, validation_dataset, test_dataset = datasets\n    train_transforms, validation_transforms, test_transforms = transforms\n\n    # Instantiate the tasksets\n    train_tasks = l2l.data.TaskDataset(\n        dataset=train_dataset,\n        task_transforms=train_transforms,\n        num_tasks=num_tasks,\n    )\n    validation_tasks = l2l.data.TaskDataset(\n        dataset=validation_dataset,\n        task_transforms=validation_transforms,\n        num_tasks=num_tasks,\n    )\n    test_tasks = l2l.data.TaskDataset(\n        dataset=test_dataset,\n        task_transforms=test_transforms,\n        num_tasks=num_tasks,\n    )\n    return BenchmarkTasksets(train_tasks, validation_tasks, test_tasks)\n'"
learn2learn/vision/benchmarks/cifarfs_benchmark.py,0,"b'#!/usr/bin/env python3\n\nimport torchvision as tv\nimport learn2learn as l2l\n\nfrom learn2learn.data.transforms import NWays, KShots, LoadData, RemapLabels, ConsecutiveLabels\n\n\ndef cifarfs_tasksets(\n    train_ways=5,\n    train_samples=10,\n    test_ways=5,\n    test_samples=10,\n    root=\'~/data\',\n    **kwargs,\n):\n    """"""Tasksets for CIFAR-FS benchmarks.""""""\n    data_transform = tv.transforms.ToTensor()\n    train_dataset = l2l.vision.datasets.CIFARFS(root=root,\n                                                transform=data_transform,\n                                                mode=\'train\',\n                                                download=True)\n    valid_dataset = l2l.vision.datasets.CIFARFS(root=root,\n                                                transform=data_transform,\n                                                mode=\'validation\',\n                                                download=True)\n    test_dataset = l2l.vision.datasets.CIFARFS(root=root,\n                                               transform=data_transform,\n                                               mode=\'test\',\n                                               download=True)\n    train_dataset = l2l.data.MetaDataset(train_dataset)\n    valid_dataset = l2l.data.MetaDataset(valid_dataset)\n    test_dataset = l2l.data.MetaDataset(test_dataset)\n\n    train_transforms = [\n        NWays(train_dataset, train_ways),\n        KShots(train_dataset, train_samples),\n        LoadData(train_dataset),\n        RemapLabels(train_dataset),\n        ConsecutiveLabels(train_dataset),\n    ]\n    valid_transforms = [\n        NWays(valid_dataset, test_ways),\n        KShots(valid_dataset, test_samples),\n        LoadData(valid_dataset),\n        ConsecutiveLabels(valid_dataset),\n        RemapLabels(valid_dataset),\n    ]\n    test_transforms = [\n        NWays(test_dataset, test_ways),\n        KShots(test_dataset, test_samples),\n        LoadData(test_dataset),\n        RemapLabels(test_dataset),\n        ConsecutiveLabels(test_dataset),\n    ]\n\n    _datasets = (train_dataset, valid_dataset, test_dataset)\n    _transforms = (train_transforms, valid_transforms, test_transforms)\n    return _datasets, _transforms\n'"
learn2learn/vision/benchmarks/fc100_benchmark.py,0,"b'#!/usr/bin/env python3\n\nimport torchvision as tv\nimport learn2learn as l2l\n\nfrom learn2learn.data.transforms import NWays, KShots, LoadData, RemapLabels, ConsecutiveLabels\n\n\ndef fc100_tasksets(\n    train_ways=5,\n    train_samples=10,\n    test_ways=5,\n    test_samples=10,\n    root=\'~/data\',\n    **kwargs,\n):\n    """"""Tasksets for FC100 benchmarks.""""""\n    data_transform = tv.transforms.ToTensor()\n    train_dataset = l2l.vision.datasets.FC100(root=root,\n                                              transform=data_transform,\n                                              mode=\'train\',\n                                              download=True)\n    valid_dataset = l2l.vision.datasets.FC100(root=root,\n                                              transform=data_transform,\n                                              mode=\'validation\',\n                                              download=True)\n    test_dataset = l2l.vision.datasets.FC100(root=root,\n                                             transform=data_transform,\n                                             mode=\'test\',\n                                             download=True)\n    train_dataset = l2l.data.MetaDataset(train_dataset)\n    valid_dataset = l2l.data.MetaDataset(valid_dataset)\n    test_dataset = l2l.data.MetaDataset(test_dataset)\n\n    train_transforms = [\n        NWays(train_dataset, train_ways),\n        KShots(train_dataset, train_samples),\n        LoadData(train_dataset),\n        RemapLabels(train_dataset),\n        ConsecutiveLabels(train_dataset),\n    ]\n    valid_transforms = [\n        NWays(valid_dataset, test_ways),\n        KShots(valid_dataset, test_samples),\n        LoadData(valid_dataset),\n        ConsecutiveLabels(valid_dataset),\n        RemapLabels(valid_dataset),\n    ]\n    test_transforms = [\n        NWays(test_dataset, test_ways),\n        KShots(test_dataset, test_samples),\n        LoadData(test_dataset),\n        RemapLabels(test_dataset),\n        ConsecutiveLabels(test_dataset),\n    ]\n\n    _datasets = (train_dataset, valid_dataset, test_dataset)\n    _transforms = (train_transforms, valid_transforms, test_transforms)\n    return _datasets, _transforms\n'"
learn2learn/vision/benchmarks/mini_imagenet_benchmark.py,0,"b'#!/usr/bin/env python3\n\nimport learn2learn as l2l\nfrom learn2learn.data.transforms import NWays, KShots, LoadData, RemapLabels, ConsecutiveLabels\n\n\ndef mini_imagenet_tasksets(\n    train_ways=5,\n    train_samples=10,\n    test_ways=5,\n    test_samples=10,\n    root=\'~/data\',\n    **kwargs,\n):\n    """"""Tasksets for mini-ImageNet benchmarks.""""""\n    train_dataset = l2l.vision.datasets.MiniImagenet(root=root,\n                                                     mode=\'train\',\n                                                     download=True)\n    valid_dataset = l2l.vision.datasets.MiniImagenet(root=root,\n                                                     mode=\'validation\',\n                                                     download=True)\n    test_dataset = l2l.vision.datasets.MiniImagenet(root=root,\n                                                    mode=\'test\',\n                                                    download=True)\n    train_dataset = l2l.data.MetaDataset(train_dataset)\n    valid_dataset = l2l.data.MetaDataset(valid_dataset)\n    test_dataset = l2l.data.MetaDataset(test_dataset)\n\n    train_transforms = [\n        NWays(train_dataset, train_ways),\n        KShots(train_dataset, train_samples),\n        LoadData(train_dataset),\n        RemapLabels(train_dataset),\n        ConsecutiveLabels(train_dataset),\n    ]\n    valid_transforms = [\n        NWays(valid_dataset, test_ways),\n        KShots(valid_dataset, test_samples),\n        LoadData(valid_dataset),\n        ConsecutiveLabels(valid_dataset),\n        RemapLabels(valid_dataset),\n    ]\n    test_transforms = [\n        NWays(test_dataset, test_ways),\n        KShots(test_dataset, test_samples),\n        LoadData(test_dataset),\n        RemapLabels(test_dataset),\n        ConsecutiveLabels(test_dataset),\n    ]\n\n    _datasets = (train_dataset, valid_dataset, test_dataset)\n    _transforms = (train_transforms, valid_transforms, test_transforms)\n    return _datasets, _transforms\n'"
learn2learn/vision/benchmarks/omniglot_benchmark.py,0,"b'#!/usr/bin/env python3\n\nimport random\nimport learn2learn as l2l\n\nfrom torchvision import transforms\nfrom PIL.Image import LANCZOS\n\n\ndef omniglot_tasksets(\n    train_ways,\n    train_samples,\n    test_ways,\n    test_samples,\n    root,\n    **kwargs\n):\n    """"""\n    Benchmark definition for Omniglot.\n    """"""\n    data_transforms = transforms.Compose([\n        transforms.Resize(28, interpolation=LANCZOS),\n        transforms.ToTensor(),\n        lambda x: 1.0 - x,\n    ])\n    omniglot = l2l.vision.datasets.FullOmniglot(\n        root=root,\n        transform=data_transforms,\n        download=True,\n    )\n    dataset = l2l.data.MetaDataset(omniglot)\n    train_dataset = dataset\n    validation_datatset = dataset\n    test_dataset = dataset\n\n    classes = list(range(1623))\n    random.shuffle(classes)\n    train_transforms = [\n        l2l.data.transforms.FusedNWaysKShots(dataset,\n                                             n=train_ways,\n                                             k=train_samples,\n                                             filter_labels=classes[:1100]),\n        l2l.data.transforms.LoadData(dataset),\n        l2l.data.transforms.RemapLabels(dataset),\n        l2l.data.transforms.ConsecutiveLabels(dataset),\n        l2l.vision.transforms.RandomClassRotation(dataset, [0.0, 90.0, 180.0, 270.0])\n    ]\n    validation_transforms = [\n        l2l.data.transforms.FusedNWaysKShots(dataset,\n                                             n=test_ways,\n                                             k=test_samples,\n                                             filter_labels=classes[1100:1200]),\n        l2l.data.transforms.LoadData(dataset),\n        l2l.data.transforms.RemapLabels(dataset),\n        l2l.data.transforms.ConsecutiveLabels(dataset),\n        l2l.vision.transforms.RandomClassRotation(dataset, [0.0, 90.0, 180.0, 270.0])\n    ]\n    test_transforms = [\n        l2l.data.transforms.FusedNWaysKShots(dataset,\n                                             n=test_ways,\n                                             k=test_samples,\n                                             filter_labels=classes[1200:]),\n        l2l.data.transforms.LoadData(dataset),\n        l2l.data.transforms.RemapLabels(dataset),\n        l2l.data.transforms.ConsecutiveLabels(dataset),\n        l2l.vision.transforms.RandomClassRotation(dataset, [0.0, 90.0, 180.0, 270.0])\n    ]\n\n    _datasets = (train_dataset, validation_datatset, test_dataset)\n    _transforms = (train_transforms, validation_transforms, test_transforms)\n    return _datasets, _transforms\n'"
learn2learn/vision/benchmarks/tiered_imagenet_benchmark.py,0,"b'#!/usr/bin/env python3\n\nimport torchvision as tv\nimport learn2learn as l2l\n\nfrom learn2learn.data.transforms import NWays, KShots, LoadData, RemapLabels, ConsecutiveLabels\n\n\ndef tiered_imagenet_tasksets(\n    train_ways=5,\n    train_samples=10,\n    test_ways=5,\n    test_samples=10,\n    root=\'~/data\',\n    **kwargs,\n):\n    """"""Tasksets for tiered-ImageNet benchmarks.""""""\n    data_transform = tv.transforms.ToTensor()\n    train_dataset = l2l.vision.datasets.TieredImagenet(root=root,\n                                                       transform=data_transform,\n                                                       mode=\'train\',\n                                                       download=True)\n    valid_dataset = l2l.vision.datasets.TieredImagenet(root=root,\n                                                       transform=data_transform,\n                                                       mode=\'validation\',\n                                                       download=True)\n    test_dataset = l2l.vision.datasets.TieredImagenet(root=root,\n                                                      transform=data_transform,\n                                                      mode=\'test\',\n                                                      download=True)\n    train_dataset = l2l.data.MetaDataset(train_dataset)\n    valid_dataset = l2l.data.MetaDataset(valid_dataset)\n    test_dataset = l2l.data.MetaDataset(test_dataset)\n\n    train_transforms = [\n        NWays(train_dataset, train_ways),\n        KShots(train_dataset, train_samples),\n        LoadData(train_dataset),\n        RemapLabels(train_dataset),\n        ConsecutiveLabels(train_dataset),\n    ]\n    valid_transforms = [\n        NWays(valid_dataset, test_ways),\n        KShots(valid_dataset, test_samples),\n        LoadData(valid_dataset),\n        ConsecutiveLabels(valid_dataset),\n        RemapLabels(valid_dataset),\n    ]\n    test_transforms = [\n        NWays(test_dataset, test_ways),\n        KShots(test_dataset, test_samples),\n        LoadData(test_dataset),\n        RemapLabels(test_dataset),\n        ConsecutiveLabels(test_dataset),\n    ]\n\n    _datasets = (train_dataset, valid_dataset, test_dataset)\n    _transforms = (train_transforms, valid_transforms, test_transforms)\n    return _datasets, _transforms\n'"
learn2learn/vision/datasets/__init__.py,0,"b'#!/usr/bin/env python3\n\n""""""\n\n**Description**\n\nSome datasets commonly used in meta-learning vision tasks.\n""""""\n\nfrom .full_omniglot import FullOmniglot\nfrom .mini_imagenet import MiniImagenet\nfrom .tiered_imagenet import TieredImagenet\nfrom .cifarfs import CIFARFS\nfrom .fc100 import FC100\nfrom .vgg_flowers import VGGFlower102\nfrom .fgvc_aircraft import FGVCAircraft\n'"
learn2learn/vision/datasets/cifarfs.py,1,"b'#!/usr/bin/env python3\n\nimport os\nimport shutil\nimport zipfile\nimport numpy as np\nimport torch\nimport torch.utils.data as data\n\nfrom torchvision.datasets import ImageFolder\nfrom learn2learn.data.utils import download_file_from_google_drive\n\n\nclass CIFARFS(ImageFolder):\n\n    """"""\n    [[Source]](https://github.com/learnables/learn2learn/blob/master/learn2learn/vision/datasets/cifarfs.py)\n\n    **Description**\n\n    The CIFAR Few-Shot dataset as originally introduced by Bertinetto et al., 2019.\n\n    It consists of 60\'000 colour images of sizes 32x32 pixels.\n    The dataset is divided in 3 splits of 64 training, 16 validation, and 20 testing classes each containing 600 examples.\n    The classes are sampled from the CIFAR-100 dataset, and we use the splits from Bertinetto et al., 2019.\n\n    **References**\n\n    1. Bertinetto et al. 2019. ""Meta-learning with differentiable closed-form solvers"". ICLR.\n\n    **Arguments**\n\n    * **root** (str) - Path to download the data.\n    * **mode** (str, *optional*, default=\'train\') - Which split to use.\n        Must be \'train\', \'validation\', or \'test\'.\n    * **transform** (Transform, *optional*, default=None) - Input pre-processing.\n    * **target_transform** (Transform, *optional*, default=None) - Target pre-processing.\n\n    **Example**\n\n    ~~~python\n    train_dataset = l2l.vision.datasets.CIFARFS(root=\'./data\', mode=\'train\')\n    train_dataset = l2l.data.MetaDataset(train_dataset)\n    train_generator = l2l.data.TaskGenerator(dataset=train_dataset, ways=ways)\n    ~~~\n\n    """"""\n\n    def __init__(self,\n                 root,\n                 mode=\'train\',\n                 transform=None,\n                 target_transform=None,\n                 download=False):\n        self.root = os.path.expanduser(root)\n        if not os.path.exists(self.root):\n            os.mkdir(self.root)\n        self.transform = transform\n        self.target_transform = target_transform\n        self.mode = mode\n        self.processed_root = os.path.join(self.root, \'cifarfs\', \'processed\')\n        self.raw_path = os.path.join(self.root, \'cifarfs\')\n\n        if not self._check_exists() and download:\n            self._download()\n        if not self._check_processed():\n            self._process_zip()\n        mode = \'val\' if mode == \'validation\' else mode\n        self.processed_root = os.path.join(self.processed_root, mode)\n        self._bookkeeping_path = os.path.join(self.root, \'cifarfs-\' + mode + \'-bookkeeping.pkl\')\n        super(CIFARFS, self).__init__(root=self.processed_root,\n                                      transform=self.transform,\n                                      target_transform=self.target_transform)\n\n    def _check_exists(self):\n        return os.path.exists(self.raw_path)\n\n    def _check_processed(self):\n        return os.path.exists(self.processed_root)\n\n    def _download(self):\n        # Download the zip, unzip it, and clean up\n        print(\'Downloading CIFARFS to \', self.root)\n        if not os.path.exists(self.root):\n            os.mkdir(self.root)\n        zip_file = os.path.join(self.root, \'cifarfs.zip\')\n        download_file_from_google_drive(\'1pTsCCMDj45kzFYgrnO67BWVbKs48Q3NI\',\n                                        zip_file)\n        with zipfile.ZipFile(zip_file, \'r\') as zfile:\n            zfile.extractall(self.raw_path)\n        os.remove(zip_file)\n\n    def _process_zip(self):\n        print(\'Creating CIFARFS splits\')\n        if not os.path.exists(self.processed_root):\n            os.mkdir(self.processed_root)\n        split_path = os.path.join(self.raw_path, \'cifar100\', \'splits\', \'bertinetto\')\n        train_split_file = os.path.join(split_path, \'train.txt\')\n        valid_split_file = os.path.join(split_path, \'val.txt\')\n        test_split_file = os.path.join(split_path, \'test.txt\')\n\n        source_dir = os.path.join(self.raw_path, \'cifar100\', \'data\')\n        for fname, dest in [(train_split_file, \'train\'),\n                            (valid_split_file, \'val\'),\n                            (test_split_file, \'test\')]:\n            dest_target = os.path.join(self.processed_root, dest)\n            if not os.path.exists(dest_target):\n                os.mkdir(dest_target)\n            with open(fname) as split:\n                for label in split.readlines():\n                    source = os.path.join(source_dir, label.strip())\n                    target = os.path.join(dest_target, label.strip())\n                    shutil.copytree(source, target)\n\n\nif __name__ == \'__main__\':\n    cifarfs = CIFARFS(\'./data\')\n'"
learn2learn/vision/datasets/fc100.py,1,"b'#!/usr/bin/env python3\n\nimport os\nimport pickle\nimport zipfile\n\nimport torch.utils.data as data\n\nfrom PIL import Image\n\nfrom learn2learn.data.utils import download_file_from_google_drive, download_file\n\n\nGOOGLE_DRIVE_FILE_ID = \'1_ZsLyqI487NRDQhwvI7rg86FK3YAZvz1\'\nDROPBOX_LINK = \'https://www.dropbox.com/s/ftsjuwsu6lfp0fz/FC100.zip?dl=1\'\n\n\nclass FC100(data.Dataset):\n    """"""\n    [[Source]](https://github.com/learnables/learn2learn/blob/master/learn2learn/vision/datasets/fc100.py)\n\n    **Description**\n\n    The FC100 dataset was originally introduced by Oreshkin et al., 2018.\n\n    It is based on CIFAR100, but unlike CIFAR-FS training, validation, and testing classes are\n    split so as to minimize the information overlap between splits.\n    The 100 classes are grouped into 20 superclasses of which 12 (60 classes) are used for training,\n    4 (20 classes) for validation, and 4 (20 classes) for testing.\n    Each class contains 600 images.\n    The specific splits are provided in the Supplementary Material of the paper.\n    Our data is downloaded from the link provided by [2].\n\n    **References**\n\n    1. Oreshkin et al. 2018. ""TADAM: Task Dependent Adaptive Metric for Improved Few-Shot Learning."" NeurIPS.\n    2. Kwoonjoon Lee. 2019. ""MetaOptNet."" [https://github.com/kjunelee/MetaOptNet](https://github.com/kjunelee/MetaOptNet)\n\n    **Arguments**\n\n    * **root** (str) - Path to download the data.\n    * **mode** (str, *optional*, default=\'train\') - Which split to use.\n        Must be \'train\', \'validation\', or \'test\'.\n    * **transform** (Transform, *optional*, default=None) - Input pre-processing.\n    * **target_transform** (Transform, *optional*, default=None) - Target pre-processing.\n\n    **Example**\n\n    ~~~python\n    train_dataset = l2l.vision.datasets.FC100(root=\'./data\', mode=\'train\')\n    train_dataset = l2l.data.MetaDataset(train_dataset)\n    train_generator = l2l.data.TaskDataset(dataset=train_dataset, num_tasks=1000)\n    ~~~\n\n    """"""\n\n    def __init__(self,\n                 root,\n                 mode=\'train\',\n                 transform=None,\n                 target_transform=None,\n                 download=False):\n        super(FC100, self).__init__()\n        self.root = os.path.expanduser(root)\n        os.makedirs(self.root, exist_ok=True)\n        self.transform = transform\n        self.target_transform = target_transform\n        if mode not in [\'train\', \'validation\', \'test\']:\n            raise ValueError(\'mode must be train, validation, or test.\')\n        self.mode = mode\n        self._bookkeeping_path = os.path.join(self.root, \'fc100-bookkeeping-\' + mode + \'.pkl\')\n\n        if not self._check_exists() and download:\n            self.download()\n\n        short_mode = \'val\' if mode == \'validation\' else mode\n        fc100_path = os.path.join(self.root, \'FC100_\' + short_mode + \'.pickle\')\n        with open(fc100_path, \'rb\') as f:\n            u = pickle._Unpickler(f)\n            u.encoding = \'latin1\'\n            archive = u.load()\n        self.images = archive[\'data\']\n        self.labels = archive[\'labels\']\n\n    def download(self):\n        archive_path = os.path.join(self.root, \'fc100.zip\')\n        print(\'Downloading FC100. (160Mb)\')\n        try:  # Download from Google Drive first\n            download_file_from_google_drive(GOOGLE_DRIVE_FILE_ID,\n                                            archive_path)\n            archive_file = zipfile.ZipFile(archive_path)\n            archive_file.extractall(self.root)\n            os.remove(archive_path)\n        except zipfile.BadZipFile:\n            download_file(DROPBOX_LINK, archive_path)\n            archive_file = zipfile.ZipFile(archive_path)\n            archive_file.extractall(self.root)\n            os.remove(archive_path)\n\n    def __getitem__(self, idx):\n        image = self.images[idx]\n        image = Image.fromarray(image)\n        label = self.labels[idx]\n        if self.transform is not None:\n            image = self.transform(image)\n        if self.target_transform is not None:\n            label = self.target_transform(label)\n        return image, label\n\n    def __len__(self):\n        return len(self.labels)\n\n    def _check_exists(self):\n        return os.path.exists(os.path.join(self.root,\n                                           \'FC100_train.pickle\'))\n\n\nif __name__ == \'__main__\':\n    dataset = FC100(root=\'~/data\')\n    img, tgt = dataset[43]\n    dataset = FC100(root=\'~/data\', mode=\'validation\')\n    img, tgt = dataset[43]\n    dataset = FC100(root=\'~/data\', mode=\'test\')\n    img, tgt = dataset[43]\n'"
learn2learn/vision/datasets/fgvc_aircraft.py,1,"b'#!/usr/bin/env python3\n\nimport os\nimport pickle\nimport tarfile\nimport requests\nfrom PIL import Image\n\nfrom torch.utils.data import Dataset\n\nDATASET_DIR = \'fgvc_aircraft\'\nDATASET_URL = \'http://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/archives/fgvc-aircraft-2013b.tar.gz\'\nDATA_DIR = os.path.join(\'fgvc-aircraft-2013b\', \'data\')\nIMAGES_DIR = os.path.join(DATA_DIR, \'images\')\nLABELS_PATH = os.path.join(DATA_DIR, \'labels.pkl\')\n\n# Splits from ""Meta-Datasets"", Triantafillou et al, 2019\nSPLITS = {\n    \'train\': [\'A340-300\', \'A318\', \'Falcon 2000\', \'F-16A/B\', \'F/A-18\', \'C-130\',\n              \'MD-80\', \'BAE 146-200\', \'777-200\', \'747-400\', \'Cessna 172\',\n              \'An-12\', \'A330-300\', \'A321\', \'Fokker 100\', \'Fokker 50\', \'DHC-1\',\n              \'Fokker 70\', \'A340-200\', \'DC-6\', \'747-200\', \'Il-76\', \'747-300\',\n              \'Model B200\', \'Saab 340\', \'Cessna 560\', \'Dornier 328\', \'E-195\',\n              \'ERJ 135\', \'747-100\', \'737-600\', \'C-47\', \'DR-400\', \'ATR-72\',\n              \'A330-200\', \'727-200\', \'737-700\', \'PA-28\', \'ERJ 145\', \'737-300\',\n              \'767-300\', \'737-500\', \'737-200\', \'DHC-6\', \'Falcon 900\', \'DC-3\',\n              \'Eurofighter Typhoon\', \'Challenger 600\', \'Hawk T1\', \'A380\',\n              \'777-300\', \'E-190\', \'DHC-8-100\', \'Cessna 525\', \'Metroliner\',\n              \'EMB-120\', \'Tu-134\', \'Embraer Legacy 600\', \'Gulfstream IV\',\n              \'Tu-154\', \'MD-87\', \'A300B4\', \'A340-600\', \'A340-500\', \'MD-11\',\n              \'707-320\', \'Cessna 208\', \'Global Express\', \'A319\', \'DH-82\'\n              ],\n    \'test\': [\'737-400\', \'737-800\', \'757-200\', \'767-400\', \'ATR-42\', \'BAE-125\',\n             \'Beechcraft 1900\', \'Boeing 717\', \'CRJ-200\', \'CRJ-700\', \'E-170\',\n             \'L-1011\', \'MD-90\', \'Saab 2000\', \'Spitfire\'\n             ],\n    \'valid\': [\'737-900\', \'757-300\', \'767-200\', \'A310\', \'A320\', \'BAE 146-300\',\n              \'CRJ-900\', \'DC-10\', \'DC-8\', \'DC-9-30\', \'DHC-8-300\', \'Gulfstream V\',\n              \'SR-20\', \'Tornado\', \'Yak-42\'\n              ],\n    \'all\': [\'A340-300\', \'A318\', \'Falcon 2000\', \'F-16A/B\', \'F/A-18\', \'C-130\',\n            \'MD-80\', \'BAE 146-200\', \'777-200\', \'747-400\', \'Cessna 172\',\n            \'An-12\', \'A330-300\', \'A321\', \'Fokker 100\', \'Fokker 50\', \'DHC-1\',\n            \'Fokker 70\', \'A340-200\', \'DC-6\', \'747-200\', \'Il-76\', \'747-300\',\n            \'Model B200\', \'Saab 340\', \'Cessna 560\', \'Dornier 328\', \'E-195\',\n            \'ERJ 135\', \'747-100\', \'737-600\', \'C-47\', \'DR-400\', \'ATR-72\',\n            \'A330-200\', \'727-200\', \'737-700\', \'PA-28\', \'ERJ 145\', \'737-300\',\n            \'767-300\', \'737-500\', \'737-200\', \'DHC-6\', \'Falcon 900\', \'DC-3\',\n            \'Eurofighter Typhoon\', \'Challenger 600\', \'Hawk T1\', \'A380\',\n            \'777-300\', \'E-190\', \'DHC-8-100\', \'Cessna 525\', \'Metroliner\',\n            \'EMB-120\', \'Tu-134\', \'Embraer Legacy 600\', \'Gulfstream IV\',\n            \'Tu-154\', \'MD-87\', \'A300B4\', \'A340-600\', \'A340-500\', \'MD-11\',\n            \'707-320\', \'Cessna 208\', \'Global Express\', \'A319\', \'DH-82\',\n            \'737-900\', \'757-300\', \'767-200\', \'A310\', \'A320\', \'BAE 146-300\',\n            \'CRJ-900\', \'DC-10\', \'DC-8\', \'DC-9-30\', \'DHC-8-300\', \'Gulfstream V\',\n            \'SR-20\', \'Tornado\', \'Yak-42\',\n            \'737-400\', \'737-800\', \'757-200\', \'767-400\', \'ATR-42\', \'BAE-125\',\n            \'Beechcraft 1900\', \'Boeing 717\', \'CRJ-200\', \'CRJ-700\', \'E-170\',\n            \'L-1011\', \'MD-90\', \'Saab 2000\', \'Spitfire\',\n            ],\n}\n\n\nclass FGVCAircraft(Dataset):\n\n    """"""\n    [[Source]](https://github.com/learnables/learn2learn/blob/master/learn2learn/vision/datasets/fgvc_aircraft.py)\n\n    **Description**\n\n    The FGVC Aircraft dataset was originally introduced by Maji et al., 2013 and then re-purposed for few-shot learning in Triantafillou et al., 2020.\n\n    The dataset consists of 10,200 images of aircraft (102 classes, each 100 images).\n    We provided the raw (un-processed) images and follow the train-validation-test splits of Triantafillou et al.\n\n    **References**\n\n    1. Maji et al. 2013. ""Fine-Grained Visual Classification of Aircraft."" arXiv [cs.CV].\n    2. Triantafillou et al. 2019. ""Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples."" ICLR \'20.\n    3. [http://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/](http://www.robots.ox.ac.uk/~vgg/data/fgvc-aircraft/)\n\n    **Arguments**\n\n    * **root** (str) - Path to download the data.\n    * **mode** (str, *optional*, default=\'train\') - Which split to use.\n        Must be \'train\', \'validation\', or \'test\'.\n    * **transform** (Transform, *optional*, default=None) - Input pre-processing.\n    * **target_transform** (Transform, *optional*, default=None) - Target pre-processing.\n    * **download** (bool, *optional*, default=False) - Whether to download the dataset.\n\n    **Example**\n\n    ~~~python\n    train_dataset = l2l.vision.datasets.FGVCAircraft(root=\'./data\', mode=\'train\', download=True)\n    train_dataset = l2l.data.MetaDataset(train_dataset)\n    train_generator = l2l.data.TaskDataset(dataset=train_dataset, num_tasks=1000)\n    ~~~\n\n    """"""\n\n    def __init__(self, root, mode=\'all\', transform=None, target_transform=None, download=False):\n        root = os.path.expanduser(root)\n        self.root = os.path.expanduser(root)\n        self.transform = transform\n        self.target_transform = target_transform\n        self._bookkeeping_path = os.path.join(self.root, \'fgvc-aircraft-\' + mode + \'-bookkeeping.pkl\')\n\n        if not self._check_exists() and download:\n            self.download()\n\n        assert mode in [\'train\', \'validation\', \'test\'], \\\n            \'mode should be one of train, validation, test.\'\n        self.load_data(mode)\n\n    def _check_exists(self):\n        data_path = os.path.join(self.root, DATASET_DIR)\n        images_path = os.path.join(data_path, IMAGES_DIR)\n        labels_path = os.path.join(data_path, LABELS_PATH)\n        return os.path.exists(data_path) and \\\n            os.path.exists(images_path) and \\\n            os.path.exists(labels_path)\n\n    def download(self):\n        if not os.path.exists(self.root):\n            os.mkdir(self.root)\n        data_path = os.path.join(self.root, DATASET_DIR)\n        if not os.path.exists(data_path):\n            os.mkdir(data_path)\n        tar_path = os.path.join(data_path, os.path.basename(DATASET_URL))\n        if not os.path.exists(tar_path):\n            print(\'Downloading FGVC Aircraft dataset. (2.75Gb)\')\n            req = requests.get(DATASET_URL)\n            with open(tar_path, \'wb\') as archive:\n                for chunk in req.iter_content(chunk_size=32768):\n                    if chunk:\n                        archive.write(chunk)\n        with tarfile.open(tar_path) as tar_file:\n            tar_file.extractall(data_path)\n        family_names = [\'images_family_train.txt\',\n                        \'images_family_val.txt\',\n                        \'images_family_test.txt\']\n        images_labels = []\n        for family in family_names:\n            with open(os.path.join(data_path, DATA_DIR, family_names[0]), \'r\') as family_file:\n                for line in family_file.readlines():\n                    image, label = line.split(\' \', 1)\n                    images_labels.append((image.strip(), label.strip()))\n        labels_path = os.path.join(data_path, LABELS_PATH)\n        with open(labels_path, \'wb\') as labels_file:\n            pickle.dump(images_labels, labels_file)\n        os.remove(tar_path)\n\n    def load_data(self, mode=\'train\'):\n        data_path = os.path.join(self.root, DATASET_DIR)\n        labels_path = os.path.join(data_path, LABELS_PATH)\n        with open(labels_path, \'rb\') as labels_file:\n            image_labels = pickle.load(labels_file)\n\n        data = []\n        mode = \'valid\' if mode == \'validation\' else mode\n        split = SPLITS[mode]\n        for image, label in image_labels:\n            if label in split:\n                image = os.path.join(data_path, IMAGES_DIR, image + \'.jpg\')\n                label = split.index(label)\n                data.append((image, label))\n        self.data = data\n\n    def __getitem__(self, i):\n        image, label = self.data[i]\n        image = Image.open(image)\n        if self.transform:\n            image = self.transform(image)\n        if self.target_transform:\n            label = self.target_transform(label)\n        return image, label\n\n    def __len__(self):\n        return len(self.data)\n\n\nif __name__ == \'__main__\':\n    assert len(SPLITS[\'all\']) == len(SPLITS[\'train\']) + len(SPLITS[\'valid\']) + len(SPLITS[\'test\'])\n    aircraft = FGVCAircraft(\'~/data\', download=True)\n    print(len(aircraft))\n'"
learn2learn/vision/datasets/full_omniglot.py,1,"b'#!/usr/bin/env python3\n\nimport os\nfrom torch.utils.data import Dataset, ConcatDataset\nfrom torchvision.datasets.omniglot import Omniglot\n\n\nclass FullOmniglot(Dataset):\n    """"""\n\n    [[Source]]()\n\n    **Description**\n\n    This class provides an interface to the Omniglot dataset.\n\n    The Omniglot dataset was introduced by Lake et al., 2015.\n    Omniglot consists of 1623 character classes from 50 different alphabets, each containing 20 samples.\n    While the original dataset is separated in background and evaluation sets,\n    this class concatenates both sets and leaves to the user the choice of classes splitting\n    as was done in Ravi and Larochelle, 2017.\n    The background and evaluation splits are available in the `torchvision` package.\n\n    **References**\n\n    1. Lake et al. 2015. \xe2\x80\x9cHuman-Level Concept Learning through Probabilistic Program Induction.\xe2\x80\x9d Science.\n    2. Ravi and Larochelle. 2017. \xe2\x80\x9cOptimization as a Model for Few-Shot Learning.\xe2\x80\x9d ICLR.\n\n    **Arguments**\n\n    * **root** (str) - Path to download the data.\n    * **transform** (Transform, *optional*, default=None) - Input pre-processing.\n    * **target_transform** (Transform, *optional*, default=None) - Target pre-processing.\n    * **download** (bool, *optional*, default=False) - Whether to download the dataset.\n\n    **Example**\n    ~~~python\n    omniglot = l2l.vision.datasets.FullOmniglot(root=\'./data\',\n                                                transform=transforms.Compose([\n                                                    transforms.Resize(28, interpolation=LANCZOS),\n                                                    transforms.ToTensor(),\n                                                    lambda x: 1.0 - x,\n                                                ]),\n                                                download=True)\n    omniglot = l2l.data.MetaDataset(omniglot)\n    ~~~\n\n    """"""\n\n    def __init__(self, root, transform=None, target_transform=None, download=False):\n        self.root = os.path.expanduser(root)\n        self.transform = transform\n        self.target_transform = target_transform\n\n        # Set up both the background and eval dataset\n        omni_background = Omniglot(self.root, background=True, download=download)\n        # Eval labels also start from 0.\n        # It\'s important to add 964 to label values in eval so they don\'t overwrite background dataset.\n        omni_evaluation = Omniglot(self.root,\n                                   background=False,\n                                   download=download,\n                                   target_transform=lambda x: x + len(omni_background._characters))\n\n        self.dataset = ConcatDataset((omni_background, omni_evaluation))\n        self._bookkeeping_path = os.path.join(self.root, \'omniglot-bookkeeping.pkl\')\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, item):\n        image, character_class = self.dataset[item]\n        if self.transform:\n            image = self.transform(image)\n\n        if self.target_transform:\n            character_class = self.target_transform(character_class)\n\n        return image, character_class\n'"
learn2learn/vision/datasets/mini_imagenet.py,2,"b'#!/usr/bin/env python3\n\nfrom __future__ import print_function\n\nimport os\nimport pickle\n\nimport numpy as np\nimport torch\nimport torch.utils.data as data\n\nfrom learn2learn.data.utils import download_file_from_google_drive, download_file\n\n\ndef download_pkl(google_drive_id, data_root, mode):\n    filename = \'mini-imagenet-cache-\' + mode\n    file_path = os.path.join(data_root, filename)\n\n    if not os.path.exists(file_path + \'.pkl\'):\n        print(\'Downloading:\', file_path + \'.pkl\')\n        download_file_from_google_drive(google_drive_id, file_path + \'.pkl\')\n    else:\n        print(""Data was already downloaded"")\n\n\ndef index_classes(items):\n    idx = {}\n    for i in items:\n        if (i not in idx):\n            idx[i] = len(idx)\n    return idx\n\n\nclass MiniImagenet(data.Dataset):\n    """"""\n    [[Source]](https://github.com/learnables/learn2learn/blob/master/learn2learn/vision/datasets/mini_imagenet.py)\n\n    **Description**\n\n    The *mini*-ImageNet dataset was originally introduced by Vinyals et al., 2016.\n\n\n    It consists of 60\'000 colour images of sizes 84x84 pixels.\n    The dataset is divided in 3 splits of 64 training, 16 validation, and 20 testing classes each containing 600 examples.\n    The classes are sampled from the ImageNet dataset, and we use the splits from Ravi & Larochelle, 2017.\n\n    **References**\n\n    1. Vinyals et al. 2016. \xe2\x80\x9cMatching Networks for One Shot Learning.\xe2\x80\x9d NeurIPS.\n    2. Ravi and Larochelle. 2017. \xe2\x80\x9cOptimization as a Model for Few-Shot Learning.\xe2\x80\x9d ICLR.\n\n    **Arguments**\n\n    * **root** (str) - Path to download the data.\n    * **mode** (str, *optional*, default=\'train\') - Which split to use.\n        Must be \'train\', \'validation\', or \'test\'.\n    * **transform** (Transform, *optional*, default=None) - Input pre-processing.\n    * **target_transform** (Transform, *optional*, default=None) - Target pre-processing.\n\n    **Example**\n\n    ~~~python\n    train_dataset = l2l.vision.datasets.MiniImagenet(root=\'./data\', mode=\'train\')\n    train_dataset = l2l.data.MetaDataset(train_dataset)\n    train_generator = l2l.data.TaskGenerator(dataset=train_dataset, ways=ways)\n    ~~~\n\n    """"""\n\n    def __init__(self,\n                 root,\n                 mode=\'train\',\n                 transform=None,\n                 target_transform=None,\n                 download=False):\n        super(MiniImagenet, self).__init__()\n        self.root = os.path.expanduser(root)\n        if not os.path.exists(self.root):\n            os.mkdir(self.root)\n        self.transform = transform\n        self.target_transform = target_transform\n        self.mode = mode\n        self._bookkeeping_path = os.path.join(self.root, \'mini-imagenet-bookkeeping-\' + mode + \'.pkl\')\n        if self.mode == \'test\':\n            google_drive_file_id = \'1wpmY-hmiJUUlRBkO9ZDCXAcIpHEFdOhD\'\n            dropbox_file_link = \'https://www.dropbox.com/s/ye9jeb5tyz0x01b/mini-imagenet-cache-test.pkl?dl=1\'\n        elif self.mode == \'train\':\n            google_drive_file_id = \'1I3itTXpXxGV68olxM5roceUMG8itH9Xj\'\n            dropbox_file_link = \'https://www.dropbox.com/s/9g8c6w345s2ek03/mini-imagenet-cache-train.pkl?dl=1\'\n        elif self.mode == \'validation\':\n            google_drive_file_id = \'1KY5e491bkLFqJDp0-UWou3463Mo8AOco\'\n            dropbox_file_link = \'https://www.dropbox.com/s/ip1b7se3gij3r1b/mini-imagenet-cache-validation.pkl?dl=1\'\n        else:\n            raise (\'ValueError\', \'Needs to be train, test or validation\')\n\n        pickle_file = os.path.join(self.root, \'mini-imagenet-cache-\' + mode + \'.pkl\')\n        try:\n            if not self._check_exists() and download:\n                print(\'Downloading mini-ImageNet --\', mode)\n                download_pkl(google_drive_file_id, self.root, mode)\n            with open(pickle_file, \'rb\') as f:\n                self.data = pickle.load(f)\n        except pickle.UnpicklingError:\n            if not self._check_exists() and download:\n                print(\'Download failed. Re-trying mini-ImageNet --\', mode)\n                download_file(dropbox_file_link, pickle_file)\n            with open(pickle_file, \'rb\') as f:\n                self.data = pickle.load(f)\n\n        self.x = torch.from_numpy(self.data[""image_data""]).permute(0, 3, 1, 2).float()\n        self.y = np.ones(len(self.x))\n\n        # TODO Remove index_classes from here\n        self.class_idx = index_classes(self.data[\'class_dict\'].keys())\n        for class_name, idxs in self.data[\'class_dict\'].items():\n            for idx in idxs:\n                self.y[idx] = self.class_idx[class_name]\n\n    def __getitem__(self, idx):\n        data = self.x[idx]\n        if self.transform:\n            data = self.transform(data)\n        return data, self.y[idx]\n\n    def __len__(self):\n        return len(self.x)\n\n    def _check_exists(self):\n        return os.path.exists(os.path.join(self.root, \'mini-imagenet-cache-\' + self.mode + \'.pkl\'))\n\n\nif __name__ == \'__main__\':\n    mi = MiniImagenet(root=\'./data\', download=True)\n    __import__(\'pdb\').set_trace()\n'"
learn2learn/vision/datasets/tiered_imagenet.py,1,"b'#!/usr/bin/env python3\n\nimport os\nimport io\nimport pickle\nimport tarfile\n\nimport numpy as np\nimport torch\nimport torch.utils.data as data\n\nfrom PIL import Image\n\nfrom learn2learn.data.utils import download_file_from_google_drive\n\n\nclass TieredImagenet(data.Dataset):\n    """"""\n    [[Source]](https://github.com/learnables/learn2learn/blob/master/learn2learn/vision/datasets/tiered_imagenet.py)\n\n    **Description**\n\n    The *tiered*-ImageNet dataset was originally introduced by Ren et al, 2018 and we download the data directly from the link provided in their repository.\n\n    Like *mini*-ImageNet, *tiered*-ImageNet builds on top of ILSVRC-12, but consists of 608 classes (779,165 images) instead of 100.\n    The train-validation-test split is made such that classes from similar categories are in the same splits.\n    There are 34 categories each containing between 10 and 30 classes.\n    Of these categories, 20 (351 classes; 448,695 images) are used for training,\n    6 (97 classes; 124,261 images) for validation, and 8 (160 class; 206,209 images) for testing.\n\n    **References**\n\n    1. Ren et al, 2018. ""Meta-Learning for Semi-Supervised Few-Shot Classification."" ICLR \'18.\n    2. Ren Mengye. 2018. ""few-shot-ssl-public"". [https://github.com/renmengye/few-shot-ssl-public](https://github.com/renmengye/few-shot-ssl-public)\n\n    **Arguments**\n\n    * **root** (str) - Path to download the data.\n    * **mode** (str, *optional*, default=\'train\') - Which split to use.\n        Must be \'train\', \'validation\', or \'test\'.\n    * **transform** (Transform, *optional*, default=None) - Input pre-processing.\n    * **target_transform** (Transform, *optional*, default=None) - Target pre-processing.\n    * **download** (bool, *optional*, default=False) - Whether to download the dataset.\n\n    **Example**\n\n    ~~~python\n    train_dataset = l2l.vision.datasets.TieredImagenet(root=\'./data\', mode=\'train\', download=True)\n    train_dataset = l2l.data.MetaDataset(train_dataset)\n    train_generator = l2l.data.TaskDataset(dataset=train_dataset, num_tasks=1000)\n    ~~~\n\n    """"""\n\n    def __init__(self, root, mode=\'train\', transform=None, target_transform=None, download=False):\n        super(TieredImagenet, self).__init__()\n        self.root = os.path.expanduser(root)\n        if not os.path.exists(self.root):\n            os.mkdir(self.root)\n        self.transform = transform\n        self.target_transform = target_transform\n        if mode not in [\'train\', \'validation\', \'test\']:\n            raise ValueError(\'mode must be train, validation, or test.\')\n        self.mode = mode\n        self._bookkeeping_path = os.path.join(self.root, \'tiered-imagenet-bookkeeping-\' + mode + \'.pkl\')\n        google_drive_file_id = \'1g1aIDy2Ar_MViF2gDXFYDBTR-HYecV07\'\n\n        if not self._check_exists() and download:\n            self.download(google_drive_file_id, self.root)\n\n        short_mode = \'val\' if mode == \'validation\' else mode\n        tiered_imaganet_path = os.path.join(self.root, \'tiered-imagenet\')\n        images_path = os.path.join(tiered_imaganet_path, short_mode + \'_images_png.pkl\')\n        with open(images_path, \'rb\') as images_file:\n            self.images = pickle.load(images_file)\n        labels_path = os.path.join(tiered_imaganet_path, short_mode + \'_labels.pkl\')\n        with open(labels_path, \'rb\') as labels_file:\n            self.labels = pickle.load(labels_file)\n            self.labels = self.labels[\'label_specific\']\n\n    def download(self, file_id, destination):\n        archive_path = os.path.join(destination, \'tiered_imagenet.tar\')\n        print(\'Downloading tiered ImageNet. (12Gb) Please be patient.\')\n        download_file_from_google_drive(file_id, archive_path)\n        archive_file = tarfile.open(archive_path)\n        archive_file.extractall(destination)\n        os.remove(archive_path)\n\n    def __getitem__(self, idx):\n        image = Image.open(io.BytesIO(self.images[idx]))\n        label = self.labels[idx]\n        if self.transform is not None:\n            image = self.transform(image)\n        if self.target_transform is not None:\n            label = self.target_transform(label)\n        return image, label\n\n    def __len__(self):\n        return len(self.labels)\n\n    def _check_exists(self):\n        return os.path.exists(os.path.join(self.root,\n                                           \'tiered-imagenet\',\n                                           \'train_images_png.pkl\'))\n\n\nif __name__ == \'__main__\':\n    dataset = TieredImagenet(root=\'~/data\')\n    img, tgt = dataset[43]\n    dataset = TieredImagenet(root=\'~/data\', mode=\'validation\')\n    img, tgt = dataset[43]\n    dataset = TieredImagenet(root=\'~/data\', mode=\'test\')\n    img, tgt = dataset[43]\n'"
learn2learn/vision/datasets/vgg_flowers.py,1,"b'#!/usr/bin/env python3\n\nimport os\nimport tarfile\nimport requests\nimport scipy.io\nfrom PIL import Image\n\nfrom torch.utils.data import Dataset\n\nDATA_DIR = \'vgg_flower102\'\nIMAGES_URL = \'http://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\'\nLABELS_URL = \'http://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat\'\nIMAGES_DIR = \'jpg\'\nLABELS_PATH = \'imagelabels.mat\'\n\n# Splits from ""Meta-Datasets"", Triantafillou et al, 2019\nSPLITS = {\n    \'train\': [90, 38, 80, 30, 29, 12, 43, 27, 4, 64, 31, 99, 8, 67, 95, 77,\n              78, 61, 88, 74, 55, 32, 21, 13, 79, 70, 51, 69, 14, 60, 11, 39,\n              63, 37, 36, 28, 48, 7, 93, 2, 18, 24, 6, 3, 44, 76, 75, 72, 52,\n              84, 73, 34, 54, 66, 59, 50, 91, 68, 100, 71, 81, 101, 92, 22,\n              33, 87, 1, 49, 20, 25, 58],\n    \'validation\': [10, 16, 17, 23, 26, 47, 53, 56, 57, 62, 82, 83, 86, 97, 102],\n    \'test\': [5, 9, 15, 19, 35, 40, 41, 42, 45, 46, 65, 85, 89, 94, 96, 98],\n    \'all\': list(range(1, 103)),\n}\n\n\nclass VGGFlower102(Dataset):\n\n    """"""\n    [[Source]](https://github.com/learnables/learn2learn/blob/master/learn2learn/vision/datasets/vgg_flowers.py)\n\n    **Description**\n\n    The VGG Flowers dataset was originally introduced by Maji et al., 2013 and then re-purposed for few-shot learning in Triantafillou et al., 2020.\n\n    The dataset consists of 102 classes of flowers, with each class consisting of 40 to 258 images.\n    We provide the raw (unprocessed) images, and follow the train-validation-test splits of Triantafillou et al.\n\n    **References**\n\n    1. Nilsback, M. and A. Zisserman. 2006. ""A Visual Vocabulary for Flower Classification."" CVPR \'06.\n    2. Triantafillou et al. 2019. ""Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples."" ICLR \'20.\n    3. [https://www.robots.ox.ac.uk/~vgg/data/flowers/](https://www.robots.ox.ac.uk/~vgg/data/flowers/)\n\n    **Arguments**\n\n    * **root** (str) - Path to download the data.\n    * **mode** (str, *optional*, default=\'train\') - Which split to use.\n        Must be \'train\', \'validation\', or \'test\'.\n    * **transform** (Transform, *optional*, default=None) - Input pre-processing.\n    * **target_transform** (Transform, *optional*, default=None) - Target pre-processing.\n    * **download** (bool, *optional*, default=False) - Whether to download the dataset.\n\n    **Example**\n\n    ~~~python\n    train_dataset = l2l.vision.datasets.VGGFlower102(root=\'./data\', mode=\'train\')\n    train_dataset = l2l.data.MetaDataset(train_dataset)\n    train_generator = l2l.data.TaskDataset(dataset=train_dataset, num_tasks=1000)\n    ~~~\n\n    """"""\n\n    def __init__(self, root, mode=\'all\', transform=None, target_transform=None, download=False):\n        root = os.path.expanduser(root)\n        self.root = os.path.expanduser(root)\n        self.transform = transform\n        self.target_transform = target_transform\n        self._bookkeeping_path = os.path.join(self.root, \'vgg-flower102-\' + mode + \'-bookkeeping.pkl\')\n\n        if not self._check_exists() and download:\n            self.download()\n\n        self.load_data(mode)\n\n    def _check_exists(self):\n        data_path = os.path.join(self.root, DATA_DIR)\n        return os.path.exists(data_path)\n\n    def download(self):\n        if not os.path.exists(self.root):\n            os.mkdir(self.root)\n        data_path = os.path.join(self.root, DATA_DIR)\n        if not os.path.exists(data_path):\n            os.mkdir(data_path)\n        tar_path = os.path.join(data_path, os.path.basename(IMAGES_URL))\n        print(\'Downloading VGG Flower102 dataset\')\n        req = requests.get(IMAGES_URL)\n        with open(tar_path, \'wb\') as archive:\n            archive.write(req.content)\n        tar_file = tarfile.open(tar_path)\n        tar_file.extractall(data_path)\n        tar_file.close()\n        os.remove(tar_path)\n\n        label_path = os.path.join(data_path, os.path.basename(LABELS_URL))\n        req = requests.get(LABELS_URL)\n        with open(label_path, \'wb\') as label_file:\n            label_file.write(req.content)\n\n    def load_data(self, mode=\'train\'):\n        data_path = os.path.join(self.root, DATA_DIR)\n        images_path = os.path.join(data_path, IMAGES_DIR)\n        labels_path = os.path.join(data_path, LABELS_PATH)\n        labels_mat = scipy.io.loadmat(labels_path)\n        image_labels = []\n        split = SPLITS[mode]\n        for idx, label in enumerate(labels_mat[\'labels\'][0], start=1):\n            if label in split:\n                image = str(idx).zfill(5)\n                image = f\'image_{image}.jpg\'\n                image = os.path.join(images_path, image)\n                label = split.index(label)\n                image_labels.append((image, label))\n        self.data = image_labels\n\n    def __getitem__(self, i):\n        image, label = self.data[i]\n        image = Image.open(image)\n        if self.transform:\n            image = self.transform(image)\n        if self.target_transform:\n            label = self.target_transform(label)\n        return image, label\n\n    def __len__(self):\n        return len(self.data)\n\n\nif __name__ == \'__main__\':\n    assert len(SPLITS[\'train\']) == 71\n    assert len(SPLITS[\'validation\']) == 15\n    assert len(SPLITS[\'test\']) == 16\n    assert len(SPLITS[\'all\']) == 102\n    flowers = VGGFlower102(\'~/vgg_data\', download=True)\n    print(len(flowers))\n'"
tests/unit/algorithms/__init__.py,0,b''
tests/unit/algorithms/maml_test.py,10,"b""#!/usr/bin/env python3\n\nimport unittest\nimport torch\nimport learn2learn as l2l\n\nNUM_INPUTS = 7\nINPUT_SIZE = 10\nHIDDEN_SIZE = 20\nINNER_LR = 0.01\nEPSILON = 1e-8\n\n\ndef close(x, y):\n    return (x - y).norm(p=2) <= EPSILON\n\n\nclass TestMAMLAlgorithm(unittest.TestCase):\n\n    def setUp(self):\n        self.model = torch.nn.Sequential(torch.nn.Linear(INPUT_SIZE, HIDDEN_SIZE),\n                                          torch.nn.ReLU(),\n                                          torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE),\n                                          torch.nn.Sigmoid(),\n                                          torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE),\n                                          torch.nn.Softmax())\n\n        self.model.register_buffer('dummy_buf', torch.zeros(1, 2, 3, 4))\n\n    def tearDown(self):\n        pass\n\n    def test_clone_module(self):\n        for first_order in [False, True]:\n            maml = l2l.algorithms.MAML(self.model,\n                                       lr=INNER_LR,\n                                       first_order=first_order)\n            X = torch.randn(NUM_INPUTS, INPUT_SIZE)\n            ref = self.model(X)\n            for clone in [maml.clone(), maml.clone()]:\n                out = clone(X)\n                self.assertTrue(close(ref, out))\n\n    def test_graph_connection(self):\n        maml = l2l.algorithms.MAML(self.model,\n                                   lr=INNER_LR,\n                                   first_order=False)\n        X = torch.randn(NUM_INPUTS, INPUT_SIZE)\n        ref = maml(X)\n        clone = maml.clone()\n        out = clone(X)\n        out.norm(p=2).backward()\n        for p in self.model.parameters():\n            self.assertTrue(hasattr(p, 'grad'))\n            self.assertTrue(p.grad.norm(p=2).item() > 0.0)\n\n    def test_adaptation(self):\n        maml = l2l.algorithms.MAML(self.model,\n                                   lr=INNER_LR,\n                                   first_order=False)\n        X = torch.randn(NUM_INPUTS, INPUT_SIZE)\n        clone = maml.clone()\n        loss = clone(X).norm(p=2)\n        clone.adapt(loss)\n        new_loss = clone(X).norm(p=2)\n        self.assertTrue(loss >= new_loss)\n        new_loss.backward()\n        for p in self.model.parameters():\n            self.assertTrue(hasattr(p, 'grad'))\n            self.assertTrue(p.grad.norm(p=2).item() > 0.0)\n\n    def test_allow_unused(self):\n        maml = l2l.algorithms.MAML(self.model,\n                                   lr=INNER_LR,\n                                   first_order=False,\n                                   allow_unused=True)\n        clone = maml.clone()\n        loss = 0.0\n        for i, p in enumerate(clone.parameters()):\n            if i % 2 == 0:\n                loss += p.norm(p=2)\n        clone.adapt(loss)\n        loss = 0.0\n        for i, p in enumerate(clone.parameters()):\n            if i % 2 == 0:\n                loss += p.norm(p=2)\n        loss.backward()\n        for p in maml.parameters():\n            self.assertTrue(hasattr(p, 'grad'))\n\n    def test_allow_nograd(self):\n        self.model[2].weight.requires_grad = False\n        maml = l2l.algorithms.MAML(self.model,\n                                   lr=INNER_LR,\n                                   first_order=False,\n                                   allow_unused=False,\n                                   allow_nograd=False)\n        clone = maml.clone()\n\n        loss = sum([p.norm(p=2) for p in clone.parameters()])\n        try:\n            # Check that without allow_nograd, adaptation fails\n            clone.adapt(loss)\n            self.assertTrue(False, 'adaptation successful despite requires_grad=False')  # Check that execution never gets here\n        except:\n            # Check that with allow_nograd, adaptation succeeds\n            clone.adapt(loss, allow_nograd=True)\n            loss = sum([p.norm(p=2) for p in clone.parameters()])\n            loss.backward()\n            self.assertTrue(self.model[2].weight.grad is None)\n            for p in self.model.parameters():\n                if p.requires_grad:\n                    self.assertTrue(p.grad is not None)\n\n        maml = l2l.algorithms.MAML(self.model,\n                                   lr=INNER_LR,\n                                   first_order=False,\n                                   allow_nograd=True)\n        clone = maml.clone()\n        loss = sum([p.norm(p=2) for p in clone.parameters()])\n        # Check that without allow_nograd, adaptation succeeds thanks to init.\n        orig_weight = self.model[2].weight.clone().detach()\n        clone.adapt(loss)\n        self.assertTrue(close(orig_weight, self.model[2].weight))\n\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/unit/algorithms/metasgd_test.py,10,"b""#!/usr/bin/env python3\n\nimport unittest\n\nimport torch\n\nimport learn2learn as l2l\n\nNUM_INPUTS = 7\nINPUT_SIZE = 10\nHIDDEN_SIZE = 20\nINNER_LR = 0.1\nEPSILON = 1e-8\n\n\ndef close(x, y):\n    return (x - y).norm(p=2) <= EPSILON\n\n\nclass TestMetaSGDAlgorithm(unittest.TestCase):\n\n    def setUp(self):\n        self.model = torch.nn.Sequential(torch.nn.Linear(INPUT_SIZE, HIDDEN_SIZE),\n                                         torch.nn.ReLU(),\n                                         torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE),\n                                         torch.nn.Sigmoid(),\n                                         torch.nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE),\n                                         torch.nn.Softmax())\n        self.model.register_buffer('dummy_buf', torch.zeros(1, 2, 3, 4))\n\n    def tearDown(self):\n        pass\n\n    def test_clone_module(self):\n        for first_order in [False, True]:\n            meta = l2l.algorithms.MetaSGD(self.model,\n                                          lr=INNER_LR,\n                                          first_order=first_order)\n            X = torch.randn(NUM_INPUTS, INPUT_SIZE)\n            ref = self.model(X)\n            for clone in [meta.clone(), meta.clone()]:\n                out = clone(X)\n                self.assertTrue(close(ref, out))\n\n    def test_graph_connection(self):\n        meta = l2l.algorithms.MetaSGD(self.model,\n                                      lr=INNER_LR,\n                                      first_order=False)\n        X = torch.randn(NUM_INPUTS, INPUT_SIZE)\n        ref = meta(X)\n        clone = meta.clone()\n        out = clone(X)\n        out.norm(p=2).backward()\n        for p in self.model.parameters():\n            self.assertTrue(hasattr(p, 'grad'))\n            self.assertTrue(p.grad.norm(p=2).item() > 0.0)\n\n    def test_meta_lr(self):\n        meta = l2l.algorithms.MetaSGD(self.model,\n                                      lr=INNER_LR,\n                                      first_order=False)\n        num_params = sum([p.numel() for p in self.model.parameters()])\n        meta_params = sum([p.numel() for p in meta.parameters()])\n        self.assertEqual(2 * num_params, meta_params)\n        for lr in meta.lrs:\n            self.assertTrue(close(lr, INNER_LR))\n\n    def test_adaptation(self):\n        meta = l2l.algorithms.MetaSGD(self.model,\n                                      lr=INNER_LR,\n                                      first_order=False)\n        X = torch.randn(NUM_INPUTS, INPUT_SIZE)\n        clone = meta.clone()\n        loss = clone(X).norm(p=2)\n        clone.adapt(loss)\n        new_loss = clone(X).norm(p=2)\n        self.assertTrue(loss >= new_loss)\n        new_loss.backward()\n        for p in meta.parameters():\n            self.assertTrue(hasattr(p, 'grad'))\n            self.assertTrue(p.grad.norm(p=2).item() > 0.0)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/unit/data/__init__.py,0,b''
tests/unit/data/metadataset_test.py,0,"b""#!/usr/bin/env python3\n\nimport unittest\nfrom unittest import TestCase\n\nimport numpy as np\nfrom numpy.testing import assert_array_equal\n\nfrom learn2learn.data import MetaDataset\nfrom .util_datasets import TestDatasets\n\n\nclass TestMetaDataset(TestCase):\n    @classmethod\n    def setUpClass(cls) -> None:\n        cls.ds = TestDatasets()\n        cls.meta_tensor_dataset = MetaDataset(cls.ds.tensor_dataset)\n        cls.meta_str_dataset = MetaDataset(cls.ds.str_dataset)\n        cls.meta_alpha_dataset = MetaDataset(cls.ds.alphabet_dataset)\n        cls.mnist_dataset = MetaDataset(cls.ds.get_mnist())\n        cls.omniglot_dataset = MetaDataset(cls.ds.get_omniglot())\n\n    def test_fails_with_non_torch_dataset(self):\n        try:\n            MetaDataset(np.random.randn(100, 100))\n            return False\n        except TypeError:\n            return True\n        finally:\n            return False\n\n    def test_data_length(self):\n        self.assertEqual(len(self.meta_tensor_dataset), self.ds.n)\n        self.assertEqual(len(self.meta_str_dataset), self.ds.n)\n        self.assertEqual(len(self.meta_alpha_dataset), 26)\n        self.assertEqual(len(self.mnist_dataset), 60000)\n        self.assertEqual(len(self.omniglot_dataset), 32460)\n\n    def test_data_labels_length(self):\n        self.assertEqual(len(self.meta_tensor_dataset.labels), len(self.ds.tensor_classes))\n        self.assertEqual(len(self.meta_str_dataset.labels), len(self.ds.str_classes))\n        self.assertEqual(len(self.meta_alpha_dataset.labels), 26)\n        self.assertEqual(len(self.mnist_dataset.labels), len(self.ds.mnist_classes))\n        self.assertEqual(len(self.omniglot_dataset.labels), len(self.ds.omniglot_classes))\n\n    def test_data_labels_values(self):\n        self.assertEqual(sorted(self.meta_tensor_dataset.labels), sorted(self.ds.tensor_classes))\n        self.assertEqual(sorted(self.meta_str_dataset.labels), sorted(self.ds.str_classes))\n        self.assertEqual(sorted(self.meta_alpha_dataset.labels), sorted(self.ds.alphabets))\n        self.assertEqual(sorted(self.omniglot_dataset.labels), sorted(self.ds.omniglot_classes))\n\n    def test_get_item(self):\n        for i in range(5):\n            rand_index = np.random.randint(0, 26)\n            data, label = self.meta_alpha_dataset[rand_index]\n            assert_array_equal(data, [rand_index for _ in range(self.ds.features)])\n            self.assertEqual(label, chr(97 + rand_index))\n\n    def test_labels_to_indices(self):\n        self.meta_alpha_dataset.create_bookkeeping()\n        dict_label_to_indices = self.meta_alpha_dataset.labels_to_indices\n        self.assertEqual(sorted(list(dict_label_to_indices.keys())), self.ds.alphabets)\n        for key in dict_label_to_indices:\n            self.assertEqual(dict_label_to_indices[key][0], ord(key) - 97)\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/unit/data/task_dataset_test.py,11,"b""#!/usr/bin/env python3\n\nimport random\nimport unittest\nfrom unittest import TestCase\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\n\nfrom learn2learn.data import MetaDataset, TaskDataset\nfrom learn2learn.data.transforms import LoadData\n\n\nNUM_TASKS = 10\nNUM_DATA = 128\nX_SHAPE = 16\nY_SHAPE = 10\nEPSILON = 1e-6\nSUBSET_SIZE = 5\nWORKERS = 4\nMETA_BSZ = 16\n\n\ndef task_equal(A, B):\n    diff = 0.0\n    for a, b in zip(A, B):\n        diff += (a-b).pow(2).sum().item()\n    return diff < EPSILON\n\ndef random_subset(task_description):\n    return random.choices(task_description, k=SUBSET_SIZE)\n\n\nclass TestTaskDataset(TestCase):\n\n    def test_instanciation(self):\n        data = torch.randn(NUM_DATA, X_SHAPE)\n        labels = torch.randint(0, Y_SHAPE, (NUM_DATA, ))\n        dataset = TensorDataset(data, labels)\n        task_dataset = TaskDataset(dataset, task_transforms=[LoadData(dataset)], num_tasks=NUM_TASKS)\n        self.assertEqual(len(task_dataset), NUM_TASKS)\n\n    def test_task_caching(self):\n        data = torch.randn(NUM_DATA, X_SHAPE)\n        labels = torch.randint(0, Y_SHAPE, (NUM_DATA, ))\n        dataset = TensorDataset(data, labels)\n        task_dataset = TaskDataset(dataset, task_transforms=[LoadData(dataset)], num_tasks=NUM_TASKS)\n        tasks = []\n        for i, task in enumerate(task_dataset, 1):\n            tasks.append(task)\n        self.assertEqual(i, NUM_TASKS)\n        for ref, task in zip(tasks, task_dataset):\n            self.assertTrue(task_equal(ref, task))\n\n        for i in range(NUM_TASKS):\n            ref = tasks[i]\n            task = task_dataset[i]\n            self.assertTrue(task_equal(ref, task))\n\n    def test_infinite_tasks(self):\n        data = torch.randn(NUM_DATA, X_SHAPE)\n        labels = torch.randint(0, Y_SHAPE, (NUM_DATA, ))\n        dataset = TensorDataset(data, labels)\n        task_dataset = TaskDataset(dataset, task_transforms=[LoadData(dataset), random_subset])\n        self.assertEqual(len(task_dataset), 1)\n        prev = task_dataset.sample()\n        for i, task in enumerate(task_dataset):\n            self.assertFalse(task_equal(prev, task))\n            prev = task\n            if i > 4:\n                break\n\n    def test_task_transforms(self):\n        data = torch.randn(NUM_DATA, X_SHAPE)\n        labels = torch.randint(0, Y_SHAPE, (NUM_DATA, ))\n        dataset = TensorDataset(data, labels)\n        task_dataset = TaskDataset(dataset,\n                                   task_transforms=[LoadData(dataset), random_subset],\n                                   num_tasks=NUM_TASKS)\n        for task in task_dataset:\n            # Tests transforms on the task_description\n            self.assertEqual(len(task[0]), SUBSET_SIZE)\n            self.assertEqual(len(task[1]), SUBSET_SIZE)\n\n            # Tests transforms on the data\n            self.assertEqual(task[0].size(1), X_SHAPE)\n            self.assertLessEqual(task[1].max(), Y_SHAPE - 1)\n            self.assertGreaterEqual(task[1].max(), 0)\n\n    def test_dataloader(self):\n        data = torch.randn(NUM_DATA, X_SHAPE)\n        labels = torch.randint(0, Y_SHAPE, (NUM_DATA, ))\n        dataset = TensorDataset(data, labels)\n        task_dataset = TaskDataset(dataset,\n                                   task_transforms=[LoadData(dataset), random_subset],\n                                   num_tasks=NUM_TASKS)\n        task_loader = DataLoader(task_dataset,\n                                 shuffle=True,\n                                 batch_size=META_BSZ,\n                                 num_workers=WORKERS,\n                                 drop_last=True)\n        for task_batch in task_loader:\n            self.assertEqual(task_batch[0].shape, (META_BSZ, X_SHAPE))\n            self.assertEqual(task_batch[1].shape, (META_BSZ, 1))\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/unit/data/transforms_test.py,13,"b""#!/usr/bin/env python3\n\nimport random\nimport unittest\nfrom unittest import TestCase\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import TensorDataset\n\nfrom learn2learn.data import MetaDataset, TaskDataset\nfrom learn2learn.data.transforms import NWays, KShots, LoadData, FilterLabels, RemapLabels\n\nNUM_TASKS = 128\nNUM_DATA = 512\nX_SHAPE = 16\nY_SHAPE = 10\nEPSILON = 1e-6\nSUBSET_SIZE = 5\nWORKERS = 4\nMETA_BSZ = 16\n\n\nclass TestTransforms(TestCase):\n\n    def test_n_ways(self):\n        data = torch.randn(NUM_DATA, X_SHAPE)\n        labels = torch.randint(0, Y_SHAPE, (NUM_DATA, ))\n        dataset = MetaDataset(TensorDataset(data, labels))\n        for ways in range(1, 10):\n            task_dataset = TaskDataset(dataset, \n                                       task_transforms=[NWays(dataset, n=ways), LoadData(dataset)],\n                                       num_tasks=NUM_TASKS)\n            for task in task_dataset:\n                bins = task[1].bincount()\n                num_classes = len(bins) - (bins == 0).sum()\n                self.assertEqual(num_classes, ways)\n\n    def test_k_shots(self):\n        data = torch.randn(NUM_DATA, X_SHAPE)\n        labels = torch.randint(0, Y_SHAPE, (NUM_DATA, ))\n        dataset = MetaDataset(TensorDataset(data, labels))\n        for replacement in [False, True]:\n            for shots in range(1, 10):\n                task_dataset = TaskDataset(dataset, \n                                           task_transforms=[KShots(dataset, k=shots, replacement=replacement),\n                                                            LoadData(dataset)],\n                                           num_tasks=NUM_TASKS)\n                for task in task_dataset:\n                    bins = task[1].bincount()\n                    correct = (bins == shots).sum()\n                    self.assertEqual(correct, Y_SHAPE)\n\n    def test_load_data(self):\n        data = torch.randn(NUM_DATA, X_SHAPE)\n        labels = torch.randint(0, Y_SHAPE, (NUM_DATA, ))\n        dataset = MetaDataset(TensorDataset(data, labels))\n        task_dataset = TaskDataset(dataset, \n                                   task_transforms=[LoadData(dataset)],\n                                   num_tasks=NUM_TASKS)\n        for task in task_dataset:\n            self.assertTrue(isinstance(task[0], torch.Tensor))\n            self.assertTrue(isinstance(task[1], torch.Tensor))\n\n    def test_filter_labels(self):\n        data = torch.randn(NUM_DATA, X_SHAPE)\n        labels = torch.randint(0, Y_SHAPE, (NUM_DATA, ))\n        chosen_labels = random.sample(list(range(Y_SHAPE)), k=Y_SHAPE//2)\n        dataset = MetaDataset(TensorDataset(data, labels))\n        task_dataset = TaskDataset(dataset, \n                                   task_transforms=[FilterLabels(dataset, chosen_labels), LoadData(dataset)],\n                                   num_tasks=NUM_TASKS)\n        for task in task_dataset:\n            for label in task[1]:\n                self.assertTrue(label in chosen_labels)\n\n    def test_remap_labels(self):\n        data = torch.randn(NUM_DATA, X_SHAPE)\n        labels = torch.randint(0, Y_SHAPE, (NUM_DATA, ))\n        dataset = MetaDataset(TensorDataset(data, labels))\n        for ways in range(1, 5):\n            task_dataset = TaskDataset(dataset,\n                                       task_transforms=[NWays(dataset, ways),\n                                                        LoadData(dataset),\n                                                        RemapLabels(dataset)],\n                                       num_tasks=NUM_TASKS)\n            for task in task_dataset:\n                for label in range(ways):\n                    self.assertTrue(label in task[1])\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/unit/data/util_datasets.py,3,"b'import string\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, TensorDataset\nfrom torchvision.datasets import MNIST\n\nfrom learn2learn.vision.datasets import FullOmniglot\n\n\nclass TempDataset(Dataset):\n    def __init__(self, data, labels):\n        self.data = data\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, item):\n        return self.data[item], self.labels[item]\n\n\nclass TestDatasets():\n    def __init__(self):\n        self.download_location = ""/tmp/datasets""\n        self.n = 1500\n        self.features = 10\n\n        self.tensor_classes = [0, 1, 2, 3, 4]\n        self.str_classes = [""0"", ""1"", ""2"", ""3"", ""4""]\n        self.alphabets = list(string.ascii_lowercase)\n        self.mnist_classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n        self.omniglot_classes = [i for i in range(1623)]\n\n        tensor_data = torch.from_numpy(np.random.randn(self.n, self.features))\n        tensor_labels = torch.from_numpy(np.random.choice(self.tensor_classes, self.n))\n\n        str_data = np.random.randn(self.n, self.features)\n        str_labels = np.random.choice(self.str_classes, self.n)\n\n        alphabet_data = np.repeat(np.arange(26), self.features).reshape(-1, self.features)\n\n        self.tensor_dataset = TensorDataset(tensor_data, tensor_labels)\n        self.str_dataset = TempDataset(str_data, str_labels)\n        self.alphabet_dataset = TempDataset(alphabet_data, self.alphabets)\n\n    def get_mnist(self):\n        return MNIST(self.download_location, train=True, download=True)\n\n    def get_omniglot(self):\n        return FullOmniglot(root=self.download_location, download=True)\n'"
tests/unit/vision/__init__.py,0,b''
tests/unit/vision/benchmarks_test.py,0,"b""#!/usr/bin/env python3\n\nimport unittest\nimport learn2learn as l2l\n\nTOO_BIG_TO_TEST = [\n    'tiered-imagenet',\n]\n\n\nclass UtilTests(unittest.TestCase):\n\n    def setUp(self):\n        pass\n\n    def tearDown(self):\n        pass\n\n    def test_tasksets(self):\n        names = l2l.vision.benchmarks.list_tasksets()\n        for name in names:\n            if name in TOO_BIG_TO_TEST:\n                continue\n            tasksets = l2l.vision.benchmarks.get_tasksets(name, root='./data')\n            self.assertTrue(hasattr(tasksets, 'train'))\n            batch = tasksets.train.sample()\n            self.assertTrue(batch is not None)\n            self.assertTrue(hasattr(tasksets, 'validation'))\n            batch = tasksets.validation.sample()\n            self.assertTrue(batch is not None)\n            self.assertTrue(hasattr(tasksets, 'test'))\n            batch = tasksets.test.sample()\n            self.assertTrue(batch is not None)\n            del tasksets, batch\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/unit/vision/cifarfs_test.py,0,"b""#!/usr/bin/env python3\n\nimport os\nimport unittest\nimport learn2learn as l2l\n\n\nclass CIFARFSTests(unittest.TestCase):\n\n    def setUp(self):\n        pass\n\n    def tearDown(self):\n        pass\n\n    def test_download(self):\n        cifarfs = l2l.vision.datasets.CIFARFS(root='./data', download=True)\n        path = os.path.join('./data', 'cifarfs', 'processed')\n        self.assertTrue(os.path.exists(path))\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/unit/vision/fc100_test.py,0,"b""#!/usr/bin/env python3\n\nimport os\nimport unittest\nimport learn2learn as l2l\n\n\nclass FC100Tests(unittest.TestCase):\n\n    def setUp(self):\n        pass\n\n    def tearDown(self):\n        pass\n\n    def test_download(self):\n        root = os.path.expanduser('./data')\n        fc100 = l2l.vision.datasets.FC100(root=root,\n                                          mode='train',\n                                          download=True)\n        image, label = fc100[12]\n        path = os.path.join(root, 'FC100_train.pickle')\n        self.assertTrue(os.path.exists(path))\n\n        fc100 = l2l.vision.datasets.FC100(root=root,\n                                          mode='validation',\n                                          download=True)\n        image, label = fc100[12]\n        path = os.path.join(root, 'FC100_val.pickle')\n        self.assertTrue(os.path.exists(path))\n\n        fc100 = l2l.vision.datasets.FC100(root=root,\n                                          mode='test',\n                                          download=True)\n        image, label = fc100[12]\n        path = os.path.join(root, 'FC100_test.pickle')\n        self.assertTrue(os.path.exists(path))\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/unit/vision/fgvc_aircraft_test.py,0,"b""#!/usr/bin/env python3\n\nimport os\nimport unittest\nimport learn2learn as l2l\n\n\nclass AircraftTests(unittest.TestCase):\n\n    def setUp(self):\n        pass\n\n    def tearDown(self):\n        pass\n\n    def test_download(self):\n        root = os.path.expanduser('~/data')\n        aircraft = l2l.vision.datasets.FGVCAircraft(root=root, mode='train', download=True)\n        image, label = aircraft[12]\n        path = os.path.join(root, 'fgvc_aircraft')\n        self.assertTrue(os.path.exists(path))\n\n        aircraft = l2l.vision.datasets.FGVCAircraft(root=root, mode='validation')\n        image, label = aircraft[12]\n        path = os.path.join(root, 'fgvc_aircraft')\n        self.assertTrue(os.path.exists(path))\n\n        aircraft = l2l.vision.datasets.FGVCAircraft(root=root, mode='test')\n        image, label = aircraft[12]\n        path = os.path.join(root, 'fgvc_aircraft')\n        self.assertTrue(os.path.exists(path))\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/unit/vision/tiered_imagenet_test_notravis.py,0,"b""#!/usr/bin/env python3\n\nimport os\nimport unittest\nimport learn2learn as l2l\n\n\nclass TieredImagenetTests(unittest.TestCase):\n\n    def setUp(self):\n        pass\n\n    def tearDown(self):\n        pass\n\n    def test_download(self):\n        root = os.path.expanduser('~/data')\n        tiered = l2l.vision.datasets.TieredImagenet(root=root, mode='train', download=True)\n        image, label = tiered[12]\n        path = os.path.join(root, 'tiered-imagenet', 'train_images_png.pkl')\n        self.assertTrue(os.path.exists(path))\n\n        tiered = l2l.vision.datasets.TieredImagenet(root=root, mode='validation', download=True)\n        image, label = tiered[12]\n        path = os.path.join(root, 'tiered-imagenet', 'val_images_png.pkl')\n        self.assertTrue(os.path.exists(path))\n\n        tiered = l2l.vision.datasets.TieredImagenet(root=root, mode='test', download=True)\n        image, label = tiered[12]\n        path = os.path.join(root, 'tiered-imagenet', 'test_images_png.pkl')\n        self.assertTrue(os.path.exists(path))\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
tests/unit/vision/vgg_flowers_test.py,0,"b""#!/usr/bin/env python3\n\nimport os\nimport unittest\nimport learn2learn as l2l\n\n\nclass FlowersTests(unittest.TestCase):\n\n    def setUp(self):\n        pass\n\n    def tearDown(self):\n        pass\n\n    def test_download(self):\n        root = os.path.expanduser('~/data')\n        flowers = l2l.vision.datasets.VGGFlower102(root=root, mode='train', download=True)\n        image, label = flowers[12]\n        path = os.path.join(root, 'vgg_flower102', 'imagelabels.mat')\n        self.assertTrue(os.path.exists(path))\n\n        flowers = l2l.vision.datasets.VGGFlower102(root=root, mode='validation')\n        image, label = flowers[12]\n\n        flowers = l2l.vision.datasets.VGGFlower102(root=root, mode='test')\n        image, label = flowers[12]\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
learn2learn/gym/envs/mujoco/__init__.py,0,b'#!/usr/bin/env python3\n\nfrom .dummy_mujoco_env import MujocoEnv\nfrom .ant_direction import AntDirectionEnv\nfrom .ant_forward_backward import AntForwardBackwardEnv\nfrom .halfcheetah_forward_backward import HalfCheetahForwardBackwardEnv\nfrom .humanoid_direction import HumanoidDirectionEnv\nfrom .humanoid_forward_backward import HumanoidForwardBackwardEnv\n'
learn2learn/gym/envs/mujoco/ant_direction.py,0,"b'#!/usr/bin/env python3\n\nimport gym\nimport numpy as np\n\nfrom gym.error import DependencyNotInstalled\ntry:\n    from gym.envs.mujoco.mujoco_env import MujocoEnv\nexcept DependencyNotInstalled:\n    from learn2learn.gym.envs.mujoco.dummy_mujoco_env import MujocoEnv\n\nfrom learn2learn.gym.envs.meta_env import MetaEnv\n\n\nclass AntDirectionEnv(MetaEnv, MujocoEnv, gym.utils.EzPickle):\n    """"""\n    [[Source]](https://github.com/learnables/learn2learn/blob/master/learn2learn/gym/envs/mujoco/ant_direction.py)\n\n    **Description**\n\n    This environment requires the Ant to learn to run in a random direction in the\n    XY plane. At each time step the ant receives a signal composed of a\n    control cost and a reward equal to its average velocity in the direction\n    of the plane. The tasks are 2d-arrays sampled uniformly along the unit circle.\n    The target direction is indicated by the vector from the origin to the sampled point.\n    The velocity is calculated as the distance (in the target direction) of the ant\'s torso\n    position before and after taking the specified action divided by a small value dt.\n    As noted in [1], a small positive bonus is added to the reward to stop the ant from\n    prematurely ending the episode.\n\n    **Credit**\n\n    Adapted from Jonas Rothfuss\' implementation.\n\n    **References**\n\n    1. Finn et al. 2017. ""Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks."" arXiv [cs.LG].\n    2. Rothfuss et al. 2018. ""ProMP: Proximal Meta-Policy Search."" arXiv [cs.LG].\n\n    """"""\n\n    def __init__(self, task=None):\n        MetaEnv.__init__(self, task)\n        MujocoEnv.__init__(self, \'ant.xml\', 5)\n        gym.utils.EzPickle.__init__(self)\n\n    # -------- MetaEnv Methods --------\n    def set_task(self, task):\n        MetaEnv.set_task(self, task)\n        self.goal_direction = task[\'direction\']\n\n    def sample_tasks(self, num_tasks):\n        directions = np.random.normal(size=(num_tasks, 2))\n        directions /= np.linalg.norm(directions, axis=1)[..., np.newaxis]\n        tasks = [{\'direction\': direction} for direction in directions]\n        return tasks\n\n    # -------- Mujoco Methods --------\n    def _get_obs(self):\n        return np.concatenate([\n            self.sim.data.qpos.flat[2:],\n            self.sim.data.qvel.flat,\n            np.clip(self.sim.data.cfrc_ext, -1, 1).flat,\n        ])\n\n    def viewer_setup(self):\n        camera_id = self.model.camera_name2id(\'track\')\n        self.viewer.cam.type = 2\n        self.viewer.cam.fixedcamid = camera_id\n        self.viewer.cam.distance = self.model.stat.extent * 0.5\n        # Hide the overlay\n        self.viewer._hide_overlay = True\n\n    def reset_model(self):\n        qpos = self.init_qpos + self.np_random.uniform(size=self.model.nq, low=-.1, high=.1)\n        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1\n        self.set_state(qpos, qvel)\n\n    # -------- Gym Methods --------\n    def step(self, action):\n        posbefore = np.copy(self.get_body_com(""torso"")[:2])\n        self.do_simulation(action, self.frame_skip)\n        posafter = self.get_body_com(""torso"")[:2]\n        forward_reward = np.sum(self.goal_direction * (posafter - posbefore)) / self.dt\n        ctrl_cost = .5 * np.square(action).sum()\n        contact_cost = 0.5 * 1e-3 * np.sum(\n            np.square(np.clip(self.sim.data.cfrc_ext, -1, 1)))\n        survive_reward = 1.0\n        reward = forward_reward - ctrl_cost - contact_cost + survive_reward\n        state = self.state_vector()\n        notdone = np.isfinite(state).all() and 1.0 >= state[2] >= 0.\n        done = not notdone\n        ob = self._get_obs()\n        return ob, reward, done, dict(\n            reward_forward=forward_reward,\n            reward_ctrl=-ctrl_cost,\n            reward_contact=-contact_cost,\n            reward_survive=survive_reward)\n\n    def reset(self, *args, **kwargs):\n        MujocoEnv.reset(self, *args, **kwargs)\n        return self._get_obs()\n\n    def render(self, mode=\'human\'):\n        if mode == \'rgb_array\':\n            self._get_viewer(mode).render()\n            # window size used for old mujoco-py:\n            width, height = 500, 500\n            data = self._get_viewer(mode).read_pixels(width,\n                                                      height,\n                                                      depth=False)\n            return data\n        elif mode == \'human\':\n            self._get_viewer(mode).render()\n\n\nif __name__ == \'__main__\':\n    env = AntDirectionEnv()\n    for task in [env.get_task(), env.sample_tasks(1)[0]]:\n        env.set_task(task)\n        env.reset()\n        action = env.action_space.sample()\n        env.step(action)\n'"
learn2learn/gym/envs/mujoco/ant_forward_backward.py,0,"b'#!/usr/bin/env python3\n\nimport gym\nimport numpy as np\n\nfrom gym.error import DependencyNotInstalled\ntry:\n    from gym.envs.mujoco.mujoco_env import MujocoEnv\nexcept DependencyNotInstalled:\n    from learn2learn.gym.envs.mujoco.dummy_mujoco_env import MujocoEnv\n\n\nfrom learn2learn.gym.envs.meta_env import MetaEnv\n\n\nclass AntForwardBackwardEnv(MetaEnv, MujocoEnv, gym.utils.EzPickle):\n    """"""\n    [[Source]](https://github.com/learnables/learn2learn/blob/master/learn2learn/gym/envs/mujoco/ant_forward_backward.py)\n\n    **Description**\n\n    This environment requires the ant to learn to run forward or backward.\n    At each time step the ant receives a signal composed of a\n    control cost and a reward equal to its average velocity in the direction\n    of the plane. The tasks are Bernoulli samples on {-1, 1} with probability 0.5, where -1 indicates the ant should\n    move backward and +1 indicates the ant should move forward.\n    The velocity is calculated as the distance (in the direction of the plane) of the ant\'s torso\n    position before and after taking the specified action divided by a small value dt.\n    As noted in [1], a small positive bonus is added to the reward to stop the ant from\n    prematurely ending the episode.\n\n    **Credit**\n\n    Adapted from Jonas Rothfuss\' implementation.\n\n    **References**\n\n    1. Finn et al. 2017. ""Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks."" arXiv [cs.LG].\n    2. Rothfuss et al. 2018. ""ProMP: Proximal Meta-Policy Search."" arXiv [cs.LG].\n\n    """"""\n\n    def __init__(self, task=None):\n        MetaEnv.__init__(self, task)\n        MujocoEnv.__init__(self, \'ant.xml\', 5)\n        gym.utils.EzPickle.__init__(self)\n\n    # -------- MetaEnv Methods --------\n    def set_task(self, task):\n        MetaEnv.set_task(self, task)\n        self.goal_direction = task[\'direction\']\n\n    def sample_tasks(self, num_tasks):\n        directions = np.random.choice((-1, 1), (num_tasks,))\n        tasks = [{\'direction\': direction} for direction in directions]\n        return tasks\n\n    # -------- Mujoco Methods --------\n    def _get_obs(self):\n        return np.concatenate([\n            self.sim.data.qpos.flat[2:],\n            self.sim.data.qvel.flat,\n            np.clip(self.sim.data.cfrc_ext, -1, 1).flat,\n        ])\n\n    def viewer_setup(self):\n        camera_id = self.model.camera_name2id(\'track\')\n        self.viewer.cam.type = 2\n        self.viewer.cam.fixedcamid = camera_id\n        self.viewer.cam.distance = self.model.stat.extent * 0.5\n        # Hide the overlay\n        self.viewer._hide_overlay = True\n\n    def reset_model(self):\n        qpos = self.init_qpos + self.np_random.uniform(size=self.model.nq, low=-.1, high=.1)\n        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1\n        self.set_state(qpos, qvel)\n\n    # -------- Gym Methods --------\n    def step(self, action):\n        xposbefore = np.copy(self.get_body_com(""torso"")[0])\n        self.do_simulation(action, self.frame_skip)\n        xposafter = self.get_body_com(""torso"")[0]\n        forward_reward = self.goal_direction * (xposafter - xposbefore) / self.dt\n        ctrl_cost = .5 * np.square(action).sum()\n        contact_cost = 0.5 * 1e-3 * np.sum(\n            np.square(np.clip(self.sim.data.cfrc_ext, -1, 1)))\n        survive_reward = 1.0\n        reward = forward_reward - ctrl_cost - contact_cost + survive_reward\n        state = self.state_vector()\n        notdone = np.isfinite(state).all() and 1.0 >= state[2] >= 0.\n        done = not notdone\n        ob = self._get_obs()\n        return ob, reward, done, dict(\n            reward_forward=forward_reward,\n            reward_ctrl=-ctrl_cost,\n            reward_contact=-contact_cost,\n            reward_survive=survive_reward)\n\n    def reset(self, *args, **kwargs):\n        MujocoEnv.reset(self, *args, **kwargs)\n        return self._get_obs()\n\n    def render(self, mode=\'human\'):\n        if mode == \'rgb_array\':\n            self._get_viewer(mode).render()\n            # window size used for old mujoco-py:\n            width, height = 500, 500\n            data = self._get_viewer(mode).read_pixels(width,\n                                                      height,\n                                                      depth=False)\n            return data\n        elif mode == \'human\':\n            self._get_viewer(mode).render()\n\n\nif __name__ == \'__main__\':\n    env = AntForwardBackwardEnv()\n    for task in [env.get_task(), env.sample_tasks(1)[0]]:\n        env.set_task(task)\n        env.reset()\n        action = env.action_space.sample()\n        env.step(action)\n'"
learn2learn/gym/envs/mujoco/dummy_mujoco_env.py,0,"b'#!/usr/bin/env python3\n\nfrom gym.error import DependencyNotInstalled\n\n\nclass MujocoEnv(object):\n    """"""Dummy class to avoid Mujoco import errors""""""\n\n    def __init__(self, model_path=None, frame_skip=None):\n        self.model_path = model_path\n        self.frame_skip = frame_skip\n\n    def _error(self):\n        """"""docstring for _print_error""""""\n        msg = \'Importing MujocoEnv failed. Please run: \\n\' + \\\n              \'\\n`pip install mujoco-py` \\n\\n\' + \\\n              \'and visit: \' + \\\n              \'https://github.com/openai/mujoco-py/\'\n        raise DependencyNotInstalled(msg)\n\n    def __hasattr__(self, *args, **kwargs):\n        """"""docstring for __hasattr__""""""\n        self._error()\n\n    def __getattr__(self, *args, **kwargs):\n        """"""docstring for __getattr__""""""\n        self._error()\n\n    def reset(self, *args, **kwargs):\n        """"""docstring for reset""""""\n        self._error()\n\n    def step(self, *args, **kwargs):\n        """"""docstring for reset""""""\n        self._error()\n\n    def seed(self, *args, **kwargs):\n        """"""docstring for reset""""""\n        self._error()\n'"
learn2learn/gym/envs/mujoco/halfcheetah_forward_backward.py,0,"b'#!/usr/bin/env python3\n\nimport gym\nimport numpy as np\n\nfrom gym.error import DependencyNotInstalled\ntry:\n    from gym.envs.mujoco.mujoco_env import MujocoEnv\nexcept DependencyNotInstalled:\n    from learn2learn.gym.envs.mujoco.dummy_mujoco_env import MujocoEnv\n\nfrom learn2learn.gym.envs.meta_env import MetaEnv\n\n\nclass HalfCheetahForwardBackwardEnv(MetaEnv, MujocoEnv, gym.utils.EzPickle):\n    """"""\n    [[Source]](https://github.com/learnables/learn2learn/blob/master/learn2learn/gym/envs/mujoco/halfcheetah_forward_backward.py)\n\n    **Description**\n\n    This environment requires the half-cheetah to learn to run forward or backward.\n    At each time step the half-cheetah receives a signal composed of a\n    control cost and a reward equal to its average velocity in the direction\n    of the plane. The tasks are Bernoulli samples on {-1, 1} with probability 0.5, where -1 indicates the half-cheetah should\n    move backward and +1 indicates the half-cheetah should move forward.\n    The velocity is calculated as the distance (in the target direction) of the half-cheetah\'s torso\n    position before and after taking the specified action divided by a small value dt.\n\n    **Credit**\n\n    Adapted from Jonas Rothfuss\' implementation.\n\n    **References**\n\n    1. Finn et al. 2017. ""Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks."" arXiv [cs.LG].\n    2. Rothfuss et al. 2018. ""ProMP: Proximal Meta-Policy Search."" arXiv [cs.LG].\n\n    """"""\n\n    def __init__(self, task=None):\n        MetaEnv.__init__(self, task)\n        MujocoEnv.__init__(self, \'half_cheetah.xml\', 5)\n        gym.utils.EzPickle.__init__(self)\n\n    # -------- MetaEnv Methods --------\n    def set_task(self, task):\n        MetaEnv.set_task(self, task)\n        self.goal_direction = task[\'direction\']\n\n    def sample_tasks(self, num_tasks):\n        directions = np.random.choice((-1.0, 1.0), (num_tasks,))\n        tasks = [{\'direction\': direction} for direction in directions]\n        return tasks\n\n    # -------- Mujoco Methods --------\n    def _get_obs(self):\n        return np.concatenate([\n            self.sim.data.qpos.flat[1:],\n            self.sim.data.qvel.flat,\n            self.get_body_com(""torso"").flat,\n        ]).astype(np.float32).flatten()\n\n    def viewer_setup(self):\n        camera_id = self.model.camera_name2id(\'track\')\n        self.viewer.cam.type = 2\n        self.viewer.cam.fixedcamid = camera_id\n        self.viewer.cam.distance = self.model.stat.extent * 0.5\n        # Hide the overlay\n        self.viewer._hide_overlay = True\n\n    def reset_model(self):\n        qpos = self.init_qpos + self.np_random.uniform(low=-.1, high=.1, size=self.model.nq)\n        qvel = self.init_qvel + self.np_random.randn(self.model.nv) * .1\n        self.set_state(qpos, qvel)\n        return self._get_obs()\n\n    # -------- Gym Methods --------\n    def step(self, action):\n        xposbefore = self.sim.data.qpos[0]\n        self.do_simulation(action, self.frame_skip)\n        xposafter = self.sim.data.qpos[0]\n\n        forward_vel = (xposafter - xposbefore) / self.dt\n        forward_reward = self.goal_direction * forward_vel\n        ctrl_cost = 0.5 * 1e-1 * np.sum(np.square(action))\n\n        observation = self._get_obs()\n        reward = forward_reward - ctrl_cost\n        done = False\n        infos = dict(reward_forward=forward_reward,\n                     reward_ctrl=-ctrl_cost, task=self._task)\n        return (observation, reward, done, infos)\n\n    def reset(self, *args, **kwargs):\n        MujocoEnv.reset(self, *args, **kwargs)\n        return self._get_obs()\n\n    def render(self, mode=\'human\'):\n        if mode == \'rgb_array\':\n            self._get_viewer(mode).render()\n            # window size used for old mujoco-py:\n            width, height = 500, 500\n            data = self._get_viewer(mode).read_pixels(width,\n                                                      height,\n                                                      depth=False)\n            return data\n        elif mode == \'human\':\n            self._get_viewer(mode).render()\n\n\nif __name__ == \'__main__\':\n    env = HalfCheetahForwardBackwardEnv()\n    for task in [env.get_task(), env.sample_tasks(1)[0]]:\n        env.set_task(task)\n        env.reset()\n        action = env.action_space.sample()\n        env.step(action)\n'"
learn2learn/gym/envs/mujoco/humanoid_direction.py,0,"b'#!/usr/bin/env python3\n\nimport gym\nimport numpy as np\n\nfrom gym.error import DependencyNotInstalled\ntry:\n    from gym.envs.mujoco.mujoco_env import MujocoEnv\nexcept DependencyNotInstalled:\n    from learn2learn.gym.envs.mujoco.dummy_mujoco_env import MujocoEnv\n\nfrom learn2learn.gym.envs.meta_env import MetaEnv\n\n\ndef mass_center(model, sim):\n    mass = np.expand_dims(model.body_mass, 1)\n    xpos = sim.data.xipos\n    return (np.sum(mass * xpos, 0) / np.sum(mass))\n\n\nclass HumanoidDirectionEnv(MetaEnv, MujocoEnv, gym.utils.EzPickle):\n    """"""\n    [[Source]](https://github.com/learnables/learn2learn/blob/master/learn2learn/gym/envs/mujoco/humanoid_direction.py)\n\n    **Description**\n\n    This environment requires the humanoid to learn to run in a random direction in the\n    XY plane. At each time step the humanoid receives a signal composed of a\n    control cost and a reward equal to its average velocity in the target direction.\n    The tasks are 2d-arrays sampled uniformly along the unit circle.\n    The target direction is indicated by the vector from the origin to the sampled point.\n    The velocity is calculated as the distance (in the target direction) of the humanoid\'s torso\n    position before and after taking the specified action divided by a small value dt.\n    A small positive bonus is added to the reward to stop the humanoid from\n    prematurely ending the episode.\n\n    **Credit**\n\n    Adapted from Jonas Rothfuss\' implementation.\n\n    **References**\n\n    1. Finn et al. 2017. ""Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks."" arXiv [cs.LG].\n    2. Rothfuss et al. 2018. ""ProMP: Proximal Meta-Policy Search."" arXiv [cs.LG].\n\n    """"""\n\n    def __init__(self, task=None):\n        MetaEnv.__init__(self, task)\n        MujocoEnv.__init__(self, \'humanoid.xml\', 5)\n        gym.utils.EzPickle.__init__(self)\n\n    # -------- MetaEnv Methods --------\n    def set_task(self, task):\n        MetaEnv.set_task(self, task)\n        self.goal_direction = task[\'direction\']\n\n    def sample_tasks(self, num_tasks):\n        directions = np.random.normal(size=(num_tasks, 2))\n        directions /= np.linalg.norm(directions, axis=1)[..., np.newaxis]\n        tasks = [{\'direction\': direction} for direction in directions]\n        return tasks\n\n    # -------- Mujoco Methods --------\n    def _get_obs(self):\n        data = self.sim.data\n        return np.concatenate([data.qpos.flat[2:],\n                               data.qvel.flat,\n                               data.cinert.flat,\n                               data.cvel.flat,\n                               data.qfrc_actuator.flat,\n                               data.cfrc_ext.flat])\n\n    def viewer_setup(self):\n        self.viewer.cam.trackbodyid = 1\n        self.viewer.cam.distance = self.model.stat.extent * 1.0\n        self.viewer.cam.elevation = -20\n\n    def reset_model(self):\n        c = 0.01\n        self.set_state(\n            self.init_qpos + self.np_random.uniform(low=-c, high=c, size=self.model.nq),\n            self.init_qvel + self.np_random.uniform(low=-c, high=c, size=self.model.nv, )\n        )\n        return self._get_obs()\n\n    # -------- Gym Methods --------\n    def step(self, action):\n        pos_before = np.copy(mass_center(self.model, self.sim)[:2])\n        self.do_simulation(action, self.frame_skip)\n        pos_after = mass_center(self.model, self.sim)[:2]\n        alive_bonus = 5.0\n        data = self.sim.data\n        lin_vel_cost = 0.25 * np.sum(self.goal_direction * (pos_after - pos_before)) / self.model.opt.timestep\n        quad_ctrl_cost = 0.1 * np.square(data.ctrl).sum()\n        quad_impact_cost = .5e-6 * np.square(data.cfrc_ext).sum()\n        quad_impact_cost = min(quad_impact_cost, 10)\n        reward = lin_vel_cost - quad_ctrl_cost - quad_impact_cost + alive_bonus\n        qpos = self.sim.data.qpos\n        done = bool((qpos[2] < 1.0) or (qpos[2] > 2.0))\n        return self._get_obs(), reward, done, dict(reward_linvel=lin_vel_cost,\n                                                   reward_quadctrl=-quad_ctrl_cost,\n                                                   reward_alive=alive_bonus,\n                                                   reward_impact=-quad_impact_cost)\n\n    def reset(self, *args, **kwargs):\n        MujocoEnv.reset(self, *args, **kwargs)\n        return self._get_obs()\n\n    def render(self, mode=\'human\'):\n        if mode == \'rgb_array\':\n            self._get_viewer(mode).render()\n            # window size used for old mujoco-py:\n            width, height = 500, 500\n            data = self._get_viewer(mode).read_pixels(width,\n                                                      height,\n                                                      depth=False)\n            return data\n        elif mode == \'human\':\n            self._get_viewer(mode).render()\n\n\nif __name__ == \'__main__\':\n    env = HumanoidDirectionEnv()\n    for task in [env.get_task(), env.sample_tasks(1)[0]]:\n        env.set_task(task)\n        env.reset()\n        action = env.action_space.sample()\n        env.step(action)\n'"
learn2learn/gym/envs/mujoco/humanoid_forward_backward.py,0,"b'#!/usr/bin/env python3\n\nimport gym\nimport numpy as np\n\nfrom gym.error import DependencyNotInstalled\ntry:\n    from gym.envs.mujoco.mujoco_env import MujocoEnv\nexcept DependencyNotInstalled:\n    from learn2learn.gym.envs.mujoco.dummy_mujoco_env import MujocoEnv\n\nfrom learn2learn.gym.envs.meta_env import MetaEnv\n\n\ndef mass_center(model, sim):\n    mass = np.expand_dims(model.body_mass, 1)\n    xpos = sim.data.xipos\n    return (np.sum(mass * xpos, 0) / np.sum(mass))\n\n\nclass HumanoidForwardBackwardEnv(MetaEnv, MujocoEnv, gym.utils.EzPickle):\n    """"""\n    [[Source]](https://github.com/learnables/learn2learn/blob/master/learn2learn/gym/envs/mujoco/humanoid_forward_backward.py)\n\n    **Description**\n\n    This environment requires the humanoid to learn to run forward or backward.\n    At each time step the humanoid receives a signal composed of a\n    control cost and a reward equal to its average velocity in the target direction.\n    The tasks are Bernoulli samples on {-1, 1} with probability 0.5, where -1 indicates the humanoid should\n    move backward and +1 indicates the humanoid should move forward.\n    The velocity is calculated as the distance (in the target direction) of the humanoid\'s torso\n    position before and after taking the specified action divided by a small value dt.\n\n    **Credit**\n\n    Adapted from Jonas Rothfuss\' implementation.\n\n    **References**\n\n    1. Finn et al. 2017. ""Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks."" arXiv [cs.LG].\n    2. Rothfuss et al. 2018. ""ProMP: Proximal Meta-Policy Search."" arXiv [cs.LG].\n\n    """"""\n\n    def __init__(self, task=None):\n        MetaEnv.__init__(self, task)\n        MujocoEnv.__init__(self, \'humanoid.xml\', 5)\n        gym.utils.EzPickle.__init__(self)\n\n    # -------- MetaEnv Methods --------\n    def set_task(self, task):\n        MetaEnv.set_task(self, task)\n        self.goal_direction = task[\'direction\']\n\n    def sample_tasks(self, num_tasks):\n        directions = np.random.choice((-1, 1), (num_tasks,))\n        tasks = [{\'direction\': direction} for direction in directions]\n        return tasks\n\n    # -------- Mujoco Methods --------\n    def _get_obs(self):\n        data = self.sim.data\n        return np.concatenate([data.qpos.flat[2:],\n                               data.qvel.flat,\n                               data.cinert.flat,\n                               data.cvel.flat,\n                               data.qfrc_actuator.flat,\n                               data.cfrc_ext.flat])\n\n    def viewer_setup(self):\n        self.viewer.cam.trackbodyid = 1\n        self.viewer.cam.distance = self.model.stat.extent * 1.0\n        self.viewer.cam.elevation = -20\n\n    def reset_model(self):\n        c = 0.01\n        self.set_state(\n            self.init_qpos + self.np_random.uniform(low=-c, high=c, size=self.model.nq),\n            self.init_qvel + self.np_random.uniform(low=-c, high=c, size=self.model.nv, )\n        )\n        return self._get_obs()\n\n    # -------- Gym Methods --------\n    def step(self, action):\n        pos_before = mass_center(self.model, self.sim)[0]\n        self.do_simulation(action, self.frame_skip)\n        pos_after = mass_center(self.model, self.sim)[0]\n        alive_bonus = 5.0\n        data = self.sim.data\n        lin_vel_cost = 0.25 * self.goal_direction * (pos_after - pos_before) / self.model.opt.timestep\n        quad_ctrl_cost = 0.1 * np.square(data.ctrl).sum()\n        quad_impact_cost = .5e-6 * np.square(data.cfrc_ext).sum()\n        quad_impact_cost = min(quad_impact_cost, 10)\n        reward = lin_vel_cost - quad_ctrl_cost - quad_impact_cost + alive_bonus\n        qpos = self.sim.data.qpos\n        done = bool((qpos[2] < 1.0) or (qpos[2] > 2.0))\n        return self._get_obs(), reward, done, dict(reward_linvel=lin_vel_cost,\n                                                   reward_quadctrl=-quad_ctrl_cost,\n                                                   reward_alive=alive_bonus,\n                                                   reward_impact=-quad_impact_cost)\n\n    def reset(self, *args, **kwargs):\n        MujocoEnv.reset(self, *args, **kwargs)\n        return self._get_obs()\n\n    def render(self, mode=\'human\'):\n        if mode == \'rgb_array\':\n            self._get_viewer(mode).render()\n            # window size used for old mujoco-py:\n            width, height = 500, 500\n            data = self._get_viewer(mode).read_pixels(width,\n                                                      height,\n                                                      depth=False)\n            return data\n        elif mode == \'human\':\n            self._get_viewer(mode).render()\n\n\nif __name__ == \'__main__\':\n    env = HumanoidForwardBackwardEnv()\n    for task in [env.get_task(), env.sample_tasks(1)[0]]:\n        env.set_task(task)\n        env.reset()\n        action = env.action_space.sample()\n        env.step(action)\n'"
learn2learn/gym/envs/particles/__init__.py,0,b'#!/usr/bin/env python3\n\nfrom .particles_2d import Particles2DEnv\n'
learn2learn/gym/envs/particles/particles_2d.py,0,"b'#!/usr/bin/env python3\n\nimport numpy as np\nfrom gym import spaces\nfrom gym.utils import seeding\n\nfrom learn2learn.gym.envs.meta_env import MetaEnv\n\n\nclass Particles2DEnv(MetaEnv):\n    """"""\n    [[Source]](https://github.com/learnables/learn2learn/blob/master/learn2learn/gym/envs/particles/particles_2d.py)\n\n    **Description**\n\n    Each task is defined by the location of the goal. A point mass\n    receives a directional force and moves accordingly\n    (clipped in [-0.1,0.1]). The reward is equal to the negative\n    distance from the goal.\n\n    **Credit**\n\n    Adapted from Jonas Rothfuss\' implementation.\n\n    """"""\n\n    def __init__(self, task=None):\n        self.seed()\n        super(Particles2DEnv, self).__init__(task)\n        self.observation_space = spaces.Box(low=-np.inf, high=np.inf,\n                                            shape=(2,), dtype=np.float32)\n        self.action_space = spaces.Box(low=-0.1, high=0.1,\n                                       shape=(2,), dtype=np.float32)\n        self.reset()\n\n    # -------- MetaEnv Methods --------\n    def sample_tasks(self, num_tasks):\n        """"""\n        Tasks correspond to a goal point chosen uniformly at random.\n        """"""\n        goals = self.np_random.uniform(-0.5, 0.5, size=(num_tasks, 2))\n        tasks = [{\'goal\': goal} for goal in goals]\n        return tasks\n\n    def set_task(self, task):\n        self._task = task\n        self._goal = task[\'goal\']\n\n    # -------- Gym Methods --------\n    def seed(self, seed=None):\n        self.np_random, seed = seeding.np_random(seed)\n        return [seed]\n\n    def reset(self, env=True):\n        """"""\n        Sets point mass position back to (0,0)\n        """"""\n        self._state = np.zeros(2, dtype=np.float32)\n        return self._state\n\n    def step(self, action):\n        """"""\n        **Description**\n\n        Given an action, clips the action to be in the\n        appropriate range and moves the point mass position\n        according to the action.\n\n        **Arguments**\n\n        action (2-element array) - Array specifying the magnitude\n        and direction of the forces to be applied in the x and y\n        planes.\n\n        **Returns**\n\n        *state, reward, done, task*\n\n        * state (arr) - is a 2-element array encoding the x,y position of\n        the point mass\n\n        * reward (float) - signal equal to the negative squared distance\n        from the goal\n\n        * done (bool) - boolean indicating whether or not the point mass\n        is epsilon or less distance from the goal\n\n        * task (dict) - dictionary of task specific parameters and their current\n        values\n\n        """"""\n        action = np.clip(action, -0.1, 0.1)\n        assert self.action_space.contains(action)\n        self._state = self._state + action\n\n        x = self._state[0] - self._goal[0]\n        y = self._state[1] - self._goal[1]\n        reward = -np.sqrt(x ** 2 + y ** 2)\n        done = ((np.abs(x) < 0.01) and (np.abs(y) < 0.01))\n\n        return self._state, reward, done, self._task\n\n    def render(self, mode=None):\n        raise NotImplementedError\n'"
