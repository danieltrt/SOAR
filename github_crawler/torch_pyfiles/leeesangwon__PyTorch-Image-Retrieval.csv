file_path,api_count,code
data_loader.py,1,"b""# -*- coding: utf_8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms, datasets\nfrom PIL import Image\n\n\ndef train_data_loader(data_path, img_size, use_augment=False):\n    if use_augment:\n        data_transforms = transforms.Compose([\n            transforms.RandomOrder([\n                transforms.RandomApply([transforms.ColorJitter(contrast=0.5)], .5),\n                transforms.Compose([\n                    transforms.RandomApply([transforms.ColorJitter(saturation=0.5)], .5),\n                    transforms.RandomApply([transforms.ColorJitter(hue=0.1)], .5),\n                ])\n            ]),\n            transforms.RandomApply([transforms.ColorJitter(brightness=0.125)], .5),\n            transforms.RandomApply([transforms.RandomRotation(15)], .5),\n            transforms.RandomResizedCrop(img_size),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])\n    else:\n        data_transforms = transforms.Compose([\n            transforms.RandomResizedCrop(img_size),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])\n\n    image_dataset = datasets.ImageFolder(data_path, data_transforms)\n\n    return image_dataset\n\n\ndef test_data_loader(data_path):\n\n    # return full path\n    queries_path = [os.path.join(data_path, 'query', path) for path in os.listdir(os.path.join(data_path, 'query'))]\n    references_path = [os.path.join(data_path, 'reference', path) for path in\n                       os.listdir(os.path.join(data_path, 'reference'))]\n\n    return queries_path, references_path\n\n\ndef test_data_generator(data_path, img_size):\n    img_size = (img_size, img_size)\n    data_transforms = transforms.Compose([\n        transforms.Resize(img_size),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n\n    test_image_dataset = TestDataset(data_path, data_transforms)\n\n    return test_image_dataset\n\n\nclass TestDataset(Dataset):\n    def __init__(self, img_path_list, transform=None):\n        self.img_path_list = img_path_list\n        self.transform = transform\n\n    def __getitem__(self, index):\n        img_path = self.img_path_list[index]\n        img = Image.open(img_path)\n        if self.transform is not None:\n            img = self.transform(img)\n        return img_path, img\n\n    def __len__(self):\n        return len(self.img_path_list)\n\n\nif __name__ == '__main__':\n    query, refer = test_data_loader('./')\n    print(query)\n    print(refer)\n"""
datasets.py,3,"b'""""""\nOriginal source: https://github.com/adambielski/siamese-triplet\n""""""\n\nimport numpy as np\nimport torch\n\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import BatchSampler\n\n\nclass BalancedBatchSampler(BatchSampler):\n    """"""\n    BatchSampler - from a MNIST-like dataset, samples n_classes and within these classes samples n_samples.\n    Returns batches of size n_classes * n_samples\n    """"""\n\n    def __init__(self, dataset, n_classes, n_samples):\n        loader = DataLoader(dataset)\n        self.labels_list = []\n        for _, label in loader:\n            self.labels_list.append(label)\n        self.labels = torch.LongTensor(self.labels_list)\n        self.labels_set = list(set(self.labels.numpy()))\n        self.label_to_indices = {label: np.where(self.labels.numpy() == label)[0]\n                                 for label in self.labels_set}\n        for l in self.labels_set:\n            np.random.shuffle(self.label_to_indices[l])\n        self.used_label_indices_count = {label: 0 for label in self.labels_set}\n        self.count = 0\n        self.n_classes = n_classes\n        self.n_samples = n_samples\n        self.dataset = dataset\n        self.batch_size = self.n_samples * self.n_classes\n\n    def __iter__(self):\n        self.count = 0\n        while self.count + self.batch_size < len(self.dataset):\n            classes = np.random.choice(self.labels_set, self.n_classes, replace=False)\n            indices = []\n            for class_ in classes:\n                indices.extend(self.label_to_indices[class_][\n                               self.used_label_indices_count[class_]:self.used_label_indices_count[\n                                                                         class_] + self.n_samples])\n                self.used_label_indices_count[class_] += self.n_samples\n                if self.used_label_indices_count[class_] + self.n_samples > len(self.label_to_indices[class_]):\n                    np.random.shuffle(self.label_to_indices[class_])\n                    self.used_label_indices_count[class_] = 0\n            yield indices\n            self.count += self.n_classes * self.n_samples\n\n    def __len__(self):\n        return len(self.dataset) // self.batch_size\n'"
inference.py,1,"b'# -*- coding: utf_8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom torch.utils.data import DataLoader\nfrom data_loader import test_data_generator\n\nimport numpy as np\n\n\ndef retrieve(model, queries, db, img_size, infer_batch_size):\n\n    query_paths = queries\n    reference_paths = db\n\n    query_img_dataset = test_data_generator(queries, img_size=img_size)\n    reference_img_dataset = test_data_generator(db, img_size=img_size)\n\n    query_loader = DataLoader(query_img_dataset, batch_size=infer_batch_size, shuffle=False, num_workers=4,\n                              pin_memory=True)\n    reference_loader = DataLoader(reference_img_dataset, batch_size=infer_batch_size, shuffle=False, num_workers=4,\n                                  pin_memory=True)\n\n    model.eval()\n    model.cuda()\n\n    query_paths, query_vecs = batch_process(model, query_loader)\n    reference_paths, reference_vecs = batch_process(model, reference_loader)\n\n    assert query_paths == queries and reference_paths == db, ""order of paths should be same""\n\n    # DBA and AQE\n    query_vecs, reference_vecs = db_augmentation(query_vecs, reference_vecs, top_k=10)\n    query_vecs, reference_vecs = average_query_expansion(query_vecs, reference_vecs, top_k=5)\n\n    sim_matrix = calculate_sim_matrix(query_vecs, reference_vecs)\n\n    indices = np.argsort(sim_matrix, axis=1)\n    indices = np.flip(indices, axis=1)\n\n    retrieval_results = {}\n\n    # Evaluation: mean average precision (mAP)\n    # You can change this part to fit your evaluation skim\n    for (i, query) in enumerate(query_paths):\n        query = query.split(\'/\')[-1].split(\'.\')[0]\n        ranked_list = [reference_paths[k].split(\'/\')[-1].split(\'.\')[0] for k in indices[i]]\n        ranked_list = ranked_list[:1000]\n\n        retrieval_results[query] = ranked_list\n\n    return retrieval_results\n\n\ndef db_augmentation(query_vecs, reference_vecs, top_k=10):\n    """"""\n    Database-side feature augmentation (DBA)\n    Albert Gordo, et al. ""End-to-end Learning of Deep Visual Representations for Image Retrieval,""\n    International Journal of Computer Vision. 2017.\n    https://link.springer.com/article/10.1007/s11263-017-1016-8\n    """"""\n    weights = np.logspace(0, -2., top_k+1)\n\n    # Query augmentation\n    sim_mat = calculate_sim_matrix(query_vecs, reference_vecs)\n    indices = np.argsort(-sim_mat, axis=1)\n\n    top_k_ref = reference_vecs[indices[:, :top_k], :]\n    query_vecs = np.tensordot(weights, np.concatenate([np.expand_dims(query_vecs, 1), top_k_ref], axis=1), axes=(0, 1))\n\n    # Reference augmentation\n    sim_mat = calculate_sim_matrix(reference_vecs, reference_vecs)\n    indices = np.argsort(-sim_mat, axis=1)\n\n    top_k_ref = reference_vecs[indices[:, :top_k+1], :]\n    reference_vecs = np.tensordot(weights, top_k_ref, axes=(0, 1))\n\n    return query_vecs, reference_vecs\n\n\ndef average_query_expansion(query_vecs, reference_vecs, top_k=5):\n    """"""\n    Average Query Expansion (AQE)\n    Ondrej Chum, et al. ""Total Recall: Automatic Query Expansion with a Generative Feature Model for Object Retrieval,""\n    International Conference of Computer Vision. 2007.\n    https://www.robots.ox.ac.uk/~vgg/publications/papers/chum07b.pdf\n    """"""\n    # Query augmentation\n    sim_mat = calculate_sim_matrix(query_vecs, reference_vecs)\n    indices = np.argsort(-sim_mat, axis=1)\n\n    top_k_ref_mean = np.mean(reference_vecs[indices[:, :top_k], :], axis=1)\n    query_vecs = np.concatenate([query_vecs, top_k_ref_mean], axis=1)\n\n    # Reference augmentation\n    sim_mat = calculate_sim_matrix(reference_vecs, reference_vecs)\n    indices = np.argsort(-sim_mat, axis=1)\n\n    top_k_ref_mean = np.mean(reference_vecs[indices[:, 1:top_k+1], :], axis=1)\n    reference_vecs = np.concatenate([reference_vecs, top_k_ref_mean], axis=1)\n\n    return query_vecs, reference_vecs\n\n\ndef calculate_sim_matrix(query_vecs, reference_vecs):\n    query_vecs, reference_vecs = postprocess(query_vecs, reference_vecs)\n    return np.dot(query_vecs, reference_vecs.T)\n\n\ndef batch_process(model, loader):\n    feature_vecs = []\n    img_paths = []\n    for data in loader:\n        paths, inputs = data\n        feature_vec = _get_feature(model, inputs.cuda())\n        feature_vec = feature_vec.detach().cpu().numpy()  # (batch_size, channels)\n        for i in range(feature_vec.shape[0]):\n            feature_vecs.append(feature_vec[i])\n        img_paths = img_paths + paths\n\n    return img_paths, np.asarray(feature_vecs)\n\n\ndef _get_features_from(model, x, feature_names):\n    features = {}\n\n    def save_feature(name):\n        def hook(m, i, o):\n            features[name] = o.data\n\n        return hook\n\n    for name, module in model.named_modules():\n        _name = name.split(\'.\')[-1]\n        if _name in feature_names:\n            module.register_forward_hook(save_feature(_name))\n\n    model(x)\n\n    return features\n\n\ndef _get_feature(model, x):\n    model_name = model.__class__.__name__\n\n    if model_name == \'EmbeddingNetwork\':\n        feature = model(x)\n    elif model_name == \'ResNet\':\n        features = _get_features_from(model, x, [\'fc\'])\n        feature = features[\'fc\']\n    elif model_name == \'DenseNet\':\n        features = _get_features_from(model, x, [\'classifier\'])\n        feature = features[\'classifier\']\n    else:\n        raise ValueError(""Invalid model name: {}"".format(model_name))\n\n    return feature\n\n\ndef postprocess(query_vecs, reference_vecs):\n    """"""\n    Postprocessing:\n    1) Moving the origin of the feature space to the center of the feature vectors\n    2) L2-normalization\n    """"""\n    # centerize\n    query_vecs, reference_vecs = _centerize(query_vecs, reference_vecs)\n\n    # l2 normalization\n    query_vecs = _l2_normalize(query_vecs)\n    reference_vecs = _l2_normalize(reference_vecs)\n\n    return query_vecs, reference_vecs\n\n\ndef _centerize(v1, v2):\n    concat = np.concatenate([v1, v2], axis=0)\n    center = np.mean(concat, axis=0)\n    return v1-center, v2-center\n\n\ndef _l2_normalize(v):\n    norm = np.expand_dims(np.linalg.norm(v, axis=1), axis=1)\n    if np.any(norm == 0):\n        return v\n    return v / norm\n'"
losses.py,28,"b'import torch\nimport torch.nn as nn\nimport numpy as np\n\n\n# Constants\nN_PAIR = \'n-pair\'\nANGULAR = \'angular\'\nN_PAIR_ANGULAR = \'n-pair-angular\'\nMAIN_LOSS_CHOICES = (N_PAIR, ANGULAR, N_PAIR_ANGULAR)\n\nCROSS_ENTROPY = \'cross-entropy\'\n\n\nclass BlendedLoss(object):\n    def __init__(self, main_loss_type, cross_entropy_flag):\n        super(BlendedLoss, self).__init__()\n        self.main_loss_type = main_loss_type\n        assert main_loss_type in MAIN_LOSS_CHOICES, ""invalid main loss: %s"" % main_loss_type\n\n        if self.main_loss_type == N_PAIR:\n            self.main_loss_fn = NPairLoss()\n        elif self.main_loss_type == ANGULAR:\n            self.main_loss_fn = AngularLoss()\n        elif self.main_loss_type == N_PAIR_ANGULAR:\n            self.main_loss_fn = NPairAngularLoss()\n        else:\n            raise ValueError\n\n        self.cross_entropy_flag = cross_entropy_flag\n        self.lambda_blending = 0\n        if cross_entropy_flag:\n            self.cross_entropy_loss_fn = nn.CrossEntropyLoss()\n            self.lambda_blending = 0.3\n\n    def calculate_loss(self, target, output_embedding, output_cross_entropy=None):\n        if target is not None:\n            target = (target,)\n\n        loss_dict = {}\n        blended_loss = 0\n        if self.cross_entropy_flag:\n            assert output_cross_entropy is not None, ""Outputs for cross entropy loss is needed""\n\n            loss_inputs = self._gen_loss_inputs(target, output_cross_entropy)\n            cross_entropy_loss = self.cross_entropy_loss_fn(*loss_inputs)\n            blended_loss += self.lambda_blending * cross_entropy_loss\n            loss_dict[CROSS_ENTROPY + \'-loss\'] = [cross_entropy_loss.item()]\n\n        loss_inputs = self._gen_loss_inputs(target, output_embedding)\n        main_loss_outputs = self.main_loss_fn(*loss_inputs)\n        main_loss = main_loss_outputs[0] if type(main_loss_outputs) in (tuple, list) else main_loss_outputs\n        blended_loss += (1-self.lambda_blending) * main_loss\n        loss_dict[self.main_loss_type+\'-loss\'] = [main_loss.item()]\n\n        return blended_loss, loss_dict\n\n    @staticmethod\n    def _gen_loss_inputs(target, embedding):\n        if type(embedding) not in (tuple, list):\n            embedding = (embedding,)\n        loss_inputs = embedding\n        if target is not None:\n            if type(target) not in (tuple, list):\n                target = (target,)\n            loss_inputs += target\n        return loss_inputs\n\n\nclass NPairLoss(nn.Module):\n    """"""\n    N-Pair loss\n    Sohn, Kihyuk. ""Improved Deep Metric Learning with Multi-class N-pair Loss Objective,"" Advances in Neural Information\n    Processing Systems. 2016.\n    http://papers.nips.cc/paper/6199-improved-deep-metric-learning-with-multi-class-n-pair-loss-objective\n    """"""\n\n    def __init__(self, l2_reg=0.02):\n        super(NPairLoss, self).__init__()\n        self.l2_reg = l2_reg\n\n    def forward(self, embeddings, target):\n        n_pairs, n_negatives = self.get_n_pairs(target)\n\n        if embeddings.is_cuda:\n            n_pairs = n_pairs.cuda()\n            n_negatives = n_negatives.cuda()\n\n        anchors = embeddings[n_pairs[:, 0]]    # (n, embedding_size)\n        positives = embeddings[n_pairs[:, 1]]  # (n, embedding_size)\n        negatives = embeddings[n_negatives]    # (n, n-1, embedding_size)\n\n        losses = self.n_pair_loss(anchors, positives, negatives) \\\n            + self.l2_reg * self.l2_loss(anchors, positives)\n\n        return losses\n\n    @staticmethod\n    def get_n_pairs(labels):\n        """"""\n        Get index of n-pairs and n-negatives\n        :param labels: label vector of mini-batch\n        :return: A tuple of n_pairs (n, 2)\n                        and n_negatives (n, n-1)\n        """"""\n        labels = labels.cpu().data.numpy()\n        n_pairs = []\n\n        for label in set(labels):\n            label_mask = (labels == label)\n            label_indices = np.where(label_mask)[0]\n            if len(label_indices) < 2:\n                continue\n            anchor, positive = np.random.choice(label_indices, 2, replace=False)\n            n_pairs.append([anchor, positive])\n\n        n_pairs = np.array(n_pairs)\n\n        n_negatives = []\n        for i in range(len(n_pairs)):\n            negative = np.concatenate([n_pairs[:i, 1], n_pairs[i+1:, 1]])\n            n_negatives.append(negative)\n\n        n_negatives = np.array(n_negatives)\n\n        return torch.LongTensor(n_pairs), torch.LongTensor(n_negatives)\n\n    @staticmethod\n    def n_pair_loss(anchors, positives, negatives):\n        """"""\n        Calculates N-Pair loss\n        :param anchors: A torch.Tensor, (n, embedding_size)\n        :param positives: A torch.Tensor, (n, embedding_size)\n        :param negatives: A torch.Tensor, (n, n-1, embedding_size)\n        :return: A scalar\n        """"""\n        anchors = torch.unsqueeze(anchors, dim=1)  # (n, 1, embedding_size)\n        positives = torch.unsqueeze(positives, dim=1)  # (n, 1, embedding_size)\n\n        x = torch.matmul(anchors, (negatives - positives).transpose(1, 2))  # (n, 1, n-1)\n        x = torch.sum(torch.exp(x), 2)  # (n, 1)\n        loss = torch.mean(torch.log(1+x))\n        return loss\n\n    @staticmethod\n    def l2_loss(anchors, positives):\n        """"""\n        Calculates L2 norm regularization loss\n        :param anchors: A torch.Tensor, (n, embedding_size)\n        :param positives: A torch.Tensor, (n, embedding_size)\n        :return: A scalar\n        """"""\n        return torch.sum(anchors ** 2 + positives ** 2) / anchors.shape[0]\n\n\nclass AngularLoss(NPairLoss):\n    """"""\n    Angular loss\n    Wang, Jian. ""Deep Metric Learning with Angular Loss,"" ICCV, 2017\n    https://arxiv.org/pdf/1708.01682.pdf\n    """"""\n\n    def __init__(self, l2_reg=0.02, angle_bound=1., lambda_ang=2):\n        super(AngularLoss, self).__init__()\n        self.l2_reg = l2_reg\n        self.angle_bound = angle_bound\n        self.lambda_ang = lambda_ang\n        self.softplus = nn.Softplus()\n\n    def forward(self, embeddings, target):\n        n_pairs, n_negatives = self.get_n_pairs(target)\n\n        if embeddings.is_cuda:\n            n_pairs = n_pairs.cuda()\n            n_negatives = n_negatives.cuda()\n\n        anchors = embeddings[n_pairs[:, 0]]  # (n, embedding_size)\n        positives = embeddings[n_pairs[:, 1]]  # (n, embedding_size)\n        negatives = embeddings[n_negatives]  # (n, n-1, embedding_size)\n\n        losses = self.angular_loss(anchors, positives, negatives, self.angle_bound) \\\n                 + self.l2_reg * self.l2_loss(anchors, positives)\n\n        return losses\n\n    @staticmethod\n    def angular_loss(anchors, positives, negatives, angle_bound=1.):\n        """"""\n        Calculates angular loss\n        :param anchors: A torch.Tensor, (n, embedding_size)\n        :param positives: A torch.Tensor, (n, embedding_size)\n        :param negatives: A torch.Tensor, (n, n-1, embedding_size)\n        :param angle_bound: tan^2 angle\n        :return: A scalar\n        """"""\n        anchors = torch.unsqueeze(anchors, dim=1)  # (n, 1, embedding_size)\n        positives = torch.unsqueeze(positives, dim=1)  # (n, 1, embedding_size)\n\n        x = 4. * angle_bound * torch.matmul((anchors + positives), negatives.transpose(1, 2)) \\\n            - 2. * (1. + angle_bound) * torch.matmul(anchors, positives.transpose(1, 2))  # (n, 1, n-1)\n\n        # Preventing overflow\n        with torch.no_grad():\n            t = torch.max(x, dim=2)[0]\n\n        x = torch.exp(x - t.unsqueeze(dim=1))\n        x = torch.log(torch.exp(-t) + torch.sum(x, 2))\n        loss = torch.mean(t + x)\n\n        return loss\n\n\nclass NPairAngularLoss(AngularLoss):\n    """"""\n    Angular loss\n    Wang, Jian. ""Deep Metric Learning with Angular Loss,"" ICCV, 2017\n    https://arxiv.org/pdf/1708.01682.pdf\n    """"""\n\n    def __init__(self, l2_reg=0.02, angle_bound=1., lambda_ang=2):\n        super(NPairAngularLoss, self).__init__()\n        self.l2_reg = l2_reg\n        self.angle_bound = angle_bound\n        self.lambda_ang = lambda_ang\n\n    def forward(self, embeddings, target):\n        n_pairs, n_negatives = self.get_n_pairs(target)\n\n        if embeddings.is_cuda:\n            n_pairs = n_pairs.cuda()\n            n_negatives = n_negatives.cuda()\n\n        anchors = embeddings[n_pairs[:, 0]]    # (n, embedding_size)\n        positives = embeddings[n_pairs[:, 1]]  # (n, embedding_size)\n        negatives = embeddings[n_negatives]    # (n, n-1, embedding_size)\n\n        losses = self.n_pair_angular_loss(anchors, positives, negatives, self.angle_bound) \\\n            + self.l2_reg * self.l2_loss(anchors, positives)\n\n        return losses\n\n    def n_pair_angular_loss(self, anchors, positives, negatives, angle_bound=1.):\n        """"""\n        Calculates N-Pair angular loss\n        :param anchors: A torch.Tensor, (n, embedding_size)\n        :param positives: A torch.Tensor, (n, embedding_size)\n        :param negatives: A torch.Tensor, (n, n-1, embedding_size)\n        :param angle_bound: tan^2 angle\n        :return: A scalar, n-pair_loss + lambda * angular_loss\n        """"""\n        n_pair = self.n_pair_loss(anchors, positives, negatives)\n        angular = self.angular_loss(anchors, positives, negatives, angle_bound)\n\n        return (n_pair + self.lambda_ang * angular) / (1+self.lambda_ang)\n'"
main.py,6,"b'# -*- coding: utf_8 -*-\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom data_loader import train_data_loader, test_data_loader\n\n# Load initial models\nfrom networks import EmbeddingNetwork\n\n# Load batch sampler and train loss\nfrom datasets import BalancedBatchSampler\nfrom losses import BlendedLoss, MAIN_LOSS_CHOICES\n\nfrom trainer import fit\nfrom inference import retrieve\n\n\ndef load(file_path):\n    model.load_state_dict(torch.load(file_path))\n    print(\'model loaded!\')\n    return model\n\n\ndef infer(model, queries, db):\n    retrieval_results = retrieve(model, queries, db, input_size, infer_batch_size)\n\n    return list(zip(range(len(retrieval_results)), retrieval_results.items()))\n\n\ndef get_arguments():\n    args = argparse.ArgumentParser()\n\n    args.add_argument(\'--dataset-path\', type=str)\n    args.add_argument(\'--model-save-dir\', type=str)\n    args.add_argument(\'--model-to-test\', type=str)\n\n    # Hyperparameters\n    args.add_argument(\'--epochs\', type=int, default=20)\n    args.add_argument(\'--model\', type=str,\n                      choices=[\'densenet161\', \'resnet101\',  \'inceptionv3\', \'seresnext\'],\n                      default=\'densenet161\')\n    args.add_argument(\'--input-size\', type=int, default=224, help=\'size of input image\')\n    args.add_argument(\'--num-classes\', type=int, default=64, help=\'number of classes for batch sampler\')\n    args.add_argument(\'--num-samples\', type=int, default=4, help=\'number of samples per class for batch sampler\')\n    args.add_argument(\'--embedding-dim\', type=int, default=128, help=\'size of embedding dimension\')\n    args.add_argument(\'--feature-extracting\', type=bool, default=False)\n    args.add_argument(\'--use-pretrained\', type=bool, default=True)\n    args.add_argument(\'--lr\', type=float, default=1e-4)\n    args.add_argument(\'--scheduler\', type=str, choices=[\'StepLR\', \'MultiStepLR\'])\n    args.add_argument(\'--attention\', action=\'store_true\')\n    args.add_argument(\'--loss-type\', type=str, choices=MAIN_LOSS_CHOICES)\n    args.add_argument(\'--cross-entropy\', action=\'store_true\')\n    args.add_argument(\'--use-augmentation\', action=\'store_true\')\n\n    # Mode selection\n    args.add_argument(\'--mode\', type=str, default=\'train\', help=\'mode selection: train or test.\')\n\n    return args.parse_args()\n\n\nif __name__ == \'__main__\':\n    config = get_arguments()\n\n    dataset_path = config.dataset_path\n\n    # Model parameters\n    model_name = config.model\n    input_size = config.input_size\n    embedding_dim = config.embedding_dim\n    feature_extracting = config.feature_extracting\n    use_pretrained = config.use_pretrained\n    attention_flag = config.attention\n\n    # Training parameters\n    nb_epoch = config.epochs\n    loss_type = config.loss_type\n    cross_entropy_flag = config.cross_entropy\n    scheduler_name = config.scheduler\n    lr = config.lr\n\n    # Mini-batch parameters\n    num_classes = config.num_classes\n    num_samples = config.num_samples\n    use_augmentation = config.use_augmentation\n\n    infer_batch_size = 64\n    log_interval = 50\n\n    """""" Model """"""\n    model = EmbeddingNetwork(model_name=model_name,\n                             embedding_dim=embedding_dim,\n                             feature_extracting=feature_extracting,\n                             use_pretrained=use_pretrained,\n                             attention_flag=attention_flag,\n                             cross_entropy_flag=cross_entropy_flag)\n\n    if torch.cuda.device_count() > 1:\n        model = nn.DataParallel(model)\n\n    if config.mode == \'train\':\n\n        """""" Load data """"""\n        print(\'dataset path\', dataset_path)\n        train_dataset_path = dataset_path + \'/train/train_data\'\n\n        img_dataset = train_data_loader(data_path=train_dataset_path, img_size=input_size,\n                                        use_augment=use_augmentation)\n\n        # Balanced batch sampler and online train loader\n        train_batch_sampler = BalancedBatchSampler(img_dataset, n_classes=num_classes, n_samples=num_samples)\n        online_train_loader = torch.utils.data.DataLoader(img_dataset,\n                                                          batch_sampler=train_batch_sampler,\n                                                          num_workers=4,\n                                                          pin_memory=True)\n\n        device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n        # Gather the parameters to be optimized/updated.\n        params_to_update = model.parameters()\n        print(""Params to learn:"")\n        if feature_extracting:\n            params_to_update = []\n            for name, param in model.named_parameters():\n                if param.requires_grad:\n                    params_to_update.append(param)\n                    print(""\\t"", name)\n        else:\n            for name, param in model.named_parameters():\n                if param.requires_grad:\n                    print(""\\t"", name)\n\n        # Send the model to GPU\n        model = model.to(device)\n\n        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n        if scheduler_name == \'StepLR\':\n            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.1)\n        elif scheduler_name == \'MultiStepLR\':\n            if use_augmentation:\n                scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 30], gamma=0.1)\n            else:\n                scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 15, 20], gamma=0.1)\n        else:\n            raise ValueError(\'Invalid scheduler\')\n\n        # Loss function\n        loss_fn = BlendedLoss(loss_type, cross_entropy_flag)\n\n        # Train (fine-tune) model\n        fit(online_train_loader, model, loss_fn, optimizer, scheduler, nb_epoch,\n            device=device, log_interval=log_interval, save_model_to=config.model_save_dir)\n\n    elif config.mode == \'test\':\n        test_dataset_path = dataset_path + \'/test/test_data\'\n        queries, db = test_data_loader(test_dataset_path)\n        model = load(file_path=config.model_to_test)\n        result_dict = infer(model, queries, db)\n'"
networks.py,5,"b'""""""\nSE-ResNet, SE_ResNeXt codes are gently borrowed from\nhttps://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/senet.py\n""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\n\nfrom senet import se_resnext101_32x4d\n\n\nclass BaseNetwork(nn.Module):\n    """""" Load Pretrained Module """"""\n\n    def __init__(self, model_name, embedding_dim, feature_extracting, use_pretrained):\n        super(BaseNetwork, self).__init__()\n        self.model_name = model_name\n        self.embedding_dim = embedding_dim\n        self.feature_extracting = feature_extracting\n        self.use_pretrained = use_pretrained\n\n        self.model_ft = initialize_model(self.model_name,\n                                         self.embedding_dim,\n                                         self.feature_extracting,\n                                         self.use_pretrained)\n\n    def forward(self, x):\n        out = self.model_ft(x)\n        return out\n\n\nclass SelfAttention(nn.Module):\n    """""" Self attention Layer\n    https://github.com/heykeetae/Self-Attention-GAN""""""\n\n    def __init__(self, in_dim, activation):\n        super(SelfAttention, self).__init__()\n        self.chanel_in = in_dim\n        self.activation = activation\n\n        self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)\n        self.key_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)\n        self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n        self.gamma = nn.Parameter(torch.zeros(1))\n\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x):\n        """"""\n            inputs :\n                x : input feature maps( B X C X W X H)\n            returns :\n                out : self attention value + input feature\n                attention: B X N X N (N is Width*Height)\n        """"""\n        m_batchsize, C, width, height = x.size()\n        proj_query = self.query_conv(x).view(m_batchsize, -1, width * height).permute(0, 2, 1)  # B X CX(N)\n        proj_key = self.key_conv(x).view(m_batchsize, -1, width * height)  # B X C x (*W*H)\n        energy = torch.bmm(proj_query, proj_key)  # transpose check\n        attention = self.softmax(energy)  # BX (N) X (N)\n\n        proj_value = self.value_conv(x).view(m_batchsize, -1, width * height)  # B X C X N\n\n        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n        out = out.view(m_batchsize, C, width, height)\n\n        out = self.gamma * out + x\n        return out\n\n\nclass EmbeddingNetwork(BaseNetwork):\n    """""" Wrapping Modules to the BaseNetwork """"""\n\n    def __init__(self, model_name, embedding_dim, feature_extracting, use_pretrained,\n                 attention_flag=False, cross_entropy_flag=False, edge_cutting=False):\n        super(EmbeddingNetwork, self).__init__(model_name, embedding_dim, feature_extracting, use_pretrained)\n        self.attention_flag = attention_flag\n        self.cross_entropy_flag = cross_entropy_flag\n        self.edge_cutting = edge_cutting\n\n        self.model_ft_convs = nn.Sequential(*list(self.model_ft.children())[:-1])\n        self.model_ft_embedding = nn.Sequential(*list(self.model_ft.children())[-1:])\n\n        if self.attention_flag:\n            if self.model_name == \'densenet161\':\n                self.attention = SelfAttention(2208, \'relu\')\n            elif self.model_name == \'resnet101\':\n                self.attention = SelfAttention(2048, \'relu\')\n            elif self.model_name == \'inceptionv3\':\n                self.attention = SelfAttention(2048, \'relu\')\n            elif self.model_name == \'seresnext\':\n                self.attention = SelfAttention(2048, \'relu\')\n\n        if self.cross_entropy_flag:\n            self.fc_cross_entropy = nn.Linear(self.model_ft.classifier.in_features, 1000)\n\n    def forward(self, x):\n        x = self.model_ft_convs(x)\n        x = F.relu(x, inplace=True)\n\n        if self.attention_flag:\n            x = self.attention(x)\n\n        if self.edge_cutting:\n            x = F.adaptive_avg_pool2d(x[:, :, 1:-1, 1:-1], output_size=1).view(x.size(0), -1)\n        else:\n            x = F.adaptive_avg_pool2d(x, output_size=1).view(x.size(0), -1)\n            # x = gem(x).view(x.size(0), -1)\n        out_embedding = self.model_ft_embedding(x)\n\n        if self.cross_entropy_flag:\n            out_cross_entropy = self.fc_cross_entropy(x)\n            return out_embedding, out_cross_entropy\n        else:\n            return out_embedding\n\n\ndef set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False\n\n\ndef initialize_model(model_name, embedding_dim, feature_extracting, use_pretrained=True):\n    if model_name == ""densenet161"":\n        model_ft = models.densenet161(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extracting)\n        num_features = model_ft.classifier.in_features\n        model_ft.classifier = nn.Linear(num_features, embedding_dim)\n    elif model_name == ""resnet101"":\n        model_ft = models.resnet101(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extracting)\n        num_features = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_features, embedding_dim)\n    elif model_name == ""inceptionv3"":\n        model_ft = models.inception_v3(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extracting)\n        num_features = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_features, embedding_dim)\n    elif model_name == ""seresnext"":\n        model_ft = se_resnext101_32x4d(num_classes=1000)\n        set_parameter_requires_grad(model_ft, feature_extracting)\n        num_features = model_ft.last_linear.in_features\n        model_ft.last_linear = nn.Linear(num_features, embedding_dim)\n    else:\n        raise ValueError\n\n    return model_ft\n\n\n# GeM Pooling\ndef gem(x, p=3, eps=1e-6):\n    return F.adaptive_avg_pool2d(x.clamp(min=eps).pow(p), output_size=1).pow(1. / p)\n'"
senet.py,2,"b'""""""\nResNet code gently borrowed from\nhttps://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n""""""\nfrom __future__ import print_function, division, absolute_import\nfrom collections import OrderedDict\nimport math\n\nimport torch.nn as nn\nfrom torch.utils import model_zoo\n\nimport torch\n\n__all__ = [\'SENet\', \'senet154\', \'se_resnet50\', \'se_resnet101\', \'se_resnet152\',\n           \'se_resnext50_32x4d\', \'se_resnext101_32x4d\']\n\npretrained_settings = {\n    \'senet154\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/senet154-c7b49a05.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnet50\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet50-ce0d4300.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnet101\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet101-7e38fcc6.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnet152\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnet152-d17c99b7.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnext50_32x4d\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext50_32x4d-a260b3a4.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n    \'se_resnext101_32x4d\': {\n        \'imagenet\': {\n            \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext101_32x4d-3b2fe3d8.pth\',\n            \'input_space\': \'RGB\',\n            \'input_size\': [3, 224, 224],\n            \'input_range\': [0, 1],\n            \'mean\': [0.485, 0.456, 0.406],\n            \'std\': [0.229, 0.224, 0.225],\n            \'num_classes\': 1000\n        }\n    },\n}\n\n\nclass SEModule(nn.Module):\n\n    def __init__(self, channels, reduction):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1,\n                             padding=0)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1,\n                             padding=0)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        module_input = x\n        x = self.avg_pool(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return module_input * x\n\n\nclass Bottleneck(nn.Module):\n    """"""\n    Base class for bottlenecks that implements `forward()` method.\n    """"""\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = self.se_module(out) + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBottleneck(Bottleneck):\n    """"""\n    Bottleneck for SENet154.\n    """"""\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes * 2)\n        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n                               stride=stride, padding=1, groups=groups,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(planes * 4)\n        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNetBottleneck(Bottleneck):\n    """"""\n    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n    (the latter is used in the torchvision implementation of ResNet).\n    """"""\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEResNetBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False,\n                               stride=stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1,\n                               groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNeXtBottleneck(Bottleneck):\n    """"""\n    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n    """"""\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None, base_width=4):\n        super(SEResNeXtBottleneck, self).__init__()\n        width = math.floor(planes * (base_width / 64)) * groups\n        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,\n                               stride=1)\n        self.bn1 = nn.BatchNorm2d(width)\n        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n                               padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(width)\n        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SENet(nn.Module):\n\n    def __init__(self, block, layers, groups, reduction, dropout_p=0.2,\n                 inplanes=128, input_3x3=True, downsample_kernel_size=3,\n                 downsample_padding=1, num_classes=1000):\n        """"""\n        Parameters\n        ----------\n        block (nn.Module): Bottleneck class.\n            - For SENet154: SEBottleneck\n            - For SE-ResNet models: SEResNetBottleneck\n            - For SE-ResNeXt models:  SEResNeXtBottleneck\n        layers (list of ints): Number of residual blocks for 4 layers of the\n            network (layer1...layer4).\n        groups (int): Number of groups for the 3x3 convolution in each\n            bottleneck block.\n            - For SENet154: 64\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models:  32\n        reduction (int): Reduction ratio for Squeeze-and-Excitation modules.\n            - For all models: 16\n        dropout_p (float or None): Drop probability for the Dropout layer.\n            If `None` the Dropout layer is not used.\n            - For SENet154: 0.2\n            - For SE-ResNet models: None\n            - For SE-ResNeXt models: None\n        inplanes (int):  Number of input channels for layer1.\n            - For SENet154: 128\n            - For SE-ResNet models: 64\n            - For SE-ResNeXt models: 64\n        input_3x3 (bool): If `True`, use three 3x3 convolutions instead of\n            a single 7x7 convolution in layer0.\n            - For SENet154: True\n            - For SE-ResNet models: False\n            - For SE-ResNeXt models: False\n        downsample_kernel_size (int): Kernel size for downsampling convolutions\n            in layer2, layer3 and layer4.\n            - For SENet154: 3\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models: 1\n        downsample_padding (int): Padding for downsampling convolutions in\n            layer2, layer3 and layer4.\n            - For SENet154: 1\n            - For SE-ResNet models: 0\n            - For SE-ResNeXt models: 0\n        num_classes (int): Number of outputs in `last_linear` layer.\n            - For all models: 1000\n        """"""\n        super(SENet, self).__init__()\n        self.inplanes = inplanes\n        if input_3x3:\n            layer0_modules = [\n                (\'conv1\', nn.Conv2d(3, 64, 3, stride=2, padding=1,\n                                    bias=False)),\n                (\'bn1\', nn.BatchNorm2d(64)),\n                (\'relu1\', nn.ReLU(inplace=True)),\n                (\'conv2\', nn.Conv2d(64, 64, 3, stride=1, padding=1,\n                                    bias=False)),\n                (\'bn2\', nn.BatchNorm2d(64)),\n                (\'relu2\', nn.ReLU(inplace=True)),\n                (\'conv3\', nn.Conv2d(64, inplanes, 3, stride=1, padding=1,\n                                    bias=False)),\n                (\'bn3\', nn.BatchNorm2d(inplanes)),\n                (\'relu3\', nn.ReLU(inplace=True)),\n            ]\n        else:\n            layer0_modules = [\n                (\'conv1\', nn.Conv2d(3, inplanes, kernel_size=7, stride=2,\n                                    padding=3, bias=False)),\n                (\'bn1\', nn.BatchNorm2d(inplanes)),\n                (\'relu1\', nn.ReLU(inplace=True)),\n            ]\n        # To preserve compatibility with Caffe weights `ceil_mode=True`\n        # is used instead of `padding=1`.\n        layer0_modules.append((\'pool\', nn.MaxPool2d(3, stride=2,\n                                                    ceil_mode=True)))\n        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n        self.layer1 = self._make_layer(\n            block,\n            planes=64,\n            blocks=layers[0],\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=1,\n            downsample_padding=0\n        )\n        self.layer2 = self._make_layer(\n            block,\n            planes=128,\n            blocks=layers[1],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer3 = self._make_layer(\n            block,\n            planes=256,\n            blocks=layers[2],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer4 = self._make_layer(\n            block,\n            planes=512,\n            blocks=layers[3],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.avg_pool = nn.AvgPool2d(7, stride=1)\n        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n                    downsample_kernel_size=1, downsample_padding=0):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=downsample_kernel_size, stride=stride,\n                          padding=downsample_padding, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, groups, reduction, stride,\n                            downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups, reduction))\n\n        return nn.Sequential(*layers)\n\n    def features(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        #print(x.size())\n        return x\n\n    def logits(self, x):\n        x = self.avg_pool(x)\n        if self.dropout is not None:\n            x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.logits(x)\n        return x\n\n\ndef initialize_pretrained_model(model, num_classes, settings):\n    assert num_classes == settings[\'num_classes\'], \\\n        \'num_classes should be {}, but is {}\'.format(\n            settings[\'num_classes\'], num_classes)\n    model.load_state_dict(model_zoo.load_url(settings[\'url\']))\n    model.input_space = settings[\'input_space\']\n    model.input_size = settings[\'input_size\']\n    model.input_range = settings[\'input_range\']\n    model.mean = settings[\'mean\']\n    model.std = settings[\'std\']\n\n\ndef senet154(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEBottleneck, [3, 8, 36, 3], groups=64, reduction=16,\n                  dropout_p=0.2, num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'senet154\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet50(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEResNetBottleneck, [3, 4, 6, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnet50\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet101(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEResNetBottleneck, [3, 4, 23, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnet101\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet152(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEResNetBottleneck, [3, 8, 36, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnet152\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnext50_32x4d(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnext50_32x4d\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnext101_32x4d(num_classes=1000, pretrained=\'imagenet\'):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 23, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings[\'se_resnext101_32x4d\'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n'"
setup.py,0,"b""from distutils.core import setup\n\nsetup(\n    version='1.0',\n    description='image retrieval',\n    install_requires=[\n        'torch==1.0.0',\n        'torchvision==0.2.1',\n    ]\n)\n"""
trainer.py,3,"b'import os\n\nimport torch\nimport numpy as np\n\n\ndef save(model, ckpt_num, dir_name):\n    os.makedirs(dir_name, exist_ok=True)\n    if torch.cuda.device_count() > 1:\n        torch.save(model.module.state_dict(), os.path.join(dir_name, \'model_%s\' % ckpt_num))\n    else:\n        torch.save(model.state_dict(), os.path.join(dir_name, \'model_%s\' % ckpt_num))\n    print(\'model saved!\')\n\n\ndef fit(train_loader, model, loss_fn, optimizer, scheduler, nb_epoch,\n        device, log_interval, start_epoch=0, save_model_to=\'/tmp/save_model_to\'):\n    """"""\n    Loaders, model, loss function and metrics should work together for a given task,\n    i.e. The model should be able to process data output of loaders,\n    loss function should process target output of loaders and outputs from the model\n\n    Examples: Classification: batch loader, classification model, NLL loss, accuracy metric\n    Siamese network: Siamese loader, siamese model, contrastive loss\n    Online triplet learning: batch loader, embedding model, online triplet loss\n    """"""\n\n    # Save pre-trained model\n    save(model, 0, save_model_to)\n\n    for epoch in range(0, start_epoch):\n        scheduler.step()\n\n    for epoch in range(start_epoch, nb_epoch):\n        scheduler.step()\n\n        # Train stage\n        train_loss = train_epoch(train_loader, model, loss_fn, optimizer, device, log_interval)\n\n        log_dict = {\'epoch\': epoch + 1,\n                    \'epoch_total\': nb_epoch,\n                    \'loss\': float(train_loss),\n                    }\n\n        message = \'Epoch: {}/{}. Train set: Average loss: {:.4f}\'.format(epoch + 1, nb_epoch, train_loss)\n \n        print(message)\n        print(log_dict)\n        if (epoch + 1) % 5 == 0:\n            save(model, epoch + 1, save_model_to)\n\n\ndef train_epoch(train_loader, model, loss_fn, optimizer, device, log_interval):\n    model.train()\n    total_loss = 0\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        target = target if len(target) > 0 else None\n        if not type(data) in (tuple, list):\n            data = (data,)\n\n        data = tuple(d.to(device) for d in data)\n        if target is not None:\n            target = target.to(device)\n\n        optimizer.zero_grad()\n        if loss_fn.cross_entropy_flag:\n            output_embedding, output_cross_entropy = model(*data)\n            blended_loss, losses = loss_fn.calculate_loss(target, output_embedding, output_cross_entropy)\n        else:\n            output_embedding = model(*data)\n            blended_loss, losses = loss_fn.calculate_loss(target, output_embedding)\n        total_loss += blended_loss.item()\n        blended_loss.backward()\n\n        optimizer.step()\n\n        # Print log\n        if batch_idx % log_interval == 0:\n            message = \'Train: [{}/{} ({:.0f}%)]\'.format(\n                batch_idx * len(data[0]), len(train_loader.dataset), 100. * batch_idx / len(train_loader))\n            for name, value in losses.items():\n                message += \'\\t{}: {:.6f}\'.format(name, np.mean(value))\n \n            print(message)\n\n    total_loss /= (batch_idx + 1)\n    return total_loss\n'"
