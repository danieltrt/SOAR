file_path,api_count,code
data_utils.py,1,"b'import argparse\nimport os\nfrom os import listdir\nfrom os.path import join\n\nfrom PIL import Image\nfrom torch.utils.data.dataset import Dataset\nfrom torchvision.transforms import Compose, CenterCrop, Scale\nfrom tqdm import tqdm\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in [\'.png\', \'.jpg\', \'.jpeg\', \'.JPG\', \'.JPEG\', \'.PNG\'])\n\n\ndef is_video_file(filename):\n    return any(filename.endswith(extension) for extension in [\'.mp4\', \'.avi\', \'.mpg\', \'.mkv\', \'.wmv\', \'.flv\'])\n\n\ndef calculate_valid_crop_size(crop_size, upscale_factor):\n    return crop_size - (crop_size % upscale_factor)\n\n\ndef input_transform(crop_size, upscale_factor):\n    return Compose([\n        CenterCrop(crop_size),\n        Scale(crop_size // upscale_factor, interpolation=Image.BICUBIC)\n    ])\n\n\ndef target_transform(crop_size):\n    return Compose([\n        CenterCrop(crop_size)\n    ])\n\n\nclass DatasetFromFolder(Dataset):\n    def __init__(self, dataset_dir, upscale_factor, input_transform=None, target_transform=None):\n        super(DatasetFromFolder, self).__init__()\n        self.image_dir = dataset_dir + \'/SRF_\' + str(upscale_factor) + \'/data\'\n        self.target_dir = dataset_dir + \'/SRF_\' + str(upscale_factor) + \'/target\'\n        self.image_filenames = [join(self.image_dir, x) for x in listdir(self.image_dir) if is_image_file(x)]\n        self.target_filenames = [join(self.target_dir, x) for x in listdir(self.target_dir) if is_image_file(x)]\n        self.input_transform = input_transform\n        self.target_transform = target_transform\n\n    def __getitem__(self, index):\n        image, _, _ = Image.open(self.image_filenames[index]).convert(\'YCbCr\').split()\n        target, _, _ = Image.open(self.target_filenames[index]).convert(\'YCbCr\').split()\n        if self.input_transform:\n            image = self.input_transform(image)\n        if self.target_transform:\n            target = self.target_transform(target)\n\n        return image, target\n\n    def __len__(self):\n        return len(self.image_filenames)\n\n\ndef generate_dataset(data_type, upscale_factor):\n    images_name = [x for x in listdir(\'data/VOC2012/\' + data_type) if is_image_file(x)]\n    crop_size = calculate_valid_crop_size(256, upscale_factor)\n    lr_transform = input_transform(crop_size, upscale_factor)\n    hr_transform = target_transform(crop_size)\n\n    root = \'data/\' + data_type\n    if not os.path.exists(root):\n        os.makedirs(root)\n    path = root + \'/SRF_\' + str(upscale_factor)\n    if not os.path.exists(path):\n        os.makedirs(path)\n    image_path = path + \'/data\'\n    if not os.path.exists(image_path):\n        os.makedirs(image_path)\n    target_path = path + \'/target\'\n    if not os.path.exists(target_path):\n        os.makedirs(target_path)\n\n    for image_name in tqdm(images_name, desc=\'generate \' + data_type + \' dataset with upscale factor = \'\n            + str(upscale_factor) + \' from VOC2012\'):\n        image = Image.open(\'data/VOC2012/\' + data_type + \'/\' + image_name)\n        target = image.copy()\n        image = lr_transform(image)\n        target = hr_transform(target)\n\n        image.save(image_path + \'/\' + image_name)\n        target.save(target_path + \'/\' + image_name)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=\'Generate Super Resolution Dataset\')\n    parser.add_argument(\'--upscale_factor\', default=3, type=int, help=\'super resolution upscale factor\')\n    opt = parser.parse_args()\n    UPSCALE_FACTOR = opt.upscale_factor\n\n    generate_dataset(data_type=\'train\', upscale_factor=UPSCALE_FACTOR)\n    generate_dataset(data_type=\'val\', upscale_factor=UPSCALE_FACTOR)\n'"
model.py,2,"b'import torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Net(nn.Module):\n    def __init__(self, upscale_factor):\n        super(Net, self).__init__()\n\n        self.conv1 = nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2))\n        self.conv2 = nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1))\n        self.conv3 = nn.Conv2d(32, 1 * (upscale_factor ** 2), (3, 3), (1, 1), (1, 1))\n        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n\n    def forward(self, x):\n        x = F.tanh(self.conv1(x))\n        x = F.tanh(self.conv2(x))\n        x = F.sigmoid(self.pixel_shuffle(self.conv3(x)))\n        return x\n\n\nif __name__ == ""__main__"":\n    model = Net(upscale_factor=3)\n    print(model)\n'"
psnrmeter.py,4,"b'from math import log10\n\nimport torch\nfrom torchnet.meter import meter\n\n\nclass PSNRMeter(meter.Meter):\n    def __init__(self):\n        super(PSNRMeter, self).__init__()\n        self.reset()\n\n    def reset(self):\n        self.n = 0\n        self.sesum = 0.0\n\n    def add(self, output, target):\n        if not torch.is_tensor(output) and not torch.is_tensor(target):\n            output = torch.from_numpy(output)\n            target = torch.from_numpy(target)\n        output = output.cpu()\n        target = target.cpu()\n        self.n += output.numel()\n        self.sesum += torch.sum((output - target) ** 2)\n\n    def value(self):\n        mse = self.sesum / max(1, self.n)\n        psnr = 10 * log10(1 / mse)\n        return psnr\n'"
test_image.py,4,"b'import argparse\nimport os\nfrom os import listdir\n\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom torch.autograd import Variable\nfrom torchvision.transforms import ToTensor\nfrom tqdm import tqdm\n\nfrom data_utils import is_image_file\nfrom model import Net\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=\'Test Super Resolution\')\n    parser.add_argument(\'--upscale_factor\', default=3, type=int, help=\'super resolution upscale factor\')\n    parser.add_argument(\'--model_name\', default=\'epoch_3_100.pt\', type=str, help=\'super resolution model name\')\n    opt = parser.parse_args()\n\n    UPSCALE_FACTOR = opt.upscale_factor\n    MODEL_NAME = opt.model_name\n\n    path = \'data/test/SRF_\' + str(UPSCALE_FACTOR) + \'/data/\'\n    images_name = [x for x in listdir(path) if is_image_file(x)]\n    model = Net(upscale_factor=UPSCALE_FACTOR)\n    if torch.cuda.is_available():\n        model = model.cuda()\n    model.load_state_dict(torch.load(\'epochs/\' + MODEL_NAME))\n\n    out_path = \'results/SRF_\' + str(UPSCALE_FACTOR) + \'/\'\n    if not os.path.exists(out_path):\n        os.makedirs(out_path)\n    for image_name in tqdm(images_name, desc=\'convert LR images to HR images\'):\n\n        img = Image.open(path + image_name).convert(\'YCbCr\')\n        y, cb, cr = img.split()\n        image = Variable(ToTensor()(y)).view(1, -1, y.size[1], y.size[0])\n        if torch.cuda.is_available():\n            image = image.cuda()\n\n        out = model(image)\n        out = out.cpu()\n        out_img_y = out.data[0].numpy()\n        out_img_y *= 255.0\n        out_img_y = out_img_y.clip(0, 255)\n        out_img_y = Image.fromarray(np.uint8(out_img_y[0]), mode=\'L\')\n        out_img_cb = cb.resize(out_img_y.size, Image.BICUBIC)\n        out_img_cr = cr.resize(out_img_y.size, Image.BICUBIC)\n        out_img = Image.merge(\'YCbCr\', [out_img_y, out_img_cb, out_img_cr]).convert(\'RGB\')\n        out_img.save(out_path + image_name)\n'"
test_video.py,5,"b'import argparse\nimport os\nfrom os import listdir\n\nimport cv2\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom torch.autograd import Variable\nfrom torchvision.transforms import ToTensor\nfrom tqdm import tqdm\n\nfrom data_utils import is_video_file\nfrom model import Net\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=\'Test Super Resolution\')\n    parser.add_argument(\'--upscale_factor\', default=3, type=int, help=\'super resolution upscale factor\')\n    parser.add_argument(\'--is_real_time\', default=False, type=bool, help=\'super resolution real time to show\')\n    parser.add_argument(\'--delay_time\', default=1, type=int, help=\'super resolution delay time to show\')\n    parser.add_argument(\'--model_name\', default=\'epoch_3_100.pt\', type=str, help=\'super resolution model name\')\n    opt = parser.parse_args()\n\n    UPSCALE_FACTOR = opt.upscale_factor\n    IS_REAL_TIME = opt.is_real_time\n    DELAY_TIME = opt.delay_time\n    MODEL_NAME = opt.model_name\n\n    path = \'data/test/SRF_\' + str(UPSCALE_FACTOR) + \'/video/\'\n    videos_name = [x for x in listdir(path) if is_video_file(x)]\n    model = Net(upscale_factor=UPSCALE_FACTOR)\n    if torch.cuda.is_available():\n        model = model.cuda()\n    # for cpu\n    # model.load_state_dict(torch.load(\'epochs/\' + MODEL_NAME, map_location=lambda storage, loc: storage))\n    model.load_state_dict(torch.load(\'epochs/\' + MODEL_NAME))\n\n    out_path = \'results/SRF_\' + str(UPSCALE_FACTOR) + \'/\'\n    if not os.path.exists(out_path):\n        os.makedirs(out_path)\n    for video_name in tqdm(videos_name, desc=\'convert LR videos to HR videos\'):\n        videoCapture = cv2.VideoCapture(path + video_name)\n        if not IS_REAL_TIME:\n            fps = videoCapture.get(cv2.CAP_PROP_FPS)\n            size = (int(videoCapture.get(cv2.CAP_PROP_FRAME_WIDTH) * UPSCALE_FACTOR),\n                    int(videoCapture.get(cv2.CAP_PROP_FRAME_HEIGHT)) * UPSCALE_FACTOR)\n            output_name = out_path + video_name.split(\'.\')[0] + \'.avi\'\n            videoWriter = cv2.VideoWriter(output_name, cv2.VideoWriter_fourcc(*\'MPEG\'), fps, size)\n        # read frame\n        success, frame = videoCapture.read()\n        while success:\n            img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)).convert(\'YCbCr\')\n            y, cb, cr = img.split()\n            image = Variable(ToTensor()(y)).view(1, -1, y.size[1], y.size[0])\n            if torch.cuda.is_available():\n                image = image.cuda()\n\n            out = model(image)\n            out = out.cpu()\n            out_img_y = out.data[0].numpy()\n            out_img_y *= 255.0\n            out_img_y = out_img_y.clip(0, 255)\n            out_img_y = Image.fromarray(np.uint8(out_img_y[0]), mode=\'L\')\n            out_img_cb = cb.resize(out_img_y.size, Image.BICUBIC)\n            out_img_cr = cr.resize(out_img_y.size, Image.BICUBIC)\n            out_img = Image.merge(\'YCbCr\', [out_img_y, out_img_cb, out_img_cr]).convert(\'RGB\')\n            out_img = cv2.cvtColor(np.asarray(out_img), cv2.COLOR_RGB2BGR)\n\n            if IS_REAL_TIME:\n                cv2.imshow(\'LR Video \', frame)\n                cv2.imshow(\'SR Video \', out_img)\n                cv2.waitKey(DELAY_TIME)\n            else:\n                # save video\n                videoWriter.write(out_img)\n            # next frame\n            success, frame = videoCapture.read()\n'"
train.py,8,"b'import argparse\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchnet as tnt\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nfrom torch.optim.lr_scheduler import MultiStepLR\nfrom torch.utils.data import DataLoader\nfrom torchnet.engine import Engine\nfrom torchnet.logger import VisdomPlotLogger\nfrom tqdm import tqdm\n\nfrom data_utils import DatasetFromFolder\nfrom model import Net\nfrom psnrmeter import PSNRMeter\n\n\ndef processor(sample):\n    data, target, training = sample\n    data = Variable(data)\n    target = Variable(target)\n    if torch.cuda.is_available():\n        data = data.cuda()\n        target = target.cuda()\n\n    output = model(data)\n    loss = criterion(output, target)\n\n    return loss, output\n\n\ndef on_sample(state):\n    state[\'sample\'].append(state[\'train\'])\n\n\ndef reset_meters():\n    meter_psnr.reset()\n    meter_loss.reset()\n\n\ndef on_forward(state):\n    meter_psnr.add(state[\'output\'].data, state[\'sample\'][1])\n    meter_loss.add(state[\'loss\'].data[0])\n\n\ndef on_start_epoch(state):\n    reset_meters()\n    scheduler.step()\n    state[\'iterator\'] = tqdm(state[\'iterator\'])\n\n\ndef on_end_epoch(state):\n    print(\'[Epoch %d] Train Loss: %.4f (PSNR: %.2f db)\' % (\n        state[\'epoch\'], meter_loss.value()[0], meter_psnr.value()))\n\n    train_loss_logger.log(state[\'epoch\'], meter_loss.value()[0])\n    train_psnr_logger.log(state[\'epoch\'], meter_psnr.value())\n\n    reset_meters()\n\n    engine.test(processor, val_loader)\n    val_loss_logger.log(state[\'epoch\'], meter_loss.value()[0])\n    val_psnr_logger.log(state[\'epoch\'], meter_psnr.value())\n\n    print(\'[Epoch %d] Val Loss: %.4f (PSNR: %.2f db)\' % (\n        state[\'epoch\'], meter_loss.value()[0], meter_psnr.value()))\n\n    torch.save(model.state_dict(), \'epochs/epoch_%d_%d.pt\' % (UPSCALE_FACTOR, state[\'epoch\']))\n\n\nif __name__ == ""__main__"":\n\n    parser = argparse.ArgumentParser(description=\'Train Super Resolution\')\n    parser.add_argument(\'--upscale_factor\', default=3, type=int, help=\'super resolution upscale factor\')\n    parser.add_argument(\'--num_epochs\', default=100, type=int, help=\'super resolution epochs number\')\n    opt = parser.parse_args()\n\n    UPSCALE_FACTOR = opt.upscale_factor\n    NUM_EPOCHS = opt.num_epochs\n\n    train_set = DatasetFromFolder(\'data/train\', upscale_factor=UPSCALE_FACTOR, input_transform=transforms.ToTensor(),\n                                  target_transform=transforms.ToTensor())\n    val_set = DatasetFromFolder(\'data/val\', upscale_factor=UPSCALE_FACTOR, input_transform=transforms.ToTensor(),\n                                target_transform=transforms.ToTensor())\n    train_loader = DataLoader(dataset=train_set, num_workers=4, batch_size=64, shuffle=True)\n    val_loader = DataLoader(dataset=val_set, num_workers=4, batch_size=64, shuffle=False)\n\n    model = Net(upscale_factor=UPSCALE_FACTOR)\n    criterion = nn.MSELoss()\n    if torch.cuda.is_available():\n        model = model.cuda()\n        criterion = criterion.cuda()\n\n    print(\'# parameters:\', sum(param.numel() for param in model.parameters()))\n\n    optimizer = optim.Adam(model.parameters(), lr=1e-2)\n    scheduler = MultiStepLR(optimizer, milestones=[30, 80], gamma=0.1)\n\n    engine = Engine()\n    meter_loss = tnt.meter.AverageValueMeter()\n    meter_psnr = PSNRMeter()\n\n    train_loss_logger = VisdomPlotLogger(\'line\', opts={\'title\': \'Train Loss\'})\n    train_psnr_logger = VisdomPlotLogger(\'line\', opts={\'title\': \'Train PSNR\'})\n    val_loss_logger = VisdomPlotLogger(\'line\', opts={\'title\': \'Val Loss\'})\n    val_psnr_logger = VisdomPlotLogger(\'line\', opts={\'title\': \'Val PSNR\'})\n\n    engine.hooks[\'on_sample\'] = on_sample\n    engine.hooks[\'on_forward\'] = on_forward\n    engine.hooks[\'on_start_epoch\'] = on_start_epoch\n    engine.hooks[\'on_end_epoch\'] = on_end_epoch\n\n    engine.train(processor, train_loader, maxepoch=NUM_EPOCHS, optimizer=optimizer)\n'"
