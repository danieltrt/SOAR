file_path,api_count,code
data_utils.py,1,"b""from os import listdir\nfrom os.path import join\n\nfrom PIL import Image\nfrom torch.utils.data.dataset import Dataset\nfrom torchvision.transforms import Compose, RandomCrop, ToTensor, ToPILImage, CenterCrop, Resize\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in ['.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG'])\n\n\ndef calculate_valid_crop_size(crop_size, upscale_factor):\n    return crop_size - (crop_size % upscale_factor)\n\n\ndef train_hr_transform(crop_size):\n    return Compose([\n        RandomCrop(crop_size),\n        ToTensor(),\n    ])\n\n\ndef train_lr_transform(crop_size, upscale_factor):\n    return Compose([\n        ToPILImage(),\n        Resize(crop_size // upscale_factor, interpolation=Image.BICUBIC),\n        ToTensor()\n    ])\n\n\ndef display_transform():\n    return Compose([\n        ToPILImage(),\n        Resize(400),\n        CenterCrop(400),\n        ToTensor()\n    ])\n\n\nclass TrainDatasetFromFolder(Dataset):\n    def __init__(self, dataset_dir, crop_size, upscale_factor):\n        super(TrainDatasetFromFolder, self).__init__()\n        self.image_filenames = [join(dataset_dir, x) for x in listdir(dataset_dir) if is_image_file(x)]\n        crop_size = calculate_valid_crop_size(crop_size, upscale_factor)\n        self.hr_transform = train_hr_transform(crop_size)\n        self.lr_transform = train_lr_transform(crop_size, upscale_factor)\n\n    def __getitem__(self, index):\n        hr_image = self.hr_transform(Image.open(self.image_filenames[index]))\n        lr_image = self.lr_transform(hr_image)\n        return lr_image, hr_image\n\n    def __len__(self):\n        return len(self.image_filenames)\n\n\nclass ValDatasetFromFolder(Dataset):\n    def __init__(self, dataset_dir, upscale_factor):\n        super(ValDatasetFromFolder, self).__init__()\n        self.upscale_factor = upscale_factor\n        self.image_filenames = [join(dataset_dir, x) for x in listdir(dataset_dir) if is_image_file(x)]\n\n    def __getitem__(self, index):\n        hr_image = Image.open(self.image_filenames[index])\n        w, h = hr_image.size\n        crop_size = calculate_valid_crop_size(min(w, h), self.upscale_factor)\n        lr_scale = Resize(crop_size // self.upscale_factor, interpolation=Image.BICUBIC)\n        hr_scale = Resize(crop_size, interpolation=Image.BICUBIC)\n        hr_image = CenterCrop(crop_size)(hr_image)\n        lr_image = lr_scale(hr_image)\n        hr_restore_img = hr_scale(lr_image)\n        return ToTensor()(lr_image), ToTensor()(hr_restore_img), ToTensor()(hr_image)\n\n    def __len__(self):\n        return len(self.image_filenames)\n\n\nclass TestDatasetFromFolder(Dataset):\n    def __init__(self, dataset_dir, upscale_factor):\n        super(TestDatasetFromFolder, self).__init__()\n        self.lr_path = dataset_dir + '/SRF_' + str(upscale_factor) + '/data/'\n        self.hr_path = dataset_dir + '/SRF_' + str(upscale_factor) + '/target/'\n        self.upscale_factor = upscale_factor\n        self.lr_filenames = [join(self.lr_path, x) for x in listdir(self.lr_path) if is_image_file(x)]\n        self.hr_filenames = [join(self.hr_path, x) for x in listdir(self.hr_path) if is_image_file(x)]\n\n    def __getitem__(self, index):\n        image_name = self.lr_filenames[index].split('/')[-1]\n        lr_image = Image.open(self.lr_filenames[index])\n        w, h = lr_image.size\n        hr_image = Image.open(self.hr_filenames[index])\n        hr_scale = Resize((self.upscale_factor * h, self.upscale_factor * w), interpolation=Image.BICUBIC)\n        hr_restore_img = hr_scale(lr_image)\n        return image_name, ToTensor()(lr_image), ToTensor()(hr_restore_img), ToTensor()(hr_image)\n\n    def __len__(self):\n        return len(self.lr_filenames)\n"""
loss.py,3,"b'import torch\nfrom torch import nn\nfrom torchvision.models.vgg import vgg16\n\n\nclass GeneratorLoss(nn.Module):\n    def __init__(self):\n        super(GeneratorLoss, self).__init__()\n        vgg = vgg16(pretrained=True)\n        loss_network = nn.Sequential(*list(vgg.features)[:31]).eval()\n        for param in loss_network.parameters():\n            param.requires_grad = False\n        self.loss_network = loss_network\n        self.mse_loss = nn.MSELoss()\n        self.tv_loss = TVLoss()\n\n    def forward(self, out_labels, out_images, target_images):\n        # Adversarial Loss\n        adversarial_loss = torch.mean(1 - out_labels)\n        # Perception Loss\n        perception_loss = self.mse_loss(self.loss_network(out_images), self.loss_network(target_images))\n        # Image Loss\n        image_loss = self.mse_loss(out_images, target_images)\n        # TV Loss\n        tv_loss = self.tv_loss(out_images)\n        return image_loss + 0.001 * adversarial_loss + 0.006 * perception_loss + 2e-8 * tv_loss\n\n\nclass TVLoss(nn.Module):\n    def __init__(self, tv_loss_weight=1):\n        super(TVLoss, self).__init__()\n        self.tv_loss_weight = tv_loss_weight\n\n    def forward(self, x):\n        batch_size = x.size()[0]\n        h_x = x.size()[2]\n        w_x = x.size()[3]\n        count_h = self.tensor_size(x[:, :, 1:, :])\n        count_w = self.tensor_size(x[:, :, :, 1:])\n        h_tv = torch.pow((x[:, :, 1:, :] - x[:, :, :h_x - 1, :]), 2).sum()\n        w_tv = torch.pow((x[:, :, :, 1:] - x[:, :, :, :w_x - 1]), 2).sum()\n        return self.tv_loss_weight * 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n\n    @staticmethod\n    def tensor_size(t):\n        return t.size()[1] * t.size()[2] * t.size()[3]\n\n\nif __name__ == ""__main__"":\n    g_loss = GeneratorLoss()\n    print(g_loss)\n'"
model.py,2,"b'import math\nimport torch\nfrom torch import nn\n\n\nclass Generator(nn.Module):\n    def __init__(self, scale_factor):\n        upsample_block_num = int(math.log(scale_factor, 2))\n\n        super(Generator, self).__init__()\n        self.block1 = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=9, padding=4),\n            nn.PReLU()\n        )\n        self.block2 = ResidualBlock(64)\n        self.block3 = ResidualBlock(64)\n        self.block4 = ResidualBlock(64)\n        self.block5 = ResidualBlock(64)\n        self.block6 = ResidualBlock(64)\n        self.block7 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64)\n        )\n        block8 = [UpsampleBLock(64, 2) for _ in range(upsample_block_num)]\n        block8.append(nn.Conv2d(64, 3, kernel_size=9, padding=4))\n        self.block8 = nn.Sequential(*block8)\n\n    def forward(self, x):\n        block1 = self.block1(x)\n        block2 = self.block2(block1)\n        block3 = self.block3(block2)\n        block4 = self.block4(block3)\n        block5 = self.block5(block4)\n        block6 = self.block6(block5)\n        block7 = self.block7(block6)\n        block8 = self.block8(block1 + block7)\n\n        return (torch.tanh(block8) + 1) / 2\n\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n            nn.LeakyReLU(0.2),\n\n            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(0.2),\n\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2),\n\n            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2),\n\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2),\n\n            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2),\n\n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2),\n\n            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2),\n\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(512, 1024, kernel_size=1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(1024, 1, kernel_size=1)\n        )\n\n    def forward(self, x):\n        batch_size = x.size(0)\n        return torch.sigmoid(self.net(x).view(batch_size))\n\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(channels)\n        self.prelu = nn.PReLU()\n        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(channels)\n\n    def forward(self, x):\n        residual = self.conv1(x)\n        residual = self.bn1(residual)\n        residual = self.prelu(residual)\n        residual = self.conv2(residual)\n        residual = self.bn2(residual)\n\n        return x + residual\n\n\nclass UpsampleBLock(nn.Module):\n    def __init__(self, in_channels, up_scale):\n        super(UpsampleBLock, self).__init__()\n        self.conv = nn.Conv2d(in_channels, in_channels * up_scale ** 2, kernel_size=3, padding=1)\n        self.pixel_shuffle = nn.PixelShuffle(up_scale)\n        self.prelu = nn.PReLU()\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.pixel_shuffle(x)\n        x = self.prelu(x)\n        return x\n'"
test_benchmark.py,6,"b""import argparse\nimport os\nfrom math import log10\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torchvision.utils as utils\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nimport pytorch_ssim\nfrom data_utils import TestDatasetFromFolder, display_transform\nfrom model import Generator\n\nparser = argparse.ArgumentParser(description='Test Benchmark Datasets')\nparser.add_argument('--upscale_factor', default=4, type=int, help='super resolution upscale factor')\nparser.add_argument('--model_name', default='netG_epoch_4_100.pth', type=str, help='generator model epoch name')\nopt = parser.parse_args()\n\nUPSCALE_FACTOR = opt.upscale_factor\nMODEL_NAME = opt.model_name\n\nresults = {'Set5': {'psnr': [], 'ssim': []}, 'Set14': {'psnr': [], 'ssim': []}, 'BSD100': {'psnr': [], 'ssim': []},\n           'Urban100': {'psnr': [], 'ssim': []}, 'SunHays80': {'psnr': [], 'ssim': []}}\n\nmodel = Generator(UPSCALE_FACTOR).eval()\nif torch.cuda.is_available():\n    model = model.cuda()\nmodel.load_state_dict(torch.load('epochs/' + MODEL_NAME))\n\ntest_set = TestDatasetFromFolder('data/test', upscale_factor=UPSCALE_FACTOR)\ntest_loader = DataLoader(dataset=test_set, num_workers=4, batch_size=1, shuffle=False)\ntest_bar = tqdm(test_loader, desc='[testing benchmark datasets]')\n\nout_path = 'benchmark_results/SRF_' + str(UPSCALE_FACTOR) + '/'\nif not os.path.exists(out_path):\n    os.makedirs(out_path)\n\nfor image_name, lr_image, hr_restore_img, hr_image in test_bar:\n    image_name = image_name[0]\n    lr_image = Variable(lr_image, volatile=True)\n    hr_image = Variable(hr_image, volatile=True)\n    if torch.cuda.is_available():\n        lr_image = lr_image.cuda()\n        hr_image = hr_image.cuda()\n\n    sr_image = model(lr_image)\n    mse = ((hr_image - sr_image) ** 2).data.mean()\n    psnr = 10 * log10(1 / mse)\n    ssim = pytorch_ssim.ssim(sr_image, hr_image).data[0]\n\n    test_images = torch.stack(\n        [display_transform()(hr_restore_img.squeeze(0)), display_transform()(hr_image.data.cpu().squeeze(0)),\n         display_transform()(sr_image.data.cpu().squeeze(0))])\n    image = utils.make_grid(test_images, nrow=3, padding=5)\n    utils.save_image(image, out_path + image_name.split('.')[0] + '_psnr_%.4f_ssim_%.4f.' % (psnr, ssim) +\n                     image_name.split('.')[-1], padding=5)\n\n    # save psnr\\ssim\n    results[image_name.split('_')[0]]['psnr'].append(psnr)\n    results[image_name.split('_')[0]]['ssim'].append(ssim)\n\nout_path = 'statistics/'\nsaved_results = {'psnr': [], 'ssim': []}\nfor item in results.values():\n    psnr = np.array(item['psnr'])\n    ssim = np.array(item['ssim'])\n    if (len(psnr) == 0) or (len(ssim) == 0):\n        psnr = 'No data'\n        ssim = 'No data'\n    else:\n        psnr = psnr.mean()\n        ssim = ssim.mean()\n    saved_results['psnr'].append(psnr)\n    saved_results['ssim'].append(ssim)\n\ndata_frame = pd.DataFrame(saved_results, results.keys())\ndata_frame.to_csv(out_path + 'srf_' + str(UPSCALE_FACTOR) + '_test_results.csv', index_label='DataSet')\n"""
test_image.py,3,"b""import argparse\nimport time\n\nimport torch\nfrom PIL import Image\nfrom torch.autograd import Variable\nfrom torchvision.transforms import ToTensor, ToPILImage\n\nfrom model import Generator\n\nparser = argparse.ArgumentParser(description='Test Single Image')\nparser.add_argument('--upscale_factor', default=4, type=int, help='super resolution upscale factor')\nparser.add_argument('--test_mode', default='GPU', type=str, choices=['GPU', 'CPU'], help='using GPU or CPU')\nparser.add_argument('--image_name', type=str, help='test low resolution image name')\nparser.add_argument('--model_name', default='netG_epoch_4_100.pth', type=str, help='generator model epoch name')\nopt = parser.parse_args()\n\nUPSCALE_FACTOR = opt.upscale_factor\nTEST_MODE = True if opt.test_mode == 'GPU' else False\nIMAGE_NAME = opt.image_name\nMODEL_NAME = opt.model_name\n\nmodel = Generator(UPSCALE_FACTOR).eval()\nif TEST_MODE:\n    model.cuda()\n    model.load_state_dict(torch.load('epochs/' + MODEL_NAME))\nelse:\n    model.load_state_dict(torch.load('epochs/' + MODEL_NAME, map_location=lambda storage, loc: storage))\n\nimage = Image.open(IMAGE_NAME)\nimage = Variable(ToTensor()(image), volatile=True).unsqueeze(0)\nif TEST_MODE:\n    image = image.cuda()\n\nstart = time.clock()\nout = model(image)\nelapsed = (time.clock() - start)\nprint('cost' + str(elapsed) + 's')\nout_img = ToPILImage()(out[0].data.cpu())\nout_img.save('out_srf_' + str(UPSCALE_FACTOR) + '_' + IMAGE_NAME)\n"""
test_video.py,5,"b'import argparse\n\nimport cv2\nimport numpy as np\nimport torch\nimport torchvision.transforms as transforms\nfrom PIL import Image\nfrom torch.autograd import Variable\nfrom torchvision.transforms import ToTensor, ToPILImage\nfrom tqdm import tqdm\n\nfrom model import Generator\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=\'Test Single Video\')\n    parser.add_argument(\'--upscale_factor\', default=4, type=int, help=\'super resolution upscale factor\')\n    parser.add_argument(\'--video_name\', type=str, help=\'test low resolution video name\')\n    parser.add_argument(\'--model_name\', default=\'netG_epoch_4_100.pth\', type=str, help=\'generator model epoch name\')\n    opt = parser.parse_args()\n\n    UPSCALE_FACTOR = opt.upscale_factor\n    VIDEO_NAME = opt.video_name\n    MODEL_NAME = opt.model_name\n\n    model = Generator(UPSCALE_FACTOR).eval()\n    if torch.cuda.is_available():\n        model = model.cuda()\n    # for cpu\n    # model.load_state_dict(torch.load(\'epochs/\' + MODEL_NAME, map_location=lambda storage, loc: storage))\n    model.load_state_dict(torch.load(\'epochs/\' + MODEL_NAME))\n\n    videoCapture = cv2.VideoCapture(VIDEO_NAME)\n    fps = videoCapture.get(cv2.CAP_PROP_FPS)\n    frame_numbers = videoCapture.get(cv2.CAP_PROP_FRAME_COUNT)\n    sr_video_size = (int(videoCapture.get(cv2.CAP_PROP_FRAME_WIDTH) * UPSCALE_FACTOR),\n                     int(videoCapture.get(cv2.CAP_PROP_FRAME_HEIGHT)) * UPSCALE_FACTOR)\n    compared_video_size = (int(videoCapture.get(cv2.CAP_PROP_FRAME_WIDTH) * UPSCALE_FACTOR * 2 + 10),\n                           int(videoCapture.get(cv2.CAP_PROP_FRAME_HEIGHT)) * UPSCALE_FACTOR + 10 + int(\n                               int(videoCapture.get(cv2.CAP_PROP_FRAME_WIDTH) * UPSCALE_FACTOR * 2 + 10) / int(\n                                   10 * int(int(\n                                       videoCapture.get(cv2.CAP_PROP_FRAME_WIDTH) * UPSCALE_FACTOR) // 5 + 1)) * int(\n                                   int(videoCapture.get(cv2.CAP_PROP_FRAME_WIDTH) * UPSCALE_FACTOR) // 5 - 9)))\n    output_sr_name = \'out_srf_\' + str(UPSCALE_FACTOR) + \'_\' + VIDEO_NAME.split(\'.\')[0] + \'.avi\'\n    output_compared_name = \'compare_srf_\' + str(UPSCALE_FACTOR) + \'_\' + VIDEO_NAME.split(\'.\')[0] + \'.avi\'\n    sr_video_writer = cv2.VideoWriter(output_sr_name, cv2.VideoWriter_fourcc(\'M\', \'P\', \'E\', \'G\'), fps, sr_video_size)\n    compared_video_writer = cv2.VideoWriter(output_compared_name, cv2.VideoWriter_fourcc(\'M\', \'P\', \'E\', \'G\'), fps,\n                                            compared_video_size)\n    # read frame\n    success, frame = videoCapture.read()\n    test_bar = tqdm(range(int(frame_numbers)), desc=\'[processing video and saving result videos]\')\n    for index in test_bar:\n        if success:\n            image = Variable(ToTensor()(frame), volatile=True).unsqueeze(0)\n            if torch.cuda.is_available():\n                image = image.cuda()\n\n            out = model(image)\n            out = out.cpu()\n            out_img = out.data[0].numpy()\n            out_img *= 255.0\n            out_img = (np.uint8(out_img)).transpose((1, 2, 0))\n            # save sr video\n            sr_video_writer.write(out_img)\n\n            # make compared video and crop shot of left top\\right top\\center\\left bottom\\right bottom\n            out_img = ToPILImage()(out_img)\n            crop_out_imgs = transforms.FiveCrop(size=out_img.width // 5 - 9)(out_img)\n            crop_out_imgs = [np.asarray(transforms.Pad(padding=(10, 5, 0, 0))(img)) for img in crop_out_imgs]\n            out_img = transforms.Pad(padding=(5, 0, 0, 5))(out_img)\n            compared_img = transforms.Resize(size=(sr_video_size[1], sr_video_size[0]), interpolation=Image.BICUBIC)(\n                ToPILImage()(frame))\n            crop_compared_imgs = transforms.FiveCrop(size=compared_img.width // 5 - 9)(compared_img)\n            crop_compared_imgs = [np.asarray(transforms.Pad(padding=(0, 5, 10, 0))(img)) for img in crop_compared_imgs]\n            compared_img = transforms.Pad(padding=(0, 0, 5, 5))(compared_img)\n            # concatenate all the pictures to one single picture\n            top_image = np.concatenate((np.asarray(compared_img), np.asarray(out_img)), axis=1)\n            bottom_image = np.concatenate(crop_compared_imgs + crop_out_imgs, axis=1)\n            bottom_image = np.asarray(transforms.Resize(\n                size=(int(top_image.shape[1] / bottom_image.shape[1] * bottom_image.shape[0]), top_image.shape[1]))(\n                ToPILImage()(bottom_image)))\n            final_image = np.concatenate((top_image, bottom_image))\n            # save compared video\n            compared_video_writer.write(final_image)\n            # next frame\n            success, frame = videoCapture.read()\n'"
train.py,13,"b""import argparse\nimport os\nfrom math import log10\n\nimport pandas as pd\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.utils as utils\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nimport pytorch_ssim\nfrom data_utils import TrainDatasetFromFolder, ValDatasetFromFolder, display_transform\nfrom loss import GeneratorLoss\nfrom model import Generator, Discriminator\n\nparser = argparse.ArgumentParser(description='Train Super Resolution Models')\nparser.add_argument('--crop_size', default=88, type=int, help='training images crop size')\nparser.add_argument('--upscale_factor', default=4, type=int, choices=[2, 4, 8],\n                    help='super resolution upscale factor')\nparser.add_argument('--num_epochs', default=100, type=int, help='train epoch number')\n\n\nif __name__ == '__main__':\n    opt = parser.parse_args()\n    \n    CROP_SIZE = opt.crop_size\n    UPSCALE_FACTOR = opt.upscale_factor\n    NUM_EPOCHS = opt.num_epochs\n    \n    train_set = TrainDatasetFromFolder('data/DIV2K_train_HR', crop_size=CROP_SIZE, upscale_factor=UPSCALE_FACTOR)\n    val_set = ValDatasetFromFolder('data/DIV2K_valid_HR', upscale_factor=UPSCALE_FACTOR)\n    train_loader = DataLoader(dataset=train_set, num_workers=4, batch_size=64, shuffle=True)\n    val_loader = DataLoader(dataset=val_set, num_workers=4, batch_size=1, shuffle=False)\n    \n    netG = Generator(UPSCALE_FACTOR)\n    print('# generator parameters:', sum(param.numel() for param in netG.parameters()))\n    netD = Discriminator()\n    print('# discriminator parameters:', sum(param.numel() for param in netD.parameters()))\n    \n    generator_criterion = GeneratorLoss()\n    \n    if torch.cuda.is_available():\n        netG.cuda()\n        netD.cuda()\n        generator_criterion.cuda()\n    \n    optimizerG = optim.Adam(netG.parameters())\n    optimizerD = optim.Adam(netD.parameters())\n    \n    results = {'d_loss': [], 'g_loss': [], 'd_score': [], 'g_score': [], 'psnr': [], 'ssim': []}\n    \n    for epoch in range(1, NUM_EPOCHS + 1):\n        train_bar = tqdm(train_loader)\n        running_results = {'batch_sizes': 0, 'd_loss': 0, 'g_loss': 0, 'd_score': 0, 'g_score': 0}\n    \n        netG.train()\n        netD.train()\n        for data, target in train_bar:\n            g_update_first = True\n            batch_size = data.size(0)\n            running_results['batch_sizes'] += batch_size\n    \n            ############################\n            # (1) Update D network: maximize D(x)-1-D(G(z))\n            ###########################\n            real_img = Variable(target)\n            if torch.cuda.is_available():\n                real_img = real_img.cuda()\n            z = Variable(data)\n            if torch.cuda.is_available():\n                z = z.cuda()\n            fake_img = netG(z)\n    \n            netD.zero_grad()\n            real_out = netD(real_img).mean()\n            fake_out = netD(fake_img).mean()\n            d_loss = 1 - real_out + fake_out\n            d_loss.backward(retain_graph=True)\n            optimizerD.step()\n    \n            ############################\n            # (2) Update G network: minimize 1-D(G(z)) + Perception Loss + Image Loss + TV Loss\n            ###########################\n            netG.zero_grad()\n            g_loss = generator_criterion(fake_out, fake_img, real_img)\n            g_loss.backward()\n            \n            fake_img = netG(z)\n            fake_out = netD(fake_img).mean()\n            \n            \n            optimizerG.step()\n\n            # loss for current batch before optimization \n            running_results['g_loss'] += g_loss.item() * batch_size\n            running_results['d_loss'] += d_loss.item() * batch_size\n            running_results['d_score'] += real_out.item() * batch_size\n            running_results['g_score'] += fake_out.item() * batch_size\n    \n            train_bar.set_description(desc='[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f' % (\n                epoch, NUM_EPOCHS, running_results['d_loss'] / running_results['batch_sizes'],\n                running_results['g_loss'] / running_results['batch_sizes'],\n                running_results['d_score'] / running_results['batch_sizes'],\n                running_results['g_score'] / running_results['batch_sizes']))\n    \n        netG.eval()\n        out_path = 'training_results/SRF_' + str(UPSCALE_FACTOR) + '/'\n        if not os.path.exists(out_path):\n            os.makedirs(out_path)\n        \n        with torch.no_grad():\n            val_bar = tqdm(val_loader)\n            valing_results = {'mse': 0, 'ssims': 0, 'psnr': 0, 'ssim': 0, 'batch_sizes': 0}\n            val_images = []\n            for val_lr, val_hr_restore, val_hr in val_bar:\n                batch_size = val_lr.size(0)\n                valing_results['batch_sizes'] += batch_size\n                lr = val_lr\n                hr = val_hr\n                if torch.cuda.is_available():\n                    lr = lr.cuda()\n                    hr = hr.cuda()\n                sr = netG(lr)\n        \n                batch_mse = ((sr - hr) ** 2).data.mean()\n                valing_results['mse'] += batch_mse * batch_size\n                batch_ssim = pytorch_ssim.ssim(sr, hr).item()\n                valing_results['ssims'] += batch_ssim * batch_size\n                valing_results['psnr'] = 10 * log10((hr.max()**2) / (valing_results['mse'] / valing_results['batch_sizes']))\n                valing_results['ssim'] = valing_results['ssims'] / valing_results['batch_sizes']\n                val_bar.set_description(\n                    desc='[converting LR images to SR images] PSNR: %.4f dB SSIM: %.4f' % (\n                        valing_results['psnr'], valing_results['ssim']))\n        \n                val_images.extend(\n                    [display_transform()(val_hr_restore.squeeze(0)), display_transform()(hr.data.cpu().squeeze(0)),\n                     display_transform()(sr.data.cpu().squeeze(0))])\n            val_images = torch.stack(val_images)\n            val_images = torch.chunk(val_images, val_images.size(0) // 15)\n            val_save_bar = tqdm(val_images, desc='[saving training results]')\n            index = 1\n            for image in val_save_bar:\n                image = utils.make_grid(image, nrow=3, padding=5)\n                utils.save_image(image, out_path + 'epoch_%d_index_%d.png' % (epoch, index), padding=5)\n                index += 1\n    \n        # save model parameters\n        torch.save(netG.state_dict(), 'epochs/netG_epoch_%d_%d.pth' % (UPSCALE_FACTOR, epoch))\n        torch.save(netD.state_dict(), 'epochs/netD_epoch_%d_%d.pth' % (UPSCALE_FACTOR, epoch))\n        # save loss\\scores\\psnr\\ssim\n        results['d_loss'].append(running_results['d_loss'] / running_results['batch_sizes'])\n        results['g_loss'].append(running_results['g_loss'] / running_results['batch_sizes'])\n        results['d_score'].append(running_results['d_score'] / running_results['batch_sizes'])\n        results['g_score'].append(running_results['g_score'] / running_results['batch_sizes'])\n        results['psnr'].append(valing_results['psnr'])\n        results['ssim'].append(valing_results['ssim'])\n    \n        if epoch % 10 == 0 and epoch != 0:\n            out_path = 'statistics/'\n            data_frame = pd.DataFrame(\n                data={'Loss_D': results['d_loss'], 'Loss_G': results['g_loss'], 'Score_D': results['d_score'],\n                      'Score_G': results['g_score'], 'PSNR': results['psnr'], 'SSIM': results['ssim']},\n                index=range(1, epoch + 1))\n            data_frame.to_csv(out_path + 'srf_' + str(UPSCALE_FACTOR) + '_train_results.csv', index_label='Epoch')\n"""
pytorch_ssim/__init__.py,4,"b'from math import exp\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\ndef gaussian(window_size, sigma):\n    gauss = torch.Tensor([exp(-(x - window_size // 2) ** 2 / float(2 * sigma ** 2)) for x in range(window_size)])\n    return gauss / gauss.sum()\n\n\ndef create_window(window_size, channel):\n    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n    return window\n\n\ndef _ssim(img1, img2, window, window_size, channel, size_average=True):\n    mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)\n    mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)\n\n    mu1_sq = mu1.pow(2)\n    mu2_sq = mu2.pow(2)\n    mu1_mu2 = mu1 * mu2\n\n    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq\n    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq\n    sigma12 = F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1_mu2\n\n    C1 = 0.01 ** 2\n    C2 = 0.03 ** 2\n\n    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n\n    if size_average:\n        return ssim_map.mean()\n    else:\n        return ssim_map.mean(1).mean(1).mean(1)\n\n\nclass SSIM(torch.nn.Module):\n    def __init__(self, window_size=11, size_average=True):\n        super(SSIM, self).__init__()\n        self.window_size = window_size\n        self.size_average = size_average\n        self.channel = 1\n        self.window = create_window(window_size, self.channel)\n\n    def forward(self, img1, img2):\n        (_, channel, _, _) = img1.size()\n\n        if channel == self.channel and self.window.data.type() == img1.data.type():\n            window = self.window\n        else:\n            window = create_window(self.window_size, channel)\n\n            if img1.is_cuda:\n                window = window.cuda(img1.get_device())\n            window = window.type_as(img1)\n\n            self.window = window\n            self.channel = channel\n\n        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n\n\ndef ssim(img1, img2, window_size=11, size_average=True):\n    (_, channel, _, _) = img1.size()\n    window = create_window(window_size, channel)\n\n    if img1.is_cuda:\n        window = window.cuda(img1.get_device())\n    window = window.type_as(img1)\n\n    return _ssim(img1, img2, window, window_size, channel, size_average)\n'"
