file_path,api_count,code
build_msra_dataset_tags.py,0,"b'""""""split the msra dataset for our model and build tags""""""\nimport os\nimport random\n\n\ndef load_dataset(path_dataset):\n    """"""Load dataset into memory from text file""""""\n    dataset = []\n    with open(path_dataset) as f:\n        words, tags = [], []\n        # Each line of the file corresponds to one word and tag\n        for line in f:\n            if line != \'\\n\':\n                line = line.strip(\'\\n\')\n                word, tag = line.split(\'\\t\')\n                try:\n                    if len(word) > 0 and len(tag) > 0:\n                        word, tag = str(word), str(tag)\n                        words.append(word)\n                        tags.append(tag)\n                except Exception as e:\n                    print(\'An exception was raised, skipping a word: {}\'.format(e))\n            else:\n                if len(words) > 0:\n                    assert len(words) == len(tags)\n                    dataset.append((words, tags))\n                    words, tags = [], []\n    return dataset\n\n\ndef save_dataset(dataset, save_dir):\n    """"""Write sentences.txt and tags.txt files in save_dir from dataset\n\n    Args:\n        dataset: ([([""a"", ""cat""], [""O"", ""O""]), ...])\n        save_dir: (string)\n    """"""\n    # Create directory if it doesn\'t exist\n    print(\'Saving in {}...\'.format(save_dir))\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    # Export the dataset\n    with open(os.path.join(save_dir, \'sentences.txt\'), \'w\') as file_sentences, \\\n        open(os.path.join(save_dir, \'tags.txt\'), \'w\') as file_tags:\n        for words, tags in dataset:\n            file_sentences.write(\'{}\\n\'.format(\' \'.join(words)))\n            file_tags.write(\'{}\\n\'.format(\' \'.join(tags)))\n    print(\'- done.\')\n\ndef build_tags(data_dir, tags_file):\n    """"""Build tags from dataset\n    """"""\n    data_types = [\'train\', \'val\', \'test\']\n    tags = set()\n    for data_type in data_types:\n        tags_path = os.path.join(data_dir, data_type, \'tags.txt\')\n        with open(tags_path, \'r\') as file:\n            for line in file:\n                tag_seq = filter(len, line.strip().split(\' \'))\n                tags.update(list(tag_seq))\n    with open(tags_file, \'w\') as file:\n        file.write(\'\\n\'.join(tags))\n    return tags\n\n\nif __name__ == \'__main__\':\n    # Check that the dataset exist, two balnk lines at the end of the file\n    path_train_val = \'data/msra/msra_train_bio\'\n    path_test = \'data/msra/msra_test_bio\'\n    msg = \'{} or {} file not found. Make sure you have downloaded the right dataset\'.format(path_train_val, path_test)\n    assert os.path.isfile(path_train_val) and os.path.isfile(path_test), msg\n\n    # Load the dataset into memory\n    print(\'Loading MSRA dataset into memory...\')\n    dataset_train_val = load_dataset(path_train_val)\n    dataset_test = load_dataset(path_test)\n    print(\'- done.\')\n\n    # Make a list that decides the order in which we go over the data\n    order = list(range(len(dataset_train_val)))\n    random.seed(2019)\n    random.shuffle(order)\n\n    # Split the dataset into train, val(split with shuffle) and test\n    train_dataset = [dataset_train_val[idx] for idx in order[:42000]]  # 42000 for train\n    val_dataset = [dataset_train_val[idx] for idx in order[42000:]]  # 3000 for val\n    test_dataset = dataset_test  # 3442 for test\n    save_dataset(train_dataset, \'data/msra/train\')\n    save_dataset(val_dataset, \'data/msra/val\')\n    save_dataset(test_dataset, \'data/msra/test\')\n\n    # Build tags from dataset\n    build_tags(\'data/msra\', \'data/msra/tags.txt\')\n\n'"
data_loader.py,2,"b'""""""Data loader""""""\n\nimport random\nimport numpy as np\nimport os\nimport sys\n\nimport torch\n\nfrom pytorch_pretrained_bert import BertTokenizer\n\nimport utils\n\nclass DataLoader(object):\n    def __init__(self, data_dir, bert_model_dir, params, token_pad_idx=0):\n        self.data_dir = data_dir\n        self.batch_size = params.batch_size\n        self.max_len = params.max_len\n        self.device = params.device\n        self.seed = params.seed\n        self.token_pad_idx = 0\n\n        tags = self.load_tags()\n        self.tag2idx = {tag: idx for idx, tag in enumerate(tags)}\n        self.idx2tag = {idx: tag for idx, tag in enumerate(tags)}\n        params.tag2idx = self.tag2idx\n        params.idx2tag = self.idx2tag\n        self.tag_pad_idx = self.tag2idx[\'O\']\n\n        self.tokenizer = BertTokenizer.from_pretrained(bert_model_dir, do_lower_case=True)\n\n    def load_tags(self):\n        tags = []\n        file_path = os.path.join(self.data_dir, \'tags.txt\')\n        with open(file_path, \'r\') as file:\n            for tag in file:\n                tags.append(tag.strip())\n        return tags\n\n    def load_sentences_tags(self, sentences_file, tags_file, d):\n        """"""Loads sentences and tags from their corresponding files. \n            Maps tokens and tags to their indices and stores them in the provided dict d.\n        """"""\n        sentences = []\n        tags = []\n\n        with open(sentences_file, \'r\') as file:\n            for line in file:\n                # replace each token by its index\n                tokens = self.tokenizer.tokenize(line.strip())\n                sentences.append(self.tokenizer.convert_tokens_to_ids(tokens))\n        \n        with open(tags_file, \'r\') as file:\n            for line in file:\n                # replace each tag by its index\n                tag_seq = [self.tag2idx.get(tag) for tag in line.strip().split(\' \')]\n                tags.append(tag_seq)\n\n        # checks to ensure there is a tag for each token\n        assert len(sentences) == len(tags)\n        for i in range(len(sentences)):\n            assert len(tags[i]) == len(sentences[i])\n\n        # storing sentences and tags in dict d\n        d[\'data\'] = sentences\n        d[\'tags\'] = tags\n        d[\'size\'] = len(sentences)\n\n    def load_data(self, data_type):\n        """"""Loads the data for each type in types from data_dir.\n\n        Args:\n            data_type: (str) has one of \'train\', \'val\', \'test\' depending on which data is required.\n        Returns:\n            data: (dict) contains the data with tags for each type in types.\n        """"""\n        data = {}\n        \n        if data_type in [\'train\', \'val\', \'test\']:\n            sentences_file = os.path.join(self.data_dir, data_type, \'sentences.txt\')\n            tags_path = os.path.join(self.data_dir, data_type, \'tags.txt\')\n            self.load_sentences_tags(sentences_file, tags_path, data)\n        else:\n            raise ValueError(""data type not in [\'train\', \'val\', \'test\']"")\n        return data\n\n    def data_iterator(self, data, shuffle=False):\n        """"""Returns a generator that yields batches data with tags.\n\n        Args:\n            data: (dict) contains data which has keys \'data\', \'tags\' and \'size\'\n            shuffle: (bool) whether the data should be shuffled\n            \n        Yields:\n            batch_data: (tensor) shape: (batch_size, max_len)\n            batch_tags: (tensor) shape: (batch_size, max_len)\n        """"""\n\n        # make a list that decides the order in which we go over the data- this avoids explicit shuffling of data\n        order = list(range(data[\'size\']))\n        if shuffle:\n            random.seed(self.seed)\n            random.shuffle(order)\n\n        # one pass over data\n        for i in range(data[\'size\']//self.batch_size):\n            # fetch sentences and tags\n            sentences = [data[\'data\'][idx] for idx in order[i*self.batch_size:(i+1)*self.batch_size]]\n            tags = [data[\'tags\'][idx] for idx in order[i*self.batch_size:(i+1)*self.batch_size]]\n\n            # batch length\n            batch_len = len(sentences)\n\n            # compute length of longest sentence in batch\n            batch_max_len = max([len(s) for s in sentences])\n            max_len = min(batch_max_len, self.max_len)\n\n            # prepare a numpy array with the data, initialising the data with pad_idx\n            batch_data = self.token_pad_idx * np.ones((batch_len, max_len))\n            batch_tags = self.tag_pad_idx * np.ones((batch_len, max_len))\n\n            # copy the data to the numpy array\n            for j in range(batch_len):\n                cur_len = len(sentences[j])\n                if cur_len <= max_len:\n                    batch_data[j][:cur_len] = sentences[j]\n                    batch_tags[j][:cur_len] = tags[j]\n                else:\n                    batch_data[j] = sentences[j][:max_len]\n                    batch_tags[j] = tags[j][:max_len]\n\n            # since all data are indices, we convert them to torch LongTensors\n            batch_data = torch.tensor(batch_data, dtype=torch.long)\n            batch_tags = torch.tensor(batch_tags, dtype=torch.long)\n\n            # shift tensors to GPU if available\n            batch_data, batch_tags = batch_data.to(self.device), batch_tags.to(self.device)\n    \n            yield batch_data, batch_tags\n\n'"
evaluate.py,5,"b'""""""Evaluate the model""""""\n\nimport argparse\nimport random\nimport logging\nimport os\n\nimport numpy as np\nimport torch\n\nfrom pytorch_pretrained_bert import BertForTokenClassification, BertConfig\n\nfrom metrics import f1_score\nfrom metrics import classification_report\n\nfrom data_loader import DataLoader\nimport utils\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--data_dir\', default=\'data/msra/\', help=""Directory containing the dataset"")\nparser.add_argument(\'--bert_model_dir\', default=\'bert-base-chinese-pytorch\', help=""Directory containing the BERT model in PyTorch"")\nparser.add_argument(\'--model_dir\', default=\'experiments/base_model\', help=""Directory containing params.json"")\nparser.add_argument(\'--seed\', type=int, default=23, help=""random seed for initialization"")\nparser.add_argument(\'--restore_file\', default=\'best\', help=""name of the file in `model_dir` containing weights to load"")\nparser.add_argument(\'--multi_gpu\', default=False, action=\'store_true\', help=""Whether to use multiple GPUs if available"")\nparser.add_argument(\'--fp16\', default=False, action=\'store_true\', help=""Whether to use 16-bit float precision instead of 32-bit"")\n\n\ndef evaluate(model, data_iterator, params, mark=\'Eval\', verbose=False):\n    """"""Evaluate the model on `steps` batches.""""""\n    # set model to evaluation mode\n    model.eval()\n\n    idx2tag = params.idx2tag\n\n    true_tags = []\n    pred_tags = []\n\n    # a running average object for loss\n    loss_avg = utils.RunningAverage()\n\n    for _ in range(params.eval_steps):\n        # fetch the next evaluation batch\n        batch_data, batch_tags = next(data_iterator)\n        batch_masks = batch_data.gt(0)\n\n        loss = model(batch_data, token_type_ids=None, attention_mask=batch_masks, labels=batch_tags)\n        if params.n_gpu > 1 and params.multi_gpu:\n            loss = loss.mean()\n        loss_avg.update(loss.item())\n        \n        batch_output = model(batch_data, token_type_ids=None, attention_mask=batch_masks)  # shape: (batch_size, max_len, num_labels)\n        \n        batch_output = batch_output.detach().cpu().numpy()\n        batch_tags = batch_tags.to(\'cpu\').numpy()\n\n        pred_tags.extend([idx2tag.get(idx) for indices in np.argmax(batch_output, axis=2) for idx in indices])\n        true_tags.extend([idx2tag.get(idx) for indices in batch_tags for idx in indices])\n    assert len(pred_tags) == len(true_tags)\n\n    # logging loss, f1 and report\n    metrics = {}\n    f1 = f1_score(true_tags, pred_tags)\n    metrics[\'loss\'] = loss_avg()\n    metrics[\'f1\'] = f1\n    metrics_str = ""; "".join(""{}: {:05.2f}"".format(k, v) for k, v in metrics.items())\n    logging.info(""- {} metrics: "".format(mark) + metrics_str)\n\n    if verbose:\n        report = classification_report(true_tags, pred_tags)\n        logging.info(report)\n    return metrics\n\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n\n    # Load the parameters from json file\n    json_path = os.path.join(args.model_dir, \'params.json\')\n    assert os.path.isfile(json_path), ""No json configuration file found at {}"".format(json_path)\n    params = utils.Params(json_path)\n\n    # Use GPUs if available\n    params.device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n    params.n_gpu = torch.cuda.device_count()\n    params.multi_gpu = args.multi_gpu\n\n    # Set the random seed for reproducible experiments\n    random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if params.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)  # set random seed for all GPUs\n    params.seed = args.seed\n\n    # Set the logger\n    utils.set_logger(os.path.join(args.model_dir, \'evaluate.log\'))\n\n    # Create the input data pipeline\n    logging.info(""Loading the dataset..."")\n\n    # Initialize the DataLoader\n    data_loader = DataLoader(args.data_dir, args.bert_model_dir, params, token_pad_idx=0)\n\n    # Load data\n    test_data = data_loader.load_data(\'test\')\n\n    # Specify the test set size\n    params.test_size = test_data[\'size\']\n    params.eval_steps = params.test_size // params.batch_size\n    test_data_iterator = data_loader.data_iterator(test_data, shuffle=False)\n\n    logging.info(""- done."")\n\n    # Define the model\n    config_path = os.path.join(args.bert_model_dir, \'bert_config.json\')\n    config = BertConfig.from_json_file(config_path)\n    model = BertForTokenClassification(config, num_labels=len(params.tag2idx))\n\n    model.to(params.device)\n    # Reload weights from the saved file\n    utils.load_checkpoint(os.path.join(args.model_dir, args.restore_file + \'.pth.tar\'), model)\n    if args.fp16:\n        model.half()\n    if params.n_gpu > 1 and args.multi_gpu:\n        model = torch.nn.DataParallel(model)\n\n    logging.info(""Starting evaluation..."")\n    test_metrics = evaluate(model, test_data_iterator, params, mark=\'Test\', verbose=True)\n\n'"
metrics.py,0,"b'""""""Thanks to https://github.com/chakki-works/seqeval\nMetrics to assess performance on sequence labeling task given prediction\nFunctions named as ``*_score`` return a scalar value to maximize: the higher\nthe better\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom collections import defaultdict\n\nimport numpy as np\n\n\ndef get_entities(seq, suffix=False):\n    """"""Gets entities from sequence.\n\n    Args:\n        seq (list): sequence of labels.\n\n    Returns:\n        list: list of (chunk_type, chunk_start, chunk_end).\n\n    Example:\n        >>> from seqeval.metrics.sequence_labeling import get_entities\n        >>> seq = [\'B-PER\', \'I-PER\', \'O\', \'B-LOC\']\n        >>> get_entities(seq)\n        [(\'PER\', 0, 1), (\'LOC\', 3, 3)]\n    """"""\n    # for nested list\n    if any(isinstance(s, list) for s in seq):\n        seq = [item for sublist in seq for item in sublist + [\'O\']]\n\n    prev_tag = \'O\'\n    prev_type = \'\'\n    begin_offset = 0\n    chunks = []\n    for i, chunk in enumerate(seq + [\'O\']):\n        if suffix:\n            tag = chunk[-1]\n            type_ = chunk.split(\'-\')[0]\n        else:\n            tag = chunk[0]\n            type_ = chunk.split(\'-\')[-1]\n\n        if end_of_chunk(prev_tag, tag, prev_type, type_):\n            chunks.append((prev_type, begin_offset, i-1))\n        if start_of_chunk(prev_tag, tag, prev_type, type_):\n            begin_offset = i\n        prev_tag = tag\n        prev_type = type_\n\n    return chunks\n\n\ndef end_of_chunk(prev_tag, tag, prev_type, type_):\n    """"""Checks if a chunk ended between the previous and current word.\n\n    Args:\n        prev_tag: previous chunk tag.\n        tag: current chunk tag.\n        prev_type: previous type.\n        type_: current type.\n\n    Returns:\n        chunk_end: boolean.\n    """"""\n    chunk_end = False\n\n    if prev_tag == \'E\': chunk_end = True\n    if prev_tag == \'S\': chunk_end = True\n\n    if prev_tag == \'B\' and tag == \'B\': chunk_end = True\n    if prev_tag == \'B\' and tag == \'S\': chunk_end = True\n    if prev_tag == \'B\' and tag == \'O\': chunk_end = True\n    if prev_tag == \'I\' and tag == \'B\': chunk_end = True\n    if prev_tag == \'I\' and tag == \'S\': chunk_end = True\n    if prev_tag == \'I\' and tag == \'O\': chunk_end = True\n\n    if prev_tag != \'O\' and prev_tag != \'.\' and prev_type != type_:\n        chunk_end = True\n\n    return chunk_end\n\n\ndef start_of_chunk(prev_tag, tag, prev_type, type_):\n    """"""Checks if a chunk started between the previous and current word.\n\n    Args:\n        prev_tag: previous chunk tag.\n        tag: current chunk tag.\n        prev_type: previous type.\n        type_: current type.\n\n    Returns:\n        chunk_start: boolean.\n    """"""\n    chunk_start = False\n\n    if tag == \'B\': chunk_start = True\n    if tag == \'S\': chunk_start = True\n\n    if prev_tag == \'E\' and tag == \'E\': chunk_start = True\n    if prev_tag == \'E\' and tag == \'I\': chunk_start = True\n    if prev_tag == \'S\' and tag == \'E\': chunk_start = True\n    if prev_tag == \'S\' and tag == \'I\': chunk_start = True\n    if prev_tag == \'O\' and tag == \'E\': chunk_start = True\n    if prev_tag == \'O\' and tag == \'I\': chunk_start = True\n\n    if tag != \'O\' and tag != \'.\' and prev_type != type_:\n        chunk_start = True\n\n    return chunk_start\n\n\ndef f1_score(y_true, y_pred, average=\'micro\', digits=2, suffix=False):\n    """"""Compute the F1 score.\n\n    The F1 score can be interpreted as a weighted average of the precision and\n    recall, where an F1 score reaches its best value at 1 and worst score at 0.\n    The relative contribution of precision and recall to the F1 score are\n    equal. The formula for the F1 score is::\n\n        F1 = 2 * (precision * recall) / (precision + recall)\n\n    Args:\n        y_true : 2d array. Ground truth (correct) target values.\n        y_pred : 2d array. Estimated targets as returned by a tagger.\n\n    Returns:\n        score : float.\n\n    Example:\n        >>> from seqeval.metrics import f1_score\n        >>> y_true = [[\'O\', \'O\', \'O\', \'B-MISC\', \'I-MISC\', \'I-MISC\', \'O\'], [\'B-PER\', \'I-PER\', \'O\']]\n        >>> y_pred = [[\'O\', \'O\', \'B-MISC\', \'I-MISC\', \'I-MISC\', \'I-MISC\', \'O\'], [\'B-PER\', \'I-PER\', \'O\']]\n        >>> f1_score(y_true, y_pred)\n        0.50\n    """"""\n    true_entities = set(get_entities(y_true, suffix))\n    pred_entities = set(get_entities(y_pred, suffix))\n\n    nb_correct = len(true_entities & pred_entities)\n    nb_pred = len(pred_entities)\n    nb_true = len(true_entities)\n\n    p = 100 * nb_correct / nb_pred if nb_pred > 0 else 0\n    r = 100 * nb_correct / nb_true if nb_true > 0 else 0\n    score = 2 * p * r / (p + r) if p + r > 0 else 0\n\n    return score\n\n\ndef accuracy_score(y_true, y_pred):\n    """"""Accuracy classification score.\n\n    In multilabel classification, this function computes subset accuracy:\n    the set of labels predicted for a sample must *exactly* match the\n    corresponding set of labels in y_true.\n\n    Args:\n        y_true : 2d array. Ground truth (correct) target values.\n        y_pred : 2d array. Estimated targets as returned by a tagger.\n\n    Returns:\n        score : float.\n\n    Example:\n        >>> from seqeval.metrics import accuracy_score\n        >>> y_true = [[\'O\', \'O\', \'O\', \'B-MISC\', \'I-MISC\', \'I-MISC\', \'O\'], [\'B-PER\', \'I-PER\', \'O\']]\n        >>> y_pred = [[\'O\', \'O\', \'B-MISC\', \'I-MISC\', \'I-MISC\', \'I-MISC\', \'O\'], [\'B-PER\', \'I-PER\', \'O\']]\n        >>> accuracy_score(y_true, y_pred)\n        0.80\n    """"""\n    if any(isinstance(s, list) for s in y_true):\n        y_true = [item for sublist in y_true for item in sublist]\n        y_pred = [item for sublist in y_pred for item in sublist]\n\n    nb_correct = sum(y_t==y_p for y_t, y_p in zip(y_true, y_pred))\n    nb_true = len(y_true)\n\n    score = nb_correct / nb_true\n\n    return score\n\n\ndef classification_report(y_true, y_pred, digits=2, suffix=False):\n    """"""Build a text report showing the main classification metrics.\n\n    Args:\n        y_true : 2d array. Ground truth (correct) target values.\n        y_pred : 2d array. Estimated targets as returned by a classifier.\n        digits : int. Number of digits for formatting output floating point values.\n\n    Returns:\n        report : string. Text summary of the precision, recall, F1 score for each class.\n\n    Examples:\n        >>> from seqeval.metrics import classification_report\n        >>> y_true = [[\'O\', \'O\', \'O\', \'B-MISC\', \'I-MISC\', \'I-MISC\', \'O\'], [\'B-PER\', \'I-PER\', \'O\']]\n        >>> y_pred = [[\'O\', \'O\', \'B-MISC\', \'I-MISC\', \'I-MISC\', \'I-MISC\', \'O\'], [\'B-PER\', \'I-PER\', \'O\']]\n        >>> print(classification_report(y_true, y_pred))\n                     precision    recall  f1-score   support\n        <BLANKLINE>\n               MISC       0.00      0.00      0.00         1\n                PER       1.00      1.00      1.00         1\n        <BLANKLINE>\n        avg / total       0.50      0.50      0.50         2\n        <BLANKLINE>\n    """"""\n    true_entities = set(get_entities(y_true, suffix))\n    pred_entities = set(get_entities(y_pred, suffix))\n\n    name_width = 0\n    d1 = defaultdict(set)\n    d2 = defaultdict(set)\n    for e in true_entities:\n        d1[e[0]].add((e[1], e[2]))\n        name_width = max(name_width, len(e[0]))\n    for e in pred_entities:\n        d2[e[0]].add((e[1], e[2]))\n\n    last_line_heading = \'avg / total\'\n    width = max(name_width, len(last_line_heading), digits)\n\n    headers = [""precision"", ""recall"", ""f1-score"", ""support""]\n    head_fmt = u\'{:>{width}s} \' + u\' {:>9}\' * len(headers)\n    report = head_fmt.format(u\'\', *headers, width=width)\n    report += u\'\\n\\n\'\n\n    row_fmt = u\'{:>{width}s} \' + u\' {:>9.{digits}f}\' * 3 + u\' {:>9}\\n\'\n\n    ps, rs, f1s, s = [], [], [], []\n    for type_name, true_entities in d1.items():\n        pred_entities = d2[type_name]\n        nb_correct = len(true_entities & pred_entities)\n        nb_pred = len(pred_entities)\n        nb_true = len(true_entities)\n\n        p = 100 * nb_correct / nb_pred if nb_pred > 0 else 0\n        r = 100 * nb_correct / nb_true if nb_true > 0 else 0\n        f1 = 2 * p * r / (p + r) if p + r > 0 else 0\n\n        report += row_fmt.format(*[type_name, p, r, f1, nb_true], width=width, digits=digits)\n\n        ps.append(p)\n        rs.append(r)\n        f1s.append(f1)\n        s.append(nb_true)\n\n    report += u\'\\n\'\n\n    # compute averages\n    report += row_fmt.format(last_line_heading,\n                             np.average(ps, weights=s),\n                             np.average(rs, weights=s),\n                             np.average(f1s, weights=s),\n                             np.sum(s),\n                             width=width, digits=digits)\n\n    return report\n'"
train.py,8,"b'""""""Train and evaluate the model""""""\n\nimport argparse\nimport random\nimport logging\nimport os\n\nimport torch\nfrom torch.optim import Adam\nimport torch.nn as nn\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom tqdm import trange\n\nfrom pytorch_pretrained_bert import BertForTokenClassification\n\nfrom data_loader import DataLoader\nfrom evaluate import evaluate\nimport utils\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--data_dir\', default=\'data/msra\', help=""Directory containing the dataset"")\nparser.add_argument(\'--bert_model_dir\', default=\'bert-base-chinese-pytorch\', help=""Directory containing the BERT model in PyTorch"")\nparser.add_argument(\'--model_dir\', default=\'experiments/base_model\', help=""Directory containing params.json"")\nparser.add_argument(\'--seed\', type=int, default=2019, help=""random seed for initialization"")\nparser.add_argument(\'--restore_file\', default=None,\n                    help=""Optional, name of the file in --model_dir containing weights to reload before training"")\nparser.add_argument(\'--multi_gpu\', default=False, action=\'store_true\', help=""Whether to use multiple GPUs if available"")\nparser.add_argument(\'--fp16\', default=False, action=\'store_true\', help=""Whether to use 16-bit float precision instead of 32-bit"")\nparser.add_argument(\'--loss_scale\', type=float, default=0,\n                        help=""Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n""\n                             ""0 (default value): dynamic loss scaling.\\n""\n                             ""Positive power of 2: static loss scaling value.\\n"")\n\n\ndef train(model, data_iterator, optimizer, scheduler, params):\n    """"""Train the model on `steps` batches""""""\n    # set model to training mode\n    model.train()\n    scheduler.step()\n\n    # a running average object for loss\n    loss_avg = utils.RunningAverage()\n    \n    # Use tqdm for progress bar\n    t = trange(params.train_steps)\n    for i in t:\n        # fetch the next training batch\n        batch_data, batch_tags = next(data_iterator)\n        batch_masks = batch_data.gt(0)\n\n        # compute model output and loss\n        loss = model(batch_data, token_type_ids=None, attention_mask=batch_masks, labels=batch_tags)\n\n        if params.n_gpu > 1 and args.multi_gpu:\n            loss = loss.mean()  # mean() to average on multi-gpu\n\n        # clear previous gradients, compute gradients of all variables wrt loss\n        model.zero_grad()\n        if args.fp16:\n            optimizer.backward(loss)\n        else:\n            loss.backward()\n\n        # gradient clipping\n        nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=params.clip_grad)\n\n        # performs updates using calculated gradients\n        optimizer.step()\n\n        # update the average loss\n        loss_avg.update(loss.item())\n        t.set_postfix(loss=\'{:05.3f}\'.format(loss_avg()))\n    \n\ndef train_and_evaluate(model, train_data, val_data, optimizer, scheduler, params, model_dir, restore_file=None):\n    """"""Train the model and evaluate every epoch.""""""\n    # reload weights from restore_file if specified\n    if restore_file is not None:\n        restore_path = os.path.join(args.model_dir, args.restore_file + \'.pth.tar\')\n        logging.info(""Restoring parameters from {}"".format(restore_path))\n        utils.load_checkpoint(restore_path, model, optimizer)\n        \n    best_val_f1 = 0.0\n    patience_counter = 0\n\n    for epoch in range(1, params.epoch_num + 1):\n        # Run one epoch\n        logging.info(""Epoch {}/{}"".format(epoch, params.epoch_num))\n\n        # Compute number of batches in one epoch\n        params.train_steps = params.train_size // params.batch_size\n        params.val_steps = params.val_size // params.batch_size\n\n        # data iterator for training\n        train_data_iterator = data_loader.data_iterator(train_data, shuffle=True)\n        # Train for one epoch on training set\n        train(model, train_data_iterator, optimizer, scheduler, params)\n\n        # data iterator for evaluation\n        train_data_iterator = data_loader.data_iterator(train_data, shuffle=False)\n        val_data_iterator = data_loader.data_iterator(val_data, shuffle=False)\n\n        # Evaluate for one epoch on training set and validation set\n        params.eval_steps = params.train_steps\n        train_metrics = evaluate(model, train_data_iterator, params, mark=\'Train\')\n        params.eval_steps = params.val_steps\n        val_metrics = evaluate(model, val_data_iterator, params, mark=\'Val\')\n        \n        val_f1 = val_metrics[\'f1\']\n        improve_f1 = val_f1 - best_val_f1\n\n        # Save weights of the network\n        model_to_save = model.module if hasattr(model, \'module\') else model  # Only save the model it-self\n        optimizer_to_save = optimizer.optimizer if args.fp16 else optimizer\n        utils.save_checkpoint({\'epoch\': epoch + 1,\n                               \'state_dict\': model_to_save.state_dict(),\n                               \'optim_dict\': optimizer_to_save.state_dict()},\n                               is_best=improve_f1>0,\n                               checkpoint=model_dir)\n        if improve_f1 > 0:\n            logging.info(""- Found new best F1"")\n            best_val_f1 = val_f1\n            if improve_f1 < params.patience:\n                patience_counter += 1\n            else:\n                patience_counter = 0\n        else:\n            patience_counter += 1\n\n        # Early stopping and logging best f1\n        if (patience_counter >= params.patience_num and epoch > params.min_epoch_num) or epoch == params.epoch_num:\n            logging.info(""Best val f1: {:05.2f}"".format(best_val_f1))\n            break\n        \n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n\n    # Load the parameters from json file\n    json_path = os.path.join(args.model_dir, \'params.json\')\n    assert os.path.isfile(json_path), ""No json configuration file found at {}"".format(json_path)\n    params = utils.Params(json_path)\n\n    # Use GPUs if available\n    params.device = torch.device(\'cuda\' if torch.cuda.is_available() else \'cpu\')\n    params.n_gpu = torch.cuda.device_count()\n    params.multi_gpu = args.multi_gpu\n\n    # Set the random seed for reproducible experiments\n    random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if params.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)  # set random seed for all GPUs\n    params.seed = args.seed\n    \n    # Set the logger\n    utils.set_logger(os.path.join(args.model_dir, \'train.log\'))\n    logging.info(""device: {}, n_gpu: {}, 16-bits training: {}"".format(params.device, params.n_gpu, args.fp16))\n\n    # Create the input data pipeline\n    logging.info(""Loading the datasets..."")\n    \n    # Initialize the DataLoader\n    data_loader = DataLoader(args.data_dir, args.bert_model_dir, params, token_pad_idx=0)\n    \n    # Load training data and test data\n    train_data = data_loader.load_data(\'train\')\n    val_data = data_loader.load_data(\'val\')\n\n    # Specify the training and validation dataset sizes\n    params.train_size = train_data[\'size\']\n    params.val_size = val_data[\'size\']\n\n    # Prepare model\n    model = BertForTokenClassification.from_pretrained(args.bert_model_dir, num_labels=len(params.tag2idx))\n    model.to(params.device)\n    if args.fp16:\n        model.half()\n\n    if params.n_gpu > 1 and args.multi_gpu:\n        model = torch.nn.DataParallel(model)\n\n    # Prepare optimizer\n    if params.full_finetuning:\n        param_optimizer = list(model.named_parameters())\n        no_decay = [\'bias\', \'LayerNorm.bias\', \'LayerNorm.weight\']\n        # no_decay = [\'bias\', \'gamma\', \'beta\']\n        optimizer_grouped_parameters = [\n            {\'params\': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \n             \'weight_decay_rate\': 0.01},\n            {\'params\': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \n             \'weight_decay_rate\': 0.0}\n        ]\n    else:\n        param_optimizer = list(model.classifier.named_parameters()) \n        optimizer_grouped_parameters = [{\'params\': [p for n, p in param_optimizer]}]\n    if args.fp16:\n        try:\n            from apex.optimizers import FP16_Optimizer\n            from apex.optimizers import FusedAdam\n        except ImportError:\n            raise ImportError(""lease install apex from https://www.github.com/nvidia/apex to use fp16 training."")\n        optimizer = FusedAdam(optimizer_grouped_parameters,\n                              lr=params.learning_rate,\n                              bias_correction=False,\n                              max_grad_norm=1.0)\n        scheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: 1/(1 + 0.05*epoch))\n        if args.loss_scale == 0:\n            optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n        else:\n            optimizer = FP16_Optimizer(optimizer, static_loss_scale=args.loss_scale)\n    else:\n        optimizer = Adam(optimizer_grouped_parameters, lr=params.learning_rate)\n        scheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: 1/(1 + 0.05*epoch))\n\n    # Train and evaluate the model\n    logging.info(""Starting training for {} epoch(s)"".format(params.epoch_num))\n    train_and_evaluate(model, train_data, val_data, optimizer, scheduler, params, args.model_dir, args.restore_file)\n\n'"
utils.py,4,"b'import json\nimport logging\nimport os\nimport shutil\n\nimport torch\nimport numpy as np\n\n\nclass Params():\n    """"""Class that loads hyperparameters from a json file.\n\n    Example:\n    ```\n    params = Params(json_path)\n    print(params.learning_rate)\n    params.learning_rate = 0.5  # change the value of learning_rate in params\n    ```\n    """"""\n\n    def __init__(self, json_path):\n        with open(json_path) as f:\n            params = json.load(f)\n            self.__dict__.update(params)\n\n    def save(self, json_path):\n        with open(json_path, \'w\') as f:\n            json.dump(self.__dict__, f, indent=4)\n\n    def update(self, json_path):\n        """"""Loads parameters from json file""""""\n        with open(json_path) as f:\n            params = json.load(f)\n            self.__dict__.update(params)\n\n    @property\n    def dict(self):\n        """"""Gives dict-like access to Params instance by `params.dict[\'learning_rate\']""""""\n        return self.__dict__\n\n\nclass RunningAverage():\n    """"""A simple class that maintains the running average of a quantity\n\n    Example:\n    ```\n    loss_avg = RunningAverage()\n    loss_avg.update(2)\n    loss_avg.update(4)\n    loss_avg() = 3\n    ```\n    """"""\n\n    def __init__(self):\n        self.steps = 0\n        self.total = 0\n\n    def update(self, val):\n        self.total += val\n        self.steps += 1\n\n    def __call__(self):\n        return self.total / float(self.steps)\n\n\ndef set_logger(log_path):\n    """"""Set the logger to log info in terminal and file `log_path`.\n\n    In general, it is useful to have a logger so that every output to the terminal is saved\n    in a permanent file. Here we save it to `model_dir/train.log`.\n\n    Example:\n    ```\n    logging.info(""Starting training..."")\n    ```\n\n    Args:\n        log_path: (string) where to log\n    """"""\n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)\n\n    if not logger.handlers:\n        # Logging to a file\n        file_handler = logging.FileHandler(log_path)\n        file_handler.setFormatter(logging.Formatter(\'%(asctime)s:%(levelname)s: %(message)s\'))\n        logger.addHandler(file_handler)\n\n        # Logging to console\n        stream_handler = logging.StreamHandler()\n        stream_handler.setFormatter(logging.Formatter(\'%(message)s\'))\n        logger.addHandler(stream_handler)\n\ndef save_checkpoint(state, is_best, checkpoint):\n    """"""Saves model and training parameters at checkpoint + \'last.pth.tar\'. If is_best==True, also saves\n    checkpoint + \'best.pth.tar\'\n\n    Args:\n        state: (dict) contains model\'s state_dict, may contain other keys such as epoch, optimizer state_dict\n        is_best: (bool) True if it is the best model seen till now\n        checkpoint: (string) folder where parameters are to be saved\n    """"""\n    filepath = os.path.join(checkpoint, \'last.pth.tar\')\n    if not os.path.exists(checkpoint):\n        print(""Checkpoint Directory does not exist! Making directory {}"".format(checkpoint))\n        os.mkdir(checkpoint)\n    torch.save(state, filepath)\n    if is_best:\n        shutil.copyfile(filepath, os.path.join(checkpoint, \'best.pth.tar\'))\n\ndef load_checkpoint(checkpoint, model, optimizer=None):\n    """"""Loads model parameters (state_dict) from file_path. If optimizer is provided, loads state_dict of\n    optimizer assuming it is present in checkpoint.\n\n    Args:\n        checkpoint: (string) filename which needs to be loaded\n        model: (torch.nn.Module) model for which the parameters are loaded\n        optimizer: (torch.optim) optional: resume optimizer from checkpoint\n    """"""\n    if not os.path.exists(checkpoint):\n        raise (""File doesn\'t exist {}"".format(checkpoint))\n    checkpoint = torch.load(checkpoint)\n    # model.load_state_dict(checkpoint[\'state_dict\'])\n    model.load_state_dict(checkpoint[\'state_dict\'])\n\n    if optimizer:\n        optimizer.load_state_dict(checkpoint[\'optim_dict\'])\n\n    return checkpoint\n\n'"
