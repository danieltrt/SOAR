file_path,api_count,code
AttModel.py,11,"b""# -*- coding: utf-8 -*-\n\n'''\nJanurary 2018 by Wei Li\nliweihfyz@sjtu.edu.cn\nhttps://www.github.cim/leviswind/transformer-pytorch\n'''\n\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import *\nimport numpy as np\nfrom modules import *\nfrom hyperparams import Hyperparams as hp\n\n\nclass AttModel(nn.Module):\n    def __init__(self, hp_, enc_voc, dec_voc):\n        '''Attention is all you nedd. https://arxiv.org/abs/1706.03762\n        Args:\n            hp: Hyper Parameters\n            enc_voc: vocabulary size of encoder language\n            dec_voc: vacabulary size of decoder language\n        '''\n        super(AttModel, self).__init__()\n        self.hp = hp_\n\n        self.enc_voc = enc_voc\n        self.dec_voc = dec_voc\n\n        # encoder\n        self.enc_emb = embedding(self.enc_voc, self.hp.hidden_units, scale=True)\n\n        if self.hp.sinusoid:\n            self.enc_positional_encoding = positional_encoding(num_units=self.hp.hidden_units,\n                                                               zeros_pad=False,\n                                                               scale=False)\n        else:\n            self.enc_positional_encoding = embedding(self.hp.maxlen, self.hp.hidden_units, zeros_pad=False, scale=False)\n        self.enc_dropout = nn.Dropout(self.hp.dropout_rate)\n        for i in range(self.hp.num_blocks):\n            self.__setattr__('enc_self_attention_%d' % i, multihead_attention(num_units=self.hp.hidden_units,\n                                                                              num_heads=self.hp.num_heads,\n                                                                              dropout_rate=self.hp.dropout_rate,\n                                                                              causality=False))\n            self.__setattr__('enc_feed_forward_%d' % i, feedforward(self.hp.hidden_units,\n                                                                    [4 * self.hp.hidden_units,\n                                                                     self.hp.hidden_units]))\n\n        # decoder\n        self.dec_emb = embedding(self.dec_voc, self.hp.hidden_units, scale=True)\n        if self.hp.sinusoid:\n            self.dec_positional_encoding = positional_encoding(num_units=self.hp.hidden_units,\n                                                               zeros_pad=False,\n                                                               scale=False)\n        else:\n            self.dec_positional_encoding = embedding(self.hp.maxlen, self.hp.hidden_units, zeros_pad=False, scale=False)\n\n        self.dec_dropout = nn.Dropout(self.hp.dropout_rate)\n        for i in range(self.hp.num_blocks):\n            self.__setattr__('dec_self_attention_%d' % i,\n                             multihead_attention(num_units=self.hp.hidden_units,\n                                                 num_heads=self.hp.num_heads,\n                                                 dropout_rate=self.hp.dropout_rate,\n                                                 causality=True))\n            self.__setattr__('dec_vanilla_attention_%d' % i,\n                             multihead_attention(num_units=self.hp.hidden_units,\n                                                 num_heads=self.hp.num_heads,\n                                                 dropout_rate=self.hp.dropout_rate,\n                                                 causality=False))\n            self.__setattr__('dec_feed_forward_%d' % i, feedforward(self.hp.hidden_units,\n                                                                    [4 * self.hp.hidden_units,\n                                                                     self.hp.hidden_units]))\n        self.logits_layer = nn.Linear(self.hp.hidden_units, self.dec_voc)\n        self.label_smoothing = label_smoothing()\n        # self.losslayer = nn.CrossEntropyLoss(reduce=False)\n\n    def forward(self, x, y):\n        # define decoder inputs\n        self.decoder_inputs = torch.cat([Variable(torch.ones(y[:, :1].size()).cuda() * 2).long(), y[:, :-1]], dim=-1)  # 2:<S>\n\n        # Encoder\n        self.enc = self.enc_emb(x)\n        # Positional Encoding\n        if self.hp.sinusoid:\n            self.enc += self.enc_positional_encoding(x)\n        else:\n            self.enc += self.enc_positional_encoding(\n                Variable(torch.unsqueeze(torch.arange(0, x.size()[1]), 0).repeat(x.size(0), 1).long().cuda()))\n        self.enc = self.enc_dropout(self.enc)\n        # Blocks\n        for i in range(self.hp.num_blocks):\n            self.enc = self.__getattr__('enc_self_attention_%d' % i)(self.enc, self.enc, self.enc)\n            # Feed Forward\n            self.enc = self.__getattr__('enc_feed_forward_%d' % i)(self.enc)\n        # Decoder\n        self.dec = self.dec_emb(self.decoder_inputs)\n        # Positional Encoding\n        if self.hp.sinusoid:\n            self.dec += self.dec_positional_encoding(self.decoder_inputs)\n        else:\n            self.dec += self.dec_positional_encoding(\n                Variable(torch.unsqueeze(torch.arange(0, self.decoder_inputs.size()[1]), 0).repeat(self.decoder_inputs.size(0), 1).long().cuda()))\n\n        # Dropout\n        self.dec = self.dec_dropout(self.dec)\n        # Blocks\n        for i in range(self.hp.num_blocks):\n            # self-attention\n            self.dec = self.__getattr__('dec_self_attention_%d' % i)(self.dec, self.dec, self.dec)\n            # vanilla attention\n            self.dec = self.__getattr__('dec_vanilla_attention_%d' % i)(self.dec, self.enc, self.enc)\n            # feed forward\n            self.dec = self.__getattr__('dec_feed_forward_%d' % i)(self.dec)\n\n        # Final linear projection\n        self.logits = self.logits_layer(self.dec)\n        self.probs = F.softmax(self.logits, dim=-1).view(-1, self.dec_voc)\n        _, self.preds = torch.max(self.logits, -1)\n        self.istarget = (1. - y.eq(0.).float()).view(-1)\n        self.acc = torch.sum(self.preds.eq(y).float().view(-1) * self.istarget) / torch.sum(self.istarget)\n\n        # Loss\n        self.y_onehot = torch.zeros(self.logits.size()[0] * self.logits.size()[1], self.dec_voc).cuda()\n        self.y_onehot = Variable(self.y_onehot.scatter_(1, y.view(-1, 1).data, 1))\n\n        self.y_smoothed = self.label_smoothing(self.y_onehot)\n\n        # self.loss = self.losslayer(self.probs, self.y_smoothed)\n        self.loss = - torch.sum(self.y_smoothed * torch.log(self.probs), dim=-1)\n        # print(self.loss)\n\n        self.mean_loss = torch.sum(self.loss * self.istarget) / torch.sum(self.istarget)\n\n        return self.mean_loss, self.preds, self.acc\n\n"""
data_load.py,0,"b'# -*- coding: utf-8 -*-\n#/usr/bin/python2\n\'\'\'\nJune 2017 by kyubyong park. \nkbpark.linguist@gmail.com.\nhttps://www.github.com/kyubyong/transformer\n\'\'\'\nfrom __future__ import print_function\nfrom hyperparams import Hyperparams as hp\n\nimport numpy as np\nimport codecs\nimport regex\nimport random\nimport torch\n\ntry:\n    xrange          # Python 2\nexcept NameError:\n    xrange = range  # Python 3\n\ndef load_de_vocab():\n    vocab = [line.split()[0] for line in codecs.open(\'preprocessed/de.vocab.tsv\', \'r\', \'utf-8\').read().splitlines() if int(line.split()[1])>=hp.min_cnt]\n    word2idx = {word: idx for idx, word in enumerate(vocab)}\n    idx2word = {idx: word for idx, word in enumerate(vocab)}\n    return word2idx, idx2word\n\ndef load_en_vocab():\n    vocab = [line.split()[0] for line in codecs.open(\'preprocessed/en.vocab.tsv\', \'r\', \'utf-8\').read().splitlines() if int(line.split()[1])>=hp.min_cnt]\n    word2idx = {word: idx for idx, word in enumerate(vocab)}\n    idx2word = {idx: word for idx, word in enumerate(vocab)}\n    return word2idx, idx2word\n\ndef create_data(source_sents, target_sents): \n    de2idx, idx2de = load_de_vocab()\n    en2idx, idx2en = load_en_vocab()\n    \n    # Index\n    x_list, y_list, Sources, Targets = [], [], [], []\n    for source_sent, target_sent in zip(source_sents, target_sents):\n        x = [de2idx.get(word, 1) for word in (source_sent + u"" </S>"").split()] # 1: OOV, </S>: End of Text\n        y = [en2idx.get(word, 1) for word in (target_sent + u"" </S>"").split()] \n        if max(len(x), len(y)) <=hp.maxlen:\n            x_list.append(np.array(x))\n            y_list.append(np.array(y))\n            Sources.append(source_sent)\n            Targets.append(target_sent)\n    \n    # Pad      \n    X = np.zeros([len(x_list), hp.maxlen], np.int32)\n    Y = np.zeros([len(y_list), hp.maxlen], np.int32)\n    for i, (x, y) in enumerate(zip(x_list, y_list)):\n        X[i] = np.lib.pad(x, [0, hp.maxlen-len(x)], \'constant\', constant_values=(0, 0))\n        Y[i] = np.lib.pad(y, [0, hp.maxlen-len(y)], \'constant\', constant_values=(0, 0))\n    \n    return X, Y, Sources, Targets\n\ndef load_train_data():\n    de_sents = [regex.sub(""[^\\s\\p{Latin}\']"", """", line) for line in codecs.open(hp.source_train, \'r\', \'utf-8\').read().split(""\\n"") if line and line[0] != ""<""]\n    en_sents = [regex.sub(""[^\\s\\p{Latin}\']"", """", line) for line in codecs.open(hp.target_train, \'r\', \'utf-8\').read().split(""\\n"") if line and line[0] != ""<""]\n    \n    X, Y, Sources, Targets = create_data(de_sents, en_sents)\n    return X, Y\n    \ndef load_test_data():\n    def _refine(line):\n        line = regex.sub(""<[^>]+>"", """", line)\n        line = regex.sub(""[^\\s\\p{Latin}\']"", """", line) \n        return line.strip()\n    \n    de_sents = [_refine(line) for line in codecs.open(hp.source_test, \'r\', \'utf-8\').read().split(""\\n"") if line and line[:4] == ""<seg""]\n    en_sents = [_refine(line) for line in codecs.open(hp.target_test, \'r\', \'utf-8\').read().split(""\\n"") if line and line[:4] == ""<seg""]\n        \n    X, Y, Sources, Targets = create_data(de_sents, en_sents)\n    return X, Sources, Targets # (1064, 150)\n\n\ndef get_batch_indices(total_length, batch_size):\n    current_index = 0\n    indexs = [i for i in xrange(total_length)]\n    random.shuffle(indexs)\n    while 1:\n        if current_index + batch_size >= total_length:\n            break\n        current_index += batch_size\n        yield indexs[current_index: current_index + batch_size], current_index\n\n\n'"
eval.py,4,"b'# -*- coding: utf-8 -*-\n\n\'\'\'\nJanurary 2018 by Wei Li\nliweihfyz@sjtu.edu.cn\nhttps://www.github.cim/leviswind/transformer-pytorch\n\'\'\'\n\nfrom __future__ import print_function\nimport codecs\nimport os\n\nimport numpy as np\n\nfrom hyperparams import Hyperparams as hp\nfrom data_load import load_test_data, load_de_vocab, load_en_vocab\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom AttModel import AttModel\nfrom torch.autograd import Variable\nimport torch\n\n\ndef eval():\n    # Load data\n    X, Sources, Targets = load_test_data()\n    de2idx, idx2de = load_de_vocab()\n    en2idx, idx2en = load_en_vocab()\n    enc_voc = len(de2idx)\n    dec_voc = len(en2idx)\n\n    # load model\n    model = AttModel(hp, enc_voc, dec_voc)\n    model.load_state_dict(torch.load(hp.model_dir + \'/model_epoch_%02d\' % hp.eval_epoch + \'.pth\'))\n    print(\'Model Loaded.\')\n    model.eval()\n    model.cuda()\n    # Inference\n    if not os.path.exists(\'results\'):\n        os.mkdir(\'results\')\n    with codecs.open(\'results/model%d.txt\' % hp.eval_epoch, \'w\', \'utf-8\') as fout:\n        list_of_refs, hypotheses = [], []\n        for i in range(len(X) // hp.batch_size):\n            # Get mini-batches\n            x = X[i * hp.batch_size: (i + 1) * hp.batch_size]\n            sources = Sources[i * hp.batch_size: (i + 1) * hp.batch_size]\n            targets = Targets[i * hp.batch_size: (i + 1) * hp.batch_size]\n\n            # Autoregressive inference\n            x_ = Variable(torch.LongTensor(x).cuda())\n            preds_t = torch.LongTensor(np.zeros((hp.batch_size, hp.maxlen), np.int32)).cuda()\n            preds = Variable(preds_t)\n            for j in range(hp.maxlen):\n\n                _, _preds, _ = model(x_, preds)\n                preds_t[:, j] = _preds.data[:, j]\n                preds = Variable(preds_t.long())\n            preds = preds.data.cpu().numpy()\n\n            # Write to file\n            for source, target, pred in zip(sources, targets, preds):  # sentence-wise\n                got = "" "".join(idx2en[idx] for idx in pred).split(""</S>"")[0].strip()\n                fout.write(""- source: "" + source + ""\\n"")\n                fout.write(""- expected: "" + target + ""\\n"")\n                fout.write(""- got: "" + got + ""\\n\\n"")\n                fout.flush()\n\n                # bleu score\n                ref = target.split()\n                hypothesis = got.split()\n                if len(ref) > 3 and len(hypothesis) > 3:\n                    list_of_refs.append([ref])\n                    hypotheses.append(hypothesis)\n            # Calculate bleu score\n            score = corpus_bleu(list_of_refs, hypotheses)\n            fout.write(""Bleu Score = "" + str(100 * score))\n\n\nif __name__ == \'__main__\':\n    eval()\n    print(\'Done\')\n\n\n'"
hyperparams.py,0,"b""# -*- coding: utf-8 -*-\n#/usr/bin/python2\n'''\nJune 2017 by kyubyong park. \nkbpark.linguist@gmail.com.\nhttps://www.github.com/kyubyong/transformer\n'''\nclass Hyperparams:\n    '''Hyperparameters'''\n    # data\n    source_train = 'corpora/train.tags.de-en.de'\n    target_train = 'corpora/train.tags.de-en.en'\n    source_test = 'corpora/IWSLT16.TED.tst2014.de-en.de.xml'\n    target_test = 'corpora/IWSLT16.TED.tst2014.de-en.en.xml'\n    \n    # training\n    batch_size = 32 # alias = N\n    lr = 0.0001 # learning rate. In paper, learning rate is adjusted to the global step.\n    logdir = 'logdir' # log directory\n\n    model_dir = './models/'  # saving directory\n\n    # model\n    maxlen = 10 # Maximum number of words in a sentence. alias = T.\n                # Feel free to increase this if you are ambitious.\n    min_cnt = 20 # words whose occurred less than min_cnt are encoded as <UNK>.\n    hidden_units = 512 # alias = C\n    num_blocks = 6 # number of encoder/decoder blocks\n    num_epochs = 20\n    num_heads = 8\n    dropout_rate = 0.1\n    sinusoid = False # If True, use sinusoid. If false, positional embedding.\n    eval_epoch = 20  # epoch of model for eval\n    preload = None  # epcho of preloaded model for resuming training\n    \n    \n    \n    \n"""
modules.py,30,"b""# -*- coding: utf-8 -*-\n\n'''\nJanurary 2018 by Wei Li\nliweihfyz@sjtu.edu.cn\nhttps://www.github.cim/leviswind/transformer-pytorch\n'''\n\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import *\nimport numpy as np\nfrom torch.nn.parameter import Parameter\n\n\nclass embedding(nn.Module):\n\n    def __init__(self, vocab_size, num_units, zeros_pad=True, scale=True):\n        '''Embeds a given Variable.\n        Args:\n          vocab_size: An int. Vocabulary size.\n          num_units: An int. Number of embedding hidden units.\n          zero_pad: A boolean. If True, all the values of the fist row (id 0)\n            should be constant zeros.\n          scale: A boolean. If True. the outputs is multiplied by sqrt num_units.\n        '''\n        super(embedding, self).__init__()\n        self.vocab_size = vocab_size\n        self.num_units = num_units\n        self.zeros_pad = zeros_pad\n        self.scale = scale\n        self.lookup_table = Parameter(torch.Tensor(vocab_size, num_units))\n        nn.init.xavier_normal(self.lookup_table.data)\n        if self.zeros_pad:\n            self.lookup_table.data[0, :].fill_(0)\n\n    def forward(self, inputs):\n        if self.zeros_pad:\n            self.padding_idx = 0\n        else:\n            self.padding_idx = -1\n        outputs = self._backend.Embedding.apply(\n            inputs, self.lookup_table, self.padding_idx, None, 2, False, False)  # copied from torch.nn.modules.sparse.py\n\n        if self.scale:\n            outputs = outputs * (self.num_units ** 0.5)\n\n        return outputs\n\n\nclass layer_normalization(nn.Module):\n\n    def __init__(self, features, epsilon=1e-8):\n        '''Applies layer normalization.\n\n        Args:\n          epsilon: A floating number. A very small number for preventing ZeroDivision Error.\n        '''\n        super(layer_normalization, self).__init__()\n        self.epsilon = epsilon\n        self.gamma = nn.Parameter(torch.ones(features))\n        self.beta = nn.Parameter(torch.zeros(features))\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.gamma * (x - mean) / (std + self.epsilon) + self.beta\n\n\nclass positional_encoding(nn.Module):\n\n    def __init__(self, num_units, zeros_pad=True, scale=True):\n        '''Sinusoidal Positional_Encoding.\n\n        Args:\n          num_units: Output dimensionality\n          zero_pad: Boolean. If True, all the values of the first row (id = 0) should be constant zero\n          scale: Boolean. If True, the output will be multiplied by sqrt num_units(check details from paper)\n        '''\n        super(positional_encoding, self).__init__()\n        self.num_units = num_units\n        self.zeros_pad = zeros_pad\n        self.scale = scale\n\n    def forward(self, inputs):\n        # inputs: A 2d Tensor with shape of (N, T).\n        N, T = inputs.size()[0: 2]\n\n        # First part of the PE function: sin and cos argument\n        position_ind = Variable(torch.unsqueeze(torch.arange(0, T), 0).repeat(N, 1).long())\n        position_enc = torch.Tensor([\n            [pos / np.power(10000, 2. * i / self.num_units) for i in range(self.num_units)]\n            for pos in range(T)])\n\n        # Second part, apply the cosine to even columns and sin to odds.\n        position_enc[:, 0::2] = torch.sin(position_enc[:, 0::2])  # dim 2i\n        position_enc[:, 1::2] = torch.cos(position_enc[:, 1::2])  # dim 2i+1\n\n        # Convert to a Variable\n        lookup_table = Variable(position_enc)\n\n        if self.zeros_pad:\n            lookup_table = torch.cat((Variable(torch.zeros(1, self.num_units)),\n                                     lookup_table[1:, :]), 0)\n            padding_idx = 0\n        else:\n            padding_idx = -1\n\n        outputs = self._backend.Embedding.apply(\n            position_ind, lookup_table, padding_idx, None, 2, False, False)   # copied from torch.nn.modules.sparse.py\n\n        if self.scale:\n            outputs = outputs * self.num_units ** 0.5\n\n        return outputs\n\n\nclass multihead_attention(nn.Module):\n\n    def __init__(self, num_units, num_heads=8, dropout_rate=0, causality=False):\n        '''Applies multihead attention.\n\n        Args:\n            num_units: A scalar. Attention size.\n            dropout_rate: A floating point number.\n            causality: Boolean. If true, units that reference the future are masked.\n            num_heads: An int. Number of heads.\n        '''\n        super(multihead_attention, self).__init__()\n        self.num_units = num_units\n        self.num_heads = num_heads\n        self.dropout_rate = dropout_rate\n        self.causality = causality\n        self.Q_proj = nn.Sequential(nn.Linear(self.num_units, self.num_units), nn.ReLU())\n        self.K_proj = nn.Sequential(nn.Linear(self.num_units, self.num_units), nn.ReLU())\n        self.V_proj = nn.Sequential(nn.Linear(self.num_units, self.num_units), nn.ReLU())\n\n        self.output_dropout = nn.Dropout(p=self.dropout_rate)\n\n        self.normalization = layer_normalization(self.num_units)\n\n    def forward(self, queries, keys, values):\n        # keys, values: same shape of [N, T_k, C_k]\n        # queries: A 3d Variable with shape of [N, T_q, C_q]\n\n        # Linear projections\n        Q = self.Q_proj(queries)  # (N, T_q, C)\n        K = self.K_proj(keys)  # (N, T_q, C)\n        V = self.V_proj(values)  # (N, T_q, C)\n\n        # Split and concat\n        Q_ = torch.cat(torch.chunk(Q, self.num_heads, dim=2), dim=0)  # (h*N, T_q, C/h)\n        K_ = torch.cat(torch.chunk(K, self.num_heads, dim=2), dim=0)  # (h*N, T_q, C/h)\n        V_ = torch.cat(torch.chunk(V, self.num_heads, dim=2), dim=0)  # (h*N, T_q, C/h)\n\n        # Multiplication\n        outputs = torch.bmm(Q_, K_.permute(0, 2, 1))  # (h*N, T_q, T_k)\n\n        # Scale\n        outputs = outputs / (K_.size()[-1] ** 0.5)\n\n        # Key Masking\n        key_masks = torch.sign(torch.abs(torch.sum(keys, dim=-1)))  # (N, T_k)\n        key_masks = key_masks.repeat(self.num_heads, 1)  # (h*N, T_k)\n        key_masks = torch.unsqueeze(key_masks, 1).repeat(1, queries.size()[1], 1)  # (h*N, T_q, T_k)\n\n        padding = Variable(torch.ones(*outputs.size()).cuda() * (-2 ** 32 + 1))\n        condition = key_masks.eq(0.).float()\n        outputs = padding * condition + outputs * (1. - condition)\n\n        # Causality = Future blinding\n        if self.causality:\n            diag_vals = torch.ones(*outputs[0, :, :].size()).cuda()  # (T_q, T_k)\n            tril = torch.tril(diag_vals, diagonal=0)  # (T_q, T_k)\n            # print(tril)\n            masks = Variable(torch.unsqueeze(tril, 0).repeat(outputs.size()[0], 1, 1))  # (h*N, T_q, T_k)\n\n            padding = Variable(torch.ones(*masks.size()).cuda() * (-2 ** 32 + 1))\n            condition = masks.eq(0.).float()\n            outputs = padding * condition + outputs * (1. - condition)\n\n        # Activation\n        outputs = F.softmax(outputs, dim=-1)  # (h*N, T_q, T_k)\n\n        # Query Masking\n        query_masks = torch.sign(torch.abs(torch.sum(queries, dim=-1)))  # (N, T_q)\n        query_masks = query_masks.repeat(self.num_heads, 1)  # (h*N, T_q)\n        query_masks = torch.unsqueeze(query_masks, 2).repeat(1, 1, keys.size()[1])  # (h*N, T_q, T_k)\n        outputs = outputs * query_masks\n\n        # Dropouts\n        outputs = self.output_dropout(outputs)  # (h*N, T_q, T_k)\n\n        # Weighted sum\n        outputs = torch.bmm(outputs, V_)  # (h*N, T_q, C/h)\n\n        # Restore shape\n        outputs = torch.cat(torch.chunk(outputs, self.num_heads, dim=0), dim=2)  # (N, T_q, C)\n\n        # Residual connection\n        outputs += queries\n\n        # Normalize\n        outputs = self.normalization(outputs)  # (N, T_q, C)\n\n        return outputs\n\n\nclass feedforward(nn.Module):\n\n    def __init__(self, in_channels, num_units=[2048, 512]):\n        '''Point-wise feed forward net.\n\n        Args:\n          in_channels: a number of channels of inputs\n          num_units: A list of two integers.\n        '''\n        super(feedforward, self).__init__()\n        self.in_channels = in_channels\n        self.num_units = num_units\n\n        # nn.Linear is faster than nn.Conv1d\n        self.conv = False\n        if self.conv:\n            params = {'in_channels': self.in_channels, 'out_channels': self.num_units[0],\n                      'kernel_size': 1, 'stride': 1, 'bias': True}\n            self.conv1 = nn.Sequential(nn.Conv1d(**params), nn.ReLU())\n            params = {'in_channels': self.num_units[0], 'out_channels': self.num_units[1],\n                      'kernel_size': 1, 'stride': 1, 'bias': True}\n            self.conv2 = nn.Conv1d(**params)\n        else:\n            self.conv1 = nn.Sequential(nn.Linear(self.in_channels, self.num_units[0]), nn.ReLU())\n            self.conv2 = nn.Linear(self.num_units[0], self.num_units[1])\n        self.normalization = layer_normalization(self.in_channels)\n\n    def forward(self, inputs):\n        if self.conv:\n            inputs = inputs.permute(0, 2, 1)\n        outputs = self.conv1(inputs)\n        outputs = self.conv2(outputs)\n\n        # Residual connection\n        outputs += inputs\n\n        # Layer normalization\n        if self.conv:\n            outputs = self.normalization(outputs.permute(0, 2, 1))\n        else:\n            outputs = self.normalization(outputs)\n\n        return outputs\n\n\nclass label_smoothing(nn.Module):\n\n    def __init__(self, epsilon=0.1):\n        '''Applies label smoothing. See https://arxiv.org/abs/1512.00567.\n\n        Args:\n            epsilon: Smoothing rate.\n        '''\n        super(label_smoothing, self).__init__()\n        self.epsilon = epsilon\n\n    def forward(self, inputs):\n        K = inputs.size()[-1]\n        return ((1 - self.epsilon) * inputs) + (self.epsilon / K)\n\n\nif __name__ == '__main__':\n    num_units = 512\n    inputs = Variable(torch.randn((100, 10)))\n    outputs = position_encoding(num_units)(inputs)\n    outputs = multihead_attention(num_units)(outputs, outputs, outputs)\n    outputs = feedforward(num_units)(outputs)\n\n    print(outputs)\n"""
prepro.py,0,"b'# -*- coding: utf-8 -*-\n#/usr/bin/python2\n\'\'\'\nJune 2017 by kyubyong park. \nkbpark.linguist@gmail.com.\nhttps://www.github.com/kyubyong/transformer\n\'\'\'\nfrom __future__ import print_function\nfrom hyperparams import Hyperparams as hp\nimport tensorflow as tf\nimport numpy as np\nimport codecs\nimport os\nimport regex\nfrom collections import Counter\n\ndef make_vocab(fpath, fname):\n    \'\'\'Constructs vocabulary.\n    \n    Args:\n      fpath: A string. Input file path.\n      fname: A string. Output file name.\n    \n    Writes vocabulary line by line to `preprocessed/fname`\n    \'\'\'  \n    text = codecs.open(fpath, \'r\', \'utf-8\').read()\n    text = regex.sub(""[^\\s\\p{Latin}\']"", """", text)\n    words = text.split()\n    word2cnt = Counter(words)\n    if not os.path.exists(\'preprocessed\'): os.mkdir(\'preprocessed\')\n    with codecs.open(\'preprocessed/{}\'.format(fname), \'w\', \'utf-8\') as fout:\n        fout.write(""{}\\t1000000000\\n{}\\t1000000000\\n{}\\t1000000000\\n{}\\t1000000000\\n"".format(""<PAD>"", ""<UNK>"", ""<S>"", ""</S>""))\n        for word, cnt in word2cnt.most_common(len(word2cnt)):\n            fout.write(u""{}\\t{}\\n"".format(word, cnt))\n\nif __name__ == \'__main__\':\n    make_vocab(hp.source_train, ""de.vocab.tsv"")\n    make_vocab(hp.target_train, ""en.vocab.tsv"")\n    print(""Done"")'"
train.py,11,"b""# -*- coding: utf-8 -*-\n\n'''\nJanurary 2018 by Wei Li\nliweihfyz@sjtu.edu.cn\nhttps://www.github.cim/leviswind/transformer-pytorch\n'''\n\nfrom __future__ import print_function\n\nfrom hyperparams import Hyperparams as hp\nfrom data_load import get_batch_indices, load_de_vocab, load_en_vocab\n\nfrom torch.autograd import Variable\nimport os\nfrom AttModel import AttModel\nimport torch\nimport torch.optim as optim\nfrom data_load import load_train_data\nimport time\nimport cPickle as pickle\nfrom tensorboardX import SummaryWriter\n\n\ndef train():\n    current_batches = 0\n    de2idx, idx2de = load_de_vocab()\n    en2idx, idx2en = load_en_vocab()\n    enc_voc = len(de2idx)\n    dec_voc = len(en2idx)\n    writer = SummaryWriter()\n    # Load data\n    X, Y = load_train_data()\n    # calc total batch count\n    num_batch = len(X) // hp.batch_size\n    model = AttModel(hp, enc_voc, dec_voc)\n    model.train()\n    model.cuda()\n    torch.backends.cudnn.benchmark = True\n    if not os.path.exists(hp.model_dir):\n        os.makedirs(hp.model_dir)\n    if hp.preload is not None and os.path.exists(hp.model_dir + '/history.pkl'):\n        with open(hp.model_dir + '/history.pkl') as in_file:\n            history = pickle.load(in_file)\n    else:\n        history = {'current_batches': 0}\n    current_batches = history['current_batches']\n    optimizer = optim.Adam(model.parameters(), lr=hp.lr, betas=[0.9, 0.98], eps=1e-8)\n    if hp.preload is not None and os.path.exists(hp.model_dir + '/optimizer.pth'):\n        optimizer.load_state_dict(torch.load(hp.model_dir + '/optimizer.pth'))\n    if hp.preload is not None and os.path.exists(hp.model_dir + '/model_epoch_%02d.pth' % hp.preload):\n        model.load_state_dict(torch.load(hp.model_dir + '/model_epoch_%02d.pth' % hp.preload))\n\n    startepoch = int(hp.preload) if hp.preload is not None else 1\n    for epoch in range(startepoch, hp.num_epochs + 1):\n        current_batch = 0\n        for index, current_index in get_batch_indices(len(X), hp.batch_size):\n            tic = time.time()\n            x_batch = Variable(torch.LongTensor(X[index]).cuda())\n            y_batch = Variable(torch.LongTensor(Y[index]).cuda())\n            toc = time.time()\n            tic_r = time.time()\n            torch.cuda.synchronize()\n            optimizer.zero_grad()\n            loss, _, acc = model(x_batch, y_batch)\n            loss.backward()\n            optimizer.step()\n            torch.cuda.synchronize()\n            toc_r = time.time()\n            current_batches += 1\n            current_batch += 1\n            if current_batches % 10 == 0:\n                writer.add_scalar('./loss', loss.data.cpu().numpy()[0], current_batches)\n                writer.add_scalar('./acc', acc.data.cpu().numpy()[0], current_batches)\n            if current_batches % 5 == 0:\n                print('epoch %d, batch %d/%d, loss %f, acc %f' % (epoch, current_batch, num_batch, loss.data[0], acc.data[0]))\n                print('batch loading used time %f, model forward used time %f' % (toc - tic, toc_r - tic_r))\n            if current_batches % 100 == 0:\n                writer.export_scalars_to_json(hp.model_dir + '/all_scalars.json')\n        with open(hp.model_dir + '/history.pkl', 'w') as out_file:\n            pickle.dump(history, out_file)\n        checkpoint_path = hp.model_dir + '/model_epoch_%02d' % epoch + '.pth'\n        torch.save(model.state_dict(), checkpoint_path)\n        torch.save(optimizer.state_dict(), hp.model_dir + '/optimizer.pth')\n\n\nif __name__ == '__main__':\n    train()\n"""
