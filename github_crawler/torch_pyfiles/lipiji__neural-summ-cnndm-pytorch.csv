file_path,api_count,code
bleu.py,0,"b'# Copyright 2017 Google Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\n\n""""""Python implementation of BLEU and smooth-BLEU.\n\ncopy from: https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py\n\nThis module provides a Python implementation of BLEU and smooth-BLEU.\nSmooth BLEU is computed following the method outlined in the paper:\nChin-Yew Lin, Franz Josef Och. ORANGE: a method for evaluating automatic\nevaluation metrics for machine translation. COLING 2004.\n""""""\n\nimport collections\nimport math\nimport os\nimport argparse\n\ndef load_lines(f_path):\n    lines = []\n    with open(f_path, ""r"") as f:\n        for line in f:\n            line = line.strip(\'\\n\').strip(\'\\r\')\n            fs = line.split()\n            lines.append(fs)\n    return lines\n \ndef _get_ngrams(segment, max_order):\n  """"""Extracts all n-grams upto a given maximum order from an input segment.\n\n  Args:\n    segment: text segment from which n-grams will be extracted.\n    max_order: maximum length in tokens of the n-grams returned by this\n        methods.\n\n  Returns:\n    The Counter containing all n-grams upto max_order in segment\n    with a count of how many times each n-gram occurred.\n  """"""\n  ngram_counts = collections.Counter()\n  for order in range(1, max_order + 1):\n    for i in range(0, len(segment) - order + 1):\n      ngram = tuple(segment[i:i+order])\n      ngram_counts[ngram] += 1\n  return ngram_counts\n\n\ndef compute_bleu(reference_corpus, translation_corpus, max_order=4,\n                 smooth=False):\n  """"""Computes BLEU score of translated segments against one or more references.\n\n  Args:\n    reference_corpus: list of lists of references for each translation. Each\n        reference should be tokenized into a list of tokens.\n    translation_corpus: list of translations to score. Each translation\n        should be tokenized into a list of tokens.\n    max_order: Maximum n-gram order to use when computing BLEU score.\n    smooth: Whether or not to apply Lin et al. 2004 smoothing.\n\n  Returns:\n    3-Tuple with the BLEU score, n-gram precisions, geometric mean of n-gram\n    precisions and brevity penalty.\n  """"""\n  matches_by_order = [0] * max_order\n  possible_matches_by_order = [0] * max_order\n  reference_length = 0\n  translation_length = 0\n  for (references, translation) in zip(reference_corpus,\n                                       translation_corpus):\n    reference_length += min(len(r) for r in references)\n    translation_length += len(translation)\n\n    merged_ref_ngram_counts = collections.Counter()\n    for reference in references:\n      merged_ref_ngram_counts |= _get_ngrams(reference, max_order)\n    translation_ngram_counts = _get_ngrams(translation, max_order)\n    overlap = translation_ngram_counts & merged_ref_ngram_counts\n    for ngram in overlap:\n      matches_by_order[len(ngram)-1] += overlap[ngram]\n    for order in range(1, max_order+1):\n      possible_matches = len(translation) - order + 1\n      if possible_matches > 0:\n        possible_matches_by_order[order-1] += possible_matches\n\n  precisions = [0] * max_order\n  for i in range(0, max_order):\n    if smooth:\n      precisions[i] = ((matches_by_order[i] + 1.) /\n                       (possible_matches_by_order[i] + 1.))\n    else:\n      if possible_matches_by_order[i] > 0:\n        precisions[i] = (float(matches_by_order[i]) /\n                         possible_matches_by_order[i])\n      else:\n        precisions[i] = 0.0\n\n  if min(precisions) > 0:\n    p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)\n    geo_mean = math.exp(p_log_sum)\n  else:\n    geo_mean = 0\n\n  ratio = float(translation_length) / reference_length\n\n  if ratio > 1.0:\n    bp = 1.\n  else:\n    bp = math.exp(1 - 1. / ratio)\n\n  bleu = geo_mean * bp\n\n  return (bleu, precisions, bp, ratio, translation_length, reference_length)\n\n\n\ndef bleu(ref_path, pred_path, smooth=True, n = 1):\n    id2f_ref = {}\n    id2f_pred = {}\n    \n    flist = os.listdir(ref_path)\n    for fname in flist:\n        id_ = fname\n        id2f_ref[id_] = ref_path + fname\n    \n    flist = os.listdir(pred_path)\n    for fname in flist:\n        id_ = fname\n        id2f_pred[id_] = pred_path + fname\n\n    assert len(id2f_ref) == len(id2f_pred)\n    \n    ref_lists = []\n    pred_lists = []\n    for fid, fpath in id2f_ref.items():\n        ref_list = load_lines(fpath)\n        assert len(ref_list) == n\n        ref_lists.append(ref_list)\n\n        pred_list = load_lines(id2f_pred[fid])\n        assert len(pred_list) == n\n        pred_lists.append(pred_list[0])\n\n\n    return compute_bleu(ref_lists, pred_lists, smooth=smooth) \n \nbleu(""./weibo/result/ground_truth/"", ""./weibo/result/summary/"", smooth=True)\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""-r"", ""--ref"", help=""reference path"")\n    parser.add_argument(""-p"", ""--pred"", help=""prediction path"")\n    args = parser.parse_args()\n\n    bleu, precisions, bp, ratio, translation_length, reference_length = bleu(args.ref, args.pred)\n    print ""BLEU = "",bleu\n    print ""BLEU1 = "",precisions[0]\n    print ""BLEU2 = "",precisions[1]\n    print ""BLEU3 = "",precisions[2]\n    print ""BLEU4 = "",precisions[3]\n    print ""ratio = "",ratio\n'"
configs.py,0,"b'# -*- coding: utf-8 -*-\n#pylint: skip-file\nimport os \n\nclass CommonConfigs(object):\n    def __init__(self, d_type):\n        self.ROOT_PATH = os.getcwd() + ""/""\n        self.TRAINING_DATA_PATH = self.ROOT_PATH + d_type + ""/train_set/""\n        self.VALIDATE_DATA_PATH = self.ROOT_PATH + d_type + ""/validate_set/""\n        self.TESTING_DATA_PATH = self.ROOT_PATH + d_type + ""/test_set/""\n        self.RESULT_PATH = self.ROOT_PATH + d_type + ""/result/""\n        self.MODEL_PATH = self.ROOT_PATH + d_type + ""/model/""\n        self.BEAM_SUMM_PATH = self.RESULT_PATH + ""/beam_summary/""\n        self.BEAM_GT_PATH = self.RESULT_PATH + ""/beam_ground_truth/""\n        self.GROUND_TRUTH_PATH = self.RESULT_PATH + ""/ground_truth/""\n        self.SUMM_PATH = self.RESULT_PATH + ""/summary/""\n        self.TMP_PATH = self.ROOT_PATH + d_type + ""/tmp/""\n\n\nclass DeepmindTraining(object):\n    IS_UNICODE = False\n    REMOVES_PUNCTION = False\n    HAS_Y = True\n    BATCH_SIZE = 32\n\nclass DeepmindTesting(object):\n    IS_UNICODE = False\n    HAS_Y = True\n    BATCH_SIZE = 100\n    MIN_LEN_PREDICT = 35\n    MAX_LEN_PREDICT = 120\n    MAX_BYTE_PREDICT = None\n    PRINT_SIZE = 500\n    REMOVES_PUNCTION = False\n\nclass DeepmindConfigs():\n    \n    cc = CommonConfigs(""deepmind"")\n   \n    CELL = ""lstm"" # gru or lstm\n    CUDA = True\n    COPY = True\n    COVERAGE = True\n    BI_RNN = True\n    BEAM_SEARCH = True\n    BEAM_SIZE = 4\n    AVG_NLL = True\n    NORM_CLIP = 2\n    if not AVG_NLL:\n        NORM_CLIP = 5\n    LR = 0.15 \n\n    DIM_X = 128\n    DIM_Y = DIM_X\n\n    MIN_LEN_X = 10\n    MIN_LEN_Y = 10\n    MAX_LEN_X = 400\n    MAX_LEN_Y = 100\n    MIN_NUM_X = 1\n    MAX_NUM_X = 1\n    MAX_NUM_Y = None\n\n    NUM_Y = 1\n    HIDDEN_SIZE = 256\n\n    UNI_LOW_FREQ_THRESHOLD = 10\n\n    PG_DICT_SIZE = 50000 # dict for acl17 paper: pointer-generator\n    \n    W_UNK = ""<unk>""\n    W_BOS = ""<bos>""\n    W_EOS = ""<eos>""\n    W_PAD = ""<pad>""\n    W_LS = ""<s>""\n    W_RS = ""</s>""\n'"
data.py,0,"b'# -*- coding: utf-8 -*-\n#pylint: skip-file\nimport sys\nimport os\nimport os.path\nimport time\nfrom operator import itemgetter\nimport numpy as np\nimport pickle\nfrom random import shuffle\n\nclass BatchData:\n    def __init__(self, flist, modules, consts, options):\n        self.batch_size = len(flist) \n        self.x = np.zeros((consts[""len_x""], self.batch_size), dtype = np.int64)\n        self.x_ext = np.zeros((consts[""len_x""], self.batch_size), dtype = np.int64)\n        self.y = np.zeros((consts[""len_y""], self.batch_size), dtype = np.int64)\n        self.y_ext = np.zeros((consts[""len_y""], self.batch_size), dtype = np.int64)\n        self.x_mask = np.zeros((consts[""len_x""], self.batch_size, 1), dtype = np.int64)\n        self.y_mask = np.zeros((consts[""len_y""], self.batch_size, 1), dtype = np.int64)\n        self.len_x = []\n        self.len_y = []\n        self.original_contents = []\n        self.original_summarys = []\n        self.x_ext_words = []\n        self.max_ext_len = 0\n\n        w2i = modules[""w2i""]\n        i2w = modules[""i2w""]\n        dict_size = len(w2i)\n\n        for idx_doc in range(len(flist)):\n            if len(flist[idx_doc]) == 2:\n                contents, summarys = flist[idx_doc]\n            else:\n                print (""ERROR!"")\n                return\n            \n            content, original_content = contents\n            summary, original_summary = summarys\n            self.original_contents.append(original_content)\n            self.original_summarys.append(original_summary)\n            xi_oovs = []\n            for idx_word in range(len(content)):\n                    # some sentences in duc is longer than len_x\n                    if idx_word == consts[""len_x""]:\n                        break\n                    w = content[idx_word]\n                    \n                    if w not in w2i: # OOV\n                        if w not in xi_oovs:\n                            xi_oovs.append(w)\n                        self.x_ext[idx_word, idx_doc] = dict_size + xi_oovs.index(w) # 500005, 51000\n                        w = i2w[modules[""lfw_emb""]]\n                    else:\n                        self.x_ext[idx_word, idx_doc] = w2i[w]\n                    \n                    self.x[idx_word, idx_doc] = w2i[w]\n                    self.x_mask[idx_word, idx_doc, 0] = 1\n            self.len_x.append(np.sum(self.x_mask[:, idx_doc, :]))\n            self.x_ext_words.append(xi_oovs)\n            if self.max_ext_len < len(xi_oovs):\n                self.max_ext_len = len(xi_oovs)\n\n            if options[""has_y""]:\n                for idx_word in range(len(summary)):\n                    w = summary[idx_word]\n                    \n                    if w not in w2i:\n                        if w in xi_oovs:\n                            self.y_ext[idx_word, idx_doc] = dict_size + xi_oovs.index(w)\n                        else:\n                            self.y_ext[idx_word, idx_doc] = w2i[i2w[modules[""lfw_emb""]]] # unk\n                        w = i2w[modules[""lfw_emb""]] \n                    else:\n                        self.y_ext[idx_word, idx_doc] =  w2i[w]\n                    self.y[idx_word, idx_doc] = w2i[w]\n                    if not options[""is_predicting""]:\n                        self.y_mask[idx_word, idx_doc, 0] = 1\n                self.len_y.append(len(summary))\n            else:\n                self.y = self.y_mask = None\n\n        max_len_x = int(np.max(self.len_x))\n        max_len_y = int(np.max(self.len_y))\n        \n        self.x = self.x[0:max_len_x, :]\n        self.x_ext = self.x_ext[0:max_len_x, :]\n        self.x_mask = self.x_mask[0:max_len_x, :, :]\n        self.y = self.y[0:max_len_y, :]\n        self.y_ext = self.y_ext[0:max_len_y, :]\n        self.y_mask = self.y_mask[0:max_len_y, :, :]\n\ndef get_data(xy_list, modules, consts, options):\n    return BatchData(xy_list,  modules, consts, options)\n\ndef batched(x_size, options, consts):\n    batch_size = consts[""testing_batch_size""] if options[""is_predicting""] else consts[""batch_size""]\n    if options[""is_debugging""]:\n        x_size = 13\n    ids = [i for i in range(x_size)]\n    if not options[""is_predicting""]:\n        shuffle(ids)\n    batch_list = []\n    batch_ids = []\n    for i in range(x_size):\n        idx = ids[i]\n        batch_ids.append(idx)\n        if len(batch_ids) == batch_size or i == (x_size - 1):\n            batch_list.append(batch_ids)\n            batch_ids = []\n    return batch_list, len(ids), len(batch_list)\n\n'"
gru_dec.py,20,"b'import torch\nimport torch as T\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport math\nfrom utils_pg import *\n\nclass GRUAttentionDecoder(nn.Module):\n    def __init__(self, input_size, hidden_size, ctx_size, device, copy, coverage, is_predicting):\n        super(GRUAttentionDecoder, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.ctx_size = ctx_size\n        self.is_predicting = is_predicting\n        self.device = device\n        self.copy = copy\n        self.coverage = coverage\n\n        self.W = nn.Parameter(torch.Tensor(2 * self.hidden_size, self.input_size))\n        self.U = nn.Parameter(torch.Tensor(2 * self.hidden_size, self.hidden_size))\n        self.b = nn.Parameter(torch.Tensor(2 * self.hidden_size))\n        \n        self.Wx = nn.Parameter(torch.Tensor(self.hidden_size, self.input_size))\n        self.Ux = nn.Parameter(torch.Tensor(self.hidden_size, self.hidden_size))\n        self.bx = nn.Parameter(torch.Tensor(self.hidden_size))\n\n        self.Wc_att = nn.Parameter(torch.Tensor(self.ctx_size, self.ctx_size))\n        self.b_att = nn.Parameter(torch.Tensor(self.ctx_size))\n\n        self.W_comb_att = nn.Parameter(torch.Tensor(self.ctx_size, self.hidden_size))\n        self.U_att = nn.Parameter(torch.Tensor(1, self.ctx_size))\n        self.U_nl = nn.Parameter(torch.Tensor(2 * self.hidden_size, self.hidden_size)) \n        self.b_nl = nn.Parameter(torch.Tensor(2 * self.hidden_size))\n\n        self.Ux_nl = nn.Parameter(torch.Tensor(self.hidden_size, self.hidden_size))\n        self.bx_nl = nn.Parameter(torch.Tensor(self.hidden_size))\n\n        self.Wc = nn.Parameter(torch.Tensor(2 * self.hidden_size, self.ctx_size))\n        self.Wcx = nn.Parameter(torch.Tensor(self.hidden_size, self.ctx_size))\n\n        if self.coverage:\n            self.W_coverage= nn.Parameter(torch.Tensor(self.ctx_size, 1))\n\n        self.init_weights()\n\n    def init_weights(self):\n        init_ortho_weight(self.W)\n        init_ortho_weight(self.U)\n        init_bias(self.b)\n        init_ortho_weight(self.Wx)\n        init_ortho_weight(self.Ux)\n        init_bias(self.bx)\n        init_ortho_weight(self.Wc_att)\n        init_bias(self.b_att)\n        init_ortho_weight(self.W_comb_att)\n        init_ortho_weight(self.U_att)\n        init_ortho_weight(self.U_nl)\n        init_bias(self.b_nl)\n        init_ortho_weight(self.Ux_nl)\n        init_bias(self.bx_nl)\n        init_ortho_weight(self.Wc)\n        init_ortho_weight(self.Wcx)\n        if self.coverage:\n            init_ortho_weight(self.W_coverage)\n\n\n    def forward(self, y_emb, context, init_state, x_mask, y_mask, xid=None, init_coverage=None):\n\n        def _get_word_atten(pctx, h1, x_mask, acc_att=None): #acc_att: B * len(x)\n            if acc_att is not None:\n                h = F.linear(h1, self.W_comb_att) + F.linear(T.transpose(acc_att, 0, 1).unsqueeze(2), self.W_coverage) # len(x) * B * ?\n            else:\n                h = F.linear(h1, self.W_comb_att)\n            unreg_att = T.tanh(pctx + h) * x_mask\n            unreg_att = F.linear(unreg_att, self.U_att)\n\n            word_atten = T.exp(unreg_att - T.max(unreg_att, 0, keepdim = True)[0]) * x_mask\n            sum_word_atten = T.sum(word_atten, 0, keepdim = True)\n            word_atten =  word_atten / sum_word_atten\n            return word_atten\n\n        def recurrence(x, xx, y_mask, pre_h, pctx, context, x_mask, acc_att=None):\n            tmp1 = T.sigmoid(F.linear(pre_h, self.U) + x) \n            r1, u1 = tmp1.chunk(2, 1) \n            h1 = T.tanh(F.linear(pre_h * r1, self.Ux) + xx)\n            h1 = u1 * pre_h + (1.0 - u1) * h1\n            h1 = y_mask * h1 + (1.0 - y_mask) * pre_h\n\n            # len(x) * batch_size * 1\n            if self.coverage:\n                word_atten = _get_word_atten(pctx, h1, x_mask, acc_att)\n            else:\n                word_atten = _get_word_atten(pctx, h1, x_mask)\n            atted_ctx = T.sum(word_atten * context, 0)\n\n            tmp2 = T.sigmoid(F.linear(atted_ctx, self.Wc) + F.linear(h1, self.U_nl) + self.b_nl)\n            r2, u2 = tmp2.chunk(2, 1)  \n            h2 = T.tanh(F.linear(atted_ctx, self.Wcx) + F.linear(h1 * r2, self.Ux_nl) + self.bx_nl)\n            h2 = u2 * h1 + (1.0 - u2) * h2\n            h2 = y_mask * h2 + (1.0 - y_mask) * h1\n\n            word_atten_ = T.transpose(word_atten.view(x_mask.size(0), -1), 0, 1) # B * len(x)\n            if self.coverage:\n                acc_att += word_atten_\n                return h2, h2, atted_ctx, word_atten_, acc_att\n            else:\n                return h2, h2, atted_ctx, word_atten_\n\n\n        hs, ss, atts, dists, xids, cs = [], [], [], [], [], []\n        hidden = init_state\n        acc_att = init_coverage\n        \n        if self.copy:\n            xid = T.transpose(xid, 0, 1) # B * len(x)\n        \n        pctx = F.linear(context, self.Wc_att, self.b_att)\n        x = F.linear(y_emb, self.W, self.b)\n        xx = F.linear(y_emb, self.Wx, self.bx)\n        \n        steps = range(y_emb.size(0))\n        for i in steps:\n            if self.coverage:\n                cs += [acc_att]\n                hidden, s, att, att_dist, acc_att = recurrence(x[i], xx[i], y_mask[i], hidden, pctx, context, x_mask, acc_att)\n            else:\n                hidden, s, att, att_dist = recurrence(x[i], xx[i], y_mask[i], hidden, pctx, context, x_mask)\n            hs += [hidden]\n            ss += [s]\n            atts += [att]\n            dists += [att_dist]\n            xids += [xid]\n        \n        if self.coverage:\n            if self.is_predicting :\n                cs += [acc_att]\n                cs = cs[1:]\n            cs = T.stack(cs).view(y_emb.size(0), *cs[0].size())\n        \n        hs = T.stack(hs).view(y_emb.size(0), *hs[0].size())\n        ss = T.stack(ss).view(y_emb.size(0), *ss[0].size())\n        atts = T.stack(atts).view(y_emb.size(0), *atts[0].size())\n        dists = T.stack(dists).view(y_emb.size(0), *dists[0].size())\n        if self.copy:\n            xids = T.stack(xids).view(y_emb.size(0), *xids[0].size())\n        \n        if self.copy and self.coverage:\n            return hs, ss, atts, dists, xids, cs\n        elif self.copy:\n            return hs, ss, atts, dists, xids\n        elif self.coverage:\n            return hs, ss, atts, dists, cs\n        else:\n            return hs, ss, atts\n\n'"
lstm_dec_v1.py,8,"b'import torch\nimport torch as T\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport math\nfrom utils_pg import *\n\nclass LSTMAttentionDecoder(nn.Module):\n    def __init__(self, input_size, hidden_size, ctx_size, device, copy, coverage, is_predicting):\n        super(LSTMAttentionDecoder, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.ctx_size = ctx_size\n        self.is_predicting = is_predicting\n        self.device = device\n        self.copy = copy\n        self.coverage = coverage \n        \n        self.lstm_1 = nn.LSTMCell(self.input_size, self.hidden_size)\n\n        self.Wc_att = nn.Parameter(torch.Tensor(self.ctx_size, self.ctx_size))\n        self.b_att = nn.Parameter(torch.Tensor(self.ctx_size))\n\n        self.W_comb_att = nn.Parameter(torch.Tensor(self.ctx_size, 2*self.hidden_size))\n        self.U_att = nn.Parameter(torch.Tensor(1, self.ctx_size))\n\n        if self.coverage:\n            self.W_coverage= nn.Parameter(torch.Tensor(self.ctx_size, 1))\n\n        self.init_weights()\n\n    def init_weights(self):\n        init_lstm_weight(self.lstm_1)\n        init_ortho_weight(self.Wc_att)\n        init_bias(self.b_att)\n        init_ortho_weight(self.W_comb_att)\n        init_ortho_weight(self.U_att)\n        if self.coverage:\n            init_ortho_weight(self.W_coverage)\n\n\n    def forward(self, y_emb, context, init_state, x_mask, y_mask, xid=None, init_coverage=None):\n        def _get_word_atten(pctx, h1, x_mask, acc_att=None): #acc_att: B * len(B)\n            if acc_att is not None:\n                h = F.linear(h1, self.W_comb_att) + F.linear(T.transpose(acc_att, 0, 1).unsqueeze(2), self.W_coverage) # len(x) * B * ?\n            else:\n                h = F.linear(h1, self.W_comb_att)\n            unreg_att = T.tanh(pctx + h) * x_mask\n            unreg_att = F.linear(unreg_att, self.U_att)\n\n            word_atten = T.exp(unreg_att - T.max(unreg_att, 0, keepdim = True)[0]) * x_mask\n            sum_word_atten = T.sum(word_atten, 0, keepdim = True)\n            word_atten =  word_atten / sum_word_atten\n            return word_atten\n\n        def recurrence(x, y_mask, hidden, pctx, context, x_mask, acc_att=None):\n            pre_h, pre_c = hidden\n\n            h1, c1 = self.lstm_1(x, hidden)  \n            h1 = y_mask * h1 + (1.0 - y_mask) * pre_h\n            c1 = y_mask * c1 + (1.0 - y_mask) * pre_c\n            \n            # len(x) * batch_size * 1\n            s = T.cat((h1.view(-1, self.hidden_size), c1.view(-1, self.hidden_size)), 1)\n            if self.coverage:\n                word_atten = _get_word_atten(pctx, s, x_mask, acc_att)\n            else:\n                word_atten = _get_word_atten(pctx, s, x_mask)\n            atted_ctx = T.sum(word_atten * context, 0)\n\n\n            word_atten_ = T.transpose(word_atten.view(x_mask.size(0), -1), 0, 1)\n            \n            if self.coverage:\n                acc_att += word_atten_\n                return (h1, c1), h1, atted_ctx, word_atten_, acc_att\n            else:\n                return (h1, c1), h1, atted_ctx, word_atten_\n\n        hs, cs, ss, atts, dists, xids, Cs = [], [], [], [], [], [], []\n        hidden = init_state\n        acc_att = init_coverage\n        if self.copy: \n            xid = T.transpose(xid, 0, 1) # B * len(x)\n\n        pctx = F.linear(context, self.Wc_att, self.b_att)\n        x = y_emb\n        \n        steps = range(y_emb.size(0))\n        for i in steps:\n            if self.coverage:\n                Cs += [acc_att]\n                hidden, s, att, att_dist, acc_att = recurrence(x[i], y_mask[i], hidden, pctx, context, x_mask, acc_att)\n            else:\n                hidden, s, att, att_dist = recurrence(x[i], y_mask[i], hidden, pctx,context, x_mask)\n            hs += [hidden[0]]\n            cs += [hidden[1]]\n            ss += [s]\n            atts += [att]\n            dists += [att_dist]\n            xids += [xid]\n        \n        if self.coverage:\n            if self.is_predicting :\n                Cs += [acc_att]\n                Cs = Cs[1:]\n            Cs = T.stack(Cs).view(y_emb.size(0), *Cs[0].size())\n        \n\n        hs = T.stack(hs).view(y_emb.size(0), *hs[0].size())\n        cs = T.stack(cs).view(y_emb.size(0), *cs[0].size())\n        ss = T.stack(ss).view(y_emb.size(0), *ss[0].size())\n        atts = T.stack(atts).view(y_emb.size(0), *atts[0].size())\n        dists = T.stack(dists).view(y_emb.size(0), *dists[0].size())\n        if self.copy:\n            xids = T.stack(xids).view(y_emb.size(0), *xids[0].size())\n        \n        if self.copy and self.coverage:\n            return (hs, cs), ss, atts, dists, xids, Cs\n        elif self.copy:\n            return (hs, cs), ss, atts, dists, xids\n        elif self.coverage:\n            return (hs, cs), ss, atts, dists, Cs\n        else:\n            return (hs, cs), ss, atts\n\n\n'"
lstm_dec_v2.py,16,"b'import torch\nimport torch as T\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport math\nfrom utils_pg import *\n\nclass LSTMAttentionDecoder(nn.Module):\n    def __init__(self, input_size, hidden_size, ctx_size, device, copy, coverage, is_predicting):\n        super(LSTMAttentionDecoder, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.ctx_size = ctx_size\n        self.is_predicting = is_predicting\n        self.device = device\n        self.copy = copy\n        self.coverage = coverage \n        \n        self.lstm_1 = nn.LSTMCell(self.input_size, self.hidden_size)\n\n        self.Wx = nn.Parameter(torch.Tensor(4 * self.hidden_size, self.ctx_size))\n        self.Ux = nn.Parameter(torch.Tensor(4 * self.hidden_size, self.hidden_size))\n        self.bx = nn.Parameter(torch.Tensor(4 * self.hidden_size))\n\n        self.Wc_att = nn.Parameter(torch.Tensor(self.ctx_size, self.ctx_size))\n        self.b_att = nn.Parameter(torch.Tensor(self.ctx_size))\n\n        self.W_comb_att = nn.Parameter(torch.Tensor(self.ctx_size, 2*self.hidden_size))\n        self.U_att = nn.Parameter(torch.Tensor(1, self.ctx_size))\n\n        if self.coverage:\n            self.W_coverage= nn.Parameter(torch.Tensor(self.ctx_size, 1))\n\n        self.init_weights()\n\n    def init_weights(self):\n        init_lstm_weight(self.lstm_1)\n        init_ortho_weight(self.Wx)\n        init_ortho_weight(self.Ux)\n        init_bias(self.bx)\n        init_ortho_weight(self.Wc_att)\n        init_bias(self.b_att)\n        init_ortho_weight(self.W_comb_att)\n        init_ortho_weight(self.U_att)\n        if self.coverage:\n            init_ortho_weight(self.W_coverage)\n\n\n    def forward(self, y_emb, context, init_state, x_mask, y_mask, xid=None, init_coverage=None):\n        def _get_word_atten(pctx, h1, x_mask, acc_att=None): #acc_att: B * len(x)\n            if acc_att is not None:\n                h = F.linear(h1, self.W_comb_att) + F.linear(T.transpose(acc_att, 0, 1).unsqueeze(2), self.W_coverage) # len(x) * B * ?\n            else:\n                h = F.linear(h1, self.W_comb_att)\n            unreg_att = T.tanh(pctx + h) * x_mask\n            unreg_att = F.linear(unreg_att, self.U_att)\n\n            word_atten = T.exp(unreg_att - T.max(unreg_att, 0, keepdim = True)[0]) * x_mask\n            sum_word_atten = T.sum(word_atten, 0, keepdim = True)\n            word_atten =  word_atten / sum_word_atten\n            return word_atten\n\n        def recurrence(x, y_mask, hidden, pctx, context, x_mask, acc_att=None):\n            pre_h, pre_c = hidden\n\n            h1, c1 = self.lstm_1(x, hidden)  \n            h1 = y_mask * h1 + (1.0 - y_mask) * pre_h\n            c1 = y_mask * c1 + (1.0 - y_mask) * pre_c\n            \n            # len(x) * batch_size * 1\n            s = T.cat((h1.view(-1, self.hidden_size), c1.view(-1, self.hidden_size)), 1)\n            if self.coverage:\n                word_atten = _get_word_atten(pctx, s, x_mask, acc_att)\n            else:\n                word_atten = _get_word_atten(pctx, s, x_mask)\n            atted_ctx = T.sum(word_atten * context, 0)\n\n            ifoc_preact = F.linear(h1, self.Ux) + F.linear(atted_ctx, self.Wx, self.bx)\n            x4i, x4f, x4o, x4c = ifoc_preact.chunk(4, 1)\n            i = torch.sigmoid(x4i)\n            f = torch.sigmoid(x4f)\n            o = torch.sigmoid(x4o)\n            c2 = f * c1 + i * torch.tanh(x4c)\n            h2 = o * torch.tanh(c2)\n            c2 = y_mask * c2 + (1.0 - y_mask) * c1\n            h2 = y_mask * h2 + (1.0 - y_mask) * h1\n\n            word_atten_ = T.transpose(word_atten.view(x_mask.size(0), -1), 0, 1)\n            \n            if self.coverage:\n                acc_att += word_atten_\n                return (h2, c2), h2, atted_ctx, word_atten_, acc_att\n            else:\n                return (h2, c2), h2, atted_ctx, word_atten_\n\n        hs, cs, ss, atts, dists, xids, Cs = [], [], [], [], [], [], []\n        hidden = init_state\n        acc_att = init_coverage\n        if self.copy: \n            xid = T.transpose(xid, 0, 1) # B * len(x)\n\n        pctx = F.linear(context, self.Wc_att, self.b_att)\n        x = y_emb\n        \n        steps = range(y_emb.size(0))\n        for i in steps:\n            if self.coverage:\n                Cs += [acc_att]\n                hidden, s, att, att_dist, acc_att = recurrence(x[i], y_mask[i], hidden, pctx, context, x_mask, acc_att)\n            else:\n                hidden, s, att, att_dist = recurrence(x[i], y_mask[i], hidden, pctx,context, x_mask)\n            hs += [hidden[0]]\n            cs += [hidden[1]]\n            ss += [s]\n            atts += [att]\n            dists += [att_dist]\n            xids += [xid]\n        \n        if self.coverage:\n            if self.is_predicting :\n                Cs += [acc_att]\n                Cs = Cs[1:]\n            Cs = T.stack(Cs).view(y_emb.size(0), *Cs[0].size())\n        \n\n        hs = T.stack(hs).view(y_emb.size(0), *hs[0].size())\n        cs = T.stack(cs).view(y_emb.size(0), *cs[0].size())\n        ss = T.stack(ss).view(y_emb.size(0), *ss[0].size())\n        atts = T.stack(atts).view(y_emb.size(0), *atts[0].size())\n        dists = T.stack(dists).view(y_emb.size(0), *dists[0].size())\n        if self.copy:\n            xids = T.stack(xids).view(y_emb.size(0), *xids[0].size())\n        \n        if self.copy and self.coverage:\n            return (hs, cs), ss, atts, dists, xids, Cs\n        elif self.copy:\n            return (hs, cs), ss, atts, dists, xids\n        elif self.coverage:\n            return (hs, cs), ss, atts, dists, Cs\n        else:\n            return (hs, cs), ss, atts\n\n\n'"
main.py,35,"b'# -*- coding: utf-8 -*-\nimport os\ncudaid = 0\nos.environ[""CUDA_VISIBLE_DEVICES""] = str(cudaid)\n\nimport sys\nimport time\nimport numpy as np\nimport pickle\nimport copy\nimport random\nfrom random import shuffle\nimport math\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nimport data as datar\nfrom model import *\nfrom utils_pg import *\nfrom configs import *\n\ncfg = DeepmindConfigs()\nTRAINING_DATASET_CLS = DeepmindTraining\nTESTING_DATASET_CLS = DeepmindTesting\n\ndef print_basic_info(modules, consts, options):\n    if options[""is_debugging""]:\n        print(""\\nWARNING: IN DEBUGGING MODE\\n"")\n    if options[""copy""]:\n        print(""USE COPY MECHANISM"")\n    if options[""coverage""]:\n        print(""USE COVERAGE MECHANISM"")\n    if  options[""avg_nll""]:\n        print(""USE AVG NLL as LOSS"")\n    else:\n        print(""USE NLL as LOSS"")\n    if options[""has_learnable_w2v""]:\n        print(""USE LEARNABLE W2V EMBEDDING"")\n    if options[""is_bidirectional""]:\n        print(""USE BI-DIRECTIONAL RNN"")\n    if options[""omit_eos""]:\n        print(""<eos> IS OMITTED IN TESTING DATA"")\n    if options[""prediction_bytes_limitation""]:\n        print(""MAXIMUM BYTES IN PREDICTION IS LIMITED"")\n    print(""RNN TYPE: "" + options[""cell""])\n    for k in consts:\n        print(k + "":"", consts[k])\n\ndef init_modules():\n    \n    init_seeds()\n\n    options = {}\n\n    options[""is_debugging""] = False\n    options[""is_predicting""] = False\n    options[""model_selection""] = False # When options[""is_predicting""] = True, true means use validation set for tuning, false is real testing.\n\n    options[""cuda""] = cfg.CUDA and torch.cuda.is_available()\n    options[""device""] = torch.device(""cuda"" if  options[""cuda""] else ""cpu"")\n    \n    #in config.py\n    options[""cell""] = cfg.CELL\n    options[""copy""] = cfg.COPY\n    options[""coverage""] = cfg.COVERAGE\n    options[""is_bidirectional""] = cfg.BI_RNN\n    options[""avg_nll""] = cfg.AVG_NLL\n\n    options[""beam_decoding""] = cfg.BEAM_SEARCH # False for greedy decoding\n    \n    assert TRAINING_DATASET_CLS.IS_UNICODE == TESTING_DATASET_CLS.IS_UNICODE\n    options[""is_unicode""] = TRAINING_DATASET_CLS.IS_UNICODE # True Chinese dataet\n    options[""has_y""] = TRAINING_DATASET_CLS.HAS_Y\n    \n    options[""has_learnable_w2v""] = True\n    options[""omit_eos""] = False # omit <eos> and continuously decode until length of sentence reaches MAX_LEN_PREDICT (for DUC testing data)\n    options[""prediction_bytes_limitation""] = False if TESTING_DATASET_CLS.MAX_BYTE_PREDICT == None else True\n\n    assert options[""is_unicode""] == False\n\n    consts = {}\n    \n    consts[""idx_gpu""] = cudaid\n\n    consts[""norm_clip""] = cfg.NORM_CLIP\n    consts[""dim_x""] = cfg.DIM_X\n    consts[""dim_y""] = cfg.DIM_Y\n    consts[""len_x""] = cfg.MAX_LEN_X + 1 # plus 1 for eos\n    consts[""len_y""] = cfg.MAX_LEN_Y + 1\n    consts[""num_x""] = cfg.MAX_NUM_X\n    consts[""num_y""] = cfg.NUM_Y\n    consts[""hidden_size""] = cfg.HIDDEN_SIZE\n\n    consts[""batch_size""] = 5 if options[""is_debugging""] else TRAINING_DATASET_CLS.BATCH_SIZE\n    if options[""is_debugging""]:\n        consts[""testing_batch_size""] = 1 if options[""beam_decoding""] else 2\n    else:\n        #consts[""testing_batch_size""] = 1 if options[""beam_decoding""] else TESTING_DATASET_CLS.BATCH_SIZE \n        consts[""testing_batch_size""] = TESTING_DATASET_CLS.BATCH_SIZE\n\n    consts[""min_len_predict""] = TESTING_DATASET_CLS.MIN_LEN_PREDICT\n    consts[""max_len_predict""] = TESTING_DATASET_CLS.MAX_LEN_PREDICT\n    consts[""max_byte_predict""] = TESTING_DATASET_CLS.MAX_BYTE_PREDICT\n    consts[""testing_print_size""] = TESTING_DATASET_CLS.PRINT_SIZE\n\n    consts[""lr""] = cfg.LR\n    consts[""beam_size""] = cfg.BEAM_SIZE\n\n    consts[""max_epoch""] = 50 if options[""is_debugging""] else 30 \n    consts[""print_time""] = 5\n    consts[""save_epoch""] = 1\n\n    assert consts[""dim_x""] == consts[""dim_y""]\n    assert consts[""beam_size""] >= 1\n\n    modules = {}\n    \n    [_, dic, hfw, w2i, i2w, w2w] = pickle.load(open(cfg.cc.TRAINING_DATA_PATH + ""dic.pkl"", ""rb"")) \n    consts[""dict_size""] = len(dic)\n    modules[""dic""] = dic\n    modules[""w2i""] = w2i\n    modules[""i2w""] = i2w\n    modules[""lfw_emb""] = modules[""w2i""][cfg.W_UNK]\n    modules[""eos_emb""] = modules[""w2i""][cfg.W_EOS]\n    consts[""pad_token_idx""] = modules[""w2i""][cfg.W_PAD]\n\n    return modules, consts, options\n\ndef greedy_decode(flist, batch, model, modules, consts, options):\n    testing_batch_size = len(flist)\n\n    dec_result = [[] for i in range(testing_batch_size)]\n    existence = [True] * testing_batch_size\n    num_left = testing_batch_size\n\n    if options[""copy""]:\n        x, word_emb, dec_state, x_mask, y, len_y, ref_sents, max_ext_len, oovs = batch\n    else:\n        x, word_emb, dec_state, x_mask, y, len_y, ref_sents = batch\n\n    next_y = torch.LongTensor(-np.ones((1, testing_batch_size), dtype=""int64"")).to(options[""device""])\n\n    if options[""cell""] == ""lstm"":\n        dec_state = (dec_state, dec_state)\n    if options[""coverage""]:\n        acc_att = Variable(torch.zeros(T.transpose(x, 0, 1).size())).to(options[""device""]) # B *len(x)\n     \n    for step in range(consts[""max_len_predict""]):\n        if num_left == 0:\n            break\n        if options[""copy""] and options[""coverage""]:\n            y_pred, dec_state, acc_att = model.decode_once(next_y, word_emb, dec_state, x_mask, x, max_ext_len, acc_att)\n        elif options[""copy""]:\n            y_pred, dec_state = model.decode_once(next_y, word_emb, dec_state, x_mask, x, max_ext_len)\n        elif options[""coverage""]:\n            y_pred, dec_state, acc_att = model.decode_once(next_y, word_emb, dec_state, x_mask, acc_att=acc_att)\n        else:\n            y_pred, dec_state = model.decode_once(next_y, word_emb, dec_state, x_mask)\n\n        dict_size = y_pred.shape[-1]\n        y_pred = y_pred.view(testing_batch_size, dict_size)\n        next_y_ = torch.argmax(y_pred, 1)\n        next_y = []\n        for e in range(testing_batch_size):\n            eid = next_y_[e].item()\n            if eid in modules[""i2w""]:\n                next_y.append(eid)\n            else:\n                next_y.append(modules[""lfw_emb""]) # unk for copy mechanism\n        next_y = np.array(next_y).reshape((1, testing_batch_size))\n        next_y = torch.LongTensor(next_y).to(options[""device""])\n\n        if options[""coverage""]:\n            acc_att = acc_att.view(testing_batch_size, acc_att.shape[-1])\n\n        if options[""cell""] == ""lstm"":\n            dec_state = (dec_state[0].view(testing_batch_size, dec_state[0].shape[-1]), dec_state[1].view(testing_batch_size, dec_state[1].shape[-1]))\n        else:\n            dec_state = dec_state.view(testing_batch_size, dec_state.shape[-1])\n\n        for idx_doc in range(testing_batch_size):\n            if existence[idx_doc] == False:\n                continue\n\n            idx_max = next_y[0, idx_doc].item()\n            if idx_max == modules[""eos_emb""] and len(dec_result[idx_doc]) >= consts[""min_len_predict""]:\n                existence[idx_doc] = False\n                num_left -= 1\n            else:\n                dec_result[idx_doc].append(str(idx_max))\n    \n    # for task with bytes-length limitation \n    if options[""prediction_bytes_limitation""]:\n        for i in range(len(dec_result)):\n            sample = dec_result[i]\n            b = 0\n            for j in range(len(sample)):\n                e = int(sample[j]) \n                if e in modules[""i2w""]:\n                    word = modules[""i2w""][e]\n                else:\n                    word = oovs[e - len(modules[""i2w""])]\n                if j == 0:\n                    b += len(word)\n                else:\n                    b += len(word) + 1 \n                if b > consts[""max_byte_predict""]:\n                    sorted_samples[i] = sorted_samples[i][0 : j]\n                    break\n\n    for idx_doc in range(testing_batch_size):\n        fname = str(flist[idx_doc])\n        if len(dec_result[idx_doc]) >= consts[""min_len_predict""]:\n            dec_words = []\n            for e in dec_result[idx_doc]:\n                e = int(e)\n                if e in modules[""i2w""]: # if not copy, the word are all in dict\n                    dec_words.append(modules[""i2w""][e])\n                else:\n                    dec_words.append(oovs[e - len(modules[""i2w""])])\n            write_for_rouge(fname, ref_sents[idx_doc], dec_words, cfg)\n        else:\n            print(""ERROR: "" + fname)\n\n\ndef beam_decode(fname, batch, model, modules, consts, options):\n    fname = str(fname)\n\n    beam_size = consts[""beam_size""]\n    num_live = 1\n    num_dead = 0\n    samples = []\n    sample_scores = np.zeros(beam_size)\n\n    last_traces = [[]]\n    last_scores = torch.FloatTensor(np.zeros(1)).to(options[""device""])\n    last_states = []\n\n    if options[""copy""]:\n        x, word_emb, dec_state, x_mask, y, len_y, ref_sents, max_ext_len, oovs = batch\n    else:\n        x, word_emb, dec_state, x_mask, y, len_y, ref_sents = batch\n    \n    next_y = torch.LongTensor(-np.ones((1, num_live), dtype=""int64"")).to(options[""device""])\n    x = x.unsqueeze(1)\n    word_emb = word_emb.unsqueeze(1)\n    x_mask = x_mask.unsqueeze(1)\n    dec_state = dec_state.unsqueeze(0)\n    if options[""cell""] == ""lstm"":\n        dec_state = (dec_state, dec_state)\n    \n    if options[""coverage""]:\n        acc_att = Variable(torch.zeros(T.transpose(x, 0, 1).size())).to(options[""device""]) # B *len(x)\n        last_acc_att = [] \n\n    for step in range(consts[""max_len_predict""]):\n        tile_word_emb = word_emb.repeat(1, num_live, 1)\n        tile_x_mask = x_mask.repeat(1, num_live, 1)\n        if options[""copy""]:\n            tile_x = x.repeat(1, num_live)\n\n        if options[""copy""] and options[""coverage""]:\n            y_pred, dec_state, acc_att = model.decode_once(next_y, tile_word_emb, dec_state, tile_x_mask, tile_x, max_ext_len, acc_att)\n        elif options[""copy""]:\n            y_pred, dec_state = model.decode_once(next_y, tile_word_emb, dec_state, tile_x_mask, tile_x, max_ext_len)\n        elif options[""coverage""]:\n            y_pred, dec_state, acc_att = model.decode_once(next_y, tile_word_emb, dec_state, tile_x_mask, acc_att=acc_att)\n        else:\n            y_pred, dec_state = model.decode_once(next_y, tile_word_emb, dec_state, tile_x_mask)\n        dict_size = y_pred.shape[-1]\n        y_pred = y_pred.view(num_live, dict_size)\n        if options[""coverage""]:\n            acc_att = acc_att.view(num_live, acc_att.shape[-1])\n\n        if options[""cell""] == ""lstm"":\n            dec_state = (dec_state[0].view(num_live, dec_state[0].shape[-1]), dec_state[1].view(num_live, dec_state[1].shape[-1]))\n        else:\n            dec_state = dec_state.view(num_live, dec_state.shape[-1])\n  \n        cand_scores = last_scores + torch.log(y_pred) # larger is better\n        cand_scores = cand_scores.flatten()\n        idx_top_joint_scores = torch.topk(cand_scores, beam_size - num_dead)[1]\n\n\n        idx_last_traces = idx_top_joint_scores / dict_size\n        idx_word_now = idx_top_joint_scores % dict_size\n        top_joint_scores = cand_scores[idx_top_joint_scores]\n\n        traces_now = []\n        scores_now = np.zeros((beam_size - num_dead))\n        states_now = []\n        if options[""coverage""]: \n            acc_att_now = []\n            last_acc_att = []\n        \n        for i, [j, k] in enumerate(zip(idx_last_traces, idx_word_now)):\n            traces_now.append(last_traces[j] + [k])\n            scores_now[i] = copy.copy(top_joint_scores[i])\n            if options[""cell""] == ""lstm"":\n                states_now.append((copy.copy(dec_state[0][j, :]), copy.copy(dec_state[1][j, :])))\n            else:\n                states_now.append(copy.copy(dec_state[j, :]))\n            if options[""coverage""]:\n                acc_att_now.append(copy.copy(acc_att[j, :]))\n\n        num_live = 0\n        last_traces = []\n        last_scores = []\n        last_states = []\n        for i in range(len(traces_now)):\n            if traces_now[i][-1] == modules[""eos_emb""] and len(traces_now[i]) >= consts[""min_len_predict""]:\n                samples.append([str(e.item()) for e in traces_now[i][:-1]])\n                sample_scores[num_dead] = scores_now[i]\n                num_dead += 1\n            else:\n                last_traces.append(traces_now[i])\n                last_scores.append(scores_now[i])\n                last_states.append(states_now[i])\n                if options[""coverage""]:\n                    last_acc_att.append(acc_att_now[i])\n                num_live += 1\n        if num_live == 0 or num_dead >= beam_size:\n            break\n\n        last_scores = torch.FloatTensor(np.array(last_scores).reshape((num_live, 1))).to(options[""device""])\n        next_y = []\n        for e in last_traces:\n            eid = e[-1].item()\n            if eid in modules[""i2w""]:\n                next_y.append(eid)\n            else:\n                next_y.append(modules[""lfw_emb""]) # unk for copy mechanism\n\n        next_y = np.array(next_y).reshape((1, num_live))\n        next_y = torch.LongTensor(next_y).to(options[""device""])\n        if options[""cell""] == ""lstm"":\n            h_states = []\n            c_states = []\n            for state in last_states:\n                h_states.append(state[0])\n                c_states.append(state[1])\n            dec_state = (torch.stack(h_states).view((num_live, h_states[0].shape[-1])),\\\n                         torch.stack(c_states).view((num_live, c_states[0].shape[-1])))\n        else:\n            dec_state = torch.stack(last_states).view((num_live, dec_state.shape[-1]))\n        if options[""coverage""]:\n            acc_att = torch.stack(last_acc_att).view((num_live, acc_att.shape[-1])) \n            \n        assert num_live + num_dead == beam_size\n\n    if num_live > 0:\n        for i in range(num_live):\n            samples.append([str(e.item()) for e in last_traces[i]])\n            sample_scores[num_dead] = last_scores[i]\n            num_dead += 1\n    \n    #weight by length\n    for i in range(len(sample_scores)):\n        sent_len = float(len(samples[i]))\n        sample_scores[i] = sample_scores[i] / sent_len #avg is better than sum.   #*  math.exp(-sent_len / 10)\n\n    idx_sorted_scores = np.argsort(sample_scores) # ascending order\n    if options[""has_y""]:\n        ly = len_y[0]\n        y_true = y[0 : ly].tolist()\n        y_true = [str(i) for i in y_true[:-1]] # delete <eos>\n\n    sorted_samples = []\n    sorted_scores = []\n    filter_idx = []\n    for e in idx_sorted_scores:\n        if len(samples[e]) >= consts[""min_len_predict""]:\n            filter_idx.append(e)\n    if len(filter_idx) == 0:\n        filter_idx = idx_sorted_scores\n    for e in filter_idx:\n        sorted_samples.append(samples[e])\n        sorted_scores.append(sample_scores[e])\n\n    num_samples = len(sorted_samples)\n    if len(sorted_samples) == 1:\n        sorted_samples = sorted_samples[0]\n        num_samples = 1\n\n    # for task with bytes-length limitation \n    if options[""prediction_bytes_limitation""]:\n        for i in range(len(sorted_samples)):\n            sample = sorted_samples[i]\n            b = 0\n            for j in range(len(sample)):\n                e = int(sample[j]) \n                if e in modules[""i2w""]:\n                    word = modules[""i2w""][e]\n                else:\n                    word = oovs[e - len(modules[""i2w""])]\n                if j == 0:\n                    b += len(word)\n                else:\n                    b += len(word) + 1 \n                if b > consts[""max_byte_predict""]:\n                    sorted_samples[i] = sorted_samples[i][0 : j]\n                    break\n\n    dec_words = []\n\n    for e in sorted_samples[-1]:\n        e = int(e)\n        if e in modules[""i2w""]: # if not copy, the word are all in dict\n            dec_words.append(modules[""i2w""][e])\n        else:\n            dec_words.append(oovs[e - len(modules[""i2w""])])\n    \n    write_for_rouge(fname, ref_sents, dec_words, cfg)\n\n    # beam search history for checking\n    if not options[""copy""]:\n        oovs = None\n    write_summ("""".join((cfg.cc.BEAM_SUMM_PATH, fname)), sorted_samples, num_samples, options, modules[""i2w""], oovs, sorted_scores)\n    write_summ("""".join((cfg.cc.BEAM_GT_PATH, fname)), y_true, 1, options, modules[""i2w""], oovs) \n\n\ndef predict(model, modules, consts, options):\n    print(""start predicting,"")\n    options[""has_y""] = TESTING_DATASET_CLS.HAS_Y\n    if options[""beam_decoding""]:\n        print(""using beam search"")\n    else:\n        print(""using greedy search"")\n    rebuild_dir(cfg.cc.BEAM_SUMM_PATH)\n    rebuild_dir(cfg.cc.BEAM_GT_PATH)\n    rebuild_dir(cfg.cc.GROUND_TRUTH_PATH)\n    rebuild_dir(cfg.cc.SUMM_PATH)\n\n    print(""loading test set..."")\n    if options[""model_selection""]:\n        xy_list = pickle.load(open(cfg.cc.VALIDATE_DATA_PATH + ""pj1000.pkl"", ""rb"")) \n    else:\n        xy_list = pickle.load(open(cfg.cc.TESTING_DATA_PATH + ""test.pkl"", ""rb"")) \n    batch_list, num_files, num_batches = datar.batched(len(xy_list), options, consts)\n\n    print(""num_files = "", num_files, "", num_batches = "", num_batches)\n    \n    running_start = time.time()\n    partial_num = 0\n    total_num = 0\n    si = 0\n    for idx_batch in range(num_batches):\n        test_idx = batch_list[idx_batch]\n        batch_raw = [xy_list[xy_idx] for xy_idx in test_idx]\n        batch = datar.get_data(batch_raw, modules, consts, options)\n        \n        assert len(test_idx) == batch.x.shape[1] # local_batch_size\n\n        x, len_x, x_mask, y, len_y, y_mask, oy, x_ext, y_ext, oovs = sort_samples(batch.x, batch.len_x, \\\n                                                             batch.x_mask, batch.y, batch.len_y, batch.y_mask, \\\n                                                             batch.original_summarys, batch.x_ext, batch.y_ext, batch.x_ext_words)\n                    \n        word_emb, dec_state = model.encode(torch.LongTensor(x).to(options[""device""]),\\\n                                           torch.LongTensor(len_x).to(options[""device""]),\\\n                                           torch.FloatTensor(x_mask).to(options[""device""]))\n\n        if options[""beam_decoding""]:\n            for idx_s in range(len(test_idx)):\n                if options[""copy""]:\n                    inputx = (torch.LongTensor(x_ext[:, idx_s]).to(options[""device""]), word_emb[:, idx_s, :], dec_state[idx_s, :],\\\n                          torch.FloatTensor(x_mask[:, idx_s, :]).to(options[""device""]), y[:, idx_s], [len_y[idx_s]], oy[idx_s],\\\n                          batch.max_ext_len, oovs[idx_s])\n                else:\n                    inputx = (torch.LongTensor(x[:, idx_s]).to(options[""device""]), word_emb[:, idx_s, :], dec_state[idx_s, :],\\\n                          torch.FloatTensor(x_mask[:, idx_s, :]).to(options[""device""]), y[:, idx_s], [len_y[idx_s]], oy[idx_s])\n\n                beam_decode(si, inputx, model, modules, consts, options)\n                si += 1\n        else:\n            if options[""copy""]:\n                inputx = (torch.LongTensor(x_ext).to(options[""device""]), word_emb, dec_state, \\\n                          torch.FloatTensor(x_mask).to(options[""device""]), y, len_y, oy, batch.max_ext_len, oovs)\n            else:\n                inputx = (torch.LongTensor(x).to(options[""device""]), word_emb, dec_state, torch.FloatTensor(x_mask).to(options[""device""]), y, len_y, oy)\n            greedy_decode(test_idx, inputx, model, modules, consts, options)\n            si += len(test_idx)\n\n        testing_batch_size = len(test_idx)\n        partial_num += testing_batch_size\n        total_num += testing_batch_size\n        if partial_num >= consts[""testing_print_size""]:\n            print(total_num, ""summs are generated"")\n            partial_num = 0\n    print (si, total_num)\n\ndef run(existing_model_name = None):\n    modules, consts, options = init_modules()\n\n    #use_gpu(consts[""idx_gpu""])\n    if options[""is_predicting""]:\n        need_load_model = True\n        training_model = False\n        predict_model = True\n    else:\n        need_load_model = False\n        training_model = True\n        predict_model = False\n\n    print_basic_info(modules, consts, options)\n\n    if training_model:\n        print (""loading train set..."")\n        if options[""is_debugging""]:\n            xy_list = pickle.load(open(cfg.cc.VALIDATE_DATA_PATH + ""pj1000.pkl"", ""rb"")) \n        else:\n            xy_list = pickle.load(open(cfg.cc.TRAINING_DATA_PATH + ""train.pkl"", ""rb"")) \n        batch_list, num_files, num_batches = datar.batched(len(xy_list), options, consts)\n        print (""num_files = "", num_files, "", num_batches = "", num_batches)\n\n    running_start = time.time()\n    if True: #TODO: refactor\n        print (""compiling model ..."" )\n        model = Model(modules, consts, options)\n        if options[""cuda""]:\n            model.cuda()\n        optimizer = torch.optim.Adagrad(model.parameters(), lr=consts[""lr""], initial_accumulator_value=0.1)\n        \n        model_name = """".join([""cnndm.s2s."", options[""cell""]])\n        existing_epoch = 0\n        if need_load_model:\n            if existing_model_name == None:\n                existing_model_name = ""cnndm.s2s.gpu4.epoch7.1""\n            print (""loading existed model:"", existing_model_name)\n            model, optimizer = load_model(cfg.cc.MODEL_PATH + existing_model_name, model, optimizer)\n\n        if training_model:\n            print (""start training model "")\n            print_size = num_files // consts[""print_time""] if num_files >= consts[""print_time""] else num_files\n\n            last_total_error = float(""inf"")\n            print (""max epoch:"", consts[""max_epoch""])\n            for epoch in range(0, consts[""max_epoch""]):\n                print (""epoch: "", epoch + existing_epoch)\n                num_partial = 1\n                total_error = 0.0\n                error_c = 0.0\n                partial_num_files = 0\n                epoch_start = time.time()\n                partial_start = time.time()\n                # shuffle the trainset\n                batch_list, num_files, num_batches = datar.batched(len(xy_list), options, consts)\n                used_batch = 0.\n                for idx_batch in range(num_batches):\n                    train_idx = batch_list[idx_batch]\n                    batch_raw = [xy_list[xy_idx] for xy_idx in train_idx]\n                    if len(batch_raw) != consts[""batch_size""]:\n                        continue\n                    local_batch_size = len(batch_raw)\n                    batch = datar.get_data(batch_raw, modules, consts, options)\n                  \n                    x, len_x, x_mask, y, len_y, y_mask, oy, x_ext, y_ext, oovs = sort_samples(batch.x, batch.len_x, \\\n                                                             batch.x_mask, batch.y, batch.len_y, batch.y_mask, \\\n                                                             batch.original_summarys, batch.x_ext, batch.y_ext, batch.x_ext_words)\n                    \n                    model.zero_grad()\n                    y_pred, cost, cost_c = model(torch.LongTensor(x).to(options[""device""]), torch.LongTensor(len_x).to(options[""device""]),\\\n                                   torch.LongTensor(y).to(options[""device""]),  torch.FloatTensor(x_mask).to(options[""device""]), \\\n                                   torch.FloatTensor(y_mask).to(options[""device""]), torch.LongTensor(x_ext).to(options[""device""]),\\\n                                   torch.LongTensor(y_ext).to(options[""device""]), \\\n                                   batch.max_ext_len)\n                    if cost_c is None:\n                        loss = cost\n                    else:\n                        loss = cost + cost_c\n                        cost_c = cost_c.item()\n                        error_c += cost_c\n                    \n                    loss.backward()\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), consts[""norm_clip""])\n                    optimizer.step()\n                    \n                    cost = cost.item()\n                    total_error += cost\n                    used_batch += 1\n                    partial_num_files += consts[""batch_size""]\n                    if partial_num_files // print_size == 1 and idx_batch < num_batches:\n                        print (idx_batch + 1, ""/"" , num_batches, ""batches have been processed,"", \\\n                                ""average cost until now:"", ""cost ="", total_error / used_batch, "","", \\\n                                ""cost_c ="", error_c / used_batch, "","", \\\n                                ""time:"", time.time() - partial_start)\n                        partial_num_files = 0\n                        if not options[""is_debugging""]:\n                            print(""save model... "",)\n                            save_model(cfg.cc.MODEL_PATH + model_name + "".gpu"" + str(consts[""idx_gpu""]) + "".epoch"" + str(epoch // consts[""save_epoch""] + existing_epoch) + ""."" + str(num_partial), model, optimizer)\n                            print(""finished"")\n                        num_partial += 1\n                print (""in this epoch, total average cost ="", total_error / used_batch, "","", \\\n                        ""cost_c ="", error_c / used_batch, "","",\\\n                        ""time:"", time.time() - epoch_start)\n\n                print_sent_dec(y_pred, y_ext, y_mask, oovs, modules, consts, options, local_batch_size)\n                \n                if last_total_error > total_error or options[""is_debugging""]:\n                    last_total_error = total_error\n                    if not options[""is_debugging""]:\n                        print (""save model... "",)\n                        save_model(cfg.cc.MODEL_PATH + model_name + "".gpu"" + str(consts[""idx_gpu""]) + "".epoch"" + str(epoch // consts[""save_epoch""] + existing_epoch) + ""."" + str(num_partial), model, optimizer)\n                        print (""finished"")\n                else:\n                    print (""optimization finished"")\n                    break\n\n            print (""save final model... ""),\n            save_model(cfg.cc.MODEL_PATH + model_name + "".final.gpu"" + str(consts[""idx_gpu""]) + "".epoch"" + str(epoch // consts[""save_epoch""] + existing_epoch) + ""."" + str(num_partial), model, optimizer)\n            print (""finished"")\n        else:\n            print (""skip training model"")\n\n        if predict_model:\n            predict(model, modules, consts, options)\n    print (""Finished, time:"", time.time() - running_start)\n\nif __name__ == ""__main__"":\n    np.set_printoptions(threshold = np.inf)\n    existing_model_name = sys.argv[1] if len(sys.argv) > 1 else None\n    run(existing_model_name)\n'"
model.py,6,"b'# -*- coding: utf-8 -*-\n#pylint: skip-file\nimport sys\nimport numpy as np\nimport torch\nimport torch as T\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nfrom utils_pg import *\nfrom gru_dec import *\nfrom lstm_dec_v2 import *\nfrom word_prob_layer import *\n\nclass Model(nn.Module):\n    def __init__(self, modules, consts, options):\n        super(Model, self).__init__()  \n        \n        self.has_learnable_w2v = options[""has_learnable_w2v""]\n        self.is_predicting = options[""is_predicting""]\n        self.is_bidirectional = options[""is_bidirectional""]\n        self.beam_decoding = options[""beam_decoding""]\n        self.cell = options[""cell""]\n        self.device = options[""device""]\n        self.copy = options[""copy""]\n        self.coverage = options[""coverage""]\n        self.avg_nll = options[""avg_nll""]\n\n        self.dim_x = consts[""dim_x""]\n        self.dim_y = consts[""dim_y""]\n        self.len_x = consts[""len_x""]\n        self.len_y = consts[""len_y""]\n        self.hidden_size = consts[""hidden_size""]\n        self.dict_size = consts[""dict_size""] \n        self.pad_token_idx = consts[""pad_token_idx""] \n        self.ctx_size = self.hidden_size * 2 if self.is_bidirectional else self.hidden_size\n\n        self.w_rawdata_emb = nn.Embedding(self.dict_size, self.dim_x, self.pad_token_idx)\n        if self.cell == ""gru"":\n            self.encoder = nn.GRU(self.dim_x, self.hidden_size, bidirectional=self.is_bidirectional)\n            self.decoder = GRUAttentionDecoder(self.dim_y, self.hidden_size, self.ctx_size, self.device, self.copy, self.coverage, self.is_predicting)\n        else:\n            self.encoder = nn.LSTM(self.dim_x, self.hidden_size, bidirectional=self.is_bidirectional)\n            self.decoder = LSTMAttentionDecoder(self.dim_y, self.hidden_size, self.ctx_size, self.device, self.copy, self.coverage, self.is_predicting)\n            \n        self.get_dec_init_state = nn.Linear(self.ctx_size, self.hidden_size)\n        self.word_prob = WordProbLayer(self.hidden_size, self.ctx_size, self.dim_y, self.dict_size, self.device, self.copy, self.coverage)\n\n        self.init_weights()\n\n    def init_weights(self):\n        init_uniform_weight(self.w_rawdata_emb.weight)\n        if self.cell == ""gru"": \n            init_gru_weight(self.encoder)\n        else:\n            init_lstm_weight(self.encoder)\n        init_linear_weight(self.get_dec_init_state)\n    \n    def nll_loss(self, y_pred, y, y_mask, avg=True):\n        cost = -T.log(T.gather(y_pred, 2, y.view(y.size(0), y.size(1), 1)))\n        cost = cost.view(y.shape)\n        y_mask = y_mask.view(y.shape)\n        if avg:\n            cost = T.sum(cost * y_mask, 0) / T.sum(y_mask, 0)\n        else:\n            cost = T.sum(cost * y_mask, 0)\n        cost = cost.view((y.size(1), -1))\n        return T.mean(cost) \n\n    def encode(self, x, len_x, mask_x):\n        self.encoder.flatten_parameters()\n        emb_x = self.w_rawdata_emb(x)\n        \n        emb_x = torch.nn.utils.rnn.pack_padded_sequence(emb_x, len_x)\n        hs, hn = self.encoder(emb_x, None)\n        hs, _ = torch.nn.utils.rnn.pad_packed_sequence(hs)\n         \n        dec_init_state = T.sum(hs * mask_x, 0) / T.sum(mask_x, 0)\n        dec_init_state = T.tanh(self.get_dec_init_state(dec_init_state))\n        return hs, dec_init_state\n\n    def decode_once(self, y, hs, dec_init_state, mask_x, x=None, max_ext_len=None, acc_att=None):\n        batch_size = hs.size(1)\n        if T.sum(y) < 0:\n            y_emb = Variable(T.zeros((1, batch_size, self.dim_y))).to(self.device)\n        else:\n            y_emb = self.w_rawdata_emb(y)\n        mask_y = Variable(T.ones((1, batch_size, 1))).to(self.device)\n\n        if self.copy and self.coverage:\n            hcs, dec_status, atted_context, att_dist, xids, C = self.decoder(y_emb, hs, dec_init_state, mask_x, mask_y, x, acc_att)\n        elif self.copy:\n            hcs, dec_status, atted_context, att_dist, xids = self.decoder(y_emb, hs, dec_init_state, mask_x, mask_y, xid=x)\n        elif self.coverage:\n            hcs, dec_status, atted_context, att_dist, C = self.decoder(y_emb, hs, dec_init_state, mask_x, mask_y, init_coverage=acc_att)\n        else:\n            hcs, dec_status, atted_context = self.decoder(y_emb, hs, dec_init_state, mask_x, mask_y)\n        \n        if self.copy:\n            y_pred = self.word_prob(dec_status, atted_context, y_emb, att_dist, xids, max_ext_len)\n        else:\n            y_pred = self.word_prob(dec_status, atted_context, y_emb)\n\n        if self.coverage:\n            return y_pred, hcs, C\n        else:\n            return y_pred, hcs\n\n    def forward(self, x, len_x, y, mask_x, mask_y, x_ext, y_ext, max_ext_len):\n        \n        hs, dec_init_state = self.encode(x, len_x, mask_x)\n\n        y_emb = self.w_rawdata_emb(y)\n        y_shifted = y_emb[:-1, :, :]\n        y_shifted = T.cat((Variable(torch.zeros(1, *y_shifted[0].size())).to(self.device), y_shifted), 0)\n        h0 = dec_init_state\n        if self.cell == ""lstm"":\n            h0 = (dec_init_state, dec_init_state)\n        if self.coverage:\n            acc_att = Variable(torch.zeros(T.transpose(x, 0, 1).size())).to(self.device) # B * len(x)\n\n        if self.copy and self.coverage:\n            hcs, dec_status, atted_context, att_dist, xids, C = self.decoder(y_shifted, hs, h0, mask_x, mask_y, x_ext, acc_att)\n        elif self.copy:\n            hcs, dec_status, atted_context, att_dist, xids = self.decoder(y_shifted, hs, h0, mask_x, mask_y, xid=x_ext)\n        elif self.coverage:\n            hcs, dec_status, atted_context, att_dist, C = self.decoder(y_shifted, hs, h0, mask_x, mask_y, init_coverage=acc_att)\n        else:\n            hcs, dec_status, atted_context = self.decoder(y_shifted, hs, h0, mask_x, mask_y)\n        \n        if self.copy:\n            y_pred = self.word_prob(dec_status, atted_context, y_shifted, att_dist, xids, max_ext_len)\n            cost = self.nll_loss(y_pred, y_ext, mask_y, self.avg_nll)\n        else:\n            y_pred = self.word_prob(dec_status, atted_context, y_shifted)\n            cost = self.nll_loss(y_pred, y, mask_y, self.avg_nll)\n        \n        if self.coverage:\n            cost_c = T.mean(T.sum(T.min(att_dist, C), 2))\n            return y_pred, cost, cost_c\n        else:\n            return y_pred, cost, None\n    \n\n'"
prepare_data.py,0,"b'# -*- coding: utf-8 -*-\nimport operator\nfrom os import makedirs\nfrom os.path import exists\nimport argparse\nfrom configs import *\nimport pickle\nimport numpy as np\nimport re\nfrom random import shuffle\nimport string\nimport struct\n\ndef run(d_type, d_path):\n    prepare_deepmind(d_path)\n\nstop_words = {""-lrb-"", ""-rrb-"", ""-""}\nunk_words = {""unk"", ""<unk>""}\n\ndef get_xy_tuple(cont, head, cfg):\n    x = read_cont(cont, cfg)\n    y = read_head(head, cfg)\n\n    if x != None and y != None:\n        return (x, y)\n    else:\n        return None\n\ndef load_lines(d_path, f_name,  configs):\n    lines = []\n    f_path = d_path + f_name\n    with open(f_path, \'r\') as f:\n        for line in f:\n            line = line.strip(""\\n"").lower()\n            fs = line.split(""<summ-content>"")\n            if len(fs) == 2:\n                xy_tuple = get_xy_tuple(fs[1], fs[0], configs)\n            else:\n                print(""ERROR:"" + line)\n                continue\n            if xy_tuple != None:\n                lines.append(xy_tuple)\n    return lines\n\ndef load_dict(d_path, f_name, dic, dic_list):\n    f_path = d_path + f_name\n    f = open(f_path, ""r"")\n    for line in f:\n        line = line.strip(\'\\n\').strip(\'\\r\').lower()\n        if line:\n            tf = line.split()\n            if len(tf) == 2:\n                dic[tf[0]] = int(tf[1])\n                dic_list.append(tf[0])\n            else:\n                print(""error:"", line)\n    return dic, dic_list\n\ndef to_dict(xys, dic):\n    # dict should not consider test set!!!!!\n    for xy in xys:\n        sents, summs = xy\n        y = summs[0]\n        for w in y:\n            if w in dic:\n                dic[w] += 1\n            else:\n                dic[w] = 1\n                \n        x = sents[0]\n        for w in x:\n            if w in dic:\n                dic[w] += 1\n            else:\n                dic[w] = 1\n    return dic\n\n\ndef del_num(s):\n    return re.sub(r""(\\b|\\s+\\-?|^\\-?)(\\d+|\\d*\\.\\d+)\\b"",""#"", s)\n\ndef read_cont(f_cont, cfg):\n    lines = []\n    line = f_cont #del_num(f_cont)\n    words = line.split()\n    num_words = len(words)\n    if num_words >= cfg.MIN_LEN_X and num_words < cfg.MAX_LEN_X:\n        lines += words\n    elif num_words >= cfg.MAX_LEN_X:\n        lines += words[0:cfg.MAX_LEN_X]\n    lines += [cfg.W_EOS]\n    return (lines, f_cont) if len(lines) >= cfg.MIN_LEN_X and len(lines) <= cfg.MAX_LEN_X+1 else None\n\ndef abstract2sents(abstract, cfg):\n  cur = 0\n  sents = []\n  while True:\n    try:\n      start_p = abstract.index(cfg.W_LS, cur)\n      end_p = abstract.index(cfg.W_RS, start_p + 1)\n      cur = end_p + len(cfg.W_RS)\n      sents.append(abstract[start_p+len(cfg.W_LS):end_p])\n    except ValueError as e: # no more sentences\n      return sents\n\ndef read_head(f_head, cfg):\n    lines = []\n\n    sents = abstract2sents(f_head, cfg)\n    line = \' \'.join(sents)\n    words = line.split()\n    num_words = len(words)\n    if num_words >= cfg.MIN_LEN_Y and num_words <= cfg.MAX_LEN_Y:\n        lines += words\n        lines += [cfg.W_EOS]\n    elif num_words > cfg.MAX_LEN_Y: # do not know if should be stoped\n        lines = words[0 : cfg.MAX_LEN_Y + 1] # one more word.\n    \n    return (lines, sents) if len(lines) >= cfg.MIN_LEN_Y and len(lines) <= cfg.MAX_LEN_Y+1  else None\n\ndef prepare_deepmind(d_path):\n    configs = DeepmindConfigs()\n    TRAINING_PATH = configs.cc.TRAINING_DATA_PATH\n    VALIDATE_PATH = configs.cc.VALIDATE_DATA_PATH\n    TESTING_PATH = configs.cc.TESTING_DATA_PATH\n    RESULT_PATH = configs.cc.RESULT_PATH\n    MODEL_PATH = configs.cc.MODEL_PATH\n    BEAM_SUMM_PATH = configs.cc.BEAM_SUMM_PATH\n    BEAM_GT_PATH = configs.cc.BEAM_GT_PATH\n    GROUND_TRUTH_PATH = configs.cc.GROUND_TRUTH_PATH\n    SUMM_PATH = configs.cc.SUMM_PATH\n    TMP_PATH = configs.cc.TMP_PATH\n\n    print (""train: "" + TRAINING_PATH)\n    print (""test: "" + TESTING_PATH)\n    print (""validate: "" + VALIDATE_PATH) \n    print (""result: "" + RESULT_PATH)\n    print (""model: "" + MODEL_PATH)\n    print (""tmp: "" + TMP_PATH)\n\n    if not exists(TRAINING_PATH):\n        makedirs(TRAINING_PATH)\n    if not exists(VALIDATE_PATH):\n        makedirs(VALIDATE_PATH)\n    if not exists(TESTING_PATH):\n        makedirs(TESTING_PATH)\n    if not exists(RESULT_PATH):\n        makedirs(RESULT_PATH)\n    if not exists(MODEL_PATH):\n        makedirs(MODEL_PATH)\n    if not exists(BEAM_SUMM_PATH):\n        makedirs(BEAM_SUMM_PATH)\n    if not exists(BEAM_GT_PATH):\n        makedirs(BEAM_GT_PATH)\n    if not exists(GROUND_TRUTH_PATH):\n        makedirs(GROUND_TRUTH_PATH)\n    if not exists(SUMM_PATH):\n        makedirs(SUMM_PATH)\n    if not exists(TMP_PATH):\n        makedirs(TMP_PATH)\n    \n        \n    print (""trainset..."")\n    train_xy_list = load_lines(d_path, ""train.txt"", configs)\n    \n    print (""dump train..."")\n    pickle.dump(train_xy_list, open(TRAINING_PATH + ""train.pkl"", ""wb""), protocol = pickle.HIGHEST_PROTOCOL)\n    \n\n    print (""fitering and building dict..."")\n    use_abisee = True\n    all_dic1 = {}\n    all_dic2 = {}\n    dic_list = []\n    all_dic1, dic_list = load_dict(d_path, ""vocab"", all_dic1, dic_list)\n    all_dic2 = to_dict(train_xy_list, all_dic2)\n    for w, tf in all_dic2.items():\n        if w not in all_dic1:\n            all_dic1[w] = tf\n\n    candiate_list = dic_list[0:configs.PG_DICT_SIZE] # 50000\n    candiate_set = set(candiate_list)\n\n    dic = {}\n    w2i = {}\n    i2w = {}\n    w2w = {}\n\n    for w in [configs.W_PAD, configs.W_UNK, configs.W_EOS]:\n    #for w in [configs.W_PAD, configs.W_UNK, configs.W_BOS, configs.W_EOS, configs.W_LS, configs.W_RS]:\n        w2i[w] = len(dic)\n        i2w[w2i[w]] = w\n        dic[w] = 10000\n        w2w[w] = w\n\n    for w, tf in all_dic1.items():\n        if w in candiate_set:\n            w2i[w] = len(dic)\n            i2w[w2i[w]] = w\n            dic[w] = tf\n            w2w[w] = w\n        else:\n            w2w[w] = configs.W_UNK \n    hfw = []\n    sorted_x = sorted(dic.items(), key=operator.itemgetter(1), reverse=True)\n    for w in sorted_x:\n        hfw.append(w[0])\n\n    assert len(hfw) == len(dic)\n    assert len(w2i) == len(dic)\n    print (""dump dict..."")\n    pickle.dump([all_dic1, dic, hfw, w2i, i2w, w2w], open(TRAINING_PATH + ""dic.pkl"", ""wb""), protocol = pickle.HIGHEST_PROTOCOL)\n    \n    print (""testset..."")\n    test_xy_list = load_lines(d_path, ""test.txt"", configs)\n\n    print (""validset..."")\n    valid_xy_list = load_lines(d_path, ""val.txt"", configs)\n\n\n    print (""#train = "", len(train_xy_list))\n    print (""#test = "", len(test_xy_list))\n    print (""#validate = "", len(valid_xy_list))\n        \n    print (""#all_dic = "", len(all_dic1), "", #dic = "", len(dic), "", #hfw = "", len(hfw))\n\n    print (""dump test..."")\n    pickle.dump(test_xy_list, open(TESTING_PATH + ""test.pkl"", ""wb""), protocol = pickle.HIGHEST_PROTOCOL)\n    shuffle(test_xy_list)\n    pickle.dump(test_xy_list[0:2000], open(TESTING_PATH + ""pj2000.pkl"", ""wb""), protocol = pickle.HIGHEST_PROTOCOL)\n\n    print (""dump validate..."")\n    pickle.dump(valid_xy_list, open(VALIDATE_PATH + ""valid.pkl"", ""wb""), protocol = pickle.HIGHEST_PROTOCOL)\n    pickle.dump(valid_xy_list[0:1000], open(VALIDATE_PATH + ""pj1000.pkl"", ""wb""), protocol = pickle.HIGHEST_PROTOCOL)\n    \n    print (""done."")\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""-d"", ""--data"", default=""deepmind"", help=""dataset path"", )\n    args = parser.parse_args()\n\n    data_type = ""deepmind""\n    # download from finished_files: https://github.com/JafferWilson/Process-Data-of-CNN-DailyMail\n    raw_path = ""/home/pijili/data/summarization-data/SDS/cnndm-pj/""\n\n    print (data_type, raw_path)\n    run(data_type, raw_path)\n'"
prepare_rouge.py,0,"b'#pylint: skip-file\nimport sys\nimport os\nfrom configs import * \n\ncfg = DeepmindConfigs()\n\n# config file for ROUGE\nROUGE_PATH = cfg.cc.RESULT_PATH \nSUMM_PATH = cfg.cc.SUMM_PATH\nMODEL_PATH = cfg.cc.GROUND_TRUTH_PATH\ni2summ = {}\nsumm2i = {}\ni2model = {}\n\n# for result\nflist = os.listdir(SUMM_PATH)\ni = 0\nfor fname in flist:\n    i2summ[str(i)] = fname\n    summ2i[fname] = str(i)\n    i += 1\n\n# for models\nflist = os.listdir(MODEL_PATH)\ni2model = {}\nfor fname in flist:\n    if fname not in summ2i:\n        raise IOError\n\n    i = summ2i[fname]\n    i2model[i] = fname\n\nassert len(i2model) == len(i2summ)\n\n# write to config file\nrouge_s = ""<ROUGE-EVAL version=\\""1.0\\"">""\nfile_id = 0\nfor file_id, fsumm in i2summ.items():\n    rouge_s +=  ""\\n<EVAL ID=\\"""" + file_id + ""\\"">"" \\\n            + ""\\n<PEER-ROOT>"" \\\n            + SUMM_PATH \\\n            + ""\\n</PEER-ROOT>"" \\\n            + ""\\n<MODEL-ROOT>"" \\\n            + ""\\n"" + MODEL_PATH \\\n            + ""\\n</MODEL-ROOT>"" \\\n            + ""\\n<INPUT-FORMAT TYPE=\\""SPL\\"">"" \\\n            + ""\\n</INPUT-FORMAT>"" \\\n            + ""\\n<PEERS>"" \\\n            + ""\\n<P ID=\\""C\\"">"" + fsumm + ""</P>"" \\\n            + ""\\n</PEERS>"" \\\n            + ""\\n<MODELS>""\n\n    rouge_s += ""\\n<M ID=\\"""" + file_id + ""\\"">"" + i2model[file_id] + ""</M>""\n    rouge_s += ""\\n</MODELS>\\n</EVAL>""\n                    \nrouge_s += ""\\n</ROUGE-EVAL>""\n\nwith open(ROUGE_PATH + ""myROUGE_Config.xml"", ""w"") as f_rouge:\n    f_rouge.write(rouge_s) \n'"
utils_pg.py,6,"b'# -*- coding: utf-8 -*-\n#pylint: skip-file\nimport numpy as np\nfrom numpy.random import random as rand\nimport pickle\nimport sys\nimport os\nimport shutil\nfrom copy import deepcopy\nimport random\n\nimport torch\nfrom torch import nn\n\n\ndef init_seeds():\n    random.seed(123)\n    torch.manual_seed(123)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(123)\n\ndef init_lstm_weight(lstm):\n    for param in lstm.parameters():\n        if len(param.shape) >= 2: # weights\n            init_ortho_weight(param.data)\n        else: # bias\n            init_bias(param.data)\n\ndef init_gru_weight(gru):\n    for param in gru.parameters():\n        if len(param.shape) >= 2: # weights\n            init_ortho_weight(param.data)\n        else: # bias\n            init_bias(param.data)\n\ndef init_linear_weight(linear):\n    init_xavier_weight(linear.weight)\n    if linear.bias is not None:\n        init_bias(linear.bias)\n\ndef init_normal_weight(w):\n    nn.init.normal_(w, mean=0, std=0.01)\n\ndef init_uniform_weight(w):\n    nn.init.uniform_(w, -0.1, 0.1)\n\ndef init_ortho_weight(w):\n    nn.init.orthogonal_(w)\n\ndef init_xavier_weight(w):\n    nn.init.xavier_normal_(w)\n\ndef init_bias(b):\n    nn.init.constant_(b, 0.)\n\ndef rebuild_dir(path):\n    if os.path.exists(path):\n        try:\n            shutil.rmtree(path)\n        except OSError:\n            pass\n    os.mkdir(path)\n\ndef save_model(f, model, optimizer):\n    torch.save({""model_state_dict"" : model.state_dict(),\n            ""optimizer_state_dict"" : optimizer.state_dict()},\n            f)\n \ndef load_model(f, model, optimizer):\n    checkpoint = torch.load(f)\n    model.load_state_dict(checkpoint[""model_state_dict""])\n    optimizer.load_state_dict(checkpoint[""optimizer_state_dict""])\n    return model, optimizer\n\ndef sort_samples(x, len_x, mask_x, y, len_y, \\\n                 mask_y, oys, x_ext, y_ext, oovs):\n    sorted_x_idx = np.argsort(len_x)[::-1]\n    \n    sorted_x_len = np.array(len_x)[sorted_x_idx]\n    sorted_x = x[:, sorted_x_idx]\n    sorted_x_mask = mask_x[:, sorted_x_idx, :]\n    sorted_oovs = [oovs[i] for i in sorted_x_idx]\n\n    sorted_y_len = np.array(len_y)[sorted_x_idx]\n    sorted_y = y[:, sorted_x_idx]\n    sorted_y_mask = mask_y[:, sorted_x_idx, :]\n    sorted_oys = [oys[i] for i in sorted_x_idx]\n    sorted_x_ext = x_ext[:, sorted_x_idx]\n    sorted_y_ext = y_ext[:, sorted_x_idx]\n    \n    return sorted_x, sorted_x_len, sorted_x_mask, sorted_y, \\\n           sorted_y_len, sorted_y_mask, sorted_oys, \\\n           sorted_x_ext, sorted_y_ext, sorted_oovs\n\ndef print_sent_dec(y_pred, y, y_mask, oovs, modules, consts, options, batch_size):\n    print(""golden truth and prediction samples:"")\n    max_y_words = np.sum(y_mask, axis = 0)\n    max_y_words = max_y_words.reshape((batch_size))\n    max_num_docs = 16 if batch_size > 16 else batch_size\n    is_unicode = options[""is_unicode""]\n    dict_size = len(modules[""i2w""])\n    for idx_doc in range(max_num_docs):\n        print(idx_doc + 1, ""----------------------------------------------------------------------------------------------------"")\n        sent_true= """"\n        for idx_word in range(max_y_words[idx_doc]):\n            i = y[idx_word, idx_doc] if options[""has_learnable_w2v""] else np.argmax(y[idx_word, idx_doc]) \n            if i in modules[""i2w""]:\n                sent_true += modules[""i2w""][i]\n            else:\n                sent_true += oovs[idx_doc][i - dict_size]\n            if not is_unicode:\n                sent_true += "" ""\n\n        if is_unicode:\n            print(sent_true.encode(""utf-8""))\n        else:\n            print(sent_true)\n\n        print()\n\n        sent_pred = """"\n        for idx_word in range(max_y_words[idx_doc]):\n            i = torch.argmax(y_pred[idx_word, idx_doc, :]).item()\n            if i in modules[""i2w""]:\n                sent_pred += modules[""i2w""][i]\n            else:\n                sent_pred += oovs[idx_doc][i - dict_size]\n            if not is_unicode:\n                sent_pred += "" ""\n        if is_unicode:\n            print(sent_pred.encode(""utf-8""))\n        else:\n            print(sent_pred)\n    print(""----------------------------------------------------------------------------------------------------"")\n    print()\n\n\ndef write_for_rouge(fname, ref_sents, dec_words, cfg):\n    dec_sents = []\n    while len(dec_words) > 0:\n        try:\n            fst_period_idx = dec_words.index(""."")\n        except ValueError:\n            fst_period_idx = len(dec_words)\n        sent = dec_words[:fst_period_idx + 1]\n        dec_words = dec_words[fst_period_idx + 1:]\n        dec_sents.append(\' \'.join(sent))\n\n    ref_file = """".join((cfg.cc.GROUND_TRUTH_PATH, fname))\n    decoded_file = """".join((cfg.cc.SUMM_PATH, fname))\n\n    with open(ref_file, ""w"") as f:\n        for idx, sent in enumerate(ref_sents):\n            sent = sent.strip()\n            f.write(sent) if idx == len(ref_sents) - 1 else f.write(sent + ""\\n"")\n    with open(decoded_file, ""w"") as f:\n        for idx, sent in enumerate(dec_sents):\n            sent = sent.strip()\n            f.write(sent) if idx == len(dec_sents) - 1 else f.write(sent + ""\\n"")\n\ndef write_summ(dst_path, summ_list, num_summ, options, i2w = None, oovs=None, score_list = None):\n    assert num_summ > 0\n    with open(dst_path, ""w"") as f_summ:\n        if num_summ == 1:\n            if score_list != None:\n                f_summ.write(str(score_list[0]))\n                f_summ.write(""\\t"")\n            if i2w != None:\n                \'\'\'\n                for e in summ_list:\n                    e = int(e)\n                    if e in i2w:\n                        print i2w[e],\n                    else:\n                        print oovs[e - len(i2w)],\n                print ""\\n""\n                \'\'\'\n                s = []\n                for e in summ_list:\n                    e = int(e)\n                    if e in i2w:\n                        s.append(i2w[e])\n                    else:\n                        s.append(oovs[e - len(i2w)])\n                s = "" "".join(s)\n            else:\n                s = "" "".join(summ_list)\n            f_summ.write(s)\n            f_summ.write(""\\n"")\n        else:\n            assert num_summ == len(summ_list)\n            if score_list != None:\n                assert num_summ == len(score_list)\n\n            for i in range(num_summ):\n                if score_list != None:\n                    f_summ.write(str(score_list[i]))\n                    f_summ.write(""\\t"")\n                if i2w != None:\n                    \'\'\'\n                    for e in summ_list[i]:\n                        e = int(e)\n                        if e in i2w:\n                            print i2w[e],\n                        else:\n                            print oovs[e - len(i2w)],\n                    print ""\\n""\n                    \'\'\'\n                    s = []\n                    for e in summ_list[i]:\n                        e = int(e)\n                        if e in i2w:\n                            s.append(i2w[e])\n                        else:\n                            s.append(oovs[e - len(i2w)])\n                    s = "" "".join(s)\n                else:\n                    s = "" "".join(summ_list[i])\n\n                f_summ.write(s)\n                f_summ.write(""\\n"")\n\n\n'"
word_prob_layer.py,10,"b'# -*- coding: utf-8 -*-\n#pylint: skip-file\nimport torch\nimport torch as T\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nfrom utils_pg import *\n\nclass WordProbLayer(nn.Module):\n    def __init__(self, hidden_size, ctx_size, dim_y, dict_size, device, copy, coverage):\n        super(WordProbLayer, self).__init__()\n        self.hidden_size = hidden_size\n        self.ctx_size = ctx_size \n        self.dim_y = dim_y\n        self.dict_size = dict_size\n        self.device = device\n        self.copy = copy\n        self.coverage = coverage\n\n        self.w_ds = nn.Parameter(torch.Tensor(self.hidden_size, self.hidden_size + self.ctx_size + self.dim_y))\n        self.b_ds = nn.Parameter(torch.Tensor(self.hidden_size)) \n        self.w_logit = nn.Parameter(torch.Tensor(self.dict_size, self.hidden_size))\n        self.b_logit = nn.Parameter(torch.Tensor(self.dict_size)) \n\n        if self.copy:\n            self.v = nn.Parameter(torch.Tensor(1, self.hidden_size + self.ctx_size + self.dim_y))\n            self.bv = nn.Parameter(torch.Tensor(1))\n\n        self.init_weights()\n\n    def init_weights(self):\n        init_xavier_weight(self.w_ds)\n        init_bias(self.b_ds)\n        init_xavier_weight(self.w_logit)\n        init_bias(self.b_logit)\n        if self.copy:\n            init_xavier_weight(self.v)\n            init_bias(self.bv)\n\n\n    def forward(self, ds, ac, y_emb, att_dist=None, xids=None, max_ext_len=None):\n        h = T.cat((ds, ac, y_emb), 2)\n        logit = T.tanh(F.linear(h, self.w_ds, self.b_ds))\n        logit = F.linear(logit, self.w_logit, self.b_logit)\n        y_dec = T.softmax(logit, dim = 2)\n        \n        if self.copy:\n            if max_ext_len > 0:\n                ext_zeros = Variable(torch.zeros(y_dec.size(0), y_dec.size(1), max_ext_len)).to(self.device)\n                y_dec = T.cat((y_dec, ext_zeros), 2)\n            g = T.sigmoid(F.linear(h, self.v, self.bv))\n            y_dec = (g * y_dec).scatter_add(2, xids, (1 - g) * att_dist)\n        \n        return y_dec\n'"
