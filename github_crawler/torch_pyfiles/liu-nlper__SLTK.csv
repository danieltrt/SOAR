file_path,api_count,code
main.py,3,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n""""""\nUsage:\n\n1. preprocessing and train\n\n    $ CUDA_VISIBLE_DEVICES=0 python3 main.py --config ./configs/demo.train.yml -p --train\n\n2. train\n\n\xe8\x8b\xa5\xe5\xb7\xb2\xe7\xbb\x8f\xe5\xae\x8c\xe6\x88\x90\xe4\xba\x86\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\xef\xbc\x8c\xe5\x88\x99\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9b\xb4\xe6\x8e\xa5\xe8\xbf\x9b\xe8\xa1\x8c\xe6\xa8\xa1\xe5\x9e\x8b\xe8\xae\xad\xe7\xbb\x83:\n\n    $ CUDA_VISIBLE_DEVICES=0 python3 main.py --config ./configs/demo.train.yml --train\n\n3. test\n\n    $ CUDA_VISIBLE_DEVICES=0 python3 main.py --config ./configs/demo.train.yml --test\n\n""""""\nimport os\nimport sys\nimport codecs\nfrom string import ascii_letters, digits\nfrom collections import Counter\nimport yaml\nimport h5py\nimport numpy as np\n\nfrom optparse import OptionParser\n\nfrom sltk.preprocessing import normalize_word\n\nfrom sltk.utils import read_conllu\nfrom sltk.utils import build_word_embed\nfrom sltk.utils import tokens2id_array\nfrom sltk.utils import check_parent_dir, object2pkl_file, read_bin\n\nfrom sltk.data import DataIter, DataUtil\n\nfrom sltk.nn.modules import SLModel\nfrom sltk.train import SLTrainer\nfrom sltk.infer import Inference\n\nimport torch\nimport torch.optim as optim\n\n\ndef parse_opts():\n    op = OptionParser()\n    op.add_option(\n        \'-c\', \'--config\', dest=\'config\', type=\'str\', help=\'\xe9\x85\x8d\xe7\xbd\xae\xe6\x96\x87\xe4\xbb\xb6\xe8\xb7\xaf\xe5\xbe\x84\')\n    op.add_option(\'--train\', dest=\'train\', action=\'store_true\', default=True, help=\'\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\xbc\x8f\')\n    op.add_option(\'--test\', dest=\'test\', action=\'store_true\', default=False, help=\'\xe6\xb5\x8b\xe8\xaf\x95\xe6\xa8\xa1\xe5\xbc\x8f\')\n    op.add_option(\n        \'-p\', \'--preprocess\', dest=\'preprocess\', action=\'store_true\', default=False, help=\'\xe6\x98\xaf\xe5\x90\xa6\xe8\xbf\x9b\xe8\xa1\x8c\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\')\n    argv = [] if not hasattr(sys.modules[\'__main__\'], \'__file__\') else sys.argv[1:]\n    (opts, args) = op.parse_args(argv)\n    if not opts.config:\n        op.print_help()\n        exit()\n    if opts.test:\n        opts.train = False\n    return opts\n\n\ndef update_feature_dict(tokens_list, feature_dict, feature_cols, feature_names,\n                        normalize=True, has_label=True):\n    """"""\n    \xe6\x9b\xb4\xe6\x96\xb0\xe7\x89\xb9\xe5\xbe\x81\xe5\xad\x97\xe5\x85\xb8\n    Args:\n        tokens_list: list(list)\n        feature_dict: dict\n        feature_cols: list(int)\n        feature_names: list(str)\n        normalize: bool, \xe6\x98\xaf\xe5\x90\xa6\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xe5\x8d\x95\xe8\xaf\x8d\n        has_label: bool\n    """"""\n    for i, col in enumerate(feature_cols):\n        for token in tokens_list[col]:\n            if normalize:\n                token = normalize_word(token)\n            feature_dict[feature_names[i]].update([token])\n    if has_label:\n        for label in tokens_list[-1]:\n            feature_dict[\'label\'].add(label)\n\n\ndef extract_feature_dict(path_data, feature_cols, feature_names, feature_dict,\n                         sentence_lens=None, normalize=True, has_label=True):\n    """"""\xe4\xbb\x8e\xe6\x95\xb0\xe6\x8d\xae\xe4\xb8\xad\xe7\xbb\x9f\xe8\xae\xa1\xe7\x89\xb9\xe5\xbe\x81\n    Args:\n        path_data: str, \xe6\x95\xb0\xe6\x8d\xae\xe8\xb7\xaf\xe5\xbe\x84\n        feature_cols: list(int), \xe7\x89\xb9\xe5\xbe\x81\xe7\x9a\x84\xe5\x88\x97\xe6\x95\xb0\n        feature_names: list(str), \xe7\x89\xb9\xe5\xbe\x81\xe5\x90\x8d\xe7\xa7\xb0\n        feature_dict: dict\n        sentence_lens: list, \xe7\x94\xa8\xe4\xba\x8e\xe8\xae\xb0\xe5\xbd\x95\xe5\x8f\xa5\xe5\xad\x90\xe9\x95\xbf\xe5\xba\xa6\n        normalize: bool, \xe6\x98\xaf\xe5\x90\xa6\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xe5\x8d\x95\xe8\xaf\x8d\n        has_label: bool, \xe6\x95\xb0\xe6\x8d\xae\xe6\x98\xaf\xe5\x90\xa6\xe5\xb8\xa6\xe6\x9c\x89\xe6\xa0\x87\xe7\xad\xbe\n    """"""\n    data_idx = 0\n    for i, tokens_list in enumerate(read_conllu(path_data)):\n        sys.stdout.write(\'`{0}`: {1}\\r\'.format(path_data, i))\n        sys.stdout.flush()\n        update_feature_dict(\n            tokens_list, feature_dict, feature_cols, feature_names,\n            normalize=normalize, has_label=has_label)\n        sentence_lens.append(len(tokens_list[0]))\n        data_idx += 1\n    return data_idx\n\n\ndef data2hdf5(path_data, data_count, feature_cols, feature_names, token2id_dict,\n              use_char=False, max_word_len=None, has_label=True, normalize=True):\n    """"""\xe5\xb0\x86\xe6\x95\xb0\xe6\x8d\xae\xe8\xbd\xac\xe4\xb8\xbaid\xe5\xbd\xa2\xe5\xbc\x8f, \xe5\xad\x98\xe5\x85\xa5hdf5\xe6\xa0\xbc\xe5\xbc\x8f\xe6\x96\x87\xe4\xbb\xb6\n    Args:\n        path_data: \xe5\x8e\x9f\xe5\xa7\x8b\xe6\x96\x87\xe4\xbb\xb6\xe8\xb7\xaf\xe5\xbe\x84\n        data_count: int, \xe6\x95\xb0\xe6\x8d\xae\xe9\x87\x8f\n        feature_cols: list(int), \xe7\x89\xb9\xe5\xbe\x81\xe7\x9a\x84\xe5\x88\x97\xe6\x95\xb0\n        feature_names: list(str), \xe7\x89\xb9\xe5\xbe\x81\xe5\x90\x8d\xe7\xa7\xb0\n        token2id_dict: dict\n        use_char: bool, \xe6\x98\xaf\xe5\x90\xa6\xe4\xbd\xbf\xe7\x94\xa8char feature\n        max_word_len: int, \xe5\x8d\x95\xe8\xaf\x8d\xe6\x9c\x80\xe5\xa4\xa7\xe9\x95\xbf\xe5\xba\xa6, \xe7\x94\xa8\xe4\xbd\x9c\xe6\x8f\x90\xe5\x8f\x96char feature\n        has_label: bool, \xe6\x95\xb0\xe6\x8d\xae\xe6\x98\xaf\xe5\x90\xa6\xe5\xb8\xa6\xe6\x9c\x89\xe6\xa0\x87\xe7\xad\xbe\n        normalize: bool, \xe6\x98\xaf\xe5\x90\xa6\xe6\xa0\x87\xe5\x87\x86\xe5\x8c\x96\xe5\x8d\x95\xe8\xaf\x8d\n    """"""\n    def padding_char(word, max_word_len):\n        """"""\n        \xe6\x88\xaa\xe5\x9b\xbe\xe9\x95\xbf\xe5\x8d\x95\xe8\xaf\x8d\xe3\x80\x81\xe8\xa1\xa5\xe5\x85\xa8\xe7\x9f\xad\xe5\x8d\x95\xe8\xaf\x8d\n        Args:\n            word: str\n            max_word_len: int, \xe5\x8d\x95\xe8\xaf\x8d\xe6\x9c\x80\xe5\xa4\xa7\xe9\x95\xbf\xe5\xba\xa6\n        Return:\n            word: str\n        """"""\n        if len(word) > max_word_len:\n            half = int(max_word_len // 2)\n            word = word[:half] + word[-(max_word_len-half):]\n            return word\n        return word + \' \' * (max_word_len - len(word))\n\n    # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96hdf5\xe6\x96\x87\xe4\xbb\xb6\n    path_hdf5 = path_data + \'.hdf5\'\n    file_hdf5 = h5py.File(path_hdf5, \'w\')\n    dt = h5py.special_dtype(vlen=np.dtype(np.int32).type)\n    dataset_dict = dict()\n    for feature_name in feature_names:\n        dataset = file_hdf5.create_dataset(feature_name, shape=(data_count,), dtype=dt)\n        dataset_dict[feature_name] = dataset\n    if use_char:\n        dataset_char = file_hdf5.create_dataset(\'char\', shape=(data_count,), dtype=dt)\n        dataset_dict[\'char\'] = dataset_char\n    dataset_label = file_hdf5.create_dataset(\'label\', shape=(data_count,), dtype=dt)\n    dataset_dict[\'label\'] = dataset_label\n\n    for i, tokens_list in enumerate(read_conllu(path_data)):\n        sys.stdout.write(\'`{0}`: {1}\\r\'.format(path_hdf5, i))\n        sys.stdout.flush()\n        for j, col in enumerate(feature_cols):\n            feature_name = feature_names[j]\n            tokens = tokens_list[col]\n            if normalize:  # normalize\n                tokens = [normalize_word(token) for token in tokens]\n            token_arr = tokens2id_array(tokens, token2id_dict[feature_name])\n            dataset_dict[feature_name][i] = token_arr\n        if use_char:  # \xe6\x8f\x90\xe5\x8f\x96char feature\n            words = \'\'.join([padding_char(word, max_word_len) for word in tokens_list[0]])\n            char_arr = tokens2id_array(words, token2id_dict[\'char\'])\n            dataset_dict[\'char\'][i] = char_arr\n        if has_label:\n            label_arr = tokens2id_array(tokens_list[-1], token2id_dict[\'label\'])\n            dataset_dict[\'label\'][i] = label_arr\n    sys.stdout.write(\'`{0}`: {1}\\n\'.format(path_hdf5, i+1))\n    sys.stdout.flush()\n\n    file_hdf5.close()\n\n\ndef preprocessing(configs):\n    """"""\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\n    Args:\n        configs: yaml configuration object\n    """"""\n    path_train = configs[\'data_params\'][\'path_train\']\n    path_dev = configs[\'data_params\'][\'path_dev\'] if \'path_dev\' in configs[\'data_params\'] else None\n    path_test = configs[\'data_params\'][\'path_test\'] if \'path_test\' in configs[\'data_params\'] else None\n\n    feature_cols = configs[\'data_params\'][\'feature_cols\']\n    feature_names = configs[\'data_params\'][\'feature_names\']\n    min_counts = configs[\'data_params\'][\'alphabet_params\'][\'min_counts\']\n    root_alphabet = configs[\'data_params\'][\'alphabet_params\'][\'path\']\n    path_pretrain_list = configs[\'data_params\'][\'path_pretrain\']\n\n    use_char = configs[\'model_params\'][\'use_char\']\n    max_word_len = configs[\'model_params\'][\'char_max_len\']\n\n    normalize = configs[\'word_norm\']\n\n    feature_dict = {}\n    for feature_name in feature_names:\n        feature_dict[feature_name] = Counter()\n    feature_dict[\'label\'] = set()\n    sentence_lens = []\n\n    # \xe5\xa4\x84\xe7\x90\x86\xe8\xae\xad\xe7\xbb\x83\xe3\x80\x81\xe5\xbc\x80\xe5\x8f\x91\xe3\x80\x81\xe6\xb5\x8b\xe8\xaf\x95\xe6\x95\xb0\xe6\x8d\xae\n    print(\'\xe8\xaf\xbb\xe5\x8f\x96\xe6\x96\x87\xe4\xbb\xb6...\')\n    data_count_train = extract_feature_dict(\n        path_train, feature_cols, feature_names, feature_dict, sentence_lens,\n        normalize=normalize, has_label=True, )\n    print(\'`{0}`: {1}\'.format(path_train, data_count_train))\n    if path_dev:\n        data_count_dev = extract_feature_dict(\n            path_dev, feature_cols, feature_names, feature_dict, sentence_lens,\n            normalize=normalize, has_label=True)\n        print(\'`{0}`: {1}\'.format(path_dev, data_count_dev))\n    if path_test:\n        data_count_test = extract_feature_dict(\n            path_test, feature_cols, feature_names, feature_dict, sentence_lens,\n            normalize=normalize, has_label=False)\n        print(\'`{0}`: {1}\'.format(path_test, data_count_test))\n\n    # for name in feature_dict:\n    #     print(name, len(feature_dict[name]))\n\n    # \xe6\x9e\x84\xe5\xbb\xbalabel alphabet\n    token2id_dict = dict()\n    label2id_dict = dict()\n    for label_idx, label in enumerate(sorted(feature_dict[\'label\'])):\n        label2id_dict[label] = label_idx + 1  # \xe4\xbb\x8e1\xe5\xbc\x80\xe5\xa7\x8b\xe7\xbc\x96\xe5\x8f\xb7\n    token2id_dict[\'label\'] = label2id_dict\n    path_label2id_pkl = os.path.join(root_alphabet, \'label.pkl\')\n    check_parent_dir(path_label2id_pkl)\n    object2pkl_file(path_label2id_pkl, label2id_dict)\n\n    # \xe6\x9e\x84\xe5\xbb\xba\xe7\x89\xb9\xe5\xbe\x81alphabet\n    for i, feature_name in enumerate(feature_names):\n        feature2id_dict = dict()\n        start_idx = 1\n        for item in sorted(feature_dict[feature_name].items(), key=lambda d: d[1], reverse=True):\n            if item[1] < min_counts[i]:\n                continue\n            feature2id_dict[item[0]] = start_idx\n            start_idx += 1\n        token2id_dict[feature_name] = feature2id_dict\n        # write to file\n        object2pkl_file(\n            os.path.join(root_alphabet, \'{0}.pkl\'.format(feature_name)), feature2id_dict)\n\n    # \xe6\x9e\x84\xe5\xbb\xbachar alphabet\n    if use_char:\n        char2id_dict = {}\n        for i, c in enumerate(ascii_letters + digits):\n            char2id_dict[c] = i + 2\n        char2id_dict[\' \'] = 0\n        token2id_dict[\'char\'] = char2id_dict\n        object2pkl_file(os.path.join(root_alphabet, \'char.pkl\'), char2id_dict)\n\n    # \xe6\x9e\x84\xe5\xbb\xbaembedding table\n    print(\'\xe6\x8a\xbd\xe5\x8f\x96\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f...\')\n    for i, feature_name in enumerate(feature_names):\n        if path_pretrain_list[i]:\n            print(\'\xe7\x89\xb9\xe5\xbe\x81`{0}`\xe4\xbd\xbf\xe7\x94\xa8\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f`{1}`:\'.format(feature_name, path_pretrain_list[i]))\n            word_embed_table, exact_match_count, fuzzy_match_count, unknown_count, \\\n                total_count = build_word_embed(token2id_dict[feature_name], path_pretrain_list[i])\n            print(\'\\t\xe7\xb2\xbe\xe7\xa1\xae\xe5\x8c\xb9\xe9\x85\x8d: {0} / {1}\'.format(exact_match_count, total_count))\n            print(\'\\t\xe6\xa8\xa1\xe7\xb3\x8a\xe5\x8c\xb9\xe9\x85\x8d: {0} / {1}\'.format(fuzzy_match_count, total_count))\n            print(\'\\tOOV: {0} / {1}\'.format(unknown_count, total_count))\n            # write to file\n            path_pkl = os.path.join(os.path.dirname(path_pretrain_list[i]), \'{0}.embed.pkl\'.format(feature_name))\n            object2pkl_file(path_pkl, word_embed_table)\n\n    # \xe5\xb0\x86\xe6\x95\xb0\xe6\x8d\xae\xe8\xbd\xac\xe4\xb8\xbaid\xe5\xbd\xa2\xe5\xbc\x8f\xef\xbc\x8c\xe5\xad\x98\xe5\x85\xa5hdf5\xe6\x96\x87\xe4\xbb\xb6\n    print(\'convert data to hdf5...\')\n    data2hdf5(\n        path_train, data_count_train, feature_cols, feature_names,\n        token2id_dict, use_char, max_word_len, has_label=True, normalize=normalize)\n    if path_dev:\n        data2hdf5(path_dev, data_count_dev, feature_cols, feature_names,\n                  token2id_dict, use_char, max_word_len, has_label=True, normalize=normalize)\n    if path_test:\n        data2hdf5(path_test, data_count_test, feature_cols, feature_names,\n                  token2id_dict, use_char, max_word_len, has_label=False, normalize=normalize)\n\n\ndef init_model(configs):\n    """"""\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\xa8\xa1\xe5\x9e\x8b\n    Returns:\n        model: SLModel\n    """"""\n    use_char = configs[\'model_params\'][\'use_char\']\n\n    feature_names = configs[\'data_params\'][\'feature_names\']\n    # init feature alphabet size dict\n    feature_size_dict = dict()\n    root_alphabet = configs[\'data_params\'][\'alphabet_params\'][\'path\']\n    for feature_name in feature_names:\n        alphabet = read_bin(os.path.join(root_alphabet, \'{0}.pkl\'.format(feature_name)))\n        feature_size_dict[feature_name] = len(alphabet) + 1\n    alphabet = read_bin(os.path.join(root_alphabet, \'label.pkl\'))\n    feature_size_dict[\'label\'] = len(alphabet) + 1\n    if use_char:\n        alphabet = read_bin(os.path.join(root_alphabet, \'char.pkl\'))\n        feature_size_dict[\'char\'] = len(alphabet) + 1\n\n    # init feature dim size dict and pretrain embed dict\n    path_pretrain_list = configs[\'data_params\'][\'path_pretrain\']\n    embed_sizes = configs[\'model_params\'][\'embed_sizes\']\n    feature_dim_dict = dict()\n    for i, feature_name in enumerate(feature_names):\n        feature_dim_dict[feature_name] = embed_sizes[i]\n    pretrained_embed_dict = dict()\n    for i, feature_name in enumerate(feature_names):\n        if path_pretrain_list[i]:\n            path_pkl = os.path.join(os.path.dirname(path_pretrain_list[i]), \'{0}.embed.pkl\'.format(feature_name))\n            embed = read_bin(path_pkl)\n            feature_dim_dict[feature_name] = embed.shape[-1]\n            pretrained_embed_dict[feature_name] = embed\n    if use_char:\n        feature_dim_dict[\'char\'] = configs[\'model_params\'][\'char_dim\']\n\n    # init requires_grad_dict\n    require_grads = configs[\'model_params\'][\'require_grads\']\n    require_grad_dict = {}\n    for i, feature_name in enumerate(feature_names):\n        require_grad_dict[feature_name] = require_grads[i]\n    if use_char:\n        require_grad_dict[\'char\'] = configs[\'model_params\'][\'char_requires_grad\']\n\n    # init char parameters\n    filter_sizes = configs[\'model_params\'][\'conv_filter_sizes\']\n    filter_nums = configs[\'model_params\'][\'conv_filter_nums\']\n\n    # init rnn parameters\n    rnn_unit_type = configs[\'model_params\'][\'rnn_type\']\n    num_rnn_units = configs[\'model_params\'][\'rnn_units\']\n    num_layers = configs[\'model_params\'][\'rnn_layers\']\n    bi_flag = configs[\'model_params\'][\'bi_flag\']\n\n    use_crf = configs[\'model_params\'][\'use_crf\']\n\n    # init other parameters\n    dropout_rate = configs[\'model_params\'][\'dropout_rate\']\n    average_batch = configs[\'model_params\'][\'average_batch\']\n    deterministic = configs[\'model_params\'][\'deterministic\']\n    use_cuda = configs[\'model_params\'][\'use_cuda\']\n\n    # init model\n    sl_model = SLModel(\n        feature_names=feature_names, feature_size_dict=feature_size_dict, feature_dim_dict=feature_dim_dict,\n        pretrained_embed_dict=pretrained_embed_dict, require_grad_dict=require_grad_dict, use_char=use_char,\n        filter_sizes=filter_sizes, filter_nums=filter_nums, rnn_unit_type=rnn_unit_type, num_rnn_units=num_rnn_units,\n        num_layers=num_layers, bi_flag=bi_flag, dropout_rate=dropout_rate, average_batch=average_batch,\n        use_crf=use_crf, use_cuda=use_cuda)\n\n    if deterministic:  # for deterministic\n        torch.backends.cudnn.enabled = False\n\n    use_cuda = configs[\'model_params\'][\'use_cuda\']\n    if use_cuda:\n        sl_model = sl_model.cuda()\n\n    return sl_model\n\n\ndef init_train_data(configs):\n    """"""\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\n    Returns:\n        data_iter_train: DataIter\n        data_iter_dev: DataIter\n    """"""\n    all_in_memory = configs[\'all_in_memory\']\n    char_max_len = configs[\'model_params\'][\'char_max_len\']\n    batch_size = configs[\'model_params\'][\'batch_size\']\n    dev_size = configs[\'model_params\'][\'dev_size\']\n    max_len_limit = configs[\'max_len_limit\']\n\n    features_names = configs[\'data_params\'][\'feature_names\']\n    data_names = [name for name in features_names]\n    use_char = configs[\'model_params\'][\'use_char\']\n    if use_char:\n        data_names.append(\'char\')\n    data_names.append(\'label\')\n\n    # load train hdf5 file\n    path_data = configs[\'data_params\'][\'path_train\'] + \'.hdf5\'\n    train_object_dict_ = h5py.File(path_data, \'r\')\n    train_object_dict = train_object_dict_\n    if all_in_memory:\n        train_object_dict = dict()\n        for data_name in data_names:  # \xe5\x85\xa8\xe9\x83\xa8\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x88\xb0\xe5\x86\x85\xe5\xad\x98\n            train_object_dict[data_name] = train_object_dict_[data_name].value\n    train_count = train_object_dict[data_names[0]].size\n\n    # load dev hdf5 file\n    if \'path_dev\' not in configs[\'data_params\'] or not configs[\'data_params\'][\'path_dev\']:\n        # \xe6\x8b\x86\xe5\x88\x86\xe8\xae\xad\xe7\xbb\x83\xe9\x9b\x86\n        data_utils = DataUtil(\n            train_count, train_object_dict, data_names, use_char=use_char, char_max_len=char_max_len,\n            batch_size=batch_size, max_len_limit=max_len_limit)\n        data_iter_train, data_iter_dev = data_utils.split_dataset(proportions=(1-dev_size, dev_size), shuffle=False)\n    else:\n        path_data = configs[\'data_params\'][\'path_dev\'] + \'.hdf5\'\n        dev_object_dict_ = h5py.File(path_data, \'r\')\n        dev_object_dict = train_object_dict_\n        if all_in_memory:\n            dev_object_dict = dict()\n            for data_name in data_names:  # \xe5\x85\xa8\xe9\x83\xa8\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x88\xb0\xe5\x86\x85\xe5\xad\x98\n                dev_object_dict[data_name] = dev_object_dict_[data_name].value\n        dev_count = dev_object_dict[data_names[0]].size\n        data_iter_dev = DataIter(\n            dev_count, dev_object_dict, data_names, use_char=use_char, char_max_len=char_max_len,\n            batch_size=batch_size, max_len_limit=max_len_limit)\n        data_iter_train = DataIter(\n            train_count, train_object_dict, data_names, use_char=use_char, char_max_len=char_max_len,\n            batch_size=batch_size, max_len_limit=max_len_limit)\n\n    return data_iter_train, data_iter_dev\n\n\ndef init_test_data(configs):\n    """"""\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\n    Returns:\n        data_iter_train: DataIter\n        data_iter_dev: DataIter\n    """"""\n    all_in_memory = configs[\'all_in_memory\']\n    char_max_len = configs[\'model_params\'][\'char_max_len\']\n    batch_size = configs[\'model_params\'][\'batch_size\']\n    dev_size = configs[\'model_params\'][\'dev_size\']\n    max_len_limit = configs[\'max_len_limit\']\n\n    features_names = configs[\'data_params\'][\'feature_names\']\n    data_names = [name for name in features_names]\n    use_char = configs[\'model_params\'][\'use_char\']\n    if use_char:\n        data_names.append(\'char\')\n    data_names.append(\'label\')\n\n    # load train hdf5 file\n    path_data = configs[\'data_params\'][\'path_test\'] + \'.hdf5\'\n    test_object_dict_ = h5py.File(path_data, \'r\')\n    test_object_dict = test_object_dict_\n    if all_in_memory:\n        test_object_dict = dict()\n        for data_name in data_names:  # \xe5\x85\xa8\xe9\x83\xa8\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x88\xb0\xe5\x86\x85\xe5\xad\x98\n            test_object_dict[data_name] = test_object_dict_[data_name].value\n    test_count = test_object_dict[data_names[0]].size\n\n    data_iter = DataIter(\n        test_count, test_object_dict, data_names, use_char=use_char, char_max_len=char_max_len,\n        batch_size=batch_size, max_len_limit=max_len_limit)\n\n    return data_iter\n\n\ndef init_optimizer(configs, model):\n    """"""\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96optimizer\n    Returns:\n        optimizer\n    """"""\n    optimizer_type = configs[\'model_params\'][\'optimizer\']\n    learning_rate = configs[\'model_params\'][\'learning_rate\']\n    l2_rate = configs[\'model_params\'][\'l2_rate\']\n    momentum = configs[\'model_params\'][\'momentum\']\n    lr_decay = 0\n    # \xe8\xbf\x87\xe6\xbb\xa4\xe4\xb8\x8d\xe9\x9c\x80\xe8\xa6\x81\xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\xe7\x9a\x84\n    parameters = filter(lambda p: p.requires_grad, model.parameters())\n\n    if optimizer_type.lower() == ""sgd"":\n        lr_decay = configs[\'model_params\'][\'lr_decay\']\n        optimizer = optim.SGD(parameters, lr=learning_rate, momentum=momentum, weight_decay=l2_rate)\n    elif optimizer_type.lower() == ""adagrad"":\n        optimizer = optim.Adagrad(parameters, lr=learning_rate, weight_decay=l2_rate)\n    elif optimizer_type.lower() == ""adadelta"":\n        optimizer = optim.Adadelta(parameters, lr=learning_rate, weight_decay=l2_rate)\n    elif optimizer_type.lower() == ""rmsprop"":\n        optimizer = optim.RMSprop(parameters, lr=learning_rate, weight_decay=l2_rate)\n    elif optimizer_type.lower() == ""adam"":\n        optimizer = optim.Adam(parameters, lr=learning_rate, weight_decay=l2_rate)\n    else:\n        print(\'\xe8\xaf\xb7\xe9\x80\x89\xe6\x8b\xa9\xe6\xad\xa3\xe7\xa1\xae\xe7\x9a\x84optimizer: {0}\'.format(optimizer_type))\n        exit()\n    return optimizer, lr_decay\n\n\ndef init_trainer(configs, data_iter_train, data_iter_dev, model, optimizer, lr_decay):\n    """"""\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96model trainer\n    Returns:\n        trainer: SLTrainer\n    """"""\n    feature_names = configs[\'data_params\'][\'feature_names\']\n    use_char = configs[\'model_params\'][\'use_char\']\n    max_len_char = configs[\'model_params\'][\'char_max_len\']\n    path_save_model = configs[\'data_params\'][\'path_model\']\n    check_parent_dir(path_save_model)\n\n    nb_epoch = configs[\'model_params\'][\'nb_epoch\']\n    max_patience = configs[\'model_params\'][\'max_patience\']\n\n    learning_rate = configs[\'model_params\'][\'learning_rate\']\n\n    trainer = SLTrainer(\n        data_iter_train=data_iter_train, data_iter_dev=data_iter_dev, feature_names=feature_names,\n        use_char=use_char, max_len_char=max_len_char, model=model, optimizer=optimizer,\n        path_save_model=path_save_model, nb_epoch=nb_epoch, max_patience=max_patience,\n        learning_rate=learning_rate, lr_decay=lr_decay)\n\n    return trainer\n\n\ndef load_model(configs):\n    """"""\xe5\x8a\xa0\xe8\xbd\xbd\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84model\n    """"""\n    model = init_model(configs)\n\n    path_model = configs[\'data_params\'][\'path_model\']\n    model_state = torch.load(path_model)\n    model.load_state_dict(model_state)\n    return model\n\n\ndef train_model(configs):\n    """"""\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b\n    """"""\n    # init model\n    sl_model = init_model(configs)\n    print(sl_model)\n\n    # init data\n    data_iter_train, data_iter_dev = init_train_data(configs)\n\n    # init optimizer\n    optimizer, lr_decay = init_optimizer(configs, model=sl_model)\n\n    # init trainer\n    model_trainer = init_trainer(\n        configs, data_iter_train, data_iter_dev, sl_model, optimizer, lr_decay)\n\n    model_trainer.fit()\n\n\ndef test_model(configs):\n    """"""\xe6\xb5\x8b\xe8\xaf\x95\xe6\xa8\xa1\xe5\x9e\x8b\n    """"""\n    # init model\n    model = load_model(configs)\n\n    # init test data\n    data_iter_test = init_test_data(configs)\n\n    # init infer\n    path_conllu_test = configs[\'data_params\'][\'path_test\']\n    if \'path_test_result\' not in configs[\'data_params\'] or \\\n       not configs[\'data_params\'][\'path_test_result\']:\n        path_result = configs[\'data_params\'][\'path_test\'] + \'.result\'\n    else:\n        path_result = configs[\'data_params\'][\'path_test_result\']\n    # label to id dict\n    path_pkl = os.path.join(configs[\'data_params\'][\'alphabet_params\'][\'path\'], \'label.pkl\')\n    label2id_dict = read_bin(path_pkl)\n    infer = Inference(\n        model=model, data_iter=data_iter_test, path_conllu=path_conllu_test,\n        path_result=path_result, label2id_dict=label2id_dict)\n\n    # do infer\n    infer.infer2file()\n\n\ndef main():\n    opts = parse_opts()\n    configs = yaml.load(codecs.open(opts.config, encoding=\'utf-8\'))\n\n    if opts.train:  # train\n        # \xe5\x88\xa4\xe6\x96\xad\xe6\x98\xaf\xe5\x90\xa6\xe9\x9c\x80\xe8\xa6\x81\xe9\xa2\x84\xe5\xa4\x84\xe7\x90\x86\n        if opts.preprocess:\n            preprocessing(configs)\n        # \xe8\xae\xad\xe7\xbb\x83\n        train_model(configs)\n    else:  # test\n        test_model(configs)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
sltk/__init__.py,0,b''
test/test_dataset.py,0,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n""""""\xe6\xb5\x8b\xe8\xaf\x95\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\n""""""\nimport sys\nimport h5py\n\n\nroot_project = \'/home/ljx/Workspace/PythonProjects/SLTK_v1.0\'\nsys.path.append(root_project)\n\nfrom sltk.data import DataIter\n\n\ndata_names = [\'word\', \'pos\', \'chunk\', \'char\', \'label\']\n\n# load hdf5 file\ndata_object_dict_ = h5py.File(\'../data/train.txt.hdf5\', \'r\')\ndata_object_dict = dict()\nfor data_name in data_names:  # \xe5\x85\xa8\xe9\x83\xa8\xe5\x8a\xa0\xe8\xbd\xbd\xe5\x88\xb0\xe5\x86\x85\xe5\xad\x98\n    data_object_dict[data_name] = data_object_dict_[data_name].value\n\ndata_count = data_object_dict[data_names[0]].size\n\ndata_iter = DataIter(\n    data_count, data_object_dict, data_names, use_char=True, char_max_len=15,\n    batch_size=32, max_len_limit=100, seed=1337)\nfor data in data_iter:\n    print(data[\'word\'].shape)\n'"
test/test_yml.py,0,"b""#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\nimport os\nimport codecs\nimport yaml\n\n\nfile_yml = codecs.open('../configs/demo.train.yml')\nconfigs = yaml.load(file_yml)\n\npath = configs['data_params']['path_train']\nprint(os.path.abspath(path))\n\nprint(configs['data_params']['feature_names'])\nprint(configs['data_params']['path_pretrain'])\n\nprint(configs['model_params']['l2_rate'])\nprint(type(configs['model_params']['l2_rate']))\nprint(type(configs['model_params']['dropout_rate']))\n"""
tools/bio2bieo.py,0,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n""""""\n    \xe5\xb0\x86BIO\xe6\xa0\x87\xe6\xb3\xa8\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xbaBIESO\n        \n    Usage:\n    \n        python3 bio2bieo.py -i input.txt -o output.txt\n""""""\nimport re\nimport sys\nimport codecs\nfrom optparse import OptionParser\n\n\ndef read_conllu(path, zip_format=True):\n    """"""\xe8\xaf\xbb\xe5\x8f\x96conllu\xe6\xa0\xbc\xe5\xbc\x8f\xe6\x96\x87\xe4\xbb\xb6\n    Args:\n         path: str\n\n    yield:\n        list(list)\n    """"""\n    pattern_space = re.compile(\'\\s+\')\n    feature_items = []\n    file_data = codecs.open(path, \'r\', encoding=\'utf-8\')\n    line = file_data.readline()\n    line_idx = 1\n    while line:\n        line = line.strip()\n        if not line:\n            # \xe5\x88\xa4\xe6\x96\xad\xe6\x98\xaf\xe5\x90\xa6\xe5\xad\x98\xe5\x9c\xa8\xe5\xa4\x9a\xe4\xb8\xaa\xe7\xa9\xba\xe8\xa1\x8c\n            if not feature_items:\n                print(\'\xe5\xad\x98\xe5\x9c\xa8\xe5\xa4\x9a\xe4\xb8\xaa\xe7\xa9\xba\xe8\xa1\x8c\xef\xbc\x81`{0}` line: {1}\'.format(path, line_idx))\n                exit()\n\n            # \xe5\xa4\x84\xe7\x90\x86\xe4\xb8\x8a\xe4\xb8\x80\xe4\xb8\xaa\xe5\xae\x9e\xe4\xbe\x8b\n            if zip_format:\n                yield list(zip(*feature_items))\n            else:\n                yield feature_items\n\n            line = file_data.readline()\n            line_idx += 1\n            feature_items = []\n        else:\n            # \xe8\xae\xb0\xe5\xbd\x95\xe7\x89\xb9\xe5\xbe\x81\n            items = pattern_space.split(line)\n            feature_items.append(items)\n\n            line = file_data.readline()\n            line_idx += 1\n    # the last one\n    if feature_items:\n        if zip_format:\n            yield list(zip(*feature_items))\n        else:\n            yield feature_items\n    file_data.close()\n\n\ndef iob_iobes(tags):\n    """"""IOB -> IOBES\n    Args:\n        tags: list(str)\n\n    Returns:\n        new_tags: list(str)\n    """"""\n    new_tags = []\n    for i, tag in enumerate(tags):\n        if tag == \'O\':\n            new_tags.append(tag)\n        elif tag.split(\'-\')[0] == \'B\':\n            if i + 1 != len(tags) and \\\n               tags[i + 1].split(\'-\')[0] == \'I\':\n                new_tags.append(tag)\n            else:\n                new_tags.append(tag.replace(\'B-\', \'S-\'))\n        elif tag.split(\'-\')[0] == \'I\':\n            if i + 1 < len(tags) and \\\n                    tags[i + 1].split(\'-\')[0] == \'I\':\n                new_tags.append(tag)\n            else:\n                new_tags.append(tag.replace(\'I-\', \'E-\'))\n        else:\n            raise Exception(\'Invalid IOB format!\')\n    return new_tags\n\n\ndef main():\n    op = OptionParser()\n    op.add_option(\'-i\', \'--input\', dest=\'input\', type=\'str\', help=\'bio\xe6\x96\x87\xe4\xbb\xb6\xe8\xb7\xaf\xe5\xbe\x84\')\n    op.add_option(\'-o\', \'--output\', dest=\'output\', type=\'str\', help=\'bieo\xe6\x96\x87\xe4\xbb\xb6\xe8\xb7\xaf\xe5\xbe\x84\')\n    argv = [] if not hasattr(sys.modules[\'__main__\'], \'__file__\') else sys.argv[1:]\n    (opts, args) = op.parse_args(argv)\n    if not opts.input or not opts.output:\n        op.print_help()\n        exit()\n    path_input = opts.input\n    path_output = opts.output\n    file_output = codecs.open(path_output, \'w\', encoding=\'utf-8\')\n    for i, items in enumerate(read_conllu(path_input)):\n        items[-1] = iob_iobes(items[-1])\n        items = list(zip(*items))\n        for item in items:\n            file_output.write(\'{0}\\n\'.format(\' \'.join(item)))\n        file_output.write(\'\\n\')\n\n        sys.stdout.write(\'sentence: {0}\\r\'.format(i))\n        sys.stdout.flush()\n    sys.stdout.write(\'sentence: {0}\\r\'.format(i+1))\n    sys.stdout.flush()\n\n    file_output.close()\n    print(\'done!\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
sltk/data/__init__.py,0,"b""from .dataset import DataIter, DataUtil\n\n__all__ = [\n    'DataIter',\n    'DataUtil',\n]\n"""
sltk/data/dataset.py,0,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\nimport math\nimport random\nimport numpy as np\n\n\nclass DataUtil(object):\n\n    def __init__(self, data_count, data_object_dict, data_names, use_char=False, char_max_len=None,\n                 data_type_dict=None, batch_size=32, max_len_limit=None, seed=1337):\n        """"""\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x88\x92\xe5\x88\x86\xe5\xb7\xa5\xe5\x85\xb7\xe7\xb1\xbb.\n        Args:\n            data_count: int, \xe6\x95\xb0\xe6\x8d\xae\xe6\x80\xbb\xe6\x95\xb0;\n            data_object_dict: dict({str, data_object}), \xe6\x95\xb0\xe6\x8d\xae\xe5\x90\x8d: \xe6\x95\xb0\xe6\x8d\xae, \xe6\x95\xb0\xe6\x8d\xae\xe6\xa0\xbc\xe5\xbc\x8f\xe4\xb8\xbanp.array\xe6\x88\x96\xe5\x8f\xaf\xe6\xa0\xb9\xe6\x8d\xae\xe4\xb8\x8b\xe6\xa0\x87\n                \xe7\xb4\xa2\xe5\xbc\x95\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe7\xb1\xbb\xe5\x9e\x8b(\xe5\xa6\x82hdf5\xe6\xa0\xbc\xe5\xbc\x8f), \xe6\x95\xb0\xe6\x8d\xae\xe7\xb1\xbb\xe5\x9e\x8b\xe4\xb8\xbaint\xe6\x88\x96np.int32, \xe5\x8d\x95\xe4\xb8\xaa\xe5\xae\x9e\xe4\xbe\x8b\xe7\x9a\x84\xe4\xb8\x8d\xe5\x90\x8c\xe7\x89\xb9\xe5\xbe\x81\xe9\x95\xbf\xe5\xba\xa6\xe5\xba\x94\xe7\x9b\xb8\xe5\x90\x8c;\n            data_names: list(str), \xe6\x95\xb0\xe6\x8d\xae\xe5\x90\x8d\xe7\xa7\xb0, e.g., [\'f1\', \'f2\', \'label\'];\n            use_char: bool, \xe6\x98\xaf\xe5\x90\xa6\xe4\xbd\xbf\xe7\x94\xa8char feature\n            char_max_len: int, \xe5\x8d\x95\xe8\xaf\x8d\xe6\x9c\x80\xe5\xa4\xa7\xe9\x95\xbf\xe5\xba\xa6\n            data_type_dict: dict({str: type}), e.g., {\'f1\': np.int32, \'label\': np.int32},\n                default is np.int32\n            batch_size: int, batch size, default is 32;\n            max_len_limit: int, batch\xe7\x9a\x84\xe6\x9c\x80\xe5\xa4\xa7\xe9\x95\xbf\xe5\xba\xa6\xe9\x99\x90\xe5\x88\xb6(\xe8\xaf\xa5\xe5\x80\xbc\xe9\x9d\x9emax length),\n                \xe8\x8b\xa5\xe6\x98\xafNone(default), \xe5\x88\x99\xe6\x8c\x89\xe7\x85\xa7batch\xe4\xb8\xad\xe6\x9c\x80\xe9\x95\xbf\xe7\x9a\x84\xe5\xae\x9e\xe4\xbe\x8b\xe4\xb8\xba\xe5\x87\x86;\n            seed: int, random seed, default is 1337.\n\n        Notes:\n            data_object_dict\xe4\xb8\xad\xe5\x8d\x95\xe4\xb8\xaa\xe5\xae\x9e\xe4\xbe\x8b\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe7\x89\xb9\xe5\xbe\x81\xe9\x95\xbf\xe5\xba\xa6\xe5\xbf\x85\xe9\xa1\xbb\xe7\x9b\xb8\xe7\xad\x89.\n        """"""\n        self._data_count = data_count\n        self._data_ids = list(range(self._data_count))\n        self._data_object_dict = data_object_dict\n        self._data_names = data_names\n        self._use_char = use_char\n        self._char_max_len = char_max_len\n        self._batch_size = batch_size\n        self._max_len_limit = max_len_limit\n        self._seed = seed\n\n        # init data type\n        self._data_type_dict = data_type_dict\n        if not self._data_type_dict:\n            self._data_type_dict = dict()\n            for data_name in self._data_names:\n                self._data_type_dict[data_name] = np.int32\n        for data_name in self._data_names:\n            if data_name not in self._data_type_dict:\n                self._data_type_dict[data_name] = np.int32\n\n    def split_dataset(self, proportions=(4, 1), shuffle=False):\n        """"""\xe5\x88\x86\xe5\x8c\x96\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86.\n        Args:\n            proportions: tuple(int), \xe5\x88\x92\xe5\x88\x86\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe7\x9a\x84\xe6\xaf\x94\xe4\xbe\x8b, \xe4\xbe\x8b\xe5\xa6\x82(4, 1)\xe8\xa1\xa8\xe7\xa4\xba\xe5\xb0\x86\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe5\x88\x92\xe5\x88\x86\xe4\xb8\xba80%\xe5\x92\x8c20%;\n                (7, 2, 1)\xe8\xa1\xa8\xe7\xa4\xba\xe5\x88\x92\xe5\x88\x86\xe4\xb8\xba70%, 20%\xe5\x92\x8c10%;\n            shuffle: bool, \xe6\x98\xaf\xe5\x90\xa6\xe6\x89\x93\xe4\xb9\xb1\xe6\x95\xb0\xe6\x8d\xae\n\n        Returns:\n            data_iter_list: DataIter object list.\n        """"""\n        if shuffle:\n            random.seed(self._seed)\n            random.shuffle(self._data_ids)\n        proportions_ = np.array(proportions) / float(sum(proportions))\n        data_sizes = (proportions_ * self._data_count).astype(np.int32)\n        data_iter_list = []\n        current_count = 0\n        for i in range(len(proportions)):\n            start, end = current_count, current_count + data_sizes[i]\n            # \xe6\x9e\x84\xe5\xbb\xbadata iter\n            data_iter = DataIter(\n                data_sizes[i], self._data_object_dict, self._data_names, self._use_char, self._char_max_len,\n                self._data_type_dict, self._batch_size, self._max_len_limit, self._seed)\n            data_iter.data_ids = self._data_ids[start: end]  # reset data_ids\n            data_iter_list.append(data_iter)\n\n        return data_iter_list\n\n\nclass DataIter(object):\n\n    def __init__(self, data_count, data_object_dict, data_names, use_char=False, char_max_len=None,\n                 data_type_dict=None, batch_size=32, max_len_limit=None, seed=1337):\n        """"""\xe6\x95\xb0\xe6\x8d\xae\xe8\xbf\xad\xe4\xbb\xa3\xe5\x99\xa8.\n        Args:\n            data_count: int, \xe6\x95\xb0\xe6\x8d\xae\xe6\x80\xbb\xe6\x95\xb0;\n            data_object_dict: dict({str, data_object}), \xe6\x95\xb0\xe6\x8d\xae\xe5\x90\x8d: \xe6\x95\xb0\xe6\x8d\xae, \xe6\x95\xb0\xe6\x8d\xae\xe6\xa0\xbc\xe5\xbc\x8f\xe4\xb8\xbanp.array\xe6\x88\x96\xe5\x8f\xaf\xe6\xa0\xb9\xe6\x8d\xae\xe4\xb8\x8b\xe6\xa0\x87\n                \xe7\xb4\xa2\xe5\xbc\x95\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe7\xb1\xbb\xe5\x9e\x8b(\xe5\xa6\x82hdf5\xe6\xa0\xbc\xe5\xbc\x8f), \xe6\x95\xb0\xe6\x8d\xae\xe7\xb1\xbb\xe5\x9e\x8b\xe4\xb8\xbaint\xe6\x88\x96np.int32, \xe5\x8d\x95\xe4\xb8\xaa\xe5\xae\x9e\xe4\xbe\x8b\xe7\x9a\x84\xe4\xb8\x8d\xe5\x90\x8c\xe7\x89\xb9\xe5\xbe\x81\xe9\x95\xbf\xe5\xba\xa6\xe5\xba\x94\xe7\x9b\xb8\xe5\x90\x8c;\n            data_names: list(str), \xe6\x95\xb0\xe6\x8d\xae\xe5\x90\x8d\xe7\xa7\xb0, e.g., [\'f1\', \'f2\', \'label\'];\n            use_char: bool, \xe6\x98\xaf\xe5\x90\xa6\xe4\xbd\xbf\xe7\x94\xa8char feature\n            char_max_len: int, \xe5\x8d\x95\xe8\xaf\x8d\xe6\x9c\x80\xe5\xa4\xa7\xe9\x95\xbf\xe5\xba\xa6\n            data_type_dict: dict({str: type}), e.g., {\'f1\': np.int32, \'label\': np.int32},\n                default is np.int32\n            batch_size: int, batch size, default is 32;\n            max_len_limit: int, batch\xe7\x9a\x84\xe6\x9c\x80\xe5\xa4\xa7\xe9\x95\xbf\xe5\xba\xa6\xe9\x99\x90\xe5\x88\xb6(\xe8\xaf\xa5\xe5\x80\xbc\xe9\x9d\x9emax length),\n                \xe8\x8b\xa5\xe6\x98\xafNone(default), \xe5\x88\x99\xe6\x8c\x89\xe7\x85\xa7batch\xe4\xb8\xad\xe6\x9c\x80\xe9\x95\xbf\xe7\x9a\x84\xe5\xae\x9e\xe4\xbe\x8b\xe4\xb8\xba\xe5\x87\x86;\n            seed: int, random seed, default is 1337.\n\n        Notes:\n            data_object_dict\xe4\xb8\xad\xe5\x8d\x95\xe4\xb8\xaa\xe5\xae\x9e\xe4\xbe\x8b\xe7\x9a\x84\xe6\x89\x80\xe6\x9c\x89\xe7\x89\xb9\xe5\xbe\x81\xe9\x95\xbf\xe5\xba\xa6\xe5\xbf\x85\xe9\xa1\xbb\xe7\x9b\xb8\xe7\xad\x89.\n        """"""\n        self._data_count = data_count\n        self._data_ids = list(range(self._data_count))\n        self._data_object_dict = data_object_dict\n        self._data_names = data_names\n        self._use_char = use_char\n        self._char_max_len = char_max_len\n        self._batch_size = batch_size\n        self._max_len_limit = max_len_limit\n        self._seed = seed\n\n        self._iter_count = math.ceil(self._data_count/float(self._batch_size))\n\n        # init data type\n        self._data_type_dict = data_type_dict\n        if not self._data_type_dict:\n            self._data_type_dict = dict()\n            for data_name in self._data_names:\n                self._data_type_dict[data_name] = np.int32\n        for data_name in self._data_names:\n            if data_name not in self._data_type_dict:\n                self._data_type_dict[data_name] = np.int32\n\n        # iter variable\n        self._iter_variable = 0\n\n    def shuffle(self):\n        """"""shuffle data.""""""\n        random.seed(self._seed)\n        random.shuffle(self._data_ids)\n\n    def _generate_batch(self, start, end):\n        """"""\xe4\xba\xa7\xe7\x94\x9f\xe6\x89\xb9\xe9\x87\x8f\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae.\n        Args:\n            start: int, \xe6\x95\xb0\xe6\x8d\xae\xe8\xb5\xb7\xe5\xa7\x8b\xe4\xbd\x8d\xe7\xbd\xae\n            end: int, \xe6\x95\xb0\xe6\x8d\xae\xe7\xbb\x93\xe6\x9d\x9f\xe4\xbd\x8d\xe7\xbd\xae\n\n        Returns:\n            pass\n        """"""\n        batch_size = end - start\n\n        # \xe8\xae\xa1\xe7\xae\x97\xe6\x89\xb9\xe9\x87\x8f\xe7\x9a\x84\xe6\x9c\x80\xe5\xa4\xa7\xe9\x95\xbf\xe5\xba\xa6\n        batch_max_len = max([len(item) for item in self._data_object_dict[self._data_names[0]][self._data_ids[start:end]]])\n        if self._max_len_limit:\n            batch_max_len = batch_max_len if batch_max_len <= self._max_len_limit else self._max_len_limit\n            if self._use_char:\n                batch_char_max_len = batch_max_len * self._char_max_len\n\n        # \xe7\x94\x9f\xe6\x88\x90\xe6\x95\xb0\xe6\x8d\xae\n        batch_dict = dict()\n        for data_name in self._data_names:\n            dtype = self._data_type_dict[data_name]\n            batch_dict[data_name] = np.zeros((batch_size, batch_max_len), dtype=dtype)\n\n            data_object = self._data_object_dict[data_name]\n            for i, item in enumerate(data_object[self._data_ids[start:end]]):\n                len_item = len(item)\n                len_item = len_item if len_item <= batch_max_len else batch_max_len\n                batch_dict[data_name][i][:len_item] = item[:len_item]\n        # char feature\n        if self._use_char:\n            batch_dict[\'char\'] = np.zeros((batch_size, batch_max_len*self._char_max_len), dtype=np.int32)\n            data_object = self._data_object_dict[\'char\']\n            for i, item in enumerate(data_object[self._data_ids[start:end]]):\n                # print(item.shape)\n                len_item = len(item)\n                len_item = len_item if len_item <= batch_char_max_len else batch_char_max_len\n                batch_dict[\'char\'][i][:len_item] = item[:len_item]\n\n        return batch_dict\n\n    @property\n    def char_max_len(self):\n        return self._char_max_len\n\n    @property\n    def iter_count(self):\n        return self._iter_count\n\n    @property\n    def data_count(self):\n        return self._data_count\n\n    @data_count.setter\n    def data_count(self, value):\n        self._data_count = value\n\n    @property\n    def batch_size(self):\n        return self._batch_size\n\n    @batch_size.setter\n    def batch_size(self, value):\n        self._batch_size = value\n\n    @property\n    def data_ids(self):\n        return self._data_ids\n\n    @data_ids.setter\n    def data_ids(self, value):\n        self._data_ids = value\n\n    @property\n    def iter_variable(self):\n        return self._iter_variable\n\n    def __len__(self):\n        return self._data_count\n\n    def __iter__(self):\n        self._iter_variable = 0\n        return self\n\n    def __next__(self):\n        start = self._iter_variable\n        end = self._iter_variable + self._batch_size\n        if end > self._data_count:\n            end = self._data_count\n        if self._iter_variable > self._data_count or start >= end:\n            self.shuffle()\n            raise StopIteration()\n        self._iter_variable = end\n        return self._generate_batch(start, end)\n'"
sltk/infer/__init__.py,0,"b""from .inference import Inference\n\n__all__ = [\n    'Inference',\n]\n"""
sltk/infer/inference.py,4,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\nimport sys\nimport codecs\nimport torch\n\nfrom ..utils import read_conllu\n\n\nclass Inference(object):\n\n    def __init__(self, **kwargs):\n        """"""\n        Args:\n            model: SLModel\n            data_iter: DataIter\n            path_conllu: str, conllu\xe6\xa0\xbc\xe5\xbc\x8f\xe7\x9a\x84\xe6\x96\x87\xe4\xbb\xb6\xe8\xb7\xaf\xe5\xbe\x84\n            path_result: str, \xe9\xa2\x84\xe6\xb5\x8b\xe7\xbb\x93\xe6\x9e\x9c\xe5\xad\x98\xe6\x94\xbe\xe8\xb7\xaf\xe5\xbe\x84\n            label2id_dict: dict({str: int})\n        """"""\n        for k in kwargs:\n            self.__setattr__(k, kwargs[k])\n\n        # id2label dict\n        self.id2label_dict = dict()\n        for label in self.label2id_dict:\n            self.id2label_dict[self.label2id_dict[label]] = label\n\n    def _get_inputs(self, feed_dict, use_cuda=True):\n        feed_tensor_dict = dict()\n        for feature_name in self.model.feature_names:\n            tensor = self.tensor_from_numpy(  # [bs, max_len]\n                feed_dict[feature_name], use_cuda=use_cuda)\n            feed_tensor_dict[feature_name] = tensor\n        if self.model.use_char:  # max_len_char\n            char_tensor = self.tensor_from_numpy(\n                feed_dict[\'char\'], use_cuda=self.model.use_cuda)\n            max_len = feed_dict[self.model.feature_names[0]].shape[-1]\n            char_tensor = char_tensor.view(-1, max_len, self.data_iter.char_max_len)\n            feed_tensor_dict[\'char\'] = char_tensor\n        return feed_tensor_dict\n\n    def infer(self):\n        """"""\xe9\xa2\x84\xe6\xb5\x8b\n        Returns:\n            labels: list of int\n        """"""\n        self.model.eval()\n        labels_pred = []\n        for feed_dict in self.data_iter:\n            feed_tensor_dict = self._get_inputs(feed_dict, self.model.use_cuda)\n\n            logits = self.model(**feed_tensor_dict)\n            # mask\n            mask = feed_tensor_dict[str(self.feature_names[0])] > 0\n            actual_lens = torch.sum(feed_tensor_dict[self.feature_names[0]] > 0, dim=1).int()\n            labels_batch = self.model.predict(logits, actual_lens, mask)\n            labels_pred.extend(labels_batch)\n            sys.stdout.write(\'sentence: {0} / {1}\\r\'.format(self.data_iter.iter_variable, self.data_iter.data_count))\n        sys.stdout.write(\'sentence: {0} / {1}\\n\'.format(self.data_iter.data_count, self.data_iter.data_count))\n\n        return labels_pred\n\n    def infer2file(self):\n        """"""\xe9\xa2\x84\xe6\xb5\x8b\xef\xbc\x8c\xe5\xb0\x86\xe7\xbb\x93\xe6\x9e\x9c\xe5\x86\x99\xe5\x85\xa5\xe6\x96\x87\xe4\xbb\xb6\n        """"""\n        self.model.eval()\n        file_result = codecs.open(self.path_result, \'w\', encoding=\'utf-8\')\n        conllu_reader = read_conllu(self.path_conllu, zip_format=False)\n        for feed_dict in self.data_iter:\n            feed_tensor_dict = self._get_inputs(feed_dict, self.model.use_cuda)\n\n            logits = self.model(**feed_tensor_dict)\n            # mask\n            mask = feed_tensor_dict[str(self.model.feature_names[0])] > 0\n            actual_lens = torch.sum(feed_tensor_dict[self.model.feature_names[0]] > 0, dim=1).int()\n            label_ids_batch = self.model.predict(logits, actual_lens, mask)\n            labels_batch = self.id2label(label_ids_batch)  # list(list(int))\n\n            # write to file\n            batch_size = len(labels_batch)\n            for i in range(batch_size):\n                feature_items = conllu_reader.__next__()\n                sent_len = len(feature_items)  # \xe5\x8f\xa5\xe5\xad\x90\xe5\xae\x9e\xe9\x99\x85\xe9\x95\xbf\xe5\xba\xa6\n                labels = labels_batch[i]\n                if len(labels) < sent_len:  # \xe8\xa1\xa5\xe5\x85\xa8\xe4\xb8\xba`O`\n                    labels = labels + [\'O\'] * (sent_len-len(labels))\n                for j in range(sent_len):\n                    file_result.write(\'{0} {1}\\n\'.format(\' \'.join(feature_items[j]), labels[j]))\n                file_result.write(\'\\n\')\n\n            sys.stdout.write(\'sentence: {0} / {1}\\r\'.format(self.data_iter.iter_variable, self.data_iter.data_count))\n            sys.stdout.flush()\n        sys.stdout.write(\'sentence: {0} / {1}\\n\'.format(self.data_iter.data_count, self.data_iter.data_count))\n        sys.stdout.flush()\n\n        file_result.close()\n\n    def id2label(self, label_ids_array):\n        """"""\xe5\xb0\x86label ids\xe8\xbd\xac\xe4\xb8\xbalabel\n        Args:\n            label_ids_array: list(np.array)\n\n        Returns:\n            labels: list(list(str))\n        """"""\n        labels = []\n        for label_array in label_ids_array:\n            temp = []\n            for idx in label_array:\n                temp.append(self.id2label_dict[idx])\n            labels.append(temp)\n        return labels\n\n    @staticmethod\n    def tensor_from_numpy(data, dtype=\'long\', use_cuda=True):\n        """"""\xe5\xb0\x86numpy\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xbatensor\n        Args:\n            data: numpy\n            dtype: long or float\n            use_cuda: bool\n        """"""\n        assert dtype in (\'long\', \'float\')\n        if dtype == \'long\':\n            data = torch.from_numpy(data).long()\n        else:\n            data = torch.from_numpy(data).float()\n        if use_cuda:\n            data = data.cuda()\n        return data\n\n'"
sltk/metrics/__init__.py,0,b''
sltk/nn/__init__.py,0,b''
sltk/preprocessing/__init__.py,0,b'from .normalize import normalize_word\n'
sltk/preprocessing/normalize.py,0,"b""#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\n\ndef normalize_word(word):\n    new_word = ''\n    for c in word:\n        if c.isdigit():\n            new_word += '0'\n        else:\n            new_word += c\n    return new_word\n"""
sltk/train/__init__.py,0,"b""from .sequence_labeling_trainer import SLTrainer\n\n__all__ = [\n    'SLTrainer',\n]\n"""
sltk/train/sequence_labeling_trainer.py,4,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n""""""\n    \xe6\xa8\xa1\xe5\x9e\x8b\xe8\xae\xad\xe7\xbb\x83\xe7\xb1\xbb.\n""""""\nimport sys\nimport numpy as np\n\nimport torch\n\n\nclass SLTrainer(object):\n\n    def __init__(self, **kwargs):\n        """"""\n        Args of data:\n            data_iter_train: \xe8\xae\xad\xe7\xbb\x83\xe6\x95\xb0\xe6\x8d\xae\xe8\xbf\xad\xe4\xbb\xa3\xe5\x99\xa8\n            data_iter_dev: \xe5\xbc\x80\xe5\x8f\x91\xe6\x95\xb0\xe6\x8d\xae\xe8\xbf\xad\xe4\xbb\xa3\xe5\x99\xa8\n            feature_names: list(str), \xe7\x89\xb9\xe5\xbe\x81\xe5\x90\x8d\xe7\xa7\xb0, \xe6\xb2\xa1\xe6\x9c\x89`label`\xe5\x92\x8c`char`\n            use_char: bool, \xe6\x98\xaf\xe5\x90\xa6\xe4\xbd\xbf\xe7\x94\xa8char feature\n            max_len_char: int, \xe5\x8d\x95\xe8\xaf\x8d\xe6\x9c\x80\xe5\xa4\xa7\xe9\x95\xbf\xe5\xba\xa6\n\n        Args of train:\n            model: \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe4\xb9\x8b\xe5\x90\x8e\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\n            optimizer: model arguments optimizer\n            lr_decay: float, \xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe8\xa1\xb0\xe5\x87\x8f\xe7\x8e\x87\n            learning_rate: float, \xe5\x88\x9d\xe5\xa7\x8b\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n            path_save_model: str, \xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbf\x9d\xe5\xad\x98\xe7\x9a\x84\xe8\xb7\xaf\xe5\xbe\x84\n\n            nb_epoch: int, \xe8\xbf\xad\xe4\xbb\xa3\xe6\xac\xa1\xe6\x95\xb0\xe4\xb8\x8a\xe9\x99\x90\n            max_patience: int, \xe5\xbc\x80\xe5\x8f\x91\xe9\x9b\x86\xe4\xb8\x8a\xe8\xbf\x9e\xe7\xbb\xadmp\xe6\xac\xa1\xe6\xb2\xa1\xe6\x9c\x89\xe6\x8f\x90\xe5\x8d\x87\xe5\x8d\xb3\xe5\x81\x9c\xe6\xad\xa2\xe8\xae\xad\xe7\xbb\x83\n        """"""\n        for k in kwargs:\n            self.__setattr__(k, kwargs[k])\n\n    def _get_inputs(self, feed_dict, use_cuda=True):\n        feed_tensor_dict = dict()\n        for feature_name in self.feature_names:\n            tensor = self.tensor_from_numpy(  # [bs, max_len]\n                feed_dict[feature_name], use_cuda=use_cuda)\n            feed_tensor_dict[feature_name] = tensor\n        if self.use_char:  # max_len_char\n            char_tensor = self.tensor_from_numpy(\n                feed_dict[\'char\'], use_cuda=self.model.use_cuda)\n            max_len = feed_dict[self.feature_names[0]].shape[-1]\n            char_tensor = char_tensor.view(-1, max_len, self.max_len_char)\n            feed_tensor_dict[\'char\'] = char_tensor\n        return feed_tensor_dict\n\n    def fit(self):\n        """"""\xe8\xae\xad\xe7\xbb\x83\xe6\xa8\xa1\xe5\x9e\x8b\n        """"""\n        best_dev_loss = 1.e8\n        current_patience = 0\n        for epoch in range(self.nb_epoch):\n            train_loss, dev_loss = 0., 0.\n            self.model.train()\n            if self.lr_decay != 0.:\n                self.optimizer = self.decay_learning_rate(epoch, self.learning_rate)\n            for i, feed_dict in enumerate(self.data_iter_train):\n                self.optimizer.zero_grad()\n                feed_tensor_dict = self._get_inputs(feed_dict, self.model.use_cuda)\n\n                labels = self.tensor_from_numpy(feed_dict[\'label\'], \'long\', self.model.use_cuda)\n\n                # mask\n                mask = feed_tensor_dict[str(self.feature_names[0])] > 0\n\n                logits = self.model(**feed_tensor_dict)\n                loss = self.model.loss(logits, mask, labels)\n                train_loss += loss.item()\n                loss.backward()\n                self.optimizer.step()\n\n                sys.stdout.write(\'Epoch {0} / {1}: {2} / {3}\\r\'.format(\n                    epoch+1, self.nb_epoch, self.data_iter_train.iter_variable, self.data_iter_train.data_count))\n            sys.stdout.write(\'Epoch {0} / {1}: {2} / {3}\\n\'.format(\n                epoch+1, self.nb_epoch, self.data_iter_train.data_count, self.data_iter_train.data_count))\n\n            # \xe8\xae\xa1\xe7\xae\x97\xe5\xbc\x80\xe5\x8f\x91\xe9\x9b\x86loss\n            self.model.eval()\n            # dev_labels_pred, dev_labels_gold = [], []\n            for feed_dict in self.data_iter_dev:\n                feed_tensor_dict = self._get_inputs(feed_dict, self.model.use_cuda)\n\n                labels = self.tensor_from_numpy(feed_dict[\'label\'], \'long\', self.model.use_cuda)\n\n                logits = self.model(**feed_tensor_dict)\n                # mask\n                mask = feed_tensor_dict[str(self.feature_names[0])] > 0\n                loss = self.model.loss(logits, mask, labels)\n                dev_loss += loss.item()\n\n            print(\'\\ttrain loss: {0}, dev loss: {1}\'.format(train_loss, dev_loss))\n\n            # \xe5\x88\xa4\xe6\x96\xad\xe6\x98\xaf\xe5\x90\xa6\xe9\x9c\x80\xe8\xa6\x81\xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\n            if dev_loss < best_dev_loss:\n                current_patience = 0\n                best_dev_loss = dev_loss\n                # \xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\n                self.save_model()\n                print(\'\\tmodel has saved to {0}!\'.format(self.path_save_model))\n            else:\n                current_patience += 1\n                print(\'\\tno improvement, current patience: {0} / {1}\'.format(\n                    current_patience, self.max_patience))\n                if self.max_patience <= current_patience:\n                    print(\'finished training! (early stopping, max patience: {0})\'.format(self.max_patience))\n                    return\n        print(\'finished training!\')\n\n    def predict(self, data_iter, has_label=False):\n        """"""\xe9\xa2\x84\xe6\xb5\x8b\n        Args:\n            data_iter: \xe6\x95\xb0\xe6\x8d\xae\xe8\xbf\xad\xe4\xbb\xa3\xe5\x99\xa8\n            has_label: bool, \xe6\x98\xaf\xe5\x90\xa6\xe5\xb8\xa6\xe6\x9c\x89label\n\n        Returns:\n            labels: list of int\n        """"""\n        labels_pred, labels_gold = [], []\n        for feed_dict in data_iter:\n            if has_label:\n                labels_gold_batch = np.array(feed_dict[\'label\']).astype(np.int32).tolist()\n                labels_gold.extend(labels_gold_batch)\n            feed_tensor_dict = self._get_inputs(feed_dict, self.model.use_cuda)\n\n            logits = self.model(**feed_tensor_dict)\n            # mask\n            mask = feed_tensor_dict[str(self.feature_names[0])] > 0\n            actual_lens = torch.sum(feed_tensor_dict[self.feature_names[0]] > 0, dim=1).int()\n            labels_batch = self.model.predict(logits, actual_lens, mask)\n            labels_pred.extend(labels_batch)\n        if has_label:\n            return labels_gold, labels_pred\n        return labels_pred\n\n    def decay_learning_rate(self, epoch, init_lr):\n        """"""\xe8\xa1\xb0\xe5\x87\x8f\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n\n        Args:\n            epoch: int, \xe8\xbf\xad\xe4\xbb\xa3\xe6\xac\xa1\xe6\x95\xb0\n            init_lr: \xe5\x88\x9d\xe5\xa7\x8b\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\n        """"""\n        lr = init_lr / (1+self.lr_decay*epoch)\n        print(\'learning rate: {0}\'.format(lr))\n        for param_group in self.optimizer.param_groups:\n            param_group[\'lr\'] = lr\n        return self.optimizer\n\n    @staticmethod\n    def tensor_from_numpy(data, dtype=\'long\', use_cuda=True):\n        """"""\xe5\xb0\x86numpy\xe8\xbd\xac\xe6\x8d\xa2\xe4\xb8\xbatensor\n        Args:\n            data: numpy\n            dtype: long or float\n            use_cuda: bool\n        """"""\n        assert dtype in (\'long\', \'float\')\n        if dtype == \'long\':\n            data = torch.from_numpy(data).long()\n        else:\n            data = torch.from_numpy(data).float()\n        if use_cuda:\n            data = data.cuda()\n        return data\n\n    def save_model(self):\n        """"""\xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\n        """"""\n        torch.save(self.model.state_dict(), self.path_save_model)\n\n    def reset_batch_size(self, batch_size):\n        self.batch_size = batch_size\n        self.data_iter_train.batch_size = batch_size\n        self.data_iter_dev.batch_size = batch_size\n\n    def set_max_patience(self, max_patience):\n        self.max_patience = max_patience\n\n    def set_learning_rate(self, learning_rate):\n        self.learning_rate = learning_rate\n        for g in self.optimizer.param_groups:\n            g[\'lr\'] = self.learning_rate\n'"
sltk/utils/__init__.py,0,b'from .conllu import *\nfrom .embedding import *\nfrom .general import *\nfrom .io import *\n'
sltk/utils/conllu.py,0,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n""""""\xe5\xa4\x84\xe7\x90\x86conllu\xe6\xa0\xbc\xe5\xbc\x8f\xe7\x9a\x84\xe6\x96\x87\xe4\xbb\xb6\n""""""\nimport re\nimport codecs\n\n\ndef read_conllu(path, zip_format=True):\n    """"""\xe8\xaf\xbb\xe5\x8f\x96conllu\xe6\xa0\xbc\xe5\xbc\x8f\xe6\x96\x87\xe4\xbb\xb6\n    Args:\n         path: str\n\n    yield:\n        list(list)\n    """"""\n    pattern_space = re.compile(\'\\s+\')\n    feature_items = []\n    file_data = codecs.open(path, \'r\', encoding=\'utf-8\')\n    line = file_data.readline()\n    line_idx = 1\n    while line:\n        line = line.strip()\n        if not line:\n            # \xe5\x88\xa4\xe6\x96\xad\xe6\x98\xaf\xe5\x90\xa6\xe5\xad\x98\xe5\x9c\xa8\xe5\xa4\x9a\xe4\xb8\xaa\xe7\xa9\xba\xe8\xa1\x8c\n            if not feature_items:\n                print(\'\xe5\xad\x98\xe5\x9c\xa8\xe5\xa4\x9a\xe4\xb8\xaa\xe7\xa9\xba\xe8\xa1\x8c\xef\xbc\x81`{0}` line: {1}\'.format(path, line_idx))\n                exit()\n\n            # \xe5\xa4\x84\xe7\x90\x86\xe4\xb8\x8a\xe4\xb8\x80\xe4\xb8\xaa\xe5\xae\x9e\xe4\xbe\x8b\n            if zip_format:\n                yield list(zip(*feature_items))\n            else:\n                yield feature_items\n\n            line = file_data.readline()\n            line_idx += 1\n            feature_items = []\n        else:\n            # \xe8\xae\xb0\xe5\xbd\x95\xe7\x89\xb9\xe5\xbe\x81\n            items = pattern_space.split(line)\n            feature_items.append(items)\n\n            line = file_data.readline()\n            line_idx += 1\n    # the last one\n    if feature_items:\n        if zip_format:\n            yield list(zip(*feature_items))\n        else:\n            yield feature_items\n    file_data.close()\n'"
sltk/utils/embedding.py,0,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n""""""\n\xe7\x94\xa8\xe4\xba\x8e\xe4\xbb\x8e\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xe6\x9e\x84\xe5\xbb\xbaembedding\xe8\xa1\xa8\n""""""\n\n\ndef load_embed_with_gensim(path_embed):\n    """"""\n    \xe8\xaf\xbb\xe5\x8f\x96\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84embedding\n    Args:\n        path_embed: str, bin or txt\n    Returns:\n        word_embed_dict: dict, \xe5\x81\xa5: word, \xe5\x80\xbc: np.array, vector\n        word_dim: int, \xe8\xaf\x8d\xe5\x90\x91\xe9\x87\x8f\xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\n    """"""\n    from gensim.models.keyedvectors import KeyedVectors\n    if path_embed.endswith(\'bin\'):\n        word_vectors = KeyedVectors.load_word2vec_format(path_embed, binary=True)\n    elif path_embed.endswith(\'txt\'):\n        word_vectors = KeyedVectors.load_word2vec_format(path_embed, binary=False)\n    else:\n        raise ValueError(\'`path_embed` must be `bin` or `txt` file!\')\n    return word_vectors, word_vectors.vector_size\n\n\ndef build_word_embed(word2id_dict, path_embed, seed=137):\n    """"""\n    \xe4\xbb\x8e\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\xe6\x9e\x84\xe5\xbb\xbaword embedding\xe8\xa1\xa8\n    Args:\n        word2id_dict: dict, \xe5\x81\xa5: word, \xe5\x80\xbc: word id\n        path_embed: str, \xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84embedding\xe6\x96\x87\xe4\xbb\xb6\xef\xbc\x8cbin or txt\n    Returns:\n        word_embed_table: np.array, shape=[word_count, embed_dim]\n        exact_match_count: int, \xe7\xb2\xbe\xe7\xa1\xae\xe5\x8c\xb9\xe9\x85\x8d\xe7\x9a\x84\xe8\xaf\x8d\xe6\x95\xb0\n        fuzzy_match_count: int, \xe7\xb2\xbe\xe7\xa1\xae\xe5\x8c\xb9\xe9\x85\x8d\xe7\x9a\x84\xe8\xaf\x8d\xe6\x95\xb0\n        unknown_count: int, \xe6\x9c\xaa\xe5\x8c\xb9\xe9\x85\x8d\xe7\x9a\x84\xe8\xaf\x8d\xe6\x95\xb0\n    """"""\n    import numpy as np\n    assert path_embed.endswith(\'bin\') or path_embed.endswith(\'txt\')\n    word2vec_model, word_dim = load_embed_with_gensim(path_embed)\n    word_count = len(word2id_dict) + 1  # 0 is for padding value\n    np.random.seed(seed)\n    scope = np.sqrt(3. / word_dim)\n    word_embed_table = np.random.uniform(\n        -scope, scope, size=(word_count, word_dim)).astype(\'float32\')\n    exact_match_count, fuzzy_match_count, unknown_count = 0, 0, 0\n    for word in word2id_dict:\n        if word in word2vec_model.vocab:\n            word_embed_table[word2id_dict[word]] = word2vec_model[word]\n            exact_match_count += 1\n        elif word.lower() in word2vec_model.vocab:\n            word_embed_table[word2id_dict[word]] = word2vec_model[word.lower()]\n            fuzzy_match_count += 1\n        else:\n            unknown_count += 1\n    total_count = exact_match_count + fuzzy_match_count + unknown_count\n    return word_embed_table, exact_match_count, fuzzy_match_count, unknown_count, total_count\n'"
sltk/utils/general.py,0,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\nimport numpy as np\n\n\ndef tokens2id_array(items, voc, oov_id=1):\n    """"""\n    \xe5\xb0\x86\xe8\xaf\x8d\xe5\xba\x8f\xe5\x88\x97\xe6\x98\xa0\xe5\xb0\x84\xe4\xb8\xbaid\xe5\xba\x8f\xe5\x88\x97\n    Args:\n        items: list, \xe8\xaf\x8d\xe5\xba\x8f\xe5\x88\x97\n        voc: item -> id\xe7\x9a\x84\xe6\x98\xa0\xe5\xb0\x84\xe8\xa1\xa8\n        oov_id: int, \xe6\x9c\xaa\xe7\x99\xbb\xe5\xbd\x95\xe8\xaf\x8d\xe7\x9a\x84\xe7\xbc\x96\xe5\x8f\xb7, default is 1\n    Returns:\n        arr: np.array, shape=[max_len,]\n    """"""\n    arr = np.zeros((len(items),), dtype=\'int32\')\n    for i in range(len(items)):\n        if items[i] in voc:\n            arr[i] = voc[items[i]]\n        else:\n            arr[i] = oov_id\n    return arr\n'"
sltk/utils/io.py,0,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\nimport os\nimport pickle\n\n\ndef check_parent_dir(path):\n    """"""\xe6\xa3\x80\xe6\x9f\xa5path\xe7\x9a\x84\xe7\x88\xb6\xe7\x9b\xae\xe5\xbd\x95\xe6\x98\xaf\xe5\x90\xa6\xe5\xad\x98\xe5\x9c\xa8\xef\xbc\x8c\xe8\x8b\xa5\xe4\xb8\x8d\xe5\xad\x98\xe5\x9c\xa8\xef\xbc\x8c\xe5\x88\x99\xe5\x88\x9b\xe5\xbb\xba\xe4\xb9\x8b\n    Args:                      \n        path: str, file path\n    """"""\n    parent_name = os.path.dirname(path)\n    if not os.path.exists(parent_name):\n        os.makedirs(parent_name)\n\n\ndef object2pkl_file(path_pkl, ob):\n    """"""\xe5\xb0\x86python\xe5\xaf\xb9\xe8\xb1\xa1\xe5\x86\x99\xe5\x85\xa5pkl\xe6\x96\x87\xe4\xbb\xb6\n    Args:\n        path_pkl: str, pkl\xe6\x96\x87\xe4\xbb\xb6\xe8\xb7\xaf\xe5\xbe\x84\n        ob: python\xe7\x9a\x84list, dict, ...\n    """"""\n    with open(path_pkl, \'wb\') as file_pkl:\n        pickle.dump(ob, file_pkl)\n\n\ndef read_bin(path):\n    """"""\xe8\xaf\xbb\xe5\x8f\x96\xe4\xba\x8c\xe8\xbf\x9b\xe5\x88\xb6\xe6\x96\x87\xe4\xbb\xb6\n    Args:\n        path: str, \xe4\xba\x8c\xe8\xbf\x9b\xe5\x88\xb6\xe6\x96\x87\xe4\xbb\xb6\xe8\xb7\xaf\xe5\xbe\x84\n    Returns:\n        pkl_ob: pkl\xe5\xaf\xb9\xe8\xb1\xa1\n    """"""\n    file = open(path, \'rb\')\n    return pickle.load(file)\n'"
sltk/nn/functional/__init__.py,0,"b""from .initialize import *\n\n__all__ = [\n    'init_cnn_weight',\n    'init_lstm_weight',\n    'init_linear',\n    'init_embedding',\n]"""
sltk/nn/functional/initialize.py,7,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\n\ndef init_cnn_weight(cnn_layer, seed=1337):\n    """"""\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96cnn\xe5\xb1\x82\xe6\x9d\x83\xe9\x87\x8d\n    Args:\n        cnn_layer: weight.size() == [nb_filter, in_channels, [kernel_size]]\n        seed: int\n    """"""\n    filter_nums = cnn_layer.weight.size(0)\n    kernel_size = cnn_layer.weight.size()[2:]\n    scope = np.sqrt(2. / (filter_nums * np.prod(kernel_size)))\n    torch.manual_seed(seed)\n    nn.init.normal_(cnn_layer.weight, -scope, scope)\n    cnn_layer.bias.data.zero_()\n\n\ndef init_lstm_weight(lstm, num_layer=1, seed=1337):\n    """"""\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96lstm\xe6\x9d\x83\xe9\x87\x8d\n    Args:\n        lstm: torch.nn.LSTM\n        num_layer: int, lstm\xe5\xb1\x82\xe6\x95\xb0\n        seed: int\n    """"""\n    for i in range(num_layer):\n        weight_h = getattr(lstm, \'weight_hh_l{0}\'.format(i))\n        scope = np.sqrt(6.0 / (weight_h.size(0)/4. + weight_h.size(1)))\n        torch.manual_seed(seed)\n        nn.init.uniform_(getattr(lstm, \'weight_hh_l{0}\'.format(i)), -scope, scope)\n\n        weight_i = getattr(lstm, \'weight_ih_l{0}\'.format(i))\n        scope = np.sqrt(6.0 / (weight_i.size(0)/4. + weight_i.size(1)))\n        torch.manual_seed(seed)\n        nn.init.uniform_(getattr(lstm, \'weight_ih_l{0}\'.format(i)), -scope, scope)\n\n    if lstm.bias:\n        for i in range(num_layer):\n            weight_h = getattr(lstm, \'bias_hh_l{0}\'.format(i))\n            weight_h.data.zero_()\n            weight_h.data[lstm.hidden_size: 2*lstm.hidden_size] = 1\n            weight_i = getattr(lstm, \'bias_ih_l{0}\'.format(i))\n            weight_i.data.zero_()\n            weight_i.data[lstm.hidden_size: 2*lstm.hidden_size] = 1\n\n\ndef init_linear(input_linear, seed=1337):\n    """"""\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe6\x9d\x83\xe9\x87\x8d\n    """"""\n    torch.manual_seed(seed)\n    scope = np.sqrt(6.0 / (input_linear.weight.size(0) + input_linear.weight.size(1)))\n    nn.init.uniform_(input_linear.weight, -scope, scope)\n    if input_linear.bias is not None:\n        input_linear.bias.data.zero_()\n\n\ndef init_embedding(input_embedding, seed=1337):\n    """"""\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96embedding\xe5\xb1\x82\xe6\x9d\x83\xe9\x87\x8d\n    """"""\n    torch.manual_seed(seed)\n    scope = np.sqrt(3.0 / input_embedding.size(1))\n    nn.init.uniform_(input_embedding, -scope, scope)\n'"
sltk/nn/modules/__init__.py,0,"b""from .sequence_labeling_model import SLModel\n\n__all__ = [\n    'SLModel',\n]"""
sltk/nn/modules/crf.py,22,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef log_sum_exp(vec, m_size):\n    """"""\n    Args:\n        vec: size=(batch_size, vanishing_dim, hidden_dim)\n        m_size: hidden_dim\n\n    Returns:\n        size=(batch_size, hidden_dim)\n    """"""\n    _, idx = torch.max(vec, 1)  # B * 1 * M\n    max_score = torch.gather(vec, 1, idx.view(-1, 1, m_size)).view(-1, 1, m_size)  # B * M\n    return max_score.view(-1, m_size) + torch.log(torch.sum(\n        torch.exp(vec - max_score.expand_as(vec)), 1)).view(-1, m_size)\n\n\nclass CRF(nn.Module):\n\n    def __init__(self, **kwargs):\n        """"""\n        Args:\n            target_size: int, target size\n            use_cuda: bool, \xe6\x98\xaf\xe5\x90\xa6\xe4\xbd\xbf\xe7\x94\xa8gpu, default is True\n            average_batch: bool, loss\xe6\x98\xaf\xe5\x90\xa6\xe4\xbd\x9c\xe5\xb9\xb3\xe5\x9d\x87, default is True\n        """"""\n        super(CRF, self).__init__()\n        for k in kwargs:\n            self.__setattr__(k, kwargs[k])\n        if not hasattr(self, \'average_batch\'):\n            self.__setattr__(\'average_batch\', True)\n        if not hasattr(self, \'use_cuda\'):\n            self.__setattr__(\'use_cuda\', True)\n\n        # init transitions\n        self.START_TAG_IDX, self.END_TAG_IDX = -2, -1\n        init_transitions = torch.zeros(self.target_size+2, self.target_size+2)\n        init_transitions[:, self.START_TAG_IDX] = -1000.\n        init_transitions[self.END_TAG_IDX, :] = -1000.\n        if self.use_cuda:\n            init_transitions = init_transitions.cuda()\n        self.transitions = nn.Parameter(init_transitions)\n\n    def _forward_alg(self, feats, mask):\n        """"""\n        Do the forward algorithm to compute the partition function (batched).\n\n        Args:\n            feats: size=(batch_size, seq_len, self.target_size+2)\n            mask: size=(batch_size, seq_len)\n\n        Returns:\n            xxx\n        """"""\n        batch_size = feats.size(0)\n        seq_len = feats.size(1)\n        tag_size = feats.size(-1)\n\n        mask = mask.transpose(1, 0).contiguous()\n        ins_num = batch_size * seq_len\n\n        feats = feats.transpose(1, 0).contiguous().view(\n            ins_num, 1, tag_size).expand(ins_num, tag_size, tag_size)\n\n        scores = feats + self.transitions.view(\n            1, tag_size, tag_size).expand(ins_num, tag_size, tag_size)\n        scores = scores.view(seq_len, batch_size, tag_size, tag_size)\n\n        seq_iter = enumerate(scores)\n        try:\n            _, inivalues = seq_iter.__next__()\n        except:\n            _, inivalues = seq_iter.next()\n        partition = inivalues[:, self.START_TAG_IDX, :].clone().view(batch_size, tag_size, 1)\n\n        for idx, cur_values in seq_iter:\n            cur_values = cur_values + partition.contiguous().view(\n                batch_size, tag_size, 1).expand(batch_size, tag_size, tag_size)\n            cur_partition = log_sum_exp(cur_values, tag_size)\n\n            mask_idx = mask[idx, :].view(batch_size, 1).expand(batch_size, tag_size)\n\n            masked_cur_partition = cur_partition.masked_select(mask_idx)\n            if masked_cur_partition.dim() != 0:\n                mask_idx = mask_idx.contiguous().view(batch_size, tag_size, 1)\n                partition.masked_scatter_(mask_idx, masked_cur_partition)\n\n        cur_values = self.transitions.view(1, tag_size, tag_size).expand(\n            batch_size, tag_size, tag_size) + partition.contiguous().view(\n                batch_size, tag_size, 1).expand(batch_size, tag_size, tag_size)\n        cur_partition = log_sum_exp(cur_values, tag_size)\n        final_partition = cur_partition[:, self.END_TAG_IDX]\n        return final_partition.sum(), scores\n\n    def _viterbi_decode(self, feats, mask):\n        """"""\n        Args:\n            feats: size=(batch_size, seq_len, self.target_size+2)\n            mask: size=(batch_size, seq_len)\n\n        Returns:\n            decode_idx: (batch_size, seq_len), viterbi decode\xe7\xbb\x93\xe6\x9e\x9c\n            path_score: size=(batch_size, 1), \xe6\xaf\x8f\xe4\xb8\xaa\xe5\x8f\xa5\xe5\xad\x90\xe7\x9a\x84\xe5\xbe\x97\xe5\x88\x86\n        """"""\n        batch_size = feats.size(0)\n        seq_len = feats.size(1)\n        tag_size = feats.size(-1)\n\n        length_mask = torch.sum(mask, dim=1).view(batch_size, 1).long()\n\n        mask = mask.transpose(1, 0).contiguous()\n        ins_num = seq_len * batch_size\n\n        feats = feats.transpose(1, 0).contiguous().view(\n            ins_num, 1, tag_size).expand(ins_num, tag_size, tag_size)\n\n        scores = feats + self.transitions.view(\n            1, tag_size, tag_size).expand(ins_num, tag_size, tag_size)\n        scores = scores.view(seq_len, batch_size, tag_size, tag_size)\n\n        seq_iter = enumerate(scores)\n        # record the position of the best score\n        back_points = list()\n        partition_history = list()\n\n        # mask = 1 + (-1) * mask\n        mask = (1 - mask.long()).byte()\n        try:\n            _, inivalues = seq_iter.__next__()\n        except:\n            _, inivalues = seq_iter.next()\n\n        partition = inivalues[:, self.START_TAG_IDX, :].clone().view(batch_size, tag_size, 1)\n        partition_history.append(partition)\n\n        for idx, cur_values in seq_iter:\n            cur_values = cur_values + partition.contiguous().view(\n                batch_size, tag_size, 1).expand(batch_size, tag_size, tag_size)\n            partition, cur_bp = torch.max(cur_values, 1)\n            partition_history.append(partition.unsqueeze(-1))\n\n            cur_bp.masked_fill_(mask[idx].view(batch_size, 1).expand(batch_size, tag_size), 0)\n            back_points.append(cur_bp)\n\n        partition_history = torch.cat(partition_history).view(\n            seq_len, batch_size, -1).transpose(1, 0).contiguous()\n\n        last_position = length_mask.view(batch_size, 1, 1).expand(batch_size, 1, tag_size) - 1\n        last_partition = torch.gather(\n            partition_history, 1, last_position).view(batch_size, tag_size, 1)\n\n        last_values = last_partition.expand(batch_size, tag_size, tag_size) + \\\n            self.transitions.view(1, tag_size, tag_size).expand(batch_size, tag_size, tag_size)\n        _, last_bp = torch.max(last_values, 1)\n        pad_zero = Variable(torch.zeros(batch_size, tag_size)).long()\n        if self.use_cuda:\n            pad_zero = pad_zero.cuda()\n        back_points.append(pad_zero)\n        back_points = torch.cat(back_points).view(seq_len, batch_size, tag_size)\n\n        pointer = last_bp[:, self.END_TAG_IDX]\n        insert_last = pointer.contiguous().view(batch_size, 1, 1).expand(batch_size, 1, tag_size)\n        back_points = back_points.transpose(1, 0).contiguous()\n\n        back_points.scatter_(1, last_position, insert_last)\n\n        back_points = back_points.transpose(1, 0).contiguous()\n\n        decode_idx = Variable(torch.LongTensor(seq_len, batch_size))\n        if self.use_cuda:\n            decode_idx = decode_idx.cuda()\n        decode_idx[-1] = pointer.data\n        for idx in range(len(back_points)-2, -1, -1):\n            pointer = torch.gather(back_points[idx], 1, pointer.contiguous().view(batch_size, 1))\n            decode_idx[idx] = pointer.view(-1).data\n        path_score = None\n        decode_idx = decode_idx.transpose(1, 0)\n        return path_score, decode_idx\n\n    def forward(self, feats, mask):\n        path_score, best_path = self._viterbi_decode(feats, mask)\n        return path_score, best_path\n\n    def _score_sentence(self, scores, mask, tags):\n        """"""\n        Args:\n            scores: size=(seq_len, batch_size, tag_size, tag_size)\n            mask: size=(batch_size, seq_len)\n            tags: size=(batch_size, seq_len)\n\n        Returns:\n            score:\n        """"""\n        batch_size = scores.size(1)\n        seq_len = scores.size(0)\n        tag_size = scores.size(-1)\n\n        new_tags = Variable(torch.LongTensor(batch_size, seq_len))\n        if self.use_cuda:\n            new_tags = new_tags.cuda()\n        for idx in range(seq_len):\n            if idx == 0:\n                new_tags[:, 0] = (tag_size - 2) * tag_size + tags[:, 0]\n            else:\n                new_tags[:, idx] = tags[:, idx-1] * tag_size + tags[:, idx]\n\n        end_transition = self.transitions[:, self.END_TAG_IDX].contiguous().view(\n            1, tag_size).expand(batch_size, tag_size)\n        length_mask = torch.sum(mask, dim=1).view(batch_size, 1).long()\n        end_ids = torch.gather(tags, 1, length_mask-1)\n\n        end_energy = torch.gather(end_transition, 1, end_ids)\n\n        new_tags = new_tags.transpose(1, 0).contiguous().view(seq_len, batch_size, 1)\n        tg_energy = torch.gather(scores.view(seq_len, batch_size, -1), 2, new_tags).view(\n            seq_len, batch_size)\n        tg_energy = tg_energy.masked_select(mask.transpose(1, 0))\n\n        gold_score = tg_energy.sum() + end_energy.sum()\n\n        return gold_score\n\n    def neg_log_likelihood_loss(self, feats, mask, tags):\n        """"""\n        Args:\n            feats: size=(batch_size, seq_len, tag_size)\n            mask: size=(batch_size, seq_len)\n            tags: size=(batch_size, seq_len)\n        """"""\n        batch_size = feats.size(0)\n        forward_score, scores = self._forward_alg(feats, mask)\n        gold_score = self._score_sentence(scores, mask, tags)\n        if self.average_batch:\n            return (forward_score - gold_score) / batch_size\n        return forward_score - gold_score\n'"
sltk/nn/modules/feature.py,5,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\nimport torch\nimport torch.nn as nn\n\nfrom ..functional import init_embedding\n\n\nclass CharFeature(nn.Module):\n\n    def __init__(self, **kwargs):\n        """"""\n        Args:\n            feature_size: int, \xe5\xad\x97\xe7\xac\xa6\xe8\xa1\xa8\xe7\x9a\x84\xe5\xa4\xa7\xe5\xb0\x8f\n            feature_dim: int, \xe5\xad\x97\xe7\xac\xa6embedding \xe7\xbb\xb4\xe5\xba\xa6\n            require_grad: bool\xef\xbc\x8cchar\xe7\x9a\x84embedding\xe8\xa1\xa8\xe6\x98\xaf\xe5\x90\xa6\xe9\x9c\x80\xe8\xa6\x81\xe6\x9b\xb4\xe6\x96\xb0\n\n            filter_sizes: list(int), \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe5\xb0\xba\xe5\xaf\xb8\n            filter_nums: list(int), \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe6\x95\xb0\xe9\x87\x8f\n        """"""\n        super(CharFeature, self).__init__()\n        for k in kwargs:\n            self.__setattr__(k, kwargs[k])\n\n        # char embedding layer\n        self.char_embedding = nn.Embedding(self.feature_size, self.feature_dim)\n        init_embedding(self.char_embedding.weight)\n\n        # cnn\n        self.char_encoders = nn.ModuleList()\n        for i, filter_size in enumerate(self.filter_sizes):\n            f = nn.Conv3d(\n                in_channels=1, out_channels=self.filter_nums[i], kernel_size=(1, filter_size, self.feature_dim))\n            self.char_encoders.append(f)\n\n    def forward(self, inputs):\n        """"""\n        Args:\n            inputs: 3D tensor, [bs, max_len, max_len_char]\n\n        Returns:\n            char_conv_outputs: 3D tensor, [bs, max_len, output_dim]\n        """"""\n        max_len, max_len_char = inputs.size(1), inputs.size(2)\n        inputs = inputs.view(-1, max_len * max_len_char)  # [bs, -1]\n        input_embed = self.char_embedding(inputs)  # [bs, ml*ml_c, feature_dim]\n        # [bs, 1, max_len, max_len_char, feature_dim]\n        input_embed = input_embed.view(-1, 1, max_len, max_len_char, self.feature_dim)\n\n        # conv\n        char_conv_outputs = []\n        for char_encoder in self.char_encoders:\n            conv_output = char_encoder(input_embed)\n            pool_output = torch.squeeze(torch.max(conv_output, -2)[0])\n            char_conv_outputs.append(pool_output)\n        char_conv_outputs = torch.cat(char_conv_outputs, dim=1)\n\n        # size=[bs, max_len, output_dim]\n        char_conv_outputs = char_conv_outputs.transpose(-2, -1).contiguous()\n\n        return char_conv_outputs\n\n\nclass WordFeature(nn.Module):\n\n    def __init__(self, **kwargs):\n        """"""\n        Args:\n             feature_names: list(str), \xe7\x89\xb9\xe5\xbe\x81\xe5\x90\x8d\xe7\xa7\xb0\n             feature_size_dict: dict({str: int}), \xe7\x89\xb9\xe5\xbe\x81\xe5\x90\x8d\xe7\xa7\xb0\xe5\x88\xb0\xe7\x89\xb9\xe5\xbe\x81alphabet\xe5\xa4\xa7\xe5\xb0\x8f\xe6\x98\xa0\xe5\xb0\x84\xe5\xad\x97\xe5\x85\xb8\n             feature_dim_dict: dict({str: int}), \xe7\x89\xb9\xe5\xbe\x81\xe5\x90\x8d\xe7\xa7\xb0\xe5\x88\xb0\xe7\x89\xb9\xe5\xbe\x81\xe7\xbb\xb4\xe5\xba\xa6\xe6\x98\xa0\xe5\xb0\x84\xe5\xad\x97\xe5\x85\xb8\n\n             require_grad_dict: dict({str: bool})\xef\xbc\x8c\xe7\x89\xb9\xe5\xbe\x81embedding\xe8\xa1\xa8\xe6\x98\xaf\xe5\x90\xa6\xe9\x9c\x80\xe8\xa6\x81\xe6\x9b\xb4\xe6\x96\xb0\xef\xbc\x8c\xe7\x89\xb9\xe5\xbe\x81\xe5\x90\x8d: bool\n             pretrained_embed_dict: dict({str: np.array}): \xe7\x89\xb9\xe5\xbe\x81\xe7\x9a\x84\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83embedding table\n        """"""\n        super(WordFeature, self).__init__()\n        for k in kwargs:\n            self.__setattr__(k, kwargs[k])\n\n        if not hasattr(self, \'require_grad_dict\'):  # \xe9\xbb\x98\xe8\xae\xa4\xe9\x9c\x80\xe8\xa6\x81\xe6\x9b\xb4\xe6\x96\xb0\n            self.require_grad_dict = dict()\n            for feature_name in self.feature_names:\n                self.require_grad_dict[feature_name] = True\n        for feature_name in self.feature_names:\n            if feature_name not in self.require_grad_dict:\n                self.require_grad_dict[feature_name] = True\n\n        if not hasattr(self, \'pretrained_embed_dict\'):  # \xe9\xbb\x98\xe8\xae\xa4\xe9\x9a\x8f\xe6\x9c\xba\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96feature embedding\n            self.pretrained_embed_dict = dict()\n            for feature_name in self.feature_names:\n                self.pretrained_embed_dict[feature_name] = None\n        for feature_name in self.feature_names:\n            if feature_name not in self.pretrained_embed_dict:\n                self.pretrained_embed_dict[feature_name] = None\n\n        # feature embedding layer\n        self.feature_embedding_list = nn.ModuleList()\n        for feature_name in self.feature_names:\n            embed = nn.Embedding(self.feature_size_dict[feature_name], self.feature_dim_dict[feature_name])\n            if self.pretrained_embed_dict[feature_name] is not None:  # \xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe5\x90\x91\xe9\x87\x8f\n                # print(\'\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83:\', feature_name)\n                embed.weight.data.copy_(torch.from_numpy(self.pretrained_embed_dict[feature_name]))\n            else:  # \xe9\x9a\x8f\xe6\x9c\xba\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n                # print(\'\xe9\x9a\x8f\xe6\x9c\xba\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96:\', feature_name)\n                init_embedding(embed.weight)\n            # \xe6\x98\xaf\xe5\x90\xa6\xe9\x9c\x80\xe8\xa6\x81\xe6\xa0\xb9\xe6\x8d\xaeembedding\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\n            embed.weight.requires_grad = self.require_grad_dict[feature_name]\n            self.feature_embedding_list.append(embed)\n\n    def forward(self, **input_dict):\n        """"""\n        Args:\n            input_dict: dict({str: LongTensor})\n\n        Returns:\n            embed_outputs: 3D tensor, [bs, max_len, input_size]\n        """"""\n        embed_outputs = []\n        for i, feature_name in enumerate(self.feature_names):\n            embed_outputs.append(self.feature_embedding_list[i](input_dict[feature_name]))\n        embed_outputs = torch.cat(embed_outputs, dim=2)  # size=[bs, max_len, input_size]\n\n        return embed_outputs\n'"
sltk/nn/modules/rnn.py,1,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\nimport torch\nimport torch.nn as nn\n\n\nclass RNN(nn.Module):\n\n    def __init__(self, **kwargs):\n        """"""\n        Args:\n            rnn_unit_type: str, options: [\'rnn\', \'lstm\', \'gru\']\n            input_dim: int, rnn\xe8\xbe\x93\xe5\x85\xa5\xe7\xbb\xb4\xe5\xba\xa6\n            num_rnn_units: int, rnn\xe5\x8d\x95\xe5\x85\x83\xe6\x95\xb0\n            num_layers: int, \xe5\xb1\x82\xe6\x95\xb0\n            bi_flag: bool, \xe6\x98\xaf\xe5\x90\xa6\xe5\x8f\x8c\xe5\x90\x91, default is True\n        """"""\n        super(RNN, self).__init__()\n        for k in kwargs:\n            self.__setattr__(k, kwargs[k])\n        if not hasattr(self, \'bi_flag\'):\n            self.__setattr__(\'bi_flag\', True)\n\n        if self.rnn_unit_type == \'rnn\':\n            self.rnn = nn.RNN(self.input_dim, self.num_rnn_units, self.num_layers, bidirectional=self.bi_flag)\n        elif self.rnn_unit_type == \'lstm\':\n            self.rnn = nn.LSTM(self.input_dim, self.num_rnn_units, self.num_layers, bidirectional=self.bi_flag)\n        elif self.rnn_unit_type == \'gru\':\n            self.rnn = nn.GRU(self.input_dim, self.num_rnn_units, self.num_layers, bidirectional=self.bi_flag)\n\n    def forward(self, feats):\n        """"""\n        Args:\n             feats: 3D tensor, shape=[bs, max_len, input_dim]\n\n        Returns:\n            rnn_outputs: 3D tensor, shape=[bs, max_len, self.rnn_unit_num]\n        """"""\n        rnn_outputs, _ = self.rnn(feats)\n        return rnn_outputs\n'"
sltk/nn/modules/sequence_labeling_model.py,4,"b'#!/usr/bin/env python\n# -*- encoding: utf-8 -*-\n""""""\n    Sequence Labeling Model.\n""""""\nimport torch\nimport torch.nn as nn\n\nfrom .rnn import RNN\nfrom .crf import CRF\nfrom .feature import CharFeature, WordFeature\n\n\nclass SLModel(nn.Module):\n\n    def __init__(self, **kwargs):\n        """"""\n        Args:\n            feature_names: list(str), \xe7\x89\xb9\xe5\xbe\x81\xe5\x90\x8d\xe7\xa7\xb0, \xe4\xb8\x8d\xe5\x8c\x85\xe6\x8b\xac`label`\xe5\x92\x8c`char`\n\n            feature_size_dict: dict({str: int}), \xe7\x89\xb9\xe5\xbe\x81\xe8\xa1\xa8\xe5\xa4\xa7\xe5\xb0\x8f\xe5\xad\x97\xe5\x85\xb8\n            feature_dim_dict: dict({str: int}), \xe8\xbe\x93\xe5\x85\xa5\xe7\x89\xb9\xe5\xbe\x81dim\xe5\xad\x97\xe5\x85\xb8\n            pretrained_embed_dict: dict({str: np.array})\n            require_grad_dict: bool, \xe6\x98\xaf\xe5\x90\xa6\xe6\x9b\xb4\xe6\x96\xb0feature embedding\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\n\n            # char parameters\n            use_char: bool, \xe6\x98\xaf\xe5\x90\xa6\xe4\xbd\xbf\xe7\x94\xa8\xe5\xad\x97\xe7\xac\xa6\xe7\x89\xb9\xe5\xbe\x81, default is False\n            filter_sizes: list(int), \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe5\xb0\xba\xe5\xaf\xb8, default is [3]\n            filter_nums: list(int), \xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xe6\x95\xb0\xe9\x87\x8f, default is [32]\n\n            # rnn parameters\n            rnn_unit_type: str, options: [\'rnn\', \'lstm\', \'gru\']\n            num_rnn_units: int, rnn\xe5\x8d\x95\xe5\x85\x83\xe6\x95\xb0\n            num_layers: int, \xe5\xb1\x82\xe6\x95\xb0\n            bi_flag: bool, \xe6\x98\xaf\xe5\x90\xa6\xe5\x8f\x8c\xe5\x90\x91, default is True\n\n            use_crf: bool, \xe6\x98\xaf\xe5\x90\xa6\xe4\xbd\xbf\xe7\x94\xa8crf\xe5\xb1\x82\n\n            dropout_rate: float, dropout rate\n\n            average_batch: bool, \xe6\x98\xaf\xe5\x90\xa6\xe5\xaf\xb9batch\xe7\x9a\x84loss\xe5\x81\x9a\xe5\xb9\xb3\xe5\x9d\x87\n            use_cuda: bool\n        """"""\n        super(SLModel, self).__init__()\n        for k in kwargs:\n            self.__setattr__(k, kwargs[k])\n\n        # word level feature layer\n        self.word_feature_layer = WordFeature(\n            feature_names=self.feature_names, feature_size_dict=self.feature_size_dict,\n            feature_dim_dict=self.feature_dim_dict, require_grad_dict=self.require_grad_dict,\n            pretrained_embed_dict=self.pretrained_embed_dict)\n        rnn_input_dim = 0\n        for name in self.feature_names:\n            rnn_input_dim += self.feature_dim_dict[name]\n\n        # char level feature layer\n        if self.use_char:\n            self.char_feature_layer = CharFeature(\n                feature_size=self.feature_size_dict[\'char\'], feature_dim=self.feature_dim_dict[\'char\'],\n                require_grad=self.require_grad_dict[\'char\'], filter_sizes=self.filter_sizes,\n                filter_nums=self.filter_nums)\n            rnn_input_dim += sum(self.filter_nums)\n\n        # feature dropout\n        self.dropout_feature = nn.Dropout(self.dropout_rate)\n\n        # rnn layer\n        self.rnn_layer = RNN(\n            rnn_unit_type=self.rnn_unit_type, input_dim=rnn_input_dim, num_rnn_units=self.num_rnn_units,\n            num_layers=self.num_layers, bi_flag=self.bi_flag)\n\n        # rnn dropout\n        self.dropout_rnn = nn.Dropout(self.dropout_rate)\n\n        # crf layer\n        self.target_size = self.feature_size_dict[\'label\']\n        args_crf = dict({\'target_size\': self.target_size, \'use_cuda\': self.use_cuda})\n        args_crf[\'average_batch\'] = self.average_batch\n        if self.use_crf:\n            self.crf_layer = CRF(**args_crf)\n\n        # dense layer\n        hidden_input_dim = self.num_rnn_units * 2 if self.bi_flag else self.num_rnn_units\n        target_size = self.target_size + 2 if self.use_crf else self.target_size\n        self.hidden2tag = nn.Linear(hidden_input_dim, target_size)\n\n        # loss\n        if not self.use_crf:\n            self.loss_function = nn.CrossEntropyLoss(ignore_index=0, size_average=False)\n        else:\n            self.loss_function = self.crf_layer.neg_log_likelihood_loss\n\n    def loss(self, feats, mask, tags):\n        """"""\n        Args:\n            feats: size=(batch_size, seq_len, tag_size)\n            mask: size=(batch_size, seq_len)\n            tags: size=(batch_size, seq_len)\n        """"""\n        if not self.use_crf:\n            batch_size, max_len = feats.size(0), feats.size(1)\n            lstm_feats = feats.view(batch_size * max_len, -1)\n            tags = tags.view(-1)\n            return self.loss_function(lstm_feats, tags)\n        else:\n            loss_value = self.loss_function(feats, mask, tags)\n        if self.average_batch:\n            batch_size = feats.size(0)\n            loss_value /= float(batch_size)\n        return loss_value\n\n    def forward(self, **feed_dict):\n        """"""\n        Args:\n             inputs: list\n        """"""\n        batch_size = feed_dict[self.feature_names[0]].size(0)\n        max_len = feed_dict[self.feature_names[0]].size(1)\n\n        # word level feature\n        word_feed_dict = {}\n        for i, feature_name in enumerate(self.feature_names):\n            word_feed_dict[feature_name] = feed_dict[feature_name]\n        word_feature = self.word_feature_layer(**word_feed_dict)\n\n        # char level feature\n        if self.use_char:\n            char_feature = self.char_feature_layer(feed_dict[\'char\'])\n            word_feature = torch.cat([word_feature, char_feature], 2)\n\n        word_feature = self.dropout_feature(word_feature)\n        word_feature = torch.transpose(word_feature, 1, 0)  # size=[max_len, bs, input_size]\n\n        # rnn layer\n        rnn_outputs = self.rnn_layer(word_feature)\n        rnn_outputs = rnn_outputs.transpose(1, 0).contiguous()  # [bs, max_len, lstm_units]\n\n        rnn_outputs = self.dropout_rnn(rnn_outputs.view(-1, rnn_outputs.size(-1)))\n        rnn_feats = self.hidden2tag(rnn_outputs)\n\n        return rnn_feats.view(batch_size, max_len, -1)\n\n    def predict(self, rnn_outputs, actual_lens, mask=None):\n        batch_size = rnn_outputs.size(0)\n        tags_list = []\n\n        if not self.use_crf:\n            _, arg_max = torch.max(rnn_outputs, dim=2)  # [batch_size, max_len]\n            for i in range(batch_size):\n                tags_list.append(arg_max[i].cpu().data.numpy()[:actual_lens.data[i]])\n        else:\n            path_score, best_paths = self.crf_layer(rnn_outputs, mask)\n            for i in range(batch_size):\n                tags_list.append(best_paths[i].cpu().data.numpy()[:actual_lens.data[i]])\n\n        return tags_list\n'"
