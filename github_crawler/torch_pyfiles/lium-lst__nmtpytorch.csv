file_path,api_count,code
setup.py,0,"b""# -*- coding: utf-8 -*-\nimport pathlib\nimport setuptools\n\n\ndef get_nmtpytorch_version():\n    with open('nmtpytorch/__init__.py') as f:\n        s = f.read().split('\\n')[0]\n        if '__version__' not in s:\n            raise RuntimeError('Can not detect version from nmtpytorch/__init__.py')\n        return eval(s.split(' ')[-1])\n\n\nwith open('README.md') as f:\n    long_description = f.read()\n\nwith open('NEWS.md') as f:\n    release_notes = f.read()\n\nlong_description = long_description.replace(\n    '## Release Notes\\n\\nSee [NEWS.md](NEWS.md).\\n', release_notes)\n\nsetuptools.setup(\n    name='nmtpytorch',\n    version=get_nmtpytorch_version(),\n    description='Sequence-to-Sequence Framework in PyTorch',\n    long_description=long_description,\n    long_description_content_type='text/markdown',\n    url='https://github.com/lium-lst/nmtpytorch',\n    author='Ozan Caglayan',\n    author_email='ozancag@gmail.com',\n    license='MIT',\n    project_urls={\n        'Wiki': 'https://github.com/lium-lst/nmtpytorch/wiki',\n    },\n    classifiers=[\n        'Intended Audience :: Science/Research',\n        'Topic :: Scientific/Engineering',\n        'License :: OSI Approved :: MIT License',\n        'Programming Language :: Python :: 3 :: Only',\n        'Programming Language :: Python :: 3.7',\n        'Operating System :: POSIX',\n    ],\n    keywords='nmt neural-mt translation sequence-to-sequence deep-learning pytorch',\n    python_requires='~=3.7',\n    install_requires=[\n        'numpy', 'scikit-learn', 'tqdm', 'pillow',\n        'torch==1.4.0', 'torchvision==0.5.0', 'pytorch-ignite==0.3.0',\n        'sacrebleu>=1.2.9',\n        'editdistance==0.4', 'subword_nmt==0.3.5',\n    ],\n    include_package_data=True,\n    exclude_package_data={'': ['.git']},\n    packages=setuptools.find_packages(),\n    scripts=[str(p) for p in pathlib.Path('bin').glob('*')],\n    zip_safe=False)\n"""
doc/conf.py,1,"b'# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(\'../nmtpytorch\'))\n\n#import pytorch_sphinx_theme\n\n\n# -- Project information -----------------------------------------------------\n\nproject = \'nmtpytorch\'\ncopyright = \'2020, Ozan Caglayan\'\nauthor = \'Ozan Caglayan\'\n\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.autosummary\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.napoleon\',\n    \'sphinx.ext.viewcode\',\n    #\'sphinxcontrib.katex\',\n    \'sphinx.ext.autosectionlabel\',\n    #\'javasphinx\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n#html_theme_path = [pytorch_sphinx_theme.get_html_theme_path()]\nhtml_logo = \'_static/img/logo.png\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n\nhtml_theme_options = {\n    \'collapse_navigation\': True,\n    # \'pytorch_project\': \'doc\',\n    # \'canonical_url\': \'https://pytorch.org/docs/stable/\',\n    # \'display_version\': True,\n    \'logo_only\': True,\n}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n\n# -- Extension configuration -------------------------------------------------'"
nmtpytorch/__init__.py,0,"b""__version__ = '4.0.0'\n"""
nmtpytorch/cleanup.py,0,"b'# -*- coding: utf-8 -*-\nimport os\nimport sys\nimport signal\nimport atexit\nimport pathlib\nimport traceback\n\n\nclass Cleanup:\n    def __init__(self):\n        self.temp_files = set()\n        self.processes = set()\n\n    def register_tmp_file(self, tmp_file):\n        """"""Add new temp file to global set.""""""\n        self.temp_files.add(pathlib.Path(tmp_file))\n\n    def register_proc(self, pid):\n        """"""Add new process to global set.""""""\n        self.processes.add(pid)\n\n    def unregister_proc(self, pid):\n        """"""Remove given PID from global set.""""""\n        self.processes.remove(pid)\n\n    def __call__(self):\n        """"""Cleanup registered temp files and kill PIDs.""""""\n        for tmp_file in filter(lambda x: x.exists(), self.temp_files):\n            tmp_file.unlink()\n\n        for proc in self.processes:\n            try:\n                os.kill(proc, signal.SIGTERM)\n            except ProcessLookupError:\n                pass\n\n    def __repr__(self):\n        repr_ = ""Cleanup Manager\\n""\n        if len(self.processes) > 0:\n            repr_ += ""Tracking Processes\\n""\n            for proc in self.processes:\n                repr_ += "" {}\\n"".format(proc)\n\n        if len(self.temp_files) > 0:\n            repr_ += ""Tracking Temporary Files\\n""\n            for tmp_file in self.temp_files:\n                repr_ += "" {}\\n"".format(tmp_file)\n\n        return repr_\n\n    @staticmethod\n    def register_exception_handler(logger, quit_on_exception=False):\n        """"""Setup exception handler.""""""\n\n        def exception_handler(exctype, val, trace):\n            """"""Let Python call this when an exception is uncaught.""""""\n            logger.info(\n                \'\'.join(traceback.format_exception(exctype, val, trace)))\n\n        def exception_handler_quits(exctype, val, trace):\n            """"""Let Python call this when an exception is uncaught.""""""\n            logger.info(\n                \'\'.join(traceback.format_exception(exctype, val, trace)))\n            sys.exit(1)\n\n        if quit_on_exception:\n            sys.excepthook = exception_handler_quits\n        else:\n            sys.excepthook = exception_handler\n\n    @staticmethod\n    def register_handler(logger, _atexit=True, _signals=True,\n                         exception_quits=False):\n        """"""Register atexit and signal handlers.""""""\n        if _atexit:\n            # Register exit handler\n            atexit.register(cleanup)\n\n        if _signals:\n            # Register SIGINT and SIGTERM\n            signal.signal(signal.SIGINT, signal_handler)\n            signal.signal(signal.SIGTERM, signal_handler)\n\n        Cleanup.register_exception_handler(logger, exception_quits)\n\n\n# Create a global cleaner\ncleanup = Cleanup()\n\n\ndef signal_handler(signum, frame):\n    """"""Let Python call this when SIGINT or SIGTERM caught.""""""\n    cleanup()\n    sys.exit(0)\n'"
nmtpytorch/config.py,1,"b'# -*- coding: utf-8 -*-\nimport os\nimport sys\nimport copy\nimport pathlib\nfrom difflib import get_close_matches\n\nfrom collections import defaultdict\n\nfrom configparser import ConfigParser, ExtendedInterpolation\nfrom ast import literal_eval\n\n\nTRAIN_DEFAULTS = {\n    \'num_workers\': 0,            # number of workers for data loading (0=disabled)\n    \'pin_memory\': False,         # pin_memory for DataLoader (Default: False)\n    \'seed\': 0,                   # > 0 if you want to reproduce a previous experiment\n    \'gclip\': 5.,                 # Clip gradients above clip_c\n    \'l2_reg\': 0.,                # L2 penalty factor\n    \'patience\': 20,              # Early stopping patience\n    \'optimizer\': \'adam\',         # adadelta, sgd, rmsprop, adam\n    \'lr\': 0.0004,                # 0 -> Use default lr from Pytorch\n    \'lr_decay\': False,           # Can only be \'plateau\' for now\n    \'lr_decay_revert\': False,    # Return back to the prev best weights after decay\n    \'lr_decay_factor\': 0.1,      # Check torch.optim.lr_scheduler\n    \'lr_decay_patience\': 10,     #\n    \'lr_decay_min\': 0.000001,    #\n    \'model_type\': \'\',            # Name of model class to train\n    \'momentum\': 0.0,             # momentum for SGD\n    \'nesterov\': False,           # Enable Nesterov for SGD\n    \'disp_freq\': 30,             # Training display frequency (/batch)\n    \'batch_size\': 32,            # Training batch size\n    \'max_epochs\': 100,           # Max number of epochs to train\n    \'max_iterations\': int(1e6),  # Max number of updates to train\n    \'eval_metrics\': \'loss\',      # comma sep. metrics, 1st -> earlystopping\n    \'eval_filters\': \'\',          # comma sep. filters to apply to refs/hyps\n    \'eval_beam\': 6,              # Validation beam size\n    \'eval_batch_size\': 16,       # batch_size for beam-search\n    \'eval_freq\': 3000,           # 0 means \'End of epochs\'\n    \'eval_max_len\': 200,         # max seq len to stop during beam search\n    \'eval_start\': 1,             # Epoch which validation will start\n    \'eval_zero\': False,          # Evaluate once before starting training\n                                 # Useful when using pretrained_file\n    \'save_best_metrics\': True,   # Save best models for each eval_metric\n    \'save_path\': \'\',             # Path to root experiment folder\n    \'save_optim_state\': False,   # Save optimizer states into checkpoint\n    \'checkpoint_freq\': 5000,     # Periodic checkpoint frequency\n    \'n_checkpoints\': 5,          # Number of checkpoints to keep\n    \'tensorboard_dir\': \'\',       # Enable TB and give global log folder\n    \'pretrained_file\': \'\',       # A .ckpt file from which layers will be initialized\n    \'pretrained_layers\': \'\',     # comma sep. list of layer prefixes to initialize\n    \'freeze_layers\': \'\',         # comma sep. list of layer prefixes to freeze\n    \'handle_oom\': False,         # Skip out-of-memory batches\n}\n\n\ndef expand_env_vars(data):\n    """"""Interpolate some environment variables.""""""\n    for key in (\'HOME\', \'USER\', \'LOCAL\', \'SCRATCH\', \'PWD\'):\n        var = \'$\' + key\n        if var in data and key in os.environ:\n            data = data.replace(var, os.environ[key])\n    return data\n\n\ndef resolve_path(value):\n    if isinstance(value, list):\n        return [resolve_path(elem) for elem in value]\n    if isinstance(value, dict):\n        return {k: resolve_path(v) for k, v in value.items()}\n    if isinstance(value, str) and value.startswith((\'~\', \'/\', \'../\', \'./\')):\n        return pathlib.Path(value).expanduser().resolve()\n    return value\n\n\ndef _parse_value(value):\n    """"""Automatic type conversion for configuration values.\n\n    Arguments:\n        value(str): A string to parse.\n    """"""\n\n    # Check for boolean or None\n    if str(value).capitalize().startswith((\'False\', \'True\', \'None\')):\n        return eval(str(value).capitalize(), {}, {})\n\n    # Detect strings, floats and ints\n    try:\n        # If this fails, this is a string\n        result = literal_eval(value)\n    except Exception:\n        result = value\n\n    return result\n\n\nclass Options:\n    @classmethod\n    def from_dict(cls, dict_, override_list=None):\n        """"""Loads object from dict.""""""\n        obj = cls.__new__(cls)\n        obj.__dict__.update(dict_)\n\n        # Test time overrides are possible as well\n        if override_list is not None:\n            overrides = obj.parse_overrides(override_list)\n            for section, ov_dict in overrides.items():\n                for key, value in ov_dict.items():\n                    obj.__dict__[section][key] = value\n\n        return obj\n\n    @classmethod\n    def parse_overrides(cls, override_list):\n        overrides = defaultdict(dict)\n        for opt in override_list:\n            section, keyvalue = opt.split(\'.\', 1)\n            key, value = keyvalue.split(\':\')\n            value = resolve_path(value)\n            overrides[section][key] = _parse_value(value)\n        return overrides\n\n    def __init__(self, filename, overrides=None):\n        self._parser = ConfigParser(interpolation=ExtendedInterpolation())\n        self.filename = filename\n        self.sections = []\n\n        with open(self.filename) as fhandle:\n            data = expand_env_vars(fhandle.read().strip())\n\n        # Read the defaults first\n        self._parser.read_dict({\'train\': TRAIN_DEFAULTS})\n\n        # Read the config\n        self._parser.read_string(data)\n\n        if overrides is not None:\n            # ex: train.batch_size:32\n            self.overrides = self.parse_overrides(overrides)\n\n        for section in self._parser.sections():\n            opts = {}\n            self.sections.append(section)\n\n            for key, value in self._parser[section].items():\n                opts[key] = resolve_path(_parse_value(value))\n\n            if section in self.overrides:\n                for (key, value) in self.overrides[section].items():\n                    opts[key] = value\n\n            setattr(self, section, opts)\n\n        # Sanity check for [train]\n        train_keys = list(self.train.keys())\n        def_keys = list(TRAIN_DEFAULTS.keys())\n        assert len(train_keys) == len(set(train_keys)), \\\n            ""Duplicate arguments found in config\'s [train] section.""\n\n        invalid_keys = set(train_keys).difference(set(TRAIN_DEFAULTS))\n        for key in invalid_keys:\n            match = get_close_matches(key, def_keys, n=1)\n            msg = ""{}:train: Unknown option \'{}\'."".format(self.filename, key)\n            if match:\n                msg += ""  Did you mean \'{}\' ?"".format(match[0])\n            print(msg)\n        if invalid_keys:\n            sys.exit(1)\n\n    def __repr__(self):\n        repr_ = """"\n        for section in self.sections:\n            opts = getattr(self, section)\n            repr_ += ""-"" * (len(section) + 2)\n            repr_ += ""\\n[{}]\\n"".format(section)\n            repr_ += ""-"" * (len(section) + 2)\n            repr_ += \'\\n\'\n            for key, value in opts.items():\n                if isinstance(value, list):\n                    repr_ += ""{:>20}:\\n"".format(key)\n                    for elem in value:\n                        repr_ += ""{:>22}\\n"".format(elem)\n                elif isinstance(value, dict):\n                    repr_ += ""{:>20}:\\n"".format(key)\n                    for kkey, vvalue in value.items():\n                        repr_ += ""{:>22}:{}\\n"".format(kkey, vvalue)\n                else:\n                    repr_ += ""{:>20}:{}\\n"".format(key, value)\n        repr_ += ""-"" * 70\n        repr_ += ""\\n""\n        return repr_\n\n    def to_dict(self):\n        """"""Serializes the instance as dict.""""""\n        dict_ = {\n            \'filename\': self.filename,\n            \'sections\': self.sections,\n        }\n        for section in self.sections:\n            dict_[section] = copy.deepcopy(getattr(self, section))\n\n        return dict_\n\n    def __getitem__(self, key):\n        return getattr(self, key)\n'"
nmtpytorch/evaluator.py,0,"b'# -*- coding: utf-8 -*-\nfrom collections import OrderedDict\n\nfrom . import metrics\nfrom .utils.filterchain import FilterChain\nfrom .utils.misc import get_language\n\n\nclass Evaluator:\n    def __init__(self, refs, beam_metrics, filters=\'\'):\n        # metrics: list of upper-case beam-search metrics\n        self.kwargs = {}\n        self.scorers = OrderedDict()\n        self.refs = list(refs.parent.glob(refs.name))\n        self.language = get_language(self.refs[0])\n        if self.language is None:\n            # Fallback to en (this is only relevant for METEOR)\n            self.language = \'en\'\n\n        self.filter = lambda s: s\n        if filters:\n            self.filter = FilterChain(filters)\n            self.refs = self.filter(refs)\n\n        assert len(self.refs) > 0, ""Number of reference files == 0""\n\n        for metric in sorted(beam_metrics):\n            self.kwargs[metric] = {\'language\': self.language}\n            self.scorers[metric] = getattr(metrics, metric + \'Scorer\')()\n\n    def score(self, hyps):\n        """"""hyps is a list of hypotheses as they come out from decoder.""""""\n        assert isinstance(hyps, list), ""hyps should be a list.""\n\n        # Post-process if requested\n        hyps = self.filter(hyps)\n\n        results = []\n        for key, scorer in self.scorers.items():\n            results.append(\n                scorer.compute(self.refs, hyps, **self.kwargs[key]))\n        return results\n'"
nmtpytorch/logger.py,0,"b""# -*- coding: utf-8 -*-\nimport pathlib\nimport logging\n\nfrom .cleanup import cleanup\n\n\ndef setup(opts=None):\n    _format = '%(message)s'\n\n    formatter = logging.Formatter(_format)\n    logger = logging.getLogger('nmtpytorch')\n    logger.setLevel(logging.DEBUG)\n\n    con_handler = logging.StreamHandler()\n    con_handler.setFormatter(formatter)\n    logger.addHandler(con_handler)\n\n    if opts is not None:\n        log_file = str(pathlib.Path(opts['save_path']) /\n                       opts['subfolder'] / opts['exp_id']) + '.log'\n        file_handler = logging.FileHandler(log_file, mode='w')\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n\n    cleanup.register_handler(logger)\n    return logger\n"""
nmtpytorch/mainloop.py,5,"b'# -*- coding: utf-8 -*-\nimport time\nimport logging\n\nimport torch\n\nfrom .evaluator import Evaluator\nfrom .optimizer import Optimizer\nfrom .monitor import Monitor\nfrom .utils.misc import get_module_groups\nfrom .utils.misc import load_pt_file, fix_seed\nfrom .utils.ml_metrics import Loss\nfrom .utils.data import make_dataloader\nfrom .utils.tensorboard import TensorBoard\n\nlogger = logging.getLogger(\'nmtpytorch\')\n\n\nclass MainLoop:\n    def __init__(self, model, train_opts, dev_mgr):\n        # Get all training options into this mainloop\n        self.__dict__.update(train_opts)\n\n        self.print = logger.info\n        self.model = model\n        self.dev_mgr = dev_mgr\n        self.epoch_valid = (self.eval_freq == 0)\n        self.oom_count = 0\n        self.loss_meter = Loss()\n        self._found_optim_state = None\n\n        # Load training and validation data & create iterators\n        self.print(\'Loading dataset(s)\')\n        self.train_iterator = make_dataloader(\n            self.model.load_data(\'train\', self.batch_size),\n            self.pin_memory, self.num_workers)\n\n        # Create monitor for validation, evaluation, checkpointing stuff\n        self.monitor = Monitor(self.save_path / self.subfolder, self.exp_id,\n                               self.model, logger, self.patience,\n                               self.eval_metrics,\n                               save_best_metrics=self.save_best_metrics,\n                               n_checkpoints=self.n_checkpoints)\n\n        # If a validation set exists\n        if \'val_set\' in self.model.opts.data and self.eval_freq >= 0:\n            if \'LOSS\' in self.monitor.eval_metrics:\n                self.vloss_iterator = make_dataloader(\n                    self.model.load_data(\'val\', self.batch_size, mode=\'eval\'))\n\n            if self.monitor.beam_metrics is not None:\n                self.beam_iterator = make_dataloader(\n                    self.model.load_data(\'val\', self.eval_batch_size, mode=\'beam\'))\n                # Create hypothesis evaluator\n                self.evaluator = Evaluator(\n                    self.model.val_refs, self.monitor.beam_metrics,\n                    filters=self.eval_filters)\n\n        # Setup model\n        self.model.setup()\n        self.model.reset_parameters()\n\n        ################################################\n        # Initialize model weights with a pretrained one\n        # This should come after model.setup()\n        ################################################\n        if train_opts[\'pretrained_file\']:\n            # Relax the strict condition for partial initialization\n            data = load_pt_file(train_opts[\'pretrained_file\'])\n            weights = data[\'model\']\n            self._found_optim_state = data.get(\'optimizer\', None)\n            if train_opts[\'pretrained_layers\']:\n                prefixes = tuple(train_opts[\'pretrained_layers\'].split(\',\'))\n                keys = [w for w in weights if w.startswith(prefixes)]\n                weights = {k: weights[k] for k in keys}\n\n            for name in get_module_groups(weights.keys()):\n                self.print(\n                    \' -> will initialize {}.* with pretrained weights.\'.format(name))\n            model.load_state_dict(weights, strict=False)\n\n        ############################\n        # Freeze layers if requested\n        ############################\n        if train_opts[\'freeze_layers\']:\n            frozen = []\n            for layer in train_opts[\'freeze_layers\'].split(\',\'):\n                for name, param in self.model.named_parameters():\n                    if name.startswith(layer):\n                        param.requires_grad = False\n                        frozen.append(name)\n\n            for name in frozen:\n                self.print(f\' -> froze parameter {name}\')\n\n        self.print(self.model)\n        self.model = self.model.to(self.dev_mgr.dev)\n\n        if self.dev_mgr.req_cpu or len(self.dev_mgr.cuda_dev_ids) == 1:\n            self.net = self.model\n        else:\n            self.net = torch.nn.DataParallel(\n                self.model, device_ids=self.dev_mgr.cuda_dev_ids, dim=1)\n\n        # Create optimizer instance\n        self.optim = Optimizer(\n            self.optimizer, self.model, lr=self.lr, momentum=self.momentum,\n            nesterov=self.nesterov, weight_decay=self.l2_reg,\n            gclip=self.gclip, lr_decay=self.lr_decay,\n            lr_decay_factor=self.lr_decay_factor,\n            lr_decay_mode=self.monitor.lr_decay_mode,\n            lr_decay_min=self.lr_decay_min,\n            lr_decay_patience=self.lr_decay_patience)\n        self.print(self.optim)\n\n        if self._found_optim_state:\n            # NOTE: This will overwrite weight_decay and lr parameters\n            # from the checkpoint without obeying to new config file!\n            self.optim.load_state_dict(self._found_optim_state)\n\n        if self.save_optim_state:\n            self.monitor.set_optimizer(self.optim)\n\n        # Create TensorBoard logger if possible and requested\n        self.tboard = TensorBoard(self.model, self.tensorboard_dir,\n                                  self.exp_id, self.subfolder)\n        self.print(self.tboard)\n\n        # Models can also use tensorboard for custom purposes\n        self.model.register_tensorboard(self.tboard)\n\n        # Shift-by-1 and reseed to reproduce batch orders independently\n        # from model initialization etc.\n        fix_seed(self.seed + 1)\n\n    def train_batch(self, batch):\n        """"""Trains a batch.""""""\n        nn_start = time.time()\n\n        # Reset gradients\n        self.optim.zero_grad()\n\n        # Forward pass with training progress\n        # NOTE: Problematic for multi-gpu\n        out = self.net(batch, uctr=self.monitor.uctr, ectr=self.monitor.ectr)\n        if \'loss\' in out:\n            # NOTE: Fix this afterwards so that every model adapts the same style\n            # Classical models have single loss\n            self.loss_meter.update(out[\'loss\'], out[\'n_items\'])\n            loss = out[\'loss\'] / out[\'n_items\']\n        else:\n            # NOTE: For now, let\'s simply average losses for MTL\n            for tid in out:\n                self.loss_meter.update(out[tid][\'loss\'], out[tid][\'n_items\'])\n            # Normalize the losses and take the average\n            # NOTE: averaging may not be a good idea if the model multiplies\n            # them with scalar weights\n            loss = sum([out[k][\'loss\'] / out[k][\'n_items\'] for k in out]) / len(out)\n\n        # Add other losses if any\n        if self.net.aux_loss:\n            loss += sum(list(self.net.aux_loss.values()))\n\n        # Backward pass\n        loss.backward()\n\n        # Update parameters (includes gradient clipping logic)\n        self.optim.step()\n\n        return time.time() - nn_start\n\n    def train_epoch(self):\n        """"""Trains a full epoch.""""""\n        self.print(\'Starting Epoch {}\'.format(self.monitor.ectr))\n\n        nn_sec = 0.0\n        eval_sec = 0.0\n        total_sec = time.time()\n        self.loss_meter.reset()\n        self.oom_count = 0\n\n        for batch in self.train_iterator:\n            batch.device(self.dev_mgr.dev)\n            self.monitor.uctr += 1\n\n            try:\n                nn_sec += self.train_batch(batch)\n            except RuntimeError as e:\n                if self.handle_oom and \'out of memory\' in e.args[0]:\n                    torch.cuda.empty_cache()\n                    self.oom_count += 1\n                else:\n                    raise e\n\n            if self.monitor.uctr % self.disp_freq == 0:\n                # Send statistics\n                self.tboard.log_scalar(\n                    \'train_LOSS\', self.loss_meter.batch_loss, self.monitor.uctr)\n\n                msg = ""Epoch {} - update {:10d} => loss: {:>7.3f}"".format(\n                    self.monitor.ectr, self.monitor.uctr,\n                    self.loss_meter.batch_loss)\n                for key, value in self.net.aux_loss.items():\n                    val = value.item()\n                    msg += \' [{}: {:.3f}]\'.format(key, val)\n                    self.tboard.log_scalar(\'train_\' + key.upper(), val, self.monitor.uctr)\n                msg += \' (#OOM: {})\'.format(self.oom_count)\n                self.print(msg)\n\n            # Do validation?\n            if (not self.epoch_valid and\n                    self.monitor.ectr >= self.eval_start and\n                    self.eval_freq > 0 and\n                    self.monitor.uctr % self.eval_freq == 0):\n                eval_start = time.time()\n                self.do_validation()\n                eval_sec += time.time() - eval_start\n\n            if (self.checkpoint_freq and self.n_checkpoints > 0 and\n                    self.monitor.uctr % self.checkpoint_freq == 0):\n                self.print(\'Saving checkpoint...\')\n                self.monitor.save_checkpoint()\n\n            # Check stopping conditions\n            if self.monitor.early_bad == self.monitor.patience:\n                self.print(""Early stopped."")\n                return False\n\n            if self.monitor.uctr == self.max_iterations:\n                self.print(""Max iterations {} reached."".format(\n                    self.max_iterations))\n                return False\n\n        # All time spent for this epoch\n        total_min = (time.time() - total_sec) / 60\n        # All time spent during forward/backward/step\n        nn_min = nn_sec / 60\n        # All time spent during validation(s)\n        eval_min = eval_sec / 60\n        # Rest is iteration overhead + checkpoint saving\n        overhead_min = total_min - nn_min - eval_min\n\n        # Compute epoch loss\n        epoch_loss = self.loss_meter.get()\n        self.monitor.train_loss.append(epoch_loss)\n\n        self.print(""--> Epoch {} finished with mean loss {:.5f}"".format(\n            self.monitor.ectr, epoch_loss))\n        self.print(""--> Overhead/Training/Evaluation: {:.2f}/{:.2f}/{:.2f} ""\n                   ""mins (total: {:.2f} mins)   ({} samples/sec)"".format(\n                       overhead_min, nn_min, eval_min, total_min,\n                       int(len(self.train_iterator.dataset) / nn_sec)))\n\n        # Do validation?\n        if self.epoch_valid and self.monitor.ectr >= self.eval_start:\n            self.do_validation()\n\n        # Check whether maximum epoch is reached\n        if self.monitor.ectr == self.max_epochs:\n            self.print(""Max epochs {} reached."".format(self.max_epochs))\n            return False\n\n        self.monitor.ectr += 1\n        return True\n\n    def do_validation(self):\n        """"""Do early-stopping validation.""""""\n        results = []\n        self.monitor.vctr += 1\n        self.net.train(False)\n        torch.set_grad_enabled(False)\n\n        # Collect simple validation stats first\n        self.print(\'Computing evaluation loss...\')\n        results.extend(self.net.test_performance(self.vloss_iterator))\n\n        if self.monitor.beam_metrics:\n            self.print(\'Performing beam search (beam_size:{})\'.format(\n                self.eval_beam))\n            beam_time = time.time()\n            # For multitask learning models, language-specific validation uses\n            # by default the 0th Topology in val_tasks\n            task = None\n            if hasattr(self.net, \'val_tasks\'):\n                task = self.net.val_tasks[0].direction\n            hyps = self.net.beam_search(\n                [self.net], self.beam_iterator, task_id=task,\n                beam_size=self.eval_beam, max_len=self.eval_max_len)\n            beam_time = time.time() - beam_time\n\n            # Compute metrics and update results\n            score_time = time.time()\n            results.extend(self.evaluator.score(hyps))\n            score_time = time.time() - score_time\n\n            self.print(""Beam Search: {:.2f} sec, Scoring: {:.2f} sec ""\n                       ""({} sent/sec)"".format(beam_time, score_time,\n                                              int(len(hyps) / beam_time)))\n\n        # Log metrics to tensorboard\n        self.tboard.log_metrics(results, self.monitor.uctr, suffix=\'val_\')\n\n        # Add new scores to history\n        self.monitor.update_scores(results)\n\n        # Do a scheduler LR step\n        lr_change = self.optim.lr_step(self.monitor.get_last_eval_score())\n        if lr_change and self.lr_decay_revert:\n            self.print(\'Reloading previous best model parameters\')\n            self.monitor.reload_previous_best()\n\n        # Check early-stop criteria and save snapshots if any\n        self.monitor.save_models()\n\n        # Dump summary and switch back to training mode\n        self.monitor.val_summary()\n        self.net.train(True)\n        torch.set_grad_enabled(True)\n\n    def __call__(self):\n        """"""Runs training loop.""""""\n        self.print(\'Training started on %s\' % time.strftime(\'%d-%m-%Y %H:%M:%S\'))\n        self.net.train(True)\n        torch.set_grad_enabled(True)\n\n        # Evaluate once before even starting training\n        if self.eval_zero:\n            self.do_validation()\n\n        while self.train_epoch():\n            pass\n\n        if self.monitor.vctr > 0:\n            self.monitor.val_summary()\n        else:\n            # No validation done, save final model\n            self.print(\'Saving final model.\')\n            self.monitor.save_model(suffix=\'final\')\n\n        self.print(\'Training finished on %s\' % time.strftime(\'%d-%m-%Y %H:%M\'))\n        # Close tensorboard\n        self.tboard.close()\n'"
nmtpytorch/monitor.py,1,"b'# -*- coding: utf-8 -*-\nfrom collections import defaultdict\n\nimport torch\n\nfrom .utils.io import FileRotator\nfrom .utils.misc import load_pt_file\nfrom .metrics import beam_metrics, metric_info\n\n\nclass Monitor:\n    """"""Class that tracks training progress. The following informations are\n    kept as object attributes:\n        self.ectr:       # of epochs done so far\n        self.uctr:       # of updates, i.e. mini-batches done so far\n        self.vctr:       # of evaluations done on val_set so far\n        self.early_bad:  # of consecutive evaluations where the model did not improve\n        self.train_loss: List of training losses\n        self.val_scores: Dict of lists keeping tracking of validation metrics\n    """"""\n    # Variables to save\n    VARS = [\'uctr\', \'ectr\', \'vctr\', \'early_bad\', \'train_loss\', \'val_scores\']\n\n    def __init__(self, save_path, exp_id,\n                 model, logger, patience, eval_metrics, history=None,\n                 save_best_metrics=False, n_checkpoints=0):\n        self.print = logger.info\n        self.save_path = save_path\n        self.exp_id = exp_id\n        self.model = model\n        self.patience = patience\n        self.eval_metrics = [e.strip() for e in eval_metrics.upper().split(\',\')]\n        self.save_best_metrics = save_best_metrics\n        self.optimizer = None\n        self.checkpoints = FileRotator(n_checkpoints)\n        self.beam_metrics = None\n\n        if history is None:\n            history = {}\n\n        self.uctr = history.pop(\'uctr\', 0)\n        self.ectr = history.pop(\'ectr\', 1)\n        self.vctr = history.pop(\'vctr\', 0)\n        self.early_bad = history.pop(\'early_bad\', 0)\n        self.train_loss = history.pop(\'train_loss\', [])\n        self.val_scores = history.pop(\'val_scores\', defaultdict(list))\n\n        if len(self.eval_metrics) > 0:\n            # To keep current best metric validation id and score\n            self.cur_bests = {}\n\n            # First metric is considered to be early-stopping metric\n            self.early_metric = self.eval_metrics[0]\n\n            # Will be used by optimizer\n            self.lr_decay_mode = metric_info[self.early_metric]\n\n            # Get metrics requiring beam_search\n            bms = set(self.eval_metrics).intersection(beam_metrics)\n            if len(bms) > 0:\n                self.beam_metrics = list(bms)\n\n    @staticmethod\n    def best_score(scores):\n        """"""Returns the best validation id and score for that.""""""\n        idx, score = sorted(enumerate(scores), key=lambda e: e[1],\n                            reverse=scores[0].higher_better)[0]\n        return (idx + 1, score)\n\n    def set_optimizer(self, optimizer):\n        """"""Sets the optimizer to save its parameters.""""""\n        self.optimizer = optimizer\n\n    def state_dict(self):\n        """"""Returns a dictionary of stateful variables.""""""\n        return {k: getattr(self, k) for k in self.VARS}\n\n    def val_summary(self):\n        """"""Prints a summary of validation results.""""""\n        self.print(\'--> This is model: {}\'.format(self.exp_id))\n        for name, (vctr, score) in self.cur_bests.items():\n            self.print(\'--> Best {} so far: {:.2f} @ validation {}\'.format(\n                name, score.score, vctr))\n\n    def save_checkpoint(self):\n        """"""Saves a checkpoint by keeping track of file rotation.""""""\n        self.checkpoints.push(\n            self.save_model(suffix=\'update{}\'.format(self.uctr)))\n\n    def reload_previous_best(self):\n        """"""Reloads the parameters from the previous best checkpoint.""""""\n        fname = self.save_path / ""{}.best.{}.ckpt"".format(\n            self.exp_id, self.early_metric.lower())\n        data = load_pt_file(fname)\n        self.model.load_state_dict(data[\'model\'], strict=True)\n\n    def save_model(self, metric=None, suffix=\'\', do_symlink=False):\n        """"""Saves a checkpoint with arbitrary suffix(es) appended.""""""\n        # Construct file name\n        fname = self.exp_id\n        if metric:\n            self.print(\'Saving best model based on {}\'.format(metric.name))\n            fname += ""-val{:03d}.best.{}_{:.3f}"".format(\n                self.vctr, metric.name.lower(), metric.score)\n        if suffix:\n            fname += ""-{}"".format(suffix)\n        fname = self.save_path / (fname + "".ckpt"")\n\n        # Save the file\n        model_dict = {\n            \'opts\': self.model.opts.to_dict(),\n            \'model\': self.model.state_dict(),\n            \'history\': self.state_dict(),\n        }\n\n        # Add optimizer states\n        if self.optimizer is not None:\n            model_dict[\'optimizer\'] = self.optimizer.state_dict()\n\n        torch.save(model_dict, fname)\n\n        # Also create a symbolic link to the above checkpoint for the metric\n        if metric and do_symlink:\n            symlink = ""{}.best.{}.ckpt"".format(self.exp_id, metric.name.lower())\n            symlink = self.save_path / symlink\n            if symlink.exists():\n                old_ckpt = symlink.resolve()\n                symlink.unlink()\n                old_ckpt.unlink()\n            symlink.symlink_to(fname.name)\n\n        return fname\n\n    def update_scores(self, results):\n        """"""Updates score lists and current bests.""""""\n        for metric in results:\n            self.print(\'Validation {} -> {}\'.format(self.vctr, metric))\n            self.val_scores[metric.name].append(metric)\n            self.cur_bests[metric.name] = self.best_score(\n                self.val_scores[metric.name])\n\n    def get_last_eval_score(self):\n        return self.cur_bests[self.early_metric][-1].score\n\n    def save_models(self):\n        cur_bests = self.cur_bests.copy()\n\n        # Let\'s start with early-stopping metric\n        vctr, metric = cur_bests.pop(self.early_metric)\n        if vctr == self.vctr:\n            self.early_bad = 0\n            self.save_model(metric=metric, do_symlink=True)\n        else:\n            # Increment counter\n            self.early_bad += 1\n\n        # If requested, save all best metric snapshots\n        if self.save_best_metrics and cur_bests:\n            for (vctr, metric) in cur_bests.values():\n                if metric.name in self.eval_metrics and vctr == self.vctr:\n                    self.save_model(metric=metric, do_symlink=True)\n\n        self.print(\'Early stopping patience: {}\'.format(\n            self.patience - self.early_bad))\n'"
nmtpytorch/optimizer.py,10,"b'# -*- coding: utf-8 -*-\nimport logging\n\nimport torch.optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.nn.utils import clip_grad_norm_\n\nlogger = logging.getLogger(\'nmtpytorch\')\n\n# Setup optimizer (should always come after model.to())\n# iterable of dicts for per-param options where each dict\n# is {\'params\' : [p1, p2, p3...]}.update(generic optimizer args)\n# Example:\n# optim.SGD([\n        # {\'params\': model.base.parameters()},\n        # {\'params\': model.classifier.parameters(), \'lr\': 1e-3}\n    # ], lr=1e-2, momentum=0.9)\n\n\nclass Optimizer:\n    # Class dict to map lowercase identifiers to actual classes\n    methods = {\n        \'adadelta\':   torch.optim.Adadelta,\n        \'adagrad\':    torch.optim.Adagrad,\n        \'adam\':       torch.optim.Adam,\n        \'sgd\':        torch.optim.SGD,\n        \'asgd\':       torch.optim.ASGD,\n        \'rprop\':      torch.optim.Rprop,\n        \'rmsprop\':    torch.optim.RMSprop,\n    }\n\n    @staticmethod\n    def get_params(model):\n        """"""Returns all name, parameter pairs with requires_grad=True.""""""\n        return list(\n            filter(lambda p: p[1].requires_grad, model.named_parameters()))\n\n    def __init__(self, name, model, lr=0, momentum=0.0,\n                 nesterov=False, weight_decay=0, gclip=0,\n                 lr_decay=False, lr_decay_factor=0.1, lr_decay_mode=\'min\',\n                 lr_decay_patience=10, lr_decay_min=0.000001):\n        self.name = name\n        self.model = model\n        self.initial_lr = lr\n        self.lr_decay = lr_decay\n        self.lr_decay_factor = lr_decay_factor\n        self.lr_decay_mode = lr_decay_mode\n        self.lr_decay_patience = lr_decay_patience\n        self.lr_decay_min = lr_decay_min\n        self.momentum = momentum\n        self.nesterov = nesterov\n        self.weight_decay = weight_decay\n        self.gclip = gclip\n\n        self.optim_args = {}\n        # If an explicit lr given, pass it to torch optimizer\n        if self.initial_lr > 0:\n            self.optim_args[\'lr\'] = self.initial_lr\n\n        if self.name == \'sgd\':\n            self.optim_args[\'momentum\'] = self.momentum\n            self.optim_args[\'nesterov\'] = self.nesterov\n\n        # Get all parameters that require grads\n        self.named_params = self.get_params(self.model)\n\n        # Filter out names for gradient clipping\n        self.params = [param for (name, param) in self.named_params]\n\n        if self.weight_decay > 0:\n            weight_group = {\n                \'params\': [p for n, p in self.named_params if \'bias\' not in n],\n                \'weight_decay\': self.weight_decay,\n            }\n            bias_group = {\n                \'params\': [p for n, p in self.named_params if \'bias\' in n],\n            }\n            self.param_groups = [weight_group, bias_group]\n\n        else:\n            self.param_groups = [{\'params\': self.params}]\n\n        # Safety check\n        n_params = len(self.params)\n        for group in self.param_groups:\n            n_params -= len(group[\'params\'])\n        assert n_params == 0, ""Not all params are passed to the optimizer.""\n\n        # Create the actual optimizer\n        self.optim = self.methods[self.name](self.param_groups, **self.optim_args)\n\n        # Get final lr that will be used\n        self.initial_lr = self.optim.defaults[\'lr\']\n        self.cur_lr = self.initial_lr\n\n        # Assign shortcuts\n        self.zero_grad = self.optim.zero_grad\n\n        if self.gclip == 0:\n            self.step = self.optim.step\n        else:\n            self.step = self._step\n\n        if self.lr_decay:\n            self.scheduler = ReduceLROnPlateau(\n                self.optim, mode=self.lr_decay_mode,\n                factor=self.lr_decay_factor, patience=self.lr_decay_patience,\n                min_lr=self.lr_decay_min)\n        else:\n            self.scheduler = None\n\n    def _step(self, closure=None):\n        """"""Gradient clipping aware step().""""""\n        clip_grad_norm_(self.params, self.gclip)\n        self.optim.step(closure)\n\n    def lr_step(self, metric):\n        if self.scheduler is not None:\n            self.scheduler.step(metric)\n            if self.get_lr() != self.cur_lr:\n                self.cur_lr = self.get_lr()\n                logger.info(\'** Learning rate changed -> {}\'.format(self.cur_lr))\n                # Signal it back\n                return True\n        return False\n\n    def get_lr(self):\n        """"""Returns current lr for parameters.""""""\n        return self.optim.param_groups[0][\'lr\']\n\n    def state_dict(self):\n        return self.optim.state_dict()\n\n    def load_state_dict(self, state_dict):\n        self.optim.load_state_dict(state_dict)\n\n    def __repr__(self):\n        repr_ = ""Optimizer => {} (lr: {}, weight_decay: {}, g_clip: {}"".format(\n            self.name, self.initial_lr, self.weight_decay, self.gclip)\n        if self.name == \'sgd\':\n            repr_ += \', momentum: {}, nesterov: {}\'.format(\n                self.momentum, self.nesterov)\n        if self.lr_decay:\n            repr_ += \', lr_decay: (patience={}, factor={})\'.format(\n                self.lr_decay_patience, self.lr_decay_factor)\n        repr_ += \')\'\n        return repr_\n'"
nmtpytorch/tester.py,1,"b'# -*- coding: utf-8 -*-\nimport time\nimport logging\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\n\nfrom .utils.misc import load_pt_file, pbar\nfrom .utils.data import make_dataloader\nfrom .utils.device import DEVICE\n\nfrom . import models\nfrom .config import Options\n\nlogger = logging.getLogger(\'nmtpytorch\')\n\n\nclass Tester:\n    """"""Tester for models without beam-search.""""""\n\n    def __init__(self, **kwargs):\n        # Store attributes directly. See bin/nmtpy for their list.\n        self.__dict__.update(kwargs)\n\n        # How many models?\n        if len(self.models) > 1:\n            raise RuntimeError(""Test mode requires single model file."")\n\n        self.model_file = self.models[0]\n\n        # Disable gradient tracking\n        torch.set_grad_enabled(False)\n\n        data = load_pt_file(self.model_file)\n        weights, _, opts = data[\'model\'], data[\'history\'], data[\'opts\']\n\n        opts = Options.from_dict(opts, override_list=self.override)\n        instance = getattr(models, opts.train[\'model_type\'])(opts=opts)\n\n        if instance.supports_beam_search:\n            logger.info(""Model supports beam-search by the way."")\n\n        # Setup layers\n        instance.setup(is_train=False)\n        # Load weights\n        instance.load_state_dict(weights, strict=False)\n        # Move to device\n        instance.to(DEVICE)\n        # Switch to eval mode\n        instance.train(False)\n\n        self.instance = instance\n\n        # Can be a comma separated list of hardcoded test splits\n        if self.splits:\n            logger.info(\'Will process ""{}""\'.format(self.splits))\n            self.splits = self.splits.split(\',\')\n        elif self.source:\n            # Split into key:value\'s and parse into dict\n            input_dict = {}\n            logger.info(\'Will process input configuration:\')\n            for data_source in self.source.split(\',\'):\n                key, path = data_source.split(\':\', 1)\n                input_dict[key] = Path(path)\n                logger.info(\' {}: {}\'.format(key, input_dict[key]))\n            self.instance.opts.data[\'new_set\'] = input_dict\n            self.splits = [\'new\']\n\n    def extract_encodings(self, instance, split):\n        """"""(Experimental) feature extraction mode.""""""\n        dataset = instance.load_data(split, self.batch_size, mode=\'eval\')\n        loader = make_dataloader(dataset)\n        n_samples = len(dataset)\n        feats = []\n        ord_feats = []\n        logger.info(\'Starting extraction\')\n        start = time.time()\n        for batch in pbar(loader, unit=\'batch\'):\n            batch.device(DEVICE)\n            out, _ = list(instance.encode(batch).values())[0]\n            feats.append(out.data.cpu().transpose(0, 1))\n        for feat in feats:\n            # this is a batch\n            ord_feats.extend([f for f in feat])\n        idxs = zip(range(n_samples), loader.batch_sampler.orig_idxs)\n        idxs = sorted(idxs, key=lambda x: x[1])\n        ord_feats = [ord_feats[i[0]].numpy() for i in idxs]\n        np.save(\'{}_{}.encodings.npy\'.format(self.model_file, split), ord_feats)\n        up_time = time.time() - start\n        logger.info(\'Took {:.3f} seconds\'.format(up_time))\n\n    def test(self, instance, split):\n        dataset = instance.load_data(split, self.batch_size, mode=\'eval\')\n        loader = make_dataloader(dataset)\n\n        logger.info(\'Starting computation\')\n        start = time.time()\n        results = instance.test_performance(\n            loader,\n            dump_file=""{}.{}"".format(self.model_file, split))\n        up_time = time.time() - start\n        logger.info(\'Took {:.3f} seconds\'.format(up_time))\n        return results\n\n    def __call__(self):\n        for input_ in self.splits:\n            if self.mode == \'eval\':\n                results = self.test(self.instance, input_)\n                for res in results:\n                    print(\'  {}: {:.5f}\'.format(res.name, res.score))\n            elif self.mode == \'enc\':\n                self.extract_encodings(self.instance, input_)\n'"
nmtpytorch/translator.py,1,"b'# -*- coding: utf-8 -*-\nimport sys\nimport math\nimport time\nimport logging\nimport pickle as pkl\nfrom pathlib import Path\n\nimport torch\n\nfrom .utils.misc import load_pt_file\nfrom .utils.filterchain import FilterChain\nfrom .utils.data import make_dataloader\nfrom .utils.topology import Topology\nfrom .utils.device import DEVICE\n\nfrom . import models\nfrom .config import Options\n\nlogger = logging.getLogger(\'nmtpytorch\')\n\n\nclass Translator:\n    """"""A utility class to pack translation related features.""""""\n\n    def __init__(self, **kwargs):\n        # Store attributes directly. See bin/nmtpy for their list.\n        self.__dict__.update(kwargs)\n\n        for key, value in kwargs.items():\n            logger.info(\'-- {} -> {}\'.format(key, value))\n\n        # How many models?\n        self.n_models = len(self.models)\n\n        # Store each model instance\n        self.instances = []\n\n        # Disable gradient tracking\n        torch.set_grad_enabled(False)\n\n        # Create model instances and move them to device\n        for model_file in self.models:\n            data = load_pt_file(model_file)\n            weights, _, opts = data[\'model\'], data[\'history\'], data[\'opts\']\n            opts = Options.from_dict(opts, override_list=self.override)\n\n            # Create model instance\n            instance = getattr(models, opts.train[\'model_type\'])(opts=opts)\n\n            if not instance.supports_beam_search:\n                logger.error(\n                    ""Model does not support beam search. Try \'nmtpy test\'"")\n                sys.exit(1)\n\n            # Setup layers\n            instance.setup(is_train=False)\n            # Load weights\n            instance.load_state_dict(weights, strict=False)\n            # Move to device\n            instance.to(DEVICE)\n            # Switch to eval mode\n            instance.train(False)\n            self.instances.append(instance)\n            logger.info(instance)\n\n        try:\n            self.beam_func = getattr(self.instances[0], self.beam_func)\n        except AttributeError as ae:\n            logger.info(f\'Error: model does not have .{self.beam_func!r}()\')\n            sys.exit(1)\n\n        # Split the string\n        self.splits = self.splits.split(\',\')\n\n        # Do some sanity-check\n        self.sanity_check()\n\n        # Setup post-processing filters\n        eval_filters = self.instances[0].opts.train[\'eval_filters\']\n\n        if self.disable_filters or not eval_filters:\n            logger.info(\'Post-processing filters disabled.\')\n            self.filter = lambda s: s\n        else:\n            logger.info(\'Post-processing filters enabled.\')\n            self.filter = FilterChain(eval_filters)\n\n        # Can be a comma separated list of hardcoded test splits\n        logger.info(\'Will translate ""{}""\'.format(self.splits))\n        if self.source:\n            # We have to have single split name in this case\n            split_set = \'{}_set\'.format(self.splits[0])\n            input_dict = self.instances[0].opts.data.get(split_set, {})\n            logger.info(\'Input configuration:\')\n            for data_source in self.source.split(\',\'):\n                key, path = data_source.split(\':\', 1)\n                input_dict[key] = Path(path)\n                logger.info(\' {}: {}\'.format(key, input_dict[key]))\n            # Overwrite config\'s set name\n            self.instances[0].opts.data[split_set] = input_dict\n\n    def sanity_check(self):\n        if self.source and len(self.splits) > 1:\n            logger.info(\'You can only give one split name when -S is provided.\')\n            sys.exit(1)\n\n        eval_filters = set(\n            [\',\'.join(i.opts.train[\'eval_filters\']) for i in self.instances])\n        assert len(eval_filters) < 2, ""eval_filters differ between instances.""\n\n        if len(self.instances) > 1:\n            logger.info(\'Make sure you ensemble models with compatible vocabularies.\')\n\n        # check that all instances can perform the task\n        if self.task_id is not None:\n            task = Topology(self.task_id)\n            incl = [task.is_included_in(i.topology) for i in self.instances]\n            assert False not in incl, \\\n                \'Not all models are compatible with task ""{}""!\'.format(task.direction)\n\n    def translate(self, split):\n        """"""Returns the hypotheses generated by translating the given split\n        using the given model instance.\n\n        Arguments:\n            split(str): A test split defined in the .conf file before\n                training.\n\n        Returns:\n            list:\n                A list of optionally post-processed string hypotheses.\n        """"""\n\n        # Load data\n        dataset = self.instances[0].load_data(split, self.batch_size, mode=\'beam\')\n\n        # NOTE: Data iteration needs to be unique for ensembling\n        # otherwise it gets too complicated\n        loader = make_dataloader(dataset)\n\n        logger.info(\'Starting translation\')\n        start = time.time()\n\n        hyps = self.beam_func(self.instances, loader, task_id=self.task_id,\n                              beam_size=self.beam_size, max_len=self.max_len,\n                              lp_alpha=self.lp_alpha, suppress_unk=self.suppress_unk,\n                              n_best=self.n_best)\n        up_time = time.time() - start\n        logger.info(\'Took {:.3f} seconds, {} sent/sec\'.format(\n            up_time, math.floor(len(hyps) / up_time)))\n\n        if hasattr(self.instances[0].dec, \'persistent_dump\') and \\\n                self.instances[0].dec.persistent_dump:\n            with open(\'{}.dump\'.format(self.models[0]), \'wb\') as f:\n                pkl.dump(self.instances[0].dec.persistence, f)\n\n        return hyps\n\n    def dump(self, hyps, split):\n        """"""Writes the results into output.\n\n        Arguments:\n            hyps(list): A list of hypotheses.\n        """"""\n        suffix = """"\n        if self.lp_alpha > 0.:\n            suffix += "".lp_{:.1f}"".format(self.lp_alpha)\n        if self.suppress_unk:\n            suffix += "".no_unk""\n        suffix += "".beam{}"".format(self.beam_size)\n        if self.n_best:\n            suffix += "".nbest""\n        output = ""{}.{}{}"".format(self.output, split, suffix)\n\n        f = open(output, \'w\')\n        if self.n_best:\n            for idx, (cands, scores) in enumerate(hyps):\n                cands = self.filter(cands)\n                sorted_cs = sorted(\n                    zip(cands, scores), key=lambda x: x[1], reverse=True)\n                for cand, score in sorted_cs:\n                    # cands is a list of n sents, scores as well\n                    f.write(\'{} ||| {} ||| {:.5f}\\n\'.format(idx, cand, score))\n        else:\n            # Post-process strings if requested\n            hyps = self.filter(hyps)\n            for line in hyps:\n                f.write(line + \'\\n\')\n        f.close()\n\n    def __call__(self):\n        """"""Dumps the hypotheses for each of the requested split/file.""""""\n        for input_ in self.splits:\n            hyps = self.translate(input_)\n            self.dump(hyps, input_)\n'"
nmtpytorch/vocabulary.py,0,"b'# -*- coding: utf-8 -*-\nimport json\nimport pathlib\nimport logging\nfrom collections import OrderedDict\n\nlogger = logging.getLogger(\'nmtpytorch\')\n\n\nclass Vocabulary:\n    r""""""Vocabulary class for integer<->token mapping.\n\n    Arguments:\n        fname (str): The filename of the JSON vocabulary file created by\n            `nmtpy-build-vocab` script.\n        short_list (int, optional): If > 0, only the most frequent `short_list`\n            items are kept in the vocabulary.\n\n    Attributes:\n        vocab (pathlib.Path): A :class:`pathlib.Path` instance holding the\n            filepath of the vocabulary file.\n        short_list (int): Short-list threshold.\n        freqs (dict): A dictionary which maps vocabulary strings to their\n            normalized frequency across the training set.\n        counts (dict): A dictionary which maps vocabulary strings to their\n            occurrence counts across the training set.\n        n_tokens (int): The final number of elements in the vocabulary.\n        has_bos (bool): `True` if the vocabulary has <bos> token.\n        has_eos (bool): `True` if the vocabulary has <eos> token.\n        has_pad (bool): `True` if the vocabulary has <pad> token.\n        has_unk (bool): `True` if the vocabulary has <unk> token.\n\n    Note:\n        The final instance can be easily queried in both directions with\n        bracket notation using integers and strings.\n\n    Example:\n        >>> vocab = Vocabulary(\'train.vocab.en\')\n        >>> vocab[\'woman\']\n        23\n        >>> vocab[23]\n        \'woman\'\n\n    Returns:\n        A :class:`Vocabulary` instance.\n    """"""\n\n    TOKENS = {""<pad>"": 0, ""<bos>"": 1, ""<eos>"": 2, ""<unk>"": 3}\n\n    def __init__(self, fname, short_list=0):\n        self.vocab = pathlib.Path(fname).expanduser()\n        self.short_list = short_list\n        self._map = None\n        self._imap = None\n        self.freqs = None\n        self.counts = None\n        self._allmap = None\n        self.n_tokens = None\n\n        # Load file\n        with open(self.vocab) as f:\n            data = json.load(f)\n\n        if self.short_list > 0:\n            # Get a slice of most frequent `short_list` items\n            data = dict(list(data.items())[:self.short_list])\n\n        self._map = {k: int(v.split()[0]) for k, v in data.items()}\n        self.counts = {k: int(v.split()[1]) for k, v in data.items()}\n\n        total_count = sum(self.counts.values())\n        self.freqs = {k: v / total_count for k, v in self.counts.items()}\n\n        # Sanity check for placeholder tokens\n        for tok, idx in self.TOKENS.items():\n            if self._map.get(tok, -1) != idx:\n                logger.info(f\'{tok} not found in {self.vocab.name!r}\')\n                setattr(self, f\'has_{tok[1:-1]}\', False)\n            else:\n                setattr(self, f\'has_{tok[1:-1]}\', True)\n\n        # Set # of tokens\n        self.n_tokens = len(self._map)\n\n        # Invert dictionary\n        self._imap = OrderedDict([(v, k) for k, v in self._map.items()])\n\n        # Merge forward and backward lookups into single dict for convenience\n        self._allmap = OrderedDict()\n        self._allmap.update(self._map)\n        self._allmap.update(self._imap)\n\n        assert len(self._allmap) == (len(self._map) + len(self._imap)), \\\n            ""Merged vocabulary size is not equal to sum of both.""\n\n    def sent_to_idxs(self, line, explicit_bos=False, explicit_eos=True):\n        """"""Convert from list of strings to list of token indices.""""""\n        tidxs = []\n\n        if explicit_bos and self.has_bos:\n            tidxs.append(self.TOKENS[""<bos>""])\n\n        if self.has_unk:\n            for tok in line.split():\n                tidxs.append(self._map.get(tok, self.TOKENS[""<unk>""]))\n        else:\n            # Remove unknown tokens from the words\n            for tok in line.split():\n                try:\n                    tidxs.append(self._map[tok])\n                except KeyError as _:\n                    # make this verbose and repetitive as this should be\n                    # used cautiously only for some specific models\n                    logger.info(\'No <unk> token, removing word from sentence\')\n\n        if explicit_eos and self.has_eos:\n            tidxs.append(self.TOKENS[""<eos>""])\n\n        return tidxs\n\n    def idxs_to_sent(self, idxs, debug=False):\n        r""""""Converts list of integers to string representation.\n\n        Arguments:\n            idxs (list): Python list of integers as previously mapped from\n                string tokens by this instance.\n            debug (bool, optional): If `True`, the string representation\n                will go beyond and include the end-of-sentence token as well.\n\n        Returns:\n            A whitespace separated string representing the given list of integers.\n\n        """"""\n        result = []\n        for idx in idxs:\n            if not debug and self.has_eos and idx == self.TOKENS[""<eos>""]:\n                break\n            result.append(self._imap.get(idx, self.TOKENS[""<unk>""]))\n\n        return "" "".join(result)\n\n    def list_of_idxs_to_sents(self, lidxs):\n        r""""""Converts list of list of integers to string representations. This is\n        handy for batched conversion after beam search for example.\n\n        Arguments:\n            lidxs(list): A list containing multiple lists of integers as\n                previously mapped from string tokens by this instance.\n\n        Returns:\n            A list of whitespace separated strings representing the given input.\n\n        """"""\n        results = []\n        unk = self.TOKENS[""<unk>""]\n        for idxs in lidxs:\n            result = []\n            for idx in idxs:\n                if idx == self.TOKENS[""<eos>""]:\n                    break\n                result.append(self._imap.get(idx, unk))\n            results.append("" "".join(result))\n        return results\n\n    def __getitem__(self, key):\n        return self._allmap[key]\n\n    def __len__(self):\n        return len(self._map)\n\n    def __repr__(self):\n        return f""Vocabulary of {self.n_tokens} items ({self.vocab.name!r})""\n'"
scripts/dump-attention.py,4,"b'#!/usr/bin/env python\nimport argparse\nimport pickle as pkl\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\nimport tqdm\n\nfrom nmtpytorch.translator import Translator\nfrom nmtpytorch.utils.data import make_dataloader\n\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(\n        prog=\'nmtpy-dump-attention\',\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        description=""generate attention pkl"",\n        argument_default=argparse.SUPPRESS)\n\n    parser.add_argument(\'-m\', \'--model\', type=str, required=True,\n                        help="".ckpt model file"")\n    parser.add_argument(\'-s\', \'--split\', type=str,\n                       help=\'test_set name given as in configuration file\')\n    parser.add_argument(\'-o\', \'--output\', type=str,\n                       help=\'output file name.\')\n\n    args = parser.parse_args()\n    translator = Translator(models=[args.model], splits=args.split,\n                            source=None, disable_filters=True, override=None,\n                            task_id=None)\n\n    model = translator.instances[0]\n\n    dataset = model.load_data(args.split, 64, mode=\'beam\')\n    loader = make_dataloader(dataset)\n    data = []\n\n    torch.set_grad_enabled(False)\n\n    # Greedy search\n    for batch in tqdm.tqdm(loader, unit=\'batch\'):\n        # Visual attention (may not be available)\n        img_att = [[] for i in range(batch.size)]\n\n        # Textual attention\n        main_att = [[] for i in range(batch.size)]\n\n        # Hierarchical attention\n        hie_att = [[] for i in range(batch.size)]\n\n        hyps = [[] for i in range(batch.size)]\n\n        fini = torch.zeros(batch.size, dtype=torch.long)\n        ctx_dict = model.encode(batch)\n\n        # Get initial hidden state\n        h_t = model.dec.f_init(ctx_dict)\n\n        y_t = model.get_bos(batch.size)\n\n        # Iterate for 100 timesteps\n        for t in range(100):\n            logp, h_t = model.dec.f_next(ctx_dict, model.dec.get_emb(y_t, t).squeeze(1), h_t)\n\n            # text attention\n            tatt = model.dec.history[\'alpha_txt\'][-1].data.clone().numpy()\n            iatt, hatt = None, None\n\n            # If decoder has .img_alpha_t\n            if hasattr(model.dec, \'img_alpha_t\'):\n                iatt = model.dec.img_alpha_t.data.clone().numpy()\n\n            if hasattr(model.dec, \'h_att\'):\n                hatt = model.dec.h_att.data.clone().numpy()\n\n            top_scores, y_t = logp.data.topk(1, largest=True)\n            hyp = y_t.numpy().tolist()\n            for idx, w in enumerate(hyp):\n                if 2 not in hyps[idx]:\n                    hyps[idx].append(w[0])\n                    main_att[idx].append(tatt[:, idx])\n                    if iatt is None:\n                        img_att[idx].append(None)\n                    else:\n                        img_att[idx].append(iatt[:, idx])\n\n                    if hatt is None:\n                        hie_att[idx].append(None)\n                    else:\n                        hie_att[idx].append(hatt[:, idx])\n\n            # Did we finish? (2 == <eos>)\n            fini = fini | y_t.eq(2).squeeze().long()\n            if fini.sum() == batch.size:\n                break\n\n        for h, sa, ia, ha in zip(hyps, main_att, img_att, hie_att):\n            d = {\n                \'hyp\': model.trg_vocab.idxs_to_sent(h),\n                \'pri_att\': np.array(sa),\n                \'sec_att\': np.array(ia) if ia is not None else None,\n                \'hie_att\': np.array(ha) if ha is not None else None,\n            }\n            data.append(d)\n\n    # Put into correct order\n    data = [data[i] for i, j in sorted(\n        enumerate(loader.batch_sampler.orig_idxs), key=lambda k: k[1])]\n\n    src_lines = []\n    with open(model.opts.data[\'{}_set\'.format(args.split)][model.sl]) as sf:\n        for line in sf:\n            src_lines.append(line.strip())\n\n    for d, line in zip(data, src_lines):\n        d[\'src\'] = line\n\n    with open(args.output, \'wb\') as f:\n        pkl.dump(data, f)\n'"
nmtpytorch/cocoeval/__init__.py,0,"b""__author__ = 'tylin'\nfrom .bleu.bleu import Bleu\nfrom .cider.cider import Cider\nfrom .rouge.rouge import Rouge\nfrom .meteor.meteor import Meteor\n"""
nmtpytorch/datasets/__init__.py,0,"b""# First the basic types\nfrom .npy import NumpyDataset\nfrom .kaldi import KaldiDataset\nfrom .imagefolder import ImageFolderDataset\nfrom .text import TextDataset\nfrom .numpy_sequence import NumpySequenceDataset\nfrom .label import LabelDataset\nfrom .shelve import ShelveDataset\n\n# Second the selector function\ndef get_dataset(type_):\n    return {\n        'numpy': NumpyDataset,\n        'numpysequence': NumpySequenceDataset,\n        'kaldi': KaldiDataset,\n        'imagefolder': ImageFolderDataset,\n        'text': TextDataset,\n        'label': LabelDataset,\n        'shelve': ShelveDataset,\n    }[type_.lower()]\n\n\n# Should always be at the end\nfrom .multimodal import MultimodalDataset\n"""
nmtpytorch/datasets/collate.py,0,"b'# -*- coding: utf-8 -*-\n\n# This will eventually disappear as this only provides .size\n# which can be inferred if we guarantee that batch_dim is always at\n# a given position regardless of input/output feature/tensor types.\n\n\nclass Batch(dict):\n    """"""A custom dictionary representing a batch.""""""\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        dim1s = set([x.size(1) for x in self.values()])\n        assert len(dim1s) == 1, \\\n            ""Incompatible batch dimension (1) between modalities.""\n        self.size = dim1s.pop()\n\n    def device(self, device):\n        self.update({k: v.to(device) for k, v in self.items()})\n\n    def __repr__(self):\n        s = ""Batch(size={})\\n"".format(self.size)\n        for data_source, tensor in self.items():\n            s += ""  {:10s} -> {} - {}\\n"".format(\n                str(data_source), tensor.shape, tensor.device)\n        return s\n\n\ndef get_collate(data_sources):\n    """"""Returns a special collate_fn which will view the underlying data\n    in terms of the given DataSource keys.""""""\n\n    def collate_fn(batch):\n        return Batch(\n            {ds: ds.torchify([elem[ds] for elem in batch]) for ds in data_sources},\n        )\n\n    return collate_fn\n'"
nmtpytorch/datasets/imagefolder.py,2,"b'# -*- coding: utf-8 -*-\nfrom functools import lru_cache\nfrom pathlib import Path\n\nfrom PIL import Image\n\nimport torch\nfrom torch.utils import data\nfrom torchvision import transforms\n\n\nclass ImageFolderDataset(data.Dataset):\n    """"""A variant of torchvision.datasets.ImageFolder which drops support for\n    target loading, i.e. this only loads images not attached to any other\n    label.\n\n    This class also makes use of ``lru_cache`` to cache an image file once\n    opened to avoid repetitive disk access.\n\n    Arguments:\n        root (str): The root folder that contains the images and index.txt\n        resize (int, optional): An optional integer to be given to\n            ``torchvision.transforms.Resize``. Default: ``None``.\n        crop (int, optional): An optional integer to be given to\n            ``torchvision.transforms.CenterCrop``. Default: ``None``.\n        replicate(int, optional): Replicate the image names ``replicate``\n            times in order to process the same image ``replicate`` times\n            if ``replicate`` sentences are available during training time.\n        warmup(bool, optional): If ``True``, the images will be read once\n            at the beginning to fill the cache.\n    """"""\n    def __init__(self, root, resize=None, crop=None,\n                 replicate=1, warmup=False, **kwargs):\n        self.root = Path(root).expanduser().resolve()\n        self.replicate = replicate\n\n        # Image list in dataset order\n        self.index = self.root / \'index.txt\'\n\n        _transforms = []\n        if resize is not None:\n            _transforms.append(transforms.Resize(resize))\n        if crop is not None:\n            _transforms.append(transforms.CenterCrop(crop))\n        _transforms.append(transforms.ToTensor())\n        _transforms.append(\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225]))\n        self.transform = transforms.Compose(_transforms)\n\n        if not self.index.exists():\n            raise(RuntimeError(\n                ""index.txt does not exist in {}"".format(self.root)))\n\n        self.image_files = []\n        with self.index.open() as f:\n            for fname in f:\n                fname = self.root / fname.strip()\n                assert fname.exists(), ""{} does not exist."".format(fname)\n                self.image_files.append(str(fname))\n\n        # Setup reader\n        self.read_image = lru_cache(maxsize=self.__len__())(self._read_image)\n\n        if warmup:\n            for idx in range(self.__len__()):\n                self[idx]\n\n        # Replicate the list if requested\n        self.image_files = self.image_files * self.replicate\n\n    def _read_image(self, fname):\n        with open(fname, \'rb\') as f:\n            img = Image.open(f).convert(\'RGB\')\n            return self.transform(img)\n\n    @staticmethod\n    def to_torch(batch, **kwargs):\n        return torch.stack(batch)\n\n    def __getitem__(self, idx):\n        return self.read_image(self.image_files[idx])\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __repr__(self):\n        s = ""{}(replicate={}) ({} samples)\\n"".format(\n            self.__class__.__name__, self.replicate, self.__len__())\n        s += "" {}\\n"".format(self.root)\n        if self.transform:\n            s += \' Transforms: {}\\n\'.format(\n                self.transform.__repr__().replace(\'\\n\', \'\\n\' + \' \'))\n        return s\n'"
nmtpytorch/datasets/kaldi.py,3,"b'# -*- coding: utf-8 -*-\nfrom pathlib import Path\nfrom tqdm import tqdm\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom ..utils.kaldi import readMatrixShape, readMatrixByOffset\n\n# TODO\n# ----\n# an lru_cache() decorated version of readMatrixByOffset() will make sure that\n# all the training data is cached into memory after 1 epoch.\n\n\nclass KaldiDataset(Dataset):\n    """"""A PyTorch dataset for Kaldi .scp/ark.\n\n    Arguments:\n        fname (str or Path): A string or ``pathlib.Path`` object for\n            a folder that contains ``feats_local.scp`` and optionally a ``segments.len``\n            file containing segment lengths.\n    """"""\n\n    def __init__(self, fname, **kwargs):\n        self.data = []\n        self.lengths = []\n        self.root = Path(fname)\n        self.scp_path = self.root / \'feats_local.scp\'\n        self.len_path = self.root / \'segments.len\'\n\n        if not self.scp_path.exists():\n            raise RuntimeError(\'{} does not exist.\'.format(self.scp_path))\n\n        if self.len_path.exists():\n            read_lengths = False\n            # Read lengths file\n            with open(self.len_path) as f:\n                for line in f:\n                    self.lengths.append(int(line.strip()))\n        else:\n            # Read them below (this is slow)\n            read_lengths = True\n\n        with open(self.scp_path) as scp_input_file:\n            for line in tqdm(scp_input_file, unit=\'segments\'):\n                uttid, pointer = line.strip().split()\n                arkfile, offset = pointer.rsplit(\':\', 1)\n                offset = int(offset)\n                self.data.append((arkfile, offset))\n                if read_lengths:\n                    with open(arkfile, ""rb"") as g:\n                        g.seek(offset)\n                        feat_len = readMatrixShape(g)[0]\n\n                    self.lengths.append(feat_len)\n\n        # Set dataset size\n        self.size = len(self.data)\n\n        if self.size != len(self.lengths):\n            raise RuntimeError(""Dataset size and lengths size does not match."")\n\n    @staticmethod\n    def to_torch(batch, **kwargs):\n        return pad_sequence(\n            [torch.FloatTensor(x) for x in batch], batch_first=False)\n\n    def __getitem__(self, idx):\n        """"""Read segment features from the actual .ark file.""""""\n        return readMatrixByOffset(*self.data[idx])\n\n    def __len__(self):\n        return self.size\n\n    def __repr__(self):\n        s = ""{} \'{}\' ({} samples)\\n"".format(\n            self.__class__.__name__, self.scp_path.name, self.__len__())\n        return s\n'"
nmtpytorch/datasets/label.py,2,"b'# -*- coding: utf-8 -*-\nfrom pathlib import Path\n\nimport torch\nfrom torch.utils.data import Dataset\n\nfrom ..utils.data import read_sentences\n\n\nclass LabelDataset(Dataset):\n    r""""""A PyTorch dataset that returns a single integer representing a category.\n\n    Arguments:\n        fname (str or Path): A string or ``pathlib.Path`` object giving\n            space delimited attributes per sentence.\n        vocab (Vocabulary): A ``Vocabulary`` instance for the labels.\n    """"""\n\n    def __init__(self, fname, vocab, **kwargs):\n        self.path = Path(fname)\n        self.vocab = vocab\n\n        # Detect glob patterns\n        self.fnames = sorted(self.path.parent.glob(self.path.name))\n\n        if len(self.fnames) == 0:\n            raise RuntimeError(\'{} does not exist.\'.format(self.path))\n        elif len(self.fnames) > 1:\n            raise RuntimeError(""Multiple source files not supported."")\n\n        # Read the label strings and map them to vocabulary\n        self.data, _ = read_sentences(\n            self.fnames[0], self.vocab, eos=False, bos=False)\n\n        # number of possible classes is the vocab size\n        self.n_classes = len(self.vocab)\n\n        # Dataset size\n        self.size = len(self.data)\n\n    @staticmethod\n    def to_torch(batch, **kwargs):\n        return torch.LongTensor(batch).t()\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\n    def __len__(self):\n        return self.size\n\n    def __repr__(self):\n        s = ""{} \'{}\' ({} samples)\\n"".format(\n            self.__class__.__name__, self.fnames[0].name, self.__len__())\n        return s\n'"
nmtpytorch/datasets/multimodal.py,2,"b'# -*- coding: utf-8 -*-\nimport logging\nfrom torch.utils.data import Dataset\nfrom torch.utils.data.sampler import BatchSampler, SequentialSampler, RandomSampler\n\nfrom . import get_dataset\nfrom .collate import get_collate\nfrom ..samplers import get_sampler\n\nlogger = logging.getLogger(\'nmtpytorch\')\n\n\nclass MultimodalDataset(Dataset):\n    """"""Returns a Dataset for parallel multimodal corpora\n\n    Arguments:\n        data(dict): [data] section\'s relevant split dictionary\n        mode(str): One of train/eval/beam.\n        batch_size(int): Batch size.\n        vocabs(dict): dictionary mapping keys to Vocabulary() objects\n        topology(Topology): A topology object.\n        bucket_by(str): String identifier of the modality which will define how\n            the batches will be bucketed, i.e. sort key. If `None`, no\n            bucketing will be performed but the layers and models should\n            support packing/padding/masking for this to work.\n        max_len(int, optional): Maximum sequence length for ``bucket_by``\n            modality to reject batches with long sequences. Does not have an effect\n            if mode != \'train\'.\n        bucket_order (str, optional): ``ascending`` or ``descending`` to\n            perform length-based curriculum learning. Default is ``None``\n            which shuffles bucket order. Does not have an effect if mode != \'train\'.\n        sampler_type(str, optional): \'bucket\' or \'approximate\' (Default: \'bucket\')\n        kwargs (dict): Additional arguments to pass to the dataset constructors.\n    """"""\n    def __init__(self, data, mode, batch_size, vocabs, topology,\n                 bucket_by, bucket_order=None, max_len=None,\n                 sampler_type=\'bucket\', **kwargs):\n        self.datasets = {}\n        self.mode = mode\n        self.vocabs = vocabs\n        self.batch_size = batch_size\n        self.topology = topology\n        self.bucket_by = bucket_by\n        self.sampler_type = sampler_type\n\n        # Disable filtering if not training\n        self.max_len = max_len if self.mode == \'train\' else None\n\n        # This is only useful for training\n        self.bucket_order = bucket_order if self.mode == \'train\' else None\n\n        # Collect dataset sizes\n        self.size_dict = {}\n\n        # For old models to work, set it to the first source\n        if self.bucket_by is None:\n            logger.info(\n                \'WARNING: Bucketing disabled. It is up to the model \'\n                \'to take care of packing/padding/masking if any.\')\n\n        for key, ds in self.topology.all.items():\n            if self.mode == \'beam\' and ds.trg:\n                # Skip target streams for beam-search\n                logger.info(""Skipping \'{}\' as target"".format(key))\n                continue\n\n            try:\n                # Get the relevant dataset class\n                dataset_constructor = get_dataset(ds._type)\n            except KeyError as ke:\n                logger.info(""ERROR: Unknown dataset type \'{}\'"".format(ds._type))\n\n            logger.info(""Initializing dataset for \'{}\'..."".format(ds))\n            if key in data:\n                # Force <eos> for target side, relax it for source side\n                kwargs[\'eos\'] = kwargs.get(\'eos\', True) or ds.trg\n                # Construct the dataset\n                self.datasets[ds] = dataset_constructor(\n                    fname=data[key],\n                    vocab=vocabs.get(key, None), bos=ds.trg, **kwargs)\n                self.size_dict[ds] = len(self.datasets[ds])\n            else:\n                logger.info(""  Skipping as \'{}\' not defined. This may be an issue."".format(key))\n\n        # Set dataset size\n        if len(set(self.size_dict.values())) > 1:\n            raise RuntimeError(""Underlying datasets are not parallel!"")\n        else:\n            self.size = list(self.size_dict.values())[0]\n\n        # Set list of available datasets\n        self.keys = list(self.datasets.keys())\n\n        # Get collator\n        self.collate_fn = get_collate(self.keys)\n\n        if self.bucket_by in self.datasets:\n            self.sort_lens = self.datasets[self.bucket_by].lengths\n            self.sampler = get_sampler(self.sampler_type)(\n                batch_size=self.batch_size,\n                sort_lens=self.sort_lens,\n                max_len=self.max_len,\n                store_indices=self.mode != \'train\',\n                order=self.bucket_order)\n        else:\n            # bucket_by only valid for training\n            if self.bucket_by:\n                self.bucket_by = None\n                logger.info(\'Disabling bucketing for data loader.\')\n            # No modality provided to bucket sequential batches\n            # Used for beam-search in image->text tasks\n            if self.mode == \'beam\':\n                sampler = SequentialSampler(self)\n                self.sampler_type = \'sequential\'\n            else:\n                sampler = RandomSampler(self)\n                self.sampler_type = \'random\'\n            self.sampler = BatchSampler(\n                sampler, batch_size=self.batch_size, drop_last=False)\n\n        # Set some metadata\n        self.n_sources = len([k for k in self.keys if k.src])\n        self.n_targets = len([k for k in self.keys if k.trg])\n\n    def __getitem__(self, idx):\n        return {k: self.datasets[k][idx] for k in self.keys}\n\n    def __len__(self):\n        return self.size\n\n    def __repr__(self):\n        s = ""{} - ({} source(s) / {} target(s))\\n"".format(\n            self.__class__.__name__, self.n_sources, self.n_targets)\n        s += ""  Sampler type: {}, bucket_by: {}\\n"".format(\n            self.sampler_type, self.bucket_by)\n\n        if self.n_sources > 0:\n            s += ""  Sources:\\n""\n            for name in filter(lambda k: k.src, self.keys):\n                dstr = self.datasets[name].__repr__()\n                s += \'    --> \' + dstr\n        if self.n_targets > 0:\n            s += ""  Targets:\\n""\n            for name in filter(lambda k: k.trg, self.keys):\n                dstr = self.datasets[name].__repr__()\n                s += \'    --> \' + dstr\n        return s\n'"
nmtpytorch/datasets/npy.py,2,"b'# -*- coding: utf-8 -*-\nfrom pathlib import Path\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\n\n\nclass NumpyDataset(Dataset):\n    r""""""A PyTorch dataset for Numpy .npy/npz serialized tensor files. The\n    serialized tensor\'s first dimension should be the batch dimension.\n\n    Arguments:\n        fname (str or Path): A string or ``pathlib.Path`` object for\n            the relevant numpy file.\n        key (str, optional): If `fname` is `.npz` file, its relevant `key`\n            will be fetched from the serialized object.\n        order_file (str, None): If given, will be used to map sample indices\n            to tensors using this list. Useful for tiled or repeated\n            experiments.\n        revert (bool, optional): If `True`, the data order will be reverted\n            for adversarial/incongruent experiments during test-time.\n    """"""\n\n    def __init__(self, fname, key=None, order_file=None, revert=False, **kwargs):\n        self.path = Path(fname)\n        if not self.path.exists():\n            raise RuntimeError(\'{} does not exist.\'.format(self.path))\n\n        if self.path.suffix == \'.npy\':\n            self.data = np.load(self.path)\n        elif self.path.suffix == \'.npz\':\n            assert key, ""A key should be provided for .npz files.""\n            self.data = np.load(self.path)[key]\n\n        if order_file:\n            with open(order_file) as orf:\n                self.order = [int(x) for x in orf.read().strip().split(\'\\n\')]\n        else:\n            self.order = list(range(self.data.shape[0]))\n\n        if revert:\n            self.order = self.order[::-1]\n\n        # Dataset size\n        self.size = len(self.order)\n\n    @staticmethod\n    def to_torch(batch, **kwargs):\n        # NOTE: Assumes x.shape == (n, *)\n        x = torch.from_numpy(np.array(batch, dtype=\'float32\'))\n        # Convert it to (t(=1 if fixed features), n, c)\n        # By default we flatten h*w to first dim for interoperability\n        # Models should further reshape the tensor for their needs\n        return x.view(*x.size()[:2], -1).permute(2, 0, 1)\n\n    def __getitem__(self, idx):\n        return self.data[self.order[idx]]\n\n    def __len__(self):\n        return self.size\n\n    def __repr__(self):\n        s = ""{} \'{}\' ({} samples)\\n"".format(\n            self.__class__.__name__, self.path.name, self.__len__())\n        return s\n'"
nmtpytorch/datasets/numpy_sequence.py,2,"b'# -*- coding: utf-8 -*-\nfrom functools import lru_cache\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nfrom ..utils.misc import pbar\n\n\nclass NumpySequenceDataset(Dataset):\n    """"""Read a sequence of numpy arrays.\n\n    Arguments:\n        fname (str or Path): Path to a list of paths to Numpy `.npy` files\n            where each file contains an array with shape `(n_features, feat_dim)`.\n            If the lines are in `<path>:<len>` format, additional length\n            information will be used for bucketing. If the file itself is\n            a `.npy` file, it will be treated as an array of numpy objects.\n            For cases where all features are the same length, you should use\n            `NumpyDataset`.\n        cache (bool, optional): Whether the accessed files will be cached\n            in memory or not.\n    """"""\n\n    def __init__(self, fname, cache=False, **kwargs):\n        self.fname = fname\n        self.data = []\n        self.lengths = []\n        self.has_lengths = False\n        self.cache = cache\n\n        if not self.fname:\n            raise RuntimeError(\'{} does not exist.\'.format(self.fname))\n\n        if str(self.fname).endswith(\'.npy\'):\n            # Loads the whole dataset at once\n            self.data = np.load(self.fname)\n            self.lengths = [x.shape[0] for x in self.data]\n            self.has_lengths = True\n            self._read = lambda x: x\n        else:\n            with open(self.fname) as f_list:\n                # Detect file format and seek back\n                self.has_lengths = \':\' in f_list.readline()\n                f_list.seek(0)\n                for line in pbar(f_list, unit=\'sents\'):\n                    if self.has_lengths:\n                        path, length = line.strip().split(\':\')\n                        self.lengths.append(int(length))\n                    else:\n                        path = line.strip()\n                    self.data.append(path)\n\n            if self.cache:\n                self._read = lru_cache(maxsize=len(self.data))(self._read_tensor)\n            else:\n                self._read = self._read_tensor\n\n        # Set dataset size\n        self.size = len(self.data)\n\n    def _read_tensor(self, fname):\n        """"""Reads the .npy file.""""""\n        return np.load(fname)\n\n    def __getitem__(self, idx):\n        # Each item is (t, feat_dim)\n        return self._read(self.data[idx])\n\n    @staticmethod\n    def to_torch(batch, **kwargs):\n        # List of (t, feat_dim)\n        max_len = max(x.shape[0] for x in batch)\n        width = batch[0].shape[1]\n        padded = [np.zeros((max_len, width)) for _ in batch]\n        for pad, x in zip(padded, batch):\n            pad[:x.shape[0]] = x\n        # padded is (n_samples, t, feat_dim)\n        # return (n, f, t) for compatibility with the other input sources\n        return torch.from_numpy(\n            np.array(padded, dtype=\'float32\')).transpose(1, 2)\n\n    def __len__(self):\n        return self.size\n\n    def __repr__(self):\n        s = ""{} (has_lengths={}) ({} samples)\\n"".format(\n            self.__class__.__name__, self.has_lengths, self.__len__())\n        s += "" {}\\n"".format(self.fname)\n        return s\n'"
nmtpytorch/datasets/shelve.py,1,"b'# -*- coding: utf-8 -*-\nimport shelve\nfrom pathlib import Path\n\nfrom sklearn import preprocessing\nimport numpy as np\nfrom torch.utils.data import Dataset\n\nfrom ..utils.data import pad_video_sequence\n\n\nclass ShelveDataset(Dataset):\n    r""""""A PyTorch dataset for Shelve serialized tensor files. The\n    serialized tensor\'s first dimension should be the batch dimension.\n\n    Arguments:\n        fname (str or Path): A string or ``pathlib.Path`` object for\n            the relevant .shelve file.\n        norm_and_scale: True or False: Should we normalise and scale\n            the image features?\n    """"""\n\n    def __init__(self, fname, key=None, norm_and_scale=False, **kwargs):\n        self.path = Path(\'{}.dat\'.format(fname))\n        if not self.path.exists():\n            raise RuntimeError(\'{} does not exist.\'.format(self.path))\n\n        self.data = shelve.open(str(fname.resolve()))\n        self.norm_and_scale = norm_and_scale\n\n        # Dataset size\n        self.size = len(self.data)\n\n        # Stores the lengths of the input video sequences to enable bucketing\n        self.lengths = self.read_sequence_lengths()\n\n    def read_sequence_lengths(self):\n        \'\'\'Returns an array with the number of video feature vectors\n        stored for each image. TODO: This is expensive and a slow\n        way to start the process.\'\'\'\n        lengths = []\n        for x in self.data:\n            lengths.append(len(self.data[str(x)]))\n        return lengths\n\n    @staticmethod\n    def to_torch(batch, **kwargs):\n        \'\'\' Pad the video sequence, if necessary.\n        Transposes the video sequence to conform to the RNN expected inputs:\n            n_samples x timesteps x feats -> timesteps x n_samples x feats\n        \'\'\'\n        batch = pad_video_sequence(batch)\n        batch = batch.transpose(0, 1)\n        return batch\n\n    def __getitem__(self, idx):\n        if self.norm_and_scale:\n            feats = self.data[str(idx)]\n            feats = preprocessing.normalize(feats)\n            return feats\n        else:\n            return np.array(self.data[str(idx)])\n\n    def __len__(self):\n        return self.size\n\n    def __repr__(self):\n        s = ""{} \'{}\' ({} samples)\\n"".format(\n            self.__class__.__name__, self.path.name, self.__len__())\n        return s\n'"
nmtpytorch/datasets/text.py,3,"b'# -*- coding: utf-8 -*-\nimport logging\nfrom pathlib import Path\n\nimport torch\n\nfrom torch.utils.data import Dataset\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom ..utils.data import read_sentences\n\nlogger = logging.getLogger(\'nmtpytorch\')\n\n\nclass TextDataset(Dataset):\n    r""""""A PyTorch dataset for sentences.\n\n    Arguments:\n        fname (str or Path): A string or ``pathlib.Path`` object giving\n            the corpus.\n        vocab (Vocabulary): A ``Vocabulary`` instance for the given corpus.\n        bos (bool, optional): If ``True``, a special beginning-of-sentence\n            ""<bos>"" marker will be prepended to sentences.\n    """"""\n\n    def __init__(self, fname, vocab, bos=False, eos=True, **kwargs):\n        self.path = Path(fname)\n        self.vocab = vocab\n        self.bos = bos\n        self.eos = eos\n\n        # Detect glob patterns\n        self.fnames = sorted(self.path.parent.glob(self.path.name))\n\n        if len(self.fnames) == 0:\n            raise RuntimeError(\'{} does not exist.\'.format(self.path))\n        elif len(self.fnames) > 1:\n            logger.info(\'Multiple files found, using first: {}\'.format(self.fnames[0]))\n\n        # Read the sentences and map them to vocabulary\n        self.data, self.lengths = read_sentences(\n            self.fnames[0], self.vocab, bos=self.bos, eos=self.eos)\n\n        # Dataset size\n        self.size = len(self.data)\n\n    @staticmethod\n    def to_torch(batch, **kwargs):\n        return pad_sequence(\n            [torch.tensor(b, dtype=torch.long) for b in batch], batch_first=False)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\n    def __len__(self):\n        return self.size\n\n    def __repr__(self):\n        s = ""{} \'{}\' ({} sentences)"".format(\n            self.__class__.__name__, self.fnames[0].name, self.__len__())\n        return s\n'"
nmtpytorch/layers/__init__.py,1,b'try:\n    import apex\n    LayerNorm = apex.normalization.FusedLayerNorm\nexcept ImportError as ie:\n    import torch\n    LayerNorm = torch.nn.LayerNorm\n\n# Basic layers\nfrom .ff import FF\nfrom .fusion import Fusion\nfrom .flatten import Flatten\nfrom .argselect import ArgSelect\nfrom .pool import Pool\nfrom .seq_conv import SequenceConvolution\nfrom .rnninit import RNNInitializer\nfrom .max_margin import MaxMargin\n\n# Embedding variants\nfrom .embedding import *\n\n# Attention layers\nfrom .attention import *\n\n# Encoder layers\nfrom .encoders import *\n\n# Decoder layers\nfrom .decoders import *\n'
nmtpytorch/layers/argselect.py,1,"b'import torch\n\n\nclass ArgSelect(torch.nn.Module):\n    """"""Dummy layer that picks one of the returned values from mostly RNN-type\n    `nn.Module` layers.""""""\n    def __init__(self, index):\n        super().__init__()\n        self.index = index\n\n    def forward(self, x):\n        return x[self.index]\n\n    def __repr__(self):\n        return ""ArgSelect(index={})"".format(self.index)\n'"
nmtpytorch/layers/ff.py,3,"b'# -*- coding: utf-8 -*-\nimport math\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom ..utils.nn import get_activation_fn\n\n\nclass FF(nn.Module):\n    """"""A smart feedforward layer with activation support.\n\n    Arguments:\n        in_features(int): Input dimensionality.\n        out_features(int): Output dimensionality.\n        bias(bool, optional): Enable/disable bias for the layer. (Default: True)\n        bias_zero(bool, optional): Start with a 0-vector bias. (Default: True)\n        activ(str, optional): A string like \'tanh\' or \'relu\' to define the\n            non-linearity type. `None` or `\'linear\'` is a linear layer (default).\n    """"""\n\n    def __init__(self, in_features, out_features, bias=True,\n                 bias_zero=True, activ=None):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.use_bias = bias\n        self.bias_zero = bias_zero\n        self.activ_type = activ\n        if self.activ_type in (None, \'linear\'):\n            self.activ_type = \'linear\'\n        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.activ = get_activation_fn(activ)\n\n        if self.use_bias:\n            self.bias = nn.Parameter(torch.Tensor(out_features))\n        else:\n            self.register_parameter(\'bias\', None)\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.weight.size(1))\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.use_bias:\n            if self.bias_zero:\n                self.bias.data.zero_()\n            else:\n                self.bias.data.uniform_(-stdv, stdv)\n\n    def forward(self, input):\n        return self.activ(F.linear(input, self.weight, self.bias))\n\n    def __repr__(self):\n        repr_ = self.__class__.__name__ + \'(\' \\\n            + \'in_features=\' + str(self.in_features) \\\n            + \', out_features=\' + str(self.out_features) \\\n            + \', activ=\' + str(self.activ_type) \\\n            + \', bias=\' + str(self.use_bias)\n        if self.use_bias:\n            repr_ += \', bias_zero=\' + str(self.bias_zero)\n        return repr_ + \')\'\n'"
nmtpytorch/layers/flatten.py,1,"b'import torch\n\n\nclass Flatten(torch.nn.Module):\n    """"""A flatten module to squeeze single dimensions.""""""\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n\n    def __repr__(self):\n        return ""Flatten()""\n'"
nmtpytorch/layers/fusion.py,2,"b'# -*- coding: utf-8 -*-\nimport operator\nfrom functools import reduce\n\nimport torch\n\nfrom . import FF\nfrom ..utils.nn import get_activation_fn\n\n\nclass Fusion(torch.nn.Module):\n    """"""A convenience fusion layer that merges an arbitrary number of inputs.\n\n    Arguments:\n        fusion_type(str, optional): One of ``concat,sum,mul`` defining the\n            fusion operation. In the default setup of ``concat``, the\n            following two arguments should be provided to create a\n            ``Linear`` adaptor which will project the concatenated vector to\n            ``output_size``.\n        input_size(int, optional): The dimensionality of the concatenated\n            input. Only necessary if ``fusion_type==concat``.\n        output_size(int, optional): The output dimensionality of the\n            concatenation. Only necessary if ``fusion_type==concat``.\n    """"""\n\n    def __init__(self, fusion_type=\'concat\', input_size=None, output_size=None,\n                 fusion_activ=None):\n        super().__init__()\n\n        self.fusion_type = fusion_type\n        self.fusion_activ = fusion_activ\n        self.forward = getattr(self, \'_{}\'.format(self.fusion_type))\n        self.activ = get_activation_fn(fusion_activ)\n        self.adaptor = lambda x: x\n\n        if self.fusion_type == \'concat\' or input_size != output_size:\n            self.adaptor = FF(input_size, output_size, bias=False, activ=None)\n\n    def _sum(self, *inputs):\n        return self.activ(self.adaptor(reduce(operator.add, inputs)))\n\n    def _mul(self, *inputs):\n        return self.activ(self.adaptor(reduce(operator.mul, inputs)))\n\n    def _concat(self, *inputs):\n        return self.activ(self.adaptor(torch.cat(inputs, dim=-1)))\n\n    def __repr__(self):\n        return ""Fusion(type={}, adaptor={}, activ={})"".format(\n            self.fusion_type,\n            getattr(self, \'adaptor\') if hasattr(self, \'adaptor\') else \'None\',\n            self.fusion_activ)\n'"
nmtpytorch/layers/max_margin.py,2,"b'# -*- coding: utf-8 -*-\nimport torch\nfrom torch import nn\n\n# Layer contributed by @elliottd\n\n\nclass MaxMargin(nn.Module):\n    """"""A max-margin layer for ranking-based loss functions.""""""\n\n    def __init__(self, margin, max_violation=False):\n        super().__init__()\n\n        assert margin > 0., ""margin must be > 0.""\n\n        # Other arguments\n        self.margin = margin\n        self.max_violation = max_violation\n\n    def forward(self, enc1, enc2):\n        """"""Computes the max-margin loss given a pair of rank-2\n           annotation matrices. The matrices must have the same number of\n           batches and the same number of feats.\n\n        Arguments:\n            enc1(Tensor): A tensor of `B*feats` representing the\n                annotation vectors of the first encoder.\n            enc2(Tensor): A tensor of `B*feats` representation the\n                annotation vectors of the second encoder.\n        """"""\n\n        assert enc1.shape == enc2.shape, \\\n            ""shapes must match: enc1 {} enc2 {}"".format(enc1.shape, enc2.shape)\n\n        enc1 = enc1 / enc1.norm(p=2, dim=1).unsqueeze(1)\n        enc2 = enc2 / enc2.norm(p=2, dim=1).unsqueeze(1)\n        loss = self.constrastive_loss(enc1, enc2)\n\n        return {\'loss\': loss}\n\n    def constrastive_loss(self, enc1, enc2):\n        if enc1.shape[0] == 1:\n            # There is no error when we have a single-instance batch.\n            # Return a dummy error of 1e-5 as a regularizer\n            return torch.tensor([1e-3], device=enc1.device)\n\n        # compute enc1-enc2 score matrix\n        scores = self.cosine_sim(enc1, enc2)\n        diagonal = scores.diag().view(enc1.size(0), 1)\n        d1 = diagonal.expand_as(scores)\n        d2 = diagonal.t().expand_as(scores)\n\n        cost_enc1 = (self.margin + scores - d2).clamp(min=0)\n        cost_enc2 = (self.margin + scores - d1).clamp(min=0)\n\n        # clear diagonals\n        mask = torch.eye(scores.size(0), device=enc1.device) > .5\n        cost_enc2 = cost_enc2.masked_fill_(mask, 0)\n        cost_enc1 = cost_enc1.masked_fill_(mask, 0)\n\n        # keep the maximum violating negative for each query\n        if self.max_violation:\n            cost_enc2 = cost_enc2.max(1)[0]\n            cost_enc1 = cost_enc1.max(0)[0]\n        denom = cost_enc1.shape[0]**2 - cost_enc1.shape[0]\n        return (cost_enc2 + cost_enc1).sum() / denom\n\n    def cosine_sim(self, one, two):\n        \'\'\'Cosine similarity between all the first and second encoder pairs\'\'\'\n        return one.mm(two.t())\n'"
nmtpytorch/layers/pool.py,4,"b'import torch\n\n\nclass Pool(torch.nn.Module):\n    """"""A pool layer with mean/max/sum/last options.""""""\n    def __init__(self, op_type, pool_dim, keepdim=True):\n        super().__init__()\n\n        self.op_type = op_type\n        self.pool_dim = pool_dim\n        self.keepdim = keepdim\n        assert self.op_type in [""last"", ""mean"", ""max"", ""sum""], \\\n            ""Pool() operation should be mean, max, sum or last.""\n\n        if self.op_type == \'last\':\n            self.__pool_fn = lambda x: x.select(\n                self.pool_dim, -1).unsqueeze(0)\n        else:\n            if self.op_type == \'max\':\n                self.__pool_fn = lambda x: torch.max(\n                    x, dim=self.pool_dim, keepdim=self.keepdim)[0]\n            elif self.op_type == \'mean\':\n                self.__pool_fn = lambda x: torch.mean(\n                    x, dim=self.pool_dim, keepdim=self.keepdim)\n            elif self.op_type == \'sum\':\n                self.__pool_fn = lambda x: torch.sum(\n                    x, dim=self.pool_dim, keepdim=self.keepdim)\n\n    def forward(self, x):\n        return self.__pool_fn(x)\n\n    def __repr__(self):\n        return ""Pool(op_type={}, pool_dim={}, keepdim={})"".format(\n            self.op_type, self.pool_dim, self.keepdim)\n'"
nmtpytorch/layers/rnninit.py,2,"b'# -*- coding: utf-8 -*-\nimport torch\nfrom torch import nn\n\nfrom . import FF\n\n\nclass RNNInitializer(nn.Module):\n    """"""RNN initializer block for encoders and decoders.\n\n    Arguments:\n        rnn_type(str): GRU or LSTM.\n        input_size(int): Input dimensionality of the feature vectors that\'ll\n            be used for initialization if ``method != zero``.\n        hidden_size(int): Output dimensionality, i.e. hidden size of the RNN\n            that will be initialized.\n        n_layers(int): Number of recurrent layers to be initialized.\n        data_source(str): The modality name to look for in the batch dictionary.\n        method(str): One of ``last_ctx|mean_ctx|feats|zero``.\n        activ(str, optional): The non-linearity to be used for all initializers\n            except \'zero\'. Default is ``None`` i.e. no non-linearity.\n    """"""\n    def __init__(self, rnn_type, input_size, hidden_size, n_layers, data_source,\n                 method, activ=None):\n        super().__init__()\n        self.rnn_type = rnn_type\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n        self.data_source = data_source\n        self.method = method\n        self.activ = activ\n\n        # Check for RNN\n        assert self.rnn_type in (\'GRU\', \'LSTM\'), \\\n            ""rnn_type \'{}\' is unknown."".format(self.rnn_type)\n\n        assert self.method in (\'mean_ctx\', \'last_ctx\', \'zero\', \'feats\'), \\\n            ""RNN init method \'{}\' is unknown."".format(self.method)\n\n        # LSTMs have also the cell state so double the output size\n        assert self.rnn_type == \'GRU\', \'LSTM support not ready yet.\'\n        self.n_states = 1 if self.rnn_type == \'GRU\' else 2\n\n        if self.method in (\'mean_ctx\', \'last_ctx\', \'feats\'):\n            self.ff = FF(\n                self.input_size, self.hidden_size * self.n_layers,\n                activ=self.activ)\n\n        # Set the actual initializer depending on the method\n        self._initializer = getattr(self, \'_init_{}\'.format(self.method))\n\n    def forward(self, ctx_dict):\n        ctx, ctx_mask = ctx_dict[self.data_source]\n        x = self._initializer(ctx, ctx_mask)\n        return torch.stack(torch.split(x, self.hidden_size, dim=-1))\n\n    def _init_zero(self, ctx, mask):\n        # h_0: (n_layers, batch_size, hidden_size)\n        return torch.zeros(\n            ctx.shape[1], self.hidden_size * self.n_layers, device=ctx.device)\n\n    def _init_feats(self, ctx, mask):\n        return self.ff(ctx)\n\n    def _init_mean_ctx(self, ctx, mask):\n        if mask is None:\n            return self.ff(ctx.mean(0))\n        else:\n            return self.ff(ctx.sum(0) / mask.sum(0).unsqueeze(1))\n\n    def _init_last_ctx(self, ctx, mask):\n        if mask is None:\n            return self.ff(ctx[-1])\n        else:\n            # Fetch last timesteps\n            last_tsteps = mask.sum(0).sub(1).long()\n            return self.ff(ctx[last_tsteps, range(ctx.shape[1])])\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'(\' \\\n            + \'in_features={}, \'.format(self.input_size) \\\n            + \'out_features={}, \'.format(self.hidden_size) \\\n            + \'activ={}, \'.format(self.activ) \\\n            + \'method={}\'.format(self.method) + \')\'\n'"
nmtpytorch/layers/seq_conv.py,2,"b'# -*- coding: utf-8 -*-\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n\n# Code contributed by @jlibovicky\n\n\nclass SequenceConvolution(nn.Module):\n    """"""1D convolution with optional max-pooling.\n\n    The layer applies 1D convolution of odd kernel size with output channel\n    counts specified by a list of integers. Then, it optionally applies 1D\n    max-pooling to reduce the sequence length.\n    """"""\n\n    def __init__(self, input_dim, filters, max_pool_stride=None, activation=\'relu\'):\n        super().__init__()\n        self.max_pool_stride = max_pool_stride\n\n        self.conv_proj = nn.ModuleList([\n            nn.Conv1d(in_channels=input_dim,\n                      out_channels=size,\n                      kernel_size=2 * k + 1,\n                      padding=k)\n            for k, size in enumerate(filters) if size > 0])\n\n        if self.max_pool_stride is not None:\n            self.max_pool = nn.MaxPool1d(\n                kernel_size=self.max_pool_stride,\n                stride=self.max_pool_stride)\n        else:\n            self.max_pool = None\n\n    def forward(self, x, mask):\n        conv_outputs = [conv(x.permute(1, 2, 0)) for conv in self.conv_proj]\n        conv_out = torch.cat(conv_outputs, dim=1)\n\n        if self.max_pool is not None:\n            conv_len = conv_out.size(-1)\n            if conv_len < self.max_pool_stride:\n                pad_size = self.max_pool_stride - conv_len\n                conv_out = F.pad(conv_out, pad=[pad_size, pad_size])\n            max_pooled_data = self.max_pool(conv_out).permute(2, 0, 1)\n            max_pooled_mask = (self.max_pool(mask.t().unsqueeze(1)).squeeze(1).t()\n                               if mask is not None else None)\n            return max_pooled_data, max_pooled_mask\n        else:\n            return conv_out.permute(2, 0, 1), mask\n'"
nmtpytorch/metrics/__init__.py,0,"b'from .metric import Metric\nfrom .multibleu import BLEUScorer\nfrom .sacrebleu import SACREBLEUScorer\nfrom .meteor import METEORScorer\nfrom .wer import WERScorer\nfrom .cer import CERScorer\nfrom .rouge import ROUGEScorer\n\nbeam_metrics = [""BLEU"", ""SACREBLEU"", ""METEOR"", ""WER"", ""CER"", ""ROUGE""]\n\nmetric_info = {\n    \'BLEU\': \'max\',\n    \'SACREBLEU\': \'max\',\n    \'METEOR\': \'max\',\n    \'ROUGE\': \'max\',\n    \'LOSS\': \'min\',\n    \'WER\': \'min\',\n    \'CER\': \'min\',\n    \'ACC\': \'max\',\n    \'RECALL\': \'max\',\n    \'PRECISION\': \'max\',\n    \'F1\': \'max\',\n}\n'"
nmtpytorch/metrics/cer.py,0,"b'# -*- coding: utf-8 -*-\nimport editdistance\n\nfrom .metric import Metric\n\n\nclass CERScorer:\n    """"""This is the same as WER but computes CER and also WER after post-processing.""""""\n    def compute(self, refs, hyps, language=None, lowercase=False):\n        if isinstance(hyps, str):\n            # hyps is a file\n            hyp_sents = open(hyps).read().strip().split(\'\\n\')\n        elif isinstance(hyps, list):\n            hyp_sents = hyps\n\n        # refs is a list, take its first item\n        with open(refs[0]) as f:\n            ref_sents = f.read().strip().split(\'\\n\')\n\n        assert len(hyp_sents) == len(ref_sents), ""CER: # of sentences does not match.""\n\n        n_ref_chars = 0\n        n_ref_tokens = 0\n        dist_chars = 0\n        dist_tokens = 0\n        for hyp, ref in zip(hyp_sents, ref_sents):\n            hyp_chars = hyp.split(\' \')\n            ref_chars = ref.split(\' \')\n            n_ref_chars += len(ref_chars)\n            dist_chars += editdistance.eval(hyp_chars, ref_chars)\n\n            # Convert char-based sentences to token-based ones\n            hyp_tokens = hyp.replace(\' \', \'\').replace(\'<s>\', \' \').strip().split(\' \')\n            ref_tokens = ref.replace(\' \', \'\').replace(\'<s>\', \' \').strip().split(\' \')\n            n_ref_tokens += len(ref_tokens)\n            dist_tokens += editdistance.eval(hyp_tokens, ref_tokens)\n\n        cer = (100 * dist_chars) / n_ref_chars\n        wer = (100 * dist_tokens) / n_ref_tokens\n\n        verbose_score = ""{:.3f}% (n_errors = {}, n_ref_chars = {}, WER = {:.3f}%)"".format(\n            cer, dist_chars, n_ref_chars, wer)\n\n        return Metric(\'CER\', cer, verbose_score, higher_better=False)\n'"
nmtpytorch/metrics/meteor.py,0,"b'# -*- coding: utf-8 -*-\nimport os\nimport shutil\nimport pathlib\nimport subprocess\n\nfrom ..utils.misc import listify, get_meteor_jar\nfrom .metric import Metric\n\n\nclass METEORScorer:\n    def __init__(self):\n        self.jar = str(get_meteor_jar())\n        self.__cmdline = [""java"", ""-Xmx2G"", ""-jar"", self.jar,\n                          ""-"", ""-"", ""-stdio""]\n        self.env = os.environ\n        self.env[\'LC_ALL\'] = \'en_US.UTF-8\'\n\n        # Sanity check\n        if shutil.which(\'java\') is None:\n            raise RuntimeError(\'METEOR requires java which is not installed.\')\n\n    def compute(self, refs, hyps, language=""auto""):\n        cmdline = self.__cmdline[:]\n        refs = listify(refs)\n\n        if isinstance(hyps, str):\n            # If file, open it for line reading\n            hyps = open(hyps)\n\n        if language == ""auto"":\n            # Take the extension of the 1st reference file, e.g. "".de""\n            language = pathlib.Path(refs[0]).suffix[1:]\n\n        cmdline.extend([""-l"", language])\n\n        # Make reference files a list\n        iters = [open(f) for f in refs]\n        iters.append(hyps)\n\n        # Run METEOR process\n        proc = subprocess.Popen(cmdline,\n                                stdout=subprocess.PIPE,\n                                stdin=subprocess.PIPE,\n                                stderr=subprocess.PIPE,\n                                env=self.env,\n                                universal_newlines=True, bufsize=1)\n\n        eval_line = \'EVAL\'\n\n        for line_ctr, lines in enumerate(zip(*iters)):\n            lines = [l.rstrip(\'\\n\') for l in lines]\n            refstr = "" ||| "".join(lines[:-1])\n            line = ""SCORE ||| "" + refstr + "" ||| "" + lines[-1]\n\n            proc.stdin.write(line + \'\\n\')\n            eval_line += \' ||| {}\'.format(proc.stdout.readline().strip())\n\n        # Send EVAL line to METEOR\n        proc.stdin.write(eval_line + \'\\n\')\n\n        # Dummy read segment scores\n        for i in range(line_ctr + 1):\n            proc.stdout.readline().strip()\n\n        # Compute final METEOR\n        try:\n            score = float(proc.stdout.readline().strip())\n            score = Metric(\'METEOR\', 100 * score)\n        except Exception as e:\n            score = Metric(\'METEOR\', 0.0)\n        finally:\n            # Close METEOR process\n            proc.stdin.close()\n            proc.terminate()\n            proc.kill()\n            proc.wait(timeout=2)\n            return score\n'"
nmtpytorch/metrics/metric.py,0,"b'# -*- coding: utf-8 -*-\nfrom functools import total_ordering\n\n\n@total_ordering\nclass Metric:\n    """"""A Metric object to represent evaluation metrics.\n\n    Arguments:\n        name(str): A name for the metric that will be kept internally\n            after upper-casing\n        score(float): A floating point score\n        detailed_score(str, optional): A custom, more detailed string\n            representing the score given above (Default: """")\n        higher_better(bool, optional): If ``False``, the smaller the better\n            (Default: ``True``)\n    """"""\n\n    def __init__(self, name, score, detailed_score="""", higher_better=True):\n        self.name = name.upper()\n        self.score = score\n        self.detailed_score = detailed_score\n        self.higher_better = higher_better\n\n    def __eq__(self, other):\n        return self.score == other.score\n\n    def __lt__(self, other):\n        return self.score < other.score\n\n    def __repr__(self):\n        rhs = (self.detailed_score if self.detailed_score\n               else ""%.2f"" % self.score)\n        return self.name + \' = \' + rhs\n'"
nmtpytorch/metrics/multibleu.py,0,"b'# -*- coding: utf-8 -*-\nimport subprocess\nimport pkg_resources\n\nfrom ..utils.misc import listify\nfrom .metric import Metric\n\nBLEU_SCRIPT = pkg_resources.resource_filename(\'nmtpytorch\',\n                                              \'lib/multi-bleu.perl\')\n\n\nclass BLEUScorer:\n    """"""BLEUScorer class.""""""\n    def __init__(self):\n        # For multi-bleu.perl we give the reference(s) files as argv,\n        # while the candidate translations are read from stdin.\n        self.__cmdline = [BLEU_SCRIPT]\n\n    def compute(self, refs, hyps, language=None, lowercase=False):\n        cmdline = self.__cmdline[:]\n\n        if lowercase:\n            cmdline.append(""-lc"")\n\n        # Make reference files a list\n        cmdline.extend(listify(refs))\n\n        if isinstance(hyps, str):\n            hypstring = open(hyps).read().strip()\n        elif isinstance(hyps, list):\n            hypstring = ""\\n"".join(hyps)\n\n        score = subprocess.run(cmdline, stdout=subprocess.PIPE,\n                               input=hypstring,\n                               universal_newlines=True).stdout.splitlines()\n\n        if len(score) == 0:\n            return Metric(\'BLEU\', 0, ""0.0"")\n        else:\n            score = score[0].strip()\n            float_score = float(score.split()[2][:-1])\n            verbose_score = score.replace(\'BLEU = \', \'\')\n            return Metric(\'BLEU\', float_score, verbose_score)\n'"
nmtpytorch/metrics/rouge.py,0,"b'# -*- coding: utf-8 -*-\nfrom .metric import Metric\nfrom ..cocoeval import Rouge\n\n\nclass ROUGEScorer:\n    def compute(self, refs, hyps, language=None, lowercase=False):\n        if isinstance(hyps, str):\n            # hyps is a file\n            hyp_sents = open(hyps).read().strip().split(\'\\n\')\n        elif isinstance(hyps, list):\n            hyp_sents = hyps\n\n        # refs is a list, take its first item\n        with open(refs[0]) as f:\n            ref_sents = f.read().strip().split(\'\\n\')\n\n        assert len(hyp_sents) == len(ref_sents), ""ROUGE: # of sentences does not match.""\n\n        rouge_scorer = Rouge()\n\n        rouge_sum = 0\n        for hyp, ref in zip(hyp_sents, ref_sents):\n            rouge_sum += rouge_scorer.calc_score([hyp], [ref])\n\n        score = (100 * rouge_sum) / len(hyp_sents)\n        verbose_score = ""{:.3f}"".format(score)\n\n        return Metric(\'ROUGE\', score, verbose_score, higher_better=True)\n'"
nmtpytorch/metrics/sacrebleu.py,0,"b'# -*- coding: utf-8 -*-\nimport subprocess\n\nfrom ..utils.misc import listify\nfrom .metric import Metric\n\n\nclass SACREBLEUScorer:\n    """"""SACREBLEUScorer class.""""""\n    def __init__(self):\n        self.__cmdline = [""sacrebleu"", ""--short""]\n\n    def compute(self, refs, hyps, language=None, lowercase=False):\n        cmdline = self.__cmdline[:]\n\n        if lowercase:\n            cmdline.append(""-lc"")\n\n        # Make reference files a list\n        cmdline.extend(listify(refs))\n\n        if isinstance(hyps, str):\n            hypstring = open(hyps).read().strip()\n        elif isinstance(hyps, list):\n            hypstring = ""\\n"".join(hyps)\n\n        score = subprocess.run(cmdline, stdout=subprocess.PIPE,\n                               input=hypstring,\n                               universal_newlines=True).stdout.splitlines()\n\n        if len(score) == 0:\n            return Metric(\'SACREBLEU\', 0, ""0.0"")\n        else:\n            score = score[0].strip()\n            float_score = float(score.split()[2])\n            verbose_score = \' \'.join(score.split()[2:])\n            return Metric(\'SACREBLEU\', float_score, verbose_score)\n'"
nmtpytorch/metrics/wer.py,0,"b'# -*- coding: utf-8 -*-\nimport editdistance\n\nfrom .metric import Metric\n\n\nclass WERScorer:\n    def compute(self, refs, hyps, language=None, lowercase=False):\n        if isinstance(hyps, str):\n            # hyps is a file\n            hyp_sents = open(hyps).read().strip().split(\'\\n\')\n        elif isinstance(hyps, list):\n            hyp_sents = hyps\n\n        # refs is a list, take its first item\n        with open(refs[0]) as f:\n            ref_sents = f.read().strip().split(\'\\n\')\n\n        assert len(hyp_sents) == len(ref_sents), ""WER: # of sentences does not match.""\n\n        n_ref_tokens = 0\n        dist = 0\n        for hyp, ref in zip(hyp_sents, ref_sents):\n            hyp_tokens = hyp.split(\' \')\n            ref_tokens = ref.split(\' \')\n            n_ref_tokens += len(ref_tokens)\n            dist += editdistance.eval(hyp_tokens, ref_tokens)\n\n        score = (100 * dist) / n_ref_tokens\n        verbose_score = ""{:.3f}% (n_errors = {}, n_ref_tokens = {})"".format(\n            score, dist, n_ref_tokens)\n\n        return Metric(\'WER\', score, verbose_score, higher_better=False)\n'"
nmtpytorch/models/__init__.py,0,b'#####\n# NMT\n#####\nfrom .nmt import NMT\nfrom .tfnmt import TransformerNMT\n\n################\n# Multimodal NMT\n################\nfrom .simple_mmt import SimpleMMT\nfrom .attentive_mmt import AttentiveMMT\n\n###############\n# Speech models\n###############\nfrom .asr import ASR\nfrom .multimodal_asr import MultimodalASR\n'
nmtpytorch/models/asr.py,2,"b'# -*- coding: utf-8 -*-\nimport logging\n\nimport torch\nfrom torch import nn\n\nfrom ..layers import BiLSTMp, ConditionalDecoder, FF\nfrom ..datasets import MultimodalDataset\nfrom ..vocabulary import Vocabulary\nfrom ..utils.topology import Topology\nfrom . import NMT\n\nlogger = logging.getLogger(\'nmtpytorch\')\n\n\n# ASR with ESPNet style BiLSTMp encoder\n\n\nclass ASR(NMT):\n    supports_beam_search = True\n\n    def set_defaults(self):\n        self.defaults = {\n            \'feat_dim\': 43,                 # Speech features dimensionality\n            \'feat_transform\': None,         # A FF to speech features: None, linear, tanh..\n            \'emb_dim\': 300,                 # Decoder embedding dim\n            \'enc_dim\': 320,                 # Encoder hidden size\n            \'enc_layers\': \'1_1_2_2_1_1\',    # layer configuration\n            \'dec_dim\': 320,                 # Decoder hidden size\n            \'proj_dim\': 300,                # Intra-LSTM projection layer\n            \'proj_activ\': \'tanh\',           # Intra-LSTM projection activation\n            \'dec_type\': \'gru\',              # Decoder type (gru|lstm)\n            \'dec_init\': \'mean_ctx\',         # How to initialize decoder\n                                            # (zero/mean_ctx/feats)\n            \'dec_init_size\': None,          # feature vector dimensionality for\n                                            # dec_init == \'feats\'\n            \'dec_init_activ\': \'tanh\',       # Decoder initialization activation func\n            \'att_type\': \'mlp\',              # Attention type (mlp|dot)\n            \'att_temp\': 1.,                 # Attention temperature\n            \'att_activ\': \'tanh\',            # Attention non-linearity (all torch nonlins)\n            \'att_mlp_bias\': False,          # Enables bias in attention mechanism\n            \'att_bottleneck\': \'hid\',        # Bottleneck dimensionality (ctx|hid)\n            \'att_transform_ctx\': True,      # Transform annotations before attention\n            \'dropout\': 0,                   # Generic dropout overall the architecture\n            \'tied_dec_embs\': False,         # Share decoder embeddings\n            \'max_len\': None,                # Reject samples if len(\'bucket_by\') > max_len\n            \'bucket_by\': None,              # A key like \'en\' to define w.r.t\n                                            # which dataset batches will be sorted\n            \'bucket_order\': None,           # Can be \'ascending\' or \'descending\'\n                                            # for curriculum learning\n                                            # NOTE: Noisy LSTM because of unhandled paddings\n            \'sampler_type\': \'bucket\',       # bucket or approximate\n            \'sched_sampling\': 0,            # Scheduled sampling ratio\n            \'bos_type\': \'emb\',          #\n            \'bos_activ\': None,          #\n            \'bos_dim\': None,            #\n            \'direction\': None,              # Network directionality, i.e. en->de\n            \'lstm_forget_bias\': False,      # Initialize forget gate bias to 1 for LSTM\n            \'lstm_bias_zero\': False,        # Use zero biases for LSTM\n            \'adaptation\': False,            # Enable/disable AM adaptation\n            \'adaptation_type\': \'early\',     # Kept for backward-compatibility\n            \'adaptation_dim\': None,         # Input dim for auxiliary feat vectors\n            \'adaptation_activ\': None,       # Non-linearity for adaptation FF\n            \'io_bias\': 0.1,                 # bias for IO adaptation\n        }\n\n    def __init__(self, opts):\n        # Don\'t call NMT init as it\'s too different from ASR\n        nn.Module.__init__(self)\n\n        # opts -> config file sections {.model, .data, .vocabulary, .train}\n        self.opts = opts\n\n        # Vocabulary objects\n        self.vocabs = {}\n\n        # Each auxiliary loss should be stored inside this dictionary\n        # in order to be taken into account by the mainloop for multi-tasking\n        self.aux_loss = {}\n\n        # Setup options\n        self.opts.model = self.set_model_options(opts.model)\n\n        # Parse topology & languages\n        self.topology = Topology(self.opts.model[\'direction\'])\n\n        # Load vocabularies here\n        for name, fname in self.opts.vocabulary.items():\n            self.vocabs[name] = Vocabulary(fname)\n\n        # Inherently non multi-lingual aware\n        self.src = self.topology.first_src\n\n        self.tl = self.topology.first_trg\n        self.trg_vocab = self.vocabs[self.tl]\n        self.n_trg_vocab = len(self.trg_vocab)\n\n        # Context size is enc_dim because of proj layers\n        self.ctx_sizes = {str(self.src): self.opts.model[\'enc_dim\']}\n\n        # Need to be set for early-stop evaluation\n        # NOTE: This should come from config or elsewhere\n        self.val_refs = self.opts.data[\'val_set\'][self.tl]\n\n    def reset_parameters(self):\n        # Use kaiming normal for everything as it is a sane default\n        # Do not touch biases for now\n        for name, param in self.named_parameters():\n            if param.requires_grad and \'bias\' not in name:\n                nn.init.kaiming_normal_(param.data)\n\n        if self.opts.model[\'lstm_bias_zero\'] or \\\n                self.opts.model[\'lstm_forget_bias\']:\n            for name, param in self.speech_enc.named_parameters():\n                if \'bias_hh\' in name or \'bias_ih\' in name:\n                    # Reset bias to 0\n                    param.data.fill_(0.0)\n                    if self.opts.model[\'lstm_forget_bias\']:\n                        # Reset forget gate bias of LSTMs to 1\n                        # the tensor organized as: inp,forg,cell,out\n                        n = param.numel()\n                        param[n // 4: n // 2].data.fill_(1.0)\n\n    def setup(self, is_train=True):\n        self.speech_enc = BiLSTMp(\n            input_size=self.opts.model[\'feat_dim\'],\n            hidden_size=self.opts.model[\'enc_dim\'],\n            proj_size=self.opts.model[\'proj_dim\'],\n            proj_activ=self.opts.model[\'proj_activ\'],\n            dropout=self.opts.model[\'dropout\'],\n            layers=self.opts.model[\'enc_layers\'])\n\n        ################\n        # Create Decoder\n        ################\n        self.dec = ConditionalDecoder(\n            input_size=self.opts.model[\'emb_dim\'],\n            hidden_size=self.opts.model[\'dec_dim\'],\n            n_vocab=self.n_trg_vocab,\n            rnn_type=self.opts.model[\'dec_type\'],\n            ctx_size_dict=self.ctx_sizes,\n            ctx_name=str(self.src),\n            tied_emb=self.opts.model[\'tied_dec_embs\'],\n            dec_init=self.opts.model[\'dec_init\'],\n            dec_init_size=self.opts.model[\'dec_init_size\'],\n            dec_init_activ=self.opts.model[\'dec_init_activ\'],\n            att_type=self.opts.model[\'att_type\'],\n            att_temp=self.opts.model[\'att_temp\'],\n            att_activ=self.opts.model[\'att_activ\'],\n            transform_ctx=self.opts.model[\'att_transform_ctx\'],\n            mlp_bias=self.opts.model[\'att_mlp_bias\'],\n            att_bottleneck=self.opts.model[\'att_bottleneck\'],\n            dropout_out=self.opts.model[\'dropout\'],\n            sched_sample=self.opts.model[\'sched_sampling\'],\n            bos_type=self.opts.model[\'bos_type\'],\n            bos_dim=self.opts.model[\'bos_dim\'],\n            bos_activ=self.opts.model[\'bos_activ\'])\n\n        if self.opts.model[\'adaptation\']:\n            out_dim = self.opts.model[\'feat_dim\']\n            if self.opts.model[\'adaptation_type\'].startswith(\'early\'):\n                # Simple single layer\n                self.vis_proj = FF(self.opts.model[\'adaptation_dim\'],\n                                   out_dim,\n                                   activ=self.opts.model[\'adaptation_activ\'],\n                                   bias=False)\n            elif self.opts.model[\'adaptation_type\'] == \'deep\':\n                # 3 layers of 512d with sigmoid NL and one output layer\n                activ = self.opts.model[\'adaptation_activ\']\n                self.vis_proj = nn.Sequential(\n                    FF(self.opts.model[\'adaptation_dim\'], 256, activ=activ),\n                    FF(256, 256, activ=activ),\n                    FF(256, 256, activ=activ),\n                    FF(256, out_dim, activ=None),\n                )\n            elif self.opts.model[\'adaptation_type\'] == \'io\':\n                self.emb_cat = nn.Embedding(3, out_dim, padding_idx=2)\n\n        if self.opts.model[\'feat_transform\']:\n            self.feat_transform = FF(self.opts.model[\'feat_dim\'],\n                                     self.opts.model[\'feat_dim\'], bias=False,\n                                     activ=self.opts.model[\'feat_transform\'])\n\n    def load_data(self, split, batch_size, mode=\'train\'):\n        """"""Loads the requested dataset split.""""""\n        dataset = MultimodalDataset(\n            data=self.opts.data[\'{}_set\'.format(split)],\n            mode=mode, batch_size=batch_size,\n            vocabs=self.vocabs, topology=self.topology,\n            bucket_by=self.opts.model[\'bucket_by\'],\n            max_len=self.opts.model[\'max_len\'],\n            bucket_order=self.opts.model[\'bucket_order\'],\n            sampler_type=self.opts.model[\'sampler_type\'])\n        logger.info(dataset)\n        return dataset\n\n    def encode(self, batch, **kwargs):\n        # Speech features -> x\n        x = batch[self.src]\n        if self.opts.model[\'feat_transform\']:\n            x = self.feat_transform(x)\n\n        if self.opts.model[\'adaptation\']:\n            if self.opts.model[\'adaptation_type\'] == \'io\':\n                x *= (torch.sigmoid(self.emb_cat(batch[\'io\'])) + self.opts.model[\'io_bias\'])\n            elif self.opts.model[\'adaptation_type\'] == \'early_mul\':\n                x *= (torch.sigmoid(self.vis_proj(batch[\'feats\'])) + self.opts.model[\'io_bias\'])\n            else:\n                x += self.vis_proj(batch[\'feats\'])\n\n        d = {str(self.src): self.speech_enc(x)}\n        if \'feats\' in batch:\n            d[\'feats\'] = (batch[\'feats\'], None)\n        return d\n'"
nmtpytorch/models/attentive_mmt.py,0,"b'# -*- coding: utf-8 -*-\nimport logging\n\nfrom torch import nn\n\nfrom ..datasets import MultimodalDataset\nfrom ..layers import ConditionalMMDecoder, TextEncoder, FF\nfrom .nmt import NMT\n\nlogger = logging.getLogger(\'nmtpytorch\')\n\n\nclass AttentiveMMT(NMT):\n    """"""An end-to-end sequence-to-sequence NMT model with visual attention over\n    pre-extracted convolutional features.\n    """"""\n    def set_defaults(self):\n        # Set parent defaults\n        super().set_defaults()\n        self.defaults.update({\n            \'fusion_type\': \'concat\',    # Multimodal context fusion (sum|mul|concat)\n            \'fusion_activ\': \'tanh\',     # Multimodal context non-linearity\n            \'vis_activ\': \'linear\',      # Visual feature transformation activ.\n            \'n_channels\': 2048,         # depends on the features used\n            \'mm_att_type\': \'md-dd\',     # multimodal attention type\n                                        # md: modality dep.\n                                        # mi: modality indep.\n                                        # dd: decoder state dep.\n                                        # di: decoder state indep.\n            \'out_logic\': \'deep\',        # simple vs deep output\n            \'persistent_dump\': False,   # To save activations during beam-search\n            \'preatt\': False,            # Apply filtered attention\n            \'preatt_activ\': \'ReLU\',     # Activation for convatt block\n            \'dropout_img\': 0.0,         # Dropout on image features\n        })\n\n    def __init__(self, opts):\n        super().__init__(opts)\n\n    def setup(self, is_train=True):\n        # Textual context dim\n        txt_ctx_size = self.ctx_sizes[self.sl]\n\n        # Add visual context transformation (sect. 3.2 in paper)\n        self.ff_img = FF(\n            self.opts.model[\'n_channels\'], txt_ctx_size,\n            activ=self.opts.model[\'vis_activ\'])\n\n        self.dropout_img = nn.Dropout(self.opts.model[\'dropout_img\'])\n\n        # Add vis ctx size\n        self.ctx_sizes[\'image\'] = txt_ctx_size\n\n        ########################\n        # Create Textual Encoder\n        ########################\n        self.enc = TextEncoder(\n            input_size=self.opts.model[\'emb_dim\'],\n            hidden_size=self.opts.model[\'enc_dim\'],\n            n_vocab=self.n_src_vocab,\n            rnn_type=self.opts.model[\'enc_type\'],\n            dropout_emb=self.opts.model[\'dropout_emb\'],\n            dropout_ctx=self.opts.model[\'dropout_ctx\'],\n            dropout_rnn=self.opts.model[\'dropout_enc\'],\n            num_layers=self.opts.model[\'n_encoders\'],\n            emb_maxnorm=self.opts.model[\'emb_maxnorm\'],\n            emb_gradscale=self.opts.model[\'emb_gradscale\'])\n\n        # Create Decoder\n        self.dec = ConditionalMMDecoder(\n            input_size=self.opts.model[\'emb_dim\'],\n            hidden_size=self.opts.model[\'dec_dim\'],\n            n_vocab=self.n_trg_vocab,\n            rnn_type=self.opts.model[\'dec_type\'],\n            ctx_size_dict=self.ctx_sizes,\n            ctx_name=str(self.sl),\n            fusion_type=self.opts.model[\'fusion_type\'],\n            fusion_activ=self.opts.model[\'fusion_activ\'],\n            tied_emb=self.opts.model[\'tied_emb\'],\n            dec_init=self.opts.model[\'dec_init\'],\n            att_type=self.opts.model[\'att_type\'],\n            mm_att_type=self.opts.model[\'mm_att_type\'],\n            out_logic=self.opts.model[\'out_logic\'],\n            att_activ=self.opts.model[\'att_activ\'],\n            transform_ctx=self.opts.model[\'att_transform_ctx\'],\n            att_ctx2hid=False,\n            mlp_bias=self.opts.model[\'att_mlp_bias\'],\n            att_bottleneck=self.opts.model[\'att_bottleneck\'],\n            dropout_out=self.opts.model[\'dropout_out\'],\n            emb_maxnorm=self.opts.model[\'emb_maxnorm\'],\n            emb_gradscale=self.opts.model[\'emb_gradscale\'],\n            persistent_dump=self.opts.model[\'persistent_dump\'])\n\n        # Share encoder and decoder weights\n        if self.opts.model[\'tied_emb\'] == \'3way\':\n            self.enc.emb.weight = self.dec.emb.weight\n\n    def load_data(self, split, batch_size, mode=\'train\'):\n        """"""Loads the requested dataset split.""""""\n        dataset = MultimodalDataset(\n            data=self.opts.data[split + \'_set\'],\n            mode=mode, batch_size=batch_size,\n            vocabs=self.vocabs, topology=self.topology,\n            bucket_by=self.opts.model[\'bucket_by\'],\n            max_len=self.opts.model.get(\'max_len\', None),\n            order_file=self.opts.data[split + \'_set\'].get(\'ord\', None))\n        logger.info(dataset)\n        return dataset\n\n    def encode(self, batch, **kwargs):\n        # Transform the features to context dim\n        feats = self.dropout_img(self.ff_img(batch[\'image\']))\n\n        # Get source language encodings (S*B*C)\n        text_encoding = self.enc(batch[self.sl])\n\n        return {\n            str(self.sl): text_encoding,\n            \'image\': (feats, None),\n        }\n'"
nmtpytorch/models/multimodal_asr.py,0,"b'# -*- coding: utf-8 -*-\nimport logging\n\nfrom ..layers import MultimodalBiLSTMp, ConditionalDecoder\nfrom . import ASR\n\nlogger = logging.getLogger(\'nmtpytorch\')\n\n\nclass MultimodalASR(ASR):\n    """"""Multimodal ASR with global features + encoder/decoder initialization.""""""\n    def set_defaults(self):\n        self.defaults = {\n            \'feat_dim\': 43,                 # Speech features dimensionality\n            \'emb_dim\': 300,                 # Decoder embedding dim\n            \'enc_dim\': 320,                 # Encoder hidden size\n            \'enc_layers\': \'1_1_2_2_1_1\',    # layer configuration\n            \'dec_dim\': 320,                 # Decoder hidden size\n            \'proj_dim\': 300,                # Intra-LSTM projection layer\n            \'proj_activ\': \'tanh\',           # Intra-LSTM projection activation\n            \'dec_type\': \'gru\',              # Decoder type (gru|lstm)\n            \'att_type\': \'mlp\',              # Attention type (mlp|dot)\n            \'att_temp\': 1.,                 # Attention temperature\n            \'att_activ\': \'tanh\',            # Attention non-linearity (all torch nonlins)\n            \'att_mlp_bias\': False,          # Enables bias in attention mechanism\n            \'att_bottleneck\': \'hid\',        # Bottleneck dimensionality (ctx|hid)\n            \'att_transform_ctx\': True,      # Transform annotations before attention\n            \'dropout\': 0,                   # Generic dropout overall the architecture\n            \'tied_dec_embs\': False,         # Share decoder embeddings\n            \'max_len\': None,                # Reject samples if len(\'bucket_by\') > max_len\n            \'bucket_by\': None,              # A key like \'en\' to define w.r.t\n                                            # which dataset batches will be sorted\n            \'bucket_order\': None,           # Can be \'ascending\' or \'descending\'\n                                            # for curriculum learning\n            \'sampler_type\': \'bucket\',       # bucket or approximate. latter is not good\n                                            # since the ASR encoder does not handle paddings\n            \'sched_sampling\': 0,            # Scheduled sampling ratio\n            \'direction\': None,              # Network directionality, i.e. en->de\n            \'lstm_forget_bias\': False,      # Initialize forget gate bias to 1 for LSTM\n            \'lstm_bias_zero\': False,        # Use zero biases for LSTM\n            \'dec_init\': \'mean_ctx\',         # How to initialize decoder\n                                            # (zero/mean_ctx/feats)\n            \'dec_init_size\': None,          # feature vector dimensionality for\n                                            # dec_init == \'feats\'\n            \'dec_init_activ\': \'tanh\',       # Decoder initialization activation func\n            \'aux_dim\': 2048,                # Feature dimension for multimodal encoder\n            \'feat_activ\': None,             # Feature non-linearity for multimodal encoder\n            \'feat_fusion\': \'init\',          # Integration type for multimodal encoder\n            \'tied_init\': False,             # Tie FFs for enc and dec init\n            \'bos_type\': \'emb\',              # \'emb\': classical learned <bos>\n                                            # \'feats\': use visual feats as <bos>\n            \'bos_activ\': None,              # activation function for \'feats\'\n            \'bos_dim\': None,                # input feats dim for bos \'feats\'\n            \'bos_bias\': False,              # bias for bos \'feats\'\n        }\n\n    def __init__(self, opts):\n        super().__init__(opts)\n\n    def setup(self, is_train=True):\n        self.speech_enc = MultimodalBiLSTMp(\n            input_size=self.opts.model[\'feat_dim\'],\n            hidden_size=self.opts.model[\'enc_dim\'],\n            proj_size=self.opts.model[\'proj_dim\'],\n            proj_activ=self.opts.model[\'proj_activ\'],\n            dropout=self.opts.model[\'dropout\'],\n            layers=self.opts.model[\'enc_layers\'],\n            feat_size=self.opts.model[\'aux_dim\'],\n            feat_activ=self.opts.model[\'feat_activ\'],\n            feat_fusion=self.opts.model[\'feat_fusion\'])\n\n        ################\n        # Create Decoder\n        ################\n        self.dec = ConditionalDecoder(\n            input_size=self.opts.model[\'emb_dim\'],\n            hidden_size=self.opts.model[\'dec_dim\'],\n            n_vocab=self.n_trg_vocab,\n            rnn_type=self.opts.model[\'dec_type\'],\n            ctx_size_dict=self.ctx_sizes,\n            ctx_name=str(self.src),\n            tied_emb=self.opts.model[\'tied_dec_embs\'],\n            dec_init=self.opts.model[\'dec_init\'],\n            dec_init_size=self.opts.model[\'dec_init_size\'],\n            dec_init_activ=self.opts.model[\'dec_init_activ\'],\n            att_type=self.opts.model[\'att_type\'],\n            att_temp=self.opts.model[\'att_temp\'],\n            att_activ=self.opts.model[\'att_activ\'],\n            transform_ctx=self.opts.model[\'att_transform_ctx\'],\n            mlp_bias=self.opts.model[\'att_mlp_bias\'],\n            att_bottleneck=self.opts.model[\'att_bottleneck\'],\n            dropout_out=self.opts.model[\'dropout\'],\n            sched_sample=self.opts.model[\'sched_sampling\'],\n            bos_type=self.opts.model[\'bos_type\'],\n            bos_dim=self.opts.model[\'bos_dim\'],\n            bos_activ=self.opts.model[\'bos_activ\'],\n            bos_bias=self.opts.model[\'bos_bias\'])\n\n        if self.opts.model[\'dec_init\'] == \'feats\' and self.opts.model[\'tied_init\']:\n            # Use the same FF for enc and dec init layers\n            self.speech_enc.ff_init_h0.weight = self.dec.ff_dec_init.weight\n            # Tie the <bos> \'feats\' layer as well\n            if self.opts.model[\'bos_type\'] == \'feats\':\n                self.dec.ff_bos.weight = self.dec.ff_dec_init.weight\n\n    def encode(self, batch, **kwargs):\n        d = {str(self.src): self.speech_enc(batch[self.src], aux=batch[\'feats\'])}\n\n        if \'feats\' in batch:\n            d[\'feats\'] = (batch[\'feats\'], None)\n        return d\n'"
nmtpytorch/models/nmt.py,7,"b'# -*- coding: utf-8 -*-\nimport logging\n\nimport torch\nfrom torch import nn\n\nfrom ..layers import TextEncoder\nfrom ..layers.decoders import get_decoder\nfrom ..utils.misc import get_n_params\nfrom ..vocabulary import Vocabulary\nfrom ..utils.topology import Topology\nfrom ..utils.ml_metrics import Loss\nfrom ..utils.device import DEVICE\nfrom ..utils.misc import pbar\nfrom ..utils.data import sort_predictions\nfrom ..datasets import MultimodalDataset\nfrom ..metrics import Metric\n\nlogger = logging.getLogger(\'nmtpytorch\')\n\n\nclass NMT(nn.Module):\n    supports_beam_search = True\n\n    def set_defaults(self):\n        self.defaults = {\n            \'emb_dim\': 128,             # Source and target embedding sizes\n            \'emb_maxnorm\': None,        # Normalize embeddings l2 norm to 1\n            \'emb_gradscale\': False,     # Scale embedding gradients w.r.t. batch frequency\n            \'enc_dim\': 256,             # Encoder hidden size\n            \'enc_type\': \'gru\',          # Encoder type (gru|lstm)\n            \'enc_lnorm\': False,         # Add layer-normalization to encoder output\n            \'enc_bidirectional\': True,  # Whether the RNN encoder should be bidirectional\n            \'n_encoders\': 1,            # Number of stacked encoders\n            \'dec_dim\': 256,             # Decoder hidden size\n            \'dec_type\': \'gru\',          # Decoder type (gru|lstm)\n            \'dec_variant\': \'cond\',      # (cond|simplegru|vector)\n            \'dec_init\': \'mean_ctx\',     # How to initialize decoder (zero/mean_ctx/feats)\n            \'dec_init_size\': None,      # feature vector dimensionality for\n            \'dec_init_activ\': \'tanh\',   # Decoder initialization activation func\n                                        # dec_init == \'feats\'\n            \'att_type\': \'mlp\',          # Attention type (mlp|dot)\n            \'att_temp\': 1.,             # Attention temperature\n            \'att_activ\': \'tanh\',        # Attention non-linearity (all torch nonlins)\n            \'att_mlp_bias\': False,      # Enables bias in attention mechanism\n            \'att_bottleneck\': \'ctx\',    # Bottleneck dimensionality (ctx|hid)\n            \'att_transform_ctx\': True,  # Transform annotations before attention\n            \'att_ctx2hid\': True,        # Add one last FC layer on top of the ctx\n            \'dropout_emb\': 0,           # Simple dropout to source embeddings\n            \'dropout_ctx\': 0,           # Simple dropout to source encodings\n            \'dropout_out\': 0,           # Simple dropout to decoder output\n            \'dropout_enc\': 0,           # Intra-encoder dropout if n_encoders > 1\n            \'tied_emb\': False,          # Share embeddings: (False|2way|3way)\n            \'direction\': None,          # Network directionality, i.e. en->de\n            \'max_len\': 80,              # Reject sentences where \'bucket_by\' length > 80\n            \'bucket_by\': None,          # A key like \'en\' to define w.r.t which dataset\n                                        # the batches will be sorted\n            \'bucket_order\': None,       # Curriculum: ascending/descending/None\n            \'sampler_type\': \'bucket\',   # bucket or approximate\n            \'sched_sampling\': 0,        # Scheduled sampling ratio\n            \'short_list\': 0,            # Short list vocabularies (0: disabled)\n            \'bos_type\': \'emb\',          # \'emb\': default learned emb\n            \'bos_activ\': None,          #\n            \'bos_dim\': None,            #\n            \'out_logic\': \'simple\',      # \'simple\' or \'deep\' output\n            \'dec_inp_activ\': None,      # Non-linearity for GRU2 input in dec\n        }\n\n    def __init__(self, opts):\n        super().__init__()\n\n        # opts -> config file sections {.model, .data, .vocabulary, .train}\n        self.opts = opts\n\n        # Vocabulary objects\n        self.vocabs = {}\n\n        # Each auxiliary loss should be stored inside this dictionary\n        # in order to be taken into account by the mainloop for multi-tasking\n        self.aux_loss = {}\n\n        # Setup options\n        self.opts.model = self.set_model_options(opts.model)\n\n        # Parse topology & languages\n        self.topology = Topology(self.opts.model[\'direction\'])\n\n        # Load vocabularies here\n        for name, fname in self.opts.vocabulary.items():\n            self.vocabs[name] = Vocabulary(fname, short_list=self.opts.model[\'short_list\'])\n\n        # Inherently non multi-lingual aware\n        slangs = self.topology.get_src_langs()\n        tlangs = self.topology.get_trg_langs()\n        if slangs:\n            self.sl = slangs[0]\n            self.src_vocab = self.vocabs[self.sl]\n            self.n_src_vocab = len(self.src_vocab)\n        if tlangs:\n            self.tl = tlangs[0]\n            self.trg_vocab = self.vocabs[self.tl]\n            self.n_trg_vocab = len(self.trg_vocab)\n            # Need to be set for early-stop evaluation\n            # NOTE: This should come from config or elsewhere\n            self.val_refs = self.opts.data[\'val_set\'][self.tl]\n\n        # Check vocabulary sizes for 3way tying\n        if self.opts.model.get(\'tied_emb\', False) not in [False, \'2way\', \'3way\']:\n            raise RuntimeError(\n                ""\'{}\' not recognized for tied_emb."".format(self.opts.model[\'tied_emb\']))\n\n        if self.opts.model.get(\'tied_emb\', False) == \'3way\':\n            assert self.n_src_vocab == self.n_trg_vocab, \\\n                ""The vocabulary sizes do not match for 3way tied embeddings.""\n\n    def __repr__(self):\n        s = super().__repr__() + \'\\n\'\n        for vocab in self.vocabs.values():\n            s += ""{}\\n"".format(vocab)\n        s += ""{}\\n"".format(get_n_params(self))\n        return s\n\n    def set_model_options(self, model_opts):\n        self.set_defaults()\n        for opt, value in model_opts.items():\n            if opt in self.defaults:\n                # Override defaults from config\n                self.defaults[opt] = value\n            else:\n                logger.info(\'Warning: unused model option: {}\'.format(opt))\n        return self.defaults\n\n    def reset_parameters(self):\n        for name, param in self.named_parameters():\n            # Skip 1-d biases and scalars\n            if param.requires_grad and param.dim() > 1:\n                nn.init.kaiming_normal_(param.data)\n        # Reset padding embedding to 0\n        if hasattr(self, \'enc\') and hasattr(self.enc, \'emb\'):\n            with torch.no_grad():\n                self.enc.emb.weight.data[0].fill_(0)\n\n    def setup(self, is_train=True):\n        """"""Sets up NN topology by creating the layers.""""""\n        ########################\n        # Create Textual Encoder\n        ########################\n        self.enc = TextEncoder(\n            input_size=self.opts.model[\'emb_dim\'],\n            hidden_size=self.opts.model[\'enc_dim\'],\n            n_vocab=self.n_src_vocab,\n            bidirectional=self.opts.model[\'enc_bidirectional\'],\n            rnn_type=self.opts.model[\'enc_type\'],\n            dropout_emb=self.opts.model[\'dropout_emb\'],\n            dropout_ctx=self.opts.model[\'dropout_ctx\'],\n            dropout_rnn=self.opts.model[\'dropout_enc\'],\n            num_layers=self.opts.model[\'n_encoders\'],\n            emb_maxnorm=self.opts.model[\'emb_maxnorm\'],\n            emb_gradscale=self.opts.model[\'emb_gradscale\'],\n            layer_norm=self.opts.model[\'enc_lnorm\'])\n\n        self.ctx_sizes = {str(self.sl): self.enc.ctx_size}\n\n        ################\n        # Create Decoder\n        ################\n        Decoder = get_decoder(self.opts.model[\'dec_variant\'])\n        self.dec = Decoder(\n            input_size=self.opts.model[\'emb_dim\'],\n            hidden_size=self.opts.model[\'dec_dim\'],\n            n_vocab=self.n_trg_vocab,\n            rnn_type=self.opts.model[\'dec_type\'],\n            ctx_size_dict=self.ctx_sizes,\n            ctx_name=str(self.sl),\n            tied_emb=self.opts.model[\'tied_emb\'],\n            dec_init=self.opts.model[\'dec_init\'],\n            dec_init_size=self.opts.model[\'dec_init_size\'],\n            dec_init_activ=self.opts.model[\'dec_init_activ\'],\n            att_type=self.opts.model[\'att_type\'],\n            att_temp=self.opts.model[\'att_temp\'],\n            att_activ=self.opts.model[\'att_activ\'],\n            att_ctx2hid=self.opts.model[\'att_ctx2hid\'],\n            transform_ctx=self.opts.model[\'att_transform_ctx\'],\n            mlp_bias=self.opts.model[\'att_mlp_bias\'],\n            att_bottleneck=self.opts.model[\'att_bottleneck\'],\n            dropout_out=self.opts.model[\'dropout_out\'],\n            emb_maxnorm=self.opts.model[\'emb_maxnorm\'],\n            emb_gradscale=self.opts.model[\'emb_gradscale\'],\n            sched_sample=self.opts.model[\'sched_sampling\'],\n            bos_type=self.opts.model[\'bos_type\'],\n            bos_dim=self.opts.model[\'bos_dim\'],\n            bos_activ=self.opts.model[\'bos_activ\'],\n            bos_bias=self.opts.model[\'bos_type\'] == \'feats\',\n            out_logic=self.opts.model[\'out_logic\'],\n            dec_inp_activ=self.opts.model[\'dec_inp_activ\'])\n\n        # Share encoder and decoder weights\n        if self.opts.model[\'tied_emb\'] == \'3way\':\n            self.enc.emb.weight = self.dec.emb.weight\n\n    def load_data(self, split, batch_size, mode=\'train\'):\n        """"""Loads the requested dataset split.""""""\n        self.dataset = MultimodalDataset(\n            data=self.opts.data[\'{}_set\'.format(split)],\n            mode=mode, batch_size=batch_size,\n            vocabs=self.vocabs, topology=self.topology,\n            bucket_by=self.opts.model[\'bucket_by\'],\n            max_len=self.opts.model[\'max_len\'],\n            bucket_order=self.opts.model[\'bucket_order\'],\n            sampler_type=self.opts.model[\'sampler_type\'])\n        logger.info(self.dataset)\n        return self.dataset\n\n    def get_bos(self, batch_size):\n        """"""Returns a representation for <bos> embeddings for decoding.""""""\n        return torch.LongTensor(batch_size).fill_(self.trg_vocab[\'<bos>\'])\n\n    def encode(self, batch, **kwargs):\n        """"""Encodes all inputs and returns a dictionary.\n\n        Arguments:\n            batch (dict): A batch of samples with keys designating the\n                information sources.\n\n        Returns:\n            dict:\n                A dictionary where keys are source modalities compatible\n                with the data loader and the values are tuples where the\n                elements are encodings and masks. The mask can be ``None``\n                if the relevant modality does not require a mask.\n        """"""\n        d = {str(self.sl): self.enc(batch[self.sl])}\n        if \'feats\' in batch:\n            d[\'feats\'] = (batch[\'feats\'], None)\n        return d\n\n    def forward(self, batch, **kwargs):\n        """"""Computes the forward-pass of the network and returns batch loss.\n\n        Arguments:\n            batch (dict): A batch of samples with keys designating the source\n                and target modalities.\n\n        Returns:\n            Tensor:\n                A scalar loss normalized w.r.t batch size and token counts.\n        """"""\n        # Get loss dict\n        result = self.dec(self.encode(batch), batch[self.tl])\n        result[\'n_items\'] = torch.nonzero(batch[self.tl][1:]).shape[0]\n        return result\n\n    def test_performance(self, data_loader, dump_file=None):\n        """"""Computes test set loss over the given DataLoader instance.""""""\n        loss = Loss()\n\n        for batch in pbar(data_loader, unit=\'batch\'):\n            batch.device(DEVICE)\n            out = self.forward(batch)\n            loss.update(out[\'loss\'], out[\'n_items\'])\n\n        return [\n            Metric(\'LOSS\', loss.get(), higher_better=False),\n        ]\n\n    def get_decoder(self, task_id=None):\n        """"""Compatibility function for multi-tasking architectures.""""""\n        return self.dec\n\n    def register_tensorboard(self, handle):\n        """"""Stores tensorboard hook for custom logging.""""""\n        self.tboard = handle\n\n    @staticmethod\n    def beam_search(models, data_loader, task_id=None, beam_size=12, max_len=200,\n                    lp_alpha=0., suppress_unk=False, n_best=False):\n        """"""An efficient implementation for beam-search algorithm.\n\n        Arguments:\n            models (list of Model): Model instance(s) derived from `nn.Module`\n                defining a set of methods. See `models/nmt.py`.\n            data_loader (DataLoader): A ``DataLoader`` instance.\n            task_id (str, optional): For multi-output models, this selects\n                the decoder. (Default: None)\n            beam_size (int, optional): The size of the beam. (Default: 12)\n            max_len (int, optional): Maximum target length to stop beam-search\n                if <eos> is still not generated. (Default: 200)\n            lp_alpha (float, optional): If > 0, applies Google\'s length-penalty\n                normalization instead of simple length normalization.\n                lp: ((5 + |Y|)^lp_alpha / (5 + 1)^lp_alpha)\n            suppress_unk (bool, optional): If `True`, suppresses the log-prob\n                of <unk> token.\n            n_best (bool, optional): If `True`, returns n-best list of the beam\n                with the associated scores.\n\n        Returns:\n            list:\n                A list of hypotheses in surface form.\n        """"""\n        def tile_ctx_dict(ctx_dict, idxs):\n            """"""Returns dict of 3D tensors repeatedly indexed along the sample axis.""""""\n            # 1st: tensor, 2nd optional mask\n            return {\n                k: (t[:, idxs], None if mask is None else mask[:, idxs])\n                for k, (t, mask) in ctx_dict.items()\n            }\n\n        def check_context_ndims(ctx_dict):\n            for name, (ctx, mask) in ctx_dict.items():\n                assert ctx.dim() == 3, \\\n                    f""{name}\'s 1st dim should always be a time dimension.""\n\n        # This is the batch-size requested by the user but with sorted\n        # batches, efficient batch-size will be <= max_batch_size\n        max_batch_size = data_loader.batch_sampler.batch_size\n        k = beam_size\n        inf = -1000\n        results = []\n        enc_args = {}\n\n        if task_id is None:\n            # For classical models that have single encoder, decoder and\n            # target vocabulary\n            decs = [m.dec for m in models]\n            f_inits = [dec.f_init for dec in decs]\n            f_nexts = [dec.f_next for dec in decs]\n            vocab = models[0].trg_vocab\n        else:\n            # A specific input-output topology has been requested\n            task = Topology(task_id)\n            enc_args[\'enc_ids\'] = task.srcs\n            # For new multi-target models: select the first target decoder\n            decs = [m.get_decoder(task.first_trg) for m in models]\n            # Get the necessary init() and next() methods\n            f_inits = [dec.f_init for dec in decs]\n            f_nexts = [dec.f_next for dec in decs]\n            # Get the corresponding vocabulary for the first target\n            vocab = models[0].vocabs[task.first_trg]\n\n        # Common parts\n        encoders = [m.encode for m in models]\n        unk = vocab[\'<unk>\']\n        eos = vocab[\'<eos>\']\n        n_vocab = len(vocab)\n\n        # Tensorized beam that will shrink and grow up to max_batch_size\n        beam_storage = torch.zeros(\n            max_len, max_batch_size, k, dtype=torch.long, device=DEVICE)\n        mask = torch.arange(max_batch_size * k, device=DEVICE)\n        nll_storage = torch.zeros(max_batch_size, device=DEVICE)\n\n        for batch in pbar(data_loader, unit=\'batch\'):\n            batch.device(DEVICE)\n\n            # Always use the initial storage\n            beam = beam_storage.narrow(1, 0, batch.size).zero_()\n\n            # Mask to apply to pdxs.view(-1) to fix indices\n            nk_mask = mask.narrow(0, 0, batch.size * k)\n\n            # nll: batch_size x 1 (will get expanded further)\n            nll = nll_storage.narrow(0, 0, batch.size).unsqueeze(1)\n\n            # Tile indices to use in the loop to expand first dim\n            tile = range(batch.size)\n\n            # Encode source modalities\n            ctx_dicts = [encode(batch, **enc_args) for encode in encoders]\n\n            # Sanity check one of the context dictionaries for dimensions\n            check_context_ndims(ctx_dicts[0])\n\n            # Get initial decoder state (N*H)\n            h_ts = [f_init(ctx_dict) for f_init, ctx_dict in zip(f_inits, ctx_dicts)]\n\n            # we always have <bos> tokens except that the returned embeddings\n            # may differ from one model to another.\n            idxs = models[0].get_bos(batch.size).to(DEVICE)\n\n            for tstep in range(max_len):\n                # Select correct positions from source context\n                ctx_dicts = [tile_ctx_dict(cd, tile) for cd in ctx_dicts]\n\n                # Get log probabilities and next state\n                # log_p: batch_size x vocab_size (t = 0)\n                #        batch_size*beam_size x vocab_size (t > 0)\n                # NOTE: get_emb does not exist in some models, fix this.\n                log_ps, h_ts = zip(\n                    *[f_next(cd, dec.get_emb(idxs, tstep), h_t[tile]) for\n                      f_next, dec, cd, h_t in zip(f_nexts, decs, ctx_dicts, h_ts)])\n\n                # Do the actual averaging of log-probabilities\n                log_p = sum(log_ps).data\n\n                if suppress_unk:\n                    log_p[:, unk] = inf\n\n                # Detect <eos>\'d hyps\n                idxs = (idxs == 2).nonzero()\n                if idxs.numel():\n                    if idxs.numel() == batch.size * k:\n                        break\n                    idxs.squeeze_(-1)\n                    # Unfavor all candidates\n                    log_p.index_fill_(0, idxs, inf)\n                    # Favor <eos> so that it gets selected\n                    log_p.view(-1).index_fill_(0, idxs * n_vocab + 2, 0)\n\n                # Expand to 3D, cross-sum scores and reduce back to 2D\n                # log_p: batch_size x vocab_size ( t = 0 )\n                #   nll: batch_size x beam_size (x 1)\n                # nll becomes: batch_size x beam_size*vocab_size here\n                # Reduce (N, K*V) to k-best\n                nll, beam[tstep] = nll.unsqueeze_(2).add(log_p.view(\n                    batch.size, -1, n_vocab)).view(batch.size, -1).topk(\n                        k, sorted=False, largest=True)\n\n                # previous indices into the beam and current token indices\n                pdxs = beam[tstep] / n_vocab\n                beam[tstep].remainder_(n_vocab)\n                idxs = beam[tstep].view(-1)\n\n                # Compute correct previous indices\n                # Mask is needed since we\'re in flattened regime\n                tile = pdxs.view(-1) + (nk_mask / k) * (k if tstep else 1)\n\n                if tstep > 0:\n                    # Permute all hypothesis history according to new order\n                    beam[:tstep] = beam[:tstep].gather(2, pdxs.repeat(tstep, 1, 1))\n\n            # Put an explicit <eos> to make idxs_to_sent happy\n            beam[max_len - 1] = eos\n\n            # Find lengths by summing tokens not in (pad,bos,eos)\n            len_penalty = beam.gt(2).float().sum(0).clamp(min=1)\n\n            if lp_alpha > 0.:\n                len_penalty = ((5 + len_penalty)**lp_alpha) / 6**lp_alpha\n\n            # Apply length normalization\n            nll.div_(len_penalty)\n\n            if n_best:\n                # each elem is sample, then candidate\n                tbeam = beam.permute(1, 2, 0).to(\'cpu\').tolist()\n                scores = nll.to(\'cpu\').tolist()\n                results.extend(\n                    [(vocab.list_of_idxs_to_sents(b), s) for b, s in zip(tbeam, scores)])\n            else:\n                # Get best-1 hypotheses\n                top_hyps = nll.topk(1, sorted=False, largest=True)[1].squeeze(1)\n                hyps = beam[:, range(batch.size), top_hyps].t().to(\'cpu\')\n                results.extend(vocab.list_of_idxs_to_sents(hyps.tolist()))\n\n        # Recover order of the samples if necessary\n        return sort_predictions(data_loader, results)\n'"
nmtpytorch/models/simple_mmt.py,1,"b'# -*- coding: utf-8 -*-\nimport torch\nimport logging\n\nfrom .nmt import NMT\nfrom ..datasets import MultimodalDataset\nfrom ..layers import MultimodalTextEncoder\nfrom ..layers import ConditionalDecoder\n\nlogger = logging.getLogger(\'nmtpytorch\')\n\n\nclass SimpleMMT(NMT):\n    """"""A encoder/decoder enriched multimodal NMT.\n\n        Integration types (feat_fusion argument)\n            \'encinit\':      Initialize RNNs in the encoder\n            \'decinit\':      Initializes first decoder RNN.\n            \'encdecinit\':   Initializes RNNs in the encoder & first decoder RNN.\n            \'trgmul\':       Multiplicative interaction with trg embs.\n            \'srcmul\':       Multiplicative interaction with src embs.\n            \'ctxmul\':       Multiplicative interaction with src encodings.\n            \'concat\':       Concat the embeddings and features (doubles RNN input)\n            \'sum\':          Sum the embeddings with projected features\n            \'prepend\':      Input sequence: [vis, embs, eos]\n            \'append\':       Input sequence: [embs, vis, eos]\n    """"""\n    def __init__(self, opts):\n        super().__init__(opts)\n\n    def set_defaults(self):\n        # Set parent defaults\n        super().set_defaults()\n        # NOTE: You should not use dec_init == feats with this model.\n        # Use ""feat_fusion:decinit"" instead.\n        self.defaults.update({\n            \'feat_dim\': 2048,           # Feature dimension for multimodal encoder\n            \'feat_activ\': None,         # Feature non-linearity for multimodal encoder\n            \'feat_fusion\': \'encinit\',   # By default initialize only the encoder\n        })\n\n    def reset_parameters(self):\n        super().reset_parameters()\n        # Reset padding embeddings to 0\n        with torch.no_grad():\n            self.enc.emb.weight.data[0].fill_(0)\n\n    def setup(self, is_train=True):\n        """"""Sets up NN topology by creating the layers.""""""\n        # Hack to sync enc-decinit computation\n        self.dec_requires_img = False\n        if self.opts.model[\'feat_fusion\']:\n            if \'decinit\' in self.opts.model[\'feat_fusion\']:\n                self.opts.model[\'dec_init\'] = \'feats\'\n                self.opts.model[\'dec_init_size\'] = self.opts.model[\'feat_dim\']\n                self.opts.model[\'dec_init_activ\'] = self.opts.model[\'feat_activ\']\n                self.dec_requires_img = True\n            elif self.opts.model[\'feat_fusion\'].startswith(\'trg\'):\n                self.dec_requires_img = True\n\n        self.enc = MultimodalTextEncoder(\n            input_size=self.opts.model[\'emb_dim\'],\n            hidden_size=self.opts.model[\'enc_dim\'],\n            n_vocab=self.n_src_vocab,\n            rnn_type=self.opts.model[\'enc_type\'],\n            dropout_emb=self.opts.model[\'dropout_emb\'],\n            dropout_ctx=self.opts.model[\'dropout_ctx\'],\n            dropout_rnn=self.opts.model[\'dropout_enc\'],\n            num_layers=self.opts.model[\'n_encoders\'],\n            emb_maxnorm=self.opts.model[\'emb_maxnorm\'],\n            emb_gradscale=self.opts.model[\'emb_gradscale\'],\n            layer_norm=self.opts.model[\'enc_lnorm\'],\n            feat_size=self.opts.model[\'feat_dim\'],\n            feat_activ=self.opts.model[\'feat_activ\'],\n            feat_fusion=self.opts.model[\'feat_fusion\'])\n\n        self.dec = ConditionalDecoder(\n            input_size=self.opts.model[\'emb_dim\'],\n            hidden_size=self.opts.model[\'dec_dim\'],\n            n_vocab=self.n_trg_vocab,\n            rnn_type=self.opts.model[\'dec_type\'],\n            ctx_size_dict=self.ctx_sizes,\n            ctx_name=str(self.sl),\n            tied_emb=self.opts.model[\'tied_emb\'],\n            dec_init=self.opts.model[\'dec_init\'],\n            dec_init_size=self.opts.model[\'dec_init_size\'],\n            dec_init_activ=self.opts.model[\'dec_init_activ\'],\n            att_type=self.opts.model[\'att_type\'],\n            att_temp=self.opts.model[\'att_temp\'],\n            att_activ=self.opts.model[\'att_activ\'],\n            transform_ctx=self.opts.model[\'att_transform_ctx\'],\n            mlp_bias=self.opts.model[\'att_mlp_bias\'],\n            att_bottleneck=self.opts.model[\'att_bottleneck\'],\n            dropout_out=self.opts.model[\'dropout_out\'],\n            emb_maxnorm=self.opts.model[\'emb_maxnorm\'],\n            emb_gradscale=self.opts.model[\'emb_gradscale\'],\n            sched_sample=self.opts.model[\'sched_sampling\'],\n            out_logic=self.opts.model[\'out_logic\'],\n            emb_interact=self.opts.model[\'feat_fusion\'],\n            emb_interact_dim=self.opts.model[\'feat_dim\'],\n            emb_interact_activ=self.opts.model[\'feat_activ\'])\n\n        # Share encoder and decoder weights\n        if self.opts.model[\'tied_emb\'] == \'3way\':\n            self.enc.emb.weight = self.dec.emb.weight\n\n        # Use the same representation everywhere\n        if self.opts.model[\'feat_fusion\'] == \'encdecinit\':\n            self.enc.ff_vis.weight = self.dec.ff_dec_init.weight\n\n    def load_data(self, split, batch_size, mode=\'train\'):\n        """"""Loads the requested dataset split.""""""\n        dataset = MultimodalDataset(\n            data=self.opts.data[split + \'_set\'],\n            mode=mode, batch_size=batch_size,\n            vocabs=self.vocabs, topology=self.topology,\n            bucket_by=self.opts.model[\'bucket_by\'],\n            max_len=self.opts.model.get(\'max_len\', None),\n            order_file=self.opts.data[split + \'_set\'].get(\'ord\', None))\n        logger.info(dataset)\n        return dataset\n\n    def encode(self, batch, **kwargs):\n        d = {str(self.sl): self.enc(batch[self.sl], v=batch.get(\'feats\', None))}\n\n        # # It may also be decoder-side integration\n        if self.dec_requires_img:\n            d[\'feats\'] = (batch[\'feats\'], None)\n\n        return d\n'"
nmtpytorch/models/tfnmt.py,3,"b'# -*- coding: utf-8 -*-\nimport logging\n\nimport torch\nfrom torch import nn\n\nfrom ..layers.transformers import *\n\nfrom . import NMT\n\nlogger = logging.getLogger(\'nmtpytorch\')\n\n\nclass TransformerNMT(NMT):\n    supports_beam_search = True\n\n    def set_defaults(self):\n        self.defaults = {\n            \'model_dim\': 512,           # model_dim\n            \'ff_dim\': 2048,             # Positionwise FF inner dimension\n            \'n_enc_layers\': 6,          # Number of encoder layers\n            \'n_dec_layers\': 6,          # Number of decoder layers\n            \'n_heads\': 8,               # Number of attention heads\n            \'direction\': None,          # Network directionality, i.e. en->de\n            \'max_len\': None,            # Reject sentences where \'bucket_by\' length > 80\n            \'bucket_by\': None,          # A key like \'en\' to define w.r.t which dataset\n                                        # the batches will be sorted\n            \'bucket_order\': None,       # Curriculum: ascending/descending/None\n            \'sampler_type\': \'bucket\',   # bucket or approximate\n            \'short_list\': 0,            # Vocabulary short listing\n        }\n\n    def __init__(self, opts):\n        super().__init__(opts)\n\n    def reset_parameters(self):\n        for name, param in self.named_parameters():\n            # Skip 1-d biases and scalars\n            if param.requires_grad and param.dim() > 1:\n                nn.init.kaiming_normal_(param.data)\n        # Reset padding embedding to 0\n        with torch.no_grad():\n            self.src_emb.weight.data[0].fill_(0)\n            self.trg_emb.weight.data[0].fill_(0)\n\n    def setup(self, is_train=True):\n        """"""Sets up NN topology by creating the layers.""""""\n        # Create the embeddings\n        self.src_emb = TFEmbedding(self.n_src_vocab, self.opts.model[\'model_dim\'])\n        self.trg_emb = TFEmbedding(self.n_trg_vocab, self.opts.model[\'model_dim\'])\n        self.enc = TFEncoder(\n            self.opts.model[\'model_dim\'], self.opts.model[\'ff_dim\'],\n            self.opts.model[\'n_heads\'], self.opts.model[\'n_enc_layers\'])\n        self.dec = TFDecoder(\n            self.opts.model[\'model_dim\'], self.opts.model[\'ff_dim\'],\n            self.opts.model[\'n_heads\'], self.opts.model[\'n_dec_layers\'])\n        self.seq_loss = torch.nn.NLLLoss(reduction=\'sum\', ignore_index=0)\n\n    def encode(self, batch, **kwargs):\n        # mask: (tstep, bsize)\n        mask = batch[self.sl].ne(0).float()\n\n        # embs: (tstep, bsize, dim)\n        embs = self.src_emb(batch[self.sl])\n        h, mask = self.enc(embs, mask=mask)\n\n        d = {str(self.sl): (h, mask)}\n        return d\n\n    def forward(self, batch, **kwargs):\n        # Get loss dict\n        enc = self.encode(batch)\n\n        dec_input = batch[self.tl]\n\n#         result = self.dec(self.encode(batch), batch[self.tl])\n        # result[\'n_items\'] = torch.nonzero(batch[self.tl][1:]).shape[0]\n        # return result\n'"
nmtpytorch/samplers/__init__.py,0,"b""# -*- coding: utf-8 -*-\nfrom .bucket import BucketBatchSampler\nfrom .approx import ApproximateBucketBatchSampler\n\ndef get_sampler(type_):\n    return {\n        'bucket': BucketBatchSampler,\n        'approximate': ApproximateBucketBatchSampler,\n    }[type_.lower()]\n"""
nmtpytorch/samplers/approx.py,0,"b'# -*- coding: utf-8 -*-\nimport math\nimport logging\nfrom collections import defaultdict\n\nimport numpy as np\n\nfrom ..utils.device import DEVICE_IDS\nfrom . import BucketBatchSampler\n\nlogger = logging.getLogger(\'nmtpytorch\')\n\n\nclass ApproximateBucketBatchSampler(BucketBatchSampler):\n    r""""""Samples batch indices from sequence-length buckets efficiently\n    with very little memory overhead.\n\n    Different from `BucketBatchSampler`, this class bins data samples w.r.t\n    lengths but does not guarantee that each bucket contains necessarily\n    same-length sequences. Further padding/packing/masking should be done\n    by detecting possible <pad> items in tensors.\n\n    Arguments:\n        batch_size (int): Size of mini-batch.\n        sort_lens (list): List of source or target lengths corresponding to each\n            item in the dataset.\n        max_len (int, optional): A maximum sequence length that will be used\n            to filter out very long sequences. ``None`` means no filtering.\n        store_indices (bool, optional): If ``True``, indices that will unsort\n            the dataset will be stored. This used by beam search/inference.\n        order (str, optional): Default is ``None``, i.e. buckets are shuffled.\n            If ``ascending`` or ``descending``, will iterate w.r.t bucket\n            lengths to implement length-based curriculum learning.\n    """"""\n\n    def __init__(self, batch_size, sort_lens, max_len=None,\n                 store_indices=False, order=None):\n        assert order in (None, \'ascending\', \'descending\'), \\\n            ""order should be None, \'ascending\' or \'descending\'""\n\n        self.batch_size = batch_size\n        self.max_len = max_len\n        self.n_rejects = 0\n        self.order = order\n        self.store_indices = store_indices\n\n        # Additional balancing logic for multi-GPU\n        self.n_devices = len(DEVICE_IDS) if DEVICE_IDS else 1\n\n        # Buckets: sort_lens -> list of sample indices\n        self.buckets = defaultdict(list)\n\n        # Pre-compute how many times a bucket will be sampled\n        self.bucket_idxs = []\n\n        # Fill the buckets while optionally filtering out long sequences\n        if self.max_len is not None:\n            for idx, len_ in enumerate(sort_lens):\n                if len_ <= self.max_len:\n                    self.buckets[len_].append(idx)\n                else:\n                    self.n_rejects += 1\n            logger.info(\'{} samples rejected because of length filtering @ {}\'.format(\n                self.n_rejects, self.max_len))\n        else:\n            # No length filtering\n            for idx, len_ in enumerate(sort_lens):\n                self.buckets[len_].append(idx)\n\n        ######################################\n        # Modified part compared to base class\n        ######################################\n        ordered_idxs = []\n        min_bucket_size = self.batch_size * 5\n        for length in sorted(self.buckets):\n            ordered_idxs.extend(self.buckets[length])\n\n        # Reset buckets\n        self.buckets = {}\n        n_elems = len(ordered_idxs)\n\n        # Bin sorted buckets approximately\n        for idx, start in enumerate(range(0, n_elems, min_bucket_size)):\n            self.buckets[idx] = ordered_idxs[start:start + min_bucket_size]\n\n        # number of elems in the last bucket\n        last_bucket_size = len(self.buckets[idx])\n        # number of elems in the last batch of last bucket\n        last_batch_size = last_bucket_size % self.batch_size\n        # how many should we remove to make the last batch divisible for\n        # many GPUs\n        n_remove_from_last = last_batch_size % self.n_devices\n        end_point = last_bucket_size - n_remove_from_last\n        self.buckets[idx] = self.buckets[idx][:end_point]\n        if n_remove_from_last > 0:\n            logger.info(\'Removed {} samples to balance buckets.\'.format(\n                n_remove_from_last))\n\n        self.stats = {k: len(self.buckets[k]) for k in sorted(self.buckets)}\n\n        for len_ in self.buckets:\n            # Convert bucket to numpy array\n            np_bucket = np.array(self.buckets[len_])\n\n            # How many batches will be done for this bucket?\n            bucket_bs = np_bucket.size / self.batch_size\n            idxs = [len_] * math.ceil(bucket_bs)\n\n            self.buckets[len_] = np_bucket\n            self.bucket_idxs.extend(idxs)\n\n        # Convert to numpy array\n        self.bucket_idxs = np.array(self.bucket_idxs)\n\n        # Set number of batches\n        self.n_batches = len(self.bucket_idxs)\n'"
nmtpytorch/samplers/bucket.py,1,"b'# -*- coding: utf-8 -*-\nimport math\nimport logging\nfrom collections import defaultdict\n\nimport numpy as np\n\nfrom torch.utils.data.sampler import Sampler\n\nlogger = logging.getLogger(\'nmtpytorch\')\n\n\nclass BucketBatchSampler(Sampler):\n    r""""""Samples batch indices from sequence-length buckets efficiently\n    with very little memory overhead.\n\n    Epoch overhead for 5M dataset with batch_size=32 is around 400ms.\n\n    Arguments:\n        batch_size (int): Size of mini-batch.\n        sort_lens (list): List of source or target lengths corresponding to each\n            item in the dataset.\n        max_len (int, optional): A maximum sequence length that will be used\n            to filter out very long sequences. ``None`` means no filtering.\n        store_indices (bool, optional): If ``True``, indices that will unsort\n            the dataset will be stored. This used by beam search/inference.\n        order (str, optional): Default is ``None``, i.e. buckets are shuffled.\n            If ``ascending`` or ``descending``, will iterate w.r.t bucket\n            lengths to implement length-based curriculum learning.\n\n    Example:\n        # Generate dummy length information\n        >> lengths = np.random.randint(1, 20, size=10000)\n        >> sampler = BucketBatchSampler(batch_size=10, sort_lens=lengths)\n        >> batch = list(sampler)[0]\n        >> batch\n        [7526, 8473, 9194, 1030, 1568, 4182, 3082, 827, 3688, 9336]\n        >> [lengths[i] for i in batch]\n        # All samples in the batch have same length\n        [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n\n    """"""\n\n    def __init__(self, batch_size, sort_lens,\n                 max_len=None, store_indices=False, order=None):\n        self.batch_size = batch_size\n        self.max_len = max_len\n        self.store_indices = store_indices\n        self.n_rejects = 0\n        self.order = order\n\n        assert sort_lens is not None, \\\n            ""BucketBatchSampler() received `sort_lens` == None""\n\n        assert self.order in (None, \'ascending\', \'descending\'), \\\n            ""order should be None, \'ascending\' or \'descending\'""\n\n        # Buckets: sort_lens -> list of sample indices\n        self.buckets = defaultdict(list)\n\n        # Fill the buckets while optionally filtering out long sequences\n        if self.max_len is not None:\n            for idx, len_ in enumerate(sort_lens):\n                if len_ <= self.max_len:\n                    self.buckets[len_].append(idx)\n                else:\n                    self.n_rejects += 1\n            logger.info(\'{} samples rejected because of length filtering @ {}\'.format(\n                self.n_rejects, self.max_len))\n        else:\n            # No length filtering\n            for idx, len_ in enumerate(sort_lens):\n                self.buckets[len_].append(idx)\n\n        # Pre-compute how many times a bucket will be sampled\n        self.bucket_idxs = []\n\n        self.stats = {k: len(self.buckets[k]) for k in sorted(self.buckets)}\n\n        for len_ in self.buckets:\n            # Convery bucket to numpy array\n            np_bucket = np.array(self.buckets[len_])\n\n            # How many batches will be done for this bucket?\n            bucket_bs = np_bucket.size / self.batch_size\n            idxs = [len_] * math.ceil(bucket_bs)\n\n            self.buckets[len_] = np_bucket\n            self.bucket_idxs.extend(idxs)\n\n        # Convert to numpy array\n        self.bucket_idxs = np.array(self.bucket_idxs)\n\n        # Set number of batches\n        self.n_batches = len(self.bucket_idxs)\n\n    def __iter__(self):\n        # Keep offsets for each bucket for efficiency\n        bucket_offsets = {}\n\n        # Random access indices\n        bucket_views = {}\n\n        # If beam-search with ordered batches, original indices will be\n        # necessary.\n        self.orig_idxs = []\n\n        # Create permuted access indices for each bucket\n        # to avoid shuffling the lists\n        for len_, elems in self.buckets.items():\n            bucket_offsets[len_] = 0\n            perms = np.random.permutation(len(elems))\n            bucket_views[len_] = perms\n\n        if self.order is None:\n            # Shuffle bucket order\n            shuf_idxs = np.random.permutation(self.bucket_idxs)\n        elif self.order == ""ascending"":\n            # Start from shortest sequences and increase\n            shuf_idxs = np.sort(self.bucket_idxs)\n        elif self.order == ""descending"":\n            # Start from longest sequences and decrease\n            shuf_idxs = -np.sort(-self.bucket_idxs)\n\n        # For each bucket, slide the window to yield the next batch\n        for bidx in shuf_idxs:\n            # Get offset pointer for this bucket: 0 initially\n            offset = bucket_offsets[bidx]\n\n            # Convert them to permuted view\n            idxs = bucket_views[bidx][offset: offset + self.batch_size]\n\n            # Increment offset\n            bucket_offsets[bidx] += len(idxs)\n\n            # Get actual sample indices\n            sidxs = self.buckets[bidx][idxs]\n\n            if self.store_indices:\n                self.orig_idxs.extend(sidxs)\n\n            # Return sample indices\n            yield sidxs\n\n    def __len__(self):\n        """"""Returns how many batches are inside.""""""\n        return self.n_batches\n\n    def __repr__(self):\n        return f""BucketBatchSampler(order={self.order}, max_len={self.max_len}, n_rejects={self.n_rejects})""\n'"
nmtpytorch/utils/__init__.py,0,"b""__all__ = ['misc', 'device', 'nn', 'data', 'io', 'topology']\n"""
nmtpytorch/utils/data.py,5,"b'# -*- coding: utf-8 -*-\nimport torch\nimport logging\nfrom torch.utils.data import DataLoader\nimport numpy as np\n\nfrom ..utils.misc import fopen, pbar\n\nlogger = logging.getLogger(\'nmtpytorch\')\n\n\ndef sort_predictions(data_loader, results):\n    """"""Recovers the dataset order when bucketing samplers are used.""""""\n    if getattr(data_loader.batch_sampler, \'store_indices\', False):\n        results = [results[i] for i, j in sorted(\n            enumerate(data_loader.batch_sampler.orig_idxs), key=lambda k: k[1])]\n    return results\n\n\ndef make_dataloader(dataset, pin_memory=False, num_workers=0):\n    if num_workers != 0:\n        logger.info(\'Forcing num_workers to 0 since it fails with torch 0.4\')\n        num_workers = 0\n\n    return DataLoader(\n        dataset, batch_sampler=dataset.sampler,\n        collate_fn=dataset.collate_fn,\n        pin_memory=pin_memory, num_workers=num_workers)\n\n\ndef sort_batch(seqbatch):\n    """"""Sorts torch tensor of integer indices by decreasing order.""""""\n    # 0 is padding_idx\n    omask = (seqbatch != 0).long()\n    olens = omask.sum(0)\n    slens, sidxs = torch.sort(olens, descending=True)\n    oidxs = torch.sort(sidxs)[1]\n    return (oidxs, sidxs, slens.data.tolist(), omask.float())\n\n\ndef pad_video_sequence(seqs):\n    """"""\n    Pads video sequences with zero vectors for minibatch processing.\n    (contributor: @elliottd)\n\n    TODO: Can we write the for loop in a more compact format?\n    """"""\n    lengths = [len(s) for s in seqs]\n    # Get the desired size of the padding vector from the input seqs data\n    feat_size = seqs[0].shape[1]\n    max_len = max(lengths)\n    tmp = []\n    for s, len_ in zip(seqs, lengths):\n        if max_len - len_ == 0:\n            tmp.append(s)\n        else:\n            inner_tmp = s\n            for i in range(max_len - len_):\n                inner_tmp = np.vstack((inner_tmp, (np.array([0.] * feat_size))))\n            tmp.append(inner_tmp)\n    padded = np.array(tmp, dtype=\'float32\')\n    return torch.FloatTensor(torch.from_numpy(padded))\n\n\ndef convert_to_onehot(idxs, n_classes):\n    """"""Returns a binary batch_size x n_classes one-hot tensor.""""""\n    out = torch.zeros(len(idxs), n_classes, device=idxs[0].device)\n    for row, indices in zip(out, idxs):\n        row.scatter_(0, indices, 1)\n    return out\n\n\ndef read_sentences(fname, vocab, bos=False, eos=True):\n    lines = []\n    lens = []\n    with fopen(fname) as f:\n        for idx, line in enumerate(pbar(f, unit=\'sents\')):\n            line = line.strip()\n\n            # Empty lines will cause a lot of headaches,\n            # get rid of them during preprocessing!\n            assert line, ""Empty line (%d) found in %s"" % (idx + 1, fname)\n\n            # Map and append\n            seq = vocab.sent_to_idxs(line, explicit_bos=bos, explicit_eos=eos)\n            lines.append(seq)\n            lens.append(len(seq))\n\n    return lines, lens\n'"
nmtpytorch/utils/device.py,2,"b'import re\nimport os\nimport shutil\nimport subprocess\n\nimport torch\n\nDEVICE = None\nDEVICE_IDS = None\n\n\nclass DeviceManager:\n    __errors = {\n        \'BadDeviceFormat\': \'Device can be cpu, gpu or [N]gpu, i.e. 2gpu\',\n        \'NoDevFiles\': \'Make sure you requested a GPU resource from your cluster.\',\n        \'NoSMI\': \'nvidia-smi is not installed. Are you on the correct node?\',\n        \'EnvVar\': \'Please set CUDA_VISIBLE_DEVICES explicitly.\',\n        \'NoMultiGPU\': \'Multi-GPU not supported for now.\',\n        \'NotEnoughGPU\': \'You requested {} GPUs while you have access to only {}.\',\n    }\n\n    def __init__(self, dev):\n        self.dev = dev.lower()\n        self.pid = os.getpid()\n        self.req_cpu = False\n        self.req_gpu = False\n        self.req_n_gpu = 0\n        self.req_multi_gpu = False\n        self.nvidia_smi = False\n        self.cuda_dev_ids = None\n\n        if not re.match(\'(cpu|[0-9]{0,1}gpu)$\', self.dev):\n            raise RuntimeError(self.__errors[\'BadDeviceFormat\'])\n\n        if self.dev == \'cpu\':\n            self.req_cpu = True\n            self.dev = torch.device(\'cpu\')\n        else:\n            self.req_gpu = True\n            if self.dev == \'gpu\':\n                self.req_n_gpu = 1\n            else:\n                self.req_n_gpu = int(self.dev[0])\n\n            self.req_multi_gpu = self.req_n_gpu > 1\n\n            # What we have\n            self.nvidia_smi = shutil.which(\'nvidia-smi\')\n            self.cuda_dev_ids = os.environ.get(\'CUDA_VISIBLE_DEVICES\', None)\n\n            if self.nvidia_smi is None:\n                raise RuntimeError(self.__errors[\'NoSMI\'])\n            if self.cuda_dev_ids == ""NoDevFiles"":\n                raise RuntimeError(self.__errors[\'NoDevFiles\'])\n            elif self.cuda_dev_ids is None:\n                raise RuntimeError(self.__errors[\'EnvVar\'])\n\n            # How many GPUs do we have access to?\n            self.cuda_dev_ids = [int(de) for de in self.cuda_dev_ids.split(\',\')]\n\n            # FIXME: Remove this once DataParallel works.\n            if self.req_n_gpu > 1 or len(self.cuda_dev_ids) > 1:\n                raise RuntimeError(self.__errors[\'NoMultiGPU\'])\n\n            if self.req_n_gpu > len(self.cuda_dev_ids):\n                raise RuntimeError(\n                    self.__errors[\'NotEnoughGPU\'].format(\n                        self.req_n_gpu, len(self.cuda_dev_ids)))\n            else:\n                self.cuda_dev_ids = self.cuda_dev_ids[:self.req_n_gpu]\n\n            # Set master device (is always cuda:0 since we force env.var\n            # restriction)\n            self.dev = torch.device(\'cuda:0\')\n\n            global DEVICE, DEVICE_IDS\n            DEVICE = self.dev\n            DEVICE_IDS = self.cuda_dev_ids\n\n    def get_cuda_mem_usage(self, name=True):\n        if self.req_cpu:\n            return None\n\n        pr = subprocess.run([\n            self.nvidia_smi,\n            ""--query-compute-apps=pid,gpu_name,used_memory"",\n            ""--format=csv,noheader""], stdout=subprocess.PIPE, universal_newlines=True)\n\n        for line in pr.stdout.strip().split(\'\\n\'):\n            pid, gpu_name, usage = line.split(\',\')\n            if int(pid) == self.pid:\n                if name:\n                    return \'{} -> {}\'.format(gpu_name.strip(), usage.strip())\n                return usage.strip()\n\n        return \'N/A\'\n\n    def __repr__(self):\n        if self.req_cpu:\n            return ""DeviceManager(dev=\'cpu\')""\n        return ""DeviceManager({}, n_gpu={})"".format(self.dev, self.req_n_gpu)\n'"
nmtpytorch/utils/filterchain.py,0,"b'# -*- coding: utf-8 -*-\nimport re\nfrom pathlib import Path\n\nfrom .misc import get_temp_file, fopen\n\n\nclass FilterChain:\n    """"""A sequential filter chain to post-process list of tokens.\n\n        Arguments:\n            filters(list): A  list of strings representing filters to apply.\n\n        Available Filters:\n            \'de-bpe\': Stitches back subword units produced with apply_bpe\n            \'de-spm\': Stitches back sentence pieces produced with spm_encode\n            \'de-segment\': Converts <tag:morpheme> to normal form\n            \'de-compond\': Stitches back German compound splittings\n            \'c2w\': Stitches back space delimited characters to words.\n                Necessary for word-level BLEU, etc. when using CharNMT.\n            \'lower\': Lowercase.\n            \'upper\': Uppercase.\n            \'de-hyphen\': De-hyphenate \'foo @-@ bar\' constructs of Moses.\n\n    """"""\n    FILTERS = {\n        \'de-bpe\': lambda s: s.replace(""@@ "", """").replace(""@@"", """"),\n        \'de-tag\': lambda s: re.sub(\'<[a-zA-Z][a-zA-Z]>\', \'\', s),\n        # Decoder for Google sentenpiece\n        # only for default params of spm_encode\n        \'de-spm\': lambda s: s.replace("" "", """").replace(""\\u2581"", "" "").strip(),\n        # Converts segmentations of <tag:morpheme> to normal form\n        \'de-segment\': lambda s: re.sub(\' *<.*?:(.*?)>\', \'\\\\1\', s),\n        # Space delim character sequence to non-tokenized normal word form\n        \'c2w\': lambda s: s.replace(\' \', \'\').replace(\'<s>\', \' \').strip(),\n        # Filters out fillers from compound splitted sentences\n        \'de-compound\': lambda s: (s.replace("" @@ "", """").replace("" @@"", """")\n                                  .replace("" @"", """").replace(""@ "", """")),\n        # de-hyphenate when -a given to Moses tokenizer\n        \'de-hyphen\': lambda s: re.sub(r\'\\s*@-@\\s*\', \'-\', s),\n        \'lower\': lambda s: s.lower(),\n        \'upper\': lambda s: s.upper(),\n    }\n\n    def __init__(self, filters):\n        assert not set(filters).difference(set(self.FILTERS.keys())), \\\n            ""Unknown evaluation filter given.""\n        self.filters = filters\n        self._funcs = [self.FILTERS[k] for k in self.filters]\n\n    def _apply(self, list_of_strs):\n        """"""Applies filters consecutively on a list of sentences.""""""\n        for func in self._funcs:\n            list_of_strs = [func(s) for s in list_of_strs]\n        return list_of_strs\n\n    def __call__(self, inp):\n        """"""Applies the filterchain on a given input.\n\n        Arguments:\n            inp(pathlib.Path or list): If a `Path` given, temporary\n                file(s) with filters applied are returned. The `Path` can\n                also be a glob expression. Otherwise, a list with filtered\n                sentences is returned.\n        """"""\n        if isinstance(inp, Path):\n            # Need to create copies of reference files with filters applied\n            # and return their paths instead\n            fnames = inp.parent.glob(inp.name)\n            new_fnames = []\n            for fname in fnames:\n                lines = []\n                f = fopen(fname)\n                for line in f:\n                    lines.append(line.strip())\n                f.close()\n                f = get_temp_file()\n                for line in self._apply(lines):\n                    f.write(line + \'\\n\')\n                f.close()\n                new_fnames.append(f.name)\n            return new_fnames\n\n        elif isinstance(inp, list):\n            return self._apply(inp)\n\n    def __repr__(self):\n        return ""FilterChain({})"".format("" -> "".join(self.filters))\n'"
nmtpytorch/utils/io.py,0,"b'# -*- coding: utf-8 -*-\nfrom collections import deque\n\n\nclass FileRotator:\n    """"""A fixed queue with Path() elements where pushing a new element pops\n    the oldest one and removes it from disk.\n\n    Arguments:\n        maxlen(int): The capacity of the queue.\n    """"""\n\n    def __init__(self, maxlen):\n        self.maxlen = maxlen\n        self.elems = deque(maxlen=self.maxlen)\n\n    def push(self, elem):\n        if len(self.elems) == self.maxlen:\n            # Remove oldest item\n            popped = self.elems.pop()\n            if popped.exists():\n                popped.unlink()\n\n        # Add new item\n        self.elems.appendleft(elem)\n\n    def __repr__(self):\n        return self.elems.__repr__()\n'"
nmtpytorch/utils/kaldi.py,0,"b'import os\nimport struct\nimport functools\n\nimport numpy\n\nERROR_BINARY = ""Binary mode header (\'\\0B\') not found when reading a matrix.""\nERROR_READ_MAT = ""Unknown matrix format \'{}\'. Supported ones: DM(float64), FM(float32).""\nERROR_WRITE_MAT = ""Unknown matrix format \'{}\'. Supported ones are float64, float32.""\n\n\ndef readString(f):\n    s = """"\n    while True:\n        c = f.read(1).decode(\'utf-8\')\n        if c == """":\n            raise ValueError(""EOF encountered while reading a string."")\n        if c == "" "":\n            return s\n        s += c\n\n\ndef readInteger(f):\n    n = ord(f.read(1))\n    a = f.read(n)[::-1]\n    try:\n        return int.from_bytes(a, byteorder=\'big\', signed=False)\n    except Exception:\n        return functools.reduce(lambda x, y: x * 256 + ord(y), a, 0)\n\n\ndef readMatrix(f):\n    header = f.read(2).decode(\'utf-8\')\n    if header != ""\\0B"":\n        raise ValueError(ERROR_BINARY)\n    mat_format = readString(f)\n    nRows = readInteger(f)\n    nCols = readInteger(f)\n    if mat_format == ""DM"":\n        data = struct.unpack(""<%dd"" % (nRows * nCols), f.read(nRows * nCols * 8))\n        data = numpy.array(data, dtype=""float64"")\n    elif mat_format == ""FM"":\n        data = struct.unpack(""<%df"" % (nRows * nCols), f.read(nRows * nCols * 4))\n        data = numpy.array(data, dtype=""float32"")\n    else:\n        raise ValueError(ERROR_READ_MAT.format(mat_format))\n    return data.reshape(nRows, nCols)\n\n\ndef readMatrixShape(f):\n    header = f.read(2).decode(\'utf-8\')\n    if header != ""\\0B"":\n        raise ValueError(\n            ""Binary mode header (\'\\0B\') not found when attempting to read a matrix."")\n    mat_format = readString(f)\n    nRows = readInteger(f)\n    nCols = readInteger(f)\n    if mat_format == ""DM"":\n        f.seek(nRows * nCols * 8, os.SEEK_CUR)\n    elif mat_format == ""FM"":\n        f.seek(nRows * nCols * 4, os.SEEK_CUR)\n    else:\n        raise ValueError(ERROR_READ_MAT.format(mat_format))\n    return nRows, nCols\n\n\ndef writeString(f, s):\n    f.write((s + "" "").encode(\'utf-8\'))\n\n\ndef writeInteger(f, a):\n    s = struct.pack(""<i"", a)\n    f.write(chr(len(s)).encode(\'utf-8\') + s)\n\n\ndef writeMatrix(f, data):\n    f.write(\'\\0B\'.encode(\'utf-8\'))      # Binary data header\n    if str(data.dtype) == ""float64"":\n        writeString(f, ""DM"")\n        writeInteger(f, data.shape[0])\n        writeInteger(f, data.shape[1])\n        f.write(struct.pack(""<%dd"" % data.size, *data.ravel()))\n    elif str(data.dtype) == ""float32"":\n        writeString(f, ""FM"")\n        writeInteger(f, data.shape[0])\n        writeInteger(f, data.shape[1])\n        f.write(struct.pack(""<%df"" % data.size, *data.ravel()))\n    else:\n        raise ValueError(ERROR_WRITE_MAT.format(str(data.dtype)))\n\n\ndef readArk(filename, limit=numpy.inf):\n    """"""\n    Reads the features in a Kaldi ark file.\n    Returns a list of feature matrices and a list of the utterance IDs.\n    """"""\n    features = []\n    uttids = []\n    with open(filename, ""rb"") as f:\n        while True:\n            try:\n                uttid = readString(f)\n            except ValueError:\n                break\n            feature = readMatrix(f)\n            features.append(feature)\n            uttids.append(uttid)\n            if len(features) == limit:\n                break\n    return features, uttids\n\n\ndef readMatrixByOffset(arkfile, offset):\n    with open(arkfile, ""rb"") as g:\n        g.seek(offset)\n        feature = readMatrix(g)\n    return feature\n\n\ndef readScp(filename, limit=numpy.inf):\n    """"""\n    Reads the features in a Kaldi script file.\n    Returns a list of feature matrices and a list of the utterance IDs.\n    """"""\n    features = []\n    uttids = []\n    with open(filename, ""r"") as f:\n        for line in f:\n            uttid, pointer = line.strip().split()\n            p = pointer.rfind("":"")\n            arkfile, offset = pointer[:p], int(pointer[p + 1:])\n            with open(arkfile, ""rb"") as g:\n                g.seek(offset)\n                feature = readMatrix(g)\n            features.append(feature)\n            uttids.append(uttid)\n            if len(features) == limit:\n                break\n    return features, uttids\n\n\ndef read_scp_info(filename, limit=numpy.inf):\n    res = []\n    with open(filename, ""r"") as f:\n        for line in f:\n            uttid, pointer = line.strip().split()\n            p = pointer.rfind("":"")\n            arkfile, offset = pointer[:p], int(pointer[p + 1:])\n            with open(arkfile, ""rb"") as g:\n                g.seek(offset)\n                feat_len, feat_dim = readMatrixShape(g)\n            res.append((uttid, arkfile, offset, feat_len, feat_dim))\n            if len(res) == limit:\n                break\n    return res\n\n\ndef read_scp_info_dic(filename, limit=numpy.inf):\n    res = {}\n    with open(filename, ""r"") as f:\n        for line in f:\n            uttid, pointer = line.strip().split()\n            p = pointer.rfind("":"")\n            arkfile, offset = pointer[:p], int(pointer[p + 1:])\n            with open(arkfile, ""rb"") as g:\n                g.seek(offset)\n                feat_len, feat_dim = readMatrixShape(g)\n            res[uttid] = ((uttid, arkfile, offset, feat_len, feat_dim))\n            if len(res) == limit:\n                break\n    return res\n\n\ndef writeArk(filename, features, uttids):\n    """"""\n    Takes a list of feature matrices and a list of utterance IDs,\n      and writes them to a Kaldi ark file.\n    Returns a list of strings in the format ""filename:offset"",\n      which can be used to write a Kaldi script file.\n    """"""\n    pointers = []\n    with open(filename, ""ab"") as f:\n        for feature, uttid in zip(features, uttids):\n            writeString(f, uttid)\n            pointers.append(""%s:%d"" % (filename, f.tell()))\n            writeMatrix(f, feature)\n    return pointers\n\n\ndef writeScp(filename, uttids, pointers):\n    """"""\n    Takes a list of utterance IDs and a list of strings in the format ""filename:offset"",\n      and writes them to a Kaldi script file.\n    """"""\n    with open(filename, ""w"") as f:\n        for uttid, pointer in zip(uttids, pointers):\n            f.write(""%s %s\\n"" % (uttid, pointer))\n'"
nmtpytorch/utils/misc.py,3,"b'# -*- coding: utf-8 -*-\nimport os\nimport bz2\nimport gzip\nimport lzma\nimport time\nimport random\nimport pathlib\nimport logging\nimport tempfile\nfrom hashlib import sha256\n\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\n\nfrom ..cleanup import cleanup\n\nlogger = logging.getLogger(\'nmtpytorch\')\n\n\nLANGUAGES = [\n    \'aa\', \'ab\', \'ae\', \'af\', \'ak\', \'am\', \'an\', \'ar\', \'as\', \'av\', \'ay\', \'az\',\n    \'ba\', \'be\', \'bg\', \'bh\', \'bi\', \'bm\', \'bn\', \'bo\', \'br\', \'bs\', \'ca\', \'ce\',\n    \'ch\', \'co\', \'cr\', \'cs\', \'cu\', \'cv\', \'cy\', \'da\', \'de\', \'dv\', \'dz\', \'ee\',\n    \'el\', \'en\', \'eo\', \'es\', \'et\', \'eu\', \'fa\', \'ff\', \'fi\', \'fj\', \'fo\', \'fr\',\n    \'fy\', \'ga\', \'gd\', \'gl\', \'gn\', \'gu\', \'gv\', \'ha\', \'he\', \'hi\', \'ho\', \'hr\',\n    \'ht\', \'hu\', \'hy\', \'hz\', \'ia\', \'id\', \'ie\', \'ig\', \'ii\', \'ik\', \'io\', \'is\',\n    \'it\', \'iu\', \'ja\', \'jv\', \'ka\', \'kg\', \'ki\', \'kj\', \'kk\', \'kl\', \'km\', \'kn\',\n    \'ko\', \'kr\', \'ks\', \'ku\', \'kv\', \'kw\', \'ky\', \'la\', \'lb\', \'lg\', \'li\', \'ln\',\n    \'lo\', \'lt\', \'lu\', \'lv\', \'mg\', \'mh\', \'mi\', \'mk\', \'ml\', \'mn\', \'mr\', \'ms\',\n    \'mt\', \'my\', \'na\', \'nb\', \'nd\', \'ne\', \'ng\', \'nl\', \'nn\', \'no\', \'nr\', \'nv\',\n    \'ny\', \'oc\', \'oj\', \'om\', \'or\', \'os\', \'pa\', \'pi\', \'pl\', \'ps\', \'pt\', \'qu\',\n    \'rm\', \'rn\', \'ro\', \'ru\', \'rw\', \'sa\', \'sc\', \'sd\', \'se\', \'sg\', \'si\', \'sk\',\n    \'sl\', \'sm\', \'sn\', \'so\', \'sq\', \'sr\', \'ss\', \'st\', \'su\', \'sv\', \'sw\', \'ta\',\n    \'te\', \'tg\', \'th\', \'ti\', \'tk\', \'tl\', \'tn\', \'to\', \'tr\', \'ts\', \'tt\', \'tw\',\n    \'ty\', \'ug\', \'uk\', \'ur\', \'uz\', \'ve\', \'vi\', \'vo\', \'wa\', \'wo\', \'xh\', \'yi\',\n    \'yo\', \'za\', \'zh\', \'zu\']\n\n\ndef fix_seed(seed=None):\n    if seed is None:\n        seed = time.time()\n\n    seed = int(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    return seed\n\n\ndef validate_or_assert(option_name, option_value, valid_options):\n    assert option_value in valid_options, \\\n        f""{option_name!r} should be one of {valid_options!r}""\n\n\ndef get_meteor_jar(ver=\'1.5\'):\n    root = pathlib.Path(os.getenv(\'HOME\')) / \'.nmtpy\' / \'meteor-data\'\n    jar = root / \'meteor-1.5.jar\'\n    assert jar.exists(), ""METEOR not installed, please run \'nmtpy-install-extra\'""\n    return jar\n\n\ndef pbar(iterator, unit=\'it\'):\n    return tqdm(iterator, unit=unit, ncols=70, smoothing=0)\n\n\ndef load_pt_file(fname, device=\'cpu\'):\n    """"""Returns saved .(ck)pt file fields.""""""\n    fname = str(pathlib.Path(fname).expanduser())\n    data = torch.load(fname, map_location=device)\n    if \'history\' not in data:\n        data[\'history\'] = {}\n    return data\n\n\ndef get_language(fname):\n    """"""Heuristic to detect the language from filename components.""""""\n    suffix = pathlib.Path(fname).suffix[1:]\n    if suffix not in LANGUAGES:\n        logger.info(f""Can not detect language from {fname}, fallback to \'en\'"")\n        suffix = \'en\'\n    return suffix\n\n\ndef listify(l):\n    """"""Encapsulate l with list[] if not.""""""\n    return [l] if not isinstance(l, list) else l\n\n\ndef flatten(l):\n    return [item for sublist in l for item in sublist]\n\n\ndef get_local_args(d):\n    return {k: v for k, v in d.items() if not k.startswith((\'__\', \'self\'))}\n\n\ndef ensure_dirs(dirs):\n    """"""Create a list of directories if not exists.""""""\n    dirs = [pathlib.Path(d) for d in listify(dirs)]\n    for d in dirs:\n        d.mkdir(parents=True, exist_ok=True)\n\n\ndef fopen(filename, key=None):\n    """"""gzip,bzip2,xz,numpy aware file opening function.""""""\n    assert \'*\' not in str(filename), ""Glob patterns not supported in fopen()""\n\n    filename = str(pathlib.Path(filename).expanduser())\n    if filename.endswith(\'.gz\'):\n        return gzip.open(filename, \'rt\')\n    elif filename.endswith(\'.bz2\'):\n        return bz2.open(filename, \'rt\')\n    elif filename.endswith((\'.xz\', \'.lzma\')):\n        return lzma.open(filename, \'rt\')\n    elif filename.endswith((\'.npy\', \'.npz\')):\n        if filename.endswith(\'.npz\'):\n            assert key is not None, ""No key= given for .npz file.""\n            return np.load(filename)[key]\n        else:\n            return np.load(filename)\n    else:\n        # Plain text\n        return open(filename, \'r\')\n\n\ndef readable_size(n):\n    """"""Return a readable size string.""""""\n    sizes = [\'K\', \'M\', \'G\']\n    fmt = \'\'\n    size = n\n    for i, s in enumerate(sizes):\n        nn = n / (1000 ** (i + 1))\n        if nn >= 1:\n            size = nn\n            fmt = sizes[i]\n        else:\n            break\n    return \'%.2f%s\' % (size, fmt)\n\n\ndef get_module_groups(layer_names):\n    groups = set()\n    for name in layer_names:\n        if \'.weight\' in name:\n            groups.add(name.split(\'.weight\')[0])\n        elif \'.bias\' in name:\n            groups.add(name.split(\'.bias\')[0])\n    return sorted(list(groups))\n\n\ndef get_n_params(module):\n    n_param_learnable = 0\n    n_param_frozen = 0\n\n    for param in module.parameters():\n        if param.requires_grad:\n            n_param_learnable += np.cumprod(param.data.size())[-1]\n        else:\n            n_param_frozen += np.cumprod(param.data.size())[-1]\n\n    n_param_all = n_param_learnable + n_param_frozen\n    return ""# parameters: {} ({} learnable)"".format(\n        readable_size(n_param_all), readable_size(n_param_learnable))\n\n\ndef get_temp_file(delete=False, close=False):\n    """"""Creates a temporary file under a folder.""""""\n    root = pathlib.Path(os.environ.get(\'NMTPY_TMP\', \'/tmp\'))\n    if not root.exists():\n        root.mkdir(parents=True, exist_ok=True)\n\n    prefix = str(root / ""nmtpytorch_{}"".format(os.getpid()))\n    t = tempfile.NamedTemporaryFile(\n        mode=\'w\', prefix=prefix, delete=delete)\n    cleanup.register_tmp_file(t.name)\n    if close:\n        t.close()\n    return t\n\n\ndef setup_experiment(opts, suffix=None, short=False):\n    """"""Return a representative string for the experiment.""""""\n\n    # subfolder is conf filename without .conf suffix\n    opts.train[\'subfolder\'] = pathlib.Path(opts.filename).stem\n\n    # add suffix to subfolder name to keep experiment names shorter\n    if suffix:\n        opts.train[\'subfolder\'] += ""-{}"".format(suffix)\n\n    # Create folders\n    folder = pathlib.Path(opts.train[\'save_path\']) / opts.train[\'subfolder\']\n    folder.mkdir(parents=True, exist_ok=True)\n\n    # Set random experiment ID\n    run_id = time.strftime(\'%Y%m%d%H%m%S\') + str(random.random())\n    run_id = sha256(run_id.encode(\'ascii\')).hexdigest()[:5]\n\n    # Finalize\n    model_type = opts.train[\'model_type\'].lower()\n    opts.train[\'exp_id\'] = f\'{model_type}-r{run_id}\'\n'"
nmtpytorch/utils/ml_metrics.py,2,"b'from collections import defaultdict\n\nimport numpy as np\nimport torch\n\nfrom sklearn.metrics import coverage_error\nfrom sklearn.metrics import label_ranking_average_precision_score as lrap\n\nfrom ignite import metrics as ig_metrics\n\nfrom .device import DEVICE\n\nfrom ..metrics import Metric\n\n\nclass Loss:\n    """"""Accumulates and computes correctly training and validation losses.""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self._loss = 0\n        self._denom = 0\n        self.batch_loss = 0\n\n    def update(self, loss, n_items):\n        # Store last batch loss\n        self.batch_loss = loss.item()\n        # Add it to cumulative loss\n        self._loss += self.batch_loss\n        # Normalize batch loss w.r.t n_items\n        self.batch_loss /= n_items\n        # Accumulate n_items inside the denominator\n        self._denom += n_items\n\n    def get(self):\n        if self._denom == 0:\n            return 0\n        return self._loss / self._denom\n\n    @property\n    def denom(self):\n        return self._denom\n\n\nclass Precision:\n    """"""Wrapper metric around `pytorch-ignite`.""""""\n    def __init__(self, is_multilabel=True):\n        self.is_multilabel = is_multilabel\n        self.__metric = ig_metrics.Precision(\n            average=True, is_multilabel=self.is_multilabel)\n\n    def update(self, y_pred, y):\n        """"""Tensors should have N x n_labels dimensions with N = batch_size.""""""\n        self.__metric.update((y_pred, y))\n\n    def compute(self):\n        """"""Once the updates are over, this returns the actual metric.""""""\n        val = 100 * self.__metric.compute()\n        return Metric(\'PRECISION\', val, higher_better=True)\n\n\nclass Recall:\n    """"""Wrapper metric around `pytorch-ignite`.""""""\n    def __init__(self, is_multilabel=True):\n        self.is_multilabel = is_multilabel\n        self.__metric = ig_metrics.Recall(\n            average=True, is_multilabel=self.is_multilabel)\n\n    def update(self, y_pred, y):\n        """"""Tensors should have N x n_labels dimensions with N = batch_size.""""""\n        self.__metric.update((y_pred, y))\n\n    def compute(self):\n        """"""Once the updates are over, this returns the actual metric.""""""\n        val = 100 * self.__metric.compute()\n        return Metric(\'RECALL\', val, higher_better=True)\n\n\nclass F1:\n    """"""Wrapper metric around `pytorch-ignite`.""""""\n    def __init__(self, is_multilabel=True):\n        self.is_multilabel = is_multilabel\n        # Create underlying metrics\n        self.__precision = ig_metrics.Precision(\n            average=False, is_multilabel=self.is_multilabel)\n\n        self.__recall = ig_metrics.Recall(\n            average=False, is_multilabel=self.is_multilabel)\n\n        num = self.__precision * self.__recall * 2\n        denom = self.__precision + self.__recall + 1e-20\n        f1 = num / denom\n        self.__metric = ig_metrics.MetricsLambda(\n            lambda t: t.mean().item(), f1)\n\n    def update(self, y_pred, y):\n        """"""Tensors should have N x n_labels dimensions with N = batch_size.""""""\n        self.__precision.update((y_pred, y))\n        self.__recall.update((y_pred, y))\n\n    def compute(self):\n        val = 100 * self.__metric.compute()\n        return Metric(\'F1\', val, higher_better=True)\n\n\nclass CoverageError:\n    def __init__(self):\n        self._cov = 0\n        self._n_items = 0\n\n    def update(self, y_true, y_pred):\n        self._cov += coverage_error(y_true, y_pred) * y_pred.shape[0]\n        self._n_items += y_pred.shape[0]\n\n    def get(self):\n        return self._cov / self._n_items\n\n\nclass LRAPScore:\n    def __init__(self):\n        self._lrap = 0\n        self._n_items = 0\n\n    def update(self, y_true, y_pred):\n        self._lrap += lrap(y_true, y_pred) * y_pred.shape[0]\n        self._n_items += y_pred.shape[0]\n\n    def get(self):\n        return self._lrap / self._n_items\n\n\nclass MeanReciprocalRank:\n    """"""Computes the mean reciprocal rank (MRR) metric for a batch along with\n    per time-step MRR statistics that accumulate.""""""\n    def __init__(self, n_classes):\n        self.denom = torch.arange(1, 1 + n_classes, device=DEVICE, dtype=torch.float)\n        self._mrr_per_timestep = defaultdict(float)\n        self._per_timestep_counts = defaultdict(int)\n\n    def update(self, y_true, y_pred):\n        # y_pred: tstep x bsize x n_classes\n        # y_true: tstep x bsize\n\n        # Get a clone to mask out zero padded elements\n        y_true_nz = y_true.clone()\n        y_true_nz[y_true_nz.eq(0)] = -1\n        y_true_nz.unsqueeze_(-1)\n\n        # Sort negative log-probabilities from most-likely to less-likely\n        sorted_logp, sorted_idxs = torch.sort(y_pred, dim=-1, descending=True)\n\n        # matches: tstep x bsize x vocab of binary indicators to mark matches\n        matches = (sorted_idxs == y_true_nz).float()\n\n        samples_per_timestep = (y_true > 1).sum(1).tolist()\n\n        # Compute MRR per timestep\n        for tstep, n_samples in enumerate(samples_per_timestep):\n            self._mrr_per_timestep[tstep + 1] += (\n                matches[tstep].sum(0) / self.denom).sum()\n            self._per_timestep_counts[tstep + 1] += n_samples\n\n    def normalized_mrr(self):\n        x, y = self.per_timestep_mrr()\n        return 100. * (x.sum() / y.sum())\n\n    def per_timestep_mrr(self):\n        timesteps = list(range(1, 1 + len(self._per_timestep_counts)))\n        counts = np.array([self._per_timestep_counts[t] for t in timesteps])\n        scores = np.array([self._mrr_per_timestep[t] for t in timesteps])\n        return scores, counts\n'"
nmtpytorch/utils/nn.py,5,"b'# -*- coding: utf-8 -*-\nimport pickle as pkl\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\n\ndef get_rnn_hidden_state(h):\n    """"""Returns h_t transparently regardless of RNN type.""""""\n    return h if not isinstance(h, tuple) else h[0]\n\n\ndef get_activation_fn(name):\n    """"""Returns a callable activation function from torch.""""""\n    if name in (None, \'linear\'):\n        return lambda x: x\n    elif name in (\'sigmoid\', \'tanh\'):\n        return getattr(torch, name)\n    else:\n        return getattr(F, name)\n\n\ndef mean_pool(data):\n    """"""Simple mean pool function for transforming 3D features of shape\n    [T]imesteps x [B]atch_size x [F]eature_size into 2D BxF features.\n    (author: @klmulligan)\n\n        Arguments:\n            data (tuple): Encoder result of form (data: Tensor(TxBxF), mask: Tensor(TxB))\n        Returns:\n            pooled_data (Tensor): Mean pooled data of shape BxF.\n    """"""\n    # Unpack\n    x, mask = data\n\n    if mask is not None:\n        return x.sum(0) / mask.sum(0).unsqueeze(1)\n    else:\n        return x.mean(0)\n\n\ndef get_partial_embedding_layer(vocab, embedding_dim, pretrained_file,\n                                freeze=\'none\', oov_zero=True):\n    """"""A partially updateable embedding layer with pretrained embeddings.\n    This is experimental and not quite tested.""""""\n    avail_idxs, miss_idxs = [], []\n    avail_embs = []\n\n    # Load the pickled dictionary\n    with open(pretrained_file, \'rb\') as f:\n        pret_dict = pkl.load(f)\n\n    for idx, word in vocab._imap.items():\n        if word in pret_dict:\n            avail_embs.append(pret_dict[word])\n            avail_idxs.append(idx)\n        else:\n            miss_idxs.append(idx)\n\n    # This matrix contains the pretrained embeddings\n    avail_embs = torch.Tensor(avail_embs)\n\n    # We don\'t need the whole dictionary anymore\n    del pret_dict\n\n    n_pretrained = len(avail_idxs)\n    n_learned = vocab.n_tokens - n_pretrained\n\n    # Sanity checks\n    assert len(avail_idxs) + len(miss_idxs) == vocab.n_tokens\n\n    # Create the layer\n    emb = nn.Embedding(vocab.n_tokens, embedding_dim, padding_idx=0)\n    if oov_zero:\n        emb.weight.data.fill_(0)\n\n    # Copy in the pretrained embeddings\n    emb.weight.data[n_learned:] = avail_embs\n    # Sanity check\n    assert torch.equal(emb.weight.data[-1], avail_embs[-1])\n\n    grad_mask = None\n    if freeze == \'all\':\n        emb.weight.requires_grad = False\n    elif freeze == \'partial\':\n        # Create bitmap gradient mask\n        grad_mask = torch.ones(vocab.n_tokens)\n        grad_mask[n_learned:].fill_(0)\n        grad_mask[0].fill_(0)\n        grad_mask.unsqueeze_(1)\n\n        def grad_mask_hook(grad):\n            return grad_mask.to(grad.device) * grad\n\n        emb.weight.register_hook(grad_mask_hook)\n\n    # Return the layer\n    return emb\n'"
nmtpytorch/utils/tensorboard.py,1,"b'# -*- coding: utf-8 -*-\nimport pathlib\n\nfrom torch.utils.tensorboard import SummaryWriter\n\n\nclass TensorBoard:\n    def __init__(self, model, log_dir, exp_id, subfolder):\n        self.model = model\n        self.log_dir = log_dir\n        self.exp_id = exp_id\n        self.subfolder = subfolder\n        self.writer = None\n        self.available = bool(self.log_dir)\n\n        # Call setup\n        self.setup()\n\n    def _nop(self, *args, **kwargs):\n        return\n\n    def setup(self):\n        """"""Setups TensorBoard logger.""""""\n        if not self.available:\n            self.replace_loggers()\n            return\n\n        # Construct full folder path\n        self.log_dir = pathlib.Path(self.log_dir).expanduser()\n        self.log_dir = self.log_dir / self.subfolder / self.exp_id\n        self.log_dir.mkdir(parents=True, exist_ok=True)\n\n        # Set up summary writer\n        self.writer = SummaryWriter(self.log_dir)\n\n    def replace_loggers(self):\n        """"""Replace all log_* methods with dummy _nop.""""""\n        self.log_metrics = self._nop\n        self.log_scalar = self._nop\n        self.log_activations = self._nop\n        self.log_gradients = self._nop\n\n    def log_metrics(self, metrics, step, suffix=\'\'):\n        """"""Logs evaluation metrics as scalars.""""""\n        for metric in metrics:\n            self.writer.add_scalar(suffix + metric.name, metric.score,\n                                   global_step=step)\n\n    def log_scalar(self, name, value, step):\n        """"""Logs single scalar value.""""""\n        self.writer.add_scalar(name, value, global_step=step)\n\n    def log_activations(self, step):\n        """"""Logs activations by layer.""""""\n        pass\n\n    def log_gradients(self, step):\n        """"""Logs gradients by layer.""""""\n        pass\n\n    def close(self):\n        """"""Closes TensorBoard handle.""""""\n        if self.available:\n            self.writer.close()\n\n    def __repr__(self):\n        if not self.log_dir:\n            return ""No \'tensorboard_dir\' given in config""\n        return ""TensorBoard is active""\n'"
nmtpytorch/utils/topology.py,0,"b'# -*- coding: utf-8 -*-\nfrom collections import UserString, OrderedDict\n\nfrom .. import datasets\n\n\nclass DataSource(UserString):\n    def __init__(self, name, _type, src=False, trg=False):\n        super().__init__(name)\n        self._type = _type\n        self.src = src\n        self.trg = trg\n        self.side = \'src\' if self.src else \'trg\'\n\n        # Assign the method that knows how to create a tensor for a batch\n        # of this type\n        klass = getattr(datasets, \'{}Dataset\'.format(_type))\n        self.kwargs = {}\n        self.torchify = lambda batch: klass.to_torch(batch, **self.kwargs)\n    def __repr__(self):\n        return ""DataSource(\'{}\', kwargs:{})"".format(self.data, self.kwargs)\n\n\nclass Topology:\n    """"""A simple object that parses the direction string provided through the\n        experiment configuration file.\n\n        A direction is a string with the following syntax:\n            feat:<type>, feat:<type>, ... -> feat:<type>, feat:<type>, ...\n\n        where\n            feat determines the name of the modality, i.e. \'en\', \'image\', etc.\n            type is the prefix of the actual ``Dataset`` class to be used\n                with this modality, i.e. Text, ImageFolder, OneHot, etc.\n            if type is omitted, the default is Text.\n\n        Example:\n            de:Text (no target side)\n            de:Text -> en:Text\n            de:Text -> en:Text, en_pos:OneHot\n            de:Text, image:ImageFolder -> en:Text\n    """"""\n    def __init__(self, direction):\n        self.direction = direction\n        self.srcs = OrderedDict()\n        self.trgs = OrderedDict()\n        self.all = OrderedDict()\n\n        parts = direction.strip().split(\'->\')\n        if len(parts) == 1:\n            srcs, trgs = parts[0].strip().split(\',\'), []\n        else:\n            srcs = parts[0].strip().split(\',\') if parts[0].strip() else []\n            trgs = parts[1].strip().split(\',\') if parts[1].strip() else []\n\n        # Temporary dict to parse sources and targets in a single loop\n        tmp = {\'srcs\': srcs, \'trgs\': trgs}\n\n        for key, values in tmp.items():\n            _dict = getattr(self, key)\n            for val in values:\n                name, *ftype = val.strip().split(\':\')\n                ftype = ftype[0] if len(ftype) > 0 else ""Text""\n                ds = DataSource(name, ftype,\n                                src=(key == \'srcs\'), trg=(key == \'trgs\'))\n                if name in self.all:\n                    raise RuntimeError(\n                        \'""{}"" already given as a data source.\'.format(name))\n                _dict[name] = ds\n                self.all[name] = ds\n\n        # Assign shortcuts\n        self.first_src = list(self.srcs.keys())[0]\n        self.first_trg = list(self.trgs.keys())[0]\n\n    def is_included_in(self, t):\n        """"""Return True if this topology is included in t, otherwise False.""""""\n        if t is None:\n            return False\n        return (self.srcs.keys() <= t.srcs.keys()) and (self.trgs.keys() <= t.trgs.keys())\n\n    def get_srcs(self, _type):\n        return [v for v in self.srcs.values() if v._type == _type]\n\n    def get_trgs(self, _type):\n        return [v for v in self.trgs.values() if v._type == _type]\n\n    def get_src_langs(self):\n        return self.get_srcs(\'Text\')\n\n    def get_trg_langs(self):\n        return self.get_trgs(\'Text\')\n\n    def __getitem__(self, key):\n        return self.all[key]\n\n    def __repr__(self):\n        s = ""Sources:\\n""\n        for x in self.srcs.values():\n            s += "" {}\\n"".format(x.__repr__())\n        s += ""Targets:\\n""\n        for x in self.trgs.values():\n            s += "" {}\\n"".format(x.__repr__())\n        return s\n'"
nmtpytorch/cocoeval/bleu/__init__.py,0,"b""__author__ = 'tylin'\n"""
nmtpytorch/cocoeval/bleu/bleu.py,0,"b'# -*- coding: utf-8 -*-\n# File Name : bleu.py\n#\n# Description : Wrapper for BLEU scorer.\n#\n# Creation Date : 06-01-2015\n# Last Modified : Thu 19 Mar 2015 09:13:28 PM PDT\n# Authors : Hao Fang <hfang@uw.edu> and Tsung-Yi Lin <tl483@cornell.edu>\n\nfrom .bleu_scorer import BleuScorer\n\n\nclass Bleu:\n    def __init__(self, n=4):\n        # default compute Blue score up to 4\n        self._n = n\n        self._hypo_for_image = {}\n        self.ref_for_image = {}\n\n    def compute_score(self, gts, res):\n\n        bleu_scorer = BleuScorer(n=self._n)\n        for id in sorted(gts.keys()):\n            hypo = res[id]\n            ref = gts[id]\n\n            # Sanity check.\n            assert isinstance(hypo, list)\n            assert isinstance(ref, list)\n            assert len(hypo) == 1\n            assert len(ref) >= 1\n\n            bleu_scorer += (hypo[0], ref)\n\n        # score, scores = bleu_scorer.compute_score(option=\'shortest\')\n        # score, scores = bleu_scorer.compute_score(option=\'average\',verbose=1)\n        score, scores = bleu_scorer.compute_score(option=\'closest\', verbose=0)\n\n        # return (bleu, bleu_info)\n        return score, scores\n\n    def method(self):\n        return ""Bleu""\n'"
nmtpytorch/cocoeval/bleu/bleu_scorer.py,0,"b'# -*- coding: utf-8 -*-\n# bleu_scorer.py\n# David Chiang <chiang@isi.edu>\n\n# Copyright (c) 2004-2006 University of Maryland. All rights\n# reserved. Do not redistribute without permission from the\n# author. Not for commercial use.\n\n# Modified by:\n# Hao Fang <hfang@uw.edu>\n# Tsung-Yi Lin <tl483@cornell.edu>\n\n\'\'\'Provides:\ncook_refs(refs, n=4): Transform a list of reference sentences as\n    strings into a form usable by cook_test().\ncook_test(test, refs, n=4): Transform a test sentence as a string\n    (together with the cooked reference sentences) into a\n    form usable by score_cooked().\n\'\'\'\n\nimport copy\nimport math\nfrom collections import defaultdict\n\n\ndef precook(s, n=4, out=False):\n    """"""Takes a string as input and returns an object that can be given to\n    either cook_refs or cook_test. This is optional: cook_refs and cook_test\n    can take string arguments as well.""""""\n    words = s.split()\n    counts = defaultdict(int)\n    for k in range(1, n + 1):\n        for i in range(len(words) - k + 1):\n            ngram = tuple(words[i: i + k])\n            counts[ngram] += 1\n    return (len(words), counts)\n\n\ndef cook_refs(refs, eff=None, n=4):  # lhuang: oracle will call with ""average""\n    \'\'\'Takes a list of reference sentences for a single segment\n    and returns an object that encapsulates everything that BLEU\n    needs to know about them.\'\'\'\n\n    reflen = []\n    maxcounts = {}\n    for ref in refs:\n        rl, counts = precook(ref, n)\n        reflen.append(rl)\n        for (ngram, count) in counts.items():\n            maxcounts[ngram] = max(maxcounts.get(ngram, 0), count)\n\n    # Calculate effective reference sentence length.\n    if eff == ""shortest"":\n        reflen = min(reflen)\n    elif eff == ""average"":\n        reflen = float(sum(reflen)) / len(reflen)\n\n    # lhuang: N.B.: leave reflen computaiton to the very end!!\n    # lhuang: N.B.: in case of ""closest"", keep a list of reflens!! (bad design)\n\n    return (reflen, maxcounts)\n\n\ndef cook_test(test, ref_len_counts, eff=None, n=4):\n    \'\'\'Takes a test sentence and returns an object that\n    encapsulates everything that BLEU needs to know about it.\'\'\'\n    (reflen, refmaxcounts) = ref_len_counts\n    testlen, counts = precook(test, n, True)\n\n    result = {}\n\n    # Calculate effective reference sentence length.\n\n    if eff == ""closest"":\n        result[""reflen""] = min((abs(l - testlen), l) for l in reflen)[1]\n    else:  # i.e., ""average"" or ""shortest"" or None\n        result[""reflen""] = reflen\n\n    result[""testlen""] = testlen\n\n    result[""guess""] = [max(0, testlen - k + 1) for k in range(1, n + 1)]\n\n    result[\'correct\'] = [0] * n\n    for (ngram, count) in counts.items():\n        result[""correct""][len(ngram) - 1] += min(refmaxcounts.get(ngram, 0),\n                                                 count)\n    return result\n\n\nclass BleuScorer:\n    """"""Bleu scorer.\n    """"""\n\n    __slots__ = ""n"", ""crefs"", ""ctest"", ""_score"", ""_ratio"", \\\n                ""_testlen"", ""_reflen"", ""special_reflen""\n    # special_reflen is used in oracle\n    # (proportional effective ref len for a node).\n\n    def copy(self):\n        \'\'\' copy the refs.\'\'\'\n        new = BleuScorer(n=self.n)\n        new.ctest = copy.copy(self.ctest)\n        new.crefs = copy.copy(self.crefs)\n        new._score = None\n        return new\n\n    def __init__(self, test=None, refs=None, n=4, special_reflen=None):\n        \'\'\' singular instance \'\'\'\n\n        self.n = n\n        self.crefs = []\n        self.ctest = []\n        self.cook_append(test, refs)\n        self.special_reflen = special_reflen\n\n    def cook_append(self, test, refs):\n        \'\'\'called by constructor and __iadd__ to\n        avoid creating new instances.\'\'\'\n\n        if refs is not None:\n            self.crefs.append(cook_refs(refs))\n            if test is not None:\n                cooked_test = cook_test(test, self.crefs[-1])\n                self.ctest.append(cooked_test)  # N.B.: -1\n            else:\n                # lens of crefs and ctest have to match\n                self.ctest.append(None)\n\n        self._score = None  # need to recompute\n\n    def ratio(self, option=None):\n        self.compute_score(option=option)\n        return self._ratio\n\n    def score_ratio(self, option=None):\n        \'\'\'return (bleu, len_ratio) pair\'\'\'\n        return (self.fscore(option=option), self.ratio(option=option))\n\n    def score_ratio_str(self, option=None):\n        return ""%.4f (%.2f)"" % self.score_ratio(option)\n\n    def reflen(self, option=None):\n        self.compute_score(option=option)\n        return self._reflen\n\n    def testlen(self, option=None):\n        self.compute_score(option=option)\n        return self._testlen\n\n    def retest(self, new_test):\n        if isinstance(new_test, str):\n            new_test = [new_test]\n        assert len(new_test) == len(self.crefs), new_test\n        self.ctest = []\n        for t, rs in zip(new_test, self.crefs):\n            self.ctest.append(cook_test(t, rs))\n        self._score = None\n\n        return self\n\n    def rescore(self, new_test):\n        \'\'\' replace test(s) with new test(s), and returns the new score.\'\'\'\n\n        return self.retest(new_test).compute_score()\n\n    def size(self):\n        assert len(self.crefs) == len(self.ctest), \\\n            ""refs/test mismatch! %d<>%d"" % (len(self.crefs), len(self.ctest))\n        return len(self.crefs)\n\n    def __iadd__(self, other):\n        \'\'\'add an instance (e.g., from another sentence).\'\'\'\n\n        if isinstance(other, tuple):\n            # avoid creating new BleuScorer instances\n            self.cook_append(other[0], other[1])\n        else:\n            assert self.compatible(other), ""incompatible BLEUs.""\n            self.ctest.extend(other.ctest)\n            self.crefs.extend(other.crefs)\n            self._score = None  # need to recompute\n\n        return self\n\n    def compatible(self, other):\n        return isinstance(other, BleuScorer) and self.n == other.n\n\n    def single_reflen(self, option=""average""):\n        return self._single_reflen(self.crefs[0][0], option)\n\n    def _single_reflen(self, reflens, option=None, testlen=None):\n\n        if option == ""shortest"":\n            reflen = min(reflens)\n        elif option == ""average"":\n            reflen = float(sum(reflens)) / len(reflens)\n        elif option == ""closest"":\n            reflen = min((abs(l - testlen), l) for l in reflens)[1]\n        else:\n            assert False, ""unsupported reflen option %s"" % option\n\n        return reflen\n\n    def recompute_score(self, option=None, verbose=0):\n        self._score = None\n        return self.compute_score(option, verbose)\n\n    def compute_score(self, option=None, verbose=0):\n        n = self.n\n        eps = 1e-9\n        tiny = 1e-15  # so that if guess is 0 still return 0\n        bleu_list = [[] for _ in range(n)]\n\n        if self._score is not None:\n            return self._score\n\n        if option is None:\n            option = ""average"" if len(self.crefs) == 1 else ""closest""\n\n        self._testlen = 0\n        self._reflen = 0\n        totalcomps = {\'testlen\': 0, \'reflen\': 0, \'guess\': [0] * n,\n                      \'correct\': [0] * n}\n\n        # for each sentence\n        for comps in self.ctest:\n            testlen = comps[\'testlen\']\n            self._testlen += testlen\n\n            if self.special_reflen is None:  # need computation\n                reflen = self._single_reflen(comps[\'reflen\'], option, testlen)\n            else:\n                reflen = self.special_reflen\n\n            self._reflen += reflen\n\n            for key in [\'guess\', \'correct\']:\n                for k in range(n):\n                    totalcomps[key][k] += comps[key][k]\n\n            # append per image bleu score\n            bleu = 1.\n            for k in range(n):\n                bleu *= (float(comps[\'correct\'][k]) + tiny) \\\n                    / (float(comps[\'guess\'][k]) + eps)\n                bleu_list[k].append(bleu ** (1. / (k + 1)))\n            ratio = (testlen + tiny) / (reflen + eps)\n            if ratio < 1:\n                for k in range(n):\n                    bleu_list[k][-1] *= math.exp(1 - 1 / ratio)\n\n            if verbose > 1:\n                print(comps, reflen)\n\n        totalcomps[\'reflen\'] = self._reflen\n        totalcomps[\'testlen\'] = self._testlen\n\n        bleus = []\n        bleu = 1.\n        for k in range(n):\n            bleu *= float(totalcomps[\'correct\'][k] + tiny) \\\n                / (totalcomps[\'guess\'][k] + eps)\n            bleus.append(bleu ** (1. / (k + 1)))\n        ratio = (self._testlen + tiny) / (self._reflen + eps)\n        if ratio < 1:\n            for k in range(n):\n                bleus[k] *= math.exp(1 - 1 / ratio)\n\n        if verbose > 0:\n            print(totalcomps)\n            print(""ratio:"", ratio)\n\n        # Normalize to percentage\n        bleus = [100 * b for b in bleus]\n        self._score = bleus\n        return self._score, bleu_list\n'"
nmtpytorch/cocoeval/cider/__init__.py,0,"b""__author__ = 'tylin'\n"""
nmtpytorch/cocoeval/cider/cider.py,0,"b'# -*- coding: utf-8 -*-\n# Filename: cider.py\n#\n# Description: Describes the class to compute the CIDEr\n#              (Consensus-Based Image Description Evaluation) Metric\n# by Vedantam, Zitnick, and Parikh (http://arxiv.org/abs/1411.5726)\n#\n# Creation Date: Sun Feb  8 14:16:54 2015\n#\n# Authors: Ramakrishna Vedantam <vrama91@vt.edu> and\n# Tsung-Yi Lin <tl483@cornell.edu>\n\nfrom .cider_scorer import CiderScorer\n\n\nclass Cider:\n    """"""Main Class to compute the CIDEr metric.""""""\n\n    def __init__(self, test=None, refs=None, n=4, sigma=6.0):\n        # set cider to sum over 1 to 4-grams\n        self._n = n\n        # set the standard deviation parameter for gaussian penalty\n        self._sigma = sigma\n\n    def compute_score(self, gts, res):\n        """"""Main function to compute CIDEr score\n\n        Arguments:\n            hypo_for_image (dict): dictionary with key <image> and\n                value <tokenized hypothesis / candidate sentence>\n            ref_for_image (dict): dictionary with key <image> and value\n                <tokenized reference sentence>\n\n        Returns:\n            cider (float): computed CIDEr score for the corpus\n        """"""\n\n        cider_scorer = CiderScorer(n=self._n, sigma=self._sigma)\n\n        for id in sorted(gts.keys()):\n            hypo = res[id]\n            ref = gts[id]\n\n            # Sanity check.\n            assert isinstance(hypo, list)\n            assert isinstance(ref, list)\n            assert len(hypo) == 1\n            assert len(ref) > 0\n\n            cider_scorer += (hypo[0], ref)\n\n        (score, scores) = cider_scorer.compute_score()\n\n        return score, scores\n\n    def method(self):\n        return ""CIDEr""\n'"
nmtpytorch/cocoeval/cider/cider_scorer.py,0,"b'# -*- coding: utf-8 -*-\n# Tsung-Yi Lin <tl483@cornell.edu>\n# Ramakrishna Vedantam <vrama91@vt.edu>\n\nimport math\nimport copy\nfrom collections import defaultdict\n\nimport numpy as np\n\n\ndef precook(s, n=4, out=False):\n    """"""\n    Takes a string as input and returns an object that can be given to\n    either cook_refs or cook_test. This is optional: cook_refs and cook_test\n    can take string arguments as well.\n    :param s: string : sentence to be converted into ngrams\n    :param n: int    : number of ngrams for which representation is calculated\n    :return: term frequency vector for occuring ngrams\n    """"""\n    words = s.split()\n    counts = defaultdict(int)\n    for k in range(1, n + 1):\n        for i in range(len(words) - k + 1):\n            ngram = tuple(words[i: i + k])\n            counts[ngram] += 1\n    return counts\n\n\ndef cook_refs(refs, n=4):\n    # lhuang: oracle will call with ""average""\n    \'\'\'Takes a list of reference sentences for a single segment\n    and returns an object that encapsulates everything that BLEU\n    needs to know about them.\n    :param refs: list of string : reference sentences for some image\n    :param n: int : number of ngrams for which (ngram) repr. is calculated\n    :return: result (list of dict)\n    \'\'\'\n    return [precook(ref, n) for ref in refs]\n\n\ndef cook_test(test, n=4):\n    \'\'\'Takes a test sentence and returns an object that\n    encapsulates everything that BLEU needs to know about it.\n    :param test: list of string : hypothesis sentence for some image\n    :param n: int : number of ngrams for which (ngram) repr. is calculated\n    :return: result (dict)\n    \'\'\'\n    return precook(test, n, True)\n\n\nclass CiderScorer:\n    """"""CIDEr scorer.""""""\n\n    def copy(self):\n        \'\'\' copy the refs.\'\'\'\n        new = CiderScorer(n=self.n)\n        new.ctest = copy.copy(self.ctest)\n        new.crefs = copy.copy(self.crefs)\n        return new\n\n    def __init__(self, test=None, refs=None, n=4, sigma=6.0):\n        """"""singular instance""""""\n        self.n = n\n        self.sigma = sigma\n        self.crefs = []\n        self.ctest = []\n        self.document_frequency = defaultdict(float)\n        self.cook_append(test, refs)\n        self.ref_len = None\n\n    def cook_append(self, test, refs):\n        \'\'\'called by constructor and __iadd__ to avoid\n        creating new instances.\'\'\'\n\n        if refs is not None:\n            self.crefs.append(cook_refs(refs))\n            if test is not None:\n                self.ctest.append(cook_test(test))  # N.B.: -1\n            else:\n                # lens of crefs and ctest have to match\n                self.ctest.append(None)\n\n    def size(self):\n        assert len(self.crefs) == len(self.ctest), \\\n            ""refs/test mismatch! %d<>%d"" % (len(self.crefs), len(self.ctest))\n        return len(self.crefs)\n\n    def __iadd__(self, other):\n        \'\'\'add an instance (e.g., from another sentence).\'\'\'\n\n        if isinstance(other, tuple):\n            # avoid creating new CiderScorer instances\n            self.cook_append(other[0], other[1])\n        else:\n            self.ctest.extend(other.ctest)\n            self.crefs.extend(other.crefs)\n        return self\n\n    def compute_doc_freq(self):\n        \'\'\'\n        Compute term frequency for reference data.\n        This will be used to compute idf (inverse document frequency later)\n        The term frequency is stored in the object\n        :return: None\n        \'\'\'\n        for refs in self.crefs:\n            # refs, k ref captions of one image\n            for ngram in set([ngram for ref in refs for (ngram, count) in ref.items()]):\n                self.document_frequency[ngram] += 1\n            # maxcounts[ngram] = max(maxcounts.get(ngram,0), count)\n\n    def compute_cider(self):\n        def counts2vec(cnts):\n            """"""\n            Function maps counts of ngram to vector of tfidf weights.\n            The function returns vec, an array of dictionary that store\n            mapping of n-gram and tf-idf weights.\n            The n-th entry of array denotes length of n-grams.\n            :param cnts:\n            :return: vec (array of dict), norm (array of float), length (int)\n            """"""\n            vec = [defaultdict(float) for _ in range(self.n)]\n            length = 0\n            norm = [0.0 for _ in range(self.n)]\n            for (ngram, term_freq) in cnts.items():\n                # give word count 1 if it doesn\'t appear in reference corpus\n                df = np.log(max(1.0, self.document_frequency[ngram]))\n                # ngram index\n                n = len(ngram) - 1\n                # tf (term_freq) * idf (precomputed idf) for n-grams\n                vec[n][ngram] = float(term_freq) * (self.ref_len - df)\n                # compute norm for the vector\n                # the norm will be used for computing similarity\n                norm[n] += pow(vec[n][ngram], 2)\n\n                if n == 1:\n                    length += term_freq\n            norm = [np.sqrt(n) for n in norm]\n            return vec, norm, length\n\n        def sim(vec_hyp, vec_ref, norm_hyp, norm_ref, length_hyp, length_ref):\n            \'\'\'\n            Compute the cosine similarity of two vectors.\n            :param vec_hyp: array of dictionary for hypothesis vector\n            :param vec_ref: array of dictionary for reference vector\n            :param norm_hyp: array of float for hypothesis vector\n            :param norm_ref: array of float for reference vector\n            :param length_hyp: int containing length of hypothesis\n            :param length_ref: int containing length of reference\n            :return: array of score for each n-grams cosine similarity\n            \'\'\'\n            delta = float(length_hyp - length_ref)\n            # measure consine similarity\n            val = np.array([0.0 for _ in range(self.n)])\n            for n in range(self.n):\n                # ngram\n                for (ngram, count) in vec_hyp[n].items():\n                    # vrama91 : added clipping\n                    val[n] += min(vec_hyp[n][ngram],\n                                  vec_ref[n][ngram]) * vec_ref[n][ngram]\n\n                if (norm_hyp[n] != 0) and (norm_ref[n] != 0):\n                    val[n] /= (norm_hyp[n] * norm_ref[n])\n\n                assert not math.isnan(val[n])\n                # vrama91: added a length based gaussian penalty\n                val[n] *= np.e**(-(delta**2) / (2 * self.sigma**2))\n            return val\n\n        # compute log reference length\n        self.ref_len = np.log(float(len(self.crefs)))\n\n        scores = []\n        for test, refs in zip(self.ctest, self.crefs):\n            # compute vector for test captions\n            vec, norm, length = counts2vec(test)\n            # compute vector for ref captions\n            score = np.array([0.0 for _ in range(self.n)])\n            for ref in refs:\n                vec_ref, norm_ref, length_ref = counts2vec(ref)\n                score += sim(vec, vec_ref, norm, norm_ref, length, length_ref)\n            # change by vrama91 - mean of ngram scores, instead of sum\n            score_avg = np.mean(score)\n            # divide by number of references\n            score_avg /= len(refs)\n            # multiply score by 10\n            score_avg *= 10.0\n            # append score of an image to the score list\n            scores.append(score_avg)\n        return scores\n\n    def compute_score(self, option=None, verbose=0):\n        # compute idf\n        self.compute_doc_freq()\n        # assert to check document frequency\n        assert len(self.ctest) >= max(self.document_frequency.values())\n        # compute cider score\n        score = self.compute_cider()\n        return np.mean(np.array(score)), np.array(score)\n'"
nmtpytorch/cocoeval/meteor/__init__.py,0,"b""__author__ = 'tylin'\n"""
nmtpytorch/cocoeval/meteor/meteor.py,0,"b'# -*- coding: utf-8 -*-\n# Python wrapper for METEOR implementation, by Xinlei Chen\n# Acknowledge Michael Denkowski for the generous discussion and help\n\nimport os\nimport shutil\nimport threading\nimport subprocess\n\nfrom ...utils.misc import get_meteor_jar\n\n\nclass Meteor:\n    def __init__(self, language, norm=False):\n        self.jar = str(get_meteor_jar())\n        self.meteor_cmd = [\'java\', \'-jar\', \'-Xmx2G\', self.jar,\n                           \'-\', \'-\', \'-stdio\', \'-l\', language]\n        self.env = os.environ\n        self.env[\'LC_ALL\'] = \'en_US.UTF_8\'\n\n        # Sanity check\n        if shutil.which(\'java\') is None:\n            raise RuntimeError(\'METEOR requires java which is not installed.\')\n\n        if norm:\n            self.meteor_cmd.append(\'-norm\')\n\n        self.meteor_p = subprocess.Popen(self.meteor_cmd,\n                                         stdin=subprocess.PIPE,\n                                         stdout=subprocess.PIPE,\n                                         stderr=subprocess.PIPE,\n                                         env=self.env,\n                                         universal_newlines=True, bufsize=1)\n        # Used to guarantee thread safety\n        self.lock = threading.Lock()\n\n    def method(self):\n        return ""METEOR""\n\n    def compute_score(self, gts, res):\n        imgIds = sorted(list(gts.keys()))\n        scores = []\n\n        eval_line = \'EVAL\'\n        self.lock.acquire()\n        for i in imgIds:\n            assert len(res[i]) == 1\n\n            hypothesis_str = res[i][0].replace(\'|||\', \'\').replace(\'  \', \' \')\n            score_line = \' ||| \'.join(\n                (\'SCORE\', \' ||| \'.join(gts[i]), hypothesis_str))\n\n            # We obtained --> SCORE ||| reference 1 words |||\n            # reference n words ||| hypothesis words\n            self.meteor_p.stdin.write(score_line + \'\\n\')\n            stat = self.meteor_p.stdout.readline().strip()\n            eval_line += \' ||| {}\'.format(stat)\n\n        # Send to METEOR\n        self.meteor_p.stdin.write(eval_line + \'\\n\')\n\n        # Collect segment scores\n        for i in range(len(imgIds)):\n            score = float(self.meteor_p.stdout.readline().strip())\n            scores.append(score)\n\n        # Final score\n        final_score = 100 * float(self.meteor_p.stdout.readline().strip())\n        self.lock.release()\n\n        return final_score, scores\n\n    def __del__(self):\n        self.lock.acquire()\n        self.meteor_p.stdin.close()\n        self.meteor_p.wait()\n        self.lock.release()\n'"
nmtpytorch/cocoeval/rouge/__init__.py,0,"b""__author__ = 'vrama91'\n"""
nmtpytorch/cocoeval/rouge/rouge.py,0,"b'# -*- coding: utf-8 -*-\n# File Name : rouge.py\n#\n# Description : Computes ROUGE-L metric as described by Lin and Hovey (2004)\n#\n# Creation Date : 2015-01-07 06:03\n# Author : Ramakrishna Vedantam <vrama91@vt.edu>\n\nimport numpy as np\n\n\ndef my_lcs(string, sub):\n    """"""\n    Calculates longest common subsequence for a pair of tokenized strings\n    :param string : list of str : tokens from a string split using whitespace\n    :param sub : list of str : shorter string, also split using whitespace\n    :returns: length (list of int): length of the longest common subsequence\n        between the two strings\n\n    my_lcs only gives length of the longest common subsequence,\n    not the actual LCS\n    """"""\n    if len(string) < len(sub):\n        sub, string = string, sub\n\n    lengths = [[0 for i in range(0, len(sub) + 1)] for j\n               in range(0, len(string) + 1)]\n\n    for j in range(1, len(sub) + 1):\n        for i in range(1, len(string) + 1):\n            if string[i - 1] == sub[j - 1]:\n                lengths[i][j] = lengths[i - 1][j - 1] + 1\n            else:\n                lengths[i][j] = max(lengths[i - 1][j], lengths[i][j - 1])\n\n    return lengths[len(string)][len(sub)]\n\n\nclass Rouge:\n    """"""Class for computing ROUGE-L score for a set of candidate sentences\n    for the MS COCO test set.""""""\n    def __init__(self):\n        # vrama91: updated the value below based on discussion with Hovey\n        self.beta = 1.2\n\n    def calc_score(self, candidate, refs):\n        """"""\n        Compute ROUGE-L score given one candidate and references for an image\n        :param candidate: str : candidate sentence to be evaluated\n        :param refs: list of str : COCO reference sentences for the particular\n            image to be evaluated\n        :returns score: int (ROUGE-L score for the candidate evaluated\n            against references)\n        """"""\n        assert len(candidate) == 1\n        assert len(refs) > 0\n        prec = []\n        rec = []\n\n        # split into tokens\n        token_c = candidate[0].split("" "")\n\n        for reference in refs:\n            # split into tokens\n            token_r = reference.split("" "")\n            # compute the longest common subsequence\n            lcs = my_lcs(token_r, token_c)\n            prec.append(lcs / float(len(token_c)))\n            rec.append(lcs / float(len(token_r)))\n\n        prec_max = max(prec)\n        rec_max = max(rec)\n\n        if prec_max != 0 and rec_max != 0:\n            score = ((1 + self.beta**2) * prec_max * rec_max)\n            score /= float(rec_max + self.beta ** 2 * prec_max)\n        else:\n            score = 0.0\n        return score\n\n    def compute_score(self, gts, res):\n        """"""\n        Computes Rouge-L score given a set of reference and candidate\n        sentences for the dataset\n\n        :param hypo_for_image: dict : candidate / test sentences with\n            ""image name"" key and ""tokenized sentences"" as values\n        :param ref_for_image: dict : reference MS-COCO sentences with\n            ""image name"" key and ""tokenized sentences"" as values\n        :returns: average_score: float (mean ROUGE-L score computed by\n            averaging scores for all the images)\n        """"""\n        score = []\n        for id in sorted(gts.keys()):\n            hypo = res[id]\n            ref = gts[id]\n\n            score.append(self.calc_score(hypo, ref))\n\n            # Sanity check.\n            assert isinstance(hypo, list)\n            assert isinstance(ref, list)\n            assert len(hypo) == 1\n            assert len(ref) > 0\n\n        average_score = np.mean(np.array(score))\n        return average_score, np.array(score)\n\n    def method(self):\n        return ""Rouge""\n'"
nmtpytorch/layers/attention/__init__.py,0,"b""from .mlp import MLPAttention\nfrom .dot import DotAttention\nfrom .hierarchical import HierarchicalAttention\nfrom .co import CoAttention\nfrom .mhco import MultiHeadCoAttention\nfrom .uniform import UniformAttention\nfrom .scaled_dot import ScaledDotAttention\n\n\ndef get_attention(type_):\n    return {\n        'mlp': MLPAttention,\n        'dot': DotAttention,\n        'hier': HierarchicalAttention,\n        'co': CoAttention,\n        'mhco': MultiHeadCoAttention,\n        'uniform': UniformAttention,\n        'scaled_dot': ScaledDotAttention,\n    }[type_]\n"""
nmtpytorch/layers/attention/co.py,4,"b'# -*- coding: utf-8 -*-\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom ...utils.nn import get_activation_fn\n\n# Code contributed by @jlibovicky\n\n\nclass CoAttention(nn.Module):\n    """"""Co-attention between two sequences.\n\n    Uses one hidden layer to compute an affinity matrix between two sequences.\n    This can be then normalized in two direction which gives us 1->2 and 2->1\n    attentions.\n\n    The co-attention is computed using a single feed-forward layer as in\n    Bahdanau\'s attention.\n    """"""\n    def __init__(self, ctx_1_dim, ctx_2_dim, bottleneck,\n                 att_activ=\'tanh\', mlp_bias=False):\n        super().__init__()\n\n        self.mlp_hid = nn.Conv2d(ctx_1_dim + ctx_2_dim, bottleneck, 1)\n        self.mlp_out = nn.Conv2d(bottleneck, 1, 1, bias=mlp_bias)\n        self.activ = get_activation_fn(att_activ)\n\n        self.project_1_to_2 = nn.Linear(ctx_1_dim + ctx_2_dim, bottleneck)\n        self.project_2_to_1 = nn.Linear(ctx_1_dim + ctx_2_dim, bottleneck)\n\n    def forward(self, ctx_1, ctx_2, ctx_1_mask=None, ctx_2_mask=None):\n        if ctx_2_mask is not None:\n            ctx_2_neg_mask = (1. - ctx_2_mask.transpose(0, 1).unsqueeze(1)) * -1e12\n\n        ctx_1_len = ctx_1.size(0)\n        ctx_2_len = ctx_2.size(0)\n        b_ctx_1 = ctx_1.permute(1, 2, 0).unsqueeze(3).repeat(1, 1, 1, ctx_2_len)\n        b_ctx_2 = ctx_2.permute(1, 2, 0).unsqueeze(2).repeat(1, 1, ctx_1_len, 1)\n\n        catted = torch.cat([b_ctx_1, b_ctx_2], dim=1)\n        hidden = self.activ(self.mlp_hid(catted))\n        affinity_matrix = self.mlp_out(hidden).squeeze(1)\n        if ctx_1_mask is not None:\n            ctx_1_neg_mask = (1. - ctx_1_mask.transpose(0, 1).unsqueeze(2)) * -1e12\n            affinity_matrix += ctx_1_neg_mask\n\n        if ctx_2_mask is not None:\n            ctx_2_neg_mask = (1. - ctx_2_mask.transpose(0, 1).unsqueeze(1)) * -1e12\n            affinity_matrix += ctx_2_neg_mask\n\n        dist_1_to_2 = F.softmax(affinity_matrix, dim=2)\n        context_1_to_2 = ctx_1.permute(1, 2, 0).matmul(dist_1_to_2).permute(2, 0, 1)\n        seq_1_to_2 = self.activ(\n            self.project_1_to_2(torch.cat([ctx_2, context_1_to_2], dim=-1)))\n\n        dist_2_to_1 = F.softmax(affinity_matrix, dim=1).transpose(1, 2)\n        context_2_to_1 = ctx_2.permute(1, 2, 0).matmul(dist_2_to_1).permute(2, 0, 1)\n        seq_2_to_1 = self.activ(\n            self.project_2_to_1(torch.cat([ctx_1, context_2_to_1], dim=-1)))\n\n        return seq_2_to_1, seq_1_to_2\n'"
nmtpytorch/layers/attention/dot.py,2,"b'# -*- coding: utf-8 -*-\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom ...utils.nn import get_activation_fn\n\n\nclass DotAttention(nn.Module):\n    """"""Attention layer with dot product.""""""\n    def __init__(self, ctx_dim, hid_dim, att_bottleneck=\'ctx\',\n                 transform_ctx=True, att_activ=\'tanh\', temp=1., ctx2hid=True,\n                 mlp_bias=None):\n        # NOTE:\n        # mlp_bias here to not break models that pass mlp_bias to all types\n        # of attentions\n        super().__init__()\n\n        self.ctx_dim = ctx_dim\n        self.hid_dim = hid_dim\n        self._ctx2hid = ctx2hid\n        self.temperature = temp\n        self.activ = get_activation_fn(att_activ)\n\n        # The common dimensionality for inner formulation\n        if isinstance(att_bottleneck, int):\n            self.mid_dim = att_bottleneck\n        else:\n            self.mid_dim = getattr(self, \'{}_dim\'.format(att_bottleneck))\n\n        # Adaptor from RNN\'s hidden dim to mid_dim\n        self.hid2ctx = nn.Linear(self.hid_dim, self.mid_dim, bias=False)\n\n        if transform_ctx or self.mid_dim != self.ctx_dim:\n            # Additional context projection within same dimensionality\n            self.ctx2ctx = nn.Linear(self.ctx_dim, self.mid_dim, bias=False)\n        else:\n            self.ctx2ctx = lambda x: x\n\n        if self._ctx2hid:\n            # ctx2hid: final transformation from ctx to hid\n            self.ctx2hid = nn.Linear(self.ctx_dim, self.hid_dim, bias=False)\n        else:\n            self.ctx2hid = lambda x: x\n\n    def forward(self, hid, ctx, ctx_mask=None):\n        r""""""Computes attention probabilities and final context using\n        decoder\'s hidden state and source annotations.\n\n        Arguments:\n            hid(Tensor): A set of decoder hidden states of shape `T*B*H`\n                where `T` == 1, `B` is batch dim and `H` is hidden state dim.\n            ctx(Tensor): A set of annotations of shape `S*B*C` where `S`\n                is the source timestep dim, `B` is batch dim and `C`\n                is annotation dim.\n            ctx_mask(FloatTensor): A binary mask of shape `S*B` with zeroes\n                in the padded positions.\n\n        Returns:\n            scores(Tensor): A tensor of shape `S*B` containing normalized\n                attention scores for each position and sample.\n            z_t(Tensor): A tensor of shape `B*H` containing the final\n                attended context vector for this target decoding timestep.\n\n        Notes:\n            This will only work when `T==1` for now.\n        """"""\n        # SxBxC\n        ctx_ = self.ctx2ctx(ctx)\n        # TxBxC\n        hid_ = self.hid2ctx(hid)\n\n        # shuffle dims to prepare for batch mat-mult -> SxB\n        scores = torch.bmm(hid_.permute(1, 0, 2), ctx_.permute(1, 2, 0)).div(\n            self.temperature).squeeze(1).t()\n\n        # Normalize attention scores correctly -> S*B\n        if ctx_mask is not None:\n            # Mask out padded positions with -inf so that they get 0 attention\n            scores.masked_fill_((1 - ctx_mask).bool(), -1e8)\n\n        alpha = F.softmax(scores, dim=0)\n\n        # Transform final context vector to H for further decoders\n        return alpha, self.ctx2hid((alpha.unsqueeze(-1) * ctx).sum(0))\n'"
nmtpytorch/layers/attention/hierarchical.py,2,"b'# -*- coding: utf-8 -*-\nimport torch\nfrom torch import nn\n\nfrom ...utils.nn import get_activation_fn\n\n\n# Libovick\xc3\xbd, J., & Helcl, J. (2017). Attention Strategies for Multi-Source\n# Sequence-to-Sequence Learning. In Proceedings of the 55th Annual Meeting of\n# the Association for Computational Linguistics (Volume 2: Short Papers)\n# (Vol. 2, pp. 196-202). [Code contributed by @jlibovicky]\n\n\nclass HierarchicalAttention(nn.Module):\n    """"""Hierarchical attention over multiple modalities.""""""\n    def __init__(self, ctx_dims, hid_dim, mid_dim, att_activ=\'tanh\'):\n        super().__init__()\n\n        self.activ = get_activation_fn(att_activ)\n        self.ctx_dims = ctx_dims\n        self.hid_dim = hid_dim\n        self.mid_dim = mid_dim\n\n        self.ctx_projs = nn.ModuleList([\n            nn.Linear(dim, mid_dim, bias=False) for dim in self.ctx_dims])\n        self.dec_proj = nn.Linear(hid_dim, mid_dim, bias=True)\n        self.mlp = nn.Linear(self.mid_dim, 1, bias=False)\n\n    def forward(self, contexts, hid):\n        dec_state_proj = self.dec_proj(hid)\n        ctx_projected = torch.cat([\n            p(ctx).unsqueeze(0) for p, ctx\n            in zip(self.ctx_projs, contexts)], dim=0)\n        energies = self.mlp(self.activ(dec_state_proj + ctx_projected))\n        att_dist = nn.functional.softmax(energies, dim=0)\n\n        ctxs_cat = torch.cat([c.unsqueeze(0) for c in contexts])\n        joint_context = (att_dist * ctxs_cat).sum(0)\n\n        return att_dist, joint_context\n'"
nmtpytorch/layers/attention/mhco.py,6,"b'# -*- coding: utf-8 -*-\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n# Code contributed by @jlibovicky\n\n\nclass MultiHeadCoAttention(nn.Module):\n    """"""Generalization of multi-head attention for co-attention.""""""\n\n    def __init__(self, ctx_1_dim, ctx_2_dim, bottleneck, head_count, dropout=0.1):\n        assert bottleneck % head_count == 0\n        self.dim_per_head = bottleneck // head_count\n        self.model_dim = bottleneck\n\n        super().__init__()\n        self.head_count = head_count\n\n        self.linear_keys_1 = nn.Linear(ctx_1_dim,\n                                       head_count * self.dim_per_head)\n        self.linear_values_1 = nn.Linear(ctx_1_dim,\n                                         head_count * self.dim_per_head)\n        self.linear_keys_2 = nn.Linear(ctx_2_dim,\n                                       head_count * self.dim_per_head)\n        self.linear_values_2 = nn.Linear(ctx_2_dim,\n                                         head_count * self.dim_per_head)\n\n        self.final_1_to_2_linear = nn.Linear(bottleneck, bottleneck)\n        self.final_2_to_1_linear = nn.Linear(bottleneck, bottleneck)\n        self.project_1_to_2 = nn.Linear(ctx_1_dim + ctx_2_dim, bottleneck)\n        self.project_2_to_1 = nn.Linear(ctx_1_dim + ctx_2_dim, bottleneck)\n\n    def forward(self, ctx_1, ctx_2, ctx_1_mask=None, ctx_2_mask=None):\n        """"""Computes the context vector and the attention vectors.""""""\n\n        def shape(x, length):\n            """"""  projection """"""\n            return x.view(\n                length, batch_size, head_count, dim_per_head).permute(1, 2, 0, 3)\n\n        def unshape(x, length):\n            """"""  compute context """"""\n            return x.transpose(1, 2).contiguous().view(\n                batch_size, length, head_count * dim_per_head).transpose(0, 1)\n\n        batch_size = ctx_1.size(1)\n        assert batch_size == ctx_2.size(1)\n        dim_per_head = self.dim_per_head\n        head_count = self.head_count\n        ctx_1_len = ctx_1.size(0)\n        ctx_2_len = ctx_2.size(0)\n\n        # 1) Project key, value, and key_2.\n        key_1_up = shape(self.linear_keys_1(ctx_1), ctx_1_len)\n        value_1_up = shape(self.linear_values_1(ctx_1), ctx_1_len)\n        key_2_up = shape(self.linear_keys_2(ctx_2), ctx_2_len)\n        value_2_up = shape(self.linear_values_2(ctx_2), ctx_2_len)\n\n        scores = torch.matmul(key_2_up, key_1_up.transpose(2, 3))\n\n        if ctx_1_mask is not None:\n            mask = ctx_1_mask.t().unsqueeze(2).unsqueeze(3).expand_as(scores)\n            scores = scores.masked_fill(mask.bool(), -1e18)\n        if ctx_2_mask is not None:\n            mask = ctx_2_mask.t().unsqueeze(1).unsqueeze(3).expand_as(scores)\n            scores = scores.masked_fill(mask.bool(), -1e18)\n\n        # 3) Apply attention dropout and compute context vectors.\n        dist_1_to_2 = F.softmax(scores, dim=2)\n        context_1_to_2 = unshape(torch.matmul(dist_1_to_2, value_1_up), ctx_2_len)\n        context_1_to_2 = self.final_1_to_2_linear(context_1_to_2)\n        seq_1_to_2 = self.activ(\n            self.project_1_to_2(torch.cat([ctx_2, context_1_to_2], dim=-1)))\n\n        dist_2_to_1 = F.softmax(scores, dim=1)\n        context_2_to_1 = unshape(\n            torch.matmul(dist_2_to_1.transpose(2, 3), value_2_up), ctx_1_len)\n        context_2_to_1 = self.final_2_to_1_linear(context_2_to_1)\n        seq_2_to_1 = self.activ(\n            self.project_2_to_1(torch.cat([ctx_1, context_2_to_1], dim=-1)))\n\n        return seq_2_to_1, seq_1_to_2\n'"
nmtpytorch/layers/attention/mlp.py,2,"b'# -*- coding: utf-8 -*-\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom .dot import DotAttention\n\n\nclass MLPAttention(DotAttention):\n    """"""Attention layer with feed-forward layer.""""""\n    def __init__(self, ctx_dim, hid_dim, att_bottleneck=\'ctx\',\n                 transform_ctx=True, att_activ=\'tanh\',\n                 mlp_bias=False, temp=1., ctx2hid=True):\n        super().__init__(ctx_dim, hid_dim, att_bottleneck, transform_ctx,\n                         att_activ, temp, ctx2hid)\n\n        if mlp_bias:\n            self.bias = nn.Parameter(torch.Tensor(self.mid_dim))\n            self.bias.data.zero_()\n        else:\n            self.register_parameter(\'bias\', None)\n\n        self.mlp = nn.Linear(self.mid_dim, 1, bias=False)\n\n    def forward(self, hid, ctx, ctx_mask=None):\n        r""""""Computes attention probabilities and final context using\n        decoder\'s hidden state and source annotations.\n\n        Arguments:\n            hid(Tensor): A set of decoder hidden states of shape `T*B*H`\n                where `T` == 1, `B` is batch dim and `H` is hidden state dim.\n            ctx(Tensor): A set of annotations of shape `S*B*C` where `S`\n                is the source timestep dim, `B` is batch dim and `C`\n                is annotation dim.\n            ctx_mask(FloatTensor): A binary mask of shape `S*B` with zeroes\n                in the padded positions.\n\n        Returns:\n            scores(Tensor): A tensor of shape `S*B` containing normalized\n                attention scores for each position and sample.\n            z_t(Tensor): A tensor of shape `B*H` containing the final\n                attended context vector for this target decoding timestep.\n\n        Notes:\n            This will only work when `T==1` for now.\n        """"""\n        # inner_sum -> SxBxC + TxBxC\n        inner_sum = self.ctx2ctx(ctx) + self.hid2ctx(hid)\n\n        if self.bias is not None:\n            inner_sum.add_(self.bias)\n\n        # Compute scores- > SxB\n        scores = self.mlp(\n            self.activ(inner_sum)).div(self.temperature).squeeze(-1)\n\n        # Normalize attention scores correctly -> S*B\n        if ctx_mask is not None:\n            # Mask out padded positions with -inf so that they get 0 attention\n            scores.masked_fill_((1 - ctx_mask).bool(), -1e8)\n\n        alpha = F.softmax(scores, dim=0)\n\n        # Transform final context vector to H for further decoders\n        return alpha, self.ctx2hid((alpha.unsqueeze(-1) * ctx).sum(0))\n'"
nmtpytorch/layers/attention/scaled_dot.py,7,"b'# -*- coding: utf-8 -*-\nimport math\n\nimport torch\n\n\nclass ScaledDotAttention(torch.nn.Module):\n    """"""Scaled Dot-product attention from `Attention is all you need`.\n\n    Arguments:\n\n    Input:\n\n    Output:\n    """"""\n\n    def __init__(self, model_dim, n_heads, causal=False):\n        super().__init__()\n        self.model_dim = model_dim\n        self.n_heads = n_heads\n        self.causal = causal\n\n        #self.k_dim = self.model_dim / self.n_heads\n        #self.v_dim = self.model_dim / self.n_heads\n\n        # Efficient linear projections for all heads\n        self.lin_k = torch.nn.Linear(\n            self.model_dim, self.model_dim, bias=False)\n        self.lin_q = torch.nn.Linear(\n            self.model_dim, self.model_dim, bias=False)\n        self.lin_v = torch.nn.Linear(\n            self.model_dim, self.model_dim, bias=False)\n\n        # Final output layer is independent of number of heads\n        self.lin_o = torch.nn.Linear(\n            self.model_dim, self.model_dim, bias=False)\n\n        self.scale = math.sqrt(self.model_dim / self.n_heads)\n\n    def forward(self, inputs):\n        """"""Scaled dot-product attention forward-pass\n\n        :param inputs: dictionary with query, key, value and mask tensors\n            the shape of the tensors are (tstep, bsize, dim) except for the\n            mask which is (tstep, bsize)\n\n        :return: foo\n        """"""\n        q, k, v, mask = inputs\n        # q is the query, v is the actual inputs and k is v\'s representation\n        # for self attention q=v=k\n        # for cross attention q != (v=k)\n        # Project keys, queries and values --> (bsize, tstep, dim)\n        tstep, bsize = mask.shape\n        head_view = (tstep, bsize, self.n_heads, -1)\n        # qp: (bsize, head, tstep, dim)\n        # vp: (bsize, head, tstep, dim)\n        # kp: (bsize, head, dim, tstep)\n        qp = self.lin_q(q).view(*head_view).permute(1, 2, 0, 3)\n        vp = self.lin_v(v).view(*head_view).permute(1, 2, 0, 3)\n        kp = self.lin_k(k).view(*head_view).permute(1, 2, 3, 0)\n\n        # z: (bsize, head, tstep, tstep)\n        z = torch.matmul(qp, kp).div(self.scale).softmax(dim=-1)\n        out = torch.matmul(z, vp).permute(2, 0, 1, 3).reshape_as(v)\n        return (v, out, mask)\n'"
nmtpytorch/layers/attention/uniform.py,2,"b'# -*- coding: utf-8 -*-\nimport torch\n\n\nclass UniformAttention(torch.nn.Module):\n    """"""A dummy non-parametric attention layer that applies uniform weights.""""""\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, hid, ctx, ctx_mask=None):\n        alpha = torch.ones(*ctx.shape[:2], device=ctx.device).div(ctx.shape[0])\n        wctx = (alpha.unsqueeze(-1) * ctx).sum(0)\n        return alpha, wctx\n'"
nmtpytorch/layers/decoders/__init__.py,0,"b'from .conditional import ConditionalDecoder\nfrom .simplegru import SimpleGRUDecoder\nfrom .conditionalmm import ConditionalMMDecoder\nfrom .multisourceconditional import MultiSourceConditionalDecoder\nfrom .xu import XuDecoder\nfrom .switchinggru import SwitchingGRUDecoder\nfrom .vector import VectorDecoder\n\n\ndef get_decoder(type_):\n    """"""Only expose ones with compatible __init__() arguments for now.""""""\n    return {\n        \'cond\': ConditionalDecoder,\n        \'simplegru\': SimpleGRUDecoder,\n        \'condmm\': ConditionalMMDecoder,\n        \'vector\': VectorDecoder,\n    }[type_]\n'"
nmtpytorch/layers/decoders/conditional.py,6,"b'# -*- coding: utf-8 -*-\nfrom collections import defaultdict\nimport random\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom ...utils.nn import get_rnn_hidden_state, get_activation_fn\nfrom .. import FF\nfrom ..attention import get_attention\n\n\nclass ConditionalDecoder(nn.Module):\n    """"""A conditional decoder with attention \xc3\xa0 la dl4mt-tutorial.""""""\n    def __init__(self, input_size, hidden_size, ctx_size_dict, ctx_name, n_vocab,\n                 rnn_type, tied_emb=False, dec_init=\'zero\', dec_init_activ=\'tanh\',\n                 dec_init_size=None, att_type=\'mlp\',\n                 att_activ=\'tanh\', att_bottleneck=\'ctx\', att_temp=1.0,\n                 att_ctx2hid=True,\n                 transform_ctx=True, mlp_bias=False, dropout_out=0,\n                 emb_maxnorm=None, emb_gradscale=False, sched_sample=0,\n                 bos_type=\'emb\', bos_dim=None, bos_activ=None, bos_bias=False,\n                 out_logic=\'simple\', emb_interact=None, emb_interact_dim=None,\n                 emb_interact_activ=None, dec_inp_activ=None):\n        super().__init__()\n\n        # Normalize case\n        self.rnn_type = rnn_type.upper()\n        self.out_logic = out_logic\n        # A persistent dictionary to save activations for further debugging\n        # Currently only used in MMT decoder\n        self.persistence = defaultdict(list)\n\n        # Safety checks\n        assert self.rnn_type in (\'GRU\', \'LSTM\'), \\\n            ""rnn_type \'{}\' not known"".format(rnn_type)\n        assert bos_type in (\'emb\', \'feats\', \'zero\'), ""Unknown bos_type""\n        assert dec_init.startswith((\'zero\', \'feats\', \'sum_ctx\', \'mean_ctx\', \'max_ctx\', \'last_ctx\')), \\\n            ""dec_init \'{}\' not known"".format(dec_init)\n\n        RNN = getattr(nn, \'{}Cell\'.format(self.rnn_type))\n        # LSTMs have also the cell state\n        self.n_states = 1 if self.rnn_type == \'GRU\' else 2\n\n        # Set custom handlers for GRU/LSTM\n        if self.rnn_type == \'GRU\':\n            self._rnn_unpack_states = lambda x: x\n            self._rnn_pack_states = lambda x: x\n        elif self.rnn_type == \'LSTM\':\n            self._rnn_unpack_states = self._lstm_unpack_states\n            self._rnn_pack_states = self._lstm_pack_states\n\n        # Set decoder initializer\n        self._init_func = getattr(self, \'_rnn_init_{}\'.format(dec_init))\n\n        # Other arguments\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.ctx_size_dict = ctx_size_dict\n        self.ctx_name = ctx_name\n        self.n_vocab = n_vocab\n        self.tied_emb = tied_emb\n        self.dec_init = dec_init\n        self.dec_init_size = dec_init_size\n        self.dec_init_activ = dec_init_activ\n        self.att_bottleneck = att_bottleneck\n        self.att_activ = att_activ\n        self.att_type = att_type\n        self.att_temp = att_temp\n        self.transform_ctx = transform_ctx\n        self.att_ctx2hid = att_ctx2hid\n        self.mlp_bias = mlp_bias\n        self.dropout_out = dropout_out\n        self.emb_maxnorm = emb_maxnorm\n        self.emb_gradscale = emb_gradscale\n        self.sched_sample = sched_sample\n        self.bos_type = bos_type\n        self.bos_dim = bos_dim\n        self.bos_activ = bos_activ\n        self.bos_bias = bos_bias\n        self.emb_interact = emb_interact\n        self.emb_interact_dim = emb_interact_dim\n        self.emb_interact_activ = emb_interact_activ\n        self.dec_inp_activ_fn = get_activation_fn(dec_inp_activ)\n\n        # no-ops\n        self.emb_fn = lambda e, f: e\n        self.v_emb = None\n\n        if self.emb_interact and self.emb_interact.startswith(\'trg\'):\n            self.ff_feats = FF(\n                self.emb_interact_dim, self.input_size, bias=True,\n                activ=self.emb_interact_activ)\n            if self.emb_interact == \'trgmul\':\n                self.emb_fn = lambda e, f: e * f\n            elif self.emb_interact == \'trgsum\':\n                self.emb_fn = lambda e, f: e + f\n            self.emb_interact = True\n        else:\n            self.emb_interact = False\n\n        if self.bos_type == \'feats\':\n            # Learn a visual <bos> embedding\n            self.ff_bos = FF(self.bos_dim, self.input_size, bias=self.bos_bias,\n                             activ=self.bos_activ)\n\n        # Create target embeddings\n        self.emb = nn.Embedding(self.n_vocab, self.input_size,\n                                padding_idx=0, max_norm=self.emb_maxnorm,\n                                scale_grad_by_freq=self.emb_gradscale)\n\n        if self.att_type:\n            # Create attention layer\n            Attention = get_attention(self.att_type)\n            self.att = Attention(\n                self.ctx_size_dict[self.ctx_name],\n                self.hidden_size,\n                transform_ctx=self.transform_ctx,\n                ctx2hid=self.att_ctx2hid,\n                mlp_bias=self.mlp_bias,\n                att_activ=self.att_activ,\n                att_bottleneck=self.att_bottleneck,\n                temp=self.att_temp)\n\n        if self.dec_init != \'zero\':\n            # For source-based inits, input size is the encoding size\n            # For \'feats\', it\'s given by dec_init_size, no need to infer\n            if self.dec_init.endswith(\'_ctx\'):\n                self.dec_init_size = self.ctx_size_dict[self.ctx_name]\n            # Add a FF layer for decoder initialization\n            self.ff_dec_init = FF(\n                self.dec_init_size,\n                self.hidden_size * self.n_states,\n                activ=self.dec_init_activ)\n\n        # Create decoders\n        self.dec0 = RNN(self.input_size, self.hidden_size)\n        if self.att_type:\n            # If no attention, do not add the 2nd GRU\n            self.dec1 = RNN(self.hidden_size, self.hidden_size)\n\n        # Output dropout\n        if self.dropout_out > 0:\n            self.do_out = nn.Dropout(p=self.dropout_out)\n\n        # Output bottleneck: maps hidden states to target emb dim\n        # simple: tanh(W*h)\n        #   deep: tanh(W*h + U*emb + V*ctx)\n        out_inp_size = self.hidden_size\n\n        # Dummy op to return back the hidden state for simple output\n        self.out_merge_fn = lambda h, e, c: h\n\n        if self.out_logic == \'deep\':\n            out_inp_size += (self.input_size + self.hidden_size)\n            self.out_merge_fn = lambda h, e, c: torch.cat((h, e, c), dim=1)\n\n        # Final transformation that receives concatenated outputs or only h\n        self.hid2out = FF(out_inp_size, self.input_size,\n                          bias_zero=True, activ=\'tanh\')\n\n        # Final softmax\n        self.out2prob = FF(self.input_size, self.n_vocab)\n\n        # Tie input embedding matrix and output embedding matrix\n        if self.tied_emb:\n            self.out2prob.weight = self.emb.weight\n\n        self.nll_loss = nn.NLLLoss(reduction=""sum"", ignore_index=0)\n\n    def _lstm_pack_states(self, h):\n        """"""Pack LSTM hidden and cell state.""""""\n        return torch.cat(h, dim=-1)\n\n    def _lstm_unpack_states(self, h):\n        """"""Unpack LSTM hidden and cell state to tuple.""""""\n        return torch.split(h, self.hidden_size, dim=-1)\n\n    def _rnn_init_zero(self, ctx_dict):\n        """"""Zero initialization.""""""\n        ctx, _ = ctx_dict[self.ctx_name]\n        return torch.zeros(\n            ctx.shape[1], self.hidden_size * self.n_states, device=ctx.device)\n\n    def _rnn_init_mean_ctx(self, ctx_dict):\n        """"""Initialization with mean-pooled source annotations.""""""\n        ctx, ctx_mask = ctx_dict[self.ctx_name]\n        return self.ff_dec_init(\n            ctx.sum(0).div(ctx_mask.unsqueeze(-1).sum(0))\n            if ctx_mask is not None else ctx.mean(0))\n\n    def _rnn_init_sum_ctx(self, ctx_dict):\n        """"""Initialization with sum-pooled source annotations.""""""\n        ctx, ctx_mask = ctx_dict[self.ctx_name]\n        assert ctx_mask is None\n        return self.ff_dec_init(ctx.sum(0))\n\n    def _rnn_init_max_ctx(self, ctx_dict):\n        """"""Initialization with max-pooled source annotations.""""""\n        ctx, ctx_mask = ctx_dict[self.ctx_name]\n        # Max-pooling may not care about mask (depends on non-linearity maybe)\n        return self.ff_dec_init(ctx.max(0)[0])\n\n    def _rnn_init_last_ctx(self, ctx_dict):\n        """"""Initialization with the last source annotation.""""""\n        ctx, ctx_mask = ctx_dict[self.ctx_name]\n        if ctx_mask is None:\n            h_0 = self.ff_dec_init(ctx[-1])\n        else:\n            last_idxs = ctx_mask.sum(0).sub(1).long()\n            h_0 = self.ff_dec_init(ctx[last_idxs, range(ctx.shape[1])])\n        return h_0\n\n    def _rnn_init_feats(self, ctx_dict):\n        """"""Feature based decoder initialization.""""""\n        return self.ff_dec_init(ctx_dict[\'feats\'][0].squeeze(0))\n\n    def get_emb(self, idxs, tstep=-1):\n        """"""Returns time-step based embeddings.""""""\n        if tstep == 0:\n            if self.bos_type == \'zero\':\n                # Constant-zero <bos> embedding\n                return torch.zeros(\n                    idxs.shape[0], self.input_size, device=idxs.device)\n            elif self.bos_type == \'feats\':\n                # Feature-based <bos> computed in f_init()\n                return self.bos\n        # For other timesteps, look up the embedding layer\n        if self.emb_interact and idxs.ndimension() == 1:\n                # beam search\n                embs = self.emb(idxs)\n                return self.emb_fn(\n                    embs.view((self.v_emb.shape[0], -1, self.v_emb.shape[1])),\n                    self.v_emb.unsqueeze(1)).view(embs.shape)\n        else:\n            return self.emb_fn(self.emb(idxs), self.v_emb)\n\n    def f_init(self, ctx_dict):\n        """"""Returns the initial h_0 for the decoder.""""""\n        self.history = defaultdict(list)\n        # Compute <bos> out of \'feats\' if requested\n        if self.bos_type == \'feats\':\n            self.bos = self.ff_bos(ctx_dict[\'feats\'][0]).squeeze(0)\n        if self.emb_interact:\n            # We have features for embedding interaction\n            self.v_emb = self.ff_feats(ctx_dict[\'feats\'][0]).squeeze(0)\n        return self._init_func(ctx_dict)\n\n    def f_next(self, ctx_dict, y, h):\n        """"""Applies one timestep of recurrence.""""""\n        # Get hidden states from the first decoder (purely cond. on LM)\n        h1_c1 = self.dec0(y, self._rnn_unpack_states(h))\n        h1 = get_rnn_hidden_state(h1_c1)\n\n        # Apply attention\n        txt_alpha_t, txt_z_t = self.att(\n            h1.unsqueeze(0), *ctx_dict[self.ctx_name])\n\n        if not self.training:\n            self.history[\'alpha_txt\'].append(txt_alpha_t)\n\n        # Run second decoder (h1 is compatible now as it was returned by GRU)\n        # Additional optional transformation is to make the comparison\n        # fair with the MMT model.\n        h2_c2 = self.dec1(self.dec_inp_activ_fn(txt_z_t), h1_c1)\n        h2 = get_rnn_hidden_state(h2_c2)\n\n        # Output logic\n        logit = self.hid2out(self.out_merge_fn(h2, y, txt_z_t))\n\n        # Apply dropout if any\n        if self.dropout_out > 0:\n            logit = self.do_out(logit)\n\n        # Transform logit to T*B*V (V: vocab_size)\n        # Compute log_softmax over token dim\n        log_p = F.log_softmax(self.out2prob(logit), dim=-1)\n\n        # Return log probs and new hidden states\n        return log_p, self._rnn_pack_states(h2_c2)\n\n    def forward(self, ctx_dict, y):\n        """"""Computes the softmax outputs given source annotations `ctx_dict[self.ctx_name]`\n        and ground-truth target token indices `y`. Only called during training.\n\n        Arguments:\n            ctx_dict(dict): A dictionary of tensors that should at least contain\n                the key `ctx_name` as the main source representation of shape\n                S*B*ctx_dim`.\n            y(Tensor): A tensor of `T*B` containing ground-truth target\n                token indices for the given batch.\n        """"""\n\n        loss = 0.0\n\n        # Get initial hidden state\n        h = self.f_init(ctx_dict)\n\n        # are we doing scheduled sampling?\n        sched = self.training and (random.random() > (1 - self.sched_sample))\n\n        # Convert token indices to embeddings -> T*B*E\n        # Skip <bos> now\n        bos = self.get_emb(y[0], 0)\n        log_p, h = self.f_next(ctx_dict, bos, h)\n        loss += self.nll_loss(log_p, y[1])\n        y_emb = self.get_emb(y[1:])\n\n        for t in range(y_emb.shape[0] - 1):\n            emb = self.emb(log_p.argmax(1)) if sched else y_emb[t]\n            log_p, h = self.f_next(ctx_dict, emb, h)\n            loss += self.nll_loss(log_p, y[t + 2])\n\n        return {\'loss\': loss}\n'"
nmtpytorch/layers/decoders/conditionalmm.py,1,"b'# -*- coding: utf-8 -*-\nimport torch.nn.functional as F\n\nfrom ...utils.nn import get_rnn_hidden_state\nfrom ..attention import HierarchicalAttention, UniformAttention, get_attention\nfrom .. import Fusion\nfrom . import ConditionalDecoder\n\n\nclass ConditionalMMDecoder(ConditionalDecoder):\n    """"""A conditional multimodal decoder with multimodal attention.""""""\n    def __init__(self, fusion_type=\'concat\', fusion_activ=None,\n                 aux_ctx_name=\'image\', mm_att_type=\'md-dd\',\n                 persistent_dump=False, **kwargs):\n        super().__init__(**kwargs)\n        self.aux_ctx_name = aux_ctx_name\n        self.mm_att_type = mm_att_type\n        self.persistent_dump = persistent_dump\n\n        if self.mm_att_type == \'uniform\':\n            # Dummy uniform attention\n            self.shared_dec_state = False\n            self.shared_att_mlp = False\n        else:\n            # Parse attention type\n            att_str = sorted(self.mm_att_type.lower().split(\'-\'))\n            assert len(att_str) == 2 and att_str[0][0] == \'d\' and att_str[1][0] == \'m\', \\\n                ""att_type should be m[d|i]-d[d-i]""\n            # Independent <d>ecoder state means shared dec state\n            self.shared_dec_state = att_str[0][1] == \'i\'\n\n            # Independent <m>odality means sharing the mlp in the MLP attention\n            self.shared_att_mlp = att_str[1][1] == \'i\'\n\n            # Sanity check\n            if self.shared_att_mlp and self.att_type != \'mlp\':\n                raise Exception(""Shared attention requires MLP attention."")\n\n        # Define (context) fusion operator\n        self.fusion_type = fusion_type\n        if fusion_type == ""hierarchical"":\n            self.fusion = HierarchicalAttention(\n                [self.hidden_size, self.hidden_size],\n                self.hidden_size, self.hidden_size)\n        else:\n            if self.att_ctx2hid:\n                # Old behaviour\n                fusion_inp_size = 2 * self.hidden_size\n            else:\n                fusion_inp_sizes = list(self.ctx_size_dict.values())\n                if fusion_type == \'concat\':\n                    fusion_inp_size = sum(fusion_inp_sizes)\n                else:\n                    fusion_inp_size = fusion_inp_sizes[0]\n            self.fusion = Fusion(\n                fusion_type, fusion_inp_size, self.hidden_size,\n                fusion_activ=fusion_activ)\n\n        # Rename textual attention layer\n        self.txt_att = self.att\n        del self.att\n\n        if self.mm_att_type == \'uniform\':\n            self.img_att = UniformAttention()\n        else:\n            # Visual attention over convolutional feature maps\n            Attention = get_attention(self.att_type)\n            self.img_att = Attention(\n                self.ctx_size_dict[self.aux_ctx_name], self.hidden_size,\n                transform_ctx=self.transform_ctx, mlp_bias=self.mlp_bias,\n                ctx2hid=self.att_ctx2hid,\n                att_activ=self.att_activ,\n                att_bottleneck=self.att_bottleneck)\n\n        # Tune multimodal attention type\n        if self.shared_att_mlp:\n            # Modality independent\n            self.txt_att.mlp.weight = self.img_att.mlp.weight\n            self.txt_att.ctx2ctx.weight = self.img_att.ctx2ctx.weight\n\n        if self.shared_dec_state:\n            # Decoder independent\n            self.txt_att.hid2ctx.weight = self.img_att.hid2ctx.weight\n\n    def f_next(self, ctx_dict, y, h):\n        # Get hidden states from the first decoder (purely cond. on LM)\n        h1_c1 = self.dec0(y, self._rnn_unpack_states(h))\n        h1 = get_rnn_hidden_state(h1_c1)\n\n        # Apply attention\n        self.txt_alpha_t, txt_z_t = self.txt_att(\n            h1.unsqueeze(0), *ctx_dict[self.ctx_name])\n        self.img_alpha_t, img_z_t = self.img_att(\n            h1.unsqueeze(0), *ctx_dict[self.aux_ctx_name])\n        # Save for reg loss terms\n        self.history[\'alpha_img\'].append(self.img_alpha_t.unsqueeze(0))\n\n        # Context will double dimensionality if fusion_type is concat\n        # z_t should be compatible with hidden_size\n        if self.fusion_type == ""hierarchical"":\n            self.h_att, z_t = self.fusion([txt_z_t, img_z_t], h1.unsqueeze(0))\n        else:\n            z_t = self.fusion(txt_z_t, img_z_t)\n\n        if not self.training and self.persistent_dump:\n            # For test-time activation debugging\n            self.persistence[\'z_t\'].append(z_t.t().cpu().numpy())\n            self.persistence[\'txt_z_t\'].append(txt_z_t.t().cpu().numpy())\n            self.persistence[\'img_z_t\'].append(img_z_t.t().cpu().numpy())\n\n        # Run second decoder (h1 is compatible now as it was returned by GRU)\n        h2_c2 = self.dec1(z_t, h1_c1)\n        h2 = get_rnn_hidden_state(h2_c2)\n\n        # This is a bottleneck to avoid going from H to V directly\n        logit = self.hid2out(self.out_merge_fn(h2, y, z_t))\n\n        # Apply dropout if any\n        if self.dropout_out > 0:\n            logit = self.do_out(logit)\n\n        # Transform logit to T*B*V (V: vocab_size)\n        # Compute log_softmax over token dim\n        log_p = F.log_softmax(self.out2prob(logit), dim=-1)\n\n        # Return log probs and new hidden states\n        return log_p, self._rnn_pack_states(h2_c2)\n'"
nmtpytorch/layers/decoders/multisourceconditional.py,1,"b'# -*- coding: utf-8 -*-\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom ...utils.nn import get_rnn_hidden_state\nfrom ..attention import get_attention, HierarchicalAttention\nfrom .. import Fusion\nfrom . import ConditionalDecoder\n\n\nclass MultiSourceConditionalDecoder(ConditionalDecoder):\n    """"""A conditional multimodal decoder with multimodal attention.""""""\n    def __init__(self, ctx_names, fusion_type=\'concat\', **kwargs):\n        super().__init__(**kwargs)\n\n        # Define (context) fusion operator\n        self.ctx_names = ctx_names\n        self.fusion_type = fusion_type\n        if fusion_type == ""hierarchical"":\n            self.fusion = HierarchicalAttention(\n                [self.hidden_size for _ in ctx_names],\n                self.hidden_size, self.hidden_size)\n        else:\n            raise NotImplementedError(""Concatenation and sum work only with two inputs now."")\n            self.fusion = Fusion(\n                fusion_type, len(ctx_names) * self.hidden_size, self.hidden_size)\n\n        attns = []\n        for ctx_name in ctx_names:\n            Attention = get_attention(self.att_type)\n            attns.append(Attention(\n                self.ctx_size_dict[ctx_name], self.hidden_size,\n                transform_ctx=self.transform_ctx, mlp_bias=self.mlp_bias,\n                att_activ=self.att_activ,\n                att_bottleneck=self.att_bottleneck))\n        self.attns = nn.ModuleList(attns)\n\n    def f_next(self, ctx_dict, y, h):\n        # Get hidden states from the first decoder (purely cond. on LM)\n        h1_c1 = self.dec0(y, self._rnn_unpack_states(h))\n        h1 = get_rnn_hidden_state(h1_c1)\n\n        # Apply attention\n        ctx_list = [att(h1.unsqueeze(0), *ctx_dict[name])[1]\n                    for att, name in zip(self.attns, self.ctx_names)]\n\n        # Context will double dimensionality if fusion_type is concat\n        # z_t should be compatible with hidden_size\n        if self.fusion_type == ""hierarchical"":\n            _, z_t = self.fusion(ctx_list, h1.unsqueeze(0))\n        else:\n            z_t = self.fusion(ctx_list)\n\n        # Run second decoder (h1 is compatible now as it was returned by GRU)\n        h2_c2 = self.dec1(z_t, h1_c1)\n        h2 = get_rnn_hidden_state(h2_c2)\n\n        # This is a bottleneck to avoid going from H to V directly\n        logit = self.hid2out(h2)\n\n        # Apply dropout if any\n        if self.dropout_out > 0:\n            logit = self.do_out(logit)\n\n        # Transform logit to T*B*V (V: vocab_size)\n        # Compute log_softmax over token dim\n        log_p = F.log_softmax(self.out2prob(logit), dim=-1)\n\n        # Return log probs and new hidden states\n        return log_p, self._rnn_pack_states(h2_c2)\n'"
nmtpytorch/layers/decoders/simplegru.py,2,"b'# -*- coding: utf-8 -*-\nimport torch\nimport torch.nn.functional as F\n\nfrom .. import FF\nfrom . import ConditionalDecoder\n\n\nclass SimpleGRUDecoder(ConditionalDecoder):\n    """"""A simple GRU decoder with a single decoder layer. It has the same\n    set of parameters as the parent class except `rnn_type`.""""""\n    def __init__(self, **kwargs):\n        # Set rnn_type to GRU\n        kwargs[\'rnn_type\'] = \'gru\'\n        super().__init__(**kwargs)\n\n        # Remove second GRU\n        # Remove and replace hid2out since we now concatenate the\n        # attention output and the hidden state\n        del self.dec1, self.hid2out\n        self.hid2out = FF(2 * self.hidden_size,\n                          self.input_size, bias_zero=True, activ=\'tanh\')\n\n    def f_next(self, ctx_dict, y, h):\n        """"""Applies one timestep of recurrence.""""""\n        # Get hidden states from the first decoder (purely cond. on LM)\n        h1 = self.dec0(y, h)\n\n        # Apply attention\n        alpha_t, z_t = self.att(h1.unsqueeze(0), *ctx_dict[self.ctx_name])\n\n        if not self.training:\n            self.history[\'alpha_txt\'].append(alpha_t)\n\n        # Concatenate attented source and hidden state & project\n        o = self.hid2out(torch.cat((h1, z_t), dim=-1))\n\n        # Apply dropout if any\n        logit = self.do_out(o) if self.dropout_out > 0 else o\n\n        # Transform logit to T*B*V (V: vocab_size)\n        # Compute log_softmax over token dim\n        log_p = F.log_softmax(self.out2prob(logit), dim=-1)\n\n        # Return log probs and new hidden states\n        return log_p, h1\n'"
nmtpytorch/layers/decoders/switchinggru.py,3,"b'# -*- coding: utf-8 -*-\nfrom collections import defaultdict\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom .. import FF\nfrom ...utils.device import DEVICE\nfrom ..attention import get_attention\n\n\nclass SwitchingGRUDecoder(nn.Module):\n    """"""A multi-source aware attention based decoder. During training,\n        this decoder will be fed by a single modality at a time while\n        during inference one of the src->trg tasks will be performed.\n    """"""\n    def __init__(self, input_size, hidden_size, modality_dict, n_vocab,\n                 tied_emb=False, dropout_out=0):\n        super().__init__()\n\n        self.hidden_size = hidden_size\n        self.input_size = input_size\n        self.n_vocab = n_vocab\n        self.tied_emb = tied_emb\n        self.dropout_out = dropout_out\n\n        # Will have N attentions for N possible input modalities\n        # dict: {en_speech: (encoding_size, att_type)}\n        atts = {}\n        for name, (enc_size, att_type) in modality_dict.items():\n            atts[name] = get_attention(att_type)(enc_size, self.hidden_size)\n\n        self.atts = nn.ModuleDict(atts)\n\n        # Create target embeddings\n        self.emb = nn.Embedding(self.n_vocab, self.input_size, padding_idx=0)\n\n        # Create first decoder layer necessary for attention\n        self.dec0 = nn.GRUCell(self.input_size, self.hidden_size)\n        self.dec1 = nn.GRUCell(self.hidden_size, self.hidden_size)\n\n        # Output dropout\n        if self.dropout_out > 0:\n            self.do_out = nn.Dropout(p=self.dropout_out)\n\n        # Output bottleneck: maps hidden states to target emb dim\n        self.hid2out = FF(self.hidden_size, self.input_size,\n                          bias_zero=True, activ=\'tanh\')\n\n        # Final softmax\n        self.out2prob = FF(self.input_size, self.n_vocab)\n\n        # Tie input embedding matrix and output embedding matrix\n        if self.tied_emb:\n            self.out2prob.weight = self.emb.weight\n\n        # Final loss\n        self.nll_loss = nn.NLLLoss(reduction=""sum"", ignore_index=0)\n\n    def f_init(self, sources):\n        """"""Returns the initial h_0 for the decoder. `sources` is not used\n        but passed for compatibility with beam search.""""""\n        self.history = defaultdict(list)\n        batch_size = next(iter(sources.values()))[0].shape[1]\n        # NOTE: Non-scatter aware, fix this\n        return torch.zeros(batch_size, self.hidden_size, device=DEVICE)\n\n    def f_next(self, sources, y, h):\n        # Get hidden states from the first decoder (purely cond. on LM)\n        h_1 = self.dec0(y, h)\n\n        # sources will always contain single modality\n        assert len(sources) == 1\n        modality = list(sources.keys())[0]\n\n        # Apply modality-specific attention\n        alpha_t, z_t = self.atts[modality](h_1.unsqueeze(0), *sources[modality])\n        self.history[\'alpha_{}\'.format(modality)].append(alpha_t)\n\n        # Run second decoder (h_1 is compatible now as it was returned by GRU)\n        h_2 = self.dec1(z_t, h_1)\n\n        # This is a bottleneck to avoid going from H to V directly\n        logit = self.hid2out(h_2)\n\n        # Apply dropout if any\n        if self.dropout_out > 0:\n            logit = self.do_out(logit)\n\n        # Transform logit to T*B*V (V: vocab_size)\n        # Compute log_softmax over token dim\n        log_p = F.log_softmax(self.out2prob(logit), dim=-1)\n\n        # Return log probs and new hidden states\n        return log_p, h_2\n\n    def forward(self, sources, y):\n        """"""Computes the softmax outputs given source annotations `sources` and\n        ground-truth target token indices `y`. Only called during training.\n\n        Arguments:\n            sources(Tensor): A tensor of `S*B*ctx_dim` representing the source\n                annotations in an order compatible with ground-truth targets.\n            y(Tensor): A tensor of `T*B` containing ground-truth target\n                token indices for the given batch.\n        """"""\n\n        loss = 0.0\n        logps = None if self.training else torch.zeros(\n            y.shape[0] - 1, y.shape[1], self.n_vocab, device=y.device)\n\n        # Convert token indices to embeddings -> T*B*E\n        y_emb = self.emb(y)\n\n        # Get initial hidden state\n        h = self.f_init(sources)\n\n        # -1: So that we skip the timestep where input is <eos>\n        for t in range(y_emb.shape[0] - 1):\n            log_p, h = self.f_next(sources, y_emb[t], h)\n            if not self.training:\n                logps[t] = log_p.data\n            loss += self.nll_loss(log_p, y[t + 1])\n\n        return {\'loss\': loss, \'logps\': logps}\n'"
nmtpytorch/layers/decoders/vector.py,1,"b'# -*- coding: utf-8 -*-\nimport torch.nn.functional as F\n\nfrom ...utils.nn import get_rnn_hidden_state\nfrom . import ConditionalDecoder\n\n# Decoder without attention that uses a single input vector.\n# Layer contributed by @loicbarrault\n\n\nclass VectorDecoder(ConditionalDecoder):\n    """"""Single-layer RNN decoder using fixed-size vector representation.""""""\n    def __init__(self, **kwargs):\n        # Disable attention\n        kwargs[\'att_type\'] = None\n        super().__init__(**kwargs)\n\n    def f_next(self, ctx_dict, y, h):\n        """"""Applies one timestep of recurrence.""""""\n        # Get hidden states from the decoder\n        h1_c1 = self.dec0(y, self._rnn_unpack_states(h))\n        h1 = get_rnn_hidden_state(h1_c1)\n\n        # Project hidden state to embedding size\n        o = self.hid2out(h1)\n\n        # Apply dropout if any\n        logit = self.do_out(o) if self.dropout_out > 0 else o\n\n        # Transform logit to T*B*V (V: vocab_size)\n        # Compute log_softmax over token dim\n        log_p = F.log_softmax(self.out2prob(logit), dim=-1)\n\n        # Return log probs and new hidden states\n        return log_p, self._rnn_pack_states(h1_c1)\n'"
nmtpytorch/layers/decoders/xu.py,7,"b'# -*- coding: utf-8 -*-\nfrom collections import defaultdict\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom ...utils.nn import get_rnn_hidden_state\nfrom .. import FF\nfrom ..attention import get_attention\n\n\nclass XuDecoder(nn.Module):\n    """"""A decoder which implements Show-attend-and-tell decoder.""""""\n    def __init__(self, input_size, hidden_size, ctx_size_dict, ctx_name, n_vocab,\n                 rnn_type, tied_emb=False, dec_init=\'zero\', att_type=\'mlp\',\n                 att_activ=\'tanh\', att_bottleneck=\'ctx\',\n                 transform_ctx=True, mlp_bias=True, dropout=0,\n                 emb_maxnorm=None, emb_gradscale=False, att_temp=1.0,\n                 selector=False, prev2out=True, ctx2out=True):\n        super().__init__()\n\n        # Normalize case\n        self.rnn_type = rnn_type.upper()\n\n        # Safety checks\n        assert self.rnn_type in (\'GRU\', \'LSTM\'), \\\n            ""rnn_type \'{}\' not known"".format(rnn_type)\n        assert dec_init in (\'zero\', \'mean_ctx\'), \\\n            ""dec_init \'{}\' not known"".format(dec_init)\n\n        RNN = getattr(nn, \'{}Cell\'.format(self.rnn_type))\n        # LSTMs have also the cell state\n        self.n_states = 1 if self.rnn_type == \'GRU\' else 2\n\n        # Set custom handlers for GRU/LSTM\n        if self.rnn_type == \'GRU\':\n            self._rnn_unpack_states = lambda x: x\n            self._rnn_pack_states = lambda x: x\n        elif self.rnn_type == \'LSTM\':\n            self._rnn_unpack_states = self._lstm_unpack_states\n            self._rnn_pack_states = self._lstm_pack_states\n\n        # Set decoder initializer\n        self._init_func = getattr(self, \'_rnn_init_{}\'.format(dec_init))\n\n        # Other arguments\n        self.n_vocab = n_vocab\n        self.dropout = dropout\n        self.ctx2out = ctx2out\n        self.selector = selector\n        self.prev2out = prev2out\n        self.tied_emb = tied_emb\n        self.dec_init = dec_init\n        self.ctx_name = ctx_name\n        self.mlp_bias = mlp_bias\n        self.att_temp = att_temp\n        self.att_activ = att_activ\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.emb_maxnorm = emb_maxnorm\n        self.emb_gradscale = emb_gradscale\n        self.transform_ctx = transform_ctx\n        self.ctx_size_dict = ctx_size_dict\n        self.att_bottleneck = att_bottleneck\n\n        # Create target embeddings\n        self.emb = nn.Embedding(self.n_vocab, self.input_size,\n                                padding_idx=0, max_norm=self.emb_maxnorm,\n                                scale_grad_by_freq=self.emb_gradscale)\n\n        # Create attention layer\n        Attention = get_attention(att_type)\n        self.att = Attention(self.ctx_size_dict[self.ctx_name], self.hidden_size,\n                             transform_ctx=self.transform_ctx,\n                             mlp_bias=self.mlp_bias,\n                             att_activ=self.att_activ,\n                             att_bottleneck=self.att_bottleneck,\n                             temp=self.att_temp, ctx2hid=False)\n\n        # Decoder initializer FF (for mean_ctx)\n        if self.dec_init == \'mean_ctx\':\n            self.ff_dec_init = FF(\n                self.ctx_size_dict[self.ctx_name],\n                self.hidden_size * self.n_states, activ=\'tanh\')\n\n        # Dropout\n        if self.dropout > 0:\n            self.do = nn.Dropout(p=self.dropout)\n\n        # Gating Scalar, i.e. selector\n        if self.selector:\n            self.ff_selector = FF(self.hidden_size, 1, activ=\'sigmoid\')\n\n        if self.ctx2out:\n            self.ff_out_ctx = FF(\n                self.ctx_size_dict[self.ctx_name], self.input_size)\n\n        # Create decoder from [y_t, z_t] to dec_dim\n        self.dec0 = RNN(\n            self.input_size + self.ctx_size_dict[self.ctx_name],\n            self.hidden_size)\n\n        # Output bottleneck: maps hidden states to target emb dim\n        self.hid2out = FF(self.hidden_size, self.input_size)\n\n        # Final softmax\n        self.out2prob = FF(self.input_size, self.n_vocab)\n\n        # Tie input embedding matrix and output embedding matrix\n        if self.tied_emb:\n            self.out2prob.weight = self.emb.weight\n\n        self.nll_loss = nn.NLLLoss(reduction=""sum"", ignore_index=0)\n\n    def _lstm_pack_states(self, h):\n        return torch.cat(h, dim=-1)\n\n    def _lstm_unpack_states(self, h):\n        # Split h_t and c_t into two tensors and return a tuple\n        return torch.split(h, self.hidden_size, dim=-1)\n\n    def _rnn_init_zero(self, ctx, ctx_mask):\n        return torch.zeros(\n            ctx.shape[1], self.hidden_size * self.n_states, device=ctx.device)\n\n    def _rnn_init_mean_ctx(self, ctx, ctx_mask):\n        mean_ctx = ctx.mean(dim=0)\n        if self.dropout > 0:\n            mean_ctx = self.do(mean_ctx)\n        return self.ff_dec_init(mean_ctx)\n\n    def f_init(self, ctx_dict):\n        """"""Returns the initial h_0, c_0 for the decoder.""""""\n        self.history = defaultdict(list)\n        return self._init_func(*ctx_dict[self.ctx_name])\n\n    def f_next(self, ctx_dict, y, h):\n        # Unpack hidden states\n        h_c = self._rnn_unpack_states(h)\n\n        # Apply attention\n        img_alpha_t, z_t = self.att(\n            h_c[0].unsqueeze(0), *ctx_dict[self.ctx_name])\n        # Save reg loss terms\n        self.history[\'alpha_img\'].append(img_alpha_t.unsqueeze(0))\n\n        if self.selector:\n            z_t *= self.ff_selector(h_c[0])\n\n        # Form RNN input by concatenating embedding and weighted sum\n        # Give h as dec\'s hidden_state\n        ht_ct = self.dec0(torch.cat([y, z_t], dim=1), h_c)\n\n        # Get h_t from the combined ht_ct vector\n        h_t = get_rnn_hidden_state(ht_ct)\n        if self.dropout > 0:\n            h_t = self.do(h_t)\n\n        # This h_t, (optionally along with y and z_t)\n        # will connect to softmax() predictions.\n        logit = self.hid2out(h_t)\n\n        if self.prev2out:\n            logit += y\n\n        if self.ctx2out:\n            logit += self.ff_out_ctx(z_t)\n\n        logit = torch.tanh(logit)\n        if self.dropout > 0:\n            logit = self.do(logit)\n\n        # Transform logit to T*B*V (V: vocab_size)\n        # Compute log_softmax over token dim\n        log_p = F.log_softmax(self.out2prob(logit), dim=-1)\n\n        # Return log probs and new hidden states\n        return log_p, self._rnn_pack_states(ht_ct)\n\n    def forward(self, ctx_dict, y):\n        loss = 0.0\n        logps = None if self.training else torch.zeros(\n            y.shape[0] - 1, y.shape[1], self.n_vocab, device=y.device)\n\n        # Convert token indices to embeddings -> T*B*E\n        y_emb = self.emb(y)\n\n        # Get initial hidden state\n        h = self.f_init(ctx_dict)\n\n        # -1: So that we skip the timestep where input is <eos>\n        for t in range(y_emb.shape[0] - 1):\n            log_p, h = self.f_next(ctx_dict, y_emb[t], h)\n            if not self.training:\n                logps[t] = log_p.data\n            loss += self.nll_loss(log_p, y[t + 1])\n\n        return {\'loss\': loss, \'logps\': logps}\n'"
nmtpytorch/layers/embedding/__init__.py,0,b'from .pembedding import PEmbedding\n'
nmtpytorch/layers/embedding/pembedding.py,0,"b'# -*- coding: utf-8 -*-\nfrom torch import nn\n\nfrom .. import FF\n\n\nclass PEmbedding(nn.Embedding):\n    """"""An extension to regular `nn.Embedding` with MLP and dropout.""""""\n    def __init__(self, num_embeddings, embedding_dim, out_dim,\n                 activ=\'linear\', dropout=0.0):\n        super().__init__(num_embeddings, embedding_dim, padding_idx=0)\n        self.proj = FF(embedding_dim, out_dim, activ=activ, bias=False)\n        self.do = nn.Dropout(dropout) if dropout > 0.0 else lambda x: x\n\n    def forward(self, input):\n        # Get the embeddings from parent\'s forward\n        return self.do(self.proj(super().forward(input)))\n'"
nmtpytorch/layers/encoders/__init__.py,0,b'from .image import ImageEncoder\nfrom .text import TextEncoder\nfrom .bilstmp import BiLSTMp\nfrom .multimodal_text import MultimodalTextEncoder\nfrom .multimodal_bilstmp import MultimodalBiLSTMp\n'
nmtpytorch/layers/encoders/bilstmp.py,1,"b'# -*- coding: utf-8 -*-\nimport logging\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom ..ff import FF\n\nlogger = logging.getLogger(\'nmtpytorch\')\n\n\nclass BiLSTMp(nn.Module):\n    """"""A bidirectional LSTM encoder for speech features. A batch should\n    only contain samples that have the same sequence length.\n\n    Arguments:\n        input_size (int): Input feature dimensionality.\n        hidden_size (int): LSTM hidden state dimensionality.\n        proj_size (int): Projection layer size.\n        proj_activ (str, optional): Non-linearity to apply to intermediate projection\n            layers. (Default: \'tanh\')\n        layers (str): A \'_\' separated list of integers that defines the subsampling\n            factor for each LSTM.\n        dropout (float, optional): Use dropout (Default: 0.)\n    Input:\n        x (Tensor): A tensor of shape (n_timesteps, n_samples, n_feats)\n            that includes acoustic features of dimension ``n_feats`` per\n            each timestep (in the first dimension).\n\n    Output:\n        hs (Tensor): A tensor of shape (n_timesteps, n_samples, hidden * 2)\n            that contains encoder hidden states for all timesteps.\n        mask (Tensor): `None` since this layer expects all equal frame inputs.\n    """"""\n    def __init__(self, input_size, hidden_size, proj_size, layers,\n                 proj_activ=\'tanh\', dropout=0):\n        super().__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.proj_size = proj_size\n        self.proj_activ = proj_activ\n        self.layers = [int(i) for i in layers.split(\'_\')]\n        self.dropout = dropout\n        self.n_layers = len(self.layers)\n\n        # Doubles its size because of concatenation of forw-backw encs\n        self.ctx_size = self.hidden_size * 2\n\n        # Fill 0-vector as <eos> to the end of the frames\n        self.pad_tuple = (0, 0, 0, 0, 0, 1)\n\n        # Projections and LSTMs\n        self.ffs = nn.ModuleList()\n        self.lstms = nn.ModuleList()\n\n        if self.dropout > 0:\n            self.do = nn.Dropout(self.dropout)\n\n        for i, ss_factor in enumerate(self.layers):\n            # Add LSTMs\n            self.lstms.append(nn.LSTM(\n                self.input_size if i == 0 else self.hidden_size,\n                self.hidden_size, bidirectional=True))\n            # Add non-linear bottlenecks\n            self.ffs.append(FF(\n                self.ctx_size, self.proj_size, activ=self.proj_activ))\n\n    def forward(self, x, **kwargs):\n        # Generate a mask to detect padded sequences\n        mask = x.ne(0).float().sum(2).ne(0).float()\n\n        if mask.eq(0).nonzero().numel() > 0:\n            logger.info(""WARNING: Non-homogeneous batch in BiLSTMp."")\n\n        # Pad with <eos> zero\n        hs = F.pad(x, self.pad_tuple)\n\n        for (ss_factor, f_lstm, f_ff) in zip(self.layers, self.lstms, self.ffs):\n            if ss_factor > 1:\n                # Skip states\n                hs = f_ff(f_lstm(hs[::ss_factor])[0])\n            else:\n                hs = f_ff(f_lstm(hs)[0])\n\n        if self.dropout > 0:\n            hs = self.do(hs)\n\n        # No mask is returned as batch should contain same-length sequences\n        return hs, None\n'"
nmtpytorch/layers/encoders/image.py,5,"b'# -*- coding: utf-8 -*-\nfrom collections import OrderedDict\n\nimport torch\nfrom torchvision import models\nfrom torchvision.models.vgg import cfgs as vgg_cfg\n\nfrom ...utils.misc import get_n_params\nfrom ..flatten import Flatten\n\n\ndef get_vgg_names(config, batch_norm=False):\n    names = []\n\n    # Counters for layer naming\n    n_block, n_conv = 1, 1\n\n    for v in config:\n        if v == \'M\':\n            names.append(\'pool%d\' % n_block)\n            n_block += 1\n            n_conv = 1\n        else:\n            conv_name = \'conv%d_%d\' % (n_block, n_conv)\n            names.append(conv_name)\n            if batch_norm:\n                names.append(\'%s+bn\' % conv_name)\n                names.append(\'%s+bn+relu\' % conv_name)\n            else:\n                names.append(\'%s+relu\' % conv_name)\n\n            n_conv += 1\n\n    return names\n\n\n# Mapping from torchvision\'s internal layer names to our naming scheme\nresnet_layers = {\n    \'conv1\': \'conv1\',\n    \'bn1\': \'bn1\',\n    \'relu\': \'relu\',\n    \'maxpool\': \'maxpool\',\n    \'layer1\': \'res2c_relu\',     # only differences are here\n    \'layer2\': \'res3d_relu\',     # only differences are here\n    \'layer3\': \'res4f_relu\',     # only differences are here\n    \'layer4\': \'res5c_relu\',     # only differences are here\n    \'avgpool\': \'avgpool\',\n    \'fc\': \'fc\',  # You\'ll never want to extract features from \'fc\'!\n}\n\n\nclass ImageEncoder:\n    CFG_MAP = {\n        # ResNet variants\n        \'resnet18\': resnet_layers,\n        \'resnet34\': resnet_layers,\n        \'resnet50\': resnet_layers,\n        \'resnet101\': resnet_layers,\n        \'resnet152\': resnet_layers,\n        # Plain VGGs\n        \'vgg11\': get_vgg_names(vgg_cfg[\'A\']),\n        \'vgg13\': get_vgg_names(vgg_cfg[\'B\']),\n        \'vgg16\': get_vgg_names(vgg_cfg[\'D\']),\n        \'vgg19\': get_vgg_names(vgg_cfg[\'E\']),\n        # Batchnorm VGGs\n        \'vgg11_bn\': get_vgg_names(vgg_cfg[\'A\'], batch_norm=True),\n        \'vgg13_bn\': get_vgg_names(vgg_cfg[\'B\'], batch_norm=True),\n        \'vgg16_bn\': get_vgg_names(vgg_cfg[\'D\'], batch_norm=True),\n        \'vgg19_bn\': get_vgg_names(vgg_cfg[\'E\'], batch_norm=True),\n    }\n\n    def __init__(self, cnn_type, pretrained=True):\n        self.pretrained = pretrained\n        self.cnn_type = cnn_type\n        self.cnn = None\n\n        assert self.cnn_type in self.CFG_MAP, \\\n            ""{} not supported by ImageEncoder"".format(self.cnn_type)\n\n        # Load vanilla CNN instance\n        self._base_cnn = getattr(models, self.cnn_type)(pretrained=pretrained)\n\n    def get_base_layers(self):\n        """"""Returns possible extraction points for the requested CNN.""""""\n        layers = self.CFG_MAP[self.cnn_type]\n        if isinstance(layers, list):\n            return layers\n        elif isinstance(layers, dict):\n            return list(layers.values())\n\n    def setup(self, layer, dropout=0., pool=None):\n        """"""Truncates the requested CNN until `layer`, `layer` included. The\n        final instance is stored under `self.cnn` and can be obtained with\n        the `.get()` method. The instance will have `requires_grad=False`\n        for all parameters by default. You can use `set_requires_grad()`\n        to selectively or completely enable `requires_grad` at layer-level.\n\n        If layer == \'penultimate\' and CNN type is VGG, whole CNN except\n        the last classification layer will be returned. In this case,\n        dropout and pool arguments are simply ignored.\n\n        Arguments:\n            layer(str): A layer name for VGG/ResNet. Possible truncation\n                points can be seen using the method `get_base_layers()`.\n            dropout(float, optional): Add an optional `Dropout` afterwards.\n                This will use `Dropout2d` if layer != \'avgpool\' (ResNet).\n            pool(tuple, optional): An optional tuple of\n                (\'Avg or Max\', kernel_size, stride) to append to the network.\n        """"""\n\n        layers = OrderedDict()\n        self.layer_map = self.CFG_MAP[self.cnn_type]\n\n        if self.cnn_type.startswith(\'vgg\'):\n            assert len(self._base_cnn.features) == len(self.layer_map)\n\n            # There\'s no named modules inside VGG, all integers\n            for module, params in zip(self.layer_map, self._base_cnn.features):\n                layers[module] = params\n                # \'penultimate\' takes all conv layers by default\n                if layer != \'penultimate\' and module == layer:\n                    break\n\n            if layer == \'penultimate\':\n                layers[\'flatten\'] = Flatten()\n                # Exclude final classification layer\n                for i in range(len(self._base_cnn.classifier) - 1):\n                    mod = self._base_cnn.classifier[i]\n                    name = ""{}{}"".format(mod.__class__.__name__, i)\n                    layers[name] = mod\n\n        elif self.cnn_type.startswith(\'resnet\'):\n            assert layer in self.layer_map.values(), \\\n                ""The given layer {} is not known."".format(layer)\n            for module, params in self._base_cnn.named_children():\n                # Add the layer with our naming scheme\n                layers[self.layer_map[module]] = params\n                # If we\'ve hit the extraction point, break the loop\n                if self.layer_map[module] == layer:\n                    break\n\n        if layer != \'penultimate\':\n            if pool is not None:\n                Pool = getattr(torch.nn, \'{}Pool2d\'.format(pool[0]))\n                layers[\'{}Pool\'.format(pool[0])] = Pool(\n                    kernel_size=pool[1], stride=pool[2])\n\n            if dropout > 0:\n                if layer == \'avgpool\':\n                    layers[\'dropout\'] = torch.nn.Dropout(p=dropout)\n                else:\n                    layers[\'dropout\'] = torch.nn.Dropout2d(p=dropout)\n\n        self.cnn = torch.nn.Sequential(layers)\n\n        # Disable requires_grad by default\n        if self.pretrained:\n            self.set_requires_grad(False)\n\n    def set_requires_grad(self, value=False, layers=\'all\'):\n        """"""Sets requires_grad for the given layer(s).\n\n        Arguments:\n            layers(str): A string or comma separated list of strings or\n                a range i.e. \'layer_from:layer_to\'\n                for which the requires_grad attribute will be set according\n                to `value`. If `all`, all layers will be affected.\n\n        Examples:\n            # Requires grad only for res4f_relu\n            set_requires_grad(val, \'res4f_relu\')\n            # Requires grad only for res4f_relu and res5c_relu\n            set_requires_grad(val, \'res4f_relu,res5c_relu\')\n            # Requires grad for all layers between [res2c_relu, res5c_relu]\n            set_requires_grad(val, \'res2c_relu:res5c_relu\')\n        """"""\n        assert self.cnn is not None, ""ImageEncoder.setup() is not called""\n        assert value in (True, False), ""value should be a boolean.""\n\n        if layers == \'all\':\n            for name, param in self.cnn.named_parameters():\n                param.requires_grad = value\n        else:\n            named_children = list(self.cnn.named_children())\n            in_range = None\n            if \':\' in layers:\n                layer_begin, layer_end = layers.split(\':\')\n                in_range = False\n                if not layer_begin:\n                    # from beginning upto layer_end\n                    layer_begin = named_children[0][0]\n                elif not layer_end:\n                    # from layer_begin upto end\n                    layer_end = named_children[-1][0]\n\n            for name, module in named_children:\n                if in_range is not None:\n                    # range given\n                    in_range = in_range or name == layer_begin\n                    if in_range:\n                        for param in module.parameters():\n                            param.requires_grad = value\n                        in_range = (name != layer_end)\n                else:\n                    # list of layer names given\n                    if name in layers.split(\',\'):\n                        for param in module.parameters():\n                            param.requires_grad = value\n\n    def get(self):\n        """"""Returns the configured CNN instance.""""""\n        return self.cnn\n\n    def get_output_shape(self):\n        """"""Returns [n,c,w,h] for the configured CNN\'s output.""""""\n\n        assert self.cnn is not None, \\\n            ""You need to first call ImageEncoder.setup()""\n\n        # Dummy test to detect output number filters\n        x = torch.zeros(1, 3, 224, 224, requires_grad=False)\n\n        # Returns (1, n_channel, w, h)\n        self.cnn.eval()\n        return list(self.cnn.forward(x).size())\n        self.cnn.train()\n\n    def __repr__(self):\n        s = ""{}(cnn_type={}, pretrained={})\\n"".format(\n            self.__class__.__name__, self.cnn_type, self.pretrained)\n        if self.cnn is not None:\n            for name, module in self.cnn.named_children():\n                s += "" - {}"".format(name)\n                params = list(module.parameters())\n                vals = set([p.requires_grad for p in params])\n                if len(vals) > 0:\n                    grad_str = vals.pop() if len(vals) == 1 else ""partial""\n                    s += ""(requires_grad={})"".format(grad_str)\n                s += ""\\n""\n            s += "" Output shape: {}\\n"".format(\n                \'x\'.join(map(str, self.get_output_shape()[1:])))\n            s += "" {}\\n"".format(get_n_params(self.cnn))\n        return s\n'"
nmtpytorch/layers/encoders/multimodal_bilstmp.py,3,"b'# -*- coding: utf-8 -*-\nimport logging\n\nimport torch\nfrom torch.nn import functional as F\n\nfrom ..ff import FF\n\nfrom . import BiLSTMp\n\nlogger = logging.getLogger(\'nmtpytorch\')\n\n\nclass MultimodalBiLSTMp(BiLSTMp):\n    """"""A bidirectional multimodal LSTM encoder for speech features.\n\n    Arguments:\n        feat_size (int): Auxiliary feature dimensionality.\n        feat_fusion(str): Type of feature fusion: \'early_concat\', \'early_sum\',\n            \'late_concat\', \'late_sum\', \'init\'.\n        feat_activ(str): Type of non-linearity if any for feature projection\n            layer.\n        input_size (int): Input speech feature dimensionality.\n        hidden_size (int): LSTM hidden state dimensionality.\n        proj_size (int): Projection layer size.\n        proj_activ (str, optional): Non-linearity to apply to intermediate projection\n            layers. (Default: \'tanh\')\n        layers (str): A \'_\' separated list of integers that defines the subsampling\n            factor for each LSTM.\n        dropout (float, optional): Use dropout (Default: 0.)\n    Input:\n        x (Tensor): A tensor of shape (n_timesteps, n_samples, n_feats)\n            that includes acoustic features of dimension ``n_feats`` per\n            each timestep (in the first dimension).\n\n    Output:\n        hs (Tensor): A tensor of shape (n_timesteps, n_samples, hidden * 2)\n            that contains encoder hidden states for all timesteps.\n        mask (Tensor): `None` since this layer expects all equal frame inputs.\n    """"""\n\n    def __init__(self, feat_size, feat_fusion, feat_activ=None, **kwargs):\n        # Call BiLSTMp.__init__ first\n        super().__init__(**kwargs)\n\n        self.feat_size = feat_size\n        self.feat_fusion = feat_fusion\n        self.feat_activ = feat_activ\n\n        # early_concat: x = layer([x; aux])\n        #  layer: feat_size + input_size -> input_size\n        if self.feat_fusion == \'early_concat\':\n            self.feat_layer = FF(\n                self.feat_size + self.input_size, self.input_size, activ=self.feat_activ)\n        # early_sum: x = x + layer(aux)\n        #  layer: feat_size -> input_size\n        elif self.feat_fusion == \'early_sum\':\n            self.feat_layer = FF(self.feat_size, self.input_size, activ=self.feat_activ)\n        # late_concat: hs = layer([hs; aux])\n        #  layer: proj_size + feat_size -> proj_size\n        elif self.feat_fusion == \'late_concat\':\n            self.feat_layer = FF(\n                self.feat_size + self.proj_size, self.proj_size, activ=self.feat_activ)\n        # late_sum: hs = hs + layer(aux)\n        #  layer: feat_size -> proj_size\n        elif self.feat_fusion == \'late_sum\':\n            self.feat_layer = FF(self.feat_size, self.proj_size, activ=self.feat_activ)\n        # init: Initialize all LSTMs\n        elif self.feat_fusion == \'init\':\n            # Use single h_0/c_0 for all stacked layers and directions for a\n            # consistent information source.\n            self.ff_init_c0 = FF(self.feat_size, self.hidden_size, activ=self.feat_activ)\n            self.ff_init_h0 = FF(self.feat_size, self.hidden_size, activ=self.feat_activ)\n\n    def forward(self, x, **kwargs):\n        # Generate a mask to detect padded sequences\n        mask = x.ne(0).float().sum(2).ne(0).float()\n\n        if mask.eq(0).nonzero().numel() > 0:\n            logger.info(""WARNING: Non-homogeneous batch in BiLSTMp."")\n\n        # Get auxiliary input\n        aux_x = kwargs[\'aux\']\n\n        ##############\n        # Encoder init\n        ##############\n        if self.feat_fusion == \'init\':\n            # Tile to 2xBxH for bidirectionality\n            c_0_ = self.ff_init_c0(aux_x).repeat(2, 1, 1)\n            h_0_ = self.ff_init_h0(aux_x).repeat(2, 1, 1)\n\n            # Should be a tuple of (h, c) for each layer\n            h_0s = [(h_0_, c_0_) for _ in range(self.n_layers)]\n        else:\n            # Dummy setup so that the below method calls are good\n            h_0s = [None for _ in range(self.n_layers)]\n            if self.feat_fusion == \'early_concat\':\n                x = self.feat_layer(\n                    torch.cat([x, aux_x.repeat(x.shape[0], 1, 1)], dim=-1))\n            elif self.feat_fusion == \'early_sum\':\n                x.add_(self.feat_layer(aux_x).unsqueeze(0))\n\n        # Pad with <eos> zero\n        hs = F.pad(x, self.pad_tuple)\n\n        ###################\n        # LSTM + Proj block\n        ###################\n        for (ss_factor, f_lstm, f_ff, h_0) in zip(self.layers, self.lstms, self.ffs, h_0s):\n            if ss_factor > 1:\n                # Skip states\n                hs = f_ff(f_lstm(hs[::ss_factor], hx=h_0)[0])\n            else:\n                hs = f_ff(f_lstm(hs, hx=h_0)[0])\n\n        #############\n        # Late Fusion\n        #############\n        if self.feat_fusion == \'late_concat\':\n            hs = self.feat_layer(\n                torch.cat([hs, aux_x.repeat(hs.shape[0], 1, 1)], dim=-1))\n        elif self.feat_fusion == \'late_sum\':\n            hs = hs + self.feat_layer(aux_x).unsqueeze(0)\n\n        if self.dropout > 0:\n            hs = self.do(hs)\n\n        # No mask is returned as batch should contain same-length sequences\n        return hs, None\n'"
nmtpytorch/layers/encoders/multimodal_text.py,4,"b'# -*- coding: utf-8 -*-\nimport torch\n\nfrom . import TextEncoder\nfrom .. import FF\n\n\nclass MultimodalTextEncoder(TextEncoder):\n    """"""A multimodal recurrent encoder with embedding layer.\n\n    Arguments:\n        feat_size (int): Auxiliary feature dimensionality.\n        feat_fusion(str): Type of feature fusion: \'early_concat\', \'early_sum\',\n            \'late_concat\', \'late_sum\', \'init\'.\n        feat_activ(str): Type of non-linearity if any for feature projection\n            layer.\n        input_size (int): Embedding dimensionality.\n        hidden_size (int): RNN hidden state dimensionality.\n        n_vocab (int): Number of tokens for the embedding layer.\n        rnn_type (str): RNN Type, i.e. GRU or LSTM.\n        num_layers (int, optional): Number of stacked RNNs (Default: 1).\n        bidirectional (bool, optional): If `False`, the RNN is unidirectional.\n        dropout_rnn (float, optional): Inter-layer dropout rate only\n            applicable if `num_layers > 1`. (Default: 0.)\n        dropout_emb(float, optional): Dropout rate for embeddings (Default: 0.)\n        dropout_ctx(float, optional): Dropout rate for the\n            encodings/annotations (Default: 0.)\n        emb_maxnorm(float, optional): If given, renormalizes embeddings so\n            that their norm is the given value.\n        emb_gradscale(bool, optional): If `True`, scales the gradients\n            per embedding w.r.t. to its frequency in the batch.\n        proj_dim(int, optional): If not `None`, add a final projection\n            layer. Can be used to adapt dimensionality for decoder.\n        proj_activ(str, optional): Non-linearity for projection layer.\n            `None` or `linear` does not apply any non-linearity.\n        layer_norm(bool, optional): Apply layer normalization at the\n            output of the encoder.\n\n    Input:\n        x (Tensor): A tensor of shape (n_timesteps, n_samples)\n            including the integer token indices for the given batch.\n        v (Tensor): A tensor of shape (...) representing a fixed-size\n            visual vector for the batch.\n\n    Output:\n        hs (Tensor): A tensor of shape (n_timesteps, n_samples, hidden)\n            that contains encoder hidden states for all timesteps. If\n            bidirectional, `hs` is doubled in size in the last dimension\n            to contain both directional states.\n        mask (Tensor): A binary mask of shape (n_timesteps, n_samples)\n            that may further be used in attention and/or decoder. `None`\n            is returned if batch contains only sentences with same lengths.\n    """"""\n    def __init__(self, feat_size, feat_fusion, feat_activ=None, **kwargs):\n        super().__init__(**kwargs)\n        self.feat_size = feat_size\n        self.feat_fusion = feat_fusion\n        self.feat_activ = feat_activ\n\n        # LSTM requires the initialization of both c_0 and h_0\n        # FIXME: Not tested at all with LSTMs, probably won\'t work!\n        self.n_init_types = 2 if self.rnn_type == \'LSTM\' else 1\n\n        ##################################################\n        # Create the necessary visual transformation layer\n        ##################################################\n        self.plain = self.feat_fusion is None or self.feat_fusion.startswith(\'trg\')\n        self.init_enc = self.feat_fusion in (\'encinit\', \'encdecinit\')\n        # No-op by default\n        self.merge_op = lambda e, *v: e\n\n        if self.init_enc:\n            self.tile_factor = self.num_layers\n            if self.bidirectional:\n                self.tile_factor *= 2\n            out_dim = self.hidden_size * self.n_init_types\n            inp_dim = self.feat_size\n        elif self.feat_fusion in (\'concat\', \'sum\', \'prepend\', \'append\', \'srcmul\', \'ctxmul\'):\n            out_dim = self.input_size\n            inp_dim = self.feat_size\n            if self.feat_fusion == \'concat\':\n                inp_dim += self.input_size\n                self.merge_op = lambda e, v: self.ff_vis(torch.cat(\n                    (e, v.expand(e.shape[0], -1, -1)), dim=-1))\n            elif self.feat_fusion == \'sum\':\n                self.merge_op = lambda e, v: e + self.ff_vis(v)\n            elif self.feat_fusion == \'srcmul\':\n                self.merge_op = lambda e, v: e * self.ff_vis(v)\n            elif self.feat_fusion == \'prepend\':\n                self.merge_op = lambda e, v: torch.cat((self.ff_vis(v), e), dim=0)\n            elif self.feat_fusion == \'append\':\n                # NOTE: note that it will append after <eos>\n                self.merge_op = lambda e, v: torch.cat((e, self.ff_vis(v)), dim=0)\n            elif self.feat_fusion == \'ctxmul\':\n                out_dim = self.hidden_size\n                if self.bidirectional:\n                    out_dim *= 2\n\n        if not self.plain:\n            self.ff_vis = FF(inp_dim, out_dim, activ=self.feat_activ)\n\n    def forward(self, x, v, **kwargs):\n        h0 = None\n        if self.init_enc:\n            h0 = self.ff_vis(v).expand(self.tile_factor, -1, -1).contiguous()\n\n        # Compute mask for possible paddings\n        zero_pos = (x == 0)\n        mask = (~zero_pos).long() if zero_pos.nonzero().numel() else None\n\n        # Fetch embeddings\n        embs = self.emb(x)\n\n        # Fuse\n        embs = self.merge_op(embs, v)\n\n        if mask is not None and embs.shape[0] != mask.shape[0]:\n            # prepend, append will cause this, enlarge the mask\n            mask = torch.cat((mask[0].unsqueeze(0), mask), dim=0)\n\n        # Apply dropout\n        if self.dropout_emb > 0:\n            embs = self.do_emb(embs)\n\n        # Encode\n        hs, _ = self.enc(embs, h0)\n        if self.feat_fusion == \'ctxmul\':\n            hs = hs * self.ff_vis(v)\n\n        # Return\n        return self.output(hs), mask\n'"
nmtpytorch/layers/encoders/text.py,1,"b'# -*- coding: utf-8 -*-\nfrom torch import nn\nfrom torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n\nfrom ...utils.data import sort_batch\nfrom .. import FF\nfrom .. import LayerNorm\n\n\nclass TextEncoder(nn.Module):\n    """"""A recurrent encoder with embedding layer.\n\n    Arguments:\n        input_size (int): Embedding dimensionality.\n        hidden_size (int): RNN hidden state dimensionality.\n        n_vocab (int): Number of tokens for the embedding layer.\n        rnn_type (str): RNN Type, i.e. GRU or LSTM.\n        num_layers (int, optional): Number of stacked RNNs (Default: 1).\n        bidirectional (bool, optional): If `False`, the RNN is unidirectional.\n        dropout_rnn (float, optional): Inter-layer dropout rate only\n            applicable if `num_layers > 1`. (Default: 0.)\n        dropout_emb(float, optional): Dropout rate for embeddings (Default: 0.)\n        dropout_ctx(float, optional): Dropout rate for the\n            encodings/annotations (Default: 0.)\n        emb_maxnorm(float, optional): If given, renormalizes embeddings so\n            that their norm is the given value.\n        emb_gradscale(bool, optional): If `True`, scales the gradients\n            per embedding w.r.t. to its frequency in the batch.\n        proj_dim(int, optional): If not `None`, add a final projection\n            layer. Can be used to adapt dimensionality for decoder.\n        proj_activ(str, optional): Non-linearity for projection layer.\n            `None` or `linear` does not apply any non-linearity.\n        layer_norm(bool, optional): Apply layer normalization at the\n            output of the encoder.\n\n    Input:\n        x (Tensor): A tensor of shape (n_timesteps, n_samples)\n            including the integer token indices for the given batch.\n\n    Output:\n        hs (Tensor): A tensor of shape (n_timesteps, n_samples, hidden)\n            that contains encoder hidden states for all timesteps. If\n            bidirectional, `hs` is doubled in size in the last dimension\n            to contain both directional states.\n        mask (Tensor): A binary mask of shape (n_timesteps, n_samples)\n            that may further be used in attention and/or decoder. `None`\n            is returned if batch contains only sentences with same lengths.\n    """"""\n    def __init__(self, input_size, hidden_size, n_vocab, rnn_type,\n                 num_layers=1, bidirectional=True,\n                 dropout_rnn=0, dropout_emb=0, dropout_ctx=0,\n                 emb_maxnorm=None, emb_gradscale=False,\n                 proj_dim=None, proj_activ=None, layer_norm=False):\n        super().__init__()\n\n        self.rnn_type = rnn_type.upper()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.n_vocab = n_vocab\n        self.num_layers = num_layers\n        self.bidirectional = bidirectional\n        self.emb_maxnorm = emb_maxnorm\n        self.emb_gradscale = emb_gradscale\n        self.proj_dim = proj_dim\n        self.proj_activ = proj_activ\n        self.layer_norm = layer_norm\n\n        # For dropout btw layers, only effective if num_layers > 1\n        self.dropout_rnn = dropout_rnn\n\n        # Our other custom dropouts after embeddings and annotations\n        self.dropout_emb = dropout_emb\n        self.dropout_ctx = dropout_ctx\n\n        self.ctx_size = self.hidden_size\n        # Doubles its size because of concatenation\n        if self.bidirectional:\n            self.ctx_size *= 2\n\n        # Embedding dropout\n        self.do_emb = nn.Dropout(self.dropout_emb)\n\n        # Create embedding layer\n        self.emb = nn.Embedding(self.n_vocab, self.input_size,\n                                padding_idx=0, max_norm=self.emb_maxnorm,\n                                scale_grad_by_freq=self.emb_gradscale)\n\n        # Create fused/cudnn encoder according to the requested type\n        RNN = getattr(nn, self.rnn_type)\n        self.enc = RNN(self.input_size, self.hidden_size,\n                       self.num_layers, bias=True, batch_first=False,\n                       dropout=self.dropout_rnn,\n                       bidirectional=self.bidirectional)\n\n        output_layers = []\n        if self.proj_dim:\n            output_layers.append(\n                FF(self.ctx_size, self.proj_dim, activ=self.proj_activ))\n            self.ctx_size = self.proj_dim\n        if self.layer_norm:\n            output_layers.append(LayerNorm(self.ctx_size))\n        if self.dropout_ctx > 0:\n            output_layers.append(nn.Dropout(p=self.dropout_ctx))\n        self.output = nn.Sequential(*output_layers)\n\n    def forward(self, x, **kwargs):\n        # Non-homogeneous batches possible\n        # sort the batch by decreasing length of sequences\n        # oidxs: to recover original order\n        # sidxs: idxs to sort the batch\n        # slens: lengths in sorted order for pack_padded_sequence()\n        oidxs, sidxs, slens, mask = sort_batch(x)\n\n        # Fetch embeddings for the sorted batch\n        embs = self.do_emb(self.emb(x[:, sidxs]))\n\n        # Pack and encode\n        packed_emb = pack_padded_sequence(embs, slens)\n\n        # We ignore last_state since we don\'t use it\n        packed_hs, _ = self.enc(packed_emb)\n\n        # Get hidden states and revert the order\n        hs = pad_packed_sequence(packed_hs)[0][:, oidxs]\n\n        return self.output(hs), mask\n'"
nmtpytorch/layers/transformers/__init__.py,0,b'from .residual_lnorm import ResidualLayerNorm\nfrom .positionwise_ff import PositionwiseFF\nfrom .embedding import TFEmbedding\nfrom .encoder import TFEncoder\nfrom .decoder import TFDecoder\n'
nmtpytorch/layers/transformers/decoder.py,3,"b'import torch\n\nfrom ..attention import ScaledDotAttention\nfrom . import ResidualLayerNorm, PositionwiseFF\n\n\nclass TFDecoder(torch.nn.Module):\n    """"""Decoder block for Transformer.\n\n    Arguments:\n\n    Input:\n\n    Output:\n    """"""\n\n    def __init__(self, model_dim, ff_dim, n_heads, n_layers):\n        super().__init__()\n        self.model_dim = model_dim\n        self.ff_dim = ff_dim\n        self.n_heads = n_heads\n        self.n_layers = n_layers\n        blocks = []\n\n        for _ in range(self.n_layers):\n            layers = torch.nn.Sequential(\n                ScaledDotAttention(self.model_dim, self.n_heads, causal=True),\n                ResidualLayerNorm(self.model_dim),\n                PositionwiseFF(self.model_dim, self.ff_dim),\n                ResidualLayerNorm(self.model_dim),\n            )\n            blocks.append(layers)\n\n        self.blocks = torch.nn.ModuleList(blocks)\n\n    def forward(self, x, mask=None, **kwargs):\n        """"""Forward-pass of the encoder block.\n\n        :param x: input tensor, shape (tstep, bsize, model_dim)\n        :param mask: mask tensor for unavailable batch positions (tstep, bsize)\n\n        :return: foo\n        """"""\n        for block in self.blocks:\n            x, mask = block((x, x, x, mask))\n        return (x, mask)\n'"
nmtpytorch/layers/transformers/embedding.py,9,"b'import torch\n\n\nclass TFEmbedding(torch.nn.Embedding):\n    """"""Position-aware embeddings for Transformer models.\n\n    Adapted from OpenNMT-py & original `Attention is all you need` paper.\n    """"""\n    def __init__(self, num_embeddings, embedding_dim, max_len=1024, dropout=0.1):\n        self.num_embeddings = num_embeddings\n        self.embedding_dim = embedding_dim\n        self.max_len = max_len\n        self.dropout = dropout\n\n        # pos_embs: (max_len, emb_dim)\n        pos_embs = torch.zeros(self.max_len, self.embedding_dim)\n        # pos: (max_len, 1)\n        pos = torch.arange(self.max_len).unsqueeze(1)\n        # divs:\n        divs = torch.pow(10000,\n                torch.arange(self.embedding_dim).float().div(self.embedding_dim))\n\n        pos_embs[:, 0::2] = torch.sin(pos / divs[0::2])\n        pos_embs[:, 1::2] = torch.cos(pos / divs[1::2])\n        # pos_embs: (max_len, 1, emb_dim)\n        pos_embs.unsqueeze_(1)\n        sqrt_dim = torch.scalar_tensor(self.embedding_dim).sqrt()\n\n        # Call parent\'s init() first\n        super().__init__(num_embeddings, embedding_dim, padding_idx=0)\n\n        # Register non-learnable params as buffers\n        self.register_buffer(\'pos_embs\', pos_embs)\n        self.register_buffer(\'sqrt_dim\', sqrt_dim)\n        # Create dropout layer\n        self.dropout_layer = torch.nn.Dropout(p=self.dropout)\n\n    def forward(self, x):\n        # Get the embeddings from parent\'s forward first\n        embs = super().forward(x)\n        return self.dropout_layer(\n            embs.mul(self.sqrt_dim) + self.pos_embs[:embs.size(0)])\n'"
nmtpytorch/layers/transformers/encoder.py,3,"b'import torch\n\nfrom ..attention import ScaledDotAttention\nfrom . import ResidualLayerNorm, PositionwiseFF\n\n\nclass TFEncoder(torch.nn.Module):\n    """"""Encoder block for Transformer.\n\n    Arguments:\n\n    Input:\n\n    Output:\n    """"""\n\n    def __init__(self, model_dim, ff_dim, n_heads, n_layers):\n        super().__init__()\n        self.model_dim = model_dim\n        self.ff_dim = ff_dim\n        self.n_heads = n_heads\n        self.n_layers = n_layers\n        blocks = []\n\n        for _ in range(self.n_layers):\n            layers = torch.nn.Sequential(\n                ScaledDotAttention(self.model_dim, self.n_heads),\n                ResidualLayerNorm(self.model_dim),\n                PositionwiseFF(self.model_dim, self.ff_dim),\n                ResidualLayerNorm(self.model_dim),\n            )\n            blocks.append(layers)\n\n        self.blocks = torch.nn.ModuleList(blocks)\n\n    def forward(self, x, mask=None, **kwargs):\n        """"""Forward-pass of the encoder block.\n\n        :param x: input tensor, shape (tstep, bsize, model_dim)\n        :param mask: mask tensor for unavailable batch positions (tstep, bsize)\n\n        :return: foo\n        """"""\n        for block in self.blocks:\n            x, mask = block((x, x, x, mask))\n        return (x, mask)\n'"
nmtpytorch/layers/transformers/positionwise_ff.py,2,"b'import torch\n\nfrom .. import FF\n\n\nclass PositionwiseFF(torch.nn.Module):\n    """"""Positionwise Feed-forward layer.\n\n    Arguments:\n\n    Input:\n\n    Output:\n    """"""\n\n    def __init__(self, model_dim, ff_dim, activ=\'relu\'):\n        super().__init__()\n        self.model_dim = model_dim\n        self.ff_dim = ff_dim\n        self.activ = activ\n\n        # Create the layers\n        self.func = torch.nn.Sequential(\n            FF(self.model_dim, self.ff_dim, activ=self.activ),\n            FF(self.ff_dim, self.model_dim, activ=None),\n        )\n\n    def forward(self, inputs):\n        x, mask = inputs\n        return (x, self.func(x), mask)\n'"
nmtpytorch/layers/transformers/residual_lnorm.py,2,"b'import torch\n\nfrom .. import LayerNorm\n\n\nclass ResidualLayerNorm(torch.nn.Module):\n    """"""Residually connected Layer Normalization layer.\n\n    Arguments:\n\n    Input:\n\n    Output:\n    """"""\n\n    def __init__(self, model_dim, affine=True, dropout=0.1):\n        super().__init__()\n        self.model_dim = model_dim\n        self.affine = affine\n        self.dropout = dropout\n\n        self.norm = LayerNorm(self.model_dim, elementwise_affine=self.affine)\n        self.dropout_layer = torch.nn.Dropout(self.dropout)\n\n    def forward(self, inputs):\n        # Unpack into `x` and `Sublayer(x)`\n        x, f_x, mask = inputs\n        return (self.norm(x + self.dropout_layer(f_x)), mask)\n'"
nmtpytorch/models/stale/acapt.py,2,"b'# -*- coding: utf-8 -*-\nimport logging\n\nimport torch\nfrom torch import nn\n\nfrom ..layers import ConditionalDecoder\nfrom ..utils.misc import get_n_params\nfrom ..vocabulary import Vocabulary\nfrom ..utils.topology import Topology\nfrom ..utils.ml_metrics import Loss\nfrom ..utils.device import DEVICE\nfrom ..utils.misc import pbar\nfrom ..datasets import MultimodalDataset\nfrom ..metrics import Metric\n\nfrom . import NMT\n\nlogger = logging.getLogger(\'nmtpytorch\')\n\n\nclass AttentiveCaptioning(NMT):\n    """"""A simple attentive captioning model based on the NMT model. This is\n    not a direct reimplementation of Show-attend-and-tell.""""""\n    supports_beam_search = True\n\n    def set_defaults(self):\n        self.defaults = {\n            \'emb_dim\': 128,             # Source and target embedding sizes\n            \'emb_maxnorm\': None,        # Normalize embeddings l2 norm to 1\n            \'emb_gradscale\': False,     # Scale embedding gradients w.r.t. batch frequency\n            \'dec_dim\': 256,             # Decoder hidden size\n            \'dec_type\': \'gru\',          # Decoder type (gru|lstm)\n            \'dec_init\': \'mean_ctx\',     # How to initialize decoder (zero/mean_ctx/feats)\n            \'dec_init_size\': None,      # feature vector dimensionality for\n            \'dec_init_activ\': \'tanh\',   # Decoder initialization activation func\n                                        # dec_init == \'feats\'\n            \'att_type\': \'mlp\',          # Attention type (mlp|dot)\n            \'att_temp\': 1.,             # Attention temperature\n            \'att_activ\': \'tanh\',        # Attention non-linearity (all torch nonlins)\n            \'att_mlp_bias\': False,      # Enables bias in attention mechanism\n            \'att_bottleneck\': \'ctx\',    # Bottleneck dimensionality (ctx|hid)\n            \'att_transform_ctx\': True,  # Transform annotations before attention\n            \'dropout_ctx\': 0,           # Simple dropout to source encodings\n            \'dropout_out\': 0,           # Simple dropout to decoder output\n            \'tied_emb\': False,          # Share embeddings: (False|2way|3way)\n            \'direction\': None,          # Network directionality, i.e. en->de\n            \'max_len\': 80,              # Reject sentences where \'bucket_by\' length > 80\n            \'bucket_by\': None,          # A key like \'en\' to define w.r.t which dataset\n                                        # the batches will be sorted\n            \'bucket_order\': None,       # Curriculum: ascending/descending/None\n            \'sampler_type\': \'bucket\',   # bucket or approximate\n            \'sched_sampling\': 0,        # Scheduled sampling ratio\n            \'bos_type\': \'emb\',          # \'emb\': default learned emb\n            \'bos_activ\': None,          #\n            \'bos_dim\': None,            #\n            \'n_channels\': 2048,         # Feature-size of conv maps\n        }\n\n    def __init__(self, opts):\n        # Don\'t call NMT init as it\'s too different from this model\n        nn.Module.__init__(self)\n\n        # opts -> config file sections {.model, .data, .vocabulary, .train}\n        self.opts = opts\n\n        # Vocabulary objects\n        self.vocabs = {}\n\n        # Each auxiliary loss should be stored inside this dictionary\n        # in order to be taken into account by the mainloop for multi-tasking\n        self.aux_loss = {}\n\n        # Setup options\n        self.opts.model = self.set_model_options(opts.model)\n\n        # Parse topology & languages\n        self.topology = Topology(self.opts.model[\'direction\'])\n\n        # Load vocabularies here\n        for name, fname in self.opts.vocabulary.items():\n            self.vocabs[name] = Vocabulary(fname)\n\n        # Inherently non multi-lingual aware\n        tlangs = self.topology.get_trg_langs()\n        self.tl = tlangs[0]\n        self.trg_vocab = self.vocabs[self.tl]\n        self.n_trg_vocab = len(self.trg_vocab)\n        # Need to be set for early-stop evaluation\n        # NOTE: This should come from config or elsewhere\n        self.val_refs = self.opts.data[\'val_set\'][self.tl]\n\n        # Check vocabulary sizes for 3way tying\n        if self.opts.model[\'tied_emb\'] not in [False, \'2way\']:\n            raise RuntimeError(\n                ""\'{}\' not recognized for tied_emb."".format(self.opts.model[\'tied_emb\']))\n\n    def __repr__(self):\n        s = super().__repr__() + \'\\n\'\n        for vocab in self.vocabs.values():\n            s += ""{}\\n"".format(vocab)\n        s += ""{}\\n"".format(get_n_params(self))\n        return s\n\n    def set_model_options(self, model_opts):\n        self.set_defaults()\n        for opt, value in model_opts.items():\n            if opt in self.defaults:\n                # Override defaults from config\n                self.defaults[opt] = value\n            else:\n                logger.info(\'Warning: unused model option: {}\'.format(opt))\n        return self.defaults\n\n    def reset_parameters(self):\n        for name, param in self.named_parameters():\n            # Skip 1-d biases and scalars\n            if param.requires_grad and param.dim() > 1:\n                nn.init.kaiming_normal_(param.data)\n\n    def setup(self, is_train=True):\n        ################\n        # Create Decoder\n        ################\n        self.dec = ConditionalDecoder(\n            input_size=self.opts.model[\'emb_dim\'],\n            hidden_size=self.opts.model[\'dec_dim\'],\n            n_vocab=self.n_trg_vocab,\n            rnn_type=self.opts.model[\'dec_type\'],\n            ctx_size_dict={\'image\': self.opts.model[\'n_channels\']},\n            ctx_name=\'image\',\n            tied_emb=self.opts.model[\'tied_emb\'],\n            dec_init=self.opts.model[\'dec_init\'],\n            dec_init_size=self.opts.model[\'dec_init_size\'],\n            dec_init_activ=self.opts.model[\'dec_init_activ\'],\n            att_type=self.opts.model[\'att_type\'],\n            att_temp=self.opts.model[\'att_temp\'],\n            att_activ=self.opts.model[\'att_activ\'],\n            transform_ctx=self.opts.model[\'att_transform_ctx\'],\n            mlp_bias=self.opts.model[\'att_mlp_bias\'],\n            att_bottleneck=self.opts.model[\'att_bottleneck\'],\n            dropout_out=self.opts.model[\'dropout_out\'],\n            emb_maxnorm=self.opts.model[\'emb_maxnorm\'],\n            emb_gradscale=self.opts.model[\'emb_gradscale\'],\n            sched_sample=self.opts.model[\'sched_sampling\'],\n            bos_type=self.opts.model[\'bos_type\'],\n            bos_dim=self.opts.model[\'bos_dim\'],\n            bos_activ=self.opts.model[\'bos_activ\'])\n\n        self.do_ctx = nn.Dropout(p=self.opts.model[\'dropout_ctx\'])\n\n    def load_data(self, split, batch_size, mode=\'train\'):\n        """"""Loads the requested dataset split.""""""\n        dataset = MultimodalDataset(\n            data=self.opts.data[\'{}_set\'.format(split)],\n            mode=mode, batch_size=batch_size,\n            vocabs=self.vocabs, topology=self.topology,\n            bucket_by=self.opts.model[\'bucket_by\'],\n            max_len=self.opts.model[\'max_len\'],\n            bucket_order=self.opts.model[\'bucket_order\'],\n            sampler_type=self.opts.model[\'sampler_type\'])\n        logger.info(dataset)\n        return dataset\n\n    def get_bos(self, batch_size):\n        """"""Returns a representation for <bos> embeddings for decoding.""""""\n        return torch.LongTensor(batch_size).fill_(self.trg_vocab[\'<bos>\'])\n\n    def encode(self, batch, **kwargs):\n        """"""Encodes all inputs and returns a dictionary.\n\n        Arguments:\n            batch (dict): A batch of samples with keys designating the\n                information sources.\n\n        Returns:\n            dict:\n                A dictionary where keys are source modalities compatible\n                with the data loader and the values are tuples where the\n                elements are encodings and masks. The mask can be ``None``\n                if the relevant modality does not require a mask.\n        """"""\n        d = {\'image\': (self.do_ctx(batch[\'image\']), None)}\n        return d\n\n    def forward(self, batch, **kwargs):\n        """"""Computes the forward-pass of the network and returns batch loss.\n\n        Arguments:\n            batch (dict): A batch of samples with keys designating the source\n                and target modalities.\n\n        Returns:\n            Tensor:\n                A scalar loss normalized w.r.t batch size and token counts.\n        """"""\n        # Get loss dict\n        result = self.dec(self.encode(batch), batch[self.tl])\n        result[\'n_items\'] = torch.nonzero(batch[self.tl][1:]).shape[0]\n        return result\n\n    def test_performance(self, data_loader, dump_file=None):\n        """"""Computes test set loss over the given DataLoader instance.""""""\n        loss = Loss()\n\n        for batch in pbar(data_loader, unit=\'batch\'):\n            batch.device(DEVICE)\n            out = self.forward(batch)\n            loss.update(out[\'loss\'], out[\'n_items\'])\n\n        return [\n            Metric(\'LOSS\', loss.get(), higher_better=False),\n        ]\n\n    def get_decoder(self, task_id=None):\n        """"""Compatibility function for multi-tasking architectures.""""""\n        return self.dec\n'"
nmtpytorch/models/stale/nli.py,1,"b'# -*- coding: utf-8 -*-\nimport logging\n\nimport torch\nfrom torch import nn\n\nfrom ..layers import FF\nfrom ..utils.nn import get_partial_embedding_layer\nfrom ..utils.misc import get_n_params\nfrom ..vocabulary import Vocabulary\nfrom ..utils.topology import Topology\nfrom ..utils.ml_metrics import Loss\nfrom ..utils.device import DEVICE\nfrom ..utils.misc import pbar\nfrom ..datasets import MultimodalDataset\nfrom ..metrics import Metric\n\nfrom . import NMT\n\nlogger = logging.getLogger(\'nmtpytorch\')\n\n\nclass NLI(NMT):\n    """"""A very simple BiLSTM NLI baseline.""""""\n    supports_beam_search = False\n\n    def set_defaults(self):\n        self.defaults = {\n            \'bidirectional\': True,      # Bi-directional LSTM\n            \'emb_dim\': 300,             # Input embedding size\n            \'inp_dim\': 300,             # Projected embedding size\n            \'enc_dim\': 300,             # Encoder hidden size\n            \'proj_dim\': 600,            # Output proj dims\n            \'nonlin\': \'tanh\',           # Non-linearity type\n            \'emb_zero_oov\': False,      # Init to zero the OOV words\n            \'n_encoders\': 1,            # Number of stacked encoders\n            \'dropout\': 0,               # Global dropout value\n            \'direction\': None,          # Network directionality, i.e. en->de\n            \'max_len\': None,            # Reject sentences where \'bucket_by\' length > 80\n            \'bucket_by\': None,          # A key like \'en\' to define w.r.t which dataset\n                                        # the batches will be sorted\n            \'bucket_order\': None,       # Curriculum: ascending/descending/None\n            \'sampler_type\': \'bucket\',   # bucket or approximate\n            \'init_emb\': None,           # Pretrained .pkl file which is a dict\n            \'init_emb_freeze\': \'none\',  # none/all/partial\n        }\n\n    def __init__(self, opts):\n        # Don\'t call NMT init as it\'s too different from this model\n        nn.Module.__init__(self)\n\n        # opts -> config file sections {.model, .data, .vocabulary, .train}\n        self.opts = opts\n\n        # Vocabulary objects\n        self.vocabs = {}\n\n        # Each auxiliary loss should be stored inside this dictionary\n        # in order to be taken into account by the mainloop for multi-tasking\n        self.aux_loss = {}\n\n        # Setup options\n        self.opts.model = self.set_model_options(opts.model)\n\n        # Parse topology & languages\n        self.topology = Topology(self.opts.model[\'direction\'])\n\n        # Load vocabularies here\n        for name, fname in self.opts.vocabulary.items():\n            self.vocabs[name] = Vocabulary(fname)\n\n    def __repr__(self):\n        s = super().__repr__() + \'\\n\'\n        for vocab in self.vocabs.values():\n            s += ""{}\\n"".format(vocab)\n        s += ""{}\\n"".format(get_n_params(self))\n        return s\n\n    def set_model_options(self, model_opts):\n        self.set_defaults()\n        for opt, value in model_opts.items():\n            if opt in self.defaults:\n                # Override defaults from config\n                self.defaults[opt] = value\n            else:\n                logger.info(\'Warning: unused model option: {}\'.format(opt))\n        return self.defaults\n\n    def reset_parameters(self):\n        return\n        for name, param in self.named_parameters():\n            if name == \'emb.weight\':\n                # NOTE: Do not reinit embeddings for now\n                continue\n            # Skip 1-d biases and scalars\n            if param.requires_grad and param.dim() > 1:\n                nn.init.kaiming_normal_(param.data)\n\n    def setup(self, is_train=True):\n        """"""Sets up NN topology by creating the layers.""""""\n        ########################\n        # Create Textual Encoder\n        ########################\n        self.lstm = nn.LSTM(\n            input_size=self.opts.model[\'inp_dim\'],\n            hidden_size=self.opts.model[\'enc_dim\'],\n            num_layers=self.opts.model[\'n_encoders\'],\n            bidirectional=self.opts.model[\'bidirectional\'])\n\n        if self.opts.model[\'init_emb\']:\n            # Use pretrained embeddings\n            # NOTE: Fix grad_mask thing\n            self.emb = get_partial_embedding_layer(\n                self.vocabs[\'pre\'], self.opts.model[\'emb_dim\'],\n                pretrained_file=self.opts.model[\'init_emb\'],\n                freeze=self.opts.model[\'init_emb_freeze\'],\n                oov_zero=self.opts.model[\'emb_zero_oov\'])\n        else:\n            # Train embedding from scratch\n            self.emb = nn.Embedding(len(self.vocabs[\'pre\']),\n                                    self.opts.model[\'emb_dim\'],\n                                    padding_idx=0)\n\n        self.emb_proj = FF(\n            self.opts.model[\'emb_dim\'], self.opts.model[\'inp_dim\'],\n            activ=self.opts.model[\'nonlin\'])\n\n        inp_dim = self.opts.model[\'enc_dim\'] * 2\n        proj_dim = self.opts.model[\'proj_dim\']\n        if self.opts.model[\'bidirectional\']:\n            inp_dim *= 2\n\n        self.output = nn.Sequential(\n            FF(inp_dim, proj_dim, activ=self.opts.model[\'nonlin\']),\n            nn.Dropout(self.opts.model[\'dropout\']),\n            FF(proj_dim, proj_dim, activ=self.opts.model[\'nonlin\']),\n            nn.Dropout(self.opts.model[\'dropout\']),\n            FF(proj_dim, proj_dim // 2, activ=self.opts.model[\'nonlin\']),\n            nn.Dropout(self.opts.model[\'dropout\']),\n            FF(proj_dim // 2, self.vocabs[\'lb\'].n_tokens, bias=False),\n            nn.LogSoftmax(dim=-1))\n        self.loss = nn.NLLLoss(reduction=\'sum\')\n\n    def load_data(self, split, batch_size, mode=\'train\'):\n        """"""Loads the requested dataset split.""""""\n        dataset = MultimodalDataset(\n            data=self.opts.data[\'{}_set\'.format(split)],\n            mode=mode, batch_size=batch_size,\n            vocabs=self.vocabs, topology=self.topology,\n            bucket_by=self.opts.model[\'bucket_by\'],\n            max_len=self.opts.model[\'max_len\'],\n            bucket_order=self.opts.model[\'bucket_order\'],\n            sampler_type=self.opts.model[\'sampler_type\'])\n        logger.info(dataset)\n        return dataset\n\n    def encode(self, batch, **kwargs):\n        pre, _ = self.lstm(self.emb_proj(self.emb(batch[\'pre\'])))\n        hyp, _ = self.lstm(self.emb_proj(self.emb(batch[\'hyp\'])))\n\n        pre_mask = (batch[\'pre\'] > 0).long().sum(0).sub(1)\n        hyp_mask = (batch[\'hyp\'] > 0).long().sum(0).sub(1)\n        last_pre = pre[pre_mask, range(pre.shape[1])]\n        last_hyp = hyp[hyp_mask, range(hyp.shape[1])]\n\n        return torch.cat((last_pre, last_hyp), dim=-1)\n\n    def forward(self, batch, **kwargs):\n        probs = self.output(self.encode(batch))\n        loss = self.loss(probs, batch[\'lb\'].squeeze(0))\n        d = {\n            \'loss\': loss,\n            \'probs\': probs,\n            \'n_items\': batch.size,\n        }\n        return d\n\n    def test_performance(self, data_loader, dump_file=None):\n        """"""Computes test set loss over the given DataLoader instance.""""""\n        loss = Loss()\n        acc = Loss()\n\n        for batch in pbar(data_loader, unit=\'batch\'):\n            batch.device(DEVICE)\n            out = self.forward(batch)\n            loss.update(out[\'loss\'], out[\'n_items\'])\n            acc.update(\n                out[\'probs\'].max(1)[1].eq(batch[\'lb\'].squeeze()).float().sum(),\n                out[\'n_items\'])\n\n        return [\n            Metric(\'LOSS\', loss.get(), higher_better=False),\n            Metric(\'ACC\', acc.get(), higher_better=True),\n        ]\n'"
nmtpytorch/models/stale/sat.py,2,"b'# -*- coding: utf-8 -*-\nimport logging\n\nimport torch\nimport torch.nn.functional as F\nfrom ..layers import ImageEncoder, XuDecoder\n\nfrom ..datasets import MultimodalDataset\n\nfrom .nmt import NMT\n\nlogger = logging.getLogger(\'nmtpytorch\')\n\n\nclass ShowAttendAndTell(NMT):\n    r""""""An Implementation of \'Show, attend and tell\' image captioning paper.\n\n    Paper: http://www.jmlr.org/proceedings/papers/v37/xuc15.pdf\n    Reference implementation: https://github.com/kelvinxu/arctic-captions\n    """"""\n    supports_beam_search = True\n\n    def set_defaults(self):\n        self.defaults = {\n            \'emb_dim\': 128,             # Source and target embedding sizes\n            \'emb_maxnorm\': None,        # Normalize embeddings l2 norm to 1\n            \'emb_gradscale\': False,     # Scale embedding gradients w.r.t. batch frequency\n            \'dec_dim\': 256,             # Decoder hidden size\n            \'dec_type\': \'gru\',          # Decoder type (gru|lstm)\n            \'dec_init\': \'mean_ctx\',     # How to initialize decoder (zero/mean_ctx)\n            \'att_type\': \'mlp\',          # Attention type (mlp|dot)\n            \'att_temp\': 1.,             # Attention temperature\n            \'att_activ\': \'tanh\',        # Attention non-linearity (all torch nonlins)\n            \'att_mlp_bias\': True,       # Enables bias in attention mechanism\n            \'att_bottleneck\': \'ctx\',    # Bottleneck dimensionality (ctx|hid)\n            \'att_transform_ctx\': True,  # Transform annotations before attention\n            \'dropout\': 0,               # Simple dropout\n            \'tied_emb\': False,          # Share embeddings: (False|2way|3way)\n            \'selector\': True,           # Selector gate\n            \'alpha_c\': 0.0,             # Attention regularization\n            \'prev2out\': True,           # Add prev embedding to output\n            \'ctx2out\': True,            # Add context to output\n            \'cnn_type\': \'resnet50\',     # A variant of VGG or ResNet\n            \'cnn_layer\': \'res5c_relu\',  # From where to extract features\n            \'cnn_pretrained\': True,     # Should we use pretrained imagenet weights\n            \'cnn_finetune\': None,       # Should we finetune part or all of CNN\n            \'pool\': None,               # (\'Avg|Max\', kernel_size, stride_size)\n            \'l2_norm\': False,           # L2 normalize features\n            \'l2_norm_dim\': -1,          # Which dimension to L2 normalize\n            \'resize\': 256,              # resize width, height for images\n            \'crop\': 224,                # center crop size after resize\n            \'replicate\': 1,             # number of captions/image\n            \'direction\': None,          # Network directionality, i.e. en->de\n            \'bucket_by\': None,          # A key like \'en\' to define w.r.t which dataset\n                                        # the batches will be sorted\n        }\n\n    def __init__(self, opts):\n        super().__init__(opts)\n        if self.opts.model[\'alpha_c\'] > 0:\n            self.aux_loss[\'alpha_reg\'] = 0.0\n\n    def setup(self, is_train=True):\n        logger.info(\'Loading CNN\')\n        cnn_encoder = ImageEncoder(\n            cnn_type=self.opts.model[\'cnn_type\'],\n            pretrained=self.opts.model[\'cnn_pretrained\'])\n\n        # Set truncation point\n        cnn_encoder.setup(\n            layer=self.opts.model[\'cnn_layer\'], pool=self.opts.model[\'pool\'])\n\n        # By default the CNN is not tuneable\n        if self.opts.model[\'cnn_finetune\'] is not None:\n            cnn_encoder.set_requires_grad(\n                value=True, layers=self.opts.model[\'cnn_finetune\'])\n\n        # Number of channels defines the spatial vector dim for us\n        self.ctx_sizes = {\'image\': cnn_encoder.get_output_shape()[1]}\n\n        # Finally set the CNN as a submodule\n        self.cnn = cnn_encoder.get()\n\n        # Nicely printed table of summary for the CNN\n        logger.info(cnn_encoder)\n\n        # Create Decoder\n        self.dec = XuDecoder(\n            input_size=self.opts.model[\'emb_dim\'],\n            hidden_size=self.opts.model[\'dec_dim\'],\n            n_vocab=self.n_trg_vocab,\n            rnn_type=self.opts.model[\'dec_type\'],\n            ctx_size_dict=self.ctx_sizes,\n            ctx_name=\'image\',\n            tied_emb=self.opts.model[\'tied_emb\'],\n            dec_init=self.opts.model[\'dec_init\'],\n            att_type=self.opts.model[\'att_type\'],\n            att_temp=self.opts.model[\'att_temp\'],\n            att_activ=self.opts.model[\'att_activ\'],\n            transform_ctx=self.opts.model[\'att_transform_ctx\'],\n            mlp_bias=self.opts.model[\'att_mlp_bias\'],\n            att_bottleneck=self.opts.model[\'att_bottleneck\'],\n            dropout=self.opts.model[\'dropout\'],\n            emb_maxnorm=self.opts.model[\'emb_maxnorm\'],\n            emb_gradscale=self.opts.model[\'emb_gradscale\'],\n            selector=self.opts.model[\'selector\'],\n            prev2out=self.opts.model[\'prev2out\'],\n            ctx2out=self.opts.model[\'ctx2out\'])\n\n    def load_data(self, split, batch_size, mode=\'train\'):\n        """"""Loads the requested dataset split.""""""\n        dataset = MultimodalDataset(\n            data=self.opts.data[split + \'_set\'],\n            mode=mode, batch_size=batch_size,\n            vocabs=self.vocabs, topology=self.topology,\n            bucket_by=self.opts.model[\'bucket_by\'],\n            max_len=self.opts.model.get(\'max_len\', None),\n            warmup=(split != \'train\'),\n            resize=self.opts.model[\'resize\'],\n            replicate=self.opts.model[\'replicate\'] if split == \'train\' else 1,\n            crop=self.opts.model[\'crop\'])\n        logger.info(dataset)\n        return dataset\n\n    def encode(self, batch, **kwargs):\n        # Get features into (n,c,w*h) and then (w*h,n,c)\n        feats = self.cnn(batch[\'image\'])\n        feats = feats.view((*feats.shape[:2], -1)).permute(2, 0, 1)\n        if self.opts.model[\'l2_norm\']:\n            feats = F.normalize(\n                feats, dim=self.opts.model[\'l2_norm_dim\']).detach()\n\n        return {\'image\': (feats, None)}\n\n    def forward(self, batch, **kwargs):\n        result = super().forward(batch)\n\n        if self.training and self.opts.model[\'alpha_c\'] > 0:\n            alpha_loss = (\n                1 - torch.cat(self.dec.history[\'alpha_img\']).sum(0)).pow(2).sum(0)\n            self.aux_loss[\'alpha_reg\'] = alpha_loss.mean().mul(\n                self.opts.model[\'alpha_c\'])\n\n        return result\n'"
