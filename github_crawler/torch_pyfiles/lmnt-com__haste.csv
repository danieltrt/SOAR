file_path,api_count,code
setup.py,1,"b'# Copyright 2020 LMNT, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport sys\n\nfrom setuptools import setup\nfrom setuptools.dist import Distribution\n\n\nVERSION = \'0.4.0\'\nDESCRIPTION = \'Haste: a fast, simple, and open RNN library.\'\nAUTHOR = \'LMNT, Inc.\'\nAUTHOR_EMAIL = \'haste@lmnt.com\'\nURL = \'https://www.lmnt.com\'\nLICENSE = \'Apache 2.0\'\nCLASSIFIERS = [\n  \'Development Status :: 4 - Beta\',\n  \'Intended Audience :: Developers\',\n  \'Intended Audience :: Education\',\n  \'Intended Audience :: Science/Research\',\n  \'License :: OSI Approved :: Apache Software License\',\n  \'Programming Language :: Python :: 3.5\',\n  \'Programming Language :: Python :: 3.6\',\n  \'Programming Language :: Python :: 3.7\',\n  \'Programming Language :: Python :: 3.8\',\n  \'Topic :: Scientific/Engineering :: Mathematics\',\n  \'Topic :: Software Development :: Libraries :: Python Modules\',\n  \'Topic :: Software Development :: Libraries\',\n]\n\n\nclass BinaryDistribution(Distribution):\n  """"""This class is needed in order to create OS specific wheels.""""""\n\n  def has_ext_modules(self):\n    return True\n\n\nif sys.argv[1] == \'haste_tf\':\n  del sys.argv[1]\n\n  with open(f\'tf/_version.py\', \'wt\') as f:\n    f.write(f\'__version__ = ""{VERSION}""\')\n\n  setup(name = \'haste_tf\',\n      version = VERSION,\n      description = DESCRIPTION,\n      author = AUTHOR,\n      author_email = AUTHOR_EMAIL,\n      url = URL,\n      license = LICENSE,\n      keywords = \'tensorflow machine learning rnn lstm gru custom op\',\n      packages = [\'haste_tf\'],\n      package_dir = { \'haste_tf\': \'tf\' },\n      package_data = { \'haste_tf\': [\'*.so\'] },\n      install_requires = [],\n      zip_safe = False,\n      distclass = BinaryDistribution,\n      classifiers = CLASSIFIERS)\nelif sys.argv[1] == \'haste_pytorch\':\n  del sys.argv[1]\n\n  import os\n  from glob import glob\n  from platform import platform\n  from torch.utils import cpp_extension\n\n  base_path = os.path.dirname(os.path.realpath(__file__))\n  if \'Windows\' in platform():\n    CUDA_HOME = os.environ.get(\'CUDA_HOME\', os.environ.get(\'CUDA_PATH\'))\n    extra_args = []\n  else:\n    CUDA_HOME = os.environ.get(\'CUDA_HOME\', \'/usr/local/cuda\')\n    extra_args = [\'-Wno-sign-compare\']\n\n  with open(f\'pytorch/_version.py\', \'wt\') as f:\n    f.write(f\'__version__ = ""{VERSION}""\')\n\n  extension = cpp_extension.CppExtension(\n      \'haste_pytorch_lib\',\n      sources = glob(\'pytorch/*.cc\'),\n      extra_compile_args = extra_args,\n      include_dirs = [os.path.join(base_path, \'lib\'), os.path.join(CUDA_HOME, \'include\')],\n      libraries = [\'haste\', \'cublas\', \'cudart\'],\n      library_dirs = [\'.\', os.path.join(CUDA_HOME, \'lib64\'), os.path.join(CUDA_HOME, \'lib\', \'x64\')])\n  setup(name = \'haste_pytorch\',\n      version = VERSION,\n      description = DESCRIPTION,\n      author = AUTHOR,\n      author_email = AUTHOR_EMAIL,\n      url = URL,\n      license = LICENSE,\n      keywords = \'pytorch machine learning rnn lstm gru custom op\',\n      packages = [\'haste_pytorch\'],\n      package_dir = { \'haste_pytorch\': \'pytorch\' },\n      install_requires = [],\n      ext_modules = [extension],\n      cmdclass = { \'build_ext\': cpp_extension.BuildExtension },\n      classifiers = CLASSIFIERS)\n'"
benchmarks/report.py,0,"b'# Copyright 2020 LMNT, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport argparse\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\n\ndef extract(x, predicate):\n  return np.array(list(filter(predicate, x)))\n\n\ndef main(args):\n  np.set_printoptions(suppress=True)\n\n  A = np.loadtxt(args.A, delimiter=\',\')\n  B = np.loadtxt(args.B, delimiter=\',\')\n\n  faster = 1.0 - A[:,-1] / B[:,-1]\n\n  print(f\'A is faster than B by:\')\n  print(f\'  mean:   {np.mean(faster)*100:7.4}%\')\n  print(f\'  std:    {np.std(faster)*100:7.4}%\')\n  print(f\'  median: {np.median(faster)*100:7.4}%\')\n  print(f\'  min:    {np.min(faster)*100:7.4}%\')\n  print(f\'  max:    {np.max(faster)*100:7.4}%\')\n\n  for batch_size in np.unique(A[:,0]):\n    for input_size in np.unique(A[:,2]):\n      a = extract(A, lambda x: x[0] == batch_size and x[2] == input_size)\n      b = extract(B, lambda x: x[0] == batch_size and x[2] == input_size)\n      fig, ax = plt.subplots(dpi=200)\n      ax.set_xticks(a[:,1])\n      ax.set_xticklabels(a[:,1].astype(np.int32), rotation=60)\n      ax.tick_params(axis=\'y\', which=\'both\', length=0)\n      ax.spines[\'top\'].set_visible(False)\n      ax.spines[\'right\'].set_visible(False)\n      plt.title(f\'batch size={int(batch_size)}, input size={int(input_size)}\')\n      plt.plot(a[:,1], a[:,-1], color=args.color[0])\n      plt.plot(a[:,1], b[:,-1], color=args.color[1])\n      plt.xlabel(\'hidden size\')\n      plt.ylabel(\'time (ms)\')\n      plt.legend(args.name, frameon=False)\n      plt.tight_layout()\n      if args.save:\n        os.makedirs(args.save[0], exist_ok=True)\n        plt.savefig(f\'{args.save[0]}/report_n={int(batch_size)}_c={int(input_size)}.png\', dpi=200)\n      else:\n        plt.show()\n\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\'--name\', nargs=2, default=[\'A\', \'B\'])\n  parser.add_argument(\'--color\', nargs=2, default=[\'#1f77b4\', \'#2ca02c\'])\n  parser.add_argument(\'--save\', nargs=1, default=None)\n  parser.add_argument(\'A\')\n  parser.add_argument(\'B\')\n  main(parser.parse_args())\n'"
validation/pytorch.py,11,"b'# Copyright 2020 LMNT, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport argparse\nimport torch\nimport haste_pytorch as haste\n\n\nRNN_MAP = {\n    \'gru\': haste.GRU,\n    \'indrnn\': haste.IndRNN,\n    \'layer_norm_gru\': haste.LayerNormGRU,\n    \'layer_norm_indrnn\': haste.LayerNormIndRNN,\n    \'layer_norm_lstm\': haste.LayerNormLSTM,\n    \'lstm\': haste.LSTM,\n}\n\nHASTE_TO_NATIVE = {\n    haste.GRU: torch.nn.GRU,\n    haste.LSTM: torch.nn.LSTM,\n}\n\nbatch_size = 32\ntime_steps = 250\ninput_size = 128\nhidden_size = 256\n\n\ndef self_consistency(rnn, x):\n  x_cuda = x.clone().cuda()\n  x_cpu = x.clone().cpu()\n  x_cuda.requires_grad_(True)\n  x_cpu.requires_grad_(True)\n\n  y1, _ = rnn.cuda().forward(x_cuda)\n  y1.backward(torch.ones_like(y1))\n  y2, _ = rnn.cpu().forward(x_cpu)\n  y2.backward(torch.ones_like(y2))\n\n  g1 = x_cpu.grad.data\n  g2 = x_cuda.grad.data\n\n  print(torch.max(torch.abs(y1.cpu()-y2.cpu())))\n  print(torch.max(torch.abs(g1.cpu()-g2.cpu())))\n\n\ndef native_consistency(haste_rnn, pytorch_rnn, x):\n  pytorch_rnn.cuda()\n  haste_rnn.cuda()\n  haste_rnn.from_native_weights(\n      pytorch_rnn.weight_ih_l0,\n      pytorch_rnn.weight_hh_l0,\n      pytorch_rnn.bias_ih_l0,\n      pytorch_rnn.bias_hh_l0)\n\n  x1 = x.clone().cuda()\n  x2 = x.clone().cuda()\n  x1.requires_grad_(True)\n  x2.requires_grad_(True)\n\n  y1, _ = haste_rnn.forward(x1)\n  y1.backward(torch.ones_like(y1))\n\n  y2, _ = pytorch_rnn.forward(x2)\n  y2.backward(torch.ones_like(y2))\n\n  g1 = x1.grad.data\n  g2 = x2.grad.data\n\n  print(torch.max(torch.abs(y1-y2)))\n  print(torch.max(torch.abs(g1-g2)))\n\n\ndef run_rnn(rnn_type, x):\n  rnn = rnn_type(input_size, hidden_size)\n  self_consistency(rnn, x)\n  if rnn_type in HASTE_TO_NATIVE:\n    pytorch_rnn = HASTE_TO_NATIVE[rnn_type](input_size, hidden_size)\n    native_consistency(rnn, pytorch_rnn, x)\n\n\ndef main(args):\n  x = torch.rand(time_steps, batch_size, input_size)\n  if args.rnn_type == \'all\':\n    for type_name, rnn_type in RNN_MAP.items():\n      print(f\'[{type_name}]\')\n      run_rnn(rnn_type, x)\n      print(\'\')\n  else:\n    print(f\'[{args.rnn_type}]\')\n    rnn_type = RNN_MAP[args.rnn_type]\n    rnn = run_rnn(rnn_type, x)\n\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      \'rnn_type\',\n      nargs=\'?\',\n      default=\'all\',\n      choices=list(RNN_MAP.keys()) + [\'all\'])\n  main(parser.parse_args())\n'"
validation/pytorch_speed.py,2,"b""import torch\nimport haste_pytorch as haste\n\nfrom time import time\n\n\nseq_len = 2500\nbatch_size = 64\ninput_size = 256\nhidden_size = 4096\n\nrnn = haste.IndRNN(input_size, hidden_size).cuda()\n\nx = torch.rand(seq_len, batch_size, input_size).cuda()\nstart = time()\nfor _ in range(10):\n  y, _ = rnn(x)\n  y.backward(torch.ones_like(y))\nend = time()\nprint(f'{end-start}')\n"""
validation/tf.py,0,"b'# Copyright 2020 LMNT, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport argparse\nimport haste_tf as haste\nimport tensorflow as tf\n\n\ndef stfu():\n  import os\n  os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'4\'\n  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n\n\ndef NativeGRUBuilder(hidden_size):\n  return tf.keras.layers.GRU(\n      hidden_size,\n      implementation=2,\n      activation=\'tanh\',\n      recurrent_activation=\'sigmoid\',\n      return_sequences=True,\n      reset_after=True)\n\n\ndef NativeLSTMBuilder(hidden_size):\n  return tf.keras.layers.LSTM(\n      hidden_size,\n      implementation=2,\n      activation=\'tanh\',\n      unit_forget_bias=False,\n      recurrent_activation=\'sigmoid\',\n      return_sequences=True)\n\n\ndef NativeGRUWeights(native_gru, haste_gru):\n  weights = haste_gru.fw_layer.get_weights()\n  native_gru.variables[0].assign(weights[\'kernel\'])\n  native_gru.variables[1].assign(weights[\'recurrent_kernel\'])\n  native_gru.variables[2].assign(tf.stack([weights[\'bias\'], weights[\'recurrent_bias\']], axis=0))\n\n\ndef NativeLSTMWeights(native_lstm, haste_lstm):\n  def swapple(x):\n    i, g, f, o = tf.split(x, 4, axis=-1)\n    return tf.concat([i, f, g, o], axis=-1)\n  weights = haste_lstm.fw_layer.get_weights()\n  native_lstm.variables[0].assign(swapple(weights[\'kernel\']))\n  native_lstm.variables[1].assign(swapple(weights[\'recurrent_kernel\']))\n  native_lstm.variables[2].assign(swapple(weights[\'bias\']))\n\n\nRNN_MAP = {\n    \'gru\': haste.GRU,\n    \'indrnn\': haste.IndRNN,\n    \'layer_norm_gru\': haste.LayerNormGRU,\n    \'layer_norm_lstm\': haste.LayerNormLSTM,\n    \'lstm\': haste.LSTM,\n}\n\nHASTE_TO_NATIVE = {\n    haste.GRU: NativeGRUBuilder,\n    haste.LSTM: NativeLSTMBuilder,\n}\n\nHASTE_TO_NATIVE_WEIGHTS = {\n    haste.GRU: NativeGRUWeights,\n    haste.LSTM: NativeLSTMWeights,\n}\n\n\nbatch_size = 32\ntime_steps = 250\ninput_size = 128\nhidden_size = 256\n\n\ndef native_consistency(haste_rnn, native_rnn, x):\n  with tf.GradientTape() as tape:\n    tape.watch(x)\n    y1, _ = haste_rnn(x, training=True)\n    g1 = tape.gradient(y1, x)\n\n  native_rnn.build(x.shape)\n  HASTE_TO_NATIVE_WEIGHTS[type(haste_rnn)](native_rnn, haste_rnn)\n\n  with tf.GradientTape() as tape:\n    tape.watch(x)\n    y2 = native_rnn(x, training=True)\n    g2 = tape.gradient(y2, x)\n\n  print(tf.reduce_max(tf.abs(y2-y1)))\n  print(tf.reduce_max(tf.abs(g2-g1)))\n\n\ndef run_rnn(rnn_type, x):\n  rnn = rnn_type(hidden_size)\n  if rnn_type in HASTE_TO_NATIVE:\n    native_rnn = HASTE_TO_NATIVE[rnn_type](hidden_size)\n    native_consistency(rnn, native_rnn, x)\n\n\ndef main(args):\n  tf.compat.v1.enable_eager_execution()\n  stfu()\n\n  x = tf.random.normal([batch_size, time_steps, input_size])\n  if args.rnn_type == \'all\':\n    for type_name, rnn_type in RNN_MAP.items():\n      print(f\'[{type_name}]\')\n      run_rnn(rnn_type, x)\n      print(\'\')\n  else:\n    print(f\'[{args.rnn_type}]\')\n    rnn_type = RNN_MAP[args.rnn_type]\n    rnn = run_rnn(rnn_type, x)\n\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      \'rnn_type\',\n      nargs=\'?\',\n      default=\'all\',\n      choices=list(RNN_MAP.keys()) + [\'all\'])\n  main(parser.parse_args())\n'"
validation/tf_pytorch.py,34,"b'# Copyright 2020 LMNT, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport argparse\nimport numpy as np\nimport tensorflow as tf\nimport haste_tf\nimport torch\nimport torch.nn as nn\nimport haste_pytorch\n\n\ndef stfu():\n  import os\n  os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'4\'\n  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n\n\ndef copy_weights_gru(rnn_tf, rnn_pt):\n  weights = rnn_tf.fw_layer.get_weights()\n  kernel = torch.Tensor(weights[\'kernel\'].numpy())\n  recurrent_kernel = torch.Tensor(weights[\'recurrent_kernel\'].numpy())\n  bias = torch.Tensor(weights[\'bias\'].numpy())\n  recurrent_bias = torch.Tensor(weights[\'recurrent_bias\'].numpy())\n\n  rnn_pt.kernel = nn.Parameter(kernel)\n  rnn_pt.recurrent_kernel = nn.Parameter(recurrent_kernel)\n  rnn_pt.bias = nn.Parameter(bias)\n  rnn_pt.recurrent_bias = nn.Parameter(recurrent_bias)\n\n\ndef copy_weights_indrnn(rnn_tf, rnn_pt):\n  weights = rnn_tf.fw_layer.get_weights()\n  kernel = torch.Tensor(weights[\'kernel\'].numpy())\n  recurrent_scale = torch.Tensor(weights[\'recurrent_scale\'].numpy())\n  bias = torch.Tensor(weights[\'bias\'].numpy())\n\n  rnn_pt.kernel = nn.Parameter(kernel)\n  rnn_pt.recurrent_scale = nn.Parameter(recurrent_scale)\n  rnn_pt.bias = nn.Parameter(bias)\n\n\ndef copy_weights_layer_norm_gru(rnn_tf, rnn_pt):\n  weights = rnn_tf.fw_layer.get_weights()\n  kernel = torch.Tensor(weights[\'kernel\'].numpy())\n  recurrent_kernel = torch.Tensor(weights[\'recurrent_kernel\'].numpy())\n  bias = torch.Tensor(weights[\'bias\'].numpy())\n  recurrent_bias = torch.Tensor(weights[\'recurrent_bias\'].numpy())\n  gamma = torch.Tensor(weights[\'gamma\'].numpy())\n\n  rnn_pt.kernel = nn.Parameter(kernel)\n  rnn_pt.recurrent_kernel = nn.Parameter(recurrent_kernel)\n  rnn_pt.bias = nn.Parameter(bias)\n  rnn_pt.recurrent_bias = nn.Parameter(recurrent_bias)\n  rnn_pt.gamma = nn.Parameter(gamma)\n\n\ndef copy_weights_layer_norm_indrnn(rnn_tf, rnn_pt):\n  weights = rnn_tf.fw_layer.get_weights()\n  kernel = torch.Tensor(weights[\'kernel\'].numpy())\n  recurrent_scale = torch.Tensor(weights[\'recurrent_scale\'].numpy())\n  bias = torch.Tensor(weights[\'bias\'].numpy())\n  gamma = torch.Tensor(weights[\'gamma\'].numpy())\n\n  rnn_pt.kernel = nn.Parameter(kernel)\n  rnn_pt.recurrent_scale = nn.Parameter(recurrent_scale)\n  rnn_pt.bias = nn.Parameter(bias)\n  rnn_pt.gamma = nn.Parameter(gamma)\n\n\ndef copy_weights_layer_norm_lstm(rnn_tf, rnn_pt):\n  weights = rnn_tf.fw_layer.get_weights()\n  kernel = torch.Tensor(weights[\'kernel\'].numpy())\n  recurrent_kernel = torch.Tensor(weights[\'recurrent_kernel\'].numpy())\n  bias = torch.Tensor(weights[\'bias\'].numpy())\n  gamma = torch.Tensor(weights[\'gamma\'].numpy())\n  gamma_h = torch.Tensor(weights[\'gamma_h\'].numpy())\n  beta_h = torch.Tensor(weights[\'beta_h\'].numpy())\n\n  rnn_pt.kernel = nn.Parameter(kernel)\n  rnn_pt.recurrent_kernel = nn.Parameter(recurrent_kernel)\n  rnn_pt.bias = nn.Parameter(bias)\n  rnn_pt.gamma = nn.Parameter(gamma)\n  rnn_pt.gamma_h = nn.Parameter(gamma_h)\n  rnn_pt.beta_h = nn.Parameter(beta_h)\n\n\ndef copy_weights_lstm(rnn_tf, rnn_pt):\n  weights = rnn_tf.fw_layer.get_weights()\n  kernel = torch.Tensor(weights[\'kernel\'].numpy())\n  recurrent_kernel = torch.Tensor(weights[\'recurrent_kernel\'].numpy())\n  bias = torch.Tensor(weights[\'bias\'].numpy())\n\n  rnn_pt.kernel = nn.Parameter(kernel)\n  rnn_pt.recurrent_kernel = nn.Parameter(recurrent_kernel)\n  rnn_pt.bias = nn.Parameter(bias)\n\n\nbatch_size = 32\ntime_steps = 250\ninput_size = 128\nhidden_size = 256\n\nRNN_MAP = {\n    \'gru\': haste_tf.GRU,\n    \'indrnn\': haste_tf.IndRNN,\n    \'layer_norm_gru\': haste_tf.LayerNormGRU,\n    \'layer_norm_indrnn\': haste_tf.LayerNormIndRNN,\n    \'layer_norm_lstm\': haste_tf.LayerNormLSTM,\n    \'lstm\': haste_tf.LSTM,\n}\n\nTF_TO_PT = {\n    haste_tf.GRU: haste_pytorch.GRU,\n    haste_tf.IndRNN: haste_pytorch.IndRNN,\n    haste_tf.LayerNormGRU: haste_pytorch.LayerNormGRU,\n    haste_tf.LayerNormIndRNN: haste_pytorch.LayerNormIndRNN,\n    haste_tf.LayerNormLSTM: haste_pytorch.LayerNormLSTM,\n    haste_tf.LSTM: haste_pytorch.LSTM,\n}\n\nWEIGHT_COPY_MAP = {\n    haste_tf.GRU: copy_weights_gru,\n    haste_tf.IndRNN: copy_weights_indrnn,\n    haste_tf.LayerNormGRU: copy_weights_layer_norm_gru,\n    haste_tf.LayerNormIndRNN: copy_weights_layer_norm_indrnn,\n    haste_tf.LayerNormLSTM: copy_weights_layer_norm_lstm,\n    haste_tf.LSTM: copy_weights_lstm,\n}\n\n\ndef run_rnn(rnn_type, x):\n  rnn_tf = rnn_type(hidden_size)\n  rnn_pt = TF_TO_PT[rnn_type](input_size, hidden_size, batch_first=True)\n\n  rnn_tf.build(x.shape)\n  WEIGHT_COPY_MAP[type(rnn_tf)](rnn_tf, rnn_pt)\n\n  x1 = tf.convert_to_tensor(x)\n  x2 = torch.Tensor(x)\n  x2.requires_grad_(True)\n  with tf.GradientTape() as tape:\n    tape.watch(x1)\n    y1, _ = rnn_tf(x1, training=True)\n    g1 = tape.gradient(y1, x1)\n\n  y2, _ = rnn_pt(x2)\n  y2.backward(torch.ones_like(y2))\n\n  print(np.amax(np.abs(y1.numpy() - y2.detach().numpy())))\n  print(np.amax(np.abs(g1.numpy() - x2.grad.data.numpy())))\n\n\ndef main(args):\n  tf.compat.v1.enable_eager_execution()\n  stfu()\n\n  x = np.random.normal(size=[time_steps, batch_size, input_size]).astype(np.float32)\n  if args.rnn_type == \'all\':\n    for type_name, rnn_type in RNN_MAP.items():\n      print(f\'[{type_name}]\')\n      run_rnn(rnn_type, x)\n      print(\'\')\n  else:\n    print(f\'[{args.rnn_type}]\')\n    rnn_type = RNN_MAP[args.rnn_type]\n    rnn = run_rnn(rnn_type, x)\n\n\nif __name__ == \'__main__\':\n  parser = argparse.ArgumentParser()\n  parser.add_argument(\n      \'rnn_type\',\n      nargs=\'?\',\n      default=\'all\',\n      choices=list(RNN_MAP.keys()) + [\'all\'])\n  main(parser.parse_args())\n'"
frameworks/pytorch/__init__.py,0,"b'# Copyright 2020 LMNT, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\nHaste: a fast, simple, and open RNN library.\n""""""\n\nimport torch as _\n\nfrom ._version import __version__  # generated in setup.py\nfrom .gru import GRU\nfrom .indrnn import IndRNN\nfrom .lstm import LSTM\nfrom .layer_norm_gru import LayerNormGRU\nfrom .layer_norm_indrnn import LayerNormIndRNN\nfrom .layer_norm_lstm import LayerNormLSTM\n\n__all__ = [\n    \'GRU\',\n    \'IndRNN\',\n    \'LSTM\',\n    \'LayerNormGRU\',\n    \'LayerNormIndRNN\',\n    \'LayerNormLSTM\'\n]\n'"
frameworks/pytorch/base_rnn.py,1,"b'# Copyright 2020 LMNT, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\nimport torch\nimport torch.nn as nn\n\n\n__all__ = [\n    \'BaseRNN\'\n]\n\n\nclass BaseRNN(nn.Module):\n  def __init__(\n      self,\n      input_size,\n      hidden_size,\n      batch_first,\n      zoneout,\n      return_state_sequence):\n    super().__init__()\n    self.input_size = input_size\n    self.hidden_size = hidden_size\n    self.batch_first = batch_first\n    self.zoneout = zoneout\n    self.return_state_sequence = return_state_sequence\n\n  def _permute(self, x):\n    if self.batch_first:\n      return x.permute(1, 0, 2)\n    return x\n\n  def _get_state(self, input, state, state_shape):\n    if state is None:\n      state = _zero_state(input, state_shape)\n    else:\n      _validate_state(state, state_shape)\n    return state\n\n  def _get_final_state(self, state, lengths):\n    if isinstance(state, tuple):\n      return tuple(self._get_final_state(s, lengths) for s in state)\n    if isinstance(state, list):\n      return [self._get_final_state(s, lengths) for s in state]\n    if self.return_state_sequence:\n      return self._permute(state[1:]).unsqueeze(0)\n    if lengths is not None:\n      cols = range(state.size(1))\n      return state[[lengths, cols]].unsqueeze(0)\n    return state[-1].unsqueeze(0)\n\n  def _get_zoneout_mask(self, input):\n    if self.zoneout:\n      zoneout_mask = input.new_empty(input.shape[0], input.shape[1], self.hidden_size)\n      zoneout_mask.bernoulli_(1.0 - self.zoneout)\n    else:\n      zoneout_mask = input.new_empty(0, 0, 0)\n    return zoneout_mask\n\n  def _is_cuda(self):\n    is_cuda = [tensor.is_cuda for tensor in list(self.parameters())]\n    if any(is_cuda) and not all(is_cuda):\n      raise ValueError(\'RNN tensors should all be CUDA tensors or none should be CUDA tensors\')\n    return any(is_cuda)\n\n\ndef _validate_state(state, state_shape):\n  """"""\n  Checks to make sure that `state` has the same nested structure and dimensions\n  as `state_shape`. `None` values in `state_shape` are a wildcard and are not\n  checked.\n\n  Arguments:\n    state: a nested structure of Tensors.\n    state_shape: a nested structure of integers or None.\n\n  Raises:\n    ValueError: if the structure and/or shapes don\'t match.\n  """"""\n  if isinstance(state, (tuple, list)):\n    # Handle nested structure.\n    if not isinstance(state_shape, (tuple, list)):\n      raise ValueError(\'RNN state has invalid structure; expected {}\'.format(state_shape))\n    for s, ss in zip(state, state_shape):\n      _validate_state(s, ss)\n  else:\n    shape = list(state.size())\n    if len(shape) != len(state_shape):\n      raise ValueError(\'RNN state dimension mismatch; expected {} got {}\'.format(len(state_shape), len(shape)))\n\n    for i, (d1, d2) in enumerate(zip(list(state.size()), state_shape)):\n      if d2 is not None and d1 != d2:\n        raise ValueError(\'RNN state size mismatch on dim {}; expected {} got {}\'.format(i, d2, d1))\n\n\ndef _zero_state(input, state_shape):\n  """"""\n  Returns a nested structure of zero Tensors with the same structure and shape\n  as `state_shape`. The returned Tensors will have the same dtype and be on the\n  same device as `input`.\n\n  Arguments:\n    input: Tensor, to specify the device and dtype of the returned tensors.\n    shape_state: nested structure of integers.\n\n  Returns:\n    zero_state: a nested structure of zero Tensors.\n\n  Raises:\n    ValueError: if `state_shape` has non-integer values.\n  """"""\n  if isinstance(state_shape, (tuple, list)) and isinstance(state_shape[0], int):\n    state = input.new_zeros(*state_shape)\n  elif isinstance(state_shape, tuple):\n    state = tuple(_zero_state(input, s) for s in state_shape)\n  elif isinstance(state_shape, list):\n    state = [_zero_state(input, s) for s in state_shape]\n  else:\n    raise ValueError(\'RNN state_shape is invalid\')\n  return state\n'"
frameworks/pytorch/gru.py,22,"b'# Copyright 2020 LMNT, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Gated Recurrent Unit""""""\n\n\nimport haste_pytorch_lib as LIB\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .base_rnn import BaseRNN\n\n\n__all__ = [\n    \'GRU\'\n]\n\n\n#@torch.jit.script\ndef GRUScript(\n    training: bool,\n    zoneout_prob: float,\n    input,\n    h0,\n    kernel,\n    recurrent_kernel,\n    bias,\n    recurrent_bias,\n    zoneout_mask):\n  time_steps = input.shape[0]\n  batch_size = input.shape[1]\n  hidden_size = recurrent_kernel.shape[0]\n\n  h = [h0]\n  Wx = input @ kernel + bias\n  for t in range(time_steps):\n    Rh = h[t] @ recurrent_kernel + recurrent_bias\n    vx = torch.chunk(Wx[t], 3, 1)\n    vh = torch.chunk(Rh, 3, 1)\n\n    z = torch.sigmoid(vx[0] + vh[0])\n    r = torch.sigmoid(vx[1] + vh[1])\n    g = torch.tanh(vx[2] + r * vh[2])\n\n    h.append(z * h[t] + (1 - z) * g)\n    if zoneout_prob:\n      if training:\n        h[-1] = (h[-1] - h[-2]) * zoneout_mask[t] + h[-2]\n      else:\n        h[-1] = zoneout_prob * h[-2] + (1 - zoneout_prob) * h[-1]\n\n  h = torch.stack(h)\n  return h\n\n\nclass GRUFunction(torch.autograd.Function):\n  @staticmethod\n  def forward(ctx, training, zoneout_prob, *inputs):\n    h, cache = LIB.gru_forward(training, zoneout_prob, *inputs)\n    ctx.save_for_backward(inputs[0], *inputs[2:], h, cache)\n    ctx.mark_non_differentiable(inputs[-1])\n    ctx.training = training\n    return h\n\n  @staticmethod\n  def backward(ctx, grad_h):\n    if not ctx.training:\n      raise RuntimeError(\'GRU backward can only be called in training mode\')\n\n    saved = [*ctx.saved_tensors]\n    saved[0] = saved[0].permute(2, 0, 1).contiguous()\n    saved[1] = saved[1].permute(1, 0).contiguous()\n    saved[2] = saved[2].permute(1, 0).contiguous()\n    grads = LIB.gru_backward(*saved, grad_h.contiguous())\n    return (None, None, *grads, None)\n\n\nclass GRU(BaseRNN):\n  """"""\n  Gated Recurrent Unit layer.\n\n  This GRU layer offers a fused, GPU-accelerated PyTorch op for inference\n  and training. There are two commonly-used variants of GRU cells. This one\n  implements 1406.1078v1 which applies the reset gate to the hidden state\n  after matrix multiplication. cuDNN also implements this variant. The other\n  variant, 1406.1078v3, applies the reset gate before matrix multiplication\n  and is currently unsupported.\n\n  This layer has built-in support for DropConnect and Zoneout, which are\n  both techniques used to regularize RNNs.\n\n  See [\\_\\_init\\_\\_](#__init__) and [forward](#forward) for usage.\n  See [from_native_weights](#from_native_weights) and\n  [to_native_weights](#to_native_weights) for compatibility with PyTorch GRUs.\n  """"""\n\n  def __init__(self,\n      input_size,\n      hidden_size,\n      batch_first=False,\n      dropout=0.0,\n      zoneout=0.0,\n      return_state_sequence=False):\n    """"""\n    Initialize the parameters of the GRU layer.\n\n    Arguments:\n      input_size: int, the feature dimension of the input.\n      hidden_size: int, the feature dimension of the output.\n      batch_first: (optional) bool, if `True`, then the input and output\n        tensors are provided as `(batch, seq, feature)`.\n      dropout: (optional) float, sets the dropout rate for DropConnect\n        regularization on the recurrent matrix.\n      zoneout: (optional) float, sets the zoneout rate for Zoneout\n        regularization.\n      return_state_sequence: (optional) bool, if `True`, the forward pass will\n        return the entire state sequence instead of just the final state. Note\n        that if the input is a padded sequence, the returned state will also\n        be a padded sequence.\n\n    Variables:\n      kernel: the input projection weight matrix. Dimensions\n        (input_size, hidden_size * 3) with `z,r,h` gate layout. Initialized\n        with Xavier uniform initialization.\n      recurrent_kernel: the recurrent projection weight matrix. Dimensions\n        (hidden_size, hidden_size * 3) with `z,r,h` gate layout. Initialized\n        with orthogonal initialization.\n      bias: the input projection bias vector. Dimensions (hidden_size * 3) with\n        `z,r,h` gate layout. Initialized to zeros.\n      recurrent_bias: the recurrent projection bias vector. Dimensions\n        (hidden_size * 3) with `z,r,h` gate layout. Initialized to zeros.\n    """"""\n    super().__init__(input_size, hidden_size, batch_first, zoneout, return_state_sequence)\n\n    if dropout < 0 or dropout > 1:\n      raise ValueError(\'GRU: dropout must be in [0.0, 1.0]\')\n    if zoneout < 0 or zoneout > 1:\n      raise ValueError(\'GRU: zoneout must be in [0.0, 1.0]\')\n\n    self.dropout = dropout\n\n    self.kernel = nn.Parameter(torch.empty(input_size, hidden_size * 3))\n    self.recurrent_kernel = nn.Parameter(torch.empty(hidden_size, hidden_size * 3))\n    self.bias = nn.Parameter(torch.empty(hidden_size * 3))\n    self.recurrent_bias = nn.Parameter(torch.empty(hidden_size * 3))\n    self.reset_parameters()\n\n  def to_native_weights(self):\n    """"""\n    Converts Haste GRU weights to native PyTorch GRU weights.\n\n    Returns:\n      weight_ih_l0: Parameter, the input-hidden weights of the GRU layer.\n      weight_hh_l0: Parameter, the hidden-hidden weights of the GRU layer.\n      bias_ih_l0: Parameter, the input-hidden bias of the GRU layer.\n      bias_hh_l0: Parameter, the hidden-hidden bias of the GRU layer.\n    """"""\n    def reorder_weights(w):\n      z, r, n = torch.chunk(w, 3, dim=-1)\n      return torch.cat([r, z, n], dim=-1)\n\n    kernel = reorder_weights(self.kernel).permute(1, 0).contiguous()\n    recurrent_kernel = reorder_weights(self.recurrent_kernel).permute(1, 0).contiguous()\n    bias1 = reorder_weights(self.bias).contiguous()\n    bias2 = reorder_weights(self.recurrent_bias).contiguous()\n\n    kernel = torch.nn.Parameter(kernel)\n    recurrent_kernel = torch.nn.Parameter(recurrent_kernel)\n    bias1 = torch.nn.Parameter(bias1)\n    bias2 = torch.nn.Parameter(bias2)\n    return kernel, recurrent_kernel, bias1, bias2\n\n  def from_native_weights(self, weight_ih_l0, weight_hh_l0, bias_ih_l0, bias_hh_l0):\n    """"""\n    Copies and converts the provided PyTorch GRU weights into this layer.\n\n    Arguments:\n      weight_ih_l0: Parameter, the input-hidden weights of the PyTorch GRU layer.\n      weight_hh_l0: Parameter, the hidden-hidden weights of the PyTorch GRU layer.\n      bias_ih_l0: Parameter, the input-hidden bias of the PyTorch GRU layer.\n      bias_hh_l0: Parameter, the hidden-hidden bias of the PyTorch GRU layer.\n    """"""\n    def reorder_weights(w):\n      r, z, n = torch.chunk(w, 3, axis=-1)\n      return torch.cat([z, r, n], dim=-1)\n\n    kernel = reorder_weights(weight_ih_l0.permute(1, 0)).contiguous()\n    recurrent_kernel = reorder_weights(weight_hh_l0.permute(1, 0)).contiguous()\n    bias = reorder_weights(bias_ih_l0).contiguous()\n    recurrent_bias = reorder_weights(bias_hh_l0).contiguous()\n\n    self.kernel = nn.Parameter(kernel)\n    self.recurrent_kernel = nn.Parameter(recurrent_kernel)\n    self.bias = nn.Parameter(bias)\n    self.recurrent_bias = nn.Parameter(recurrent_bias)\n\n  def reset_parameters(self):\n    """"""Resets this layer\'s parameters to their initial values.""""""\n    hidden_size = self.hidden_size\n    for i in range(3):\n      nn.init.xavier_uniform_(self.kernel[:, i*hidden_size:(i+1)*hidden_size])\n      nn.init.orthogonal_(self.recurrent_kernel[:, i*hidden_size:(i+1)*hidden_size])\n    nn.init.zeros_(self.bias)\n    nn.init.zeros_(self.recurrent_bias)\n\n  def forward(self, input, state=None, lengths=None):\n    """"""\n    Runs a forward pass of the GRU layer.\n\n    Arguments:\n      input: Tensor, a batch of input sequences to pass through the GRU.\n        Dimensions (seq_len, batch_size, input_size) if `batch_first` is\n        `False`, otherwise (batch_size, seq_len, input_size).\n      lengths: (optional) Tensor, list of sequence lengths for each batch\n        element. Dimension (batch_size). This argument may be omitted if\n        all batch elements are unpadded and have the same sequence length.\n\n    Returns:\n      output: Tensor, the output of the GRU layer. Dimensions\n        (seq_len, batch_size, hidden_size) if `batch_first` is `False` (default)\n        or (batch_size, seq_len, hidden_size) if `batch_first` is `True`. Note\n        that if `lengths` was specified, the `output` tensor will not be\n        masked. It\'s the caller\'s responsibility to either not use the invalid\n        entries or to mask them out before using them.\n      h_n: the hidden state for the last sequence item. Dimensions\n        (1, batch_size, hidden_size).\n    """"""\n    input = self._permute(input)\n    state_shape = [1, input.shape[1], self.hidden_size]\n    h0 = self._get_state(input, state, state_shape)\n    h = self._impl(input, h0[0], self._get_zoneout_mask(input))\n    state = self._get_final_state(h, lengths)\n    output = self._permute(h[1:])\n    return output, state\n\n  def _impl(self, input, state, zoneout_mask):\n    if self._is_cuda():\n      return GRUFunction.apply(\n          self.training,\n          self.zoneout,\n          input.contiguous(),\n          state.contiguous(),\n          self.kernel.contiguous(),\n          F.dropout(self.recurrent_kernel, self.dropout, self.training).contiguous(),\n          self.bias.contiguous(),\n          self.recurrent_bias.contiguous(),\n          zoneout_mask.contiguous())\n    else:\n      return GRUScript(\n          self.training,\n          self.zoneout,\n          input.contiguous(),\n          state.contiguous(),\n          self.kernel.contiguous(),\n          F.dropout(self.recurrent_kernel, self.dropout, self.training).contiguous(),\n          self.bias.contiguous(),\n          self.recurrent_bias.contiguous(),\n          zoneout_mask.contiguous())\n'"
frameworks/pytorch/indrnn.py,9,"b'# Copyright 2020 LMNT, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Independently Recurrent Neural Network""""""\n\n\nimport haste_pytorch_lib as LIB\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .base_rnn import BaseRNN\n\n\n__all__ = [\n    \'IndRNN\'\n]\n\n\n#@torch.jit.script\ndef IndRNNScript(\n    training: bool,\n    zoneout_prob: float,\n    input,\n    h0,\n    kernel,\n    recurrent_scale,\n    bias,\n    zoneout_mask):\n  time_steps = input.shape[0]\n\n  h = [h0]\n  Wx = input @ kernel + bias\n  for t in range(time_steps):\n    h.append(torch.tanh(Wx[t] + h[-1] * recurrent_scale))\n    if zoneout_prob:\n      if training:\n        h[-1] = (h[-1] - h[-2]) * zoneout_mask[t] + h[-2]\n      else:\n        h[-1] = zoneout_prob * h[-2] + (1 - zoneout_prob) * h[-1]\n  h = torch.stack(h)\n  return h\n\n\nclass IndRNNFunction(torch.autograd.Function):\n  @staticmethod\n  def forward(ctx, training, zoneout_prob, *inputs):\n    h = LIB.indrnn_forward(training, zoneout_prob, *inputs)\n    ctx.save_for_backward(inputs[0], *inputs[2:], h)\n    ctx.training = training\n    return h\n\n  @staticmethod\n  def backward(ctx, grad_h):\n    if not ctx.training:\n      raise RuntimeError(\'IndRNN backward can only be called in training mode\')\n    saved = [*ctx.saved_tensors]\n    saved[0] = saved[0].permute(2, 0, 1).contiguous()\n    saved[1] = saved[1].permute(1, 0).contiguous()\n    grads = LIB.indrnn_backward(*saved, grad_h.contiguous())\n    return (None, None, *grads, None)\n\n\nclass IndRNN(BaseRNN):\n  """"""\n  Independently Recurrent Neural Network layer.\n\n  This layer offers a fused, GPU-accelerated PyTorch op for inference and\n  training. It also supports Zoneout regularization.\n\n  See [\\_\\_init\\_\\_](#__init__) and [forward](#forward) for usage.\n  """"""\n\n  def __init__(\n      self,\n      input_size,\n      hidden_size,\n      batch_first=False,\n      zoneout=0.0,\n      return_state_sequence=False):\n    """"""\n    Initialize the parameters of the IndRNN layer.\n\n    Arguments:\n      input_size: int, the feature dimension of the input.\n      hidden_size: int, the feature dimension of the output.\n      batch_first: (optional) bool, if `True`, then the input and output\n        tensors are provided as `(batch, seq, feature)`.\n      zoneout: (optional) float, sets the zoneout rate for Zoneout\n        regularization.\n      return_state_sequence: (optional) bool, if `True`, the forward pass will\n        return the entire state sequence instead of just the final state. Note\n        that if the input is a padded sequence, the returned state will also\n        be a padded sequence.\n\n    Variables:\n      kernel: the input projection weight matrix. Dimensions\n        (input_size, hidden_size). Initialized with Xavier uniform\n        initialization.\n      recurrent_scale: the recurrent scale weight vector. Dimensions\n        (hidden_size). Initialized uniformly in [-0.5, 0.5]. Note that this\n        initialization scheme is different than in the original authors\'\n        implementation. See https://github.com/lmnt-com/haste/issues/7 for\n        details.\n      bias: the RNN bias vector. Dimensions (hidden_size). Initialized to zeros.\n    """"""\n    super().__init__(input_size, hidden_size, batch_first, zoneout, return_state_sequence)\n\n    if zoneout < 0 or zoneout > 1:\n      raise ValueError(\'IndRNN: zoneout must be in [0.0, 1.0]\')\n\n    self.input_size = input_size\n    self.hidden_size = hidden_size\n    self.batch_first = batch_first\n    self.zoneout = zoneout\n\n    self.kernel = nn.Parameter(torch.empty(input_size, hidden_size))\n    self.recurrent_scale = nn.Parameter(torch.empty(hidden_size))\n    self.bias = nn.Parameter(torch.empty(hidden_size))\n    self.reset_parameters()\n\n  def reset_parameters(self):\n    nn.init.xavier_uniform_(self.kernel)\n    nn.init.uniform_(self.recurrent_scale, -0.5, 0.5)\n    nn.init.zeros_(self.bias)\n\n  def forward(self, input, state=None, lengths=None):\n    """"""\n    Runs a forward pass of the IndRNN layer.\n\n    Arguments:\n      input: Tensor, a batch of input sequences to pass through the GRU.\n        Dimensions (seq_len, batch_size, input_size) if `batch_first` is\n        `False`, otherwise (batch_size, seq_len, input_size).\n      state: (optional) Tensor, the initial state for each batch element in\n        `input`. Dimensions (1, batch_size, hidden_size). Defaults to zeros.\n      lengths: (optional) Tensor, list of sequence lengths for each batch\n        element. Dimension (batch_size). This argument may be omitted if\n        all batch elements are unpadded and have the same sequence length.\n\n    Returns:\n      output: Tensor, the output of the GRU layer. Dimensions\n        (seq_len, batch_size, hidden_size) if `batch_first` is `False` (default)\n        or (batch_size, seq_len, hidden_size) if `batch_first` is `True`. Note\n        that if `lengths` was specified, the `output` tensor will not be\n        masked. It\'s the caller\'s responsibility to either not use the invalid\n        entries or to mask them out before using them.\n      state: the hidden state for the last sequence item. Dimensions\n        (1, batch_size, hidden_size).\n    """"""\n    input = self._permute(input)\n    state_shape = [1, input.shape[1], self.hidden_size]\n    h0 = self._get_state(input, state, state_shape)\n    h = self._impl(input, h0[0], self._get_zoneout_mask(input))\n    state = self._get_final_state(h, lengths)\n    output = self._permute(h[1:])\n    return output, state\n\n  def _impl(self, input, state, zoneout_mask):\n    if self._is_cuda():\n      return IndRNNFunction.apply(\n        self.training,\n        self.zoneout,\n        input.contiguous(),\n        state.contiguous(),\n        self.kernel.contiguous(),\n        self.recurrent_scale.contiguous(),\n        self.bias.contiguous(),\n        zoneout_mask.contiguous())\n    else:\n      return IndRNNScript(\n        self.training,\n        self.zoneout,\n        input.contiguous(),\n        state.contiguous(),\n        self.kernel.contiguous(),\n        self.recurrent_scale.contiguous(),\n        self.bias.contiguous(),\n        zoneout_mask.contiguous())\n'"
frameworks/pytorch/layer_norm_gru.py,15,"b'# Copyright 2020 LMNT, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Layer Normalized Gated Recurrent Unit""""""\n\n\nimport haste_pytorch_lib as LIB\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .base_rnn import BaseRNN\n\n\n__all__ = [\n    \'LayerNormGRU\'\n]\n\n\n#@torch.jit.script\ndef LayerNormGRUScript(\n    training: bool,\n    zoneout_prob: float,\n    input,\n    h0,\n    kernel,\n    recurrent_kernel,\n    bias,\n    recurrent_bias,\n    gamma,\n    zoneout_mask):\n  time_steps = input.shape[0]\n  batch_size = input.shape[1]\n  hidden_size = recurrent_kernel.shape[0]\n\n  h = [h0]\n  Wx = F.layer_norm(input @ kernel, (hidden_size * 3,), weight=gamma[0]) + bias\n  for t in range(time_steps):\n    Rh = F.layer_norm(h[t] @ recurrent_kernel, (hidden_size * 3,), weight=gamma[1]) + recurrent_bias\n    vx = torch.chunk(Wx[t], 3, 1)\n    vh = torch.chunk(Rh, 3, 1)\n\n    z = torch.sigmoid(vx[0] + vh[0])\n    r = torch.sigmoid(vx[1] + vh[1])\n    g = torch.tanh(vx[2] + r * vh[2])\n\n    h.append(z * h[t] + (1 - z) * g)\n    if zoneout_prob:\n      if training:\n        h[-1] = (h[-1] - h[-2]) * zoneout_mask[t] + h[-2]\n      else:\n        h[-1] = zoneout_prob * h[-2] + (1 - zoneout_prob) * h[-1]\n\n  h = torch.stack(h)\n  return h\n\n\nclass LayerNormGRUFunction(torch.autograd.Function):\n  @staticmethod\n  def forward(ctx, training, zoneout_prob, *inputs):\n    output = LIB.layer_norm_gru_forward(training, zoneout_prob, *inputs)\n    ctx.save_for_backward(inputs[0], *inputs[2:], *output)\n    ctx.mark_non_differentiable(inputs[-1])\n    ctx.training = training\n    return output[0]\n\n  @staticmethod\n  def backward(ctx, grad_h):\n    if not ctx.training:\n      raise RuntimeError(\'LayerNormGRU backward can only be called in training mode\')\n\n    saved = [*ctx.saved_tensors]\n    saved[0] = saved[0].permute(2, 0, 1).contiguous()\n    saved[1] = saved[1].permute(1, 0).contiguous()\n    saved[2] = saved[2].permute(1, 0).contiguous()\n    grads = LIB.layer_norm_gru_backward(*saved, grad_h.contiguous())\n    return (None, None, *grads, None)\n\n\nclass LayerNormGRU(BaseRNN):\n  """"""\n  Layer Normalized Gated Recurrent Unit layer.\n\n  This GRU layer applies layer normalization to the input and recurrent output\n  activations of a standard GRU. The implementation is fused and\n  GPU-accelerated. There are two commonly-used variants of GRU cells. This one\n  implements 1406.1078v1 which applies the reset gate to the hidden state\n  after matrix multiplication. The other variant, 1406.1078v3, applies the\n  reset gate before matrix multiplication and is currently unsupported.\n\n  This layer has built-in support for DropConnect and Zoneout, which are\n  both techniques used to regularize RNNs.\n\n  See [\\_\\_init\\_\\_](#__init__) and [forward](#forward) for usage.\n  """"""\n\n  def __init__(self,\n      input_size,\n      hidden_size,\n      batch_first=False,\n      dropout=0.0,\n      zoneout=0.0,\n      return_state_sequence=False):\n    """"""\n    Initialize the parameters of the GRU layer.\n\n    Arguments:\n      input_size: int, the feature dimension of the input.\n      hidden_size: int, the feature dimension of the output.\n      batch_first: (optional) bool, if `True`, then the input and output\n        tensors are provided as `(batch, seq, feature)`.\n      dropout: (optional) float, sets the dropout rate for DropConnect\n        regularization on the recurrent matrix.\n      zoneout: (optional) float, sets the zoneout rate for Zoneout\n        regularization.\n      return_state_sequence: (optional) bool, if `True`, the forward pass will\n        return the entire state sequence instead of just the final state. Note\n        that if the input is a padded sequence, the returned state will also\n        be a padded sequence.\n\n    Variables:\n      kernel: the input projection weight matrix. Dimensions\n        (input_size, hidden_size * 3) with `z,r,h` gate layout. Initialized\n        with Xavier uniform initialization.\n      recurrent_kernel: the recurrent projection weight matrix. Dimensions\n        (hidden_size, hidden_size * 3) with `z,r,h` gate layout. Initialized\n        with orthogonal initialization.\n      bias: the input projection bias vector. Dimensions (hidden_size * 3) with\n        `z,r,h` gate layout. Initialized to zeros.\n      recurrent_bias: the recurrent projection bias vector. Dimensions\n        (hidden_size * 3) with `z,r,h` gate layout. Initialized to zeros.\n      gamma: the input and recurrent normalization gain. Dimensions\n        (2, hidden_size * 3) with `gamma[0]` specifying the input gain and\n        `gamma[1]` specifying the recurrent gain. Initialized to ones.\n    """"""\n    super().__init__(input_size, hidden_size, batch_first, zoneout, return_state_sequence)\n\n    if dropout < 0 or dropout > 1:\n      raise ValueError(\'LayerNormGRU: dropout must be in [0.0, 1.0]\')\n    if zoneout < 0 or zoneout > 1:\n      raise ValueError(\'LayerNormGRU: zoneout must be in [0.0, 1.0]\')\n\n    self.dropout = dropout\n\n    self.kernel = nn.Parameter(torch.empty(input_size, hidden_size * 3))\n    self.recurrent_kernel = nn.Parameter(torch.empty(hidden_size, hidden_size * 3))\n    self.bias = nn.Parameter(torch.empty(hidden_size * 3))\n    self.recurrent_bias = nn.Parameter(torch.empty(hidden_size * 3))\n    self.gamma = nn.Parameter(torch.empty(2, hidden_size * 3))\n    self.reset_parameters()\n\n  def reset_parameters(self):\n    """"""Resets this layer\'s parameters to their initial values.""""""\n    hidden_size = self.hidden_size\n    for i in range(3):\n      nn.init.xavier_uniform_(self.kernel[:, i*hidden_size:(i+1)*hidden_size])\n      nn.init.orthogonal_(self.recurrent_kernel[:, i*hidden_size:(i+1)*hidden_size])\n    nn.init.zeros_(self.bias)\n    nn.init.zeros_(self.recurrent_bias)\n    nn.init.ones_(self.gamma)\n\n  def forward(self, input, state=None, lengths=None):\n    """"""\n    Runs a forward pass of the GRU layer.\n\n    Arguments:\n      input: Tensor, a batch of input sequences to pass through the GRU.\n        Dimensions (seq_len, batch_size, input_size) if `batch_first` is\n        `False`, otherwise (batch_size, seq_len, input_size).\n      state: (optional) Tensor, the intial state for each batch element in\n        `input`. Dimensions (1, batch_size, hidden_size). Defaults to zeros.\n      lengths: (optional) Tensor, list of sequence lengths for each batch\n        element. Dimension (batch_size). This argument may be omitted if\n        all batch elements are unpadded and have the same sequence length.\n\n    Returns:\n      output: Tensor, the output of the GRU layer. Dimensions\n        (seq_len, batch_size, hidden_size) if `batch_first` is `False` (default)\n        or (batch_size, seq_len, hidden_size) if `batch_first` is `True`. Note\n        that if `lengths` was specified, the `output` tensor will not be\n        masked. It\'s the caller\'s responsibility to either not use the invalid\n        entries or to mask them out before using them.\n      h_n: the hidden state for the last sequence item. Dimensions\n        (1, batch_size, hidden_size).\n    """"""\n    input = self._permute(input)\n    state_shape = [1, input.shape[1], self.hidden_size]\n    h0 = self._get_state(input, state, state_shape)\n    h = self._impl(input, h0[0], self._get_zoneout_mask(input))\n    state = self._get_final_state(h, lengths)\n    output = self._permute(h[1:])\n    return output, state\n\n  def _impl(self, input, state, zoneout_mask):\n    if self._is_cuda():\n      return LayerNormGRUFunction.apply(\n          self.training,\n          self.zoneout,\n          input.contiguous(),\n          state.contiguous(),\n          self.kernel.contiguous(),\n          F.dropout(self.recurrent_kernel, self.dropout, self.training).contiguous(),\n          self.bias.contiguous(),\n          self.recurrent_bias.contiguous(),\n          self.gamma.contiguous(),\n          zoneout_mask.contiguous())\n    else:\n      return LayerNormGRUScript(\n          self.training,\n          self.zoneout,\n          input.contiguous(),\n          state.contiguous(),\n          self.kernel.contiguous(),\n          F.dropout(self.recurrent_kernel, self.dropout, self.training).contiguous(),\n          self.bias.contiguous(),\n          self.recurrent_bias.contiguous(),\n          self.gamma.contiguous(),\n          zoneout_mask.contiguous())\n'"
frameworks/pytorch/layer_norm_indrnn.py,10,"b'# Copyright 2020 LMNT, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Layer Normalized Independently Recurrent Neural Network""""""\n\n\nimport haste_pytorch_lib as LIB\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .base_rnn import BaseRNN\n\n\n__all__ = [\n    \'LayerNormIndRNN\'\n]\n\n\n#@torch.jit.script\ndef LayerNormIndRNNScript(\n    training: bool,\n    zoneout_prob: float,\n    input,\n    h0,\n    kernel,\n    recurrent_scale,\n    bias,\n    gamma,\n    zoneout_mask):\n  time_steps = input.shape[0]\n  hidden_size = kernel.shape[1]\n\n  h = [h0]\n  Wx = F.layer_norm(input @ kernel, (hidden_size,), weight=gamma[0]) + bias\n  for t in range(time_steps):\n    h.append(torch.tanh(Wx[t] + h[-1] * recurrent_scale))\n    if zoneout_prob:\n      if training:\n        h[-1] = (h[-1] - h[-2]) * zoneout_mask[t] + h[-2]\n      else:\n        h[-1] = zoneout_prob * h[-2] + (1 - zoneout_prob) * h[-1]\n  h = torch.stack(h)\n  return h\n\n\nclass LayerNormIndRNNFunction(torch.autograd.Function):\n  @staticmethod\n  def forward(ctx, training, zoneout_prob, *inputs):\n    output = LIB.layer_norm_indrnn_forward(training, zoneout_prob, *inputs)\n    ctx.save_for_backward(inputs[0], *inputs[2:], *output)\n    ctx.training = training\n    return output[0]\n\n  @staticmethod\n  def backward(ctx, grad_h):\n    if not ctx.training:\n      raise RuntimeError(\'LayerNormIndRNN backward can only be called in training mode\')\n\n    saved = [*ctx.saved_tensors]\n    saved[0] = saved[0].permute(2, 0, 1).contiguous()\n    saved[1] = saved[1].permute(1, 0).contiguous()\n    grads = LIB.layer_norm_indrnn_backward(*saved, grad_h.contiguous())\n    return (None, None, *grads, None)\n\n\nclass LayerNormIndRNN(BaseRNN):\n  """"""\n  Layer Normalized Independently Recurrent Neural Network layer.\n\n  This IndRNN layer applies layer normalization to the input activations of a\n  standard IndRNN. The implementation is fused and GPU-accelerated.\n\n  This layer has built-in support for Zoneout regularization.\n\n  See [\\_\\_init\\_\\_](#__init__) and [forward](#forward) for usage.\n  """"""\n\n  def __init__(\n      self,\n      input_size,\n      hidden_size,\n      batch_first=False,\n      zoneout=0.0,\n      return_state_sequence=False):\n    """"""\n    Initialize the parameters of the IndRNN layer.\n\n    Arguments:\n      input_size: int, the feature dimension of the input.\n      hidden_size: int, the feature dimension of the output.\n      batch_first: (optional) bool, if `True`, then the input and output\n        tensors are provided as `(batch, seq, feature)`.\n      zoneout: (optional) float, sets the zoneout rate for Zoneout\n        regularization.\n      return_state_sequence: (optional) bool, if `True`, the forward pass will\n        return the entire state sequence instead of just the final state. Note\n        that if the input is a padded sequence, the returned state will also\n        be a padded sequence.\n\n    Variables:\n      kernel: the input projection weight matrix. Dimensions\n        (input_size, hidden_size). Initialized with Xavier uniform\n        initialization.\n      recurrent_scale: the recurrent scale weight vector. Dimensions\n        (hidden_size). Initialized uniformly in [-0.5, 0.5]. Note that this\n        initialization scheme is different than in the original authors\'\n        implementation. See https://github.com/lmnt-com/haste/issues/7 for\n        details.\n      bias: the RNN bias vector. Dimensions (hidden_size). Initialized to zeros.\n      gamma: the input and recurrent normalization gain. Dimensions\n        (2, hidden_size) with `gamma[0]` specifying the input gain and\n        `gamma[1]` specifying the recurrent gain. Initialized to ones.\n    """"""\n    super().__init__(input_size, hidden_size, batch_first, zoneout, return_state_sequence)\n\n    if zoneout < 0 or zoneout > 1:\n      raise ValueError(\'LayerNormIndRNN: zoneout must be in [0.0, 1.0]\')\n\n    self.input_size = input_size\n    self.hidden_size = hidden_size\n    self.batch_first = batch_first\n    self.zoneout = zoneout\n\n    self.kernel = nn.Parameter(torch.empty(input_size, hidden_size))\n    self.recurrent_scale = nn.Parameter(torch.empty(hidden_size))\n    self.bias = nn.Parameter(torch.empty(hidden_size))\n    self.gamma = nn.Parameter(torch.empty(2, hidden_size))\n    self.reset_parameters()\n\n  def reset_parameters(self):\n    """"""Resets this layer\'s parameters to their initial values.""""""\n    nn.init.xavier_uniform_(self.kernel)\n    nn.init.uniform_(self.recurrent_scale, -0.5, 0.5)\n    nn.init.zeros_(self.bias)\n    nn.init.ones_(self.gamma)\n\n  def forward(self, input, state=None, lengths=None):\n    """"""\n    Runs a forward pass of the IndRNN layer.\n\n    Arguments:\n      input: Tensor, a batch of input sequences to pass through the GRU.\n        Dimensions (seq_len, batch_size, input_size) if `batch_first` is\n        `False`, otherwise (batch_size, seq_len, input_size).\n      state: (optional) Tensor, the initial state for each batch element in\n        `input`. Dimensions (1, batch_size, hidden_size). Defaults to zeros.\n      lengths: (optional) Tensor, list of sequence lengths for each batch\n        element. Dimension (batch_size). This argument may be omitted if\n        all batch elements are unpadded and have the same sequence length.\n\n    Returns:\n      output: Tensor, the output of the GRU layer. Dimensions\n        (seq_len, batch_size, hidden_size) if `batch_first` is `False` (default)\n        or (batch_size, seq_len, hidden_size) if `batch_first` is `True`. Note\n        that if `lengths` was specified, the `output` tensor will not be\n        masked. It\'s the caller\'s responsibility to either not use the invalid\n        entries or to mask them out before using them.\n      state: the hidden state for the last sequence item. Dimensions\n        (1, batch_size, hidden_size).\n    """"""\n    input = self._permute(input)\n    state_shape = [1, input.shape[1], self.hidden_size]\n    h0 = self._get_state(input, state, state_shape)\n    h = self._impl(input, h0[0], self._get_zoneout_mask(input))\n    state = self._get_final_state(h, lengths)\n    output = self._permute(h[1:])\n    return output, state\n\n  def _impl(self, input, state, zoneout_mask):\n    if self._is_cuda():\n      return LayerNormIndRNNFunction.apply(\n        self.training,\n        self.zoneout,\n        input.contiguous(),\n        state.contiguous(),\n        self.kernel.contiguous(),\n        self.recurrent_scale.contiguous(),\n        self.bias.contiguous(),\n        self.gamma.contiguous(),\n        zoneout_mask.contiguous())\n    else:\n      return LayerNormIndRNNScript(\n        self.training,\n        self.zoneout,\n        input.contiguous(),\n        state.contiguous(),\n        self.kernel.contiguous(),\n        self.recurrent_scale.contiguous(),\n        self.bias.contiguous(),\n        self.gamma.contiguous(),\n        zoneout_mask.contiguous())\n'"
frameworks/pytorch/layer_norm_lstm.py,18,"b'# Copyright 2020 LMNT, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Layer Normalized Long Short-Term Memory""""""\n\n\nimport haste_pytorch_lib as LIB\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .base_rnn import BaseRNN\n\n\n__all__ = [\n    \'LayerNormLSTM\'\n]\n\n\n#@torch.jit.script\ndef LayerNormLSTMScript(\n    training: bool,\n    zoneout_prob: float,\n    input,\n    h0,\n    c0,\n    kernel,\n    recurrent_kernel,\n    bias,\n    gamma,\n    gamma_h,\n    beta_h,\n    zoneout_mask):\n  time_steps = input.shape[0]\n  batch_size = input.shape[1]\n  hidden_size = recurrent_kernel.shape[0]\n\n  h = [h0]\n  c = [c0]\n  Wx = F.layer_norm(input @ kernel, (hidden_size * 4,), weight=gamma[0])\n  for t in range(time_steps):\n    v = F.layer_norm(h[t] @ recurrent_kernel, (hidden_size * 4,), weight=gamma[1]) + Wx[t] + bias\n    i, g, f, o = torch.chunk(v, 4, 1)\n    i = torch.sigmoid(i)\n    g = torch.tanh(g)\n    f = torch.sigmoid(f)\n    o = torch.sigmoid(o)\n    c.append(f * c[t] + i * g)\n    h.append(o * torch.tanh(F.layer_norm(c[-1], (hidden_size,), weight=gamma_h, bias=beta_h)))\n    if zoneout_prob:\n      if training:\n        h[-1] = (h[-1] - h[-2]) * zoneout_mask[t] + h[-2]\n      else:\n        h[-1] = zoneout_prob * h[-2] + (1 - zoneout_prob) * h[-1]\n  h = torch.stack(h)\n  c = torch.stack(c)\n  return h, c\n\n\nclass LayerNormLSTMFunction(torch.autograd.Function):\n  @staticmethod\n  def forward(ctx, training, zoneout_prob, *inputs):\n    outputs = LIB.layer_norm_lstm_forward(training, zoneout_prob, *inputs)\n    ctx.save_for_backward(inputs[0], *inputs[3:], *outputs)\n    ctx.mark_non_differentiable(inputs[-1])  # zoneout mask is non-differentiable\n    ctx.training = training\n    return outputs[0], outputs[1]\n\n  @staticmethod\n  def backward(ctx, grad_h, grad_c):\n    if not ctx.training:\n      raise RuntimeError(\'LayerNormLSTM backward can only be called in training mode\')\n\n    saved = [*ctx.saved_tensors]\n    saved[0] = saved[0].permute(2, 0, 1).contiguous()  # x -> x_t\n    saved[1] = saved[1].permute(1, 0).contiguous()     # kernel -> kernel_t\n    saved[2] = saved[2].permute(1, 0).contiguous()     # recurrent_kernel -> recurrent_kernel_t\n    grads = LIB.layer_norm_lstm_backward(*saved, grad_h.contiguous(), grad_c.contiguous())\n    return (None, None, *grads, None)\n\n\nclass LayerNormLSTM(BaseRNN):\n  """"""\n  Layer Normalized Long Short-Term Memory layer.\n\n  This LSTM layer applies layer normalization to the input, recurrent, and\n  output activations of a standard LSTM. The implementation is fused and\n  GPU-accelerated. DropConnect and Zoneout regularization are built-in, and\n  this layer allows setting a non-zero initial forget gate bias.\n\n  Details about the exact function this layer implements can be found at\n  https://github.com/lmnt-com/haste/issues/1.\n\n  See [\\_\\_init\\_\\_](#__init__) and [forward](#forward) for usage.\n  """"""\n\n  def __init__(self,\n      input_size,\n      hidden_size,\n      batch_first=False,\n      forget_bias=1.0,\n      dropout=0.0,\n      zoneout=0.0,\n      return_state_sequence=False):\n    """"""\n    Initialize the parameters of the LSTM layer.\n\n    Arguments:\n      input_size: int, the feature dimension of the input.\n      hidden_size: int, the feature dimension of the output.\n      batch_first: (optional) bool, if `True`, then the input and output\n        tensors are provided as `(batch, seq, feature)`.\n      forget_bias: (optional) float, sets the initial bias of the forget gate\n        for this LSTM cell.\n      dropout: (optional) float, sets the dropout rate for DropConnect\n        regularization on the recurrent matrix.\n      zoneout: (optional) float, sets the zoneout rate for Zoneout\n        regularization.\n      return_state_sequence: (optional) bool, if `True`, the forward pass will\n        return the entire state sequence instead of just the final state. Note\n        that if the input is a padded sequence, the returned state will also\n        be a padded sequence.\n\n    Variables:\n      kernel: the input projection weight matrix. Dimensions\n        (input_size, hidden_size * 4) with `i,g,f,o` gate layout. Initialized\n        with Xavier uniform initialization.\n      recurrent_kernel: the recurrent projection weight matrix. Dimensions\n        (hidden_size, hidden_size * 4) with `i,g,f,o` gate layout. Initialized\n        with orthogonal initialization.\n      bias: the projection bias vector. Dimensions (hidden_size * 4) with\n        `i,g,f,o` gate layout. The forget gate biases are initialized to\n        `forget_bias` and the rest are zeros.\n      gamma: the input and recurrent normalization gain. Dimensions\n        (2, hidden_size * 4) with `gamma[0]` specifying the input gain and\n        `gamma[1]` specifying the recurrent gain. Initialized to ones.\n      gamma_h: the output normalization gain. Dimensions (hidden_size).\n        Initialized to ones.\n      beta_h: the output normalization bias. Dimensions (hidden_size).\n        Initialized to zeros.\n    """"""\n    super().__init__(input_size, hidden_size, batch_first, zoneout, return_state_sequence)\n\n    if dropout < 0 or dropout > 1:\n      raise ValueError(\'LayerNormLSTM: dropout must be in [0.0, 1.0]\')\n    if zoneout < 0 or zoneout > 1:\n      raise ValueError(\'LayerNormLSTM: zoneout must be in [0.0, 1.0]\')\n\n    self.forget_bias = forget_bias\n    self.dropout = dropout\n\n    self.kernel = nn.Parameter(torch.empty(input_size, hidden_size * 4))\n    self.recurrent_kernel = nn.Parameter(torch.empty(hidden_size, hidden_size * 4))\n    self.bias = nn.Parameter(torch.empty(hidden_size * 4))\n    self.gamma = nn.Parameter(torch.empty(2, hidden_size * 4))\n    self.gamma_h = nn.Parameter(torch.empty(hidden_size))\n    self.beta_h = nn.Parameter(torch.empty(hidden_size))\n    self.reset_parameters()\n\n  def reset_parameters(self):\n    """"""Resets this layer\'s parameters to their initial values.""""""\n    hidden_size = self.hidden_size\n    for i in range(4):\n      nn.init.xavier_uniform_(self.kernel[:, i*hidden_size:(i+1)*hidden_size])\n      nn.init.orthogonal_(self.recurrent_kernel[:, i*hidden_size:(i+1)*hidden_size])\n    nn.init.zeros_(self.bias)\n    nn.init.constant_(self.bias[hidden_size*2:hidden_size*3], self.forget_bias)\n    nn.init.ones_(self.gamma)\n    nn.init.ones_(self.gamma_h)\n    nn.init.zeros_(self.beta_h)\n\n  def forward(self, input, state=None, lengths=None):\n    """"""\n    Runs a forward pass of the LSTM layer.\n\n    Arguments:\n      input: Tensor, a batch of input sequences to pass through the LSTM.\n        Dimensions (seq_len, batch_size, input_size) if `batch_first` is\n        `False`, otherwise (batch_size, seq_len, input_size).\n      lengths: (optional) Tensor, list of sequence lengths for each batch\n        element. Dimension (batch_size). This argument may be omitted if\n        all batch elements are unpadded and have the same sequence length.\n\n    Returns:\n      output: Tensor, the output of the LSTM layer. Dimensions\n        (seq_len, batch_size, hidden_size) if `batch_first` is `False` (default)\n        or (batch_size, seq_len, hidden_size) if `batch_first` is `True`. Note\n        that if `lengths` was specified, the `output` tensor will not be\n        masked. It\'s the caller\'s responsibility to either not use the invalid\n        entries or to mask them out before using them.\n      (h_n, c_n): the hidden and cell states, respectively, for the last\n        sequence item. Dimensions (1, batch_size, hidden_size).\n    """"""\n    input = self._permute(input)\n    state_shape = [1, input.shape[1], self.hidden_size]\n    state_shape = (state_shape, state_shape)\n    h0, c0 = self._get_state(input, state, state_shape)\n    h, c = self._impl(input, (h0[0], c0[0]), self._get_zoneout_mask(input))\n    state = self._get_final_state((h, c), lengths)\n    output = self._permute(h[1:])\n    return output, state\n\n  def _impl(self, input, state, zoneout_mask):\n    if self._is_cuda():\n      return LayerNormLSTMFunction.apply(\n          self.training,\n          self.zoneout,\n          input.contiguous(),\n          state[0].contiguous(),\n          state[1].contiguous(),\n          self.kernel.contiguous(),\n          F.dropout(self.recurrent_kernel, self.dropout, self.training).contiguous(),\n          self.bias.contiguous(),\n          self.gamma.contiguous(),\n          self.gamma_h.contiguous(),\n          self.beta_h.contiguous(),\n          zoneout_mask.contiguous())\n    else:\n      return LayerNormLSTMScript(\n          self.training,\n          self.zoneout,\n          input.contiguous(),\n          state[0].contiguous(),\n          state[1].contiguous(),\n          self.kernel.contiguous(),\n          F.dropout(self.recurrent_kernel, self.dropout, self.training).contiguous(),\n          self.bias.contiguous(),\n          self.gamma.contiguous(),\n          self.gamma_h.contiguous(),\n          self.beta_h.contiguous(),\n          zoneout_mask.contiguous())\n'"
frameworks/pytorch/lstm.py,23,"b'# Copyright 2020 LMNT, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Long Short-Term Memory""""""\n\n\nimport haste_pytorch_lib as LIB\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .base_rnn import BaseRNN\n\n\n__all__ = [\n    \'LSTM\'\n]\n\n\n#@torch.jit.script\ndef LSTMScript(\n    training: bool,\n    zoneout_prob: float,\n    input,\n    h0,\n    c0,\n    kernel,\n    recurrent_kernel,\n    bias,\n    zoneout_mask):\n  time_steps = input.shape[0]\n  batch_size = input.shape[1]\n  hidden_size = recurrent_kernel.shape[0]\n\n  h = [h0]\n  c = [c0]\n  Wx = input @ kernel\n  for t in range(time_steps):\n    v = h[t] @ recurrent_kernel + Wx[t] + bias\n    i, g, f, o = torch.chunk(v, 4, 1)\n    i = torch.sigmoid(i)\n    g = torch.tanh(g)\n    f = torch.sigmoid(f)\n    o = torch.sigmoid(o)\n    c.append(f * c[t] + i * g)\n    h.append(o * torch.tanh(c[-1]))\n    if zoneout_prob:\n      if training:\n        h[-1] = (h[-1] - h[-2]) * zoneout_mask[t] + h[-2]\n      else:\n        h[-1] = zoneout_prob * h[-2] + (1 - zoneout_prob) * h[-1]\n  h = torch.stack(h)\n  c = torch.stack(c)\n  return h, c\n\n\nclass LSTMFunction(torch.autograd.Function):\n  @staticmethod\n  def forward(ctx, training, zoneout_prob, *inputs):\n    h, c, cache = LIB.lstm_forward(training, zoneout_prob, *inputs)\n    ctx.save_for_backward(inputs[0], *inputs[3:], h, c, cache)\n    ctx.mark_non_differentiable(inputs[-1])\n    ctx.training = training\n    return h, c\n\n  @staticmethod\n  def backward(ctx, grad_h, grad_c):\n    if not ctx.training:\n      raise RuntimeError(\'LSTM backward can only be called in training mode\')\n\n    saved = [*ctx.saved_tensors]\n    saved[0] = saved[0].permute(2, 0, 1).contiguous()\n    saved[1] = saved[1].permute(1, 0).contiguous()\n    saved[2] = saved[2].permute(1, 0).contiguous()\n    grads = LIB.lstm_backward(*saved, grad_h.contiguous(), grad_c.contiguous())\n    return (None, None, *grads, None)\n\n\nclass LSTM(BaseRNN):\n  """"""\n  Long Short-Term Memory layer.\n\n  This LSTM layer offers a fused, GPU-accelerated PyTorch op for inference\n  and training. Although this implementation is comparable in performance to\n  cuDNN\'s LSTM, it offers additional options not typically found in other\n  high-performance implementations. DropConnect and Zoneout regularization are\n  built-in, and this layer allows setting a non-zero initial forget gate bias.\n\n  See [\\_\\_init\\_\\_](#__init__) and [forward](#forward) for general usage.\n  See [from_native_weights](#from_native_weights) and\n  [to_native_weights](#to_native_weights) for compatibility with PyTorch LSTMs.\n  """"""\n\n  def __init__(self,\n      input_size,\n      hidden_size,\n      batch_first=False,\n      forget_bias=1.0,\n      dropout=0.0,\n      zoneout=0.0,\n      return_state_sequence=False):\n    """"""\n    Initialize the parameters of the LSTM layer.\n\n    Arguments:\n      input_size: int, the feature dimension of the input.\n      hidden_size: int, the feature dimension of the output.\n      batch_first: (optional) bool, if `True`, then the input and output\n        tensors are provided as `(batch, seq, feature)`.\n      forget_bias: (optional) float, sets the initial bias of the forget gate\n        for this LSTM cell.\n      dropout: (optional) float, sets the dropout rate for DropConnect\n        regularization on the recurrent matrix.\n      zoneout: (optional) float, sets the zoneout rate for Zoneout\n        regularization.\n      return_state_sequence: (optional) bool, if `True`, the forward pass will\n        return the entire state sequence instead of just the final state. Note\n        that if the input is a padded sequence, the returned state will also\n        be a padded sequence.\n\n    Variables:\n      kernel: the input projection weight matrix. Dimensions\n        (input_size, hidden_size * 4) with `i,g,f,o` gate layout. Initialized\n        with Xavier uniform initialization.\n      recurrent_kernel: the recurrent projection weight matrix. Dimensions\n        (hidden_size, hidden_size * 4) with `i,g,f,o` gate layout. Initialized\n        with orthogonal initialization.\n      bias: the projection bias vector. Dimensions (hidden_size * 4) with\n        `i,g,f,o` gate layout. The forget gate biases are initialized to\n        `forget_bias` and the rest are zeros.\n    """"""\n    super().__init__(input_size, hidden_size, batch_first, zoneout, return_state_sequence)\n\n    if dropout < 0 or dropout > 1:\n      raise ValueError(\'LSTM: dropout must be in [0.0, 1.0]\')\n    if zoneout < 0 or zoneout > 1:\n      raise ValueError(\'LSTM: zoneout must be in [0.0, 1.0]\')\n\n    self.forget_bias = forget_bias\n    self.dropout = dropout\n\n    self.kernel = nn.Parameter(torch.empty(input_size, hidden_size * 4))\n    self.recurrent_kernel = nn.Parameter(torch.empty(hidden_size, hidden_size * 4))\n    self.bias = nn.Parameter(torch.empty(hidden_size * 4))\n    self.reset_parameters()\n\n  def to_native_weights(self):\n    """"""\n    Converts Haste LSTM weights to native PyTorch LSTM weights.\n\n    Returns:\n      weight_ih_l0: Parameter, the input-hidden weights of the LSTM layer.\n      weight_hh_l0: Parameter, the hidden-hidden weights of the LSTM layer.\n      bias_ih_l0: Parameter, the input-hidden bias of the LSTM layer.\n      bias_hh_l0: Parameter, the hidden-hidden bias of the LSTM layer.\n    """"""\n    def reorder_weights(w):\n      i, g, f, o = torch.chunk(w, 4, dim=-1)\n      return torch.cat([i, f, g, o], dim=-1)\n    kernel = reorder_weights(self.kernel).permute(1, 0).contiguous()\n    recurrent_kernel = reorder_weights(self.recurrent_kernel).permute(1, 0).contiguous()\n    half_bias = reorder_weights(self.bias) / 2.0\n\n    kernel = torch.nn.Parameter(kernel)\n    recurrent_kernel = torch.nn.Parameter(recurrent_kernel)\n    bias1 = torch.nn.Parameter(half_bias)\n    bias2 = torch.nn.Parameter(half_bias.clone())\n    return kernel, recurrent_kernel, bias1, bias2\n\n  def from_native_weights(self, weight_ih_l0, weight_hh_l0, bias_ih_l0, bias_hh_l0):\n    """"""\n    Copies and converts the provided PyTorch LSTM weights into this layer.\n\n    Arguments:\n      weight_ih_l0: Parameter, the input-hidden weights of the PyTorch LSTM layer.\n      weight_hh_l0: Parameter, the hidden-hidden weights of the PyTorch LSTM layer.\n      bias_ih_l0: Parameter, the input-hidden bias of the PyTorch LSTM layer.\n      bias_hh_l0: Parameter, the hidden-hidden bias of the PyTorch LSTM layer.\n    """"""\n    def reorder_weights(w):\n      i, f, g, o = torch.chunk(w, 4, dim=-1)\n      return torch.cat([i, g, f, o], dim=-1)\n    kernel = reorder_weights(weight_ih_l0.permute(1, 0)).contiguous()\n    recurrent_kernel = reorder_weights(weight_hh_l0.permute(1, 0)).contiguous()\n    bias = reorder_weights(bias_ih_l0 + bias_hh_l0).contiguous()\n\n    self.kernel = nn.Parameter(kernel)\n    self.recurrent_kernel = nn.Parameter(recurrent_kernel)\n    self.bias = nn.Parameter(bias)\n\n  def reset_parameters(self):\n    """"""Resets this layer\'s parameters to their initial values.""""""\n    hidden_size = self.hidden_size\n    for i in range(4):\n      nn.init.xavier_uniform_(self.kernel[:, i*hidden_size:(i+1)*hidden_size])\n      nn.init.orthogonal_(self.recurrent_kernel[:, i*hidden_size:(i+1)*hidden_size])\n    nn.init.zeros_(self.bias)\n    nn.init.constant_(self.bias[hidden_size*2:hidden_size*3], self.forget_bias)\n\n  def forward(self, input, state=None, lengths=None):\n    """"""\n    Runs a forward pass of the LSTM layer.\n\n    Arguments:\n      input: Tensor, a batch of input sequences to pass through the LSTM.\n        Dimensions (seq_len, batch_size, input_size) if `batch_first` is\n        `False`, otherwise (batch_size, seq_len, input_size).\n      lengths: (optional) Tensor, list of sequence lengths for each batch\n        element. Dimension (batch_size). This argument may be omitted if\n        all batch elements are unpadded and have the same sequence length.\n\n    Returns:\n      output: Tensor, the output of the LSTM layer. Dimensions\n        (seq_len, batch_size, hidden_size) if `batch_first` is `False` (default)\n        or (batch_size, seq_len, hidden_size) if `batch_first` is `True`. Note\n        that if `lengths` was specified, the `output` tensor will not be\n        masked. It\'s the caller\'s responsibility to either not use the invalid\n        entries or to mask them out before using them.\n      (h_n, c_n): the hidden and cell states, respectively, for the last\n        sequence item. Dimensions (1, batch_size, hidden_size).\n    """"""\n    input = self._permute(input)\n    state_shape = [1, input.shape[1], self.hidden_size]\n    state_shape = (state_shape, state_shape)\n    h0, c0 = self._get_state(input, state, state_shape)\n    h, c = self._impl(input, (h0[0], c0[0]), self._get_zoneout_mask(input))\n    state = self._get_final_state((h, c), lengths)\n    output = self._permute(h[1:])\n    return output, state\n\n  def _impl(self, input, state, zoneout_mask):\n    if self._is_cuda():\n      return LSTMFunction.apply(\n          self.training,\n          self.zoneout,\n          input.contiguous(),\n          state[0].contiguous(),\n          state[1].contiguous(),\n          self.kernel.contiguous(),\n          F.dropout(self.recurrent_kernel, self.dropout, self.training).contiguous(),\n          self.bias.contiguous(),\n          zoneout_mask.contiguous())\n    else:\n      return LSTMScript(\n          self.training,\n          self.zoneout,\n          input.contiguous(),\n          state[0].contiguous(),\n          state[1].contiguous(),\n          self.kernel.contiguous(),\n          F.dropout(self.recurrent_kernel, self.dropout, self.training).contiguous(),\n          self.bias.contiguous(),\n          zoneout_mask.contiguous())\n'"
frameworks/tf/__init__.py,0,"b'# Copyright 2020 LMNT, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""\nHaste: a fast, simple, and open RNN library.\n""""""\n\n\nfrom ._version import __version__  # generated in setup.py\nfrom .gru import GRU\nfrom .gru_cell import GRUCell\nfrom .indrnn import IndRNN\nfrom .layer_norm import LayerNorm\nfrom .layer_norm_gru import LayerNormGRU\nfrom .layer_norm_gru_cell import LayerNormGRUCell\nfrom .layer_norm_indrnn import LayerNormIndRNN\nfrom .layer_norm_lstm import LayerNormLSTM\nfrom .layer_norm_lstm_cell import LayerNormLSTMCell\nfrom .lstm import LSTM\nfrom .zoneout_wrapper import ZoneoutWrapper\n\n\n__all__ = [\n    \'GRU\',\n    \'GRUCell\',\n    \'IndRNN\',\n    \'LayerNorm\',\n    \'LayerNormGRU\',\n    \'LayerNormGRUCell\',\n    \'LayerNormIndRNN\',\n    \'LayerNormLSTM\',\n    \'LayerNormLSTMCell\',\n    \'LSTM\',\n    \'ZoneoutWrapper\'\n]\n'"
frameworks/tf/base_rnn.py,0,"b'# Copyright 2020 LMNT, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Base RNN layer class.""""""\n\n\nimport tensorflow as tf\n\nfrom tensorflow.compat import v1\n\n\n__all__ = [\n    \'BaseRNN\'\n]\n\n\ndef reverse_sequence(sequence, sequence_length):\n  """"""\n  Reverses a batched sequence in time-major order [T,N,...]. The input sequence\n  may be padded, in which case sequence_length specifies the unpadded length of\n  each sequence.\n  """"""\n  if sequence_length is None:\n    return tf.reverse(sequence, axis=[0])\n  return tf.reverse_sequence(sequence, sequence_length, seq_axis=0, batch_axis=1)\n\n\ndef transpose(tensor_or_tuple, perm):\n  """"""Transposes the given tensor or tuple of tensors by the same permutation.""""""\n  if isinstance(tensor_or_tuple, tuple):\n    return tuple([tf.transpose(tensor, perm) for tensor in tensor_or_tuple])\n  return tf.transpose(tensor_or_tuple, perm)\n\n\nclass BaseRNN(tf.Module):\n  def __init__(self, rnn_class, num_units, direction, default_name, **kwargs):\n    assert direction in [\'unidirectional\', \'bidirectional\']\n\n    self.default_name = default_name\n    if direction == \'bidirectional\':\n      name = kwargs.pop(\'name\', None)\n      super().__init__(name)\n      self.realname = name\n      self.fw_layer = rnn_class(num_units, name=\'fw\', **kwargs)\n      self.bw_layer = rnn_class(num_units, name=\'bw\', **kwargs)\n    else:\n      super().__init__()\n      self.fw_layer = rnn_class(num_units, **kwargs)\n      self.bw_layer = None\n\n  def build(self, shape):\n    """"""\n    Creates the variables of the layer.\n\n    Calling this method is optional for users of the RNN class. It is called\n    internally with the correct shape when `__call__` is invoked.\n\n    Arguments:\n      shape: instance of `TensorShape`.\n    """"""\n    if self.bidirectional:\n      with self.name_scope, v1.variable_scope(self.realname, self.default_name):\n        self.fw_layer.build(shape)\n        self.bw_layer.build(shape)\n    else:\n      self.fw_layer.build(shape)\n\n  @property\n  def output_size(self):\n    if self.bidirectional:\n      return self.fw_layer.output_size, self.bw_layer.output_size\n    return self.fw_layer.output_size\n\n  @property\n  def state_size(self):\n    if self.bidirectional:\n      return self.fw_layer.state_size, self.bw_layer.state_size\n    return self.fw_layer.state_size\n\n  def __call__(self, inputs, training, sequence_length=None, time_major=False):\n    """"""\n    Runs the RNN layer.\n\n    Arguments:\n      inputs: Tensor, a rank 3 input tensor with shape [N,T,C] if `time_major`\n        is `False`, or with shape [T,N,C] if `time_major` is `True`.\n      training: bool, `True` if running in training mode, `False` if running\n        in inference mode.\n      sequence_length: (optional) Tensor, a rank 1 tensor with shape [N] and\n        dtype of `tf.int32` or `tf.int64`. This tensor specifies the unpadded\n        length of each example in the input minibatch.\n      time_major: (optional) bool, specifies whether `input` has shape [N,T,C]\n        (`time_major=False`) or shape [T,N,C] (`time_major=True`).\n\n    Returns:\n      A pair, `(output, state)` for unidirectional layers, or a pair\n      `([output_fw, output_bw], [state_fw, state_bw])` for bidirectional\n      layers.\n    """"""\n    if not time_major:\n      inputs = transpose(inputs, [1, 0, 2])\n\n    result, state = self.fw_layer(inputs, sequence_length, training)\n\n    if self.bidirectional:\n      inputs = reverse_sequence(inputs, sequence_length)\n      bw_result, bw_state = self.bw_layer(inputs, sequence_length, training)\n      result = result, reverse_sequence(bw_result, sequence_length)\n      state = state, bw_state\n\n    if not time_major:\n      result = transpose(result, [1, 0, 2])\n\n    return result, state\n\n  @property\n  def bidirectional(self):\n    """"""`True` if this is a bidirectional RNN, `False` otherwise.""""""\n    return self.bw_layer is not None\n'"
frameworks/tf/gru.py,0,"b'# Copyright 2020 LMNT, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Gated Recurrent Unit""""""\n\n\nimport pkg_resources\nimport tensorflow as tf\n\nfrom tensorflow.compat import v1\nfrom .base_rnn import BaseRNN\nfrom .weight_config import WeightConfig\n\n\n__all__ = [\n    \'GRU\'\n]\n\n\nLIB = tf.load_op_library(pkg_resources.resource_filename(__name__, \'libhaste_tf.so\'))\n\n\n@tf.RegisterGradient(""HasteGru"")\ndef gru_gradient(op, *grads):\n  training = op.get_attr(\'training\')\n  if not training:\n    raise ValueError((\'GRU can only compute gradients if `training=True` was specified during the \'\n                      \'forward pass.\\nFailed op: {}\').format(op.name))\n\n  # Extract inputs and outputs from the op.\n  x = op.inputs[0]\n  W = op.inputs[1]\n  R = op.inputs[2]\n  bx = op.inputs[3]\n  br = op.inputs[4]\n  zoneout_mask = op.inputs[5]\n  h = op.outputs[0]\n  v = op.outputs[1]\n\n  # Pre-transpose matrices for better performance.\n  x = tf.transpose(x, [2, 0, 1])\n  W = tf.transpose(W, [1, 0])\n  R = tf.transpose(R, [1, 0])\n\n  dx, dW, dR, dbx, dbr = LIB.haste_gru_grad(x, W, R, bx, br, h, v, grads[0], zoneout_mask)\n\n  return [dx, dW, dR, dbx, dbr, None]\n\n\nclass GRULayer(tf.Module):\n  def __init__(self,\n        num_units,\n        kernel_initializer=None,\n        recurrent_initializer=None,\n        bias_initializer=None,\n        recurrent_bias_initializer=None,\n        kernel_transform=None,\n        recurrent_transform=None,\n        bias_transform=None,\n        recurrent_bias_transform=None,\n        dropout=0.0,\n        zoneout=0.0,\n        dtype=None,\n        name=None):\n    super(GRULayer, self).__init__(name)\n    self.realname = name\n    self.num_units = num_units\n\n    identity = lambda x: x\n    self.kernel_config = WeightConfig(v1.initializers.glorot_uniform(), None, identity)\n    self.recurrent_config = WeightConfig(v1.initializers.orthogonal(), None, identity)\n    self.bias_config = WeightConfig(v1.initializers.zeros(), None, identity)\n    self.recurrent_bias_config = WeightConfig(v1.initializers.zeros(), None, identity)\n\n    self.kernel_config.override(kernel_initializer, None, kernel_transform)\n    self.recurrent_config.override(recurrent_initializer, None, recurrent_transform)\n    self.bias_config.override(bias_initializer, None, bias_transform)\n    self.recurrent_bias_config.override(recurrent_bias_initializer, None, recurrent_bias_transform)\n\n    self.dropout = dropout\n    self.zoneout = zoneout\n    self.dtype = dtype or tf.float32\n    self.built = False\n\n  def build(self, shape):\n    if self.built:\n      return\n\n    num_units = self.num_units\n    input_size = int(shape[-1])\n\n    def build_weights(initializer, shape):\n      weights = [initializer(shape, dtype=self.dtype) for _ in range(3)]\n      weights = tf.concat(weights, axis=-1)\n      return weights\n\n    kernel_weights = build_weights(self.kernel_config.initializer, [input_size, num_units])\n    recurrent_weights = build_weights(self.recurrent_config.initializer, [num_units, num_units])\n    biases = build_weights(self.bias_config.initializer, [num_units])\n    recurrent_biases = build_weights(self.recurrent_bias_config.initializer, [num_units])\n\n    weights = tf.concat([kernel_weights, recurrent_weights], axis=0)\n    biases = tf.concat([biases, recurrent_biases], axis=0)\n\n    with self.name_scope, v1.variable_scope(self.realname, \'gru_cell\'):\n      self._kernel = v1.get_variable(\'kernel\', initializer=weights)\n      self._bias = v1.get_variable(\'bias\', initializer=biases)\n    self.built = True\n\n  def get_weights(self):\n    input_size = self._kernel.shape.as_list()[0] - self.num_units\n    kernel, recurrent_kernel = tf.split(self._kernel, [input_size, self.num_units], axis=0)\n    bias, recurrent_bias = tf.split(self._bias, 2, axis=0)\n    return {\n        \'kernel\': self.kernel_config.transform(kernel),\n        \'recurrent_kernel\': self.recurrent_config.transform(recurrent_kernel),\n        \'bias\': self.bias_config.transform(bias),\n        \'recurrent_bias\': self.recurrent_bias_config.transform(recurrent_bias)\n    }\n\n  def __call__(self, inputs, sequence_length, training):\n    self.build(inputs.shape)\n\n    shape = tf.shape(inputs)\n    time_steps = shape[0]\n    batch_size = shape[1]\n\n    # Use an empty zoneout mask if no zoneout is going to be applied.\n    # Sadly, we can\'t pass `None` to the op but at least we won\'t be wasting\n    # memory or bandwidth on this tensor.\n    zoneout_mask = tf.zeros([0, 0, 0], dtype=self.dtype)\n    if self.zoneout:\n      zoneout_mask = 1.0 - self.zoneout\n      zoneout_mask += tf.random.uniform([time_steps, batch_size, self.num_units], dtype=self.dtype)\n      zoneout_mask = tf.floor(zoneout_mask)\n\n    weights = self.get_weights()\n    result, _ = LIB.haste_gru(\n        inputs,\n        weights[\'kernel\'],\n        tf.nn.dropout(weights[\'recurrent_kernel\'], rate=self.dropout),\n        weights[\'bias\'],\n        weights[\'recurrent_bias\'],\n        zoneout_mask,\n        training=training,\n        zoneout_prob=self.zoneout)\n\n    if sequence_length is not None:\n      # 0-indexed tensors, so length-1.\n      indices = sequence_length\n      indices = tf.stack([indices, tf.range(batch_size, dtype=sequence_length.dtype)], axis=-1)\n      state = tf.gather_nd(result, indices)\n    else:\n      state = result[-1]\n\n    return result[1:], state\n\n\nclass GRU(BaseRNN):\n  """"""\n  Gated Recurrent Unit layer.\n\n  This GRU layer offers a fused, GPU-accelerated TensorFlow op for inference\n  and training. There are two commonly-used variants of GRU cells. This one\n  implements 1406.1078v1 which applies the reset gate to the hidden state\n  after matrix multiplication. cuDNN also implements this variant. The other\n  variant, 1406.1078v3, applies the reset gate before matrix multiplication\n  and is currently unsupported.\n\n  This layer has built-in support for DropConnect and Zoneout, which are\n  both techniques used to regularize RNNs.\n  """"""\n\n  def __init__(self, num_units, direction=\'unidirectional\', **kwargs):\n    """"""\n    Initialize the parameters of the GRU layer.\n\n    Arguments:\n      num_units: int, the number of units in the LSTM cell.\n      direction: string, \'unidirectional\' or \'bidirectional\'.\n      **kwargs: Dict, keyword arguments (see below).\n\n    Keyword Arguments:\n      kernel_initializer: (optional) the initializer to use for the input\n        matrix weights. Defaults to `glorot_uniform`.\n      recurrent_initializer: (optional) the initializer to use for the\n        recurrent matrix weights. Defaults to `orthogonal`.\n      bias_initializer: (optional) the initializer to use for input bias\n        vectors. Defaults to `zeros`.\n      recurrent_bias_initializer: (optional) the initializer to use for\n        recurrent bias vectors. Defaults to `zeros`.\n      kernel_transform: (optional) a function with signature\n        `(kernel: Tensor) -> Tensor` that transforms the kernel before it is\n        used. Defaults to the identity function.\n      recurrent_transform: (optional) a function with signature\n        `(recurrent_kernel: Tensor) -> Tensor` that transforms the recurrent\n        kernel before it is used. Defaults to the identity function.\n      bias_transform: (optional) a function with signature\n        `(bias: Tensor) -> Tensor` that transforms the bias before it is used.\n        Defaults to the identity function.\n      recurrent_bias_transform: (optional) a function with signature\n        `(recurrent_bias: Tensor) -> Tensor` that transforms the recurrent bias\n        before it is used. Defaults to the identity function.\n      dropout: (optional) float, sets the dropout rate for DropConnect\n        regularization on the recurrent matrix. Defaults to 0.\n      zoneout: (optional) float, sets the zoneout rate for Zoneout\n        regularization. Defaults to 0.\n      dtype: (optional) the data type for this layer. Defaults to `tf.float32`.\n      name: (optional) string, the name for this layer.\n    """"""\n    super().__init__(GRULayer, num_units, direction, \'gru_cell\', **kwargs)\n'"
frameworks/tf/gru_cell.py,0,"b'# Copyright 2020 LMNT, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""A GRU cell compatible with the Haste GRU layer.""""""\n\n\nimport tensorflow as tf\n\nfrom tensorflow.compat import v1\nfrom tensorflow.compat.v1.nn import rnn_cell\n\n\nclass GRUCell(rnn_cell.RNNCell):\n  """"""\n  A GRU cell that\'s compatible with the Haste GRU layer.\n\n  This cell can be used on hardware other than GPUs and with other TensorFlow\n  classes that operate on RNN cells (e.g. `dynamic_rnn`, `BasicDecoder`, cell\n  wrappers, etc.).\n  """"""\n  def __init__(self, num_units, name=None, **kwargs):\n    super(GRUCell, self).__init__(name=name, **kwargs)\n\n    self.realname = name\n    self.num_units = num_units\n    self.built = False\n\n  @property\n  def state_size(self):\n    return self.num_units\n\n  @property\n  def output_size(self):\n    return self.num_units\n\n  def build(self, shape):\n    if self.built:\n      return\n\n    num_units = self.num_units\n    input_size = int(shape[-1])\n    dtype = self.dtype or tf.float32\n\n    kernel_initializer = v1.initializers.glorot_uniform(dtype=dtype)\n    bias_initializer = v1.initializers.zeros(dtype=dtype)\n\n    with tf.name_scope(self.name), v1.variable_scope(self.realname, \'gru_cell\'):\n      self._kernel = v1.get_variable(\'kernel\', initializer=lambda: kernel_initializer([input_size + num_units, num_units * 3]))\n      self._bias = v1.get_variable(\'bias\', initializer=lambda: bias_initializer([num_units * 6]))\n\n    self.kernel, self.recurrent_kernel = tf.split(self._kernel, [input_size, num_units], axis=0)\n    self.bias, self.recurrent_bias = tf.split(self._bias, 2, axis=0)\n\n    self.built = True\n\n  def __call__(self, inputs, state, scope=None):\n    self.build(inputs.shape)\n\n    h_proj = tf.nn.xw_plus_b(state, self.recurrent_kernel, self.recurrent_bias)\n    x = tf.nn.xw_plus_b(inputs, self.kernel, self.bias)\n    h_z, h_r, h_g = tf.split(h_proj, 3, axis=-1)\n    x_z, x_r, x_g = tf.split(x, 3, axis=-1)\n    z = tf.nn.sigmoid(h_z + x_z)\n    r = tf.nn.sigmoid(h_r + x_r)\n    g = tf.nn.tanh(r * h_g + x_g)\n    h = z * state + (1 - z) * g\n    return h, h\n'"
frameworks/tf/indrnn.py,0,"b'# Copyright 2020 LMNT, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Independently Recurrent Neural Network""""""\n\n\nimport pkg_resources\nimport tensorflow as tf\n\nfrom tensorflow.compat import v1\nfrom tensorflow.compat.v1.nn import rnn_cell\n\nfrom .base_rnn import BaseRNN\nfrom .weight_config import WeightConfig\n\n\n__all__ = [\n    \'IndRNN\'\n]\n\n\nLIB = tf.load_op_library(pkg_resources.resource_filename(__name__, \'libhaste_tf.so\'))\n\n\n@tf.RegisterGradient(""HasteIndrnn"")\ndef indrnn_gradient(op, *grads):\n  training = op.get_attr(\'training\')\n  if not training:\n    raise ValueError((\'IndRNN can only compute gradients if `training=True` was specified during \'\n                      \'the forward pass.\\nFailed op: {}\').format(op.name))\n\n  # Extract inputs and outputs from the op.\n  x = op.inputs[0]\n  W = op.inputs[1]\n  u = op.inputs[2]\n  b = op.inputs[3]\n  zoneout_mask = op.inputs[4]\n  h = op.outputs[0]\n\n  # Pre-transpose matrices for better performance.\n  x = tf.transpose(x, [2, 0, 1])\n  W = tf.transpose(W, [1, 0])\n\n  dx, dW, du, db = LIB.haste_indrnn_grad(x, W, u, b, zoneout_mask, h, grads[0])\n  return [dx, dW, du, db, None]\n\n\ndef _get_initializer(initializer):\n  if not isinstance(initializer, dict):\n    return initializer\n  if \'uniform\' in initializer:\n    value = initializer[\'uniform\']\n    return v1.initializers.random_uniform(-value, value)\n  if \'normal\' in initializer:\n    value = initializer[\'normal\']\n    return v1.initializers.truncated_normal(stddev=value)\n  raise ValueError(f\'Unknown initializer {initializer}\')\n\n\nclass IndRNNLayer(tf.Module):\n  def __init__(self,\n        num_units,\n        kernel_initializer=None,\n        recurrent_initializer=None,\n        bias_initializer=None,\n        kernel_transform=None,\n        recurrent_transform=None,\n        bias_transform=None,\n        zoneout=0.0,\n        dtype=None,\n        name=None):\n    super().__init__(name)\n    self.realname = name\n    self.num_units = num_units\n\n    identity = lambda x: x\n    self.kernel_config = WeightConfig(v1.initializers.glorot_uniform(), None, identity)\n    self.recurrent_config = WeightConfig(v1.initializers.random_uniform(-0.5, 0.5), None, identity)\n    self.bias_config = WeightConfig(v1.initializers.zeros(), None, identity)\n\n    self.kernel_config.override(_get_initializer(kernel_initializer), None, kernel_transform)\n    self.recurrent_config.override(_get_initializer(recurrent_initializer), None, recurrent_transform)\n    self.bias_config.override(_get_initializer(bias_initializer), None, bias_transform)\n\n    self.zoneout = zoneout\n    self.dtype = dtype or tf.float32\n    self.kernel = None\n    self.recurrent_scale = None\n    self.bias = None\n    self.recurrent_bias = None\n    self.built = False\n\n  def build(self, shape):\n    if self.built:\n      return\n\n    num_units = self.num_units\n    input_size = int(shape[-1])\n\n    kernel_shape = tf.TensorShape([input_size, num_units])\n    recurrent_shape = tf.TensorShape([num_units])\n    bias_shape = tf.TensorShape([num_units])\n\n    kernel_weights = self.kernel_config.initializer(kernel_shape, dtype=self.dtype)\n    recurrent_weights = self.recurrent_config.initializer(recurrent_shape, dtype=self.dtype)\n    biases = self.bias_config.initializer(bias_shape)\n\n    with self.name_scope, v1.variable_scope(self.realname, \'indrnn_cell\'):\n      self.kernel = v1.get_variable(\'kernel\', initializer=kernel_weights)\n      self.recurrent_scale = v1.get_variable(\'recurrent_scale\', initializer=recurrent_weights)\n      self.bias = v1.get_variable(\'bias\', initializer=biases)\n\n    self.built = True\n\n  def get_weights(self):\n    return {\n        \'kernel\': self.kernel_config.transform(self.kernel),\n        \'recurrent_scale\': self.recurrent_config.transform(self.recurrent_scale),\n        \'bias\': self.bias_config.transform(self.bias)\n    }\n\n  def __call__(self, inputs, sequence_length, training):\n    self.build(inputs.shape)\n\n    shape = tf.shape(inputs)\n    time_steps = shape[0]\n    batch_size = shape[1]\n\n    # Use an empty zoneout mask if no zoneout is going to be applied.\n    # Sadly, we can\'t pass `None` to the op but at least we won\'t be wasting\n    # memory or bandwidth on this tensor.\n    zoneout_mask = tf.zeros([0, 0, 0], dtype=self.dtype)\n    if self.zoneout:\n      zoneout_mask = 1.0 - self.zoneout\n      zoneout_mask += tf.random.uniform([time_steps, batch_size, self.num_units], dtype=self.dtype)\n      zoneout_mask = tf.floor(zoneout_mask)\n\n    weights = self.get_weights()\n    result = LIB.haste_indrnn(\n        inputs,\n        weights[\'kernel\'],\n        weights[\'recurrent_scale\'],\n        weights[\'bias\'],\n        zoneout_mask,\n        training=training,\n        zoneout_prob=self.zoneout)\n\n    if sequence_length is not None:\n      # 0-indexed tensors, so length-1.\n      indices = sequence_length\n      indices = tf.stack([indices, tf.range(batch_size, dtype=sequence_length.dtype)], axis=-1)\n      state = tf.gather_nd(result, indices)\n    else:\n      state = result[-1]\n\n    return result[1:], state\n\n\nclass IndRNN(BaseRNN):\n  """"""\n  Independently Recurrent Neural Network layer.\n\n  This layer offers a fused, GPU-accelerated TensorFlow op for inference and\n  training. It also supports Zoneout regularization.\n  """"""\n\n  def __init__(self, num_units, direction=\'unidirectional\', **kwargs):\n    """"""\n    Initialize the parameters of the IndRNN layer.\n\n    Arguments:\n      num_units: int, the number of units in the IndRNN cell.\n      direction: string, \'unidirectional\' or \'bidirectional\'.\n      **kwargs: Dict, keyword arguments (see below).\n\n    Keyword Arguments:\n      kernel_initializer: (optional) the initializer to use for the input\n        matrix weights. Defaults to `glorot_uniform`.\n      recurrent_initializer: (optional) the initializer to use for the\n        recurrent scale weights. Defaults to uniform random in [-0.5, 0.5].\n        Note that this initialization scheme is different than in the original\n        authors\' implementation. See https://github.com/lmnt-com/haste/issues/7\n        for details.\n      bias_initializer: (optional) the initializer to use for the bias vector.\n        Defaults to `zeros`.\n      kernel_transform: (optional) a function with signature\n        `(kernel: Tensor) -> Tensor` that transforms the kernel before it is\n        used. Defaults to the identity function.\n      recurrent_transform: (optional) a function with signature\n        `(recurrent_scale: Tensor) -> Tensor` that transforms the recurrent\n        scale vector before it is used. Defaults to the identity function.\n      bias_transform: (optional) a function with signature\n        `(bias: Tensor) -> Tensor` that transforms the bias before it is used.\n        Defaults to the identity function.\n      zoneout: (optional) float, sets the zoneout rate for Zoneout\n        regularization. Defaults to 0.\n      dtype: (optional) the data type for this layer. Defaults to `tf.float32`.\n      name: (optional) string, the name for this layer.\n    """"""\n    super().__init__(IndRNNLayer, num_units, direction, \'indrnn_cell\', **kwargs)\n'"
frameworks/tf/layer_norm.py,0,"b'# Copyright 2020 LMNT, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Layer Normalization""""""\n\n\nimport pkg_resources\nimport tensorflow as tf\n\nfrom tensorflow.compat import v1\n\n\n__all__ = [\n    \'LayerNorm\'\n]\n\n\nLIB = tf.load_op_library(pkg_resources.resource_filename(__name__, \'libhaste_tf.so\'))\n\n\n@tf.RegisterGradient(""HasteLayerNorm"")\ndef layer_norm_gradient(op, *grads):\n  x = op.inputs[0]\n  gamma = op.inputs[1]\n  beta = op.inputs[2]\n  cache = op.outputs[1]\n\n  return LIB.haste_layer_norm_grad(x, gamma, beta, grads[0], cache)\n\n\nclass LayerNorm(tf.Module):\n  """"""\n  Layer normalization layer.\n\n  This class exposes a fused and GPU-accelerated implementation of layer\n  normalization as described by [Ba et al.](https://arxiv.org/abs/1607.06450)\n  """"""\n\n  def __init__(self, name=None):\n    """"""\n    Initialize the parameters of the layer normalization layer.\n\n    Arguments:\n      name: (optional) string, the name for this layer.\n    """"""\n    super(LayerNorm, self).__init__(name)\n    self.realname = name\n    self.gamma = None\n    self.beta = None\n    self.built = False\n\n  def build(self, shape):\n    """"""\n    Creates the variables of the layer.\n\n    Calling this method is optional for users of the LayerNorm class. It is\n    called internally with the correct shape when `__call__` is invoked.\n\n    Arguments:\n      shape: instance of `TensorShape`.\n    """"""\n    if self.built:\n      return\n    hidden_size = int(shape[-1])\n    with self.name_scope, v1.variable_scope(self.realname, \'layer_norm\'):\n      self.gamma = v1.get_variable(\'gamma\', shape=[hidden_size], initializer=v1.initializers.ones())\n      self.beta = v1.get_variable(\'beta\', shape=[hidden_size], initializer=v1.initializers.zeros())\n    self.built = True\n\n  def __call__(self, x):\n    """"""\n    Runs the layer.\n\n    Arguments:\n      x: Tensor, a rank R tensor.\n\n    Returns:\n      y: Tensor, a rank R tensor with the last dimension normalized.\n    """"""\n    self.build(x.shape)\n    y, _ = LIB.haste_layer_norm(x, self.gamma, self.beta)\n    return y\n'"
frameworks/tf/layer_norm_gru.py,0,"b'# Copyright 2020 LMNT, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Layer Normalized Gated Recurrent Unit""""""\n\n\nimport pkg_resources\nimport tensorflow as tf\n\nfrom tensorflow.compat import v1\nfrom .base_rnn import BaseRNN\nfrom .weight_config import WeightConfig\n\n\n__all__ = [\n    \'LayerNormGRU\'\n]\n\n\nLIB = tf.load_op_library(pkg_resources.resource_filename(__name__, \'libhaste_tf.so\'))\n\n\n@tf.RegisterGradient(""HasteLayerNormGru"")\ndef layer_norm_gru_gradient(op, *grads):\n  training = op.get_attr(\'training\')\n  if not training:\n    raise ValueError((\'LayerNormGRU can only compute gradients if `training=True` was specified \'\n                      \'during the forward pass.\\nFailed op: {}\').format(op.name))\n\n  # Extract inputs and outputs from the op.\n  x = op.inputs[0]\n  W = op.inputs[1]\n  R = op.inputs[2]\n  bx = op.inputs[3]\n  br = op.inputs[4]\n  gamma = op.inputs[5]\n  zoneout_mask = op.inputs[6]\n  h = op.outputs[0]\n  cache = op.outputs[1]\n\n  # Pre-transpose matrices for better performance.\n  x = tf.transpose(x, [2, 0, 1])\n  W = tf.transpose(W, [1, 0])\n  R = tf.transpose(R, [1, 0])\n\n  grads = LIB.haste_layer_norm_gru_grad(x, W, R, bx, br, gamma, h, cache, grads[0], zoneout_mask)\n\n  return [*grads, None]\n\n\nclass LayerNormGRULayer(tf.Module):\n  def __init__(self,\n        num_units,\n        kernel_initializer=None,\n        recurrent_initializer=None,\n        bias_initializer=None,\n        recurrent_bias_initializer=None,\n        kernel_transform=None,\n        recurrent_transform=None,\n        bias_transform=None,\n        recurrent_bias_transform=None,\n        dropout=0.0,\n        zoneout=0.0,\n        dtype=None,\n        name=None):\n    super(LayerNormGRULayer, self).__init__(name)\n    self.realname = name\n    self.num_units = num_units\n\n    identity = lambda x: x\n    self.kernel_config = WeightConfig(v1.initializers.glorot_uniform(), None, identity)\n    self.recurrent_config = WeightConfig(v1.initializers.orthogonal(), None, identity)\n    self.bias_config = WeightConfig(v1.initializers.zeros(), None, identity)\n    self.recurrent_bias_config = WeightConfig(v1.initializers.zeros(), None, identity)\n\n    self.kernel_config.override(kernel_initializer, None, kernel_transform)\n    self.recurrent_config.override(recurrent_initializer, None, recurrent_transform)\n    self.bias_config.override(bias_initializer, None, bias_transform)\n    self.recurrent_bias_config.override(recurrent_bias_initializer, None, recurrent_bias_transform)\n\n    self.dropout = dropout\n    self.zoneout = zoneout\n    self.dtype = dtype or tf.float32\n    self.built = False\n\n  def build(self, shape):\n    if self.built:\n      return\n\n    num_units = self.num_units\n    input_size = int(shape[-1])\n\n    def build_weights(initializer, shape):\n      weights = [initializer(shape, dtype=self.dtype) for _ in range(3)]\n      weights = tf.concat(weights, axis=-1)\n      return weights\n\n    kernel_weights = build_weights(self.kernel_config.initializer, [input_size, num_units])\n    recurrent_weights = build_weights(self.recurrent_config.initializer, [num_units, num_units])\n    biases = build_weights(self.bias_config.initializer, [num_units])\n    recurrent_biases = build_weights(self.recurrent_bias_config.initializer, [num_units])\n\n    weights = tf.concat([kernel_weights, recurrent_weights], axis=0)\n    biases = tf.concat([biases, recurrent_biases], axis=0)\n\n    with self.name_scope, v1.variable_scope(self.realname, \'gru_cell\'):\n      self._kernel = v1.get_variable(\'kernel\', initializer=weights)\n      self._bias = v1.get_variable(\'bias\', initializer=biases)\n      self.gamma = v1.get_variable(\'gamma\', shape=[2, self.num_units * 3], initializer=v1.initializers.ones())\n    self.built = True\n\n  def get_weights(self):\n    input_size = self._kernel.shape.as_list()[0] - self.num_units\n    kernel, recurrent_kernel = tf.split(self._kernel, [input_size, self.num_units], axis=0)\n    bias, recurrent_bias = tf.split(self._bias, 2, axis=0)\n    return {\n        \'kernel\': self.kernel_config.transform(kernel),\n        \'recurrent_kernel\': self.recurrent_config.transform(recurrent_kernel),\n        \'bias\': self.bias_config.transform(bias),\n        \'recurrent_bias\': self.recurrent_bias_config.transform(recurrent_bias),\n        \'gamma\': self.gamma,\n    }\n\n  def __call__(self, inputs, sequence_length, training):\n    self.build(inputs.shape)\n\n    shape = tf.shape(inputs)\n    time_steps = shape[0]\n    batch_size = shape[1]\n\n    # Use an empty zoneout mask if no zoneout is going to be applied.\n    # Sadly, we can\'t pass `None` to the op but at least we won\'t be wasting\n    # memory or bandwidth on this tensor.\n    zoneout_mask = tf.zeros([0, 0, 0], dtype=self.dtype)\n    if self.zoneout:\n      zoneout_mask = 1.0 - self.zoneout\n      zoneout_mask += tf.random.uniform([time_steps, batch_size, self.num_units], dtype=self.dtype)\n      zoneout_mask = tf.floor(zoneout_mask)\n\n    weights = self.get_weights()\n    result, _ = LIB.haste_layer_norm_gru(\n        inputs,\n        weights[\'kernel\'],\n        tf.nn.dropout(weights[\'recurrent_kernel\'], rate=self.dropout),\n        weights[\'bias\'],\n        weights[\'recurrent_bias\'],\n        weights[\'gamma\'],\n        zoneout_mask,\n        training=training,\n        zoneout_prob=self.zoneout)\n\n    if sequence_length is not None:\n      # 0-indexed tensors, so length-1.\n      indices = sequence_length\n      indices = tf.stack([indices, tf.range(batch_size, dtype=sequence_length.dtype)], axis=-1)\n      state = tf.gather_nd(result, indices)\n    else:\n      state = result[-1]\n\n    return result[1:], state\n\n\nclass LayerNormGRU(BaseRNN):\n  """"""\n  Layer Normalized Gated Recurrent Unit layer.\n\n  This GRU layer applies layer normalization to the input and recurrent output\n  activations of a standard GRU. The implementation is fused and\n  GPU-accelerated. There are two commonly-used variants of GRU cells. This one\n  implements 1406.1078v1 which applies the reset gate to the hidden state\n  after matrix multiplication. The other variant, 1406.1078v3, applies the\n  reset gate before matrix multiplication and is currently unsupported.\n\n  This layer has built-in support for DropConnect and Zoneout, which are\n  both techniques used to regularize RNNs.\n  """"""\n\n  def __init__(self, num_units, direction=\'unidirectional\', **kwargs):\n    """"""\n    Initialize the parameters of the GRU layer.\n\n    Arguments:\n      num_units: int, the number of units in the GRU cell.\n      direction: string, \'unidirectional\' or \'bidirectional\'.\n      **kwargs: Dict, keyword arguments (see below).\n\n    Keyword Arguments:\n      kernel_initializer: (optional) the initializer to use for the input\n        matrix weights. Defaults to `glorot_uniform`.\n      recurrent_initializer: (optional) the initializer to use for the\n        recurrent matrix weights. Defaults to `orthogonal`.\n      bias_initializer: (optional) the initializer to use for input bias\n        vectors. Defaults to `zeros`.\n      recurrent_bias_initializer: (optional) the initializer to use for\n        recurrent bias vectors. Defaults to `zeros`.\n      kernel_transform: (optional) a function with signature\n        `(kernel: Tensor) -> Tensor` that transforms the kernel before it is\n        used. Defaults to the identity function.\n      recurrent_transform: (optional) a function with signature\n        `(recurrent_kernel: Tensor) -> Tensor` that transforms the recurrent\n        kernel before it is used. Defaults to the identity function.\n      bias_transform: (optional) a function with signature\n        `(bias: Tensor) -> Tensor` that transforms the bias before it is used.\n        Defaults to the identity function.\n      recurrent_bias_transform: (optional) a function with signature\n        `(recurrent_bias: Tensor) -> Tensor` that transforms the recurrent bias\n        before it is used. Defaults to the identity function.\n      dropout: (optional) float, sets the dropout rate for DropConnect\n        regularization on the recurrent matrix. Defaults to 0.\n      zoneout: (optional) float, sets the zoneout rate for Zoneout\n        regularization. Defaults to 0.\n      dtype: (optional) the data type for this layer. Defaults to `tf.float32`.\n      name: (optional) string, the name for this layer.\n    """"""\n    super().__init__(LayerNormGRULayer, num_units, direction, \'gru_cell\', **kwargs)\n'"
frameworks/tf/layer_norm_gru_cell.py,0,"b'# Copyright 2020 LMNT, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""A GRU cell compatible with the Haste LayerNormGRU layer.""""""\n\n\nimport tensorflow as tf\n\nfrom tensorflow.compat import v1\nfrom tensorflow.compat.v1.nn import rnn_cell\n\n\n__all__ = [\n    \'LayerNormGRUCell\'\n]\n\n\nclass LayerNormGRUCell(rnn_cell.RNNCell):\n  """"""\n  A GRU cell that\'s compatible with the Haste LayerNormGRU layer.\n\n  This cell can be used on hardware other than GPUs and with other TensorFlow\n  classes that operate on RNN cells (e.g. `dynamic_rnn`, `BasicDecoder`, cell\n  wrappers, etc.).\n  """"""\n\n  def __init__(self,\n        num_units,\n        forget_bias=1.0,\n        dropout=0.0,\n        dtype=None,\n        name=None,\n        **kwargs):\n    super(LayerNormGRUCell, self).__init__(dtype=dtype, name=name, **kwargs)\n    self.realname = name\n    self.num_units = num_units\n\n    self.forget_bias = forget_bias\n    self.dropout = dropout\n    self.kernel = None\n    self.recurrent_kernel = None\n    self.bias = None\n    self.recurrent_bias = None\n    self.gamma = None\n    self.built = False\n\n  @property\n  def state_size(self):\n    return self.num_units\n\n  @property\n  def output_size(self):\n    return self.num_units\n\n  def build(self, shape):\n    if self.built:\n      return\n\n    num_units = self.num_units\n    input_size = int(shape[-1])\n    dtype = self.dtype or tf.float32\n\n    kernel_initializer = v1.initializers.glorot_uniform(dtype=dtype)\n    bias_initializer = v1.initializers.zeros(dtype=dtype)\n\n    with tf.name_scope(self.name), v1.variable_scope(self.realname, \'gru_cell\'):\n      self._kernel = v1.get_variable(\'kernel\', initializer=lambda: kernel_initializer([input_size + num_units, num_units * 3]))\n      self._bias = v1.get_variable(\'bias\', initializer=lambda: bias_initializer([num_units * 6]))\n      self.gamma = v1.get_variable(\'gamma\', initializer=v1.initializers.ones())\n\n    self.kernel, self.recurrent_kernel = tf.split(self._kernel, [input_size, num_units], axis=0)\n    self.bias, self.recurrent_bias = tf.split(self._bias, 2, axis=0)\n\n    self.built = True\n\n  def __call__(self, inputs, state, scope=None):\n    self.build(inputs.shape)\n\n    x = self._layer_norm(inputs @ self.kernel, self.gamma[0]) + self.bias\n    h_proj = self._layer_norm(state @ self.recurrent_kernel, self.gamma[1]) + self.recurrent_bias\n    h_z, h_r, h_g = tf.split(h_proj, 3, axis=-1)\n    x_z, x_r, x_g = tf.split(x, 3, axis=-1)\n    z = tf.nn.sigmoid(h_z + x_z)\n    r = tf.nn.sigmoid(h_r + x_r)\n    g = tf.nn.tanh(r * h_g + x_g)\n    h = z * state + (1 - z) * g\n    return h, h\n\n  def _layer_norm(self, x, gamma):\n    mean, variance = tf.nn.moments(x, axes=[-1], keepdims=True)\n    return tf.nn.batch_normalization(x, mean, variance, None, gamma, 1e-5)\n'"
frameworks/tf/layer_norm_indrnn.py,0,"b'# Copyright 2020 LMNT, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Layer Normalized Independently Recurrent Neural Network""""""\n\n\nimport pkg_resources\nimport tensorflow as tf\n\nfrom tensorflow.compat import v1\nfrom tensorflow.compat.v1.nn import rnn_cell\n\nfrom .base_rnn import BaseRNN\nfrom .weight_config import WeightConfig\n\n\n__all__ = [\n    \'LayerNormIndRNN\'\n]\n\n\nLIB = tf.load_op_library(pkg_resources.resource_filename(__name__, \'libhaste_tf.so\'))\n\n\n@tf.RegisterGradient(""HasteLayerNormIndrnn"")\ndef layer_norm_indrnn_gradient(op, *grads):\n  training = op.get_attr(\'training\')\n  if not training:\n    raise ValueError((\'LayerNormIndRNN can only compute gradients if `training=True` was specified \'\n                      \'during the forward pass.\\nFailed op: {}\').format(op.name))\n\n  # Extract inputs and outputs from the op.\n  x = op.inputs[0]\n  W = op.inputs[1]\n  u = op.inputs[2]\n  b = op.inputs[3]\n  gamma = op.inputs[4]\n  zoneout_mask = op.inputs[5]\n  h = op.outputs[0]\n  cache = op.outputs[1]\n\n  # Pre-transpose matrices for better performance.\n  x = tf.transpose(x, [2, 0, 1])\n  W = tf.transpose(W, [1, 0])\n\n  grads = LIB.haste_layer_norm_indrnn_grad(x, W, u, b, gamma, zoneout_mask, h, cache, grads[0])\n  return [*grads, None]\n\n\ndef _get_initializer(initializer):\n  if not isinstance(initializer, dict):\n    return initializer\n  if \'uniform\' in initializer:\n    value = initializer[\'uniform\']\n    return v1.initializers.random_uniform(-value, value)\n  if \'normal\' in initializer:\n    value = initializer[\'normal\']\n    return v1.initializers.truncated_normal(stddev=value)\n  raise ValueError(f\'Unknown initializer {initializer}\')\n\n\nclass LayerNormIndRNNLayer(tf.Module):\n  def __init__(self,\n        num_units,\n        kernel_initializer=None,\n        recurrent_initializer=None,\n        bias_initializer=None,\n        kernel_transform=None,\n        recurrent_transform=None,\n        bias_transform=None,\n        zoneout=0.0,\n        dtype=None,\n        name=None):\n    super().__init__(name)\n    self.realname = name\n    self.num_units = num_units\n\n    identity = lambda x: x\n    self.kernel_config = WeightConfig(v1.initializers.glorot_uniform(), None, identity)\n    self.recurrent_config = WeightConfig(v1.initializers.random_uniform(-0.5, 0.5), None, identity)\n    self.bias_config = WeightConfig(v1.initializers.zeros(), None, identity)\n\n    self.kernel_config.override(_get_initializer(kernel_initializer), None, kernel_transform)\n    self.recurrent_config.override(_get_initializer(recurrent_initializer), None, recurrent_transform)\n    self.bias_config.override(_get_initializer(bias_initializer), None, bias_transform)\n\n    self.zoneout = zoneout\n    self.dtype = dtype or tf.float32\n    self.kernel = None\n    self.recurrent_scale = None\n    self.bias = None\n    self.gamma = None\n    self.recurrent_bias = None\n    self.built = False\n\n  def build(self, shape):\n    if self.built:\n      return\n\n    num_units = self.num_units\n    input_size = int(shape[-1])\n\n    kernel_shape = tf.TensorShape([input_size, num_units])\n    recurrent_shape = tf.TensorShape([num_units])\n    bias_shape = tf.TensorShape([num_units])\n\n    kernel_weights = self.kernel_config.initializer(kernel_shape, dtype=self.dtype)\n    recurrent_weights = self.recurrent_config.initializer(recurrent_shape, dtype=self.dtype)\n    biases = self.bias_config.initializer(bias_shape)\n\n    with self.name_scope, v1.variable_scope(self.realname, \'indrnn_cell\'):\n      self.kernel = v1.get_variable(\'kernel\', initializer=kernel_weights)\n      self.recurrent_scale = v1.get_variable(\'recurrent_scale\', initializer=recurrent_weights)\n      self.bias = v1.get_variable(\'bias\', initializer=biases)\n      self.gamma = v1.get_variable(\'gamma\', shape=[2, self.num_units], initializer=v1.initializers.ones())\n    self.built = True\n\n  def get_weights(self):\n    return {\n        \'kernel\': self.kernel_config.transform(self.kernel),\n        \'recurrent_scale\': self.recurrent_config.transform(self.recurrent_scale),\n        \'bias\': self.bias_config.transform(self.bias),\n        \'gamma\': self.gamma,\n    }\n\n  def __call__(self, inputs, sequence_length, training):\n    self.build(inputs.shape)\n\n    shape = tf.shape(inputs)\n    time_steps = shape[0]\n    batch_size = shape[1]\n\n    # Use an empty zoneout mask if no zoneout is going to be applied.\n    # Sadly, we can\'t pass `None` to the op but at least we won\'t be wasting\n    # memory or bandwidth on this tensor.\n    zoneout_mask = tf.zeros([0, 0, 0], dtype=self.dtype)\n    if self.zoneout:\n      zoneout_mask = 1.0 - self.zoneout\n      zoneout_mask += tf.random.uniform([time_steps, batch_size, self.num_units], dtype=self.dtype)\n      zoneout_mask = tf.floor(zoneout_mask)\n\n    weights = self.get_weights()\n    result, _ = LIB.haste_layer_norm_indrnn(\n        inputs,\n        weights[\'kernel\'],\n        weights[\'recurrent_scale\'],\n        weights[\'bias\'],\n        weights[\'gamma\'],\n        zoneout_mask,\n        training=training,\n        zoneout_prob=self.zoneout)\n\n    if sequence_length is not None:\n      # 0-indexed tensors, so length-1.\n      indices = sequence_length\n      indices = tf.stack([indices, tf.range(batch_size, dtype=sequence_length.dtype)], axis=-1)\n      state = tf.gather_nd(result, indices)\n    else:\n      state = result[-1]\n\n    return result[1:], state\n\n\nclass LayerNormIndRNN(BaseRNN):\n  """"""\n  Layer Normalized Independently Recurrent Neural Network layer.\n\n  This IndRNN layer applies layer normalization to the input activations of a\n  standard IndRNN. The implementation is fused and GPU-accelerated.\n\n  This layer has built-in support for Zoneout regularization.\n  """"""\n\n  def __init__(self, num_units, direction=\'unidirectional\', **kwargs):\n    """"""\n    Initialize the parameters of the IndRNN layer.\n\n    Arguments:\n      num_units: int, the number of units in the IndRNN cell.\n      direction: string, \'unidirectional\' or \'bidirectional\'.\n      **kwargs: Dict, keyword arguments (see below).\n\n    Keyword Arguments:\n      kernel_initializer: (optional) the initializer to use for the input\n        matrix weights. Defaults to `glorot_uniform`.\n      recurrent_initializer: (optional) the initializer to use for the\n        recurrent scale weights. Defaults to uniform random in [-0.5, 0.5].\n        Note that this initialization scheme is different than in the original\n        authors\' implementation. See https://github.com/lmnt-com/haste/issues/7\n        for details.\n      bias_initializer: (optional) the initializer to use for the bias vector.\n        Defaults to `zeros`.\n      kernel_transform: (optional) a function with signature\n        `(kernel: Tensor) -> Tensor` that transforms the kernel before it is\n        used. Defaults to the identity function.\n      recurrent_transform: (optional) a function with signature\n        `(recurrent_scale: Tensor) -> Tensor` that transforms the recurrent\n        scale vector before it is used. Defaults to the identity function.\n      bias_transform: (optional) a function with signature\n        `(bias: Tensor) -> Tensor` that transforms the bias before it is used.\n        Defaults to the identity function.\n      zoneout: (optional) float, sets the zoneout rate for Zoneout\n        regularization. Defaults to 0.\n      dtype: (optional) the data type for this layer. Defaults to `tf.float32`.\n      name: (optional) string, the name for this layer.\n    """"""\n    super().__init__(LayerNormIndRNNLayer, num_units, direction, \'indrnn_cell\', **kwargs)\n'"
frameworks/tf/layer_norm_lstm.py,0,"b'# Copyright 2020 LMNT, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Layer Normalized Long Short-Term Memory""""""\n\n\nimport pkg_resources\nimport tensorflow as tf\n\nfrom tensorflow.compat import v1\nfrom tensorflow.compat.v1.nn import rnn_cell\nfrom .base_rnn import BaseRNN\nfrom .weight_config import WeightConfig\n\n\n__all__ = [\n    \'LayerNormLSTM\'\n]\n\n\nLIB = tf.load_op_library(pkg_resources.resource_filename(__name__, \'libhaste_tf.so\'))\n\n\n@tf.RegisterGradient(""HasteLayerNormLstm"")\ndef lstm_gradient(op, *grads):\n  training = op.get_attr(\'training\')\n  if not training:\n    raise ValueError((\'LSTM can only compute gradients if `training=True` was specified during the \'\n                      \'forward pass.\\nFailed op: {}\').format(op.name))\n\n  # Extract inputs and outputs from the op.\n  x = op.inputs[0]\n  W = op.inputs[1]\n  R = op.inputs[2]\n  b = op.inputs[3]\n  gamma = op.inputs[4]\n  gamma_h = op.inputs[5]\n  beta_h = op.inputs[6]\n  zoneout_mask = op.inputs[7]\n  h = op.outputs[0]\n  c = op.outputs[1]\n  cache = op.outputs[2]\n\n  # Pre-transpose matrices for better performance.\n  x = tf.transpose(x, [2, 0, 1])\n  W = tf.transpose(W, [1, 0])\n  R = tf.transpose(R, [1, 0])\n\n  dx, dW, dR, db, dgamma, dgamma_h, dbeta_h = LIB.haste_layer_norm_lstm_grad(\n      x,\n      W,\n      R,\n      b,\n      gamma,\n      gamma_h,\n      beta_h,\n      h,\n      c,\n      cache,\n      grads[0],\n      grads[1],\n      zoneout_mask)\n  return [dx, dW, dR, db, dgamma, dgamma_h, dbeta_h, None]\n\n\nclass LayerNormLSTMLayer(tf.Module):\n  def __init__(self,\n        num_units,\n        kernel_initializer=None,\n        recurrent_initializer=None,\n        bias_initializer=None,\n        kernel_transform=None,\n        recurrent_transform=None,\n        bias_transform=None,\n        forget_bias=1.0,\n        dropout=0.0,\n        zoneout=0.0,\n        dtype=None,\n        name=None):\n    super(LayerNormLSTMLayer, self).__init__(name)\n    self.realname = name\n    self.num_units = num_units\n\n    identity = lambda x: x\n    self.kernel_config = WeightConfig(v1.initializers.glorot_uniform(), None, identity)\n    self.recurrent_config = WeightConfig(v1.initializers.orthogonal(), None, identity)\n    self.bias_config = WeightConfig(v1.initializers.zeros(), None, identity)\n\n    self.kernel_config.override(kernel_initializer, None, kernel_transform)\n    self.recurrent_config.override(recurrent_initializer, None, recurrent_transform)\n    self.bias_config.override(bias_initializer, None, bias_transform)\n\n    self.forget_bias = forget_bias\n    self.dropout = dropout\n    self.zoneout = zoneout\n    self.dtype = dtype or tf.float32\n    self.kernel = None\n    self.bias = None\n    self.gamma = None\n    self.gamma_h = None\n    self.beta_h = None\n    self.built = False\n\n  def build(self, shape):\n    if self.built:\n      return\n\n    num_units = self.num_units\n    input_size = int(shape[-1])\n\n    kernel_shape = tf.TensorShape([input_size, num_units])\n    recurrent_shape = tf.TensorShape([num_units, num_units])\n    bias_shape = tf.TensorShape([num_units])\n\n    kernel_weights = [self.kernel_config.initializer(kernel_shape, dtype=self.dtype) for _ in range(4)]\n    recurrent_weights = [self.recurrent_config.initializer(recurrent_shape, dtype=self.dtype) for _ in range(4)]\n    if self.forget_bias:\n      biases = [tf.zeros(bias_shape, dtype=self.dtype) for _ in range(4)]\n      biases[2] = tf.constant(self.forget_bias, shape=bias_shape, dtype=self.dtype)\n    else:\n      biases = [self.bias_config.initializer(bias_shape, dtype=self.dtype) for _ in range(4)]\n\n    kernel_weights = tf.concat(kernel_weights, axis=-1)\n    recurrent_weights = tf.concat(recurrent_weights, axis=-1)\n    biases = tf.concat(biases, axis=-1)\n\n    # Use the same format as LSTMBlockCell.\n    with self.name_scope, v1.variable_scope(self.realname, \'lstm_cell\'):\n      weights = tf.concat([kernel_weights, recurrent_weights], axis=0)\n      self.kernel = v1.get_variable(\'kernel\', initializer=weights)\n      self.bias = v1.get_variable(\'bias\', initializer=biases)\n      self.gamma = v1.get_variable(\'gamma\', shape=[2, self.num_units * 4], initializer=v1.initializers.ones())\n      self.gamma_h = v1.get_variable(\'gamma_h\', shape=[self.num_units], initializer=v1.initializers.ones())\n      self.beta_h = v1.get_variable(\'beta_h\', shape=[self.num_units], initializer=v1.initializers.zeros())\n    self.built = True\n\n  def get_weights(self):\n    kernel = self.kernel[:-self.num_units]\n    recurrent_kernel = self.kernel[-self.num_units:]\n    return {\n        \'kernel\': self.kernel_config.transform(kernel),\n        \'recurrent_kernel\': self.recurrent_config.transform(recurrent_kernel),\n        \'bias\': self.bias_config.transform(self.bias),\n        \'gamma\': self.gamma,\n        \'gamma_h\': self.gamma_h,\n        \'beta_h\': self.beta_h,\n    }\n\n  @property\n  def state_size(self):\n    return rnn_cell.LSTMStateTuple(self.num_units, self.num_units)\n\n  @property\n  def output_size(self):\n    return self.num_units\n\n  def __call__(self, x, sequence_length, training):\n    self.build(x.shape)\n\n    shape = tf.shape(x)\n    time_steps = shape[0]\n    batch_size = shape[1]\n\n    # Use an empty zoneout mask if no zoneout is going to be applied.\n    # Sadly, we can\'t pass `None` to the op but at least we won\'t be wasting\n    # memory or bandwidth on this tensor.\n    zoneout_mask = tf.zeros([0, 0, 0], dtype=self.dtype)\n    if self.zoneout:\n      zoneout_mask = 1.0 - self.zoneout\n      zoneout_mask += tf.random.uniform([time_steps, batch_size, self.num_units], dtype=self.dtype)\n      zoneout_mask = tf.floor(zoneout_mask)\n\n    weights = self.get_weights()\n    h, c, _ = LIB.haste_layer_norm_lstm(\n        x,\n        weights[\'kernel\'],\n        tf.nn.dropout(weights[\'recurrent_kernel\'], rate=self.dropout),\n        weights[\'bias\'],\n        weights[\'gamma\'],\n        weights[\'gamma_h\'],\n        weights[\'beta_h\'],\n        zoneout_mask,\n        training=training,\n        zoneout_prob=self.zoneout)\n\n    if sequence_length is not None:\n      indices = sequence_length\n      indices = tf.stack([indices, tf.range(batch_size, dtype=sequence_length.dtype)], axis=-1)\n      state = rnn_cell.LSTMStateTuple(tf.gather_nd(c, indices), tf.gather_nd(h, indices))\n    else:\n      state = rnn_cell.LSTMStateTuple(c[-1], h[-1])\n\n    return h[1:], state\n\n\nclass LayerNormLSTM(BaseRNN):\n  """"""\n  Layer Normalized Long Short-Term Memory layer.\n\n  This LSTM layer applies layer normalization to the input, recurrent, and\n  output activations of a standard LSTM. The implementation is fused and\n  GPU-accelerated. DropConnect and Zoneout regularization are built-in, and\n  this layer allows setting a non-zero initial forget gate bias.\n\n  Details about the exact function this layer implements can be found at\n  https://github.com/lmnt-com/haste/issues/1.\n  """"""\n\n  def __init__(self, num_units, direction=\'unidirectional\', **kwargs):\n    """"""\n    Initialize the parameters of the LSTM layer.\n\n    Arguments:\n      num_units: int, the number of units in the LSTM cell.\n      direction: string, \'unidirectional\' or \'bidirectional\'.\n      **kwargs: Dict, keyword arguments (see below).\n\n    Keyword Arguments:\n      kernel_initializer: (optional) the initializer to use for the input\n        matrix weights. Defaults to `glorot_uniform`.\n      recurrent_initializer: (optional) the initializer to use for the\n        recurrent matrix weights. Defaults to `orthogonal`.\n      bias_initializer: (optional) the initializer to use for both input and\n        recurrent bias vectors. Defaults to `zeros` unless `forget_bias` is\n        non-zero (see below).\n      kernel_transform: (optional) a function with signature\n        `(kernel: Tensor) -> Tensor` that transforms the kernel before it is\n        used. Defaults to the identity function.\n      recurrent_transform: (optional) a function with signature\n        `(recurrent_kernel: Tensor) -> Tensor` that transforms the recurrent\n        kernel before it is used. Defaults to the identity function.\n      bias_transform: (optional) a function with signature\n        `(bias: Tensor) -> Tensor` that transforms the bias before it is used.\n        Defaults to the identity function.\n      forget_bias: (optional) float, sets the initial weights for the forget\n        gates. Defaults to 1 and overrides the `bias_initializer` unless this\n        argument is set to 0.\n      dropout: (optional) float, sets the dropout rate for DropConnect\n        regularization on the recurrent matrix. Defaults to 0.\n      zoneout: (optional) float, sets the zoneout rate for Zoneout\n        regularization. Defaults to 0.\n      dtype: (optional) the data type for this layer. Defaults to `tf.float32`.\n      name: (optional) string, the name for this layer.\n    """"""\n    super().__init__(LayerNormLSTMLayer, num_units, direction, \'lstm_cell\', **kwargs)\n'"
frameworks/tf/layer_norm_lstm_cell.py,0,"b'# Copyright 2020 LMNT, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""An LSTM cell compatible with the Haste LayerNormLSTM layer.""""""\n\n\nimport tensorflow as tf\n\nfrom tensorflow.compat import v1\nfrom tensorflow.compat.v1.nn import rnn_cell\n\n\n__all__ = [\n    \'LayerNormLSTMCell\'\n]\n\n\nclass LayerNormLSTMCell(rnn_cell.RNNCell):\n  """"""\n  An LSTM cell that\'s compatible with the Haste LayerNormLSTM layer.\n\n  This cell can be used on hardware other than GPUs and with other TensorFlow\n  classes that operate on RNN cells (e.g. `dynamic_rnn`, `BasicDecoder`, cell\n  wrappers, etc.).\n  """"""\n\n  def __init__(self,\n        num_units,\n        forget_bias=1.0,\n        dropout=0.0,\n        dtype=None,\n        name=None,\n        **kwargs):\n    super(LayerNormLSTMCell, self).__init__(dtype=dtype, name=name, **kwargs)\n    self.realname = name\n    self.num_units = num_units\n\n    self.forget_bias = forget_bias\n    self.dropout = dropout\n    self.kernel = None\n    self.recurrent_kernel = None\n    self.bias = None\n    self.gamma = None\n    self.gamma_h = None\n    self.beta_h = None\n    self.built = False\n\n  @property\n  def state_size(self):\n    return rnn_cell.LSTMStateTuple(self.num_units, self.num_units)\n\n  @property\n  def output_size(self):\n    return self.num_units\n\n  def build(self, shape):\n    num_units = self.num_units\n    input_size = int(shape[-1])\n\n    # No user-supplied initializers here since this class should only really\n    # be used for inference on a pre-trained model.\n    with tf.name_scope(self.name), v1.variable_scope(self.realname, \'lstm_cell\'):\n      self._kernel = v1.get_variable(\'kernel\', shape=[input_size + num_units, num_units * 4])\n      self.kernel, self.recurrent_kernel = tf.split(self._kernel, [input_size, num_units], axis=0)\n      self.bias = v1.get_variable(\'bias\', shape=[num_units * 4], initializer=v1.initializers.zeros())\n      self.gamma = v1.get_variable(\'gamma\', shape=[2, num_units * 4], initializer=v1.initializers.ones())\n      self.gamma_h = v1.get_variable(\'gamma_h\', shape=[num_units], initializer=v1.initializers.ones())\n      self.beta_h = v1.get_variable(\'beta_h\', shape=[num_units], initializer=v1.initializers.zeros())\n      self.null = tf.zeros_like(self.gamma[0])\n    self.built = True\n\n  def __call__(self, inputs, state, scope=None):\n    self.build(inputs.shape)\n\n    R = tf.nn.dropout(self.recurrent_kernel, rate=self.dropout)\n\n    Wx = self._layer_norm(tf.matmul(inputs, self.kernel), self.gamma[0], self.null)\n    Rh = self._layer_norm(tf.matmul(state.h, R), self.gamma[1], self.null)\n    v = Wx + Rh + self.bias\n    v_i, v_g, v_f, v_o = tf.split(v, 4, axis=-1)\n    i = tf.nn.sigmoid(v_i)\n    g = tf.nn.tanh   (v_g)\n    f = tf.nn.sigmoid(v_f)\n    o = tf.nn.sigmoid(v_o)\n    c_new = f * state.c + i * g\n    c_tanh = tf.nn.tanh(self._layer_norm(c_new, self.gamma_h, self.beta_h))\n    h_new = o * c_tanh\n\n    return h_new, rnn_cell.LSTMStateTuple(c_new, h_new)\n\n  def _layer_norm(self, x, gamma, beta):\n    mean, variance = tf.nn.moments(x, axes=[-1], keepdims=True)\n    return tf.nn.batch_normalization(x, mean, variance, beta, gamma, 1e-5)\n'"
frameworks/tf/lstm.py,0,"b'# Copyright 2020 LMNT, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""Long Short-Term Memory""""""\n\n\nimport pkg_resources\nimport tensorflow as tf\n\nfrom tensorflow.compat import v1\nfrom tensorflow.compat.v1.nn import rnn_cell\n\nfrom .base_rnn import BaseRNN\nfrom .weight_config import WeightConfig\n\n\n__all__ = [\n    \'LSTM\'\n]\n\n\nLIB = tf.load_op_library(pkg_resources.resource_filename(__name__, \'libhaste_tf.so\'))\n\n\n@tf.RegisterGradient(""HasteLstm"")\ndef lstm_gradient(op, *grads):\n  training = op.get_attr(\'training\')\n  if not training:\n    raise ValueError((\'LSTM can only compute gradients if `training=True` was specified during the \'\n                      \'forward pass.\\nFailed op: {}\').format(op.name))\n\n  # Extract inputs and outputs from the op.\n  x = op.inputs[0]\n  W = op.inputs[1]\n  R = op.inputs[2]\n  b = op.inputs[3]\n  zoneout_mask = op.inputs[4]\n  h = op.outputs[0]\n  c = op.outputs[1]\n  v = op.outputs[2]\n\n  # Pre-transpose matrices for better performance.\n  x = tf.transpose(x, [2, 0, 1])\n  W = tf.transpose(W, [1, 0])\n  R = tf.transpose(R, [1, 0])\n\n  dx, dW, dR, db = LIB.haste_lstm_grad(x, W, R, b, h, c, v, grads[0], grads[1], zoneout_mask)\n  return [dx, dW, dR, db, None]\n\n\nclass LSTMLayer(tf.Module):\n  def __init__(self,\n        num_units,\n        kernel_initializer=None,\n        recurrent_initializer=None,\n        bias_initializer=None,\n        kernel_transform=None,\n        recurrent_transform=None,\n        bias_transform=None,\n        forget_bias=1.0,\n        dropout=0.0,\n        zoneout=0.0,\n        dtype=None,\n        name=None,\n        cudnn_compat=False):\n    super(LSTMLayer, self).__init__(name)\n    self.realname = name\n    self.input_size = None\n    self.num_units = num_units\n\n    identity = lambda x: x\n    self.kernel_config = WeightConfig(v1.initializers.glorot_uniform(), None, identity)\n    self.recurrent_config = WeightConfig(v1.initializers.orthogonal(), None, identity)\n    self.bias_config = WeightConfig(v1.initializers.zeros(), None, identity)\n\n    self.kernel_config.override(kernel_initializer, None, kernel_transform)\n    self.recurrent_config.override(recurrent_initializer, None, recurrent_transform)\n    self.bias_config.override(bias_initializer, None, bias_transform)\n\n    self.forget_bias = forget_bias\n    self.dropout = dropout\n    self.zoneout = zoneout\n    self.dtype = dtype or tf.float32\n    self.cudnn_compat = cudnn_compat\n    self.opaque = None\n    self.kernel = None\n    self.bias = None\n    self.built = False\n\n  def build(self, shape):\n    if self.built:\n      return\n\n    num_units = self.num_units\n    input_size = int(shape[-1])\n\n    kernel_shape = tf.TensorShape([input_size, num_units])\n    recurrent_shape = tf.TensorShape([num_units, num_units])\n    bias_shape = tf.TensorShape([num_units])\n\n    kernel_weights = [self.kernel_config.initializer(kernel_shape, dtype=self.dtype) for _ in range(4)]\n    recurrent_weights = [self.recurrent_config.initializer(recurrent_shape, dtype=self.dtype) for _ in range(4)]\n    if self.forget_bias:\n      biases = [tf.zeros(bias_shape, dtype=self.dtype) for _ in range(4)]\n      biases[2] = tf.constant(self.forget_bias, shape=bias_shape, dtype=self.dtype)\n    else:\n      biases = [self.bias_config.initializer(bias_shape, dtype=self.dtype) for _ in range(4)]\n\n    kernel_weights = tf.concat(kernel_weights, axis=-1)\n    recurrent_weights = tf.concat(recurrent_weights, axis=-1)\n    biases = tf.concat(biases, axis=-1)\n\n    if not self.cudnn_compat:\n      # Use the same format as LSTMBlockCell.\n      with self.name_scope, v1.variable_scope(self.realname, \'lstm_cell\'):\n        weights = tf.concat([kernel_weights, recurrent_weights], axis=0)\n        self.kernel = v1.get_variable(\'kernel\', initializer=weights)\n        self.bias = v1.get_variable(\'bias\', initializer=biases)\n    else:\n      # Use the same format as CudnnLSTM.\n      with self.name_scope, v1.variable_scope(self.realname, \'lstm_cell\'):\n        with v1.variable_scope(\'cudnn_lstm\'):\n          # Sigh, cuDNN uses two bias vectors instead of just one.\n          extra_biases = [self.bias_initializer(tf.TensorShape([num_units]), dtype=self.dtype) for _ in range(4)]\n          extra_biases = tf.concat(extra_biases, axis=-1)\n          kernel_weights = tf.reshape(kernel_weights, [-1])\n          recurrent_weights = tf.reshape(recurrent_weights, [-1])\n          opaque_initial_value = tf.concat([kernel_weights, recurrent_weights, biases, extra_biases], axis=-1)\n          self.opaque = v1.get_variable(\'opaque_kernel\', initializer=opaque_initial_value)\n\n    self.input_size = input_size\n    self.built = True\n\n  def get_weights(self):\n    if self.cudnn_compat:\n      # Split into 3 variables.\n      W_size = 4 * self.input_size * self.num_units\n      R_size = 4 * self.num_units * self.num_units\n      b_size = 8 * self.num_units\n      kernel, recurrent_kernel, bias = tf.split(opaque, [W_size, R_size, b_size])\n\n      # Convert from cuDNN [i, f, g, o] format to TF and LMNT [i, g, f, o] format.\n      # Note that we only use a single bias vector so we sum the two separate ones\n      # and then reorder formats.\n      Wi, Wf, Wg, Wo = tf.split(kernel, 4)\n      Ri, Rf, Rg, Ro = tf.split(recurrent_kernel, 4)\n      bi, bf, bg, bo = tf.split(tf.reduce_sum(tf.split(bias, 2), axis=0), 4)\n      kernel = tf.concat([Wi, Wg, Wf, Wo], axis=0)\n      recurrent_kernel = tf.concat([Ri, Rg, Rf, Ro], axis=0)\n      bias = tf.concat([bi, bg, bf, bo], axis=0)\n\n      # Shape them correctly.\n      kernel = tf.reshape(kernel, [4 * self.num_units, self.input_size])\n      recurrent_kernel = tf.reshape(recurrent_kernel, [4 * self.num_units, self.num_units])\n      bias = tf.reshape(bias, [4 * self.num_units])\n\n      # Pre-transpose the kernels.\n      kernel = tf.transpose(kernel, [1, 0])\n      recurrent_kernel = tf.transpose(recurrent_kernel, [1, 0])\n    else:\n      kernel = self.kernel[:-self.num_units]\n      recurrent_kernel = self.kernel[-self.num_units:]\n      bias = self.bias\n    return {\n        \'kernel\': self.kernel_config.transform(kernel),\n        \'recurrent_kernel\': self.recurrent_config.transform(recurrent_kernel),\n        \'bias\': self.bias_config.transform(bias)\n    }\n\n  @property\n  def state_size(self):\n    return rnn_cell.LSTMStateTuple(self.num_units, self.num_units)\n\n  @property\n  def output_size(self):\n    return self.num_units\n\n  def __call__(self, x, sequence_length, training):\n    self.build(x.shape)\n\n    shape = tf.shape(x)\n    time_steps = shape[0]\n    batch_size = shape[1]\n\n    # Use an empty zoneout mask if no zoneout is going to be applied.\n    # Sadly, we can\'t pass `None` to the op but at least we won\'t be wasting\n    # memory or bandwidth on this tensor.\n    zoneout_mask = tf.zeros([0, 0, 0], dtype=self.dtype)\n    if self.zoneout:\n      zoneout_mask = 1.0 - self.zoneout\n      zoneout_mask += tf.random.uniform([time_steps, batch_size, self.num_units], dtype=self.dtype)\n      zoneout_mask = tf.floor(zoneout_mask)\n\n    weights = self.get_weights()\n    h, c, _ = LIB.haste_lstm(\n        x,\n        weights[\'kernel\'],\n        tf.nn.dropout(weights[\'recurrent_kernel\'], rate=self.dropout),\n        weights[\'bias\'],\n        zoneout_mask,\n        training=training,\n        zoneout_prob=self.zoneout)\n\n    if sequence_length is not None:\n      indices = sequence_length\n      indices = tf.stack([indices, tf.range(batch_size, dtype=sequence_length.dtype)], axis=-1)\n      state = rnn_cell.LSTMStateTuple(tf.gather_nd(c, indices), tf.gather_nd(h, indices))\n    else:\n      state = rnn_cell.LSTMStateTuple(c[-1], h[-1])\n\n    return h[1:], state\n\n\nclass LSTM(BaseRNN):\n  """"""\n  Long Short-Term Memory layer.\n\n  This LSTM layer offers a fused, GPU-accelerated TensorFlow op for inference\n  and training. Its weights and variables are compatible with `BasicLSTMCell`,\n  `LSTMCell`, and `LSTMBlockCell` by default, and is able to load weights\n  from `tf.contrib.cudnn_rnn.CudnnLSTM` when `cudnn_compat=True` is specified.\n\n  Although this implementation is comparable in performance to cuDNN\'s LSTM,\n  it offers additional options not typically found in other high-performance\n  implementations. DropConnect and Zoneout regularization are built-in, and\n  this layer allows setting a non-zero initial forget gate bias.\n  """"""\n\n  def __init__(self, num_units, direction=\'unidirectional\', **kwargs):\n    """"""\n    Initialize the parameters of the LSTM layer.\n\n    Arguments:\n      num_units: int, the number of units in the LSTM cell.\n      direction: string, \'unidirectional\' or \'bidirectional\'.\n      **kwargs: Dict, keyword arguments (see below).\n\n    Keyword Arguments:\n      kernel_initializer: (optional) the initializer to use for the input\n        matrix weights. Defaults to `glorot_uniform`.\n      recurrent_initializer: (optional) the initializer to use for the\n        recurrent matrix weights. Defaults to `orthogonal`.\n      bias_initializer: (optional) the initializer to use for both input and\n        recurrent bias vectors. Defaults to `zeros` unless `forget_bias` is\n        non-zero (see below).\n      kernel_transform: (optional) a function with signature\n        `(kernel: Tensor) -> Tensor` that transforms the kernel before it is\n        used. Defaults to the identity function.\n      recurrent_transform: (optional) a function with signature\n        `(recurrent_kernel: Tensor) -> Tensor` that transforms the recurrent\n        kernel before it is used. Defaults to the identity function.\n      bias_transform: (optional) a function with signature\n        `(bias: Tensor) -> Tensor` that transforms the bias before it is used.\n        Defaults to the identity function.\n      forget_bias: (optional) float, sets the initial weights for the forget\n        gates. Defaults to 1 and overrides the `bias_initializer` unless this\n        argument is set to 0.\n      dropout: (optional) float, sets the dropout rate for DropConnect\n        regularization on the recurrent matrix. Defaults to 0.\n      zoneout: (optional) float, sets the zoneout rate for Zoneout\n        regularization. Defaults to 0.\n      dtype: (optional) the data type for this layer. Defaults to `tf.float32`.\n      name: (optional) string, the name for this layer.\n      cudnn_compat: (optional) bool, if `True`, the variables created by this\n        layer are compatible with `tf.contrib.cudnn_rnn.CudnnLSTM`. Note that\n        this should only be set if you\'re restoring variables from a cuDNN\n        model. It\'s currently not possible to train a model with\n        `cudnn_compat=True` and restore it with CudnnLSTM. Defaults to `False`.\n    """"""\n    super().__init__(LSTMLayer, num_units, direction, \'lstm_cell\', **kwargs)\n'"
frameworks/tf/weight_config.py,0,"b'# Copyright 2020 LMNT, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\nclass WeightConfig:\n  __slots__ = [\'initializer\', \'constraint\', \'transform\']\n\n  def __init__(self, initializer=None, constraint=None, transform=None):\n    self.initializer = initializer\n    self.constraint = constraint\n    self.transform = transform\n\n  def override(self, initializer, constraint, transform):\n    if initializer is not None:\n      self.initializer = initializer\n    if constraint is not None:\n      self.constraint = constraint\n    if transform is not None:\n      self.transform = transform\n    return self\n'"
frameworks/tf/zoneout_wrapper.py,0,"b'# Copyright 2020 LMNT, Inc. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n""""""An RNN cell wrapper that applies Zoneout.""""""\n\n\nimport tensorflow as tf\n\nfrom tensorflow.compat.v1.nn import rnn_cell\n\n\n__all__ = [\n    \'ZoneoutWrapper\'\n]\n\n\nclass ZoneoutWrapper(rnn_cell.RNNCell):\n  """"""\n  An LSTM/GRU cell wrapper that applies zoneout to the inner cell\'s hidden state.\n\n  The zoneout paper applies zoneout to both the cell state and hidden state,\n  each with its own zoneout rate. This class (and the `LSTM` implementation in Haste)\n  applies zoneout to the hidden state and not the cell state.\n  """"""\n\n  def __init__(self, cell, rate, training):\n    """"""\n    Initialize the parameters of the zoneout wrapper.\n\n    Arguments:\n      cell: RNNCell, an instance of {`BasicLSTMCell`, `LSTMCell`,\n        `LSTMBlockCell`, `haste_tf.GRUCell`} on which to apply zoneout.\n      rate: float, 0 <= rate <= 1, the percent of hidden units to zone out per\n        time step.\n      training: bool, `True` if used during training, `False` if used during\n        inference.\n    """"""\n    super(ZoneoutWrapper, self).__init__()\n    self.cell = cell\n    self.rate = rate\n    self.training = training\n\n  @property\n  def state_size(self):\n    return self.cell.state_size\n\n  @property\n  def output_size(self):\n    return self.cell.output_size\n\n  def __call__(self, inputs, state, scope=None):\n    """"""\n    Runs one step of the RNN cell with zoneout applied.\n\n    Arguments:\n      see documentation for the inner cell.\n    """"""\n\n    output, new_state = self.cell(inputs, state, scope)\n\n    # Zoneout disabled\n    if not self.rate:\n      return output, new_state\n\n    if isinstance(new_state, rnn_cell.LSTMStateTuple):\n      zoned_out_h = self._apply_zoneout(new_state.h, state.h)\n      return output, rnn_cell.LSTMStateTuple(new_state.c, zoned_out_h)\n    elif isinstance(new_state, list) and len(new_state) == 1:\n      return output, self._apply_zoneout(new_state[0], state[0])\n    elif isinstance(new_state, tf.Tensor):\n      return output, self._apply_zoneout(new_state, state)\n    else:\n      raise ValueError((\'ZoneoutWrapper wraps cells that return LSTMStateTuple or \'\n          \'unnested state Tensors. Please use one of the following cell types:\\n\'\n          \'  tf.nn.rnn_cell.BasicLSTMCell\\n\'\n          \'  tf.nn.rnn_cell.LSTMCell\\n\'\n          \'  tf.contrib.rnn.LSTMBlockCell\\n\'\n          \'  haste_tf.GRUCell\'))\n\n  def _apply_zoneout(self, new_tensor, old_tensor):\n    if self.training:\n      mask = self._build_mask(tf.shape(new_tensor))\n      zoned_out = (new_tensor - old_tensor) * mask + old_tensor\n    else:\n      zoned_out = self.rate * old_tensor + (1.0 - self.rate) * new_tensor\n    return zoned_out\n\n  def _build_mask(self, shape):\n    mask = 1 - self.rate\n    mask += tf.random.uniform(shape)\n    return tf.floor(mask)\n'"
