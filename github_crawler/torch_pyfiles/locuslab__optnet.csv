file_path,api_count,code
cls/densenet.py,8,"b'import torch\n\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\nimport torchvision.models as models\n\nimport sys\nimport math\n\nclass Bottleneck(nn.Module):\n    def __init__(self, nChannels, growthRate):\n        super(Bottleneck, self).__init__()\n        interChannels = 4*growthRate\n        self.bn1 = nn.BatchNorm2d(nChannels)\n        self.conv1 = nn.Conv2d(nChannels, interChannels, kernel_size=1,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(interChannels)\n        self.conv2 = nn.Conv2d(interChannels, growthRate, kernel_size=3,\n                               padding=1, bias=False)\n\n    def forward(self, x):\n        out = self.conv1(F.relu(self.bn1(x)))\n        out = self.conv2(F.relu(self.bn2(out)))\n        out = torch.cat((x, out), 1)\n        return out\n\nclass SingleLayer(nn.Module):\n    def __init__(self, nChannels, growthRate):\n        super(SingleLayer, self).__init__()\n        self.bn1 = nn.BatchNorm2d(nChannels)\n        self.conv1 = nn.Conv2d(nChannels, growthRate, kernel_size=3,\n                               padding=1, bias=False)\n\n    def forward(self, x):\n        out = self.conv1(F.relu(self.bn1(x)))\n        out = torch.cat((x, out), 1)\n        return out\n\nclass Transition(nn.Module):\n    def __init__(self, nChannels, nOutChannels):\n        super(Transition, self).__init__()\n        self.bn1 = nn.BatchNorm2d(nChannels)\n        self.conv1 = nn.Conv2d(nChannels, nOutChannels, kernel_size=1,\n                               bias=False)\n\n    def forward(self, x):\n        out = self.conv1(F.relu(self.bn1(x)))\n        out = F.avg_pool2d(out, 2)\n        return out\n\n\nclass DenseNet(nn.Module):\n    def __init__(self, growthRate, depth, reduction, nClasses, bottleneck):\n        super(DenseNet, self).__init__()\n\n        nDenseBlocks = (depth-4) // 3\n        if bottleneck:\n            nDenseBlocks //= 2\n\n        nChannels = 2*growthRate\n        self.conv1 = nn.Conv2d(3, nChannels, kernel_size=3, padding=1,\n                               bias=False)\n        self.dense1 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n        nChannels += nDenseBlocks*growthRate\n        nOutChannels = int(math.floor(nChannels*reduction))\n        self.trans1 = Transition(nChannels, nOutChannels)\n\n        nChannels = nOutChannels\n        self.dense2 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n        nChannels += nDenseBlocks*growthRate\n        nOutChannels = int(math.floor(nChannels*reduction))\n        self.trans2 = Transition(nChannels, nOutChannels)\n\n        nChannels = nOutChannels\n        self.dense3 = self._make_dense(nChannels, growthRate, nDenseBlocks, bottleneck)\n        nChannels += nDenseBlocks*growthRate\n\n        self.bn1 = nn.BatchNorm2d(nChannels)\n        self.fc = nn.Linear(nChannels, nClasses)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.bias.data.zero_()\n\n    def _make_dense(self, nChannels, growthRate, nDenseBlocks, bottleneck):\n        layers = []\n        for i in range(int(nDenseBlocks)):\n            if bottleneck:\n                layers.append(Bottleneck(nChannels, growthRate))\n            else:\n                layers.append(SingleLayer(nChannels, growthRate))\n            nChannels += growthRate\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.trans1(self.dense1(out))\n        out = self.trans2(self.dense2(out))\n        out = self.dense3(out)\n        out = torch.squeeze(F.avg_pool2d(F.relu(self.bn1(out)), 8))\n        out = F.log_softmax(self.fc(out))\n        return out\n'"
cls/models.py,28,"b""import torch\n\nimport torch.nn as nn\nfrom torch.autograd import Function, Variable\nfrom torch.nn.parameter import Parameter\nimport torch.nn.functional as F\n\nfrom qpth.qp import QPFunction, QPSolvers\n\nclass Lenet(nn.Module):\n    def __init__(self, nHidden, nCls=10, proj='softmax'):\n        super(Lenet, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, kernel_size=5)\n        self.conv2 = nn.Conv2d(20, 50, kernel_size=5)\n        self.fc1 = nn.Linear(50*4*4, nHidden)\n        self.fc2 = nn.Linear(nHidden, nCls)\n\n        self.proj = proj\n        self.nCls = nCls\n\n        if proj == 'simproj':\n            self.Q = Variable(0.5*torch.eye(nCls).double().cuda())\n            self.G = Variable(-torch.eye(nCls).double().cuda())\n            self.h = Variable(-1e-5*torch.ones(nCls).double().cuda())\n            self.A = Variable((torch.ones(1, nCls)).double().cuda())\n            self.b = Variable(torch.Tensor([1.]).double().cuda())\n            def projF(x):\n                nBatch = x.size(0)\n                Q = self.Q.unsqueeze(0).expand(nBatch, nCls, nCls)\n                G = self.G.unsqueeze(0).expand(nBatch, nCls, nCls)\n                h = self.h.unsqueeze(0).expand(nBatch, nCls)\n                A = self.A.unsqueeze(0).expand(nBatch, 1, nCls)\n                b = self.b.unsqueeze(0).expand(nBatch, 1)\n                x = QPFunction()(Q, -x.double(), G, h, A, b).float()\n                x = x.log()\n                return x\n            self.projF = projF\n        else:\n            self.projF = F.log_softmax\n\n    def forward(self, x):\n        nBatch = x.size(0)\n\n        x = F.max_pool2d(self.conv1(x), 2)\n        x = F.max_pool2d(self.conv2(x), 2)\n        x = x.view(nBatch, -1)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return self.projF(x)\n\nclass LenetOptNet(nn.Module):\n    def __init__(self, nHidden=50, nineq=200, neq=0, eps=1e-4):\n        super(LenetOptNet, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, kernel_size=5)\n        self.conv2 = nn.Conv2d(20, 50, kernel_size=5)\n\n        self.qp_o = nn.Linear(50*4*4, nHidden)\n        self.qp_z0 = nn.Linear(50*4*4, nHidden)\n        self.qp_s0 = nn.Linear(50*4*4, nineq)\n\n        assert(neq==0)\n        self.M = Variable(torch.tril(torch.ones(nHidden, nHidden)).cuda())\n        self.L = Parameter(torch.tril(torch.rand(nHidden, nHidden).cuda()))\n        self.G = Parameter(torch.Tensor(nineq,nHidden).uniform_(-1,1).cuda())\n        # self.z0 = Parameter(torch.zeros(nHidden).cuda())\n        # self.s0 = Parameter(torch.ones(nineq).cuda())\n\n        self.nHidden = nHidden\n        self.nineq = nineq\n        self.neq = neq\n        self.eps = eps\n\n    def forward(self, x):\n        nBatch = x.size(0)\n\n        x = F.max_pool2d(self.conv1(x), 2)\n        x = F.max_pool2d(self.conv2(x), 2)\n        x = x.view(nBatch, -1)\n\n        L = self.M*self.L\n        Q = L.mm(L.t()) + self.eps*Variable(torch.eye(self.nHidden)).cuda()\n        Q = Q.unsqueeze(0).expand(nBatch, self.nHidden, self.nHidden)\n        G = self.G.unsqueeze(0).expand(nBatch, self.nineq, self.nHidden)\n        z0 = self.qp_z0(x)\n        s0 = self.qp_s0(x)\n        h = z0.mm(self.G.t())+s0\n        e = Variable(torch.Tensor())\n        inputs = self.qp_o(x)\n        x = QPFunction()(Q, inputs, G, h, e, e)\n        x = x[:,:10]\n\n        return F.log_softmax(x)\n\nclass FC(nn.Module):\n    def __init__(self, nHidden, bn):\n        super().__init__()\n        self.bn = bn\n\n        self.fc1 = nn.Linear(784, nHidden)\n        if bn:\n            self.bn1 = nn.BatchNorm1d(nHidden)\n            self.bn2 = nn.BatchNorm1d(10)\n        self.fc2 = nn.Linear(nHidden, 10)\n        self.fc3 = nn.Linear(10, 10)\n\n    def forward(self, x):\n        nBatch = x.size(0)\n\n        # FC-ReLU-(BN)-FC-ReLU-(BN)-FC-Softmax\n        x = x.view(nBatch, -1)\n        x = F.relu(self.fc1(x))\n        if self.bn:\n            x = self.bn1(x)\n        x = F.relu(self.fc2(x))\n        if self.bn:\n            x = self.bn2(x)\n        x = self.fc3(x)\n        return F.log_softmax(x)\n\nclass OptNet(nn.Module):\n    def __init__(self, nFeatures, nHidden, nCls, bn, nineq=200, neq=0, eps=1e-4):\n        super().__init__()\n\n        self.nFeatures = nFeatures\n        self.nHidden = nHidden\n        self.bn = bn\n        self.nCls = nCls\n\n        if bn:\n            self.bn1 = nn.BatchNorm1d(nHidden)\n            self.bn2 = nn.BatchNorm1d(nCls)\n\n        self.fc1 = nn.Linear(nFeatures, nHidden)\n        self.fc2 = nn.Linear(nHidden, nCls)\n\n        # self.qp_z0 = nn.Linear(nCls, nCls)\n        # self.qp_s0 = nn.Linear(nCls, nineq)\n\n        assert(neq==0)\n        self.M = Variable(torch.tril(torch.ones(nCls, nCls)).cuda())\n        self.L = Parameter(torch.tril(torch.rand(nCls, nCls).cuda()))\n        self.G = Parameter(torch.Tensor(nineq,nCls).uniform_(-1,1).cuda())\n        self.z0 = Parameter(torch.zeros(nCls).cuda())\n        self.s0 = Parameter(torch.ones(nineq).cuda())\n\n        self.nineq = nineq\n        self.neq = neq\n        self.eps = eps\n\n    def forward(self, x):\n        nBatch = x.size(0)\n\n        # FC-ReLU-(BN)-FC-ReLU-(BN)-QP-Softmax\n        x = x.view(nBatch, -1)\n        x = F.relu(self.fc1(x))\n        if self.bn:\n            x = self.bn1(x)\n        x = F.relu(self.fc2(x))\n        if self.bn:\n            x = self.bn2(x)\n\n        L = self.M*self.L\n        Q = L.mm(L.t()) + self.eps*Variable(torch.eye(self.nCls)).cuda()\n        Q = Q.unsqueeze(0).expand(nBatch, self.nCls, self.nCls)\n        G = self.G.unsqueeze(0).expand(nBatch, self.nineq, self.nCls)\n        # z0 = self.qp_z0(x)\n        # s0 = self.qp_s0(x)\n        z0 = self.z0.unsqueeze(0).expand(nBatch, self.nCls)\n        s0 = self.s0.unsqueeze(0).expand(nBatch, self.nineq)\n        h = z0.mm(self.G.t())+s0\n        e = Variable(torch.Tensor())\n        inputs = x\n        x = QPFunction(verbose=-1)(\n            Q.double(), inputs.double(), G.double(), h.double(), e, e)\n        x = x.float()\n        # x = x[:,:10].float()\n\n        return F.log_softmax(x)\n\nclass OptNetEq(nn.Module):\n    def __init__(self, nFeatures, nHidden, nCls, neq, Qpenalty=0.1, eps=1e-4):\n        super().__init__()\n\n        self.nFeatures = nFeatures\n        self.nHidden = nHidden\n        self.nCls = nCls\n\n        self.fc1 = nn.Linear(nFeatures, nHidden)\n        self.fc2 = nn.Linear(nHidden, nCls)\n\n        self.Q = Variable(Qpenalty*torch.eye(nHidden).double().cuda())\n        self.G = Variable(-torch.eye(nHidden).double().cuda())\n        self.h = Variable(torch.zeros(nHidden).double().cuda())\n        self.A = Parameter(torch.rand(neq,nHidden).double().cuda())\n        self.b = Variable(torch.ones(self.A.size(0)).double().cuda())\n\n        self.neq = neq\n\n    def forward(self, x):\n        nBatch = x.size(0)\n\n        # FC-ReLU-QP-FC-Softmax\n        x = x.view(nBatch, -1)\n        x = F.relu(self.fc1(x))\n\n        Q = self.Q.unsqueeze(0).expand(nBatch, self.Q.size(0), self.Q.size(1))\n        p = -x.view(nBatch,-1)\n        G = self.G.unsqueeze(0).expand(nBatch, self.G.size(0), self.G.size(1))\n        h = self.h.unsqueeze(0).expand(nBatch, self.h.size(0))\n        A = self.A.unsqueeze(0).expand(nBatch, self.A.size(0), self.A.size(1))\n        b = self.b.unsqueeze(0).expand(nBatch, self.b.size(0))\n\n        x = QPFunction(verbose=False)(Q, p.double(), G, h, A, b).float()\n        x = self.fc2(x)\n\n        return F.log_softmax(x)\n"""
cls/plot.py,0,"b'#!/usr/bin/env python3\n\nimport argparse\nimport os\nimport numpy as np\n\nimport math\n\nimport matplotlib as mpl\nmpl.use(\'Agg\')\nimport matplotlib.pyplot as plt\nplt.style.use(\'bmh\')\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'expDir\', type=str)\n    args = parser.parse_args()\n\n    trainP = os.path.join(args.expDir, \'train.csv\')\n    trainData = np.loadtxt(trainP, delimiter=\',\').reshape(-1, 3)\n    testP = os.path.join(args.expDir, \'test.csv\')\n    testData = np.loadtxt(testP, delimiter=\',\').reshape(-1, 3)\n\n    trainI, trainLoss, trainErr = np.split(trainData, [1,2], axis=1)\n    trainI, trainLoss, trainErr = [x.ravel() for x in\n                                   (trainI, trainLoss, trainErr)]\n\n    N = len(trainI) // math.ceil(trainI[-1])\n    trainI_, trainLoss_, trainErr_ = rolling(N, trainI, trainLoss, trainErr)\n\n    testI, testLoss, testErr = np.split(testData, [1,2], axis=1)\n\n    fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n    plt.plot(trainI, trainLoss, label=\'Train\')\n    # plt.plot(trainI_, trainLoss_, label=\'Train\')\n    plt.plot(testI, testLoss, label=\'Test\')\n    plt.xlabel(\'Epoch\')\n    plt.ylabel(\'Cross-Entropy Loss\')\n    # ax.set_ylim([1e-2, 1e0])\n    plt.legend()\n    ax.set_yscale(\'log\')\n    loss_fname = os.path.join(args.expDir, \'loss.png\')\n    plt.tight_layout()\n    plt.savefig(loss_fname)\n    print(\'Created {}\'.format(loss_fname))\n\n    fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n    # plt.plot(trainI, trainErr, label=\'Train\')\n    plt.plot(trainI_, trainErr_, label=\'Train\')\n    plt.plot(testI, testErr, label=\'Test\')\n    plt.xlabel(\'Epoch\')\n    plt.ylabel(\'Error\')\n    ax.set_yscale(\'log\')\n    ax.set_ylim(ymin=1)\n    # ax.set_ylim([0.5,1.2])\n    plt.legend()\n    err_fname = os.path.join(args.expDir, \'error.png\')\n    plt.tight_layout()\n    plt.savefig(err_fname)\n    print(\'Created {}\'.format(err_fname))\n\n    loss_err_fname = os.path.join(args.expDir, \'loss-error.png\')\n    os.system(\'convert +append ""{}"" ""{}"" ""{}""\'.format(loss_fname, err_fname, loss_err_fname))\n    print(\'Created {}\'.format(loss_err_fname))\n\ndef rolling(N, i, loss, err):\n    i_ = i[N-1:]\n    K = np.full(N, 1./N)\n    loss_ = np.convolve(loss, K, \'valid\')\n    err_ = np.convolve(err, K, \'valid\')\n    return i_, loss_, err_\n\nif __name__ == \'__main__\':\n    main()\n'"
cls/train.py,11,"b'#!/usr/bin/env python3\n\nimport json\n\nimport argparse\n\ntry: import setGPU\nexcept ImportError: pass\n\nimport torch\n\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport torch.nn.functional as F\nfrom torch.autograd import Function, Variable\n\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nfrom torchvision.utils import save_image\n\nfrom torch.utils.data import DataLoader\n\nimport os\nimport sys\nimport math\n\nimport shutil\n\nimport setproctitle\n\nimport densenet\nimport models\n# import make_graph\n\nimport sys\nfrom IPython.core import ultratb\nsys.excepthook = ultratb.FormattedTB(mode=\'Verbose\',\n     color_scheme=\'Linux\', call_pdb=1)\n\ndef get_loaders(args):\n    kwargs = {\'num_workers\': 1, \'pin_memory\': True} if args.cuda else {}\n    if args.dataset == \'mnist\':\n        trainLoader = torch.utils.data.DataLoader(\n            dset.MNIST(\'data/mnist\', train=True, download=True,\n                           transform=transforms.Compose([\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.1307,), (0.3081,))\n                           ])),\n            batch_size=args.batchSz, shuffle=True, **kwargs)\n        testLoader = torch.utils.data.DataLoader(\n            dset.MNIST(\'data/mnist\', train=False, transform=transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize((0.1307,), (0.3081,))\n            ])),\n            batch_size=args.batchSz, shuffle=False, **kwargs)\n    elif args.dataset == \'cifar-10\':\n        normMean = [0.49139968, 0.48215827, 0.44653124]\n        normStd = [0.24703233, 0.24348505, 0.26158768]\n        normTransform = transforms.Normalize(normMean, normStd)\n\n        trainTransform = transforms.Compose([\n            transforms.RandomCrop(32, padding=4),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            normTransform\n        ])\n        testTransform = transforms.Compose([\n            transforms.ToTensor(),\n            normTransform\n        ])\n\n        trainLoader = DataLoader(\n            dset.CIFAR10(root=\'data/cifar\', train=True, download=True,\n                        transform=trainTransform),\n            batch_size=args.batchSz, shuffle=True, **kwargs)\n        testLoader = DataLoader(\n            dset.CIFAR10(root=\'data/cifar\', train=False, download=True,\n                        transform=testTransform),\n            batch_size=args.batchSz, shuffle=False, **kwargs)\n    else:\n        assert(False)\n\n    return trainLoader, testLoader\n\ndef get_net(args):\n    if args.model == \'densenet\':\n        net = densenet.DenseNet(growthRate=12, depth=100, reduction=0.5,\n                                bottleneck=True, nClasses=10)\n    elif args.model == \'lenet\':\n        net = models.Lenet(args.nHidden, 10, args.proj)\n    elif args.model == \'lenet-optnet\':\n        net = models.LenetOptNet(args.nHidden, args.nineq)\n    elif args.model == \'fc\':\n        net = models.FC(args.nHidden, args.bn)\n    elif args.model == \'optnet\':\n        net = models.OptNet(28*28, args.nHidden, 10, args.bn, args.nineq)\n    elif args.model == \'optnet-eq\':\n        net = models.OptNetEq(28*28, args.nHidden, 10, args.neq)\n    else:\n        assert(False)\n\n    return net\n\ndef get_optimizer(args, params):\n    if args.dataset == \'mnist\':\n        if args.model == \'optnet-eq\':\n            params = list(params)\n            A_param = params.pop(0)\n            assert(A_param.size() == (args.neq, args.nHidden))\n            optimizer = optim.Adam([\n                {\'params\': params, \'lr\': 1e-3},\n                {\'params\': [A_param], \'lr\': 1e-1}\n            ])\n        else:\n            optimizer = optim.Adam(params)\n    elif args.dataset in (\'cifar-10\', \'cifar-100\'):\n        if args.opt == \'sgd\':\n            optimizer = optim.SGD(params, lr=1e-1, momentum=0.9, weight_decay=args.weightDecay)\n        elif args.opt == \'adam\':\n            optimizer = optim.Adam(params, weight_decay=args.weightDecay)\n    else:\n        assert(False)\n\n    return optimizer\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--batchSz\', type=int, default=64)\n    parser.add_argument(\'--no-cuda\', action=\'store_true\')\n    parser.add_argument(\'--save\', type=str)\n    parser.add_argument(\'--work\', type=str, default=\'work\')\n    parser.add_argument(\'--seed\', type=int, default=1)\n    parser.add_argument(\'--nEpoch\', type=int, default=1000)\n    parser.add_argument(\'--weightDecay\', type=float, default=1e-4)\n    parser.add_argument(\'--opt\', type=str, default=\'sgd\',\n                        choices=(\'sgd\', \'adam\'))\n    parser.add_argument(\'dataset\', type=str,\n                        choices=[\'mnist\', \'cifar-10\', \'cifar-100\', \'svhn\'])\n    subparsers = parser.add_subparsers(dest=\'model\')\n    lenetP = subparsers.add_parser(\'lenet\')\n    lenetP.add_argument(\'--nHidden\', type=int, default=50)\n    lenetP.add_argument(\'--proj\', type=str, choices=(\'softmax\', \'simproj\'))\n    lenetOptnetP = subparsers.add_parser(\'lenet-optnet\')\n    lenetOptnetP.add_argument(\'--nHidden\', type=int, default=50)\n    lenetOptnetP.add_argument(\'--nineq\', type=int, default=100)\n    lenetOptnetP.add_argument(\'--eps\', type=float, default=1e-4)\n    densenetP = subparsers.add_parser(\'densenet\')\n    fcP = subparsers.add_parser(\'fc\')\n    fcP.add_argument(\'--nHidden\', type=int, default=500)\n    fcP.add_argument(\'--bn\', action=\'store_true\')\n    optnetP = subparsers.add_parser(\'optnet\')\n    optnetP.add_argument(\'--nHidden\', type=int, default=500)\n    optnetP.add_argument(\'--eps\', default=1e-4)\n    optnetP.add_argument(\'--nineq\', type=int, default=10)\n    optnetP.add_argument(\'--bn\', action=\'store_true\')\n    optnetEqP = subparsers.add_parser(\'optnet-eq\')\n    optnetEqP.add_argument(\'--nHidden\', type=int, default=100)\n    optnetEqP.add_argument(\'--neq\', type=int, default=50)\n    args = parser.parse_args()\n\n    args.cuda = not args.no_cuda and torch.cuda.is_available()\n    if args.save is None:\n        t = \'{}.{}\'.format(args.dataset, args.model)\n        if args.model == \'lenet\':\n            t += \'.nHidden:{}.proj:{}\'.format(args.nHidden, args.proj)\n        elif args.model == \'fc\':\n            t += \'.nHidden:{}\'.format(args.nHidden)\n            if args.bn:\n                t += \'.bn\'\n        elif args.model == \'optnet\':\n            t += \'.nHidden:{}.nineq:{}.eps:{}\'.format(args.nHidden, args.nineq, args.eps)\n            if args.bn:\n                t += \'.bn\'\n        elif args.model == \'optnet-eq\':\n            t += \'.nHidden:{}.neq:{}\'.format(args.nHidden, args.neq)\n        elif args.model == \'lenet-optnet\':\n            t += \'.nHidden:{}.nineq:{}.eps:{}\'.format(args.nHidden, args.nineq, args.eps)\n    setproctitle.setproctitle(\'bamos.\'+t)\n    args.save = os.path.join(args.work, t)\n\n    torch.manual_seed(args.seed)\n    if args.cuda:\n        torch.cuda.manual_seed(args.seed)\n\n    if os.path.exists(args.save):\n        shutil.rmtree(args.save)\n    os.makedirs(args.save, exist_ok=True)\n\n    trainLoader, testLoader = get_loaders(args)\n    net = get_net(args)\n    optimizer = get_optimizer(args, net.parameters())\n\n    args.nparams = sum([p.data.nelement() for p in net.parameters()])\n    with open(os.path.join(args.save, \'meta.json\'), \'w\') as f:\n        json.dump(vars(args), f, sort_keys=True, indent=2)\n\n    print(\'  + Number of params: {}\'.format(args.nparams))\n    if args.cuda:\n        net = net.cuda()\n\n    trainF = open(os.path.join(args.save, \'train.csv\'), \'w\')\n    testF = open(os.path.join(args.save, \'test.csv\'), \'w\')\n\n    for epoch in range(1, args.nEpoch + 1):\n        adjust_opt(args, optimizer, epoch)\n        train(args, epoch, net, trainLoader, optimizer, trainF)\n        test(args, epoch, net, testLoader, optimizer, testF)\n        try:\n            torch.save(net, os.path.join(args.save, \'latest.pth\'))\n        except:\n            pass\n        os.system(\'./plot.py ""{}"" &\'.format(args.save))\n\n    trainF.close()\n    testF.close()\n\ndef train(args, epoch, net, trainLoader, optimizer, trainF):\n    net.train()\n    nProcessed = 0\n    nTrain = len(trainLoader.dataset)\n    for batch_idx, (data, target) in enumerate(trainLoader):\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data), Variable(target)\n        optimizer.zero_grad()\n        output = net(data)\n        loss = F.nll_loss(output, target)\n        # make_graph.save(\'/tmp/t.dot\', loss.creator); assert(False)\n        loss.backward()\n        optimizer.step()\n        nProcessed += len(data)\n        pred = output.data.max(1)[1] # get the index of the max log-probability\n        incorrect = pred.ne(target.data).cpu().sum()\n        err = 100.*incorrect/len(data)\n        partialEpoch = epoch + batch_idx / len(trainLoader) - 1\n        print(\'Train Epoch: {:.2f} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tError: {:.6f}\'.format(\n            partialEpoch, nProcessed, nTrain, 100. * batch_idx / len(trainLoader),\n            loss.data[0], err))\n\n        trainF.write(\'{},{},{}\\n\'.format(partialEpoch, loss.data[0], err))\n        trainF.flush()\n\ndef test(args, epoch, net, testLoader, optimizer, testF):\n    net.eval()\n    test_loss = 0\n    incorrect = 0\n    for data, target in testLoader:\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data, volatile=True), Variable(target)\n        output = net(data)\n        test_loss += F.nll_loss(output, target).data[0]\n        pred = output.data.max(1)[1] # get the index of the max log-probability\n        incorrect += pred.ne(target.data).cpu().sum()\n\n    test_loss = test_loss\n    test_loss /= len(testLoader) # loss function already averages over batch size\n    nTotal = len(testLoader.dataset)\n    err = 100.*incorrect/nTotal\n    print(\'\\nTest set: Average loss: {:.4f}, Error: {}/{} ({:.0f}%)\\n\'.format(\n        test_loss, incorrect, nTotal, err))\n\n    testF.write(\'{},{},{}\\n\'.format(epoch, test_loss, err))\n    testF.flush()\n\ndef adjust_opt(args, optimizer, epoch):\n    if args.model == \'densenet\':\n        if args.opt == \'sgd\':\n            if epoch == 150: update_lr(optimizer, 1e-2)\n            elif epoch == 225: update_lr(optimizer, 1e-3)\n            else: return\n\ndef update_lr(optimizer, lr):\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\nif __name__==\'__main__\':\n    main()\n'"
denoising/create.py,1,"b'#!/usr/bin/env python3\n\nimport argparse\nimport numpy as np\nimport numpy.random as npr\nimport torch\n\nimport os, sys\nimport shutil\n\nimport matplotlib as mpl\nmpl.use(\'Agg\')\nimport matplotlib.pyplot as plt\nplt.style.use(\'bmh\')\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--minBps\', type=int, default=1)\n    parser.add_argument(\'--maxBps\', type=int, default=10)\n    parser.add_argument(\'--seqLen\', type=int, default=100)\n    parser.add_argument(\'--minHeight\', type=int, default=10)\n    parser.add_argument(\'--maxHeight\', type=int, default=100)\n    parser.add_argument(\'--noise\', type=float, default=10)\n    parser.add_argument(\'--nSamples\', type=int, default=10000)\n    parser.add_argument(\'--save\', type=str, default=\'data/synthetic\')\n    args = parser.parse_args()\n\n    npr.seed(0)\n\n    save = args.save\n    if os.path.isdir(save):\n        shutil.rmtree(save)\n    os.makedirs(save)\n\n    X, Y = [], []\n    for i in range(args.nSamples):\n        Xi, Yi = sample(args)\n        X.append(Xi); Y.append(Yi)\n        if i == 0:\n            fig, ax = plt.subplots(1, 1)\n            plt.plot(Xi, label=\'Corrupted\')\n            plt.plot(Yi, label=\'Original\')\n            plt.legend()\n            f = os.path.join(args.save, ""example.png"")\n            fig.savefig(f)\n            print(""Created {}"".format(f))\n\n    X = np.array(X)\n    Y = np.array(Y)\n\n    for loc,arr in ((\'features.pt\', X), (\'labels.pt\', Y)):\n        fname = os.path.join(args.save, loc)\n        with open(fname, \'wb\') as f:\n            torch.save(torch.Tensor(arr), f)\n        print(""Created {}"".format(fname))\n\ndef sample(args):\n    nBps = npr.randint(args.minBps, args.maxBps)\n    bpLocs = [0] + sorted(npr.choice(args.seqLen-2, nBps-1, replace=False)+1) + [args.seqLen]\n    bpDiffs = np.diff(bpLocs)\n    heights = npr.randint(args.minHeight, args.maxHeight, nBps)\n    Y = []\n    for d, h in zip(bpDiffs, heights):\n        Y += [h]*d\n    Y = np.array(Y, dtype=np.float)\n\n    X = Y + npr.normal(0, args.noise, (args.seqLen))\n    return X, Y\n\nif __name__==\'__main__\':\n    main()\n'"
denoising/main.py,11,"b'#!/usr/bin/env python3\n\nimport argparse\nimport csv\nimport os\nimport shutil\nfrom tqdm import tqdm\n\ntry: import setGPU\nexcept ImportError: pass\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\nimport numpy as np\nimport numpy.random as npr\n\nimport sys\n\nimport matplotlib as mpl\nmpl.use(\'Agg\')\nimport matplotlib.pyplot as plt\nplt.style.use(\'bmh\')\n\nimport setproctitle\n\nimport models\n\nimport sys\nfrom IPython.core import ultratb\nsys.excepthook = ultratb.FormattedTB(mode=\'Verbose\',\n     color_scheme=\'Linux\', call_pdb=1)\n\ndef print_header(msg):\n    print(\'===>\', msg)\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--no-cuda\', action=\'store_true\')\n    parser.add_argument(\'--batchSz\', type=int, default=150)\n    parser.add_argument(\'--testBatchSz\', type=int, default=100)\n    parser.add_argument(\'--nEpoch\', type=int, default=100)\n    parser.add_argument(\'--testPct\', type=float, default=0.1)\n    parser.add_argument(\'--work\', type=str, default=\'work\')\n    parser.add_argument(\'--save\', type=str)\n    subparsers = parser.add_subparsers(dest=\'model\')\n    subparsers.required = True\n    reluP = subparsers.add_parser(\'relu\')\n    reluP.add_argument(\'--nHidden\', type=int, default=50)\n    reluP.add_argument(\'--bn\', action=\'store_true\')\n    optnetP = subparsers.add_parser(\'optnet\')\n    # optnetP.add_argument(\'--nHidden\', type=int, default=50)\n    # optnetP.add_argument(\'--nineq\', type=int, default=100)\n    optnetP.add_argument(\'--eps\', type=float, default=1e-4)\n    optnetP.add_argument(\'--tvInit\', action=\'store_true\')\n    optnetP.add_argument(\'--learnD\', action=\'store_true\')\n    optnetP.add_argument(\'--Dpenalty\', type=float, default=1e-1)\n    args = parser.parse_args()\n\n    args.cuda = not args.no_cuda and torch.cuda.is_available()\n    # args.save = args.save or \'work/{}.{}\'.format(args.dataset, args.model)\n    if args.save is None:\n        t = os.path.join(args.work, args.model)\n        if args.model == \'optnet\':\n            t += \'.eps={}\'.format(args.eps)\n            if args.tvInit:\n                t += \'.tvInit\'\n            if args.learnD:\n                t += \'.learnD.{}\'.format(args.Dpenalty)\n        elif args.model == \'relu\':\n            t += \'.nHidden:{}\'.format(args.nHidden)\n            if args.bn:\n                t += \'.bn\'\n        args.save = t\n    setproctitle.setproctitle(\'bamos.\' + args.save)\n\n    with open(\'data/synthetic/features.pt\', \'rb\') as f:\n        X = torch.load(f)\n    with open(\'data/synthetic/labels.pt\', \'rb\') as f:\n        Y = torch.load(f)\n\n    N, nFeatures = X.size()\n\n    nTrain = int(N*(1.-args.testPct))\n    nTest = N-nTrain\n\n    trainX = X[:nTrain]\n    trainY = Y[:nTrain]\n    testX = X[nTrain:]\n    testY = Y[nTrain:]\n\n    assert(nTrain % args.batchSz == 0)\n    assert(nTest % args.testBatchSz == 0)\n\n    save = args.save\n    if os.path.isdir(save):\n        shutil.rmtree(save)\n    os.makedirs(save)\n\n    npr.seed(1)\n\n    print_header(\'Building model\')\n    if args.model == \'relu\':\n        # nHidden = 2*nFeatures-1\n        nHidden = args.nHidden\n        model = models.ReluNet(nFeatures, nHidden, args.bn)\n    elif args.model == \'optnet\':\n        if args.learnD:\n            model = models.OptNet_LearnD(nFeatures, args)\n        else:\n            model = models.OptNet(nFeatures, args)\n\n    if args.cuda:\n        model = model.cuda()\n\n    fields = [\'epoch\', \'loss\']\n    trainF = open(os.path.join(save, \'train.csv\'), \'w\')\n    trainW = csv.writer(trainF)\n    trainW.writerow(fields)\n    trainF.flush()\n    testF = open(os.path.join(save, \'test.csv\'), \'w\')\n    testW = csv.writer(testF)\n    testW.writerow(fields)\n    testF.flush()\n\n\n    if args.model == \'optnet\':\n        if args.tvInit: lr = 1e-4\n        elif args.learnD: lr = 1e-2\n        else: lr = 1e-3\n    else:\n        lr = 1e-3\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    writeParams(args, model, \'init\')\n    test(args, 0, model, testF, testW, testX, testY)\n    for epoch in range(1, args.nEpoch+1):\n        # update_lr(optimizer, epoch)\n        train(args, epoch, model, trainF, trainW, trainX, trainY, optimizer)\n        test(args, epoch, model, testF, testW, testX, testY)\n        torch.save(model, os.path.join(args.save, \'latest.pth\'))\n        writeParams(args, model, \'latest\')\n        os.system(\'./plot.py ""{}"" &\'.format(args.save))\n\ndef writeParams(args, model, tag):\n    if args.model == \'optnet\' and args.learnD:\n        D = model.D.data.cpu().numpy()\n        np.savetxt(os.path.join(args.save, \'D.{}\'.format(tag)), D)\n\ndef train(args, epoch, model, trainF, trainW, trainX, trainY, optimizer):\n    batchSz = args.batchSz\n\n    batch_data_t = torch.FloatTensor(batchSz, trainX.size(1))\n    batch_targets_t = torch.FloatTensor(batchSz, trainY.size(1))\n    if args.cuda:\n        batch_data_t = batch_data_t.cuda()\n        batch_targets_t = batch_targets_t.cuda()\n    batch_data = Variable(batch_data_t, requires_grad=False)\n    batch_targets = Variable(batch_targets_t, requires_grad=False)\n    for i in range(0, trainX.size(0), batchSz):\n        batch_data.data[:] = trainX[i:i+batchSz]\n        batch_targets.data[:] = trainY[i:i+batchSz]\n        # Fixed batch size for debugging:\n        # batch_data.data[:] = trainX[:batchSz]\n        # batch_targets.data[:] = trainY[:batchSz]\n\n        optimizer.zero_grad()\n        preds = model(batch_data)\n        mseLoss = nn.MSELoss()(preds, batch_targets)\n        if args.model == \'optnet\' and args.learnD:\n            loss = mseLoss + args.Dpenalty*(model.D.norm(1))\n        else:\n            loss = mseLoss\n        loss.backward()\n        optimizer.step()\n\n        print(\'Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.4f}\'.format(\n            epoch, i+batchSz, trainX.size(0),\n            float(i+batchSz)/trainX.size(0)*100,\n            mseLoss.data[0]))\n\n        trainW.writerow((epoch-1+float(i+batchSz)/trainX.size(0), mseLoss.data[0]))\n        trainF.flush()\n\ndef test(args, epoch, model, testF, testW, testX, testY):\n    batchSz = args.testBatchSz\n\n    test_loss = 0\n    batch_data_t = torch.FloatTensor(batchSz, testX.size(1))\n    batch_targets_t = torch.FloatTensor(batchSz, testY.size(1))\n    if args.cuda:\n        batch_data_t = batch_data_t.cuda()\n        batch_targets_t = batch_targets_t.cuda()\n    batch_data = Variable(batch_data_t, volatile=True)\n    batch_targets = Variable(batch_targets_t, volatile=True)\n\n    for i in range(0, testX.size(0), batchSz):\n        print(\'Testing model: {}/{}\'.format(i, testX.size(0)), end=\'\\r\')\n        batch_data.data[:] = testX[i:i+batchSz]\n        batch_targets.data[:] = testY[i:i+batchSz]\n        output = model(batch_data)\n        if i == 0:\n            testOut = os.path.join(args.save, \'test-imgs\')\n            os.makedirs(testOut, exist_ok=True)\n            for j in range(4):\n                X = batch_data.data[j].cpu().numpy()\n                Y = batch_targets.data[j].cpu().numpy()\n                Yhat = output[j].data.cpu().numpy()\n\n                fig, ax = plt.subplots(1, 1)\n                plt.plot(X, label=\'Corrupted\')\n                plt.plot(Y, label=\'Original\')\n                plt.plot(Yhat, label=\'Predicted\')\n                plt.legend()\n                f = os.path.join(testOut, \'{}.png\'.format(j))\n                fig.savefig(f)\n        test_loss += nn.MSELoss()(output, batch_targets)\n\n    nBatches = testX.size(0)/batchSz\n    test_loss = test_loss.data[0]/nBatches\n    print(\'TEST SET RESULTS:\' + \' \' * 20)\n    print(\'Average loss: {:.4f}\'.format(test_loss))\n\n    testW.writerow((epoch, test_loss))\n    testF.flush()\n\nif __name__==\'__main__\':\n    main()\n'"
denoising/main.tv.py,2,"b'#!/usr/bin/env python3\n\nimport argparse\nimport csv\nimport os\nimport shutil\nfrom tqdm import tqdm\n\nimport cvxpy as cp\n\nimport torch\n\nimport numpy as np\nimport numpy.random as npr\n\nimport sys\nfrom IPython.core import ultratb\nsys.excepthook = ultratb.FormattedTB(mode=\'Verbose\',\n     color_scheme=\'Linux\', call_pdb=1)\n\nimport matplotlib as mpl\nmpl.use(\'Agg\')\nimport matplotlib.pyplot as plt\nplt.style.use(\'bmh\')\nmpl.rc(\'font\',**{\'family\':\'sans-serif\',\'sans-serif\':[\'Helvetica\']})\nmpl.rc(\'text\', usetex=True)\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--nEpoch\', type=int, default=50)\n    parser.add_argument(\'--testPct\', type=float, default=0.1)\n    parser.add_argument(\'--workDir\', type=str, default=\'work/tv\')\n    args = parser.parse_args()\n\n    with open(\'data/synthetic/features.pt\', \'rb\') as f:\n        X = torch.load(f).numpy()\n    with open(\'data/synthetic/labels.pt\', \'rb\') as f:\n        Y = torch.load(f).numpy()\n\n    N, nFeatures = X.shape\n    nTrain = int(N*(1.-args.testPct))\n    nTest = N-nTrain\n\n    trainX = X[:nTrain]\n    trainY = Y[:nTrain]\n    testX = X[nTrain:]\n    testY = Y[nTrain:]\n\n    workDir = args.workDir\n    if os.path.isdir(workDir):\n        shutil.rmtree(workDir)\n    os.makedirs(workDir)\n\n    npr.seed(1)\n\n    X_ = cp.Parameter(nFeatures)\n    Y_ = cp.Variable(nFeatures)\n    lams = list(np.linspace(0,100,101))\n    mses = []\n\n    def getMse(lam):\n        prob = cp.Problem(cp.Minimize(0.5*cp.sum_squares(X_-Y_)+lam*cp.tv(Y_)))\n        mses_lam = []\n\n        # testOut = os.path.join(workDir, \'test-imgs\', \'lam-{:07.2f}\'.format(lam))\n        # os.makedirs(testOut, exist_ok=True)\n\n        for i in range(nTest):\n            X_.value = testX[i]\n            prob.solve(cp.SCS)\n            assert(\'optimal\' in prob.status)\n            Yhat = np.array(Y_.value).ravel()\n            mse = np.mean(np.square(testY[i] - Yhat))\n\n            mses_lam.append(mse)\n\n            # if i <= 4:\n            #     fig, ax = plt.subplots(1, 1)\n            #     plt.plot(testX[i], label=\'Corrupted\')\n            #     plt.plot(testY[i], label=\'Original\')\n            #     plt.plot(Yhat, label=\'Predicted\')\n            #     plt.legend()\n            #     f = os.path.join(testOut, \'{}.png\'.format(i))\n            #     fig.savefig(f)\n            #     plt.close(fig)\n\n        return np.mean(mses_lam)\n\n    for lam in lams:\n        mses.append(getMse(lam))\n        print(lam, mses[-1])\n\n    xMin, xMax = (1, 30)\n\n    with open(os.path.join(workDir, \'mses.csv\'), \'w\') as f:\n        for lam,mse in zip(lams,mses):\n            f.write(\'{},{}\\n\'.format(lam,mse))\n\n    fig, ax = plt.subplots(1, 1)\n    plt.plot(lams, mses)\n    plt.xlabel(""$\\lambda$"")\n    plt.ylabel(""MSE"")\n    # plt.xlim(xmin=0)\n    # ax.set_yscale(\'log\')\n    for ext in [\'pdf\', \'png\']:\n        f = os.path.join(workDir, ""loss.""+ext)\n        fig.savefig(f)\n        print(""Created {}"".format(f))\n\nif __name__==\'__main__\':\n    main()\n'"
denoising/models.py,42,"b'import os\nimport numpy as np\n\nimport torch\n\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.nn.parameter import Parameter\n\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\nfrom block import block\n\nfrom qpth.qp import QPFunction\n\nclass ReluNet(nn.Module):\n    def __init__(self, nFeatures, nHidden, bn=False):\n        super().__init__()\n        self.bn = bn\n\n        self.fc1 = nn.Linear(nFeatures, nHidden)\n        self.fc2 = nn.Linear(nHidden, nFeatures)\n        if bn:\n            self.bn1 = nn.BatchNorm1d(nHidden)\n\n    def __call__(self, x):\n        x = F.relu(self.fc1(x))\n        if self.bn:\n            x = self.bn1(x)\n        x = self.fc2(x)\n        return x\n\nclass OptNet(nn.Module):\n    def __init__(self, nFeatures, args):\n        super(OptNet, self).__init__()\n\n        nHidden, neq, nineq = 2*nFeatures-1,0,2*nFeatures-2\n        assert(neq==0)\n\n        self.fc1 = nn.Linear(nFeatures, nHidden)\n        self.M = Variable(torch.tril(torch.ones(nHidden, nHidden)).cuda())\n\n        if args.tvInit:\n            Q = 1e-8*torch.eye(nHidden)\n            Q[:nFeatures,:nFeatures] = torch.eye(nFeatures)\n            self.L = Parameter(torch.potrf(Q))\n\n            D = torch.zeros(nFeatures-1, nFeatures)\n            D[:nFeatures-1,:nFeatures-1] = torch.eye(nFeatures-1)\n            D[:nFeatures-1,1:nFeatures] -= torch.eye(nFeatures-1)\n            G_ = block((( D, -torch.eye(nFeatures-1)),\n                        (-D, -torch.eye(nFeatures-1))))\n            self.G = Parameter(G_)\n            self.s0 = Parameter(torch.ones(2*nFeatures-2)+1e-6*torch.randn(2*nFeatures-2))\n            G_pinv = (G_.t().mm(G_)+1e-5*torch.eye(nHidden)).inverse().mm(G_.t())\n            self.z0 = Parameter(-G_pinv.mv(self.s0.data)+1e-6*torch.randn(nHidden))\n\n            lam = 21.21\n            W_fc1, b_fc1 = self.fc1.weight, self.fc1.bias\n            W_fc1.data[:,:] = 1e-3*torch.randn((2*nFeatures-1, nFeatures))\n            # W_fc1.data[:,:] = 0.0\n            W_fc1.data[:nFeatures,:nFeatures] += -torch.eye(nFeatures)\n            # b_fc1.data[:] = torch.zeros(2*nFeatures-1)\n            b_fc1.data[:] = 0.0\n            b_fc1.data[nFeatures:2*nFeatures-1] = lam\n        else:\n            self.L = Parameter(torch.tril(torch.rand(nHidden, nHidden)))\n            self.G = Parameter(torch.Tensor(nineq,nHidden).uniform_(-1,1))\n            self.z0 = Parameter(torch.zeros(nHidden))\n            self.s0 = Parameter(torch.ones(nineq))\n\n        self.nFeatures = nFeatures\n        self.nHidden = nHidden\n        self.neq = neq\n        self.nineq = nineq\n        self.args = args\n\n    def cuda(self):\n        # TODO: Is there a more automatic way?\n        for x in [self.L, self.G, self.z0, self.s0]:\n            x.data = x.data.cuda()\n\n        return super().cuda()\n\n    def forward(self, x):\n        nBatch = x.size(0)\n\n        x = self.fc1(x)\n\n        L = self.M*self.L\n        Q = L.mm(L.t()) + self.args.eps*Variable(torch.eye(self.nHidden)).cuda()\n        Q = Q.unsqueeze(0).expand(nBatch, self.nHidden, self.nHidden)\n        G = self.G.unsqueeze(0).expand(nBatch, self.nineq, self.nHidden)\n        h = self.G.mv(self.z0)+self.s0\n        h = h.unsqueeze(0).expand(nBatch, self.nineq)\n        e = Variable(torch.Tensor())\n        x = QPFunction()(Q, x, G, h, e, e)\n        x = x[:,:self.nFeatures]\n\n        return x\n\nclass OptNet_LearnD(nn.Module):\n    def __init__(self, nFeatures, args):\n        super().__init__()\n\n        nHidden, neq, nineq = 2*nFeatures-1,0,2*nFeatures-2\n        assert(neq==0)\n\n        # self.fc1 = nn.Linear(nFeatures, nHidden)\n        self.M = Variable(torch.tril(torch.ones(nHidden, nHidden)).cuda())\n\n        Q = 1e-8*torch.eye(nHidden)\n        Q[:nFeatures,:nFeatures] = torch.eye(nFeatures)\n        self.L = Variable(torch.potrf(Q))\n\n        self.D = Parameter(0.3*torch.randn(nFeatures-1, nFeatures))\n        # self.lam = Parameter(20.*torch.ones(1))\n        self.h = Variable(torch.zeros(nineq))\n\n        self.nFeatures = nFeatures\n        self.nHidden = nHidden\n        self.neq = neq\n        self.nineq = nineq\n        self.args = args\n\n    def cuda(self):\n        # TODO: Is there a more automatic way?\n        for x in [self.L, self.D, self.h]:\n            x.data = x.data.cuda()\n\n        return super().cuda()\n\n    def forward(self, x):\n        nBatch = x.size(0)\n\n        L = self.M*self.L\n        Q = L.mm(L.t()) + self.args.eps*Variable(torch.eye(self.nHidden)).cuda()\n        Q = Q.unsqueeze(0).expand(nBatch, self.nHidden, self.nHidden)\n        nI = Variable(-torch.eye(self.nFeatures-1).type_as(Q.data))\n        G = torch.cat((\n              torch.cat(( self.D, nI), 1),\n              torch.cat((-self.D, nI), 1)\n        ))\n        G = G.unsqueeze(0).expand(nBatch, self.nineq, self.nHidden)\n        h = self.h.unsqueeze(0).expand(nBatch, self.nineq)\n        e = Variable(torch.Tensor())\n        # p = torch.cat((-x, self.lam.unsqueeze(0).expand(nBatch, self.nFeatures-1)), 1)\n        p = torch.cat((-x, Parameter(13.*torch.ones(nBatch, self.nFeatures-1).cuda())), 1)\n        x = QPFunction()(Q.double(), p.double(), G.double(), h.double(), e, e).float()\n        x = x[:,:self.nFeatures]\n\n        return x\n'"
denoising/plot.py,0,"b'#!/usr/bin/env python3\n\nimport argparse\n\nimport matplotlib as mpl\nmpl.use(\'Agg\')\nimport matplotlib.pyplot as plt\nplt.style.use(\'bmh\')\nimport pandas as pd\nimport numpy as np\nimport math\n\nimport os\nimport sys\nimport json\nimport glob\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'workDir\', type=str)\n    args = parser.parse_args()\n\n    trainF = os.path.join(args.workDir, \'train.csv\')\n    testF = os.path.join(args.workDir, \'test.csv\')\n\n    trainDf = pd.read_csv(trainF, sep=\',\')\n    testDf = pd.read_csv(testF, sep=\',\')\n\n    plotLoss(trainDf, testDf, args.workDir)\n\n    initDf = os.path.join(args.workDir, \'D.init\')\n    if os.path.exists(initDf):\n        initD = np.loadtxt(initDf)\n        latestD = np.loadtxt(os.path.join(args.workDir, \'D.latest\'))\n        plotD(initD, latestD, args.workDir)\n\ndef plotLoss(trainDf, testDf, workDir):\n    # fig, ax = plt.subplots(1, 1, figsize=(5,2))\n    fig, ax = plt.subplots(1, 1)\n    # fig.tight_layout()\n\n    trainEpoch = trainDf[\'epoch\'].values\n    trainLoss = trainDf[\'loss\'].values\n\n    N = len(trainEpoch) // math.ceil(trainEpoch[-1])\n    trainEpoch_, trainLoss_ = rolling(N, trainEpoch, trainLoss)\n    plt.plot(trainEpoch_, trainLoss_, label=\'Train\')\n    # plt.plot(trainEpoch, trainLoss, label=\'Train\')\n    if not testDf.empty:\n        plt.plot(testDf[\'epoch\'].values, testDf[\'loss\'].values, label=\'Test\')\n    plt.xlabel(""Epoch"")\n    plt.ylabel(""MSE"")\n    plt.xlim(xmin=0)\n    plt.grid(b=True, which=\'major\', color=\'k\', linestyle=\'-\')\n    plt.grid(b=True, which=\'minor\', color=\'k\', linestyle=\'--\', alpha=0.2)\n    plt.legend()\n    ax.set_yscale(\'log\')\n    for ext in [\'pdf\', \'png\']:\n        f = os.path.join(workDir, ""loss.""+ext)\n        fig.savefig(f)\n        print(""Created {}"".format(f))\n\ndef plotD(initD, latestD, workDir):\n    def p(D, fname):\n        plt.clf()\n        lim = max(np.abs(np.min(D)), np.abs(np.max(D)))\n        clim = (-lim, lim)\n        plt.imshow(D, cmap=\'bwr\', interpolation=\'nearest\', clim=clim)\n        plt.colorbar()\n        plt.savefig(os.path.join(workDir, fname))\n\n    p(initD, \'initD.png\')\n    p(latestD, \'latestD.png\')\n\n    latestDs = latestD**6\n    latestDs = latestDs/np.sum(latestDs, axis=1)[:,None]\n    I = np.argsort(latestDs.dot(np.arange(latestDs.shape[1])))\n    latestDs = latestD[I]\n    initDs = initD[I]\n\n    p(initDs, \'initD_sorted.png\')\n    p(latestDs, \'latestD_sorted.png\')\n\n    # Dcombined = np.concatenate((initDs, np.zeros((initD.shape[0], 10)), latestDs), axis=1)\n    # p(Dcombined, \'Dcombined.png\')\n\ndef rolling(N, i, loss):\n    i_ = i[N-1:]\n    K = np.full(N, 1./N)\n    loss_ = np.convolve(loss, K, \'valid\')\n    return i_, loss_\n\nif __name__ == \'__main__\':\n    main()\n'"
profile/optnet-forward.py,4,"b'#!/usr/bin/env python3\n\nimport argparse\nimport sys\n\nimport numpy as np\nimport numpy.random as npr\n\nimport adact\nimport adact_forward_ip as aip\n\nimport itertools\nimport time\n\nimport torch\n\nimport sys\nfrom IPython.core import ultratb\nsys.excepthook = ultratb.FormattedTB(mode=\'Verbose\',\n     color_scheme=\'Linux\', call_pdb=1)\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--nTrials\', type=int, default=5)\n    parser.add_argument(\'--nBatch\', type=int, default=128)\n    args = parser.parse_args()\n\n    npr.seed(0)\n\n    # print(\'==== CPU ===\\n\')\n    # prof(args, False)\n\n    print(\'\\n\\n==== GPU ===\\n\')\n    prof(args, True)\n\ndef prof(args, cuda):\n    print(\'|    nz |   neq | nineq | single | batched |\')\n    print(\'|-------+-------+-------+--------+---------|\')\n    for nz,neq,nineq in itertools.product([10,100,200], [0,10,50], [10,50]):\n        if nz >= neq and nz >= nineq:\n            times = []\n            for i in range(args.nTrials):\n                times.append(prof_instance(nz, neq, nineq, args.nBatch, cuda))\n            times = np.array(times)\n            cp, pdipm = times.mean(axis=0)\n            cp_sd, pdipm_sd = times.std(axis=0)\n            print(""| {:5d} | {:5d} | {:5d} | {:.3f} +/- {:.3f} | {:.3f} +/- {:.3f} |"".format(\n                nz, neq, nineq, cp, cp_sd, pdipm, pdipm_sd))\n\ndef prof_instance(nz, neq, nineq, nBatch, cuda):\n    L = np.tril(npr.uniform(0,1, (nz,nz))) + np.eye(nz,nz)\n    G = npr.randn(nineq,nz)\n    A = npr.randn(neq,nz)\n    z0 = npr.randn(nz)\n    s0 = np.ones(nineq)\n    p = npr.randn(nBatch,nz)\n\n    p, L, G, A, z0, s0 = [torch.Tensor(x) for x in [p, L, G, A, z0, s0]]\n    Q = torch.mm(L, L.t())+0.001*torch.eye(nz).type_as(L)\n    if cuda:\n        p, L, Q, G, A, z0, s0 = [x.cuda() for x in [p, L, Q, G, A, z0, s0]]\n    b = torch.mv(A, z0) if neq > 0 else None\n    h = torch.mv(G, z0)+s0\n\n    af = adact.AdactFunction()\n\n    single_results = []\n    start = time.time()\n    U_Q, U_S, R = aip.pre_factor_kkt(Q, G, A)\n    for i in range(nBatch):\n        single_results.append(aip.forward_single(p[i], Q, G, A, b, h, U_Q, U_S, R))\n    single_time = time.time()-start\n\n    start = time.time()\n    Q_LU, S_LU, R = aip.pre_factor_kkt_batch(Q, G, A, nBatch)\n    zhat_b, nu_b, lam_b = aip.forward_batch(p, Q, G, A, b, h, Q_LU, S_LU, R)\n    batched_time = time.time()-start\n\n    zhat_diff = (single_results[0][0] - zhat_b[0]).norm()\n    lam_diff = (single_results[0][2] - lam_b[0]).norm()\n    eps = 0.1 # Pretty relaxed.\n    if zhat_diff > eps or lam_diff > eps:\n        print(\'===========\')\n        print(""Warning: Single and batched solutions might not match."")\n        print(""  + zhat_diff: {}"".format(zhat_diff))\n        print(""  + lam_diff: {}"".format(lam_diff))\n        print(""  + (nz, neq, nineq, nBatch) = ({}, {}, {}, {})"".format(\n            nz, neq, nineq, nBatch))\n        print(\'===========\')\n\n    return single_time, batched_time\n\nif __name__==\'__main__\':\n    main()\n'"
profile/optnet-single.py,4,"b'#!/usr/bin/env python3\n\nimport argparse\nimport sys\n\nimport numpy as np\nimport numpy.random as npr\n\nimport adact\nimport adact_forward_ip as aip\n\nimport itertools\nimport time\n\nimport torch\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--nTrials\', type=int, default=5)\n    parser.add_argument(\'--nBatch\', type=int, default=128)\n    args = parser.parse_args()\n\n    npr.seed(0)\n\n    print(\'==== CPU ===\\n\')\n    prof(args, False)\n\n    print(\'\\n\\n==== GPU ===\\n\')\n    prof(args, True)\n\ndef prof(args, cuda):\n    print(\'|    nz |   neq | nineq | cvxpy | pdipm |\')\n    print(\'|-------+-------+-------+-------+-------|\')\n    for nz,neq,nineq in itertools.product([10,100,200], [0,10,50], [10,50]):\n        if nz >= neq and nz >= nineq:\n            times = []\n            for i in range(args.nTrials):\n                times.append(prof_instance(nz, neq, nineq, args.nBatch, cuda))\n            times = np.array(times)\n            cp, pdipm = times.mean(axis=0)\n            cp_sd, pdipm_sd = times.std(axis=0)\n            print(""| {:5d} | {:5d} | {:5d} | {:.3f} +/- {:.3f} | {:.3f} +/- {:.3f} |"".format(\n                nz, neq, nineq, cp, cp_sd, pdipm, pdipm_sd))\n\ndef prof_instance(nz, neq, nineq, nIter, cuda):\n    L = np.tril(npr.uniform(0,1, (nz,nz))) + np.eye(nz,nz)\n    G = npr.randn(nineq,nz)\n    A = npr.randn(neq,nz)\n    z0 = npr.randn(nz)\n    s0 = np.ones(nineq)\n    p = npr.randn(nz)\n\n    p, L, G, A, z0, s0 = [torch.Tensor(x) for x in [p, L, G, A, z0, s0]]\n    Q = torch.mm(L, L.t())+0.001*torch.eye(nz).type_as(L)\n    if cuda:\n        p, L, Q, G, A, z0, s0 = [x.cuda() for x in [p, L, Q, G, A, z0, s0]]\n\n    af = adact.AdactFunction()\n\n    start = time.time()\n    # One-time cost for numpy conversion.\n    p_np, L_np, G_np, A_np, z0_np, s0_np = [adact.toNp(v) for v in [p, L, G, A, z0, s0]]\n    cp = time.time()-start\n    for i in range(nIter):\n        start = time.time()\n        zhat, nu, lam = af.forward_single_np(p_np, L_np, G_np, A_np, z0_np, s0_np)\n        cp += time.time()-start\n\n    b = torch.mv(A, z0) if neq > 0 else None\n    h = torch.mv(G, z0)+s0\n    L_Q, L_S, R = aip.pre_factor_kkt(Q, G, A, nineq, neq)\n    pdipm = []\n    for i in range(nIter):\n        start = time.time()\n        zhat_ip, nu_ip, lam_ip = aip.forward_single(p, Q, G, A, b, h, L_Q, L_S, R)\n        pdipm.append(time.time()-start)\n    return cp, np.sum(pdipm)\n\nif __name__==\'__main__\':\n    main()\n'"
sudoku/create.py,1,"b'#!/usr/bin/env python3\n#\n# Some portions from: https://www.ocf.berkeley.edu/~arel/sudoku/main.html\n\nimport argparse\nimport numpy as np\nimport numpy.random as npr\nimport torch\n\nfrom tqdm import tqdm\n\nimport os, sys\nimport shutil\n\nimport random, copy\n\nimport matplotlib as mpl\nmpl.use(\'Agg\')\nimport matplotlib.pyplot as plt\nplt.style.use(\'bmh\')\n\nimport sys\nfrom IPython.core import ultratb\nsys.excepthook = ultratb.FormattedTB(mode=\'Verbose\',\n     color_scheme=\'Linux\', call_pdb=1)\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--boardSz\', type=int, default=2)\n    parser.add_argument(\'--nSamples\', type=int, default=10000)\n    parser.add_argument(\'--data\', type=str, default=\'data\')\n    args = parser.parse_args()\n\n    npr.seed(0)\n\n    save = os.path.join(args.data, str(args.boardSz))\n    if os.path.isdir(save):\n        shutil.rmtree(save)\n    os.makedirs(save)\n\n    X = []\n    Y = []\n    for i in tqdm(range(args.nSamples)):\n        Xi, Yi = sample(args)\n        X.append(Xi)\n        Y.append(Yi)\n\n    X = np.array(X)\n    Y = np.array(Y)\n\n    for loc,arr in ((\'features.pt\', X), (\'labels.pt\', Y)):\n        fname = os.path.join(save, loc)\n        with open(fname, \'wb\') as f:\n            torch.save(torch.Tensor(arr), f)\n        print(""Created {}"".format(fname))\n\ndef sample(args):\n    solution = construct_puzzle_solution(args.boardSz)\n    Nsq = args.boardSz*args.boardSz\n    nKeep = npr.randint(0, Nsq)\n    board, nKept = pluck(copy.deepcopy(solution), nKeep)\n    solution = toOneHot(solution)\n    board = toOneHot(board)\n    return board, solution\n\ndef toOneHot(X):\n    X = np.array(X)\n    Nsq = X.shape[0]\n    Y = np.zeros((Nsq, Nsq, Nsq))\n    for i in range(1,Nsq+1):\n        Y[:,:,i-1][X == i] = 1.0\n    return Y\n\ndef construct_puzzle_solution(N):\n    """"""\n    Randomly arrange numbers in a grid while making all rows, columns and\n    squares (sub-grids) contain the numbers 1 through Nsq.\n\n    For example, ""sample"" (above) could be the output of this function. """"""\n    # Loop until we\'re able to fill all N^4 cells with numbers, while\n    # satisfying the constraints above.\n    Nsq = N*N\n    while True:\n        try:\n            puzzle  = [[0]*Nsq for i in range(Nsq)] # start with blank puzzle\n            rows    = [set(range(1,Nsq+1)) for i in range(Nsq)] # set of available\n            columns = [set(range(1,Nsq+1)) for i in range(Nsq)] #   numbers for each\n            squares = [set(range(1,Nsq+1)) for i in range(Nsq)] #   row, column and square\n            for i in range(Nsq):\n                for j in range(Nsq):\n                    # pick a number for cell (i,j) from the set of remaining available numbers\n                    choices = rows[i].intersection(columns[j]).intersection(\n                        squares[(i//N)*N + j//N])\n                    choice  = random.choice(list(choices))\n\n                    puzzle[i][j] = choice\n\n                    rows[i].discard(choice)\n                    columns[j].discard(choice)\n                    squares[(i//N)*N + j//N].discard(choice)\n\n            # success! every cell is filled.\n            return puzzle\n\n        except IndexError:\n            # if there is an IndexError, we have worked ourselves in a corner (we just start over)\n            pass\n\ndef pluck(puzzle, nKeep=0):\n    """"""\n    Randomly pluck out K cells (numbers) from the solved puzzle grid, ensuring that any\n    plucked number can still be deduced from the remaining cells.\n\n    For deduction to be possible, each other cell in the plucked number\'s row, column,\n    or square must not be able to contain that number. """"""\n\n    Nsq = len(puzzle)\n    N = int(np.sqrt(Nsq))\n\n\n    def canBeA(puz, i, j, c):\n        """"""\n        Answers the question: can the cell (i,j) in the puzzle ""puz"" contain the number\n        in cell ""c""? """"""\n        v = puz[c//Nsq][c%Nsq]\n        if puz[i][j] == v: return True\n        if puz[i][j] in range(1,Nsq+1): return False\n\n        for m in range(Nsq): # test row, col, square\n            # if not the cell itself, and the mth cell of the group contains the value v, then ""no""\n            if not (m==c//Nsq and j==c%Nsq) and puz[m][j] == v: return False\n            if not (i==c//Nsq and m==c%Nsq) and puz[i][m] == v: return False\n            if not ((i//N)*N + m//N==c//Nsq and (j//N)*N + m%N==c%Nsq) \\\n               and puz[(i//N)*N + m//N][(j//N)*N + m%N] == v:\n                return False\n\n        return True\n\n\n    """"""\n    starts with a set of all N^4 cells, and tries to remove one (randomly) at a time\n    but not before checking that the cell can still be deduced from the remaining cells. """"""\n    cells     = set(range(Nsq*Nsq))\n    cellsleft = cells.copy()\n    while len(cells) > nKeep and len(cellsleft):\n        cell = random.choice(list(cellsleft)) # choose a cell from ones we haven\'t tried\n        cellsleft.discard(cell) # record that we are trying this cell\n\n        # row, col and square record whether another cell in those groups could also take\n        # on the value we are trying to pluck. (If another cell can, then we can\'t use the\n        # group to deduce this value.) If all three groups are True, then we cannot pluck\n        # this cell and must try another one.\n        row = col = square = False\n\n        for i in range(Nsq):\n            if i != cell//Nsq:\n                if canBeA(puzzle, i, cell%Nsq, cell): row = True\n            if i != cell%Nsq:\n                if canBeA(puzzle, cell//Nsq, i, cell): col = True\n            if not (((cell//Nsq)/N)*N + i//N == cell//Nsq and ((cell//Nsq)%N)*N + i%N == cell%Nsq):\n                if canBeA(puzzle, ((cell//Nsq)//N)*N + i//N,\n                          ((cell//Nsq)%N)*N + i%N, cell): square = True\n\n        if row and col and square:\n            continue # could not pluck this cell, try again.\n        else:\n            # this is a pluckable cell!\n            puzzle[cell//Nsq][cell%Nsq] = 0 # 0 denotes a blank cell\n            cells.discard(cell) # remove from the set of visible cells (pluck it)\n            # we don\'t need to reset ""cellsleft"" because if a cell was not pluckable\n            # earlier, then it will still not be pluckable now (with less information\n            # on the board).\n\n    # This is the puzzle we found, in all its glory.\n    return (puzzle, len(cells))\n\nif __name__==\'__main__\':\n    main()\n'"
sudoku/models.py,50,"b'import os\nimport numpy as np\n\nfrom itertools import product\n\nimport scipy.sparse as spa\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Module\nimport torch.optim as optim\n\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.nn.parameter import Parameter\n\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\nimport cvxpy as cp\n\nfrom block import block\n\nfrom qpth.qp import SpQPFunction, QPFunction\n\ntry:\n    from osqpth.osqpth import OSQP, DiffModes\nexcept:\n    pass\n\n\nclass FC(nn.Module):\n    def __init__(self, nFeatures, nHidden, bn=False):\n        super().__init__()\n        self.bn = bn\n\n        fcs = []\n        prevSz = nFeatures\n        for sz in nHidden:\n            fc = nn.Linear(prevSz, sz)\n            prevSz = sz\n            fcs.append(fc)\n        for sz in list(reversed(nHidden))+[nFeatures]:\n            fc = nn.Linear(prevSz, sz)\n            prevSz = sz\n            fcs.append(fc)\n        self.fcs = nn.ModuleList(fcs)\n\n    def __call__(self, x):\n        nBatch = x.size(0)\n        Nsq = x.size(1)\n        in_x = x\n        x = x.view(nBatch, -1)\n\n        for fc in self.fcs:\n            x = F.relu(fc(x))\n\n        x = x.view_as(in_x)\n        ex = x.exp()\n        exs = ex.sum(3).expand(nBatch, Nsq, Nsq, Nsq)\n        x = ex/exs\n\n        return x\n\nclass Conv(nn.Module):\n    def __init__(self, boardSz):\n        super().__init__()\n\n        self.boardSz = boardSz\n\n        convs = []\n        Nsq = boardSz**2\n        prevSz = Nsq\n        szs = [512]*10 + [Nsq]\n        for sz in szs:\n            conv = nn.Conv2d(prevSz, sz, kernel_size=3, padding=1)\n            convs.append(conv)\n            prevSz = sz\n\n        self.convs = nn.ModuleList(convs)\n\n    def __call__(self, x):\n        nBatch = x.size(0)\n        Nsq = x.size(1)\n\n        for i in range(len(self.convs)-1):\n            x = F.relu(self.convs[i](x))\n        x = self.convs[-1](x)\n\n        ex = x.exp()\n        exs = ex.sum(3).expand(nBatch, Nsq, Nsq, Nsq)\n        x = ex/exs\n\n        return x\n\ndef get_sudoku_matrix(n):\n    X = np.array([[cp.Variable(n**2) for i in range(n**2)] for j in range(n**2)])\n    cons = ([x >= 0 for row in X for x in row] +\n            [cp.sum(x) == 1 for row in X for x in row] +\n            [sum(row) == np.ones(n**2) for row in X] +\n            [sum([row[i] for row in X]) == np.ones(n**2) for i in range(n**2)] +\n            [sum([sum(row[i:i+n]) for row in X[j:j+n]]) == np.ones(n**2) for i in range(0,n**2,n) for j in range(0, n**2, n)])\n    f = sum([cp.sum(x) for row in X for x in row])\n    prob = cp.Problem(cp.Minimize(f), cons)\n\n    A = np.asarray(prob.get_problem_data(cp.ECOS)[0][""A""].todense())\n    A0 = [A[0]]\n    rank = 1\n    for i in range(1,A.shape[0]):\n        if np.linalg.matrix_rank(A0+[A[i]], tol=1e-12) > rank:\n            A0.append(A[i])\n            rank += 1\n\n    return np.array(A0)\n\n\nclass OptNetEq(nn.Module):\n    def __init__(self, n, Qpenalty, qp_solver, trueInit=False):\n        super().__init__()\n\n        self.qp_solver = qp_solver\n\n        nx = (n**2)**3\n        self.Q = Variable(Qpenalty*torch.eye(nx).double().cuda())\n        self.Q_idx = spa.csc_matrix(self.Q.detach().cpu().numpy()).nonzero()\n\n        self.G = Variable(-torch.eye(nx).double().cuda())\n        self.h = Variable(torch.zeros(nx).double().cuda())\n        t = get_sudoku_matrix(n)\n\n        if trueInit:\n            self.A = Parameter(torch.DoubleTensor(t).cuda())\n        else:\n            self.A = Parameter(torch.rand(t.shape).double().cuda())\n        self.log_z0 = Parameter(torch.zeros(nx).double().cuda())\n        # self.b = Variable(torch.ones(self.A.size(0)).double().cuda())\n\n        if self.qp_solver == \'osqpth\':\n            t = torch.cat((self.A, self.G), dim=0)\n            self.AG_idx = spa.csc_matrix(t.detach().cpu().numpy()).nonzero()\n\n    # @profile\n    def forward(self, puzzles):\n        nBatch = puzzles.size(0)\n\n        p = -puzzles.view(nBatch, -1)\n        b = self.A.mv(self.log_z0.exp())\n\n        if self.qp_solver == \'qpth\':\n            y = QPFunction(verbose=-1)(\n                self.Q, p.double(), self.G, self.h, self.A, b\n            ).float().view_as(puzzles)\n        elif self.qp_solver == \'osqpth\':\n            _l = torch.cat(\n                (b, torch.full(self.h.shape, float(\'-inf\'),\n                            device=self.h.device, dtype=self.h.dtype)),\n                dim=0)\n            _u = torch.cat((b, self.h), dim=0)\n            Q_data = self.Q[self.Q_idx[0], self.Q_idx[1]]\n\n            AG = torch.cat((self.A, self.G), dim=0)\n            AG_data = AG[self.AG_idx[0], self.AG_idx[1]]\n            y = OSQP(self.Q_idx, self.Q.shape, self.AG_idx, AG.shape,\n                     diff_mode=DiffModes.FULL)(\n                Q_data, p.double(), AG_data, _l, _u).float().view_as(puzzles)\n        else:\n            assert False\n\n        return y\n\n\nclass SpOptNetEq(nn.Module):\n    def __init__(self, n, Qpenalty, trueInit=False):\n        super().__init__()\n        nx = (n**2)**3\n        self.nx = nx\n\n        spTensor = torch.cuda.sparse.DoubleTensor\n        iTensor = torch.cuda.LongTensor\n        dTensor = torch.cuda.DoubleTensor\n\n        self.Qi = iTensor([range(nx), range(nx)])\n        self.Qv = Variable(dTensor(nx).fill_(Qpenalty))\n        self.Qsz = torch.Size([nx, nx])\n\n        self.Gi = iTensor([range(nx), range(nx)])\n        self.Gv = Variable(dTensor(nx).fill_(-1.0))\n        self.Gsz = torch.Size([nx, nx])\n        self.h = Variable(torch.zeros(nx).double().cuda())\n\n        t = get_sudoku_matrix(n)\n        neq = t.shape[0]\n        if trueInit:\n            I = t != 0\n            self.Av = Parameter(dTensor(t[I]))\n            Ai_np = np.nonzero(t)\n            self.Ai = torch.stack((torch.LongTensor(Ai_np[0]),\n                                   torch.LongTensor(Ai_np[1]))).cuda()\n            self.Asz = torch.Size([neq, nx])\n        else:\n            # TODO: This is very dense:\n            self.Ai = torch.stack((iTensor(list(range(neq))).unsqueeze(1).repeat(1, nx).view(-1),\n                                iTensor(list(range(nx))).repeat(neq)))\n            self.Av = Parameter(dTensor(neq*nx).uniform_())\n            self.Asz = torch.Size([neq, nx])\n        self.b = Variable(torch.ones(neq).double().cuda())\n\n    def forward(self, puzzles):\n        nBatch = puzzles.size(0)\n\n        p = -puzzles.view(nBatch,-1).double()\n\n        return SpQPFunction(\n            self.Qi, self.Qsz, self.Gi, self.Gsz, self.Ai, self.Asz, verbose=-1)(\n                self.Qv.expand(nBatch, self.Qv.size(0)),\n                p,\n                self.Gv.expand(nBatch, self.Gv.size(0)),\n                self.h.expand(nBatch, self.h.size(0)),\n                self.Av.expand(nBatch, self.Av.size(0)),\n                self.b.expand(nBatch, self.b.size(0))\n        ).float().view_as(puzzles)\n\n\nclass OptNetIneq(nn.Module):\n    def __init__(self, n, Qpenalty, nineq):\n        super().__init__()\n        nx = (n**2)**3\n        self.Q = Variable(Qpenalty*torch.eye(nx).double().cuda())\n        self.G1 = Variable(-torch.eye(nx).double().cuda())\n        self.h1 = Variable(torch.zeros(nx).double().cuda())\n        # if trueInit:\n        #     self.A = Parameter(torch.DoubleTensor(get_sudoku_matrix(n)).cuda())\n        # else:\n        #     # t = get_sudoku_matrix(n)\n        #     # self.A = Parameter(torch.rand(t.shape).double().cuda())\n        #     # import IPython, sys; IPython.embed(); sys.exit(-1)\n        self.A = Parameter(torch.rand(50,nx).double().cuda())\n        self.G2 = Parameter(torch.Tensor(128, nx).uniform_(-1,1).double().cuda())\n        self.z2 = Parameter(torch.zeros(nx).double().cuda())\n        self.s2 = Parameter(torch.ones(128).double().cuda())\n        # self.b = Variable(torch.ones(self.A.size(0)).double().cuda())\n\n    def forward(self, puzzles):\n        nBatch = puzzles.size(0)\n\n        p = -puzzles.view(nBatch,-1)\n\n        h2 = self.G2.mv(self.z2)+self.s2\n        G = torch.cat((self.G1, self.G2), 0)\n        h = torch.cat((self.h1, h2), 0)\n        e = Variable(torch.Tensor())\n\n        return QPFunction(verbose=False)(\n            self.Q, p.double(), G, h, e, e\n        ).float().view_as(puzzles)\n\nclass OptNetLatent(nn.Module):\n    def __init__(self, n, Qpenalty, nLatent, nineq, trueInit=False):\n        super().__init__()\n        nx = (n**2)**3\n        self.fc_in = nn.Linear(nx, nLatent)\n        self.Q = Variable(Qpenalty*torch.eye(nLatent).cuda())\n        self.G = Parameter(torch.Tensor(nineq, nLatent).uniform_(-1,1).cuda())\n        self.z = Parameter(torch.zeros(nLatent).cuda())\n        self.s = Parameter(torch.ones(nineq).cuda())\n        self.fc_out = nn.Linear(nLatent, nx)\n\n    def forward(self, puzzles):\n        nBatch = puzzles.size(0)\n\n        x = puzzles.view(nBatch,-1)\n        x = self.fc_in(x)\n\n        e = Variable(torch.Tensor())\n\n        h = self.G.mv(self.z)+self.s\n        x = QPFunction(verbose=False)(\n            self.Q, x, self.G, h, e, e,\n        )\n\n        x = self.fc_out(x)\n        x = x.view_as(puzzles)\n        return x\n\n\n# if __name__==""__main__"":\n#     sudoku = SolveSudoku(2, 0.2)\n#     puzzle = [[4, 0, 0, 0], [0,0,4,0], [0,2,0,0], [0,0,0,1]]\n#     Y = Variable(torch.DoubleTensor(np.array([[np.array(np.eye(5,4,-1)[i,:]) for i in row] for row in puzzle])).cuda())\n#     solution = sudoku(Y.unsqueeze(0))\n#     print(solution.view(1,4,4,4))\n'"
sudoku/plot.py,0,"b'#!/usr/bin/env python3\n\nimport argparse\n\nimport matplotlib as mpl\nmpl.use(\'Agg\')\nimport matplotlib.pyplot as plt\nplt.style.use(\'bmh\')\nimport pandas as pd\nimport numpy as np\nimport math\n\nimport os\nimport sys\nimport json\nimport glob\n\ndef main():\n    # import sys\n    # from IPython.core import ultratb\n    # sys.excepthook = ultratb.FormattedTB(mode=\'Verbose\',\n    #     color_scheme=\'Linux\', call_pdb=1)\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'workDir\', type=str)\n    args = parser.parse_args()\n\n    trainF = os.path.join(args.workDir, \'train.csv\')\n    testF = os.path.join(args.workDir, \'test.csv\')\n\n    trainDf = pd.read_csv(trainF, sep=\',\')\n    testDf = pd.read_csv(testF, sep=\',\')\n\n    plotLoss(trainDf, testDf, args.workDir)\n    plotErr(trainDf, testDf, args.workDir)\n\n    initDf = os.path.join(args.workDir, \'D.init\')\n    if os.path.exists(initDf):\n        initD = np.loadtxt(initDf)\n        latestD = np.loadtxt(os.path.join(args.workDir, \'D.latest\'))\n        plotD(initD, latestD, args.workDir)\n\n    loss_fname = os.path.join(args.workDir, \'loss.png\')\n    err_fname = os.path.join(args.workDir, \'err.png\')\n    loss_err_fname = os.path.join(args.workDir, \'loss-error.png\')\n    os.system(\'convert +append ""{}"" ""{}"" ""{}""\'.format(loss_fname, err_fname, loss_err_fname))\n    print(\'Created {}\'.format(loss_err_fname))\n\ndef plotLoss(trainDf, testDf, workDir):\n    # fig, ax = plt.subplots(1, 1, figsize=(5,2))\n    fig, ax = plt.subplots(1, 1)\n    # fig.tight_layout()\n\n    trainEpoch = trainDf[\'epoch\'].values\n    trainLoss = trainDf[\'loss\'].values\n\n    N = np.argmax(trainEpoch==1.0)\n    trainEpoch = trainEpoch[N-1:]\n    trainLoss = np.convolve(trainLoss, np.full(N, 1./N), mode=\'valid\')\n    plt.plot(trainEpoch, trainLoss, label=\'Train\')\n    if not testDf.empty:\n        plt.plot(testDf[\'epoch\'].values, testDf[\'loss\'].values, label=\'Test\')\n    plt.xlabel(""Epoch"")\n    plt.ylabel(""MSE"")\n    plt.xlim(xmin=0)\n    plt.grid(b=True, which=\'major\', color=\'k\', linestyle=\'-\')\n    plt.grid(b=True, which=\'minor\', color=\'k\', linestyle=\'--\', alpha=0.2)\n    plt.legend()\n    # ax.set_yscale(\'log\')\n    ax.set_ylim(0, None)\n    for ext in [\'pdf\', \'png\']:\n        f = os.path.join(workDir, ""loss.""+ext)\n        fig.savefig(f)\n        print(""Created {}"".format(f))\n\ndef plotErr(trainDf, testDf, workDir):\n    # fig, ax = plt.subplots(1, 1, figsize=(5,2))\n    fig, ax = plt.subplots(1, 1)\n    # fig.tight_layout()\n\n    trainEpoch = trainDf[\'epoch\'].values\n    trainLoss = trainDf[\'err\'].values\n\n    N = np.argmax(trainEpoch==1.0)\n    trainEpoch = trainEpoch[N-1:]\n    trainLoss = np.convolve(trainLoss, np.full(N, 1./N), mode=\'valid\')\n    plt.plot(trainEpoch, trainLoss, label=\'Train\')\n    if not testDf.empty:\n        plt.plot(testDf[\'epoch\'].values, testDf[\'err\'].values, label=\'Test\')\n    plt.xlabel(""Epoch"")\n    plt.ylabel(""Error"")\n    plt.xlim(xmin=0)\n    plt.grid(b=True, which=\'major\', color=\'k\', linestyle=\'-\')\n    plt.grid(b=True, which=\'minor\', color=\'k\', linestyle=\'--\', alpha=0.2)\n    plt.legend()\n    # ax.set_yscale(\'log\')\n    ax.set_ylim(0, None)\n    for ext in [\'pdf\', \'png\']:\n        f = os.path.join(workDir, ""err.""+ext)\n        fig.savefig(f)\n        print(""Created {}"".format(f))\n\ndef plotD(initD, latestD, workDir):\n    def p(D, fname):\n        plt.clf()\n        lim = max(np.abs(np.min(D)), np.abs(np.max(D)))\n        clim = (-lim, lim)\n        plt.imshow(D, cmap=\'bwr\', interpolation=\'nearest\', clim=clim)\n        plt.colorbar()\n        plt.savefig(os.path.join(workDir, fname))\n\n    p(initD, \'initD.png\')\n    p(latestD, \'latestD.png\')\n\n    latestDs = latestD**6\n    latestDs = latestDs/np.sum(latestDs, axis=1)[:,None]\n    I = np.argsort(latestDs.dot(np.arange(latestDs.shape[1])))\n    latestDs = latestD[I]\n    initDs = initD[I]\n\n    p(initDs, \'initD_sorted.png\')\n    p(latestDs, \'latestD_sorted.png\')\n\n    # Dcombined = np.concatenate((initDs, np.zeros((initD.shape[0], 10)), latestDs), axis=1)\n    # p(Dcombined, \'Dcombined.png\')\n\nif __name__ == \'__main__\':\n    main()\n'"
sudoku/prof-sparse.py,6,"b""#!/usr/bin/env python3\n\nimport argparse\nimport csv\nimport os\nimport shutil\nfrom tqdm import tqdm\nimport time\n\ntry: import setGPU\nexcept ImportError: pass\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\nimport numpy as np\nimport numpy.random as npr\n\nimport sys\n\nimport matplotlib as mpl\nmpl.use('Agg')\nimport matplotlib.pyplot as plt\nplt.style.use('bmh')\n\nimport setproctitle\n\nimport models\n\nimport sys\nfrom IPython.core import ultratb\nsys.excepthook = ultratb.FormattedTB(mode='Verbose',\n     color_scheme='Linux', call_pdb=1)\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--no-cuda', action='store_true')\n    parser.add_argument('--nTrials', type=int, default=5)\n    # parser.add_argument('--boardSz', type=int, default=2)\n    # parser.add_argument('--batchSz', type=int, default=150)\n    parser.add_argument('--Qpenalty', type=float, default=0.1)\n    args = parser.parse_args()\n\n    args.cuda = not args.no_cuda and torch.cuda.is_available()\n    setproctitle.setproctitle('bamos.sudoku.prof-sparse')\n\n    print('=== nTrials: {}'.format(args.nTrials))\n    print('| {:8s} | {:8s} | {:21s} | {:21s} |'.format(\n        'boardSz', 'batchSz', 'dense forward (s)', 'sparse forward (s)'))\n    for boardSz in [2,3]:\n        with open('data/{}/features.pt'.format(boardSz), 'rb') as f:\n            X = torch.load(f)\n        with open('data/{}/labels.pt'.format(boardSz), 'rb') as f:\n            Y = torch.load(f)\n        N, nFeatures = X.size(0), int(np.prod(X.size()[1:]))\n\n        for batchSz in [1, 64, 128]:\n            dmodel = models.OptNetEq(boardSz, args.Qpenalty, trueInit=True)\n            spmodel = models.SpOptNetEq(boardSz, args.Qpenalty, trueInit=True)\n            if args.cuda:\n                dmodel = dmodel.cuda()\n                spmodel = spmodel.cuda()\n\n            dtimes = []\n            sptimes = []\n            for i in range(args.nTrials):\n                Xbatch = Variable(X[i*batchSz:(i+1)*batchSz])\n                Ybatch = Variable(Y[i*batchSz:(i+1)*batchSz])\n                if args.cuda:\n                    Xbatch = Xbatch.cuda()\n                    Ybatch = Ybatch.cuda()\n\n                # Make sure buffers are initialized.\n                # dmodel(Xbatch)\n                # spmodel(Xbatch)\n\n                start = time.time()\n                # dmodel(Xbatch)\n                dtimes.append(time.time()-start)\n\n                start = time.time()\n                spmodel(Xbatch)\n                sptimes.append(time.time()-start)\n\n            print('| {:8d} | {:8d} | {:.2e} +/- {:.2e} | {:.2e} +/- {:.2e} |'.format(\n                boardSz, batchSz, np.mean(dtimes), np.std(dtimes),\n                np.mean(sptimes), np.std(sptimes)))\n\nif __name__=='__main__':\n    main()\n"""
sudoku/train.py,13,"b'#!/usr/bin/env python3\n\nimport argparse\nimport csv\nimport os\nimport shutil\nfrom tqdm import tqdm\n\ntry: import setGPU\nexcept ImportError: pass\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\nimport numpy as np\nimport numpy.random as npr\n\nimport sys\nimport time\n\nimport matplotlib as mpl\nmpl.use(\'Agg\')\nimport matplotlib.pyplot as plt\nplt.style.use(\'bmh\')\n\nimport setproctitle\n\nimport models\n\nimport sys\nfrom IPython.core import ultratb\nsys.excepthook = ultratb.FormattedTB(mode=\'Verbose\',\n     color_scheme=\'Linux\', call_pdb=1)\n\ndef print_header(msg):\n    print(\'===>\', msg)\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--no-cuda\', action=\'store_true\')\n    parser.add_argument(\'--boardSz\', type=int, default=2)\n    parser.add_argument(\'--batchSz\', type=int, default=150)\n    parser.add_argument(\'--testBatchSz\', type=int, default=200)\n    parser.add_argument(\'--nEpoch\', type=int, default=100)\n    parser.add_argument(\'--testPct\', type=float, default=0.1)\n    parser.add_argument(\'--save\', type=str)\n    parser.add_argument(\'--work\', type=str, default=\'work\')\n    parser.add_argument(\'--qp-solver\', type=str, default=\'qpth\',\n                        choices=[\'qpth\', \'osqpth\'])\n    subparsers = parser.add_subparsers(dest=\'model\')\n    subparsers.required = True\n    fcP = subparsers.add_parser(\'fc\')\n    fcP.add_argument(\'--nHidden\', type=int, nargs=\'+\', default=[100,100])\n    fcP.add_argument(\'--bn\', action=\'store_true\')\n    convP = subparsers.add_parser(\'conv\')\n    convP.add_argument(\'--nHidden\', type=int, default=50)\n    convP.add_argument(\'--bn\', action=\'store_true\')\n    spOptnetEqP = subparsers.add_parser(\'spOptnetEq\')\n    spOptnetEqP.add_argument(\'--Qpenalty\', type=float, default=0.1)\n    optnetEqP = subparsers.add_parser(\'optnetEq\')\n    optnetEqP.add_argument(\'--Qpenalty\', type=float, default=0.1)\n    optnetIneqP = subparsers.add_parser(\'optnetIneq\')\n    optnetIneqP.add_argument(\'--Qpenalty\', type=float, default=0.1)\n    optnetIneqP.add_argument(\'--nineq\', type=int, default=100)\n    optnetLatent = subparsers.add_parser(\'optnetLatent\')\n    optnetLatent.add_argument(\'--Qpenalty\', type=float, default=0.1)\n    optnetLatent.add_argument(\'--nLatent\', type=int, default=100)\n    optnetLatent.add_argument(\'--nineq\', type=int, default=100)\n    args = parser.parse_args()\n\n    args.cuda = not args.no_cuda and torch.cuda.is_available()\n    t = \'{}.{}\'.format(args.boardSz, args.model)\n    if args.model == \'optnetEq\' or args.model == \'spOptnetEq\':\n        t += \'.Qpenalty={}\'.format(args.Qpenalty)\n    elif args.model == \'optnetIneq\':\n        t += \'.Qpenalty={}\'.format(args.Qpenalty)\n        t += \'.nineq={}\'.format(args.nineq)\n    elif args.model == \'optnetLatent\':\n        t += \'.Qpenalty={}\'.format(args.Qpenalty)\n        t += \'.nLatent={}\'.format(args.nLatent)\n        t += \'.nineq={}\'.format(args.nineq)\n    elif args.model == \'fc\':\n        t += \'.nHidden:{}\'.format(\',\'.join([str(x) for x in args.nHidden]))\n        if args.bn:\n            t += \'.bn\'\n    if args.save is None:\n        args.save = os.path.join(args.work, t)\n    setproctitle.setproctitle(\'bamos.sudoku.\' + t)\n\n    with open(\'data/{}/features.pt\'.format(args.boardSz), \'rb\') as f:\n        X = torch.load(f)\n    with open(\'data/{}/labels.pt\'.format(args.boardSz), \'rb\') as f:\n        Y = torch.load(f)\n\n    N, nFeatures = X.size(0), int(np.prod(X.size()[1:]))\n\n    nTrain = int(N*(1.-args.testPct))\n    nTest = N-nTrain\n\n    trainX = X[:nTrain]\n    trainY = Y[:nTrain]\n    testX = X[nTrain:]\n    testY = Y[nTrain:]\n\n    assert(nTrain % args.batchSz == 0)\n    assert(nTest % args.testBatchSz == 0)\n\n    save = args.save\n    if os.path.isdir(save):\n        shutil.rmtree(save)\n    os.makedirs(save)\n\n    npr.seed(1)\n\n    print_header(\'Building model\')\n    if args.model == \'fc\':\n        nHidden = args.nHidden\n        model = models.FC(nFeatures, nHidden, args.bn)\n    elif args.model == \'conv\':\n        model = models.Conv(args.boardSz)\n    elif args.model == \'optnetEq\':\n        model = models.OptNetEq(\n            n=args.boardSz, Qpenalty=args.Qpenalty, qp_solver=args.qp_solver,\n            trueInit=False)\n    elif args.model == \'spOptnetEq\':\n        model = models.SpOptNetEq(args.boardSz, args.Qpenalty, trueInit=False)\n    elif args.model == \'optnetIneq\':\n        model = models.OptNetIneq(args.boardSz, args.Qpenalty, args.nineq)\n    elif args.model == \'optnetLatent\':\n        model = models.OptNetLatent(args.boardSz, args.Qpenalty, args.nLatent, args.nineq)\n    else:\n        assert False\n\n    if args.cuda:\n        model = model.cuda()\n\n    fields = [\'epoch\', \'loss\', \'err\']\n    trainF = open(os.path.join(save, \'train.csv\'), \'w\')\n    trainW = csv.writer(trainF)\n    trainW.writerow(fields)\n    trainF.flush()\n    fields = [\'epoch\', \'loss\', \'err\']\n    testF = open(os.path.join(save, \'test.csv\'), \'w\')\n    testW = csv.writer(testF)\n    testW.writerow(fields)\n    testF.flush()\n\n\n    if \'optnet\' in args.model:\n        # if args.tvInit: lr = 1e-4\n        # elif args.learnD: lr = 1e-2\n        # else: lr = 1e-3\n        lr = 1e-1\n    else:\n        lr = 1e-3\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n\n    # writeParams(args, model, \'init\')\n    # test(args, 0, model, testF, testW, testX, testY)\n    for epoch in range(1, args.nEpoch+1):\n        # update_lr(optimizer, epoch)\n        train(args, epoch, model, trainF, trainW, trainX, trainY, optimizer)\n        test(args, epoch, model, testF, testW, testX, testY)\n        torch.save(model, os.path.join(args.save, \'latest.pth\'))\n        # writeParams(args, model, \'latest\')\n        os.system(\'./plot.py ""{}"" &\'.format(args.save))\n\ndef writeParams(args, model, tag):\n    if args.model == \'optnet\':\n        A = model.A.data.cpu().numpy()\n        np.savetxt(os.path.join(args.save, \'A.{}\'.format(tag)), A)\n\n# @profile\ndef train(args, epoch, model, trainF, trainW, trainX, trainY, optimizer):\n    batchSz = args.batchSz\n\n    batch_data_t = torch.FloatTensor(batchSz, trainX.size(1), trainX.size(2), trainX.size(3))\n    batch_targets_t = torch.FloatTensor(batchSz, trainY.size(1), trainX.size(2), trainX.size(3))\n    if args.cuda:\n        batch_data_t = batch_data_t.cuda()\n        batch_targets_t = batch_targets_t.cuda()\n    batch_data = Variable(batch_data_t, requires_grad=False)\n    batch_targets = Variable(batch_targets_t, requires_grad=False)\n    for i in range(0, trainX.size(0), batchSz):\n        start = time.time()\n        batch_data.data[:] = trainX[i:i+batchSz]\n        batch_targets.data[:] = trainY[i:i+batchSz]\n        # Fixed batch size for debugging:\n        # batch_data.data[:] = trainX[:batchSz]\n        # batch_targets.data[:] = trainY[:batchSz]\n\n        optimizer.zero_grad()\n        preds = model(batch_data)\n        loss = nn.MSELoss()(preds, batch_targets)\n        loss.backward()\n        optimizer.step()\n\n        err = computeErr(preds.data)/batchSz\n        print(\'Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.4f} Err: {:.4f} Time: {:.2f}s\'.format(\n            epoch, i+batchSz, trainX.size(0),\n            float(i+batchSz)/trainX.size(0)*100,\n            loss.item(), err, time.time()-start))\n\n        trainW.writerow(\n            (epoch-1+float(i+batchSz)/trainX.size(0), loss.item(), err))\n        trainF.flush()\n\ndef test(args, epoch, model, testF, testW, testX, testY):\n    batchSz = args.testBatchSz\n\n    test_loss = 0\n    batch_data_t = torch.FloatTensor(batchSz, testX.size(1), testX.size(2), testX.size(3))\n    batch_targets_t = torch.FloatTensor(batchSz, testY.size(1), testX.size(2), testX.size(3))\n    if args.cuda:\n        batch_data_t = batch_data_t.cuda()\n        batch_targets_t = batch_targets_t.cuda()\n    batch_data = Variable(batch_data_t, volatile=True)\n    batch_targets = Variable(batch_targets_t, volatile=True)\n\n    nErr = 0\n    for i in range(0, testX.size(0), batchSz):\n        print(\'Testing model: {}/{}\'.format(i, testX.size(0)), end=\'\\r\')\n        batch_data.data[:] = testX[i:i+batchSz]\n        batch_targets.data[:] = testY[i:i+batchSz]\n        output = model(batch_data)\n        test_loss += nn.MSELoss()(output, batch_targets)\n        nErr += computeErr(output.data)\n\n    nBatches = testX.size(0)/batchSz\n    test_loss = test_loss.item()/nBatches\n    test_err = nErr/testX.size(0)\n    print(\'TEST SET RESULTS:\' + \' \' * 20)\n    print(\'Average loss: {:.4f}\'.format(test_loss))\n    print(\'Err: {:.4f}\'.format(test_err))\n\n    testW.writerow((epoch, test_loss, test_err))\n    testF.flush()\n\ndef computeErr(pred):\n    batchSz = pred.size(0)\n    nsq = int(pred.size(1))\n    n = int(np.sqrt(nsq))\n    s = (nsq-1)*nsq//2 # 0 + 1 + ... + n^2-1\n    I = torch.max(pred, 3)[1].squeeze().view(batchSz, nsq, nsq)\n\n    def invalidGroups(x):\n        valid = (x.min(1)[0] == 0)\n        valid *= (x.max(1)[0] == nsq-1)\n        valid *= (x.sum(1) == s)\n        return ~valid\n\n    boardCorrect = torch.ones(batchSz).type_as(pred)\n    for j in range(nsq):\n        # Check the jth row and column.\n        boardCorrect[invalidGroups(I[:,j,:])] = 0\n        boardCorrect[invalidGroups(I[:,:,j])] = 0\n\n        # Check the jth block.\n        row, col = n*(j // n), n*(j % n)\n        M = invalidGroups(I[:,row:row+n,col:col+n].contiguous().view(batchSz,-1))\n        boardCorrect[M] = 0\n\n        if boardCorrect.sum() == 0:\n            return batchSz\n\n    return batchSz-boardCorrect.sum().item()\n\nif __name__==\'__main__\':\n    main()\n'"
sudoku/true-Qpenalty-errors.py,5,"b""#!/usr/bin/env python3\n\nimport torch\nfrom torch.autograd import Variable\n\nimport numpy as np\n\nimport matplotlib as mpl\nmpl.use('Agg')\nimport matplotlib.pyplot as plt\nplt.style.use('bmh')\n\nimport models\nfrom train import computeErr\n\nbatchSz = 128\n\nboards = {}\nfor boardSz in (2,3):\n    with open('data/{}/features.pt'.format(boardSz), 'rb') as f:\n        unsolvedBoards = Variable(torch.load(f).cuda()[:,:,:,:])\n        nBoards = unsolvedBoards.size(0)\n    with open('data/{}/labels.pt'.format(boardSz), 'rb') as f:\n        solvedBoards = Variable(torch.load(f).cuda()[:nBoards,:,:,:])\n    boards[boardSz] = (unsolvedBoards, solvedBoards)\n\nnBatches = nBoards//batchSz\nresults = {}\nstartIdx = 0\n\nranges = {\n    2: np.linspace(0.1, 2.0, num=11),\n    3: np.linspace(0.1, 1.0, num=10)\n}\n\nfor i in range(nBatches):\n    nSeen = (i+1)*batchSz\n    print('=== {} Boards Seen ==='.format(nSeen))\n\n    for boardSz in (2,3):\n        unsolvedBoards, solvedBoards = boards[boardSz]\n\n        print('--- Board Sz: {} ---'.format(boardSz))\n        print('| {:15s} | {:15s} | {:15s} |'.format('Qpenalty', '% Boards Wrong', '# Blanks Wrong'))\n\n        for j,Qpenalty in enumerate(ranges[boardSz]):\n            model = models.OptNetEq(boardSz, Qpenalty, trueInit=True).cuda()\n            X_batch = unsolvedBoards[startIdx:startIdx+batchSz]\n            Y_batch = solvedBoards[startIdx:startIdx+batchSz]\n            preds = model(X_batch).data\n            err = computeErr(preds)\n\n            # nWrong is not an exact metric because a board might have multiple solutions.\n            predBoards = torch.max(preds, 3)[1].squeeze().view(batchSz, -1)\n            trueBoards = torch.max(Y_batch.data, 3)[1].squeeze().view(batchSz, -1)\n            nWrong = ((predBoards-trueBoards).abs().cpu().numpy() > 1e-7).sum(axis=1)\n\n            results_key = (boardSz, j)\n            if results_key not in results:\n                results_j = {'err': err, 'nWrong': nWrong}\n                results[results_key] = results_j\n            else:\n                results_j = results[results_key]\n                results_j['err'] += err\n                results_j['nWrong'] = np.concatenate((results_j['nWrong'], nWrong))\n\n            err = results_j['err']/(batchSz*(i+1))\n            nWrong = np.mean(results_j['nWrong'])\n            print('| {:15f} | {:15f} | {:15f} |'.format(Qpenalty, err, nWrong))\n\n    print('='*50)\n    print('\\n\\n')\n\n    startIdx += batchSz\n"""
tests/optnet-back.py,5,"b""#!/usr/bin/env python3\n#\n# Run these tests with: nosetests -v -d test-adact-back.py\n#   This will run all functions even if one throws an assertion.\n#\n# For debugging: ./test-adact-back.py\n#   Easier to print statements.\n#   This will exit after the first assertion.\n\nimport os\nimport sys\n\nimport torch\n\nimport numpy as np\nimport numpy.random as npr\nimport numpy.testing as npt\nnp.set_printoptions(precision=2)\n\nimport numdifftools as nd\nimport cvxpy as cp\n\nfrom torch.autograd import Function, Variable\n\nimport adact\nimport adact_forward_ip as aip\n\nfrom solver import BlockSolver as Solver\n\nfrom nose.tools import with_setup, assert_almost_equal\n\nimport sys\nfrom IPython.core import ultratb\nsys.excepthook = ultratb.FormattedTB(mode='Verbose',\n     color_scheme='Linux', call_pdb=1)\n\nATOL=1e-2\nRTOL=1e-7\nverbose = True\ncuda = True\n\ndef test_back():\n    npr.seed(1)\n    nBatch, nz, neq, nineq = 1, 10, 1, 3\n    # nz, neq, nineq = 3,3,3\n\n    L = np.tril(np.random.randn(nz,nz)) + 2.*np.eye(nz,nz)\n    Q = L.dot(L.T)+1e-4*np.eye(nz)\n    G = 100.*npr.randn(nineq,nz)\n    A = 100.*npr.randn(neq,nz)\n    z0 = 1.*npr.randn(nz)\n    s0 = 100.*np.ones(nineq)\n    s0[:nineq//2] = 1e-6\n    # print(np.linalg.norm(L))\n    # print(np.linalg.norm(G))\n    # print(np.linalg.norm(A))\n    # print(np.linalg.norm(z0))\n    # print(np.linalg.norm(s0))\n\n    p = npr.randn(nBatch,nz)\n    # print(np.linalg.norm(p))\n    truez = npr.randn(nBatch,nz)\n\n    af = adact.AdactFunction()\n    zhat_0, nu_0, lam_0 = af.forward_single_np(p[0], L, G, A, z0, s0)\n    dl_dzhat_0 = zhat_0-truez[0]\n    S = Solver(L, A, G, z0, s0, 1e-8)\n    S.reinit(lam_0, zhat_0)\n    dp_0, dL_0, dG_0, dA_0, dz0_0, ds0_0 = af.backward_single_np_solver(\n        S, zhat_0, nu_0, lam_0, dl_dzhat_0, L, G, A, z0, s0)\n    # zhat_1, nu_1, lam_1 = af.forward_single_np(p[1], L, G, A, z0, s0)\n    # dl_dzhat_1 = zhat_1-truez[1]\n    # S.reinit(lam_1, zhat_1)\n    # dp_1, dL_1, dG_1, dA_1, dz0_1, ds0_1 = af.backward_single_np_solver(\n    #     S, zhat_1, nu_1, lam_1, dl_dzhat_1, L, G, A, z0, s0)\n\n    p, L, G, A, z0, s0, truez = [torch.DoubleTensor(x) for x in [p, L, G, A, z0, s0, truez]]\n    Q = torch.mm(L, L.t())+0.001*torch.eye(nz).type_as(L)\n    if cuda:\n        p, L, Q, G, A, z0, s0, truez = [x.cuda() for x in [p, L, Q, G, A, z0, s0, truez]]\n    p, L, G, A, z0, s0 = [Variable(x) for x in [p, L, G, A, z0, s0]]\n    for x in [p, L, G, A, z0, s0]: x.requires_grad = True\n\n    # Q_LU, S_LU, R = aip.pre_factor_kkt_batch(Q, G, A, nBatch)\n    # b = torch.mv(A, z0) if neq > 0 else None\n    # h = torch.mv(G, z0)+s0\n    # zhat_b, nu_b, lam_b = aip.forward_batch(p, Q, G, A, b, h, Q_LU, S_LU, R)\n\n    zhats = af(p, L, G, A, z0, s0)\n    dl_dzhat = zhats.data - truez\n    zhats.backward(dl_dzhat)\n    dp, dL, dG, dA, dz0, ds0 = [x.grad.clone() for x in [p, L, G, A, z0, s0]]\n\nif __name__=='__main__':\n    test_back()\n"""
tests/optnet-np.py,3,"b""#!/usr/bin/env python3\n#\n# Run these tests with: nosetests -v -d test-adact-np.py\n#   This will run all functions even if one throws an assertion.\n#\n# For debugging: ./test-adact-back.py\n#   Easier to print statements.\n#   This will exit after the first assertion.\n\nimport os\nimport sys\n\nimport torch\n\nimport numpy as np\nimport numpy.random as npr\nimport numpy.testing as npt\nnp.set_printoptions(precision=2)\n\nimport numdifftools as nd\nimport cvxpy as cp\n\nimport adact\nimport adact_forward_ip as aip\n\nfrom solver import BlockSolver as Solver\n\nfrom nose.tools import with_setup, assert_almost_equal\n\nATOL=1e-2\nRTOL=1e-7\n\nnpr.seed(1)\nnz, neq, nineq = 5,0,4\n# nz, neq, nineq = 3,3,3\n\nL = np.tril(np.random.randn(nz,nz)) + 2.*np.eye(nz,nz)\nQ = L.dot(L.T)+1e-8*np.eye(nz)\nG = 1000.*npr.randn(nineq,nz)\nA = 10000.*npr.randn(neq,nz)\nz0 = 1.*npr.randn(nz)\ns0 = 100.*np.ones(nineq)\n\np = npr.randn(nz)\ntruez = npr.randn(nz)\n\naf = adact.AdactFunction()\n\nzhat, nu, lam = af.forward_single_np(p, L, G, A, z0, s0)\ndl_dzhat = zhat-truez\n\n# dp, dL, dG, dA, dz0, ds0 = af.backward_single_np(zhat, nu, lam, dl_dzhat, L, G, A, z0, s0)\n\nS = Solver(L, A, G, z0, s0, 1e-8)\nS.reinit(lam, zhat)\ndp, dL, dG, dA, dz0, ds0 = af.backward_single_np_solver(S, zhat, nu, lam, dl_dzhat, L, G, A, z0, s0)\n\nverbose = True\n\n\ndef test_ip_forward():\n    p_t, Q_t, G_t, A_t, z0_t, s0_t = [torch.Tensor(x) for x in [p, Q, G, A, z0, s0]]\n    b = torch.mv(A_t, z0_t) if neq > 0 else None\n    h = torch.mv(G_t,z0_t)+s0_t\n    L_Q, L_S, R = aip.pre_factor_kkt(Q_t, G_t, A_t)\n\n    zhat_ip, nu_ip, lam_ip = aip.forward_single(p_t, Q_t, G_t, A_t, b, h, L_Q, L_S, R)\n    # Unnecessary clones here because of a pytorch bug when calling numpy\n    # on a tensor with a non-zero offset.\n    npt.assert_allclose(zhat, zhat_ip.clone().numpy(), rtol=RTOL, atol=ATOL)\n    if neq > 0:\n        npt.assert_allclose(nu, nu_ip.clone().numpy(), rtol=RTOL, atol=ATOL)\n    npt.assert_allclose(lam, lam_ip.clone().numpy(), rtol=RTOL, atol=ATOL)\n\ndef test_dl_dz0():\n    def f(z0):\n        zhat, nu, lam = af.forward_single_np(p, L, G, A, z0, s0)\n        return 0.5*np.sum(np.square(zhat - truez))\n\n    df = nd.Gradient(f)\n    dz0_fd = df(z0)\n    if verbose:\n        print('dz0_fd: ', dz0_fd)\n        print('dz0: ', dz0)\n    npt.assert_allclose(dz0_fd, dz0, rtol=RTOL, atol=ATOL)\n\ndef test_dl_ds0():\n    def f(s0):\n        zhat, nu, lam = af.forward_single_np(p, L, G, A, z0, s0)\n        return 0.5*np.sum(np.square(zhat - truez))\n\n    df = nd.Gradient(f)\n    ds0_fd = df(s0)\n    if verbose:\n        print('ds0_fd: ', ds0_fd)\n        print('ds0: ', ds0)\n    npt.assert_allclose(ds0_fd, ds0, rtol=RTOL, atol=ATOL)\n\ndef test_dl_dp():\n    def f(p):\n        zhat, nu, lam = af.forward_single_np(p, L, G, A, z0, s0)\n        return 0.5*np.sum(np.square(zhat - truez))\n\n    df = nd.Gradient(f)\n    dp_fd = df(p)\n    if verbose:\n        print('dp_fd: ', dp_fd)\n        print('dp: ', dp)\n    npt.assert_allclose(dp_fd, dp, rtol=RTOL, atol=ATOL)\n\ndef test_dl_dp_batch():\n    def f(p):\n        zhat, nu, lam = af.forward_single_np(p, L, G, A, z0, s0)\n        return 0.5*np.sum(np.square(zhat - truez))\n\n    df = nd.Gradient(f)\n    dp_fd = df(p)\n    if verbose:\n        print('dp_fd: ', dp_fd)\n        print('dp: ', dp)\n    npt.assert_allclose(dp_fd, dp, rtol=RTOL, atol=ATOL)\n\ndef test_dl_dA():\n    def f(A):\n        A = A.reshape(neq,nz)\n        zhat, nu, lam = af.forward_single_np(p, L, G, A, z0, s0)\n        return 0.5*np.sum(np.square(zhat - truez))\n\n    df = nd.Gradient(f)\n    dA_fd = df(A.ravel()).reshape(neq, nz)\n    if verbose:\n        print('dA_fd[1,:]: ', dA_fd[1,:])\n        print('dA[1,:]: ', dA[1,:])\n    npt.assert_allclose(dA_fd, dA, rtol=RTOL, atol=ATOL)\n\ndef test_dl_dG():\n    def f(G):\n        G = G.reshape(nineq,nz)\n        zhat, nu, lam = af.forward_single_np(p, L, G, A, z0, s0)\n        return 0.5*np.sum(np.square(zhat - truez))\n\n    df = nd.Gradient(f)\n    dG_fd = df(G.ravel()).reshape(nineq, nz)\n    if verbose:\n        print('dG_fd[1,:]: ', dG_fd[1,:])\n        print('dG[1,:]: ', dG[1,:])\n    npt.assert_allclose(dG_fd, dG, rtol=RTOL, atol=ATOL)\n\ndef test_dl_dL():\n    def f(l0):\n        L_ = np.copy(L)\n        L_[:,0] = l0\n        zhat, nu, lam = af.forward_single_np(p, L_, G, A, z0, s0)\n        return 0.5*np.sum(np.square(zhat - truez))\n\n    df = nd.Gradient(f)\n    dL_fd = df(L[:,0])\n    dl0 = np.array(dL[:,0]).ravel()\n    if verbose:\n        print('dL_fd: ', dL_fd)\n        print('dL: ', dl0)\n    npt.assert_allclose(dL_fd, dl0, rtol=RTOL, atol=ATOL)\n\nif __name__=='__main__':\n    # test_ip_forward()\n    test_dl_dp()\n    # test_dl_dp_batch()\n    # test_dl_dz0()\n    # test_dl_ds0()\n    # if neq > 0:\n    #     test_dl_dA()\n    # test_dl_dG()\n    # test_dl_dL()\n"""
util/init.plot.py,0,"b""#!/usr/bin/env python3\n\nimport numpy as np\nimport numpy.random as npr\n\nimport matplotlib as mpl\nmpl.use('Agg')\nimport matplotlib.pyplot as plt\nplt.style.use('bmh')\n\nfor i in range(5):\n    nz, neq, nineq = 2,0,10\n    G = npr.uniform(-1., 1., (nineq,nz))\n    z0 = np.zeros(nz)\n    s0 = np.ones(nineq)\n\n    l, u = -4, 4\n    b = np.linspace(l, u, num=1000)\n    C, D = np.meshgrid(b, b)\n    Z = []\n    for c,d in zip(C.ravel(), D.ravel()):\n        x = np.array([c,d])\n        z = np.all(G.dot(x) <= G.dot(z0)+s0).astype(np.float32)\n        Z.append(z)\n    Z = np.array(Z).reshape(C.shape)\n\n    fig, ax = plt.subplots(1, 1, figsize=(8,8))\n    plt.axis([l, u, l, u])\n    CS = plt.contourf(C, D, Z, cmap=plt.cm.Blues)\n    f = 'data/2016-11-02/init.{}.png'.format(i)\n    plt.savefig(f)\n    print('created '+f)\n"""
