file_path,api_count,code
prof-gurobi.py,4,"b'#!/usr/bin/env python3\n\nimport argparse\nimport sys\n\nimport numpy as np\nimport numpy.random as npr\n\n# import qpth.solvers.pdipm.single as pdipm_s\nimport qpth.solvers.pdipm.batch as pdipm_b\n\nimport itertools\nimport time\n\nimport torch\n\n# import gurobipy as gpy\n\nfrom IPython.core import ultratb\nsys.excepthook = ultratb.FormattedTB(mode=\'Verbose\',\n                                     color_scheme=\'Linux\', call_pdb=1)\n\nimport setproctitle\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--nTrials\', type=int, default=10)\n    args = parser.parse_args()\n    setproctitle.setproctitle(\'bamos.optnet.prof-gurobi\')\n\n    npr.seed(0)\n\n    prof(args)\n\n\ndef prof(args):\n    print(\'|  \\# Vars | \\# Batch | Gurobi | single | batched |\')\n    print(\'|----------+----------+--------+--------+---------|\')\n    # for nz, nBatch in itertools.product([100,500], [1, 64, 128]):\n    for nz, nBatch in itertools.product([100], [1, 64, 128]):\n        times = []\n        for i in range(args.nTrials):\n            times.append(prof_instance(nz, nBatch))\n        times = np.array(times)\n        print((""| {:5d} "" * 2 + ""| ${:.5e} \\pm {:.5e}$ s "" * 3 + \'|\').format(\n            *([nz, nBatch] + [item for sublist in zip(times.mean(axis=0), times.std(axis=0))\n                              for item in sublist])))\n\n\ndef prof_instance(nz, nBatch, cuda=True):\n    nineq, neq = 100, 0\n    assert(neq == 0)\n    L = npr.rand(nBatch, nz, nz)\n    Q = np.matmul(L, L.transpose((0, 2, 1))) + 1e-3 * np.eye(nz, nz)\n    G = npr.randn(nBatch, nineq, nz)\n    z0 = npr.randn(nBatch, nz)\n    s0 = npr.rand(nBatch, nineq)\n    p = npr.randn(nBatch, nz)\n    h = np.matmul(G, np.expand_dims(z0, axis=(2))).squeeze(2) + s0\n    A = npr.randn(nBatch, neq, nz)\n    b = np.matmul(A, np.expand_dims(z0, axis=(2))).squeeze(2)\n\n    # zhat_g = []\n    # gurobi_time = 0.0\n    # for i in range(nBatch):\n    #     m = gpy.Model()\n    #     zhat = m.addVars(nz, lb=-gpy.GRB.INFINITY, ub=gpy.GRB.INFINITY)\n\n    #     obj = 0.0\n    #     for j in range(nz):\n    #         for k in range(nz):\n    #             obj += 0.5 * Q[i, j, k] * zhat[j] * zhat[k]\n    #         obj += p[i, j] * zhat[j]\n    #     m.setObjective(obj)\n    #     for j in range(nineq):\n    #         con = 0\n    #         for k in range(nz):\n    #             con += G[i, j, k] * zhat[k]\n    #         m.addConstr(con <= h[i, j])\n    #     m.setParam(\'OutputFlag\', False)\n    #     start = time.time()\n    #     m.optimize()\n    #     gurobi_time += time.time() - start\n    #     t = np.zeros(nz)\n    #     for j in range(nz):\n    #         t[j] = zhat[j].x\n    # zhat_g.append(t)\n    gurobi_time = -1\n\n    p, L, Q, G, z0, s0, h = [torch.Tensor(x) for x in [p, L, Q, G, z0, s0, h]]\n    if cuda:\n        p, L, Q, G, z0, s0, h = [x.cuda() for x in [p, L, Q, G, z0, s0, h]]\n    if neq > 0:\n        A = torch.Tensor(A)\n        b = torch.Tensor(b)\n    else:\n        A, b = [torch.Tensor()] * 2\n    if cuda:\n        A = A.cuda()\n        b = b.cuda()\n\n    # af = adact.AdactFunction()\n\n    # single_results = []\n    start = time.time()\n    # for i in range(nBatch):\n    # A_i = A[i] if neq > 0 else A\n    # b_i = b[i] if neq > 0 else b\n    # U_Q, U_S, R = pdipm_s.pre_factor_kkt(Q[i], G[i], A_i)\n    # single_results.append(pdipm_s.forward(p[i], Q[i], G[i], A_i, b_i, h[i],\n    #                                       U_Q, U_S, R))\n    single_time = time.time() - start\n\n    start = time.time()\n    Q_LU, S_LU, R = pdipm_b.pre_factor_kkt(Q, G, A)\n    zhat_b, nu_b, lam_b, s_b = pdipm_b.forward(Q, p, G, h, A, b, Q_LU, S_LU, R)\n    batched_time = time.time() - start\n\n    # Usually between 1e-4 and 1e-5:\n    # print(\'Diff between gurobi and pdipm: \',\n    #       np.linalg.norm(zhat_g[0]-zhat_b[0].cpu().numpy()))\n    # import IPython, sys; IPython.embed(); sys.exit(-1)\n\n    # import IPython, sys; IPython.embed(); sys.exit(-1)\n    # zhat_diff = (single_results[0][0] - zhat_b[0]).norm()\n    # lam_diff = (single_results[0][2] - lam_b[0]).norm()\n    # eps = 0.1 # Pretty relaxed.\n    # if zhat_diff > eps or lam_diff > eps:\n    #     print(\'===========\')\n    #     print(""Warning: Single and batched solutions might not match."")\n    #     print(""  + zhat_diff: {}"".format(zhat_diff))\n    #     print(""  + lam_diff: {}"".format(lam_diff))\n    #     print(""  + (nz, neq, nineq, nBatch) = ({}, {}, {}, {})"".format(\n    #         nz, neq, nineq, nBatch))\n    #     print(\'===========\')\n\n    return gurobi_time, single_time, batched_time\n\n\nif __name__ == \'__main__\':\n    main()\n'"
prof-linear.py,7,"b'#!/usr/bin/env python\n\n# import setGPU\nimport argparse\nimport sys\n\nimport numpy as np\nimport numpy.random as npr\n\nfrom qpth.qp import QPFunction\n# import qpth.solvers.pdipm.single as pdipm_s\n# import qpth.solvers.pdipm.batch as pdipm_b\n\nimport time\n\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\n\nfrom IPython.core import ultratb\nsys.excepthook = ultratb.FormattedTB(mode=\'Verbose\',\n                                     color_scheme=\'Linux\', call_pdb=1)\n\nimport setproctitle\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--nTrials\', type=int, default=10)\n    args = parser.parse_args()\n    setproctitle.setproctitle(\'bamos.optnet.prof-linear\')\n\n    npr.seed(0)\n\n    prof(args)\n\n\ndef prof(args):\n    print(\'|  \\# Vars | \\# Batch | Linear f/b | qpth f/b |\')\n    # for nz, nBatch in itertools.product([100,500], [1, 64, 128]):\n    nBatch = 128\n    all_linearf, all_qpthf = [], []\n    all_linearb, all_qpthb = [], []\n    for nz in [10, 50, 100, 500]:\n        linearf_times, qpthf_times, linearb_times, qpthb_times = \\\n            prof_instance(nz, nBatch, args.nTrials)\n        all_linearf.append((linearf_times.mean(), linearf_times.std()))\n        all_qpthf.append((qpthf_times.mean(), qpthf_times.std()))\n        all_linearb.append((linearb_times.mean(), linearb_times.std()))\n        all_qpthb.append((qpthb_times.mean(), qpthb_times.std()))\n        print((""| {:5d} "" * 2 + ""| ${:.3e} \\pm {:.3e}$ s "" * 4 + \'|\').format(\n            nz, nBatch,\n            linearf_times.mean(), linearf_times.std(),\n            linearb_times.mean(), linearb_times.std(),\n            qpthf_times.mean(), qpthf_times.std(),\n            qpthb_times.mean(), qpthb_times.std()))\n\n    print(\'linearf = \', all_linearf)\n    print(\'qpthf = \', all_qpthf)\n    print(\'linearb = \', all_linearb)\n    print(\'qpthb = \', all_qpthb)\n\n\ndef prof_instance(nz, nBatch, nTrials, cuda=True):\n    nineq, neq = nz, 0\n    assert(neq == 0)\n    L = npr.rand(nBatch, nz, nz)\n    Q = np.matmul(L, L.transpose((0, 2, 1))) + 1e-3 * np.eye(nz, nz)\n    G = npr.randn(nBatch, nineq, nz)\n    z0 = npr.randn(nBatch, nz)\n    s0 = npr.rand(nBatch, nineq)\n    p = npr.randn(nBatch, nz)\n    h = np.matmul(G, np.expand_dims(z0, axis=(2))).squeeze(2) + s0\n    A = npr.randn(nBatch, neq, nz)\n    b = np.matmul(A, np.expand_dims(z0, axis=(2))).squeeze(2)\n\n    lm = nn.Linear(nz, nz)\n\n    p, L, Q, G, z0, s0, h = [torch.Tensor(x) for x in [p, L, Q, G, z0, s0, h]]\n    if cuda:\n        p, L, Q, G, z0, s0, h = [x.cuda() for x in [p, L, Q, G, z0, s0, h]]\n        lm = lm.cuda()\n    if neq > 0:\n        A = torch.Tensor(A)\n        b = torch.Tensor(b)\n    else:\n        A, b = [torch.Tensor()] * 2\n    if cuda:\n        A = A.cuda()\n        b = b.cuda()\n\n    p, L, Q, G, z0, s0, h, A, b = [\n        Variable(x) for x in [p, L, Q, G, z0, s0, h, A, b]]\n    p.requires_grad = True\n\n    linearf_times = []\n    linearb_times = []\n    for i in range(nTrials + 1):\n        start = time.time()\n        zhat_l = lm(p)\n        linearf_times.append(time.time() - start)\n        start = time.time()\n        zhat_l.backward(torch.ones(nBatch, nz).cuda())\n        linearb_times.append(time.time() - start)\n    linearf_times = linearf_times[1:]\n    linearb_times = linearb_times[1:]\n\n    qpthf_times = []\n    qpthb_times = []\n    for i in range(nTrials + 1):\n        start = time.time()\n        qpf = QPFunction()\n        zhat_b = qpf(Q, p, G, h, A, b)\n        qpthf_times.append(time.time() - start)\n\n        start = time.time()\n        zhat_b.backward(torch.ones(nBatch, nz).cuda())\n        qpthb_times.append(time.time() - start)\n    qpthf_times = qpthf_times[1:]\n    qpthb_times = qpthb_times[1:]\n\n    return np.array(linearf_times), np.array(qpthf_times), \\\n        np.array(linearb_times), np.array(qpthb_times)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
setup.py,0,"b'from setuptools import find_packages, setup\n\nsetup(\n    name=\'qpth\',\n    version=\'0.0.15\',\n    description=""A fast and differentiable QP solver for PyTorch."",\n    author=\'Brandon Amos\',\n    author_email=\'bamos@cs.cmu.edu\',\n    platforms=[\'any\'],\n    license=""Apache 2.0"",\n    url=\'https://github.com/locuslab/qpth\',\n    packages=find_packages(),\n    install_requires=[\n        \'numpy>=1<2\',\n    ]\n)\n'"
test.py,38,"b""#!/usr/bin/env python3\n#\n# Run these tests with: nosetests -v -d <file>.py\n#   This will run all functions even if one throws an assertion.\n#\n# For debugging: ./<file>.py\n#   Easier to print statements.\n#   This will exit qfter the first assertion.\n\nimport torch\nfrom torch.autograd import Variable\n\nimport numpy as np\nimport numpy.random as npr\nimport numpy.testing as npt\nfrom numpy.testing import dec\nnp.set_printoptions(precision=6)\n\nimport numdifftools as nd\n\nimport sys\nsys.path.append('..')\nimport qpth\nfrom qpth.util import bdiag, expandParam, extract_nBatch\nimport qpth.solvers.cvxpy as qp_cvxpy\n\n# import qpth.solvers.pdipm.single as pdipm_s\nimport qpth.solvers.pdipm.batch as pdipm_b\nimport qpth.solvers.pdipm.spbatch as pdipm_spb\n\n# from IPython.core import ultratb\n# sys.excepthook = ultratb.FormattedTB(mode='Verbose',\n#      color_scheme='Linux', call_pdb=1)\n\nATOL = 1e-2\nRTOL = 1e-4\n\ncuda = False\nverbose = True\n\n\ndef get_grads(nBatch=1, nz=10, neq=1, nineq=3, Qscale=1.,\n              Gscale=1., hscale=1., Ascale=1., bscale=1.):\n    assert(nBatch == 1)\n    npr.seed(1)\n    L = np.random.randn(nz, nz)\n    Q = Qscale * L.dot(L.T)\n    G = Gscale * npr.randn(nineq, nz)\n    # h = hscale*npr.randn(nineq)\n    z0 = npr.randn(nz)\n    s0 = npr.rand(nineq)\n    h = G.dot(z0) + s0\n    A = Ascale * npr.randn(neq, nz)\n    # b = bscale*npr.randn(neq)\n    b = A.dot(z0)\n\n    p = npr.randn(nBatch, nz)\n    # print(np.linalg.norm(p))\n    truez = npr.randn(nBatch, nz)\n\n    Q, p, G, h, A, b, truez = [x.astype(np.float64) for x in\n                               [Q, p, G, h, A, b, truez]]\n    _, zhat, nu, lam, slacks = qp_cvxpy.forward_single_np(Q, p[0], G, h, A, b)\n\n    grads = get_grads_torch(Q, p, G, h, A, b, truez)\n    return [p[0], Q, G, h, A, b, truez], grads\n\n\ndef get_grads_torch(Q, p, G, h, A, b, truez):\n    Q, p, G, h, A, b, truez = [\n        torch.DoubleTensor(x) if len(x) > 0 else torch.DoubleTensor()\n        for x in [Q, p, G, h, A, b, truez]]\n    if cuda:\n        Q, p, G, h, A, b, truez = [x.cuda() for x in [Q, p, G, h, A, b, truez]]\n\n    Q, p, G, h, A, b = [Variable(x) for x in [Q, p, G, h, A, b]]\n    for x in [Q, p, G, h]:\n        x.requires_grad = True\n\n    # nBatch = 1\n    if b.nelement() > 0:\n        A.requires_grad = True\n        b.requires_grad = True\n\n    # zhats = qpth.qp.QPFunction(solver=qpth.qp.QPSolvers.CVXPY)(Q, p, G, h, A, b)\n    zhats = qpth.qp.QPFunction()(Q, p, G, h, A, b)\n\n    dl_dzhat = zhats.data - truez\n    zhats.backward(dl_dzhat)\n\n    grads = [x.grad.data.squeeze(0).cpu().numpy() for x in [Q, p, G, h]]\n    if A.nelement() > 0:\n        grads += [x.grad.data.squeeze(0).cpu().numpy() for x in [A, b]]\n    else:\n        grads += [None, None]\n    return grads\n\n\ndef test_dl_dp():\n    nz, neq, nineq = 10, 2, 3\n    [p, Q, G, h, A, b, truez], [dQ, dp, dG, dh, dA, db] = get_grads(\n        nz=nz, neq=neq, nineq=nineq, Qscale=100., Gscale=100., Ascale=100.)\n\n    def f(p):\n        _, zhat, nu, lam, slacks = qp_cvxpy.forward_single_np(Q, p, G, h, A, b)\n        return 0.5 * np.sum(np.square(zhat - truez))\n\n    df = nd.Gradient(f)\n    dp_fd = df(p)\n    if verbose:\n        print('dp_fd: ', dp_fd)\n        print('dp: ', dp)\n    npt.assert_allclose(dp_fd, dp, rtol=RTOL, atol=ATOL)\n\n\ndef test_dl_dG():\n    nz, neq, nineq = 10, 0, 3\n    [p, Q, G, h, A, b, truez], [dQ, dp, dG, dh, dA, db] = get_grads(\n        nz=nz, neq=neq, nineq=nineq)\n\n    def f(G):\n        G = G.reshape(nineq, nz)\n        _, zhat, nu, lam, slacks = qp_cvxpy.forward_single_np(Q, p, G, h, A, b)\n        return 0.5 * np.sum(np.square(zhat - truez))\n\n    df = nd.Gradient(f)\n    dG_fd = df(G.ravel()).reshape(nineq, nz)\n    if verbose:\n        # print('dG_fd[1,:]: ', dG_fd[1,:])\n        # print('dG[1,:]: ', dG[1,:])\n        print('dG_fd: ', dG_fd)\n        print('dG: ', dG)\n    npt.assert_allclose(dG_fd, dG, rtol=RTOL, atol=ATOL)\n\n\ndef test_dl_dh():\n    nz, neq, nineq = 10, 0, 3\n    [p, Q, G, h, A, b, truez], [dQ, dp, dG, dh, dA, db] = get_grads(\n        nz=nz, neq=neq, nineq=nineq, Qscale=1., Gscale=1.)\n\n    def f(h):\n        _, zhat, nu, lam, slacks = qp_cvxpy.forward_single_np(Q, p, G, h, A, b)\n        return 0.5 * np.sum(np.square(zhat - truez))\n\n    df = nd.Gradient(f)\n    dh_fd = df(h)\n    if verbose:\n        print('dh_fd: ', dh_fd)\n        print('dh: ', dh)\n    npt.assert_allclose(dh_fd, dh, rtol=RTOL, atol=ATOL)\n\n\ndef test_dl_dA():\n    nz, neq, nineq = 10, 3, 1\n    [p, Q, G, h, A, b, truez], [dQ, dp, dG, dh, dA, db] = get_grads(\n        nz=nz, neq=neq, nineq=nineq, Qscale=100., Gscale=100., Ascale=100.)\n\n    def f(A):\n        A = A.reshape(neq, nz)\n        _, zhat, nu, lam, slacks = qp_cvxpy.forward_single_np(Q, p, G, h, A, b)\n        return 0.5 * np.sum(np.square(zhat - truez))\n\n    df = nd.Gradient(f)\n    dA_fd = df(A.ravel()).reshape(neq, nz)\n    if verbose:\n        # print('dA_fd[0,:]: ', dA_fd[0,:])\n        # print('dA[0,:]: ', dA[0,:])\n        print('dA_fd: ', dA_fd)\n        print('dA: ', dA)\n    npt.assert_allclose(dA_fd, dA, rtol=RTOL, atol=ATOL)\n\n\ndef test_dl_db():\n    nz, neq, nineq = 10, 3, 1\n    [p, Q, G, h, A, b, truez], [dQ, dp, dG, dh, dA, db] = get_grads(\n        nz=nz, neq=neq, nineq=nineq, Qscale=100., Gscale=100., Ascale=100.)\n\n    def f(b):\n        _, zhat, nu, lam, slacks = qp_cvxpy.forward_single_np(Q, p, G, h, A, b)\n        return 0.5 * np.sum(np.square(zhat - truez))\n\n    df = nd.Gradient(f)\n    db_fd = df(b)\n    if verbose:\n        print('db_fd: ', db_fd)\n        print('db: ', db)\n    npt.assert_allclose(db_fd, db, rtol=RTOL, atol=ATOL)\n\n\ndef get_kkt_problem():\n    def cast(m):\n        # return m.cuda().double()\n        return m.double()\n\n    nBatch, nx, nineq, neq = 2, 5, 4, 3\n    Q = cast(torch.randn(nx, nx))\n    Q = Q.mm(Q.t())\n    p = cast(torch.randn(nx))\n    G = cast(torch.randn(nBatch, nineq, nx))\n    h = cast(torch.zeros(nBatch, nineq))\n    A = cast(torch.randn(neq, nx))\n    b = cast(torch.randn(neq))\n\n    nBatch = extract_nBatch(Q, p, G, h, A, b)\n    Q, _ = expandParam(Q, nBatch, 3)\n    p, _ = expandParam(p, nBatch, 2)\n    G, _ = expandParam(G, nBatch, 3)\n    h, _ = expandParam(h, nBatch, 2)\n    A, _ = expandParam(A, nBatch, 3)\n    b, _ = expandParam(b, nBatch, 2)\n\n    d = torch.rand(nBatch, nineq).type_as(Q)\n    D = bdiag(d)\n    rx = torch.rand(nBatch, nx).type_as(Q)\n    rs = torch.rand(nBatch, nineq).type_as(Q)\n    rz = torch.rand(nBatch, nineq).type_as(Q)\n    ry = torch.rand(nBatch, neq).type_as(Q)\n\n    return Q, p, G, h, A, b, d, D, rx, rs, rz, ry\n\n\ndef test_lu_kkt_solver():\n    Q, p, G, h, A, b, d, D, rx, rs, rz, ry = get_kkt_problem()\n\n    dx, ds, dz, dy = pdipm_b.factor_solve_kkt(Q, D, G, A, rx, rs, rz, ry)\n\n    Q_LU, S_LU, R = pdipm_b.pre_factor_kkt(Q, G, A)\n    pdipm_b.factor_kkt(S_LU, R, d)\n    dx_, ds_, dz_, dy_ = pdipm_b.solve_kkt(Q_LU, d, G, A, S_LU, rx, rs, rz, ry)\n\n    npt.assert_allclose(dx.numpy(), dx_.numpy(), rtol=RTOL, atol=ATOL)\n    npt.assert_allclose(ds.numpy(), ds_.numpy(), rtol=RTOL, atol=ATOL)\n    npt.assert_allclose(dz.numpy(), dz_.numpy(), rtol=RTOL, atol=ATOL)\n    npt.assert_allclose(dy.numpy(), dy_.numpy(), rtol=RTOL, atol=ATOL)\n\n\ndef test_ir_kkt_solver():\n    Q, p, G, h, A, b, d, D, rx, rs, rz, ry = get_kkt_problem()\n\n    dx, ds, dz, dy = pdipm_b.factor_solve_kkt(Q, D, G, A, rx, rs, rz, ry)\n    dx_, ds_, dz_, dy_ = pdipm_b.solve_kkt_ir(\n        Q, D, G, A, rx, rs, rz, ry, niter=1)\n\n    npt.assert_allclose(dx.numpy(), dx_.numpy(), rtol=RTOL, atol=ATOL)\n    npt.assert_allclose(ds.numpy(), ds_.numpy(), rtol=RTOL, atol=ATOL)\n    npt.assert_allclose(dz.numpy(), dz_.numpy(), rtol=RTOL, atol=ATOL)\n    npt.assert_allclose(dy.numpy(), dy_.numpy(), rtol=RTOL, atol=ATOL)\n\n\n@npt.dec.skipif(\n    not torch.cuda.is_available() or not hasattr(torch, 'spbqrfactsolve'))\ndef test_sparse_forward():\n    torch.manual_seed(0)\n\n    nBatch, nx, nineq, neq = 2, 5, 4, 3\n\n    def cast(m):\n        return m.cuda().double()\n\n    spTensor = torch.cuda.sparse.DoubleTensor\n    iTensor = torch.cuda.LongTensor\n\n    Qi = iTensor([range(nx), range(nx)])\n    Qv = cast(torch.ones(nBatch, nx))\n    Qsz = torch.Size([nx, nx])\n    Q0 = spTensor(Qi, Qv[0], Qsz)\n\n    Gi = iTensor([range(nineq), range(nineq)])\n    Gv = cast(torch.randn(nBatch, nineq))\n    Gsz = torch.Size([nineq, nx])\n    G0 = spTensor(Gi, Gv[0], Gsz)\n    h = cast(torch.randn(nBatch, nineq))\n\n    Ai = iTensor([range(neq), range(neq)])\n    Av = Gv[:, :neq].clone()\n    Asz = torch.Size([neq, nx])\n    A0 = spTensor(Ai, Av[0], Asz)\n    b = h[:, :neq].clone()\n\n    p = cast(torch.randn(nBatch, nx))\n\n    from IPython.core import ultratb\n    sys.excepthook = ultratb.FormattedTB(mode='Verbose',\n                                         color_scheme='Linux', call_pdb=1)\n    xhats0_cp = qpth.qp.QPFunction(solver=qpth.qp.QPSolvers.CVXPY)(\n        *[Variable(y) for y in\n          [Q0.to_dense(), p[0], G0.to_dense(), h[0], A0.to_dense(), b[0]]]).data.squeeze()\n\n    xhats, nus, lams, slacks = pdipm_spb.forward(Qi, Qv, Qsz, p, Gi, Gv, Gsz, h,\n                                                 Ai, Av, Asz, b, verbose=-1,\n                                                 notImprovedLim=3, maxIter=20)\n    npt.assert_allclose(xhats0_cp.cpu().numpy(),\n                        xhats[0].cpu().numpy(), rtol=RTOL, atol=ATOL)\n\n    Qv, p, Gv, h, Av, b = [Variable(x) for x in [Qv, p, Gv, h, Av, b]]\n    xhats_qpf = qpth.qp.SpQPFunction(Qi, Qsz, Gi, Gsz, Ai, Asz)(\n        Qv, p, Gv, h, Av, b\n    ).data\n    npt.assert_allclose(xhats.cpu().numpy(),\n                        xhats_qpf.cpu().numpy(), rtol=RTOL, atol=ATOL)\n\n\n@npt.dec.skipif(\n    not torch.cuda.is_available() or not hasattr(torch, 'spbqrfactsolve'))\ndef test_sparse_backward():\n    torch.manual_seed(0)\n\n    nBatch, nx, nineq, neq = 1, 5, 4, 3\n\n    def cast(m):\n        return m.cuda().double()\n\n    spTensor = torch.cuda.sparse.DoubleTensor\n    iTensor = torch.cuda.LongTensor\n\n    Qi = iTensor([range(nx), range(nx)])\n    Qv = cast(torch.ones(nBatch, nx))\n    Qsz = torch.Size([nx, nx])\n    Q0 = spTensor(Qi, Qv[0], Qsz)\n\n    Gi = iTensor([range(nineq), range(nineq)])\n    Gv = cast(torch.randn(nBatch, nineq))\n    Gsz = torch.Size([nineq, nx])\n    G0 = spTensor(Gi, Gv[0], Gsz)\n    h = cast(torch.randn(nBatch, nineq))\n\n    Ai = iTensor([range(neq), range(neq)])\n    Av = Gv[:, :neq].clone()\n    Asz = torch.Size([neq, nx])\n    A0 = spTensor(Ai, Av[0], Asz)\n    b = h[:, :neq].clone()\n\n    p = cast(torch.randn(nBatch, nx))\n    truex = Variable(cast(torch.randn(nBatch, nx)))\n\n    Qv, p, Gv, h, Av, b = [Variable(x) for x in [Qv, p, Gv, h, Av, b]]\n    for x in [Qv, p, Gv, h, Av, b]:\n        x.requires_grad = True\n    xhats = qpth.qp.SpQPFunction(Qi, Qsz, Gi, Gsz, Ai, Asz)(\n        Qv, p, Gv, h, Av, b\n    )\n    loss = torch.norm(xhats - truex)\n    loss.backward()\n\n    # dQv, dGv, dAv = Qv.grad, Gv.grad, Av.grad\n    dQv = Qv.grad\n\n    Q0 = Q0.to_dense()\n    p0 = p[0].data\n    G0 = G0.to_dense()\n    h0 = h[0].data\n    A0 = A0.to_dense()\n    b0 = b[0].data\n    Q0, p0, G0, h0, A0, b0 = [Variable(y) for y in [Q0, p0, G0, h0, A0, b0]]\n    for x in [Q0, p0, G0, h0, A0, b0]:\n        x.requires_grad = True\n    xhats_dense = qpth.qp.QPFunction()(Q0, p0, G0, h0, A0, b0)\n    loss_dense = torch.norm(xhats_dense - truex)\n    loss_dense.backward()\n\n    # dQ, dG, dA = Q0.grad, G0.grad, A0.grad\n    dQ = Q0.grad\n\n    npt.assert_allclose(dQv.squeeze().data.cpu().numpy(), dQ.data.diag().cpu().numpy(),\n                        rtol=RTOL, atol=ATOL)\n    # TODO: dG/dGv don't match\n    # TODO: dA/dAv don't match\n\n\nif __name__ == '__main__':\n    test_dl_dp()\n    test_dl_dG()\n    test_dl_dh()\n    test_dl_dA()\n    test_dl_db()\n    test_lu_kkt_solver()\n    test_ir_kkt_solver()\n    # test_sparse_forward()\n    # test_sparse_backward()\n"""
qpth/__init__.py,0,"b""__all__ = ['qp', 'util', 'solvers']\n\nfrom . import qp\nfrom . import solvers\n"""
qpth/qp.py,23,"b'import torch\nfrom torch.autograd import Function\n\nfrom .util import bger, expandParam, extract_nBatch\nfrom . import solvers\nfrom .solvers.pdipm import batch as pdipm_b\nfrom .solvers.pdipm import spbatch as pdipm_spb\n# from .solvers.pdipm import single as pdipm_s\n\nfrom enum import Enum\n\n\nclass QPSolvers(Enum):\n    PDIPM_BATCHED = 1\n    CVXPY = 2\n\n\ndef QPFunction(eps=1e-12, verbose=0, notImprovedLim=3,\n                 maxIter=20, solver=QPSolvers.PDIPM_BATCHED,\n                 check_Q_spd=True):\n    class QPFunctionFn(Function):\n        @staticmethod\n        def forward(ctx, Q_, p_, G_, h_, A_, b_):\n            """"""Solve a batch of QPs.\n\n            This function solves a batch of QPs, each optimizing over\n            `nz` variables and having `nineq` inequality constraints\n            and `neq` equality constraints.\n            The optimization problem for each instance in the batch\n            (dropping indexing from the notation) is of the form\n\n                \\hat z =   argmin_z 1/2 z^T Q z + p^T z\n                        subject to Gz <= h\n                                    Az  = b\n\n            where Q \\in S^{nz,nz},\n                S^{nz,nz} is the set of all positive semi-definite matrices,\n                p \\in R^{nz}\n                G \\in R^{nineq,nz}\n                h \\in R^{nineq}\n                A \\in R^{neq,nz}\n                b \\in R^{neq}\n\n            These parameters should all be passed to this function as\n            Variable- or Parameter-wrapped Tensors.\n            (See torch.autograd.Variable and torch.nn.parameter.Parameter)\n\n            If you want to solve a batch of QPs where `nz`, `nineq` and `neq`\n            are the same, but some of the contents differ across the\n            minibatch, you can pass in tensors in the standard way\n            where the first dimension indicates the batch example.\n            This can be done with some or all of the coefficients.\n\n            You do not need to add an extra dimension to coefficients\n            that will not change across all of the minibatch examples.\n            This function is able to infer such cases.\n\n            If you don\'t want to use any equality or inequality constraints,\n            you can set the appropriate values to:\n\n                e = Variable(torch.Tensor())\n\n            Parameters:\n            Q:  A (nBatch, nz, nz) or (nz, nz) Tensor.\n            p:  A (nBatch, nz) or (nz) Tensor.\n            G:  A (nBatch, nineq, nz) or (nineq, nz) Tensor.\n            h:  A (nBatch, nineq) or (nineq) Tensor.\n            A:  A (nBatch, neq, nz) or (neq, nz) Tensor.\n            b:  A (nBatch, neq) or (neq) Tensor.\n\n            Returns: \\hat z: a (nBatch, nz) Tensor.\n            """"""\n            nBatch = extract_nBatch(Q_, p_, G_, h_, A_, b_)\n            Q, _ = expandParam(Q_, nBatch, 3)\n            p, _ = expandParam(p_, nBatch, 2)\n            G, _ = expandParam(G_, nBatch, 3)\n            h, _ = expandParam(h_, nBatch, 2)\n            A, _ = expandParam(A_, nBatch, 3)\n            b, _ = expandParam(b_, nBatch, 2)\n\n            if check_Q_spd:\n                for i in range(nBatch):\n                    e, _ = torch.eig(Q[i])\n                    if not torch.all(e[:,0] > 0):\n                        raise RuntimeError(\'Q is not SPD.\')\n\n            _, nineq, nz = G.size()\n            neq = A.size(1) if A.nelement() > 0 else 0\n            assert(neq > 0 or nineq > 0)\n            ctx.neq, ctx.nineq, ctx.nz = neq, nineq, nz\n\n            if solver == QPSolvers.PDIPM_BATCHED:\n                ctx.Q_LU, ctx.S_LU, ctx.R = pdipm_b.pre_factor_kkt(Q, G, A)\n                zhats, ctx.nus, ctx.lams, ctx.slacks = pdipm_b.forward(\n                    Q, p, G, h, A, b, ctx.Q_LU, ctx.S_LU, ctx.R,\n                    eps, verbose, notImprovedLim, maxIter)\n            elif solver == QPSolvers.CVXPY:\n                vals = torch.Tensor(nBatch).type_as(Q)\n                zhats = torch.Tensor(nBatch, ctx.nz).type_as(Q)\n                lams = torch.Tensor(nBatch, ctx.nineq).type_as(Q)\n                nus = torch.Tensor(nBatch, ctx.neq).type_as(Q) \\\n                    if ctx.neq > 0 else torch.Tensor()\n                slacks = torch.Tensor(nBatch, ctx.nineq).type_as(Q)\n                for i in range(nBatch):\n                    Ai, bi = (A[i], b[i]) if neq > 0 else (None, None)\n                    vals[i], zhati, nui, lami, si = solvers.cvxpy.forward_single_np(\n                        *[x.cpu().numpy() if x is not None else None\n                        for x in (Q[i], p[i], G[i], h[i], Ai, bi)])\n                    # if zhati[0] is None:\n                    #     import IPython, sys; IPython.embed(); sys.exit(-1)\n                    zhats[i] = torch.Tensor(zhati)\n                    lams[i] = torch.Tensor(lami)\n                    slacks[i] = torch.Tensor(si)\n                    if neq > 0:\n                        nus[i] = torch.Tensor(nui)\n\n                ctx.vals = vals\n                ctx.lams = lams\n                ctx.nus = nus\n                ctx.slacks = slacks\n            else:\n                assert False\n\n            ctx.save_for_backward(zhats, Q_, p_, G_, h_, A_, b_)\n            return zhats\n\n        @staticmethod\n        def backward(ctx, dl_dzhat):\n            zhats, Q, p, G, h, A, b = ctx.saved_tensors\n            nBatch = extract_nBatch(Q, p, G, h, A, b)\n            Q, Q_e = expandParam(Q, nBatch, 3)\n            p, p_e = expandParam(p, nBatch, 2)\n            G, G_e = expandParam(G, nBatch, 3)\n            h, h_e = expandParam(h, nBatch, 2)\n            A, A_e = expandParam(A, nBatch, 3)\n            b, b_e = expandParam(b, nBatch, 2)\n\n            # neq, nineq, nz = ctx.neq, ctx.nineq, ctx.nz\n            neq, nineq = ctx.neq, ctx.nineq\n\n\n            if solver == QPSolvers.CVXPY:\n                ctx.Q_LU, ctx.S_LU, ctx.R = pdipm_b.pre_factor_kkt(Q, G, A)\n\n            # Clamp here to avoid issues coming up when the slacks are too small.\n            # TODO: A better fix would be to get lams and slacks from the\n            # solver that don\'t have this issue.\n            d = torch.clamp(ctx.lams, min=1e-8) / torch.clamp(ctx.slacks, min=1e-8)\n\n            pdipm_b.factor_kkt(ctx.S_LU, ctx.R, d)\n            dx, _, dlam, dnu = pdipm_b.solve_kkt(\n                ctx.Q_LU, d, G, A, ctx.S_LU,\n                dl_dzhat, torch.zeros(nBatch, nineq).type_as(G),\n                torch.zeros(nBatch, nineq).type_as(G),\n                torch.zeros(nBatch, neq).type_as(G) if neq > 0 else torch.Tensor())\n\n            dps = dx\n            dGs = bger(dlam, zhats) + bger(ctx.lams, dx)\n            if G_e:\n                dGs = dGs.mean(0)\n            dhs = -dlam\n            if h_e:\n                dhs = dhs.mean(0)\n            if neq > 0:\n                dAs = bger(dnu, zhats) + bger(ctx.nus, dx)\n                dbs = -dnu\n                if A_e:\n                    dAs = dAs.mean(0)\n                if b_e:\n                    dbs = dbs.mean(0)\n            else:\n                dAs, dbs = None, None\n            dQs = 0.5 * (bger(dx, zhats) + bger(zhats, dx))\n            if Q_e:\n                dQs = dQs.mean(0)\n            if p_e:\n                dps = dps.mean(0)\n\n\n            grads = (dQs, dps, dGs, dhs, dAs, dbs)\n\n            return grads\n    return QPFunctionFn.apply\n\n\nclass SpQPFunction(Function):\n    def __init__(self, Qi, Qsz, Gi, Gsz, Ai, Asz,\n                 eps=1e-12, verbose=0, notImprovedLim=3, maxIter=20):\n        self.Qi, self.Qsz = Qi, Qsz\n        self.Gi, self.Gsz = Gi, Gsz\n        self.Ai, self.Asz = Ai, Asz\n\n        self.eps = eps\n        self.verbose = verbose\n        self.notImprovedLim = notImprovedLim\n        self.maxIter = maxIter\n\n        self.nineq, self.nz = Gsz\n        self.neq, _ = Asz\n\n    def forward(self, Qv, p, Gv, h, Av, b):\n        self.nBatch = Qv.size(0)\n\n        zhats, self.nus, self.lams, self.slacks = pdipm_spb.forward(\n            self.Qi, Qv, self.Qsz, p, self.Gi, Gv, self.Gsz, h,\n            self.Ai, Av, self.Asz, b, self.eps, self.verbose,\n            self.notImprovedLim, self.maxIter)\n\n        self.save_for_backward(zhats, Qv, p, Gv, h, Av, b)\n        return zhats\n\n    def backward(self, dl_dzhat):\n        zhats, Qv, p, Gv, h, Av, b = self.saved_tensors\n\n        Di = type(self.Qi)([range(self.nineq), range(self.nineq)])\n        Dv = self.lams / self.slacks\n        Dsz = torch.Size([self.nineq, self.nineq])\n        dx, _, dlam, dnu = pdipm_spb.solve_kkt(\n            self.Qi, Qv, self.Qsz, Di, Dv, Dsz,\n            self.Gi, Gv, self.Gsz,\n            self.Ai, Av, self.Asz, dl_dzhat,\n            type(p)(self.nBatch, self.nineq).zero_(),\n            type(p)(self.nBatch, self.nineq).zero_(),\n            type(p)(self.nBatch, self.neq).zero_())\n\n        dps = dx\n\n        dGs = bger(dlam, zhats) + bger(self.lams, dx)\n        GM = torch.cuda.sparse.DoubleTensor(\n            self.Gi, Gv[0].clone().fill_(1.0), self.Gsz\n        ).to_dense().byte().expand_as(dGs)\n        dGs = dGs[GM].view_as(Gv)\n\n        dhs = -dlam\n\n        dAs = bger(dnu, zhats) + bger(self.nus, dx)\n        AM = torch.cuda.sparse.DoubleTensor(\n            self.Ai, Av[0].clone().fill_(1.0), self.Asz\n        ).to_dense().byte().expand_as(dAs)\n        dAs = dAs[AM].view_as(Av)\n\n        dbs = -dnu\n\n        dQs = 0.5 * (bger(dx, zhats) + bger(zhats, dx))\n        QM = torch.cuda.sparse.DoubleTensor(\n            self.Qi, Qv[0].clone().fill_(1.0), self.Qsz\n        ).to_dense().byte().expand_as(dQs)\n        dQs = dQs[QM].view_as(Qv)\n\n        grads = (dQs, dps, dGs, dhs, dAs, dbs)\n\n        return grads\n'"
qpth/util.py,2,"b'import torch\nimport numpy as np\n\n\ndef print_header(msg):\n    print(\'===>\', msg)\n\n\ndef to_np(t):\n    if t is None:\n        return None\n    elif t.nelement() == 0:\n        return np.array([])\n    else:\n        return t.cpu().numpy()\n\n\ndef bger(x, y):\n    return x.unsqueeze(2).bmm(y.unsqueeze(1))\n\n\ndef get_sizes(G, A=None):\n    if G.dim() == 2:\n        nineq, nz = G.size()\n        nBatch = 1\n    elif G.dim() == 3:\n        nBatch, nineq, nz = G.size()\n    if A is not None:\n        neq = A.size(1) if A.nelement() > 0 else 0\n    else:\n        neq = None\n    # nBatch = batchedTensor.size(0) if batchedTensor is not None else None\n    return nineq, nz, neq, nBatch\n\n\ndef bdiag(d):\n    nBatch, sz = d.size()\n    D = torch.zeros(nBatch, sz, sz).type_as(d)\n    I = torch.eye(sz).repeat(nBatch, 1, 1).type_as(d).bool()\n    D[I] = d.squeeze().view(-1)\n    return D\n\n\ndef expandParam(X, nBatch, nDim):\n    if X.ndimension() in (0, nDim) or X.nelement() == 0:\n        return X, False\n    elif X.ndimension() == nDim - 1:\n        return X.unsqueeze(0).expand(*([nBatch] + list(X.size()))), True\n    else:\n        raise RuntimeError(""Unexpected number of dimensions."")\n\n\ndef extract_nBatch(Q, p, G, h, A, b):\n    dims = [3, 2, 3, 2, 3, 2]\n    params = [Q, p, G, h, A, b]\n    for param, dim in zip(params, dims):\n        if param.ndimension() == dim:\n            return param.size(0)\n    return 1\n'"
qpth/solvers/__init__.py,0,"b""__all__ = ['cvxpy', 'pdipm']\n\nfrom . import cvxpy\n"""
qpth/solvers/cvxpy.py,0,"b""import cvxpy as cp\nimport numpy as np\n\n\ndef forward_single_np(Q, p, G, h, A, b):\n    nz, neq, nineq = p.shape[0], A.shape[0] if A is not None else 0, G.shape[0]\n\n    z_ = cp.Variable(nz)\n\n    obj = cp.Minimize(0.5 * cp.quad_form(z_, Q) + p.T * z_)\n    eqCon = A * z_ == b if neq > 0 else None\n    if nineq > 0:\n        slacks = cp.Variable(nineq)\n        ineqCon = G * z_ + slacks == h\n        slacksCon = slacks >= 0\n    else:\n        ineqCon = slacks = slacksCon = None\n    cons = [x for x in [eqCon, ineqCon, slacksCon] if x is not None]\n    prob = cp.Problem(obj, cons)\n    prob.solve()  # solver=cp.SCS, max_iters=5000, verbose=False)\n    # prob.solve(solver=cp.SCS, max_iters=10000, verbose=True)\n    assert('optimal' in prob.status)\n    zhat = np.array(z_.value).ravel()\n    nu = np.array(eqCon.dual_value).ravel() if eqCon is not None else None\n    if ineqCon is not None:\n        lam = np.array(ineqCon.dual_value).ravel()\n        slacks = np.array(slacks.value).ravel()\n    else:\n        lam = slacks = None\n\n    return prob.value, zhat, nu, lam, slacks\n"""
qpth/solvers/pdipm/__init__.py,0,b''
qpth/solvers/pdipm/batch.py,67,"b'import torch\nfrom enum import Enum\n# from block import block\n\nfrom qpth.util import get_sizes, bdiag\n\n\ndef lu_hack(x):\n    data, pivots = x.lu(pivot=not x.is_cuda)\n    if x.is_cuda:\n        if x.ndimension() == 2:\n            pivots = torch.arange(1, 1+x.size(0)).int().cuda()\n        elif x.ndimension() == 3:\n            pivots = torch.arange(\n                1, 1+x.size(1),\n            ).unsqueeze(0).repeat(x.size(0), 1).int().cuda()\n        else:\n            assert False\n    return (data, pivots)\n\n\nINACC_ERR = """"""\n--------\nqpth warning: Returning an inaccurate and potentially incorrect solution.\n\nSome residual is large.\nYour problem may be infeasible or difficult.\n\nYou can try using the CVXPY solver to see if your problem is feasible\nand you can use the verbose option to check the convergence status of\nour solver while increasing the number of iterations.\n\nAdvanced users:\nYou can also try to enable iterative refinement in the solver:\nhttps://github.com/locuslab/qpth/issues/6\n--------\n""""""\n\n\nclass KKTSolvers(Enum):\n    LU_FULL = 1\n    LU_PARTIAL = 2\n    IR_UNOPT = 3\n\n\ndef forward(Q, p, G, h, A, b, Q_LU, S_LU, R, eps=1e-12, verbose=0, notImprovedLim=3,\n            maxIter=20, solver=KKTSolvers.LU_PARTIAL):\n    """"""\n    Q_LU, S_LU, R = pre_factor_kkt(Q, G, A)\n    """"""\n    nineq, nz, neq, nBatch = get_sizes(G, A)\n\n    # Find initial values\n    if solver == KKTSolvers.LU_FULL:\n        D = torch.eye(nineq).repeat(nBatch, 1, 1).type_as(Q)\n        x, s, z, y = factor_solve_kkt(\n            Q, D, G, A, p,\n            torch.zeros(nBatch, nineq).type_as(Q),\n            -h, -b if b is not None else None)\n    elif solver == KKTSolvers.LU_PARTIAL:\n        d = torch.ones(nBatch, nineq).type_as(Q)\n        factor_kkt(S_LU, R, d)\n        x, s, z, y = solve_kkt(\n            Q_LU, d, G, A, S_LU,\n            p, torch.zeros(nBatch, nineq).type_as(Q),\n            -h, -b if neq > 0 else None)\n    elif solver == KKTSolvers.IR_UNOPT:\n        D = torch.eye(nineq).repeat(nBatch, 1, 1).type_as(Q)\n        x, s, z, y = solve_kkt_ir(\n            Q, D, G, A, p,\n            torch.zeros(nBatch, nineq).type_as(Q),\n            -h, -b if b is not None else None)\n    else:\n        assert False\n\n    # Make all of the slack variables >= 1.\n    M = torch.min(s, 1)[0]\n    M = M.view(M.size(0), 1).repeat(1, nineq)\n    I = M < 0\n    s[I] -= M[I] - 1\n\n    # Make all of the inequality dual variables >= 1.\n    M = torch.min(z, 1)[0]\n    M = M.view(M.size(0), 1).repeat(1, nineq)\n    I = M < 0\n    z[I] -= M[I] - 1\n\n    best = {\'resids\': None, \'x\': None, \'z\': None, \'s\': None, \'y\': None}\n    nNotImproved = 0\n\n    for i in range(maxIter):\n        # affine scaling direction\n        rx = (torch.bmm(y.unsqueeze(1), A).squeeze(1) if neq > 0 else 0.) + \\\n            torch.bmm(z.unsqueeze(1), G).squeeze(1) + \\\n            torch.bmm(x.unsqueeze(1), Q.transpose(1, 2)).squeeze(1) + \\\n            p\n        rs = z\n        rz = torch.bmm(x.unsqueeze(1), G.transpose(1, 2)).squeeze(1) + s - h\n        ry = torch.bmm(x.unsqueeze(1), A.transpose(\n            1, 2)).squeeze(1) - b if neq > 0 else 0.0\n        mu = torch.abs((s * z).sum(1).squeeze() / nineq)\n        z_resid = torch.norm(rz, 2, 1).squeeze()\n        y_resid = torch.norm(ry, 2, 1).squeeze() if neq > 0 else 0\n        pri_resid = y_resid + z_resid\n        dual_resid = torch.norm(rx, 2, 1).squeeze()\n        resids = pri_resid + dual_resid + nineq * mu\n\n        d = z / s\n        try:\n            factor_kkt(S_LU, R, d)\n        except:\n            return best[\'x\'], best[\'y\'], best[\'z\'], best[\'s\']\n\n        if verbose == 1:\n            print(\'iter: {}, pri_resid: {:.5e}, dual_resid: {:.5e}, mu: {:.5e}\'.format(\n                i, pri_resid.mean(), dual_resid.mean(), mu.mean()))\n        if best[\'resids\'] is None:\n            best[\'resids\'] = resids\n            best[\'x\'] = x.clone()\n            best[\'z\'] = z.clone()\n            best[\'s\'] = s.clone()\n            best[\'y\'] = y.clone() if y is not None else None\n            nNotImproved = 0\n        else:\n            I = resids < best[\'resids\']\n            if I.sum() > 0:\n                nNotImproved = 0\n            else:\n                nNotImproved += 1\n            I_nz = I.repeat(nz, 1).t()\n            I_nineq = I.repeat(nineq, 1).t()\n            best[\'resids\'][I] = resids[I]\n            best[\'x\'][I_nz] = x[I_nz]\n            best[\'z\'][I_nineq] = z[I_nineq]\n            best[\'s\'][I_nineq] = s[I_nineq]\n            if neq > 0:\n                I_neq = I.repeat(neq, 1).t()\n                best[\'y\'][I_neq] = y[I_neq]\n        if nNotImproved == notImprovedLim or best[\'resids\'].max() < eps or mu.min() > 1e32:\n            if best[\'resids\'].max() > 1. and verbose >= 0:\n                print(INACC_ERR)\n            return best[\'x\'], best[\'y\'], best[\'z\'], best[\'s\']\n\n        if solver == KKTSolvers.LU_FULL:\n            D = bdiag(d)\n            dx_aff, ds_aff, dz_aff, dy_aff = factor_solve_kkt(\n                Q, D, G, A, rx, rs, rz, ry)\n        elif solver == KKTSolvers.LU_PARTIAL:\n            dx_aff, ds_aff, dz_aff, dy_aff = solve_kkt(\n                Q_LU, d, G, A, S_LU, rx, rs, rz, ry)\n        elif solver == KKTSolvers.IR_UNOPT:\n            D = bdiag(d)\n            dx_aff, ds_aff, dz_aff, dy_aff = solve_kkt_ir(\n                Q, D, G, A, rx, rs, rz, ry)\n        else:\n            assert False\n\n        # compute centering directions\n        alpha = torch.min(torch.min(get_step(z, dz_aff),\n                                    get_step(s, ds_aff)),\n                          torch.ones(nBatch).type_as(Q))\n        alpha_nineq = alpha.repeat(nineq, 1).t()\n        t1 = s + alpha_nineq * ds_aff\n        t2 = z + alpha_nineq * dz_aff\n        t3 = torch.sum(t1 * t2, 1).squeeze()\n        t4 = torch.sum(s * z, 1).squeeze()\n        sig = (t3 / t4)**3\n\n        rx = torch.zeros(nBatch, nz).type_as(Q)\n        rs = ((-mu * sig).repeat(nineq, 1).t() + ds_aff * dz_aff) / s\n        rz = torch.zeros(nBatch, nineq).type_as(Q)\n        ry = torch.zeros(nBatch, neq).type_as(Q) if neq > 0 else torch.Tensor()\n\n        if solver == KKTSolvers.LU_FULL:\n            D = bdiag(d)\n            dx_cor, ds_cor, dz_cor, dy_cor = factor_solve_kkt(\n                Q, D, G, A, rx, rs, rz, ry)\n        elif solver == KKTSolvers.LU_PARTIAL:\n            dx_cor, ds_cor, dz_cor, dy_cor = solve_kkt(\n                Q_LU, d, G, A, S_LU, rx, rs, rz, ry)\n        elif solver == KKTSolvers.IR_UNOPT:\n            D = bdiag(d)\n            dx_cor, ds_cor, dz_cor, dy_cor = solve_kkt_ir(\n                Q, D, G, A, rx, rs, rz, ry)\n        else:\n            assert False\n\n        dx = dx_aff + dx_cor\n        ds = ds_aff + ds_cor\n        dz = dz_aff + dz_cor\n        dy = dy_aff + dy_cor if neq > 0 else None\n        alpha = torch.min(0.999 * torch.min(get_step(z, dz),\n                                            get_step(s, ds)),\n                          torch.ones(nBatch).type_as(Q))\n        alpha_nineq = alpha.repeat(nineq, 1).t()\n        alpha_neq = alpha.repeat(neq, 1).t() if neq > 0 else None\n        alpha_nz = alpha.repeat(nz, 1).t()\n\n        x += alpha_nz * dx\n        s += alpha_nineq * ds\n        z += alpha_nineq * dz\n        y = y + alpha_neq * dy if neq > 0 else None\n\n    if best[\'resids\'].max() > 1. and verbose >= 0:\n        print(INACC_ERR)\n    return best[\'x\'], best[\'y\'], best[\'z\'], best[\'s\']\n\n\ndef get_step(v, dv):\n    a = -v / dv\n    a[dv > 0] = max(1.0, a.max())\n    return a.min(1)[0].squeeze()\n\n\ndef unpack_kkt(v, nz, nineq, neq):\n    i = 0\n    x = v[:, i:i + nz]\n    i += nz\n    s = v[:, i:i + nineq]\n    i += nineq\n    z = v[:, i:i + nineq]\n    i += nineq\n    y = v[:, i:i + neq]\n    return x, s, z, y\n\n\ndef kkt_resid_reg(Q_tilde, D_tilde, G, A, eps, dx, ds, dz, dy, rx, rs, rz, ry):\n    dx, ds, dz, dy, rx, rs, rz, ry = [\n        x.unsqueeze(2) if x is not None else None for x in\n        [dx, ds, dz, dy, rx, rs, rz, ry]\n    ]\n    resx = Q_tilde.bmm(dx) + G.transpose(1, 2).bmm(dz) + rx\n    if dy is not None:\n        resx += A.transpose(1, 2).bmm(dy)\n    ress = D_tilde.bmm(ds) + dz + rs\n    resz = G.bmm(dx) + ds - eps * dz + rz\n    resy = A.bmm(dx) - eps * dy + ry if dy is not None else None\n    resx, ress, resz, resy = (\n        v.squeeze(2) if v is not None else None for v in (resx, ress, resz, resy))\n    return resx, ress, resz, resy\n\n\ndef solve_kkt_ir(Q, D, G, A, rx, rs, rz, ry, niter=1):\n    """"""Inefficient iterative refinement.""""""\n    nineq, nz, neq, nBatch = get_sizes(G, A)\n\n    eps = 1e-7\n    Q_tilde = Q + eps * torch.eye(nz).type_as(Q).repeat(nBatch, 1, 1)\n    D_tilde = D + eps * torch.eye(nineq).type_as(Q).repeat(nBatch, 1, 1)\n\n    dx, ds, dz, dy = factor_solve_kkt_reg(\n        Q_tilde, D_tilde, G, A, rx, rs, rz, ry, eps)\n    res = kkt_resid_reg(Q, D, G, A, eps,\n                        dx, ds, dz, dy, rx, rs, rz, ry)\n    resx, ress, resz, resy = res\n    res = resx\n    for k in range(niter):\n        ddx, dds, ddz, ddy = factor_solve_kkt_reg(Q_tilde, D_tilde, G, A, -resx, -ress, -resz,\n                                                  -resy if resy is not None else None,\n                                                  eps)\n        dx, ds, dz, dy = [v + dv if v is not None else None\n                          for v, dv in zip((dx, ds, dz, dy), (ddx, dds, ddz, ddy))]\n        res = kkt_resid_reg(Q, D, G, A, eps,\n                            dx, ds, dz, dy, rx, rs, rz, ry)\n        resx, ress, resz, resy = res\n        # res = torch.cat(resx)\n        res = resx\n\n    return dx, ds, dz, dy\n\n\ndef factor_solve_kkt_reg(Q_tilde, D, G, A, rx, rs, rz, ry, eps):\n    nineq, nz, neq, nBatch = get_sizes(G, A)\n\n    H_ = torch.zeros(nBatch, nz + nineq, nz + nineq).type_as(Q_tilde)\n    H_[:, :nz, :nz] = Q_tilde\n    H_[:, -nineq:, -nineq:] = D\n    if neq > 0:\n        # H_ = torch.cat([torch.cat([Q, torch.zeros(nz,nineq).type_as(Q)], 1),\n        # torch.cat([torch.zeros(nineq, nz).type_as(Q), D], 1)], 0)\n        A_ = torch.cat([torch.cat([G, torch.eye(nineq).type_as(Q_tilde).repeat(nBatch, 1, 1)], 2),\n                        torch.cat([A, torch.zeros(nBatch, neq, nineq).type_as(Q_tilde)], 2)], 1)\n        g_ = torch.cat([rx, rs], 1)\n        h_ = torch.cat([rz, ry], 1)\n    else:\n        A_ = torch.cat(\n            [G, torch.eye(nineq).type_as(Q_tilde).repeat(nBatch, 1, 1)], 2)\n        g_ = torch.cat([rx, rs], 1)\n        h_ = rz\n\n    H_LU = lu_hack(H_)\n\n    invH_A_ = A_.transpose(1, 2).lu_solve(*H_LU)\n    invH_g_ = g_.unsqueeze(2).lu_solve(*H_LU).squeeze(2)\n\n    S_ = torch.bmm(A_, invH_A_)\n    S_ -= eps * torch.eye(neq + nineq).type_as(Q_tilde).repeat(nBatch, 1, 1)\n    S_LU = lu_hack(S_)\n    t_ = torch.bmm(invH_g_.unsqueeze(1), A_.transpose(1, 2)).squeeze(1) - h_\n    w_ = -t_.unsqueeze(2).lu_solve(*S_LU).squeeze(2)\n    t_ = -g_ - w_.unsqueeze(1).bmm(A_).squeeze()\n    v_ = t_.unsqueeze(2).lu_solve(*H_LU).squeeze(2)\n\n    dx = v_[:, :nz]\n    ds = v_[:, nz:]\n    dz = w_[:, :nineq]\n    dy = w_[:, nineq:] if neq > 0 else None\n\n    return dx, ds, dz, dy\n\n\ndef factor_solve_kkt(Q, D, G, A, rx, rs, rz, ry):\n    nineq, nz, neq, nBatch = get_sizes(G, A)\n\n    H_ = torch.zeros(nBatch, nz + nineq, nz + nineq).type_as(Q)\n    H_[:, :nz, :nz] = Q\n    H_[:, -nineq:, -nineq:] = D\n    if neq > 0:\n        A_ = torch.cat([torch.cat([G, torch.eye(nineq).type_as(Q).repeat(nBatch, 1, 1)], 2),\n                        torch.cat([A, torch.zeros(nBatch, neq, nineq).type_as(Q)], 2)], 1)\n        g_ = torch.cat([rx, rs], 1)\n        h_ = torch.cat([rz, ry], 1)\n    else:\n        A_ = torch.cat([G, torch.eye(nineq).type_as(Q)], 1)\n        g_ = torch.cat([rx, rs], 1)\n        h_ = rz\n\n    H_LU = lu_hack(H_)\n\n    invH_A_ = A_.transpose(1, 2).lu_solve(*H_LU)\n    invH_g_ = g_.unsqueeze(2).lu_solve(*H_LU).squeeze(2)\n\n    S_ = torch.bmm(A_, invH_A_)\n    S_LU = lu_hack(S_)\n    t_ = torch.bmm(invH_g_.unsqueeze(1), A_.transpose(1, 2)).squeeze(1) - h_\n    w_ = -t_.unsqueeze(2).lu_solve(*S_LU).squeeze(2)\n    t_ = -g_ - w_.unsqueeze(1).bmm(A_).squeeze()\n    v_ = t_.unsqueeze(2).lu_solve(*H_LU).squeeze(2)\n\n    dx = v_[:, :nz]\n    ds = v_[:, nz:]\n    dz = w_[:, :nineq]\n    dy = w_[:, nineq:] if neq > 0 else None\n\n    return dx, ds, dz, dy\n\n\ndef solve_kkt(Q_LU, d, G, A, S_LU, rx, rs, rz, ry):\n    """""" Solve KKT equations for the affine step""""""\n    nineq, nz, neq, nBatch = get_sizes(G, A)\n\n    invQ_rx = rx.unsqueeze(2).lu_solve(*Q_LU).squeeze(2)\n    if neq > 0:\n        h = torch.cat((invQ_rx.unsqueeze(1).bmm(A.transpose(1, 2)).squeeze(1) - ry,\n                       invQ_rx.unsqueeze(1).bmm(G.transpose(1, 2)).squeeze(1) + rs / d - rz), 1)\n    else:\n        h = invQ_rx.unsqueeze(1).bmm(G.transpose(1, 2)).squeeze(1) + rs / d - rz\n\n    w = -(h.unsqueeze(2).lu_solve(*S_LU)).squeeze(2)\n\n    g1 = -rx - w[:, neq:].unsqueeze(1).bmm(G).squeeze(1)\n    if neq > 0:\n        g1 -= w[:, :neq].unsqueeze(1).bmm(A).squeeze(1)\n    g2 = -rs - w[:, neq:]\n\n    dx = g1.unsqueeze(2).lu_solve(*Q_LU).squeeze(2)\n    ds = g2 / d\n    dz = w[:, neq:]\n    dy = w[:, :neq] if neq > 0 else None\n\n    return dx, ds, dz, dy\n\n\ndef pre_factor_kkt(Q, G, A):\n    """""" Perform all one-time factorizations and cache relevant matrix products""""""\n    nineq, nz, neq, nBatch = get_sizes(G, A)\n\n    try:\n        Q_LU = lu_hack(Q)\n    except:\n        raise RuntimeError(""""""\nqpth Error: Cannot perform LU factorization on Q.\nPlease make sure that your Q matrix is PSD and has\na non-zero diagonal.\n"""""")\n\n    # S = [ A Q^{-1} A^T        A Q^{-1} G^T          ]\n    #     [ G Q^{-1} A^T        G Q^{-1} G^T + D^{-1} ]\n    #\n    # We compute a partial LU decomposition of the S matrix\n    # that can be completed once D^{-1} is known.\n    # See the \'Block LU factorization\' part of our website\n    # for more details.\n\n    G_invQ_GT = torch.bmm(G, G.transpose(1, 2).lu_solve(*Q_LU))\n    R = G_invQ_GT.clone()\n    S_LU_pivots = torch.IntTensor(range(1, 1 + neq + nineq)).unsqueeze(0) \\\n        .repeat(nBatch, 1).type_as(Q).int()\n    if neq > 0:\n        invQ_AT = A.transpose(1, 2).lu_solve(*Q_LU)\n        A_invQ_AT = torch.bmm(A, invQ_AT)\n        G_invQ_AT = torch.bmm(G, invQ_AT)\n\n        LU_A_invQ_AT = lu_hack(A_invQ_AT)\n        P_A_invQ_AT, L_A_invQ_AT, U_A_invQ_AT = torch.lu_unpack(*LU_A_invQ_AT)\n        P_A_invQ_AT = P_A_invQ_AT.type_as(A_invQ_AT)\n\n        S_LU_11 = LU_A_invQ_AT[0]\n        U_A_invQ_AT_inv = (P_A_invQ_AT.bmm(L_A_invQ_AT)\n                           ).lu_solve(*LU_A_invQ_AT)\n        S_LU_21 = G_invQ_AT.bmm(U_A_invQ_AT_inv)\n        T = G_invQ_AT.transpose(1, 2).lu_solve(*LU_A_invQ_AT)\n        S_LU_12 = U_A_invQ_AT.bmm(T)\n        S_LU_22 = torch.zeros(nBatch, nineq, nineq).type_as(Q)\n        S_LU_data = torch.cat((torch.cat((S_LU_11, S_LU_12), 2),\n                               torch.cat((S_LU_21, S_LU_22), 2)),\n                              1)\n        S_LU_pivots[:, :neq] = LU_A_invQ_AT[1]\n\n        R -= G_invQ_AT.bmm(T)\n    else:\n        S_LU_data = torch.zeros(nBatch, nineq, nineq).type_as(Q)\n\n    S_LU = [S_LU_data, S_LU_pivots]\n    return Q_LU, S_LU, R\n\n\nfactor_kkt_eye = None\n\n\ndef factor_kkt(S_LU, R, d):\n    """""" Factor the U22 block that we can only do after we know D. """"""\n    nBatch, nineq = d.size()\n    neq = S_LU[1].size(1) - nineq\n    # TODO: There\'s probably a better way to add a batched diagonal.\n    global factor_kkt_eye\n    if factor_kkt_eye is None or factor_kkt_eye.size() != d.size():\n        # print(\'Updating batchedEye size.\')\n        factor_kkt_eye = torch.eye(nineq).repeat(\n            nBatch, 1, 1).type_as(R).bool()\n    T = R.clone()\n    T[factor_kkt_eye] += (1. / d).squeeze().view(-1)\n\n    T_LU = lu_hack(T)\n\n    if not T.is_cuda:\n        # TODO: Don\'t use pivoting in most cases because\n        # torch.lu_unpack is inefficient here:\n        oldPivotsPacked = S_LU[1][:, -nineq:] - neq\n        oldPivots, _, _ = torch.lu_unpack(\n            T_LU[0], oldPivotsPacked, unpack_data=False)\n        newPivotsPacked = T_LU[1]\n        newPivots, _, _ = torch.lu_unpack(\n            T_LU[0], newPivotsPacked, unpack_data=False)\n\n        # Re-pivot the S_LU_21 block.\n        if neq > 0:\n            S_LU_21 = S_LU[0][:, -nineq:, :neq]\n            S_LU[0][:, -nineq:,\n                    :neq] = newPivots.transpose(1, 2).bmm(oldPivots.bmm(S_LU_21))\n\n        # Add the new S_LU_22 block pivots.\n        S_LU[1][:, -nineq:] = newPivotsPacked + neq\n\n    # Add the new S_LU_22 block.\n    S_LU[0][:, -nineq:, -nineq:] = T_LU[0]\n'"
qpth/solvers/pdipm/single.py,67,"b'import torch\nimport numpy as np\n\nfrom qpth.util import get_sizes\n\n# TODO: Add more comments describing the math here.\n# https://stanford.edu/~boyd/papers/pdf/code_gen_impl.pdf\n\n\ndef forward(inputs_i, Q, G, A, b, h, U_Q, U_S, R, verbose=False):\n    """"""\n    b = A z_0\n    h = G z_0 + s_0\n    U_Q, U_S, R = pre_factor_kkt(Q, G, A, nineq, neq)\n    """"""\n    nineq, nz, neq, _ = get_sizes(G, A)\n\n    # find initial values\n    d = torch.ones(nineq).type_as(Q)\n    nb = -b if b is not None else None\n    factor_kkt(U_S, R, d)\n    x, s, z, y = solve_kkt(\n        U_Q, d, G, A, U_S,\n        inputs_i, torch.zeros(nineq).type_as(Q), -h, nb)\n    # x1, s1, z1, y1 = factor_solve_kkt(Q, torch.eye(nineq).type_as(Q), G, A, inputs_i,\n    # torch.zeros(nineq).type_as(Q), -h, nb)\n\n    if torch.min(s) < 0:\n        s -= torch.min(s) - 1\n    if torch.min(z) < 0:\n        z -= torch.min(z) - 1\n\n    prev_resid = None\n    for i in range(20):\n        # affine scaling direction\n        rx = (torch.mv(A.t(), y) if neq > 0 else 0.) + \\\n            torch.mv(G.t(), z) + torch.mv(Q, x) + inputs_i\n        rs = z\n        rz = torch.mv(G, x) + s - h\n        ry = torch.mv(A, x) - b if neq > 0 else torch.Tensor([0.])\n        mu = torch.dot(s, z) / nineq\n        pri_resid = torch.norm(ry) + torch.norm(rz)\n        dual_resid = torch.norm(rx)\n        resid = pri_resid + dual_resid + nineq * mu\n        d = z / s\n        if verbose:\n            print((""primal_res = {0:.5g}, dual_res = {1:.5g}, "" +\n                   ""gap = {2:.5g}, kappa(d) = {3:.5g}"").format(\n                pri_resid, dual_resid, mu, min(d) / max(d)))\n        # if (pri_resid < 5e-4 and dual_resid < 5e-4 and mu < 4e-4):\n        improved = (prev_resid is None) or (resid < prev_resid + 1e-6)\n        if not improved or resid < 1e-6:\n            return x, y, z\n        prev_resid = resid\n\n        factor_kkt(U_S, R, d)\n        dx_aff, ds_aff, dz_aff, dy_aff = solve_kkt(U_Q, d, G, A, U_S,\n                                                   rx, rs, rz, ry)\n        # D = torch.diag((z/s).cpu()).type_as(Q)\n        # dx_aff1, ds_aff1, dz_aff1, dy_aff1 = factor_solve_kkt(Q, D, G, A, rx, rs, rz, ry)\n\n        # compute centering directions\n        alpha = min(min(get_step(z, dz_aff), get_step(s, ds_aff)), 1.0)\n        sig = (torch.dot(s + alpha * ds_aff, z +\n                         alpha * dz_aff) / (torch.dot(s, z)))**3\n        dx_cor, ds_cor, dz_cor, dy_cor = solve_kkt(\n            U_Q, d, G, A, U_S, torch.zeros(nz).type_as(Q),\n            (-mu * sig * torch.ones(nineq).type_as(Q) + ds_aff * dz_aff) / s,\n            torch.zeros(nineq).type_as(Q), torch.zeros(neq).type_as(Q))\n        # dx_cor, ds_cor, dz_cor, dy_cor = factor_solve_kkt(Q, D, G, A,\n        #     torch.zeros(nz).type_as(Q),\n        #     (-mu*sig*torch.ones(nineq).type_as(Q) + ds_aff*dz_aff)/s,\n        #     torch.zeros(nineq).type_as(Q), torch.zeros(neq).type_as(Q))\n\n        dx = dx_aff + dx_cor\n        ds = ds_aff + ds_cor\n        dz = dz_aff + dz_cor\n        dy = dy_aff + dy_cor if neq > 0 else None\n        alpha = min(1.0, 0.999 * min(get_step(s, ds), get_step(z, dz)))\n        dx_norm = torch.norm(dx)\n        dz_norm = torch.norm(dz)\n        if np.isnan(dx_norm) or dx_norm > 1e5 or dz_norm > 1e5:\n            # Overflow, return early\n            return x, y, z\n\n        x += alpha * dx\n        s += alpha * ds\n        z += alpha * dz\n        y = y + alpha * dy if neq > 0 else None\n\n    return x, y, z\n\n\ndef get_step(v, dv):\n    I = dv < 1e-12\n    if torch.sum(I) > 0:  # TODO: Use something like torch.any(dv < 0)\n        a = -v / dv\n        return torch.min(a[I])\n    else:\n        return 1\n\n\ndef solve_kkt(U_Q, d, G, A, U_S, rx, rs, rz, ry, dbg=False):\n    """""" Solve KKT equations for the affine step""""""\n    nineq, nz, neq, _ = get_sizes(G, A)\n\n    invQ_rx = torch.potrs(rx.view(-1, 1), U_Q).view(-1)\n    if neq > 0:\n        h = torch.cat([torch.mv(A, invQ_rx) - ry,\n                       torch.mv(G, invQ_rx) + rs / d - rz], 0)\n    else:\n        h = torch.mv(G, invQ_rx) + rs / d - rz\n\n    w = -torch.potrs(h.view(-1, 1), U_S).view(-1)\n\n    g1 = -rx - torch.mv(G.t(), w[neq:])\n    if neq > 0:\n        g1 -= torch.mv(A.t(), w[:neq])\n    g2 = -rs - w[neq:]\n\n    dx = torch.potrs(g1.view(-1, 1), U_Q).view(-1)\n    ds = g2 / d\n    dz = w[neq:]\n    dy = w[:neq] if neq > 0 else None\n\n    # if np.all(np.array([x.norm() for x in [rx, rs, rz, ry]]) != 0):\n    if dbg:\n        import IPython\n        import sys\n        IPython.embed()\n        sys.exit(-1)\n\n    # if rs.norm() > 0: import IPython, sys; IPython.embed(); sys.exit(-1)\n    return dx, ds, dz, dy\n\n\ndef pre_factor_kkt(Q, G, A):\n    """""" Perform all one-time factorizations and cache relevant matrix products""""""\n    nineq, nz, neq, _ = get_sizes(G, A)\n\n    # S = [ A Q^{-1} A^T        A Q^{-1} G^T           ]\n    #     [ G Q^{-1} A^T        G Q^{-1} G^T + D^{-1} ]\n\n    U_Q = torch.potrf(Q)\n    # partial cholesky of S matrix\n    U_S = torch.zeros(neq + nineq, neq + nineq).type_as(Q)\n\n    G_invQ_GT = torch.mm(G, torch.potrs(G.t(), U_Q))\n    R = G_invQ_GT\n    if neq > 0:\n        invQ_AT = torch.potrs(A.t(), U_Q)\n        A_invQ_AT = torch.mm(A, invQ_AT)\n        G_invQ_AT = torch.mm(G, invQ_AT)\n\n        # TODO: torch.potrf sometimes says the matrix is not PSD but\n        # numpy does? I filed an issue at\n        # https://github.com/pytorch/pytorch/issues/199\n        try:\n            U11 = torch.potrf(A_invQ_AT)\n        except:\n            U11 = torch.Tensor(np.linalg.cholesky(\n                A_invQ_AT.cpu().numpy())).type_as(A_invQ_AT)\n\n        # TODO: torch.trtrs is currently not implemented on the GPU\n        # and we are using gesv as a workaround.\n        U12 = torch.gesv(G_invQ_AT.t(), U11.t())[0]\n        U_S[:neq, :neq] = U11\n        U_S[:neq, neq:] = U12\n        R -= torch.mm(U12.t(), U12)\n\n    return U_Q, U_S, R\n\n\ndef factor_kkt(U_S, R, d):\n    """""" Factor the U22 block that we can only do after we know D. """"""\n    nineq = R.size(0)\n    U_S[-nineq:, -nineq:] = torch.potrf(R + torch.diag(1 / d.cpu()).type_as(d))\n\n\ndef factor_solve_kkt(Q, D, G, A, rx, rs, rz, ry):\n    nineq, nz, neq, _ = get_sizes(G, A)\n\n    if neq > 0:\n        H_ = torch.cat([torch.cat([Q, torch.zeros(nz, nineq).type_as(Q)], 1),\n                        torch.cat([torch.zeros(nineq, nz).type_as(Q), D], 1)], 0)\n        A_ = torch.cat([torch.cat([G, torch.eye(nineq).type_as(Q)], 1),\n                        torch.cat([A, torch.zeros(neq, nineq).type_as(Q)], 1)], 0)\n        g_ = torch.cat([rx, rs], 0)\n        h_ = torch.cat([rz, ry], 0)\n    else:\n        H_ = torch.cat([torch.cat([Q, torch.zeros(nz, nineq).type_as(Q)], 1),\n                        torch.cat([torch.zeros(nineq, nz).type_as(Q), D], 1)], 0)\n        A_ = torch.cat([G, torch.eye(nineq).type_as(Q)], 1)\n        g_ = torch.cat([rx, rs], 0)\n        h_ = rz\n\n    U_H_ = torch.potrf(H_)\n\n    invH_A_ = torch.potrs(A_.t(), U_H_)\n    invH_g_ = torch.potrs(g_.view(-1, 1), U_H_).view(-1)\n\n    S_ = torch.mm(A_, invH_A_)\n    U_S_ = torch.potrf(S_)\n    t_ = torch.mv(A_, invH_g_).view(-1, 1) - h_\n    w_ = -torch.potrs(t_, U_S_).view(-1)\n    v_ = torch.potrs(-g_.view(-1, 1) - torch.mv(A_.t(), w_), U_H_).view(-1)\n\n    return v_[:nz], v_[nz:], w_[:nineq], w_[nineq:] if neq > 0 else None\n'"
qpth/solvers/pdipm/spbatch.py,40,"b'import numpy as np\nimport torch\nfrom enum import Enum\n# from block import block\n\n\nINACC_ERR = """"""\n--------\nqpth warning: Returning an inaccurate and potentially incorrect solutino.\n\nSome residual is large.\nYour problem may be infeasible or difficult.\n\nYou can try using the CVXPY solver to see if your problem is feasible\nand you can use the verbose option to check the convergence status of\nour solver while increasing the number of iterations.\n\nAdvanced users:\nYou can also try to enable iterative refinement in the solver:\nhttps://github.com/locuslab/qpth/issues/6\n--------\n""""""\n\n\nclass KKTSolvers(Enum):\n    QR = 1\n\n\ndef forward(Qi, Qv, Qsz, p, Gi, Gv, Gsz, h, Ai, Av, Asz, b,\n            eps=1e-12, verbose=0, notImprovedLim=3, maxIter=20):\n    spTensor = torch.cuda.sparse.DoubleTensor if Qv.is_cuda else torch.sparse.DoubleTensor\n    nBatch = Qv.size(0)\n    Qs = [spTensor(Qi, Qv[j], Qsz) for j in range(nBatch)]\n    Gs = [spTensor(Gi, Gv[j], Gsz) for j in range(nBatch)]\n    As = [spTensor(Ai, Av[j], Asz) for j in range(nBatch)]\n\n    nineq, nz = Gsz\n    neq, _ = Asz\n\n    solver = KKTSolvers.QR\n\n    KKTeps = 1e-7  # For the regularized KKT matrix.\n\n    # Find initial values\n    if solver == KKTSolvers.QR:\n        Di = torch.LongTensor([range(nineq), range(nineq)]).type_as(Qi)\n        Dv = torch.ones(nBatch, nineq).type_as(Qv)\n        Dsz = torch.Size([nineq, nineq])\n        Ks, K, Didx = cat_kkt(Qi, Qv, Qsz, Gi, Gv, Gsz,\n                              Ai, Av, Asz, Di, Dv, Dsz, 0.0)\n        Ktildes, Ktilde, Didxtilde = cat_kkt(\n            Qi, Qv, Qsz, Gi, Gv, Gsz, Ai, Av, Asz, Di, Dv, Dsz, KKTeps)\n        assert torch.norm((Didx - Didxtilde).float()) == 0.0\n        x, s, z, y = solve_kkt(Ks, K, Ktildes, Ktilde,\n                               p, torch.zeros(nBatch, nineq).type_as(p),\n                               -h, -b if b is not None else None)\n    else:\n        assert False\n\n    M = torch.min(s, 1)[0].repeat(1, nineq)\n    I = M < 0\n    s[I] -= M[I] - 1\n\n    M = torch.min(z, 1)[0].repeat(1, nineq)\n    I = M < 0\n    z[I] -= M[I] - 1\n\n    best = {\'resids\': None, \'x\': None, \'z\': None, \'s\': None, \'y\': None}\n    nNotImproved = 0\n\n    for i in range(maxIter):\n        # affine scaling direction\n        rx = torch.cat([(torch.mm(As[j].t(), y[j].unsqueeze(1)) if neq > 0 else 0.) +\n                        torch.mm(Gs[j].t(), z[j].unsqueeze(1)) +\n                        torch.mm(Qs[j], x[j].unsqueeze(1)) +\n                        p[j] for j in range(nBatch)], 1).t()\n        rs = z\n        rz = torch.cat([torch.mm(Gs[j], x[j].unsqueeze(1)) +\n                        s[j] - h[j] for j in range(nBatch)], 1).t()\n        ry = torch.cat([torch.mm(As[j], x[j].unsqueeze(1)) - b[j]\n                        for j in range(nBatch)], 1).t()\n        mu = torch.abs((s * z).sum(1).squeeze() / nineq)\n        z_resid = torch.norm(rz, 2, 1).squeeze()\n        y_resid = torch.norm(ry, 2, 1).squeeze() if neq > 0 else 0\n        pri_resid = y_resid + z_resid\n        dual_resid = torch.norm(rx, 2, 1).squeeze()\n        resids = pri_resid + dual_resid + nineq * mu\n\n        if verbose == 1:\n            print(\'iter: {}, pri_resid: {:.5e}, dual_resid: {:.5e}, mu: {:.5e}\'.format(\n                i, pri_resid.mean(), dual_resid.mean(), mu.mean()))\n        if best[\'resids\'] is None:\n            best[\'resids\'] = resids\n            best[\'x\'] = x.clone()\n            best[\'z\'] = z.clone()\n            best[\'s\'] = s.clone()\n            best[\'y\'] = y.clone() if y is not None else None\n            nNotImproved = 0\n        else:\n            I = resids < best[\'resids\']\n            if I.sum() > 0:\n                nNotImproved = 0\n            else:\n                nNotImproved += 1\n            I_nz = I.repeat(nz, 1).t()\n            I_nineq = I.repeat(nineq, 1).t()\n            best[\'resids\'][I] = resids[I]\n            best[\'x\'][I_nz] = x[I_nz]\n            best[\'z\'][I_nineq] = z[I_nineq]\n            best[\'s\'][I_nineq] = s[I_nineq]\n            if neq > 0:\n                I_neq = I.repeat(neq, 1).t()\n                best[\'y\'][I_neq] = y[I_neq]\n        if nNotImproved == notImprovedLim or best[\'resids\'].max() < eps or mu.min() > 1e32:\n            if best[\'resids\'].max() > 1. and verbose >= 0:\n                print(INACC_ERR)\n            return best[\'x\'], best[\'y\'], best[\'z\'], best[\'s\']\n\n        if solver == KKTSolvers.QR:\n            D = z / s\n            K[1].t()[Didx] = D.t()\n            Ktilde[1].t()[Didx] = D.t() + KKTeps\n            # TODO: Share memory between these or handle batched sparse\n            # matrices differently.\n            for j in range(nBatch):\n                Ks[j]._values()[Didx] = D[j]\n                Ktildes[j]._values()[Didx] = D[j] + KKTeps\n            dx_aff, ds_aff, dz_aff, dy_aff = solve_kkt(\n                Ks, K, Ktildes, Ktilde, rx, rs, rz, ry)\n        else:\n            assert False\n\n        # compute centering directions\n        alpha = torch.min(torch.min(get_step(z, dz_aff),\n                                    get_step(s, ds_aff)),\n                          torch.ones(nBatch).type_as(Qv))\n        alpha_nineq = alpha.repeat(nineq, 1).t()\n        t1 = s + alpha_nineq * ds_aff\n        t2 = z + alpha_nineq * dz_aff\n        t3 = torch.sum(t1 * t2, 1).squeeze()\n        t4 = torch.sum(s * z, 1).squeeze()\n        sig = (t3 / t4)**3\n\n        rx = torch.zeros(nBatch, nz).type_as(Qv)\n        rs = ((-mu * sig).repeat(nineq, 1).t() + ds_aff * dz_aff) / s\n        rz = torch.zeros(nBatch, nineq).type_as(Qv)\n        ry = torch.zeros(nBatch, neq).type_as(Qv)\n\n        if solver == KKTSolvers.QR:\n            dx_cor, ds_cor, dz_cor, dy_cor = solve_kkt(\n                Ks, K, Ktildes, Ktilde, rx, rs, rz, ry)\n        else:\n            assert False\n\n        dx = dx_aff + dx_cor\n        ds = ds_aff + ds_cor\n        dz = dz_aff + dz_cor\n        dy = dy_aff + dy_cor if neq > 0 else None\n        alpha = torch.min(0.999 * torch.min(get_step(z, dz),\n                                            get_step(s, ds)),\n                          torch.ones(nBatch).type_as(Qv))\n\n        alpha_nineq = alpha.repeat(nineq, 1).t()\n        alpha_neq = alpha.repeat(neq, 1).t() if neq > 0 else None\n        alpha_nz = alpha.repeat(nz, 1).t()\n\n        x += alpha_nz * dx\n        s += alpha_nineq * ds\n        z += alpha_nineq * dz\n        y = y + alpha_neq * dy if neq > 0 else None\n\n    if best[\'resids\'].max() > 1. and verbose >= 0:\n        print(INACC_ERR)\n    return best[\'x\'], best[\'y\'], best[\'z\'], best[\'s\']\n\n\ndef get_step(v, dv):\n    # nBatch = v.size(0)\n    a = -v / dv\n    a[dv > 0] = max(1.0, a.max())\n    return a.min(1)[0].squeeze()\n\n\ndef cat_kkt(Qi, Qv, Qsz, Gi, Gv, Gsz, Ai, Av, Asz, Di, Dv, Dsz, eps):\n    nBatch = Qv.size(0)\n\n    nineq, nz = Gsz\n    neq, _ = Asz\n\n    Di = Di + nz\n\n    Gi_L = Gi.clone()\n    Gi_L[0, :] += nz + nineq\n    Gv_L = Gv\n\n    Gi_U = torch.stack([Gi[1, :], Gi[0, :]])\n    Gi_U[1, :] += nz + nineq\n    Gv_U = Gv\n\n    Ai_L = Ai.clone()\n    Ai_L[0, :] += nz + 2 * nineq\n    Av_L = Av\n\n    Ai_U = torch.stack([Ai[1, :], Ai[0, :]])\n    Ai_U[1, :] += nz + 2 * nineq\n    Av_U = Av\n\n    Ii_L = type(Qi)([range(nineq), range(nineq)])\n    Ii_U = Ii_L.clone()\n    Ii_L[0, :] += nz + nineq\n    Ii_L[1, :] += nz\n    Ii_U[0, :] += nz\n    Ii_U[1, :] += nz + nineq\n    Iv_L = type(Qv)(nBatch, nineq).fill_(1.0)\n    Iv_U = Iv_L.clone()\n\n    Ii_11 = type(Qi)([range(nz + nineq), range(nz + nineq)])\n    Iv_11 = type(Qv)(nBatch, nz + nineq).fill_(eps)\n    Ii_22 = type(Qi)([range(nz + nineq, nz + 2 * nineq + neq),\n                      range(nz + nineq, nz + 2 * nineq + neq)])\n    Iv_22 = type(Qv)(nBatch, nineq + neq).fill_(-eps)\n\n    Ki = torch.cat((Qi, Di, Gi_L, Gi_U, Ai_L, Ai_U,\n                    Ii_L, Ii_U, Ii_11, Ii_22), 1)\n    Kv = torch.cat((Qv, Dv, Gv_L, Gv_U, Av_L, Av_U,\n                    Iv_L, Iv_U, Iv_11, Iv_22), 1)\n    k = nz + 2 * nineq + neq\n    Ksz = torch.Size([k, k])\n\n    I = torch.LongTensor(np.lexsort(\n        (Ki[1].cpu().numpy(), Ki[0].cpu().numpy()))).cuda()\n    Ki = Ki.t()[I].t().contiguous()\n    Kv = Kv.t()[I].t().contiguous()\n\n    Ks = [torch.cuda.sparse.DoubleTensor(\n        Ki, Kv[i], Ksz).coalesce() for i in range(nBatch)]\n    Ki = Ks[0]._indices()\n    Kv = torch.stack([Ks[i]._values() for i in range(nBatch)])\n\n    Didx = torch.nonzero(\n        (Ki[0] == Ki[1]).__and__(nz <= Ki[0]).__and__(Ki[0] < nz + nineq)).squeeze()\n\n    return Ks, [Ki, Kv, Ksz], Didx\n\n\ndef solve_kkt(Ks, K, Ktildes, Ktilde,\n              rx, rs, rz, ry, niter=1):\n    nBatch = len(Ks)\n    nz = rx.size(1)\n    nineq = rz.size(1)\n    neq = ry.size(1)\n\n    r = -torch.cat((rx, rs, rz, ry), 1)\n\n    l = torch.spbqrfactsolve(*([r] + Ktilde))\n    res = torch.stack([r[i] - torch.mm(Ks[i], l[i].unsqueeze(1))\n                       for i in range(nBatch)])\n    for k in range(niter):\n        d = torch.spbqrfactsolve(*([res] + Ktilde))\n        l = l + d\n        res = torch.stack([r[i] - torch.mm(Ks[i], l[i].unsqueeze(1))\n                           for i in range(nBatch)])\n\n    solx = l[:, :nz]\n    sols = l[:, nz:nz + nineq]\n    solz = l[:, nz + nineq:nz + 2 * nineq]\n    soly = l[:, nz + 2 * nineq:nz + 2 * nineq + neq]\n\n    return solx, sols, solz, soly\n'"
