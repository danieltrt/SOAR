file_path,api_count,code
__init__.py,0,b'\n\n'
run_ner_crf.py,34,"b'import glob\nimport logging\nimport os\nimport json\nimport time\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\nfrom torch.utils.data.distributed import DistributedSampler\nfrom callback.optimizater.adamw import AdamW\nfrom callback.lr_scheduler import get_linear_schedule_with_warmup\nfrom callback.progressbar import ProgressBar\nfrom tools.common import seed_everything,json_to_text\nfrom tools.common import init_logger, logger\n\nfrom models.transformers import WEIGHTS_NAME, BertConfig, AlbertConfig\nfrom models.bert_for_ner import BertCrfForNer\nfrom models.albert_for_ner import AlbertCrfForNer\nfrom processors.utils_ner import CNerTokenizer, get_entities\nfrom processors.ner_seq import convert_examples_to_features\nfrom processors.ner_seq import ner_processors as processors\nfrom processors.ner_seq import collate_fn\nfrom metrics.ner_metrics import SeqEntityScore\nfrom tools.finetuning_argparse import get_argparse\n\nMODEL_CLASSES = {\n    ## bert ernie bert_wwm bert_wwwm_ext\n    \'bert\': (BertConfig, BertCrfForNer, CNerTokenizer),\n    \'albert\': (AlbertConfig, AlbertCrfForNer, CNerTokenizer)\n}\n\ndef train(args, train_dataset, model, tokenizer):\n    """""" Train the model """"""\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size,\n                                  collate_fn=collate_fn)\n    if args.max_steps > 0:\n        t_total = args.max_steps\n        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n    else:\n        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n    # Prepare optimizer and schedule (linear warmup and decay)\n    no_decay = [""bias"", ""LayerNorm.weight""]\n    bert_param_optimizer = list(model.bert.named_parameters())\n    crf_param_optimizer = list(model.crf.named_parameters())\n    linear_param_optimizer = list(model.classifier.named_parameters())\n    optimizer_grouped_parameters = [\n        {\'params\': [p for n, p in bert_param_optimizer if not any(nd in n for nd in no_decay)],\n         \'weight_decay\': args.weight_decay, \'lr\': args.learning_rate},\n        {\'params\': [p for n, p in bert_param_optimizer if any(nd in n for nd in no_decay)], \'weight_decay\': 0.0,\n         \'lr\': args.learning_rate},\n\n        {\'params\': [p for n, p in crf_param_optimizer if not any(nd in n for nd in no_decay)],\n         \'weight_decay\': args.weight_decay, \'lr\': args.crf_learning_rate},\n        {\'params\': [p for n, p in crf_param_optimizer if any(nd in n for nd in no_decay)], \'weight_decay\': 0.0,\n         \'lr\': args.crf_learning_rate},\n\n        {\'params\': [p for n, p in linear_param_optimizer if not any(nd in n for nd in no_decay)],\n         \'weight_decay\': args.weight_decay, \'lr\': args.crf_learning_rate},\n        {\'params\': [p for n, p in linear_param_optimizer if any(nd in n for nd in no_decay)], \'weight_decay\': 0.0,\n         \'lr\': args.crf_learning_rate}\n    ]\n    args.warmup_steps = int(t_total * args.warmup_proportion)\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps,\n                                                num_training_steps=t_total)\n    # Check if saved optimizer or scheduler states exist\n    if os.path.isfile(os.path.join(args.model_name_or_path, ""optimizer.pt"")) and os.path.isfile(\n            os.path.join(args.model_name_or_path, ""scheduler.pt"")):\n        # Load in optimizer and scheduler states\n        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, ""optimizer.pt"")))\n        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, ""scheduler.pt"")))\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(""Please install apex from https://www.github.com/nvidia/apex to use fp16 training."")\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n    # multi-gpu training (should be after apex fp16 initialization)\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n    # Distributed training (should be after apex fp16 initialization)\n    if args.local_rank != -1:\n        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n                                                          output_device=args.local_rank,\n                                                          find_unused_parameters=True)\n    # Train!\n    logger.info(""***** Running training *****"")\n    logger.info(""  Num examples = %d"", len(train_dataset))\n    logger.info(""  Num Epochs = %d"", args.num_train_epochs)\n    logger.info(""  Instantaneous batch size per GPU = %d"", args.per_gpu_train_batch_size)\n    logger.info(""  Total train batch size (w. parallel, distributed & accumulation) = %d"",\n                args.train_batch_size\n                * args.gradient_accumulation_steps\n                * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n                )\n    logger.info(""  Gradient Accumulation steps = %d"", args.gradient_accumulation_steps)\n    logger.info(""  Total optimization steps = %d"", t_total)\n\n    global_step = 0\n    steps_trained_in_current_epoch = 0\n    # Check if continuing training from a checkpoint\n    if os.path.exists(args.model_name_or_path) and ""checkpoint"" in args.model_name_or_path:\n        # set global_step to gobal_step of last saved checkpoint from model path\n        global_step = int(args.model_name_or_path.split(""-"")[-1].split(""/"")[0])\n        epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n        steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n        logger.info(""  Continuing training from checkpoint, will skip to saved global_step"")\n        logger.info(""  Continuing training from epoch %d"", epochs_trained)\n        logger.info(""  Continuing training from global step %d"", global_step)\n        logger.info(""  Will skip the first %d steps in the first epoch"", steps_trained_in_current_epoch)\n\n    tr_loss, logging_loss = 0.0, 0.0\n    model.zero_grad()\n    seed_everything(args.seed)  # Added here for reproductibility (even between python 2 and 3)\n    for _ in range(int(args.num_train_epochs)):\n        pbar = ProgressBar(n_total=len(train_dataloader), desc=\'Training\')\n        for step, batch in enumerate(train_dataloader):\n            # Skip past any already trained steps if resuming training\n            if steps_trained_in_current_epoch > 0:\n                steps_trained_in_current_epoch -= 1\n                continue\n            model.train()\n            batch = tuple(t.to(args.device) for t in batch)\n            inputs = {""input_ids"": batch[0], ""attention_mask"": batch[1], ""labels"": batch[3], \'input_lens\': batch[4]}\n            if args.model_type != ""distilbert"":\n                # XLM and RoBERTa don""t use segment_ids\n                inputs[""token_type_ids""] = (batch[2] if args.model_type in [""bert"", ""xlnet""] else None)\n            outputs = model(**inputs)\n            loss = outputs[0]  # model outputs are always tuple in pytorch-transformers (see doc)\n            if args.n_gpu > 1:\n                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n            pbar(step, {\'loss\': loss.item()})\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                if args.fp16:\n                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n                else:\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n                scheduler.step()  # Update learning rate schedule\n                optimizer.step()\n                model.zero_grad()\n                global_step += 1\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n                    # Log metrics\n                    print("" "")\n                    if args.local_rank == -1:\n                        # Only evaluate when single GPU otherwise metrics may not average well\n                        evaluate(args, model, tokenizer)\n                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n                    # Save model checkpoint\n                    output_dir = os.path.join(args.output_dir, ""checkpoint-{}"".format(global_step))\n                    if not os.path.exists(output_dir):\n                        os.makedirs(output_dir)\n                    model_to_save = (\n                        model.module if hasattr(model, ""module"") else model\n                    )  # Take care of distributed/parallel training\n                    model_to_save.save_pretrained(output_dir)\n                    torch.save(args, os.path.join(output_dir, ""training_args.bin""))\n                    logger.info(""Saving model checkpoint to %s"", output_dir)\n                    tokenizer.save_vocabulary(output_dir)\n                    torch.save(optimizer.state_dict(), os.path.join(output_dir, ""optimizer.pt""))\n                    torch.save(scheduler.state_dict(), os.path.join(output_dir, ""scheduler.pt""))\n                    logger.info(""Saving optimizer and scheduler states to %s"", output_dir)\n        logger.info(""\\n"")\n        if \'cuda\' in str(args.device):\n            torch.cuda.empty_cache()\n    return global_step, tr_loss / global_step\n\n\ndef evaluate(args, model, tokenizer, prefix=""""):\n    metric = SeqEntityScore(args.id2label, markup=args.markup)\n    eval_output_dir = args.output_dir\n    if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n        os.makedirs(eval_output_dir)\n    eval_dataset = load_and_cache_examples(args, args.task_name, tokenizer, data_type=\'dev\')\n    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n    # Note that DistributedSampler samples randomly\n    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size,\n                                 collate_fn=collate_fn)\n    # Eval!\n    logger.info(""***** Running evaluation %s *****"", prefix)\n    logger.info(""  Num examples = %d"", len(eval_dataset))\n    logger.info(""  Batch size = %d"", args.eval_batch_size)\n    eval_loss = 0.0\n    nb_eval_steps = 0\n    pbar = ProgressBar(n_total=len(eval_dataloader), desc=""Evaluating"")\n    if isinstance(model, nn.DataParallel):\n        model = model.module\n    for step, batch in enumerate(eval_dataloader):\n        model.eval()\n        batch = tuple(t.to(args.device) for t in batch)\n        with torch.no_grad():\n            inputs = {""input_ids"": batch[0], ""attention_mask"": batch[1], ""labels"": batch[3], \'input_lens\': batch[4]}\n            if args.model_type != ""distilbert"":\n                # XLM and RoBERTa don""t use segment_ids\n                inputs[""token_type_ids""] = (batch[2] if args.model_type in [""bert"", ""xlnet""] else None)\n            outputs = model(**inputs)\n            tmp_eval_loss, logits = outputs[:2]\n            tags = model.crf.decode(logits, inputs[\'attention_mask\'])\n        if args.n_gpu > 1:\n            tmp_eval_loss = tmp_eval_loss.mean()  # mean() to average on multi-gpu parallel evaluating\n        eval_loss += tmp_eval_loss.item()\n        nb_eval_steps += 1\n        out_label_ids = inputs[\'labels\'].cpu().numpy().tolist()\n        input_lens = inputs[\'input_lens\'].cpu().numpy().tolist()\n        tags = tags.squeeze(0).cpu().numpy().tolist()\n        for i, label in enumerate(out_label_ids):\n            temp_1 = []\n            temp_2 = []\n            for j, m in enumerate(label):\n                if j == 0:\n                    continue\n                elif j == input_lens[i] - 1:\n                    metric.update(pred_paths=[temp_2], label_paths=[temp_1])\n                    break\n                else:\n                    temp_1.append(args.id2label[out_label_ids[i][j]])\n                    temp_2.append(args.id2label[tags[i][j]])\n        pbar(step)\n    logger.info(""\\n"")\n    eval_loss = eval_loss / nb_eval_steps\n    eval_info, entity_info = metric.result()\n    results = {f\'{key}\': value for key, value in eval_info.items()}\n    results[\'loss\'] = eval_loss\n    logger.info(""***** Eval results %s *****"", prefix)\n    info = ""-"".join([f\' {key}: {value:.4f} \' for key, value in results.items()])\n    logger.info(info)\n    logger.info(""***** Entity results %s *****"", prefix)\n    for key in sorted(entity_info.keys()):\n        logger.info(""******* %s results ********"" % key)\n        info = ""-"".join([f\' {key}: {value:.4f} \' for key, value in entity_info[key].items()])\n        logger.info(info)\n    return results\n\n\ndef predict(args, model, tokenizer, prefix=""""):\n    pred_output_dir = args.output_dir\n    if not os.path.exists(pred_output_dir) and args.local_rank in [-1, 0]:\n        os.makedirs(pred_output_dir)\n    test_dataset = load_and_cache_examples(args, args.task_name, tokenizer, data_type=\'test\')\n    # Note that DistributedSampler samples randomly\n    test_sampler = SequentialSampler(test_dataset) if args.local_rank == -1 else DistributedSampler(test_dataset)\n    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=1, collate_fn=collate_fn)\n    # Eval!\n    logger.info(""***** Running prediction %s *****"", prefix)\n    logger.info(""  Num examples = %d"", len(test_dataset))\n    logger.info(""  Batch size = %d"", 1)\n    results = []\n    output_predict_file = os.path.join(pred_output_dir, prefix, ""test_prediction.json"")\n    pbar = ProgressBar(n_total=len(test_dataloader), desc=""Predicting"")\n\n    if isinstance(model, nn.DataParallel):\n        model = model.module\n    for step, batch in enumerate(test_dataloader):\n        model.eval()\n        batch = tuple(t.to(args.device) for t in batch)\n        with torch.no_grad():\n            inputs = {""input_ids"": batch[0], ""attention_mask"": batch[1], ""labels"": None, \'input_lens\': batch[4]}\n            if args.model_type != ""distilbert"":\n                # XLM and RoBERTa don""t use segment_ids\n                inputs[""token_type_ids""] = (batch[2] if args.model_type in [""bert"", ""xlnet""] else None)\n            outputs = model(**inputs)\n            logits = outputs[0]\n            tags = model.crf.decode(logits, inputs[\'attention_mask\'])\n            tags  = tags.squeeze(0).cpu().numpy().tolist()\n        preds = tags[0][1:-1]  # [CLS]XXXX[SEP]\n        label_entities = get_entities(preds, args.id2label, args.markup)\n        json_d = {}\n        json_d[\'id\'] = step\n        json_d[\'tag_seq\'] = "" "".join([args.id2label[x] for x in preds])\n        json_d[\'entities\'] = label_entities\n        results.append(json_d)\n        pbar(step)\n    logger.info(""\\n"")\n    with open(output_predict_file, ""w"") as writer:\n        for record in results:\n            writer.write(json.dumps(record) + \'\\n\')\n    if args.task_name == \'cluener\':\n        output_submit_file = os.path.join(pred_output_dir, prefix, ""test_submit.json"")\n        test_text = []\n        with open(os.path.join(args.data_dir,""test.json""), \'r\') as fr:\n            for line in fr:\n                test_text.append(json.loads(line))\n        test_submit = []\n        for x, y in zip(test_text, results):\n            json_d = {}\n            json_d[\'id\'] = x[\'id\']\n            json_d[\'label\'] = {}\n            entities = y[\'entities\']\n            words = list(x[\'text\'])\n            if len(entities) != 0:\n                for subject in entities:\n                    tag = subject[0]\n                    start = subject[1]\n                    end = subject[2]\n                    word = """".join(words[start:end + 1])\n                    if tag in json_d[\'label\']:\n                        if word in json_d[\'label\'][tag]:\n                            json_d[\'label\'][tag][word].append([start, end])\n                        else:\n                            json_d[\'label\'][tag][word] = [[start, end]]\n                    else:\n                        json_d[\'label\'][tag] = {}\n                        json_d[\'label\'][tag][word] = [[start, end]]\n            test_submit.append(json_d)\n        json_to_text(output_submit_file,test_submit)\n\ndef load_and_cache_examples(args, task, tokenizer, data_type=\'train\'):\n    if args.local_rank not in [-1, 0] and not evaluate:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n    processor = processors[task]()\n    # Load data features from cache or dataset file\n    cached_features_file = os.path.join(args.data_dir, \'cached_crf-{}_{}_{}_{}\'.format(\n        data_type,\n        list(filter(None, args.model_name_or_path.split(\'/\'))).pop(),\n        str(args.train_max_seq_length if data_type == \'train\' else args.eval_max_seq_length),\n        str(task)))\n    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n        logger.info(""Loading features from cached file %s"", cached_features_file)\n        features = torch.load(cached_features_file)\n    else:\n        logger.info(""Creating features from dataset file at %s"", args.data_dir)\n        label_list = processor.get_labels()\n        if data_type == \'train\':\n            examples = processor.get_train_examples(args.data_dir)\n        elif data_type == \'dev\':\n            examples = processor.get_dev_examples(args.data_dir)\n        else:\n            examples = processor.get_test_examples(args.data_dir)\n        features = convert_examples_to_features(examples=examples,\n                                                tokenizer=tokenizer,\n                                                label_list=label_list,\n                                                max_seq_length=args.train_max_seq_length if data_type == \'train\' \\\n                                                    else args.eval_max_seq_length,\n                                                cls_token_at_end=bool(args.model_type in [""xlnet""]),\n                                                pad_on_left=bool(args.model_type in [\'xlnet\']),\n                                                cls_token=tokenizer.cls_token,\n                                                cls_token_segment_id=2 if args.model_type in [""xlnet""] else 0,\n                                                sep_token=tokenizer.sep_token,\n                                                # pad on the left for xlnet\n                                                pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n                                                pad_token_segment_id=4 if args.model_type in [\'xlnet\'] else 0,\n                                                )\n        if args.local_rank in [-1, 0]:\n            logger.info(""Saving features into cached file %s"", cached_features_file)\n            torch.save(features, cached_features_file)\n    if args.local_rank == 0 and not evaluate:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n    # Convert to Tensors and build dataset\n    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n    all_label_ids = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n    all_lens = torch.tensor([f.input_len for f in features], dtype=torch.long)\n    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_lens, all_label_ids)\n    return dataset\n\n\ndef main():\n    args = get_argparse().parse_args()\n\n    if not os.path.exists(args.output_dir):\n        os.mkdir(args.output_dir)\n    args.output_dir = args.output_dir + \'{}\'.format(args.model_type)\n    if not os.path.exists(args.output_dir):\n        os.mkdir(args.output_dir)\n    time_ = time.strftime(""%Y-%m-%d-%H:%M:%S"", time.localtime())\n    init_logger(log_file=args.output_dir + f\'/{args.model_type}-{args.task_name}-{time_}.log\')\n    if os.path.exists(args.output_dir) and os.listdir(\n            args.output_dir) and args.do_train and not args.overwrite_output_dir:\n        raise ValueError(\n            ""Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome."".format(\n                args.output_dir))\n    # Setup distant debugging if needed\n    if args.server_ip and args.server_port:\n        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n        import ptvsd\n        print(""Waiting for debugger attach"")\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n        ptvsd.wait_for_attach()\n    # Setup CUDA, GPU & distributed training\n    if args.local_rank == -1 or args.no_cuda:\n        device = torch.device(""cuda"" if torch.cuda.is_available() and not args.no_cuda else ""cpu"")\n        args.n_gpu = torch.cuda.device_count()\n    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n        torch.cuda.set_device(args.local_rank)\n        device = torch.device(""cuda"", args.local_rank)\n        torch.distributed.init_process_group(backend=""nccl"")\n        args.n_gpu = 1\n    args.device = device\n    logger.warning(\n        ""Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s"",\n        args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16, )\n    # Set seed\n    seed_everything(args.seed)\n    # Prepare NER task\n    args.task_name = args.task_name.lower()\n    if args.task_name not in processors:\n        raise ValueError(""Task not found: %s"" % (args.task_name))\n    processor = processors[args.task_name]()\n    label_list = processor.get_labels()\n    args.id2label = {i: label for i, label in enumerate(label_list)}\n    args.label2id = {label: i for i, label in enumerate(label_list)}\n    num_labels = len(label_list)\n\n    # Load pretrained model and tokenizer\n    if args.local_rank not in [-1, 0]:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n    args.model_type = args.model_type.lower()\n    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n    config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path,\n                                          num_labels=num_labels, cache_dir=args.cache_dir if args.cache_dir else None, )\n    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n                                                do_lower_case=args.do_lower_case,\n                                                cache_dir=args.cache_dir if args.cache_dir else None, )\n    model = model_class.from_pretrained(args.model_name_or_path, from_tf=bool("".ckpt"" in args.model_name_or_path),\n                                        config=config, cache_dir=args.cache_dir if args.cache_dir else None)\n    if args.local_rank == 0:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n\n    model.to(args.device)\n    logger.info(""Training/evaluation parameters %s"", args)\n    # Training\n    if args.do_train:\n        train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, data_type=\'train\')\n        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n        logger.info("" global_step = %s, average loss = %s"", global_step, tr_loss)\n    # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n        # Create output directory if needed\n        if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n            os.makedirs(args.output_dir)\n        logger.info(""Saving model checkpoint to %s"", args.output_dir)\n        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n        # They can then be reloaded using `from_pretrained()`\n        model_to_save = (\n            model.module if hasattr(model, ""module"") else model\n        )  # Take care of distributed/parallel training\n        model_to_save.save_pretrained(args.output_dir)\n        tokenizer.save_vocabulary(args.output_dir)\n        # Good practice: save your training arguments together with the trained model\n        torch.save(args, os.path.join(args.output_dir, ""training_args.bin""))\n    # Evaluation\n    results = {}\n    if args.do_eval and args.local_rank in [-1, 0]:\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n        checkpoints = [args.output_dir]\n        if args.eval_all_checkpoints:\n            checkpoints = list(\n                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + ""/**/"" + WEIGHTS_NAME, recursive=True))\n            )\n            logging.getLogger(""pytorch_transformers.modeling_utils"").setLevel(logging.WARN)  # Reduce logging\n        logger.info(""Evaluate the following checkpoints: %s"", checkpoints)\n        for checkpoint in checkpoints:\n            global_step = checkpoint.split(""-"")[-1] if len(checkpoints) > 1 else """"\n            prefix = checkpoint.split(\'/\')[-1] if checkpoint.find(\'checkpoint\') != -1 else """"\n            model = model_class.from_pretrained(checkpoint, config=config)\n            model.to(args.device)\n            result = evaluate(args, model, tokenizer, prefix=prefix)\n            if global_step:\n                result = {""{}_{}"".format(global_step, k): v for k, v in result.items()}\n            results.update(result)\n        output_eval_file = os.path.join(args.output_dir, ""eval_results.txt"")\n        with open(output_eval_file, ""w"") as writer:\n            for key in sorted(results.keys()):\n                writer.write(""{} = {}\\n"".format(key, str(results[key])))\n\n    if args.do_predict and args.local_rank in [-1, 0]:\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n        checkpoints = [args.output_dir]\n        if args.predict_checkpoints > 0:\n            checkpoints = list(\n                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \'/**/\' + WEIGHTS_NAME, recursive=True)))\n            logging.getLogger(""transformers.modeling_utils"").setLevel(logging.WARN)  # Reduce logging\n            checkpoints = [x for x in checkpoints if x.split(\'-\')[-1] == str(args.predict_checkpoints)]\n        logger.info(""Predict the following checkpoints: %s"", checkpoints)\n        for checkpoint in checkpoints:\n            prefix = checkpoint.split(\'/\')[-1] if checkpoint.find(\'checkpoint\') != -1 else """"\n            model = model_class.from_pretrained(checkpoint, config=config)\n            model.to(args.device)\n            predict(args, model, tokenizer, prefix=prefix)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
run_ner_softmax.py,34,"b'import argparse\nimport glob\nimport logging\nimport os\nimport json\nimport time\nimport numpy as np\nimport torch\nfrom torch.nn import CrossEntropyLoss\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\nfrom torch.utils.data.distributed import DistributedSampler\nfrom callback.optimizater.adamw import AdamW\nfrom callback.lr_scheduler import get_linear_schedule_with_warmup\nfrom callback.progressbar import ProgressBar\nfrom callback.adversarial import FGM\n\nfrom tools.common import seed_everything\nfrom tools.common import init_logger, logger\n\nfrom models.transformers import WEIGHTS_NAME,BertConfig,AlbertConfig\nfrom models.bert_for_ner import BertSoftmaxForNer\nfrom models.albert_for_ner import AlbertSoftmaxForNer\nfrom processors.utils_ner import CNerTokenizer,get_entities\nfrom processors.ner_seq import convert_examples_to_features\nfrom processors.ner_seq import ner_processors as processors\nfrom processors.ner_seq import collate_fn\nfrom metrics.ner_metrics import SeqEntityScore\nfrom tools.finetuning_argparse import get_argparse\n\nMODEL_CLASSES = {\n    ## bert ernie bert_wwm bert_wwwm_ext\n    \'bert\': (BertConfig, BertSoftmaxForNer, CNerTokenizer),\n    \'albert\': (AlbertConfig, AlbertSoftmaxForNer, CNerTokenizer),\n}\n\ndef train(args, train_dataset, model, tokenizer):\n    """""" Train the model """"""\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size,\n                                  collate_fn=collate_fn)\n    if args.max_steps > 0:\n        t_total = args.max_steps\n        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n    else:\n        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n\n    # Prepare optimizer and schedule (linear warmup and decay)\n    no_decay = [""bias"", ""LayerNorm.weight""]\n    optimizer_grouped_parameters = [\n        {""params"": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n        ""weight_decay"": args.weight_decay,},\n        {""params"": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], ""weight_decay"": 0.0},\n    ]\n    args.warmup_steps = int(t_total * args.warmup_proportion)\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps,\n                                                num_training_steps=t_total)\n    # Check if saved optimizer or scheduler states exist\n    if os.path.isfile(os.path.join(args.model_name_or_path, ""optimizer.pt"")) and os.path.isfile(\n        os.path.join(args.model_name_or_path, ""scheduler.pt"")):\n        # Load in optimizer and scheduler states\n        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, ""optimizer.pt"")))\n        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, ""scheduler.pt"")))\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(""Please install apex from https://www.github.com/nvidia/apex to use fp16 training."")\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n    # multi-gpu training (should be after apex fp16 initialization)\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n    # Distributed training (should be after apex fp16 initialization)\n    if args.local_rank != -1:\n        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n                                                          output_device=args.local_rank,\n                                                          find_unused_parameters=True)\n    # Train!\n    logger.info(""***** Running training *****"")\n    logger.info(""  Num examples = %d"", len(train_dataset))\n    logger.info(""  Num Epochs = %d"", args.num_train_epochs)\n    logger.info(""  Instantaneous batch size per GPU = %d"", args.per_gpu_train_batch_size)\n    logger.info(""  Total train batch size (w. parallel, distributed & accumulation) = %d"",\n        args.train_batch_size\n        * args.gradient_accumulation_steps\n        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n    )\n    logger.info(""  Gradient Accumulation steps = %d"", args.gradient_accumulation_steps)\n    logger.info(""  Total optimization steps = %d"", t_total)\n    global_step = 0\n    steps_trained_in_current_epoch = 0\n    # Check if continuing training from a checkpoint\n    if os.path.exists(args.model_name_or_path) and ""checkpoint"" in args.model_name_or_path:\n        # set global_step to gobal_step of last saved checkpoint from model path\n        global_step = int(args.model_name_or_path.split(""-"")[-1].split(""/"")[0])\n        epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n        steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n        logger.info(""  Continuing training from checkpoint, will skip to saved global_step"")\n        logger.info(""  Continuing training from epoch %d"", epochs_trained)\n        logger.info(""  Continuing training from global step %d"", global_step)\n        logger.info(""  Will skip the first %d steps in the first epoch"", steps_trained_in_current_epoch)\n\n    tr_loss, logging_loss = 0.0, 0.0\n    if args.do_adv:\n        fgm = FGM(model, emb_name=args.adv_name, epsilon=args.adv_epsilon)\n    model.zero_grad()\n    seed_everything(args.seed)  # Added here for reproductibility (even between python 2 and 3)\n    for _ in range(int(args.num_train_epochs)):\n        pbar = ProgressBar(n_total=len(train_dataloader), desc=\'Training\')\n        for step, batch in enumerate(train_dataloader):\n            # Skip past any already trained steps if resuming training\n            if steps_trained_in_current_epoch > 0:\n                steps_trained_in_current_epoch -= 1\n                continue\n            model.train()\n            batch = tuple(t.to(args.device) for t in batch)\n            inputs = {""input_ids"": batch[0], ""attention_mask"": batch[1], ""labels"": batch[3]}\n            if args.model_type != ""distilbert"":\n                # XLM and RoBERTa don""t use segment_ids\n                inputs[""token_type_ids""] = (batch[2] if args.model_type in [""bert"", ""xlnet""] else None)\n            outputs = model(**inputs)\n            loss = outputs[0]  # model outputs are always tuple in pytorch-transformers (see doc)\n            if args.n_gpu > 1:\n                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n            if args.do_adv:\n                fgm.attack()\n                loss_adv = model(**inputs)[0]\n                if args.n_gpu>1:\n                    loss_adv = loss_adv.mean()\n                loss_adv.backward()\n                fgm.restore()\n            pbar(step, {\'loss\': loss.item()})\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                if args.fp16:\n                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n                else:\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n                scheduler.step()  # Update learning rate schedule\n                optimizer.step()\n                model.zero_grad()\n                global_step += 1\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n                    # Log metrics\n                    print("" "")\n                    if args.local_rank == -1:\n                        # Only evaluate when single GPU otherwise metrics may not average well\n                        evaluate(args, model, tokenizer)\n\n                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n                    # Save model checkpoint\n                    output_dir = os.path.join(args.output_dir, ""checkpoint-{}"".format(global_step))\n                    if not os.path.exists(output_dir):\n                        os.makedirs(output_dir)\n                    # Take care of distributed/parallel training\n                    model_to_save = (model.module if hasattr(model, ""module"") else model)\n                    model_to_save.save_pretrained(output_dir)\n                    torch.save(args, os.path.join(output_dir, ""training_args.bin""))\n                    tokenizer.save_vocabulary(output_dir)\n                    logger.info(""Saving model checkpoint to %s"", output_dir)\n                    torch.save(optimizer.state_dict(), os.path.join(output_dir, ""optimizer.pt""))\n                    torch.save(scheduler.state_dict(), os.path.join(output_dir, ""scheduler.pt""))\n                    logger.info(""Saving optimizer and scheduler states to %s"", output_dir)\n        logger.info(""\\n"")\n        if \'cuda\' in str(args.device):\n            torch.cuda.empty_cache()\n    return global_step, tr_loss / global_step\n\ndef evaluate(args, model, tokenizer, prefix=""""):\n    metric = SeqEntityScore(args.id2label,markup=args.markup)\n    eval_output_dir = args.output_dir\n    if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n        os.makedirs(eval_output_dir)\n    eval_dataset = load_and_cache_examples(args, args.task_name,tokenizer, data_type=\'dev\')\n    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n    # Note that DistributedSampler samples randomly\n    eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size,\n                                 collate_fn=collate_fn)\n    # Eval!\n    logger.info(""***** Running evaluation %s *****"", prefix)\n    logger.info(""  Num examples = %d"", len(eval_dataset))\n    logger.info(""  Batch size = %d"", args.eval_batch_size)\n    eval_loss = 0.0\n    nb_eval_steps = 0\n    pbar = ProgressBar(n_total=len(eval_dataloader), desc=""Evaluating"")\n    for step, batch in enumerate(eval_dataloader):\n        model.eval()\n        batch = tuple(t.to(args.device) for t in batch)\n        with torch.no_grad():\n            inputs = {""input_ids"": batch[0], ""attention_mask"": batch[1], ""labels"": batch[3]}\n            if args.model_type != ""distilbert"":\n                # XLM and RoBERTa don""t use segment_ids\n                inputs[""token_type_ids""] = (batch[2] if args.model_type in [""bert"", ""xlnet""] else None)\n            outputs = model(**inputs)\n        tmp_eval_loss, logits = outputs[:2]\n        if args.n_gpu > 1:\n            tmp_eval_loss = tmp_eval_loss.mean()  # mean() to average on multi-gpu parallel evaluating\n        eval_loss += tmp_eval_loss.item()\n        nb_eval_steps += 1\n        preds = np.argmax(logits.cpu().numpy(), axis=2).tolist()\n        out_label_ids = inputs[\'labels\'].cpu().numpy().tolist()\n        input_lens = batch[4].cpu().numpy().tolist()\n        for i, label in enumerate(out_label_ids):\n            temp_1 = []\n            temp_2 = []\n            for j, m in enumerate(label):\n                if j == 0:\n                    continue\n                elif j == input_lens[i]-1:\n                    metric.update(pred_paths=[temp_2], label_paths=[temp_1])\n                    break\n                else:\n                    temp_1.append(args.id2label[out_label_ids[i][j]])\n                    temp_2.append(preds[i][j])\n        pbar(step)\n    logger.info(""\\n"")\n    eval_loss = eval_loss / nb_eval_steps\n    eval_info, entity_info = metric.result()\n    results = {f\'{key}\': value for key, value in eval_info.items()}\n    results[\'loss\'] = eval_loss\n    logger.info(""***** Eval results %s *****"", prefix)\n    info = ""-"".join([f\' {key}: {value:.4f} \' for key, value in results.items()])\n    logger.info(info)\n    logger.info(""***** Entity results %s *****"", prefix)\n    for key in sorted(entity_info.keys()):\n        logger.info(""******* %s results ********""%key)\n        info = ""-"".join([f\' {key}: {value:.4f} \' for key, value in entity_info[key].items()])\n        logger.info(info)\n    return results\n\ndef predict(args, model, tokenizer, prefix=""""):\n    pred_output_dir = args.output_dir\n    if not os.path.exists(pred_output_dir) and args.local_rank in [-1, 0]:\n        os.makedirs(pred_output_dir)\n\n    test_dataset = load_and_cache_examples(args, args.task_name,tokenizer, data_type=\'test\')\n    # Note that DistributedSampler samples randomly\n    test_sampler = SequentialSampler(test_dataset) if args.local_rank == -1 else DistributedSampler(test_dataset)\n    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=1,collate_fn=collate_fn)\n    # Eval!\n    logger.info(""***** Running prediction %s *****"", prefix)\n    logger.info(""  Num examples = %d"", len(test_dataset))\n    logger.info(""  Batch size = %d"", 1)\n\n    results = []\n    output_submit_file = os.path.join(pred_output_dir, prefix, ""test_prediction.json"")\n    pbar = ProgressBar(n_total=len(test_dataloader), desc=""Predicting"")\n    for step, batch in enumerate(test_dataloader):\n        model.eval()\n        batch = tuple(t.to(args.device) for t in batch)\n        with torch.no_grad():\n            inputs = {""input_ids"": batch[0], ""attention_mask"": batch[1], ""labels"": None}\n            if args.model_type != ""distilbert"":\n                # XLM and RoBERTa don""t use segment_ids\n                inputs[""token_type_ids""] = (batch[2] if args.model_type in [""bert"", ""xlnet""] else None)\n            outputs = model(**inputs)\n        logits = outputs[0]\n        preds = logits.detach().cpu().numpy()\n        preds = np.argmax(preds, axis=2).tolist()\n        preds = preds[0][1:-1] # [CLS]XXXX[SEP]\n        tags = [args.id2label[x] for x in preds]\n        label_entities = get_entities(preds, args.id2label, args.markup)\n        json_d = {}\n        json_d[\'id\'] = step\n        json_d[\'tag_seq\'] = "" "".join(tags)\n        json_d[\'entities\'] = label_entities\n        results.append(json_d)\n        pbar(step)\n    logger.info(""\\n"")\n    with open(output_submit_file, ""w"") as writer:\n        for record in results:\n            writer.write(json.dumps(record) + \'\\n\')\n\ndef load_and_cache_examples(args, task, tokenizer, data_type=\'train\'):\n    if args.local_rank not in [-1, 0] and not evaluate:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n    processor = processors[task]()\n    # Load data features from cache or dataset file\n    cached_features_file = os.path.join(args.data_dir, \'cached_soft-{}_{}_{}_{}\'.format(\n        data_type,\n        list(filter(None, args.model_name_or_path.split(\'/\'))).pop(),\n        str(args.train_max_seq_length if data_type==\'train\' else args.eval_max_seq_length),\n        str(task)))\n    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n        logger.info(""Loading features from cached file %s"", cached_features_file)\n        features = torch.load(cached_features_file)\n    else:\n        logger.info(""Creating features from dataset file at %s"", args.data_dir)\n        label_list = processor.get_labels()\n        if data_type == \'train\':\n            examples = processor.get_train_examples(args.data_dir)\n        elif data_type == \'dev\':\n            examples = processor.get_dev_examples(args.data_dir)\n        else:\n            examples = processor.get_test_examples(args.data_dir)\n        features = convert_examples_to_features(examples=examples,\n                                                tokenizer=tokenizer,\n                                                label_list=label_list,\n                                                max_seq_length=args.train_max_seq_length if data_type==\'train\' \\\n                                                               else args.eval_max_seq_length,\n                                                cls_token_at_end=bool(args.model_type in [""xlnet""]),\n                                                pad_on_left=bool(args.model_type in [\'xlnet\']),\n                                                cls_token = tokenizer.cls_token,\n                                                cls_token_segment_id=2 if args.model_type in [""xlnet""] else 0,\n                                                sep_token=tokenizer.sep_token,\n                                                # pad on the left for xlnet\n                                                pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n                                                pad_token_segment_id=4 if args.model_type in [\'xlnet\'] else 0,\n                                                )\n        if args.local_rank in [-1, 0]:\n            logger.info(""Saving features into cached file %s"", cached_features_file)\n            torch.save(features, cached_features_file)\n    if args.local_rank == 0 and not evaluate:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n    # Convert to Tensors and build dataset\n    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n    all_label_ids = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n    all_lens = torch.tensor([f.input_len for f in features], dtype=torch.long)\n    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_lens,all_label_ids)\n    return dataset\n\ndef main():\n    args = get_argparse().parse_args()\n    if not os.path.exists(args.output_dir):\n        os.mkdir(args.output_dir)\n    args.output_dir = args.output_dir + \'{}\'.format(args.model_type)\n    if not os.path.exists(args.output_dir):\n        os.mkdir(args.output_dir)\n    time_ = time.strftime(""%Y-%m-%d-%H:%M:%S"", time.localtime())\n    init_logger(log_file=args.output_dir + f\'/{args.model_type}-{args.task_name}-{time_}.log\')\n    if os.path.exists(args.output_dir) and os.listdir(\n            args.output_dir) and args.do_train and not args.overwrite_output_dir:\n        raise ValueError(\n            ""Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome."".format(\n                args.output_dir))\n    # Setup distant debugging if needed\n    if args.server_ip and args.server_port:\n        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n        import ptvsd\n        print(""Waiting for debugger attach"")\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n        ptvsd.wait_for_attach()\n    # Setup CUDA, GPU & distributed training\n    if args.local_rank == -1 or args.no_cuda:\n        device = torch.device(""cuda"" if torch.cuda.is_available() and not args.no_cuda else ""cpu"")\n        args.n_gpu = torch.cuda.device_count()\n    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n        torch.cuda.set_device(args.local_rank)\n        device = torch.device(""cuda"", args.local_rank)\n        torch.distributed.init_process_group(backend=""nccl"")\n        args.n_gpu = 1\n    args.device = device\n    logger.warning(\n                ""Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s"",\n                args.local_rank,device,args.n_gpu, bool(args.local_rank != -1),args.fp16,)\n    # Set seed\n    seed_everything(args.seed)\n    # Prepare NER task\n    args.task_name = args.task_name.lower()\n    if args.task_name not in processors:\n        raise ValueError(""Task not found: %s"" % (args.task_name))\n    processor = processors[args.task_name]()\n    label_list = processor.get_labels()\n    args.id2label = {i: label for i, label in enumerate(label_list)}\n    args.label2id = {label: i for i, label in enumerate(label_list)}\n    num_labels = len(label_list)\n\n    # Load pretrained model and tokenizer\n    if args.local_rank not in [-1, 0]:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n    args.model_type = args.model_type.lower()\n    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n    config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path,\n                                          num_labels=num_labels,\n                                          loss_type = args.loss_type,\n                                          cache_dir=args.cache_dir if args.cache_dir else None,)\n    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n                                                do_lower_case=args.do_lower_case,\n                                                cache_dir=args.cache_dir if args.cache_dir else None,)\n    model = model_class.from_pretrained(args.model_name_or_path,from_tf=bool("".ckpt"" in args.model_name_or_path),\n                                        config=config,cache_dir=args.cache_dir if args.cache_dir else None,)\n    if args.local_rank == 0:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n\n    model.to(args.device)\n    logger.info(""Training/evaluation parameters %s"", args)\n    # Training\n    if args.do_train:\n        train_dataset = load_and_cache_examples(args, args.task_name,tokenizer, data_type=\'train\')\n        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n        logger.info("" global_step = %s, average loss = %s"", global_step, tr_loss)\n    # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n        # Create output directory if needed\n        if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n            os.makedirs(args.output_dir)\n        logger.info(""Saving model checkpoint to %s"", args.output_dir)\n        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n        # They can then be reloaded using `from_pretrained()`\n        model_to_save = (\n            model.module if hasattr(model, ""module"") else model\n        )  # Take care of distributed/parallel training\n        model_to_save.save_pretrained(args.output_dir)\n        tokenizer.save_vocabulary(args.output_dir)\n        # Good practice: save your training arguments together with the trained model\n        torch.save(args, os.path.join(args.output_dir, ""training_args.bin""))\n    # Evaluation\n    results = {}\n    if args.do_eval and args.local_rank in [-1, 0]:\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n        checkpoints = [args.output_dir]\n        if args.eval_all_checkpoints:\n            checkpoints = list(\n                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + ""/**/"" + WEIGHTS_NAME, recursive=True))\n            )\n            logging.getLogger(""pytorch_transformers.modeling_utils"").setLevel(logging.WARN)  # Reduce logging\n        logger.info(""Evaluate the following checkpoints: %s"", checkpoints)\n        for checkpoint in checkpoints:\n            global_step = checkpoint.split(""-"")[-1] if len(checkpoints) > 1 else """"\n            prefix = checkpoint.split(\'/\')[-1] if checkpoint.find(\'checkpoint\') != -1 else """"\n            model = model_class.from_pretrained(checkpoint)\n            model.to(args.device)\n            result = evaluate(args, model, tokenizer, prefix=prefix)\n            if global_step:\n                result = {""{}_{}"".format(global_step, k): v for k, v in result.items()}\n            results.update(result)\n        output_eval_file = os.path.join(args.output_dir, ""eval_results.txt"")\n        with open(output_eval_file, ""w"") as writer:\n            for key in sorted(results.keys()):\n                writer.write(""{} = {}\\n"".format(key, str(results[key])))\n\n    if args.do_predict and args.local_rank in [-1, 0]:\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n        checkpoints = [args.output_dir]\n        if args.predict_checkpoints > 0:\n            checkpoints = list(\n                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \'/**/\' + WEIGHTS_NAME, recursive=True)))\n            logging.getLogger(""transformers.modeling_utils"").setLevel(logging.WARN)  # Reduce logging\n            checkpoints = [x for x in checkpoints if x.split(\'-\')[-1] == str(args.predict_checkpoints)]\n        logger.info(""Predict the following checkpoints: %s"", checkpoints)\n        for checkpoint in checkpoints:\n            prefix = checkpoint.split(\'/\')[-1] if checkpoint.find(\'checkpoint\') != -1 else """"\n            model = model_class.from_pretrained(checkpoint)\n            model.to(args.device)\n            predict(args, model, tokenizer,prefix=prefix)\n\nif __name__ == ""__main__"":\n    main()\n'"
run_ner_span.py,40,"b'import argparse\nimport glob\nimport logging\nimport os\nimport json\nimport time\nimport torch\nfrom torch.nn import CrossEntropyLoss\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\nfrom torch.utils.data.distributed import DistributedSampler\nfrom callback.optimizater.adamw import AdamW\nfrom callback.lr_scheduler import get_linear_schedule_with_warmup\nfrom callback.progressbar import ProgressBar\nfrom callback.adversarial import FGM\nfrom tools.common import seed_everything, json_to_text\nfrom tools.common import init_logger, logger\n\nfrom models.transformers import WEIGHTS_NAME, BertConfig, AlbertConfig\nfrom models.bert_for_ner import BertSpanForNer\nfrom models.albert_for_ner import AlbertSpanForNer\nfrom processors.utils_ner import CNerTokenizer\nfrom processors.ner_span import convert_examples_to_features\nfrom processors.ner_span import ner_processors as processors\nfrom processors.ner_span import collate_fn\nfrom metrics.ner_metrics import SpanEntityScore\nfrom processors.utils_ner import bert_extract_item\nfrom tools.finetuning_argparse import get_argparse\n\nMODEL_CLASSES = {\n    ## bert ernie bert_wwm bert_wwwm_ext\n    \'bert\': (BertConfig, BertSpanForNer, CNerTokenizer),\n    \'albert\': (AlbertConfig, AlbertSpanForNer, CNerTokenizer)\n}\n\n\ndef train(args, train_dataset, model, tokenizer):\n    """""" Train the model """"""\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size,\n                                  collate_fn=collate_fn)\n    if args.max_steps > 0:\n        t_total = args.max_steps\n        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n    else:\n        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n    # Prepare optimizer and schedule (linear warmup and decay)\n    no_decay = [""bias"", ""LayerNorm.weight""]\n    bert_parameters = model.bert.named_parameters()\n    start_parameters = model.start_fc.named_parameters()\n    end_parameters = model.end_fc.named_parameters()\n    optimizer_grouped_parameters = [\n        {""params"": [p for n, p in bert_parameters if not any(nd in n for nd in no_decay)],\n         ""weight_decay"": args.weight_decay, \'lr\': args.learning_rate},\n        {""params"": [p for n, p in bert_parameters if any(nd in n for nd in no_decay)], ""weight_decay"": 0.0\n            , \'lr\': args.learning_rate},\n\n        {""params"": [p for n, p in start_parameters if not any(nd in n for nd in no_decay)],\n         ""weight_decay"": args.weight_decay, \'lr\': 0.001},\n        {""params"": [p for n, p in start_parameters if any(nd in n for nd in no_decay)], ""weight_decay"": 0.0\n            , \'lr\': 0.001},\n\n        {""params"": [p for n, p in end_parameters if not any(nd in n for nd in no_decay)],\n         ""weight_decay"": args.weight_decay, \'lr\': 0.001},\n        {""params"": [p for n, p in end_parameters if any(nd in n for nd in no_decay)], ""weight_decay"": 0.0\n            , \'lr\': 0.001},\n    ]\n    # optimizer_grouped_parameters = [\n    #     {""params"": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n    #      ""weight_decay"": args.weight_decay, },\n    #     {""params"": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], ""weight_decay"": 0.0},\n    # ]\n    args.warmup_steps = int(t_total * args.warmup_proportion)\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps,\n                                                num_training_steps=t_total)\n    # Check if saved optimizer or scheduler states exist\n    if os.path.isfile(os.path.join(args.model_name_or_path, ""optimizer.pt"")) and os.path.isfile(\n            os.path.join(args.model_name_or_path, ""scheduler.pt"")):\n        # Load in optimizer and scheduler states\n        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, ""optimizer.pt"")))\n        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, ""scheduler.pt"")))\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(""Please install apex from https://www.github.com/nvidia/apex to use fp16 training."")\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n    # multi-gpu training (should be after apex fp16 initialization)\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n    # Distributed training (should be after apex fp16 initialization)\n    if args.local_rank != -1:\n        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n                                                          output_device=args.local_rank,\n                                                          find_unused_parameters=True)\n    # Train!\n    logger.info(""***** Running training *****"")\n    logger.info(""  Num examples = %d"", len(train_dataset))\n    logger.info(""  Num Epochs = %d"", args.num_train_epochs)\n    logger.info(""  Instantaneous batch size per GPU = %d"", args.per_gpu_train_batch_size)\n    logger.info(""  Total train batch size (w. parallel, distributed & accumulation) = %d"",\n                args.train_batch_size\n                * args.gradient_accumulation_steps\n                * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n                )\n    logger.info(""  Gradient Accumulation steps = %d"", args.gradient_accumulation_steps)\n    logger.info(""  Total optimization steps = %d"", t_total)\n\n    global_step = 0\n    steps_trained_in_current_epoch = 0\n    # Check if continuing training from a checkpoint\n    if os.path.exists(args.model_name_or_path) and ""checkpoint"" in args.model_name_or_path:\n        # set global_step to gobal_step of last saved checkpoint from model path\n        global_step = int(args.model_name_or_path.split(""-"")[-1].split(""/"")[0])\n        epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n        steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n        logger.info(""  Continuing training from checkpoint, will skip to saved global_step"")\n        logger.info(""  Continuing training from epoch %d"", epochs_trained)\n        logger.info(""  Continuing training from global step %d"", global_step)\n        logger.info(""  Will skip the first %d steps in the first epoch"", steps_trained_in_current_epoch)\n    tr_loss, logging_loss = 0.0, 0.0\n    if args.do_adv:\n        fgm = FGM(model, emb_name=args.adv_name, epsilon=args.adv_epsilon)\n    model.zero_grad()\n    seed_everything(args.seed)  # Added here for reproductibility (even between python 2 and 3)\n    for _ in range(int(args.num_train_epochs)):\n        pbar = ProgressBar(n_total=len(train_dataloader), desc=\'Training\')\n        for step, batch in enumerate(train_dataloader):\n            # Skip past any already trained steps if resuming training\n            if steps_trained_in_current_epoch > 0:\n                steps_trained_in_current_epoch -= 1\n                continue\n            model.train()\n            batch = tuple(t.to(args.device) for t in batch)\n            inputs = {""input_ids"": batch[0], ""attention_mask"": batch[1],\n                      ""start_positions"": batch[3], ""end_positions"": batch[4]}\n            if args.model_type != ""distilbert"":\n                # XLM and RoBERTa don""t use segment_ids\n                inputs[""token_type_ids""] = (batch[2] if args.model_type in [""bert"", ""xlnet""] else None)\n            outputs = model(**inputs)\n            loss = outputs[0]  # model outputs are always tuple in pytorch-transformers (see doc)\n            if args.n_gpu > 1:\n                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n            if args.do_adv:\n                fgm.attack()\n                loss_adv = model(**inputs)[0]\n                if args.n_gpu > 1:\n                    loss_adv = loss_adv.mean()\n                loss_adv.backward()\n                fgm.restore()\n            pbar(step, {\'loss\': loss.item()})\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                if args.fp16:\n                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n                else:\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n                scheduler.step()  # Update learning rate schedule\n                optimizer.step()\n                model.zero_grad()\n                global_step += 1\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n                    # Log metrics\n                    logger.info(""\\n"")\n                    if args.local_rank == -1:\n                        # Only evaluate when single GPU otherwise metrics may not average well\n                        evaluate(args, model, tokenizer)\n                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n                    # Save model checkpoint\n                    output_dir = os.path.join(args.output_dir, ""checkpoint-{}"".format(global_step))\n                    if not os.path.exists(output_dir):\n                        os.makedirs(output_dir)\n                    model_to_save = (\n                        model.module if hasattr(model, ""module"") else model\n                    )  # Take care of distributed/parallel training\n                    model_to_save.save_pretrained(output_dir)\n                    torch.save(args, os.path.join(output_dir, ""training_args.bin""))\n                    tokenizer.save_vocabulary(output_dir)\n                    logger.info(""Saving model checkpoint to %s"", output_dir)\n                    torch.save(optimizer.state_dict(), os.path.join(output_dir, ""optimizer.pt""))\n                    torch.save(scheduler.state_dict(), os.path.join(output_dir, ""scheduler.pt""))\n                    logger.info(""Saving optimizer and scheduler states to %s"", output_dir)\n        logger.info(""\\n"")\n        if \'cuda\' in str(args.device):\n            torch.cuda.empty_cache()\n    return global_step, tr_loss / global_step\n\n\ndef evaluate(args, model, tokenizer, prefix=""""):\n    metric = SpanEntityScore(args.id2label)\n    eval_output_dir = args.output_dir\n    if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n        os.makedirs(eval_output_dir)\n    eval_features = load_and_cache_examples(args, args.task_name, tokenizer, data_type=\'dev\')\n    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n    # Eval!\n    logger.info(""***** Running evaluation %s *****"", prefix)\n    logger.info(""  Num examples = %d"", len(eval_features))\n    logger.info(""  Batch size = %d"", args.eval_batch_size)\n    eval_loss = 0.0\n    nb_eval_steps = 0\n    pbar = ProgressBar(n_total=len(eval_features), desc=""Evaluating"")\n    for step, f in enumerate(eval_features):\n        input_lens = f.input_len\n        input_ids = torch.tensor([f.input_ids[:input_lens]], dtype=torch.long).to(args.device)\n        input_mask = torch.tensor([f.input_mask[:input_lens]], dtype=torch.long).to(args.device)\n        segment_ids = torch.tensor([f.segment_ids[:input_lens]], dtype=torch.long).to(args.device)\n        start_ids = torch.tensor([f.start_ids[:input_lens]], dtype=torch.long).to(args.device)\n        end_ids = torch.tensor([f.end_ids[:input_lens]], dtype=torch.long).to(args.device)\n        subjects = f.subjects\n        model.eval()\n        with torch.no_grad():\n            inputs = {""input_ids"": input_ids, ""attention_mask"": input_mask,\n                      ""start_positions"": start_ids, ""end_positions"": end_ids}\n            if args.model_type != ""distilbert"":\n                # XLM and RoBERTa don""t use segment_ids\n                inputs[""token_type_ids""] = (segment_ids if args.model_type in [""bert"", ""xlnet""] else None)\n            outputs = model(**inputs)\n        tmp_eval_loss, start_logits, end_logits = outputs[:3]\n        R = bert_extract_item(start_logits, end_logits)\n        T = subjects\n        metric.update(true_subject=T, pred_subject=R)\n        if args.n_gpu > 1:\n            tmp_eval_loss = tmp_eval_loss.mean()  # mean() to average on multi-gpu parallel evaluating\n        eval_loss += tmp_eval_loss.item()\n        nb_eval_steps += 1\n        pbar(step)\n    logger.info(""\\n"")\n    eval_loss = eval_loss / nb_eval_steps\n    eval_info, entity_info = metric.result()\n    results = {f\'{key}\': value for key, value in eval_info.items()}\n    results[\'loss\'] = eval_loss\n    logger.info(""***** Eval results %s *****"", prefix)\n    info = ""-"".join([f\' {key}: {value:.4f} \' for key, value in results.items()])\n    logger.info(info)\n    logger.info(""***** Entity results %s *****"", prefix)\n    for key in sorted(entity_info.keys()):\n        logger.info(""******* %s results ********"" % key)\n        info = ""-"".join([f\' {key}: {value:.4f} \' for key, value in entity_info[key].items()])\n        logger.info(info)\n    return results\n\n\ndef predict(args, model, tokenizer, prefix=""""):\n    pred_output_dir = args.output_dir\n    if not os.path.exists(pred_output_dir) and args.local_rank in [-1, 0]:\n        os.makedirs(pred_output_dir)\n    test_dataset = load_and_cache_examples(args, args.task_name, tokenizer, data_type=\'test\')\n    print(len(test_dataset))\n    # Note that DistributedSampler samples randomly\n    test_sampler = SequentialSampler(test_dataset) if args.local_rank == -1 else DistributedSampler(test_dataset)\n    test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=1, collate_fn=collate_fn)\n    # Eval!\n    logger.info(""***** Running prediction %s *****"", prefix)\n    logger.info(""  Num examples = %d"", len(test_dataset))\n    logger.info(""  Batch size = %d"", 1)\n\n    results = []\n    output_predict_file = os.path.join(pred_output_dir, prefix, ""test_predict.json"")\n    pbar = ProgressBar(n_total=len(test_dataloader), desc=""Predicting"")\n    for step, batch in enumerate(test_dataloader):\n        model.eval()\n        batch = tuple(t.to(args.device) for t in batch)\n        with torch.no_grad():\n            inputs = {""input_ids"": batch[0], ""attention_mask"": batch[1], ""start_positions"": None, ""end_positions"": None}\n            if args.model_type != ""distilbert"":\n                # XLM and RoBERTa don""t use segment_ids\n                inputs[""token_type_ids""] = (batch[2] if args.model_type in [""bert"", ""xlnet""] else None)\n            outputs = model(**inputs)\n        start_logits, end_logits = outputs[:2]\n        R = bert_extract_item(start_logits, end_logits)\n        if R:\n            label_entities = [[args.id2label[x[0]], x[1], x[2]] for x in R]\n        else:\n            label_entities = []\n        json_d = {}\n        json_d[\'id\'] = step\n        json_d[\'entities\'] = label_entities\n        results.append(json_d)\n        pbar(step)\n    logger.info(""\\n"")\n    with open(output_predict_file, ""w"") as writer:\n        for record in results:\n            writer.write(json.dumps(record) + \'\\n\')\n    if args.task_name == ""cluener"":\n        output_submit_file = os.path.join(pred_output_dir, prefix, ""test_submit.json"")\n        test_text = []\n        with open(os.path.join(args.data_dir, ""test.json""), \'r\') as fr:\n            for line in fr:\n                test_text.append(json.loads(line))\n        test_submit = []\n        for x, y in zip(test_text, results):\n            json_d = {}\n            json_d[\'id\'] = x[\'id\']\n            json_d[\'label\'] = {}\n            entities = y[\'entities\']\n            words = list(x[\'text\'])\n            if len(entities) != 0:\n                for subject in entities:\n                    tag = subject[0]\n                    start = subject[1]\n                    end = subject[2]\n                    word = """".join(words[start:end + 1])\n                    if tag in json_d[\'label\']:\n                        if word in json_d[\'label\'][tag]:\n                            json_d[\'label\'][tag][word].append([start, end])\n                        else:\n                            json_d[\'label\'][tag][word] = [[start, end]]\n                    else:\n                        json_d[\'label\'][tag] = {}\n                        json_d[\'label\'][tag][word] = [[start, end]]\n            test_submit.append(json_d)\n        json_to_text(output_submit_file, test_submit)\n\n\ndef load_and_cache_examples(args, task, tokenizer, data_type=\'train\'):\n    if args.local_rank not in [-1, 0] and not evaluate:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n    processor = processors[task]()\n    # Load data features from cache or dataset file\n    cached_features_file = os.path.join(args.data_dir, \'cached_span-{}_{}_{}_{}\'.format(\n        data_type,\n        list(filter(None, args.model_name_or_path.split(\'/\'))).pop(),\n        str(args.train_max_seq_length if data_type == \'train\' else args.eval_max_seq_length),\n        str(task)))\n    if os.path.exists(cached_features_file) and not args.overwrite_cache:\n        logger.info(""Loading features from cached file %s"", cached_features_file)\n        features = torch.load(cached_features_file)\n    else:\n        logger.info(""Creating features from dataset file at %s"", args.data_dir)\n        label_list = processor.get_labels()\n        if data_type == \'train\':\n            examples = processor.get_train_examples(args.data_dir)\n        elif data_type == \'dev\':\n            examples = processor.get_dev_examples(args.data_dir)\n        else:\n            examples = processor.get_test_examples(args.data_dir)\n        features = convert_examples_to_features(examples=examples,\n                                                tokenizer=tokenizer,\n                                                label_list=label_list,\n                                                max_seq_length=args.train_max_seq_length if data_type == \'train\' \\\n                                                    else args.eval_max_seq_length,\n                                                cls_token_at_end=bool(args.model_type in [""xlnet""]),\n                                                pad_on_left=bool(args.model_type in [\'xlnet\']),\n                                                cls_token=tokenizer.cls_token,\n                                                cls_token_segment_id=2 if args.model_type in [""xlnet""] else 0,\n                                                sep_token=tokenizer.sep_token,\n                                                # pad on the left for xlnet\n                                                pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n                                                pad_token_segment_id=4 if args.model_type in [\'xlnet\'] else 0,\n                                                )\n        if args.local_rank in [-1, 0]:\n            logger.info(""Saving features into cached file %s"", cached_features_file)\n            torch.save(features, cached_features_file)\n    if args.local_rank == 0 and not evaluate:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n    # Convert to Tensors and build dataset\n    if data_type == \'dev\':\n        return features\n    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n    all_start_ids = torch.tensor([f.start_ids for f in features], dtype=torch.long)\n    all_end_ids = torch.tensor([f.end_ids for f in features], dtype=torch.long)\n    all_input_lens = torch.tensor([f.input_len for f in features], dtype=torch.long)\n    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_start_ids, all_end_ids, all_input_lens)\n    return dataset\n\n\ndef main():\n    args = get_argparse().parse_args()\n    if not os.path.exists(args.output_dir):\n        os.mkdir(args.output_dir)\n    args.output_dir = args.output_dir + \'{}\'.format(args.model_type)\n    if not os.path.exists(args.output_dir):\n        os.mkdir(args.output_dir)\n    time_ = time.strftime(""%Y-%m-%d-%H:%M:%S"", time.localtime())\n    init_logger(log_file=args.output_dir + f\'/{args.model_type}-{args.task_name}-{time_}.log\')\n    if os.path.exists(args.output_dir) and os.listdir(\n            args.output_dir) and args.do_train and not args.overwrite_output_dir:\n        raise ValueError(\n            ""Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome."".format(\n                args.output_dir))\n    # Setup distant debugging if needed\n    if args.server_ip and args.server_port:\n        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n        import ptvsd\n        print(""Waiting for debugger attach"")\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n        ptvsd.wait_for_attach()\n    # Setup CUDA, GPU & distributed training\n    if args.local_rank == -1 or args.no_cuda:\n        device = torch.device(""cuda"" if torch.cuda.is_available() and not args.no_cuda else ""cpu"")\n        args.n_gpu = torch.cuda.device_count()\n    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n        torch.cuda.set_device(args.local_rank)\n        device = torch.device(""cuda"", args.local_rank)\n        torch.distributed.init_process_group(backend=""nccl"")\n        args.n_gpu = 1\n    args.device = device\n    logger.warning(""Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s"",\n                   args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16, )\n    # Set seed\n    seed_everything(args.seed)\n    # Prepare NER task\n    args.task_name = args.task_name.lower()\n    if args.task_name not in processors:\n        raise ValueError(""Task not found: %s"" % (args.task_name))\n    processor = processors[args.task_name]()\n    label_list = processor.get_labels()\n    args.id2label = {i: label for i, label in enumerate(label_list)}\n    args.label2id = {label: i for i, label in enumerate(label_list)}\n    num_labels = len(label_list)\n\n    # Load pretrained model and tokenizer\n    if args.local_rank not in [-1, 0]:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n    args.model_type = args.model_type.lower()\n    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n    config = config_class.from_pretrained(args.config_name if args.config_name else args.model_name_or_path,\n                                          num_labels=num_labels,\n                                          loss_type=args.loss_type,\n                                          cache_dir=args.cache_dir if args.cache_dir else None,\n                                          soft_label=True)\n    tokenizer = tokenizer_class.from_pretrained(args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n                                                do_lower_case=args.do_lower_case,\n                                                cache_dir=args.cache_dir if args.cache_dir else None, )\n    model = model_class.from_pretrained(args.model_name_or_path,\n                                        from_tf=bool("".ckpt"" in args.model_name_or_path),\n                                        config=config)\n    if args.local_rank == 0:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n\n    model.to(args.device)\n    logger.info(""Training/evaluation parameters %s"", args)\n    # Training\n    if args.do_train:\n        train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, data_type=\'train\')\n        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n        logger.info("" global_step = %s, average loss = %s"", global_step, tr_loss)\n    # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n        # Create output directory if needed\n        if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n            os.makedirs(args.output_dir)\n        logger.info(""Saving model checkpoint to %s"", args.output_dir)\n        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n        # They can then be reloaded using `from_pretrained()`\n        model_to_save = (\n            model.module if hasattr(model, ""module"") else model\n        )  # Take care of distributed/parallel training\n        model_to_save.save_pretrained(args.output_dir)\n        tokenizer.save_vocabulary(args.output_dir)\n        # Good practice: save your training arguments together with the trained model\n        torch.save(args, os.path.join(args.output_dir, ""training_args.bin""))\n    # Evaluation\n    results = {}\n    if args.do_eval and args.local_rank in [-1, 0]:\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n        checkpoints = [args.output_dir]\n        if args.eval_all_checkpoints:\n            checkpoints = list(\n                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + ""/**/"" + WEIGHTS_NAME, recursive=True))\n            )\n            logging.getLogger(""pytorch_transformers.modeling_utils"").setLevel(logging.WARN)  # Reduce logging\n        logger.info(""Evaluate the following checkpoints: %s"", checkpoints)\n        for checkpoint in checkpoints:\n            global_step = checkpoint.split(""-"")[-1] if len(checkpoints) > 1 else """"\n            prefix = checkpoint.split(\'/\')[-1] if checkpoint.find(\'checkpoint\') != -1 else """"\n            model = model_class.from_pretrained(checkpoint)\n            model.to(args.device)\n            result = evaluate(args, model, tokenizer, prefix=prefix)\n            if global_step:\n                result = {""{}_{}"".format(global_step, k): v for k, v in result.items()}\n            results.update(result)\n        output_eval_file = os.path.join(args.output_dir, ""eval_results.txt"")\n        with open(output_eval_file, ""w"") as writer:\n            for key in sorted(results.keys()):\n                writer.write(""{} = {}\\n"".format(key, str(results[key])))\n\n    if args.do_predict and args.local_rank in [-1, 0]:\n        tokenizer = tokenizer_class.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n        checkpoints = [args.output_dir]\n        if args.predict_checkpoints > 0:\n            checkpoints = list(\n                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \'/**/\' + WEIGHTS_NAME, recursive=True)))\n            logging.getLogger(""transformers.modeling_utils"").setLevel(logging.WARN)  # Reduce logging\n            checkpoints = [x for x in checkpoints if x.split(\'-\')[-1] == str(args.predict_checkpoints)]\n        logger.info(""Predict the following checkpoints: %s"", checkpoints)\n        for checkpoint in checkpoints:\n            prefix = checkpoint.split(\'/\')[-1] if checkpoint.find(\'checkpoint\') != -1 else """"\n            model = model_class.from_pretrained(checkpoint)\n            model.to(args.device)\n            predict(args, model, tokenizer, prefix=prefix)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
callback/__init__.py,0,b''
callback/adversarial.py,5,"b""import torch\r\n\r\nclass FGM():\r\n    '''\r\n    Example\r\n    # \xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\r\n    fgm = FGM(model,epsilon=1,emb_name='word_embeddings.')\r\n    for batch_input, batch_label in data:\r\n        # \xe6\xad\xa3\xe5\xb8\xb8\xe8\xae\xad\xe7\xbb\x83\r\n        loss = model(batch_input, batch_label)\r\n        loss.backward() # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0\xe6\xad\xa3\xe5\xb8\xb8\xe7\x9a\x84grad\r\n        # \xe5\xaf\xb9\xe6\x8a\x97\xe8\xae\xad\xe7\xbb\x83\r\n        fgm.attack() # \xe5\x9c\xa8embedding\xe4\xb8\x8a\xe6\xb7\xbb\xe5\x8a\xa0\xe5\xaf\xb9\xe6\x8a\x97\xe6\x89\xb0\xe5\x8a\xa8\r\n        loss_adv = model(batch_input, batch_label)\r\n        loss_adv.backward() # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x8c\xe5\xb9\xb6\xe5\x9c\xa8\xe6\xad\xa3\xe5\xb8\xb8\xe7\x9a\x84grad\xe5\x9f\xba\xe7\xa1\x80\xe4\xb8\x8a\xef\xbc\x8c\xe7\xb4\xaf\xe5\x8a\xa0\xe5\xaf\xb9\xe6\x8a\x97\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\r\n        fgm.restore() # \xe6\x81\xa2\xe5\xa4\x8dembedding\xe5\x8f\x82\xe6\x95\xb0\r\n        # \xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xef\xbc\x8c\xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\r\n        optimizer.step()\r\n        model.zero_grad()\r\n    '''\r\n    def __init__(self, model,emb_name,epsilon=1.0):\r\n        # emb_name\xe8\xbf\x99\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xe8\xa6\x81\xe6\x8d\xa2\xe6\x88\x90\xe4\xbd\xa0\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xb8\xadembedding\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe5\x90\x8d\r\n        self.model = model\r\n        self.epsilon = epsilon\r\n        self.emb_name = emb_name\r\n        self.backup = {}\r\n\r\n    def attack(self):\r\n        for name, param in self.model.named_parameters():\r\n            if param.requires_grad and self.emb_name in name:\r\n                self.backup[name] = param.data.clone()\r\n                norm = torch.norm(param.grad)\r\n                if norm!=0 and not torch.isnan(norm):\r\n                    r_at = self.epsilon * param.grad / norm\r\n                    param.data.add_(r_at)\r\n\r\n    def restore(self):\r\n        for name, param in self.model.named_parameters():\r\n            if param.requires_grad and self.emb_name in name:\r\n                assert name in self.backup\r\n                param.data = self.backup[name]\r\n        self.backup = {}\r\n\r\nclass PGD():\r\n    '''\r\n    Example\r\n    pgd = PGD(model,emb_name='word_embeddings.',epsilon=1.0,alpha=0.3)\r\n    K = 3\r\n    for batch_input, batch_label in data:\r\n        # \xe6\xad\xa3\xe5\xb8\xb8\xe8\xae\xad\xe7\xbb\x83\r\n        loss = model(batch_input, batch_label)\r\n        loss.backward() # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0\xe6\xad\xa3\xe5\xb8\xb8\xe7\x9a\x84grad\r\n        pgd.backup_grad()\r\n        # \xe5\xaf\xb9\xe6\x8a\x97\xe8\xae\xad\xe7\xbb\x83\r\n        for t in range(K):\r\n            pgd.attack(is_first_attack=(t==0)) # \xe5\x9c\xa8embedding\xe4\xb8\x8a\xe6\xb7\xbb\xe5\x8a\xa0\xe5\xaf\xb9\xe6\x8a\x97\xe6\x89\xb0\xe5\x8a\xa8, first attack\xe6\x97\xb6\xe5\xa4\x87\xe4\xbb\xbdparam.data\r\n            if t != K-1:\r\n                model.zero_grad()\r\n            else:\r\n                pgd.restore_grad()\r\n            loss_adv = model(batch_input, batch_label)\r\n            loss_adv.backward() # \xe5\x8f\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x8c\xe5\xb9\xb6\xe5\x9c\xa8\xe6\xad\xa3\xe5\xb8\xb8\xe7\x9a\x84grad\xe5\x9f\xba\xe7\xa1\x80\xe4\xb8\x8a\xef\xbc\x8c\xe7\xb4\xaf\xe5\x8a\xa0\xe5\xaf\xb9\xe6\x8a\x97\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\xa2\xaf\xe5\xba\xa6\r\n        pgd.restore() # \xe6\x81\xa2\xe5\xa4\x8dembedding\xe5\x8f\x82\xe6\x95\xb0\r\n        # \xe6\xa2\xaf\xe5\xba\xa6\xe4\xb8\x8b\xe9\x99\x8d\xef\xbc\x8c\xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\r\n        optimizer.step()\r\n        model.zero_grad()\r\n    '''\r\n    def __init__(self, model,emb_name,epsilon=1.,alpha=0.3):\r\n        # emb_name\xe8\xbf\x99\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xe8\xa6\x81\xe6\x8d\xa2\xe6\x88\x90\xe4\xbd\xa0\xe6\xa8\xa1\xe5\x9e\x8b\xe4\xb8\xadembedding\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe5\x90\x8d\r\n        self.model = model\r\n        self.emb_name = emb_name\r\n        self.epsilon = epsilon\r\n        self.alpha = alpha\r\n        self.emb_backup = {}\r\n        self.grad_backup = {}\r\n\r\n    def attack(self,is_first_attack=False):\r\n        for name, param in self.model.named_parameters():\r\n            if param.requires_grad and self.emb_name in name:\r\n                if is_first_attack:\r\n                    self.emb_backup[name] = param.data.clone()\r\n                norm = torch.norm(param.grad)\r\n                if norm != 0:\r\n                    r_at = self.alpha * param.grad / norm\r\n                    param.data.add_(r_at)\r\n                    param.data = self.project(name, param.data, self.epsilon)\r\n\r\n    def restore(self):\r\n        for name, param in self.model.named_parameters():\r\n            if param.requires_grad and self.emb_name in name:\r\n                assert name in self.emb_backup\r\n                param.data = self.emb_backup[name]\r\n        self.emb_backup = {}\r\n\r\n    def project(self, param_name, param_data, epsilon):\r\n        r = param_data - self.emb_backup[param_name]\r\n        if torch.norm(r) > epsilon:\r\n            r = epsilon * r / torch.norm(r)\r\n        return self.emb_backup[param_name] + r\r\n\r\n    def backup_grad(self):\r\n        for name, param in self.model.named_parameters():\r\n            if param.requires_grad:\r\n                self.grad_backup[name] = param.grad.clone()\r\n\r\n    def restore_grad(self):\r\n        for name, param in self.model.named_parameters():\r\n            if param.requires_grad:\r\n                param.grad = self.grad_backup[name]"""
callback/lr_finder.py,15,"b'import copy\nimport os\nimport torch\nfrom torch.optim.lr_scheduler import _LRScheduler\nimport matplotlib.pyplot as plt\n\ntry:\n    from apex import amp\n\n    IS_AMP_AVAILABLE = True\nexcept ImportError:\n    import logging\n    logging.basicConfig()\n    logger = logging.getLogger(__name__)\n    logger.warning(\n        ""To enable mixed precision training, please install `apex`. ""\n        ""Or you can re-install this package by the following command:\\n""\n        \'  pip install torch-lr-finder -v --global-option=""amp""\'\n    )\n    IS_AMP_AVAILABLE = False\n    del logging\n\n\nclass LRFinder(object):\n    """"""Learning rate range test.\n\n    The learning rate range test increases the learning rate in a pre-training run\n    between two boundaries in a linear or exponential manner. It provides valuable\n    information on how well the network can be trained over a range of learning rates\n    and what is the optimal learning rate.\n    Arguments:\n        model (torch.nn.Module): wrapped model.\n        optimizer (torch.optim.Optimizer): wrapped optimizer where the defined learning\n            is assumed to be the lower boundary of the range test.\n        criterion (torch.nn.Module): wrapped loss function.\n        device (str or torch.device, optional): a string (""cpu"" or ""cuda"") with an\n            optional ordinal for the device type (e.g. ""cuda:X"", where is the ordinal).\n            Alternatively, can be an object representing the device on which the\n            computation will take place. Default: None, uses the same device as `model`.\n        memory_cache (boolean, optional): if this flag is set to True, `state_dict` of\n            model and optimizer will be cached in memory. Otherwise, they will be saved\n            to files under the `cache_dir`.\n        cache_dir (string, optional): path for storing temporary files. If no path is\n            specified, system-wide temporary directory is used. Notice that this\n            parameter will be ignored if `memory_cache` is True.\n\n    Example:\n        >>> lr_finder = LRFinder(net, optimizer, criterion, device=""cuda"")\n        >>> lr_finder.range_test(dataloader, end_lr=100, num_iter=100)\n        >>> lr_finder.plot() # to inspect the loss-learning rate graph\n        >>> lr_finder.reset() # to reset the model and optimizer to their initial state\n\n    Reference:\n    Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186\n    fastai/lr_find: https://github.com/fastai/fastai\n    """"""\n\n    def __init__(\n        self,\n        model,\n        optimizer,\n        criterion,\n        device=None,\n        memory_cache=True,\n        cache_dir=None,\n    ):\n        # Check if the optimizer is already attached to a scheduler\n        self.optimizer = optimizer\n        self._check_for_scheduler()\n\n        self.model = model\n        self.criterion = criterion\n        self.history = {""lr"": [], ""loss"": []}\n        self.best_loss = None\n        self.memory_cache = memory_cache\n        self.cache_dir = cache_dir\n\n        # Save the original state of the model and optimizer so they can be restored if\n        # needed\n        self.model_device = next(self.model.parameters()).device\n        self.state_cacher = StateCacher(memory_cache, cache_dir=cache_dir)\n        self.state_cacher.store(""model"", self.model.state_dict())\n        self.state_cacher.store(""optimizer"", self.optimizer.state_dict())\n\n        # If device is None, use the same as the model\n        if device:\n            self.device = device\n        else:\n            self.device = self.model_device\n\n    def reset(self):\n        """"""Restores the model and optimizer to their initial states.""""""\n\n        self.model.load_state_dict(self.state_cacher.retrieve(""model""))\n        self.optimizer.load_state_dict(self.state_cacher.retrieve(""optimizer""))\n        self.model.to(self.model_device)\n\n    def range_test(\n        self,\n        train_loader,\n        val_loader=None,\n        start_lr=None,\n        end_lr=10,\n        num_iter=100,\n        step_mode=""exp"",\n        smooth_f=0.05,\n        diverge_th=5,\n        accumulation_steps=1,\n    ):\n        """"""Performs the learning rate range test.\n\n        Arguments:\n            train_loader (torch.utils.data.DataLoader): the training set data laoder.\n            val_loader (torch.utils.data.DataLoader, optional): if `None` the range test\n                will only use the training loss. When given a data loader, the model is\n                evaluated after each iteration on that dataset and the evaluation loss\n                is used. Note that in this mode the test takes significantly longer but\n                generally produces more precise results. Default: None.\n            start_lr (float, optional): the starting learning rate for the range test.\n                Default: None (uses the learning rate from the optimizer).\n            end_lr (float, optional): the maximum learning rate to test. Default: 10.\n            num_iter (int, optional): the number of iterations over which the test\n                occurs. Default: 100.\n            step_mode (str, optional): one of the available learning rate policies,\n                linear or exponential (""linear"", ""exp""). Default: ""exp"".\n            smooth_f (float, optional): the loss smoothing factor within the [0, 1[\n                interval. Disabled if set to 0, otherwise the loss is smoothed using\n                exponential smoothing. Default: 0.05.\n            diverge_th (int, optional): the test is stopped when the loss surpasses the\n                threshold:  diverge_th * best_loss. Default: 5.\n            accumulation_steps (int, optional): steps for gradient accumulation. If it\n                is 1, gradients are not accumulated. Default: 1.\n\n        Example (fastai approach):\n            >>> lr_finder = LRFinder(net, optimizer, criterion, device=""cuda"")\n            >>> lr_finder.range_test(dataloader, end_lr=100, num_iter=100)\n\n        Example (Leslie Smith\'s approach):\n            >>> lr_finder = LRFinder(net, optimizer, criterion, device=""cuda"")\n            >>> lr_finder.range_test(trainloader, val_loader=val_loader, end_lr=1, num_iter=100, step_mode=""linear"")\n\n        Gradient accumulation is supported; example:\n            >>> train_data = ...    # prepared dataset\n            >>> desired_bs, real_bs = 32, 4         # batch size\n            >>> accumulation_steps = desired_bs // real_bs     # required steps for accumulation\n            >>> dataloader = torch.utils.data.DataLoader(train_data, batch_size=real_bs, shuffle=True)\n            >>> acc_lr_finder = LRFinder(net, optimizer, criterion, device=""cuda"")\n            >>> acc_lr_finder.range_test(dataloader, end_lr=10, num_iter=100, accumulation_steps=accumulation_steps)\n\n        Reference:\n        [Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU & Distributed setups](\n        https://medium.com/huggingface/ec88c3e51255)\n        [thomwolf/gradient_accumulation](https://gist.github.com/thomwolf/ac7a7da6b1888c2eeac8ac8b9b05d3d3)\n        """"""\n\n        # Reset test results\n        self.history = {""lr"": [], ""loss"": []}\n        self.best_loss = None\n\n        # Move the model to the proper device\n        self.model.to(self.device)\n\n        # Check if the optimizer is already attached to a scheduler\n        self._check_for_scheduler()\n\n        # Set the starting learning rate\n        if start_lr:\n            self._set_learning_rate(start_lr)\n\n        # Initialize the proper learning rate policy\n        if step_mode.lower() == ""exp"":\n            lr_schedule = ExponentialLR(self.optimizer, end_lr, num_iter)\n        elif step_mode.lower() == ""linear"":\n            lr_schedule = LinearLR(self.optimizer, end_lr, num_iter)\n        else:\n            raise ValueError(""expected one of (exp, linear), got {}"".format(step_mode))\n\n        if smooth_f < 0 or smooth_f >= 1:\n            raise ValueError(""smooth_f is outside the range [0, 1["")\n\n        # Create an iterator to get data batch by batch\n        iter_wrapper = DataLoaderIterWrapper(train_loader)\n        for iteration in tqdm(range(num_iter)):\n            # Train on batch and retrieve loss\n            loss = self._train_batch(iter_wrapper, accumulation_steps)\n            if val_loader:\n                loss = self._validate(val_loader)\n\n            # Update the learning rate\n            lr_schedule.step()\n            self.history[""lr""].append(lr_schedule.get_lr()[0])\n\n            # Track the best loss and smooth it if smooth_f is specified\n            if iteration == 0:\n                self.best_loss = loss\n            else:\n                if smooth_f > 0:\n                    loss = smooth_f * loss + (1 - smooth_f) * self.history[""loss""][-1]\n                if loss < self.best_loss:\n                    self.best_loss = loss\n\n            # Check if the loss has diverged; if it has, stop the test\n            self.history[""loss""].append(loss)\n            if loss > diverge_th * self.best_loss:\n                print(""Stopping early, the loss has diverged"")\n                break\n\n        print(""Learning rate search finished. See the graph with {finder_name}.plot()"")\n\n    def _set_learning_rate(self, new_lrs):\n        if not isinstance(new_lrs, list):\n            new_lrs = [new_lrs] * len(self.optimizer.param_groups)\n        if len(new_lrs) != len(self.optimizer.param_groups):\n            raise ValueError(\n                ""Length of `new_lrs` is not equal to the number of parameter groups ""\n                + ""in the given optimizer""\n            )\n\n        for param_group, new_lr in zip(self.optimizer.param_groups, new_lrs):\n            param_group[""lr""] = new_lr\n\n    def _check_for_scheduler(self):\n        for param_group in self.optimizer.param_groups:\n            if ""initial_lr"" in param_group:\n                raise RuntimeError(""Optimizer already has a scheduler attached to it"")\n\n    def _train_batch(self, iter_wrapper, accumulation_steps):\n        self.model.train()\n        total_loss = None  # for late initialization\n\n        self.optimizer.zero_grad()\n        for i in range(accumulation_steps):\n            inputs, labels = next(iter_wrapper)\n            inputs, labels = self._move_to_device(inputs, labels)\n\n            # Forward pass\n            outputs = self.model(inputs)\n            loss = self.criterion(outputs, labels)\n\n            # Loss should be averaged in each step\n            loss /= accumulation_steps\n\n            # Backward pass\n            if IS_AMP_AVAILABLE and hasattr(self.optimizer, ""_amp_stash""):\n                # For minor performance optimization, see also:\n                # https://nvidia.github.io/apex/advanced.html#gradient-accumulation-across-iterations\n                delay_unscale = ((i + 1) % accumulation_steps) != 0\n\n                with amp.scale_loss(\n                    loss, self.optimizer, delay_unscale=delay_unscale\n                ) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n\n            if total_loss is None:\n                total_loss = loss\n            else:\n                total_loss += loss\n\n        self.optimizer.step()\n\n        return total_loss.item()\n\n    def _move_to_device(self, inputs, labels):\n        def move(obj, device):\n            if isinstance(obj, tuple):\n                return tuple(move(o, device) for o in obj)\n            elif torch.is_tensor(obj):\n                return obj.to(device)\n            elif isinstance(obj, list):\n                return [move(o, device) for o in obj]\n            else:\n                return obj\n\n        inputs = move(inputs, self.device)\n        labels = move(labels, self.device)\n        return inputs, labels\n\n    def _validate(self, dataloader):\n        # Set model to evaluation mode and disable gradient computation\n        running_loss = 0\n        self.model.eval()\n        with torch.no_grad():\n            for inputs, labels, *_ in dataloader:\n                # Move data to the correct device\n                inputs, labels = self._move_to_device(inputs, labels)\n\n                if isinstance(inputs, tuple) or isinstance(inputs, list):\n                    batch_size = inputs[0].size(0)\n                else:\n                    batch_size = inputs.size(0)\n\n                # Forward pass and loss computation\n                outputs = self.model(inputs)\n                loss = self.criterion(outputs, labels)\n                running_loss += loss.item() * batch_size\n\n        return running_loss / len(dataloader.dataset)\n\n    def plot(self, skip_start=10, skip_end=5, log_lr=True, show_lr=None, ax=None):\n        """"""Plots the learning rate range test.\n\n        Arguments:\n            skip_start (int, optional): number of batches to trim from the start.\n                Default: 10.\n            skip_end (int, optional): number of batches to trim from the start.\n                Default: 5.\n            log_lr (bool, optional): True to plot the learning rate in a logarithmic\n                scale; otherwise, plotted in a linear scale. Default: True.\n            show_lr (float, optional): if set, adds a vertical line to visualize the\n                specified learning rate. Default: None.\n            ax (matplotlib.axes.Axes, optional): the plot is created in the specified\n                matplotlib axes object and the figure is not be shown. If `None`, then\n                the figure and axes object are created in this method and the figure is\n                shown . Default: None.\n\n        Returns:\n            The matplotlib.axes.Axes object that contains the plot.\n        """"""\n\n        if skip_start < 0:\n            raise ValueError(""skip_start cannot be negative"")\n        if skip_end < 0:\n            raise ValueError(""skip_end cannot be negative"")\n        if show_lr is not None and not isinstance(show_lr, float):\n            raise ValueError(""show_lr must be float"")\n\n        # Get the data to plot from the history dictionary. Also, handle skip_end=0\n        # properly so the behaviour is the expected\n        lrs = self.history[""lr""]\n        losses = self.history[""loss""]\n        if skip_end == 0:\n            lrs = lrs[skip_start:]\n            losses = losses[skip_start:]\n        else:\n            lrs = lrs[skip_start:-skip_end]\n            losses = losses[skip_start:-skip_end]\n\n        # Create the figure and axes object if axes was not already given\n        fig = None\n        if ax is None:\n            fig, ax = plt.subplots()\n\n        # Plot loss as a function of the learning rate\n        ax.plot(lrs, losses)\n        if log_lr:\n            ax.set_xscale(""log"")\n        ax.set_xlabel(""Learning rate"")\n        ax.set_ylabel(""Loss"")\n\n        if show_lr is not None:\n            ax.axvline(x=show_lr, color=""red"")\n\n        # Show only if the figure was created internally\n        if fig is not None:\n            plt.show()\n\n        return ax\n\n\nclass LinearLR(_LRScheduler):\n    """"""Linearly increases the learning rate between two boundaries over a number of\n    iterations.\n\n    Arguments:\n        optimizer (torch.optim.Optimizer): wrapped optimizer.\n        end_lr (float): the final learning rate.\n        num_iter (int): the number of iterations over which the test occurs.\n        last_epoch (int, optional): the index of last epoch. Default: -1.\n    """"""\n\n    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n        self.end_lr = end_lr\n        self.num_iter = num_iter\n        super(LinearLR, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        curr_iter = self.last_epoch + 1\n        r = curr_iter / self.num_iter\n        return [base_lr + r * (self.end_lr - base_lr) for base_lr in self.base_lrs]\n\n\nclass ExponentialLR(_LRScheduler):\n    """"""Exponentially increases the learning rate between two boundaries over a number of\n    iterations.\n\n    Arguments:\n        optimizer (torch.optim.Optimizer): wrapped optimizer.\n        end_lr (float): the final learning rate.\n        num_iter (int): the number of iterations over which the test occurs.\n        last_epoch (int, optional): the index of last epoch. Default: -1.\n    """"""\n\n    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n        self.end_lr = end_lr\n        self.num_iter = num_iter\n        super(ExponentialLR, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        curr_iter = self.last_epoch + 1\n        r = curr_iter / self.num_iter\n        return [base_lr * (self.end_lr / base_lr) ** r for base_lr in self.base_lrs]\n\n\nclass StateCacher(object):\n    def __init__(self, in_memory, cache_dir=None):\n        self.in_memory = in_memory\n        self.cache_dir = cache_dir\n\n        if self.cache_dir is None:\n            import tempfile\n\n            self.cache_dir = tempfile.gettempdir()\n        else:\n            if not os.path.isdir(self.cache_dir):\n                raise ValueError(""Given `cache_dir` is not a valid directory."")\n\n        self.cached = {}\n\n    def store(self, key, state_dict):\n        if self.in_memory:\n            self.cached.update({key: copy.deepcopy(state_dict)})\n        else:\n            fn = os.path.join(self.cache_dir, ""state_{}_{}.pt"".format(key, id(self)))\n            self.cached.update({key: fn})\n            torch.save(state_dict, fn)\n\n    def retrieve(self, key):\n        if key not in self.cached:\n            raise KeyError(""Target {} was not cached."".format(key))\n\n        if self.in_memory:\n            return self.cached.get(key)\n        else:\n            fn = self.cached.get(key)\n            if not os.path.exists(fn):\n                raise RuntimeError(\n                    ""Failed to load state in {}. File doesn\'t exist anymore."".format(fn)\n                )\n            state_dict = torch.load(fn, map_location=lambda storage, location: storage)\n            return state_dict\n\n    def __del__(self):\n        """"""Check whether there are unused cached files existing in `cache_dir` before\n        this instance being destroyed.""""""\n\n        if self.in_memory:\n            return\n\n        for k in self.cached:\n            if os.path.exists(self.cached[k]):\n                os.remove(self.cached[k])\n\n\nclass DataLoaderIterWrapper(object):\n    """"""A wrapper for iterating `torch.utils.data.DataLoader` with the ability to reset\n    itself while `StopIteration` is raised.""""""\n\n    def __init__(self, data_loader, auto_reset=True):\n        self.data_loader = data_loader\n        self.auto_reset = auto_reset\n        self._iterator = iter(data_loader)\n\n    def __next__(self):\n        # Get a new set of inputs and labels\n        try:\n            inputs, labels, *_ = next(self._iterator)\n        except StopIteration:\n            if not self.auto_reset:\n                raise\n            self._iterator = iter(self.data_loader)\n            inputs, labels, *_ = next(self._iterator)\n\n        return inputs, labels\n'"
callback/lr_scheduler.py,3,"b'import math\r\nimport numpy as np\r\nimport warnings\r\nfrom torch.optim.optimizer import Optimizer\r\nfrom torch.optim.lr_scheduler import LambdaLR\r\n\r\ndef get_constant_schedule(optimizer, last_epoch=-1):\r\n    """""" Create a schedule with a constant learning rate.\r\n    """"""\r\n    return LambdaLR(optimizer, lambda _: 1, last_epoch=last_epoch)\r\n\r\n\r\ndef get_constant_schedule_with_warmup(optimizer, num_warmup_steps, last_epoch=-1):\r\n    """""" Create a schedule with a constant learning rate preceded by a warmup\r\n    period during which the learning rate increases linearly between 0 and 1.\r\n    """"""\r\n    def lr_lambda(current_step):\r\n        if current_step < num_warmup_steps:\r\n            return float(current_step) / float(max(1.0, num_warmup_steps))\r\n        return 1.\r\n\r\n    return LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\r\n\r\n\r\ndef get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, last_epoch=-1):\r\n    """""" Create a schedule with a learning rate that decreases linearly after\r\n    linearly increasing during a warmup period.\r\n    """"""\r\n    def lr_lambda(current_step):\r\n        if current_step < num_warmup_steps:\r\n            return float(current_step) / float(max(1, num_warmup_steps))\r\n        return max(0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps)))\r\n\r\n    return LambdaLR(optimizer, lr_lambda, last_epoch)\r\n\r\n\r\ndef get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_cycles=.5, last_epoch=-1):\r\n    """""" Create a schedule with a learning rate that decreases following the\r\n    values of the cosine function between 0 and `pi * cycles` after a warmup\r\n    period during which it increases linearly between 0 and 1.\r\n    """"""\r\n    def lr_lambda(current_step):\r\n        if current_step < num_warmup_steps:\r\n            return float(current_step) / float(max(1, num_warmup_steps))\r\n        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\r\n        return max(0., 0.5 * (1. + math.cos(math.pi * float(num_cycles) * 2. * progress)))\r\n\r\n    return LambdaLR(optimizer, lr_lambda, last_epoch)\r\n\r\n\r\ndef get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_cycles=1., last_epoch=-1):\r\n    """""" Create a schedule with a learning rate that decreases following the\r\n    values of the cosine function with several hard restarts, after a warmup\r\n    period during which it increases linearly between 0 and 1.\r\n    """"""\r\n    def lr_lambda(current_step):\r\n        if current_step < num_warmup_steps:\r\n            return float(current_step) / float(max(1, num_warmup_steps))\r\n        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\r\n        if progress >= 1.:\r\n            return 0.\r\n        return max(0., 0.5 * (1. + math.cos(math.pi * ((float(num_cycles) * progress) % 1.))))\r\n\r\n    return LambdaLR(optimizer, lr_lambda, last_epoch)\r\n\r\n\r\nclass CustomDecayLR(object):\r\n    \'\'\'\r\n    \xe8\x87\xaa\xe5\xae\x9a\xe4\xb9\x89\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe5\x8f\x98\xe5\x8c\x96\xe6\x9c\xba\xe5\x88\xb6\r\n        Example:\r\n        >>> scheduler = CustomDecayLR(optimizer)\r\n        >>> for epoch in range(100):\r\n        >>>     scheduler.epoch_step()\r\n        >>>     train(...)\r\n        >>>         ...\r\n        >>>         optimizer.zero_grad()\r\n        >>>         loss.backward()\r\n        >>>         optimizer.step()\r\n        >>>     validate(...)\r\n    \'\'\'\r\n    def __init__(self,optimizer,lr):\r\n        self.optimizer = optimizer\r\n        self.lr = lr\r\n\r\n    def epoch_step(self,epoch):\r\n        lr = self.lr\r\n        if epoch > 12:\r\n            lr = lr / 1000\r\n        elif epoch > 8:\r\n            lr = lr / 100\r\n        elif epoch > 4:\r\n            lr = lr / 10\r\n        for param_group in self.optimizer.param_groups:\r\n            param_group[\'lr\'] = lr\r\n\r\nclass BertLR(object):\r\n    \'\'\'\r\n    Bert\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x86\x85\xe5\xae\x9a\xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe5\x8f\x98\xe5\x8c\x96\xe6\x9c\xba\xe5\x88\xb6\r\n    Example:\r\n        >>> scheduler = BertLR(optimizer)\r\n        >>> for epoch in range(100):\r\n        >>>     scheduler.step()\r\n        >>>     train(...)\r\n        >>>         ...\r\n        >>>         optimizer.zero_grad()\r\n        >>>         loss.backward()\r\n        >>>         optimizer.step()\r\n        >>>         scheduler.batch_step()\r\n        >>>     validate(...)\r\n    \'\'\'\r\n    def __init__(self,optimizer,learning_rate,t_total,warmup):\r\n        self.learning_rate = learning_rate\r\n        self.optimizer = optimizer\r\n        self.t_total = t_total\r\n        self.warmup = warmup\r\n\r\n    # \xe7\xba\xbf\xe6\x80\xa7\xe9\xa2\x84\xe7\x83\xad\xe6\x96\xb9\xe5\xbc\x8f\r\n    def warmup_linear(self,x, warmup=0.002):\r\n        if x < warmup:\r\n            return x / warmup\r\n        return 1.0 - x\r\n\r\n    def batch_step(self,training_step):\r\n        lr_this_step = self.learning_rate * self.warmup_linear(training_step / self.t_total,self.warmup)\r\n        for param_group in self.optimizer.param_groups:\r\n            param_group[\'lr\'] = lr_this_step\r\n\r\nclass CyclicLR(object):\r\n    \'\'\'\r\n    Cyclical learning rates for training neural networks\r\n    Example:\r\n        >>> scheduler = CyclicLR(optimizer)\r\n        >>> for epoch in range(100):\r\n        >>>     scheduler.step()\r\n        >>>     train(...)\r\n        >>>         ...\r\n        >>>         optimizer.zero_grad()\r\n        >>>         loss.backward()\r\n        >>>         optimizer.step()\r\n        >>>         scheduler.batch_step()\r\n        >>>     validate(...)\r\n    \'\'\'\r\n    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\r\n                 step_size=2000, mode=\'triangular\', gamma=1.,\r\n                 scale_fn=None, scale_mode=\'cycle\', last_batch_iteration=-1):\r\n\r\n        if not isinstance(optimizer, Optimizer):\r\n            raise TypeError(\'{} is not an Optimizer\'.format(\r\n                type(optimizer).__name__))\r\n\r\n        self.optimizer = optimizer\r\n\r\n        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\r\n            if len(base_lr) != len(optimizer.param_groups):\r\n                raise ValueError(""expected {} base_lr, got {}"".format(\r\n                    len(optimizer.param_groups), len(base_lr)))\r\n            self.base_lrs = list(base_lr)\r\n        else:\r\n            self.base_lrs = [base_lr] * len(optimizer.param_groups)\r\n\r\n        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\r\n            if len(max_lr) != len(optimizer.param_groups):\r\n                raise ValueError(""expected {} max_lr, got {}"".format(\r\n                    len(optimizer.param_groups), len(max_lr)))\r\n            self.max_lrs = list(max_lr)\r\n        else:\r\n            self.max_lrs = [max_lr] * len(optimizer.param_groups)\r\n\r\n        self.step_size = step_size\r\n\r\n        if mode not in [\'triangular\', \'triangular2\', \'exp_range\'] \\\r\n                and scale_fn is None:\r\n            raise ValueError(\'mode is invalid and scale_fn is None\')\r\n\r\n        self.mode = mode\r\n        self.gamma = gamma\r\n\r\n        if scale_fn is None:\r\n            if self.mode == \'triangular\':\r\n                self.scale_fn = self._triangular_scale_fn\r\n                self.scale_mode = \'cycle\'\r\n            elif self.mode == \'triangular2\':\r\n                self.scale_fn = self._triangular2_scale_fn\r\n                self.scale_mode = \'cycle\'\r\n            elif self.mode == \'exp_range\':\r\n                self.scale_fn = self._exp_range_scale_fn\r\n                self.scale_mode = \'iterations\'\r\n        else:\r\n            self.scale_fn = scale_fn\r\n            self.scale_mode = scale_mode\r\n\r\n        self.batch_step(last_batch_iteration + 1)\r\n        self.last_batch_iteration = last_batch_iteration\r\n\r\n    def _triangular_scale_fn(self, x):\r\n        return 1.\r\n\r\n    def _triangular2_scale_fn(self, x):\r\n        return 1 / (2. ** (x - 1))\r\n\r\n    def _exp_range_scale_fn(self, x):\r\n        return self.gamma**(x)\r\n\r\n    def get_lr(self):\r\n        step_size = float(self.step_size)\r\n        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\r\n        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\r\n\r\n        lrs = []\r\n        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\r\n        for param_group, base_lr, max_lr in param_lrs:\r\n            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\r\n            if self.scale_mode == \'cycle\':\r\n                lr = base_lr + base_height * self.scale_fn(cycle)\r\n            else:\r\n                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\r\n            lrs.append(lr)\r\n        return lrs\r\n\r\n    def batch_step(self, batch_iteration=None):\r\n        if batch_iteration is None:\r\n            batch_iteration = self.last_batch_iteration + 1\r\n        self.last_batch_iteration = batch_iteration\r\n        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\r\n            param_group[\'lr\'] = lr\r\n\r\nclass ReduceLROnPlateau(object):\r\n    """"""Reduce learning rate when a metric has stopped improving.\r\n    Models often benefit from reducing the learning rate by a factor\r\n    of 2-10 once learning stagnates. This scheduler reads a metrics\r\n    quantity and if no improvement is seen for a \'patience\' number\r\n    of epochs, the learning rate is reduced.\r\n\r\n    Args:\r\n        factor: factor by which the learning rate will\r\n            be reduced. new_lr = lr * factor\r\n        patience: number of epochs with no improvement\r\n            after which learning rate will be reduced.\r\n        verbose: int. 0: quiet, 1: update messages.\r\n        mode: one of {min, max}. In `min` mode,\r\n            lr will be reduced when the quantity\r\n            monitored has stopped decreasing; in `max`\r\n            mode it will be reduced when the quantity\r\n            monitored has stopped increasing.\r\n        epsilon: threshold for measuring the new optimum,\r\n            to only focus on significant changes.\r\n        cooldown: number of epochs to wait before resuming\r\n            normal operation after lr has been reduced.\r\n        min_lr: lower bound on the learning rate.\r\n\r\n\r\n    Example:\r\n        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\r\n        >>> scheduler = ReduceLROnPlateau(optimizer, \'min\')\r\n        >>> for epoch in range(10):\r\n        >>>     train(...)\r\n        >>>     val_acc, val_loss = validate(...)\r\n        >>>     scheduler.epoch_step(val_loss, epoch)\r\n    """"""\r\n\r\n    def __init__(self, optimizer, mode=\'min\', factor=0.1, patience=10,\r\n                 verbose=0, epsilon=1e-4, cooldown=0, min_lr=0,eps=1e-8):\r\n\r\n        super(ReduceLROnPlateau, self).__init__()\r\n        assert isinstance(optimizer, Optimizer)\r\n        if factor >= 1.0:\r\n            raise ValueError(\'ReduceLROnPlateau \'\r\n                             \'does not support a factor >= 1.0.\')\r\n        self.factor = factor\r\n        self.min_lr = min_lr\r\n        self.epsilon = epsilon\r\n        self.patience = patience\r\n        self.verbose = verbose\r\n        self.cooldown = cooldown\r\n        self.cooldown_counter = 0  # Cooldown counter.\r\n        self.monitor_op = None\r\n        self.wait = 0\r\n        self.best = 0\r\n        self.mode = mode\r\n        self.optimizer = optimizer\r\n        self.eps = eps\r\n        self._reset()\r\n\r\n    def _reset(self):\r\n        """"""Resets wait counter and cooldown counter.\r\n        """"""\r\n        if self.mode not in [\'min\', \'max\']:\r\n            raise RuntimeError(\'Learning Rate Plateau Reducing mode %s is unknown!\')\r\n        if self.mode == \'min\':\r\n            self.monitor_op = lambda a, b: np.less(a, b - self.epsilon)\r\n            self.best = np.Inf\r\n        else:\r\n            self.monitor_op = lambda a, b: np.greater(a, b + self.epsilon)\r\n            self.best = -np.Inf\r\n        self.cooldown_counter = 0\r\n        self.wait = 0\r\n\r\n    def reset(self):\r\n        self._reset()\r\n\r\n    def epoch_step(self, metrics, epoch):\r\n        current = metrics\r\n        if current is None:\r\n            warnings.warn(\'Learning Rate Plateau Reducing requires metrics available!\', RuntimeWarning)\r\n        else:\r\n            if self.in_cooldown():\r\n                self.cooldown_counter -= 1\r\n                self.wait = 0\r\n\r\n            if self.monitor_op(current, self.best):\r\n                self.best = current\r\n                self.wait = 0\r\n            elif not self.in_cooldown():\r\n                if self.wait >= self.patience:\r\n                    for param_group in self.optimizer.param_groups:\r\n                        old_lr = float(param_group[\'lr\'])\r\n                        if old_lr > self.min_lr + self.eps:\r\n                            new_lr = old_lr * self.factor\r\n                            new_lr = max(new_lr, self.min_lr)\r\n                            param_group[\'lr\'] = new_lr\r\n                            if self.verbose > 0:\r\n                                print(\'\\nEpoch %05d: reducing learning rate to %s.\' % (epoch, new_lr))\r\n                            self.cooldown_counter = self.cooldown\r\n                            self.wait = 0\r\n                self.wait += 1\r\n\r\n    def in_cooldown(self):\r\n        return self.cooldown_counter > 0\r\n\r\nclass ReduceLRWDOnPlateau(ReduceLROnPlateau):\r\n    """"""Reduce learning rate and weight decay when a metric has stopped\r\n    improving. Models often benefit from reducing the learning rate by\r\n    a factor of 2-10 once learning stagnates. This scheduler reads a metric\r\n    quantity and if no improvement is seen for a \'patience\' number\r\n    of epochs, the learning rate and weight decay factor is reduced for\r\n    optimizers that implement the the weight decay method from the paper\r\n    `Fixing Weight Decay Regularization in Adam`_.\r\n\r\n    .. _Fixing Weight Decay Regularization in Adam:\r\n        https://arxiv.org/abs/1711.05101\r\n    for AdamW or SGDW\r\n    Example:\r\n        >>> optimizer = AdamW(model.parameters(), lr=0.1, weight_decay=1e-3)\r\n        >>> scheduler = ReduceLRWDOnPlateau(optimizer, \'min\')\r\n        >>> for epoch in range(10):\r\n        >>>     train(...)\r\n        >>>     val_loss = validate(...)\r\n        >>>     # Note that step should be called after validate()\r\n        >>>     scheduler.epoch_step(val_loss)\r\n    """"""\r\n    def epoch_step(self, metrics, epoch):\r\n        current = metrics\r\n        if current is None:\r\n            warnings.warn(\'Learning Rate Plateau Reducing requires metrics available!\', RuntimeWarning)\r\n        else:\r\n            if self.in_cooldown():\r\n                self.cooldown_counter -= 1\r\n                self.wait = 0\r\n\r\n            if self.monitor_op(current, self.best):\r\n                self.best = current\r\n                self.wait = 0\r\n            elif not self.in_cooldown():\r\n                if self.wait >= self.patience:\r\n                    for param_group in self.optimizer.param_groups:\r\n                        old_lr = float(param_group[\'lr\'])\r\n                        if old_lr > self.min_lr + self.eps:\r\n                            new_lr = old_lr * self.factor\r\n                            new_lr = max(new_lr, self.min_lr)\r\n                            param_group[\'lr\'] = new_lr\r\n                            if self.verbose > 0:\r\n                                print(\'\\nEpoch %d: reducing learning rate to %s.\' % (epoch, new_lr))\r\n                        if param_group[\'weight_decay\'] != 0:\r\n                            old_weight_decay = float(param_group[\'weight_decay\'])\r\n                            new_weight_decay = max(old_weight_decay * self.factor, self.min_lr)\r\n                            if old_weight_decay > new_weight_decay + self.eps:\r\n                                param_group[\'weight_decay\'] = new_weight_decay\r\n                                if self.verbose:\r\n                                    print(\'\\nEpoch {epoch}: reducing weight decay factor of group {i} to {new_weight_decay:.4e}.\')\r\n                    self.cooldown_counter = self.cooldown\r\n                    self.wait = 0\r\n                self.wait += 1\r\n\r\nclass CosineLRWithRestarts(object):\r\n    """"""Decays learning rate with cosine annealing, normalizes weight decay\r\n    hyperparameter value, implements restarts.\r\n    https://arxiv.org/abs/1711.05101\r\n\r\n    Args:\r\n        optimizer (Optimizer): Wrapped optimizer.\r\n        batch_size: minibatch size\r\n        epoch_size: training samples per epoch\r\n        restart_period: epoch count in the first restart period\r\n        t_mult: multiplication factor by which the next restart period will extend/shrink\r\n\r\n    Example:\r\n        >>> scheduler = CosineLRWithRestarts(optimizer, 32, 1024, restart_period=5, t_mult=1.2)\r\n        >>> for epoch in range(100):\r\n        >>>     scheduler.step()\r\n        >>>     train(...)\r\n        >>>         ...\r\n        >>>         optimizer.zero_grad()\r\n        >>>         loss.backward()\r\n        >>>         optimizer.step()\r\n        >>>         scheduler.batch_step()\r\n        >>>     validate(...)\r\n    """"""\r\n\r\n    def __init__(self, optimizer, batch_size, epoch_size, restart_period=100,\r\n                 t_mult=2, last_epoch=-1, eta_threshold=1000, verbose=False):\r\n        if not isinstance(optimizer, Optimizer):\r\n            raise TypeError(\'{} is not an Optimizer\'.format(\r\n                type(optimizer).__name__))\r\n        self.optimizer = optimizer\r\n        if last_epoch == -1:\r\n            for group in optimizer.param_groups:\r\n                group.setdefault(\'initial_lr\', group[\'lr\'])\r\n        else:\r\n            for i, group in enumerate(optimizer.param_groups):\r\n                if \'initial_lr\' not in group:\r\n                    raise KeyError(""param \'initial_lr\' is not specified ""\r\n                                   ""in param_groups[{}] when resuming an""\r\n                                   "" optimizer"".format(i))\r\n        self.base_lrs = list(map(lambda group: group[\'initial_lr\'],\r\n                                 optimizer.param_groups))\r\n\r\n        self.last_epoch = last_epoch\r\n        self.batch_size = batch_size\r\n        self.iteration = 0\r\n        self.epoch_size = epoch_size\r\n        self.eta_threshold = eta_threshold\r\n        self.t_mult = t_mult\r\n        self.verbose = verbose\r\n        self.base_weight_decays = list(map(lambda group: group[\'weight_decay\'],\r\n                                           optimizer.param_groups))\r\n        self.restart_period = restart_period\r\n        self.restarts = 0\r\n        self.t_epoch = -1\r\n        self.batch_increments = []\r\n        self._set_batch_increment()\r\n\r\n    def _schedule_eta(self):\r\n        """"""\r\n        Threshold value could be adjusted to shrink eta_min and eta_max values.\r\n        """"""\r\n        eta_min = 0\r\n        eta_max = 1\r\n        if self.restarts <= self.eta_threshold:\r\n            return eta_min, eta_max\r\n        else:\r\n            d = self.restarts - self.eta_threshold\r\n            k = d * 0.09\r\n            return (eta_min + k, eta_max - k)\r\n\r\n    def get_lr(self, t_cur):\r\n        eta_min, eta_max = self._schedule_eta()\r\n\r\n        eta_t = (eta_min + 0.5 * (eta_max - eta_min)\r\n                 * (1. + math.cos(math.pi *\r\n                                  (t_cur / self.restart_period))))\r\n\r\n        weight_decay_norm_multi = math.sqrt(self.batch_size /\r\n                                            (self.epoch_size *\r\n                                             self.restart_period))\r\n        lrs = [base_lr * eta_t for base_lr in self.base_lrs]\r\n        weight_decays = [base_weight_decay * eta_t * weight_decay_norm_multi\r\n                         for base_weight_decay in self.base_weight_decays]\r\n\r\n        if self.t_epoch % self.restart_period < self.t_epoch:\r\n            if self.verbose:\r\n                print(""Restart at epoch {}"".format(self.last_epoch))\r\n            self.restart_period *= self.t_mult\r\n            self.restarts += 1\r\n            self.t_epoch = 0\r\n\r\n        return zip(lrs, weight_decays)\r\n\r\n    def _set_batch_increment(self):\r\n        d, r = divmod(self.epoch_size, self.batch_size)\r\n        batches_in_epoch = d + 2 if r > 0 else d + 1\r\n        self.iteration = 0\r\n        self.batch_increments = list(np.linspace(0, 1, batches_in_epoch))\r\n\r\n    def batch_step(self):\r\n        self.last_epoch += 1\r\n        self.t_epoch += 1\r\n        self._set_batch_increment()\r\n        try:\r\n            t_cur = self.t_epoch + self.batch_increments[self.iteration]\r\n            self.iteration += 1\r\n        except (IndexError):\r\n            raise RuntimeError(""Epoch size and batch size used in the ""\r\n                               ""training loop and while initializing ""\r\n                               ""scheduler should be the same."")\r\n\r\n        for param_group, (lr, weight_decay) in zip(self.optimizer.param_groups,self.get_lr(t_cur)):\r\n            param_group[\'lr\'] = lr\r\n            param_group[\'weight_decay\'] = weight_decay\r\n\r\n\r\nclass NoamLR(object):\r\n    \'\'\'\r\n    \xe4\xb8\xbb\xe8\xa6\x81\xe5\x8f\x82\xe8\x80\x83\xe8\xae\xba\xe6\x96\x87<< Attention Is All You Need>>\xe4\xb8\xad\xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xe6\x9b\xb4\xe6\x96\xb0\xe6\x96\xb9\xe5\xbc\x8f\r\n    Example:\r\n        >>> scheduler = NoamLR(d_model,factor,warm_up,optimizer)\r\n        >>> for epoch in range(100):\r\n        >>>     scheduler.step()\r\n        >>>     train(...)\r\n        >>>         ...\r\n        >>>         glopab_step += 1\r\n        >>>         optimizer.zero_grad()\r\n        >>>         loss.backward()\r\n        >>>         optimizer.step()\r\n        >>>         scheduler.batch_step(global_step)\r\n        >>>     validate(...)\r\n    \'\'\'\r\n    def __init__(self,d_model,factor,warm_up,optimizer):\r\n        self.optimizer = optimizer\r\n        self.warm_up = warm_up\r\n        self.factor = factor\r\n        self.d_model = d_model\r\n        self._lr = 0\r\n\r\n    def get_lr(self,step):\r\n        lr = self.factor * (self.d_model ** (-0.5) * min(step ** (-0.5),step * self.warm_up ** (-1.5)))\r\n        return lr\r\n\r\n    def batch_step(self,step):\r\n        \'\'\'\r\n        update parameters and rate\r\n        :return:\r\n        \'\'\'\r\n        lr = self.get_lr(step)\r\n        for p in self.optimizer.param_groups:\r\n            p[\'lr\'] = lr\r\n        self._lr = lr\r\n'"
callback/modelcheckpoint.py,4,"b'from pathlib import Path\r\nimport numpy as np\r\nimport torch\r\nfrom ..tools.common import logger\r\n\r\nclass ModelCheckpoint(object):\r\n    \'\'\'\r\n    \xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbf\x9d\xe5\xad\x98\xef\xbc\x8c\xe4\xb8\xa4\xe7\xa7\x8d\xe6\xa8\xa1\xe5\xbc\x8f\xef\xbc\x9a\r\n    1. \xe7\x9b\xb4\xe6\x8e\xa5\xe4\xbf\x9d\xe5\xad\x98\xe6\x9c\x80\xe5\xa5\xbd\xe6\xa8\xa1\xe5\x9e\x8b\r\n    2. \xe6\x8c\x89\xe7\x85\xa7epoch\xe9\xa2\x91\xe7\x8e\x87\xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\r\n    \'\'\'\r\n    def __init__(self, checkpoint_dir,\r\n                 monitor,\r\n                 arch,mode=\'min\',\r\n                 epoch_freq=1,\r\n                 best = None,\r\n                 save_best_only = True):\r\n        if isinstance(checkpoint_dir,Path):\r\n            checkpoint_dir = checkpoint_dir\r\n        else:\r\n            checkpoint_dir = Path(checkpoint_dir)\r\n        assert checkpoint_dir.is_dir()\r\n        checkpoint_dir.mkdir(exist_ok=True)\r\n        self.base_path = checkpoint_dir\r\n        self.arch = arch\r\n        self.monitor = monitor\r\n        self.epoch_freq = epoch_freq\r\n        self.save_best_only = save_best_only\r\n\r\n        # \xe8\xae\xa1\xe7\xae\x97\xe6\xa8\xa1\xe5\xbc\x8f\r\n        if mode == \'min\':\r\n            self.monitor_op = np.less\r\n            self.best = np.Inf\r\n\r\n        elif mode == \'max\':\r\n            self.monitor_op = np.greater\r\n            self.best = -np.Inf\r\n        # \xe8\xbf\x99\xe9\x87\x8c\xe4\xb8\xbb\xe8\xa6\x81\xe9\x87\x8d\xe6\x96\xb0\xe5\x8a\xa0\xe8\xbd\xbd\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x97\xb6\xe5\x80\x99\r\n        #\xe5\xaf\xb9best\xe9\x87\x8d\xe6\x96\xb0\xe8\xb5\x8b\xe5\x80\xbc\r\n        if best:\r\n            self.best = best\r\n\r\n        if save_best_only:\r\n            self.model_name = f""BEST_{arch}_MODEL.pth""\r\n\r\n    def epoch_step(self, state,current):\r\n        \'\'\'\r\n        \xe6\xad\xa3\xe5\xb8\xb8\xe6\xa8\xa1\xe5\x9e\x8b\r\n        :param state: \xe9\x9c\x80\xe8\xa6\x81\xe4\xbf\x9d\xe5\xad\x98\xe7\x9a\x84\xe4\xbf\xa1\xe6\x81\xaf\r\n        :param current: \xe5\xbd\x93\xe5\x89\x8d\xe5\x88\xa4\xe6\x96\xad\xe6\x8c\x87\xe6\xa0\x87\r\n        :return:\r\n        \'\'\'\r\n        # \xe6\x98\xaf\xe5\x90\xa6\xe4\xbf\x9d\xe5\xad\x98\xe6\x9c\x80\xe5\xa5\xbd\xe6\xa8\xa1\xe5\x9e\x8b\r\n        if self.save_best_only:\r\n            if self.monitor_op(current, self.best):\r\n                logger.info(f""\\nEpoch {state[\'epoch\']}: {self.monitor} improved from {self.best:.5f} to {current:.5f}"")\r\n                self.best = current\r\n                state[\'best\'] = self.best\r\n                best_path = self.base_path/ self.model_name\r\n                torch.save(state, str(best_path))\r\n        # \xe6\xaf\x8f\xe9\x9a\x94\xe5\x87\xa0\xe4\xb8\xaaepoch\xe4\xbf\x9d\xe5\xad\x98\xe4\xb8\x8b\xe6\xa8\xa1\xe5\x9e\x8b\r\n        else:\r\n            filename = self.base_path / f""EPOCH_{state[\'epoch\']}_{state[self.monitor]}_{self.arch}_MODEL.pth""\r\n            if state[\'epoch\'] % self.epoch_freq == 0:\r\n                logger.info(f""\\nEpoch {state[\'epoch\']}: save model to disk."")\r\n                torch.save(state, str(filename))\r\n\r\n    def bert_epoch_step(self, state,current):\r\n        \'\'\'\r\n        \xe9\x80\x82\xe5\x90\x88bert\xe7\xb1\xbb\xe5\x9e\x8b\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe9\x80\x82\xe5\x90\x88pytorch_transformer\xe6\xa8\xa1\xe5\x9d\x97\r\n        :param state:\r\n        :param current:\r\n        :return:\r\n        \'\'\'\r\n        model_to_save = state[\'model\']\r\n        if self.save_best_only:\r\n            if self.monitor_op(current, self.best):\r\n                logger.info(f""\\nEpoch {state[\'epoch\']}: {self.monitor} improved from {self.best:.5f} to {current:.5f}"")\r\n                self.best = current\r\n                state[\'best\'] = self.best\r\n                model_to_save.save_pretrained(str(self.base_path))\r\n                output_config_file = self.base_path / \'configs.json\'\r\n                with open(str(output_config_file), \'w\') as f:\r\n                    f.write(model_to_save.config.to_json_string())\r\n                state.pop(""model"")\r\n                torch.save(state,self.base_path / \'checkpoint_info.bin\')\r\n        else:\r\n            if state[\'epoch\'] % self.epoch_freq == 0:\r\n                save_path = self.base_path / f""checkpoint-epoch-{state[\'epoch\']}""\r\n                save_path.mkdir(exist_ok=True)\r\n                logger.info(f""\\nEpoch {state[\'epoch\']}: save model to disk."")\r\n                model_to_save.save_pretrained(save_path)\r\n                output_config_file = save_path / \'configs.json\'\r\n                with open(str(output_config_file), \'w\') as f:\r\n                    f.write(model_to_save.config.to_json_string())\r\n                state.pop(""model"")\r\n                torch.save(state, save_path / \'checkpoint_info.bin\')\r\n'"
callback/progressbar.py,0,"b'import time\r\nclass ProgressBar(object):\r\n    \'\'\'\r\n    custom progress bar\r\n    Example:\r\n        >>> pbar = ProgressBar(n_total=30,desc=\'training\')\r\n        >>> step = 2\r\n        >>> pbar(step=step)\r\n    \'\'\'\r\n    def __init__(self, n_total,width=30,desc = \'Training\'):\r\n        self.width = width\r\n        self.n_total = n_total\r\n        self.start_time = time.time()\r\n        self.desc = desc\r\n\r\n    def __call__(self, step, info={}):\r\n        now = time.time()\r\n        current = step + 1\r\n        recv_per = current / self.n_total\r\n        bar = f\'[{self.desc}] {current}/{self.n_total} [\'\r\n        if recv_per >= 1:\r\n            recv_per = 1\r\n        prog_width = int(self.width * recv_per)\r\n        if prog_width > 0:\r\n            bar += \'=\' * (prog_width - 1)\r\n            if current< self.n_total:\r\n                bar += "">""\r\n            else:\r\n                bar += \'=\'\r\n        bar += \'.\' * (self.width - prog_width)\r\n        bar += \']\'\r\n        show_bar = f""\\r{bar}""\r\n        time_per_unit = (now - self.start_time) / current\r\n        if current < self.n_total:\r\n            eta = time_per_unit * (self.n_total - current)\r\n            if eta > 3600:\r\n                eta_format = (\'%d:%02d:%02d\' %\r\n                              (eta // 3600, (eta % 3600) // 60, eta % 60))\r\n            elif eta > 60:\r\n                eta_format = \'%d:%02d\' % (eta // 60, eta % 60)\r\n            else:\r\n                eta_format = \'%ds\' % eta\r\n            time_info = f\' - ETA: {eta_format}\'\r\n        else:\r\n            if time_per_unit >= 1:\r\n                time_info = f\' {time_per_unit:.1f}s/step\'\r\n            elif time_per_unit >= 1e-3:\r\n                time_info = f\' {time_per_unit * 1e3:.1f}ms/step\'\r\n            else:\r\n                time_info = f\' {time_per_unit * 1e6:.1f}us/step\'\r\n\r\n        show_bar += time_info\r\n        if len(info) != 0:\r\n            show_info = f\'{show_bar} \' + \\\r\n                        ""-"".join([f\' {key}: {value:.4f} \' for key, value in info.items()])\r\n            print(show_info, end=\'\')\r\n        else:\r\n            print(show_bar, end=\'\')\r\n'"
callback/trainingmonitor.py,0,"b'# encoding:utf-8\nimport numpy as np\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nfrom ..tools.common import load_json\nfrom ..tools.common import save_json\nplt.switch_backend(\'agg\')\n\nclass TrainingMonitor():\n    def __init__(self, file_dir, arch, add_test=False):\n        \'\'\'\n        :param startAt: \xe9\x87\x8d\xe6\x96\xb0\xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84epoch\xe7\x82\xb9\n        \'\'\'\n        if isinstance(file_dir, Path):\n            pass\n        else:\n            file_dir = Path(file_dir)\n        file_dir.mkdir(parents=True, exist_ok=True)\n\n        self.arch = arch\n        self.file_dir = file_dir\n        self.H = {}\n        self.add_test = add_test\n        self.json_path = file_dir / (arch + ""_training_monitor.json"")\n\n    def reset(self,start_at):\n        if start_at > 0:\n            if self.json_path is not None:\n                if self.json_path.exists():\n                    self.H = load_json(self.json_path)\n                    for k in self.H.keys():\n                        self.H[k] = self.H[k][:start_at]\n\n    def epoch_step(self, logs={}):\n        for (k, v) in logs.items():\n            l = self.H.get(k, [])\n            # np.float32\xe4\xbc\x9a\xe6\x8a\xa5\xe9\x94\x99\n            if not isinstance(v, np.float):\n                v = round(float(v), 4)\n            l.append(v)\n            self.H[k] = l\n\n        # \xe5\x86\x99\xe5\x85\xa5\xe6\x96\x87\xe4\xbb\xb6\n        if self.json_path is not None:\n            save_json(data = self.H,file_path=self.json_path)\n\n        # \xe4\xbf\x9d\xe5\xad\x98train\xe5\x9b\xbe\xe5\x83\x8f\n        if len(self.H[""loss""]) == 1:\n            self.paths = {key: self.file_dir / (self.arch + f\'_{key.upper()}\') for key in self.H.keys()}\n\n        if len(self.H[""loss""]) > 1:\n            # \xe6\x8c\x87\xe6\xa0\x87\xe5\x8f\x98\xe5\x8c\x96\n            # \xe6\x9b\xb2\xe7\xba\xbf\n            # \xe9\x9c\x80\xe8\xa6\x81\xe6\x88\x90\xe5\xaf\xb9\xe5\x87\xba\xe7\x8e\xb0\n            keys = [key for key, _ in self.H.items() if \'_\' not in key]\n            for key in keys:\n                N = np.arange(0, len(self.H[key]))\n                plt.style.use(""ggplot"")\n                plt.figure()\n                plt.plot(N, self.H[key], label=f""train_{key}"")\n                plt.plot(N, self.H[f""valid_{key}""], label=f""valid_{key}"")\n                if self.add_test:\n                    plt.plot(N, self.H[f""test_{key}""], label=f""test_{key}"")\n                plt.legend()\n                plt.xlabel(""Epoch #"")\n                plt.ylabel(key)\n                plt.title(f""Training {key} [Epoch {len(self.H[key])}]"")\n                plt.savefig(str(self.paths[key]))\n                plt.close()\n'"
losses/__init__.py,0,b'\n\n'
losses/dice_loss.py,2,"b'import torch\r\nfrom torch import nn\r\n\r\nclass DiceLoss(nn.Module):\r\n    """"""DiceLoss implemented from \'Dice Loss for Data-imbalanced NLP Tasks\'\r\n    Useful in dealing with unbalanced data\r\n    """"""\r\n    def __init__(self):\r\n        super(DiceLoss, self).__init__()\r\n\r\n    def forward(self,input, target):\r\n        \'\'\'\r\n        input: [N, C]\r\n        target: [N, ]\r\n        \'\'\'\r\n        prob = torch.softmax(input, dim=1)\r\n        prob = torch.gather(prob, dim=1, index=target.unsqueeze(1))\r\n        dsc_i = 1 - ((1 - prob) * prob) / ((1 - prob) * prob + 1)\r\n        dice_loss = dsc_i.mean()\r\n        return dice_loss\r\n\r\n\r\n\r\n'"
losses/focal_loss.py,3,"b'import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nclass FocalLoss(nn.Module):\r\n    \'\'\'Multi-class Focal loss implementation\'\'\'\r\n    def __init__(self, gamma=2, weight=None,ignore_index=-100):\r\n        super(FocalLoss, self).__init__()\r\n        self.gamma = gamma\r\n        self.weight = weight\r\n        self.ignore_index=ignore_index\r\n\r\n    def forward(self, input, target):\r\n        """"""\r\n        input: [N, C]\r\n        target: [N, ]\r\n        """"""\r\n        logpt = F.log_softmax(input, dim=1)\r\n        pt = torch.exp(logpt)\r\n        logpt = (1-pt)**self.gamma * logpt\r\n        loss = F.nll_loss(logpt, target, self.weight,ignore_index=self.ignore_index)\r\n        return loss\r\n'"
losses/label_smoothing.py,2,"b""import torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nclass LabelSmoothingCrossEntropy(nn.Module):\r\n    def __init__(self, eps=0.1, reduction='mean',ignore_index=-100):\r\n        super(LabelSmoothingCrossEntropy, self).__init__()\r\n        self.eps = eps\r\n        self.reduction = reduction\r\n        self.ignore_index = ignore_index\r\n\r\n    def forward(self, output, target):\r\n        c = output.size()[-1]\r\n        log_preds = F.log_softmax(output, dim=-1)\r\n        if self.reduction=='sum':\r\n            loss = -log_preds.sum()\r\n        else:\r\n            loss = -log_preds.sum(dim=-1)\r\n            if self.reduction=='mean':\r\n                loss = loss.mean()\r\n        return loss*self.eps/c + (1-self.eps) * F.nll_loss(log_preds, target, reduction=self.reduction,\r\n                                                           ignore_index=self.ignore_index)"""
metrics/__init__.py,0,b'\n\n'
metrics/ner_metrics.py,0,"b'import torch\r\nfrom collections import Counter\r\nfrom processors.utils_ner import get_entities\r\n\r\nclass SeqEntityScore(object):\r\n    def __init__(self, id2label,markup=\'bios\'):\r\n        self.id2label = id2label\r\n        self.markup = markup\r\n        self.reset()\r\n\r\n    def reset(self):\r\n        self.origins = []\r\n        self.founds = []\r\n        self.rights = []\r\n\r\n    def compute(self, origin, found, right):\r\n        recall = 0 if origin == 0 else (right / origin)\r\n        precision = 0 if found == 0 else (right / found)\r\n        f1 = 0. if recall + precision == 0 else (2 * precision * recall) / (precision + recall)\r\n        return recall, precision, f1\r\n\r\n    def result(self):\r\n        class_info = {}\r\n        origin_counter = Counter([x[0] for x in self.origins])\r\n        found_counter = Counter([x[0] for x in self.founds])\r\n        right_counter = Counter([x[0] for x in self.rights])\r\n        for type_, count in origin_counter.items():\r\n            origin = count\r\n            found = found_counter.get(type_, 0)\r\n            right = right_counter.get(type_, 0)\r\n            recall, precision, f1 = self.compute(origin, found, right)\r\n            class_info[type_] = {""acc"": round(precision, 4), \'recall\': round(recall, 4), \'f1\': round(f1, 4)}\r\n        origin = len(self.origins)\r\n        found = len(self.founds)\r\n        right = len(self.rights)\r\n        recall, precision, f1 = self.compute(origin, found, right)\r\n        return {\'acc\': precision, \'recall\': recall, \'f1\': f1}, class_info\r\n\r\n    def update(self, label_paths, pred_paths):\r\n        \'\'\'\r\n        labels_paths: [[],[],[],....]\r\n        pred_paths: [[],[],[],.....]\r\n\r\n        :param label_paths:\r\n        :param pred_paths:\r\n        :return:\r\n        Example:\r\n            >>> labels_paths = [[\'O\', \'O\', \'O\', \'B-MISC\', \'I-MISC\', \'I-MISC\', \'O\'], [\'B-PER\', \'I-PER\', \'O\']]\r\n            >>> pred_paths = [[\'O\', \'O\', \'B-MISC\', \'I-MISC\', \'I-MISC\', \'I-MISC\', \'O\'], [\'B-PER\', \'I-PER\', \'O\']]\r\n        \'\'\'\r\n        for label_path, pre_path in zip(label_paths, pred_paths):\r\n            label_entities = get_entities(label_path, self.id2label,self.markup)\r\n            pre_entities = get_entities(pre_path, self.id2label,self.markup)\r\n            self.origins.extend(label_entities)\r\n            self.founds.extend(pre_entities)\r\n            self.rights.extend([pre_entity for pre_entity in pre_entities if pre_entity in label_entities])\r\n\r\nclass SpanEntityScore(object):\r\n    def __init__(self, id2label):\r\n        self.id2label = id2label\r\n        self.reset()\r\n\r\n    def reset(self):\r\n        self.origins = []\r\n        self.founds = []\r\n        self.rights = []\r\n\r\n    def compute(self, origin, found, right):\r\n        recall = 0 if origin == 0 else (right / origin)\r\n        precision = 0 if found == 0 else (right / found)\r\n        f1 = 0. if recall + precision == 0 else (2 * precision * recall) / (precision + recall)\r\n        return recall, precision, f1\r\n\r\n    def result(self):\r\n        class_info = {}\r\n        origin_counter = Counter([self.id2label[x[0]] for x in self.origins])\r\n        found_counter = Counter([self.id2label[x[0]] for x in self.founds])\r\n        right_counter = Counter([self.id2label[x[0]] for x in self.rights])\r\n        for type_, count in origin_counter.items():\r\n            origin = count\r\n            found = found_counter.get(type_, 0)\r\n            right = right_counter.get(type_, 0)\r\n            recall, precision, f1 = self.compute(origin, found, right)\r\n            class_info[type_] = {""acc"": round(precision, 4), \'recall\': round(recall, 4), \'f1\': round(f1, 4)}\r\n        origin = len(self.origins)\r\n        found = len(self.founds)\r\n        right = len(self.rights)\r\n        recall, precision, f1 = self.compute(origin, found, right)\r\n        return {\'acc\': precision, \'recall\': recall, \'f1\': f1}, class_info\r\n\r\n    def update(self, true_subject, pred_subject):\r\n        self.origins.extend(true_subject)\r\n        self.founds.extend(pred_subject)\r\n        self.rights.extend([pre_entity for pre_entity in pred_subject if pre_entity in true_subject])\r\n\r\n\r\n\r\n'"
models/__init__.py,0,b'\n\n'
models/albert_for_ner.py,5,"b""import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom .layers.crf import CRF\r\nfrom .transformers.modeling_albert import AlbertPreTrainedModel\r\nfrom .transformers.modeling_albert import AlbertModel\r\nfrom .layers.linears import PoolerEndLogits, PoolerStartLogits\r\nfrom torch.nn import CrossEntropyLoss\r\nfrom losses.focal_loss import FocalLoss\r\nfrom losses.label_smoothing import LabelSmoothingCrossEntropy\r\n\r\nclass AlbertSoftmaxForNer(AlbertPreTrainedModel):\r\n    def __init__(self, config):\r\n        super(AlbertSoftmaxForNer, self).__init__(config)\r\n        self.num_labels = config.num_labels\r\n        self.loss_type = config.loss_type\r\n        self.bert = AlbertModel(config)\r\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\r\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\r\n        self.init_weights()\r\n\r\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None,\r\n                position_ids=None, head_mask=None, labels=None):\r\n        outputs = self.bert(input_ids = input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids,\r\n                            position_ids=position_ids,head_mask=head_mask)\r\n        sequence_output = outputs[0]\r\n        sequence_output = self.dropout(sequence_output)\r\n        logits = self.classifier(sequence_output)\r\n        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\r\n        if labels is not None:\r\n            assert self.loss_type in ['lsr', 'focal', 'ce']\r\n            if self.loss_type =='lsr':\r\n                loss_fct = LabelSmoothingCrossEntropy(ignore_index=0)\r\n            elif self.loss_type == 'focal':\r\n                loss_fct = FocalLoss(ignore_index=0)\r\n            else:\r\n                loss_fct = CrossEntropyLoss(ignore_index=0)\r\n            # Only keep active parts of the loss\r\n            if attention_mask is not None:\r\n                active_loss = attention_mask.view(-1) == 1\r\n                active_logits = logits.view(-1, self.num_labels)[active_loss]\r\n                active_labels = labels.view(-1)[active_loss]\r\n                loss = loss_fct(active_logits, active_labels)\r\n            else:\r\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\r\n            outputs = (loss,) + outputs\r\n        return outputs  # (loss), scores, (hidden_states), (attentions)\r\n\r\nclass AlbertCrfForNer(AlbertPreTrainedModel):\r\n    def __init__(self, config):\r\n        super(AlbertCrfForNer, self).__init__(config)\r\n        self.bert = AlbertModel(config)\r\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\r\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\r\n        self.crf = CRF(num_tags=config.num_labels, batch_first=True)\r\n        self.init_weights()\r\n\r\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None,labels=None,input_lens=None):\r\n        outputs = self.bert(input_ids = input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)\r\n        sequence_output = outputs[0]\r\n        sequence_output = self.dropout(sequence_output)\r\n        logits = self.classifier(sequence_output)\r\n        outputs = (logits,)\r\n        if labels is not None:\r\n            loss = self.crf(emissions = logits, tags=labels, mask=attention_mask)\r\n            outputs =(-1*loss,)+outputs\r\n        return outputs # (loss), scores\r\n\r\nclass AlbertSpanForNer(AlbertPreTrainedModel):\r\n    def __init__(self, config,):\r\n        super(AlbertSpanForNer, self).__init__(config)\r\n        self.soft_label = config.soft_label\r\n        self.num_labels = config.num_labels\r\n        self.loss_type = config.loss_type\r\n        self.bert = AlbertModel(config)\r\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\r\n        self.start_fc = PoolerStartLogits(config.hidden_size, self.num_labels)\r\n        if self.soft_label:\r\n            self.end_fc = PoolerEndLogits(config.hidden_size + self.num_labels, self.num_labels)\r\n        else:\r\n            self.end_fc = PoolerEndLogits(config.hidden_size + 1, self.num_labels)\r\n        self.init_weights()\r\n\r\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, start_positions=None,end_positions=None):\r\n        outputs = self.bert(input_ids = input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)\r\n        sequence_output = outputs[0]\r\n        sequence_output = self.dropout(sequence_output)\r\n        start_logits = self.start_fc(sequence_output)\r\n        if start_positions is not None and self.training:\r\n            if self.soft_label:\r\n                batch_size = input_ids.size(0)\r\n                seq_len = input_ids.size(1)\r\n                label_logits = torch.FloatTensor(batch_size, seq_len, self.num_labels)\r\n                label_logits.zero_()\r\n                label_logits = label_logits.to(input_ids.device)\r\n                label_logits.scatter_(2, start_positions.unsqueeze(2), 1)\r\n            else:\r\n                label_logits = start_positions.unsqueeze(2).float()\r\n        else:\r\n            label_logits = F.softmax(start_logits, -1)\r\n            if not self.soft_label:\r\n                label_logits = torch.argmax(label_logits, -1).unsqueeze(2).float()\r\n        end_logits = self.end_fc(sequence_output, label_logits)\r\n        outputs = (start_logits, end_logits,) + outputs[2:]\r\n\r\n        if start_positions is not None and end_positions is not None:\r\n            assert self.loss_type in ['lsr','focal','ce']\r\n            if self.loss_type =='lsr':\r\n                loss_fct = LabelSmoothingCrossEntropy()\r\n            elif self.loss_type == 'focal':\r\n                loss_fct = FocalLoss()\r\n            else:\r\n                loss_fct = CrossEntropyLoss()\r\n            start_logits = start_logits.view(-1, self.num_labels)\r\n            end_logits = end_logits.view(-1, self.num_labels)\r\n            active_loss = attention_mask.view(-1) == 1\r\n            active_start_logits = start_logits[active_loss]\r\n            active_start_labels = start_positions.view(-1)[active_loss]\r\n            active_end_logits = end_logits[active_loss]\r\n            active_end_labels = end_positions.view(-1)[active_loss]\r\n\r\n            start_loss = loss_fct(active_start_logits, active_start_labels)\r\n            end_loss = loss_fct(active_end_logits, active_end_labels)\r\n            total_loss = (start_loss + end_loss) / 2\r\n            outputs = (total_loss,) + outputs\r\n        return outputs\r\n\r\n"""
models/bert_for_ner.py,5,"b""import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom .layers.crf import CRF\r\nfrom .transformers.modeling_bert import BertPreTrainedModel\r\nfrom .transformers.modeling_bert import BertModel\r\nfrom .layers.linears import PoolerEndLogits, PoolerStartLogits\r\nfrom torch.nn import CrossEntropyLoss\r\nfrom losses.focal_loss import FocalLoss\r\nfrom losses.label_smoothing import LabelSmoothingCrossEntropy\r\n\r\nclass BertSoftmaxForNer(BertPreTrainedModel):\r\n    def __init__(self, config):\r\n        super(BertSoftmaxForNer, self).__init__(config)\r\n        self.num_labels = config.num_labels\r\n        self.bert = BertModel(config)\r\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\r\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\r\n        self.loss_type = config.loss_type\r\n        self.init_weights()\r\n\r\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None,\r\n                position_ids=None, head_mask=None, labels=None):\r\n        outputs = self.bert(input_ids = input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)\r\n        sequence_output = outputs[0]\r\n        sequence_output = self.dropout(sequence_output)\r\n        logits = self.classifier(sequence_output)\r\n        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\r\n        if labels is not None:\r\n            assert self.loss_type in ['lsr', 'focal', 'ce']\r\n            if self.loss_type == 'lsr':\r\n                loss_fct = LabelSmoothingCrossEntropy(ignore_index=0)\r\n            elif self.loss_type == 'focal':\r\n                loss_fct = FocalLoss(ignore_index=0)\r\n            else:\r\n                loss_fct = CrossEntropyLoss(ignore_index=0)\r\n            # Only keep active parts of the loss\r\n            if attention_mask is not None:\r\n                active_loss = attention_mask.view(-1) == 1\r\n                active_logits = logits.view(-1, self.num_labels)[active_loss]\r\n                active_labels = labels.view(-1)[active_loss]\r\n                loss = loss_fct(active_logits, active_labels)\r\n            else:\r\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\r\n            outputs = (loss,) + outputs\r\n        return outputs  # (loss), scores, (hidden_states), (attentions)\r\n\r\nclass BertCrfForNer(BertPreTrainedModel):\r\n    def __init__(self, config):\r\n        super(BertCrfForNer, self).__init__(config)\r\n        self.bert = BertModel(config)\r\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\r\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\r\n        self.crf = CRF(num_tags=config.num_labels, batch_first=True)\r\n        self.init_weights()\r\n\r\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None,labels=None,input_lens=None):\r\n        outputs =self.bert(input_ids = input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)\r\n        sequence_output = outputs[0]\r\n        sequence_output = self.dropout(sequence_output)\r\n        logits = self.classifier(sequence_output)\r\n        outputs = (logits,)\r\n        if labels is not None:\r\n            loss = self.crf(emissions = logits, tags=labels, mask=attention_mask)\r\n            outputs =(-1*loss,)+outputs\r\n        return outputs # (loss), scores\r\n\r\nclass BertSpanForNer(BertPreTrainedModel):\r\n    def __init__(self, config,):\r\n        super(BertSpanForNer, self).__init__(config)\r\n        self.soft_label = config.soft_label\r\n        self.num_labels = config.num_labels\r\n        self.loss_type = config.loss_type\r\n        self.bert = BertModel(config)\r\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\r\n        self.start_fc = PoolerStartLogits(config.hidden_size, self.num_labels)\r\n        if self.soft_label:\r\n            self.end_fc = PoolerEndLogits(config.hidden_size + self.num_labels, self.num_labels)\r\n        else:\r\n            self.end_fc = PoolerEndLogits(config.hidden_size + 1, self.num_labels)\r\n        self.init_weights()\r\n\r\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, start_positions=None,end_positions=None):\r\n        outputs = self.bert(input_ids = input_ids,attention_mask=attention_mask,token_type_ids=token_type_ids)\r\n        sequence_output = outputs[0]\r\n        sequence_output = self.dropout(sequence_output)\r\n        start_logits = self.start_fc(sequence_output)\r\n        if start_positions is not None and self.training:\r\n            if self.soft_label:\r\n                batch_size = input_ids.size(0)\r\n                seq_len = input_ids.size(1)\r\n                label_logits = torch.FloatTensor(batch_size, seq_len, self.num_labels)\r\n                label_logits.zero_()\r\n                label_logits = label_logits.to(input_ids.device)\r\n                label_logits.scatter_(2, start_positions.unsqueeze(2), 1)\r\n            else:\r\n                label_logits = start_positions.unsqueeze(2).float()\r\n        else:\r\n            label_logits = F.softmax(start_logits, -1)\r\n            if not self.soft_label:\r\n                label_logits = torch.argmax(label_logits, -1).unsqueeze(2).float()\r\n        end_logits = self.end_fc(sequence_output, label_logits)\r\n        outputs = (start_logits, end_logits,) + outputs[2:]\r\n\r\n        if start_positions is not None and end_positions is not None:\r\n            assert self.loss_type in ['lsr', 'focal', 'ce']\r\n            if self.loss_type =='lsr':\r\n                loss_fct = LabelSmoothingCrossEntropy()\r\n            elif self.loss_type == 'focal':\r\n                loss_fct = FocalLoss()\r\n            else:\r\n                loss_fct = CrossEntropyLoss()\r\n            start_logits = start_logits.view(-1, self.num_labels)\r\n            end_logits = end_logits.view(-1, self.num_labels)\r\n            active_loss = attention_mask.view(-1) == 1\r\n            active_start_logits = start_logits[active_loss]\r\n            active_end_logits = end_logits[active_loss]\r\n\r\n            active_start_labels = start_positions.view(-1)[active_loss]\r\n            active_end_labels = end_positions.view(-1)[active_loss]\r\n\r\n            start_loss = loss_fct(active_start_logits, active_start_labels)\r\n            end_loss = loss_fct(active_end_logits, active_end_labels)\r\n            total_loss = (start_loss + end_loss) / 2\r\n            outputs = (total_loss,) + outputs\r\n        return outputs\r\n\r\n"""
processors/__init__.py,0,b'\n\n'
processors/ner_seq.py,1,"b'"""""" Named entity recognition fine-tuning: utilities to work with CLUENER task. """"""\nimport torch\nimport logging\nimport os\nimport copy\nimport json\nfrom .utils_ner import DataProcessor\nlogger = logging.getLogger(__name__)\n\nclass InputExample(object):\n    """"""A single training/test example for token classification.""""""\n    def __init__(self, guid, text_a, labels):\n        """"""Constructs a InputExample.\n        Args:\n            guid: Unique id for the example.\n            text_a: list. The words of the sequence.\n            labels: (Optional) list. The labels for each word of the sequence. This should be\n            specified for train and dev examples, but not for test examples.\n        """"""\n        self.guid = guid\n        self.text_a = text_a\n        self.labels = labels\n\n    def __repr__(self):\n        return str(self.to_json_string())\n    def to_dict(self):\n        """"""Serializes this instance to a Python dictionary.""""""\n        output = copy.deepcopy(self.__dict__)\n        return output\n    def to_json_string(self):\n        """"""Serializes this instance to a JSON string.""""""\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\n\nclass InputFeatures(object):\n    """"""A single set of features of data.""""""\n    def __init__(self, input_ids, input_mask, input_len,segment_ids, label_ids):\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.label_ids = label_ids\n        self.input_len = input_len\n\n    def __repr__(self):\n        return str(self.to_json_string())\n\n    def to_dict(self):\n        """"""Serializes this instance to a Python dictionary.""""""\n        output = copy.deepcopy(self.__dict__)\n        return output\n\n    def to_json_string(self):\n        """"""Serializes this instance to a JSON string.""""""\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\n\ndef collate_fn(batch):\n    """"""\n    batch should be a list of (sequence, target, length) tuples...\n    Returns a padded tensor of sequences sorted from longest to shortest,\n    """"""\n    all_input_ids, all_attention_mask, all_token_type_ids, all_lens, all_labels = map(torch.stack, zip(*batch))\n    max_len = max(all_lens).item()\n    all_input_ids = all_input_ids[:, :max_len]\n    all_attention_mask = all_attention_mask[:, :max_len]\n    all_token_type_ids = all_token_type_ids[:, :max_len]\n    all_labels = all_labels[:,:max_len]\n    return all_input_ids, all_attention_mask, all_token_type_ids, all_labels,all_lens\n\ndef convert_examples_to_features(examples,label_list,max_seq_length,tokenizer,\n                                 cls_token_at_end=False,cls_token=""[CLS]"",cls_token_segment_id=1,\n                                 sep_token=""[SEP]"",pad_on_left=False,pad_token=0,pad_token_segment_id=0,\n                                 sequence_a_segment_id=0,mask_padding_with_zero=True,):\n    """""" Loads a data file into a list of `InputBatch`s\n        `cls_token_at_end` define the location of the CLS token:\n            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n    """"""\n    label_map = {label: i for i, label in enumerate(label_list)}\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            logger.info(""Writing example %d of %d"", ex_index, len(examples))\n        tokens = tokenizer.tokenize(example.text_a)\n        label_ids = [label_map[x] for x in example.labels]\n        # Account for [CLS] and [SEP] with ""- 2"".\n        special_tokens_count = 2\n        if len(tokens) > max_seq_length - special_tokens_count:\n            tokens = tokens[: (max_seq_length - special_tokens_count)]\n            label_ids = label_ids[: (max_seq_length - special_tokens_count)]\n\n        # The convention in BERT is:\n        # (a) For sequence pairs:\n        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n        #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n        # (b) For single sequences:\n        #  tokens:   [CLS] the dog is hairy . [SEP]\n        #  type_ids:   0   0   0   0  0     0   0\n        #\n        # Where ""type_ids"" are used to indicate whether this is the first\n        # sequence or the second sequence. The embedding vectors for `type=0` and\n        # `type=1` were learned during pre-training and are added to the wordpiece\n        # embedding vector (and position vector). This is not *strictly* necessary\n        # since the [SEP] token unambiguously separates the sequences, but it makes\n        # it easier for the model to learn the concept of sequences.\n        #\n        # For classification tasks, the first vector (corresponding to [CLS]) is\n        # used as as the ""sentence vector"". Note that this only makes sense because\n        # the entire model is fine-tuned.\n        tokens += [sep_token]\n        label_ids += [label_map[\'O\']]\n        segment_ids = [sequence_a_segment_id] * len(tokens)\n\n        if cls_token_at_end:\n            tokens += [cls_token]\n            label_ids += [label_map[\'O\']]\n            segment_ids += [cls_token_segment_id]\n        else:\n            tokens = [cls_token] + tokens\n            label_ids = [label_map[\'O\']] + label_ids\n            segment_ids = [cls_token_segment_id] + segment_ids\n\n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n        # tokens are attended to.\n        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n        input_len = len(label_ids)\n        # Zero-pad up to the sequence length.\n        padding_length = max_seq_length - len(input_ids)\n        if pad_on_left:\n            input_ids = ([pad_token] * padding_length) + input_ids\n            input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n            segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n            label_ids = ([pad_token] * padding_length) + label_ids\n        else:\n            input_ids += [pad_token] * padding_length\n            input_mask += [0 if mask_padding_with_zero else 1] * padding_length\n            segment_ids += [pad_token_segment_id] * padding_length\n            label_ids += [pad_token] * padding_length\n\n        assert len(input_ids) == max_seq_length\n        assert len(input_mask) == max_seq_length\n        assert len(segment_ids) == max_seq_length\n        assert len(label_ids) == max_seq_length\n        if ex_index < 5:\n            logger.info(""*** Example ***"")\n            logger.info(""guid: %s"", example.guid)\n            logger.info(""tokens: %s"", "" "".join([str(x) for x in tokens]))\n            logger.info(""input_ids: %s"", "" "".join([str(x) for x in input_ids]))\n            logger.info(""input_mask: %s"", "" "".join([str(x) for x in input_mask]))\n            logger.info(""segment_ids: %s"", "" "".join([str(x) for x in segment_ids]))\n            logger.info(""label_ids: %s"", "" "".join([str(x) for x in label_ids]))\n\n        features.append(InputFeatures(input_ids=input_ids, input_mask=input_mask,input_len = input_len,\n                                      segment_ids=segment_ids, label_ids=label_ids))\n    return features\n\n\nclass CnerProcessor(DataProcessor):\n    """"""Processor for the chinese ner data set.""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(self._read_text(os.path.join(data_dir, ""train.char.bmes"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(self._read_text(os.path.join(data_dir, ""dev.char.bmes"")), ""dev"")\n\n    def get_test_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(self._read_text(os.path.join(data_dir, ""test.char.bmes"")), ""test"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""X"",\'B-CONT\',\'B-EDU\',\'B-LOC\',\'B-NAME\',\'B-ORG\',\'B-PRO\',\'B-RACE\',\'B-TITLE\',\n                \'I-CONT\',\'I-EDU\',\'I-LOC\',\'I-NAME\',\'I-ORG\',\'I-PRO\',\'I-RACE\',\'I-TITLE\',\n                \'O\',\'S-NAME\',\'S-ORG\',\'S-RACE\',""[START]"", ""[END]""]\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, i)\n            text_a= line[\'words\']\n            # BIOS\n            labels = []\n            for x in line[\'labels\']:\n                if \'M-\' in x:\n                    labels.append(x.replace(\'M-\',\'I-\'))\n                elif \'E-\' in x:\n                    labels.append(x.replace(\'E-\', \'I-\'))\n                else:\n                    labels.append(x)\n            examples.append(InputExample(guid=guid, text_a=text_a, labels=labels))\n        return examples\n\nclass CluenerProcessor(DataProcessor):\n    """"""Processor for the chinese ner data set.""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(self._read_json(os.path.join(data_dir, ""train.json"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(self._read_json(os.path.join(data_dir, ""dev.json"")), ""dev"")\n\n    def get_test_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(self._read_json(os.path.join(data_dir, ""test.json"")), ""test"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""X"", ""B-address"", ""B-book"", ""B-company"", \'B-game\', \'B-government\', \'B-movie\', \'B-name\',\n                \'B-organization\', \'B-position\',\'B-scene\',""I-address"",\n                ""I-book"", ""I-company"", \'I-game\', \'I-government\', \'I-movie\', \'I-name\',\n                \'I-organization\', \'I-position\',\'I-scene\',\n                ""S-address"", ""S-book"", ""S-company"", \'S-game\', \'S-government\', \'S-movie\',\n                \'S-name\', \'S-organization\', \'S-position\',\n                \'S-scene\',\'O\',""[START]"", ""[END]""]\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            guid = ""%s-%s"" % (set_type, i)\n            text_a= line[\'words\']\n            # BIOS\n            labels = line[\'labels\']\n            examples.append(InputExample(guid=guid, text_a=text_a, labels=labels))\n        return examples\n\nner_processors = {\n    ""cner"": CnerProcessor,\n    \'cluener\':CluenerProcessor\n}\n'"
processors/ner_span.py,1,"b'"""""" Named entity recognition fine-tuning: utilities to work with CoNLL-2003 task. """"""\r\nimport torch\r\nimport logging\r\nimport os\r\nimport copy\r\nimport json\r\nfrom .utils_ner import DataProcessor,get_entities\r\nlogger = logging.getLogger(__name__)\r\n\r\nclass InputExample(object):\r\n    """"""A single training/test example for token classification.""""""\r\n    def __init__(self, guid, text_a, subject):\r\n        self.guid = guid\r\n        self.text_a = text_a\r\n        self.subject = subject\r\n    def __repr__(self):\r\n        return str(self.to_json_string())\r\n    def to_dict(self):\r\n        """"""Serializes this instance to a Python dictionary.""""""\r\n        output = copy.deepcopy(self.__dict__)\r\n        return output\r\n    def to_json_string(self):\r\n        """"""Serializes this instance to a JSON string.""""""\r\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\r\n\r\nclass InputFeature(object):\r\n    """"""A single set of features of data.""""""\r\n\r\n    def __init__(self, input_ids, input_mask, input_len, segment_ids, start_ids,end_ids, subjects):\r\n        self.input_ids = input_ids\r\n        self.input_mask = input_mask\r\n        self.segment_ids = segment_ids\r\n        self.start_ids = start_ids\r\n        self.input_len = input_len\r\n        self.end_ids = end_ids\r\n        self.subjects = subjects\r\n\r\n    def __repr__(self):\r\n        return str(self.to_json_string())\r\n\r\n    def to_dict(self):\r\n        """"""Serializes this instance to a Python dictionary.""""""\r\n        output = copy.deepcopy(self.__dict__)\r\n        return output\r\n\r\n    def to_json_string(self):\r\n        """"""Serializes this instance to a JSON string.""""""\r\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\r\n\r\ndef collate_fn(batch):\r\n    """"""\r\n    batch should be a list of (sequence, target, length) tuples...\r\n    Returns a padded tensor of sequences sorted from longest to shortest,\r\n    """"""\r\n    all_input_ids, all_input_mask, all_segment_ids, all_start_ids,all_end_ids,all_lens = map(torch.stack, zip(*batch))\r\n    max_len = max(all_lens).item()\r\n    all_input_ids = all_input_ids[:, :max_len]\r\n    all_input_mask = all_input_mask[:, :max_len]\r\n    all_segment_ids = all_segment_ids[:, :max_len]\r\n    all_start_ids = all_start_ids[:,:max_len]\r\n    all_end_ids = all_end_ids[:, :max_len]\r\n    return all_input_ids, all_input_mask, all_segment_ids, all_start_ids,all_end_ids,all_lens\r\n\r\ndef convert_examples_to_features(examples,label_list,max_seq_length,tokenizer,\r\n                                 cls_token_at_end=False,cls_token=""[CLS]"",cls_token_segment_id=1,\r\n                                 sep_token=""[SEP]"",pad_on_left=False,pad_token=0,pad_token_segment_id=0,\r\n                                 sequence_a_segment_id=0,mask_padding_with_zero=True,):\r\n    """""" Loads a data file into a list of `InputBatch`s\r\n        `cls_token_at_end` define the location of the CLS token:\r\n            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\r\n            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\r\n        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\r\n    """"""\r\n    label2id = {label: i for i, label in enumerate(label_list)}\r\n    features = []\r\n    for (ex_index, example) in enumerate(examples):\r\n        if ex_index % 10000 == 0:\r\n            logger.info(""Writing example %d of %d"", ex_index, len(examples))\r\n        textlist = example.text_a\r\n        subjects = example.subject\r\n        tokens = tokenizer.tokenize(textlist)\r\n        start_ids = [0] * len(tokens)\r\n        end_ids = [0] * len(tokens)\r\n        subjects_id = []\r\n        for subject in subjects:\r\n            label = subject[0]\r\n            start = subject[1]\r\n            end = subject[2]\r\n            start_ids[start] = label2id[label]\r\n            end_ids[end] = label2id[label]\r\n            subjects_id.append((label2id[label], start, end))\r\n        # Account for [CLS] and [SEP] with ""- 2"".\r\n        special_tokens_count = 2\r\n        if len(tokens) > max_seq_length - special_tokens_count:\r\n            tokens = tokens[: (max_seq_length - special_tokens_count)]\r\n            start_ids = start_ids[: (max_seq_length - special_tokens_count)]\r\n            end_ids = end_ids[: (max_seq_length - special_tokens_count)]\r\n\r\n        # The convention in BERT is:\r\n        # (a) For sequence pairs:\r\n        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\r\n        #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\r\n        # (b) For single sequences:\r\n        #  tokens:   [CLS] the dog is hairy . [SEP]\r\n        #  type_ids:   0   0   0   0  0     0   0\r\n        #\r\n        # Where ""type_ids"" are used to indicate whether this is the first\r\n        # sequence or the second sequence. The embedding vectors for `type=0` and\r\n        # `type=1` were learned during pre-training and are added to the wordpiece\r\n        # embedding vector (and position vector). This is not *strictly* necessary\r\n        # since the [SEP] token unambiguously separates the sequences, but it makes\r\n        # it easier for the model to learn the concept of sequences.\r\n        #\r\n        # For classification tasks, the first vector (corresponding to [CLS]) is\r\n        # used as as the ""sentence vector"". Note that this only makes sense because\r\n        # the entire model is fine-tuned.\r\n        tokens += [sep_token]\r\n        start_ids += [0]\r\n        end_ids += [0]\r\n        segment_ids = [sequence_a_segment_id] * len(tokens)\r\n        if cls_token_at_end:\r\n            tokens += [cls_token]\r\n            start_ids += [0]\r\n            end_ids += [0]\r\n            segment_ids += [cls_token_segment_id]\r\n        else:\r\n            tokens = [cls_token] + tokens\r\n            start_ids = [0]+ start_ids\r\n            end_ids = [0]+ end_ids\r\n            segment_ids = [cls_token_segment_id] + segment_ids\r\n\r\n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\r\n        # The mask has 1 for real tokens and 0 for padding tokens. Only real\r\n        # tokens are attended to.\r\n        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\r\n        input_len = len(input_ids)\r\n        # Zero-pad up to the sequence length.\r\n        padding_length = max_seq_length - len(input_ids)\r\n        if pad_on_left:\r\n            input_ids = ([pad_token] * padding_length) + input_ids\r\n            input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\r\n            segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\r\n            start_ids = ([0] * padding_length) + start_ids\r\n            end_ids = ([0] * padding_length) + end_ids\r\n        else:\r\n            input_ids += [pad_token] * padding_length\r\n            input_mask += [0 if mask_padding_with_zero else 1] * padding_length\r\n            segment_ids += [pad_token_segment_id] * padding_length\r\n            start_ids += ([0] * padding_length)\r\n            end_ids += ([0] * padding_length)\r\n\r\n        assert len(input_ids) == max_seq_length\r\n        assert len(input_mask) == max_seq_length\r\n        assert len(segment_ids) == max_seq_length\r\n        assert len(start_ids) == max_seq_length\r\n        assert len(end_ids) == max_seq_length\r\n\r\n        if ex_index < 5:\r\n            logger.info(""*** Example ***"")\r\n            logger.info(""guid: %s"", example.guid)\r\n            logger.info(""tokens: %s"", "" "".join([str(x) for x in tokens]))\r\n            logger.info(""input_ids: %s"", "" "".join([str(x) for x in input_ids]))\r\n            logger.info(""input_mask: %s"", "" "".join([str(x) for x in input_mask]))\r\n            logger.info(""segment_ids: %s"", "" "".join([str(x) for x in segment_ids]))\r\n            logger.info(""start_ids: %s"" % "" "".join([str(x) for x in start_ids]))\r\n            logger.info(""end_ids: %s"" % "" "".join([str(x) for x in end_ids]))\r\n\r\n        features.append(InputFeature(input_ids=input_ids,\r\n                                  input_mask=input_mask,\r\n                                  segment_ids=segment_ids,\r\n                                  start_ids=start_ids,\r\n                                  end_ids=end_ids,\r\n                                  subjects=subjects_id,\r\n                                  input_len=input_len))\r\n    return features\r\n\r\nclass CnerProcessor(DataProcessor):\r\n    """"""Processor for the chinese ner data set.""""""\r\n\r\n    def get_train_examples(self, data_dir):\r\n        """"""See base class.""""""\r\n        return self._create_examples(self._read_text(os.path.join(data_dir, ""train.char.bmes"")), ""train"")\r\n\r\n    def get_dev_examples(self, data_dir):\r\n        """"""See base class.""""""\r\n        return self._create_examples(self._read_text(os.path.join(data_dir, ""dev.char.bmes"")), ""dev"")\r\n\r\n    def get_test_examples(self, data_dir):\r\n        """"""See base class.""""""\r\n        return self._create_examples(self._read_text(os.path.join(data_dir, ""test.char.bmes"")), ""test"")\r\n\r\n    def get_labels(self):\r\n        """"""See base class.""""""\r\n        return [""O"", ""CONT"", ""ORG"",""LOC"",\'EDU\',\'NAME\',\'PRO\',\'RACE\',\'TITLE\']\r\n\r\n    def _create_examples(self, lines, set_type):\r\n        """"""Creates examples for the training and dev sets.""""""\r\n        examples = []\r\n        for (i, line) in enumerate(lines):\r\n            if i == 0:\r\n                continue\r\n            guid = ""%s-%s"" % (set_type, i)\r\n            text_a = line[\'words\']\r\n            labels = []\r\n            for x in line[\'labels\']:\r\n                if \'M-\' in x:\r\n                    labels.append(x.replace(\'M-\',\'I-\'))\r\n                elif \'E-\' in x:\r\n                    labels.append(x.replace(\'E-\', \'I-\'))\r\n                else:\r\n                    labels.append(x)\r\n            subject = get_entities(labels,id2label=None,markup=\'bios\')\r\n            examples.append(InputExample(guid=guid, text_a=text_a, subject=subject))\r\n        return examples\r\n\r\nclass CluenerProcessor(DataProcessor):\r\n    """"""Processor for the chinese ner data set.""""""\r\n\r\n    def get_train_examples(self, data_dir):\r\n        """"""See base class.""""""\r\n        return self._create_examples(self._read_json(os.path.join(data_dir, ""train.json"")), ""train"")\r\n\r\n    def get_dev_examples(self, data_dir):\r\n        """"""See base class.""""""\r\n        return self._create_examples(self._read_json(os.path.join(data_dir, ""dev.json"")), ""dev"")\r\n\r\n    def get_test_examples(self, data_dir):\r\n        """"""See base class.""""""\r\n        return self._create_examples(self._read_json(os.path.join(data_dir, ""test.json"")), ""test"")\r\n\r\n    def get_labels(self):\r\n        """"""See base class.""""""\r\n        return [""O"", ""address"", ""book"",""company"",\'game\',\'government\',\'movie\',\'name\',\'organization\',\'position\',\'scene\']\r\n\r\n    def _create_examples(self, lines, set_type):\r\n        """"""Creates examples for the training and dev sets.""""""\r\n        examples = []\r\n        for (i, line) in enumerate(lines):\r\n            guid = ""%s-%s"" % (set_type, i)\r\n            text_a = line[\'words\']\r\n            labels = line[\'labels\']\r\n            subject = get_entities(labels,id2label=None,markup=\'bios\')\r\n            examples.append(InputExample(guid=guid, text_a=text_a, subject=subject))\r\n        return examples\r\n\r\nner_processors = {\r\n    ""cner"": CnerProcessor,\r\n    \'cluener\':CluenerProcessor\r\n}\r\n\r\n\r\n'"
processors/utils_ner.py,2,"b'import csv\r\nimport json\r\nimport torch\r\nfrom models.transformers import BertTokenizer\r\n\r\nclass CNerTokenizer(BertTokenizer):\r\n    def __init__(self, vocab_file, do_lower_case=False):\r\n        super().__init__(vocab_file=str(vocab_file), do_lower_case=do_lower_case)\r\n        self.vocab_file = str(vocab_file)\r\n        self.do_lower_case = do_lower_case\r\n\r\n    def tokenize(self, text):\r\n        _tokens = []\r\n        for c in text:\r\n            if self.do_lower_case:\r\n                c = c.lower()\r\n            if c in self.vocab:\r\n                _tokens.append(c)\r\n            else:\r\n                _tokens.append(\'[UNK]\')\r\n        return _tokens\r\n\r\nclass DataProcessor(object):\r\n    """"""Base class for data converters for sequence classification data sets.""""""\r\n\r\n    def get_train_examples(self, data_dir):\r\n        """"""Gets a collection of `InputExample`s for the train set.""""""\r\n        raise NotImplementedError()\r\n\r\n    def get_dev_examples(self, data_dir):\r\n        """"""Gets a collection of `InputExample`s for the dev set.""""""\r\n        raise NotImplementedError()\r\n\r\n    def get_labels(self):\r\n        """"""Gets the list of labels for this data set.""""""\r\n        raise NotImplementedError()\r\n\r\n    @classmethod\r\n    def _read_tsv(cls, input_file, quotechar=None):\r\n        """"""Reads a tab separated value file.""""""\r\n        with open(input_file, ""r"", encoding=""utf-8-sig"") as f:\r\n            reader = csv.reader(f, delimiter=""\\t"", quotechar=quotechar)\r\n            lines = []\r\n            for line in reader:\r\n                lines.append(line)\r\n            return lines\r\n\r\n    @classmethod\r\n    def _read_text(self,input_file):\r\n        lines = []\r\n        with open(input_file,\'r\') as f:\r\n            words = []\r\n            labels = []\r\n            for line in f:\r\n                if line.startswith(""-DOCSTART-"") or line == """" or line == ""\\n"":\r\n                    if words:\r\n                        lines.append({""words"":words,""labels"":labels})\r\n                        words = []\r\n                        labels = []\r\n                else:\r\n                    splits = line.split("" "")\r\n                    words.append(splits[0])\r\n                    if len(splits) > 1:\r\n                        labels.append(splits[-1].replace(""\\n"", """"))\r\n                    else:\r\n                        # Examples could have no label for mode = ""test""\r\n                        labels.append(""O"")\r\n            if words:\r\n                lines.append({""words"":words,""labels"":labels})\r\n        return lines\r\n\r\n    @classmethod\r\n    def _read_json(self,input_file):\r\n        lines = []\r\n        with open(input_file,\'r\') as f:\r\n            for line in f:\r\n                line = json.loads(line.strip())\r\n                text = line[\'text\']\r\n                label_entities = line.get(\'label\',None)\r\n                words = list(text)\r\n                labels = [\'O\'] * len(words)\r\n                if label_entities is not None:\r\n                    for key,value in label_entities.items():\r\n                        for sub_name,sub_index in value.items():\r\n                            for start_index,end_index in sub_index:\r\n                                assert  \'\'.join(words[start_index:end_index+1]) == sub_name\r\n                                if start_index == end_index:\r\n                                    labels[start_index] = \'S-\'+key\r\n                                else:\r\n                                    labels[start_index] = \'B-\'+key\r\n                                    labels[start_index+1:end_index+1] = [\'I-\'+key]*(len(sub_name)-1)\r\n                lines.append({""words"": words, ""labels"": labels})\r\n        return lines\r\n\r\ndef get_entity_bios(seq,id2label):\r\n    """"""Gets entities from sequence.\r\n    note: BIOS\r\n    Args:\r\n        seq (list): sequence of labels.\r\n    Returns:\r\n        list: list of (chunk_type, chunk_start, chunk_end).\r\n    Example:\r\n        # >>> seq = [\'B-PER\', \'I-PER\', \'O\', \'S-LOC\']\r\n        # >>> get_entity_bios(seq)\r\n        [[\'PER\', 0,1], [\'LOC\', 3, 3]]\r\n    """"""\r\n    chunks = []\r\n    chunk = [-1, -1, -1]\r\n    for indx, tag in enumerate(seq):\r\n        if not isinstance(tag, str):\r\n            tag = id2label[tag]\r\n        if tag.startswith(""S-""):\r\n            if chunk[2] != -1:\r\n                chunks.append(chunk)\r\n            chunk = [-1, -1, -1]\r\n            chunk[1] = indx\r\n            chunk[2] = indx\r\n            chunk[0] = tag.split(\'-\')[1]\r\n            chunks.append(chunk)\r\n            chunk = (-1, -1, -1)\r\n        if tag.startswith(""B-""):\r\n            if chunk[2] != -1:\r\n                chunks.append(chunk)\r\n            chunk = [-1, -1, -1]\r\n            chunk[1] = indx\r\n            chunk[0] = tag.split(\'-\')[1]\r\n        elif tag.startswith(\'I-\') and chunk[1] != -1:\r\n            _type = tag.split(\'-\')[1]\r\n            if _type == chunk[0]:\r\n                chunk[2] = indx\r\n            if indx == len(seq) - 1:\r\n                chunks.append(chunk)\r\n        else:\r\n            if chunk[2] != -1:\r\n                chunks.append(chunk)\r\n            chunk = [-1, -1, -1]\r\n    return chunks\r\n\r\ndef get_entity_bio(seq,id2label):\r\n    """"""Gets entities from sequence.\r\n    note: BIO\r\n    Args:\r\n        seq (list): sequence of labels.\r\n    Returns:\r\n        list: list of (chunk_type, chunk_start, chunk_end).\r\n    Example:\r\n        seq = [\'B-PER\', \'I-PER\', \'O\', \'B-LOC\']\r\n        get_entity_bio(seq)\r\n        #output\r\n        [[\'PER\', 0,1], [\'LOC\', 3, 3]]\r\n    """"""\r\n    chunks = []\r\n    chunk = [-1, -1, -1]\r\n    for indx, tag in enumerate(seq):\r\n        if not isinstance(tag, str):\r\n            tag = id2label[tag]\r\n        if tag.startswith(""B-""):\r\n            if chunk[2] != -1:\r\n                chunks.append(chunk)\r\n            chunk = [-1, -1, -1]\r\n            chunk[1] = indx\r\n            chunk[0] = tag.split(\'-\')[1]\r\n            chunk[2] = indx\r\n            if indx == len(seq) - 1:\r\n                chunks.append(chunk)\r\n        elif tag.startswith(\'I-\') and chunk[1] != -1:\r\n            _type = tag.split(\'-\')[1]\r\n            if _type == chunk[0]:\r\n                chunk[2] = indx\r\n\r\n            if indx == len(seq) - 1:\r\n                chunks.append(chunk)\r\n        else:\r\n            if chunk[2] != -1:\r\n                chunks.append(chunk)\r\n            chunk = [-1, -1, -1]\r\n    return chunks\r\n\r\ndef get_entities(seq,id2label,markup=\'bios\'):\r\n    \'\'\'\r\n    :param seq:\r\n    :param id2label:\r\n    :param markup:\r\n    :return:\r\n    \'\'\'\r\n    assert markup in [\'bio\',\'bios\']\r\n    if markup ==\'bio\':\r\n        return get_entity_bio(seq,id2label)\r\n    else:\r\n        return get_entity_bios(seq,id2label)\r\n\r\ndef bert_extract_item(start_logits, end_logits):\r\n    S = []\r\n    start_pred = torch.argmax(start_logits, -1).cpu().numpy()[0][1:-1]\r\n    end_pred = torch.argmax(end_logits, -1).cpu().numpy()[0][1:-1]\r\n    for i, s_l in enumerate(start_pred):\r\n        if s_l == 0:\r\n            continue\r\n        for j, e_l in enumerate(end_pred[i:]):\r\n            if s_l == e_l:\r\n                S.append((s_l, i, i + j))\r\n                break\r\n    return S\r\n'"
tools/__init__.py,0,b'\n\n'
tools/common.py,14,"b'import os\r\nimport random\r\nimport torch\r\nimport numpy as np\r\nimport json\r\nimport pickle\r\nimport torch.nn as nn\r\nfrom collections import OrderedDict\r\nfrom pathlib import Path\r\nimport logging\r\n\r\nlogger = logging.getLogger()\r\ndef print_config(config):\r\n    info = ""Running with the following configs:\\n""\r\n    for k, v in config.items():\r\n        info += f""\\t{k} : {str(v)}\\n""\r\n    print(""\\n"" + info + ""\\n"")\r\n    return\r\n\r\ndef init_logger(log_file=None, log_file_level=logging.NOTSET):\r\n    \'\'\'\r\n    Example:\r\n        >>> init_logger(log_file)\r\n        >>> logger.info(""abc\'"")\r\n    \'\'\'\r\n    if isinstance(log_file,Path):\r\n        log_file = str(log_file)\r\n    log_format = logging.Formatter(fmt=\'%(asctime)s - %(levelname)s - %(name)s -   %(message)s\',\r\n                                   datefmt=\'%m/%d/%Y %H:%M:%S\')\r\n\r\n    logger = logging.getLogger()\r\n    logger.setLevel(logging.INFO)\r\n    console_handler = logging.StreamHandler()\r\n    console_handler.setFormatter(log_format)\r\n    logger.handlers = [console_handler]\r\n    if log_file and log_file != \'\':\r\n        file_handler = logging.FileHandler(log_file)\r\n        file_handler.setLevel(log_file_level)\r\n        # file_handler.setFormatter(log_format)\r\n        logger.addHandler(file_handler)\r\n    return logger\r\n\r\ndef seed_everything(seed=1029):\r\n    \'\'\'\r\n    \xe8\xae\xbe\xe7\xbd\xae\xe6\x95\xb4\xe4\xb8\xaa\xe5\xbc\x80\xe5\x8f\x91\xe7\x8e\xaf\xe5\xa2\x83\xe7\x9a\x84seed\r\n    :param seed:\r\n    :param device:\r\n    :return:\r\n    \'\'\'\r\n    random.seed(seed)\r\n    os.environ[\'PYTHONHASHSEED\'] = str(seed)\r\n    np.random.seed(seed)\r\n    torch.manual_seed(seed)\r\n    torch.cuda.manual_seed(seed)\r\n    torch.cuda.manual_seed_all(seed)\r\n    # some cudnn methods can be random even after fixing the seed\r\n    # unless you tell it to be deterministic\r\n    torch.backends.cudnn.deterministic = True\r\n\r\n\r\ndef prepare_device(n_gpu_use):\r\n    """"""\r\n    setup GPU device if available, move model into configured device\r\n    # \xe5\xa6\x82\xe6\x9e\x9cn_gpu_use\xe4\xb8\xba\xe6\x95\xb0\xe5\xad\x97\xef\xbc\x8c\xe5\x88\x99\xe4\xbd\xbf\xe7\x94\xa8range\xe7\x94\x9f\xe6\x88\x90list\r\n    # \xe5\xa6\x82\xe6\x9e\x9c\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaalist\xef\xbc\x8c\xe5\x88\x99\xe9\xbb\x98\xe8\xae\xa4\xe4\xbd\xbf\xe7\x94\xa8list[0]\xe4\xbd\x9c\xe4\xb8\xbacontroller\r\n     """"""\r\n    if not n_gpu_use:\r\n        device_type = \'cpu\'\r\n    else:\r\n        n_gpu_use = n_gpu_use.split("","")\r\n        device_type = f""cuda:{n_gpu_use[0]}""\r\n    n_gpu = torch.cuda.device_count()\r\n    if len(n_gpu_use) > 0 and n_gpu == 0:\r\n        logger.warning(""Warning: There\\\'s no GPU available on this machine, training will be performed on CPU."")\r\n        device_type = \'cpu\'\r\n    if len(n_gpu_use) > n_gpu:\r\n        msg = f""Warning: The number of GPU\\\'s configured to use is {n_gpu_use}, but only {n_gpu} are available on this machine.""\r\n        logger.warning(msg)\r\n        n_gpu_use = range(n_gpu)\r\n    device = torch.device(device_type)\r\n    list_ids = n_gpu_use\r\n    return device, list_ids\r\n\r\n\r\ndef model_device(n_gpu, model):\r\n    \'\'\'\r\n    \xe5\x88\xa4\xe6\x96\xad\xe7\x8e\xaf\xe5\xa2\x83 cpu\xe8\xbf\x98\xe6\x98\xafgpu\r\n    \xe6\x94\xaf\xe6\x8c\x81\xe5\x8d\x95\xe6\x9c\xba\xe5\xa4\x9a\xe5\x8d\xa1\r\n    :param n_gpu:\r\n    :param model:\r\n    :return:\r\n    \'\'\'\r\n    device, device_ids = prepare_device(n_gpu)\r\n    if len(device_ids) > 1:\r\n        logger.info(f""current {len(device_ids)} GPUs"")\r\n        model = torch.nn.DataParallel(model, device_ids=device_ids)\r\n    if len(device_ids) == 1:\r\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = str(device_ids[0])\r\n    model = model.to(device)\r\n    return model, device\r\n\r\n\r\ndef restore_checkpoint(resume_path, model=None):\r\n    \'\'\'\r\n    \xe5\x8a\xa0\xe8\xbd\xbd\xe6\xa8\xa1\xe5\x9e\x8b\r\n    :param resume_path:\r\n    :param model:\r\n    :param optimizer:\r\n    :return:\r\n    \xe6\xb3\xa8\xe6\x84\x8f\xef\xbc\x9a \xe5\xa6\x82\xe6\x9e\x9c\xe6\x98\xaf\xe5\x8a\xa0\xe8\xbd\xbdBert\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe8\xaf\x9d\xef\xbc\x8c\xe9\x9c\x80\xe8\xa6\x81\xe8\xb0\x83\xe6\x95\xb4\xef\xbc\x8c\xe4\xb8\x8d\xe8\x83\xbd\xe4\xbd\xbf\xe7\x94\xa8\xe8\xaf\xa5\xe6\xa8\xa1\xe5\xbc\x8f\r\n    \xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8\xe6\xa8\xa1\xe5\x9d\x97\xe8\x87\xaa\xe5\xb8\xa6\xe7\x9a\x84Bert_model.from_pretrained(state_dict = your save state_dict)\r\n    \'\'\'\r\n    if isinstance(resume_path, Path):\r\n        resume_path = str(resume_path)\r\n    checkpoint = torch.load(resume_path)\r\n    best = checkpoint[\'best\']\r\n    start_epoch = checkpoint[\'epoch\'] + 1\r\n    states = checkpoint[\'state_dict\']\r\n    if isinstance(model, nn.DataParallel):\r\n        model.module.load_state_dict(states)\r\n    else:\r\n        model.load_state_dict(states)\r\n    return [model,best,start_epoch]\r\n\r\n\r\ndef save_pickle(data, file_path):\r\n    \'\'\'\r\n    \xe4\xbf\x9d\xe5\xad\x98\xe6\x88\x90pickle\xe6\x96\x87\xe4\xbb\xb6\r\n    :param data:\r\n    :param file_name:\r\n    :param pickle_path:\r\n    :return:\r\n    \'\'\'\r\n    if isinstance(file_path, Path):\r\n        file_path = str(file_path)\r\n    with open(file_path, \'wb\') as f:\r\n        pickle.dump(data, f)\r\n\r\n\r\ndef load_pickle(input_file):\r\n    \'\'\'\r\n    \xe8\xaf\xbb\xe5\x8f\x96pickle\xe6\x96\x87\xe4\xbb\xb6\r\n    :param pickle_path:\r\n    :param file_name:\r\n    :return:\r\n    \'\'\'\r\n    with open(str(input_file), \'rb\') as f:\r\n        data = pickle.load(f)\r\n    return data\r\n\r\n\r\ndef save_json(data, file_path):\r\n    \'\'\'\r\n    \xe4\xbf\x9d\xe5\xad\x98\xe6\x88\x90json\xe6\x96\x87\xe4\xbb\xb6\r\n    :param data:\r\n    :param json_path:\r\n    :param file_name:\r\n    :return:\r\n    \'\'\'\r\n    if not isinstance(file_path, Path):\r\n        file_path = Path(file_path)\r\n    # if isinstance(data,dict):\r\n    #     data = json.dumps(data)\r\n    with open(str(file_path), \'w\') as f:\r\n        json.dump(data, f)\r\n\r\ndef save_numpy(data, file_path):\r\n    \'\'\'\r\n    \xe4\xbf\x9d\xe5\xad\x98\xe6\x88\x90.npy\xe6\x96\x87\xe4\xbb\xb6\r\n    :param data:\r\n    :param file_path:\r\n    :return:\r\n    \'\'\'\r\n    if not isinstance(file_path, Path):\r\n        file_path = Path(file_path)\r\n    np.save(str(file_path),data)\r\n\r\ndef load_numpy(file_path):\r\n    \'\'\'\r\n    \xe5\x8a\xa0\xe8\xbd\xbd.npy\xe6\x96\x87\xe4\xbb\xb6\r\n    :param file_path:\r\n    :return:\r\n    \'\'\'\r\n    if not isinstance(file_path, Path):\r\n        file_path = Path(file_path)\r\n    np.load(str(file_path))\r\n\r\ndef load_json(file_path):\r\n    \'\'\'\r\n    \xe5\x8a\xa0\xe8\xbd\xbdjson\xe6\x96\x87\xe4\xbb\xb6\r\n    :param json_path:\r\n    :param file_name:\r\n    :return:\r\n    \'\'\'\r\n    if not isinstance(file_path, Path):\r\n        file_path = Path(file_path)\r\n    with open(str(file_path), \'r\') as f:\r\n        data = json.load(f)\r\n    return data\r\n\r\ndef json_to_text(file_path,data):\r\n    \'\'\'\r\n    \xe5\xb0\x86json list\xe5\x86\x99\xe5\x85\xa5text\xe6\x96\x87\xe4\xbb\xb6\xe4\xb8\xad\r\n    :param file_path:\r\n    :param data:\r\n    :return:\r\n    \'\'\'\r\n    if not isinstance(file_path, Path):\r\n        file_path = Path(file_path)\r\n    with open(str(file_path), \'w\') as fw:\r\n        for line in data:\r\n            line = json.dumps(line, ensure_ascii=False)\r\n            fw.write(line + \'\\n\')\r\n\r\ndef save_model(model, model_path):\r\n    """""" \xe5\xad\x98\xe5\x82\xa8\xe4\xb8\x8d\xe5\x90\xab\xe6\x9c\x89\xe6\x98\xbe\xe5\x8d\xa1\xe4\xbf\xa1\xe6\x81\xaf\xe7\x9a\x84state_dict\xe6\x88\x96model\r\n    :param model:\r\n    :param model_name:\r\n    :param only_param:\r\n    :return:\r\n    """"""\r\n    if isinstance(model_path, Path):\r\n        model_path = str(model_path)\r\n    if isinstance(model, nn.DataParallel):\r\n        model = model.module\r\n    state_dict = model.state_dict()\r\n    for key in state_dict:\r\n        state_dict[key] = state_dict[key].cpu()\r\n    torch.save(state_dict, model_path)\r\n\r\ndef load_model(model, model_path):\r\n    \'\'\'\r\n    \xe5\x8a\xa0\xe8\xbd\xbd\xe6\xa8\xa1\xe5\x9e\x8b\r\n    :param model:\r\n    :param model_name:\r\n    :param model_path:\r\n    :param only_param:\r\n    :return:\r\n    \'\'\'\r\n    if isinstance(model_path, Path):\r\n        model_path = str(model_path)\r\n    logging.info(f""loading model from {str(model_path)} ."")\r\n    states = torch.load(model_path)\r\n    state = states[\'state_dict\']\r\n    if isinstance(model, nn.DataParallel):\r\n        model.module.load_state_dict(state)\r\n    else:\r\n        model.load_state_dict(state)\r\n    return model\r\n\r\n\r\nclass AverageMeter(object):\r\n    \'\'\'\r\n    computes and stores the average and current value\r\n    Example:\r\n        >>> loss = AverageMeter()\r\n        >>> for step,batch in enumerate(train_data):\r\n        >>>     pred = self.model(batch)\r\n        >>>     raw_loss = self.metrics(pred,target)\r\n        >>>     loss.update(raw_loss.item(),n = 1)\r\n        >>> cur_loss = loss.avg\r\n    \'\'\'\r\n\r\n    def __init__(self):\r\n        self.reset()\r\n\r\n    def reset(self):\r\n        self.val = 0\r\n        self.avg = 0\r\n        self.sum = 0\r\n        self.count = 0\r\n\r\n    def update(self, val, n=1):\r\n        self.val = val\r\n        self.sum += val * n\r\n        self.count += n\r\n        self.avg = self.sum / self.count\r\n\r\n\r\ndef summary(model, *inputs, batch_size=-1, show_input=True):\r\n    \'\'\'\r\n    \xe6\x89\x93\xe5\x8d\xb0\xe6\xa8\xa1\xe5\x9e\x8b\xe7\xbb\x93\xe6\x9e\x84\xe4\xbf\xa1\xe6\x81\xaf\r\n    :param model:\r\n    :param inputs:\r\n    :param batch_size:\r\n    :param show_input:\r\n    :return:\r\n    Example:\r\n        >>> print(""model summary info: "")\r\n        >>> for step,batch in enumerate(train_data):\r\n        >>>     summary(self.model,*batch,show_input=True)\r\n        >>>     break\r\n    \'\'\'\r\n\r\n    def register_hook(module):\r\n        def hook(module, input, output=None):\r\n            class_name = str(module.__class__).split(""."")[-1].split(""\'"")[0]\r\n            module_idx = len(summary)\r\n\r\n            m_key = f""{class_name}-{module_idx + 1}""\r\n            summary[m_key] = OrderedDict()\r\n            summary[m_key][""input_shape""] = list(input[0].size())\r\n            summary[m_key][""input_shape""][0] = batch_size\r\n\r\n            if show_input is False and output is not None:\r\n                if isinstance(output, (list, tuple)):\r\n                    for out in output:\r\n                        if isinstance(out, torch.Tensor):\r\n                            summary[m_key][""output_shape""] = [\r\n                                [-1] + list(out.size())[1:]\r\n                            ][0]\r\n                        else:\r\n                            summary[m_key][""output_shape""] = [\r\n                                [-1] + list(out[0].size())[1:]\r\n                            ][0]\r\n                else:\r\n                    summary[m_key][""output_shape""] = list(output.size())\r\n                    summary[m_key][""output_shape""][0] = batch_size\r\n\r\n            params = 0\r\n            if hasattr(module, ""weight"") and hasattr(module.weight, ""size""):\r\n                params += torch.prod(torch.LongTensor(list(module.weight.size())))\r\n                summary[m_key][""trainable""] = module.weight.requires_grad\r\n            if hasattr(module, ""bias"") and hasattr(module.bias, ""size""):\r\n                params += torch.prod(torch.LongTensor(list(module.bias.size())))\r\n            summary[m_key][""nb_params""] = params\r\n\r\n        if (not isinstance(module, nn.Sequential) and not isinstance(module, nn.ModuleList) and not (module == model)):\r\n            if show_input is True:\r\n                hooks.append(module.register_forward_pre_hook(hook))\r\n            else:\r\n                hooks.append(module.register_forward_hook(hook))\r\n\r\n    # create properties\r\n    summary = OrderedDict()\r\n    hooks = []\r\n\r\n    # register hook\r\n    model.apply(register_hook)\r\n    model(*inputs)\r\n\r\n    # remove these hooks\r\n    for h in hooks:\r\n        h.remove()\r\n\r\n    print(""-----------------------------------------------------------------------"")\r\n    if show_input is True:\r\n        line_new = f""{\'Layer (type)\':>25}  {\'Input Shape\':>25} {\'Param #\':>15}""\r\n    else:\r\n        line_new = f""{\'Layer (type)\':>25}  {\'Output Shape\':>25} {\'Param #\':>15}""\r\n    print(line_new)\r\n    print(""======================================================================="")\r\n\r\n    total_params = 0\r\n    total_output = 0\r\n    trainable_params = 0\r\n    for layer in summary:\r\n        # input_shape, output_shape, trainable, nb_params\r\n        if show_input is True:\r\n            line_new = ""{:>25}  {:>25} {:>15}"".format(\r\n                layer,\r\n                str(summary[layer][""input_shape""]),\r\n                ""{0:,}"".format(summary[layer][""nb_params""]),\r\n            )\r\n        else:\r\n            line_new = ""{:>25}  {:>25} {:>15}"".format(\r\n                layer,\r\n                str(summary[layer][""output_shape""]),\r\n                ""{0:,}"".format(summary[layer][""nb_params""]),\r\n            )\r\n\r\n        total_params += summary[layer][""nb_params""]\r\n        if show_input is True:\r\n            total_output += np.prod(summary[layer][""input_shape""])\r\n        else:\r\n            total_output += np.prod(summary[layer][""output_shape""])\r\n        if ""trainable"" in summary[layer]:\r\n            if summary[layer][""trainable""] == True:\r\n                trainable_params += summary[layer][""nb_params""]\r\n\r\n        print(line_new)\r\n\r\n    print(""======================================================================="")\r\n    print(f""Total params: {total_params:0,}"")\r\n    print(f""Trainable params: {trainable_params:0,}"")\r\n    print(f""Non-trainable params: {(total_params - trainable_params):0,}"")\r\n    print(""-----------------------------------------------------------------------"")'"
tools/convert_albert_tf_checkpoint_to_pytorch.py,3,"b'""""""Convert ALBERT checkpoint.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport torch\nfrom models.transformers.modeling_albert import AlbertConfig, AlbertForPreTraining, load_tf_weights_in_albert\n# from model.modeling_albert_bright import AlbertConfig, AlbertForPreTraining, load_tf_weights_in_albert\nimport logging\nlogging.basicConfig(level=logging.INFO)\n\ndef convert_tf_checkpoint_to_pytorch(tf_checkpoint_path, bert_config_file, pytorch_dump_path):\n    # Initialise PyTorch model\n    config = AlbertConfig.from_pretrained(bert_config_file)\n    # print(""Building PyTorch model from configuration: {}"".format(str(config)))\n    model = AlbertForPreTraining(config)\n    # Load weights from tf checkpoint\n    load_tf_weights_in_albert(model, config, tf_checkpoint_path)\n\n    # Save pytorch-model\n    print(""Save PyTorch model to {}"".format(pytorch_dump_path))\n    torch.save(model.state_dict(), pytorch_dump_path)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    ## Required parameters\n    parser.add_argument(""--tf_checkpoint_path"",\n                        default = None,\n                        type = str,\n                        required = True,\n                        help = ""Path to the TensorFlow checkpoint path."")\n    parser.add_argument(""--bert_config_file"",\n                        default = None,\n                        type = str,\n                        required = True,\n                        help = ""The config json file corresponding to the pre-trained BERT model. \\n""\n                            ""This specifies the model architecture."")\n    parser.add_argument(""--pytorch_dump_path"",\n                        default = None,\n                        type = str,\n                        required = True,\n                        help = ""Path to the output PyTorch model."")\n    args = parser.parse_args()\n    convert_tf_checkpoint_to_pytorch(args.tf_checkpoint_path,args.bert_config_file,\n                                     args.pytorch_dump_path)\n\n\'\'\'\npython convert_albert_tf_checkpoint_to_pytorch.py \\\n    --tf_checkpoint_path=./prev_trained_model/albert_large_zh \\\n    --bert_config_file=./prev_trained_model/albert_large_zh/config.json \\\n    --pytorch_dump_path=./prev_trained_model/albert_large_zh/pytorch_model.bin\n    \n\nfrom model.modeling_albert_bright import AlbertConfig, AlbertForPreTraining, load_tf_weights_in_albert\npython convert_albert_tf_checkpoint_to_pytorch.py \\\n    --tf_checkpoint_path=./prev_trained_model/albert_base_bright \\\n    --bert_config_file=./prev_trained_model/albert_base_bright/config.json \\\n    --pytorch_dump_path=./prev_trained_model/albert_base_bright/pytorch_model.bin\n\'\'\''"
tools/download_clue_data.py,0,"b'"""""" Script for downloading all CLUE data.\r\nFor licence information, see the original dataset information links\r\navailable from: https://www.cluebenchmarks.com/\r\nExample usage:\r\n  python download_clue_data.py --data_dir data --tasks all\r\n""""""\r\n\r\nimport os\r\nimport sys\r\nimport argparse\r\nimport urllib.request\r\nimport zipfile\r\n\r\nTASKS = [""afqmc"", ""cmnli"", ""copa"", ""csl"", ""iflytek"", ""tnews"", ""wsc"",""cmrc"",""chid"",""drcd"",\'cluener\']\r\n\r\nTASK2PATH = {\r\n    ""afqmc"": ""https://storage.googleapis.com/cluebenchmark/tasks/afqmc_public.zip"",\r\n    ""cmnli"": ""https://storage.googleapis.com/cluebenchmark/tasks/cmnli_public.zip"",\r\n    ""copa"": ""https://storage.googleapis.com/cluebenchmark/tasks/copa_public.zip"",\r\n    ""csl"": ""https://storage.googleapis.com/cluebenchmark/tasks/csl_public.zip"",\r\n    ""iflytek"": ""https://storage.googleapis.com/cluebenchmark/tasks/iflytek_public.zip"",\r\n    ""tnews"": ""https://storage.googleapis.com/cluebenchmark/tasks/tnews_public.zip"",\r\n    ""wsc"": ""https://storage.googleapis.com/cluebenchmark/tasks/wsc_public.zip"",\r\n    \'cmrc\': ""https://storage.googleapis.com/cluebenchmark/tasks/cmrc2018_public.zip"",\r\n    ""chid"": ""https://storage.googleapis.com/cluebenchmark/tasks/chid_public.zip"",\r\n    ""drcd"": ""https://storage.googleapis.com/cluebenchmark/tasks/drcd_public.zip"",\r\n    \'cluener\':\'https://storage.googleapis.com/cluebenchmark/tasks/cluener_public.zip\'\r\n}\r\n\r\ndef download_and_extract(task, data_dir):\r\n    print(""Downloading and extracting %s..."" % task)\r\n    if not os.path.isdir(data_dir):\r\n        os.mkdir(data_dir)\r\n    data_file = os.path.join(data_dir, ""%s_public.zip"" % task)\r\n    save_dir = os.path.join(data_dir,task)\r\n    if not os.path.isdir(save_dir):\r\n        os.mkdir(save_dir)\r\n    urllib.request.urlretrieve(TASK2PATH[task], data_file)\r\n    with zipfile.ZipFile(data_file) as zip_ref:\r\n        zip_ref.extractall(save_dir)\r\n    os.remove(data_file)\r\n    print(f""\\tCompleted! Downloaded {task} data to directory {save_dir}"")\r\n\r\ndef get_tasks(task_names):\r\n    task_names = task_names.split("","")\r\n    if ""all"" in task_names:\r\n        tasks = TASKS\r\n    else:\r\n        tasks = []\r\n        for task_name in task_names:\r\n            assert task_name in TASKS, ""Task %s not found!"" % task_name\r\n            tasks.append(task_name)\r\n    return tasks\r\n\r\ndef main(arguments):\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\r\n        ""-d"", ""--data_dir"", help=""directory to save data to"", type=str, default=""../CLUEdatasets""\r\n    )\r\n    parser.add_argument(\r\n        ""-t"",\r\n        ""--tasks"",\r\n        help=""tasks to download data for as a comma separated string"",\r\n        type=str,\r\n        default=""all"",\r\n    )\r\n    args = parser.parse_args(arguments)\r\n\r\n    if not os.path.exists(args.data_dir):\r\n        os.mkdir(args.data_dir)\r\n    tasks = get_tasks(args.tasks)\r\n\r\n    for task in tasks:\r\n        download_and_extract(task, args.data_dir)\r\n\r\nif __name__ == ""__main__"":\r\n    sys.exit(main(sys.argv[1:]))\r\n\r\n\'\'\'\r\npython tools/download_clue_data.py --data_dir=./CLUEdatasets --tasks=cluener\r\n\'\'\''"
tools/finetuning_argparse.py,0,"b'import argparse\n\ndef get_argparse():\n    parser = argparse.ArgumentParser()\n    # Required parameters\n    parser.add_argument(""--task_name"", default=None, type=str, required=True,\n                        help=""The name of the task to train selected in the list: "")\n    parser.add_argument(""--data_dir"", default=None, type=str, required=True,\n                        help=""The input data dir. Should contain the training files for the CoNLL-2003 NER task."", )\n    parser.add_argument(""--model_type"", default=None, type=str, required=True,\n                        help=""Model type selected in the list: "")\n    parser.add_argument(""--model_name_or_path"", default=None, type=str, required=True,\n                        help=""Path to pre-trained model or shortcut name selected in the list: "" )\n    parser.add_argument(""--output_dir"", default=None, type=str, required=True,\n                        help=""The output directory where the model predictions and checkpoints will be written."", )\n\n    # Other parameters\n    parser.add_argument(\'--markup\', default=\'bios\', type=str,\n                        choices=[\'bios\', \'bio\'])\n    parser.add_argument(\'--loss_type\', default=\'ce\', type=str,\n                        choices=[\'lsr\', \'focal\', \'ce\'])\n    parser.add_argument(""--config_name"", default="""", type=str,\n                        help=""Pretrained config name or path if not the same as model_name"")\n    parser.add_argument(""--tokenizer_name"", default="""", type=str,\n                        help=""Pretrained tokenizer name or path if not the same as model_name"", )\n    parser.add_argument(""--cache_dir"", default="""", type=str,\n                        help=""Where do you want to store the pre-trained models downloaded from s3"", )\n    parser.add_argument(""--train_max_seq_length"", default=128, type=int,\n                        help=""The maximum total input sequence length after tokenization. Sequences longer ""\n                             ""than this will be truncated, sequences shorter will be padded."", )\n    parser.add_argument(""--eval_max_seq_length"", default=512, type=int,\n                        help=""The maximum total input sequence length after tokenization. Sequences longer ""\n                             ""than this will be truncated, sequences shorter will be padded."", )\n    parser.add_argument(""--do_train"", action=""store_true"",\n                        help=""Whether to run training."")\n    parser.add_argument(""--do_eval"", action=""store_true"",\n                        help=""Whether to run eval on the dev set."")\n    parser.add_argument(""--do_predict"", action=""store_true"",\n                        help=""Whether to run predictions on the test set."")\n    parser.add_argument(""--evaluate_during_training"", action=""store_true"",\n                        help=""Whether to run evaluation during training at each logging step."", )\n    parser.add_argument(""--do_lower_case"", action=""store_true"",\n                        help=""Set this flag if you are using an uncased model."")\n    # adversarial training\n    parser.add_argument(""--do_adv"", action=""store_true"",\n                        help=""Whether to adversarial training."")\n    parser.add_argument(\'--adv_epsilon\', default=1.0, type=float,\n                        help=""Epsilon for adversarial."")\n    parser.add_argument(\'--adv_name\', default=\'word_embeddings\', type=str,\n                        help=""name for adversarial layer."")\n\n    parser.add_argument(""--per_gpu_train_batch_size"", default=8, type=int,\n                        help=""Batch size per GPU/CPU for training."")\n    parser.add_argument(""--per_gpu_eval_batch_size"", default=8, type=int,\n                        help=""Batch size per GPU/CPU for evaluation."")\n    parser.add_argument(""--gradient_accumulation_steps"", type=int, default=1,\n                        help=""Number of updates steps to accumulate before performing a backward/update pass."", )\n    parser.add_argument(""--learning_rate"", default=5e-5, type=float,\n                        help=""The initial learning rate for Adam."")\n    parser.add_argument(""--crf_learning_rate"", default=5e-5, type=float,\n                        help=""The initial learning rate for crf and linear layer."")\n    parser.add_argument(""--weight_decay"", default=0.01, type=float,\n                        help=""Weight decay if we apply some."")\n    parser.add_argument(""--adam_epsilon"", default=1e-8, type=float,\n                        help=""Epsilon for Adam optimizer."")\n    parser.add_argument(""--max_grad_norm"", default=1.0, type=float,\n                        help=""Max gradient norm."")\n    parser.add_argument(""--num_train_epochs"", default=3.0, type=float,\n                        help=""Total number of training epochs to perform."")\n    parser.add_argument(""--max_steps"", default=-1, type=int,\n                        help=""If > 0: set total number of training steps to perform. Override num_train_epochs."", )\n\n    parser.add_argument(""--warmup_proportion"", default=0.1, type=float,\n                        help=""Proportion of training to perform linear learning rate warmup for,E.g., 0.1 = 10% of training."")\n    parser.add_argument(""--logging_steps"", type=int, default=50,\n                        help=""Log every X updates steps."")\n    parser.add_argument(""--save_steps"", type=int, default=50, help=""Save checkpoint every X updates steps."")\n    parser.add_argument(""--eval_all_checkpoints"", action=""store_true"",\n                        help=""Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number"", )\n    parser.add_argument(""--predict_checkpoints"",type=int, default=0,\n                        help=""predict checkpoints starting with the same prefix as model_name ending and ending with step number"")\n    parser.add_argument(""--no_cuda"", action=""store_true"", help=""Avoid using CUDA when available"")\n    parser.add_argument(""--overwrite_output_dir"", action=""store_true"",\n                        help=""Overwrite the content of the output directory"")\n    parser.add_argument(""--overwrite_cache"", action=""store_true"",\n                        help=""Overwrite the cached training and evaluation sets"")\n    parser.add_argument(""--seed"", type=int, default=42, help=""random seed for initialization"")\n    parser.add_argument(""--fp16"", action=""store_true"",\n                        help=""Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit"", )\n    parser.add_argument(""--fp16_opt_level"", type=str, default=""O1"",\n                        help=""For fp16: Apex AMP optimization level selected in [\'O0\', \'O1\', \'O2\', and \'O3\'].""\n                             ""See details at https://nvidia.github.io/apex/amp.html"", )\n    parser.add_argument(""--local_rank"", type=int, default=-1, help=""For distributed training: local_rank"")\n    parser.add_argument(""--server_ip"", type=str, default="""", help=""For distant debugging."")\n    parser.add_argument(""--server_port"", type=str, default="""", help=""For distant debugging."")\n    return parser'"
tools/plot.py,0,"b'import numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom sklearn.metrics import confusion_matrix\r\nplt.switch_backend(\'agg\')\r\n\r\ndef plot_confusion_matrix(y_true, y_pred, classes,\r\n                          save_path,normalize=False,title=None,\r\n                          cmap=plt.cm.Blues):\r\n    """"""\r\n    This function prints and plots the confusion matrix.\r\n    Normalization can be applied by setting `normalize=True`.\r\n    """"""\r\n    if not title:\r\n        if normalize:\r\n            title = \'Normalized confusion matrix\'\r\n        else:\r\n            title = \'Confusion matrix, without normalization\'\r\n    # Compute confusion matrix\r\n    cm = confusion_matrix(y_true=y_true, y_pred=y_pred)\r\n    # Only use the labels that appear in the data\r\n    if normalize:\r\n        cm = cm.astype(\'float\') / cm.sum(axis=1)[:, np.newaxis]\r\n        print(""Normalized confusion matrix"")\r\n    else:\r\n        print(\'Confusion matrix, without normalization\')\r\n    # --- plot--- #\r\n    plt.rcParams[\'savefig.dpi\'] = 200\r\n    plt.rcParams[\'figure.dpi\'] = 200\r\n    plt.rcParams[\'figure.figsize\'] = [20, 20]  # plot\r\n    plt.rcParams.update({\'font.size\': 10})\r\n    fig, ax = plt.subplots()\r\n    im = ax.imshow(cm, interpolation=\'nearest\', cmap=cmap)\r\n    # --- bar --- #\r\n    from mpl_toolkits.axes_grid1 import make_axes_locatable\r\n    divider = make_axes_locatable(ax)\r\n    cax = divider.append_axes(""right"", size=""5%"", pad=0.05)\r\n    plt.colorbar(im, cax=cax)\r\n    # --- bar --- #\r\n    # ax.figure.colorbar(im, ax=ax)\r\n    # We want to show all ticks...\r\n    ax.set(xticks=np.arange(cm.shape[1]),\r\n           yticks=np.arange(cm.shape[0]),\r\n           # ... and label them with the respective list entries\r\n           xticklabels=classes, yticklabels=classes,\r\n           title=title,\r\n           ylabel=\'True label\',\r\n           xlabel=\'Predicted label\')\r\n\r\n    # Rotate the tick labels and set their alignment.\r\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=""right"",\r\n             rotation_mode=""anchor"")\r\n    # Loop over data dimensions and create text annotations.\r\n    fmt = \'.2f\' if normalize else \'d\'\r\n    thresh = cm.max() / 2.\r\n    for i in range(cm.shape[0]):\r\n        for j in range(cm.shape[1]):\r\n            ax.text(j, i, format(cm[i, j], fmt),\r\n                    ha=""center"", va=""center"",\r\n                    color=""white"" if cm[i, j] > thresh else ""black"")\r\n    fig.tight_layout()\r\n    plt.savefig(save_path)\r\n\r\nif __name__ == ""__main__"":\r\n    y_true = [\'O\', \'O\', \'O\', \'B-MISC\', \'I-MISC\', \'I-MISC\', \'O\', \'B-PER\', \'I-PER\', \'O\']\r\n    y_pred = [\'O\', \'O\', \'B-MISC\', \'I-MISC\', \'I-MISC\', \'I-MISC\', \'O\',\'B-PER\', \'I-PER\', \'O\']\r\n    classes = [\'O\',\'B-MISC\', \'I-MISC\',\'B-PER\', \'I-PER\']\r\n    save_path = \'./ner_confusion_matrix.png\'\r\n    plot_confusion_matrix(y_true,y_pred,classes,save_path)'"
callback/optimizater/__init__.py,0,b''
callback/optimizater/adabound.py,6,"b'import torch\r\nimport math\r\nfrom torch.optim.optimizer import Optimizer\r\n\r\nclass AdaBound(Optimizer):\r\n    """"""Implements AdaBound algorithm.\r\n    It has been proposed in `Adaptive Gradient Methods with Dynamic Bound of Learning Rate`_.\r\n    Arguments:\r\n        params (iterable): iterable of parameters to optimize or dicts defining\r\n            parameter groups\r\n        lr (float, optional): Adam learning rate (default: 1e-3)\r\n        betas (Tuple[float, float], optional): coefficients used for computing\r\n            running averages of gradient and its square (default: (0.9, 0.999))\r\n        final_lr (float, optional): final (SGD) learning rate (default: 0.1)\r\n        gamma (float, optional): convergence speed of the bound functions (default: 1e-3)\r\n        eps (float, optional): term added to the denominator to improve\r\n            numerical stability (default: 1e-8)\r\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\r\n        amsbound (boolean, optional): whether to use the AMSBound variant of this algorithm\r\n    .. Adaptive Gradient Methods with Dynamic Bound of Learning Rate:\r\n        https://openreview.net/forum?id=Bkg3g2R9FX\r\n    Example:\r\n        >>> model = LSTM()\r\n        >>> optimizer = AdaBound(model.parameters())\r\n    """"""\r\n\r\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), final_lr=0.1, gamma=1e-3,\r\n                 eps=1e-8, weight_decay=0, amsbound=False):\r\n        if not 0.0 <= lr:\r\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\r\n        if not 0.0 <= eps:\r\n            raise ValueError(""Invalid epsilon value: {}"".format(eps))\r\n        if not 0.0 <= betas[0] < 1.0:\r\n            raise ValueError(""Invalid beta parameter at index 0: {}"".format(betas[0]))\r\n        if not 0.0 <= betas[1] < 1.0:\r\n            raise ValueError(""Invalid beta parameter at index 1: {}"".format(betas[1]))\r\n        if not 0.0 <= final_lr:\r\n            raise ValueError(""Invalid final learning rate: {}"".format(final_lr))\r\n        if not 0.0 <= gamma < 1.0:\r\n            raise ValueError(""Invalid gamma parameter: {}"".format(gamma))\r\n        defaults = dict(lr=lr, betas=betas, final_lr=final_lr, gamma=gamma, eps=eps,\r\n                        weight_decay=weight_decay, amsbound=amsbound)\r\n        super(AdaBound, self).__init__(params, defaults)\r\n\r\n        self.base_lrs = list(map(lambda group: group[\'lr\'], self.param_groups))\r\n\r\n    def __setstate__(self, state):\r\n        super(AdaBound, self).__setstate__(state)\r\n        for group in self.param_groups:\r\n            group.setdefault(\'amsbound\', False)\r\n\r\n    def step(self, closure=None):\r\n        """"""Performs a single optimization step.\r\n        Arguments:\r\n            closure (callable, optional): A closure that reevaluates the model\r\n                and returns the loss.\r\n        """"""\r\n        loss = None\r\n        if closure is not None:\r\n            loss = closure()\r\n        for group, base_lr in zip(self.param_groups, self.base_lrs):\r\n            for p in group[\'params\']:\r\n                if p.grad is None:\r\n                    continue\r\n                grad = p.grad.data\r\n                if grad.is_sparse:\r\n                    raise RuntimeError(\r\n                        \'Adam does not support sparse gradients, please consider SparseAdam instead\')\r\n                amsbound = group[\'amsbound\']\r\n                state = self.state[p]\r\n                # State initialization\r\n                if len(state) == 0:\r\n                    state[\'step\'] = 0\r\n                    # Exponential moving average of gradient values\r\n                    state[\'exp_avg\'] = torch.zeros_like(p.data)\r\n                    # Exponential moving average of squared gradient values\r\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\r\n                    if amsbound:\r\n                        # Maintains max of all exp. moving avg. of sq. grad. values\r\n                        state[\'max_exp_avg_sq\'] = torch.zeros_like(p.data)\r\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\r\n                if amsbound:\r\n                    max_exp_avg_sq = state[\'max_exp_avg_sq\']\r\n                beta1, beta2 = group[\'betas\']\r\n                state[\'step\'] += 1\r\n                if group[\'weight_decay\'] != 0:\r\n                    grad = grad.add(group[\'weight_decay\'], p.data)\r\n                # Decay the first and second moment running average coefficient\r\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n                if amsbound:\r\n                    # Maintains the maximum of all 2nd moment running avg. till now\r\n                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\r\n                    # Use the max. for normalizing running avg. of gradient\r\n                    denom = max_exp_avg_sq.sqrt().add_(group[\'eps\'])\r\n                else:\r\n                    denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\r\n\r\n                bias_correction1 = 1 - beta1 ** state[\'step\']\r\n                bias_correction2 = 1 - beta2 ** state[\'step\']\r\n                step_size = group[\'lr\'] * math.sqrt(bias_correction2) / bias_correction1\r\n\r\n                # Applies bounds on actual learning rate\r\n                # lr_scheduler cannot affect final_lr, this is a workaround to apply lr decay\r\n                final_lr = group[\'final_lr\'] * group[\'lr\'] / base_lr\r\n                lower_bound = final_lr * (1 - 1 / (group[\'gamma\'] * state[\'step\'] + 1))\r\n                upper_bound = final_lr * (1 + 1 / (group[\'gamma\'] * state[\'step\']))\r\n                step_size = torch.full_like(denom, step_size)\r\n                step_size.div_(denom).clamp_(lower_bound, upper_bound).mul_(exp_avg)\r\n                p.data.add_(-step_size)\r\n        return loss'"
callback/optimizater/adafactor.py,19,"b""import operator\r\nimport torch\r\nfrom copy import copy\r\nimport functools\r\nfrom math import sqrt\r\nfrom torch.optim.optimizer import Optimizer\r\n\r\n\r\nclass AdaFactor(Optimizer):\r\n    '''\r\n    # Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf\r\n    # inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch\r\n    Example:\r\n        >>> model = LSTM()\r\n        >>> optimizer = AdaFactor(model.parameters(),lr= lr)\r\n    '''\r\n\r\n    def __init__(self, params, lr=None, beta1=0.9, beta2=0.999, eps1=1e-30,\r\n                 eps2=1e-3, cliping_threshold=1, non_constant_decay=True,\r\n                 enable_factorization=True, ams_grad=True, weight_decay=0):\r\n\r\n        enable_momentum = beta1 != 0\r\n        if non_constant_decay:\r\n            ams_grad = False\r\n\r\n        defaults = dict(lr=lr, beta1=beta1, beta2=beta2, eps1=eps1,\r\n                        eps2=eps2, cliping_threshold=cliping_threshold,\r\n                        weight_decay=weight_decay, ams_grad=ams_grad,\r\n                        enable_factorization=enable_factorization,\r\n                        enable_momentum=enable_momentum,\r\n                        non_constant_decay=non_constant_decay)\r\n\r\n        super(AdaFactor, self).__init__(params, defaults)\r\n\r\n    def __setstate__(self, state):\r\n        super(AdaFactor, self).__setstate__(state)\r\n\r\n    def _experimental_reshape(self, shape):\r\n        temp_shape = shape[2:]\r\n        if len(temp_shape) == 1:\r\n            new_shape = (shape[0], shape[1] * shape[2])\r\n        else:\r\n            tmp_div = len(temp_shape) // 2 + len(temp_shape) % 2\r\n            new_shape = (shape[0] * functools.reduce(operator.mul,\r\n                                                     temp_shape[tmp_div:], 1),\r\n                         shape[1] * functools.reduce(operator.mul,\r\n                                                     temp_shape[:tmp_div], 1))\r\n        return new_shape, copy(shape)\r\n\r\n    def _check_shape(self, shape):\r\n        '''\r\n        output1 - True - algorithm for matrix, False - vector;\r\n        output2 - need reshape\r\n        '''\r\n        if len(shape) > 2:\r\n            return True, True\r\n        elif len(shape) == 2:\r\n            return True, False\r\n        elif len(shape) == 2 and (shape[0] == 1 or shape[1] == 1):\r\n            return False, False\r\n        else:\r\n            return False, False\r\n\r\n    def _rms(self, x):\r\n        return sqrt(torch.mean(x.pow(2)))\r\n\r\n    def step(self, closure=None):\r\n        loss = None\r\n        if closure is not None:\r\n            loss = closure()\r\n        for group in self.param_groups:\r\n            for p in group['params']:\r\n                if p.grad is None:\r\n                    continue\r\n                grad = p.grad.data\r\n\r\n                if grad.is_sparse:\r\n                    raise RuntimeError('Adam does not support sparse \\\r\n                                       gradients, use SparseAdam instead')\r\n\r\n                is_matrix, is_need_reshape = self._check_shape(grad.size())\r\n                new_shape = p.data.size()\r\n                if is_need_reshape and group['enable_factorization']:\r\n                    new_shape, old_shape = \\\r\n                        self._experimental_reshape(p.data.size())\r\n                    grad = grad.view(new_shape)\r\n\r\n                state = self.state[p]\r\n                if len(state) == 0:\r\n                    state['step'] = 0\r\n                    if group['enable_momentum']:\r\n                        state['exp_avg'] = torch.zeros(new_shape,\r\n                                                       dtype=torch.float32,\r\n                                                       device=p.grad.device)\r\n\r\n                    if is_matrix and group['enable_factorization']:\r\n                        state['exp_avg_sq_R'] = \\\r\n                            torch.zeros((1, new_shape[1]),\r\n                                        dtype=torch.float32,\r\n                                        device=p.grad.device)\r\n                        state['exp_avg_sq_C'] = \\\r\n                            torch.zeros((new_shape[0], 1),\r\n                                        dtype=torch.float32,\r\n                                        device=p.grad.device)\r\n                    else:\r\n                        state['exp_avg_sq'] = torch.zeros(new_shape,\r\n                                                          dtype=torch.float32,\r\n                                                          device=p.grad.device)\r\n                    if group['ams_grad']:\r\n                        state['exp_avg_sq_hat'] = \\\r\n                            torch.zeros(new_shape, dtype=torch.float32,\r\n                                        device=p.grad.device)\r\n\r\n                if group['enable_momentum']:\r\n                    exp_avg = state['exp_avg']\r\n\r\n                if is_matrix and group['enable_factorization']:\r\n                    exp_avg_sq_r = state['exp_avg_sq_R']\r\n                    exp_avg_sq_c = state['exp_avg_sq_C']\r\n                else:\r\n                    exp_avg_sq = state['exp_avg_sq']\r\n\r\n                if group['ams_grad']:\r\n                    exp_avg_sq_hat = state['exp_avg_sq_hat']\r\n\r\n                state['step'] += 1\r\n                lr_t = group['lr']\r\n                lr_t *= max(group['eps2'], self._rms(p.data))\r\n\r\n                if group['enable_momentum']:\r\n                    if group['non_constant_decay']:\r\n                        beta1_t = group['beta1'] * \\\r\n                                  (1 - group['beta1'] ** (state['step'] - 1)) \\\r\n                                  / (1 - group['beta1'] ** state['step'])\r\n                    else:\r\n                        beta1_t = group['beta1']\r\n                    exp_avg.mul_(beta1_t).add_(1 - beta1_t, grad)\r\n\r\n                if group['non_constant_decay']:\r\n                    beta2_t = group['beta2'] * \\\r\n                              (1 - group['beta2'] ** (state['step'] - 1)) / \\\r\n                              (1 - group['beta2'] ** state['step'])\r\n                else:\r\n                    beta2_t = group['beta2']\r\n\r\n                if is_matrix and group['enable_factorization']:\r\n                    exp_avg_sq_r.mul_(beta2_t). \\\r\n                        add_(1 - beta2_t, torch.sum(torch.mul(grad, grad).\r\n                                                    add_(group['eps1']),\r\n                                                    dim=0, keepdim=True))\r\n                    exp_avg_sq_c.mul_(beta2_t). \\\r\n                        add_(1 - beta2_t, torch.sum(torch.mul(grad, grad).\r\n                                                    add_(group['eps1']),\r\n                                                    dim=1, keepdim=True))\r\n                    v = torch.mul(exp_avg_sq_c,\r\n                                  exp_avg_sq_r).div_(torch.sum(exp_avg_sq_r))\r\n                else:\r\n                    exp_avg_sq.mul_(beta2_t). \\\r\n                        addcmul_(1 - beta2_t, grad, grad). \\\r\n                        add_((1 - beta2_t) * group['eps1'])\r\n                    v = exp_avg_sq\r\n                g = grad\r\n                if group['enable_momentum']:\r\n                    g = torch.div(exp_avg, 1 - beta1_t ** state['step'])\r\n                if group['ams_grad']:\r\n                    torch.max(exp_avg_sq_hat, v, out=exp_avg_sq_hat)\r\n                    v = exp_avg_sq_hat\r\n                    u = torch.div(g, (torch.div(v, 1 - beta2_t **\r\n                                                state['step'])).sqrt().add_(group['eps1']))\r\n                else:\r\n                    u = torch.div(g, v.sqrt())\r\n                u.div_(max(1, self._rms(u) / group['cliping_threshold']))\r\n                p.data.add_(-lr_t * (u.view(old_shape) if is_need_reshape and\r\n                                                          group['enable_factorization'] else u))\r\n                if group['weight_decay'] != 0:\r\n                    p.data.add_(-group['weight_decay'] * lr_t, p.data)\r\n        return loss\r\n"""
callback/optimizater/adamw.py,3,"b'import torch\r\nimport math\r\nfrom torch.optim.optimizer import Optimizer\r\n\r\nclass AdamW(Optimizer):\r\n    """""" Implements Adam algorithm with weight decay fix.\r\n\r\n    Parameters:\r\n        lr (float): learning rate. Default 1e-3.\r\n        betas (tuple of 2 floats): Adams beta parameters (b1, b2). Default: (0.9, 0.999)\r\n        eps (float): Adams epsilon. Default: 1e-6\r\n        weight_decay (float): Weight decay. Default: 0.0\r\n        correct_bias (bool): can be set to False to avoid correcting bias in Adam (e.g. like in Bert TF repository). Default True.\r\n    Example:\r\n        >>> model = LSTM()\r\n        >>> optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\r\n    """"""\r\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6, weight_decay=0.0, correct_bias=True):\r\n        if lr < 0.0:\r\n            raise ValueError(""Invalid learning rate: {} - should be >= 0.0"".format(lr))\r\n        if not 0.0 <= betas[0] < 1.0:\r\n            raise ValueError(""Invalid beta parameter: {} - should be in [0.0, 1.0["".format(betas[0]))\r\n        if not 0.0 <= betas[1]  < 1.0:\r\n            raise ValueError(""Invalid beta parameter: {} - should be in [0.0, 1.0["".format(betas[1]))\r\n        if not 0.0 <= eps:\r\n            raise ValueError(""Invalid epsilon value: {} - should be >= 0.0"".format(eps))\r\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,\r\n                        correct_bias=correct_bias)\r\n        super(AdamW, self).__init__(params, defaults)\r\n\r\n    def step(self, closure=None):\r\n        """"""Performs a single optimization step.\r\n\r\n        Arguments:\r\n            closure (callable, optional): A closure that reevaluates the model\r\n                and returns the loss.\r\n        """"""\r\n        loss = None\r\n        if closure is not None:\r\n            loss = closure()\r\n\r\n        for group in self.param_groups:\r\n            for p in group[\'params\']:\r\n                if p.grad is None:\r\n                    continue\r\n                grad = p.grad.data\r\n                if grad.is_sparse:\r\n                    raise RuntimeError(\'Adam does not support sparse gradients, please consider SparseAdam instead\')\r\n\r\n                state = self.state[p]\r\n\r\n                # State initialization\r\n                if len(state) == 0:\r\n                    state[\'step\'] = 0\r\n                    # Exponential moving average of gradient values\r\n                    state[\'exp_avg\'] = torch.zeros_like(p.data)\r\n                    # Exponential moving average of squared gradient values\r\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\r\n\r\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\r\n                beta1, beta2 = group[\'betas\']\r\n\r\n                state[\'step\'] += 1\r\n\r\n                # Decay the first and second moment running average coefficient\r\n                # In-place operations to update the averages at the same time\r\n                exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\r\n                exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)\r\n                denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\r\n\r\n                step_size = group[\'lr\']\r\n                if group[\'correct_bias\']:  # No bias correction for Bert\r\n                    bias_correction1 = 1.0 - beta1 ** state[\'step\']\r\n                    bias_correction2 = 1.0 - beta2 ** state[\'step\']\r\n                    step_size = step_size * math.sqrt(bias_correction2) / bias_correction1\r\n\r\n                p.data.addcdiv_(-step_size, exp_avg, denom)\r\n\r\n                # Just adding the square of the weights to the loss function is *not*\r\n                # the correct way of using L2 regularization/weight decay with Adam,\r\n                # since that will interact with the m and v parameters in strange ways.\r\n                #\r\n                # Instead we want to decay the weights in a manner that doesn\'t interact\r\n                # with the m/v parameters. This is equivalent to adding the square\r\n                # of the weights to the loss with plain (non-momentum) SGD.\r\n                # Add weight decay at the end (fixed version)\r\n                if group[\'weight_decay\'] > 0.0:\r\n                    p.data.add_(-group[\'lr\'] * group[\'weight_decay\'], p.data)\r\n\r\n        return loss\r\n'"
callback/optimizater/lamb.py,3,"b'import torch\r\nfrom torch.optim.optimizer import Optimizer\r\n\r\n\r\nclass Lamb(Optimizer):\r\n    r""""""Implements Lamb algorithm.\r\n    It has been proposed in `Large Batch Optimization for Deep Learning: Training BERT in 76 minutes`_.\r\n    Arguments:\r\n        params (iterable): iterable of parameters to optimize or dicts defining\r\n            parameter groups\r\n        lr (float, optional): learning rate (default: 1e-3)\r\n        betas (Tuple[float, float], optional): coefficients used for computing\r\n            running averages of gradient and its square (default: (0.9, 0.999))\r\n        eps (float, optional): term added to the denominator to improve\r\n            numerical stability (default: 1e-8)\r\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\r\n        adam (bool, optional): always use trust ratio = 1, which turns this into\r\n            Adam. Useful for comparison purposes.\r\n    .. _Large Batch Optimization for Deep Learning: Training BERT in 76 minutes:\r\n        https://arxiv.org/abs/1904.00962\r\n    Example:\r\n        >>> model = ResNet()\r\n        >>> optimizer = Lamb(model.parameters(), lr=1e-2, weight_decay=1e-5)\r\n    """"""\r\n\r\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6,\r\n                 weight_decay=0, adam=False):\r\n        if not 0.0 <= lr:\r\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\r\n        if not 0.0 <= eps:\r\n            raise ValueError(""Invalid epsilon value: {}"".format(eps))\r\n        if not 0.0 <= betas[0] < 1.0:\r\n            raise ValueError(""Invalid beta parameter at index 0: {}"".format(betas[0]))\r\n        if not 0.0 <= betas[1] < 1.0:\r\n            raise ValueError(""Invalid beta parameter at index 1: {}"".format(betas[1]))\r\n        defaults = dict(lr=lr, betas=betas, eps=eps,\r\n                        weight_decay=weight_decay)\r\n        self.adam = adam\r\n        super(Lamb, self).__init__(params, defaults)\r\n\r\n    def step(self, closure=None):\r\n        """"""Performs a single optimization step.\r\n        Arguments:\r\n            closure (callable, optional): A closure that reevaluates the model\r\n                and returns the loss.\r\n        """"""\r\n        loss = None\r\n        if closure is not None:\r\n            loss = closure()\r\n\r\n        for group in self.param_groups:\r\n            for p in group[\'params\']:\r\n                if p.grad is None:\r\n                    continue\r\n                grad = p.grad.data\r\n                if grad.is_sparse:\r\n                    raise RuntimeError(\'Lamb does not support sparse gradients, consider SparseAdam instad.\')\r\n\r\n                state = self.state[p]\r\n\r\n                # State initialization\r\n                if len(state) == 0:\r\n                    state[\'step\'] = 0\r\n                    # Exponential moving average of gradient values\r\n                    state[\'exp_avg\'] = torch.zeros_like(p.data)\r\n                    # Exponential moving average of squared gradient values\r\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\r\n\r\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\r\n                beta1, beta2 = group[\'betas\']\r\n\r\n                state[\'step\'] += 1\r\n\r\n                # Decay the first and second moment running average coefficient\r\n                # m_t\r\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n                # v_t\r\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n\r\n                # Paper v3 does not use debiasing.\r\n                # bias_correction1 = 1 - beta1 ** state[\'step\']\r\n                # bias_correction2 = 1 - beta2 ** state[\'step\']\r\n                # Apply bias to lr to avoid broadcast.\r\n                step_size = group[\'lr\'] # * math.sqrt(bias_correction2) / bias_correction1\r\n\r\n                weight_norm = p.data.pow(2).sum().sqrt().clamp(0, 10)\r\n\r\n                adam_step = exp_avg / exp_avg_sq.sqrt().add(group[\'eps\'])\r\n                if group[\'weight_decay\'] != 0:\r\n                    adam_step.add_(group[\'weight_decay\'], p.data)\r\n\r\n                adam_norm = adam_step.pow(2).sum().sqrt()\r\n                if weight_norm == 0 or adam_norm == 0:\r\n                    trust_ratio = 1\r\n                else:\r\n                    trust_ratio = weight_norm / adam_norm\r\n                state[\'weight_norm\'] = weight_norm\r\n                state[\'adam_norm\'] = adam_norm\r\n                state[\'trust_ratio\'] = trust_ratio\r\n                if self.adam:\r\n                    trust_ratio = 1\r\n\r\n                p.data.add_(-step_size * trust_ratio, adam_step)\r\n\r\n        return loss'"
callback/optimizater/lars.py,2,"b'import torch\r\nfrom torch.optim.optimizer import Optimizer\r\n\r\nclass Lars(Optimizer):\r\n    r""""""Implements the LARS optimizer from https://arxiv.org/pdf/1708.03888.pdf\r\n\r\n    Args:\r\n        params (iterable): iterable of parameters to optimize or dicts defining\r\n            parameter groups\r\n        lr (float): learning rate\r\n        momentum (float, optional): momentum factor (default: 0)\r\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\r\n        dampening (float, optional): dampening for momentum (default: 0)\r\n        nesterov (bool, optional): enables Nesterov momentum (default: False)\r\n        scale_clip (tuple, optional): the lower and upper bounds for the weight norm in local LR of LARS\r\n    Example:\r\n        >>> model = ResNet()\r\n        >>> optimizer = Lars(model.parameters(), lr=1e-2, weight_decay=1e-5)\r\n    """"""\r\n\r\n    def __init__(self, params, lr, momentum=0, dampening=0,\r\n                 weight_decay=0, nesterov=False, scale_clip=None):\r\n        if lr < 0.0:\r\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\r\n        if momentum < 0.0:\r\n            raise ValueError(""Invalid momentum value: {}"".format(momentum))\r\n        if weight_decay < 0.0:\r\n            raise ValueError(""Invalid weight_decay value: {}"".format(weight_decay))\r\n\r\n        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\r\n                        weight_decay=weight_decay, nesterov=nesterov)\r\n        if nesterov and (momentum <= 0 or dampening != 0):\r\n            raise ValueError(""Nesterov momentum requires a momentum and zero dampening"")\r\n        super(Lars, self).__init__(params, defaults)\r\n        # LARS arguments\r\n        self.scale_clip = scale_clip\r\n        if self.scale_clip is None:\r\n            self.scale_clip = (0, 10)\r\n\r\n    def __setstate__(self, state):\r\n        super(Lars, self).__setstate__(state)\r\n        for group in self.param_groups:\r\n            group.setdefault(\'nesterov\', False)\r\n\r\n    def step(self, closure=None):\r\n        """"""Performs a single optimization step.\r\n\r\n        Arguments:\r\n            closure (callable, optional): A closure that reevaluates the model\r\n                and returns the loss.\r\n        """"""\r\n        loss = None\r\n        if closure is not None:\r\n            loss = closure()\r\n\r\n        for group in self.param_groups:\r\n            weight_decay = group[\'weight_decay\']\r\n            momentum = group[\'momentum\']\r\n            dampening = group[\'dampening\']\r\n            nesterov = group[\'nesterov\']\r\n\r\n            for p in group[\'params\']:\r\n                if p.grad is None:\r\n                    continue\r\n                d_p = p.grad.data\r\n                if weight_decay != 0:\r\n                    d_p.add_(weight_decay, p.data)\r\n                if momentum != 0:\r\n                    param_state = self.state[p]\r\n                    if \'momentum_buffer\' not in param_state:\r\n                        buf = param_state[\'momentum_buffer\'] = torch.clone(d_p).detach()\r\n                    else:\r\n                        buf = param_state[\'momentum_buffer\']\r\n                        buf.mul_(momentum).add_(1 - dampening, d_p)\r\n                    if nesterov:\r\n                        d_p = d_p.add(momentum, buf)\r\n                    else:\r\n                        d_p = buf\r\n\r\n                # LARS\r\n                p_norm = p.data.pow(2).sum().sqrt()\r\n                update_norm = d_p.pow(2).sum().sqrt()\r\n                # Compute the local LR\r\n                if p_norm == 0 or update_norm == 0:\r\n                    local_lr = 1\r\n                else:\r\n                    local_lr = p_norm / update_norm\r\n\r\n                p.data.add_(-group[\'lr\'] * local_lr, d_p)\r\n\r\n        return loss'"
callback/optimizater/lookahead.py,4,"b'import torch\r\nfrom torch.optim.optimizer import Optimizer\r\nfrom collections import defaultdict\r\n\r\nclass Lookahead(Optimizer):\r\n    \'\'\'\r\n    PyTorch implementation of the lookahead wrapper.\r\n    Lookahead Optimizer: https://arxiv.org/abs/1907.08610\r\n\r\n    We found that evaluation performance is typically better using the slow weights.\r\n    This can be done in PyTorch with something like this in your eval loop:\r\n    if args.lookahead:\r\n        optimizer._backup_and_load_cache()\r\n        val_loss = eval_func(model)\r\n        optimizer._clear_and_load_backup()\r\n    \'\'\'\r\n    def __init__(self, optimizer,alpha=0.5, k=6,pullback_momentum=""none""):\r\n        \'\'\'\r\n        :param optimizer:inner optimizer\r\n        :param k (int): number of lookahead steps\r\n        :param alpha(float): linear interpolation factor. 1.0 recovers the inner optimizer.\r\n        :param pullback_momentum (str): change to inner optimizer momentum on interpolation update\r\n        \'\'\'\r\n        if not 0.0 <= alpha <= 1.0:\r\n            raise ValueError(f\'Invalid slow update rate: {alpha}\')\r\n        if not 1 <= k:\r\n            raise ValueError(f\'Invalid lookahead steps: {k}\')\r\n        self.optimizer = optimizer\r\n        self.param_groups = self.optimizer.param_groups\r\n        self.alpha = alpha\r\n        self.k = k\r\n        self.step_counter = 0\r\n        assert pullback_momentum in [""reset"", ""pullback"", ""none""]\r\n        self.pullback_momentum = pullback_momentum\r\n        self.state = defaultdict(dict)\r\n\r\n        # Cache the current optimizer parameters\r\n        for group in self.optimizer.param_groups:\r\n            for p in group[\'params\']:\r\n                param_state = self.state[p]\r\n                param_state[\'cached_params\'] = torch.zeros_like(p.data)\r\n                param_state[\'cached_params\'].copy_(p.data)\r\n\r\n    def __getstate__(self):\r\n        return {\r\n            \'state\': self.state,\r\n            \'optimizer\': self.optimizer,\r\n            \'alpha\': self.alpha,\r\n            \'step_counter\': self.step_counter,\r\n            \'k\':self.k,\r\n            \'pullback_momentum\': self.pullback_momentum\r\n        }\r\n\r\n    def zero_grad(self):\r\n        self.optimizer.zero_grad()\r\n\r\n    def state_dict(self):\r\n        return self.optimizer.state_dict()\r\n\r\n    def load_state_dict(self, state_dict):\r\n        self.optimizer.load_state_dict(state_dict)\r\n\r\n    def _backup_and_load_cache(self):\r\n        """"""Useful for performing evaluation on the slow weights (which typically generalize better)\r\n        """"""\r\n        for group in self.optimizer.param_groups:\r\n            for p in group[\'params\']:\r\n                param_state = self.state[p]\r\n                param_state[\'backup_params\'] = torch.zeros_like(p.data)\r\n                param_state[\'backup_params\'].copy_(p.data)\r\n                p.data.copy_(param_state[\'cached_params\'])\r\n\r\n    def _clear_and_load_backup(self):\r\n        for group in self.optimizer.param_groups:\r\n            for p in group[\'params\']:\r\n                param_state = self.state[p]\r\n                p.data.copy_(param_state[\'backup_params\'])\r\n                del param_state[\'backup_params\']\r\n\r\n    def step(self, closure=None):\r\n        """"""Performs a single Lookahead optimization step.\r\n        Arguments:\r\n            closure (callable, optional): A closure that reevaluates the model\r\n                and returns the loss.\r\n        """"""\r\n        loss = self.optimizer.step(closure)\r\n        self.step_counter += 1\r\n\r\n        if self.step_counter >= self.k:\r\n            self.step_counter = 0\r\n            # Lookahead and cache the current optimizer parameters\r\n            for group in self.optimizer.param_groups:\r\n                for p in group[\'params\']:\r\n                    param_state = self.state[p]\r\n                    p.data.mul_(self.alpha).add_(1.0 - self.alpha, param_state[\'cached_params\'])  # crucial line\r\n                    param_state[\'cached_params\'].copy_(p.data)\r\n                    if self.pullback_momentum == ""pullback"":\r\n                        internal_momentum = self.optimizer.state[p][""momentum_buffer""]\r\n                        self.optimizer.state[p][""momentum_buffer""] = internal_momentum.mul_(self.alpha).add_(\r\n                            1.0 - self.alpha, param_state[""cached_mom""])\r\n                        param_state[""cached_mom""] = self.optimizer.state[p][""momentum_buffer""]\r\n                    elif self.pullback_momentum == ""reset"":\r\n                        self.optimizer.state[p][""momentum_buffer""] = torch.zeros_like(p.data)\r\n\r\n        return loss\r\n'"
callback/optimizater/nadam.py,1,"b'import torch\r\nimport math\r\nfrom torch.optim.optimizer import Optimizer\r\n\r\nclass Nadam(Optimizer):\r\n    """"""Implements Nadam algorithm (a variant of Adam based on Nesterov momentum).\r\n\r\n    It has been proposed in `Incorporating Nesterov Momentum into Adam`__.\r\n\r\n    Arguments:\r\n        params (iterable): iterable of parameters to optimize or dicts defining\r\n            parameter groups\r\n        lr (float, optional): learning rate (default: 2e-3)\r\n        betas (Tuple[float, float], optional): coefficients used for computing\r\n            running averages of gradient and its square\r\n        eps (float, optional): term added to the denominator to improve\r\n            numerical stability (default: 1e-8)\r\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\r\n        schedule_decay (float, optional): momentum schedule decay (default: 4e-3)\r\n\r\n    __ http://cs229.stanford.edu/proj2015/054_report.pdf\r\n    __ http://www.cs.toronto.edu/~fritz/absps/momentum.pdf\r\n\r\n        Originally taken from: https://github.com/pytorch/pytorch/pull/1408\r\n        NOTE: Has potential issues but does work well on some problems.\r\n    Example:\r\n        >>> model = LSTM()\r\n        >>> optimizer = Nadam(model.parameters())\r\n    """"""\r\n\r\n    def __init__(self, params, lr=2e-3, betas=(0.9, 0.999), eps=1e-8,\r\n                 weight_decay=0, schedule_decay=4e-3):\r\n        defaults = dict(lr=lr, betas=betas, eps=eps,\r\n                        weight_decay=weight_decay, schedule_decay=schedule_decay)\r\n        super(Nadam, self).__init__(params, defaults)\r\n\r\n    def step(self, closure=None):\r\n        """"""Performs a single optimization step.\r\n\r\n        Arguments:\r\n            closure (callable, optional): A closure that reevaluates the model\r\n                and returns the loss.\r\n        """"""\r\n        loss = None\r\n        if closure is not None:\r\n            loss = closure()\r\n\r\n        for group in self.param_groups:\r\n            for p in group[\'params\']:\r\n                if p.grad is None:\r\n                    continue\r\n                grad = p.grad.data\r\n                state = self.state[p]\r\n\r\n                # State initialization\r\n                if len(state) == 0:\r\n                    state[\'step\'] = 0\r\n                    state[\'m_schedule\'] = 1.\r\n                    state[\'exp_avg\'] = grad.new().resize_as_(grad).zero_()\r\n                    state[\'exp_avg_sq\'] = grad.new().resize_as_(grad).zero_()\r\n\r\n                # Warming momentum schedule\r\n                m_schedule = state[\'m_schedule\']\r\n                schedule_decay = group[\'schedule_decay\']\r\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\r\n                beta1, beta2 = group[\'betas\']\r\n                eps = group[\'eps\']\r\n                state[\'step\'] += 1\r\n                t = state[\'step\']\r\n\r\n                if group[\'weight_decay\'] != 0:\r\n                    grad = grad.add(group[\'weight_decay\'], p.data)\r\n\r\n                momentum_cache_t = beta1 * \\\r\n                    (1. - 0.5 * (0.96 ** (t * schedule_decay)))\r\n                momentum_cache_t_1 = beta1 * \\\r\n                    (1. - 0.5 * (0.96 ** ((t + 1) * schedule_decay)))\r\n                m_schedule_new = m_schedule * momentum_cache_t\r\n                m_schedule_next = m_schedule * momentum_cache_t * momentum_cache_t_1\r\n                state[\'m_schedule\'] = m_schedule_new\r\n\r\n                # Decay the first and second moment running average coefficient\r\n                exp_avg.mul_(beta1).add_(1. - beta1, grad)\r\n                exp_avg_sq.mul_(beta2).addcmul_(1. - beta2, grad, grad)\r\n                exp_avg_sq_prime = exp_avg_sq / (1. - beta2 ** t)\r\n                denom = exp_avg_sq_prime.sqrt_().add_(eps)\r\n\r\n                p.data.addcdiv_(-group[\'lr\'] * (1. - momentum_cache_t) / (1. - m_schedule_new), grad, denom)\r\n                p.data.addcdiv_(-group[\'lr\'] * momentum_cache_t_1 / (1. - m_schedule_next), exp_avg, denom)\r\n\r\n        return loss'"
callback/optimizater/novograd.py,3,"b'import torch\r\nimport math\r\nfrom torch.optim.optimizer import Optimizer\r\n\r\n\r\nclass NovoGrad(Optimizer):\r\n    """"""Implements NovoGrad algorithm.\r\n    Arguments:\r\n        params (iterable): iterable of parameters to optimize or dicts defining\r\n            parameter groups\r\n        lr (float, optional): learning rate (default: 1e-2)\r\n        betas (Tuple[float, float], optional): coefficients used for computing\r\n            running averages of gradient and its square (default: (0.95, 0.98))\r\n        eps (float, optional): term added to the denominator to improve\r\n            numerical stability (default: 1e-8)\r\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\r\n    Example:\r\n        >>> model = ResNet()\r\n        >>> optimizer = NovoGrad(model.parameters(), lr=1e-2, weight_decay=1e-5)\r\n    """"""\r\n\r\n    def __init__(self, params, lr=0.01, betas=(0.95, 0.98), eps=1e-8,\r\n                 weight_decay=0, grad_averaging=False):\r\n        if lr < 0.0:\r\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\r\n        if not 0.0 <= betas[0] < 1.0:\r\n            raise ValueError(""Invalid beta parameter at index 0: {}"".format(betas[0]))\r\n        if not 0.0 <= betas[1] < 1.0:\r\n            raise ValueError(""Invalid beta parameter at index 1: {}"".format(betas[1]))\r\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, grad_averaging=grad_averaging)\r\n        super().__init__(params, defaults)\r\n\r\n    def step(self, closure=None):\r\n        loss = None\r\n        if closure is not None:\r\n            loss = closure()\r\n        for group in self.param_groups:\r\n            for p in group[\'params\']:\r\n                if p.grad is None:\r\n                    continue\r\n                grad = p.grad.data\r\n                if grad.is_sparse:\r\n                    raise RuntimeError(\'NovoGrad does not support sparse gradients\')\r\n                state = self.state[p]\r\n                g_2 = torch.sum(grad ** 2)\r\n                if len(state) == 0:\r\n                    state[\'step\'] = 0\r\n                    state[\'moments\'] = grad.div(g_2.sqrt() + group[\'eps\']) + \\\r\n                                       group[\'weight_decay\'] * p.data\r\n                    state[\'grads_ema\'] = g_2\r\n                moments = state[\'moments\']\r\n                grads_ema = state[\'grads_ema\']\r\n                beta1, beta2 = group[\'betas\']\r\n                state[\'step\'] += 1\r\n                grads_ema.mul_(beta2).add_(1 - beta2, g_2)\r\n\r\n                denom = grads_ema.sqrt().add_(group[\'eps\'])\r\n                grad.div_(denom)\r\n                # weight decay\r\n                if group[\'weight_decay\'] != 0:\r\n                    decayed_weights = torch.mul(p.data, group[\'weight_decay\'])\r\n                    grad.add_(decayed_weights)\r\n\r\n                # Momentum --> SAG\r\n                if group[\'grad_averaging\']:\r\n                    grad.mul_(1.0 - beta1)\r\n\r\n                moments.mul_(beta1).add_(grad)  # velocity\r\n\r\n                bias_correction1 = 1 - beta1 ** state[\'step\']\r\n                bias_correction2 = 1 - beta2 ** state[\'step\']\r\n                step_size = group[\'lr\'] * math.sqrt(bias_correction2) / bias_correction1\r\n                p.data.add_(-step_size, moments)\r\n\r\n        return loss\r\n'"
callback/optimizater/planradam.py,3,"b""import torch\r\nimport math\r\nfrom torch.optim.optimizer import Optimizer\r\nclass PlainRAdam(Optimizer):\r\n\r\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\r\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\r\n\r\n        super(PlainRAdam, self).__init__(params, defaults)\r\n\r\n    def __setstate__(self, state):\r\n        super(PlainRAdam, self).__setstate__(state)\r\n\r\n    def step(self, closure=None):\r\n\r\n        loss = None\r\n        if closure is not None:\r\n            loss = closure()\r\n\r\n        for group in self.param_groups:\r\n\r\n            for p in group['params']:\r\n                if p.grad is None:\r\n                    continue\r\n                grad = p.grad.data.float()\r\n                if grad.is_sparse:\r\n                    raise RuntimeError('RAdam does not support sparse gradients')\r\n\r\n                p_data_fp32 = p.data.float()\r\n\r\n                state = self.state[p]\r\n\r\n                if len(state) == 0:\r\n                    state['step'] = 0\r\n                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\r\n                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\r\n                else:\r\n                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\r\n                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\r\n\r\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\r\n                beta1, beta2 = group['betas']\r\n\r\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n\r\n                state['step'] += 1\r\n                beta2_t = beta2 ** state['step']\r\n                N_sma_max = 2 / (1 - beta2) - 1\r\n                N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\r\n\r\n                if group['weight_decay'] != 0:\r\n                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\r\n\r\n                # more conservative since it's an approximated value\r\n                if N_sma >= 5:\r\n                    step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\r\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\r\n                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\r\n                else:\r\n                    step_size = group['lr'] / (1 - beta1 ** state['step'])\r\n                    p_data_fp32.add_(-step_size, exp_avg)\r\n\r\n                p.data.copy_(p_data_fp32)\r\n\r\n        return loss"""
callback/optimizater/radam.py,3,"b'import torch\r\nimport math\r\nfrom torch.optim.optimizer import Optimizer\r\nclass RAdam(Optimizer):\r\n    """"""Implements the RAdam optimizer from https://arxiv.org/pdf/1908.03265.pdf\r\n    Args:\r\n        params (iterable): iterable of parameters to optimize or dicts defining parameter groups\r\n        lr (float, optional): learning rate\r\n        betas (Tuple[float, float], optional): coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999))\r\n        eps (float, optional): term added to the denominator to improve numerical stability (default: 1e-8)\r\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\r\n    Example:\r\n        >>> model = ResNet()\r\n        >>> optimizer = RAdam(model.parameters(), lr=0.001)\r\n    """"""\r\n\r\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\r\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\r\n        self.buffer = [[None, None, None] for ind in range(10)]\r\n        super(RAdam, self).__init__(params, defaults)\r\n\r\n    def __setstate__(self, state):\r\n        super(RAdam, self).__setstate__(state)\r\n\r\n    def step(self, closure=None):\r\n\r\n        loss = None\r\n        if closure is not None:\r\n            loss = closure()\r\n\r\n        for group in self.param_groups:\r\n\r\n            for p in group[\'params\']:\r\n                if p.grad is None:\r\n                    continue\r\n                grad = p.grad.data.float()\r\n                if grad.is_sparse:\r\n                    raise RuntimeError(\'RAdam does not support sparse gradients\')\r\n\r\n                p_data_fp32 = p.data.float()\r\n\r\n                state = self.state[p]\r\n\r\n                if len(state) == 0:\r\n                    state[\'step\'] = 0\r\n                    state[\'exp_avg\'] = torch.zeros_like(p_data_fp32)\r\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p_data_fp32)\r\n                else:\r\n                    state[\'exp_avg\'] = state[\'exp_avg\'].type_as(p_data_fp32)\r\n                    state[\'exp_avg_sq\'] = state[\'exp_avg_sq\'].type_as(p_data_fp32)\r\n\r\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\r\n                beta1, beta2 = group[\'betas\']\r\n\r\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n\r\n                state[\'step\'] += 1\r\n                buffered = self.buffer[int(state[\'step\'] % 10)]\r\n                if state[\'step\'] == buffered[0]:\r\n                    N_sma, step_size = buffered[1], buffered[2]\r\n                else:\r\n                    buffered[0] = state[\'step\']\r\n                    beta2_t = beta2 ** state[\'step\']\r\n                    N_sma_max = 2 / (1 - beta2) - 1\r\n                    N_sma = N_sma_max - 2 * state[\'step\'] * beta2_t / (1 - beta2_t)\r\n                    buffered[1] = N_sma\r\n\r\n                    # more conservative since it\'s an approximated value\r\n                    if N_sma >= 5:\r\n                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state[\'step\'])\r\n                    else:\r\n                        step_size = 1.0 / (1 - beta1 ** state[\'step\'])\r\n                    buffered[2] = step_size\r\n\r\n                if group[\'weight_decay\'] != 0:\r\n                    p_data_fp32.add_(-group[\'weight_decay\'] * group[\'lr\'], p_data_fp32)\r\n\r\n                # more conservative since it\'s an approximated value\r\n                if N_sma >= 5:\r\n                    denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\r\n                    p_data_fp32.addcdiv_(-step_size * group[\'lr\'], exp_avg, denom)\r\n                else:\r\n                    p_data_fp32.add_(-step_size * group[\'lr\'], exp_avg)\r\n\r\n                p.data.copy_(p_data_fp32)\r\n\r\n        return loss'"
callback/optimizater/ralamb.py,3,"b""import math\r\nimport torch\r\nfrom torch.optim.optimizer import Optimizer\r\n\r\nclass Ralamb(Optimizer):\r\n    '''\r\n    RAdam + LARS\r\n    Example:\r\n        >>> model = ResNet()\r\n        >>> optimizer = Ralamb(model.parameters(), lr=0.001)\r\n    '''\r\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\r\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\r\n        self.buffer = [[None, None, None] for ind in range(10)]\r\n        super(Ralamb, self).__init__(params, defaults)\r\n\r\n    def __setstate__(self, state):\r\n        super(Ralamb, self).__setstate__(state)\r\n\r\n    def step(self, closure=None):\r\n\r\n        loss = None\r\n        if closure is not None:\r\n            loss = closure()\r\n\r\n        for group in self.param_groups:\r\n\r\n            for p in group['params']:\r\n                if p.grad is None:\r\n                    continue\r\n                grad = p.grad.data.float()\r\n                if grad.is_sparse:\r\n                    raise RuntimeError('Ralamb does not support sparse gradients')\r\n\r\n                p_data_fp32 = p.data.float()\r\n\r\n                state = self.state[p]\r\n\r\n                if len(state) == 0:\r\n                    state['step'] = 0\r\n                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\r\n                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\r\n                else:\r\n                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\r\n                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\r\n\r\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\r\n                beta1, beta2 = group['betas']\r\n\r\n                # Decay the first and second moment running average coefficient\r\n                # m_t\r\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n                # v_t\r\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n\r\n                state['step'] += 1\r\n                buffered = self.buffer[int(state['step'] % 10)]\r\n\r\n                if state['step'] == buffered[0]:\r\n                    N_sma, radam_step_size = buffered[1], buffered[2]\r\n                else:\r\n                    buffered[0] = state['step']\r\n                    beta2_t = beta2 ** state['step']\r\n                    N_sma_max = 2 / (1 - beta2) - 1\r\n                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\r\n                    buffered[1] = N_sma\r\n\r\n                    # more conservative since it's an approximated value\r\n                    if N_sma >= 5:\r\n                        radam_step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\r\n                    else:\r\n                        radam_step_size = 1.0 / (1 - beta1 ** state['step'])\r\n                    buffered[2] = radam_step_size\r\n\r\n                if group['weight_decay'] != 0:\r\n                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\r\n\r\n                # more conservative since it's an approximated value\r\n                radam_step = p_data_fp32.clone()\r\n                if N_sma >= 5:\r\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\r\n                    radam_step.addcdiv_(-radam_step_size * group['lr'], exp_avg, denom)\r\n                else:\r\n                    radam_step.add_(-radam_step_size * group['lr'], exp_avg)\r\n\r\n                radam_norm = radam_step.pow(2).sum().sqrt()\r\n                weight_norm = p.data.pow(2).sum().sqrt().clamp(0, 10)\r\n                if weight_norm == 0 or radam_norm == 0:\r\n                    trust_ratio = 1\r\n                else:\r\n                    trust_ratio = weight_norm / radam_norm\r\n\r\n                state['weight_norm'] = weight_norm\r\n                state['adam_norm'] = radam_norm\r\n                state['trust_ratio'] = trust_ratio\r\n\r\n                if N_sma >= 5:\r\n                    p_data_fp32.addcdiv_(-radam_step_size * group['lr'] * trust_ratio, exp_avg, denom)\r\n                else:\r\n                    p_data_fp32.add_(-radam_step_size * group['lr'] * trust_ratio, exp_avg)\r\n\r\n                p.data.copy_(p_data_fp32)\r\n\r\n        return loss"""
callback/optimizater/ralars.py,4,"b'import math\r\nimport torch\r\nfrom torch.optim.optimizer import Optimizer\r\n\r\n\r\nclass RaLars(Optimizer):\r\n    """"""Implements the RAdam optimizer from https://arxiv.org/pdf/1908.03265.pdf\r\n    with optional Layer-wise adaptive Scaling from https://arxiv.org/pdf/1708.03888.pdf\r\n\r\n    Args:\r\n        params (iterable): iterable of parameters to optimize or dicts defining parameter groups\r\n        lr (float, optional): learning rate\r\n        betas (Tuple[float, float], optional): coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999))\r\n        eps (float, optional): term added to the denominator to improve numerical stability (default: 1e-8)\r\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\r\n        scale_clip (float, optional): the maximal upper bound for the scale factor of LARS\r\n    Example:\r\n        >>> model = ResNet()\r\n        >>> optimizer = RaLars(model.parameters(), lr=0.001)\r\n    """"""\r\n\r\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0,\r\n                 scale_clip=None):\r\n        if not 0.0 <= lr:\r\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\r\n        if not 0.0 <= eps:\r\n            raise ValueError(""Invalid epsilon value: {}"".format(eps))\r\n        if not 0.0 <= betas[0] < 1.0:\r\n            raise ValueError(""Invalid beta parameter at index 0: {}"".format(betas[0]))\r\n        if not 0.0 <= betas[1] < 1.0:\r\n            raise ValueError(""Invalid beta parameter at index 1: {}"".format(betas[1]))\r\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\r\n        super(RaLars, self).__init__(params, defaults)\r\n        # LARS arguments\r\n        self.scale_clip = scale_clip\r\n        if self.scale_clip is None:\r\n            self.scale_clip = (0, 10)\r\n\r\n    def step(self, closure=None):\r\n        """"""Performs a single optimization step.\r\n        Arguments:\r\n            closure (callable, optional): A closure that reevaluates the model\r\n                and returns the loss.\r\n        """"""\r\n        loss = None\r\n        if closure is not None:\r\n            loss = closure()\r\n\r\n        for group in self.param_groups:\r\n\r\n            # Get group-shared variables\r\n            beta1, beta2 = group[\'betas\']\r\n            sma_inf = group.get(\'sma_inf\')\r\n            # Compute max length of SMA on first step\r\n            if not isinstance(sma_inf, float):\r\n                group[\'sma_inf\'] = 2 / (1 - beta2) - 1\r\n                sma_inf = group.get(\'sma_inf\')\r\n\r\n            for p in group[\'params\']:\r\n                if p.grad is None:\r\n                    continue\r\n                grad = p.grad.data\r\n                if grad.is_sparse:\r\n                    raise RuntimeError(\'RAdam does not support sparse gradients\')\r\n\r\n                state = self.state[p]\r\n\r\n                # State initialization\r\n                if len(state) == 0:\r\n                    state[\'step\'] = 0\r\n                    # Exponential moving average of gradient values\r\n                    state[\'exp_avg\'] = torch.zeros_like(p.data)\r\n                    # Exponential moving average of squared gradient values\r\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\r\n\r\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\r\n\r\n                state[\'step\'] += 1\r\n\r\n                # Decay the first and second moment running average coefficient\r\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n\r\n                # Bias correction\r\n                bias_correction1 = 1 - beta1 ** state[\'step\']\r\n                bias_correction2 = 1 - beta2 ** state[\'step\']\r\n\r\n                # Compute length of SMA\r\n                sma_t = sma_inf - 2 * state[\'step\'] * (1 - bias_correction2) / bias_correction2\r\n\r\n                update = torch.zeros_like(p.data)\r\n                if sma_t > 4:\r\n                    # \xc2\xa0Variance rectification term\r\n                    r_t = math.sqrt((sma_t - 4) * (sma_t - 2) * sma_inf / ((sma_inf - 4) * (sma_inf - 2) * sma_t))\r\n                    # \xc2\xa0Adaptive momentum\r\n                    update.addcdiv_(r_t, exp_avg / bias_correction1,\r\n                                    (exp_avg_sq / bias_correction2).sqrt().add_(group[\'eps\']))\r\n                else:\r\n                    # Unadapted momentum\r\n                    update.add_(exp_avg / bias_correction1)\r\n\r\n                # Weight decay\r\n                if group[\'weight_decay\'] != 0:\r\n                    update.add_(group[\'weight_decay\'], p.data)\r\n\r\n                # LARS\r\n                p_norm = p.data.pow(2).sum().sqrt()\r\n                update_norm = update.pow(2).sum().sqrt()\r\n                phi_p = p_norm.clamp(*self.scale_clip)\r\n                # Compute the local LR\r\n                if phi_p == 0 or update_norm == 0:\r\n                    local_lr = 1\r\n                else:\r\n                    local_lr = phi_p / update_norm\r\n\r\n                state[\'local_lr\'] = local_lr\r\n\r\n                p.data.add_(-group[\'lr\'] * local_lr, update)\r\n\r\n        return loss\r\n'"
callback/optimizater/sgdw.py,2,"b'import torch\r\nfrom torch.optim.optimizer import Optimizer\r\n\r\nclass SGDW(Optimizer):\r\n    r""""""Implements stochastic gradient descent (optionally with momentum) with\r\n    weight decay from the paper `Fixing Weight Decay Regularization in Adam`_.\r\n\r\n    Nesterov momentum is based on the formula from\r\n    `On the importance of initialization and momentum in deep learning`__.\r\n\r\n    Args:\r\n        params (iterable): iterable of parameters to optimize or dicts defining\r\n            parameter groups\r\n        lr (float): learning rate\r\n        momentum (float, optional): momentum factor (default: 0)\r\n        weight_decay (float, optional): weight decay factor (default: 0)\r\n        dampening (float, optional): dampening for momentum (default: 0)\r\n        nesterov (bool, optional): enables Nesterov momentum (default: False)\r\n\r\n    .. _Fixing Weight Decay Regularization in Adam:\r\n        https://arxiv.org/abs/1711.05101\r\n\r\n    Example:\r\n        >>> model = LSTM()\r\n        >>> optimizer = SGDW(model.parameters(), lr=0.1, momentum=0.9,weight_decay=1e-5)\r\n    """"""\r\n    def __init__(self, params, lr=0.1, momentum=0, dampening=0,\r\n                 weight_decay=0, nesterov=False):\r\n        if lr < 0.0:\r\n            raise ValueError(f""Invalid learning rate: {lr}"")\r\n        if momentum < 0.0:\r\n            raise ValueError(f""Invalid momentum value: {momentum}"")\r\n        if weight_decay < 0.0:\r\n            raise ValueError(f""Invalid weight_decay value: {weight_decay}"")\r\n\r\n        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\r\n                        weight_decay=weight_decay, nesterov=nesterov)\r\n        if nesterov and (momentum <= 0 or dampening != 0):\r\n            raise ValueError(""Nesterov momentum requires a momentum and zero dampening"")\r\n        super(SGDW, self).__init__(params, defaults)\r\n\r\n    def __setstate__(self, state):\r\n        super(SGDW, self).__setstate__(state)\r\n        for group in self.param_groups:\r\n            group.setdefault(\'nesterov\', False)\r\n\r\n    def step(self, closure=None):\r\n        """"""Performs a single optimization step.\r\n\r\n        Arguments:\r\n            closure (callable, optional): A closure that reevaluates the model\r\n                and returns the loss.\r\n        """"""\r\n        loss = None\r\n        if closure is not None:\r\n            loss = closure()\r\n\r\n        for group in self.param_groups:\r\n            weight_decay = group[\'weight_decay\']\r\n            momentum = group[\'momentum\']\r\n            dampening = group[\'dampening\']\r\n            nesterov = group[\'nesterov\']\r\n            for p in group[\'params\']:\r\n                if p.grad is None:\r\n                    continue\r\n                d_p = p.grad.data\r\n                if momentum != 0:\r\n                    param_state = self.state[p]\r\n                    if \'momentum_buffer\' not in param_state:\r\n                        buf = param_state[\'momentum_buffer\'] = torch.zeros_like(p.data)\r\n                        buf.mul_(momentum).add_(d_p)\r\n                    else:\r\n                        buf = param_state[\'momentum_buffer\']\r\n                        buf.mul_(momentum).add_(1 - dampening, d_p)\r\n                    if nesterov:\r\n                        d_p = d_p.add(momentum, buf)\r\n                    else:\r\n                        d_p = buf\r\n                if weight_decay != 0:\r\n                    p.data.add_(-weight_decay, p.data)\r\n                p.data.add_(-group[\'lr\'], d_p)\r\n        return loss'"
datasets/cluener/__init__.py,0,b'\n\n'
models/layers/__init__.py,0,b''
models/layers/crf.py,67,"b'import torch\nimport torch.nn as nn\nfrom typing import List, Optional\n\nclass CRF(nn.Module):\n    """"""Conditional random field.\n    This module implements a conditional random field [LMP01]_. The forward computation\n    of this class computes the log likelihood of the given sequence of tags and\n    emission score tensor. This class also has `~CRF.decode` method which finds\n    the best tag sequence given an emission score tensor using `Viterbi algorithm`_.\n    Args:\n        num_tags: Number of tags.\n        batch_first: Whether the first dimension corresponds to the size of a minibatch.\n    Attributes:\n        start_transitions (`~torch.nn.Parameter`): Start transition score tensor of size\n            ``(num_tags,)``.\n        end_transitions (`~torch.nn.Parameter`): End transition score tensor of size\n            ``(num_tags,)``.\n        transitions (`~torch.nn.Parameter`): Transition score tensor of size\n            ``(num_tags, num_tags)``.\n    .. [LMP01] Lafferty, J., McCallum, A., Pereira, F. (2001).\n       ""Conditional random fields: Probabilistic models for segmenting and\n       labeling sequence data"". *Proc. 18th International Conf. on Machine\n       Learning*. Morgan Kaufmann. pp. 282\xe2\x80\x93289.\n    .. _Viterbi algorithm: https://en.wikipedia.org/wiki/Viterbi_algorithm\n    """"""\n\n    def __init__(self, num_tags: int, batch_first: bool = False) -> None:\n        if num_tags <= 0:\n            raise ValueError(f\'invalid number of tags: {num_tags}\')\n        super().__init__()\n        self.num_tags = num_tags\n        self.batch_first = batch_first\n        self.start_transitions = nn.Parameter(torch.empty(num_tags))\n        self.end_transitions = nn.Parameter(torch.empty(num_tags))\n        self.transitions = nn.Parameter(torch.empty(num_tags, num_tags))\n\n        self.reset_parameters()\n\n    def reset_parameters(self) -> None:\n        """"""Initialize the transition parameters.\n        The parameters will be initialized randomly from a uniform distribution\n        between -0.1 and 0.1.\n        """"""\n        nn.init.uniform_(self.start_transitions, -0.1, 0.1)\n        nn.init.uniform_(self.end_transitions, -0.1, 0.1)\n        nn.init.uniform_(self.transitions, -0.1, 0.1)\n\n    def __repr__(self) -> str:\n        return f\'{self.__class__.__name__}(num_tags={self.num_tags})\'\n\n    def forward(self, emissions: torch.Tensor,\n                tags: torch.LongTensor,\n                mask: Optional[torch.ByteTensor] = None,\n                reduction: str = \'mean\') -> torch.Tensor:\n        """"""Compute the conditional log likelihood of a sequence of tags given emission scores.\n        Args:\n            emissions (`~torch.Tensor`): Emission score tensor of size\n                ``(seq_length, batch_size, num_tags)`` if ``batch_first`` is ``False``,\n                ``(batch_size, seq_length, num_tags)`` otherwise.\n            tags (`~torch.LongTensor`): Sequence of tags tensor of size\n                ``(seq_length, batch_size)`` if ``batch_first`` is ``False``,\n                ``(batch_size, seq_length)`` otherwise.\n            mask (`~torch.ByteTensor`): Mask tensor of size ``(seq_length, batch_size)``\n                if ``batch_first`` is ``False``, ``(batch_size, seq_length)`` otherwise.\n            reduction: Specifies  the reduction to apply to the output:\n                ``none|sum|mean|token_mean``. ``none``: no reduction will be applied.\n                ``sum``: the output will be summed over batches. ``mean``: the output will be\n                averaged over batches. ``token_mean``: the output will be averaged over tokens.\n        Returns:\n            `~torch.Tensor`: The log likelihood. This will have size ``(batch_size,)`` if\n            reduction is ``none``, ``()`` otherwise.\n        """"""\n        if reduction not in (\'none\', \'sum\', \'mean\', \'token_mean\'):\n            raise ValueError(f\'invalid reduction: {reduction}\')\n        if mask is None:\n            mask = torch.ones_like(tags, dtype=torch.uint8, device=tags.device)\n        if mask.dtype != torch.uint8:\n            mask = mask.byte()\n        self._validate(emissions, tags=tags, mask=mask)\n\n        if self.batch_first:\n            emissions = emissions.transpose(0, 1)\n            tags = tags.transpose(0, 1)\n            mask = mask.transpose(0, 1)\n\n        # shape: (batch_size,)\n        numerator = self._compute_score(emissions, tags, mask)\n        # shape: (batch_size,)\n        denominator = self._compute_normalizer(emissions, mask)\n        # shape: (batch_size,)\n        llh = numerator - denominator\n\n        if reduction == \'none\':\n            return llh\n        if reduction == \'sum\':\n            return llh.sum()\n        if reduction == \'mean\':\n            return llh.mean()\n        return llh.sum() / mask.float().sum()\n\n    def decode(self, emissions: torch.Tensor,\n               mask: Optional[torch.ByteTensor] = None,\n               nbest: Optional[int] = None,\n               pad_tag: Optional[int] = None) -> List[List[List[int]]]:\n        """"""Find the most likely tag sequence using Viterbi algorithm.\n        Args:\n            emissions (`~torch.Tensor`): Emission score tensor of size\n                ``(seq_length, batch_size, num_tags)`` if ``batch_first`` is ``False``,\n                ``(batch_size, seq_length, num_tags)`` otherwise.\n            mask (`~torch.ByteTensor`): Mask tensor of size ``(seq_length, batch_size)``\n                if ``batch_first`` is ``False``, ``(batch_size, seq_length)`` otherwise.\n            nbest (`int`): Number of most probable paths for each sequence\n            pad_tag (`int`): Tag at padded positions. Often input varies in length and\n                the length will be padded to the maximum length in the batch. Tags at\n                the padded positions will be assigned with a padding tag, i.e. `pad_tag`\n        Returns:\n            A PyTorch tensor of the best tag sequence for each batch of shape\n            (nbest, batch_size, seq_length)\n        """"""\n        if nbest is None:\n            nbest = 1\n        if mask is None:\n            mask = torch.ones(emissions.shape[:2], dtype=torch.uint8,\n                              device=emissions.device)\n        if mask.dtype != torch.uint8:\n            mask = mask.byte()\n        self._validate(emissions, mask=mask)\n\n        if self.batch_first:\n            emissions = emissions.transpose(0, 1)\n            mask = mask.transpose(0, 1)\n\n        if nbest == 1:\n            return self._viterbi_decode(emissions, mask, pad_tag).unsqueeze(0)\n        return self._viterbi_decode_nbest(emissions, mask, nbest, pad_tag)\n\n    def _validate(self, emissions: torch.Tensor,\n                  tags: Optional[torch.LongTensor] = None,\n                  mask: Optional[torch.ByteTensor] = None) -> None:\n        if emissions.dim() != 3:\n            raise ValueError(f\'emissions must have dimension of 3, got {emissions.dim()}\')\n        if emissions.size(2) != self.num_tags:\n            raise ValueError(\n                f\'expected last dimension of emissions is {self.num_tags}, \'\n                f\'got {emissions.size(2)}\')\n\n        if tags is not None:\n            if emissions.shape[:2] != tags.shape:\n                raise ValueError(\n                    \'the first two dimensions of emissions and tags must match, \'\n                    f\'got {tuple(emissions.shape[:2])} and {tuple(tags.shape)}\')\n\n        if mask is not None:\n            if emissions.shape[:2] != mask.shape:\n                raise ValueError(\n                    \'the first two dimensions of emissions and mask must match, \'\n                    f\'got {tuple(emissions.shape[:2])} and {tuple(mask.shape)}\')\n            no_empty_seq = not self.batch_first and mask[0].all()\n            no_empty_seq_bf = self.batch_first and mask[:, 0].all()\n            if not no_empty_seq and not no_empty_seq_bf:\n                raise ValueError(\'mask of the first timestep must all be on\')\n\n    def _compute_score(self, emissions: torch.Tensor,\n                       tags: torch.LongTensor,\n                       mask: torch.ByteTensor) -> torch.Tensor:\n        # emissions: (seq_length, batch_size, num_tags)\n        # tags: (seq_length, batch_size)\n        # mask: (seq_length, batch_size)\n        seq_length, batch_size = tags.shape\n        mask = mask.float()\n\n        # Start transition score and first emission\n        # shape: (batch_size,)\n        score = self.start_transitions[tags[0]]\n        score += emissions[0, torch.arange(batch_size), tags[0]]\n\n        for i in range(1, seq_length):\n            # Transition score to next tag, only added if next timestep is valid (mask == 1)\n            # shape: (batch_size,)\n            score += self.transitions[tags[i - 1], tags[i]] * mask[i]\n\n            # Emission score for next tag, only added if next timestep is valid (mask == 1)\n            # shape: (batch_size,)\n            score += emissions[i, torch.arange(batch_size), tags[i]] * mask[i]\n\n        # End transition score\n        # shape: (batch_size,)\n        seq_ends = mask.long().sum(dim=0) - 1\n        # shape: (batch_size,)\n        last_tags = tags[seq_ends, torch.arange(batch_size)]\n        # shape: (batch_size,)\n        score += self.end_transitions[last_tags]\n\n        return score\n\n    def _compute_normalizer(self, emissions: torch.Tensor,\n                            mask: torch.ByteTensor) -> torch.Tensor:\n        # emissions: (seq_length, batch_size, num_tags)\n        # mask: (seq_length, batch_size)\n        seq_length = emissions.size(0)\n\n        # Start transition score and first emission; score has size of\n        # (batch_size, num_tags) where for each batch, the j-th column stores\n        # the score that the first timestep has tag j\n        # shape: (batch_size, num_tags)\n        score = self.start_transitions + emissions[0]\n\n        for i in range(1, seq_length):\n            # Broadcast score for every possible next tag\n            # shape: (batch_size, num_tags, 1)\n            broadcast_score = score.unsqueeze(2)\n\n            # Broadcast emission score for every possible current tag\n            # shape: (batch_size, 1, num_tags)\n            broadcast_emissions = emissions[i].unsqueeze(1)\n\n            # Compute the score tensor of size (batch_size, num_tags, num_tags) where\n            # for each sample, entry at row i and column j stores the sum of scores of all\n            # possible tag sequences so far that end with transitioning from tag i to tag j\n            # and emitting\n            # shape: (batch_size, num_tags, num_tags)\n            next_score = broadcast_score + self.transitions + broadcast_emissions\n\n            # Sum over all possible current tags, but we\'re in score space, so a sum\n            # becomes a log-sum-exp: for each sample, entry i stores the sum of scores of\n            # all possible tag sequences so far, that end in tag i\n            # shape: (batch_size, num_tags)\n            next_score = torch.logsumexp(next_score, dim=1)\n\n            # Set score to the next score if this timestep is valid (mask == 1)\n            # shape: (batch_size, num_tags)\n            score = torch.where(mask[i].unsqueeze(1), next_score, score)\n\n        # End transition score\n        # shape: (batch_size, num_tags)\n        score += self.end_transitions\n\n        # Sum (log-sum-exp) over all possible tags\n        # shape: (batch_size,)\n        return torch.logsumexp(score, dim=1)\n\n    def _viterbi_decode(self, emissions: torch.FloatTensor,\n                        mask: torch.ByteTensor,\n                        pad_tag: Optional[int] = None) -> List[List[int]]:\n        # emissions: (seq_length, batch_size, num_tags)\n        # mask: (seq_length, batch_size)\n        # return: (batch_size, seq_length)\n        if pad_tag is None:\n            pad_tag = 0\n\n        device = emissions.device\n        seq_length, batch_size = mask.shape\n\n        # Start transition and first emission\n        # shape: (batch_size, num_tags)\n        score = self.start_transitions + emissions[0]\n        history_idx = torch.zeros((seq_length, batch_size, self.num_tags),\n                                  dtype=torch.long, device=device)\n        oor_idx = torch.zeros((batch_size, self.num_tags),\n                              dtype=torch.long, device=device)\n        oor_tag = torch.full((seq_length, batch_size), pad_tag,\n                             dtype=torch.long, device=device)\n\n        # - score is a tensor of size (batch_size, num_tags) where for every batch,\n        #   value at column j stores the score of the best tag sequence so far that ends\n        #   with tag j\n        # - history_idx saves where the best tags candidate transitioned from; this is used\n        #   when we trace back the best tag sequence\n        # - oor_idx saves the best tags candidate transitioned from at the positions\n        #   where mask is 0, i.e. out of range (oor)\n\n        # Viterbi algorithm recursive case: we compute the score of the best tag sequence\n        # for every possible next tag\n        for i in range(1, seq_length):\n            # Broadcast viterbi score for every possible next tag\n            # shape: (batch_size, num_tags, 1)\n            broadcast_score = score.unsqueeze(2)\n\n            # Broadcast emission score for every possible current tag\n            # shape: (batch_size, 1, num_tags)\n            broadcast_emission = emissions[i].unsqueeze(1)\n\n            # Compute the score tensor of size (batch_size, num_tags, num_tags) where\n            # for each sample, entry at row i and column j stores the score of the best\n            # tag sequence so far that ends with transitioning from tag i to tag j and emitting\n            # shape: (batch_size, num_tags, num_tags)\n            next_score = broadcast_score + self.transitions + broadcast_emission\n\n            # Find the maximum score over all possible current tag\n            # shape: (batch_size, num_tags)\n            next_score, indices = next_score.max(dim=1)\n\n            # Set score to the next score if this timestep is valid (mask == 1)\n            # and save the index that produces the next score\n            # shape: (batch_size, num_tags)\n            score = torch.where(mask[i].unsqueeze(-1), next_score, score)\n            indices = torch.where(mask[i].unsqueeze(-1), indices, oor_idx)\n            history_idx[i - 1] = indices\n\n        # End transition score\n        # shape: (batch_size, num_tags)\n        end_score = score + self.end_transitions\n        _, end_tag = end_score.max(dim=1)\n\n        # shape: (batch_size,)\n        seq_ends = mask.long().sum(dim=0) - 1\n\n        # insert the best tag at each sequence end (last position with mask == 1)\n        history_idx = history_idx.transpose(1, 0).contiguous()\n        history_idx.scatter_(1, seq_ends.view(-1, 1, 1).expand(-1, 1, self.num_tags),\n                             end_tag.view(-1, 1, 1).expand(-1, 1, self.num_tags))\n        history_idx = history_idx.transpose(1, 0).contiguous()\n\n        # The most probable path for each sequence\n        best_tags_arr = torch.zeros((seq_length, batch_size),\n                                    dtype=torch.long, device=device)\n        best_tags = torch.zeros(batch_size, 1, dtype=torch.long, device=device)\n        for idx in range(seq_length - 1, -1, -1):\n            best_tags = torch.gather(history_idx[idx], 1, best_tags)\n            best_tags_arr[idx] = best_tags.data.view(batch_size)\n\n        return torch.where(mask, best_tags_arr, oor_tag).transpose(0, 1)\n\n    def _viterbi_decode_nbest(self, emissions: torch.FloatTensor,\n                              mask: torch.ByteTensor,\n                              nbest: int,\n                              pad_tag: Optional[int] = None) -> List[List[List[int]]]:\n        # emissions: (seq_length, batch_size, num_tags)\n        # mask: (seq_length, batch_size)\n        # return: (nbest, batch_size, seq_length)\n        if pad_tag is None:\n            pad_tag = 0\n\n        device = emissions.device\n        seq_length, batch_size = mask.shape\n\n        # Start transition and first emission\n        # shape: (batch_size, num_tags)\n        score = self.start_transitions + emissions[0]\n        history_idx = torch.zeros((seq_length, batch_size, self.num_tags, nbest),\n                                  dtype=torch.long, device=device)\n        oor_idx = torch.zeros((batch_size, self.num_tags, nbest),\n                              dtype=torch.long, device=device)\n        oor_tag = torch.full((seq_length, batch_size, nbest), pad_tag,\n                             dtype=torch.long, device=device)\n\n        # + score is a tensor of size (batch_size, num_tags) where for every batch,\n        #   value at column j stores the score of the best tag sequence so far that ends\n        #   with tag j\n        # + history_idx saves where the best tags candidate transitioned from; this is used\n        #   when we trace back the best tag sequence\n        # - oor_idx saves the best tags candidate transitioned from at the positions\n        #   where mask is 0, i.e. out of range (oor)\n\n        # Viterbi algorithm recursive case: we compute the score of the best tag sequence\n        # for every possible next tag\n        for i in range(1, seq_length):\n            if i == 1:\n                broadcast_score = score.unsqueeze(-1)\n                broadcast_emission = emissions[i].unsqueeze(1)\n                # shape: (batch_size, num_tags, num_tags)\n                next_score = broadcast_score + self.transitions + broadcast_emission\n            else:\n                broadcast_score = score.unsqueeze(-1)\n                broadcast_emission = emissions[i].unsqueeze(1).unsqueeze(2)\n                # shape: (batch_size, num_tags, nbest, num_tags)\n                next_score = broadcast_score + self.transitions.unsqueeze(1) + broadcast_emission\n\n            # Find the top `nbest` maximum score over all possible current tag\n            # shape: (batch_size, nbest, num_tags)\n            next_score, indices = next_score.view(batch_size, -1, self.num_tags).topk(nbest, dim=1)\n\n            if i == 1:\n                score = score.unsqueeze(-1).expand(-1, -1, nbest)\n                indices = indices * nbest\n\n            # convert to shape: (batch_size, num_tags, nbest)\n            next_score = next_score.transpose(2, 1)\n            indices = indices.transpose(2, 1)\n\n            # Set score to the next score if this timestep is valid (mask == 1)\n            # and save the index that produces the next score\n            # shape: (batch_size, num_tags, nbest)\n            score = torch.where(mask[i].unsqueeze(-1).unsqueeze(-1), next_score, score)\n            indices = torch.where(mask[i].unsqueeze(-1).unsqueeze(-1), indices, oor_idx)\n            history_idx[i - 1] = indices\n\n        # End transition score shape: (batch_size, num_tags, nbest)\n        end_score = score + self.end_transitions.unsqueeze(-1)\n        _, end_tag = end_score.view(batch_size, -1).topk(nbest, dim=1)\n\n        # shape: (batch_size,)\n        seq_ends = mask.long().sum(dim=0) - 1\n\n        # insert the best tag at each sequence end (last position with mask == 1)\n        history_idx = history_idx.transpose(1, 0).contiguous()\n        history_idx.scatter_(1, seq_ends.view(-1, 1, 1, 1).expand(-1, 1, self.num_tags, nbest),\n                             end_tag.view(-1, 1, 1, nbest).expand(-1, 1, self.num_tags, nbest))\n        history_idx = history_idx.transpose(1, 0).contiguous()\n\n        # The most probable path for each sequence\n        best_tags_arr = torch.zeros((seq_length, batch_size, nbest),\n                                    dtype=torch.long, device=device)\n        best_tags = torch.arange(nbest, dtype=torch.long, device=device) \\\n                         .view(1, -1).expand(batch_size, -1)\n        for idx in range(seq_length - 1, -1, -1):\n            best_tags = torch.gather(history_idx[idx].view(batch_size, -1), 1, best_tags)\n            best_tags_arr[idx] = best_tags.data.view(batch_size, -1) // nbest\n\n        return torch.where(mask.unsqueeze(-1), best_tags_arr, oor_tag).permute(2, 1, 0)'"
models/layers/linears.py,3,"b'import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nclass FeedForwardNetwork(nn.Module):\r\n    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0):\r\n        super(FeedForwardNetwork, self).__init__()\r\n        self.dropout_rate = dropout_rate\r\n        self.linear1 = nn.Linear(input_size, hidden_size)\r\n        self.linear2 = nn.Linear(hidden_size, output_size)\r\n\r\n    def forward(self, x):\r\n        x_proj = F.dropout(F.relu(self.linear1(x)), p=self.dropout_rate, training=self.training)\r\n        x_proj = self.linear2(x_proj)\r\n        return x_proj\r\n\r\n\r\nclass PoolerStartLogits(nn.Module):\r\n    def __init__(self, hidden_size, num_classes):\r\n        super(PoolerStartLogits, self).__init__()\r\n        self.dense = nn.Linear(hidden_size, num_classes)\r\n\r\n    def forward(self, hidden_states, p_mask=None):\r\n        x = self.dense(hidden_states)\r\n        return x\r\n\r\nclass PoolerEndLogits(nn.Module):\r\n    def __init__(self, hidden_size, num_classes):\r\n        super(PoolerEndLogits, self).__init__()\r\n        self.dense_0 = nn.Linear(hidden_size, hidden_size)\r\n        self.activation = nn.Tanh()\r\n        self.LayerNorm = nn.LayerNorm(hidden_size)\r\n        self.dense_1 = nn.Linear(hidden_size, num_classes)\r\n\r\n    def forward(self, hidden_states, start_positions=None, p_mask=None):\r\n        x = self.dense_0(torch.cat([hidden_states, start_positions], dim=-1))\r\n        x = self.activation(x)\r\n        x = self.LayerNorm(x)\r\n        x = self.dense_1(x)\r\n        return x\r\n'"
models/transformers/__init__.py,0,"b'__version__ = ""2.1.1""\n\n# Work around to update TensorFlow\'s absl.logging threshold which alters the\n# default Python logging output behavior when present.\n# see: https://github.com/abseil/abseil-py/issues/99\n# and: https://github.com/tensorflow/tensorflow/issues/26691#issuecomment-500369493\ntry:\n    import absl.logging\n    absl.logging.set_verbosity(\'info\')\n    absl.logging.set_stderrthreshold(\'info\')\n    absl.logging._warn_preinit_stderr = False\nexcept:\n    pass\n\nimport logging\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n# Files and general utilities\nfrom .file_utils import (TRANSFORMERS_CACHE, PYTORCH_TRANSFORMERS_CACHE, PYTORCH_PRETRAINED_BERT_CACHE,\n                         cached_path, add_start_docstrings, add_end_docstrings,\n                         WEIGHTS_NAME, TF2_WEIGHTS_NAME, TF_WEIGHTS_NAME, CONFIG_NAME,\n                         is_tf_available, is_torch_available)\n\n# Tokenizers\nfrom .tokenization_utils import (PreTrainedTokenizer)\nfrom .tokenization_auto import AutoTokenizer\nfrom .tokenization_bert import BertTokenizer, BasicTokenizer, WordpieceTokenizer\nfrom .tokenization_openai import OpenAIGPTTokenizer\nfrom .tokenization_transfo_xl import (TransfoXLTokenizer, TransfoXLCorpus)\nfrom .tokenization_gpt2 import GPT2Tokenizer\nfrom .tokenization_ctrl import CTRLTokenizer\nfrom .tokenization_xlnet import XLNetTokenizer, SPIECE_UNDERLINE\nfrom .tokenization_xlm import XLMTokenizer\nfrom .tokenization_roberta import RobertaTokenizer\nfrom .tokenization_distilbert import DistilBertTokenizer\nfrom .tokenization_albert import FullTokenizer\n\n# Configurations\nfrom .configuration_utils import PretrainedConfig\nfrom .configuration_auto import AutoConfig\nfrom .configuration_bert import BertConfig, BERT_PRETRAINED_CONFIG_ARCHIVE_MAP\nfrom .configuration_openai import OpenAIGPTConfig, OPENAI_GPT_PRETRAINED_CONFIG_ARCHIVE_MAP\nfrom .configuration_transfo_xl import TransfoXLConfig, TRANSFO_XL_PRETRAINED_CONFIG_ARCHIVE_MAP\nfrom .configuration_gpt2 import GPT2Config, GPT2_PRETRAINED_CONFIG_ARCHIVE_MAP\nfrom .configuration_ctrl import CTRLConfig, CTRL_PRETRAINED_CONFIG_ARCHIVE_MAP\nfrom .configuration_xlnet import XLNetConfig, XLNET_PRETRAINED_CONFIG_ARCHIVE_MAP\nfrom .configuration_ctrl import CTRLConfig, CTRL_PRETRAINED_CONFIG_ARCHIVE_MAP\nfrom .configuration_xlm import XLMConfig, XLM_PRETRAINED_CONFIG_ARCHIVE_MAP\nfrom .configuration_roberta import RobertaConfig, ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP\nfrom .configuration_distilbert import DistilBertConfig, DISTILBERT_PRETRAINED_CONFIG_ARCHIVE_MAP\nfrom .configuration_albert import AlbertConfig\n\n# Modeling\nif is_torch_available():\n    from .modeling_utils import (PreTrainedModel, prune_layer, Conv1D)\n    from .modeling_auto import (AutoModel, AutoModelForSequenceClassification, AutoModelForQuestionAnswering,\n                                AutoModelWithLMHead)\n\n    from .modeling_bert import (BertPreTrainedModel, BertModel, BertForPreTraining,\n                                BertForMaskedLM, BertForNextSentencePrediction,\n                                BertForSequenceClassification, BertForMultipleChoice,\n                                BertForTokenClassification, BertForQuestionAnswering,\n                                load_tf_weights_in_bert, BERT_PRETRAINED_MODEL_ARCHIVE_MAP)\n    from .modeling_openai import (OpenAIGPTPreTrainedModel, OpenAIGPTModel,\n                                OpenAIGPTLMHeadModel, OpenAIGPTDoubleHeadsModel,\n                                load_tf_weights_in_openai_gpt, OPENAI_GPT_PRETRAINED_MODEL_ARCHIVE_MAP)\n    from .modeling_transfo_xl import (TransfoXLPreTrainedModel, TransfoXLModel, TransfoXLLMHeadModel,\n                                    load_tf_weights_in_transfo_xl, TRANSFO_XL_PRETRAINED_MODEL_ARCHIVE_MAP)\n    from .modeling_gpt2 import (GPT2PreTrainedModel, GPT2Model,\n                                GPT2LMHeadModel, GPT2DoubleHeadsModel,\n                                load_tf_weights_in_gpt2, GPT2_PRETRAINED_MODEL_ARCHIVE_MAP)\n    from .modeling_ctrl import (CTRLPreTrainedModel, CTRLModel,\n                                CTRLLMHeadModel,\n                                CTRL_PRETRAINED_MODEL_ARCHIVE_MAP)\n    from .modeling_xlnet import (XLNetPreTrainedModel, XLNetModel, XLNetLMHeadModel,\n                                XLNetForSequenceClassification, XLNetForMultipleChoice,\n                                XLNetForQuestionAnsweringSimple, XLNetForQuestionAnswering,\n                                load_tf_weights_in_xlnet, XLNET_PRETRAINED_MODEL_ARCHIVE_MAP)\n    from .modeling_xlm import (XLMPreTrainedModel , XLMModel,\n                            XLMWithLMHeadModel, XLMForSequenceClassification,\n                            XLMForQuestionAnswering, XLMForQuestionAnsweringSimple,\n                            XLM_PRETRAINED_MODEL_ARCHIVE_MAP)\n    from .modeling_roberta import (RobertaForMaskedLM, RobertaModel,\n                                RobertaForSequenceClassification, RobertaForMultipleChoice,\n                                ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP)\n    from .modeling_distilbert import (DistilBertForMaskedLM, DistilBertModel,\n                                DistilBertForSequenceClassification, DistilBertForQuestionAnswering,\n                                DISTILBERT_PRETRAINED_MODEL_ARCHIVE_MAP)\n    from .modeling_albert import AlbertModel\n\nif not is_tf_available() and not is_torch_available():\n    logger.warning(""Neither PyTorch nor TensorFlow >= 2.0 have been found.""\n                   ""Models won\'t be available and only tokenizers, configuration""\n                   ""and file/data utilities can be used."")\n'"
models/transformers/__main__.py,1,"b'# coding: utf8\ndef main():\n    import sys\n    if (len(sys.argv) < 4 or len(sys.argv) > 6) or sys.argv[1] not in [""bert"", ""gpt"", ""transfo_xl"", ""gpt2"", ""xlnet"", ""xlm""]:\n        print(\n        ""This command line utility let you convert original (author released) model checkpoint to pytorch.\\n""\n        ""It should be used as one of: \\n""\n        "">> transformers bert TF_CHECKPOINT TF_CONFIG PYTORCH_DUMP_OUTPUT, \\n""\n        "">> transformers gpt OPENAI_GPT_CHECKPOINT_FOLDER_PATH PYTORCH_DUMP_OUTPUT [OPENAI_GPT_CONFIG], \\n""\n        "">> transformers transfo_xl TF_CHECKPOINT_OR_DATASET PYTORCH_DUMP_OUTPUT [TF_CONFIG] or \\n""\n        "">> transformers gpt2 TF_CHECKPOINT PYTORCH_DUMP_OUTPUT [GPT2_CONFIG] or \\n""\n        "">> transformers xlnet TF_CHECKPOINT TF_CONFIG PYTORCH_DUMP_OUTPUT [FINETUNING_TASK_NAME] or \\n""\n        "">> transformers xlm XLM_CHECKPOINT_PATH PYTORCH_DUMP_OUTPUT"")\n    else:\n        if sys.argv[1] == ""bert"":\n            try:\n                from convert_bert_original_tf_checkpoint_to_pytorch import convert_tf_checkpoint_to_pytorch\n            except ImportError:\n                print(""transformers can only be used from the commandline to convert TensorFlow models in PyTorch, ""\n                    ""In that case, it requires TensorFlow to be installed. Please see ""\n                    ""https://www.tensorflow.org/install/ for installation instructions."")\n                raise\n\n            if len(sys.argv) != 5:\n                # pylint: disable=line-too-long\n                print(""Should be used as `transformers bert TF_CHECKPOINT TF_CONFIG PYTORCH_DUMP_OUTPUT`"")\n            else:\n                PYTORCH_DUMP_OUTPUT = sys.argv.pop()\n                TF_CONFIG = sys.argv.pop()\n                TF_CHECKPOINT = sys.argv.pop()\n                convert_tf_checkpoint_to_pytorch(TF_CHECKPOINT, TF_CONFIG, PYTORCH_DUMP_OUTPUT)\n        elif sys.argv[1] == ""gpt"":\n            from .convert_openai_original_tf_checkpoint_to_pytorch import convert_openai_checkpoint_to_pytorch\n            if len(sys.argv) < 4 or len(sys.argv) > 5:\n                # pylint: disable=line-too-long\n                print(""Should be used as `transformers gpt OPENAI_GPT_CHECKPOINT_FOLDER_PATH PYTORCH_DUMP_OUTPUT [OPENAI_GPT_CONFIG]`"")\n            else:\n                OPENAI_GPT_CHECKPOINT_FOLDER_PATH = sys.argv[2]\n                PYTORCH_DUMP_OUTPUT = sys.argv[3]\n                if len(sys.argv) == 5:\n                    OPENAI_GPT_CONFIG = sys.argv[4]\n                else:\n                    OPENAI_GPT_CONFIG = """"\n                convert_openai_checkpoint_to_pytorch(OPENAI_GPT_CHECKPOINT_FOLDER_PATH,\n                                                    OPENAI_GPT_CONFIG,\n                                                    PYTORCH_DUMP_OUTPUT)\n        elif sys.argv[1] == ""transfo_xl"":\n            try:\n                from .convert_transfo_xl_original_tf_checkpoint_to_pytorch import convert_transfo_xl_checkpoint_to_pytorch\n            except ImportError:\n                print(""transformers can only be used from the commandline to convert TensorFlow models in PyTorch, ""\n                    ""In that case, it requires TensorFlow to be installed. Please see ""\n                    ""https://www.tensorflow.org/install/ for installation instructions."")\n                raise\n            if len(sys.argv) < 4 or len(sys.argv) > 5:\n                # pylint: disable=line-too-long\n                print(""Should be used as `transformers transfo_xl TF_CHECKPOINT/TF_DATASET_FILE PYTORCH_DUMP_OUTPUT [TF_CONFIG]`"")\n            else:\n                if \'ckpt\' in sys.argv[2].lower():\n                    TF_CHECKPOINT = sys.argv[2]\n                    TF_DATASET_FILE = """"\n                else:\n                    TF_DATASET_FILE = sys.argv[2]\n                    TF_CHECKPOINT = """"\n                PYTORCH_DUMP_OUTPUT = sys.argv[3]\n                if len(sys.argv) == 5:\n                    TF_CONFIG = sys.argv[4]\n                else:\n                    TF_CONFIG = """"\n                convert_transfo_xl_checkpoint_to_pytorch(TF_CHECKPOINT, TF_CONFIG, PYTORCH_DUMP_OUTPUT, TF_DATASET_FILE)\n        elif sys.argv[1] == ""gpt2"":\n            try:\n                from convert_gpt2_original_tf_checkpoint_to_pytorch import convert_gpt2_checkpoint_to_pytorch\n            except ImportError:\n                print(""transformers can only be used from the commandline to convert TensorFlow models in PyTorch, ""\n                    ""In that case, it requires TensorFlow to be installed. Please see ""\n                    ""https://www.tensorflow.org/install/ for installation instructions."")\n                raise\n\n            if len(sys.argv) < 4 or len(sys.argv) > 5:\n                # pylint: disable=line-too-long\n                print(""Should be used as `transformers gpt2 TF_CHECKPOINT PYTORCH_DUMP_OUTPUT [TF_CONFIG]`"")\n            else:\n                TF_CHECKPOINT = sys.argv[2]\n                PYTORCH_DUMP_OUTPUT = sys.argv[3]\n                if len(sys.argv) == 5:\n                    TF_CONFIG = sys.argv[4]\n                else:\n                    TF_CONFIG = """"\n                convert_gpt2_checkpoint_to_pytorch(TF_CHECKPOINT, TF_CONFIG, PYTORCH_DUMP_OUTPUT)\n        elif sys.argv[1] == ""xlnet"":\n            try:\n                from convert_xlnet_original_tf_checkpoint_to_pytorch import convert_xlnet_checkpoint_to_pytorch\n            except ImportError:\n                print(""transformers can only be used from the commandline to convert TensorFlow models in PyTorch, ""\n                    ""In that case, it requires TensorFlow to be installed. Please see ""\n                    ""https://www.tensorflow.org/install/ for installation instructions."")\n                raise\n\n            if len(sys.argv) < 5 or len(sys.argv) > 6:\n                # pylint: disable=line-too-long\n                print(""Should be used as `transformers xlnet TF_CHECKPOINT TF_CONFIG PYTORCH_DUMP_OUTPUT [FINETUNING_TASK_NAME]`"")\n            else:\n                TF_CHECKPOINT = sys.argv[2]\n                TF_CONFIG = sys.argv[3]\n                PYTORCH_DUMP_OUTPUT = sys.argv[4]\n                if len(sys.argv) == 6:\n                    FINETUNING_TASK = sys.argv[5]\n                else:\n                    FINETUNING_TASK = None\n\n                convert_xlnet_checkpoint_to_pytorch(TF_CHECKPOINT,\n                                                    TF_CONFIG,\n                                                    PYTORCH_DUMP_OUTPUT,\n                                                    FINETUNING_TASK)\n        elif sys.argv[1] == ""xlm"":\n            from .convert_xlm_original_pytorch_checkpoint_to_pytorch import convert_xlm_checkpoint_to_pytorch\n\n            if len(sys.argv) != 4:\n                # pylint: disable=line-too-long\n                print(""Should be used as `transformers xlm XLM_CHECKPOINT_PATH PYTORCH_DUMP_OUTPUT`"")\n            else:\n                XLM_CHECKPOINT_PATH = sys.argv[2]\n                PYTORCH_DUMP_OUTPUT = sys.argv[3]\n\n                convert_xlm_checkpoint_to_pytorch(XLM_CHECKPOINT_PATH, PYTORCH_DUMP_OUTPUT)\n\nif __name__ == \'__main__\':\n    main()\n'"
models/transformers/configuration_albert.py,0,"b'"""""" BERT model configuration """"""\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport json\nimport logging\nimport sys\nfrom io import open\n\nfrom .configuration_utils import PretrainedConfig\nlogger = logging.getLogger(__name__)\n\nclass AlbertConfig(PretrainedConfig):\n    r""""""\n        Arguments:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n            hidden_size: Size of the encoder layers and the pooler layer.\n            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n            num_attention_heads: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            intermediate_size: The size of the ""intermediate"" (i.e., feed-forward)\n                layer in the Transformer encoder.\n            hidden_act: The non-linear activation function (function or string) in the\n                encoder and pooler. If string, ""gelu"", ""relu"" and ""swish"" are supported.\n            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            attention_probs_dropout_prob: The dropout ratio for the attention\n                probabilities.\n            max_position_embeddings: The maximum sequence length that this model might\n                ever be used with. Typically set this to something large just in case\n                (e.g., 512 or 1024 or 2048).\n            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n                `BertModel`.\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n            layer_norm_eps: The epsilon used by LayerNorm.\n    """"""\n    def __init__(self,\n                 vocab_size_or_config_json_file=30000,\n                 embedding_size=128,\n                 hidden_size=4096,\n                 num_hidden_layers=12,\n                 num_hidden_groups=1,\n                 num_attention_heads=64,\n                 intermediate_size=16384,\n                 inner_group_num=1,\n                 hidden_act=""gelu_new"",\n                 hidden_dropout_prob=0,\n                 attention_probs_dropout_prob=0,\n                 max_position_embeddings=512,\n                 type_vocab_size=2,\n                 initializer_range=0.02,\n                 layer_norm_eps=1e-12,\n                 **kwargs):\n        super(AlbertConfig, self).__init__(**kwargs)\n        if isinstance(vocab_size_or_config_json_file, str) or (sys.version_info[0] == 2\n                        and isinstance(vocab_size_or_config_json_file, unicode)):\n            with open(vocab_size_or_config_json_file, ""r"", encoding=\'utf-8\') as reader:\n                json_config = json.loads(reader.read())\n            for key, value in json_config.items():\n                self.__dict__[key] = value\n        elif isinstance(vocab_size_or_config_json_file, int):\n            self.vocab_size = vocab_size_or_config_json_file\n            self.hidden_size = hidden_size\n            self.num_hidden_layers = num_hidden_layers\n            self.num_attention_heads = num_attention_heads\n            self.hidden_act = hidden_act\n            self.intermediate_size = intermediate_size\n            self.hidden_dropout_prob = hidden_dropout_prob\n            self.attention_probs_dropout_prob = attention_probs_dropout_prob\n            self.max_position_embeddings = max_position_embeddings\n            self.type_vocab_size = type_vocab_size\n            self.initializer_range = initializer_range\n            self.layer_norm_eps = layer_norm_eps\n            self.embedding_size = embedding_size\n            self.inner_group_num = inner_group_num\n            self.num_hidden_groups = num_hidden_groups\n        else:\n            raise ValueError(""First argument must be either a vocabulary size (int)""\n                             "" or the path to a pretrained model config file (str)"")\n'"
models/transformers/configuration_auto.py,0,"b'# coding=utf-8\n# Copyright 2018 The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" Auto Model class. """"""\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport logging\n\nfrom .configuration_bert import BertConfig\nfrom .configuration_openai import OpenAIGPTConfig\nfrom .configuration_gpt2 import GPT2Config\nfrom .configuration_transfo_xl import TransfoXLConfig\nfrom .configuration_xlnet import XLNetConfig\nfrom .configuration_xlm import XLMConfig\nfrom .configuration_roberta import RobertaConfig\nfrom .configuration_distilbert import DistilBertConfig\nfrom .configuration_ctrl import CTRLConfig\n\nlogger = logging.getLogger(__name__)\n\n\nclass AutoConfig(object):\n    r"""""":class:`~transformers.AutoConfig` is a generic configuration class\n        that will be instantiated as one of the configuration classes of the library\n        when created with the `AutoConfig.from_pretrained(pretrained_model_name_or_path)`\n        class method.\n\n        The `from_pretrained()` method take care of returning the correct model class instance\n        using pattern matching on the `pretrained_model_name_or_path` string.\n\n        The base model class to instantiate is selected as the first pattern matching\n        in the `pretrained_model_name_or_path` string (in the following order):\n            - contains `distilbert`: DistilBertConfig (DistilBERT model)\n            - contains `bert`: BertConfig (Bert model)\n            - contains `openai-gpt`: OpenAIGPTConfig (OpenAI GPT model)\n            - contains `gpt2`: GPT2Config (OpenAI GPT-2 model)\n            - contains `transfo-xl`: TransfoXLConfig (Transformer-XL model)\n            - contains `xlnet`: XLNetConfig (XLNet model)\n            - contains `xlm`: XLMConfig (XLM model)\n            - contains `roberta`: RobertaConfig (RoBERTa model)\n            - contains `ctrl` : CTRLConfig (CTRL model)\n        This class cannot be instantiated using `__init__()` (throw an error).\n    """"""\n    def __init__(self):\n        raise EnvironmentError(""AutoConfig is designed to be instantiated ""\n            ""using the `AutoConfig.from_pretrained(pretrained_model_name_or_path)` method."")\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n        r"""""" Instantiate a one of the configuration classes of the library\n        from a pre-trained model configuration.\n\n        The configuration class to instantiate is selected as the first pattern matching\n        in the `pretrained_model_name_or_path` string (in the following order):\n            - contains `distilbert`: DistilBertConfig (DistilBERT model)\n            - contains `bert`: BertConfig (Bert model)\n            - contains `openai-gpt`: OpenAIGPTConfig (OpenAI GPT model)\n            - contains `gpt2`: GPT2Config (OpenAI GPT-2 model)\n            - contains `transfo-xl`: TransfoXLConfig (Transformer-XL model)\n            - contains `xlnet`: XLNetConfig (XLNet model)\n            - contains `xlm`: XLMConfig (XLM model)\n            - contains `roberta`: RobertaConfig (RoBERTa model)\n            - contains `ctrl` : CTRLConfig (CTRL model)\n        Params:\n            pretrained_model_name_or_path: either:\n\n                - a string with the `shortcut name` of a pre-trained model configuration to load from cache or download, e.g.: ``bert-base-uncased``.\n                - a path to a `directory` containing a configuration file saved using the :func:`~transformers.PretrainedConfig.save_pretrained` method, e.g.: ``./my_model_directory/``.\n                - a path or url to a saved configuration JSON `file`, e.g.: ``./my_model_directory/configuration.json``.\n\n            cache_dir: (`optional`) string:\n                Path to a directory in which a downloaded pre-trained model\n                configuration should be cached if the standard cache should not be used.\n\n            kwargs: (`optional`) dict: key/value pairs with which to update the configuration object after loading.\n\n                - The values in kwargs of any keys which are configuration attributes will be used to override the loaded values.\n                - Behavior concerning key/value pairs whose keys are *not* configuration attributes is controlled by the `return_unused_kwargs` keyword parameter.\n\n            force_download: (`optional`) boolean, default False:\n                Force to (re-)download the model weights and configuration files and override the cached versions if they exists.\n\n            proxies: (`optional`) dict, default None:\n                A dictionary of proxy servers to use by protocol or endpoint, e.g.: {\'http\': \'foo.bar:3128\', \'http://hostname\': \'foo.bar:4012\'}.\n                The proxies are used on each request.\n\n            return_unused_kwargs: (`optional`) bool:\n\n                - If False, then this function returns just the final configuration object.\n                - If True, then this functions returns a tuple `(config, unused_kwargs)` where `unused_kwargs` is a dictionary consisting of the key/value pairs whose keys are not configuration attributes: ie the part of kwargs which has not been used to update `config` and is otherwise ignored.\n\n        Examples::\n\n            config = AutoConfig.from_pretrained(\'bert-base-uncased\')    # Download configuration from S3 and cache.\n            config = AutoConfig.from_pretrained(\'./test/bert_saved_model/\')  # E.g. config (or model) was saved using `save_pretrained(\'./test/saved_model/\')`\n            config = AutoConfig.from_pretrained(\'./test/bert_saved_model/my_configuration.json\')\n            config = AutoConfig.from_pretrained(\'bert-base-uncased\', output_attention=True, foo=False)\n            assert config.output_attention == True\n            config, unused_kwargs = AutoConfig.from_pretrained(\'bert-base-uncased\', output_attention=True,\n                                                               foo=False, return_unused_kwargs=True)\n            assert config.output_attention == True\n            assert unused_kwargs == {\'foo\': False}\n\n        """"""\n        if \'distilbert\' in pretrained_model_name_or_path:\n            return DistilBertConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\n        elif \'roberta\' in pretrained_model_name_or_path:\n            return RobertaConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\n        elif \'bert\' in pretrained_model_name_or_path:\n            return BertConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\n        elif \'openai-gpt\' in pretrained_model_name_or_path:\n            return OpenAIGPTConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\n        elif \'gpt2\' in pretrained_model_name_or_path:\n            return GPT2Config.from_pretrained(pretrained_model_name_or_path, **kwargs)\n        elif \'transfo-xl\' in pretrained_model_name_or_path:\n            return TransfoXLConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\n        elif \'xlnet\' in pretrained_model_name_or_path:\n            return XLNetConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\n        elif \'xlm\' in pretrained_model_name_or_path:\n            return XLMConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\n        elif \'ctrl\' in pretrained_model_name_or_path:\n            return CTRLConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\n        raise ValueError(""Unrecognized model identifier in {}. Should contains one of ""\n                         ""\'bert\', \'openai-gpt\', \'gpt2\', \'transfo-xl\', \'xlnet\', ""\n                         ""\'xlm\', \'roberta\', \'ctrl\'"".format(pretrained_model_name_or_path))\n'"
models/transformers/configuration_bert.py,0,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" BERT model configuration """"""\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport json\nimport logging\nimport sys\nfrom io import open\n\nfrom .configuration_utils import PretrainedConfig\n\nlogger = logging.getLogger(__name__)\n\nBERT_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n    \'bert-base-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json"",\n    \'bert-large-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-config.json"",\n    \'bert-base-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json"",\n    \'bert-large-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-config.json"",\n    \'bert-base-multilingual-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json"",\n    \'bert-base-multilingual-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-config.json"",\n    \'bert-base-chinese\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-config.json"",\n    \'bert-base-german-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-cased-config.json"",\n    \'bert-large-uncased-whole-word-masking\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-config.json"",\n    \'bert-large-cased-whole-word-masking\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-config.json"",\n    \'bert-large-uncased-whole-word-masking-finetuned-squad\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-config.json"",\n    \'bert-large-cased-whole-word-masking-finetuned-squad\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-finetuned-squad-config.json"",\n    \'bert-base-cased-finetuned-mrpc\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-mrpc-config.json"",\n    \'bert-base-german-dbmdz-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-config.json"",\n    \'bert-base-german-dbmdz-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-uncased-config.json"",\n}\n\n\nclass BertConfig(PretrainedConfig):\n    r""""""\n        :class:`~transformers.BertConfig` is the configuration class to store the configuration of a\n        `BertModel`.\n\n\n        Arguments:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n            hidden_size: Size of the encoder layers and the pooler layer.\n            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n            num_attention_heads: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            intermediate_size: The size of the ""intermediate"" (i.e., feed-forward)\n                layer in the Transformer encoder.\n            hidden_act: The non-linear activation function (function or string) in the\n                encoder and pooler. If string, ""gelu"", ""relu"", ""swish"" and ""gelu_new"" are supported.\n            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            attention_probs_dropout_prob: The dropout ratio for the attention\n                probabilities.\n            max_position_embeddings: The maximum sequence length that this model might\n                ever be used with. Typically set this to something large just in case\n                (e.g., 512 or 1024 or 2048).\n            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n                `BertModel`.\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n            layer_norm_eps: The epsilon used by LayerNorm.\n    """"""\n    pretrained_config_archive_map = BERT_PRETRAINED_CONFIG_ARCHIVE_MAP\n\n    def __init__(self,\n                 vocab_size_or_config_json_file=30522,\n                 hidden_size=768,\n                 num_hidden_layers=12,\n                 num_attention_heads=12,\n                 intermediate_size=3072,\n                 hidden_act=""gelu"",\n                 hidden_dropout_prob=0.1,\n                 attention_probs_dropout_prob=0.1,\n                 max_position_embeddings=512,\n                 type_vocab_size=2,\n                 initializer_range=0.02,\n                 layer_norm_eps=1e-12,\n                 **kwargs):\n        super(BertConfig, self).__init__(**kwargs)\n        if isinstance(vocab_size_or_config_json_file, str) or (sys.version_info[0] == 2\n                        and isinstance(vocab_size_or_config_json_file, unicode)):\n            with open(vocab_size_or_config_json_file, ""r"", encoding=\'utf-8\') as reader:\n                json_config = json.loads(reader.read())\n            for key, value in json_config.items():\n                self.__dict__[key] = value\n        elif isinstance(vocab_size_or_config_json_file, int):\n            self.vocab_size = vocab_size_or_config_json_file\n            self.hidden_size = hidden_size\n            self.num_hidden_layers = num_hidden_layers\n            self.num_attention_heads = num_attention_heads\n            self.hidden_act = hidden_act\n            self.intermediate_size = intermediate_size\n            self.hidden_dropout_prob = hidden_dropout_prob\n            self.attention_probs_dropout_prob = attention_probs_dropout_prob\n            self.max_position_embeddings = max_position_embeddings\n            self.type_vocab_size = type_vocab_size\n            self.initializer_range = initializer_range\n            self.layer_norm_eps = layer_norm_eps\n        else:\n            raise ValueError(""First argument must be either a vocabulary size (int)""\n                             "" or the path to a pretrained model config file (str)"")\n'"
models/transformers/configuration_ctrl.py,0,"b'# coding=utf-8\n# Copyright 2018 Salesforce and HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" Salesforce CTRL configuration """"""\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport json\nimport logging\nimport sys\nfrom io import open\n\nfrom .configuration_utils import PretrainedConfig\n\nlogger = logging.getLogger(__name__)\n\nCTRL_PRETRAINED_CONFIG_ARCHIVE_MAP = {""ctrl"": ""https://storage.googleapis.com/sf-ctrl/pytorch/ctrl-config.json""}\n\nclass CTRLConfig(PretrainedConfig):\n    """"""Configuration class to store the configuration of a `CTRLModel`.\n\n    Args:\n        vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `CTRLModel` or a configuration json file.\n        n_positions: Number of positional embeddings.\n        n_ctx: Size of the causal mask (usually same as n_positions).\n        dff: Size of the inner dimension of the FFN.\n        n_embd: Dimensionality of the embeddings and hidden states.\n        n_layer: Number of hidden layers in the Transformer encoder.\n        n_head: Number of attention heads for each attention layer in\n            the Transformer encoder.\n        layer_norm_epsilon: epsilon to use in the layer norm layers\n        resid_pdrop: The dropout probabilitiy for all fully connected\n            layers in the embeddings, encoder, and pooler.\n        attn_pdrop: The dropout ratio for the attention\n            probabilities.\n        embd_pdrop: The dropout ratio for the embeddings.\n        initializer_range: The sttdev of the truncated_normal_initializer for\n            initializing all weight matrices.\n    """"""\n    pretrained_config_archive_map = CTRL_PRETRAINED_CONFIG_ARCHIVE_MAP\n\n    def __init__(\n        self,\n        vocab_size_or_config_json_file=246534,\n        n_positions=256,\n        n_ctx=256,\n        n_embd=1280,\n        dff=8192,\n        n_layer=48,\n        n_head=16,\n        resid_pdrop=0.1,\n        embd_pdrop=0.1,\n        attn_pdrop=0.1,\n        layer_norm_epsilon=1e-6,\n        initializer_range=0.02,\n\n        num_labels=1,\n        summary_type=\'cls_index\',\n        summary_use_proj=True,\n        summary_activation=None,\n        summary_proj_to_labels=True,\n        summary_first_dropout=0.1,\n        **kwargs\n    ):\n        """"""Constructs CTRLConfig.\n\n        Args:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `CTRLModel` or a configuration json file.\n            n_positions: Number of positional embeddings.\n            n_ctx: Size of the causal mask (usually same as n_positions).\n            dff: Size of the inner dimension of the FFN.\n            n_embd: Dimensionality of the embeddings and hidden states.\n            n_layer: Number of hidden layers in the Transformer encoder.\n            n_head: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            layer_norm_epsilon: epsilon to use in the layer norm layers\n            resid_pdrop: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            attn_pdrop: The dropout ratio for the attention\n                probabilities.\n            embd_pdrop: The dropout ratio for the embeddings.\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n        """"""\n        super(CTRLConfig, self).__init__(**kwargs)\n\n        self.vocab_size = vocab_size_or_config_json_file if isinstance(vocab_size_or_config_json_file, int) else -1\n        self.n_ctx = n_ctx\n        self.n_positions = n_positions\n        self.n_embd = n_embd\n        self.n_layer = n_layer\n        self.n_head = n_head\n        self.dff = dff\n        self.resid_pdrop = resid_pdrop\n        self.embd_pdrop = embd_pdrop\n        self.attn_pdrop = attn_pdrop\n        self.layer_norm_epsilon = layer_norm_epsilon\n        self.initializer_range = initializer_range\n\n        self.num_labels = num_labels\n        self.summary_type = summary_type\n        self.summary_use_proj = summary_use_proj\n        self.summary_activation = summary_activation\n        self.summary_first_dropout = summary_first_dropout\n        self.summary_proj_to_labels = summary_proj_to_labels\n        if isinstance(vocab_size_or_config_json_file, str) or (sys.version_info[0] == 2\n                        and isinstance(vocab_size_or_config_json_file, unicode)):\n            with open(vocab_size_or_config_json_file, ""r"", encoding=""utf-8"") as reader:\n                json_config = json.loads(reader.read())\n            for key, value in json_config.items():\n                self.__dict__[key] = value\n        elif not isinstance(vocab_size_or_config_json_file, int):\n            raise ValueError(\n                ""First argument must be either a vocabulary size (int)""\n                ""or the path to a pretrained model config file (str)""\n            )\n\n    @property\n    def max_position_embeddings(self):\n        return self.n_positions\n\n    @property\n    def hidden_size(self):\n        return self.n_embd\n\n    @property\n    def num_attention_heads(self):\n        return self.n_head\n\n    @property\n    def num_hidden_layers(self):\n        return self.n_layer\n'"
models/transformers/configuration_distilbert.py,0,"b'# coding=utf-8\n# Copyright 2019-present, the HuggingFace Inc. team, The Google AI Language Team and Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" DistilBERT model configuration """"""\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport sys\nimport json\nimport logging\nfrom io import open\n\nfrom .configuration_utils import PretrainedConfig\n\nlogger = logging.getLogger(__name__)\n\nDISTILBERT_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n    \'distilbert-base-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-config.json"",\n    \'distilbert-base-uncased-distilled-squad\': ""https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-distilled-squad-config.json""\n}\n\n\nclass DistilBertConfig(PretrainedConfig):\n    pretrained_config_archive_map = DISTILBERT_PRETRAINED_CONFIG_ARCHIVE_MAP\n\n    def __init__(self,\n                 vocab_size_or_config_json_file=30522,\n                 max_position_embeddings=512,\n                 sinusoidal_pos_embds=False,\n                 n_layers=6,\n                 n_heads=12,\n                 dim=768,\n                 hidden_dim=4*768,\n                 dropout=0.1,\n                 attention_dropout=0.1,\n                 activation=\'gelu\',\n                 initializer_range=0.02,\n                 tie_weights_=True,\n                 qa_dropout=0.1,\n                 seq_classif_dropout=0.2,\n                 **kwargs):\n        super(DistilBertConfig, self).__init__(**kwargs)\n\n        if isinstance(vocab_size_or_config_json_file, str) or (sys.version_info[0] == 2\n                        and isinstance(vocab_size_or_config_json_file, unicode)):\n            with open(vocab_size_or_config_json_file, ""r"", encoding=\'utf-8\') as reader:\n                json_config = json.loads(reader.read())\n            for key, value in json_config.items():\n                self.__dict__[key] = value\n        elif isinstance(vocab_size_or_config_json_file, int):\n            self.vocab_size = vocab_size_or_config_json_file\n            self.max_position_embeddings = max_position_embeddings\n            self.sinusoidal_pos_embds = sinusoidal_pos_embds\n            self.n_layers = n_layers\n            self.n_heads = n_heads\n            self.dim = dim\n            self.hidden_dim = hidden_dim\n            self.dropout = dropout\n            self.attention_dropout = attention_dropout\n            self.activation = activation\n            self.initializer_range = initializer_range\n            self.tie_weights_ = tie_weights_\n            self.qa_dropout = qa_dropout\n            self.seq_classif_dropout = seq_classif_dropout\n        else:\n            raise ValueError(""First argument must be either a vocabulary size (int)""\n                             "" or the path to a pretrained model config file (str)"")\n    @property\n    def hidden_size(self):\n        return self.dim\n\n    @property\n    def num_attention_heads(self):\n        return self.n_heads\n\n    @property\n    def num_hidden_layers(self):\n        return self.n_layers\n'"
models/transformers/configuration_gpt2.py,0,"b'# coding=utf-8\n# Copyright 2018 The OpenAI Team Authors and HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" OpenAI GPT-2 configuration """"""\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport json\nimport logging\nimport sys\nfrom io import open\n\nfrom .configuration_utils import PretrainedConfig\n\nlogger = logging.getLogger(__name__)\n\nGPT2_PRETRAINED_CONFIG_ARCHIVE_MAP = {""gpt2"": ""https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json"",\n                                      ""gpt2-medium"": ""https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-config.json"",\n                                      ""gpt2-large"": ""https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-config.json"",\n                                      ""distilgpt2"": ""https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-config.json"",}\n\nclass GPT2Config(PretrainedConfig):\n    """"""Configuration class to store the configuration of a `GPT2Model`.\n\n    Args:\n        vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `GPT2Model` or a configuration json file.\n        n_positions: Number of positional embeddings.\n        n_ctx: Size of the causal mask (usually same as n_positions).\n        n_embd: Dimensionality of the embeddings and hidden states.\n        n_layer: Number of hidden layers in the Transformer encoder.\n        n_head: Number of attention heads for each attention layer in\n            the Transformer encoder.\n        layer_norm_epsilon: epsilon to use in the layer norm layers\n        resid_pdrop: The dropout probabilitiy for all fully connected\n            layers in the embeddings, encoder, and pooler.\n        attn_pdrop: The dropout ratio for the attention\n            probabilities.\n        embd_pdrop: The dropout ratio for the embeddings.\n        initializer_range: The sttdev of the truncated_normal_initializer for\n            initializing all weight matrices.\n    """"""\n    pretrained_config_archive_map = GPT2_PRETRAINED_CONFIG_ARCHIVE_MAP\n\n    def __init__(\n        self,\n        vocab_size_or_config_json_file=50257,\n        n_positions=1024,\n        n_ctx=1024,\n        n_embd=768,\n        n_layer=12,\n        n_head=12,\n        resid_pdrop=0.1,\n        embd_pdrop=0.1,\n        attn_pdrop=0.1,\n        layer_norm_epsilon=1e-5,\n        initializer_range=0.02,\n\n        num_labels=1,\n        summary_type=\'cls_index\',\n        summary_use_proj=True,\n        summary_activation=None,\n        summary_proj_to_labels=True,\n        summary_first_dropout=0.1,\n        **kwargs\n    ):\n        """"""Constructs GPT2Config.\n\n        Args:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `GPT2Model` or a configuration json file.\n            n_positions: Number of positional embeddings.\n            n_ctx: Size of the causal mask (usually same as n_positions).\n            n_embd: Dimensionality of the embeddings and hidden states.\n            n_layer: Number of hidden layers in the Transformer encoder.\n            n_head: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            layer_norm_epsilon: epsilon to use in the layer norm layers\n            resid_pdrop: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            attn_pdrop: The dropout ratio for the attention\n                probabilities.\n            embd_pdrop: The dropout ratio for the embeddings.\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n        """"""\n        super(GPT2Config, self).__init__(**kwargs)\n\n        if isinstance(vocab_size_or_config_json_file, str) or (sys.version_info[0] == 2\n                        and isinstance(vocab_size_or_config_json_file, unicode)):\n            with open(vocab_size_or_config_json_file, ""r"", encoding=""utf-8"") as reader:\n                json_config = json.loads(reader.read())\n            for key, value in json_config.items():\n                self.__dict__[key] = value\n        elif isinstance(vocab_size_or_config_json_file, int):\n            self.vocab_size = vocab_size_or_config_json_file\n            self.n_ctx = n_ctx\n            self.n_positions = n_positions\n            self.n_embd = n_embd\n            self.n_layer = n_layer\n            self.n_head = n_head\n            self.resid_pdrop = resid_pdrop\n            self.embd_pdrop = embd_pdrop\n            self.attn_pdrop = attn_pdrop\n            self.layer_norm_epsilon = layer_norm_epsilon\n            self.initializer_range = initializer_range\n\n            self.num_labels = num_labels\n            self.summary_type = summary_type\n            self.summary_use_proj = summary_use_proj\n            self.summary_activation = summary_activation\n            self.summary_first_dropout = summary_first_dropout\n            self.summary_proj_to_labels = summary_proj_to_labels\n        else:\n            raise ValueError(\n                ""First argument must be either a vocabulary size (int)""\n                ""or the path to a pretrained model config file (str)""\n            )\n\n    @property\n    def max_position_embeddings(self):\n        return self.n_positions\n\n    @property\n    def hidden_size(self):\n        return self.n_embd\n\n    @property\n    def num_attention_heads(self):\n        return self.n_head\n\n    @property\n    def num_hidden_layers(self):\n        return self.n_layer\n'"
models/transformers/configuration_openai.py,0,"b'# coding=utf-8\n# Copyright 2018 The OpenAI Team Authors and HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" OpenAI GPT configuration """"""\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport json\nimport logging\nimport sys\nfrom io import open\n\nfrom .configuration_utils import PretrainedConfig\n\nlogger = logging.getLogger(__name__)\n\nOPENAI_GPT_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n    ""openai-gpt"": ""https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-config.json""\n}\n\nclass OpenAIGPTConfig(PretrainedConfig):\n    """"""\n    Configuration class to store the configuration of a `OpenAIGPTModel`.\n\n    Args:\n        vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `OpenAIGPTModel` or a configuration json file.\n        n_positions: Number of positional embeddings.\n        n_ctx: Size of the causal mask (usually same as n_positions).\n        n_embd: Dimensionality of the embeddings and hidden states.\n        n_layer: Number of hidden layers in the Transformer encoder.\n        n_head: Number of attention heads for each attention layer in\n            the Transformer encoder.\n        afn: The non-linear activation function (function or string) in the\n            encoder and pooler. If string, ""gelu"", ""relu"" and ""swish"" are supported.\n        resid_pdrop: The dropout probabilitiy for all fully connected\n            layers in the embeddings, encoder, and pooler.\n        attn_pdrop: The dropout ratio for the attention\n            probabilities.\n        embd_pdrop: The dropout ratio for the embeddings.\n        layer_norm_epsilon: epsilon to use in the layer norm layers\n        initializer_range: The sttdev of the truncated_normal_initializer for\n            initializing all weight matrices.\n        predict_special_tokens: should we predict special tokens (when the model has a LM head)\n    """"""\n    pretrained_config_archive_map = OPENAI_GPT_PRETRAINED_CONFIG_ARCHIVE_MAP\n\n    def __init__(\n        self,\n        vocab_size_or_config_json_file=40478,\n        n_positions=512,\n        n_ctx=512,\n        n_embd=768,\n        n_layer=12,\n        n_head=12,\n        afn=""gelu"",\n        resid_pdrop=0.1,\n        embd_pdrop=0.1,\n        attn_pdrop=0.1,\n        layer_norm_epsilon=1e-5,\n        initializer_range=0.02,\n        predict_special_tokens=True,\n\n        num_labels=1,\n        summary_type=\'cls_index\',\n        summary_use_proj=True,\n        summary_activation=None,\n        summary_proj_to_labels=True,\n        summary_first_dropout=0.1,\n        **kwargs\n    ):\n        """"""Constructs OpenAIGPTConfig.\n        """"""\n        super(OpenAIGPTConfig, self).__init__(**kwargs)\n\n        if isinstance(vocab_size_or_config_json_file, str) or (sys.version_info[0] == 2\n                        and isinstance(vocab_size_or_config_json_file, unicode)):\n            with open(vocab_size_or_config_json_file, ""r"", encoding=""utf-8"") as reader:\n                json_config = json.loads(reader.read())\n            for key, value in json_config.items():\n                self.__dict__[key] = value\n        elif isinstance(vocab_size_or_config_json_file, int):\n            self.vocab_size = vocab_size_or_config_json_file\n            self.n_ctx = n_ctx\n            self.n_positions = n_positions\n            self.n_embd = n_embd\n            self.n_layer = n_layer\n            self.n_head = n_head\n            self.afn = afn\n            self.resid_pdrop = resid_pdrop\n            self.embd_pdrop = embd_pdrop\n            self.attn_pdrop = attn_pdrop\n            self.layer_norm_epsilon = layer_norm_epsilon\n            self.initializer_range = initializer_range\n            self.predict_special_tokens = predict_special_tokens\n\n            self.num_labels = num_labels\n            self.summary_type = summary_type\n            self.summary_use_proj = summary_use_proj\n            self.summary_activation = summary_activation\n            self.summary_first_dropout = summary_first_dropout\n            self.summary_proj_to_labels = summary_proj_to_labels\n        else:\n            raise ValueError(\n                ""First argument must be either a vocabulary size (int)""\n                ""or the path to a pretrained model config file (str)""\n            )\n\n    @property\n    def max_position_embeddings(self):\n        return self.n_positions\n\n    @property\n    def hidden_size(self):\n        return self.n_embd\n\n    @property\n    def num_attention_heads(self):\n        return self.n_head\n\n    @property\n    def num_hidden_layers(self):\n        return self.n_layer\n'"
models/transformers/configuration_roberta.py,0,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" RoBERTa configuration """"""\n\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport logging\n\nfrom .configuration_bert import BertConfig\n\nlogger = logging.getLogger(__name__)\n\nROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n    \'roberta-base\': ""https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json"",\n    \'roberta-large\': ""https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-config.json"",\n    \'roberta-large-mnli\': ""https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-mnli-config.json"",\n}\n\n\nclass RobertaConfig(BertConfig):\n    pretrained_config_archive_map = ROBERTA_PRETRAINED_CONFIG_ARCHIVE_MAP\n'"
models/transformers/configuration_transfo_xl.py,0,"b'# coding=utf-8\n# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" Transformer XL configuration """"""\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport json\nimport logging\nimport sys\nfrom io import open\n\nfrom .configuration_utils import PretrainedConfig\n\nlogger = logging.getLogger(__name__)\n\nTRANSFO_XL_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n    \'transfo-xl-wt103\': ""https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-config.json"",\n}\n\nclass TransfoXLConfig(PretrainedConfig):\n    """"""Configuration class to store the configuration of a `TransfoXLModel`.\n\n        Args:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `TransfoXLModel` or a configuration json file.\n            cutoffs: cutoffs for the adaptive softmax\n            d_model: Dimensionality of the model\'s hidden states.\n            d_embed: Dimensionality of the embeddings\n            d_head: Dimensionality of the model\'s heads.\n            div_val: divident value for adapative input and softmax\n            pre_lnorm: apply LayerNorm to the input instead of the output\n            d_inner: Inner dimension in FF\n            n_layer: Number of hidden layers in the Transformer encoder.\n            n_head: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            tgt_len: number of tokens to predict\n            ext_len: length of the extended context\n            mem_len: length of the retained previous heads\n            same_length: use the same attn length for all tokens\n            proj_share_all_but_first: True to share all but first projs, False not to share.\n            attn_type: attention type. 0 for Transformer-XL, 1 for Shaw et al, 2 for Vaswani et al, 3 for Al Rfou et al.\n            clamp_len: use the same pos embeddings after clamp_len\n            sample_softmax: number of samples in sampled softmax\n            adaptive: use adaptive softmax\n            tie_weight: tie the word embedding and softmax weights\n            dropout: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            dropatt: The dropout ratio for the attention probabilities.\n            untie_r: untie relative position biases\n            embd_pdrop: The dropout ratio for the embeddings.\n            init: parameter initializer to use\n            init_range: parameters initialized by U(-init_range, init_range).\n            proj_init_std: parameters initialized by N(0, init_std)\n            init_std: parameters initialized by N(0, init_std)\n    """"""\n    pretrained_config_archive_map = TRANSFO_XL_PRETRAINED_CONFIG_ARCHIVE_MAP\n\n    def __init__(self,\n                 vocab_size_or_config_json_file=267735,\n                 cutoffs=[20000, 40000, 200000],\n                 d_model=1024,\n                 d_embed=1024,\n                 n_head=16,\n                 d_head=64,\n                 d_inner=4096,\n                 div_val=4,\n                 pre_lnorm=False,\n                 n_layer=18,\n                 tgt_len=128,\n                 ext_len=0,\n                 mem_len=1600,\n                 clamp_len=1000,\n                 same_length=True,\n                 proj_share_all_but_first=True,\n                 attn_type=0,\n                 sample_softmax=-1,\n                 adaptive=True,\n                 tie_weight=True,\n                 dropout=0.1,\n                 dropatt=0.0,\n                 untie_r=True,\n                 init=""normal"",\n                 init_range=0.01,\n                 proj_init_std=0.01,\n                 init_std=0.02,\n                 layer_norm_epsilon=1e-5,\n                 **kwargs):\n        """"""Constructs TransfoXLConfig.\n        """"""\n        super(TransfoXLConfig, self).__init__(**kwargs)\n        self.n_token = vocab_size_or_config_json_file if isinstance(vocab_size_or_config_json_file, int) else -1\n        self.cutoffs = []\n        self.cutoffs.extend(cutoffs)\n        self.tie_weight = tie_weight\n        if proj_share_all_but_first:\n            self.tie_projs = [False] + [True] * len(self.cutoffs)\n        else:\n            self.tie_projs = [False] + [False] * len(self.cutoffs)\n        self.d_model = d_model\n        self.d_embed = d_embed\n        self.d_head = d_head\n        self.d_inner = d_inner\n        self.div_val = div_val\n        self.pre_lnorm = pre_lnorm\n        self.n_layer = n_layer\n        self.n_head = n_head\n        self.tgt_len = tgt_len\n        self.ext_len = ext_len\n        self.mem_len = mem_len\n        self.same_length = same_length\n        self.attn_type = attn_type\n        self.clamp_len = clamp_len\n        self.sample_softmax = sample_softmax\n        self.adaptive = adaptive\n        self.dropout = dropout\n        self.dropatt = dropatt\n        self.untie_r = untie_r\n        self.init = init\n        self.init_range = init_range\n        self.proj_init_std = proj_init_std\n        self.init_std = init_std\n        self.layer_norm_epsilon = layer_norm_epsilon\n\n        if isinstance(vocab_size_or_config_json_file, str) or (sys.version_info[0] == 2\n                        and isinstance(vocab_size_or_config_json_file, unicode)):\n            with open(vocab_size_or_config_json_file, ""r"", encoding=\'utf-8\') as reader:\n                json_config = json.loads(reader.read())\n            for key, value in json_config.items():\n                self.__dict__[key] = value\n        elif not isinstance(vocab_size_or_config_json_file, int):\n            raise ValueError(""First argument must be either a vocabulary size (int)""\n                             "" or the path to a pretrained model config file (str)"")\n\n    @property\n    def max_position_embeddings(self):\n        return self.tgt_len + self.ext_len + self.mem_len\n\n    @property\n    def vocab_size(self):\n        return self.n_token\n\n    @vocab_size.setter\n    def vocab_size(self, value):\n        self.n_token = value\n\n    @property\n    def hidden_size(self):\n        return self.d_model\n\n    @property\n    def num_attention_heads(self):\n        return self.n_head\n\n    @property\n    def num_hidden_layers(self):\n        return self.n_layer\n'"
models/transformers/configuration_utils.py,0,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" Configuration base class and utilities.""""""\n\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport copy\nimport json\nimport logging\nimport os\nfrom io import open\n\nfrom .file_utils import cached_path, CONFIG_NAME\n\nlogger = logging.getLogger(__name__)\n\nclass PretrainedConfig(object):\n    r"""""" Base class for all configuration classes.\n        Handles a few parameters common to all models\' configurations as well as methods for loading/downloading/saving configurations.\n\n        Note:\n            A configuration file can be loaded and saved to disk. Loading the configuration file and using this file to initialize a model does **not** load the model weights.\n            It only affects the model\'s configuration.\n\n        Class attributes (overridden by derived classes):\n            - ``pretrained_config_archive_map``: a python ``dict`` of with `short-cut-names` (string) as keys and `url` (string) of associated pretrained model configurations as values.\n\n        Parameters:\n            ``finetuning_task``: string, default `None`. Name of the task used to fine-tune the model. This can be used when converting from an original (TensorFlow or PyTorch) checkpoint.\n            ``num_labels``: integer, default `2`. Number of classes to use when the model is a classification model (sequences/tokens)\n            ``output_attentions``: boolean, default `False`. Should the model returns attentions weights.\n            ``output_hidden_states``: string, default `False`. Should the model returns all hidden-states.\n            ``torchscript``: string, default `False`. Is the model used with Torchscript.\n    """"""\n    pretrained_config_archive_map = {}\n\n    def __init__(self, **kwargs):\n        self.finetuning_task = kwargs.pop(\'finetuning_task\', None)\n        self.num_labels = kwargs.pop(\'num_labels\', 2)\n        self.output_attentions = kwargs.pop(\'output_attentions\', False)\n        self.output_hidden_states = kwargs.pop(\'output_hidden_states\', False)\n        self.output_past = kwargs.pop(\'output_past\', True)  # Not used by all models\n        self.torchscript = kwargs.pop(\'torchscript\', False)  # Only used by PyTorch models\n        self.use_bfloat16 = kwargs.pop(\'use_bfloat16\', False)\n        self.pruned_heads = kwargs.pop(\'pruned_heads\', {})\n\n    def save_pretrained(self, save_directory):\n        """""" Save a configuration object to the directory `save_directory`, so that it\n            can be re-loaded using the :func:`~transformers.PretrainedConfig.from_pretrained` class method.\n        """"""\n        assert os.path.isdir(save_directory), ""Saving path should be a directory where the model and configuration can be saved""\n\n        # If we save using the predefined names, we can load using `from_pretrained`\n        output_config_file = os.path.join(save_directory, CONFIG_NAME)\n\n        self.to_json_file(output_config_file)\n        logger.info(""Configuration saved in {}"".format(output_config_file))\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n        r"""""" Instantiate a :class:`~transformers.PretrainedConfig` (or a derived class) from a pre-trained model configuration.\n\n        Parameters:\n            pretrained_model_name_or_path: either:\n\n                - a string with the `shortcut name` of a pre-trained model configuration to load from cache or download, e.g.: ``bert-base-uncased``.\n                - a path to a `directory` containing a configuration file saved using the :func:`~transformers.PretrainedConfig.save_pretrained` method, e.g.: ``./my_model_directory/``.\n                - a path or url to a saved configuration JSON `file`, e.g.: ``./my_model_directory/configuration.json``.\n\n            cache_dir: (`optional`) string:\n                Path to a directory in which a downloaded pre-trained model\n                configuration should be cached if the standard cache should not be used.\n\n            kwargs: (`optional`) dict: key/value pairs with which to update the configuration object after loading.\n\n                - The values in kwargs of any keys which are configuration attributes will be used to override the loaded values.\n                - Behavior concerning key/value pairs whose keys are *not* configuration attributes is controlled by the `return_unused_kwargs` keyword parameter.\n\n            force_download: (`optional`) boolean, default False:\n                Force to (re-)download the model weights and configuration files and override the cached versions if they exists.\n\n            proxies: (`optional`) dict, default None:\n                A dictionary of proxy servers to use by protocol or endpoint, e.g.: {\'http\': \'foo.bar:3128\', \'http://hostname\': \'foo.bar:4012\'}.\n                The proxies are used on each request.\n\n            return_unused_kwargs: (`optional`) bool:\n\n                - If False, then this function returns just the final configuration object.\n                - If True, then this functions returns a tuple `(config, unused_kwargs)` where `unused_kwargs` is a dictionary consisting of the key/value pairs whose keys are not configuration attributes: ie the part of kwargs which has not been used to update `config` and is otherwise ignored.\n\n        Examples::\n\n            # We can\'t instantiate directly the base class `PretrainedConfig` so let\'s show the examples on a\n            # derived class: BertConfig\n            config = BertConfig.from_pretrained(\'bert-base-uncased\')    # Download configuration from S3 and cache.\n            config = BertConfig.from_pretrained(\'./test/saved_model/\')  # E.g. config (or model) was saved using `save_pretrained(\'./test/saved_model/\')`\n            config = BertConfig.from_pretrained(\'./test/saved_model/my_configuration.json\')\n            config = BertConfig.from_pretrained(\'bert-base-uncased\', output_attention=True, foo=False)\n            assert config.output_attention == True\n            config, unused_kwargs = BertConfig.from_pretrained(\'bert-base-uncased\', output_attention=True,\n                                                               foo=False, return_unused_kwargs=True)\n            assert config.output_attention == True\n            assert unused_kwargs == {\'foo\': False}\n\n        """"""\n        cache_dir = kwargs.pop(\'cache_dir\', None)\n        force_download = kwargs.pop(\'force_download\', False)\n        proxies = kwargs.pop(\'proxies\', None)\n        return_unused_kwargs = kwargs.pop(\'return_unused_kwargs\', False)\n\n        if pretrained_model_name_or_path in cls.pretrained_config_archive_map:\n            config_file = cls.pretrained_config_archive_map[pretrained_model_name_or_path]\n        elif os.path.isdir(pretrained_model_name_or_path):\n            config_file = os.path.join(pretrained_model_name_or_path, CONFIG_NAME)\n        else:\n            config_file = pretrained_model_name_or_path\n        # redirect to the cache, if necessary\n        try:\n            resolved_config_file = cached_path(config_file, cache_dir=cache_dir, force_download=force_download, proxies=proxies)\n        except EnvironmentError:\n            if pretrained_model_name_or_path in cls.pretrained_config_archive_map:\n                msg = ""Couldn\'t reach server at \'{}\' to download pretrained model configuration file."".format(\n                        config_file)\n            else:\n                msg = ""Model name \'{}\' was not found in model name list ({}). "" \\\n                      ""We assumed \'{}\' was a path or url to a configuration file named {} or "" \\\n                      ""a directory containing such a file but couldn\'t find any such file at this path or url."".format(\n                        pretrained_model_name_or_path,\n                        \', \'.join(cls.pretrained_config_archive_map.keys()),\n                        config_file, CONFIG_NAME)\n            raise EnvironmentError(msg)\n\n        if resolved_config_file == config_file:\n            logger.info(""loading configuration file {}"".format(config_file))\n        else:\n            logger.info(""loading configuration file {} from cache at {}"".format(\n                config_file, resolved_config_file))\n\n        # Load config\n        config = cls.from_json_file(resolved_config_file)\n\n        if hasattr(config, \'pruned_heads\'):\n            config.pruned_heads = dict((int(key), value) for key, value in config.pruned_heads.items())\n\n        # Update config with kwargs if needed\n        to_remove = []\n        for key, value in kwargs.items():\n            if hasattr(config, key):\n                setattr(config, key, value)\n                to_remove.append(key)\n            else:\n                setattr(config, key, value)\n        for key in to_remove:\n            kwargs.pop(key, None)\n\n        logger.info(""Model config %s"", str(config))\n        if return_unused_kwargs:\n            return config, kwargs\n        else:\n            return config\n\n    @classmethod\n    def from_dict(cls, json_object):\n        """"""Constructs a `Config` from a Python dictionary of parameters.""""""\n        config = cls(vocab_size_or_config_json_file=-1)\n        for key, value in json_object.items():\n            setattr(config, key, value)\n        return config\n\n    @classmethod\n    def from_json_file(cls, json_file):\n        """"""Constructs a `BertConfig` from a json file of parameters.""""""\n        with open(json_file, ""r"", encoding=\'utf-8\') as reader:\n            text = reader.read()\n        return cls.from_dict(json.loads(text))\n\n    def __eq__(self, other):\n        return self.__dict__ == other.__dict__\n\n    def __repr__(self):\n        return str(self.to_json_string())\n\n    def to_dict(self):\n        """"""Serializes this instance to a Python dictionary.""""""\n        output = copy.deepcopy(self.__dict__)\n        return output\n\n    def to_json_string(self):\n        """"""Serializes this instance to a JSON string.""""""\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\n\n    def to_json_file(self, json_file_path):\n        """""" Save this instance to a json file.""""""\n        with open(json_file_path, ""w"", encoding=\'utf-8\') as writer:\n            writer.write(self.to_json_string())\n'"
models/transformers/configuration_xlm.py,0,"b'# coding=utf-8\n# Copyright 2019-present, Facebook, Inc and the HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" XLM configuration """"""\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport json\nimport logging\nimport sys\nfrom io import open\n\nfrom .configuration_utils import PretrainedConfig\n\nlogger = logging.getLogger(__name__)\n\nXLM_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n    \'xlm-mlm-en-2048\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-en-2048-config.json"",\n    \'xlm-mlm-ende-1024\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-ende-1024-config.json"",\n    \'xlm-mlm-enfr-1024\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-enfr-1024-config.json"",\n    \'xlm-mlm-enro-1024\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-enro-1024-config.json"",\n    \'xlm-mlm-tlm-xnli15-1024\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-tlm-xnli15-1024-config.json"",\n    \'xlm-mlm-xnli15-1024\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-xnli15-1024-config.json"",\n    \'xlm-clm-enfr-1024\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-clm-enfr-1024-config.json"",\n    \'xlm-clm-ende-1024\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-clm-ende-1024-config.json"",\n    \'xlm-mlm-17-1280\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-17-1280-config.json"",\n    \'xlm-mlm-100-1280\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-100-1280-config.json"",\n}\n\n\nclass XLMConfig(PretrainedConfig):\n    """"""Configuration class to store the configuration of a `XLMModel`.\n\n    Args:\n        vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `XLMModel`.\n        d_model: Size of the encoder layers and the pooler layer.\n        n_layer: Number of hidden layers in the Transformer encoder.\n        n_head: Number of attention heads for each attention layer in\n            the Transformer encoder.\n        d_inner: The size of the ""intermediate"" (i.e., feed-forward)\n            layer in the Transformer encoder.\n        ff_activation: The non-linear activation function (function or string) in the\n            encoder and pooler. If string, ""gelu"", ""relu"" and ""swish"" are supported.\n        untie_r: untie relative position biases\n        attn_type: \'bi\' for XLM, \'uni\' for Transformer-XL\n\n        dropout: The dropout probabilitiy for all fully connected\n            layers in the embeddings, encoder, and pooler.\n        max_position_embeddings: The maximum sequence length that this model might\n            ever be used with. Typically set this to something large just in case\n            (e.g., 512 or 1024 or 2048).\n        initializer_range: The sttdev of the truncated_normal_initializer for\n            initializing all weight matrices.\n        layer_norm_eps: The epsilon used by LayerNorm.\n\n        dropout: float, dropout rate.\n        init: str, the initialization scheme, either ""normal"" or ""uniform"".\n        init_range: float, initialize the parameters with a uniform distribution\n            in [-init_range, init_range]. Only effective when init=""uniform"".\n        init_std: float, initialize the parameters with a normal distribution\n            with mean 0 and stddev init_std. Only effective when init=""normal"".\n        mem_len: int, the number of tokens to cache.\n        reuse_len: int, the number of tokens in the currect batch to be cached\n            and reused in the future.\n        bi_data: bool, whether to use bidirectional input pipeline.\n            Usually set to True during pretraining and False during finetuning.\n        clamp_len: int, clamp all relative distances larger than clamp_len.\n            -1 means no clamping.\n        same_length: bool, whether to use the same attention length for each token.\n    """"""\n    pretrained_config_archive_map = XLM_PRETRAINED_CONFIG_ARCHIVE_MAP\n\n    def __init__(self,\n                 vocab_size_or_config_json_file=30145,\n                 emb_dim=2048,\n                 n_layers=12,\n                 n_heads=16,\n                 dropout=0.1,\n                 attention_dropout=0.1,\n                 gelu_activation=True,\n                 sinusoidal_embeddings=False,\n                 causal=False,\n                 asm=False,\n                 n_langs=1,\n                 use_lang_emb=True,\n                 max_position_embeddings=512,\n                 embed_init_std=2048 ** -0.5,\n                 layer_norm_eps=1e-12,\n                 init_std=0.02,\n                 bos_index=0,\n                 eos_index=1,\n                 pad_index=2,\n                 unk_index=3,\n                 mask_index=5,\n                 is_encoder=True,\n\n                 finetuning_task=None,\n                 num_labels=2,\n                 summary_type=\'first\',\n                 summary_use_proj=True,\n                 summary_activation=None,\n                 summary_proj_to_labels=True,\n                 summary_first_dropout=0.1,\n                 start_n_top=5,\n                 end_n_top=5,\n                 **kwargs):\n        """"""Constructs XLMConfig.\n        """"""\n        super(XLMConfig, self).__init__(**kwargs)\n\n        if isinstance(vocab_size_or_config_json_file, str) or (sys.version_info[0] == 2\n                        and isinstance(vocab_size_or_config_json_file, unicode)):\n            with open(vocab_size_or_config_json_file, ""r"", encoding=\'utf-8\') as reader:\n                json_config = json.loads(reader.read())\n            for key, value in json_config.items():\n                self.__dict__[key] = value\n        elif isinstance(vocab_size_or_config_json_file, int):\n            self.n_words = vocab_size_or_config_json_file\n            self.emb_dim = emb_dim\n            self.n_layers = n_layers\n            self.n_heads = n_heads\n            self.dropout = dropout\n            self.attention_dropout = attention_dropout\n            self.gelu_activation = gelu_activation\n            self.sinusoidal_embeddings = sinusoidal_embeddings\n            self.causal = causal\n            self.asm = asm\n            self.n_langs = n_langs\n            self.use_lang_emb = use_lang_emb\n            self.layer_norm_eps = layer_norm_eps\n            self.bos_index = bos_index\n            self.eos_index = eos_index\n            self.pad_index = pad_index\n            self.unk_index = unk_index\n            self.mask_index = mask_index\n            self.is_encoder = is_encoder\n            self.max_position_embeddings = max_position_embeddings\n            self.embed_init_std = embed_init_std\n            self.init_std = init_std\n            self.finetuning_task = finetuning_task\n            self.num_labels = num_labels\n            self.summary_type = summary_type\n            self.summary_use_proj = summary_use_proj\n            self.summary_activation = summary_activation\n            self.summary_proj_to_labels = summary_proj_to_labels\n            self.summary_first_dropout = summary_first_dropout\n            self.start_n_top = start_n_top\n            self.end_n_top = end_n_top\n        else:\n            raise ValueError(""First argument must be either a vocabulary size (int)""\n                             "" or the path to a pretrained model config file (str)"")\n\n    @property\n    def vocab_size(self):\n        return self.n_words\n\n    @vocab_size.setter\n    def vocab_size(self, value):\n        self.n_words = value\n\n    @property\n    def hidden_size(self):\n        return self.emb_dim\n\n    @property\n    def num_attention_heads(self):\n        return self.n_heads\n\n    @property\n    def num_hidden_layers(self):\n        return self.n_layers\n'"
models/transformers/configuration_xlnet.py,0,"b'# coding=utf-8\n# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" XLNet configuration """"""\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport json\nimport logging\nimport sys\nfrom io import open\n\nfrom .configuration_utils import PretrainedConfig\n\nlogger = logging.getLogger(__name__)\n\nXLNET_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n    \'xlnet-base-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-config.json"",\n    \'xlnet-large-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-large-cased-config.json"",\n}\n\n\nclass XLNetConfig(PretrainedConfig):\n    """"""Configuration class to store the configuration of a ``XLNetModel``.\n\n    Args:\n        vocab_size_or_config_json_file: Vocabulary size of ``inputs_ids`` in ``XLNetModel``.\n        d_model: Size of the encoder layers and the pooler layer.\n        n_layer: Number of hidden layers in the Transformer encoder.\n        n_head: Number of attention heads for each attention layer in\n            the Transformer encoder.\n        d_inner: The size of the ""intermediate"" (i.e., feed-forward)\n            layer in the Transformer encoder.\n        ff_activation: The non-linear activation function (function or string) in the\n            encoder and pooler. If string, ""gelu"", ""relu"" and ""swish"" are supported.\n        untie_r: untie relative position biases\n        attn_type: \'bi\' for XLNet, \'uni\' for Transformer-XL\n\n        dropout: The dropout probabilitiy for all fully connected\n            layers in the embeddings, encoder, and pooler.\n        initializer_range: The sttdev of the truncated_normal_initializer for\n            initializing all weight matrices.\n        layer_norm_eps: The epsilon used by LayerNorm.\n\n        dropout: float, dropout rate.\n        init: str, the initialization scheme, either ""normal"" or ""uniform"".\n        init_range: float, initialize the parameters with a uniform distribution\n            in [-init_range, init_range]. Only effective when init=""uniform"".\n        init_std: float, initialize the parameters with a normal distribution\n            with mean 0 and stddev init_std. Only effective when init=""normal"".\n        mem_len: int, the number of tokens to cache.\n        reuse_len: int, the number of tokens in the currect batch to be cached\n            and reused in the future.\n        bi_data: bool, whether to use bidirectional input pipeline.\n            Usually set to True during pretraining and False during finetuning.\n        clamp_len: int, clamp all relative distances larger than clamp_len.\n            -1 means no clamping.\n        same_length: bool, whether to use the same attention length for each token.\n        finetuning_task: name of the glue task on which the model was fine-tuned if any\n    """"""\n    pretrained_config_archive_map = XLNET_PRETRAINED_CONFIG_ARCHIVE_MAP\n\n    def __init__(self,\n                 vocab_size_or_config_json_file=32000,\n                 d_model=1024,\n                 n_layer=24,\n                 n_head=16,\n                 d_inner=4096,\n                 max_position_embeddings=512,\n                 ff_activation=""gelu"",\n                 untie_r=True,\n                 attn_type=""bi"",\n\n                 initializer_range=0.02,\n                 layer_norm_eps=1e-12,\n\n                 dropout=0.1,\n                 mem_len=None,\n                 reuse_len=None,\n                 bi_data=False,\n                 clamp_len=-1,\n                 same_length=False,\n\n                 finetuning_task=None,\n                 num_labels=2,\n                 summary_type=\'last\',\n                 summary_use_proj=True,\n                 summary_activation=\'tanh\',\n                 summary_last_dropout=0.1,\n                 start_n_top=5,\n                 end_n_top=5,\n                 **kwargs):\n        """"""Constructs XLNetConfig.\n        """"""\n        super(XLNetConfig, self).__init__(**kwargs)\n\n        if isinstance(vocab_size_or_config_json_file, str) or (sys.version_info[0] == 2\n                        and isinstance(vocab_size_or_config_json_file, unicode)):\n            with open(vocab_size_or_config_json_file, ""r"", encoding=\'utf-8\') as reader:\n                json_config = json.loads(reader.read())\n            for key, value in json_config.items():\n                setattr(config, key, value)\n        elif isinstance(vocab_size_or_config_json_file, int):\n            self.n_token = vocab_size_or_config_json_file\n            self.d_model = d_model\n            self.n_layer = n_layer\n            self.n_head = n_head\n            assert d_model % n_head == 0\n            self.d_head = d_model // n_head\n            self.ff_activation = ff_activation\n            self.d_inner = d_inner\n            self.untie_r = untie_r\n            self.attn_type = attn_type\n\n            self.initializer_range = initializer_range\n            self.layer_norm_eps = layer_norm_eps\n\n            self.dropout = dropout\n            self.mem_len = mem_len\n            self.reuse_len = reuse_len\n            self.bi_data = bi_data\n            self.clamp_len = clamp_len\n            self.same_length = same_length\n\n            self.finetuning_task = finetuning_task\n            self.num_labels = num_labels\n            self.summary_type = summary_type\n            self.summary_use_proj = summary_use_proj\n            self.summary_activation = summary_activation\n            self.summary_last_dropout = summary_last_dropout\n            self.start_n_top = start_n_top\n            self.end_n_top = end_n_top\n        else:\n            raise ValueError(""First argument must be either a vocabulary size (int)""\n                             "" or the path to a pretrained model config file (str)"")\n\n    @property\n    def max_position_embeddings(self):\n        return -1\n\n    @property\n    def vocab_size(self):\n        return self.n_token\n\n    @vocab_size.setter\n    def vocab_size(self, value):\n        self.n_token = value\n\n    @property\n    def hidden_size(self):\n        return self.d_model\n\n    @property\n    def num_attention_heads(self):\n        return self.n_head\n\n    @property\n    def num_hidden_layers(self):\n        return self.n_layer\n'"
models/transformers/file_utils.py,2,"b'""""""\nUtilities for working with the local dataset cache.\nThis file is adapted from the AllenNLP library at https://github.com/allenai/allennlp\nCopyright by the AllenNLP authors.\n""""""\nfrom __future__ import (absolute_import, division, print_function, unicode_literals)\n\nimport sys\nimport json\nimport logging\nimport os\nimport six\nimport shutil\nimport tempfile\nimport fnmatch\nfrom functools import wraps\nfrom hashlib import sha256\nfrom io import open\n\nimport boto3\nfrom botocore.config import Config\nfrom botocore.exceptions import ClientError\nimport requests\nfrom tqdm import tqdm\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\ntry:\n    import tensorflow as tf\n    assert hasattr(tf, \'__version__\') and int(tf.__version__[0]) >= 2\n    _tf_available = True  # pylint: disable=invalid-name\n    logger.info(""TensorFlow version {} available."".format(tf.__version__))\nexcept (ImportError, AssertionError):\n    _tf_available = False  # pylint: disable=invalid-name\n\ntry:\n    import torch\n    _torch_available = True  # pylint: disable=invalid-name\n    logger.info(""PyTorch version {} available."".format(torch.__version__))\nexcept ImportError:\n    _torch_available = False  # pylint: disable=invalid-name\n\n\ntry:\n    from torch.hub import _get_torch_home\n    torch_cache_home = _get_torch_home()\nexcept ImportError:\n    torch_cache_home = os.path.expanduser(\n        os.getenv(\'TORCH_HOME\', os.path.join(\n            os.getenv(\'XDG_CACHE_HOME\', \'~/.cache\'), \'torch\')))\ndefault_cache_path = os.path.join(torch_cache_home, \'transformers\')\n\ntry:\n    from urllib.parse import urlparse\nexcept ImportError:\n    from urlparse import urlparse\n\ntry:\n    from pathlib import Path\n    PYTORCH_PRETRAINED_BERT_CACHE = Path(\n        os.getenv(\'PYTORCH_TRANSFORMERS_CACHE\', os.getenv(\'PYTORCH_PRETRAINED_BERT_CACHE\', default_cache_path)))\nexcept (AttributeError, ImportError):\n    PYTORCH_PRETRAINED_BERT_CACHE = os.getenv(\'PYTORCH_TRANSFORMERS_CACHE\',\n                                              os.getenv(\'PYTORCH_PRETRAINED_BERT_CACHE\',\n                                                        default_cache_path))\n\nPYTORCH_TRANSFORMERS_CACHE = PYTORCH_PRETRAINED_BERT_CACHE  # Kept for backward compatibility\nTRANSFORMERS_CACHE = PYTORCH_PRETRAINED_BERT_CACHE  # Kept for backward compatibility\n\nWEIGHTS_NAME = ""pytorch_model.bin""\nTF2_WEIGHTS_NAME = \'tf_model.h5\'\nTF_WEIGHTS_NAME = \'model.ckpt\'\nCONFIG_NAME = ""config.json""\n\ndef is_torch_available():\n    return _torch_available\n\ndef is_tf_available():\n    return _tf_available\n\nif not six.PY2:\n    def add_start_docstrings(*docstr):\n        def docstring_decorator(fn):\n            fn.__doc__ = \'\'.join(docstr) + fn.__doc__\n            return fn\n        return docstring_decorator\n\n    def add_end_docstrings(*docstr):\n        def docstring_decorator(fn):\n            fn.__doc__ = fn.__doc__ + \'\'.join(docstr)\n            return fn\n        return docstring_decorator\nelse:\n    # Not possible to update class docstrings on python2\n    def add_start_docstrings(*docstr):\n        def docstring_decorator(fn):\n            return fn\n        return docstring_decorator\n\n    def add_end_docstrings(*docstr):\n        def docstring_decorator(fn):\n            return fn\n        return docstring_decorator\n\ndef url_to_filename(url, etag=None):\n    """"""\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the url\'s, delimited\n    by a period.\n    If the url ends with .h5 (Keras HDF5 weights) ands \'.h5\' to the name\n    so that TF 2.0 can identify it as a HDF5 file\n    (see https://github.com/tensorflow/tensorflow/blob/00fad90125b18b80fe054de1055770cfb8fe4ba3/tensorflow/python/keras/engine/network.py#L1380)\n    """"""\n    url_bytes = url.encode(\'utf-8\')\n    url_hash = sha256(url_bytes)\n    filename = url_hash.hexdigest()\n\n    if etag:\n        etag_bytes = etag.encode(\'utf-8\')\n        etag_hash = sha256(etag_bytes)\n        filename += \'.\' + etag_hash.hexdigest()\n\n    if url.endswith(\'.h5\'):\n        filename += \'.h5\'\n\n    return filename\n\n\ndef filename_to_url(filename, cache_dir=None):\n    """"""\n    Return the url and etag (which may be ``None``) stored for `filename`.\n    Raise ``EnvironmentError`` if `filename` or its stored metadata do not exist.\n    """"""\n    if cache_dir is None:\n        cache_dir = TRANSFORMERS_CACHE\n    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n        cache_dir = str(cache_dir)\n\n    cache_path = os.path.join(cache_dir, filename)\n    if not os.path.exists(cache_path):\n        raise EnvironmentError(""file {} not found"".format(cache_path))\n\n    meta_path = cache_path + \'.json\'\n    if not os.path.exists(meta_path):\n        raise EnvironmentError(""file {} not found"".format(meta_path))\n\n    with open(meta_path, encoding=""utf-8"") as meta_file:\n        metadata = json.load(meta_file)\n    url = metadata[\'url\']\n    etag = metadata[\'etag\']\n\n    return url, etag\n\n\ndef cached_path(url_or_filename, cache_dir=None, force_download=False, proxies=None):\n    """"""\n    Given something that might be a URL (or might be a local path),\n    determine which. If it\'s a URL, download the file and cache it, and\n    return the path to the cached file. If it\'s already a local path,\n    make sure the file exists and then return the path.\n    Args:\n        cache_dir: specify a cache directory to save the file to (overwrite the default cache dir).\n        force_download: if True, re-dowload the file even if it\'s already cached in the cache dir.\n    """"""\n    if cache_dir is None:\n        cache_dir = TRANSFORMERS_CACHE\n    if sys.version_info[0] == 3 and isinstance(url_or_filename, Path):\n        url_or_filename = str(url_or_filename)\n    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n        cache_dir = str(cache_dir)\n\n    parsed = urlparse(url_or_filename)\n\n    if parsed.scheme in (\'http\', \'https\', \'s3\'):\n        # URL, so get it from the cache (downloading if necessary)\n        return get_from_cache(url_or_filename, cache_dir=cache_dir, force_download=force_download, proxies=proxies)\n    elif os.path.exists(url_or_filename):\n        # File, and it exists.\n        return url_or_filename\n    elif parsed.scheme == \'\':\n        # File, but it doesn\'t exist.\n        raise EnvironmentError(""file {} not found"".format(url_or_filename))\n    else:\n        # Something unknown\n        raise ValueError(""unable to parse {} as a URL or as a local path"".format(url_or_filename))\n\n\ndef split_s3_path(url):\n    """"""Split a full s3 path into the bucket name and path.""""""\n    parsed = urlparse(url)\n    if not parsed.netloc or not parsed.path:\n        raise ValueError(""bad s3 path {}"".format(url))\n    bucket_name = parsed.netloc\n    s3_path = parsed.path\n    # Remove \'/\' at beginning of path.\n    if s3_path.startswith(""/""):\n        s3_path = s3_path[1:]\n    return bucket_name, s3_path\n\n\ndef s3_request(func):\n    """"""\n    Wrapper function for s3 requests in order to create more helpful error\n    messages.\n    """"""\n\n    @wraps(func)\n    def wrapper(url, *args, **kwargs):\n        try:\n            return func(url, *args, **kwargs)\n        except ClientError as exc:\n            if int(exc.response[""Error""][""Code""]) == 404:\n                raise EnvironmentError(""file {} not found"".format(url))\n            else:\n                raise\n\n    return wrapper\n\n\n@s3_request\ndef s3_etag(url, proxies=None):\n    """"""Check ETag on S3 object.""""""\n    s3_resource = boto3.resource(""s3"", config=Config(proxies=proxies))\n    bucket_name, s3_path = split_s3_path(url)\n    s3_object = s3_resource.Object(bucket_name, s3_path)\n    return s3_object.e_tag\n\n\n@s3_request\ndef s3_get(url, temp_file, proxies=None):\n    """"""Pull a file directly from S3.""""""\n    s3_resource = boto3.resource(""s3"", config=Config(proxies=proxies))\n    bucket_name, s3_path = split_s3_path(url)\n    s3_resource.Bucket(bucket_name).download_fileobj(s3_path, temp_file)\n\n\ndef http_get(url, temp_file, proxies=None):\n    req = requests.get(url, stream=True, proxies=proxies)\n    content_length = req.headers.get(\'Content-Length\')\n    total = int(content_length) if content_length is not None else None\n    progress = tqdm(unit=""B"", total=total)\n    for chunk in req.iter_content(chunk_size=1024):\n        if chunk: # filter out keep-alive new chunks\n            progress.update(len(chunk))\n            temp_file.write(chunk)\n    progress.close()\n\n\ndef get_from_cache(url, cache_dir=None, force_download=False, proxies=None):\n    """"""\n    Given a URL, look for the corresponding dataset in the local cache.\n    If it\'s not there, download it. Then return the path to the cached file.\n    """"""\n    if cache_dir is None:\n        cache_dir = TRANSFORMERS_CACHE\n    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n        cache_dir = str(cache_dir)\n    if sys.version_info[0] == 2 and not isinstance(cache_dir, str):\n        cache_dir = str(cache_dir)\n\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    # Get eTag to add to filename, if it exists.\n    if url.startswith(""s3://""):\n        etag = s3_etag(url, proxies=proxies)\n    else:\n        try:\n            response = requests.head(url, allow_redirects=True, proxies=proxies)\n            if response.status_code != 200:\n                etag = None\n            else:\n                etag = response.headers.get(""ETag"")\n        except EnvironmentError:\n            etag = None\n\n    if sys.version_info[0] == 2 and etag is not None:\n        etag = etag.decode(\'utf-8\')\n    filename = url_to_filename(url, etag)\n\n    # get cache path to put the file\n    cache_path = os.path.join(cache_dir, filename)\n\n    # If we don\'t have a connection (etag is None) and can\'t identify the file\n    # try to get the last downloaded one\n    if not os.path.exists(cache_path) and etag is None:\n        matching_files = fnmatch.filter(os.listdir(cache_dir), filename + \'.*\')\n        matching_files = list(filter(lambda s: not s.endswith(\'.json\'), matching_files))\n        if matching_files:\n            cache_path = os.path.join(cache_dir, matching_files[-1])\n\n    if not os.path.exists(cache_path) or force_download:\n        # Download to temporary file, then copy to cache dir once finished.\n        # Otherwise you get corrupt cache entries if the download gets interrupted.\n        with tempfile.NamedTemporaryFile() as temp_file:\n            logger.info(""%s not found in cache or force_download set to True, downloading to %s"", url, temp_file.name)\n\n            # GET file object\n            if url.startswith(""s3://""):\n                s3_get(url, temp_file, proxies=proxies)\n            else:\n                http_get(url, temp_file, proxies=proxies)\n\n            # we are copying the file before closing it, so flush to avoid truncation\n            temp_file.flush()\n            # shutil.copyfileobj() starts at the current position, so go to the start\n            temp_file.seek(0)\n\n            logger.info(""copying %s to cache at %s"", temp_file.name, cache_path)\n            with open(cache_path, \'wb\') as cache_file:\n                shutil.copyfileobj(temp_file, cache_file)\n\n            logger.info(""creating metadata file for %s"", cache_path)\n            meta = {\'url\': url, \'etag\': etag}\n            meta_path = cache_path + \'.json\'\n            with open(meta_path, \'w\') as meta_file:\n                output_string = json.dumps(meta)\n                if sys.version_info[0] == 2 and isinstance(output_string, str):\n                    output_string = unicode(output_string, \'utf-8\')  # The beauty of python 2\n                meta_file.write(output_string)\n\n            logger.info(""removing temp file %s"", temp_file.name)\n\n    return cache_path\n'"
models/transformers/modeling_albert.py,81,"b'""""""PyTorch ALBERT model. """"""\nfrom __future__ import absolute_import, division, print_function, unicode_literals\nimport logging\nimport math\nimport os\nimport sys\nimport torch\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss, MSELoss\nfrom .modeling_utils import PreTrainedModel, prune_linear_layer\nfrom .configuration_albert import AlbertConfig\nfrom .file_utils import add_start_docstrings\nlogger = logging.getLogger(__name__)\n\nALBERT_PRETRAINED_MODEL_ARCHIVE_MAP = {\n    \'albert-base\': """",\n    \'albert-large\': """",\n    \'albert-xlarge\': """",\n    \'albert-xxlarge\': """",\n}\ndef load_tf_weights_in_albert(model, config, tf_checkpoint_path):\n    """""" Load tf checkpoints in a pytorch model.\n    """"""\n    try:\n        import re\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        logger.error(""Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see ""\n                     ""https://www.tensorflow.org/install/ for installation instructions."")\n        raise\n    tf_path = os.path.abspath(tf_checkpoint_path)\n    logger.info(""Converting TensorFlow checkpoint from {}"".format(tf_path))\n    if not os.path.exists(tf_path+\'/checkpoint\'):\n        tf_path = tf_path + ""/variables/variables""\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    names = []\n    arrays = []\n    for name, shape in init_vars:\n        logger.info(""Loading TF weight {} with shape {}"".format(name, shape))\n        array = tf.train.load_variable(tf_path, name)\n        names.append(name)\n        arrays.append(array)\n    for name, array in zip(names, arrays):\n        name = name.replace(""attention_1"",""attention"")\n        name = name.replace(""ffn_1"",""ffn"")\n        name = name.split(\'/\')\n        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n        # which are not required for using pretrained model\n        if any(n in [""adam_v"", ""adam_m"", ""global_step""] for n in name):\n            logger.info(""Skipping {}"".format(""/"".join(name)))\n            continue\n        pointer = model\n        for m_name in name:\n            if re.fullmatch(r\'[A-Za-z]+_\\d+\', m_name):\n                l = re.split(r\'_(\\d+)\', m_name)\n            elif re.fullmatch(r\'[A-Za-z]+_+[A-Za-z]+_\\d+\', m_name):\n                l = re.split(r\'_(\\d+)\', m_name)\n            else:\n                l = [m_name]\n            if l[0] in [\'LayerNorm\', \'attention\', \'ffn\'] and len(l) >= 2:\n                l = [""_"".join(l[:-1])]\n            if l[0] == \'kernel\' or l[0] == \'gamma\':\n                pointer = getattr(pointer, \'weight\')\n            elif l[0] == \'output_bias\' or l[0] == \'beta\':\n                pointer = getattr(pointer, \'bias\')\n            elif l[0] == \'output_weights\':\n                pointer = getattr(pointer, \'weight\')\n            elif l[0] == \'squad\':\n                pointer = getattr(pointer, \'classifier\')\n            else:\n                try:\n                    pointer = getattr(pointer, l[0])\n                except AttributeError:\n                    logger.info(""Skipping {}"".format(""/"".join(name)))\n                    continue\n            if len(l) >= 2:\n                num = int(l[1])\n                pointer = pointer[num]\n\n        if m_name[-11:] == \'_embeddings\':\n            pointer = getattr(pointer, \'weight\')\n        elif m_name == \'kernel\':\n            array = np.transpose(array)\n        try:\n            assert pointer.shape == array.shape\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        logger.info(""Initialize PyTorch weight {}"".format(name))\n        pointer.data = torch.from_numpy(array)\n    return model\n\ndef gelu(x):\n    """""" Original Implementation of the gelu activation function in Google Bert repo when initially created.\n        For information: OpenAI GPT\'s gelu is slightly different (and gives slightly different results):\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n        Also see https://arxiv.org/abs/1606.08415\n    """"""\n    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n\ndef gelu_new(x):\n    """""" Implementation of the gelu activation function currently in Google Bert repo (identical to OpenAI GPT).\n        Also see https://arxiv.org/abs/1606.08415\n    """"""\n    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n\ndef swish(x):\n    return x * torch.sigmoid(x)\n\nACT2FN = {""gelu"": gelu, ""relu"": torch.nn.functional.relu, ""swish"": swish, ""gelu_new"": gelu_new}\nAlbertLayerNorm = torch.nn.LayerNorm\n\nclass AlbertEmbeddings(nn.Module):\n    """"""Construct the embeddings from word, position and token_type embeddings.\n    """"""\n    def __init__(self, config):\n        super(AlbertEmbeddings, self).__init__()\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.embedding_size, padding_idx=0)\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.embedding_size)\n        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.embedding_size)\n        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n        self.LayerNorm = AlbertLayerNorm(config.embedding_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, input_ids, token_type_ids=None, position_ids=None):\n        seq_length = input_ids.size(1)\n        if position_ids is None:\n            position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n        words_embeddings = self.word_embeddings(input_ids)\n        position_embeddings = self.position_embeddings(position_ids)\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings\n\nclass AlbertSelfAttention(nn.Module):\n    def __init__(self, config):\n        super(AlbertSelfAttention, self).__init__()\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                ""The hidden size (%d) is not a multiple of the number of attention ""\n                ""heads (%d)"" % (config.hidden_size, config.num_attention_heads))\n        self.output_attentions = config.output_attentions\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(self, hidden_states, attention_mask=None, head_mask=None):\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n\n        # Take the dot product between ""query"" and ""key"" to get the raw attention scores.\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n            attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout(attention_probs)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attention_probs = attention_probs * head_mask\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n        outputs = (context_layer, attention_probs) if self.output_attentions else (context_layer,)\n        return outputs\n\nclass AlbertSelfOutput(nn.Module):\n    def __init__(self, config):\n        super(AlbertSelfOutput, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        return hidden_states\n\nclass AlbertAttention(nn.Module):\n    def __init__(self, config):\n        super(AlbertAttention, self).__init__()\n        self.self = AlbertSelfAttention(config)\n        self.output = AlbertSelfOutput(config)\n        self.pruned_heads = set()\n\n    def prune_heads(self, heads):\n        if len(heads) == 0:\n            return\n        mask = torch.ones(self.self.num_attention_heads, self.self.attention_head_size)\n        heads = set(heads) - self.pruned_heads  # Convert to set and emove already pruned heads\n        for head in heads:\n            # Compute how many pruned heads are before the head and move the index accordingly\n            head = head - sum(1 if h < head else 0 for h in self.pruned_heads)\n            mask[head] = 0\n        mask = mask.view(-1).contiguous().eq(1)\n        index = torch.arange(len(mask))[mask].long()\n\n        # Prune linear layers\n        self.self.query = prune_linear_layer(self.self.query, index)\n        self.self.key = prune_linear_layer(self.self.key, index)\n        self.self.value = prune_linear_layer(self.self.value, index)\n        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n\n        # Update hyper params and store pruned heads\n        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n        self.pruned_heads = self.pruned_heads.union(heads)\n\n    def forward(self, input_tensor, attention_mask=None, head_mask=None):\n        self_outputs = self.self(input_tensor, attention_mask, head_mask)\n        attention_output = self.output(self_outputs[0], input_tensor)\n        outputs = (attention_output,self_outputs)\n        return outputs\n\nclass AlbertOutput(nn.Module):\n    def __init__(self, config):\n        super(AlbertOutput, self).__init__()\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        return hidden_states\n\nclass AlbertIntermediate(nn.Module):\n    def __init__(self, config):\n        super(AlbertIntermediate, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n        self.output = AlbertOutput(config)\n        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.intermediate_act_fn = config.hidden_act\n\n    def forward(self, hidden_states):\n        intermediate_output = self.dense(hidden_states)\n        intermediate_output = self.intermediate_act_fn(intermediate_output)\n        output = self.output(intermediate_output)\n        return output\n\nclass AlbertFFN(nn.Module):\n    def __init__(self, config):\n        super(AlbertFFN, self).__init__()\n        self.intermediate = AlbertIntermediate(config)\n\n    def forward(self, attention_output):\n        output = self.intermediate(attention_output)\n        return output\n\nclass AlbertLayer(nn.Module):\n    def __init__(self, config):\n        super(AlbertLayer, self).__init__()\n        self.attention = AlbertAttention(config)\n        self.ffn = AlbertFFN(config)\n        self.LayerNorm = AlbertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.LayerNorm_1 = AlbertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n\n    def forward(self, hidden_states, attention_mask=None, head_mask=None):\n        attention_outputs = self.attention(hidden_states, attention_mask, head_mask)\n        attention_output = self.LayerNorm(attention_outputs[0] + hidden_states)\n        ffn_output = self.ffn(attention_output)\n        ffn_output = self.LayerNorm_1(ffn_output+attention_output)\n        outputs = (ffn_output,) + attention_outputs[1:] # add attentions if we output them\n        return outputs\n\nclass AlbertGroup(nn.Module):\n    def __init__(self, config):\n        super(AlbertGroup, self).__init__()\n        self.inner_group_num = config.inner_group_num\n        self.inner_group = nn.ModuleList([AlbertLayer(config) for _ in range(config.inner_group_num)])\n\n    def forward(self, hidden_states, attention_mask, head_mask):\n        layer_attentions = ()\n        layer_hidden_states = ()\n        for inner_group_idx in range(self.inner_group_num):\n            layer_module = self.inner_group[inner_group_idx]\n            layer_outputs = layer_module(hidden_states, attention_mask, head_mask)\n            hidden_states = layer_outputs[0]\n            layer_attentions = layer_attentions + (layer_outputs[1],)\n            layer_hidden_states = layer_hidden_states + (hidden_states,)\n        return (layer_hidden_states, layer_attentions)\n\nclass AlbertTransformer(nn.Module):\n    def __init__(self, config):\n        super(AlbertTransformer, self).__init__()\n        self.output_attentions = config.output_attentions\n        self.output_hidden_states = config.output_hidden_states\n        self.num_hidden_layers = config.num_hidden_layers\n        self.num_hidden_groups = config.num_hidden_groups\n        self.group = nn.ModuleList([AlbertGroup(config) for _ in range(config.num_hidden_groups)])\n\n    def forward(self, hidden_states, attention_mask, head_mask):\n        all_hidden_states = ()\n        all_attentions = ()\n        for layer_idx in range(self.num_hidden_layers):\n            if self.output_hidden_states and layer_idx == 0:\n                all_hidden_states = all_hidden_states + (hidden_states,)\n            group_idx = int(layer_idx / self.num_hidden_layers * self.num_hidden_groups)\n            layer_module = self.group[group_idx]\n            layer_outputs = layer_module(hidden_states, attention_mask, head_mask[layer_idx])\n            hidden_states = layer_outputs[0][-1]\n            if self.output_attentions:\n                all_attentions = all_attentions + layer_outputs[1]\n            if self.output_hidden_states:\n                all_hidden_states = all_hidden_states + layer_outputs[0]\n        outputs = (hidden_states,)\n        if self.output_hidden_states:\n            outputs = outputs + (all_hidden_states,)\n        if self.output_attentions:\n            outputs = outputs + (all_attentions,)\n        return outputs  # last-layer hidden state, (all hidden states), (all attentions)\n\nclass AlbertEncoder(nn.Module):\n    def __init__(self, config):\n        super(AlbertEncoder, self).__init__()\n        self.hidden_size = config.hidden_size\n        self.embedding_size = config.embedding_size\n        self.embedding_hidden_mapping_in = nn.Linear(self.embedding_size, self.hidden_size)\n        self.transformer = AlbertTransformer(config)\n\n    def forward(self, hidden_states, attention_mask=None, head_mask=None):\n        if self.embedding_size != self.hidden_size:\n            prev_output = self.embedding_hidden_mapping_in(hidden_states)\n        else:\n            prev_output = hidden_states\n        outputs = self.transformer(prev_output, attention_mask, head_mask)\n        return outputs  # last-layer hidden state, (all hidden states), (all attentions)\n\nclass AlbertPooler(nn.Module):\n    def __init__(self, config):\n        super(AlbertPooler, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.activation = nn.Tanh()\n\n    def forward(self, hidden_states):\n        # We ""pool"" the model by simply taking the hidden state corresponding\n        # to the first token.\n        first_token_tensor = hidden_states[:, 0]\n        pooled_output = self.dense(first_token_tensor)\n        pooled_output = self.activation(pooled_output)\n        return pooled_output\n\nclass AlbertPredictionHeadTransform(nn.Module):\n    def __init__(self, config):\n        super(AlbertPredictionHeadTransform, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.embedding_size)\n        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n            self.transform_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.transform_act_fn = config.hidden_act\n        self.LayerNorm = AlbertLayerNorm(config.embedding_size, eps=config.layer_norm_eps)\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.transform_act_fn(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states)\n        return hidden_states\n\nclass AlbertLMPredictionHead(nn.Module):\n    def __init__(self, config):\n        super(AlbertLMPredictionHead, self).__init__()\n        self.transform = AlbertPredictionHeadTransform(config)\n        # The output weights are the same as the input embeddings, but there is\n        # an output-only bias for each token.\n        self.decoder = nn.Linear(config.embedding_size,config.vocab_size,bias=False)\n        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n\n    def forward(self, hidden_states):\n        hidden_states = self.transform(hidden_states)\n        hidden_states = self.decoder(hidden_states) + self.bias\n        return hidden_states\n\nclass AlbertOnlyMLMHead(nn.Module):\n    def __init__(self, config):\n        super(AlbertOnlyMLMHead, self).__init__()\n        self.predictions = AlbertLMPredictionHead(config)\n\n    def forward(self, sequence_output):\n        prediction_scores = self.predictions(sequence_output)\n        return prediction_scores\n\nclass AlbertOnlyNSPHead(nn.Module):\n    def __init__(self, config):\n        super(AlbertOnlyNSPHead, self).__init__()\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n\n    def forward(self, pooled_output):\n        seq_relationship_score = self.seq_relationship(pooled_output)\n        return seq_relationship_score\n\nclass AlbertPreTrainingHeads(nn.Module):\n    def __init__(self, config):\n        super(AlbertPreTrainingHeads, self).__init__()\n        self.predictions = AlbertLMPredictionHead(config)\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n\n    def forward(self, sequence_output, pooled_output):\n        prediction_scores = self.predictions(sequence_output)\n        seq_relationship_score = self.seq_relationship(pooled_output)\n        return prediction_scores, seq_relationship_score\n\nclass AlbertPreTrainedModel(PreTrainedModel):\n    """""" An abstract class to handle weights initialization and\n        a simple interface for dowloading and loading pretrained models.\n    """"""\n    config_class = AlbertConfig\n    pretrained_model_archive_map = ALBERT_PRETRAINED_MODEL_ARCHIVE_MAP\n    load_tf_weights = load_tf_weights_in_albert\n    base_model_prefix = ""bert""\n\n    def _init_weights(self, module):\n        """""" Initialize the weights """"""\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n        elif isinstance(module, AlbertLayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n\n\nALBERT_START_DOCSTRING = r""""""    The ALBERT model was proposed in\n    `ALBERT: A Lite BERT for Self-supervised Learning of Language Representations`_\n    by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut. \n    This model is a PyTorch `torch.nn.Module`_ sub-class. Use it as a regular PyTorch Module and\n    refer to the PyTorch documentation for all matter related to general usage and behavior.\n    .. _`BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`:\n        https://arxiv.org/abs/1909.11942\n    .. _`torch.nn.Module`:\n        https://pytorch.org/docs/stable/nn.html#module\n    Parameters:\n        config (:class:`~transformers.ALbertConfig`): Model configuration class with all the parameters of the model. \n            Initializing with a config file does not load the weights associated with the model, only the configuration.\n            Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model weights.\n""""""\n\nALBERT_INPUTS_DOCSTRING = r""""""\n    Inputs:\n        **input_ids**: ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Indices of input sequence tokens in the vocabulary.\n            To match pre-training, ALBERT input sequence should be formatted with [CLS] and [SEP] tokens as follows:\n            (a) For sequence pairs:\n                ``tokens:         [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]``\n                ``token_type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1``\n            (b) For single sequences:\n                ``tokens:         [CLS] the dog is hairy . [SEP]``\n                ``token_type_ids:   0   0   0   0  0     0   0``\n            ALBert is a model with absolute position embeddings so it\'s usually advised to pad the inputs on\n            the right rather than the left.\n            Indices can be obtained using :class:`transformers.BertTokenizer`.\n            See :func:`transformers.PreTrainedTokenizer.encode` and\n            :func:`transformers.PreTrainedTokenizer.convert_tokens_to_ids` for details.\n        **attention_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, sequence_length)``:\n            Mask to avoid performing attention on padding token indices.\n            Mask values selected in ``[0, 1]``:\n            ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n        **token_type_ids**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Segment token indices to indicate first and second portions of the inputs.\n            Indices are selected in ``[0, 1]``: ``0`` corresponds to a `sentence A` token, ``1``\n            corresponds to a `sentence B` token\n            (see `BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`_ for more details).\n        **position_ids**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Indices of positions of each input sequence tokens in the position embeddings.\n            Selected in the range ``[0, config.max_position_embeddings - 1]``.\n        **head_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(num_heads,)`` or ``(num_layers, num_heads)``:\n            Mask to nullify selected heads of the self-attention modules.\n            Mask values selected in ``[0, 1]``:\n            ``1`` indicates the head is **not masked**, ``0`` indicates the head is **masked**.\n""""""\n\n@add_start_docstrings(""The bare Albert Model transformer outputting raw hidden-states without any specific head on top."",\n                      ALBERT_START_DOCSTRING, ALBERT_INPUTS_DOCSTRING)\nclass AlbertModel(AlbertPreTrainedModel):\n    r""""""\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **last_hidden_state**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, hidden_size)``\n            Sequence of hidden-states at the output of the last layer of the model.\n        **pooler_output**: ``torch.FloatTensor`` of shape ``(batch_size, hidden_size)``\n            Last layer hidden-state of the first token of the sequence (classification token)\n            further processed by a Linear layer and a Tanh activation function. The Linear\n            layer weights are trained from the next sentence prediction (classification)\n            objective during Bert pretraining. This output is usually *not* a good summary\n            of the semantic content of the input, you\'re often better with averaging or pooling\n            the sequence of hidden-states for the whole input sequence.\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n    Examples::\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n        model = BertModel.from_pretrained(\'bert-base-uncased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids)\n        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n    """"""\n\n    def __init__(self, config):\n        super(AlbertModel, self).__init__(config)\n\n        self.embeddings = AlbertEmbeddings(config)\n        self.encoder = AlbertEncoder(config)\n        self.pooler = AlbertPooler(config)\n\n        self.init_weights()\n\n    def _resize_token_embeddings(self, new_num_tokens):\n        old_embeddings = self.embeddings.word_embeddings\n        new_embeddings = self._get_resized_embeddings(old_embeddings, new_num_tokens)\n        self.embeddings.word_embeddings = new_embeddings\n        return self.embeddings.word_embeddings\n\n    def _prune_heads(self, heads_to_prune):\n        """""" Prunes heads of the model.\n            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n            See base class PreTrainedModel\n        """"""\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n        if attention_mask is None:\n            attention_mask = torch.ones_like(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        # We create a 3D attention mask from a 2D tensor mask.\n        # Sizes are [batch_size, 1, 1, to_seq_length]\n        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n        # this attention mask is more simple than the triangular masking of causal attention\n        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        if head_mask is not None:\n            if head_mask.dim() == 1:\n                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n                head_mask = head_mask.expand(self.config.num_hidden_layers, -1, -1, -1, -1)\n            elif head_mask.dim() == 2:\n                head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(\n                    -1)  # We can specify head_mask for each layer\n            head_mask = head_mask.to(\n                dtype=next(self.parameters()).dtype)  # switch to fload if need + fp16 compatibility\n        else:\n            head_mask = [None] * self.config.num_hidden_layers\n\n        embedding_output = self.embeddings(input_ids, position_ids=position_ids, token_type_ids=token_type_ids)\n        encoder_outputs = self.encoder(embedding_output,\n                                       extended_attention_mask,\n                                       head_mask=head_mask)\n        sequence_output = encoder_outputs[0]\n        pooled_output = self.pooler(sequence_output)\n\n        outputs = (sequence_output, pooled_output,) + encoder_outputs[\n                                                      1:]  # add hidden_states and attentions if they are here\n        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\n\n@add_start_docstrings(""""""Bert Model with two heads on top as done during the pre-training:\n    a `masked language modeling` head and a `next sentence prediction (classification)` head. """""",\n                      ALBERT_START_DOCSTRING, ALBERT_INPUTS_DOCSTRING)\nclass AlbertForPreTraining(AlbertPreTrainedModel):\n    r""""""\n        **masked_lm_labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Labels for computing the masked language modeling loss.\n            Indices should be in ``[-1, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring)\n            Tokens with indices set to ``-1`` are ignored (masked), the loss is only computed for the tokens with labels\n            in ``[0, ..., config.vocab_size]``\n        **next_sentence_label**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair (see ``input_ids`` docstring)\n            Indices should be in ``[0, 1]``.\n            ``0`` indicates sequence B is a continuation of sequence A,\n            ``1`` indicates sequence B is a random sequence.\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when both ``masked_lm_labels`` and ``next_sentence_label`` are provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Total loss as the sum of the masked language modeling loss and the next sequence prediction (classification) loss.\n        **prediction_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.vocab_size)``\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n        **seq_relationship_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, 2)``\n            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n    Examples::\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n        model = BertForPreTraining.from_pretrained(\'bert-base-uncased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids)\n        prediction_scores, seq_relationship_scores = outputs[:2]\n    """"""\n\n    def __init__(self, config):\n        super(AlbertForPreTraining, self).__init__(config)\n        self.bert = AlbertModel(config)\n        self.cls = AlbertPreTrainingHeads(config)\n\n        self.init_weights()\n        self.tie_weights()\n\n    def tie_weights(self):\n        """""" Make sure we are sharing the input and output embeddings.\n            Export to TorchScript can\'t handle parameter sharing so we are cloning them instead.\n        """"""\n        self._tie_or_clone_weights(self.cls.predictions.decoder,\n                                   self.bert.embeddings.word_embeddings)\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n                masked_lm_labels=None, next_sentence_label=None):\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids,\n                            head_mask=head_mask)\n\n        sequence_output, pooled_output = outputs[:2]\n        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n\n        outputs = (prediction_scores, seq_relationship_score,) + outputs[\n                                                                 2:]  # add hidden states and attention if they are here\n\n        if masked_lm_labels is not None and next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n            total_loss = masked_lm_loss + next_sentence_loss\n            outputs = (total_loss,) + outputs\n        return outputs  # (loss), prediction_scores, seq_relationship_score, (hidden_states), (attentions)\n\n@add_start_docstrings(""""""Bert Model with a `language modeling` head on top. """""",\n                      ALBERT_START_DOCSTRING, ALBERT_INPUTS_DOCSTRING)\nclass AlbertForMaskedLM(AlbertPreTrainedModel):\n    r""""""\n        **masked_lm_labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Labels for computing the masked language modeling loss.\n            Indices should be in ``[-1, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring)\n            Tokens with indices set to ``-1`` are ignored (masked), the loss is only computed for the tokens with labels\n            in ``[0, ..., config.vocab_size]``\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``masked_lm_labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Masked language modeling loss.\n        **prediction_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.vocab_size)``\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n    Examples::\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n        model = BertForMaskedLM.from_pretrained(\'bert-base-uncased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, masked_lm_labels=input_ids)\n        loss, prediction_scores = outputs[:2]\n    """"""\n\n    def __init__(self, config):\n        super(AlbertForMaskedLM, self).__init__(config)\n\n        self.bert = AlbertModel(config)\n        self.cls = AlbertOnlyMLMHead(config)\n\n        self.init_weights()\n        self.tie_weights()\n\n    def tie_weights(self):\n        """""" Make sure we are sharing the input and output embeddings.\n            Export to TorchScript can\'t handle parameter sharing so we are cloning them instead.\n        """"""\n        self._tie_or_clone_weights(self.cls.predictions.decoder,\n                                   self.bert.embeddings.word_embeddings)\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n                masked_lm_labels=None):\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids,\n                            head_mask=head_mask)\n\n        sequence_output = outputs[0]\n        prediction_scores = self.cls(sequence_output)\n\n        outputs = (prediction_scores,) + outputs[2:]  # Add hidden states and attention if they are here\n        if masked_lm_labels is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n            outputs = (masked_lm_loss,) + outputs\n\n        return outputs  # (masked_lm_loss), prediction_scores, (hidden_states), (attentions)\n\n\n@add_start_docstrings(""""""Bert Model with a `next sentence prediction (classification)` head on top. """""",\n                      ALBERT_START_DOCSTRING, ALBERT_INPUTS_DOCSTRING)\nclass AlbertForNextSentencePrediction(AlbertPreTrainedModel):\n    r""""""\n        **next_sentence_label**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair (see ``input_ids`` docstring)\n            Indices should be in ``[0, 1]``.\n            ``0`` indicates sequence B is a continuation of sequence A,\n            ``1`` indicates sequence B is a random sequence.\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``next_sentence_label`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Next sequence prediction (classification) loss.\n        **seq_relationship_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, 2)``\n            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n    Examples::\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n        model = BertForNextSentencePrediction.from_pretrained(\'bert-base-uncased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids)\n        seq_relationship_scores = outputs[0]\n    """"""\n\n    def __init__(self, config):\n        super(AlbertForNextSentencePrediction, self).__init__(config)\n\n        self.bert = AlbertModel(config)\n        self.cls = AlbertOnlyNSPHead(config)\n\n        self.init_weights()\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n                next_sentence_label=None):\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids,\n                            head_mask=head_mask)\n\n        pooled_output = outputs[1]\n\n        seq_relationship_score = self.cls(pooled_output)\n\n        outputs = (seq_relationship_score,) + outputs[2:]  # add hidden states and attention if they are here\n        if next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n            outputs = (next_sentence_loss,) + outputs\n\n        return outputs  # (next_sentence_loss), seq_relationship_score, (hidden_states), (attentions)\n\n\n@add_start_docstrings(""""""Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of\n    the pooled output) e.g. for GLUE tasks. """""",\n                      ALBERT_START_DOCSTRING, ALBERT_INPUTS_DOCSTRING)\nclass AlbertForSequenceClassification(AlbertPreTrainedModel):\n    r""""""\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the sequence classification/regression loss.\n            Indices should be in ``[0, ..., config.num_labels - 1]``.\n            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Classification (or regression if config.num_labels==1) loss.\n        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n    Examples::\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n        model = BertForSequenceClassification.from_pretrained(\'bert-base-uncased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, labels=labels)\n        loss, logits = outputs[:2]\n    """"""\n\n    def __init__(self, config):\n        super(AlbertForSequenceClassification, self).__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = AlbertModel(config)\n        self.dropout = nn.Dropout(0.1 if config.hidden_dropout_prob == 0 else config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n\n        self.init_weights()\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None,\n                position_ids=None, head_mask=None, labels=None):\n\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids,\n                            head_mask=head_mask)\n\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output+0.1)\n        logits = self.classifier(pooled_output)\n\n        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n\n        if labels is not None:\n            if self.num_labels == 1:\n                #  We are doing regression\n                loss_fct = MSELoss()\n                loss = loss_fct(logits.view(-1), labels.view(-1))\n            else:\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            outputs = (loss,) + outputs\n\n        return outputs  # (loss), logits, (hidden_states), (attentions)\n\n\n@add_start_docstrings(""""""Bert Model with a multiple choice classification head on top (a linear layer on top of\n    the pooled output and a softmax) e.g. for RocStories/SWAG tasks. """""",\n                      ALBERT_START_DOCSTRING, ALBERT_INPUTS_DOCSTRING)\nclass AlbertForMultipleChoice(AlbertPreTrainedModel):\n    r""""""\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the multiple choice classification loss.\n            Indices should be in ``[0, ..., num_choices]`` where `num_choices` is the size of the second dimension\n            of the input tensors. (see `input_ids` above)\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Classification loss.\n        **classification_scores**: ``torch.FloatTensor`` of shape ``(batch_size, num_choices)`` where `num_choices` is the size of the second dimension\n            of the input tensors. (see `input_ids` above).\n            Classification scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n    Examples::\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n        model = BertForMultipleChoice.from_pretrained(\'bert-base-uncased\')\n        choices = [""Hello, my dog is cute"", ""Hello, my cat is amazing""]\n        input_ids = torch.tensor([tokenizer.encode(s) for s in choices]).unsqueeze(0)  # Batch size 1, 2 choices\n        labels = torch.tensor(1).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, labels=labels)\n        loss, classification_scores = outputs[:2]\n    """"""\n\n    def __init__(self, config):\n        super(AlbertForMultipleChoice, self).__init__(config)\n\n        self.bert = AlbertModel(config)\n        self.dropout = nn.Dropout(0.1 if config.hidden_dropout_prob == 0 else config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n\n        self.init_weights()\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None,\n                position_ids=None, head_mask=None, labels=None):\n        num_choices = input_ids.shape[1]\n\n        input_ids = input_ids.view(-1, input_ids.size(-1))\n        attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n        position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids,\n                            head_mask=head_mask)\n        pooled_output = outputs[1]\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        reshaped_logits = logits.view(-1, num_choices)\n        outputs = (reshaped_logits,) + outputs[2:]  # add hidden states and attention if they are here\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(reshaped_logits, labels)\n            outputs = (loss,) + outputs\n\n        return outputs  # (loss), reshaped_logits, (hidden_states), (attentions)\n\n\n@add_start_docstrings(""""""Bert Model with a token classification head on top (a linear layer on top of\n    the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. """""",\n                      ALBERT_START_DOCSTRING, ALBERT_INPUTS_DOCSTRING)\n\nclass AlbertForTokenClassification(AlbertPreTrainedModel):\n    r""""""\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Labels for computing the token classification loss.\n            Indices should be in ``[0, ..., config.num_labels - 1]``.\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Classification loss.\n        **scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.num_labels)``\n            Classification scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n    Examples::\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n        model = BertForTokenClassification.from_pretrained(\'bert-base-uncased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        labels = torch.tensor([1] * input_ids.size(1)).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, labels=labels)\n        loss, scores = outputs[:2]\n    """"""\n\n    def __init__(self, config):\n        super(AlbertForTokenClassification, self).__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = AlbertModel(config)\n        self.dropout = nn.Dropout(0.1 if config.hidden_dropout_prob == 0 else config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None,\n                position_ids=None, head_mask=None, labels=None):\n\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids,\n                            head_mask=head_mask)\n\n        sequence_output = outputs[0]\n\n        sequence_output = self.dropout(sequence_output)\n        logits = self.classifier(sequence_output)\n\n        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            # Only keep active parts of the loss\n            if attention_mask is not None:\n                active_loss = attention_mask.view(-1) == 1\n                active_logits = logits.view(-1, self.num_labels)[active_loss]\n                active_labels = labels.view(-1)[active_loss]\n                loss = loss_fct(active_logits, active_labels)\n            else:\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            outputs = (loss,) + outputs\n\n        return outputs  # (loss), scores, (hidden_states), (attentions)\n\n\n@add_start_docstrings(""""""Bert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear layers on top of\n    the hidden-states output to compute `span start logits` and `span end logits`). """""",\n                      ALBERT_START_DOCSTRING, ALBERT_INPUTS_DOCSTRING)\nclass AlbertForQuestionAnswering(AlbertPreTrainedModel):\n    r""""""\n        **start_positions**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`).\n            Position outside of the sequence are not taken into account for computing the loss.\n        **end_positions**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`).\n            Position outside of the sequence are not taken into account for computing the loss.\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.\n        **start_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length,)``\n            Span-start scores (before SoftMax).\n        **end_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length,)``\n            Span-end scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n    Examples::\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n        model = BertForQuestionAnswering.from_pretrained(\'bert-base-uncased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        start_positions = torch.tensor([1])\n        end_positions = torch.tensor([3])\n        outputs = model(input_ids, start_positions=start_positions, end_positions=end_positions)\n        loss, start_scores, end_scores = outputs[:2]\n    """"""\n\n    def __init__(self, config):\n        super(AlbertForQuestionAnswering, self).__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = AlbertModel(config)\n        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n                start_positions=None, end_positions=None):\n\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids,\n                            head_mask=head_mask)\n\n        sequence_output = outputs[0]\n\n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        outputs = (start_logits, end_logits,) + outputs[2:]\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions.clamp_(0, ignored_index)\n            end_positions.clamp_(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n            outputs = (total_loss,) + outputs\n\n        return outputs  # (loss), start_logits, end_logits, (hidden_states), (attentions)\n'"
models/transformers/modeling_albert_bright.py,74,"b'""""""PyTorch brightmart version  ALBERT model. """"""\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\nimport logging\r\nimport os\r\n\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import CrossEntropyLoss, MSELoss\r\n\r\nfrom .modeling_utils import PreTrainedModel, prune_linear_layer\r\nfrom .configuration_albert import AlbertConfig\r\nfrom .file_utils import add_start_docstrings\r\nfrom .modeling_bert import (ACT2FN,\r\n                            BertSelfAttention,\r\n                            BertIntermediate,\r\n                            BertPooler,\r\n                            BertPredictionHeadTransform)\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\nALBERT_PRETRAINED_MODEL_ARCHIVE_MAP = {\r\n    \'albert-base\': """",\r\n    \'albert-large\': """",\r\n    \'albert-xlarge\': """",\r\n    \'albert-xxlarge\': """",\r\n}\r\ndef load_tf_weights_in_albert(model, config, tf_checkpoint_path):\r\n    """""" Load tf checkpoints in a pytorch model.\r\n    """"""\r\n    try:\r\n        import re\r\n        import numpy as np\r\n        import tensorflow as tf\r\n    except ImportError:\r\n        logger.error(""Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see ""\r\n                     ""https://www.tensorflow.org/install/ for installation instructions."")\r\n        raise\r\n    tf_path = os.path.abspath(tf_checkpoint_path)\r\n    logger.info(""Converting TensorFlow checkpoint from {}"".format(tf_path))\r\n    # Load weights from TF model\r\n    init_vars = tf.train.list_variables(tf_path)\r\n    names = []\r\n    arrays = []\r\n    for name, shape in init_vars:\r\n        logger.info(""Loading TF weight {} with shape {}"".format(name, shape))\r\n        array = tf.train.load_variable(tf_path, name)\r\n        names.append(name)\r\n        arrays.append(array)\r\n    for name, array in zip(names, arrays):\r\n        name = name.split(\'/\')\r\n        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\r\n        # which are not required for using pretrained model\r\n        if any(n in [""adam_v"", ""adam_m"", ""global_step""] for n in name):\r\n            logger.info(""Skipping {}"".format(""/"".join(name)))\r\n            continue\r\n        pointer = model\r\n        for m_name in name:\r\n            if re.fullmatch(r\'[A-Za-z]+_\\d+\', m_name):\r\n                l = re.split(r\'_(\\d+)\', m_name)\r\n            else:\r\n                l = [m_name]\r\n            if l[0] == \'kernel\' or l[0] == \'gamma\':\r\n                pointer = getattr(pointer, \'weight\')\r\n            elif l[0] == \'output_bias\' or l[0] == \'beta\':\r\n                pointer = getattr(pointer, \'bias\')\r\n            elif l[0] == \'output_weights\':\r\n                pointer = getattr(pointer, \'weight\')\r\n            elif l[0] == \'squad\':\r\n                pointer = getattr(pointer, \'classifier\')\r\n            else:\r\n                try:\r\n                    pointer = getattr(pointer, l[0])\r\n                except AttributeError:\r\n                    logger.info(""Skipping {}"".format(""/"".join(name)))\r\n                    continue\r\n            if len(l) >= 2:\r\n                num = int(l[1])\r\n                pointer = pointer[num]\r\n        if m_name[-11:] == \'_embeddings\':\r\n            pointer = getattr(pointer, \'weight\')\r\n        elif m_name[-13:] == \'_embeddings_2\':\r\n            pointer = getattr(pointer, \'weight\')\r\n            array = np.transpose(array)\r\n        elif m_name == \'kernel\':\r\n            array = np.transpose(array)\r\n        try:\r\n            assert pointer.shape == array.shape\r\n        except AssertionError as e:\r\n            e.args += (pointer.shape, array.shape)\r\n            raise\r\n        logger.info(""Initialize PyTorch weight {}"".format(name))\r\n        pointer.data = torch.from_numpy(array)\r\n    return model\r\n\r\nAlbertLayerNorm = torch.nn.LayerNorm\r\nclass AlbertEmbeddings(nn.Module):\r\n    """"""Construct the embeddings from word, position and token_type embeddings.\r\n    """"""\r\n    def __init__(self, config):\r\n        super(AlbertEmbeddings, self).__init__()\r\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.embedding_size, padding_idx=0)\r\n        # project layer\r\n        self.word_embeddings_2 = nn.Linear(config.embedding_size, config.hidden_size, bias=False)\r\n\r\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\r\n        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\r\n\r\n        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\r\n        # any TensorFlow checkpoint file\r\n        self.LayerNorm =AlbertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\r\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\r\n\r\n    def forward(self, input_ids, token_type_ids=None, position_ids=None):\r\n        seq_length = input_ids.size(1)\r\n        if position_ids is None:\r\n            position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\r\n            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\r\n        if token_type_ids is None:\r\n            token_type_ids = torch.zeros_like(input_ids)\r\n\r\n        words_embeddings = self.word_embeddings(input_ids)\r\n        # project transform\r\n        words_embeddings = self.word_embeddings_2(words_embeddings)\r\n        position_embeddings = self.position_embeddings(position_ids)\r\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\r\n\r\n        embeddings = words_embeddings + position_embeddings + token_type_embeddings\r\n        embeddings = self.LayerNorm(embeddings)\r\n        embeddings = self.dropout(embeddings)\r\n        return embeddings\r\n\r\nclass AlbertSelfOutput(nn.Module):\r\n    def __init__(self, config):\r\n        super(AlbertSelfOutput, self).__init__()\r\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\r\n        self.LayerNorm = AlbertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\r\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\r\n\r\n    def forward(self, hidden_states, input_tensor):\r\n        hidden_states = self.dense(hidden_states)\r\n        hidden_states = self.dropout(hidden_states)\r\n        # postln\r\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\r\n        return hidden_states\r\n\r\n\r\nclass AlbertAttention(nn.Module):\r\n    def __init__(self, config):\r\n        super(AlbertAttention, self).__init__()\r\n        self.self = BertSelfAttention(config)\r\n        self.output = AlbertSelfOutput(config)\r\n        self.pruned_heads = set()\r\n\r\n    def prune_heads(self, heads):\r\n        if len(heads) == 0:\r\n            return\r\n        mask = torch.ones(self.self.num_attention_heads, self.self.attention_head_size)\r\n        heads = set(heads) - self.pruned_heads  # Convert to set and emove already pruned heads\r\n        for head in heads:\r\n            # Compute how many pruned heads are before the head and move the index accordingly\r\n            head = head - sum(1 if h < head else 0 for h in self.pruned_heads)\r\n            mask[head] = 0\r\n        mask = mask.view(-1).contiguous().eq(1)\r\n        index = torch.arange(len(mask))[mask].long()\r\n\r\n        # Prune linear layers\r\n        self.self.query = prune_linear_layer(self.self.query, index)\r\n        self.self.key = prune_linear_layer(self.self.key, index)\r\n        self.self.value = prune_linear_layer(self.self.value, index)\r\n        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\r\n\r\n        # Update hyper params and store pruned heads\r\n        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\r\n        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\r\n        self.pruned_heads = self.pruned_heads.union(heads)\r\n\r\n    def forward(self, input_tensor, attention_mask=None, head_mask=None):\r\n        # postln\r\n        self_outputs = self.self(input_tensor, attention_mask, head_mask)\r\n        attention_output = self.output(self_outputs[0], input_tensor)\r\n        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\r\n        return outputs\r\n\r\nclass AlbertOutput(nn.Module):\r\n    def __init__(self, config):\r\n        super(AlbertOutput, self).__init__()\r\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\r\n        self.LayerNorm = AlbertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\r\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\r\n\r\n    def forward(self, hidden_states, input_tensor):\r\n        hidden_states = self.dense(hidden_states)\r\n        hidden_states = self.dropout(hidden_states)\r\n        # postln\r\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\r\n        return hidden_states\r\n\r\nclass BertLayer(nn.Module):\r\n    def __init__(self, config):\r\n        super(BertLayer, self).__init__()\r\n        self.attention = AlbertAttention(config)\r\n        self.intermediate = BertIntermediate(config)\r\n        self.output = AlbertOutput(config)\r\n\r\n    def forward(self, hidden_states, attention_mask=None, head_mask=None):\r\n        attention_outputs = self.attention(hidden_states, attention_mask, head_mask)\r\n        attention_output = attention_outputs[0]\r\n        # postln\r\n        attention_output_pre = attention_output\r\n        intermediate_output = self.intermediate(attention_output_pre)\r\n        layer_output = self.output(intermediate_output, attention_output)\r\n        outputs = (layer_output,) + attention_outputs[1:]  # add attentions if we output them\r\n        return outputs\r\n\r\nclass AlbertEncoder(nn.Module):\r\n    def __init__(self, config):\r\n        super(AlbertEncoder, self).__init__()\r\n        self.output_attentions = config.output_attentions\r\n        self.output_hidden_states = config.output_hidden_states\r\n        self.num_hidden_layers = config.num_hidden_layers\r\n        self.layer_shared = BertLayer(config)\r\n\r\n    def forward(self, hidden_states, attention_mask=None, head_mask=None):\r\n        all_hidden_states = ()\r\n        all_attentions = ()\r\n        for i in range(self.num_hidden_layers):\r\n            layer_module = self.layer_shared\r\n            if self.output_hidden_states:\r\n                all_hidden_states = all_hidden_states + (hidden_states,)\r\n            layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i])\r\n            hidden_states = layer_outputs[0]\r\n\r\n            if self.output_attentions:\r\n                all_attentions = all_attentions + (layer_outputs[1],)\r\n        # Add last layer\r\n        if self.output_hidden_states:\r\n            all_hidden_states = all_hidden_states + (hidden_states,)\r\n        outputs = (hidden_states,)\r\n        if self.output_hidden_states:\r\n            outputs = outputs + (all_hidden_states,)\r\n        if self.output_attentions:\r\n            outputs = outputs + (all_attentions,)\r\n        return outputs  # last-layer hidden state, (all hidden states), (all attentions)\r\n\r\nclass AlbertLMPredictionHead(nn.Module):\r\n    def __init__(self, config):\r\n        super(AlbertLMPredictionHead, self).__init__()\r\n        self.transform = BertPredictionHeadTransform(config)\r\n        # The output weights are the same as the input embeddings, but there is\r\n        # an output-only bias for each token.\r\n        self.project_layer = nn.Linear(config.hidden_size, config.embedding_size, bias=False)\r\n        self.decoder = nn.Linear(config.embedding_size,\r\n                                 config.vocab_size,\r\n                                 bias=False)\r\n        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\r\n\r\n    def forward(self, hidden_states):\r\n        hidden_states = self.transform(hidden_states)\r\n        hidden_states = self.project_layer(hidden_states)\r\n        hidden_states = self.decoder(hidden_states) + self.bias\r\n        return hidden_states\r\n\r\nclass AlbertOnlyMLMHead(nn.Module):\r\n    def __init__(self, config):\r\n        super(AlbertOnlyMLMHead, self).__init__()\r\n        self.predictions = AlbertLMPredictionHead(config)\r\n\r\n    def forward(self, sequence_output):\r\n        prediction_scores = self.predictions(sequence_output)\r\n        return prediction_scores\r\n\r\nclass AlbertOnlyNSPHead(nn.Module):\r\n    def __init__(self, config):\r\n        super(AlbertOnlyNSPHead, self).__init__()\r\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\r\n\r\n    def forward(self, pooled_output):\r\n        seq_relationship_score = self.seq_relationship(pooled_output)\r\n        return seq_relationship_score\r\n\r\nclass AlbertPreTrainingHeads(nn.Module):\r\n    def __init__(self, config):\r\n        super(AlbertPreTrainingHeads, self).__init__()\r\n        self.predictions = AlbertLMPredictionHead(config)\r\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\r\n\r\n    def forward(self, sequence_output, pooled_output):\r\n        prediction_scores = self.predictions(sequence_output)\r\n        seq_relationship_score = self.seq_relationship(pooled_output)\r\n        return prediction_scores, seq_relationship_score\r\n\r\nclass AlbertPreTrainedModel(PreTrainedModel):\r\n    """""" An abstract class to handle weights initialization and\r\n        a simple interface for dowloading and loading pretrained models.\r\n    """"""\r\n    config_class = AlbertConfig\r\n    pretrained_model_archive_map = ALBERT_PRETRAINED_MODEL_ARCHIVE_MAP\r\n    load_tf_weights = load_tf_weights_in_albert\r\n    base_model_prefix = ""bert""\r\n\r\n    def _init_weights(self, module):\r\n        """""" Initialize the weights """"""\r\n        if isinstance(module, (nn.Linear, nn.Embedding)):\r\n            # Slightly different from the TF version which uses truncated_normal for initialization\r\n            # cf https://github.com/pytorch/pytorch/pull/5617\r\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\r\n        elif isinstance(module, AlbertLayerNorm):\r\n            module.bias.data.zero_()\r\n            module.weight.data.fill_(1.0)\r\n        if isinstance(module, nn.Linear) and module.bias is not None:\r\n            module.bias.data.zero_()\r\n\r\nBERT_START_DOCSTRING = r""""""    The BERT model was proposed in\r\n    `BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`_\r\n    by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It\'s a bidirectional transformer\r\n    pre-trained using a combination of masked language modeling objective and next sentence prediction\r\n    on a large corpus comprising the Toronto Book Corpus and Wikipedia.\r\n\r\n    This model is a PyTorch `torch.nn.Module`_ sub-class. Use it as a regular PyTorch Module and\r\n    refer to the PyTorch documentation for all matter related to general usage and behavior.\r\n\r\n    .. _`BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`:\r\n        https://arxiv.org/abs/1810.04805\r\n\r\n    .. _`torch.nn.Module`:\r\n        https://pytorch.org/docs/stable/nn.html#module\r\n\r\n    Parameters:\r\n        config (:class:`~transformers.BertConfig`): Model configuration class with all the parameters of the model. \r\n            Initializing with a config file does not load the weights associated with the model, only the configuration.\r\n            Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model weights.\r\n""""""\r\n\r\nBERT_INPUTS_DOCSTRING = r""""""\r\n    Inputs:\r\n        **input_ids**: ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\r\n            Indices of input sequence tokens in the vocabulary.\r\n            To match pre-training, BERT input sequence should be formatted with [CLS] and [SEP] tokens as follows:\r\n\r\n            (a) For sequence pairs:\r\n\r\n                ``tokens:         [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]``\r\n\r\n                ``token_type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1``\r\n\r\n            (b) For single sequences:\r\n\r\n                ``tokens:         [CLS] the dog is hairy . [SEP]``\r\n\r\n                ``token_type_ids:   0   0   0   0  0     0   0``\r\n\r\n            Bert is a model with absolute position embeddings so it\'s usually advised to pad the inputs on\r\n            the right rather than the left.\r\n\r\n            Indices can be obtained using :class:`transformers.BertTokenizer`.\r\n            See :func:`transformers.PreTrainedTokenizer.encode` and\r\n            :func:`transformers.PreTrainedTokenizer.convert_tokens_to_ids` for details.\r\n        **attention_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, sequence_length)``:\r\n            Mask to avoid performing attention on padding token indices.\r\n            Mask values selected in ``[0, 1]``:\r\n            ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\r\n        **token_type_ids**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\r\n            Segment token indices to indicate first and second portions of the inputs.\r\n            Indices are selected in ``[0, 1]``: ``0`` corresponds to a `sentence A` token, ``1``\r\n            corresponds to a `sentence B` token\r\n            (see `BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`_ for more details).\r\n        **position_ids**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\r\n            Indices of positions of each input sequence tokens in the position embeddings.\r\n            Selected in the range ``[0, config.max_position_embeddings - 1]``.\r\n        **head_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(num_heads,)`` or ``(num_layers, num_heads)``:\r\n            Mask to nullify selected heads of the self-attention modules.\r\n            Mask values selected in ``[0, 1]``:\r\n            ``1`` indicates the head is **not masked**, ``0`` indicates the head is **masked**.\r\n""""""\r\n\r\n\r\n@add_start_docstrings(""The bare Bert Model transformer outputting raw hidden-states without any specific head on top."",\r\n                      BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\r\nclass AlbertModel(AlbertPreTrainedModel):\r\n    r""""""\r\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\r\n        **last_hidden_state**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, hidden_size)``\r\n            Sequence of hidden-states at the output of the last layer of the model.\r\n        **pooler_output**: ``torch.FloatTensor`` of shape ``(batch_size, hidden_size)``\r\n            Last layer hidden-state of the first token of the sequence (classification token)\r\n            further processed by a Linear layer and a Tanh activation function. The Linear\r\n            layer weights are trained from the next sentence prediction (classification)\r\n            objective during Bert pretraining. This output is usually *not* a good summary\r\n            of the semantic content of the input, you\'re often better with averaging or pooling\r\n            the sequence of hidden-states for the whole input sequence.\r\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\r\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\r\n            of shape ``(batch_size, sequence_length, hidden_size)``:\r\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\r\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\r\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\r\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\r\n\r\n    Examples::\r\n\r\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\r\n        model = BertModel.from_pretrained(\'bert-base-uncased\')\r\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\r\n        outputs = model(input_ids)\r\n        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\r\n\r\n    """"""\r\n\r\n    def __init__(self, config):\r\n        super(AlbertModel, self).__init__(config)\r\n\r\n        self.embeddings = AlbertEmbeddings(config)\r\n        self.encoder = AlbertEncoder(config)\r\n        self.pooler = BertPooler(config)\r\n\r\n        self.init_weights()\r\n\r\n    def _resize_token_embeddings(self, new_num_tokens):\r\n        old_embeddings = self.embeddings.word_embeddings\r\n        new_embeddings = self._get_resized_embeddings(old_embeddings, new_num_tokens)\r\n        self.embeddings.word_embeddings = new_embeddings\r\n        return self.embeddings.word_embeddings\r\n\r\n    def _prune_heads(self, heads_to_prune):\r\n        """""" Prunes heads of the model.\r\n            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\r\n            See base class PreTrainedModel\r\n        """"""\r\n        for layer, heads in heads_to_prune.items():\r\n            self.encoder.layer[layer].attention.prune_heads(heads)\r\n\r\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\r\n        if attention_mask is None:\r\n            attention_mask = torch.ones_like(input_ids)\r\n        if token_type_ids is None:\r\n            token_type_ids = torch.zeros_like(input_ids)\r\n\r\n        # We create a 3D attention mask from a 2D tensor mask.\r\n        # Sizes are [batch_size, 1, 1, to_seq_length]\r\n        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\r\n        # this attention mask is more simple than the triangular masking of causal attention\r\n        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\r\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\r\n\r\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\r\n        # masked positions, this operation will create a tensor which is 0.0 for\r\n        # positions we want to attend and -10000.0 for masked positions.\r\n        # Since we are adding it to the raw scores before the softmax, this is\r\n        # effectively the same as removing these entirely.\r\n        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\r\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\r\n\r\n        # Prepare head mask if needed\r\n        # 1.0 in head_mask indicate we keep the head\r\n        # attention_probs has shape bsz x n_heads x N x N\r\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\r\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\r\n        if head_mask is not None:\r\n            if head_mask.dim() == 1:\r\n                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\r\n                head_mask = head_mask.expand(self.config.num_hidden_layers, -1, -1, -1, -1)\r\n            elif head_mask.dim() == 2:\r\n                head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(\r\n                    -1)  # We can specify head_mask for each layer\r\n            head_mask = head_mask.to(\r\n                dtype=next(self.parameters()).dtype)  # switch to fload if need + fp16 compatibility\r\n        else:\r\n            head_mask = [None] * self.config.num_hidden_layers\r\n\r\n        embedding_output = self.embeddings(input_ids, position_ids=position_ids, token_type_ids=token_type_ids)\r\n        encoder_outputs = self.encoder(embedding_output,\r\n                                       extended_attention_mask,\r\n                                       head_mask=head_mask)\r\n        sequence_output = encoder_outputs[0]\r\n        pooled_output = self.pooler(sequence_output)\r\n\r\n        outputs = (sequence_output, pooled_output,) + encoder_outputs[\r\n                                                      1:]  # add hidden_states and attentions if they are here\r\n        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\r\n\r\n\r\n@add_start_docstrings(""""""Bert Model with two heads on top as done during the pre-training:\r\n    a `masked language modeling` head and a `next sentence prediction (classification)` head. """""",\r\n                      BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\r\nclass AlbertForPreTraining(AlbertPreTrainedModel):\r\n    r""""""\r\n        **masked_lm_labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\r\n            Labels for computing the masked language modeling loss.\r\n            Indices should be in ``[-1, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring)\r\n            Tokens with indices set to ``-1`` are ignored (masked), the loss is only computed for the tokens with labels\r\n            in ``[0, ..., config.vocab_size]``\r\n        **next_sentence_label**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\r\n            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair (see ``input_ids`` docstring)\r\n            Indices should be in ``[0, 1]``.\r\n            ``0`` indicates sequence B is a continuation of sequence A,\r\n            ``1`` indicates sequence B is a random sequence.\r\n\r\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\r\n        **loss**: (`optional`, returned when both ``masked_lm_labels`` and ``next_sentence_label`` are provided) ``torch.FloatTensor`` of shape ``(1,)``:\r\n            Total loss as the sum of the masked language modeling loss and the next sequence prediction (classification) loss.\r\n        **prediction_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.vocab_size)``\r\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\r\n        **seq_relationship_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, 2)``\r\n            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation before SoftMax).\r\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\r\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\r\n            of shape ``(batch_size, sequence_length, hidden_size)``:\r\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\r\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\r\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\r\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\r\n\r\n    Examples::\r\n\r\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\r\n        model = BertForPreTraining.from_pretrained(\'bert-base-uncased\')\r\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\r\n        outputs = model(input_ids)\r\n        prediction_scores, seq_relationship_scores = outputs[:2]\r\n\r\n    """"""\r\n\r\n    def __init__(self, config):\r\n        super(AlbertForPreTraining, self).__init__(config)\r\n\r\n        self.bert = AlbertModel(config)\r\n        self.cls = AlbertPreTrainingHeads(config)\r\n\r\n        self.init_weights()\r\n        self.tie_weights()\r\n\r\n    def tie_weights(self):\r\n        """""" Make sure we are sharing the input and output embeddings.\r\n            Export to TorchScript can\'t handle parameter sharing so we are cloning them instead.\r\n        """"""\r\n        self._tie_or_clone_weights(self.cls.predictions.decoder,\r\n                                       self.bert.embeddings.word_embeddings)\r\n\r\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\r\n                masked_lm_labels=None, next_sentence_label=None):\r\n        outputs = self.bert(input_ids,\r\n                            attention_mask=attention_mask,\r\n                            token_type_ids=token_type_ids,\r\n                            position_ids=position_ids,\r\n                            head_mask=head_mask)\r\n\r\n        sequence_output, pooled_output = outputs[:2]\r\n        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\r\n\r\n        outputs = (prediction_scores, seq_relationship_score,) + outputs[\r\n                                                                 2:]  # add hidden states and attention if they are here\r\n\r\n        if masked_lm_labels is not None and next_sentence_label is not None:\r\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\r\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\r\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\r\n            total_loss = masked_lm_loss + next_sentence_loss\r\n            outputs = (total_loss,) + outputs\r\n\r\n        return outputs  # (loss), prediction_scores, seq_relationship_score, (hidden_states), (attentions)\r\n\r\n\r\n@add_start_docstrings(""""""Bert Model with a `language modeling` head on top. """""",\r\n                      BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\r\nclass AlbertForMaskedLM(AlbertPreTrainedModel):\r\n    r""""""\r\n        **masked_lm_labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\r\n            Labels for computing the masked language modeling loss.\r\n            Indices should be in ``[-1, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring)\r\n            Tokens with indices set to ``-1`` are ignored (masked), the loss is only computed for the tokens with labels\r\n            in ``[0, ..., config.vocab_size]``\r\n\r\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\r\n        **loss**: (`optional`, returned when ``masked_lm_labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\r\n            Masked language modeling loss.\r\n        **prediction_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.vocab_size)``\r\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\r\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\r\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\r\n            of shape ``(batch_size, sequence_length, hidden_size)``:\r\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\r\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\r\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\r\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\r\n\r\n    Examples::\r\n\r\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\r\n        model = BertForMaskedLM.from_pretrained(\'bert-base-uncased\')\r\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\r\n        outputs = model(input_ids, masked_lm_labels=input_ids)\r\n        loss, prediction_scores = outputs[:2]\r\n\r\n    """"""\r\n\r\n    def __init__(self, config):\r\n        super(AlbertForMaskedLM, self).__init__(config)\r\n\r\n        self.bert = AlbertModel(config)\r\n        self.cls = AlbertOnlyMLMHead(config)\r\n\r\n        self.init_weights()\r\n        self.tie_weights()\r\n\r\n    def tie_weights(self):\r\n        """""" Make sure we are sharing the input and output embeddings.\r\n            Export to TorchScript can\'t handle parameter sharing so we are cloning them instead.\r\n        """"""\r\n        self._tie_or_clone_weights(self.cls.predictions.decoder,\r\n                                   self.bert.embeddings.word_embeddings)\r\n\r\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\r\n                masked_lm_labels=None):\r\n        outputs = self.bert(input_ids,\r\n                            attention_mask=attention_mask,\r\n                            token_type_ids=token_type_ids,\r\n                            position_ids=position_ids,\r\n                            head_mask=head_mask)\r\n\r\n        sequence_output = outputs[0]\r\n        prediction_scores = self.cls(sequence_output)\r\n\r\n        outputs = (prediction_scores,) + outputs[2:]  # Add hidden states and attention if they are here\r\n        if masked_lm_labels is not None:\r\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\r\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\r\n            outputs = (masked_lm_loss,) + outputs\r\n\r\n        return outputs  # (masked_lm_loss), prediction_scores, (hidden_states), (attentions)\r\n\r\n\r\n@add_start_docstrings(""""""Bert Model with a `next sentence prediction (classification)` head on top. """""",\r\n                      BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\r\nclass AlbertForNextSentencePrediction(AlbertPreTrainedModel):\r\n    r""""""\r\n        **next_sentence_label**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\r\n            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair (see ``input_ids`` docstring)\r\n            Indices should be in ``[0, 1]``.\r\n            ``0`` indicates sequence B is a continuation of sequence A,\r\n            ``1`` indicates sequence B is a random sequence.\r\n\r\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\r\n        **loss**: (`optional`, returned when ``next_sentence_label`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\r\n            Next sequence prediction (classification) loss.\r\n        **seq_relationship_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, 2)``\r\n            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation before SoftMax).\r\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\r\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\r\n            of shape ``(batch_size, sequence_length, hidden_size)``:\r\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\r\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\r\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\r\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\r\n\r\n    Examples::\r\n\r\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\r\n        model = BertForNextSentencePrediction.from_pretrained(\'bert-base-uncased\')\r\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\r\n        outputs = model(input_ids)\r\n        seq_relationship_scores = outputs[0]\r\n\r\n    """"""\r\n\r\n    def __init__(self, config):\r\n        super(AlbertForNextSentencePrediction, self).__init__(config)\r\n\r\n        self.bert = AlbertModel(config)\r\n        self.cls = AlbertOnlyNSPHead(config)\r\n\r\n        self.init_weights()\r\n\r\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\r\n                next_sentence_label=None):\r\n        outputs = self.bert(input_ids,\r\n                            attention_mask=attention_mask,\r\n                            token_type_ids=token_type_ids,\r\n                            position_ids=position_ids,\r\n                            head_mask=head_mask)\r\n\r\n        pooled_output = outputs[1]\r\n\r\n        seq_relationship_score = self.cls(pooled_output)\r\n\r\n        outputs = (seq_relationship_score,) + outputs[2:]  # add hidden states and attention if they are here\r\n        if next_sentence_label is not None:\r\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\r\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\r\n            outputs = (next_sentence_loss,) + outputs\r\n\r\n        return outputs  # (next_sentence_loss), seq_relationship_score, (hidden_states), (attentions)\r\n\r\n\r\n@add_start_docstrings(""""""Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of\r\n    the pooled output) e.g. for GLUE tasks. """""",\r\n                      BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\r\nclass AlbertForSequenceClassification(AlbertPreTrainedModel):\r\n    r""""""\r\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\r\n            Labels for computing the sequence classification/regression loss.\r\n            Indices should be in ``[0, ..., config.num_labels - 1]``.\r\n            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\r\n            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\r\n\r\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\r\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\r\n            Classification (or regression if config.num_labels==1) loss.\r\n        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\r\n            Classification (or regression if config.num_labels==1) scores (before SoftMax).\r\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\r\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\r\n            of shape ``(batch_size, sequence_length, hidden_size)``:\r\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\r\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\r\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\r\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\r\n\r\n    Examples::\r\n\r\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\r\n        model = BertForSequenceClassification.from_pretrained(\'bert-base-uncased\')\r\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\r\n        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\r\n        outputs = model(input_ids, labels=labels)\r\n        loss, logits = outputs[:2]\r\n\r\n    """"""\r\n\r\n    def __init__(self, config):\r\n        super(AlbertForSequenceClassification, self).__init__(config)\r\n        self.num_labels = config.num_labels\r\n\r\n        self.bert = AlbertModel(config)\r\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\r\n        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\r\n\r\n        self.init_weights()\r\n\r\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None,\r\n                position_ids=None, head_mask=None, labels=None):\r\n\r\n        outputs = self.bert(input_ids,\r\n                            attention_mask=attention_mask,\r\n                            token_type_ids=token_type_ids,\r\n                            position_ids=position_ids,\r\n                            head_mask=head_mask)\r\n\r\n        pooled_output = outputs[1]\r\n\r\n        pooled_output = self.dropout(pooled_output)\r\n        logits = self.classifier(pooled_output)\r\n\r\n        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\r\n\r\n        if labels is not None:\r\n            if self.num_labels == 1:\r\n                #  We are doing regression\r\n                loss_fct = MSELoss()\r\n                loss = loss_fct(logits.view(-1), labels.view(-1))\r\n            else:\r\n                loss_fct = CrossEntropyLoss()\r\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\r\n            outputs = (loss,) + outputs\r\n\r\n        return outputs  # (loss), logits, (hidden_states), (attentions)\r\n\r\n\r\n@add_start_docstrings(""""""Bert Model with a multiple choice classification head on top (a linear layer on top of\r\n    the pooled output and a softmax) e.g. for RocStories/SWAG tasks. """""",\r\n                      BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\r\nclass AlbertForMultipleChoice(AlbertPreTrainedModel):\r\n    r""""""\r\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\r\n            Labels for computing the multiple choice classification loss.\r\n            Indices should be in ``[0, ..., num_choices]`` where `num_choices` is the size of the second dimension\r\n            of the input tensors. (see `input_ids` above)\r\n\r\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\r\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\r\n            Classification loss.\r\n        **classification_scores**: ``torch.FloatTensor`` of shape ``(batch_size, num_choices)`` where `num_choices` is the size of the second dimension\r\n            of the input tensors. (see `input_ids` above).\r\n            Classification scores (before SoftMax).\r\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\r\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\r\n            of shape ``(batch_size, sequence_length, hidden_size)``:\r\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\r\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\r\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\r\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\r\n\r\n    Examples::\r\n\r\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\r\n        model = BertForMultipleChoice.from_pretrained(\'bert-base-uncased\')\r\n        choices = [""Hello, my dog is cute"", ""Hello, my cat is amazing""]\r\n        input_ids = torch.tensor([tokenizer.encode(s) for s in choices]).unsqueeze(0)  # Batch size 1, 2 choices\r\n        labels = torch.tensor(1).unsqueeze(0)  # Batch size 1\r\n        outputs = model(input_ids, labels=labels)\r\n        loss, classification_scores = outputs[:2]\r\n\r\n    """"""\r\n\r\n    def __init__(self, config):\r\n        super(AlbertForMultipleChoice, self).__init__(config)\r\n\r\n        self.bert = AlbertModel(config)\r\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\r\n        self.classifier = nn.Linear(config.hidden_size, 1)\r\n\r\n        self.init_weights()\r\n\r\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None,\r\n                position_ids=None, head_mask=None, labels=None):\r\n        num_choices = input_ids.shape[1]\r\n\r\n        input_ids = input_ids.view(-1, input_ids.size(-1))\r\n        attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\r\n        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\r\n        position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\r\n\r\n        outputs = self.bert(input_ids,\r\n                            attention_mask=attention_mask,\r\n                            token_type_ids=token_type_ids,\r\n                            position_ids=position_ids,\r\n                            head_mask=head_mask)\r\n\r\n        pooled_output = outputs[1]\r\n\r\n        pooled_output = self.dropout(pooled_output)\r\n        logits = self.classifier(pooled_output)\r\n        reshaped_logits = logits.view(-1, num_choices)\r\n\r\n        outputs = (reshaped_logits,) + outputs[2:]  # add hidden states and attention if they are here\r\n\r\n        if labels is not None:\r\n            loss_fct = CrossEntropyLoss()\r\n            loss = loss_fct(reshaped_logits, labels)\r\n            outputs = (loss,) + outputs\r\n\r\n        return outputs  # (loss), reshaped_logits, (hidden_states), (attentions)\r\n\r\n\r\n@add_start_docstrings(""""""Bert Model with a token classification head on top (a linear layer on top of\r\n    the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. """""",\r\n                      BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\r\nclass AlbertForTokenClassification(AlbertPreTrainedModel):\r\n    r""""""\r\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\r\n            Labels for computing the token classification loss.\r\n            Indices should be in ``[0, ..., config.num_labels - 1]``.\r\n\r\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\r\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\r\n            Classification loss.\r\n        **scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.num_labels)``\r\n            Classification scores (before SoftMax).\r\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\r\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\r\n            of shape ``(batch_size, sequence_length, hidden_size)``:\r\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\r\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\r\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\r\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\r\n\r\n    Examples::\r\n\r\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\r\n        model = BertForTokenClassification.from_pretrained(\'bert-base-uncased\')\r\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\r\n        labels = torch.tensor([1] * input_ids.size(1)).unsqueeze(0)  # Batch size 1\r\n        outputs = model(input_ids, labels=labels)\r\n        loss, scores = outputs[:2]\r\n\r\n    """"""\r\n\r\n    def __init__(self, config):\r\n        super(AlbertForTokenClassification, self).__init__(config)\r\n        self.num_labels = config.num_labels\r\n\r\n        self.bert = AlbertModel(config)\r\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\r\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\r\n\r\n        self.init_weights()\r\n\r\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None,\r\n                position_ids=None, head_mask=None, labels=None):\r\n\r\n        outputs = self.bert(input_ids,\r\n                            attention_mask=attention_mask,\r\n                            token_type_ids=token_type_ids,\r\n                            position_ids=position_ids,\r\n                            head_mask=head_mask)\r\n\r\n        sequence_output = outputs[0]\r\n\r\n        sequence_output = self.dropout(sequence_output)\r\n        logits = self.classifier(sequence_output)\r\n\r\n        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\r\n        if labels is not None:\r\n            loss_fct = CrossEntropyLoss()\r\n            # Only keep active parts of the loss\r\n            if attention_mask is not None:\r\n                active_loss = attention_mask.view(-1) == 1\r\n                active_logits = logits.view(-1, self.num_labels)[active_loss]\r\n                active_labels = labels.view(-1)[active_loss]\r\n                loss = loss_fct(active_logits, active_labels)\r\n            else:\r\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\r\n            outputs = (loss,) + outputs\r\n\r\n        return outputs  # (loss), scores, (hidden_states), (attentions)\r\n\r\n\r\n@add_start_docstrings(""""""Bert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear layers on top of\r\n    the hidden-states output to compute `span start logits` and `span end logits`). """""",\r\n                      BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\r\nclass AlbertForQuestionAnswering(AlbertPreTrainedModel):\r\n    r""""""\r\n        **start_positions**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\r\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\r\n            Positions are clamped to the length of the sequence (`sequence_length`).\r\n            Position outside of the sequence are not taken into account for computing the loss.\r\n        **end_positions**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\r\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\r\n            Positions are clamped to the length of the sequence (`sequence_length`).\r\n            Position outside of the sequence are not taken into account for computing the loss.\r\n\r\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\r\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\r\n            Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.\r\n        **start_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length,)``\r\n            Span-start scores (before SoftMax).\r\n        **end_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length,)``\r\n            Span-end scores (before SoftMax).\r\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\r\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\r\n            of shape ``(batch_size, sequence_length, hidden_size)``:\r\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\r\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\r\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\r\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\r\n\r\n    Examples::\r\n\r\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\r\n        model = BertForQuestionAnswering.from_pretrained(\'bert-base-uncased\')\r\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\r\n        start_positions = torch.tensor([1])\r\n        end_positions = torch.tensor([3])\r\n        outputs = model(input_ids, start_positions=start_positions, end_positions=end_positions)\r\n        loss, start_scores, end_scores = outputs[:2]\r\n\r\n    """"""\r\n\r\n    def __init__(self, config):\r\n        super(AlbertForQuestionAnswering, self).__init__(config)\r\n        self.num_labels = config.num_labels\r\n\r\n        self.bert = AlbertModel(config)\r\n        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\r\n\r\n        self.init_weights()\r\n\r\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\r\n                start_positions=None, end_positions=None):\r\n\r\n        outputs = self.bert(input_ids,\r\n                            attention_mask=attention_mask,\r\n                            token_type_ids=token_type_ids,\r\n                            position_ids=position_ids,\r\n                            head_mask=head_mask)\r\n\r\n        sequence_output = outputs[0]\r\n\r\n        logits = self.qa_outputs(sequence_output)\r\n        start_logits, end_logits = logits.split(1, dim=-1)\r\n        start_logits = start_logits.squeeze(-1)\r\n        end_logits = end_logits.squeeze(-1)\r\n\r\n        outputs = (start_logits, end_logits,) + outputs[2:]\r\n        if start_positions is not None and end_positions is not None:\r\n            # If we are on multi-GPU, split add a dimension\r\n            if len(start_positions.size()) > 1:\r\n                start_positions = start_positions.squeeze(-1)\r\n            if len(end_positions.size()) > 1:\r\n                end_positions = end_positions.squeeze(-1)\r\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\r\n            ignored_index = start_logits.size(1)\r\n            start_positions.clamp_(0, ignored_index)\r\n            end_positions.clamp_(0, ignored_index)\r\n\r\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\r\n            start_loss = loss_fct(start_logits, start_positions)\r\n            end_loss = loss_fct(end_logits, end_positions)\r\n            total_loss = (start_loss + end_loss) / 2\r\n            outputs = (total_loss,) + outputs\r\n\r\n        return outputs  # (loss), start_logits, end_logits, (hidden_states), (attentions)\r\n'"
models/transformers/modeling_auto.py,0,"b'# coding=utf-8\n# Copyright 2018 The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" Auto Model class. """"""\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport logging\n\nfrom .modeling_bert import BertModel, BertForMaskedLM, BertForSequenceClassification, BertForQuestionAnswering\nfrom .modeling_openai import OpenAIGPTModel, OpenAIGPTLMHeadModel\nfrom .modeling_gpt2 import GPT2Model, GPT2LMHeadModel\nfrom .modeling_ctrl import CTRLModel, CTRLLMHeadModel\nfrom .modeling_transfo_xl import TransfoXLModel, TransfoXLLMHeadModel\nfrom .modeling_xlnet import XLNetModel, XLNetLMHeadModel, XLNetForSequenceClassification, XLNetForQuestionAnswering\nfrom .modeling_xlm import XLMModel, XLMWithLMHeadModel, XLMForSequenceClassification, XLMForQuestionAnswering\nfrom .modeling_roberta import RobertaModel, RobertaForMaskedLM, RobertaForSequenceClassification\nfrom .modeling_distilbert import DistilBertModel, DistilBertForQuestionAnswering, DistilBertForMaskedLM, DistilBertForSequenceClassification\n\nfrom .modeling_utils import PreTrainedModel, SequenceSummary\n\nfrom .file_utils import add_start_docstrings\n\nlogger = logging.getLogger(__name__)\n\n\nclass AutoModel(object):\n    r""""""\n        :class:`~transformers.AutoModel` is a generic model class\n        that will be instantiated as one of the base model classes of the library\n        when created with the `AutoModel.from_pretrained(pretrained_model_name_or_path)`\n        class method.\n\n        The `from_pretrained()` method takes care of returning the correct model class instance\n        using pattern matching on the `pretrained_model_name_or_path` string.\n\n        The base model class to instantiate is selected as the first pattern matching\n        in the `pretrained_model_name_or_path` string (in the following order):\n            - contains `distilbert`: DistilBertModel (DistilBERT model)\n            - contains `roberta`: RobertaModel (RoBERTa model)\n            - contains `bert`: BertModel (Bert model)\n            - contains `openai-gpt`: OpenAIGPTModel (OpenAI GPT model)\n            - contains `gpt2`: GPT2Model (OpenAI GPT-2 model)\n            - contains `ctrl`: CTRLModel (Salesforce CTRL  model)\n            - contains `transfo-xl`: TransfoXLModel (Transformer-XL model)\n            - contains `xlnet`: XLNetModel (XLNet model)\n            - contains `xlm`: XLMModel (XLM model)\n\n        This class cannot be instantiated using `__init__()` (throws an error).\n    """"""\n    def __init__(self):\n        raise EnvironmentError(""AutoModel is designed to be instantiated ""\n            ""using the `AutoModel.from_pretrained(pretrained_model_name_or_path)` method."")\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n        r"""""" Instantiates one of the base model classes of the library\n        from a pre-trained model configuration.\n\n        The model class to instantiate is selected as the first pattern matching\n        in the `pretrained_model_name_or_path` string (in the following order):\n            - contains `distilbert`: DistilBertModel (DistilBERT model)\n            - contains `roberta`: RobertaModel (RoBERTa model)\n            - contains `bert`: BertModel (Bert model)\n            - contains `openai-gpt`: OpenAIGPTModel (OpenAI GPT model)\n            - contains `gpt2`: GPT2Model (OpenAI GPT-2 model)\n            - contains `ctrl`: CTRLModel (Salesforce CTRL  model)\n            - contains `transfo-xl`: TransfoXLModel (Transformer-XL model)\n            - contains `xlnet`: XLNetModel (XLNet model)\n            - contains `xlm`: XLMModel (XLM model)\n\n            The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated)\n            To train the model, you should first set it back in training mode with `model.train()`\n\n        Params:\n            pretrained_model_name_or_path: either:\n\n                - a string with the `shortcut name` of a pre-trained model to load from cache or download, e.g.: ``bert-base-uncased``.\n                - a path to a `directory` containing model weights saved using :func:`~transformers.PreTrainedModel.save_pretrained`, e.g.: ``./my_model_directory/``.\n                - a path or url to a `tensorflow index checkpoint file` (e.g. `./tf_model/model.ckpt.index`). In this case, ``from_tf`` should be set to True and a configuration object should be provided as ``config`` argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n\n            model_args: (`optional`) Sequence of positional arguments:\n                All remaning positional arguments will be passed to the underlying model\'s ``__init__`` method\n\n            config: (`optional`) instance of a class derived from :class:`~transformers.PretrainedConfig`:\n                Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:\n\n                - the model is a model provided by the library (loaded with the ``shortcut-name`` string of a pretrained model), or\n                - the model was saved using :func:`~transformers.PreTrainedModel.save_pretrained` and is reloaded by suppling the save directory.\n                - the model is loaded by suppling a local directory as ``pretrained_model_name_or_path`` and a configuration JSON file named `config.json` is found in the directory.\n\n            state_dict: (`optional`) dict:\n                an optional state dictionnary for the model to use instead of a state dictionary loaded from saved weights file.\n                This option can be used if you want to create a model from a pretrained configuration but load your own weights.\n                In this case though, you should check if using :func:`~transformers.PreTrainedModel.save_pretrained` and :func:`~transformers.PreTrainedModel.from_pretrained` is not a simpler option.\n\n            cache_dir: (`optional`) string:\n                Path to a directory in which a downloaded pre-trained model\n                configuration should be cached if the standard cache should not be used.\n\n            force_download: (`optional`) boolean, default False:\n                Force to (re-)download the model weights and configuration files and override the cached versions if they exists.\n\n            proxies: (`optional`) dict, default None:\n                A dictionary of proxy servers to use by protocol or endpoint, e.g.: {\'http\': \'foo.bar:3128\', \'http://hostname\': \'foo.bar:4012\'}.\n                The proxies are used on each request.\n\n            output_loading_info: (`optional`) boolean:\n                Set to ``True`` to also return a dictionnary containing missing keys, unexpected keys and error messages.\n\n            kwargs: (`optional`) Remaining dictionary of keyword arguments:\n                Can be used to update the configuration object (after it being loaded) and initiate the model. (e.g. ``output_attention=True``). Behave differently depending on whether a `config` is provided or automatically loaded:\n\n                - If a configuration is provided with ``config``, ``**kwargs`` will be directly passed to the underlying model\'s ``__init__`` method (we assume all relevant updates to the configuration have already been done)\n                - If a configuration is not provided, ``kwargs`` will be first passed to the configuration class initialization function (:func:`~transformers.PretrainedConfig.from_pretrained`). Each key of ``kwargs`` that corresponds to a configuration attribute will be used to override said attribute with the supplied ``kwargs`` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model\'s ``__init__`` function.\n\n        Examples::\n\n            model = AutoModel.from_pretrained(\'bert-base-uncased\')    # Download model and configuration from S3 and cache.\n            model = AutoModel.from_pretrained(\'./test/bert_model/\')  # E.g. model was saved using `save_pretrained(\'./test/saved_model/\')`\n            model = AutoModel.from_pretrained(\'bert-base-uncased\', output_attention=True)  # Update configuration during loading\n            assert model.config.output_attention == True\n            # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n            config = AutoConfig.from_json_file(\'./tf_model/bert_tf_model_config.json\')\n            model = AutoModel.from_pretrained(\'./tf_model/bert_tf_checkpoint.ckpt.index\', from_tf=True, config=config)\n\n        """"""\n        if \'distilbert\' in pretrained_model_name_or_path:\n            return DistilBertModel.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n        elif \'roberta\' in pretrained_model_name_or_path:\n            return RobertaModel.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n        elif \'bert\' in pretrained_model_name_or_path:\n            return BertModel.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n        elif \'openai-gpt\' in pretrained_model_name_or_path:\n            return OpenAIGPTModel.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n        elif \'gpt2\' in pretrained_model_name_or_path:\n            return GPT2Model.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n        elif \'transfo-xl\' in pretrained_model_name_or_path:\n            return TransfoXLModel.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n        elif \'xlnet\' in pretrained_model_name_or_path:\n            return XLNetModel.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n        elif \'xlm\' in pretrained_model_name_or_path:\n            return XLMModel.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n        elif \'ctrl\' in pretrained_model_name_or_path:\n            return CTRLModel.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n        raise ValueError(""Unrecognized model identifier in {}. Should contains one of ""\n                         ""\'bert\', \'openai-gpt\', \'gpt2\', \'transfo-xl\', \'xlnet\', ""\n                         ""\'xlm\', \'roberta, \'ctrl\'"".format(pretrained_model_name_or_path))\n\n\nclass AutoModelWithLMHead(object):\n    r""""""\n        :class:`~transformers.AutoModelWithLMHead` is a generic model class\n        that will be instantiated as one of the language modeling model classes of the library\n        when created with the `AutoModelWithLMHead.from_pretrained(pretrained_model_name_or_path)`\n        class method.\n\n        The `from_pretrained()` method takes care of returning the correct model class instance\n        using pattern matching on the `pretrained_model_name_or_path` string.\n\n        The model class to instantiate is selected as the first pattern matching\n        in the `pretrained_model_name_or_path` string (in the following order):\n            - contains `distilbert`: DistilBertForMaskedLM (DistilBERT model)\n            - contains `roberta`: RobertaForMaskedLM (RoBERTa model)\n            - contains `bert`: BertForMaskedLM (Bert model)\n            - contains `openai-gpt`: OpenAIGPTLMHeadModel (OpenAI GPT model)\n            - contains `gpt2`: GPT2LMHeadModel (OpenAI GPT-2 model)\n            - contains `ctrl`: CTRLLMModel (Salesforce CTRL model)\n            - contains `transfo-xl`: TransfoXLLMHeadModel (Transformer-XL model)\n            - contains `xlnet`: XLNetLMHeadModel (XLNet model)\n            - contains `xlm`: XLMWithLMHeadModel (XLM model)\n\n        This class cannot be instantiated using `__init__()` (throws an error).\n    """"""\n    def __init__(self):\n        raise EnvironmentError(""AutoModelWithLMHead is designed to be instantiated ""\n            ""using the `AutoModelWithLMHead.from_pretrained(pretrained_model_name_or_path)` method."")\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n        r"""""" Instantiates one of the language modeling model classes of the library\n        from a pre-trained model configuration.\n\n        The `from_pretrained()` method takes care of returning the correct model class instance\n        using pattern matching on the `pretrained_model_name_or_path` string.\n\n        The model class to instantiate is selected as the first pattern matching\n        in the `pretrained_model_name_or_path` string (in the following order):\n            - contains `distilbert`: DistilBertForMaskedLM (DistilBERT model)\n            - contains `roberta`: RobertaForMaskedLM (RoBERTa model)\n            - contains `bert`: BertForMaskedLM (Bert model)\n            - contains `openai-gpt`: OpenAIGPTLMHeadModel (OpenAI GPT model)\n            - contains `gpt2`: GPT2LMHeadModel (OpenAI GPT-2 model)\n            - contains `transfo-xl`: TransfoXLLMHeadModel (Transformer-XL model)\n            - contains `xlnet`: XLNetLMHeadModel (XLNet model)\n            - contains `xlm`: XLMWithLMHeadModel (XLM model)\n\n        The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated)\n        To train the model, you should first set it back in training mode with `model.train()`\n\n        Params:\n            pretrained_model_name_or_path: either:\n\n                - a string with the `shortcut name` of a pre-trained model to load from cache or download, e.g.: ``bert-base-uncased``.\n                - a path to a `directory` containing model weights saved using :func:`~transformers.PreTrainedModel.save_pretrained`, e.g.: ``./my_model_directory/``.\n                - a path or url to a `tensorflow index checkpoint file` (e.g. `./tf_model/model.ckpt.index`). In this case, ``from_tf`` should be set to True and a configuration object should be provided as ``config`` argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n\n            model_args: (`optional`) Sequence of positional arguments:\n                All remaning positional arguments will be passed to the underlying model\'s ``__init__`` method\n\n            config: (`optional`) instance of a class derived from :class:`~transformers.PretrainedConfig`:\n                Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:\n\n                - the model is a model provided by the library (loaded with the ``shortcut-name`` string of a pretrained model), or\n                - the model was saved using :func:`~transformers.PreTrainedModel.save_pretrained` and is reloaded by suppling the save directory.\n                - the model is loaded by suppling a local directory as ``pretrained_model_name_or_path`` and a configuration JSON file named `config.json` is found in the directory.\n\n            state_dict: (`optional`) dict:\n                an optional state dictionnary for the model to use instead of a state dictionary loaded from saved weights file.\n                This option can be used if you want to create a model from a pretrained configuration but load your own weights.\n                In this case though, you should check if using :func:`~transformers.PreTrainedModel.save_pretrained` and :func:`~transformers.PreTrainedModel.from_pretrained` is not a simpler option.\n\n            cache_dir: (`optional`) string:\n                Path to a directory in which a downloaded pre-trained model\n                configuration should be cached if the standard cache should not be used.\n\n            force_download: (`optional`) boolean, default False:\n                Force to (re-)download the model weights and configuration files and override the cached versions if they exists.\n\n            proxies: (`optional`) dict, default None:\n                A dictionary of proxy servers to use by protocol or endpoint, e.g.: {\'http\': \'foo.bar:3128\', \'http://hostname\': \'foo.bar:4012\'}.\n                The proxies are used on each request.\n\n            output_loading_info: (`optional`) boolean:\n                Set to ``True`` to also return a dictionnary containing missing keys, unexpected keys and error messages.\n\n            kwargs: (`optional`) Remaining dictionary of keyword arguments:\n                Can be used to update the configuration object (after it being loaded) and initiate the model. (e.g. ``output_attention=True``). Behave differently depending on whether a `config` is provided or automatically loaded:\n\n                - If a configuration is provided with ``config``, ``**kwargs`` will be directly passed to the underlying model\'s ``__init__`` method (we assume all relevant updates to the configuration have already been done)\n                - If a configuration is not provided, ``kwargs`` will be first passed to the configuration class initialization function (:func:`~transformers.PretrainedConfig.from_pretrained`). Each key of ``kwargs`` that corresponds to a configuration attribute will be used to override said attribute with the supplied ``kwargs`` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model\'s ``__init__`` function.\n\n        Examples::\n\n            model = AutoModelWithLMHead.from_pretrained(\'bert-base-uncased\')    # Download model and configuration from S3 and cache.\n            model = AutoModelWithLMHead.from_pretrained(\'./test/bert_model/\')  # E.g. model was saved using `save_pretrained(\'./test/saved_model/\')`\n            model = AutoModelWithLMHead.from_pretrained(\'bert-base-uncased\', output_attention=True)  # Update configuration during loading\n            assert model.config.output_attention == True\n            # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n            config = AutoConfig.from_json_file(\'./tf_model/bert_tf_model_config.json\')\n            model = AutoModelWithLMHead.from_pretrained(\'./tf_model/bert_tf_checkpoint.ckpt.index\', from_tf=True, config=config)\n\n        """"""\n        if \'distilbert\' in pretrained_model_name_or_path:\n            return DistilBertForMaskedLM.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n        elif \'roberta\' in pretrained_model_name_or_path:\n            return RobertaForMaskedLM.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n        elif \'bert\' in pretrained_model_name_or_path:\n            return BertForMaskedLM.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n        elif \'openai-gpt\' in pretrained_model_name_or_path:\n            return OpenAIGPTLMHeadModel.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n        elif \'gpt2\' in pretrained_model_name_or_path:\n            return GPT2LMHeadModel.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n        elif \'transfo-xl\' in pretrained_model_name_or_path:\n            return TransfoXLLMHeadModel.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n        elif \'xlnet\' in pretrained_model_name_or_path:\n            return XLNetLMHeadModel.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n        elif \'xlm\' in pretrained_model_name_or_path:\n            return XLMWithLMHeadModel.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n        elif \'ctrl\' in pretrained_model_name_or_path:\n            return CTRLLMHeadModel.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n        raise ValueError(""Unrecognized model identifier in {}. Should contains one of ""\n                         ""\'bert\', \'openai-gpt\', \'gpt2\', \'transfo-xl\', \'xlnet\', ""\n                         ""\'xlm\', \'roberta\',\'ctrl\'"".format(pretrained_model_name_or_path))\n\n\nclass AutoModelForSequenceClassification(object):\n    r""""""\n        :class:`~transformers.AutoModelForSequenceClassification` is a generic model class\n        that will be instantiated as one of the sequence classification model classes of the library\n        when created with the `AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path)`\n        class method.\n\n        The `from_pretrained()` method takes care of returning the correct model class instance\n        using pattern matching on the `pretrained_model_name_or_path` string.\n\n        The model class to instantiate is selected as the first pattern matching\n        in the `pretrained_model_name_or_path` string (in the following order):\n            - contains `distilbert`: DistilBertForSequenceClassification (DistilBERT model)\n            - contains `roberta`: RobertaForSequenceClassification (RoBERTa model)\n            - contains `bert`: BertForSequenceClassification (Bert model)\n            - contains `xlnet`: XLNetForSequenceClassification (XLNet model)\n            - contains `xlm`: XLMForSequenceClassification (XLM model)\n\n        This class cannot be instantiated using `__init__()` (throws an error).\n    """"""\n    def __init__(self):\n        raise EnvironmentError(""AutoModelWithLMHead is designed to be instantiated ""\n            ""using the `AutoModelWithLMHead.from_pretrained(pretrained_model_name_or_path)` method."")\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n        r"""""" Instantiates one of the sequence classification model classes of the library\n        from a pre-trained model configuration.\n\n        The `from_pretrained()` method takes care of returning the correct model class instance\n        using pattern matching on the `pretrained_model_name_or_path` string.\n\n        The model class to instantiate is selected as the first pattern matching\n        in the `pretrained_model_name_or_path` string (in the following order):\n            - contains `distilbert`: DistilBertForSequenceClassification (DistilBERT model)\n            - contains `roberta`: RobertaForSequenceClassification (RoBERTa model)\n            - contains `bert`: BertForSequenceClassification (Bert model)\n            - contains `xlnet`: XLNetForSequenceClassification (XLNet model)\n            - contains `xlm`: XLMForSequenceClassification (XLM model)\n\n        The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated)\n        To train the model, you should first set it back in training mode with `model.train()`\n\n        Params:\n            pretrained_model_name_or_path: either:\n\n                - a string with the `shortcut name` of a pre-trained model to load from cache or download, e.g.: ``bert-base-uncased``.\n                - a path to a `directory` containing model weights saved using :func:`~transformers.PreTrainedModel.save_pretrained`, e.g.: ``./my_model_directory/``.\n                - a path or url to a `tensorflow index checkpoint file` (e.g. `./tf_model/model.ckpt.index`). In this case, ``from_tf`` should be set to True and a configuration object should be provided as ``config`` argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n\n            model_args: (`optional`) Sequence of positional arguments:\n                All remaning positional arguments will be passed to the underlying model\'s ``__init__`` method\n\n            config: (`optional`) instance of a class derived from :class:`~transformers.PretrainedConfig`:\n                Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:\n\n                - the model is a model provided by the library (loaded with the ``shortcut-name`` string of a pretrained model), or\n                - the model was saved using :func:`~transformers.PreTrainedModel.save_pretrained` and is reloaded by suppling the save directory.\n                - the model is loaded by suppling a local directory as ``pretrained_model_name_or_path`` and a configuration JSON file named `config.json` is found in the directory.\n\n            state_dict: (`optional`) dict:\n                an optional state dictionnary for the model to use instead of a state dictionary loaded from saved weights file.\n                This option can be used if you want to create a model from a pretrained configuration but load your own weights.\n                In this case though, you should check if using :func:`~transformers.PreTrainedModel.save_pretrained` and :func:`~transformers.PreTrainedModel.from_pretrained` is not a simpler option.\n\n            cache_dir: (`optional`) string:\n                Path to a directory in which a downloaded pre-trained model\n                configuration should be cached if the standard cache should not be used.\n\n            force_download: (`optional`) boolean, default False:\n                Force to (re-)download the model weights and configuration files and override the cached versions if they exists.\n\n            proxies: (`optional`) dict, default None:\n                A dictionary of proxy servers to use by protocol or endpoint, e.g.: {\'http\': \'foo.bar:3128\', \'http://hostname\': \'foo.bar:4012\'}.\n                The proxies are used on each request.\n\n            output_loading_info: (`optional`) boolean:\n                Set to ``True`` to also return a dictionnary containing missing keys, unexpected keys and error messages.\n\n            kwargs: (`optional`) Remaining dictionary of keyword arguments:\n                Can be used to update the configuration object (after it being loaded) and initiate the model. (e.g. ``output_attention=True``). Behave differently depending on whether a `config` is provided or automatically loaded:\n\n                - If a configuration is provided with ``config``, ``**kwargs`` will be directly passed to the underlying model\'s ``__init__`` method (we assume all relevant updates to the configuration have already been done)\n                - If a configuration is not provided, ``kwargs`` will be first passed to the configuration class initialization function (:func:`~transformers.PretrainedConfig.from_pretrained`). Each key of ``kwargs`` that corresponds to a configuration attribute will be used to override said attribute with the supplied ``kwargs`` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model\'s ``__init__`` function.\n\n        Examples::\n\n            model = AutoModelForSequenceClassification.from_pretrained(\'bert-base-uncased\')    # Download model and configuration from S3 and cache.\n            model = AutoModelForSequenceClassification.from_pretrained(\'./test/bert_model/\')  # E.g. model was saved using `save_pretrained(\'./test/saved_model/\')`\n            model = AutoModelForSequenceClassification.from_pretrained(\'bert-base-uncased\', output_attention=True)  # Update configuration during loading\n            assert model.config.output_attention == True\n            # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n            config = AutoConfig.from_json_file(\'./tf_model/bert_tf_model_config.json\')\n            model = AutoModelForSequenceClassification.from_pretrained(\'./tf_model/bert_tf_checkpoint.ckpt.index\', from_tf=True, config=config)\n\n        """"""\n        if \'distilbert\' in pretrained_model_name_or_path:\n            return DistilBertForSequenceClassification.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n        elif \'roberta\' in pretrained_model_name_or_path:\n            return RobertaForSequenceClassification.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n        elif \'bert\' in pretrained_model_name_or_path:\n            return BertForSequenceClassification.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n        elif \'xlnet\' in pretrained_model_name_or_path:\n            return XLNetForSequenceClassification.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n        elif \'xlm\' in pretrained_model_name_or_path:\n            return XLMForSequenceClassification.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n\n        raise ValueError(""Unrecognized model identifier in {}. Should contains one of ""\n                         ""\'bert\', \'xlnet\', \'xlm\', \'roberta\'"".format(pretrained_model_name_or_path))\n\n\nclass AutoModelForQuestionAnswering(object):\n    r""""""\n        :class:`~transformers.AutoModelForQuestionAnswering` is a generic model class\n        that will be instantiated as one of the question answering model classes of the library\n        when created with the `AutoModelForQuestionAnswering.from_pretrained(pretrained_model_name_or_path)`\n        class method.\n\n        The `from_pretrained()` method takes care of returning the correct model class instance\n        using pattern matching on the `pretrained_model_name_or_path` string.\n\n        The model class to instantiate is selected as the first pattern matching\n        in the `pretrained_model_name_or_path` string (in the following order):\n            - contains `distilbert`: DistilBertForQuestionAnswering (DistilBERT model)\n            - contains `bert`: BertForQuestionAnswering (Bert model)\n            - contains `xlnet`: XLNetForQuestionAnswering (XLNet model)\n            - contains `xlm`: XLMForQuestionAnswering (XLM model)\n\n        This class cannot be instantiated using `__init__()` (throws an error).\n    """"""\n    def __init__(self):\n        raise EnvironmentError(""AutoModelWithLMHead is designed to be instantiated ""\n            ""using the `AutoModelWithLMHead.from_pretrained(pretrained_model_name_or_path)` method."")\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n        r"""""" Instantiates one of the question answering model classes of the library\n        from a pre-trained model configuration.\n\n        The `from_pretrained()` method takes care of returning the correct model class instance\n        using pattern matching on the `pretrained_model_name_or_path` string.\n\n        The model class to instantiate is selected as the first pattern matching\n        in the `pretrained_model_name_or_path` string (in the following order):\n            - contains `distilbert`: DistilBertForQuestionAnswering (DistilBERT model)\n            - contains `bert`: BertForQuestionAnswering (Bert model)\n            - contains `xlnet`: XLNetForQuestionAnswering (XLNet model)\n            - contains `xlm`: XLMForQuestionAnswering (XLM model)\n\n        The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated)\n        To train the model, you should first set it back in training mode with `model.train()`\n\n        Params:\n            pretrained_model_name_or_path: either:\n\n                - a string with the `shortcut name` of a pre-trained model to load from cache or download, e.g.: ``bert-base-uncased``.\n                - a path to a `directory` containing model weights saved using :func:`~transformers.PreTrainedModel.save_pretrained`, e.g.: ``./my_model_directory/``.\n                - a path or url to a `tensorflow index checkpoint file` (e.g. `./tf_model/model.ckpt.index`). In this case, ``from_tf`` should be set to True and a configuration object should be provided as ``config`` argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n\n            model_args: (`optional`) Sequence of positional arguments:\n                All remaning positional arguments will be passed to the underlying model\'s ``__init__`` method\n\n            config: (`optional`) instance of a class derived from :class:`~transformers.PretrainedConfig`:\n                Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:\n\n                - the model is a model provided by the library (loaded with the ``shortcut-name`` string of a pretrained model), or\n                - the model was saved using :func:`~transformers.PreTrainedModel.save_pretrained` and is reloaded by suppling the save directory.\n                - the model is loaded by suppling a local directory as ``pretrained_model_name_or_path`` and a configuration JSON file named `config.json` is found in the directory.\n\n            state_dict: (`optional`) dict:\n                an optional state dictionnary for the model to use instead of a state dictionary loaded from saved weights file.\n                This option can be used if you want to create a model from a pretrained configuration but load your own weights.\n                In this case though, you should check if using :func:`~transformers.PreTrainedModel.save_pretrained` and :func:`~transformers.PreTrainedModel.from_pretrained` is not a simpler option.\n\n            cache_dir: (`optional`) string:\n                Path to a directory in which a downloaded pre-trained model\n                configuration should be cached if the standard cache should not be used.\n\n            force_download: (`optional`) boolean, default False:\n                Force to (re-)download the model weights and configuration files and override the cached versions if they exists.\n\n            proxies: (`optional`) dict, default None:\n                A dictionary of proxy servers to use by protocol or endpoint, e.g.: {\'http\': \'foo.bar:3128\', \'http://hostname\': \'foo.bar:4012\'}.\n                The proxies are used on each request.\n\n            output_loading_info: (`optional`) boolean:\n                Set to ``True`` to also return a dictionnary containing missing keys, unexpected keys and error messages.\n\n            kwargs: (`optional`) Remaining dictionary of keyword arguments:\n                Can be used to update the configuration object (after it being loaded) and initiate the model. (e.g. ``output_attention=True``). Behave differently depending on whether a `config` is provided or automatically loaded:\n\n                - If a configuration is provided with ``config``, ``**kwargs`` will be directly passed to the underlying model\'s ``__init__`` method (we assume all relevant updates to the configuration have already been done)\n                - If a configuration is not provided, ``kwargs`` will be first passed to the configuration class initialization function (:func:`~transformers.PretrainedConfig.from_pretrained`). Each key of ``kwargs`` that corresponds to a configuration attribute will be used to override said attribute with the supplied ``kwargs`` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model\'s ``__init__`` function.\n\n        Examples::\n\n            model = AutoModelForQuestionAnswering.from_pretrained(\'bert-base-uncased\')    # Download model and configuration from S3 and cache.\n            model = AutoModelForQuestionAnswering.from_pretrained(\'./test/bert_model/\')  # E.g. model was saved using `save_pretrained(\'./test/saved_model/\')`\n            model = AutoModelForQuestionAnswering.from_pretrained(\'bert-base-uncased\', output_attention=True)  # Update configuration during loading\n            assert model.config.output_attention == True\n            # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n            config = AutoConfig.from_json_file(\'./tf_model/bert_tf_model_config.json\')\n            model = AutoModelForQuestionAnswering.from_pretrained(\'./tf_model/bert_tf_checkpoint.ckpt.index\', from_tf=True, config=config)\n\n        """"""\n        if \'distilbert\' in pretrained_model_name_or_path:\n            return DistilBertForQuestionAnswering.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n        elif \'bert\' in pretrained_model_name_or_path:\n            return BertForQuestionAnswering.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n        elif \'xlnet\' in pretrained_model_name_or_path:\n            return XLNetForQuestionAnswering.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n        elif \'xlm\' in pretrained_model_name_or_path:\n            return XLMForQuestionAnswering.from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n\n        raise ValueError(""Unrecognized model identifier in {}. Should contains one of ""\n                         ""\'bert\', \'xlnet\', \'xlm\'"".format(pretrained_model_name_or_path))\n'"
models/transformers/modeling_bert.py,81,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""PyTorch BERT model. """"""\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport json\nimport logging\nimport math\nimport os\nimport sys\nfrom io import open\n\nimport torch\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss, MSELoss\n\nfrom .modeling_utils import PreTrainedModel, prune_linear_layer\nfrom .configuration_bert import BertConfig\nfrom .file_utils import add_start_docstrings\n\nlogger = logging.getLogger(__name__)\n\nBERT_PRETRAINED_MODEL_ARCHIVE_MAP = {\n    \'bert-base-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin"",\n    \'bert-large-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin"",\n    \'bert-base-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin"",\n    \'bert-large-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-pytorch_model.bin"",\n    \'bert-base-multilingual-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-pytorch_model.bin"",\n    \'bert-base-multilingual-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-pytorch_model.bin"",\n    \'bert-base-chinese\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-pytorch_model.bin"",\n    \'bert-base-german-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-cased-pytorch_model.bin"",\n    \'bert-large-uncased-whole-word-masking\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-pytorch_model.bin"",\n    \'bert-large-cased-whole-word-masking\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-pytorch_model.bin"",\n    \'bert-large-uncased-whole-word-masking-finetuned-squad\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-pytorch_model.bin"",\n    \'bert-large-cased-whole-word-masking-finetuned-squad\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-finetuned-squad-pytorch_model.bin"",\n    \'bert-base-cased-finetuned-mrpc\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-mrpc-pytorch_model.bin"",\n    \'bert-base-german-dbmdz-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-pytorch_model.bin"",\n    \'bert-base-german-dbmdz-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-uncased-pytorch_model.bin"",\n}\n\ndef load_tf_weights_in_bert(model, config, tf_checkpoint_path):\n    """""" Load tf checkpoints in a pytorch model.\n    """"""\n    try:\n        import re\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        logger.error(""Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see ""\n            ""https://www.tensorflow.org/install/ for installation instructions."")\n        raise\n    tf_path = os.path.abspath(tf_checkpoint_path)\n    logger.info(""Converting TensorFlow checkpoint from {}"".format(tf_path))\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    names = []\n    arrays = []\n    for name, shape in init_vars:\n        logger.info(""Loading TF weight {} with shape {}"".format(name, shape))\n        array = tf.train.load_variable(tf_path, name)\n        names.append(name)\n        arrays.append(array)\n\n    for name, array in zip(names, arrays):\n        name = name.split(\'/\')\n        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n        # which are not required for using pretrained model\n        if any(n in [""adam_v"", ""adam_m"", ""global_step""] for n in name):\n            logger.info(""Skipping {}"".format(""/"".join(name)))\n            continue\n        pointer = model\n        for m_name in name:\n            if re.fullmatch(r\'[A-Za-z]+_\\d+\', m_name):\n                l = re.split(r\'_(\\d+)\', m_name)\n            else:\n                l = [m_name]\n            if l[0] == \'kernel\' or l[0] == \'gamma\':\n                pointer = getattr(pointer, \'weight\')\n            elif l[0] == \'output_bias\' or l[0] == \'beta\':\n                pointer = getattr(pointer, \'bias\')\n            elif l[0] == \'output_weights\':\n                pointer = getattr(pointer, \'weight\')\n            elif l[0] == \'squad\':\n                pointer = getattr(pointer, \'classifier\')\n            else:\n                try:\n                    pointer = getattr(pointer, l[0])\n                except AttributeError:\n                    logger.info(""Skipping {}"".format(""/"".join(name)))\n                    continue\n            if len(l) >= 2:\n                num = int(l[1])\n                pointer = pointer[num]\n        if m_name[-11:] == \'_embeddings\':\n            pointer = getattr(pointer, \'weight\')\n        elif m_name == \'kernel\':\n            array = np.transpose(array)\n        try:\n            assert pointer.shape == array.shape\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        logger.info(""Initialize PyTorch weight {}"".format(name))\n        pointer.data = torch.from_numpy(array)\n    return model\n\n\ndef gelu(x):\n    """""" Original Implementation of the gelu activation function in Google Bert repo when initially created.\n        For information: OpenAI GPT\'s gelu is slightly different (and gives slightly different results):\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n        Also see https://arxiv.org/abs/1606.08415\n    """"""\n    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n\ndef gelu_new(x):\n    """""" Implementation of the gelu activation function currently in Google Bert repo (identical to OpenAI GPT).\n        Also see https://arxiv.org/abs/1606.08415\n    """"""\n    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n\ndef swish(x):\n    return x * torch.sigmoid(x)\n\n\nACT2FN = {""gelu"": gelu, ""relu"": torch.nn.functional.relu, ""swish"": swish, ""gelu_new"": gelu_new}\n\n\nBertLayerNorm = torch.nn.LayerNorm\n\nclass BertEmbeddings(nn.Module):\n    """"""Construct the embeddings from word, position and token_type embeddings.\n    """"""\n    def __init__(self, config):\n        super(BertEmbeddings, self).__init__()\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n\n        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n        # any TensorFlow checkpoint file\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, input_ids, token_type_ids=None, position_ids=None):\n        seq_length = input_ids.size(1)\n        if position_ids is None:\n            position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        words_embeddings = self.word_embeddings(input_ids)\n        position_embeddings = self.position_embeddings(position_ids)\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n\n        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings\n\n\nclass BertSelfAttention(nn.Module):\n    def __init__(self, config):\n        super(BertSelfAttention, self).__init__()\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                ""The hidden size (%d) is not a multiple of the number of attention ""\n                ""heads (%d)"" % (config.hidden_size, config.num_attention_heads))\n        self.output_attentions = config.output_attentions\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(self, hidden_states, attention_mask=None, head_mask=None):\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n\n        # Take the dot product between ""query"" and ""key"" to get the raw attention scores.\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n            attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout(attention_probs)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attention_probs = attention_probs * head_mask\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n\n        outputs = (context_layer, attention_probs) if self.output_attentions else (context_layer,)\n        return outputs\n\n\nclass BertSelfOutput(nn.Module):\n    def __init__(self, config):\n        super(BertSelfOutput, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass BertAttention(nn.Module):\n    def __init__(self, config):\n        super(BertAttention, self).__init__()\n        self.self = BertSelfAttention(config)\n        self.output = BertSelfOutput(config)\n        self.pruned_heads = set()\n\n    def prune_heads(self, heads):\n        if len(heads) == 0:\n            return\n        mask = torch.ones(self.self.num_attention_heads, self.self.attention_head_size)\n        heads = set(heads) - self.pruned_heads  # Convert to set and emove already pruned heads\n        for head in heads:\n            # Compute how many pruned heads are before the head and move the index accordingly\n            head = head - sum(1 if h < head else 0 for h in self.pruned_heads)\n            mask[head] = 0\n        mask = mask.view(-1).contiguous().eq(1)\n        index = torch.arange(len(mask))[mask].long()\n\n        # Prune linear layers\n        self.self.query = prune_linear_layer(self.self.query, index)\n        self.self.key = prune_linear_layer(self.self.key, index)\n        self.self.value = prune_linear_layer(self.self.value, index)\n        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n\n        # Update hyper params and store pruned heads\n        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n        self.pruned_heads = self.pruned_heads.union(heads)\n\n    def forward(self, input_tensor, attention_mask=None, head_mask=None):\n        self_outputs = self.self(input_tensor, attention_mask, head_mask)\n        attention_output = self.output(self_outputs[0], input_tensor)\n        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n        return outputs\n\n\nclass BertIntermediate(nn.Module):\n    def __init__(self, config):\n        super(BertIntermediate, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.intermediate_act_fn = config.hidden_act\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.intermediate_act_fn(hidden_states)\n        return hidden_states\n\n\nclass BertOutput(nn.Module):\n    def __init__(self, config):\n        super(BertOutput, self).__init__()\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass BertLayer(nn.Module):\n    def __init__(self, config):\n        super(BertLayer, self).__init__()\n        self.attention = BertAttention(config)\n        self.intermediate = BertIntermediate(config)\n        self.output = BertOutput(config)\n\n    def forward(self, hidden_states, attention_mask=None, head_mask=None):\n        attention_outputs = self.attention(hidden_states, attention_mask, head_mask)\n        attention_output = attention_outputs[0]\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        outputs = (layer_output,) + attention_outputs[1:]  # add attentions if we output them\n        return outputs\n\n\nclass BertEncoder(nn.Module):\n    def __init__(self, config):\n        super(BertEncoder, self).__init__()\n        self.output_attentions = config.output_attentions\n        self.output_hidden_states = config.output_hidden_states\n        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n\n    def forward(self, hidden_states, attention_mask=None, head_mask=None):\n        all_hidden_states = ()\n        all_attentions = ()\n        for i, layer_module in enumerate(self.layer):\n            if self.output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states,)\n\n            layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i])\n            hidden_states = layer_outputs[0]\n\n            if self.output_attentions:\n                all_attentions = all_attentions + (layer_outputs[1],)\n\n        # Add last layer\n        if self.output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_states,)\n\n        outputs = (hidden_states,)\n        if self.output_hidden_states:\n            outputs = outputs + (all_hidden_states,)\n        if self.output_attentions:\n            outputs = outputs + (all_attentions,)\n        return outputs  # last-layer hidden state, (all hidden states), (all attentions)\n\n\nclass BertPooler(nn.Module):\n    def __init__(self, config):\n        super(BertPooler, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.activation = nn.Tanh()\n\n    def forward(self, hidden_states):\n        # We ""pool"" the model by simply taking the hidden state corresponding\n        # to the first token.\n        first_token_tensor = hidden_states[:, 0]\n        pooled_output = self.dense(first_token_tensor)\n        pooled_output = self.activation(pooled_output)\n        return pooled_output\n\n\nclass BertPredictionHeadTransform(nn.Module):\n    def __init__(self, config):\n        super(BertPredictionHeadTransform, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n            self.transform_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.transform_act_fn = config.hidden_act\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.transform_act_fn(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states)\n        return hidden_states\n\n\nclass BertLMPredictionHead(nn.Module):\n    def __init__(self, config):\n        super(BertLMPredictionHead, self).__init__()\n        self.transform = BertPredictionHeadTransform(config)\n\n        # The output weights are the same as the input embeddings, but there is\n        # an output-only bias for each token.\n        self.decoder = nn.Linear(config.hidden_size,\n                                 config.vocab_size,\n                                 bias=False)\n\n        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n\n    def forward(self, hidden_states):\n        hidden_states = self.transform(hidden_states)\n        hidden_states = self.decoder(hidden_states) + self.bias\n        return hidden_states\n\n\nclass BertOnlyMLMHead(nn.Module):\n    def __init__(self, config):\n        super(BertOnlyMLMHead, self).__init__()\n        self.predictions = BertLMPredictionHead(config)\n\n    def forward(self, sequence_output):\n        prediction_scores = self.predictions(sequence_output)\n        return prediction_scores\n\n\nclass BertOnlyNSPHead(nn.Module):\n    def __init__(self, config):\n        super(BertOnlyNSPHead, self).__init__()\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n\n    def forward(self, pooled_output):\n        seq_relationship_score = self.seq_relationship(pooled_output)\n        return seq_relationship_score\n\n\nclass BertPreTrainingHeads(nn.Module):\n    def __init__(self, config):\n        super(BertPreTrainingHeads, self).__init__()\n        self.predictions = BertLMPredictionHead(config)\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n\n    def forward(self, sequence_output, pooled_output):\n        prediction_scores = self.predictions(sequence_output)\n        seq_relationship_score = self.seq_relationship(pooled_output)\n        return prediction_scores, seq_relationship_score\n\n\nclass BertPreTrainedModel(PreTrainedModel):\n    """""" An abstract class to handle weights initialization and\n        a simple interface for dowloading and loading pretrained models.\n    """"""\n    config_class = BertConfig\n    pretrained_model_archive_map = BERT_PRETRAINED_MODEL_ARCHIVE_MAP\n    load_tf_weights = load_tf_weights_in_bert\n    base_model_prefix = ""bert""\n\n    def _init_weights(self, module):\n        """""" Initialize the weights """"""\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n        elif isinstance(module, BertLayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n\n\nBERT_START_DOCSTRING = r""""""    The BERT model was proposed in\n    `BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`_\n    by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It\'s a bidirectional transformer\n    pre-trained using a combination of masked language modeling objective and next sentence prediction\n    on a large corpus comprising the Toronto Book Corpus and Wikipedia.\n\n    This model is a PyTorch `torch.nn.Module`_ sub-class. Use it as a regular PyTorch Module and\n    refer to the PyTorch documentation for all matter related to general usage and behavior.\n\n    .. _`BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`:\n        https://arxiv.org/abs/1810.04805\n\n    .. _`torch.nn.Module`:\n        https://pytorch.org/docs/stable/nn.html#module\n\n    Parameters:\n        config (:class:`~transformers.BertConfig`): Model configuration class with all the parameters of the model. \n            Initializing with a config file does not load the weights associated with the model, only the configuration.\n            Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model weights.\n""""""\n\nBERT_INPUTS_DOCSTRING = r""""""\n    Inputs:\n        **input_ids**: ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Indices of input sequence tokens in the vocabulary.\n            To match pre-training, BERT input sequence should be formatted with [CLS] and [SEP] tokens as follows:\n\n            (a) For sequence pairs:\n\n                ``tokens:         [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]``\n                \n                ``token_type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1``\n\n            (b) For single sequences:\n\n                ``tokens:         [CLS] the dog is hairy . [SEP]``\n                \n                ``token_type_ids:   0   0   0   0  0     0   0``\n\n            Bert is a model with absolute position embeddings so it\'s usually advised to pad the inputs on\n            the right rather than the left.\n\n            Indices can be obtained using :class:`transformers.BertTokenizer`.\n            See :func:`transformers.PreTrainedTokenizer.encode` and\n            :func:`transformers.PreTrainedTokenizer.convert_tokens_to_ids` for details.\n        **attention_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, sequence_length)``:\n            Mask to avoid performing attention on padding token indices.\n            Mask values selected in ``[0, 1]``:\n            ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n        **token_type_ids**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Segment token indices to indicate first and second portions of the inputs.\n            Indices are selected in ``[0, 1]``: ``0`` corresponds to a `sentence A` token, ``1``\n            corresponds to a `sentence B` token\n            (see `BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`_ for more details).\n        **position_ids**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Indices of positions of each input sequence tokens in the position embeddings.\n            Selected in the range ``[0, config.max_position_embeddings - 1]``.\n        **head_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(num_heads,)`` or ``(num_layers, num_heads)``:\n            Mask to nullify selected heads of the self-attention modules.\n            Mask values selected in ``[0, 1]``:\n            ``1`` indicates the head is **not masked**, ``0`` indicates the head is **masked**.\n""""""\n\n@add_start_docstrings(""The bare Bert Model transformer outputting raw hidden-states without any specific head on top."",\n                      BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\nclass BertModel(BertPreTrainedModel):\n    r""""""\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **last_hidden_state**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, hidden_size)``\n            Sequence of hidden-states at the output of the last layer of the model.\n        **pooler_output**: ``torch.FloatTensor`` of shape ``(batch_size, hidden_size)``\n            Last layer hidden-state of the first token of the sequence (classification token)\n            further processed by a Linear layer and a Tanh activation function. The Linear\n            layer weights are trained from the next sentence prediction (classification)\n            objective during Bert pretraining. This output is usually *not* a good summary\n            of the semantic content of the input, you\'re often better with averaging or pooling\n            the sequence of hidden-states for the whole input sequence.\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n        model = BertModel.from_pretrained(\'bert-base-uncased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids)\n        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n\n    """"""\n    def __init__(self, config):\n        super(BertModel, self).__init__(config)\n\n        self.embeddings = BertEmbeddings(config)\n        self.encoder = BertEncoder(config)\n        self.pooler = BertPooler(config)\n\n        self.init_weights()\n\n    def _resize_token_embeddings(self, new_num_tokens):\n        old_embeddings = self.embeddings.word_embeddings\n        new_embeddings = self._get_resized_embeddings(old_embeddings, new_num_tokens)\n        self.embeddings.word_embeddings = new_embeddings\n        return self.embeddings.word_embeddings\n\n    def _prune_heads(self, heads_to_prune):\n        """""" Prunes heads of the model.\n            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n            See base class PreTrainedModel\n        """"""\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n        if attention_mask is None:\n            attention_mask = torch.ones_like(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        # We create a 3D attention mask from a 2D tensor mask.\n        # Sizes are [batch_size, 1, 1, to_seq_length]\n        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n        # this attention mask is more simple than the triangular masking of causal attention\n        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        if head_mask is not None:\n            if head_mask.dim() == 1:\n                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n                head_mask = head_mask.expand(self.config.num_hidden_layers, -1, -1, -1, -1)\n            elif head_mask.dim() == 2:\n                head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)  # We can specify head_mask for each layer\n            head_mask = head_mask.to(dtype=next(self.parameters()).dtype) # switch to fload if need + fp16 compatibility\n        else:\n            head_mask = [None] * self.config.num_hidden_layers\n\n        embedding_output = self.embeddings(input_ids, position_ids=position_ids, token_type_ids=token_type_ids)\n        encoder_outputs = self.encoder(embedding_output,\n                                       extended_attention_mask,\n                                       head_mask=head_mask)\n        sequence_output = encoder_outputs[0]\n        pooled_output = self.pooler(sequence_output)\n\n        outputs = (sequence_output, pooled_output,) + encoder_outputs[1:]  # add hidden_states and attentions if they are here\n        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\n\n\n@add_start_docstrings(""""""Bert Model with two heads on top as done during the pre-training:\n    a `masked language modeling` head and a `next sentence prediction (classification)` head. """""",\n    BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\nclass BertForPreTraining(BertPreTrainedModel):\n    r""""""\n        **masked_lm_labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Labels for computing the masked language modeling loss.\n            Indices should be in ``[-1, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring)\n            Tokens with indices set to ``-1`` are ignored (masked), the loss is only computed for the tokens with labels\n            in ``[0, ..., config.vocab_size]``\n        **next_sentence_label**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair (see ``input_ids`` docstring)\n            Indices should be in ``[0, 1]``.\n            ``0`` indicates sequence B is a continuation of sequence A,\n            ``1`` indicates sequence B is a random sequence.\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when both ``masked_lm_labels`` and ``next_sentence_label`` are provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Total loss as the sum of the masked language modeling loss and the next sequence prediction (classification) loss.\n        **prediction_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.vocab_size)``\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n        **seq_relationship_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, 2)``\n            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n        model = BertForPreTraining.from_pretrained(\'bert-base-uncased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids)\n        prediction_scores, seq_relationship_scores = outputs[:2]\n\n    """"""\n    def __init__(self, config):\n        super(BertForPreTraining, self).__init__(config)\n\n        self.bert = BertModel(config)\n        self.cls = BertPreTrainingHeads(config)\n\n        self.init_weights()\n        self.tie_weights()\n\n    def tie_weights(self):\n        """""" Make sure we are sharing the input and output embeddings.\n            Export to TorchScript can\'t handle parameter sharing so we are cloning them instead.\n        """"""\n        self._tie_or_clone_weights(self.cls.predictions.decoder,\n                                   self.bert.embeddings.word_embeddings)\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n                masked_lm_labels=None, next_sentence_label=None):\n\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids, \n                            head_mask=head_mask)\n\n        sequence_output, pooled_output = outputs[:2]\n        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n\n        outputs = (prediction_scores, seq_relationship_score,) + outputs[2:]  # add hidden states and attention if they are here\n\n        if masked_lm_labels is not None and next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n            total_loss = masked_lm_loss + next_sentence_loss\n            outputs = (total_loss,) + outputs\n\n        return outputs  # (loss), prediction_scores, seq_relationship_score, (hidden_states), (attentions)\n\n\n@add_start_docstrings(""""""Bert Model with a `language modeling` head on top. """""",\n    BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\nclass BertForMaskedLM(BertPreTrainedModel):\n    r""""""\n        **masked_lm_labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Labels for computing the masked language modeling loss.\n            Indices should be in ``[-1, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring)\n            Tokens with indices set to ``-1`` are ignored (masked), the loss is only computed for the tokens with labels\n            in ``[0, ..., config.vocab_size]``\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``masked_lm_labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Masked language modeling loss.\n        **prediction_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.vocab_size)``\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n        model = BertForMaskedLM.from_pretrained(\'bert-base-uncased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, masked_lm_labels=input_ids)\n        loss, prediction_scores = outputs[:2]\n\n    """"""\n    def __init__(self, config):\n        super(BertForMaskedLM, self).__init__(config)\n\n        self.bert = BertModel(config)\n        self.cls = BertOnlyMLMHead(config)\n\n        self.init_weights()\n        self.tie_weights()\n\n    def tie_weights(self):\n        """""" Make sure we are sharing the input and output embeddings.\n            Export to TorchScript can\'t handle parameter sharing so we are cloning them instead.\n        """"""\n        self._tie_or_clone_weights(self.cls.predictions.decoder,\n                                   self.bert.embeddings.word_embeddings)\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n                masked_lm_labels=None):\n\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids, \n                            head_mask=head_mask)\n\n        sequence_output = outputs[0]\n        prediction_scores = self.cls(sequence_output)\n\n        outputs = (prediction_scores,) + outputs[2:]  # Add hidden states and attention if they are here\n        if masked_lm_labels is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n            outputs = (masked_lm_loss,) + outputs\n\n        return outputs  # (masked_lm_loss), prediction_scores, (hidden_states), (attentions)\n\n\n@add_start_docstrings(""""""Bert Model with a `next sentence prediction (classification)` head on top. """""",\n    BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\nclass BertForNextSentencePrediction(BertPreTrainedModel):\n    r""""""\n        **next_sentence_label**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair (see ``input_ids`` docstring)\n            Indices should be in ``[0, 1]``.\n            ``0`` indicates sequence B is a continuation of sequence A,\n            ``1`` indicates sequence B is a random sequence.\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``next_sentence_label`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Next sequence prediction (classification) loss.\n        **seq_relationship_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, 2)``\n            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n        model = BertForNextSentencePrediction.from_pretrained(\'bert-base-uncased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids)\n        seq_relationship_scores = outputs[0]\n\n    """"""\n    def __init__(self, config):\n        super(BertForNextSentencePrediction, self).__init__(config)\n\n        self.bert = BertModel(config)\n        self.cls = BertOnlyNSPHead(config)\n\n        self.init_weights()\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n                next_sentence_label=None):\n\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids, \n                            head_mask=head_mask)\n\n        pooled_output = outputs[1]\n\n        seq_relationship_score = self.cls(pooled_output)\n\n        outputs = (seq_relationship_score,) + outputs[2:]  # add hidden states and attention if they are here\n        if next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n            outputs = (next_sentence_loss,) + outputs\n\n        return outputs  # (next_sentence_loss), seq_relationship_score, (hidden_states), (attentions)\n\n\n@add_start_docstrings(""""""Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of\n    the pooled output) e.g. for GLUE tasks. """""",\n    BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\nclass BertForSequenceClassification(BertPreTrainedModel):\n    r""""""\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the sequence classification/regression loss.\n            Indices should be in ``[0, ..., config.num_labels - 1]``.\n            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Classification (or regression if config.num_labels==1) loss.\n        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n        model = BertForSequenceClassification.from_pretrained(\'bert-base-uncased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, labels=labels)\n        loss, logits = outputs[:2]\n\n    """"""\n    def __init__(self, config):\n        super(BertForSequenceClassification, self).__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n\n        self.init_weights()\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None,\n                position_ids=None, head_mask=None, labels=None):\n\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids, \n                            head_mask=head_mask)\n\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n\n        if labels is not None:\n            if self.num_labels == 1:\n                #  We are doing regression\n                loss_fct = MSELoss()\n                loss = loss_fct(logits.view(-1), labels.view(-1))\n            else:\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            outputs = (loss,) + outputs\n\n        return outputs  # (loss), logits, (hidden_states), (attentions)\n\n\n@add_start_docstrings(""""""Bert Model with a multiple choice classification head on top (a linear layer on top of\n    the pooled output and a softmax) e.g. for RocStories/SWAG tasks. """""",\n    BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\nclass BertForMultipleChoice(BertPreTrainedModel):\n    r""""""\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the multiple choice classification loss.\n            Indices should be in ``[0, ..., num_choices]`` where `num_choices` is the size of the second dimension\n            of the input tensors. (see `input_ids` above)\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Classification loss.\n        **classification_scores**: ``torch.FloatTensor`` of shape ``(batch_size, num_choices)`` where `num_choices` is the size of the second dimension\n            of the input tensors. (see `input_ids` above).\n            Classification scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n        model = BertForMultipleChoice.from_pretrained(\'bert-base-uncased\')\n        choices = [""Hello, my dog is cute"", ""Hello, my cat is amazing""]\n        input_ids = torch.tensor([tokenizer.encode(s) for s in choices]).unsqueeze(0)  # Batch size 1, 2 choices\n        labels = torch.tensor(1).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, labels=labels)\n        loss, classification_scores = outputs[:2]\n\n    """"""\n    def __init__(self, config):\n        super(BertForMultipleChoice, self).__init__(config)\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n\n        self.init_weights()\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None,\n                position_ids=None, head_mask=None, labels=None):\n        num_choices = input_ids.shape[1]\n\n        input_ids = input_ids.view(-1, input_ids.size(-1))\n        attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n        position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids,\n                            head_mask=head_mask)\n\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        reshaped_logits = logits.view(-1, num_choices)\n\n        outputs = (reshaped_logits,) + outputs[2:]  # add hidden states and attention if they are here\n\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(reshaped_logits, labels)\n            outputs = (loss,) + outputs\n\n        return outputs  # (loss), reshaped_logits, (hidden_states), (attentions)\n\n\n@add_start_docstrings(""""""Bert Model with a token classification head on top (a linear layer on top of\n    the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. """""",\n    BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\nclass BertForTokenClassification(BertPreTrainedModel):\n    r""""""\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Labels for computing the token classification loss.\n            Indices should be in ``[0, ..., config.num_labels - 1]``.\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Classification loss.\n        **scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.num_labels)``\n            Classification scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n        model = BertForTokenClassification.from_pretrained(\'bert-base-uncased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        labels = torch.tensor([1] * input_ids.size(1)).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, labels=labels)\n        loss, scores = outputs[:2]\n\n    """"""\n    def __init__(self, config):\n        super(BertForTokenClassification, self).__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None,\n                position_ids=None, head_mask=None, labels=None):\n\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids, \n                            head_mask=head_mask)\n\n        sequence_output = outputs[0]\n\n        sequence_output = self.dropout(sequence_output)\n        logits = self.classifier(sequence_output)\n\n        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            # Only keep active parts of the loss\n            if attention_mask is not None:\n                active_loss = attention_mask.view(-1) == 1\n                active_logits = logits.view(-1, self.num_labels)[active_loss]\n                active_labels = labels.view(-1)[active_loss]\n                loss = loss_fct(active_logits, active_labels)\n            else:\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            outputs = (loss,) + outputs\n\n        return outputs  # (loss), scores, (hidden_states), (attentions)\n\n\n@add_start_docstrings(""""""Bert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear layers on top of\n    the hidden-states output to compute `span start logits` and `span end logits`). """""",\n    BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\nclass BertForQuestionAnswering(BertPreTrainedModel):\n    r""""""\n        **start_positions**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`).\n            Position outside of the sequence are not taken into account for computing the loss.\n        **end_positions**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`).\n            Position outside of the sequence are not taken into account for computing the loss.\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.\n        **start_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length,)``\n            Span-start scores (before SoftMax).\n        **end_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length,)``\n            Span-end scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n        model = BertForQuestionAnswering.from_pretrained(\'bert-base-uncased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        start_positions = torch.tensor([1])\n        end_positions = torch.tensor([3])\n        outputs = model(input_ids, start_positions=start_positions, end_positions=end_positions)\n        loss, start_scores, end_scores = outputs[:2]\n\n    """"""\n    def __init__(self, config):\n        super(BertForQuestionAnswering, self).__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config)\n        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n                start_positions=None, end_positions=None):\n\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids, \n                            head_mask=head_mask)\n\n        sequence_output = outputs[0]\n\n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        outputs = (start_logits, end_logits,) + outputs[2:]\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions.clamp_(0, ignored_index)\n            end_positions.clamp_(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n            outputs = (total_loss,) + outputs\n\n        return outputs  # (loss), start_logits, end_logits, (hidden_states), (attentions)\n'"
models/transformers/modeling_ctrl.py,52,"b'# coding=utf-8\n# Copyright 2018 Salesforce and HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" PyTorch CTRL model.""""""\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport collections\nimport json\nimport logging\nimport math\nimport os\nimport sys\nfrom io import open\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\nfrom torch.nn.parameter import Parameter\n\nfrom .modeling_utils import PreTrainedModel, Conv1D, prune_conv1d_layer, SequenceSummary\nfrom .configuration_ctrl import CTRLConfig\nfrom .file_utils import add_start_docstrings\n\nlogger = logging.getLogger(__name__)\n\nCTRL_PRETRAINED_MODEL_ARCHIVE_MAP = {""ctrl"": ""https://storage.googleapis.com/sf-ctrl/pytorch/seqlen256_v1.bin""}\n\n\ndef angle_defn(pos, i, d_model_size):\n    angle_rates = 1 / torch.pow(10000, (2 * (i//2)) / d_model_size)\n    return pos * angle_rates\n\ndef positional_encoding(position, d_model_size, dtype):\n    # create the sinusoidal pattern for the positional encoding\n    angle_rads = (angle_defn(torch.arange(position, dtype=dtype).unsqueeze(1),\n                  torch.arange(d_model_size, dtype=dtype).unsqueeze(0),\n                  d_model_size))\n\n    sines = torch.sin(angle_rads[:, 0::2])\n    cosines = torch.cos(angle_rads[:, 1::2])\n\n    pos_encoding = torch.cat([sines, cosines], dim=-1)\n    return pos_encoding\n\ndef scaled_dot_product_attention(q, k, v, mask, attention_mask=None, head_mask=None):\n    # calculate attention\n    matmul_qk = torch.matmul(q, k.permute(0,1,3,2))\n\n    dk = k.shape[-1]\n    scaled_attention_logits = matmul_qk / np.sqrt(dk)\n\n    if mask is not None:\n        scaled_attention_logits += (mask * -1e4)\n\n    if attention_mask is not None:\n        # Apply the attention mask\n        scaled_attention_logits = scaled_attention_logits + attention_mask\n\n    attention_weights = torch.softmax(scaled_attention_logits, dim=-1) \n\n    # Mask heads if we want to\n    if head_mask is not None:\n        attention_weights = attention_weights * head_mask\n\n    output = torch.matmul(attention_weights, v)\n\n    return output, attention_weights\n\n\nclass MultiHeadAttention(torch.nn.Module):\n    def __init__(self, d_model_size, num_heads, output_attentions=False):\n        super(MultiHeadAttention, self).__init__()\n        self.output_attentions = output_attentions\n        self.num_heads = num_heads\n        self.d_model_size = d_model_size\n\n        self.depth = int(d_model_size / self.num_heads)\n\n        self.Wq = torch.nn.Linear(d_model_size, d_model_size)\n        self.Wk = torch.nn.Linear(d_model_size, d_model_size)\n        self.Wv = torch.nn.Linear(d_model_size, d_model_size)\n\n        self.dense = torch.nn.Linear(d_model_size, d_model_size)\n\n    def split_into_heads(self, x, batch_size):\n        x = x.reshape(batch_size, -1, self.num_heads, self.depth)\n        return x.permute([0, 2, 1, 3])\n\n    def forward(self, v, k, q, mask, layer_past=None, attention_mask=None, head_mask=None):\n        batch_size = q.shape[0]\n\n        q = self.Wq(q)\n        k = self.Wk(k)\n        v = self.Wv(v)\n\n        q = self.split_into_heads(q, batch_size)\n        k = self.split_into_heads(k, batch_size)\n        v = self.split_into_heads(v, batch_size)\n        if layer_past is not None:\n            past_key, past_value = layer_past[0], layer_past[1]\n            k = torch.cat((past_key, k), dim=-2)\n            v = torch.cat((past_value, v), dim=-2)\n        present = torch.stack((k, v))\n\n        output = scaled_dot_product_attention(q, k, v, mask, attention_mask, head_mask)\n        scaled_attention = output[0].permute([0, 2, 1, 3])\n        attn = output[1]\n        original_size_attention = scaled_attention.reshape(batch_size, -1, self.d_model_size)\n        output = self.dense(original_size_attention)\n\n        outputs = (output, present)\n        if self.output_attentions:\n            outputs = outputs + (attn,)\n        return outputs\n\n\n\ndef point_wise_feed_forward_network(d_model_size, dff):\n    return torch.nn.Sequential(torch.nn.Linear(d_model_size, dff),\n                               torch.nn.ReLU(),\n                               torch.nn.Linear(dff, d_model_size))\n\n\nclass EncoderLayer(torch.nn.Module):\n    def __init__(self, d_model_size, num_heads, dff, rate=0.1, output_attentions=False):\n        super(EncoderLayer, self).__init__()\n\n        self.multi_head_attention = MultiHeadAttention(d_model_size, num_heads, output_attentions)\n        self.ffn = point_wise_feed_forward_network(d_model_size, dff)\n\n        self.layernorm1 = torch.nn.LayerNorm(d_model_size, eps=1e-6)\n        self.layernorm2 = torch.nn.LayerNorm(d_model_size, eps=1e-6)\n\n        self.dropout1 = torch.nn.Dropout(rate)\n        self.dropout2 = torch.nn.Dropout(rate)\n\n    def forward(self, x, mask, layer_past=None, attention_mask=None, head_mask=None):\n        normed = self.layernorm1(x)\n        attn_outputs = self.multi_head_attention(normed, normed, normed, mask,\n                                                      layer_past=layer_past,\n                                                      attention_mask=attention_mask,\n                                                      head_mask=head_mask)\n        attn_output = attn_outputs[0]\n        attn_output = self.dropout1(attn_output)\n        out1 = x + attn_output\n\n        out2 = self.layernorm2(out1)\n        ffn_output = self.ffn(out2)\n        ffn_output = self.dropout2(ffn_output)\n        out2 = out1 + ffn_output\n\n        outputs = (out2,) + attn_outputs[1:]\n        return outputs\n\n\nclass CTRLPreTrainedModel(PreTrainedModel):\n    """""" An abstract class to handle weights initialization and\n        a simple interface for dowloading and loading pretrained models.\n    """"""\n    config_class = CTRLConfig\n    pretrained_model_archive_map = CTRL_PRETRAINED_MODEL_ARCHIVE_MAP\n    base_model_prefix = ""transformer""\n\n    def _init_weights(self, module):\n        """""" Initialize the weights.\n        """"""\n        if isinstance(module, (nn.Linear, nn.Embedding, Conv1D)):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if isinstance(module, (nn.Linear, Conv1D)) and module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n\nCTRL_START_DOCSTRING = r""""""    CTRL model was proposed in \n    `CTRL: A Conditional Transformer Language Model for Controllable Generation`_\n    by Nitish Shirish Keskar*, Bryan McCann*, Lav R. Varshney, Caiming Xiong and Richard Socher.\n    It\'s a causal (unidirectional) transformer pre-trained using language modeling on a very large\n    corpus of ~140 GB of text data with the first token reserved as a control code (such as Links, Books, Wikipedia etc.).\n\n    This model is a PyTorch `torch.nn.Module`_ sub-class. Use it as a regular PyTorch Module and\n    refer to the PyTorch documentation for all matter related to general usage and behavior.\n\n    .. _`CTRL: A Conditional Transformer Language Model for Controllable Generation`:\n        https://www.github.com/salesforce/ctrl\n\n    .. _`torch.nn.Module`:\n        https://pytorch.org/docs/stable/nn.html#module\n\n    Parameters:\n        config (:class:`~transformers.CTRLConfig`): Model configuration class with all the parameters of the model.\n            Initializing with a config file does not load the weights associated with the model, only the configuration.\n            Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model weights.\n""""""\n\nCTRL_INPUTS_DOCSTRING = r""""""    Inputs:\n        **input_ids**: ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Indices of input sequence tokens in the vocabulary.\n            CTRL is a model with absolute position embeddings so it\'s usually advised to pad the inputs on\n            the right rather than the left.\n            Indices can be obtained using :class:`transformers.CTRLTokenizer`.\n            See :func:`transformers.PreTrainedTokenizer.encode` and\n            :func:`transformers.PreTrainedTokenizer.convert_tokens_to_ids` for details.\n        **past**:\n            list of ``torch.FloatTensor`` (one for each layer):\n            that contains pre-computed hidden-states (key and values in the attention blocks) as computed by the model\n            (see `past` output below). Can be used to speed up sequential decoding.\n        **attention_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, sequence_length)``:\n            Mask to avoid performing attention on padding token indices.\n            Mask values selected in ``[0, 1]``:\n            ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n        **token_type_ids**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            A parallel sequence of tokens (can be used to indicate various portions of the inputs).\n            The embeddings from these tokens will be summed with the respective token embeddings.\n            Indices are selected in the vocabulary (unlike BERT which has a specific vocabulary for segment indices).\n        **position_ids**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Indices of positions of each input sequence tokens in the position embeddings.\n            Selected in the range ``[0, config.max_position_embeddings - 1]``.\n        **head_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(num_heads,)`` or ``(num_layers, num_heads)``:\n            Mask to nullify selected heads of the self-attention modules.\n            Mask values selected in ``[0, 1]``:\n            ``1`` indicates the head is **not masked**, ``0`` indicates the head is **masked**.\n""""""\n\n@add_start_docstrings(""The bare CTRL Model transformer outputting raw hidden-states without any specific head on top."",\n                                            CTRL_START_DOCSTRING, CTRL_INPUTS_DOCSTRING)\nclass CTRLModel(CTRLPreTrainedModel):\n    r""""""\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **last_hidden_state**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, hidden_size)``\n            Sequence of hidden-states at the last layer of the model.\n        **past**:\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            that contains pre-computed hidden-states (key and values in the attention blocks).\n            Can be used (see `past` input) to speed up sequential decoding.\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = CTRLTokenizer.from_pretrained(\'ctrl\')\n        model = CTRLModel.from_pretrained(\'ctrl\')\n        input_ids = torch.tensor(tokenizer.encode(""Links Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids)\n        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n\n    """"""\n    def __init__(self, config):\n        super(CTRLModel, self).__init__(config)\n        self.output_hidden_states = config.output_hidden_states\n        self.output_attentions = config.output_attentions\n        self.output_past = config.output_past\n\n        self.d_model_size = config.n_embd\n        self.num_layers = config.n_layer\n\n        self.pos_encoding = positional_encoding(config.n_positions, self.d_model_size, torch.float)\n\n        self.w = nn.Embedding(config.vocab_size, config.n_embd)\n\n        self.dropout = nn.Dropout(config.embd_pdrop)\n        self.h = nn.ModuleList([EncoderLayer(config.n_embd,\n                                             config.n_head,\n                                             config.dff,\n                                             config.resid_pdrop,\n                                             config.output_attentions) for _ in range(config.n_layer)])\n        self.layernorm = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n\n        self.init_weights()\n\n    def _resize_token_embeddings(self, new_num_tokens):\n        self.w = self._get_resized_embeddings(self.w, new_num_tokens)\n        return self.w\n\n    def _prune_heads(self, heads_to_prune):\n        """""" Prunes heads of the model.\n                heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n        """"""\n        for layer, heads in heads_to_prune.items():\n            self.h[layer].attn.prune_heads(heads)\n\n    def forward(self, input_ids, past=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n        input_shape = input_ids.size()\n        input_ids = input_ids.view(-1, input_shape[-1])\n        if past is None:\n            past_length = 0\n            past = [None] * len(self.h)\n        else:\n            past_length = past[0][0].size(-2)\n        if position_ids is None:\n            position_ids = torch.arange(past_length, input_ids.size(-1) + past_length, dtype=torch.long, device=input_ids.device)\n            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n\n        # Attention mask.\n        if attention_mask is not None:\n            attention_mask = attention_mask.view(-1, input_shape[-1])\n            # We create a 3D attention mask from a 2D tensor mask.\n            # Sizes are [batch_size, 1, 1, to_seq_length]\n            # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n            # this attention mask is more simple than the triangular masking of causal attention\n            # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n\n            # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n            # masked positions, this operation will create a tensor which is 0.0 for\n            # positions we want to attend and -10000.0 for masked positions.\n            # Since we are adding it to the raw scores before the softmax, this is\n            # effectively the same as removing these entirely.\n            attention_mask = attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n            attention_mask = (1.0 - attention_mask) * -10000.0\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # head_mask has shape n_layer x batch x n_heads x N x N\n        if head_mask is not None:\n            if head_mask.dim() == 1:\n                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n                head_mask = head_mask.expand(self.config.n_layer, -1, -1, -1, -1)\n            elif head_mask.dim() == 2:\n                head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)  # We can specify head_mask for each layer\n            head_mask = head_mask.to(dtype=next(self.parameters()).dtype) # switch to fload if need + fp16 compatibility\n        else:\n            head_mask = [None] * self.config.n_layer\n\n        if token_type_ids is not None:\n            token_type_ids = token_type_ids.view(-1, input_shape[-1])\n            token_type_embeds = self.w(token_type_ids)\n            token_type_embeds *= np.sqrt(self.d_model_size)\n        else:\n            token_type_embeds = 0\n        position_ids = position_ids.view(-1, input_shape[-1])\n\n        inputs_embeds = self.w(input_ids)\n        # inputs_embeds = embedded.unsqueeze(0) if len(input_ids.shape)<2 else embedded\n        seq_len = input_ids.shape[-1]\n        mask = torch.triu(torch.ones(seq_len, seq_len), 1).to(inputs_embeds.device)\n\n        inputs_embeds *= np.sqrt(self.d_model_size)\n\n        pos_embeds = self.pos_encoding[position_ids, :].to(inputs_embeds.device)\n\n        hidden_states = inputs_embeds + pos_embeds + token_type_embeds\n\n        hidden_states = self.dropout(hidden_states)\n\n        output_shape = input_shape + (inputs_embeds.size(-1),)\n        presents = ()\n        all_hidden_states = ()\n        all_attentions = []\n        for i, (h, layer_past) in enumerate(zip(self.h, past)):\n            if self.output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states.view(*output_shape),)\n            outputs = h(hidden_states,\n                        mask,\n                        layer_past=layer_past,\n                        attention_mask=attention_mask,\n                        head_mask=head_mask[i])\n            hidden_states, present = outputs[:2]\n            if self.output_past:\n                presents = presents + (present,)\n\n            if self.output_attentions:\n                all_attentions.append(outputs[2])\n\n        hidden_states = self.layernorm(hidden_states)\n        hidden_states = hidden_states.view(*output_shape)\n        if self.output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_states,)\n\n        outputs = (hidden_states,)\n        if self.output_past:\n            outputs = outputs + (presents,)\n        if self.output_hidden_states:\n            outputs = outputs + (all_hidden_states,)\n        if self.output_attentions:\n            # let the number of heads free (-1) so we can extract attention even after head pruning\n            attention_output_shape = input_shape[:-1] + (-1,) + all_attentions[0].shape[-2:]\n            all_attentions = tuple(t.view(*attention_output_shape) for t in all_attentions)\n            outputs = outputs + (all_attentions,)\n        return outputs\n\n\n@add_start_docstrings(""""""The CTRL Model transformer with a language modeling head on top\n(linear layer with weights tied to the input embeddings). """""", CTRL_START_DOCSTRING, CTRL_INPUTS_DOCSTRING)\nclass CTRLLMHeadModel(CTRLPreTrainedModel):\n    r""""""\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Labels for language modeling.\n            Note that the labels **are shifted** inside the model, i.e. you can set ``lm_labels = input_ids``\n            Indices are selected in ``[-1, 0, ..., config.vocab_size]``\n            All labels set to ``-1`` are ignored (masked), the loss is only\n            computed for labels in ``[0, ..., config.vocab_size]``\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Language modeling loss.\n        **prediction_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.vocab_size)``\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n        **past**:\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            that contains pre-computed hidden-states (key and values in the attention blocks).\n            Can be used (see `past` input) to speed up sequential decoding.\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        import torch\n        from transformers import CTRLTokenizer, CTRLLMHeadModel\n\n        tokenizer = CTRLTokenizer.from_pretrained(\'ctrl\')\n        model = CTRLLMHeadModel.from_pretrained(\'ctrl\')\n\n        input_ids = torch.tensor(tokenizer.encode(""Links Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, labels=input_ids)\n        loss, logits = outputs[:2]\n\n    """"""\n    def __init__(self, config):\n        super(CTRLLMHeadModel, self).__init__(config)\n        self.transformer = CTRLModel(config)\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=True)\n\n        self.init_weights()\n        self.tie_weights()\n\n    def tie_weights(self):\n        """""" Make sure we are sharing the input and output embeddings.\n                Export to TorchScript can\'t handle parameter sharing so we are cloning them instead.\n        """"""\n        self._tie_or_clone_weights(self.lm_head, self.transformer.w)\n\n    def forward(self, input_ids, past=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n                labels=None):\n        transformer_outputs = self.transformer(input_ids,\n                                               past=past,\n                                               attention_mask=attention_mask,\n                                               token_type_ids=token_type_ids,\n                                               position_ids=position_ids,\n                                               head_mask=head_mask)\n\n        hidden_states = transformer_outputs[0]\n\n        lm_logits = self.lm_head(hidden_states)\n\n        outputs = (lm_logits,) + transformer_outputs[1:]\n\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = lm_logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)),\n                            shift_labels.view(-1))\n            outputs = (loss,) + outputs\n\n        return outputs  # (loss), lm_logits, presents, (all hidden_states), (attentions)\n'"
models/transformers/modeling_distilbert.py,57,"b'# coding=utf-8\n# Copyright 2019-present, the HuggingFace Inc. team, The Google AI Language Team and Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" PyTorch DistilBERT model\n    adapted in part from Facebook, Inc XLM model (https://github.com/facebookresearch/XLM)\n    and in part from HuggingFace PyTorch version of Google AI Bert model (https://github.com/google-research/bert)\n""""""\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport json\nimport logging\nimport math\nimport copy\nimport sys\nfrom io import open\n\nimport itertools\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\n\nfrom .modeling_utils import PreTrainedModel, prune_linear_layer\nfrom .configuration_distilbert import DistilBertConfig\nfrom .file_utils import add_start_docstrings\n\nimport logging\nlogger = logging.getLogger(__name__)\n\n\nDISTILBERT_PRETRAINED_MODEL_ARCHIVE_MAP = {\n    \'distilbert-base-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-pytorch_model.bin"",\n    \'distilbert-base-uncased-distilled-squad\': ""https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-distilled-squad-pytorch_model.bin""\n}\n\n\n### UTILS AND BUILDING BLOCKS OF THE ARCHITECTURE ###\ndef gelu(x):\n    return 0.5 * x * (1.0 + torch.erf(x / math.sqrt(2.0)))\n\ndef create_sinusoidal_embeddings(n_pos, dim, out):\n    position_enc = np.array([\n        [pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)]\n        for pos in range(n_pos)\n    ])\n    out[:, 0::2] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\n    out[:, 1::2] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n    out.detach_()\n    out.requires_grad = False\n\nclass Embeddings(nn.Module):\n    def __init__(self,\n                 config):\n        super(Embeddings, self).__init__()\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.dim, padding_idx=0)\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.dim)\n        if config.sinusoidal_pos_embds:\n            create_sinusoidal_embeddings(n_pos=config.max_position_embeddings,\n                                         dim=config.dim,\n                                         out=self.position_embeddings.weight)\n\n        self.LayerNorm = nn.LayerNorm(config.dim, eps=1e-12)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, input_ids):\n        """"""\n        Parameters\n        ----------\n        input_ids: torch.tensor(bs, max_seq_length)\n            The token ids to embed.\n\n        Outputs\n        -------\n        embeddings: torch.tensor(bs, max_seq_length, dim)\n            The embedded tokens (plus position embeddings, no token_type embeddings)\n        """"""\n        seq_length = input_ids.size(1)\n        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device) # (max_seq_length)\n        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)                      # (bs, max_seq_length)\n\n        word_embeddings = self.word_embeddings(input_ids)                   # (bs, max_seq_length, dim)\n        position_embeddings = self.position_embeddings(position_ids)        # (bs, max_seq_length, dim)\n\n        embeddings = word_embeddings + position_embeddings  # (bs, max_seq_length, dim)\n        embeddings = self.LayerNorm(embeddings)             # (bs, max_seq_length, dim)\n        embeddings = self.dropout(embeddings)               # (bs, max_seq_length, dim)\n        return embeddings\n\nclass MultiHeadSelfAttention(nn.Module):\n    def __init__(self, config):\n        super(MultiHeadSelfAttention, self).__init__()\n\n        self.n_heads = config.n_heads\n        self.dim = config.dim\n        self.dropout = nn.Dropout(p=config.attention_dropout)\n        self.output_attentions = config.output_attentions\n\n        assert self.dim % self.n_heads == 0\n\n        self.q_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n        self.k_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n        self.v_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n        self.out_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n\n        self.pruned_heads = set()\n\n    def prune_heads(self, heads):\n        attention_head_size = self.dim // self.n_heads\n        if len(heads) == 0:\n            return\n        mask = torch.ones(self.n_heads, attention_head_size)\n        heads = set(heads) - self.pruned_heads\n        for head in heads:\n            head -= sum(1 if h < head else 0 for h in self.pruned_heads)\n            mask[head] = 0\n        mask = mask.view(-1).contiguous().eq(1)\n        index = torch.arange(len(mask))[mask].long()\n        # Prune linear layers\n        self.q_lin = prune_linear_layer(self.q_lin, index)\n        self.k_lin = prune_linear_layer(self.k_lin, index)\n        self.v_lin = prune_linear_layer(self.v_lin, index)\n        self.out_lin = prune_linear_layer(self.out_lin, index, dim=1)\n        # Update hyper params\n        self.n_heads = self.n_heads - len(heads)\n        self.dim = attention_head_size * self.n_heads\n        self.pruned_heads = self.pruned_heads.union(heads)\n\n    def forward(self, query, key, value, mask, head_mask = None):\n        """"""\n        Parameters\n        ----------\n        query: torch.tensor(bs, seq_length, dim)\n        key: torch.tensor(bs, seq_length, dim)\n        value: torch.tensor(bs, seq_length, dim)\n        mask: torch.tensor(bs, seq_length)\n\n        Outputs\n        -------\n        weights: torch.tensor(bs, n_heads, seq_length, seq_length)\n            Attention weights\n        context: torch.tensor(bs, seq_length, dim)\n            Contextualized layer. Optional: only if `output_attentions=True`\n        """"""\n        bs, q_length, dim = query.size()\n        k_length = key.size(1)\n        # assert dim == self.dim, \'Dimensions do not match: %s input vs %s configured\' % (dim, self.dim)\n        # assert key.size() == value.size()\n\n        dim_per_head = self.dim // self.n_heads\n\n        mask_reshp = (bs, 1, 1, k_length)\n\n        def shape(x):\n            """""" separate heads """"""\n            return x.view(bs, -1, self.n_heads, dim_per_head).transpose(1, 2)\n\n        def unshape(x):\n            """""" group heads """"""\n            return x.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * dim_per_head)\n\n        q = shape(self.q_lin(query))           # (bs, n_heads, q_length, dim_per_head)\n        k = shape(self.k_lin(key))             # (bs, n_heads, k_length, dim_per_head)\n        v = shape(self.v_lin(value))           # (bs, n_heads, k_length, dim_per_head)\n\n        q = q / math.sqrt(dim_per_head)                     # (bs, n_heads, q_length, dim_per_head)\n        scores = torch.matmul(q, k.transpose(2,3))          # (bs, n_heads, q_length, k_length)\n        mask = (mask==0).view(mask_reshp).expand_as(scores) # (bs, n_heads, q_length, k_length)\n        scores.masked_fill_(mask, -float(\'inf\'))            # (bs, n_heads, q_length, k_length)\n\n        weights = nn.Softmax(dim=-1)(scores)   # (bs, n_heads, q_length, k_length)\n        weights = self.dropout(weights)        # (bs, n_heads, q_length, k_length)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            weights = weights * head_mask\n\n        context = torch.matmul(weights, v)     # (bs, n_heads, q_length, dim_per_head)\n        context = unshape(context)             # (bs, q_length, dim)\n        context = self.out_lin(context)        # (bs, q_length, dim)\n\n        if self.output_attentions:\n            return (context, weights)\n        else:\n            return (context,)\n\nclass FFN(nn.Module):\n    def __init__(self, config):\n        super(FFN, self).__init__()\n        self.dropout = nn.Dropout(p=config.dropout)\n        self.lin1 = nn.Linear(in_features=config.dim, out_features=config.hidden_dim)\n        self.lin2 = nn.Linear(in_features=config.hidden_dim, out_features=config.dim)\n        assert config.activation in [\'relu\', \'gelu\'], ""activation ({}) must be in [\'relu\', \'gelu\']"".format(config.activation)\n        self.activation = gelu if config.activation == \'gelu\' else nn.ReLU()\n\n    def forward(self, input):\n        x = self.lin1(input)\n        x = self.activation(x)\n        x = self.lin2(x)\n        x = self.dropout(x)\n        return x\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, config):\n        super(TransformerBlock, self).__init__()\n\n        self.n_heads = config.n_heads\n        self.dim = config.dim\n        self.hidden_dim = config.hidden_dim\n        self.dropout = nn.Dropout(p=config.dropout)\n        self.activation = config.activation\n        self.output_attentions = config.output_attentions\n\n        assert config.dim % config.n_heads == 0\n\n        self.attention = MultiHeadSelfAttention(config)\n        self.sa_layer_norm = nn.LayerNorm(normalized_shape=config.dim, eps=1e-12)\n\n        self.ffn = FFN(config)\n        self.output_layer_norm = nn.LayerNorm(normalized_shape=config.dim, eps=1e-12)\n\n    def forward(self, x, attn_mask=None, head_mask=None):\n        """"""\n        Parameters\n        ----------\n        x: torch.tensor(bs, seq_length, dim)\n        attn_mask: torch.tensor(bs, seq_length)\n\n        Outputs\n        -------\n        sa_weights: torch.tensor(bs, n_heads, seq_length, seq_length)\n            The attention weights\n        ffn_output: torch.tensor(bs, seq_length, dim)\n            The output of the transformer block contextualization.\n        """"""\n        # Self-Attention\n        sa_output = self.attention(query=x, key=x, value=x, mask=attn_mask, head_mask=head_mask)\n        if self.output_attentions:\n            sa_output, sa_weights = sa_output                  # (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\n        else: # To handle these `output_attention` or `output_hidden_states` cases returning tuples\n            assert type(sa_output) == tuple\n            sa_output = sa_output[0]\n        sa_output = self.sa_layer_norm(sa_output + x)          # (bs, seq_length, dim)\n\n        # Feed Forward Network\n        ffn_output = self.ffn(sa_output)                             # (bs, seq_length, dim)\n        ffn_output = self.output_layer_norm(ffn_output + sa_output)  # (bs, seq_length, dim)\n\n        output = (ffn_output,)\n        if self.output_attentions:\n            output = (sa_weights,) + output\n        return output\n\n\nclass Transformer(nn.Module):\n    def __init__(self, config):\n        super(Transformer, self).__init__()\n        self.n_layers = config.n_layers\n        self.output_attentions = config.output_attentions\n        self.output_hidden_states = config.output_hidden_states\n\n        layer = TransformerBlock(config)\n        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.n_layers)])\n\n    def forward(self, x, attn_mask=None, head_mask=None):\n        """"""\n        Parameters\n        ----------\n        x: torch.tensor(bs, seq_length, dim)\n            Input sequence embedded.\n        attn_mask: torch.tensor(bs, seq_length)\n            Attention mask on the sequence.\n\n        Outputs\n        -------\n        hidden_state: torch.tensor(bs, seq_length, dim)\n            Sequence of hiddens states in the last (top) layer\n        all_hidden_states: Tuple[torch.tensor(bs, seq_length, dim)]\n            Tuple of length n_layers with the hidden states from each layer.\n            Optional: only if output_hidden_states=True\n        all_attentions: Tuple[torch.tensor(bs, n_heads, seq_length, seq_length)]\n            Tuple of length n_layers with the attention weights from each layer\n            Optional: only if output_attentions=True\n        """"""\n        all_hidden_states = ()\n        all_attentions = ()\n\n        hidden_state = x\n        for i, layer_module in enumerate(self.layer):\n            if self.output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_state,)\n\n            layer_outputs = layer_module(x=hidden_state,\n                                         attn_mask=attn_mask,\n                                         head_mask=head_mask[i])\n            hidden_state = layer_outputs[-1]\n\n            if self.output_attentions:\n                assert len(layer_outputs) == 2\n                attentions = layer_outputs[0]\n                all_attentions = all_attentions + (attentions,)\n            else:\n                assert len(layer_outputs) == 1\n\n        # Add last layer\n        if self.output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_state,)\n\n        outputs = (hidden_state,)\n        if self.output_hidden_states:\n            outputs = outputs + (all_hidden_states,)\n        if self.output_attentions:\n            outputs = outputs + (all_attentions,)\n        return outputs  # last-layer hidden state, (all hidden states), (all attentions)\n\n\n### INTERFACE FOR ENCODER AND TASK SPECIFIC MODEL ###\nclass DistilBertPreTrainedModel(PreTrainedModel):\n    """""" An abstract class to handle weights initialization and\n        a simple interface for downloading and loading pretrained models.\n    """"""\n    config_class = DistilBertConfig\n    pretrained_model_archive_map = DISTILBERT_PRETRAINED_MODEL_ARCHIVE_MAP\n    load_tf_weights = None\n    base_model_prefix = ""distilbert""\n\n    def __init__(self, *inputs, **kwargs):\n        super(DistilBertPreTrainedModel, self).__init__(*inputs, **kwargs)\n    \n    def _init_weights(self, module):\n        """""" Initialize the weights.\n        """"""\n        if isinstance(module, nn.Embedding):\n            if module.weight.requires_grad:\n                module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n\n\nDISTILBERT_START_DOCSTRING = r""""""\n    DistilBERT is a small, fast, cheap and light Transformer model\n    trained by distilling Bert base. It has 40% less parameters than\n    `bert-base-uncased`, runs 60% faster while preserving over 95% of\n    Bert\'s performances as measured on the GLUE language understanding benchmark.\n\n    Here are the differences between the interface of Bert and DistilBert:\n\n    - DistilBert doesn\'t have `token_type_ids`, you don\'t need to indicate which token belongs to which segment. Just separate your segments with the separation token `tokenizer.sep_token` (or `[SEP]`)\n    - DistilBert doesn\'t have options to select the input positions (`position_ids` input). This could be added if necessary though, just let\'s us know if you need this option.\n\n    For more information on DistilBERT, please refer to our\n    `detailed blog post`_\n    \n    .. _`detailed blog post`:\n        https://medium.com/huggingface/distilbert-8cf3380435b5\n\n    Parameters:\n        config (:class:`~transformers.DistilBertConfig`): Model configuration class with all the parameters of the model. \n            Initializing with a config file does not load the weights associated with the model, only the configuration.\n            Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model weights.\n""""""\n\nDISTILBERT_INPUTS_DOCSTRING = r""""""\n    Inputs:\n        **input_ids** ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Indices of input sequence tokens in the vocabulary.\n            The input sequences should start with `[CLS]` and end with `[SEP]` tokens.\n            \n            For now, ONLY BertTokenizer(`bert-base-uncased`) is supported and you should use this tokenizer when using DistilBERT.\n        **attention_mask**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Mask to avoid performing attention on padding token indices.\n            Mask values selected in ``[0, 1]``:\n            ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n        **head_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(num_heads,)`` or ``(num_layers, num_heads)``:\n            Mask to nullify selected heads of the self-attention modules.\n            Mask values selected in ``[0, 1]``:\n            ``1`` indicates the head is **not masked**, ``0`` indicates the head is **masked**.\n""""""\n\n@add_start_docstrings(""The bare DistilBERT encoder/transformer outputting raw hidden-states without any specific head on top."",\n                      DISTILBERT_START_DOCSTRING, DISTILBERT_INPUTS_DOCSTRING)\nclass DistilBertModel(DistilBertPreTrainedModel):\n    r""""""\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **last_hidden_state**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, hidden_size)``\n            Sequence of hidden-states at the output of the last layer of the model.\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = DistilBertTokenizer.from_pretrained(\'distilbert-base-uncased\')\n        model = DistilBertModel.from_pretrained(\'distilbert-base-uncased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids)\n        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n\n    """"""\n    def __init__(self, config):\n        super(DistilBertModel, self).__init__(config)\n\n        self.embeddings = Embeddings(config)   # Embeddings\n        self.transformer = Transformer(config) # Encoder\n\n        self.init_weights()\n\n    def _resize_token_embeddings(self, new_num_tokens):\n        old_embeddings = self.embeddings.word_embeddings\n        new_embeddings = self._get_resized_embeddings(old_embeddings, new_num_tokens)\n        self.embeddings.word_embeddings = new_embeddings\n        return self.embeddings.word_embeddings\n\n    def _prune_heads(self, heads_to_prune):\n        """""" Prunes heads of the model.\n            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n            See base class PreTrainedModel\n        """"""\n        for layer, heads in heads_to_prune.items():\n            self.transformer.layer[layer].attention.prune_heads(heads)\n\n    def forward(self,\n                input_ids, attention_mask=None, head_mask=None):\n        if attention_mask is None:\n            attention_mask = torch.ones_like(input_ids) # (bs, seq_length)\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        if head_mask is not None:\n            if head_mask.dim() == 1:\n                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n                head_mask = head_mask.expand(self.config.num_hidden_layers, -1, -1, -1, -1)\n            elif head_mask.dim() == 2:\n                head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)  # We can specify head_mask for each layer\n            head_mask = head_mask.to(dtype=next(self.parameters()).dtype) # switch to fload if need + fp16 compatibility\n        else:\n            head_mask = [None] * self.config.num_hidden_layers\n\n        embedding_output = self.embeddings(input_ids)   # (bs, seq_length, dim)\n        tfmr_output = self.transformer(x=embedding_output,\n                                       attn_mask=attention_mask,\n                                       head_mask=head_mask)\n        hidden_state = tfmr_output[0]\n        output = (hidden_state, ) + tfmr_output[1:]\n\n        return output # last-layer hidden-state, (all hidden_states), (all attentions)\n\n\n@add_start_docstrings(""""""DistilBert Model with a `masked language modeling` head on top. """""",\n                      DISTILBERT_START_DOCSTRING, DISTILBERT_INPUTS_DOCSTRING)\nclass DistilBertForMaskedLM(DistilBertPreTrainedModel):\n    r""""""\n        **masked_lm_labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Labels for computing the masked language modeling loss.\n            Indices should be in ``[-1, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring)\n            Tokens with indices set to ``-1`` are ignored (masked), the loss is only computed for the tokens with labels\n            in ``[0, ..., config.vocab_size]``\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``masked_lm_labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Masked language modeling loss.\n        **prediction_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.vocab_size)``\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = DistilBertTokenizer.from_pretrained(\'distilbert-base-uncased\')\n        model = DistilBertForMaskedLM.from_pretrained(\'distilbert-base-uncased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, masked_lm_labels=input_ids)\n        loss, prediction_scores = outputs[:2]\n\n    """"""\n    def __init__(self, config):\n        super(DistilBertForMaskedLM, self).__init__(config)\n        self.output_attentions = config.output_attentions\n        self.output_hidden_states = config.output_hidden_states\n\n        self.distilbert = DistilBertModel(config)\n        self.vocab_transform = nn.Linear(config.dim, config.dim)\n        self.vocab_layer_norm = nn.LayerNorm(config.dim, eps=1e-12)\n        self.vocab_projector = nn.Linear(config.dim, config.vocab_size)\n\n        self.init_weights()\n        self.tie_weights()\n\n        self.mlm_loss_fct = nn.CrossEntropyLoss(ignore_index=-1)\n\n    def tie_weights(self):\n        """""" Make sure we are sharing the input and output embeddings.\n            Export to TorchScript can\'t handle parameter sharing so we are cloning them instead.\n        """"""\n        self._tie_or_clone_weights(self.vocab_projector,\n                                   self.distilbert.embeddings.word_embeddings)\n\n    def forward(self, input_ids, attention_mask=None, head_mask=None, masked_lm_labels=None):\n        dlbrt_output = self.distilbert(input_ids=input_ids,\n                                       attention_mask=attention_mask,\n                                       head_mask=head_mask)\n        hidden_states = dlbrt_output[0]                              # (bs, seq_length, dim)\n        prediction_logits = self.vocab_transform(hidden_states)      # (bs, seq_length, dim)\n        prediction_logits = gelu(prediction_logits)                  # (bs, seq_length, dim)\n        prediction_logits = self.vocab_layer_norm(prediction_logits) # (bs, seq_length, dim)\n        prediction_logits = self.vocab_projector(prediction_logits)  # (bs, seq_length, vocab_size)\n\n        outputs = (prediction_logits, ) + dlbrt_output[1:]\n        if masked_lm_labels is not None:\n            mlm_loss = self.mlm_loss_fct(prediction_logits.view(-1, prediction_logits.size(-1)),\n                                         masked_lm_labels.view(-1))\n            outputs = (mlm_loss,) + outputs     \n\n        return outputs # (mlm_loss), prediction_logits, (all hidden_states), (all attentions)\n\n\n@add_start_docstrings(""""""DistilBert Model transformer with a sequence classification/regression head on top (a linear layer on top of\n                         the pooled output) e.g. for GLUE tasks. """""",\n                      DISTILBERT_START_DOCSTRING, DISTILBERT_INPUTS_DOCSTRING)\nclass DistilBertForSequenceClassification(DistilBertPreTrainedModel):\n    r""""""\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the sequence classification/regression loss.\n            Indices should be in ``[0, ..., config.num_labels - 1]``.\n            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Classification (or regression if config.num_labels==1) loss.\n        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = DistilBertTokenizer.from_pretrained(\'distilbert-base-uncased\')\n        model = DistilBertForSequenceClassification.from_pretrained(\'distilbert-base-uncased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, labels=labels)\n        loss, logits = outputs[:2]\n\n    """"""\n    def __init__(self, config):\n        super(DistilBertForSequenceClassification, self).__init__(config)\n        self.num_labels = config.num_labels\n\n        self.distilbert = DistilBertModel(config)\n        self.pre_classifier = nn.Linear(config.dim, config.dim)\n        self.classifier = nn.Linear(config.dim, config.num_labels)\n        self.dropout = nn.Dropout(config.seq_classif_dropout)\n\n        self.init_weights()\n\n    def forward(self, input_ids,  attention_mask=None, head_mask=None, labels=None):\n        distilbert_output = self.distilbert(input_ids=input_ids,\n                                            attention_mask=attention_mask,\n                                            head_mask=head_mask)\n        hidden_state = distilbert_output[0]                    # (bs, seq_len, dim)\n        pooled_output = hidden_state[:, 0]                    # (bs, dim)\n        pooled_output = self.pre_classifier(pooled_output)   # (bs, dim)\n        pooled_output = nn.ReLU()(pooled_output)             # (bs, dim)\n        pooled_output = self.dropout(pooled_output)         # (bs, dim)\n        logits = self.classifier(pooled_output)              # (bs, dim)\n\n        outputs = (logits,) + distilbert_output[1:]\n        if labels is not None:\n            if self.num_labels == 1:\n                loss_fct = nn.MSELoss()\n                loss = loss_fct(logits.view(-1), labels.view(-1))\n            else:\n                loss_fct = nn.CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            outputs = (loss,) + outputs\n\n        return outputs  # (loss), logits, (hidden_states), (attentions)\n\n\n@add_start_docstrings(""""""DistilBert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear layers on top of\n                         the hidden-states output to compute `span start logits` and `span end logits`). """""",\n                      DISTILBERT_START_DOCSTRING, DISTILBERT_INPUTS_DOCSTRING)\nclass DistilBertForQuestionAnswering(DistilBertPreTrainedModel):\n    r""""""\n        **start_positions**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`).\n            Position outside of the sequence are not taken into account for computing the loss.\n        **end_positions**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`).\n            Position outside of the sequence are not taken into account for computing the loss.\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.\n        **start_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length,)``\n            Span-start scores (before SoftMax).\n        **end_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length,)``\n            Span-end scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = DistilBertTokenizer.from_pretrained(\'distilbert-base-uncased\')\n        model = DistilBertForQuestionAnswering.from_pretrained(\'distilbert-base-uncased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        start_positions = torch.tensor([1])\n        end_positions = torch.tensor([3])\n        outputs = model(input_ids, start_positions=start_positions, end_positions=end_positions)\n        loss, start_scores, end_scores = outputs[:3]\n\n    """"""\n    def __init__(self, config):\n        super(DistilBertForQuestionAnswering, self).__init__(config)\n\n        self.distilbert = DistilBertModel(config)\n        self.qa_outputs = nn.Linear(config.dim, config.num_labels)\n        assert config.num_labels == 2\n        self.dropout = nn.Dropout(config.qa_dropout)\n\n        self.init_weights()\n        \n    def forward(self, input_ids, attention_mask=None, head_mask=None, start_positions=None, end_positions=None):\n        distilbert_output = self.distilbert(input_ids=input_ids,\n                                            attention_mask=attention_mask,\n                                            head_mask=head_mask)\n        hidden_states = distilbert_output[0]                                 # (bs, max_query_len, dim)\n\n        hidden_states = self.dropout(hidden_states)                       # (bs, max_query_len, dim)\n        logits = self.qa_outputs(hidden_states)                           # (bs, max_query_len, 2)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)                           # (bs, max_query_len)\n        end_logits = end_logits.squeeze(-1)                               # (bs, max_query_len)\n\n        outputs = (start_logits, end_logits,) + distilbert_output[1:]\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions.clamp_(0, ignored_index)\n            end_positions.clamp_(0, ignored_index)\n\n            loss_fct = nn.CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n            outputs = (total_loss,) + outputs\n\n        return outputs  # (loss), start_logits, end_logits, (hidden_states), (attentions)\n'"
models/transformers/modeling_gpt2.py,48,"b'# coding=utf-8\n# Copyright 2018 The OpenAI Team Authors and HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""PyTorch OpenAI GPT-2 model.""""""\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport collections\nimport json\nimport logging\nimport math\nimport os\nimport sys\nfrom io import open\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\nfrom torch.nn.parameter import Parameter\n\nfrom .modeling_utils import PreTrainedModel, Conv1D, prune_conv1d_layer, SequenceSummary\nfrom .configuration_gpt2 import GPT2Config\nfrom .file_utils import add_start_docstrings\n\nlogger = logging.getLogger(__name__)\n\nGPT2_PRETRAINED_MODEL_ARCHIVE_MAP = {""gpt2"": ""https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin"",\n                                     ""gpt2-medium"": ""https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-pytorch_model.bin"",\n                                     ""gpt2-large"": ""https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-pytorch_model.bin"",\n                                     ""distilgpt2"": ""https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-pytorch_model.bin"",}\n\ndef load_tf_weights_in_gpt2(model, config, gpt2_checkpoint_path):\n    """""" Load tf checkpoints in a pytorch model\n    """"""\n    try:\n        import re\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        logger.error(""Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see ""\n            ""https://www.tensorflow.org/install/ for installation instructions."")\n        raise\n    tf_path = os.path.abspath(gpt2_checkpoint_path)\n    logger.info(""Converting TensorFlow checkpoint from {}"".format(tf_path))\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    names = []\n    arrays = []\n    for name, shape in init_vars:\n        logger.info(""Loading TF weight {} with shape {}"".format(name, shape))\n        array = tf.train.load_variable(tf_path, name)\n        names.append(name)\n        arrays.append(array.squeeze())\n\n    for name, array in zip(names, arrays):\n        name = name[6:]  # skip ""model/""\n        name = name.split(\'/\')\n        pointer = model\n        for m_name in name:\n            if re.fullmatch(r\'[A-Za-z]+\\d+\', m_name):\n                l = re.split(r\'(\\d+)\', m_name)\n            else:\n                l = [m_name]\n            if l[0] == \'w\' or l[0] == \'g\':\n                pointer = getattr(pointer, \'weight\')\n            elif l[0] == \'b\':\n                pointer = getattr(pointer, \'bias\')\n            elif l[0] == \'wpe\' or l[0] == \'wte\':\n                pointer = getattr(pointer, l[0])\n                pointer = getattr(pointer, \'weight\')\n            else:\n                pointer = getattr(pointer, l[0])\n            if len(l) >= 2:\n                num = int(l[1])\n                pointer = pointer[num]\n        try:\n            assert pointer.shape == array.shape\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        logger.info(""Initialize PyTorch weight {}"".format(name))\n        pointer.data = torch.from_numpy(array)\n    return model\n\n\ndef gelu(x):\n    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n\n\nclass Attention(nn.Module):\n    def __init__(self, nx, n_ctx, config, scale=False):\n        super(Attention, self).__init__()\n        self.output_attentions = config.output_attentions\n\n        n_state = nx  # in Attention: n_state=768 (nx=n_embd)\n        # [switch nx => n_state from Block to Attention to keep identical to TF implem]\n        assert n_state % config.n_head == 0\n        self.register_buffer(""bias"", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n        self.n_head = config.n_head\n        self.split_size = n_state\n        self.scale = scale\n\n        self.c_attn = Conv1D(n_state * 3, nx)\n        self.c_proj = Conv1D(n_state, nx)\n        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n        self.pruned_heads = set()\n\n    def prune_heads(self, heads):\n        if len(heads) == 0:\n            return\n        mask = torch.ones(self.n_head, self.split_size // self.n_head)\n        heads = set(heads) - self.pruned_heads  # Convert to set and emove already pruned heads\n        for head in heads:\n            # Compute how many pruned heads are before the head and move the index accordingly\n            head = head - sum(1 if h < head else 0 for h in self.pruned_heads)\n            mask[head] = 0\n        mask = mask.view(-1).contiguous().eq(1)\n        index = torch.arange(len(mask))[mask].long()\n        index_attn = torch.cat([index, index + self.split_size, index + (2*self.split_size)])\n\n        # Prune conv1d layers\n        self.c_attn = prune_conv1d_layer(self.c_attn, index_attn, dim=1)\n        self.c_proj = prune_conv1d_layer(self.c_proj, index, dim=0)\n\n        # Update hyper params\n        self.split_size = (self.split_size // self.n_head) * (self.n_head - len(heads))\n        self.n_head = self.n_head - len(heads)\n        self.pruned_heads = self.pruned_heads.union(heads)\n\n    def _attn(self, q, k, v, attention_mask=None, head_mask=None):\n        w = torch.matmul(q, k)\n        if self.scale:\n            w = w / math.sqrt(v.size(-1))\n        nd, ns = w.size(-2), w.size(-1)\n        b = self.bias[:, :, ns-nd:ns, :ns]\n        w = w * b - 1e4 * (1 - b)\n\n        if attention_mask is not None:\n            # Apply the attention mask\n            w = w + attention_mask\n\n        w = nn.Softmax(dim=-1)(w)\n        w = self.attn_dropout(w)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            w = w * head_mask\n\n        outputs = [torch.matmul(w, v)]\n        if self.output_attentions:\n            outputs.append(w)\n        return outputs\n\n    def merge_heads(self, x):\n        x = x.permute(0, 2, 1, 3).contiguous()\n        new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)\n        return x.view(*new_x_shape)  # in Tensorflow implem: fct merge_states\n\n    def split_heads(self, x, k=False):\n        new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)\n        x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states\n        if k:\n            return x.permute(0, 2, 3, 1)  # (batch, head, head_features, seq_length)\n        else:\n            return x.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)\n\n    def forward(self, x, layer_past=None, attention_mask=None, head_mask=None):\n        x = self.c_attn(x)\n        query, key, value = x.split(self.split_size, dim=2)\n        query = self.split_heads(query)\n        key = self.split_heads(key, k=True)\n        value = self.split_heads(value)\n        if layer_past is not None:\n            past_key, past_value = layer_past[0].transpose(-2, -1), layer_past[1]  # transpose back cf below\n            key = torch.cat((past_key, key), dim=-1)\n            value = torch.cat((past_value, value), dim=-2)\n        present = torch.stack((key.transpose(-2, -1), value))  # transpose to have same shapes for stacking\n\n        attn_outputs = self._attn(query, key, value, attention_mask, head_mask)\n        a = attn_outputs[0]\n\n        a = self.merge_heads(a)\n        a = self.c_proj(a)\n        a = self.resid_dropout(a)\n\n        outputs = [a, present] + attn_outputs[1:]\n        return outputs  # a, present, (attentions)\n\n\nclass MLP(nn.Module):\n    def __init__(self, n_state, config):  # in MLP: n_state=3072 (4 * n_embd)\n        super(MLP, self).__init__()\n        nx = config.n_embd\n        self.c_fc = Conv1D(n_state, nx)\n        self.c_proj = Conv1D(nx, n_state)\n        self.act = gelu\n        self.dropout = nn.Dropout(config.resid_pdrop)\n\n    def forward(self, x):\n        h = self.act(self.c_fc(x))\n        h2 = self.c_proj(h)\n        return self.dropout(h2)\n\n\nclass Block(nn.Module):\n    def __init__(self, n_ctx, config, scale=False):\n        super(Block, self).__init__()\n        nx = config.n_embd\n        self.ln_1 = nn.LayerNorm(nx, eps=config.layer_norm_epsilon)\n        self.attn = Attention(nx, n_ctx, config, scale)\n        self.ln_2 = nn.LayerNorm(nx, eps=config.layer_norm_epsilon)\n        self.mlp = MLP(4 * nx, config)\n\n    def forward(self, x, layer_past=None, attention_mask=None, head_mask=None):\n        output_attn = self.attn(self.ln_1(x),\n                                layer_past=layer_past,\n                                attention_mask=attention_mask,\n                                head_mask=head_mask)\n        a = output_attn[0]  # output_attn: a, present, (attentions)\n\n        x = x + a\n        m = self.mlp(self.ln_2(x))\n        x = x + m\n\n        outputs = [x] + output_attn[1:]\n        return outputs  # x, present, (attentions)\n\n\nclass GPT2PreTrainedModel(PreTrainedModel):\n    """""" An abstract class to handle weights initialization and\n        a simple interface for dowloading and loading pretrained models.\n    """"""\n    config_class = GPT2Config\n    pretrained_model_archive_map = GPT2_PRETRAINED_MODEL_ARCHIVE_MAP\n    load_tf_weights = load_tf_weights_in_gpt2\n    base_model_prefix = ""transformer""\n\n    def __init__(self, *inputs, **kwargs):\n        super(GPT2PreTrainedModel, self).__init__(*inputs, **kwargs)\n\n    def _init_weights(self, module):\n        """""" Initialize the weights.\n        """"""\n        if isinstance(module, (nn.Linear, nn.Embedding, Conv1D)):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if isinstance(module, (nn.Linear, Conv1D)) and module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n\nGPT2_START_DOCSTRING = r""""""    OpenAI GPT-2 model was proposed in\n    `Language Models are Unsupervised Multitask Learners`_\n    by Alec Radford*, Jeffrey Wu*, Rewon Child, David Luan, Dario Amodei** and Ilya Sutskever**.\n    It\'s a causal (unidirectional) transformer pre-trained using  language modeling on a very large\n    corpus of ~40 GB of text data.\n\n    This model is a PyTorch `torch.nn.Module`_ sub-class. Use it as a regular PyTorch Module and\n    refer to the PyTorch documentation for all matter related to general usage and behavior.\n\n    .. _`Language Models are Unsupervised Multitask Learners`:\n        https://openai.com/blog/better-language-models/\n\n    .. _`torch.nn.Module`:\n        https://pytorch.org/docs/stable/nn.html#module\n\n    Parameters:\n        config (:class:`~transformers.GPT2Config`): Model configuration class with all the parameters of the model.\n            Initializing with a config file does not load the weights associated with the model, only the configuration.\n            Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model weights.\n""""""\n\nGPT2_INPUTS_DOCSTRING = r""""""    Inputs:\n        **input_ids**: ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Indices of input sequence tokens in the vocabulary.\n            GPT-2 is a model with absolute position embeddings so it\'s usually advised to pad the inputs on\n            the right rather than the left.\n            Indices can be obtained using :class:`transformers.GPT2Tokenizer`.\n            See :func:`transformers.PreTrainedTokenizer.encode` and\n            :func:`transformers.PreTrainedTokenizer.convert_tokens_to_ids` for details.\n        **past**:\n            list of ``torch.FloatTensor`` (one for each layer):\n            that contains pre-computed hidden-states (key and values in the attention blocks) as computed by the model\n            (see `past` output below). Can be used to speed up sequential decoding.\n        **attention_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, sequence_length)``:\n            Mask to avoid performing attention on padding token indices.\n            Mask values selected in ``[0, 1]``:\n            ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n        **token_type_ids**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            A parallel sequence of tokens (can be used to indicate various portions of the inputs).\n            The embeddings from these tokens will be summed with the respective token embeddings.\n            Indices are selected in the vocabulary (unlike BERT which has a specific vocabulary for segment indices).\n        **position_ids**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Indices of positions of each input sequence tokens in the position embeddings.\n            Selected in the range ``[0, config.max_position_embeddings - 1]``.\n        **head_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(num_heads,)`` or ``(num_layers, num_heads)``:\n            Mask to nullify selected heads of the self-attention modules.\n            Mask values selected in ``[0, 1]``:\n            ``1`` indicates the head is **not masked**, ``0`` indicates the head is **masked**.\n""""""\n\n@add_start_docstrings(""The bare GPT2 Model transformer outputting raw hidden-states without any specific head on top."",\n                      GPT2_START_DOCSTRING, GPT2_INPUTS_DOCSTRING)\nclass GPT2Model(GPT2PreTrainedModel):\n    r""""""\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **last_hidden_state**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, hidden_size)``\n            Sequence of hidden-states at the last layer of the model.\n        **past**:\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            that contains pre-computed hidden-states (key and values in the attention blocks).\n            Can be used (see `past` input) to speed up sequential decoding.\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = GPT2Tokenizer.from_pretrained(\'gpt2\')\n        model = GPT2Model.from_pretrained(\'gpt2\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids)\n        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n\n    """"""\n    def __init__(self, config):\n        super(GPT2Model, self).__init__(config)\n        self.output_hidden_states = config.output_hidden_states\n        self.output_attentions = config.output_attentions\n        self.output_past = config.output_past\n\n        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n        self.wpe = nn.Embedding(config.n_positions, config.n_embd)\n        self.drop = nn.Dropout(config.embd_pdrop)\n        self.h = nn.ModuleList([Block(config.n_ctx, config, scale=True) for _ in range(config.n_layer)])\n        self.ln_f = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n\n        self.init_weights()\n\n    def _resize_token_embeddings(self, new_num_tokens):\n        self.wte = self._get_resized_embeddings(self.wte, new_num_tokens)\n        return self.wte\n\n    def _prune_heads(self, heads_to_prune):\n        """""" Prunes heads of the model.\n            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n        """"""\n        for layer, heads in heads_to_prune.items():\n            self.h[layer].attn.prune_heads(heads)\n\n    def forward(self, input_ids, past=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n        input_shape = input_ids.size()\n        input_ids = input_ids.view(-1, input_shape[-1])\n        if token_type_ids is not None:\n            token_type_ids = token_type_ids.view(-1, input_shape[-1])\n        if position_ids is not None:\n            position_ids = position_ids.view(-1, input_shape[-1])\n\n        if past is None:\n            past_length = 0\n            past = [None] * len(self.h)\n        else:\n            past_length = past[0][0].size(-2)\n        if position_ids is None:\n            position_ids = torch.arange(past_length, input_ids.size(-1) + past_length, dtype=torch.long, device=input_ids.device)\n            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n\n        # Attention mask.\n        if attention_mask is not None:\n            attention_mask = attention_mask.view(-1, input_shape[-1])\n            # We create a 3D attention mask from a 2D tensor mask.\n            # Sizes are [batch_size, 1, 1, to_seq_length]\n            # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n            # this attention mask is more simple than the triangular masking of causal attention\n            # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n\n            # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n            # masked positions, this operation will create a tensor which is 0.0 for\n            # positions we want to attend and -10000.0 for masked positions.\n            # Since we are adding it to the raw scores before the softmax, this is\n            # effectively the same as removing these entirely.\n            attention_mask = attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n            attention_mask = (1.0 - attention_mask) * -10000.0\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # head_mask has shape n_layer x batch x n_heads x N x N\n        if head_mask is not None:\n            if head_mask.dim() == 1:\n                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n                head_mask = head_mask.expand(self.config.n_layer, -1, -1, -1, -1)\n            elif head_mask.dim() == 2:\n                head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)  # We can specify head_mask for each layer\n            head_mask = head_mask.to(dtype=next(self.parameters()).dtype) # switch to fload if need + fp16 compatibility\n        else:\n            head_mask = [None] * self.config.n_layer\n\n        inputs_embeds = self.wte(input_ids)\n        position_embeds = self.wpe(position_ids)\n        if token_type_ids is not None:\n            token_type_embeds = self.wte(token_type_ids)\n        else:\n            token_type_embeds = 0\n        hidden_states = inputs_embeds + position_embeds + token_type_embeds\n        hidden_states = self.drop(hidden_states)\n\n        output_shape = input_shape + (hidden_states.size(-1),)\n\n        presents = ()\n        all_attentions = []\n        all_hidden_states = ()\n        for i, (block, layer_past) in enumerate(zip(self.h, past)):\n            if self.output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states.view(*output_shape),)\n\n            outputs = block(hidden_states,\n                            layer_past=layer_past,\n                            attention_mask=attention_mask,\n                            head_mask=head_mask[i])\n\n            hidden_states, present = outputs[:2]\n            if self.output_past:\n                presents = presents + (present,)\n\n            if self.output_attentions:\n                all_attentions.append(outputs[2])\n\n        hidden_states = self.ln_f(hidden_states)\n\n        hidden_states = hidden_states.view(*output_shape)\n        # Add last hidden state\n        if self.output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_states,)\n\n        outputs = (hidden_states,)\n        if self.output_past:\n            outputs = outputs + (presents,)\n        if self.output_hidden_states:\n            outputs = outputs + (all_hidden_states,)\n        if self.output_attentions:\n            # let the number of heads free (-1) so we can extract attention even after head pruning\n            attention_output_shape = input_shape[:-1] + (-1,) + all_attentions[0].shape[-2:]\n            all_attentions = tuple(t.view(*attention_output_shape) for t in all_attentions)\n            outputs = outputs + (all_attentions,)\n        return outputs  # last hidden state, (presents), (all hidden_states), (attentions)\n\n\n@add_start_docstrings(""""""The GPT2 Model transformer with a language modeling head on top\n(linear layer with weights tied to the input embeddings). """""", GPT2_START_DOCSTRING, GPT2_INPUTS_DOCSTRING)\nclass GPT2LMHeadModel(GPT2PreTrainedModel):\n    r""""""\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Labels for language modeling.\n            Note that the labels **are shifted** inside the model, i.e. you can set ``lm_labels = input_ids``\n            Indices are selected in ``[-1, 0, ..., config.vocab_size]``\n            All labels set to ``-1`` are ignored (masked), the loss is only\n            computed for labels in ``[0, ..., config.vocab_size]``\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Language modeling loss.\n        **prediction_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.vocab_size)``\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n        **past**:\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            that contains pre-computed hidden-states (key and values in the attention blocks).\n            Can be used (see `past` input) to speed up sequential decoding.\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        import torch\n        from transformers import GPT2Tokenizer, GPT2LMHeadModel\n\n        tokenizer = GPT2Tokenizer.from_pretrained(\'gpt2\')\n        model = GPT2LMHeadModel.from_pretrained(\'gpt2\')\n\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, labels=input_ids)\n        loss, logits = outputs[:2]\n\n    """"""\n    def __init__(self, config):\n        super(GPT2LMHeadModel, self).__init__(config)\n        self.transformer = GPT2Model(config)\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n\n        self.init_weights()\n        self.tie_weights()\n\n    def tie_weights(self):\n        """""" Make sure we are sharing the input and output embeddings.\n            Export to TorchScript can\'t handle parameter sharing so we are cloning them instead.\n        """"""\n        self._tie_or_clone_weights(self.lm_head,\n                                   self.transformer.wte)\n\n    def forward(self, input_ids, past=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n                labels=None):\n        transformer_outputs = self.transformer(input_ids,\n                                               past=past,\n                                               attention_mask=attention_mask,\n                                               token_type_ids=token_type_ids,\n                                               position_ids=position_ids,\n                                               head_mask=head_mask)\n        hidden_states = transformer_outputs[0]\n\n        lm_logits = self.lm_head(hidden_states)\n\n        outputs = (lm_logits,) + transformer_outputs[1:]\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = lm_logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)),\n                            shift_labels.view(-1))\n            outputs = (loss,) + outputs\n\n        return outputs  # (loss), lm_logits, presents, (all hidden_states), (attentions)\n\n\n@add_start_docstrings(""""""The GPT2 Model transformer with a language modeling and a multiple-choice classification\nhead on top e.g. for RocStories/SWAG tasks. The two heads are two linear layers.\nThe language modeling head has its weights tied to the input embeddings,\nthe classification head takes as input the input of a specified classification token index in the input sequence).\n"""""", GPT2_START_DOCSTRING, GPT2_INPUTS_DOCSTRING)\nclass GPT2DoubleHeadsModel(GPT2PreTrainedModel):\n    r""""""\n        **mc_token_ids**: (`optional`, default to index of the last token of the input) ``torch.LongTensor`` of shape ``(batch_size, num_choices)``:\n            Index of the classification token in each input sequence.\n            Selected in the range ``[0, input_ids.size(-1) - 1[``.\n        **lm_labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Labels for language modeling.\n            Note that the labels **are shifted** inside the model, i.e. you can set ``lm_labels = input_ids``\n            Indices are selected in ``[-1, 0, ..., config.vocab_size]``\n            All labels set to ``-1`` are ignored (masked), the loss is only\n            computed for labels in ``[0, ..., config.vocab_size]``\n        **mc_labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size)``:\n            Labels for computing the multiple choice classification loss.\n            Indices should be in ``[0, ..., num_choices]`` where `num_choices` is the size of the second dimension\n            of the input tensors. (see `input_ids` above)\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **lm_loss**: (`optional`, returned when ``lm_labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Language modeling loss.\n        **mc_loss**: (`optional`, returned when ``multiple_choice_labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Multiple choice classification loss.\n        **lm_prediction_scores**: ``torch.FloatTensor`` of shape ``(batch_size, num_choices, sequence_length, config.vocab_size)``\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n        **mc_prediction_scores**: ``torch.FloatTensor`` of shape ``(batch_size, num_choices)``\n            Prediction scores of the multiplechoice classification head (scores for each choice before SoftMax).\n        **past**:\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            that contains pre-computed hidden-states (key and values in the attention blocks).\n            Can be used (see `past` input) to speed up sequential decoding.\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        import torch\n        from transformers import GPT2Tokenizer, GPT2DoubleHeadsModel\n        \n        tokenizer = GPT2Tokenizer.from_pretrained(\'gpt2\')\n        model = GPT2DoubleHeadsModel.from_pretrained(\'gpt2\')\n        \n        # Add a [CLS] to the vocabulary (we should train it also!)\n        tokenizer.add_special_tokens({\'cls_token\': \'[CLS]\'})\n        model.resize_token_embeddings(len(tokenizer))  # Update the model embeddings with the new vocabulary size\n        print(tokenizer.cls_token_id, len(tokenizer))  # The newly token the last token of the vocabulary\n        \n        choices = [""Hello, my dog is cute [CLS]"", ""Hello, my cat is cute [CLS]""]\n        encoded_choices = [tokenizer.encode(s) for s in choices]\n        cls_token_location = [tokens.index(tokenizer.cls_token_id) for tokens in encoded_choices]\n\n        input_ids = torch.tensor(encoded_choices).unsqueeze(0)  # Batch size: 1, number of choices: 2\n        mc_token_ids = torch.tensor([cls_token_location])  # Batch size: 1\n\n        outputs = model(input_ids, mc_token_ids=mc_token_ids)\n        lm_prediction_scores, mc_prediction_scores = outputs[:2]\n\n    """"""\n    def __init__(self, config):\n        super(GPT2DoubleHeadsModel, self).__init__(config)\n        self.transformer = GPT2Model(config)\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        self.multiple_choice_head = SequenceSummary(config)\n\n        self.init_weights()\n        self.tie_weights()\n\n    def tie_weights(self):\n        """""" Make sure we are sharing the input and output embeddings.\n            Export to TorchScript can\'t handle parameter sharing so we are cloning them instead.\n        """"""\n        self._tie_or_clone_weights(self.lm_head,\n                                   self.transformer.wte)\n\n    def forward(self, input_ids, past=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n                mc_token_ids=None, lm_labels=None, mc_labels=None):\n        transformer_outputs = self.transformer(input_ids,\n                                               past=past,\n                                               attention_mask=attention_mask,\n                                               token_type_ids=token_type_ids,\n                                               position_ids=position_ids,\n                                               head_mask=head_mask)\n\n        hidden_states = transformer_outputs[0]\n\n        lm_logits = self.lm_head(hidden_states)\n        mc_logits = self.multiple_choice_head(hidden_states, mc_token_ids).squeeze(-1)\n\n        outputs = (lm_logits, mc_logits) + transformer_outputs[1:]\n        if mc_labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(mc_logits.view(-1, mc_logits.size(-1)),\n                            mc_labels.view(-1))\n            outputs = (loss,) + outputs\n        if lm_labels is not None:\n            shift_logits = lm_logits[..., :-1, :].contiguous()\n            shift_labels = lm_labels[..., 1:].contiguous()\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)),\n                            shift_labels.view(-1))\n            outputs = (loss,) + outputs\n\n        return outputs  # (lm loss), (mc loss), lm logits, mc logits, presents, (all hidden_states), (attentions)\n'"
models/transformers/modeling_openai.py,46,"b'# coding=utf-8\n# Copyright 2018 The OpenAI Team Authors and HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""PyTorch OpenAI GPT model.""""""\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport collections\nimport json\nimport logging\nimport math\nimport os\nimport sys\nfrom io import open\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\nfrom torch.nn.parameter import Parameter\n\nfrom .modeling_utils import PreTrainedModel, Conv1D, prune_conv1d_layer, SequenceSummary\nfrom .configuration_openai import OpenAIGPTConfig\nfrom .file_utils import add_start_docstrings\n\nlogger = logging.getLogger(__name__)\n\nOPENAI_GPT_PRETRAINED_MODEL_ARCHIVE_MAP = {""openai-gpt"": ""https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-pytorch_model.bin""}\n\n\ndef load_tf_weights_in_openai_gpt(model, config, openai_checkpoint_folder_path):\n    """""" Load tf pre-trained weights in a pytorch model (from NumPy arrays here)\n    """"""\n    import re\n    import numpy as np\n\n    if \'.ckpt\' in openai_checkpoint_folder_path:\n        openai_checkpoint_folder_path = os.path.dirname(openai_checkpoint_folder_path)\n\n    logger.info(""Loading weights from {}"".format(openai_checkpoint_folder_path))\n\n    names = json.load(open(openai_checkpoint_folder_path + \'/parameters_names.json\', ""r"", encoding=\'utf-8\'))\n    shapes = json.load(open(openai_checkpoint_folder_path + \'/params_shapes.json\', ""r"", encoding=\'utf-8\'))\n    offsets = np.cumsum([np.prod(shape) for shape in shapes])\n    init_params = [np.load(openai_checkpoint_folder_path + \'/params_{}.npy\'.format(n)) for n in range(10)]\n    init_params = np.split(np.concatenate(init_params, 0), offsets)[:-1]\n    init_params = [param.reshape(shape) for param, shape in zip(init_params, shapes)]\n\n    # This was used when we had a single embedding matrix for positions and tokens\n    # init_params[0] = np.concatenate([init_params[1], init_params[0]], 0)\n    # del init_params[1]\n    init_params = [arr.squeeze() for arr in init_params]\n\n    try:\n        assert model.tokens_embed.weight.shape == init_params[1].shape\n        assert model.positions_embed.weight.shape == init_params[0].shape\n    except AssertionError as e:\n        e.args += (model.tokens_embed.weight.shape, init_params[1].shape)\n        e.args += (model.positions_embed.weight.shape, init_params[0].shape)\n        raise\n\n    model.tokens_embed.weight.data = torch.from_numpy(init_params[1])\n    model.positions_embed.weight.data = torch.from_numpy(init_params[0])\n    names.pop(0)\n    # Pop position and token embedding arrays\n    init_params.pop(0)\n    init_params.pop(0)\n\n    for name, array in zip(names, init_params): # names[1:n_transfer], init_params[1:n_transfer]):\n        name = name[6:]  # skip ""model/""\n        assert name[-2:] == "":0""\n        name = name[:-2]\n        name = name.split(\'/\')\n        pointer = model\n        for m_name in name:\n            if re.fullmatch(r\'[A-Za-z]+\\d+\', m_name):\n                l = re.split(r\'(\\d+)\', m_name)\n            else:\n                l = [m_name]\n            if l[0] == \'g\':\n                pointer = getattr(pointer, \'weight\')\n            elif l[0] == \'b\':\n                pointer = getattr(pointer, \'bias\')\n            elif l[0] == \'w\':\n                pointer = getattr(pointer, \'weight\')\n            else:\n                pointer = getattr(pointer, l[0])\n            if len(l) >= 2:\n                num = int(l[1])\n                pointer = pointer[num]\n        try:\n            assert pointer.shape == array.shape\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        try:\n            assert pointer.shape == array.shape\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        logger.info(""Initialize PyTorch weight {}"".format(name))\n        pointer.data = torch.from_numpy(array)\n    return model\n\n\ndef gelu(x):\n    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n\n\ndef swish(x):\n    return x * torch.sigmoid(x)\n\n\nACT_FNS = {""relu"": nn.ReLU, ""swish"": swish, ""gelu"": gelu}\n\n\nclass Attention(nn.Module):\n    def __init__(self, nx, n_ctx, config, scale=False):\n        super(Attention, self).__init__()\n        n_state = nx  # in Attention: n_state=768 (nx=n_embd)\n        # [switch nx => n_state from Block to Attention to keep identical to TF implem]\n        assert n_state % config.n_head == 0\n        self.register_buffer(""bias"", torch.tril(torch.ones(n_ctx, n_ctx)).view(1, 1, n_ctx, n_ctx))\n        self.n_head = config.n_head\n        self.split_size = n_state\n        self.scale = scale\n\n        self.output_attentions = config.output_attentions\n\n        self.c_attn = Conv1D(n_state * 3, nx)\n        self.c_proj = Conv1D(n_state, nx)\n        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n        self.pruned_heads = set()\n\n    def prune_heads(self, heads):\n        if len(heads) == 0:\n            return\n        mask = torch.ones(self.n_head, self.split_size // self.n_head)\n        heads = set(heads) - self.pruned_heads\n        for head in heads:\n            head -= sum(1 if h < head else 0 for h in self.pruned_heads)\n            mask[head] = 0\n        mask = mask.view(-1).contiguous().eq(1)\n        index = torch.arange(len(mask))[mask].long()\n        index_attn = torch.cat([index, index + self.split_size, index + (2*self.split_size)])\n        # Prune conv1d layers\n        self.c_attn = prune_conv1d_layer(self.c_attn, index_attn, dim=1)\n        self.c_proj = prune_conv1d_layer(self.c_proj, index, dim=0)\n        # Update hyper params\n        self.split_size = (self.split_size // self.n_head) * (self.n_head - len(heads))\n        self.n_head = self.n_head - len(heads)\n        self.pruned_heads = self.pruned_heads.union(heads)\n\n    def _attn(self, q, k, v, attention_mask=None, head_mask=None):\n        w = torch.matmul(q, k)\n        if self.scale:\n            w = w / math.sqrt(v.size(-1))\n        # w = w * self.bias + -1e9 * (1 - self.bias)  # TF implem method: mask_attn_weights\n        # XD: self.b may be larger than w, so we need to crop it\n        b = self.bias[:, :, : w.size(-2), : w.size(-1)]\n        w = w * b + - 1e4 * (1 - b)\n\n        if attention_mask is not None:\n            # Apply the attention mask\n            w = w + attention_mask\n\n        w = nn.Softmax(dim=-1)(w)\n        w = self.attn_dropout(w)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            w = w * head_mask\n\n        outputs = [torch.matmul(w, v)]\n        if self.output_attentions:\n            outputs.append(w)\n        return outputs\n\n    def merge_heads(self, x):\n        x = x.permute(0, 2, 1, 3).contiguous()\n        new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)\n        return x.view(*new_x_shape)  # in Tensorflow implem: fct merge_states\n\n    def split_heads(self, x, k=False):\n        new_x_shape = x.size()[:-1] + (self.n_head, x.size(-1) // self.n_head)\n        x = x.view(*new_x_shape)  # in Tensorflow implem: fct split_states\n        if k:\n            return x.permute(0, 2, 3, 1)\n        else:\n            return x.permute(0, 2, 1, 3)\n\n    def forward(self, x, attention_mask=None, head_mask=None):\n        x = self.c_attn(x)\n        query, key, value = x.split(self.split_size, dim=2)\n        query = self.split_heads(query)\n        key = self.split_heads(key, k=True)\n        value = self.split_heads(value)\n\n        attn_outputs = self._attn(query, key, value, attention_mask, head_mask)\n        a = attn_outputs[0]\n\n        a = self.merge_heads(a)\n        a = self.c_proj(a)\n        a = self.resid_dropout(a)\n\n        outputs = [a] + attn_outputs[1:]\n        return outputs  # a, (attentions)\n\n\nclass MLP(nn.Module):\n    def __init__(self, n_state, config):  # in MLP: n_state=3072 (4 * n_embd)\n        super(MLP, self).__init__()\n        nx = config.n_embd\n        self.c_fc = Conv1D(n_state, nx)\n        self.c_proj = Conv1D(nx, n_state)\n        self.act = ACT_FNS[config.afn]\n        self.dropout = nn.Dropout(config.resid_pdrop)\n\n    def forward(self, x):\n        h = self.act(self.c_fc(x))\n        h2 = self.c_proj(h)\n        return self.dropout(h2)\n\n\nclass Block(nn.Module):\n    def __init__(self, n_ctx, config, scale=False):\n        super(Block, self).__init__()\n        nx = config.n_embd\n        self.attn = Attention(nx, n_ctx, config, scale)\n        self.ln_1 = nn.LayerNorm(nx, eps=config.layer_norm_epsilon)\n        self.mlp = MLP(4 * nx, config)\n        self.ln_2 = nn.LayerNorm(nx, eps=config.layer_norm_epsilon)\n\n    def forward(self, x, attention_mask=None, head_mask=None):\n        attn_outputs = self.attn(x, attention_mask=attention_mask, head_mask=head_mask)\n        a = attn_outputs[0]\n\n        n = self.ln_1(x + a)\n        m = self.mlp(n)\n        h = self.ln_2(n + m)\n\n        outputs = [h] + attn_outputs[1:]\n        return outputs\n\n\nclass OpenAIGPTPreTrainedModel(PreTrainedModel):\n    """""" An abstract class to handle weights initialization and\n        a simple interface for dowloading and loading pretrained models.\n    """"""\n    config_class = OpenAIGPTConfig\n    pretrained_model_archive_map = OPENAI_GPT_PRETRAINED_MODEL_ARCHIVE_MAP\n    load_tf_weights = load_tf_weights_in_openai_gpt\n    base_model_prefix = ""transformer""\n\n    def _init_weights(self, module):\n        """""" Initialize the weights.\n        """"""\n        if isinstance(module, (nn.Linear, nn.Embedding, Conv1D)):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if isinstance(module, (nn.Linear, Conv1D)) and module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n\nOPENAI_GPT_START_DOCSTRING = r""""""    OpenAI GPT model was proposed in\n    `Improving Language Understanding by Generative Pre-Training`_\n    by Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever.\n    It\'s a causal (unidirectional) transformer pre-trained using language modeling on a large\n    corpus will long range dependencies, the Toronto Book Corpus.\n\n    This model is a PyTorch `torch.nn.Module`_ sub-class. Use it as a regular PyTorch Module and\n    refer to the PyTorch documentation for all matter related to general usage and behavior.\n\n    .. _`Improving Language Understanding by Generative Pre-Training`:\n        https://openai.com/blog/language-unsupervised/\n\n    .. _`torch.nn.Module`:\n        https://pytorch.org/docs/stable/nn.html#module\n\n    Parameters:\n        config (:class:`~transformers.OpenAIGPTConfig`): Model configuration class with all the parameters of the model.\n            Initializing with a config file does not load the weights associated with the model, only the configuration.\n            Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model weights.\n""""""\n\nOPENAI_GPT_INPUTS_DOCSTRING = r""""""    Inputs:\n        **input_ids**: ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Indices of input sequence tokens in the vocabulary.\n            GPT is a model with absolute position embeddings so it\'s usually advised to pad the inputs on\n            the right rather than the left.\n            Indices can be obtained using :class:`transformers.BPT2Tokenizer`.\n            See :func:`transformers.PreTrainedTokenizer.encode` and\n            :func:`transformers.PreTrainedTokenizer.convert_tokens_to_ids` for details.\n        **attention_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, sequence_length)``:\n            Mask to avoid performing attention on padding token indices.\n            Mask values selected in ``[0, 1]``:\n            ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n        **token_type_ids**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            A parallel sequence of tokens (can be used to indicate various portions of the inputs).\n            The embeddings from these tokens will be summed with the respective token embeddings.\n            Indices are selected in the vocabulary (unlike BERT which has a specific vocabulary for segment indices)\n        **position_ids**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Indices of positions of each input sequence tokens in the position embeddings.\n            Selected in the range ``[0, config.max_position_embeddings - 1]``.\n        **head_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(num_heads,)`` or ``(num_layers, num_heads)``:\n            Mask to nullify selected heads of the self-attention modules.\n            Mask values selected in ``[0, 1]``:\n            ``1`` indicates the head is **not masked**, ``0`` indicates the head is **masked**.\n""""""\n\n@add_start_docstrings(""The bare OpenAI GPT transformer model outputting raw hidden-states without any specific head on top."",\n                      OPENAI_GPT_START_DOCSTRING, OPENAI_GPT_INPUTS_DOCSTRING)\nclass OpenAIGPTModel(OpenAIGPTPreTrainedModel):\n    r""""""\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **last_hidden_state**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, hidden_size)``\n            Sequence of hidden-states at the last layer of the model.\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = OpenAIGPTTokenizer.from_pretrained(\'openai-gpt\')\n        model = OpenAIGPTModel.from_pretrained(\'openai-gpt\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids)\n        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n\n    """"""\n    def __init__(self, config):\n        super(OpenAIGPTModel, self).__init__(config)\n        self.output_attentions = config.output_attentions\n        self.output_hidden_states = config.output_hidden_states\n\n        self.tokens_embed = nn.Embedding(config.vocab_size, config.n_embd)\n        self.positions_embed = nn.Embedding(config.n_positions, config.n_embd)\n        self.drop = nn.Dropout(config.embd_pdrop)\n        self.h = nn.ModuleList([Block(config.n_ctx, config, scale=True) for _ in range(config.n_layer)])\n\n        self.init_weights()\n\n    def _resize_token_embeddings(self, new_num_tokens):\n        self.tokens_embed = self._get_resized_embeddings(self.tokens_embed, new_num_tokens)\n        return self.tokens_embed\n\n    def _prune_heads(self, heads_to_prune):\n        """""" Prunes heads of the model.\n            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n        """"""\n        for layer, heads in heads_to_prune.items():\n            self.h[layer].attn.prune_heads(heads)\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n        if position_ids is None:\n            # This was used when we had a single embedding matrice from position and token embeddings\n            # start = self.config.vocab_size + self.config.n_special\n            # end = start + input_ids.size(-1)\n            # position_ids = torch.arange(start, end, dtype=torch.long, device=input_ids.device)\n            position_ids = torch.arange(input_ids.size(-1), dtype=torch.long, device=input_ids.device)\n            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n\n        # Attention mask.\n        if attention_mask is not None:\n            # We create a 3D attention mask from a 2D tensor mask.\n            # Sizes are [batch_size, 1, 1, to_seq_length]\n            # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n            # this attention mask is more simple than the triangular masking of causal attention\n            # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n\n            # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n            # masked positions, this operation will create a tensor which is 0.0 for\n            # positions we want to attend and -10000.0 for masked positions.\n            # Since we are adding it to the raw scores before the softmax, this is\n            # effectively the same as removing these entirely.\n            attention_mask = attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n            attention_mask = (1.0 - attention_mask) * -10000.0\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # head_mask has shape n_layer x batch x n_heads x N x N\n        if head_mask is not None:\n            if head_mask.dim() == 1:\n                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n                head_mask = head_mask.expand(self.config.n_layer, -1, -1, -1, -1)\n            elif head_mask.dim() == 2:\n                head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)  # We can specify head_mask for each layer\n            head_mask = head_mask.to(dtype=next(self.parameters()).dtype) # switch to fload if need + fp16 compatibility\n        else:\n            head_mask = [None] * self.config.n_layer\n\n        input_shape = input_ids.size()\n        input_ids = input_ids.view(-1, input_ids.size(-1))\n        position_ids = position_ids.view(-1, position_ids.size(-1))\n\n        inputs_embeds = self.tokens_embed(input_ids)\n        position_embeds = self.positions_embed(position_ids)\n        if token_type_ids is not None:\n            token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n            token_type_embeds = self.tokens_embed(token_type_ids)\n        else:\n            token_type_embeds = 0\n        hidden_states = inputs_embeds + position_embeds + token_type_embeds\n        hidden_states = self.drop(hidden_states)\n\n        output_shape = input_shape + (hidden_states.size(-1),)\n\n        all_attentions = ()\n        all_hidden_states = ()\n        for i, block in enumerate(self.h):\n            if self.output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states.view(*output_shape),)\n\n            outputs = block(hidden_states, attention_mask, head_mask[i])\n            hidden_states = outputs[0]\n            if self.output_attentions:\n                all_attentions = all_attentions + (outputs[1],)\n\n        # Add last layer\n        if self.output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_states.view(*output_shape),)\n\n        outputs = (hidden_states.view(*output_shape),)\n        if self.output_hidden_states:\n            outputs = outputs + (all_hidden_states,)\n        if self.output_attentions:\n            outputs = outputs + (all_attentions,)\n        return outputs  # last hidden state, (all hidden states), (all attentions)\n\n\n@add_start_docstrings(""""""OpenAI GPT Model transformer with a language modeling head on top\n(linear layer with weights tied to the input embeddings). """""", OPENAI_GPT_START_DOCSTRING, OPENAI_GPT_INPUTS_DOCSTRING)\nclass OpenAIGPTLMHeadModel(OpenAIGPTPreTrainedModel):\n    r""""""\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Labels for language modeling.\n            Note that the labels **are shifted** inside the model, i.e. you can set ``labels = input_ids``\n            Indices are selected in ``[-1, 0, ..., config.vocab_size]``\n            All labels set to ``-1`` are ignored (masked), the loss is only\n            computed for labels in ``[0, ..., config.vocab_size]``\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Language modeling loss.\n        **prediction_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.vocab_size)``\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = OpenAIGPTTokenizer.from_pretrained(\'openai-gpt\')\n        model = OpenAIGPTLMHeadModel.from_pretrained(\'openai-gpt\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, labels=input_ids)\n        loss, logits = outputs[:2]\n\n    """"""\n    def __init__(self, config):\n        super(OpenAIGPTLMHeadModel, self).__init__(config)\n        self.transformer = OpenAIGPTModel(config)\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n\n        self.init_weights()\n        self.tie_weights()\n\n    def tie_weights(self):\n        """""" Make sure we are sharing the input and output embeddings.\n            Export to TorchScript can\'t handle parameter sharing so we are cloning them instead.\n        """"""\n        self._tie_or_clone_weights(self.lm_head,\n                                   self.transformer.tokens_embed)\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n                labels=None):\n        transformer_outputs = self.transformer(input_ids,\n                                               attention_mask=attention_mask,\n                                               token_type_ids=token_type_ids,\n                                               position_ids=position_ids,\n                                               head_mask=head_mask)\n        hidden_states = transformer_outputs[0]\n        lm_logits = self.lm_head(hidden_states)\n\n        outputs = (lm_logits,) + transformer_outputs[1:]\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = lm_logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)),\n                            shift_labels.view(-1))\n            outputs = (loss,) + outputs\n\n        return outputs  # (loss), lm_logits, (all hidden states), (all attentions)\n\n\n@add_start_docstrings(""""""OpenAI GPT Model transformer with a language modeling and a multiple-choice classification\nhead on top e.g. for RocStories/SWAG tasks. The two heads are two linear layers.\nThe language modeling head has its weights tied to the input embeddings,\nthe classification head takes as input the input of a specified classification token index in the input sequence).\n"""""", OPENAI_GPT_START_DOCSTRING, OPENAI_GPT_INPUTS_DOCSTRING)\nclass OpenAIGPTDoubleHeadsModel(OpenAIGPTPreTrainedModel):\n    r""""""\n        **mc_token_ids**: (`optional`, default to index of the last token of the input) ``torch.LongTensor`` of shape ``(batch_size, num_choices)``:\n            Index of the classification token in each input sequence.\n            Selected in the range ``[0, input_ids.size(-1) - 1[``.\n        **lm_labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Labels for language modeling.\n            Note that the labels **are shifted** inside the model, i.e. you can set ``lm_labels = input_ids``\n            Indices are selected in ``[-1, 0, ..., config.vocab_size]``\n            All labels set to ``-1`` are ignored (masked), the loss is only\n            computed for labels in ``[0, ..., config.vocab_size]``\n        **mc_labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size)``:\n            Labels for computing the multiple choice classification loss.\n            Indices should be in ``[0, ..., num_choices]`` where `num_choices` is the size of the second dimension\n            of the input tensors. (see `input_ids` above)\n\n            `multiple_choice_labels`: optional multiple choice labels: ``torch.LongTensor`` of shape [batch_size]\n                with indices selected in [0, ..., num_choices].\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **lm_loss**: (`optional`, returned when ``lm_labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Language modeling loss.\n        **mc_loss**: (`optional`, returned when ``multiple_choice_labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Multiple choice classification loss.\n        **lm_prediction_scores**: ``torch.FloatTensor`` of shape ``(batch_size, num_choices, sequence_length, config.vocab_size)``\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n        **mc_prediction_scores**: ``torch.FloatTensor`` of shape ``(batch_size, num_choices)``\n            Prediction scores of the multiplechoice classification head (scores for each choice before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = OpenAIGPTTokenizer.from_pretrained(\'openai-gpt\')\n        model = OpenAIGPTDoubleHeadsModel.from_pretrained(\'openai-gpt\')\n        tokenizer.add_special_tokens({\'cls_token\': \'[CLS]\'})  # Add a [CLS] to the vocabulary (we should train it also!)\n        choices = [""Hello, my dog is cute [CLS]"", ""Hello, my cat is cute [CLS]""]\n        input_ids = torch.tensor([tokenizer.encode(s) for s in choices]).unsqueeze(0)  # Batch size 1, 2 choices\n        mc_token_ids = torch.tensor([input_ids.size(-1), input_ids.size(-1)]).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, mc_token_ids=mc_token_ids)\n        lm_prediction_scores, mc_prediction_scores = outputs[:2]\n\n    """"""\n    def __init__(self, config):\n        super(OpenAIGPTDoubleHeadsModel, self).__init__(config)\n\n        self.transformer = OpenAIGPTModel(config)\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        self.multiple_choice_head = SequenceSummary(config)\n\n        self.init_weights()\n        self.tie_weights()\n\n    def tie_weights(self):\n        """""" Make sure we are sharing the input and output embeddings.\n            Export to TorchScript can\'t handle parameter sharing so we are cloning them instead.\n        """"""\n        self._tie_or_clone_weights(self.lm_head,\n                                   self.transformer.tokens_embed)\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n                mc_token_ids=None, lm_labels=None, mc_labels=None):\n        transformer_outputs = self.transformer(input_ids,\n                                               attention_mask=attention_mask,\n                                               token_type_ids=token_type_ids,\n                                               position_ids=position_ids,\n                                               head_mask=head_mask)\n        hidden_states = transformer_outputs[0]\n\n        lm_logits = self.lm_head(hidden_states)\n        mc_logits = self.multiple_choice_head(hidden_states, mc_token_ids).squeeze(-1)\n\n        outputs = (lm_logits, mc_logits) + transformer_outputs[1:]\n        if mc_labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(mc_logits.view(-1, mc_logits.size(-1)),\n                            mc_labels.view(-1))\n            outputs = (loss,) + outputs\n        if lm_labels is not None:\n            shift_logits = lm_logits[..., :-1, :].contiguous()\n            shift_labels = lm_labels[..., 1:].contiguous()\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)),\n                            shift_labels.view(-1))\n            outputs = (loss,) + outputs\n\n        return outputs  # (lm loss), (mc loss), lm logits, mc logits, (all hidden_states), (attentions)\n'"
models/transformers/modeling_roberta.py,42,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""PyTorch RoBERTa model. """"""\n\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport logging\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss, MSELoss\n\nfrom .modeling_bert import BertEmbeddings, BertLayerNorm, BertModel, BertPreTrainedModel, gelu\nfrom .configuration_roberta import RobertaConfig\nfrom .file_utils import add_start_docstrings\n\nlogger = logging.getLogger(__name__)\n\nROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP = {\n    \'roberta-base\': ""https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin"",\n    \'roberta-large\': ""https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-pytorch_model.bin"",\n    \'roberta-large-mnli\': ""https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-mnli-pytorch_model.bin"",\n}\n\nclass RobertaEmbeddings(BertEmbeddings):\n    """"""\n    Same as BertEmbeddings with a tiny tweak for positional embeddings indexing.\n    """"""\n    def __init__(self, config):\n        super(RobertaEmbeddings, self).__init__(config)\n        self.padding_idx = 1\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=self.padding_idx)\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size,\n                                                padding_idx=self.padding_idx)\n\n    def forward(self, input_ids, token_type_ids=None, position_ids=None):\n        seq_length = input_ids.size(1)\n        if position_ids is None:\n            # Position numbers begin at padding_idx+1. Padding symbols are ignored.\n            # cf. fairseq\'s `utils.make_positions`\n            position_ids = torch.arange(self.padding_idx+1, seq_length+self.padding_idx+1, dtype=torch.long, device=input_ids.device)\n            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n        return super(RobertaEmbeddings, self).forward(input_ids,\n                                                      token_type_ids=token_type_ids,\n                                                      position_ids=position_ids)\n\n\nROBERTA_START_DOCSTRING = r""""""    The RoBERTa model was proposed in\n    `RoBERTa: A Robustly Optimized BERT Pretraining Approach`_\n    by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer,\n    Veselin Stoyanov. It is based on Google\'s BERT model released in 2018.\n    \n    It builds on BERT and modifies key hyperparameters, removing the next-sentence pretraining\n    objective and training with much larger mini-batches and learning rates.\n    \n    This implementation is the same as BertModel with a tiny embeddings tweak as well as a setup for Roberta pretrained \n    models.\n\n    This model is a PyTorch `torch.nn.Module`_ sub-class. Use it as a regular PyTorch Module and\n    refer to the PyTorch documentation for all matter related to general usage and behavior.\n\n    .. _`RoBERTa: A Robustly Optimized BERT Pretraining Approach`:\n        https://arxiv.org/abs/1907.11692\n\n    .. _`torch.nn.Module`:\n        https://pytorch.org/docs/stable/nn.html#module\n\n    Parameters:\n        config (:class:`~transformers.RobertaConfig`): Model configuration class with all the parameters of the \n            model. Initializing with a config file does not load the weights associated with the model, only the configuration.\n            Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model weights.\n""""""\n\nROBERTA_INPUTS_DOCSTRING = r""""""\n    Inputs:\n        **input_ids**: ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Indices of input sequence tokens in the vocabulary.\n            To match pre-training, RoBERTa input sequence should be formatted with <s> and </s> tokens as follows:\n\n            (a) For sequence pairs:\n\n                ``tokens:         <s> Is this Jacksonville ? </s> </s> No it is not . </s>``\n\n            (b) For single sequences:\n\n                ``tokens:         <s> the dog is hairy . </s>``\n\n            Fully encoded sequences or sequence pairs can be obtained using the RobertaTokenizer.encode function with \n            the ``add_special_tokens`` parameter set to ``True``.\n\n            RoBERTa is a model with absolute position embeddings so it\'s usually advised to pad the inputs on\n            the right rather than the left.\n\n            See :func:`transformers.PreTrainedTokenizer.encode` and\n            :func:`transformers.PreTrainedTokenizer.convert_tokens_to_ids` for details.\n        **attention_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, sequence_length)``:\n            Mask to avoid performing attention on padding token indices.\n            Mask values selected in ``[0, 1]``:\n            ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n        **token_type_ids**: (`optional` need to be trained) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Optional segment token indices to indicate first and second portions of the inputs.\n            This embedding matrice is not trained (not pretrained during RoBERTa pretraining), you will have to train it\n            during finetuning.\n            Indices are selected in ``[0, 1]``: ``0`` corresponds to a `sentence A` token, ``1``\n            corresponds to a `sentence B` token\n            (see `BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`_ for more details).\n        **position_ids**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Indices of positions of each input sequence tokens in the position embeddings.\n            Selected in the range ``[0, config.max_position_embeddings - 1[``.\n        **head_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(num_heads,)`` or ``(num_layers, num_heads)``:\n            Mask to nullify selected heads of the self-attention modules.\n            Mask values selected in ``[0, 1]``:\n            ``1`` indicates the head is **not masked**, ``0`` indicates the head is **masked**.\n""""""\n\n@add_start_docstrings(""The bare RoBERTa Model transformer outputting raw hidden-states without any specific head on top."",\n                      ROBERTA_START_DOCSTRING, ROBERTA_INPUTS_DOCSTRING)\nclass RobertaModel(BertModel):\n    r""""""\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **last_hidden_state**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, hidden_size)``\n            Sequence of hidden-states at the output of the last layer of the model.\n        **pooler_output**: ``torch.FloatTensor`` of shape ``(batch_size, hidden_size)``\n            Last layer hidden-state of the first token of the sequence (classification token)\n            further processed by a Linear layer and a Tanh activation function. The Linear\n            layer weights are trained from the next sentence prediction (classification)\n            objective during Bert pretraining. This output is usually *not* a good summary\n            of the semantic content of the input, you\'re often better with averaging or pooling\n            the sequence of hidden-states for the whole input sequence.\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = RobertaTokenizer.from_pretrained(\'roberta-base\')\n        model = RobertaModel.from_pretrained(\'roberta-base\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids)\n        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n\n    """"""\n    config_class = RobertaConfig\n    pretrained_model_archive_map = ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP\n    base_model_prefix = ""roberta""\n\n    def __init__(self, config):\n        super(RobertaModel, self).__init__(config)\n\n        self.embeddings = RobertaEmbeddings(config)\n        self.init_weights()\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n        if input_ids[:, 0].sum().item() != 0:\n            logger.warning(""A sequence with no special tokens has been passed to the RoBERTa model. ""\n                           ""This model requires special tokens in order to work. ""\n                           ""Please specify add_special_tokens=True in your tokenize.encode()""\n                           ""or tokenizer.convert_tokens_to_ids()."")\n        return super(RobertaModel, self).forward(input_ids,\n                                                 attention_mask=attention_mask,\n                                                 token_type_ids=token_type_ids,\n                                                 position_ids=position_ids,\n                                                 head_mask=head_mask)\n\n\n@add_start_docstrings(""""""RoBERTa Model with a `language modeling` head on top. """""",\n    ROBERTA_START_DOCSTRING, ROBERTA_INPUTS_DOCSTRING)\nclass RobertaForMaskedLM(BertPreTrainedModel):\n    r""""""\n        **masked_lm_labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Labels for computing the masked language modeling loss.\n            Indices should be in ``[-1, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring)\n            Tokens with indices set to ``-1`` are ignored (masked), the loss is only computed for the tokens with labels\n            in ``[0, ..., config.vocab_size]``\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``masked_lm_labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Masked language modeling loss.\n        **prediction_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.vocab_size)``\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = RobertaTokenizer.from_pretrained(\'roberta-base\')\n        model = RobertaForMaskedLM.from_pretrained(\'roberta-base\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, masked_lm_labels=input_ids)\n        loss, prediction_scores = outputs[:2]\n\n    """"""\n    config_class = RobertaConfig\n    pretrained_model_archive_map = ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP\n    base_model_prefix = ""roberta""\n\n    def __init__(self, config):\n        super(RobertaForMaskedLM, self).__init__(config)\n\n        self.roberta = RobertaModel(config)\n        self.lm_head = RobertaLMHead(config)\n\n        self.init_weights()\n        self.tie_weights()\n\n    def tie_weights(self):\n        """""" Make sure we are sharing the input and output embeddings.\n            Export to TorchScript can\'t handle parameter sharing so we are cloning them instead.\n        """"""\n        self._tie_or_clone_weights(self.lm_head.decoder, self.roberta.embeddings.word_embeddings)\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n                masked_lm_labels=None):\n        outputs = self.roberta(input_ids,\n                               attention_mask=attention_mask,\n                               token_type_ids=token_type_ids,\n                               position_ids=position_ids,\n                               head_mask=head_mask)\n        sequence_output = outputs[0]\n        prediction_scores = self.lm_head(sequence_output)\n\n        outputs = (prediction_scores,) + outputs[2:]  # Add hidden states and attention if they are here\n\n        if masked_lm_labels is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n            outputs = (masked_lm_loss,) + outputs\n\n        return outputs  # (masked_lm_loss), prediction_scores, (hidden_states), (attentions)\n\n\nclass RobertaLMHead(nn.Module):\n    """"""Roberta Head for masked language modeling.""""""\n\n    def __init__(self, config):\n        super(RobertaLMHead, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.layer_norm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n\n        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n\n    def forward(self, features, **kwargs):\n        x = self.dense(features)\n        x = gelu(x)\n        x = self.layer_norm(x)\n\n        # project back to size of vocabulary with bias\n        x = self.decoder(x) + self.bias\n\n        return x\n\n\n@add_start_docstrings(""""""RoBERTa Model transformer with a sequence classification/regression head on top (a linear layer \n    on top of the pooled output) e.g. for GLUE tasks. """""",\n    ROBERTA_START_DOCSTRING, ROBERTA_INPUTS_DOCSTRING)\nclass RobertaForSequenceClassification(BertPreTrainedModel):\n    r""""""\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the sequence classification/regression loss.\n            Indices should be in ``[0, ..., config.num_labels]``.\n            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Classification (or regression if config.num_labels==1) loss.\n        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = RobertaTokenizer.from_pretrained(\'roberta-base\')\n        model = RobertaForSequenceClassification.from_pretrained(\'roberta-base\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, labels=labels)\n        loss, logits = outputs[:2]\n\n    """"""\n    config_class = RobertaConfig\n    pretrained_model_archive_map = ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP\n    base_model_prefix = ""roberta""\n\n    def __init__(self, config):\n        super(RobertaForSequenceClassification, self).__init__(config)\n        self.num_labels = config.num_labels\n\n        self.roberta = RobertaModel(config)\n        self.classifier = RobertaClassificationHead(config)\n    \n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n                labels=None):\n        outputs = self.roberta(input_ids,\n                               attention_mask=attention_mask,\n                               token_type_ids=token_type_ids,\n                               position_ids=position_ids,\n                               head_mask=head_mask)\n        sequence_output = outputs[0]\n        logits = self.classifier(sequence_output)\n\n        outputs = (logits,) + outputs[2:]\n        if labels is not None:\n            if self.num_labels == 1:\n                #  We are doing regression\n                loss_fct = MSELoss()\n                loss = loss_fct(logits.view(-1), labels.view(-1))\n            else:\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            outputs = (loss,) + outputs\n\n        return outputs  # (loss), logits, (hidden_states), (attentions)\n\n@add_start_docstrings(""""""Roberta Model with a multiple choice classification head on top (a linear layer on top of\n    the pooled output and a softmax) e.g. for RocStories/SWAG tasks. """""",\n    ROBERTA_START_DOCSTRING, ROBERTA_INPUTS_DOCSTRING)\nclass RobertaForMultipleChoice(BertPreTrainedModel):\n    r""""""\n    Inputs:\n        **input_ids**: ``torch.LongTensor`` of shape ``(batch_size, num_choices, sequence_length)``:\n            Indices of input sequence tokens in the vocabulary.\n            The second dimension of the input (`num_choices`) indicates the number of choices to score.\n            To match pre-training, RoBerta input sequence should be formatted with [CLS] and [SEP] tokens as follows:\n\n            (a) For sequence pairs:\n\n                ``tokens:         [CLS] is this jack ##son ##ville ? [SEP] [SEP] no it is not . [SEP]``\n\n                ``token_type_ids:   0   0  0    0    0     0       0   0   0     1  1  1  1   1   1``\n\n            (b) For single sequences:\n\n                ``tokens:         [CLS] the dog is hairy . [SEP]``\n\n                ``token_type_ids:   0   0   0   0  0     0   0``\n\n            Indices can be obtained using :class:`transformers.BertTokenizer`.\n            See :func:`transformers.PreTrainedTokenizer.encode` and\n            :func:`transformers.PreTrainedTokenizer.convert_tokens_to_ids` for details.\n        **token_type_ids**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, num_choices, sequence_length)``:\n            Segment token indices to indicate first and second portions of the inputs.\n            The second dimension of the input (`num_choices`) indicates the number of choices to score.\n            Indices are selected in ``[0, 1]``: ``0`` corresponds to a `sentence A` token, ``1``\n        **attention_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, num_choices, sequence_length)``:\n            Mask to avoid performing attention on padding token indices.\n            The second dimension of the input (`num_choices`) indicates the number of choices to score.\n            Mask values selected in ``[0, 1]``:\n            ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n        **head_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(num_heads,)`` or ``(num_layers, num_heads)``:\n            Mask to nullify selected heads of the self-attention modules.\n            Mask values selected in ``[0, 1]``:\n            ``1`` indicates the head is **not masked**, ``0`` indicates the head is **masked**.\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the multiple choice classification loss.\n            Indices should be in ``[0, ..., num_choices]`` where `num_choices` is the size of the second dimension\n            of the input tensors. (see `input_ids` above)\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Classification loss.\n        **classification_scores**: ``torch.FloatTensor`` of shape ``(batch_size, num_choices)`` where `num_choices` is the size of the second dimension\n            of the input tensors. (see `input_ids` above).\n            Classification scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = RobertaTokenizer.from_pretrained(\'roberta-base\')\n        model = RobertaForMultipleChoice.from_pretrained(\'roberta-base\')\n        choices = [""Hello, my dog is cute"", ""Hello, my cat is amazing""]\n        input_ids = torch.tensor([tokenizer.encode(s, add_special_tokens=True) for s in choices]).unsqueeze(0)  # Batch size 1, 2 choices\n        labels = torch.tensor(1).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, labels=labels)\n        loss, classification_scores = outputs[:2]\n\n    """"""\n    config_class = RobertaConfig\n    pretrained_model_archive_map = ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP\n    base_model_prefix = ""roberta""\n\n    def __init__(self, config):\n        super(RobertaForMultipleChoice, self).__init__(config)\n\n        self.roberta = RobertaModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n\n        self.init_weights()\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None,\n                position_ids=None, head_mask=None):\n        num_choices = input_ids.shape[1]\n\n        flat_input_ids = input_ids.view(-1, input_ids.size(-1))\n        flat_position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n        outputs = self.roberta(flat_input_ids, position_ids=flat_position_ids, token_type_ids=flat_token_type_ids,\n                            attention_mask=flat_attention_mask, head_mask=head_mask)\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        reshaped_logits = logits.view(-1, num_choices)\n\n        outputs = (reshaped_logits,) + outputs[2:]  # add hidden states and attention if they are here\n\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(reshaped_logits, labels)\n            outputs = (loss,) + outputs\n\n        return outputs  # (loss), reshaped_logits, (hidden_states), (attentions)\n\n\n\nclass RobertaClassificationHead(nn.Module):\n    """"""Head for sentence-level classification tasks.""""""\n\n    def __init__(self, config):\n        super(RobertaClassificationHead, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n\n    def forward(self, features, **kwargs):\n        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n        x = self.dropout(x)\n        x = self.dense(x)\n        x = torch.tanh(x)\n        x = self.dropout(x)\n        x = self.out_proj(x)\n        return x\n'"
models/transformers/modeling_transfo_xl.py,54,"b'# coding=utf-8\n# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" PyTorch Transformer XL model.\n    Adapted from https://github.com/kimiyoung/transformer-xl.\n    In particular https://github.com/kimiyoung/transformer-xl/blob/master/pytorch/mem_transformer.py\n""""""\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport os\nimport json\nimport math\nimport logging\nimport collections\nimport sys\nfrom io import open\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import CrossEntropyLoss\nfrom torch.nn.parameter import Parameter\n\nfrom .modeling_utils import PreTrainedModel, Conv1D, prune_conv1d_layer, SequenceSummary\nfrom .configuration_transfo_xl import TransfoXLConfig\nfrom .modeling_transfo_xl_utilities import ProjectedAdaptiveLogSoftmax, sample_logits\nfrom .file_utils import add_start_docstrings\n\nlogger = logging.getLogger(__name__)\n\nTRANSFO_XL_PRETRAINED_MODEL_ARCHIVE_MAP = {\n    \'transfo-xl-wt103\': ""https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-pytorch_model.bin"",\n}\n\ndef build_tf_to_pytorch_map(model, config):\n    """""" A map of modules from TF to PyTorch.\n        This time I use a map to keep the PyTorch model as identical to the original PyTorch model as possible.\n    """"""\n    tf_to_pt_map = {}\n\n    if hasattr(model, \'transformer\'):\n        # We are loading in a TransfoXLLMHeadModel => we will load also the Adaptive Softmax\n        tf_to_pt_map.update({\n            ""transformer/adaptive_softmax/cutoff_0/cluster_W"": model.crit.cluster_weight,\n            ""transformer/adaptive_softmax/cutoff_0/cluster_b"": model.crit.cluster_bias})\n        for i, (out_l, proj_l, tie_proj) in enumerate(zip(\n                                model.crit.out_layers,\n                                model.crit.out_projs,\n                                config.tie_projs)):\n            layer_str = ""transformer/adaptive_softmax/cutoff_%d/"" % i\n            if config.tie_weight:\n                tf_to_pt_map.update({\n                    layer_str + \'b\': out_l.bias})\n            else:\n                raise NotImplementedError\n                # I don\'t think this is implemented in the TF code\n                tf_to_pt_map.update({\n                    layer_str + \'lookup_table\': out_l.weight,\n                    layer_str + \'b\': out_l.bias})\n            if not tie_proj:\n                tf_to_pt_map.update({\n                    layer_str + \'proj\': proj_l\n                    })\n        # Now load the rest of the transformer\n        model = model.transformer\n\n    # Embeddings\n    for i, (embed_l, proj_l) in enumerate(zip(model.word_emb.emb_layers, model.word_emb.emb_projs)):\n        layer_str = ""transformer/adaptive_embed/cutoff_%d/"" % i\n        tf_to_pt_map.update({\n            layer_str + \'lookup_table\': embed_l.weight,\n            layer_str + \'proj_W\': proj_l\n            })\n\n    # Transformer blocks\n    for i, b in enumerate(model.layers):\n        layer_str = ""transformer/layer_%d/"" % i\n        tf_to_pt_map.update({\n            layer_str + ""rel_attn/LayerNorm/gamma"": b.dec_attn.layer_norm.weight,\n            layer_str + ""rel_attn/LayerNorm/beta"": b.dec_attn.layer_norm.bias,\n            layer_str + ""rel_attn/o/kernel"": b.dec_attn.o_net.weight,\n            layer_str + ""rel_attn/qkv/kernel"": b.dec_attn.qkv_net.weight,\n            layer_str + ""rel_attn/r/kernel"": b.dec_attn.r_net.weight,\n            layer_str + ""ff/LayerNorm/gamma"": b.pos_ff.layer_norm.weight,\n            layer_str + ""ff/LayerNorm/beta"": b.pos_ff.layer_norm.bias,\n            layer_str + ""ff/layer_1/kernel"": b.pos_ff.CoreNet[0].weight,\n            layer_str + ""ff/layer_1/bias"": b.pos_ff.CoreNet[0].bias,\n            layer_str + ""ff/layer_2/kernel"": b.pos_ff.CoreNet[3].weight,\n            layer_str + ""ff/layer_2/bias"": b.pos_ff.CoreNet[3].bias,\n        })\n\n    # Relative positioning biases\n    if config.untie_r:\n        r_r_list = []\n        r_w_list = []\n        for b in model.layers:\n            r_r_list.append(b.dec_attn.r_r_bias)\n            r_w_list.append(b.dec_attn.r_w_bias)\n    else:\n        r_r_list = [model.r_r_bias]\n        r_w_list = [model.r_w_bias]\n    tf_to_pt_map.update({\n        \'transformer/r_r_bias\': r_r_list,\n        \'transformer/r_w_bias\': r_w_list})\n    return tf_to_pt_map\n\ndef load_tf_weights_in_transfo_xl(model, config, tf_path):\n    """""" Load tf checkpoints in a pytorch model\n    """"""\n    try:\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        logger.error(""Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see ""\n            ""https://www.tensorflow.org/install/ for installation instructions."")\n        raise\n    # Build TF to PyTorch weights loading map\n    tf_to_pt_map = build_tf_to_pytorch_map(model, config)\n\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    tf_weights = {}\n    for name, shape in init_vars:\n        logger.info(""Loading TF weight {} with shape {}"".format(name, shape))\n        array = tf.train.load_variable(tf_path, name)\n        tf_weights[name] = array\n\n    for name, pointer in tf_to_pt_map.items():\n        assert name in tf_weights\n        array = tf_weights[name]\n        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n        # which are not required for using pretrained model\n        if \'kernel\' in name or \'proj\' in name:\n            array = np.transpose(array)\n        if (\'r_r_bias\' in name or \'r_w_bias\' in name) and len(pointer) > 1:\n            # Here we will split the TF weigths\n            assert len(pointer) == array.shape[0]\n            for i, p_i in enumerate(pointer):\n                arr_i = array[i, ...]\n                try:\n                    assert p_i.shape == arr_i.shape\n                except AssertionError as e:\n                    e.args += (p_i.shape, arr_i.shape)\n                    raise\n                logger.info(""Initialize PyTorch weight {} for layer {}"".format(name, i))\n                p_i.data = torch.from_numpy(arr_i)\n        else:\n            try:\n                assert pointer.shape == array.shape\n            except AssertionError as e:\n                e.args += (pointer.shape, array.shape)\n                raise\n            logger.info(""Initialize PyTorch weight {}"".format(name))\n            pointer.data = torch.from_numpy(array)\n        tf_weights.pop(name, None)\n        tf_weights.pop(name + \'/Adam\', None)\n        tf_weights.pop(name + \'/Adam_1\', None)\n\n    logger.info(""Weights not copied to PyTorch model: {}"".format(\', \'.join(tf_weights.keys())))\n    return model\n\n\nclass PositionalEmbedding(nn.Module):\n    def __init__(self, demb):\n        super(PositionalEmbedding, self).__init__()\n\n        self.demb = demb\n\n        inv_freq = 1 / (10000 ** (torch.arange(0.0, demb, 2.0) / demb))\n        self.register_buffer(\'inv_freq\', inv_freq)\n\n    def forward(self, pos_seq, bsz=None):\n        sinusoid_inp = torch.ger(pos_seq, self.inv_freq)\n        pos_emb = torch.cat([sinusoid_inp.sin(), sinusoid_inp.cos()], dim=-1)\n\n        if bsz is not None:\n            return pos_emb[:,None,:].expand(-1, bsz, -1)\n        else:\n            return pos_emb[:,None,:]\n\n\n\nclass PositionwiseFF(nn.Module):\n    def __init__(self, d_model, d_inner, dropout, pre_lnorm=False, layer_norm_epsilon=1e-5):\n        super(PositionwiseFF, self).__init__()\n\n        self.d_model = d_model\n        self.d_inner = d_inner\n        self.dropout = dropout\n\n        self.CoreNet = nn.Sequential(\n            nn.Linear(d_model, d_inner), nn.ReLU(inplace=True),\n            nn.Dropout(dropout),\n            nn.Linear(d_inner, d_model),\n            nn.Dropout(dropout),\n        )\n\n        self.layer_norm = nn.LayerNorm(d_model, eps=layer_norm_epsilon)\n\n        self.pre_lnorm = pre_lnorm\n\n    def forward(self, inp):\n        if self.pre_lnorm:\n            ##### layer normalization + positionwise feed-forward\n            core_out = self.CoreNet(self.layer_norm(inp))\n\n            ##### residual connection\n            output = core_out + inp\n        else:\n            ##### positionwise feed-forward\n            core_out = self.CoreNet(inp)\n\n            ##### residual connection + layer normalization\n            output = self.layer_norm(inp + core_out)\n\n        return output\n\n\nclass RelPartialLearnableMultiHeadAttn(nn.Module):\n    def __init__(self, n_head, d_model, d_head, dropout, dropatt=0,\n                 tgt_len=None, ext_len=None, mem_len=None, pre_lnorm=False,\n                 r_r_bias=None, r_w_bias=None, output_attentions=False,\n                 layer_norm_epsilon=1e-5):\n        super(RelPartialLearnableMultiHeadAttn, self).__init__()\n\n        self.output_attentions = output_attentions\n        self.n_head = n_head\n        self.d_model = d_model\n        self.d_head = d_head\n        self.dropout = dropout\n\n        self.qkv_net = nn.Linear(d_model, 3 * n_head * d_head, bias=False)\n\n        self.drop = nn.Dropout(dropout)\n        self.dropatt = nn.Dropout(dropatt)\n        self.o_net = nn.Linear(n_head * d_head, d_model, bias=False)\n\n        self.layer_norm = nn.LayerNorm(d_model, eps=layer_norm_epsilon)\n\n        self.scale = 1 / (d_head ** 0.5)\n\n        self.pre_lnorm = pre_lnorm\n\n        if r_r_bias is None or r_w_bias is None: # Biases are not shared\n            self.r_r_bias = nn.Parameter(torch.FloatTensor(self.n_head, self.d_head))\n            self.r_w_bias = nn.Parameter(torch.FloatTensor(self.n_head, self.d_head))\n        else:\n            self.r_r_bias = r_r_bias\n            self.r_w_bias = r_w_bias\n\n        self.r_net = nn.Linear(self.d_model, self.n_head * self.d_head, bias=False)\n\n    def _rel_shift(self, x):\n        zero_pad_shape = (x.size(0), 1) + x.size()[2:]\n        zero_pad = torch.zeros(zero_pad_shape, device=x.device, dtype=x.dtype)\n        x_padded = torch.cat([zero_pad, x], dim=1)\n\n        x_padded_shape = (x.size(1) + 1, x.size(0)) + x.size()[2:]\n        x_padded = x_padded.view(*x_padded_shape)\n\n        x = x_padded[1:].view_as(x)\n\n        return x\n\n    def forward(self, w, r, attn_mask=None, mems=None, head_mask=None):\n        qlen, rlen, bsz = w.size(0), r.size(0), w.size(1)\n\n        if mems is not None:\n            cat = torch.cat([mems, w], 0)\n            if self.pre_lnorm:\n                w_heads = self.qkv_net(self.layer_norm(cat))\n            else:\n                w_heads = self.qkv_net(cat)\n            r_head_k = self.r_net(r)\n\n            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)\n            w_head_q = w_head_q[-qlen:]\n        else:\n            if self.pre_lnorm:\n                w_heads = self.qkv_net(self.layer_norm(w))\n            else:\n                w_heads = self.qkv_net(w)\n            r_head_k = self.r_net(r)\n\n            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)\n\n        klen = w_head_k.size(0)\n\n        w_head_q = w_head_q.view(qlen, bsz, self.n_head, self.d_head)           # qlen x bsz x n_head x d_head\n        w_head_k = w_head_k.view(klen, bsz, self.n_head, self.d_head)           # qlen x bsz x n_head x d_head\n        w_head_v = w_head_v.view(klen, bsz, self.n_head, self.d_head)           # qlen x bsz x n_head x d_head\n\n        r_head_k = r_head_k.view(rlen, self.n_head, self.d_head)                # qlen x n_head x d_head\n\n        #### compute attention score\n        rw_head_q = w_head_q + self.r_w_bias                                    # qlen x bsz x n_head x d_head\n        AC = torch.einsum(\'ibnd,jbnd->ijbn\', (rw_head_q, w_head_k))             # qlen x klen x bsz x n_head\n\n        rr_head_q = w_head_q + self.r_r_bias\n        BD = torch.einsum(\'ibnd,jnd->ijbn\', (rr_head_q, r_head_k))              # qlen x klen x bsz x n_head\n        BD = self._rel_shift(BD)\n\n        # [qlen x klen x bsz x n_head]\n        attn_score = AC + BD\n        attn_score.mul_(self.scale)\n\n        #### compute attention probability\n        if attn_mask is not None and torch.sum(attn_mask).item():\n            attn_mask = (attn_mask == 1)  # Switch to bool\n            if attn_mask.dim() == 2:\n                if next(self.parameters()).dtype == torch.float16:\n                    attn_score = attn_score.float().masked_fill(\n                        attn_mask[None,:,:,None], -65000).type_as(attn_score)\n                else:\n                    attn_score = attn_score.float().masked_fill(\n                        attn_mask[None,:,:,None], -1e30).type_as(attn_score)\n            elif attn_mask.dim() == 3:\n                if next(self.parameters()).dtype == torch.float16:\n                    attn_score = attn_score.float().masked_fill(\n                        attn_mask[:,:,:,None], -65000).type_as(attn_score)\n                else:\n                    attn_score = attn_score.float().masked_fill(\n                        attn_mask[:,:,:,None], -1e30).type_as(attn_score)\n\n        # [qlen x klen x bsz x n_head]\n        attn_prob = F.softmax(attn_score, dim=1)\n        attn_prob = self.dropatt(attn_prob)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attn_prob = attn_prob * head_mask\n\n        #### compute attention vector\n        attn_vec = torch.einsum(\'ijbn,jbnd->ibnd\', (attn_prob, w_head_v))\n\n        # [qlen x bsz x n_head x d_head]\n        attn_vec = attn_vec.contiguous().view(\n            attn_vec.size(0), attn_vec.size(1), self.n_head * self.d_head)\n\n        ##### linear projection\n        attn_out = self.o_net(attn_vec)\n        attn_out = self.drop(attn_out)\n\n        if self.pre_lnorm:\n            ##### residual connection\n            outputs = [w + attn_out]\n        else:\n            ##### residual connection + layer normalization\n            outputs = [self.layer_norm(w + attn_out)]\n\n        if self.output_attentions:\n            outputs.append(attn_prob)\n\n        return outputs\n\n\nclass RelPartialLearnableDecoderLayer(nn.Module):\n    def __init__(self, n_head, d_model, d_head, d_inner, dropout, layer_norm_epsilon=1e-5,\n                 **kwargs):\n        super(RelPartialLearnableDecoderLayer, self).__init__()\n\n        self.dec_attn = RelPartialLearnableMultiHeadAttn(n_head, d_model,\n                            d_head, dropout, layer_norm_epsilon=layer_norm_epsilon, **kwargs)\n        self.pos_ff = PositionwiseFF(d_model, d_inner, dropout, \n                                     pre_lnorm=kwargs.get(\'pre_lnorm\'),\n                                     layer_norm_epsilon=layer_norm_epsilon)\n\n    def forward(self, dec_inp, r, dec_attn_mask=None, mems=None, head_mask=None):\n\n        attn_outputs = self.dec_attn(dec_inp, r,\n                               attn_mask=dec_attn_mask,\n                               mems=mems, head_mask=head_mask)\n        ff_output = self.pos_ff(attn_outputs[0])\n\n        outputs = [ff_output] + attn_outputs[1:]\n\n        return outputs\n\n\nclass AdaptiveEmbedding(nn.Module):\n    def __init__(self, n_token, d_embed, d_proj, cutoffs, div_val=1,\n                 sample_softmax=False):\n        super(AdaptiveEmbedding, self).__init__()\n\n        self.n_token = n_token\n        self.d_embed = d_embed\n\n        self.cutoffs = cutoffs + [n_token]\n        self.div_val = div_val\n        self.d_proj = d_proj\n\n        self.emb_scale = d_proj ** 0.5\n\n        self.cutoff_ends = [0] + self.cutoffs\n\n        self.emb_layers = nn.ModuleList()\n        self.emb_projs = nn.ParameterList()\n        if div_val == 1:\n            self.emb_layers.append(\n                nn.Embedding(n_token, d_embed, sparse=sample_softmax>0)\n            )\n            if d_proj != d_embed:\n                self.emb_projs.append(nn.Parameter(torch.FloatTensor(d_proj, d_embed)))\n        else:\n            for i in range(len(self.cutoffs)):\n                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i+1]\n                d_emb_i = d_embed // (div_val ** i)\n                self.emb_layers.append(nn.Embedding(r_idx-l_idx, d_emb_i))\n                self.emb_projs.append(nn.Parameter(torch.FloatTensor(d_proj, d_emb_i)))\n\n    def forward(self, inp):\n        if self.div_val == 1:\n            embed = self.emb_layers[0](inp)\n            if self.d_proj != self.d_embed:\n                embed  = F.linear(embed, self.emb_projs[0])\n        else:\n            param = next(self.parameters())\n            inp_flat = inp.view(-1)\n            emb_flat = torch.zeros([inp_flat.size(0), self.d_proj],\n                dtype=param.dtype, device=param.device)\n            for i in range(len(self.cutoffs)):\n                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]\n\n                mask_i = (inp_flat >= l_idx) & (inp_flat < r_idx)\n                indices_i = mask_i.nonzero().squeeze()\n\n                if indices_i.numel() == 0:\n                    continue\n\n                inp_i = inp_flat.index_select(0, indices_i) - l_idx\n                emb_i = self.emb_layers[i](inp_i)\n                emb_i = F.linear(emb_i, self.emb_projs[i])\n\n                emb_flat.index_copy_(0, indices_i, emb_i)\n\n            embed_shape = inp.size() + (self.d_proj,)\n            embed = emb_flat.view(embed_shape)\n\n        embed.mul_(self.emb_scale)\n\n        return embed\n\n\nclass TransfoXLPreTrainedModel(PreTrainedModel):\n    """""" An abstract class to handle weights initialization and\n        a simple interface for dowloading and loading pretrained models.\n    """"""\n    config_class = TransfoXLConfig\n    pretrained_model_archive_map = TRANSFO_XL_PRETRAINED_MODEL_ARCHIVE_MAP\n    load_tf_weights = load_tf_weights_in_transfo_xl\n    base_model_prefix = ""transformer""\n\n    def _init_weight(self, weight):\n        if self.config.init == \'uniform\':\n            nn.init.uniform_(weight, -self.config.init_range, self.config.init_range)\n        elif self.config.init == \'normal\':\n            nn.init.normal_(weight, 0.0, self.config.init_std)\n\n    def _init_bias(self, bias):\n        nn.init.constant_(bias, 0.0)\n\n    def _init_weights(self, m):\n        """""" Initialize the weights.\n        """"""\n        classname = m.__class__.__name__\n        if classname.find(\'Linear\') != -1:\n            if hasattr(m, \'weight\') and m.weight is not None:\n                self._init_weight(m.weight)\n            if hasattr(m, \'bias\') and m.bias is not None:\n                self._init_bias(m.bias)\n        elif classname.find(\'AdaptiveEmbedding\') != -1:\n            if hasattr(m, \'emb_projs\'):\n                for i in range(len(m.emb_projs)):\n                    if m.emb_projs[i] is not None:\n                        nn.init.normal_(m.emb_projs[i], 0.0, self.config.proj_init_std)\n        elif classname.find(\'Embedding\') != -1:\n            if hasattr(m, \'weight\'):\n                self._init_weight(m.weight)\n        elif classname.find(\'ProjectedAdaptiveLogSoftmax\') != -1:\n            if hasattr(m, \'cluster_weight\') and m.cluster_weight is not None:\n                self._init_weight(m.cluster_weight)\n            if hasattr(m, \'cluster_bias\') and m.cluster_bias is not None:\n                self._init_bias(m.cluster_bias)\n            if hasattr(m, \'out_projs\'):\n                for i in range(len(m.out_projs)):\n                    if m.out_projs[i] is not None:\n                        nn.init.normal_(m.out_projs[i], 0.0, self.config.proj_init_std)\n        elif classname.find(\'LayerNorm\') != -1:\n            if hasattr(m, \'weight\'):\n                nn.init.normal_(m.weight, 1.0, self.config.init_std)\n            if hasattr(m, \'bias\') and m.bias is not None:\n                self._init_bias(m.bias)\n        else:\n            if hasattr(m, \'r_emb\'):\n                self._init_weight(m.r_emb)\n            if hasattr(m, \'r_w_bias\'):\n                self._init_weight(m.r_w_bias)\n            if hasattr(m, \'r_r_bias\'):\n                self._init_weight(m.r_r_bias)\n            if hasattr(m, \'r_bias\'):\n                self._init_bias(m.r_bias)\n\n\nTRANSFO_XL_START_DOCSTRING = r""""""    The Transformer-XL model was proposed in\n    `Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context`_\n    by Zihang Dai*, Zhilin Yang*, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov.\n    It\'s a causal (uni-directional) transformer with relative positioning (sinuso\xc3\xafdal) embeddings which can reuse\n    previously computed hidden-states to attend to longer context (memory).\n    This model also uses adaptive softmax inputs and outputs (tied).\n\n    This model is a PyTorch `torch.nn.Module`_ sub-class. Use it as a regular PyTorch Module and\n    refer to the PyTorch documentation for all matter related to general usage and behavior.\n\n    .. _`Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context`:\n        https://arxiv.org/abs/1901.02860\n\n    .. _`torch.nn.Module`:\n        https://pytorch.org/docs/stable/nn.html#module\n\n    Parameters:\n        config (:class:`~transformers.TransfoXLConfig`): Model configuration class with all the parameters of the model.\n            Initializing with a config file does not load the weights associated with the model, only the configuration.\n            Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model weights.\n""""""\n\nTRANSFO_XL_INPUTS_DOCSTRING = r""""""\n    Inputs:\n        **input_ids**: ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Indices of input sequence tokens in the vocabulary.\n            Transformer-XL is a model with relative position embeddings so you can either pad the inputs on\n            the right or on the left.\n            Indices can be obtained using :class:`transformers.TransfoXLTokenizer`.\n            See :func:`transformers.PreTrainedTokenizer.encode` and\n            :func:`transformers.PreTrainedTokenizer.convert_tokens_to_ids` for details.\n        **mems**: (`optional`)\n            list of ``torch.FloatTensor`` (one for each layer):\n            that contains pre-computed hidden-states (key and values in the attention blocks) as computed by the model\n            (see `mems` output below). Can be used to speed up sequential decoding and attend to longer context.\n        **head_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(num_heads,)`` or ``(num_layers, num_heads)``:\n            Mask to nullify selected heads of the self-attention modules.\n            Mask values selected in ``[0, 1]``:\n            ``1`` indicates the head is **not masked**, ``0`` indicates the head is **masked**.\n""""""\n\n@add_start_docstrings(""The bare Bert Model transformer outputting raw hidden-states without any specific head on top."",\n                      TRANSFO_XL_START_DOCSTRING, TRANSFO_XL_INPUTS_DOCSTRING)\nclass TransfoXLModel(TransfoXLPreTrainedModel):\n    r""""""\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **last_hidden_state**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, hidden_size)``\n            Sequence of hidden-states at the last layer of the model.\n        **mems**:\n            list of ``torch.FloatTensor`` (one for each layer):\n            that contains pre-computed hidden-states (key and values in the attention blocks) as computed by the model\n            (see `mems` input above). Can be used to speed up sequential decoding and attend to longer context.\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = TransfoXLTokenizer.from_pretrained(\'transfo-xl-wt103\')\n        model = TransfoXLModel.from_pretrained(\'transfo-xl-wt103\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids)\n        last_hidden_states, mems = outputs[:2]\n\n    """"""\n    def __init__(self, config):\n        super(TransfoXLModel, self).__init__(config)\n        self.output_attentions = config.output_attentions\n        self.output_hidden_states = config.output_hidden_states\n\n        self.n_token = config.n_token\n\n        self.d_embed = config.d_embed\n        self.d_model = config.d_model\n        self.n_head = config.n_head\n        self.d_head = config.d_head\n\n        self.word_emb = AdaptiveEmbedding(config.n_token, config.d_embed, config.d_model, config.cutoffs,\n                                          div_val=config.div_val)\n\n        self.drop = nn.Dropout(config.dropout)\n\n        self.n_layer = config.n_layer\n\n        self.tgt_len = config.tgt_len\n        self.mem_len = config.mem_len\n        self.ext_len = config.ext_len\n        self.max_klen = config.tgt_len + config.ext_len + config.mem_len\n\n        self.attn_type = config.attn_type\n\n        if not config.untie_r:\n            self.r_w_bias = nn.Parameter(torch.FloatTensor(self.n_head, self.d_head))\n            self.r_r_bias = nn.Parameter(torch.FloatTensor(self.n_head, self.d_head))\n\n        self.layers = nn.ModuleList()\n        if config.attn_type == 0: # the default attention\n            for i in range(config.n_layer):\n                self.layers.append(\n                    RelPartialLearnableDecoderLayer(\n                        config.n_head, config.d_model, config.d_head, config.d_inner, config.dropout,\n                        tgt_len=config.tgt_len, ext_len=config.ext_len, mem_len=config.mem_len,\n                        dropatt=config.dropatt, pre_lnorm=config.pre_lnorm,\n                        r_w_bias=None if config.untie_r else self.r_w_bias,\n                        r_r_bias=None if config.untie_r else self.r_r_bias,\n                        output_attentions=self.output_attentions,\n                        layer_norm_epsilon=config.layer_norm_epsilon)\n                )\n        else: # learnable embeddings and absolute embeddings are not used in our pretrained checkpoints\n            raise NotImplementedError  # Removed them to avoid maintaining dead code\n\n        self.same_length = config.same_length\n        self.clamp_len = config.clamp_len\n\n        if self.attn_type == 0: # default attention\n            self.pos_emb = PositionalEmbedding(self.d_model)\n        else: # learnable embeddings and absolute embeddings\n            raise NotImplementedError  # Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint\n\n        self.init_weights()\n\n    def _resize_token_embeddings(self, new_num_tokens):\n        return self.word_emb\n\n    def backward_compatible(self):\n        self.sample_softmax = -1\n\n    def reset_length(self, tgt_len, ext_len, mem_len):\n        self.tgt_len = tgt_len\n        self.mem_len = mem_len\n        self.ext_len = ext_len\n\n    def _prune_heads(self, heads):\n        logger.info(""Head pruning is not implemented for Transformer-XL model"")\n        pass\n\n    def init_mems(self, data):\n        if self.mem_len > 0:\n            mems = []\n            param = next(self.parameters())\n            for i in range(self.n_layer):\n                empty = torch.zeros(self.mem_len, data.size(1), self.config.d_model,\n                                    dtype=param.dtype, device=param.device)\n                mems.append(empty)\n\n            return mems\n        else:\n            return None\n\n    def _update_mems(self, hids, mems, qlen, mlen):\n        # does not deal with None\n        if mems is None: return None\n\n        # mems is not None\n        assert len(hids) == len(mems), \'len(hids) != len(mems)\'\n\n        # There are `mlen + qlen` steps that can be cached into mems\n        # For the next step, the last `ext_len` of the `qlen` tokens\n        # will be used as the extended context. Hence, we only cache\n        # the tokens from `mlen + qlen - self.ext_len - self.mem_len`\n        # to `mlen + qlen - self.ext_len`.\n        with torch.no_grad():\n            new_mems = []\n            end_idx = mlen + max(0, qlen - 0 - self.ext_len)\n            beg_idx = max(0, end_idx - self.mem_len)\n            for i in range(len(hids)):\n\n                cat = torch.cat([mems[i], hids[i]], dim=0)\n                new_mems.append(cat[beg_idx:end_idx].detach())\n\n        return new_mems\n\n    def forward(self, input_ids, mems=None, head_mask=None):\n        # the original code for Transformer-XL used shapes [len, bsz] but we want a unified interface in the library\n        # so we transpose here from shape [bsz, len] to shape [len, bsz]\n        input_ids = input_ids.transpose(0, 1).contiguous()\n\n        if mems is None:\n            mems = self.init_mems(input_ids)\n\n        qlen, bsz = input_ids.size()\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] (a head_mask for each layer)\n        # and head_mask is converted to shape [num_hidden_layers x qlen x klen x bsz x n_head]\n        if head_mask is not None:\n            if head_mask.dim() == 1:\n                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(0).unsqueeze(0)\n                head_mask = head_mask.expand(self.n_layer, -1, -1, -1, -1)\n            elif head_mask.dim() == 2:\n                head_mask = head_mask.unsqueeze(1).unsqueeze(1).unsqueeze(1)\n            head_mask = head_mask.to(dtype=next(self.parameters()).dtype) # switch to fload if need + fp16 compatibility\n        else:\n            head_mask = [None] * self.n_layer\n\n        word_emb = self.word_emb(input_ids)\n\n        mlen = mems[0].size(0) if mems is not None else 0\n        klen = mlen + qlen\n        if self.same_length:\n            all_ones = word_emb.new_ones((qlen, klen), dtype=torch.uint8)\n            mask_len = klen - self.mem_len\n            if mask_len > 0:\n                mask_shift_len = qlen - mask_len\n            else:\n                mask_shift_len = qlen\n            dec_attn_mask = (torch.triu(all_ones, 1+mlen)\n                    + torch.tril(all_ones, -mask_shift_len))[:, :, None] # -1\n        else:\n            dec_attn_mask = torch.triu(\n                word_emb.new_ones((qlen, klen), dtype=torch.uint8), diagonal=1+mlen)[:,:,None]\n\n        hids = []\n        attentions = []\n        if self.attn_type == 0: # default\n            pos_seq = torch.arange(klen-1, -1, -1.0, device=word_emb.device,\n                                   dtype=word_emb.dtype)\n            if self.clamp_len > 0:\n                pos_seq.clamp_(max=self.clamp_len)\n            pos_emb = self.pos_emb(pos_seq)\n\n            core_out = self.drop(word_emb)\n            pos_emb = self.drop(pos_emb)\n\n            for i, layer in enumerate(self.layers):\n                hids.append(core_out)\n                mems_i = None if mems is None else mems[i]\n                layer_outputs = layer(core_out, pos_emb, dec_attn_mask=dec_attn_mask,\n                                      mems=mems_i, head_mask=head_mask[i])\n                core_out = layer_outputs[0]\n                if self.output_attentions:\n                    attentions.append(layer_outputs[1])\n        else: # learnable embeddings and absolute embeddings\n            raise NotImplementedError  # Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint\n\n        core_out = self.drop(core_out)\n\n        new_mems = self._update_mems(hids, mems, mlen, qlen)\n\n        # We transpose back here to shape [bsz, len, hidden_dim]\n        outputs = [core_out.transpose(0, 1).contiguous(), new_mems]\n        if self.output_hidden_states:\n            # Add last layer and transpose to library standard shape [bsz, len, hidden_dim]\n            hids.append(core_out)\n            hids = list(t.transpose(0, 1).contiguous() for t in hids)\n            outputs.append(hids)\n        if self.output_attentions:\n            # Transpose to library standard shape [bsz, n_heads, query_seq_len, key_seq_len]\n            attentions = list(t.permute(2, 3, 0, 1).contiguous() for t in attentions)\n            outputs.append(attentions)\n\n        return outputs  # last hidden state, new_mems, (all hidden states), (all attentions)\n\n\n@add_start_docstrings(""""""The Transformer-XL Model with a language modeling head on top\n    (adaptive softmax with weights tied to the adaptive input embeddings)"""""",\n    TRANSFO_XL_START_DOCSTRING, TRANSFO_XL_INPUTS_DOCSTRING)\nclass TransfoXLLMHeadModel(TransfoXLPreTrainedModel):\n    r""""""\n        **lm_labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Labels for language modeling.\n            Note that the labels **are shifted** inside the model, i.e. you can set ``lm_labels = input_ids``\n            Indices are selected in ``[-1, 0, ..., config.vocab_size]``\n            All labels set to ``-1`` are ignored (masked), the loss is only\n            computed for labels in ``[0, ..., config.vocab_size]``\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``lm_labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Language modeling loss.\n        **prediction_scores**: ``None`` if ``lm_labels`` is provided else ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.vocab_size)``\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n            We don\'t output them when the loss is computed to speedup adaptive softmax decoding.\n        **mems**:\n            list of ``torch.FloatTensor`` (one for each layer):\n            that contains pre-computed hidden-states (key and values in the attention blocks) as computed by the model\n            (see `mems` input above). Can be used to speed up sequential decoding and attend to longer context.\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = TransfoXLTokenizer.from_pretrained(\'transfo-xl-wt103\')\n        model = TransfoXLLMHeadModel.from_pretrained(\'transfo-xl-wt103\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids)\n        prediction_scores, mems = outputs[:2]\n\n    """"""\n    def __init__(self, config):\n        super(TransfoXLLMHeadModel, self).__init__(config)\n        self.transformer = TransfoXLModel(config)\n        self.sample_softmax = config.sample_softmax\n        # use sampled softmax\n        if config.sample_softmax > 0:\n            self.out_layer = nn.Linear(config.d_model, config.n_token)\n            self.sampler = LogUniformSampler(config.n_token, config.sample_softmax)\n        # use adaptive softmax (including standard softmax)\n        else:\n            self.crit = ProjectedAdaptiveLogSoftmax(config.n_token, config.d_embed, config.d_model,\n                                                    config.cutoffs, div_val=config.div_val)\n        self.init_weights()\n        self.tie_weights()\n\n    def tie_weights(self):\n        """"""\n        Run this to be sure output and input (adaptive) softmax weights are tied\n        """"""\n        # sampled softmax\n        if self.sample_softmax > 0:\n            if self.config.tie_weight:\n                self.out_layer.weight = self.transformer.word_emb.weight\n        # adaptive softmax (including standard softmax)\n        else:\n            if self.config.tie_weight:\n                for i in range(len(self.crit.out_layers)):\n                    self._tie_or_clone_weights(self.crit.out_layers[i],\n                                               self.transformer.word_emb.emb_layers[i])\n            if self.config.tie_projs:\n                for i, tie_proj in enumerate(self.config.tie_projs):\n                    if tie_proj and self.config.div_val == 1 and self.config.d_model != self.config.d_embed:\n                        if self.config.torchscript:\n                            self.crit.out_projs[i] = nn.Parameter(self.transformer.word_emb.emb_projs[0].clone())\n                        else:\n                            self.crit.out_projs[i] = self.transformer.word_emb.emb_projs[0]\n                    elif tie_proj and self.config.div_val != 1:\n                        if self.config.torchscript:\n                            self.crit.out_projs[i] = nn.Parameter(self.transformer.word_emb.emb_projs[i].clone())\n                        else:\n                            self.crit.out_projs[i] = self.transformer.word_emb.emb_projs[i]\n\n    def reset_length(self, tgt_len, ext_len, mem_len):\n        self.transformer.reset_length(tgt_len, ext_len, mem_len)\n\n    def init_mems(self, data):\n        return self.transformer.init_mems(data)\n\n    def forward(self, input_ids, mems=None, head_mask=None, labels=None):\n        bsz = input_ids.size(0)\n        tgt_len = input_ids.size(1)\n\n        transformer_outputs = self.transformer(input_ids, mems=mems, head_mask=head_mask)\n\n        last_hidden = transformer_outputs[0]\n        pred_hid = last_hidden[:, -tgt_len:]\n        outputs = transformer_outputs[1:]\n        if self.sample_softmax > 0 and self.training:\n            assert self.config.tie_weight\n            logit = sample_logits(self.transformer.word_emb, self.out_layer.bias, labels, pred_hid, self.sampler)\n            softmax_output = -F.log_softmax(logit, -1)[:, :, 0]\n            outputs = [softmax_output] + outputs\n            if labels is not None:\n                # TODO: This is not implemented\n                raise NotImplementedError\n        else:\n            softmax_output = self.crit(pred_hid.view(-1, pred_hid.size(-1)), labels)\n            if labels is None:\n                softmax_output = softmax_output.view(bsz, tgt_len, -1)\n                outputs = [softmax_output] + outputs\n            else:\n                softmax_output = softmax_output.view(bsz, tgt_len)\n                outputs = [softmax_output, None] + outputs\n\n        return outputs  # (loss), logits or None if labels is not None (speed up adaptive softmax), new_mems, (all hidden states), (all attentions)\n'"
models/transformers/modeling_transfo_xl_utilities.py,23,"b'# coding=utf-8\n# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" Utilities for PyTorch Transformer XL model.\n    Directly adapted from https://github.com/kimiyoung/transformer-xl.\n""""""\n\nfrom collections import defaultdict\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# CUDA_MAJOR = int(torch.version.cuda.split(\'.\')[0])\n# CUDA_MINOR = int(torch.version.cuda.split(\'.\')[1])\n\nclass ProjectedAdaptiveLogSoftmax(nn.Module):\n    def __init__(self, n_token, d_embed, d_proj, cutoffs, div_val=1,\n                 keep_order=False):\n        super(ProjectedAdaptiveLogSoftmax, self).__init__()\n\n        self.n_token = n_token\n        self.d_embed = d_embed\n        self.d_proj = d_proj\n\n        self.cutoffs = cutoffs + [n_token]\n        self.cutoff_ends = [0] + self.cutoffs\n        self.div_val = div_val\n\n        self.shortlist_size = self.cutoffs[0]\n        self.n_clusters = len(self.cutoffs) - 1\n        self.head_size = self.shortlist_size + self.n_clusters\n\n        if self.n_clusters > 0:\n            self.cluster_weight = nn.Parameter(torch.zeros(self.n_clusters, self.d_embed))\n            self.cluster_bias = nn.Parameter(torch.zeros(self.n_clusters))\n\n        self.out_layers = nn.ModuleList()\n        self.out_projs = nn.ParameterList()\n\n        if div_val == 1:\n            for i in range(len(self.cutoffs)):\n                if d_proj != d_embed:\n                    self.out_projs.append(\n                        nn.Parameter(torch.FloatTensor(d_proj, d_embed))\n                    )\n                else:\n                    self.out_projs.append(None)\n\n            self.out_layers.append(nn.Linear(d_embed, n_token))\n        else:\n            for i in range(len(self.cutoffs)):\n                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i+1]\n                d_emb_i = d_embed // (div_val ** i)\n\n                self.out_projs.append(\n                    nn.Parameter(torch.FloatTensor(d_proj, d_emb_i))\n                )\n\n                self.out_layers.append(nn.Linear(d_emb_i, r_idx-l_idx))\n\n        self.keep_order = keep_order\n\n    def _compute_logit(self, hidden, weight, bias, proj):\n        if proj is None:\n            logit = F.linear(hidden, weight, bias=bias)\n        else:\n            # if CUDA_MAJOR <= 9 and CUDA_MINOR <= 1:\n            proj_hid = F.linear(hidden, proj.t().contiguous())\n            logit = F.linear(proj_hid, weight, bias=bias)\n            # else:\n            #     logit = torch.einsum(\'bd,de,ev->bv\', (hidden, proj, weight.t()))\n            #     if bias is not None:\n            #         logit = logit + bias\n\n        return logit\n\n    def forward(self, hidden, labels=None, keep_order=False):\n        \'\'\'\n            Params:\n                hidden :: [len*bsz x d_proj]\n                labels :: [len*bsz]\n            Return:\n                if labels is None:\n                    out :: [len*bsz] Negative log likelihood\n                else:\n                    out :: [len*bsz x n_tokens] log probabilities of tokens over the vocabulary\n            We could replace this implementation by the native PyTorch one\n            if their\'s had an option to set bias on all clusters in the native one.\n            here: https://github.com/pytorch/pytorch/blob/dbe6a7a9ff1a364a8706bf5df58a1ca96d2fd9da/torch/nn/modules/adaptive.py#L138\n        \'\'\'\n\n        if labels is not None:\n            labels = labels.view(-1)\n            if hidden.size(0) != labels.size(0):\n                raise RuntimeError(\'Input and labels should have the same size \'\n                                \'in the batch dimension.\')\n\n        if self.n_clusters == 0:\n            logit = self._compute_logit(hidden, self.out_layers[0].weight,\n                                        self.out_layers[0].bias, self.out_projs[0])\n            if labels is not None:\n                out = -F.log_softmax(logit, dim=-1) \\\n                        .gather(1, labels.unsqueeze(1)).squeeze(1)\n            else:\n                out = F.log_softmax(logit, dim=-1)\n        else:\n            # construct weights and biases\n            weights, biases = [], []\n            for i in range(len(self.cutoffs)):\n                if self.div_val == 1:\n                    l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]\n                    weight_i = self.out_layers[0].weight[l_idx:r_idx]\n                    bias_i = self.out_layers[0].bias[l_idx:r_idx]\n                else:\n                    weight_i = self.out_layers[i].weight\n                    bias_i = self.out_layers[i].bias\n\n                if i == 0:\n                    weight_i = torch.cat(\n                        [weight_i, self.cluster_weight], dim=0)\n                    bias_i = torch.cat(\n                        [bias_i, self.cluster_bias], dim=0)\n\n                weights.append(weight_i)\n                biases.append(bias_i)\n\n            head_weight, head_bias, head_proj = weights[0], biases[0], self.out_projs[0]\n\n            head_logit = self._compute_logit(hidden, head_weight, head_bias, head_proj)\n            head_logprob = F.log_softmax(head_logit, dim=1)\n\n            if labels is None:\n                out = hidden.new_empty((head_logit.size(0), self.n_token))\n            else:\n                out = torch.zeros_like(labels, dtype=hidden.dtype, device=hidden.device)\n\n            offset = 0\n            cutoff_values = [0] + self.cutoffs\n            for i in range(len(cutoff_values) - 1):\n                l_idx, r_idx = cutoff_values[i], cutoff_values[i + 1]\n\n                if labels is not None:\n                    mask_i = (labels >= l_idx) & (labels < r_idx)\n                    indices_i = mask_i.nonzero().squeeze()\n\n                    if indices_i.numel() == 0:\n                        continue\n\n                    target_i = labels.index_select(0, indices_i) - l_idx\n                    head_logprob_i = head_logprob.index_select(0, indices_i)\n                    hidden_i = hidden.index_select(0, indices_i)\n                else:\n                    hidden_i = hidden\n\n                if i == 0:\n                    if labels is not None:\n                        logprob_i = head_logprob_i.gather(1, target_i[:, None]).squeeze(1)\n                    else:\n                        out[:, :self.cutoffs[0]] = head_logprob[:, :self.cutoffs[0]]\n                else:\n                    weight_i, bias_i, proj_i = weights[i], biases[i], self.out_projs[i]\n\n                    tail_logit_i = self._compute_logit(hidden_i, weight_i, bias_i, proj_i)\n                    tail_logprob_i = F.log_softmax(tail_logit_i, dim=1)\n                    cluster_prob_idx = self.cutoffs[0] + i - 1  # No probability for the head cluster\n                    if labels is not None:\n                        logprob_i = head_logprob_i[:, cluster_prob_idx] \\\n                                + tail_logprob_i.gather(1, target_i[:, None]).squeeze(1)\n                    else:\n                        logprob_i = head_logprob[:, cluster_prob_idx, None] + tail_logprob_i\n                        out[:, l_idx:r_idx] = logprob_i\n\n                if labels is not None:\n                    if (hasattr(self, \'keep_order\') and self.keep_order) or keep_order:\n                        out.index_copy_(0, indices_i, -logprob_i)\n                    else:\n                        out[offset:offset+logprob_i.size(0)].copy_(-logprob_i)\n                    offset += logprob_i.size(0)\n\n        return out\n\n\n    def log_prob(self, hidden):\n        r"""""" Computes log probabilities for all :math:`n\\_classes`\n        From: https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/adaptive.py\n        Args:\n            hidden (Tensor): a minibatch of examples\n        Returns:\n            log-probabilities of for each class :math:`c`\n            in range :math:`0 <= c <= n\\_classes`, where :math:`n\\_classes` is a\n            parameter passed to ``AdaptiveLogSoftmaxWithLoss`` constructor.\n        Shape:\n            - Input: :math:`(N, in\\_features)`\n            - Output: :math:`(N, n\\_classes)`\n        """"""\n        if self.n_clusters == 0:\n            logit = self._compute_logit(hidden, self.out_layers[0].weight,\n                                        self.out_layers[0].bias, self.out_projs[0])\n            return F.log_softmax(logit, dim=-1)\n        else:\n            # construct weights and biases\n            weights, biases = [], []\n            for i in range(len(self.cutoffs)):\n                if self.div_val == 1:\n                    l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]\n                    weight_i = self.out_layers[0].weight[l_idx:r_idx]\n                    bias_i = self.out_layers[0].bias[l_idx:r_idx]\n                else:\n                    weight_i = self.out_layers[i].weight\n                    bias_i = self.out_layers[i].bias\n\n                if i == 0:\n                    weight_i = torch.cat(\n                        [weight_i, self.cluster_weight], dim=0)\n                    bias_i = torch.cat(\n                        [bias_i, self.cluster_bias], dim=0)\n\n                weights.append(weight_i)\n                biases.append(bias_i)\n\n            head_weight, head_bias, head_proj = weights[0], biases[0], self.out_projs[0]\n            head_logit = self._compute_logit(hidden, head_weight, head_bias, head_proj)\n\n            out = hidden.new_empty((head_logit.size(0), self.n_token))\n            head_logprob = F.log_softmax(head_logit, dim=1)\n\n            cutoff_values = [0] + self.cutoffs\n            for i in range(len(cutoff_values) - 1):\n                start_idx, stop_idx = cutoff_values[i], cutoff_values[i + 1]\n\n                if i == 0:\n                    out[:, :self.cutoffs[0]] = head_logprob[:, :self.cutoffs[0]]\n                else:\n                    weight_i, bias_i, proj_i = weights[i], biases[i], self.out_projs[i]\n\n                    tail_logit_i = self._compute_logit(hidden, weight_i, bias_i, proj_i)\n                    tail_logprob_i = F.log_softmax(tail_logit_i, dim=1)\n\n                    logprob_i = head_logprob[:, -i] + tail_logprob_i\n                    out[:, start_idx, stop_idx] = logprob_i\n\n            return out\n\n\nclass LogUniformSampler(object):\n    def __init__(self, range_max, n_sample):\n        """"""\n        Reference : https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/python/ops/candidate_sampling_ops.py\n            `P(class) = (log(class + 2) - log(class + 1)) / log(range_max + 1)`\n\n        expected count can be approximated by 1 - (1 - p)^n\n        and we use a numerically stable version -expm1(num_tries * log1p(-p))\n\n        Our implementation fixes num_tries at 2 * n_sample, and the actual #samples will vary from run to run\n        """"""\n        with torch.no_grad():\n            self.range_max = range_max\n            log_indices = torch.arange(1., range_max+2., 1.).log_()\n            self.dist = (log_indices[1:] - log_indices[:-1]) / log_indices[-1]\n\n            self.log_q = (- (-self.dist.double().log1p_() * 2 * n_sample).expm1_()).log_().float()\n\n        self.n_sample = n_sample\n\n    def sample(self, labels):\n        """"""\n            labels: [b1, b2]\n        Return\n            true_log_probs: [b1, b2]\n            samp_log_probs: [n_sample]\n            neg_samples: [n_sample]\n        """"""\n\n        # neg_samples = torch.empty(0).long()\n        n_sample = self.n_sample\n        n_tries = 2 * n_sample\n\n        with torch.no_grad():\n            neg_samples = torch.multinomial(self.dist, n_tries, replacement=True).unique()\n            device = labels.device\n            neg_samples = neg_samples.to(device)\n            true_log_probs = self.log_q[labels].to(device)\n            samp_log_probs = self.log_q[neg_samples].to(device)\n            return true_log_probs, samp_log_probs, neg_samples\n\ndef sample_logits(embedding, bias, labels, inputs, sampler):\n    """"""\n        embedding: an nn.Embedding layer\n        bias: [n_vocab]\n        labels: [b1, b2]\n        inputs: [b1, b2, n_emb]\n        sampler: you may use a LogUniformSampler\n    Return\n        logits: [b1, b2, 1 + n_sample]\n    """"""\n    true_log_probs, samp_log_probs, neg_samples = sampler.sample(labels)\n    n_sample = neg_samples.size(0)\n    b1, b2 = labels.size(0), labels.size(1)\n    all_ids = torch.cat([labels.view(-1), neg_samples])\n    all_w = embedding(all_ids)\n    true_w = all_w[: -n_sample].view(b1, b2, -1)\n    sample_w = all_w[- n_sample:].view(n_sample, -1)\n\n    all_b = bias[all_ids]\n    true_b = all_b[: -n_sample].view(b1, b2)\n    sample_b = all_b[- n_sample:]\n\n    hit = (labels[:, :, None] == neg_samples).detach()\n\n    true_logits = torch.einsum(\'ijk,ijk->ij\',\n        [true_w, inputs]) + true_b - true_log_probs\n    sample_logits = torch.einsum(\'lk,ijk->ijl\',\n        [sample_w, inputs]) + sample_b - samp_log_probs\n    sample_logits.masked_fill_(hit, -1e30)\n    logits = torch.cat([true_logits[:, :, None], sample_logits], -1)\n\n    return logits\n'"
models/transformers/modeling_utils.py,42,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""PyTorch BERT model.""""""\n\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport copy\nimport json\nimport logging\nimport os\nfrom io import open\n\nimport six\nimport torch\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss\nfrom torch.nn import functional as F\n\nfrom .configuration_utils import PretrainedConfig\nfrom .file_utils import cached_path, WEIGHTS_NAME, TF_WEIGHTS_NAME, TF2_WEIGHTS_NAME\n\nlogger = logging.getLogger(__name__)\n\n\ntry:\n    from torch.nn import Identity\nexcept ImportError:\n    # Older PyTorch compatibility\n    class Identity(nn.Module):\n        r""""""A placeholder identity operator that is argument-insensitive.\n        """"""\n        def __init__(self, *args, **kwargs):\n            super(Identity, self).__init__()\n\n        def forward(self, input):\n            return input\n\nclass PreTrainedModel(nn.Module):\n    r"""""" Base class for all models.\n\n        :class:`~transformers.PreTrainedModel` takes care of storing the configuration of the models and handles methods for loading/downloading/saving models\n        as well as a few methods commons to all models to (i) resize the input embeddings and (ii) prune heads in the self-attention heads.\n\n        Class attributes (overridden by derived classes):\n            - ``config_class``: a class derived from :class:`~transformers.PretrainedConfig` to use as configuration class for this model architecture.\n            - ``pretrained_model_archive_map``: a python ``dict`` of with `short-cut-names` (string) as keys and `url` (string) of associated pretrained weights as values.\n            - ``load_tf_weights``: a python ``method`` for loading a TensorFlow checkpoint in a PyTorch model, taking as arguments:\n\n                - ``model``: an instance of the relevant subclass of :class:`~transformers.PreTrainedModel`,\n                - ``config``: an instance of the relevant subclass of :class:`~transformers.PretrainedConfig`,\n                - ``path``: a path (string) to the TensorFlow checkpoint.\n\n            - ``base_model_prefix``: a string indicating the attribute associated to the base model in derived classes of the same architecture adding modules on top of the base model.\n    """"""\n    config_class = None\n    pretrained_model_archive_map = {}\n    load_tf_weights = lambda model, config, path: None\n    base_model_prefix = """"\n\n    def __init__(self, config, *inputs, **kwargs):\n        super(PreTrainedModel, self).__init__()\n        if not isinstance(config, PretrainedConfig):\n            raise ValueError(\n                ""Parameter config in `{}(config)` should be an instance of class `PretrainedConfig`. ""\n                ""To create a model from a pretrained model use ""\n                ""`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`"".format(\n                    self.__class__.__name__, self.__class__.__name__\n                ))\n        # Save config in model\n        self.config = config\n\n    def _get_resized_embeddings(self, old_embeddings, new_num_tokens=None):\n        """""" Build a resized Embedding Module from a provided token Embedding Module.\n            Increasing the size will add newly initialized vectors at the end\n            Reducing the size will remove vectors from the end\n\n        Args:\n            new_num_tokens: (`optional`) int\n                New number of tokens in the embedding matrix.\n                Increasing the size will add newly initialized vectors at the end\n                Reducing the size will remove vectors from the end\n                If not provided or None: return the provided token Embedding Module.\n        Return: ``torch.nn.Embeddings``\n            Pointer to the resized Embedding Module or the old Embedding Module if new_num_tokens is None\n        """"""\n        if new_num_tokens is None:\n            return old_embeddings\n\n        old_num_tokens, old_embedding_dim = old_embeddings.weight.size()\n        if old_num_tokens == new_num_tokens:\n            return old_embeddings\n\n        # Build new embeddings\n        new_embeddings = nn.Embedding(new_num_tokens, old_embedding_dim)\n        new_embeddings.to(old_embeddings.weight.device)\n\n        # initialize all new embeddings (in particular added tokens)\n        self._init_weights(new_embeddings)\n\n        # Copy word embeddings from the previous weights\n        num_tokens_to_copy = min(old_num_tokens, new_num_tokens)\n        new_embeddings.weight.data[:num_tokens_to_copy, :] = old_embeddings.weight.data[:num_tokens_to_copy, :]\n\n        return new_embeddings\n\n    def _tie_or_clone_weights(self, first_module, second_module):\n        """""" Tie or clone module weights depending of weither we are using TorchScript or not\n        """"""\n        if self.config.torchscript:\n            first_module.weight = nn.Parameter(second_module.weight.clone())\n        else:\n            first_module.weight = second_module.weight\n\n        if hasattr(first_module, \'bias\') and first_module.bias is not None:\n            first_module.bias.data = torch.nn.functional.pad(\n                first_module.bias.data,\n                (0, first_module.weight.shape[0] - first_module.bias.shape[0]),\n                \'constant\',\n                0\n            )\n\n    def _tie_or_clone_data(self, first_module, second_module):\n        """""" Tie or clone module weights depending of weither we are using TorchScript or not\n        """"""\n\n        if self.config.torchscript:\n            first_module.weight.data = nn.Parameter(second_module.weight.data.t().clone())\n        else:\n            first_module.weight.data = second_module.weight.data.t()\n        if hasattr(first_module, \'bias\') and first_module.bias is not None:\n            first_module.bias.data = torch.nn.functional.pad(\n                first_module.bias.data,\n                (0, first_module.weight.shape[0] - first_module.bias.shape[0]),\n                \'constant\',\n                0\n            )\n\n    def resize_token_embeddings(self, new_num_tokens=None):\n        """""" Resize input token embeddings matrix of the model if new_num_tokens != config.vocab_size.\n        Take care of tying weights embeddings afterwards if the model class has a `tie_weights()` method.\n\n        Arguments:\n\n            new_num_tokens: (`optional`) int:\n                New number of tokens in the embedding matrix. Increasing the size will add newly initialized vectors at the end. Reducing the size will remove vectors from the end.\n                If not provided or None: does nothing and just returns a pointer to the input tokens ``torch.nn.Embeddings`` Module of the model.\n\n        Return: ``torch.nn.Embeddings``\n            Pointer to the input tokens Embeddings Module of the model\n        """"""\n        base_model = getattr(self, self.base_model_prefix, self)  # get the base model if needed\n        model_embeds = base_model._resize_token_embeddings(new_num_tokens)\n        if new_num_tokens is None:\n            return model_embeds\n\n        # Update base model and current model config\n        self.config.vocab_size = new_num_tokens\n        base_model.vocab_size = new_num_tokens\n\n        # Tie weights again if needed\n        if hasattr(self, \'tie_weights\'):\n            self.tie_weights()\n\n        return model_embeds\n\n    def init_weights(self):\n        """""" Initialize and prunes weights if needed. """"""\n        # Initialize weights\n        self.apply(self._init_weights)\n\n        # Prune heads if needed\n        if self.config.pruned_heads:\n            self.prune_heads(self.config.pruned_heads)\n\n    def prune_heads(self, heads_to_prune):\n        """""" Prunes heads of the base model.\n\n            Arguments:\n\n                heads_to_prune: dict with keys being selected layer indices (`int`) and associated values being the list of heads to prune in said layer (list of `int`).\n                E.g. {1: [0, 2], 2: [2, 3]} will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.\n        """"""\n        base_model = getattr(self, self.base_model_prefix, self)  # get the base model if needed\n\n        # save new sets of pruned heads as union of previously stored pruned heads and newly pruned heads\n        for layer, heads in heads_to_prune.items():\n            union_heads = set(self.config.pruned_heads.get(layer, [])) | set(heads)\n            self.config.pruned_heads[layer] = list(union_heads)  # Unfortunately we have to store it as list for JSON\n\n        base_model._prune_heads(heads_to_prune)\n\n    def save_pretrained(self, save_directory):\n        """""" Save a model and its configuration file to a directory, so that it\n            can be re-loaded using the `:func:`~transformers.PreTrainedModel.from_pretrained`` class method.\n        """"""\n        assert os.path.isdir(save_directory), ""Saving path should be a directory where the model and configuration can be saved""\n\n        # Only save the model it-self if we are using distributed training\n        model_to_save = self.module if hasattr(self, \'module\') else self\n\n        # Save configuration file\n        model_to_save.config.save_pretrained(save_directory)\n\n        # If we save using the predefined names, we can load using `from_pretrained`\n        output_model_file = os.path.join(save_directory, WEIGHTS_NAME)\n        torch.save(model_to_save.state_dict(), output_model_file)\n        logger.info(""Model weights saved in {}"".format(output_model_file))\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n        r""""""Instantiate a pretrained pytorch model from a pre-trained model configuration.\n\n        The model is set in evaluation mode by default using ``model.eval()`` (Dropout modules are deactivated)\n        To train the model, you should first set it back in training mode with ``model.train()``\n\n        The warning ``Weights from XXX not initialized from pretrained model`` means that the weights of XXX do not come pre-trained with the rest of the model.\n        It is up to you to train those weights with a downstream fine-tuning task.\n\n        The warning ``Weights from XXX not used in YYY`` means that the layer XXX is not used by YYY, therefore those weights are discarded.\n\n        Parameters:\n            pretrained_model_name_or_path: either:\n\n                - a string with the `shortcut name` of a pre-trained model to load from cache or download, e.g.: ``bert-base-uncased``.\n                - a path to a `directory` containing model weights saved using :func:`~transformers.PreTrainedModel.save_pretrained`, e.g.: ``./my_model_directory/``.\n                - a path or url to a `tensorflow index checkpoint file` (e.g. `./tf_model/model.ckpt.index`). In this case, ``from_tf`` should be set to True and a configuration object should be provided as ``config`` argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n                - None if you are both providing the configuration and state dictionary (resp. with keyword arguments ``config`` and ``state_dict``)\n\n            model_args: (`optional`) Sequence of positional arguments:\n                All remaning positional arguments will be passed to the underlying model\'s ``__init__`` method\n\n            config: (`optional`) instance of a class derived from :class:`~transformers.PretrainedConfig`:\n                Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:\n\n                - the model is a model provided by the library (loaded with the ``shortcut-name`` string of a pretrained model), or\n                - the model was saved using :func:`~transformers.PreTrainedModel.save_pretrained` and is reloaded by suppling the save directory.\n                - the model is loaded by suppling a local directory as ``pretrained_model_name_or_path`` and a configuration JSON file named `config.json` is found in the directory.\n\n            state_dict: (`optional`) dict:\n                an optional state dictionnary for the model to use instead of a state dictionary loaded from saved weights file.\n                This option can be used if you want to create a model from a pretrained configuration but load your own weights.\n                In this case though, you should check if using :func:`~transformers.PreTrainedModel.save_pretrained` and :func:`~transformers.PreTrainedModel.from_pretrained` is not a simpler option.\n\n            cache_dir: (`optional`) string:\n                Path to a directory in which a downloaded pre-trained model\n                configuration should be cached if the standard cache should not be used.\n\n            force_download: (`optional`) boolean, default False:\n                Force to (re-)download the model weights and configuration files and override the cached versions if they exists.\n\n            proxies: (`optional`) dict, default None:\n                A dictionary of proxy servers to use by protocol or endpoint, e.g.: {\'http\': \'foo.bar:3128\', \'http://hostname\': \'foo.bar:4012\'}.\n                The proxies are used on each request.\n\n            output_loading_info: (`optional`) boolean:\n                Set to ``True`` to also return a dictionnary containing missing keys, unexpected keys and error messages.\n\n            kwargs: (`optional`) Remaining dictionary of keyword arguments:\n                Can be used to update the configuration object (after it being loaded) and initiate the model. (e.g. ``output_attention=True``). Behave differently depending on whether a `config` is provided or automatically loaded:\n\n                - If a configuration is provided with ``config``, ``**kwargs`` will be directly passed to the underlying model\'s ``__init__`` method (we assume all relevant updates to the configuration have already been done)\n                - If a configuration is not provided, ``kwargs`` will be first passed to the configuration class initialization function (:func:`~transformers.PretrainedConfig.from_pretrained`). Each key of ``kwargs`` that corresponds to a configuration attribute will be used to override said attribute with the supplied ``kwargs`` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model\'s ``__init__`` function.\n\n        Examples::\n\n            model = BertModel.from_pretrained(\'bert-base-uncased\')    # Download model and configuration from S3 and cache.\n            model = BertModel.from_pretrained(\'./test/saved_model/\')  # E.g. model was saved using `save_pretrained(\'./test/saved_model/\')`\n            model = BertModel.from_pretrained(\'bert-base-uncased\', output_attention=True)  # Update configuration during loading\n            assert model.config.output_attention == True\n            # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n            config = BertConfig.from_json_file(\'./tf_model/my_tf_model_config.json\')\n            model = BertModel.from_pretrained(\'./tf_model/my_tf_checkpoint.ckpt.index\', from_tf=True, config=config)\n\n        """"""\n        config = kwargs.pop(\'config\', None)\n        state_dict = kwargs.pop(\'state_dict\', None)\n        cache_dir = kwargs.pop(\'cache_dir\', None)\n        from_tf = kwargs.pop(\'from_tf\', False)\n        force_download = kwargs.pop(\'force_download\', False)\n        proxies = kwargs.pop(\'proxies\', None)\n        output_loading_info = kwargs.pop(\'output_loading_info\', False)\n\n        # Load config\n        if config is None:\n            config, model_kwargs = cls.config_class.from_pretrained(\n                pretrained_model_name_or_path, *model_args,\n                cache_dir=cache_dir, return_unused_kwargs=True,\n                force_download=force_download,\n                **kwargs\n            )\n        else:\n            model_kwargs = kwargs\n\n        # Load model\n        if pretrained_model_name_or_path is not None:\n            if pretrained_model_name_or_path in cls.pretrained_model_archive_map:\n                archive_file = cls.pretrained_model_archive_map[pretrained_model_name_or_path]\n            elif os.path.isdir(pretrained_model_name_or_path):\n                if from_tf and os.path.isfile(os.path.join(pretrained_model_name_or_path, TF_WEIGHTS_NAME + "".index"")):\n                    # Load from a TF 1.0 checkpoint\n                    archive_file = os.path.join(pretrained_model_name_or_path, TF_WEIGHTS_NAME + "".index"")\n                elif from_tf and os.path.isfile(os.path.join(pretrained_model_name_or_path, TF2_WEIGHTS_NAME)):\n                    # Load from a TF 2.0 checkpoint\n                    archive_file = os.path.join(pretrained_model_name_or_path, TF2_WEIGHTS_NAME)\n                elif os.path.isfile(os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)):\n                    # Load from a PyTorch checkpoint\n                    archive_file = os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)\n                else:\n                    raise EnvironmentError(""Error no file named {} found in directory {} or `from_tf` set to False"".format(\n                        [WEIGHTS_NAME, TF2_WEIGHTS_NAME, TF_WEIGHTS_NAME + "".index""],\n                        pretrained_model_name_or_path))\n            elif os.path.isfile(pretrained_model_name_or_path):\n                archive_file = pretrained_model_name_or_path\n            else:\n                assert from_tf, ""Error finding file {}, no file or TF 1.X checkpoint found"".format(pretrained_model_name_or_path)\n                archive_file = pretrained_model_name_or_path + "".index""\n\n            # redirect to the cache, if necessary\n            try:\n                resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir, force_download=force_download, proxies=proxies)\n            except EnvironmentError:\n                if pretrained_model_name_or_path in cls.pretrained_model_archive_map:\n                    msg = ""Couldn\'t reach server at \'{}\' to download pretrained weights."".format(\n                            archive_file)\n                else:\n                    msg = ""Model name \'{}\' was not found in model name list ({}). "" \\\n                        ""We assumed \'{}\' was a path or url to model weight files named one of {} but "" \\\n                        ""couldn\'t find any such file at this path or url."".format(\n                            pretrained_model_name_or_path,\n                            \', \'.join(cls.pretrained_model_archive_map.keys()),\n                            archive_file,\n                            [WEIGHTS_NAME, TF2_WEIGHTS_NAME, TF_WEIGHTS_NAME])\n                raise EnvironmentError(msg)\n\n            if resolved_archive_file == archive_file:\n                logger.info(""loading weights file {}"".format(archive_file))\n            else:\n                logger.info(""loading weights file {} from cache at {}"".format(\n                    archive_file, resolved_archive_file))\n        else:\n            resolved_archive_file = None\n\n        # Instantiate model.\n        model = cls(config, *model_args, **model_kwargs)\n\n        if state_dict is None and not from_tf:\n            state_dict = torch.load(resolved_archive_file, map_location=\'cpu\')\n\n        missing_keys = []\n        unexpected_keys = []\n        error_msgs = []\n\n        if from_tf:\n            if resolved_archive_file.endswith(\'.index\'):\n                # Load from a TensorFlow 1.X checkpoint - provided by original authors\n                model = cls.load_tf_weights(model, config, resolved_archive_file[:-6])  # Remove the \'.index\'\n            else:\n                # Load from our TensorFlow 2.0 checkpoints\n                try:\n                    from transformers import load_tf2_checkpoint_in_pytorch_model\n                    model = load_tf2_checkpoint_in_pytorch_model(model, resolved_archive_file, allow_missing_keys=True)\n                except ImportError as e:\n                    logger.error(""Loading a TensorFlow model in PyTorch, requires both PyTorch and TensorFlow to be installed. Please see ""\n                        ""https://pytorch.org/ and https://www.tensorflow.org/install/ for installation instructions."")\n                    raise e\n        else:\n            # Convert old format to new format if needed from a PyTorch state_dict\n            old_keys = []\n            new_keys = []\n            for key in state_dict.keys():\n                new_key = None\n                if \'gamma\' in key:\n                    new_key = key.replace(\'gamma\', \'weight\')\n                if \'beta\' in key:\n                    new_key = key.replace(\'beta\', \'bias\')\n                if new_key:\n                    old_keys.append(key)\n                    new_keys.append(new_key)\n            for old_key, new_key in zip(old_keys, new_keys):\n                state_dict[new_key] = state_dict.pop(old_key)\n\n            # copy state_dict so _load_from_state_dict can modify it\n            metadata = getattr(state_dict, \'_metadata\', None)\n            state_dict = state_dict.copy()\n            if metadata is not None:\n                state_dict._metadata = metadata\n\n            def load(module, prefix=\'\'):\n                local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n                module._load_from_state_dict(\n                    state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n                for name, child in module._modules.items():\n                    if child is not None:\n                        load(child, prefix + name + \'.\')\n\n            # Make sure we are able to load base models as well as derived models (with heads)\n            start_prefix = \'\'\n            model_to_load = model\n            if not hasattr(model, cls.base_model_prefix) and any(s.startswith(cls.base_model_prefix) for s in state_dict.keys()):\n                start_prefix = cls.base_model_prefix + \'.\'\n            if hasattr(model, cls.base_model_prefix) and not any(s.startswith(cls.base_model_prefix) for s in state_dict.keys()):\n                model_to_load = getattr(model, cls.base_model_prefix)\n\n            load(model_to_load, prefix=start_prefix)\n            if len(missing_keys) > 0:\n                logger.info(""Weights of {} not initialized from pretrained model: {}"".format(\n                    model.__class__.__name__, missing_keys))\n            if len(unexpected_keys) > 0:\n                logger.info(""Weights from pretrained model not used in {}: {}"".format(\n                    model.__class__.__name__, unexpected_keys))\n            if len(error_msgs) > 0:\n                raise RuntimeError(\'Error(s) in loading state_dict for {}:\\n\\t{}\'.format(\n                                model.__class__.__name__, ""\\n\\t"".join(error_msgs)))\n\n        if hasattr(model, \'tie_weights\'):\n            model.tie_weights()  # make sure word embedding weights are still tied\n\n        # Set model in evaluation mode to desactivate DropOut modules by default\n        model.eval()\n\n        if output_loading_info:\n            loading_info = {""missing_keys"": missing_keys, ""unexpected_keys"": unexpected_keys, ""error_msgs"": error_msgs}\n            return model, loading_info\n\n        return model\n\n\nclass Conv1D(nn.Module):\n    def __init__(self, nf, nx):\n        """""" Conv1D layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2)\n            Basically works like a Linear layer but the weights are transposed\n        """"""\n        super(Conv1D, self).__init__()\n        self.nf = nf\n        w = torch.empty(nx, nf)\n        nn.init.normal_(w, std=0.02)\n        self.weight = nn.Parameter(w)\n        self.bias = nn.Parameter(torch.zeros(nf))\n\n    def forward(self, x):\n        size_out = x.size()[:-1] + (self.nf,)\n        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n        x = x.view(*size_out)\n        return x\n\n\nclass PoolerStartLogits(nn.Module):\n    """""" Compute SQuAD start_logits from sequence hidden states. """"""\n    def __init__(self, config):\n        super(PoolerStartLogits, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, 1)\n\n    def forward(self, hidden_states, p_mask=None):\n        """""" Args:\n            **p_mask**: (`optional`) ``torch.FloatTensor`` of shape `(batch_size, seq_len)`\n                invalid position mask such as query and special symbols (PAD, SEP, CLS)\n                1.0 means token should be masked.\n        """"""\n        x = self.dense(hidden_states).squeeze(-1)\n\n        if p_mask is not None:\n            if next(self.parameters()).dtype == torch.float16:\n                x = x * (1 - p_mask) - 65500 * p_mask\n            else:\n                x = x * (1 - p_mask) - 1e30 * p_mask\n\n        return x\n\n\nclass PoolerEndLogits(nn.Module):\n    """""" Compute SQuAD end_logits from sequence hidden states and start token hidden state.\n    """"""\n    def __init__(self, config):\n        super(PoolerEndLogits, self).__init__()\n        self.dense_0 = nn.Linear(config.hidden_size * 2, config.hidden_size)\n        self.activation = nn.Tanh()\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dense_1 = nn.Linear(config.hidden_size, 1)\n\n    def forward(self, hidden_states, start_states=None, start_positions=None, p_mask=None):\n        """""" Args:\n            One of ``start_states``, ``start_positions`` should be not None.\n            If both are set, ``start_positions`` overrides ``start_states``.\n\n            **start_states**: ``torch.LongTensor`` of shape identical to hidden_states\n                hidden states of the first tokens for the labeled span.\n            **start_positions**: ``torch.LongTensor`` of shape ``(batch_size,)``\n                position of the first token for the labeled span:\n            **p_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, seq_len)``\n                Mask of invalid position such as query and special symbols (PAD, SEP, CLS)\n                1.0 means token should be masked.\n        """"""\n        assert start_states is not None or start_positions is not None, ""One of start_states, start_positions should be not None""\n        if start_positions is not None:\n            slen, hsz = hidden_states.shape[-2:]\n            start_positions = start_positions[:, None, None].expand(-1, -1, hsz) # shape (bsz, 1, hsz)\n            start_states = hidden_states.gather(-2, start_positions) # shape (bsz, 1, hsz)\n            start_states = start_states.expand(-1, slen, -1) # shape (bsz, slen, hsz)\n\n        x = self.dense_0(torch.cat([hidden_states, start_states], dim=-1))\n        x = self.activation(x)\n        x = self.LayerNorm(x)\n        x = self.dense_1(x).squeeze(-1)\n\n        if p_mask is not None:\n            if next(self.parameters()).dtype == torch.float16:\n                x = x * (1 - p_mask) - 65500 * p_mask\n            else:\n                x = x * (1 - p_mask) - 1e30 * p_mask\n\n        return x\n\n\nclass PoolerAnswerClass(nn.Module):\n    """""" Compute SQuAD 2.0 answer class from classification and start tokens hidden states. """"""\n    def __init__(self, config):\n        super(PoolerAnswerClass, self).__init__()\n        self.dense_0 = nn.Linear(config.hidden_size * 2, config.hidden_size)\n        self.activation = nn.Tanh()\n        self.dense_1 = nn.Linear(config.hidden_size, 1, bias=False)\n\n    def forward(self, hidden_states, start_states=None, start_positions=None, cls_index=None):\n        """"""\n        Args:\n            One of ``start_states``, ``start_positions`` should be not None.\n            If both are set, ``start_positions`` overrides ``start_states``.\n\n            **start_states**: ``torch.LongTensor`` of shape identical to ``hidden_states``.\n                hidden states of the first tokens for the labeled span.\n            **start_positions**: ``torch.LongTensor`` of shape ``(batch_size,)``\n                position of the first token for the labeled span.\n            **cls_index**: torch.LongTensor of shape ``(batch_size,)``\n                position of the CLS token. If None, take the last token.\n\n            note(Original repo):\n                no dependency on end_feature so that we can obtain one single `cls_logits`\n                for each sample\n        """"""\n        hsz = hidden_states.shape[-1]\n        assert start_states is not None or start_positions is not None, ""One of start_states, start_positions should be not None""\n        if start_positions is not None:\n            start_positions = start_positions[:, None, None].expand(-1, -1, hsz) # shape (bsz, 1, hsz)\n            start_states = hidden_states.gather(-2, start_positions).squeeze(-2) # shape (bsz, hsz)\n\n        if cls_index is not None:\n            cls_index = cls_index[:, None, None].expand(-1, -1, hsz) # shape (bsz, 1, hsz)\n            cls_token_state = hidden_states.gather(-2, cls_index).squeeze(-2) # shape (bsz, hsz)\n        else:\n            cls_token_state = hidden_states[:, -1, :] # shape (bsz, hsz)\n\n        x = self.dense_0(torch.cat([start_states, cls_token_state], dim=-1))\n        x = self.activation(x)\n        x = self.dense_1(x).squeeze(-1)\n\n        return x\n\n\nclass SQuADHead(nn.Module):\n    r"""""" A SQuAD head inspired by XLNet.\n\n    Parameters:\n        config (:class:`~transformers.XLNetConfig`): Model configuration class with all the parameters of the model.\n\n    Inputs:\n        **hidden_states**: ``torch.FloatTensor`` of shape ``(batch_size, seq_len, hidden_size)``\n            hidden states of sequence tokens\n        **start_positions**: ``torch.LongTensor`` of shape ``(batch_size,)``\n            position of the first token for the labeled span.\n        **end_positions**: ``torch.LongTensor`` of shape ``(batch_size,)``\n            position of the last token for the labeled span.\n        **cls_index**: torch.LongTensor of shape ``(batch_size,)``\n            position of the CLS token. If None, take the last token.\n        **is_impossible**: ``torch.LongTensor`` of shape ``(batch_size,)``\n            Whether the question has a possible answer in the paragraph or not.\n        **p_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, seq_len)``\n            Mask of invalid position such as query and special symbols (PAD, SEP, CLS)\n            1.0 means token should be masked.\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned if both ``start_positions`` and ``end_positions`` are provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Classification loss as the sum of start token, end token (and is_impossible if provided) classification losses.\n        **start_top_log_probs**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)\n            ``torch.FloatTensor`` of shape ``(batch_size, config.start_n_top)``\n            Log probabilities for the top config.start_n_top start token possibilities (beam-search).\n        **start_top_index**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)\n            ``torch.LongTensor`` of shape ``(batch_size, config.start_n_top)``\n            Indices for the top config.start_n_top start token possibilities (beam-search).\n        **end_top_log_probs**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)\n            ``torch.FloatTensor`` of shape ``(batch_size, config.start_n_top * config.end_n_top)``\n            Log probabilities for the top ``config.start_n_top * config.end_n_top`` end token possibilities (beam-search).\n        **end_top_index**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)\n            ``torch.LongTensor`` of shape ``(batch_size, config.start_n_top * config.end_n_top)``\n            Indices for the top ``config.start_n_top * config.end_n_top`` end token possibilities (beam-search).\n        **cls_logits**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)\n            ``torch.FloatTensor`` of shape ``(batch_size,)``\n            Log probabilities for the ``is_impossible`` label of the answers.\n    """"""\n    def __init__(self, config):\n        super(SQuADHead, self).__init__()\n        self.start_n_top = config.start_n_top\n        self.end_n_top = config.end_n_top\n\n        self.start_logits = PoolerStartLogits(config)\n        self.end_logits = PoolerEndLogits(config)\n        self.answer_class = PoolerAnswerClass(config)\n\n    def forward(self, hidden_states, start_positions=None, end_positions=None,\n                cls_index=None, is_impossible=None, p_mask=None):\n        outputs = ()\n\n        start_logits = self.start_logits(hidden_states, p_mask=p_mask)\n\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, let\'s remove the dimension added by batch splitting\n            for x in (start_positions, end_positions, cls_index, is_impossible):\n                if x is not None and x.dim() > 1:\n                    x.squeeze_(-1)\n\n            # during training, compute the end logits based on the ground truth of the start position\n            end_logits = self.end_logits(hidden_states, start_positions=start_positions, p_mask=p_mask)\n\n            loss_fct = CrossEntropyLoss()\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n\n            if cls_index is not None and is_impossible is not None:\n                # Predict answerability from the representation of CLS and START\n                cls_logits = self.answer_class(hidden_states, start_positions=start_positions, cls_index=cls_index)\n                loss_fct_cls = nn.BCEWithLogitsLoss()\n                cls_loss = loss_fct_cls(cls_logits, is_impossible)\n\n                # note(zhiliny): by default multiply the loss by 0.5 so that the scale is comparable to start_loss and end_loss\n                total_loss += cls_loss * 0.5\n\n            outputs = (total_loss,) + outputs\n\n        else:\n            # during inference, compute the end logits based on beam search\n            bsz, slen, hsz = hidden_states.size()\n            start_log_probs = F.softmax(start_logits, dim=-1) # shape (bsz, slen)\n\n            start_top_log_probs, start_top_index = torch.topk(start_log_probs, self.start_n_top, dim=-1) # shape (bsz, start_n_top)\n            start_top_index_exp = start_top_index.unsqueeze(-1).expand(-1, -1, hsz) # shape (bsz, start_n_top, hsz)\n            start_states = torch.gather(hidden_states, -2, start_top_index_exp) # shape (bsz, start_n_top, hsz)\n            start_states = start_states.unsqueeze(1).expand(-1, slen, -1, -1) # shape (bsz, slen, start_n_top, hsz)\n\n            hidden_states_expanded = hidden_states.unsqueeze(2).expand_as(start_states) # shape (bsz, slen, start_n_top, hsz)\n            p_mask = p_mask.unsqueeze(-1) if p_mask is not None else None\n            end_logits = self.end_logits(hidden_states_expanded, start_states=start_states, p_mask=p_mask)\n            end_log_probs = F.softmax(end_logits, dim=1) # shape (bsz, slen, start_n_top)\n\n            end_top_log_probs, end_top_index = torch.topk(end_log_probs, self.end_n_top, dim=1) # shape (bsz, end_n_top, start_n_top)\n            end_top_log_probs = end_top_log_probs.view(-1, self.start_n_top * self.end_n_top)\n            end_top_index = end_top_index.view(-1, self.start_n_top * self.end_n_top)\n\n            start_states = torch.einsum(""blh,bl->bh"", hidden_states, start_log_probs)\n            cls_logits = self.answer_class(hidden_states, start_states=start_states, cls_index=cls_index)\n\n            outputs = (start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits) + outputs\n\n        # return start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits\n        # or (if labels are provided) (total_loss,)\n        return outputs\n\n\nclass SequenceSummary(nn.Module):\n    r"""""" Compute a single vector summary of a sequence hidden states according to various possibilities:\n        Args of the config class:\n            summary_type:\n                - \'last\' => [default] take the last token hidden state (like XLNet)\n                - \'first\' => take the first token hidden state (like Bert)\n                - \'mean\' => take the mean of all tokens hidden states\n                - \'cls_index\' => supply a Tensor of classification token position (GPT/GPT-2)\n                - \'attn\' => Not implemented now, use multi-head attention\n            summary_use_proj: Add a projection after the vector extraction\n            summary_proj_to_labels: If True, the projection outputs to config.num_labels classes (otherwise to hidden_size). Default: False.\n            summary_activation: \'tanh\' => add a tanh activation to the output, Other => no activation. Default\n            summary_first_dropout: Add a dropout before the projection and activation\n            summary_last_dropout: Add a dropout after the projection and activation\n    """"""\n    def __init__(self, config):\n        super(SequenceSummary, self).__init__()\n\n        self.summary_type = config.summary_type if hasattr(config, \'summary_use_proj\') else \'last\'\n        if self.summary_type == \'attn\':\n            # We should use a standard multi-head attention module with absolute positional embedding for that.\n            # Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276\n            # We can probably just use the multi-head attention module of PyTorch >=1.1.0\n            raise NotImplementedError\n\n        self.summary = Identity()\n        if hasattr(config, \'summary_use_proj\') and config.summary_use_proj:\n            if hasattr(config, \'summary_proj_to_labels\') and config.summary_proj_to_labels and config.num_labels > 0:\n                num_classes = config.num_labels\n            else:\n                num_classes = config.hidden_size\n            self.summary = nn.Linear(config.hidden_size, num_classes)\n\n        self.activation = Identity()\n        if hasattr(config, \'summary_activation\') and config.summary_activation == \'tanh\':\n            self.activation = nn.Tanh()\n\n        self.first_dropout = Identity()\n        if hasattr(config, \'summary_first_dropout\') and config.summary_first_dropout > 0:\n            self.first_dropout = nn.Dropout(config.summary_first_dropout)\n\n        self.last_dropout = Identity()\n        if hasattr(config, \'summary_last_dropout\') and config.summary_last_dropout > 0:\n            self.last_dropout = nn.Dropout(config.summary_last_dropout)\n\n    def forward(self, hidden_states, cls_index=None):\n        """""" hidden_states: float Tensor in shape [bsz, ..., seq_len, hidden_size], the hidden-states of the last layer.\n            cls_index: [optional] position of the classification token if summary_type == \'cls_index\',\n                shape (bsz,) or more generally (bsz, ...) where ... are optional leading dimensions of hidden_states.\n                if summary_type == \'cls_index\' and cls_index is None:\n                    we take the last token of the sequence as classification token\n        """"""\n        if self.summary_type == \'last\':\n            output = hidden_states[:, -1]\n        elif self.summary_type == \'first\':\n            output = hidden_states[:, 0]\n        elif self.summary_type == \'mean\':\n            output = hidden_states.mean(dim=1)\n        elif self.summary_type == \'cls_index\':\n            if cls_index is None:\n                cls_index = torch.full_like(hidden_states[..., :1, :], hidden_states.shape[-2]-1, dtype=torch.long)\n            else:\n                cls_index = cls_index.unsqueeze(-1).unsqueeze(-1)\n                cls_index = cls_index.expand((-1,) * (cls_index.dim()-1) + (hidden_states.size(-1),))\n            # shape of cls_index: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states\n            output = hidden_states.gather(-2, cls_index).squeeze(-2) # shape (bsz, XX, hidden_size)\n        elif self.summary_type == \'attn\':\n            raise NotImplementedError\n\n        output = self.first_dropout(output)\n        output = self.summary(output)\n        output = self.activation(output)\n        output = self.last_dropout(output)\n\n        return output\n\n\ndef prune_linear_layer(layer, index, dim=0):\n    """""" Prune a linear layer (a model parameters) to keep only entries in index.\n        Return the pruned layer as a new layer with requires_grad=True.\n        Used to remove heads.\n    """"""\n    index = index.to(layer.weight.device)\n    W = layer.weight.index_select(dim, index).clone().detach()\n    if layer.bias is not None:\n        if dim == 1:\n            b = layer.bias.clone().detach()\n        else:\n            b = layer.bias[index].clone().detach()\n    new_size = list(layer.weight.size())\n    new_size[dim] = len(index)\n    new_layer = nn.Linear(new_size[1], new_size[0], bias=layer.bias is not None).to(layer.weight.device)\n    new_layer.weight.requires_grad = False\n    new_layer.weight.copy_(W.contiguous())\n    new_layer.weight.requires_grad = True\n    if layer.bias is not None:\n        new_layer.bias.requires_grad = False\n        new_layer.bias.copy_(b.contiguous())\n        new_layer.bias.requires_grad = True\n    return new_layer\n\n\ndef prune_conv1d_layer(layer, index, dim=1):\n    """""" Prune a Conv1D layer (a model parameters) to keep only entries in index.\n        A Conv1D work as a Linear layer (see e.g. BERT) but the weights are transposed.\n        Return the pruned layer as a new layer with requires_grad=True.\n        Used to remove heads.\n    """"""\n    index = index.to(layer.weight.device)\n    W = layer.weight.index_select(dim, index).clone().detach()\n    if dim == 0:\n        b = layer.bias.clone().detach()\n    else:\n        b = layer.bias[index].clone().detach()\n    new_size = list(layer.weight.size())\n    new_size[dim] = len(index)\n    new_layer = Conv1D(new_size[1], new_size[0]).to(layer.weight.device)\n    new_layer.weight.requires_grad = False\n    new_layer.weight.copy_(W.contiguous())\n    new_layer.weight.requires_grad = True\n    new_layer.bias.requires_grad = False\n    new_layer.bias.copy_(b.contiguous())\n    new_layer.bias.requires_grad = True\n    return new_layer\n\n\ndef prune_layer(layer, index, dim=None):\n    """""" Prune a Conv1D or nn.Linear layer (a model parameters) to keep only entries in index.\n        Return the pruned layer as a new layer with requires_grad=True.\n        Used to remove heads.\n    """"""\n    if isinstance(layer, nn.Linear):\n        return prune_linear_layer(layer, index, dim=0 if dim is None else dim)\n    elif isinstance(layer, Conv1D):\n        return prune_conv1d_layer(layer, index, dim=1 if dim is None else dim)\n    else:\n        raise ValueError(""Can\'t prune layer of class {}"".format(layer.__class__))\n'"
models/transformers/modeling_xlm.py,70,"b'# coding=utf-8\n# Copyright 2019-present, Facebook, Inc and the HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" PyTorch XLM model.\n""""""\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport logging\nimport math\n\nimport itertools\nimport numpy as np\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.nn import CrossEntropyLoss, MSELoss\n\nfrom .modeling_utils import PreTrainedModel, prune_linear_layer, SequenceSummary, SQuADHead\nfrom .configuration_xlm import XLMConfig\nfrom .file_utils import add_start_docstrings\n\nlogger = logging.getLogger(__name__)\n\nXLM_PRETRAINED_MODEL_ARCHIVE_MAP = {\n    \'xlm-mlm-en-2048\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-en-2048-pytorch_model.bin"",\n    \'xlm-mlm-ende-1024\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-ende-1024-pytorch_model.bin"",\n    \'xlm-mlm-enfr-1024\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-enfr-1024-pytorch_model.bin"",\n    \'xlm-mlm-enro-1024\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-enro-1024-pytorch_model.bin"",\n    \'xlm-mlm-tlm-xnli15-1024\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-tlm-xnli15-1024-pytorch_model.bin"",\n    \'xlm-mlm-xnli15-1024\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-xnli15-1024-pytorch_model.bin"",\n    \'xlm-clm-enfr-1024\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-clm-enfr-1024-pytorch_model.bin"",\n    \'xlm-clm-ende-1024\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-clm-ende-1024-pytorch_model.bin"",\n    \'xlm-mlm-17-1280\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-17-1280-pytorch_model.bin"",\n    \'xlm-mlm-100-1280\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-100-1280-pytorch_model.bin"",\n}\n\n\ndef create_sinusoidal_embeddings(n_pos, dim, out):\n    position_enc = np.array([\n        [pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)]\n        for pos in range(n_pos)\n    ])\n    out[:, 0::2] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\n    out[:, 1::2] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n    out.detach_()\n    out.requires_grad = False\n\n\ndef gelu(x):\n    """"""\n    GELU activation\n    https://arxiv.org/abs/1606.08415\n    https://github.com/huggingface/pytorch-openai-transformer-lm/blob/master/model_pytorch.py#L14\n    https://github.com/huggingface/transformers/blob/master/modeling.py\n    """"""\n    # return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n    return 0.5 * x * (1.0 + torch.erf(x / math.sqrt(2.0)))\n\n\ndef get_masks(slen, lengths, causal, padding_mask=None):\n    """"""\n    Generate hidden states mask, and optionally an attention mask.\n    """"""\n    bs = lengths.size(0)\n    if padding_mask is not None:\n        mask = padding_mask\n    else:\n        assert lengths.max().item() <= slen\n        alen = torch.arange(slen, dtype=torch.long, device=lengths.device)\n        mask = alen < lengths[:, None]\n\n    # attention mask is the same as mask, or triangular inferior attention (causal)\n    if causal:\n        attn_mask = alen[None, None, :].repeat(bs, slen, 1) <= alen[None, :, None]\n    else:\n        attn_mask = mask\n\n    # sanity check\n    assert mask.size() == (bs, slen)\n    assert causal is False or attn_mask.size() == (bs, slen, slen)\n\n    return mask, attn_mask\n\n\nclass MultiHeadAttention(nn.Module):\n\n    NEW_ID = itertools.count()\n\n    def __init__(self, n_heads, dim, config):\n        super(MultiHeadAttention, self).__init__()\n        self.layer_id = next(MultiHeadAttention.NEW_ID)\n        self.output_attentions = config.output_attentions\n        self.dim = dim\n        self.n_heads = n_heads\n        self.dropout = config.attention_dropout\n        assert self.dim % self.n_heads == 0\n\n        self.q_lin = nn.Linear(dim, dim)\n        self.k_lin = nn.Linear(dim, dim)\n        self.v_lin = nn.Linear(dim, dim)\n        self.out_lin = nn.Linear(dim, dim)\n        self.pruned_heads = set()\n\n    def prune_heads(self, heads):\n        attention_head_size = self.dim // self.n_heads\n        if len(heads) == 0:\n            return\n        mask = torch.ones(self.n_heads, attention_head_size)\n        heads = set(heads) - self.pruned_heads\n        for head in heads:\n            head -= sum(1 if h < head else 0 for h in self.pruned_heads)\n            mask[head] = 0\n        mask = mask.view(-1).contiguous().eq(1)\n        index = torch.arange(len(mask))[mask].long()\n        # Prune linear layers\n        self.q_lin = prune_linear_layer(self.q_lin, index)\n        self.k_lin = prune_linear_layer(self.k_lin, index)\n        self.v_lin = prune_linear_layer(self.v_lin, index)\n        self.out_lin = prune_linear_layer(self.out_lin, index, dim=1)\n        # Update hyper params\n        self.n_heads = self.n_heads - len(heads)\n        self.dim = attention_head_size * self.n_heads\n        self.pruned_heads = self.pruned_heads.union(heads)\n\n    def forward(self, input, mask, kv=None, cache=None, head_mask=None):\n        """"""\n        Self-attention (if kv is None) or attention over source sentence (provided by kv).\n        """"""\n        # Input is (bs, qlen, dim)\n        # Mask is (bs, klen) (non-causal) or (bs, klen, klen)\n        bs, qlen, dim = input.size()\n        if kv is None:\n            klen = qlen if cache is None else cache[\'slen\'] + qlen\n        else:\n            klen = kv.size(1)\n        # assert dim == self.dim, \'Dimensions do not match: %s input vs %s configured\' % (dim, self.dim)\n        n_heads = self.n_heads\n        dim_per_head = self.dim // n_heads\n        mask_reshape = (bs, 1, qlen, klen) if mask.dim() == 3 else (bs, 1, 1, klen)\n\n        def shape(x):\n            """"""  projection """"""\n            return x.view(bs, -1, self.n_heads, dim_per_head).transpose(1, 2)\n\n        def unshape(x):\n            """"""  compute context """"""\n            return x.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * dim_per_head)\n\n        q = shape(self.q_lin(input))                                          # (bs, n_heads, qlen, dim_per_head)\n        if kv is None:\n            k = shape(self.k_lin(input))                                      # (bs, n_heads, qlen, dim_per_head)\n            v = shape(self.v_lin(input))                                      # (bs, n_heads, qlen, dim_per_head)\n        elif cache is None or self.layer_id not in cache:\n            k = v = kv\n            k = shape(self.k_lin(k))                                          # (bs, n_heads, qlen, dim_per_head)\n            v = shape(self.v_lin(v))                                          # (bs, n_heads, qlen, dim_per_head)\n\n        if cache is not None:\n            if self.layer_id in cache:\n                if kv is None:\n                    k_, v_ = cache[self.layer_id]\n                    k = torch.cat([k_, k], dim=2)                             # (bs, n_heads, klen, dim_per_head)\n                    v = torch.cat([v_, v], dim=2)                             # (bs, n_heads, klen, dim_per_head)\n                else:\n                    k, v = cache[self.layer_id]\n            cache[self.layer_id] = (k, v)\n\n        q = q / math.sqrt(dim_per_head)                                       # (bs, n_heads, qlen, dim_per_head)\n        scores = torch.matmul(q, k.transpose(2, 3))                           # (bs, n_heads, qlen, klen)\n        mask = (mask == 0).view(mask_reshape).expand_as(scores)               # (bs, n_heads, qlen, klen)\n        scores.masked_fill_(mask, -float(\'inf\'))                              # (bs, n_heads, qlen, klen)\n\n        weights = F.softmax(scores.float(), dim=-1).type_as(scores)           # (bs, n_heads, qlen, klen)\n        weights = F.dropout(weights, p=self.dropout, training=self.training)  # (bs, n_heads, qlen, klen)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            weights = weights * head_mask\n\n        context = torch.matmul(weights, v)                                    # (bs, n_heads, qlen, dim_per_head)\n        context = unshape(context)                                            # (bs, qlen, dim)\n\n        outputs = (self.out_lin(context),)\n        if self.output_attentions:\n            outputs = outputs + (weights,)\n        return outputs\n\n\nclass TransformerFFN(nn.Module):\n\n    def __init__(self, in_dim, dim_hidden, out_dim, config):\n        super(TransformerFFN, self).__init__()\n        self.dropout = config.dropout\n        self.lin1 = nn.Linear(in_dim, dim_hidden)\n        self.lin2 = nn.Linear(dim_hidden, out_dim)\n        self.act = gelu if config.gelu_activation else F.relu\n\n    def forward(self, input):\n        x = self.lin1(input)\n        x = self.act(x)\n        x = self.lin2(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        return x\n\n\nclass XLMPreTrainedModel(PreTrainedModel):\n    """""" An abstract class to handle weights initialization and\n        a simple interface for dowloading and loading pretrained models.\n    """"""\n    config_class = XLMConfig\n    pretrained_model_archive_map = XLM_PRETRAINED_MODEL_ARCHIVE_MAP\n    load_tf_weights = None\n    base_model_prefix = ""transformer""\n\n    def __init__(self, *inputs, **kwargs):\n        super(XLMPreTrainedModel, self).__init__(*inputs, **kwargs)\n\n    def _init_weights(self, module):\n        """""" Initialize the weights. """"""\n        if isinstance(module, nn.Embedding):\n            if self.config is not None and self.config.embed_init_std is not None:\n                nn.init.normal_(module.weight, mean=0, std=self.config.embed_init_std)\n        if isinstance(module, nn.Linear):\n            if self.config is not None and self.config.init_std is not None:\n                nn.init.normal_(module.weight, mean=0, std=self.config.init_std)\n                if hasattr(module, \'bias\') and module.bias is not None:\n                    nn.init.constant_(module.bias, 0.)\n        if isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n\nXLM_START_DOCSTRING = r""""""    The XLM model was proposed in\n    `Cross-lingual Language Model Pretraining`_\n    by Guillaume Lample*, Alexis Conneau*. It\'s a transformer pre-trained using one of the following objectives:\n\n        - a causal language modeling (CLM) objective (next token prediction),\n        - a masked language modeling (MLM) objective (Bert-like), or\n        - a Translation Language Modeling (TLM) object (extension of Bert\'s MLM to multiple language inputs)\n\n    Original code can be found `here`_.\n\n    This model is a PyTorch `torch.nn.Module`_ sub-class. Use it as a regular PyTorch Module and\n    refer to the PyTorch documentation for all matter related to general usage and behavior.\n\n    .. _`Cross-lingual Language Model Pretraining`:\n        https://arxiv.org/abs/1901.07291\n\n    .. _`torch.nn.Module`:\n        https://pytorch.org/docs/stable/nn.html#module\n\n    .. _`here`:\n        https://github.com/facebookresearch/XLM\n\n    Parameters:\n        config (:class:`~transformers.XLMConfig`): Model configuration class with all the parameters of the model.\n            Initializing with a config file does not load the weights associated with the model, only the configuration.\n            Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model weights.\n""""""\n\nXLM_INPUTS_DOCSTRING = r""""""\n    Inputs:\n        **input_ids**: ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Indices of input sequence tokens in the vocabulary.\n\n            XLM is a model with absolute position embeddings so it\'s usually advised to pad the inputs on\n            the right rather than the left.\n\n            Indices can be obtained using :class:`transformers.XLMTokenizer`.\n            See :func:`transformers.PreTrainedTokenizer.encode` and\n            :func:`transformers.PreTrainedTokenizer.convert_tokens_to_ids` for details.\n        **attention_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, sequence_length)``:\n            Mask to avoid performing attention on padding token indices.\n            Mask values selected in ``[0, 1]``:\n            ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n        **langs**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            A parallel sequence of tokens to be used to indicate the language of each token in the input.\n            Indices are languages ids which can be obtained from the language names by using two conversion mappings\n            provided in the configuration of the model (only provided for multilingual models).\n            More precisely, the `language name -> language id` mapping is in `model.config.lang2id` (dict str -> int) and\n            the `language id -> language name` mapping is `model.config.id2lang` (dict int -> str).\n        **token_type_ids**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            A parallel sequence of tokens (can be used to indicate various portions of the inputs).\n            The embeddings from these tokens will be summed with the respective token embeddings.\n            Indices are selected in the vocabulary (unlike BERT which has a specific vocabulary for segment indices).\n        **position_ids**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Indices of positions of each input sequence tokens in the position embeddings.\n            Selected in the range ``[0, config.max_position_embeddings - 1]``.\n        **lengths**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Length of each sentence that can be used to avoid performing attention on padding token indices.\n            You can also use `attention_mask` for the same result (see above), kept here for compatbility.\n            Indices selected in ``[0, ..., input_ids.size(-1)]``:\n        **cache**:\n            dictionary with ``torch.FloatTensor`` that contains pre-computed\n            hidden-states (key and values in the attention blocks) as computed by the model\n            (see `cache` output below). Can be used to speed up sequential decoding.\n            The dictionary object will be modified in-place during the forward pass to add newly computed hidden-states.\n        **head_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(num_heads,)`` or ``(num_layers, num_heads)``:\n            Mask to nullify selected heads of the self-attention modules.\n            Mask values selected in ``[0, 1]``:\n            ``1`` indicates the head is **not masked**, ``0`` indicates the head is **masked**.\n""""""\n\n@add_start_docstrings(""The bare XLM Model transformer outputting raw hidden-states without any specific head on top."",\n                      XLM_START_DOCSTRING, XLM_INPUTS_DOCSTRING)\nclass XLMModel(XLMPreTrainedModel):\n    r""""""\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **last_hidden_state**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, hidden_size)``\n            Sequence of hidden-states at the last layer of the model.\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = XLMTokenizer.from_pretrained(\'xlm-mlm-en-2048\')\n        model = XLMModel.from_pretrained(\'xlm-mlm-en-2048\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids)\n        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n\n    """"""\n    def __init__(self, config):  #, dico, is_encoder, with_output):\n        super(XLMModel, self).__init__(config)\n        self.output_attentions = config.output_attentions\n        self.output_hidden_states = config.output_hidden_states\n\n        # encoder / decoder, output layer\n        self.is_encoder = config.is_encoder\n        self.is_decoder = not config.is_encoder\n        if self.is_decoder:\n            raise NotImplementedError(""Currently XLM can only be used as an encoder"")\n        # self.with_output = with_output\n        self.causal = config.causal\n\n        # dictionary / languages\n        self.n_langs = config.n_langs\n        self.use_lang_emb = config.use_lang_emb\n        self.n_words = config.n_words\n        self.eos_index = config.eos_index\n        self.pad_index = config.pad_index\n        # self.dico = dico\n        # self.id2lang = config.id2lang\n        # self.lang2id = config.lang2id\n        # assert len(self.dico) == self.n_words\n        # assert len(self.id2lang) == len(self.lang2id) == self.n_langs\n\n        # model parameters\n        self.dim = config.emb_dim       # 512 by default\n        self.hidden_dim = self.dim * 4  # 2048 by default\n        self.n_heads = config.n_heads   # 8 by default\n        self.n_layers = config.n_layers\n        self.dropout = config.dropout\n        self.attention_dropout = config.attention_dropout\n        assert self.dim % self.n_heads == 0, \'transformer dim must be a multiple of n_heads\'\n\n        # embeddings\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, self.dim)\n        if config.sinusoidal_embeddings:\n            create_sinusoidal_embeddings(config.max_position_embeddings, self.dim, out=self.position_embeddings.weight)\n        if config.n_langs > 1 and config.use_lang_emb:\n            self.lang_embeddings = nn.Embedding(self.n_langs, self.dim)\n        self.embeddings = nn.Embedding(self.n_words, self.dim, padding_idx=self.pad_index)\n        self.layer_norm_emb = nn.LayerNorm(self.dim, eps=config.layer_norm_eps)\n\n        # transformer layers\n        self.attentions = nn.ModuleList()\n        self.layer_norm1 = nn.ModuleList()\n        self.ffns = nn.ModuleList()\n        self.layer_norm2 = nn.ModuleList()\n        # if self.is_decoder:\n        #     self.layer_norm15 = nn.ModuleList()\n        #     self.encoder_attn = nn.ModuleList()\n\n        for _ in range(self.n_layers):\n            self.attentions.append(MultiHeadAttention(self.n_heads, self.dim, config=config))\n            self.layer_norm1.append(nn.LayerNorm(self.dim, eps=config.layer_norm_eps))\n            # if self.is_decoder:\n            #     self.layer_norm15.append(nn.LayerNorm(self.dim, eps=config.layer_norm_eps))\n            #     self.encoder_attn.append(MultiHeadAttention(self.n_heads, self.dim, dropout=self.attention_dropout))\n            self.ffns.append(TransformerFFN(self.dim, self.hidden_dim, self.dim, config=config))\n            self.layer_norm2.append(nn.LayerNorm(self.dim, eps=config.layer_norm_eps))\n\n        if hasattr(config, ""pruned_heads""):\n            pruned_heads = config.pruned_heads.copy().items()\n            config.pruned_heads = {}\n            for layer, heads in pruned_heads:\n                if self.attentions[int(layer)].n_heads == config.n_heads:\n                    self.prune_heads({int(layer): list(map(int, heads))})\n\n        self.init_weights()\n\n    def _resize_token_embeddings(self, new_num_tokens):\n        self.embeddings = self._get_resized_embeddings(self.embeddings, new_num_tokens)\n        return self.embeddings\n\n    def _prune_heads(self, heads_to_prune):\n        """""" Prunes heads of the model.\n            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n            See base class PreTrainedModel\n        """"""\n        for layer, heads in heads_to_prune.items():\n            self.attentions[layer].prune_heads(heads)\n\n    def forward(self, input_ids, attention_mask=None, langs=None, token_type_ids=None, position_ids=None,\n                lengths=None, cache=None, head_mask=None):  # removed: src_enc=None, src_len=None\n        if lengths is None:\n            lengths = (input_ids != self.pad_index).sum(dim=1).long()\n        # mask = input_ids != self.pad_index\n\n        # check inputs\n        bs, slen = input_ids.size()\n        assert lengths.size(0) == bs\n        assert lengths.max().item() <= slen\n        # input_ids = input_ids.transpose(0, 1)  # batch size as dimension 0\n        # assert (src_enc is None) == (src_len is None)\n        # if src_enc is not None:\n        #     assert self.is_decoder\n        #     assert src_enc.size(0) == bs\n\n        # generate masks\n        mask, attn_mask = get_masks(slen, lengths, self.causal, padding_mask=attention_mask)\n        # if self.is_decoder and src_enc is not None:\n        #     src_mask = torch.arange(src_len.max(), dtype=torch.long, device=lengths.device) < src_len[:, None]\n\n        # position_ids\n        if position_ids is None:\n            position_ids = input_ids.new((slen,)).long()\n            position_ids = torch.arange(slen, out=position_ids).unsqueeze(0)\n        else:\n            assert position_ids.size() == (bs, slen)  # (slen, bs)\n            # position_ids = position_ids.transpose(0, 1)\n\n        # langs\n        if langs is not None:\n            assert langs.size() == (bs, slen)  # (slen, bs)\n            # langs = langs.transpose(0, 1)\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x qlen x klen]\n        if head_mask is not None:\n            if head_mask.dim() == 1:\n                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n                head_mask = head_mask.expand(self.n_layers, -1, -1, -1, -1)\n            elif head_mask.dim() == 2:\n                head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)  # We can specify head_mask for each layer\n            head_mask = head_mask.to(dtype=next(self.parameters()).dtype) # switch to fload if need + fp16 compatibility\n        else:\n            head_mask = [None] * self.n_layers\n\n        # do not recompute cached elements\n        if cache is not None:\n            _slen = slen - cache[\'slen\']\n            input_ids = input_ids[:, -_slen:]\n            position_ids = position_ids[:, -_slen:]\n            if langs is not None:\n                langs = langs[:, -_slen:]\n            mask = mask[:, -_slen:]\n            attn_mask = attn_mask[:, -_slen:]\n\n        # embeddings\n        tensor = self.embeddings(input_ids)\n        tensor = tensor + self.position_embeddings(position_ids).expand_as(tensor)\n        if langs is not None and self.use_lang_emb:\n            tensor = tensor + self.lang_embeddings(langs)\n        if token_type_ids is not None:\n            tensor = tensor + self.embeddings(token_type_ids)\n        tensor = self.layer_norm_emb(tensor)\n        tensor = F.dropout(tensor, p=self.dropout, training=self.training)\n        tensor *= mask.unsqueeze(-1).to(tensor.dtype)\n\n        # transformer layers\n        hidden_states = ()\n        attentions = ()\n        for i in range(self.n_layers):\n            if self.output_hidden_states:\n                hidden_states = hidden_states + (tensor,)\n\n            # self attention\n            attn_outputs = self.attentions[i](tensor, attn_mask, cache=cache, head_mask=head_mask[i])\n            attn = attn_outputs[0]\n            if self.output_attentions:\n                attentions = attentions + (attn_outputs[1],)\n            attn = F.dropout(attn, p=self.dropout, training=self.training)\n            tensor = tensor + attn\n            tensor = self.layer_norm1[i](tensor)\n\n            # encoder attention (for decoder only)\n            # if self.is_decoder and src_enc is not None:\n            #     attn = self.encoder_attn[i](tensor, src_mask, kv=src_enc, cache=cache)\n            #     attn = F.dropout(attn, p=self.dropout, training=self.training)\n            #     tensor = tensor + attn\n            #     tensor = self.layer_norm15[i](tensor)\n\n            # FFN\n            tensor = tensor + self.ffns[i](tensor)\n            tensor = self.layer_norm2[i](tensor)\n            tensor *= mask.unsqueeze(-1).to(tensor.dtype)\n\n        # Add last hidden state\n        if self.output_hidden_states:\n            hidden_states = hidden_states + (tensor,)\n\n        # update cache length\n        if cache is not None:\n            cache[\'slen\'] += tensor.size(1)\n\n        # move back sequence length to dimension 0\n        # tensor = tensor.transpose(0, 1)\n\n        outputs = (tensor,)\n        if self.output_hidden_states:\n            outputs = outputs + (hidden_states,)\n        if self.output_attentions:\n            outputs = outputs + (attentions,)\n        return outputs  # outputs, (hidden_states), (attentions)\n\n\nclass XLMPredLayer(nn.Module):\n    """"""\n    Prediction layer (cross_entropy or adaptive_softmax).\n    """"""\n    def __init__(self, config):\n        super(XLMPredLayer, self).__init__()\n        self.asm = config.asm\n        self.n_words = config.n_words\n        self.pad_index = config.pad_index\n        dim = config.emb_dim\n\n        if config.asm is False:\n            self.proj = nn.Linear(dim, config.n_words, bias=True)\n        else:\n            self.proj = nn.AdaptiveLogSoftmaxWithLoss(\n                in_features=dim,\n                n_classes=config.n_words,\n                cutoffs=config.asm_cutoffs,\n                div_value=config.asm_div_value,\n                head_bias=True,  # default is False\n            )\n\n    def forward(self, x, y=None):\n        """""" Compute the loss, and optionally the scores.\n        """"""\n        outputs = ()\n        if self.asm is False:\n            scores = self.proj(x)\n            outputs = (scores,) + outputs\n            if y is not None:\n                loss = F.cross_entropy(scores.view(-1, self.n_words), y.view(-1), reduction=\'elementwise_mean\')\n                outputs = (loss,) + outputs\n        else:\n            scores = self.proj.log_prob(x)\n            outputs = (scores,) + outputs\n            if y is not None:\n                _, loss = self.proj(x, y)\n                outputs = (loss,) + outputs\n\n        return outputs\n\n\n@add_start_docstrings(""""""The XLM Model transformer with a language modeling head on top\n    (linear layer with weights tied to the input embeddings). """""",\n    XLM_START_DOCSTRING, XLM_INPUTS_DOCSTRING)\nclass XLMWithLMHeadModel(XLMPreTrainedModel):\n    r""""""\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Labels for language modeling.\n            Note that the labels **are shifted** inside the model, i.e. you can set ``lm_labels = input_ids``\n            Indices are selected in ``[-1, 0, ..., config.vocab_size]``\n            All labels set to ``-1`` are ignored (masked), the loss is only\n            computed for labels in ``[0, ..., config.vocab_size]``\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Language modeling loss.\n        **prediction_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.vocab_size)``\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = XLMTokenizer.from_pretrained(\'xlm-mlm-en-2048\')\n        model = XLMWithLMHeadModel.from_pretrained(\'xlm-mlm-en-2048\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids)\n        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n\n    """"""\n    def __init__(self, config):\n        super(XLMWithLMHeadModel, self).__init__(config)\n        self.transformer = XLMModel(config)\n        self.pred_layer = XLMPredLayer(config)\n\n        self.init_weights()\n        self.tie_weights()\n\n    def tie_weights(self):\n        """""" Make sure we are sharing the embeddings\n        """"""\n        self._tie_or_clone_weights(self.pred_layer.proj, self.transformer.embeddings)\n\n    def forward(self, input_ids, attention_mask=None, langs=None, token_type_ids=None, position_ids=None,\n                lengths=None, cache=None, head_mask=None, labels=None):\n        transformer_outputs = self.transformer(input_ids,\n                                               attention_mask=attention_mask,\n                                               langs=langs,\n                                               token_type_ids=token_type_ids,\n                                               position_ids=position_ids,\n                                               lengths=lengths, \n                                               cache=cache,\n                                               head_mask=head_mask)\n\n        output = transformer_outputs[0]\n        outputs = self.pred_layer(output, labels)\n        outputs = outputs + transformer_outputs[1:]  # Keep new_mems and attention/hidden states if they are here\n\n        return outputs\n\n\n@add_start_docstrings(""""""XLM Model with a sequence classification/regression head on top (a linear layer on top of\n    the pooled output) e.g. for GLUE tasks. """""",\n    XLM_START_DOCSTRING, XLM_INPUTS_DOCSTRING)\nclass XLMForSequenceClassification(XLMPreTrainedModel):\n    r""""""\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the sequence classification/regression loss.\n            Indices should be in ``[0, ..., config.num_labels - 1]``.\n            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Classification (or regression if config.num_labels==1) loss.\n        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = XLMTokenizer.from_pretrained(\'xlm-mlm-en-2048\')\n        model = XLMForSequenceClassification.from_pretrained(\'xlm-mlm-en-2048\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, labels=labels)\n        loss, logits = outputs[:2]\n\n    """"""\n    def __init__(self, config):\n        super(XLMForSequenceClassification, self).__init__(config)\n        self.num_labels = config.num_labels\n\n        self.transformer = XLMModel(config)\n        self.sequence_summary = SequenceSummary(config)\n\n        self.init_weights()\n\n    def forward(self, input_ids, attention_mask=None, langs=None, token_type_ids=None, position_ids=None,\n                lengths=None, cache=None, head_mask=None, labels=None):\n        transformer_outputs = self.transformer(input_ids,\n                                               attention_mask=attention_mask,\n                                               langs=langs,\n                                               token_type_ids=token_type_ids,\n                                               position_ids=position_ids,\n                                               lengths=lengths, \n                                               cache=cache,\n                                               head_mask=head_mask)\n\n        output = transformer_outputs[0]\n        logits = self.sequence_summary(output)\n\n        outputs = (logits,) + transformer_outputs[1:]  # Keep new_mems and attention/hidden states if they are here\n\n        if labels is not None:\n            if self.num_labels == 1:\n                #  We are doing regression\n                loss_fct = MSELoss()\n                loss = loss_fct(logits.view(-1), labels.view(-1))\n            else:\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            outputs = (loss,) + outputs\n\n        return outputs\n\n\n@add_start_docstrings(""""""XLM Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear layers on top of\n    the hidden-states output to compute `span start logits` and `span end logits`). """""",\n    XLM_START_DOCSTRING, XLM_INPUTS_DOCSTRING)\nclass XLMForQuestionAnsweringSimple(XLMPreTrainedModel):\n    r""""""\n        **start_positions**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`).\n            Position outside of the sequence are not taken into account for computing the loss.\n        **end_positions**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`).\n            Position outside of the sequence are not taken into account for computing the loss.\n        **is_impossible**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels whether a question has an answer or no answer (SQuAD 2.0)\n        **cls_index**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for position (index) of the classification token to use as input for computing plausibility of the answer.\n        **p_mask**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Optional mask of tokens which can\'t be in answers (e.g. [CLS], [PAD], ...) \n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.\n        **start_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length,)``\n            Span-start scores (before SoftMax).\n        **end_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length,)``\n            Span-end scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = XLMTokenizer.from_pretrained(\'xlm-mlm-en-2048\')\n        model = XLMForQuestionAnsweringSimple.from_pretrained(\'xlm-mlm-en-2048\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        start_positions = torch.tensor([1])\n        end_positions = torch.tensor([3])\n        outputs = model(input_ids, start_positions=start_positions, end_positions=end_positions)\n        loss, start_scores, end_scores = outputs[:2]\n\n    """"""\n    def __init__(self, config):\n        super(XLMForQuestionAnsweringSimple, self).__init__(config)\n\n        self.transformer = XLMModel(config)\n        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    def forward(self, input_ids, attention_mask=None, langs=None, token_type_ids=None, position_ids=None,\n                lengths=None, cache=None, head_mask=None, start_positions=None, end_positions=None):\n        transformer_outputs = self.transformer(input_ids,\n                                               attention_mask=attention_mask,\n                                               langs=langs,\n                                               token_type_ids=token_type_ids,\n                                               position_ids=position_ids,\n                                               lengths=lengths, \n                                               cache=cache,\n                                               head_mask=head_mask)\n\n        sequence_output = transformer_outputs[0]\n\n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        outputs = (start_logits, end_logits,)\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions.clamp_(0, ignored_index)\n            end_positions.clamp_(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n            outputs = (total_loss,) + outputs\n\n        outputs = outputs + transformer_outputs[1:]  # Keep new_mems and attention/hidden states if they are here\n\n        return outputs\n\n\n@add_start_docstrings(""""""XLM Model with a beam-search span classification head on top for extractive question-answering tasks like SQuAD (a linear layers on top of\n    the hidden-states output to compute `span start logits` and `span end logits`). """""",\n    XLM_START_DOCSTRING, XLM_INPUTS_DOCSTRING)\nclass XLMForQuestionAnswering(XLMPreTrainedModel):\n    r""""""\n        **start_positions**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`).\n            Position outside of the sequence are not taken into account for computing the loss.\n        **end_positions**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`).\n            Position outside of the sequence are not taken into account for computing the loss.\n        **is_impossible**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels whether a question has an answer or no answer (SQuAD 2.0)\n        **cls_index**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for position (index) of the classification token to use as input for computing plausibility of the answer.\n        **p_mask**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Optional mask of tokens which can\'t be in answers (e.g. [CLS], [PAD], ...) \n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.\n        **start_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length,)``\n            Span-start scores (before SoftMax).\n        **end_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length,)``\n            Span-end scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = XLMTokenizer.from_pretrained(\'xlm-mlm-en-2048\')\n        model = XLMForQuestionAnswering.from_pretrained(\'xlm-mlm-en-2048\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        start_positions = torch.tensor([1])\n        end_positions = torch.tensor([3])\n        outputs = model(input_ids, start_positions=start_positions, end_positions=end_positions)\n        loss, start_scores, end_scores = outputs[:2]\n\n    """"""\n    def __init__(self, config):\n        super(XLMForQuestionAnswering, self).__init__(config)\n\n        self.transformer = XLMModel(config)\n        self.qa_outputs = SQuADHead(config)\n\n        self.init_weights()\n\n    def forward(self, input_ids, attention_mask=None, langs=None, token_type_ids=None, position_ids=None,\n                lengths=None, cache=None, head_mask=None, start_positions=None, end_positions=None,\n                is_impossible=None, cls_index=None, p_mask=None):\n        transformer_outputs = self.transformer(input_ids,\n                                               attention_mask=attention_mask,\n                                               langs=langs,\n                                               token_type_ids=token_type_ids,\n                                               position_ids=position_ids,\n                                               lengths=lengths, \n                                               cache=cache,\n                                               head_mask=head_mask)\n\n        output = transformer_outputs[0]\n\n        outputs = self.qa_outputs(output, start_positions=start_positions, end_positions=end_positions,\n                                  cls_index=cls_index, is_impossible=is_impossible, p_mask=p_mask)\n\n        outputs = outputs + transformer_outputs[1:]  # Keep new_mems and attention/hidden states if they are here\n\n        return outputs\n'"
models/transformers/modeling_xlnet.py,144,"b'# coding=utf-8\n# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" PyTorch XLNet model.\n""""""\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport json\nimport logging\nimport math\nimport os\nimport sys\nfrom io import open\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.nn import CrossEntropyLoss, MSELoss\n\nfrom .modeling_utils import PreTrainedModel, prune_linear_layer, SequenceSummary, PoolerAnswerClass, PoolerEndLogits, PoolerStartLogits\nfrom .configuration_xlnet import XLNetConfig\nfrom .file_utils import add_start_docstrings\n\n\nlogger = logging.getLogger(__name__)\n\nXLNET_PRETRAINED_MODEL_ARCHIVE_MAP = {\n    \'xlnet-base-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-pytorch_model.bin"",\n    \'xlnet-large-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-large-cased-pytorch_model.bin"",\n}\n\n\ndef build_tf_xlnet_to_pytorch_map(model, config, tf_weights=None):\n    """""" A map of modules from TF to PyTorch.\n        I use a map to keep the PyTorch model as\n        identical to the original PyTorch model as possible.\n    """"""\n\n    tf_to_pt_map = {}\n\n    if hasattr(model, \'transformer\'):\n        if hasattr(model, \'lm_loss\'):\n            # We will load also the output bias\n            tf_to_pt_map[\'model/lm_loss/bias\'] = model.lm_loss.bias\n        if hasattr(model, \'sequence_summary\') and \'model/sequnece_summary/summary/kernel\' in tf_weights:\n            # We will load also the sequence summary\n            tf_to_pt_map[\'model/sequnece_summary/summary/kernel\'] = model.sequence_summary.summary.weight\n            tf_to_pt_map[\'model/sequnece_summary/summary/bias\'] = model.sequence_summary.summary.bias\n        if hasattr(model, \'logits_proj\') and config.finetuning_task is not None \\\n                and \'model/regression_{}/logit/kernel\'.format(config.finetuning_task) in tf_weights:\n            tf_to_pt_map[\'model/regression_{}/logit/kernel\'.format(config.finetuning_task)] = model.logits_proj.weight\n            tf_to_pt_map[\'model/regression_{}/logit/bias\'.format(config.finetuning_task)] = model.logits_proj.bias\n\n        # Now load the rest of the transformer\n        model = model.transformer\n\n    # Embeddings and output\n    tf_to_pt_map.update({\'model/transformer/word_embedding/lookup_table\': model.word_embedding.weight,\n                         \'model/transformer/mask_emb/mask_emb\': model.mask_emb})\n\n    # Transformer blocks\n    for i, b in enumerate(model.layer):\n        layer_str = ""model/transformer/layer_%d/"" % i\n        tf_to_pt_map.update({\n            layer_str + ""rel_attn/LayerNorm/gamma"": b.rel_attn.layer_norm.weight,\n            layer_str + ""rel_attn/LayerNorm/beta"": b.rel_attn.layer_norm.bias,\n            layer_str + ""rel_attn/o/kernel"": b.rel_attn.o,\n            layer_str + ""rel_attn/q/kernel"": b.rel_attn.q,\n            layer_str + ""rel_attn/k/kernel"": b.rel_attn.k,\n            layer_str + ""rel_attn/r/kernel"": b.rel_attn.r,\n            layer_str + ""rel_attn/v/kernel"": b.rel_attn.v,\n            layer_str + ""ff/LayerNorm/gamma"": b.ff.layer_norm.weight,\n            layer_str + ""ff/LayerNorm/beta"": b.ff.layer_norm.bias,\n            layer_str + ""ff/layer_1/kernel"": b.ff.layer_1.weight,\n            layer_str + ""ff/layer_1/bias"": b.ff.layer_1.bias,\n            layer_str + ""ff/layer_2/kernel"": b.ff.layer_2.weight,\n            layer_str + ""ff/layer_2/bias"": b.ff.layer_2.bias,\n        })\n\n    # Relative positioning biases\n    if config.untie_r:\n        r_r_list = []\n        r_w_list = []\n        r_s_list = []\n        seg_embed_list = []\n        for b in model.layer:\n            r_r_list.append(b.rel_attn.r_r_bias)\n            r_w_list.append(b.rel_attn.r_w_bias)\n            r_s_list.append(b.rel_attn.r_s_bias)\n            seg_embed_list.append(b.rel_attn.seg_embed)\n    else:\n        r_r_list = [model.r_r_bias]\n        r_w_list = [model.r_w_bias]\n        r_s_list = [model.r_s_bias]\n        seg_embed_list = [model.seg_embed]\n    tf_to_pt_map.update({\n        \'model/transformer/r_r_bias\': r_r_list,\n        \'model/transformer/r_w_bias\': r_w_list,\n        \'model/transformer/r_s_bias\': r_s_list,\n        \'model/transformer/seg_embed\': seg_embed_list})\n    return tf_to_pt_map\n\ndef load_tf_weights_in_xlnet(model, config, tf_path):\n    """""" Load tf checkpoints in a pytorch model\n    """"""\n    try:\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        logger.error(""Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see ""\n            ""https://www.tensorflow.org/install/ for installation instructions."")\n        raise\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    tf_weights = {}\n    for name, shape in init_vars:\n        logger.info(""Loading TF weight {} with shape {}"".format(name, shape))\n        array = tf.train.load_variable(tf_path, name)\n        tf_weights[name] = array\n\n    # Build TF to PyTorch weights loading map\n    tf_to_pt_map = build_tf_xlnet_to_pytorch_map(model, config, tf_weights)\n\n    for name, pointer in tf_to_pt_map.items():\n        logger.info(""Importing {}"".format(name))\n        if name not in tf_weights:\n            logger.info(""{} not in tf pre-trained weights, skipping"".format(name))\n            continue\n        array = tf_weights[name]\n        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n        # which are not required for using pretrained model\n        if \'kernel\' in name and (\'ff\' in name or \'summary\' in name or \'logit\' in name):\n            logger.info(""Transposing"")\n            array = np.transpose(array)\n        if isinstance(pointer, list):\n            # Here we will split the TF weigths\n            assert len(pointer) == array.shape[0]\n            for i, p_i in enumerate(pointer):\n                arr_i = array[i, ...]\n                try:\n                    assert p_i.shape == arr_i.shape\n                except AssertionError as e:\n                    e.args += (p_i.shape, arr_i.shape)\n                    raise\n                logger.info(""Initialize PyTorch weight {} for layer {}"".format(name, i))\n                p_i.data = torch.from_numpy(arr_i)\n        else:\n            try:\n                assert pointer.shape == array.shape\n            except AssertionError as e:\n                e.args += (pointer.shape, array.shape)\n                raise\n            logger.info(""Initialize PyTorch weight {}"".format(name))\n            pointer.data = torch.from_numpy(array)\n        tf_weights.pop(name, None)\n        tf_weights.pop(name + \'/Adam\', None)\n        tf_weights.pop(name + \'/Adam_1\', None)\n\n    logger.info(""Weights not copied to PyTorch model: {}"".format(\', \'.join(tf_weights.keys())))\n    return model\n\n\ndef gelu(x):\n    """""" Implementation of the gelu activation function.\n        XLNet is using OpenAI GPT\'s gelu (not exactly the same as BERT)\n        Also see https://arxiv.org/abs/1606.08415\n    """"""\n    cdf = 0.5 * (1.0 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n    return x * cdf\n\n\ndef swish(x):\n    return x * torch.sigmoid(x)\n\n\nACT2FN = {""gelu"": gelu, ""relu"": torch.nn.functional.relu, ""swish"": swish}\n\n\ntry:\n    from apex.normalization.fused_layer_norm import FusedLayerNorm as XLNetLayerNorm\nexcept (ImportError, AttributeError) as e:\n    logger.info(""Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex ."")\n    from torch.nn import LayerNorm as XLNetLayerNorm\n\nclass XLNetRelativeAttention(nn.Module):\n    def __init__(self, config):\n        super(XLNetRelativeAttention, self).__init__()\n        self.output_attentions = config.output_attentions\n\n        if config.d_model % config.n_head != 0:\n            raise ValueError(\n                ""The hidden size (%d) is not a multiple of the number of attention ""\n                ""heads (%d)"" % (config.d_model, config.n_head))\n\n        self.n_head = config.n_head\n        self.d_head = config.d_head\n        self.d_model = config.d_model\n        self.scale = 1 / (config.d_head ** 0.5)\n\n        self.q = nn.Parameter(torch.FloatTensor(config.d_model, self.n_head, self.d_head))\n        self.k = nn.Parameter(torch.FloatTensor(config.d_model, self.n_head, self.d_head))\n        self.v = nn.Parameter(torch.FloatTensor(config.d_model, self.n_head, self.d_head))\n        self.o = nn.Parameter(torch.FloatTensor(config.d_model, self.n_head, self.d_head))\n        self.r = nn.Parameter(torch.FloatTensor(config.d_model, self.n_head, self.d_head))\n\n        self.r_r_bias = nn.Parameter(torch.FloatTensor(self.n_head, self.d_head))\n        self.r_s_bias = nn.Parameter(torch.FloatTensor(self.n_head, self.d_head))\n        self.r_w_bias = nn.Parameter(torch.FloatTensor(self.n_head, self.d_head))\n        self.seg_embed = nn.Parameter(torch.FloatTensor(2, self.n_head, self.d_head))\n\n        self.layer_norm = XLNetLayerNorm(config.d_model, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def prune_heads(self, heads):\n        raise NotImplementedError\n\n    @staticmethod\n    def rel_shift(x, klen=-1):\n        """"""perform relative shift to form the relative attention score.""""""\n        x_size = x.shape\n\n        x = x.reshape(x_size[1], x_size[0], x_size[2], x_size[3])\n        x = x[1:, ...]\n        x = x.reshape(x_size[0], x_size[1] - 1, x_size[2], x_size[3])\n        # x = x[:, 0:klen, :, :]\n        x = torch.index_select(x, 1, torch.arange(klen, device=x.device, dtype=torch.long))\n\n        return x\n\n    @staticmethod\n    def rel_shift_bnij(x, klen=-1):\n        x_size = x.shape\n\n        x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])\n        x = x[:, :, 1:, :]\n        x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3]-1)\n        # Note: the tensor-slice form was faster in my testing than torch.index_select\n        #       However, tracing doesn\'t like the nature of the slice, and if klen changes\n        #       during the run then it\'ll fail, whereas index_select will be fine.\n        x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))\n        # x = x[:, :, :, :klen]\n\n        return x\n\n    def rel_attn_core(self, q_head, k_head_h, v_head_h, k_head_r, seg_mat=None, attn_mask=None, head_mask=None):\n        """"""Core relative positional attention operations.""""""\n\n        # content based attention score\n        ac = torch.einsum(\'ibnd,jbnd->bnij\', q_head + self.r_w_bias, k_head_h)\n\n        # position based attention score\n        bd = torch.einsum(\'ibnd,jbnd->bnij\', q_head + self.r_r_bias, k_head_r)\n        bd = self.rel_shift_bnij(bd, klen=ac.shape[3])\n\n        # segment based attention score\n        if seg_mat is None:\n            ef = 0\n        else:\n            ef = torch.einsum(\'ibnd,snd->ibns\', q_head + self.r_s_bias, self.seg_embed)\n            ef = torch.einsum(\'ijbs,ibns->bnij\', seg_mat, ef)\n\n        # merge attention scores and perform masking\n        attn_score = (ac + bd + ef) * self.scale\n        if attn_mask is not None:\n            # attn_score = attn_score * (1 - attn_mask) - 1e30 * attn_mask\n            if attn_mask.dtype == torch.float16:\n                attn_score = attn_score - 65500 * torch.einsum(\'ijbn->bnij\', attn_mask)\n            else:\n                attn_score = attn_score - 1e30 * torch.einsum(\'ijbn->bnij\', attn_mask)\n\n        # attention probability\n        attn_prob = F.softmax(attn_score, dim=3)\n        attn_prob = self.dropout(attn_prob)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attn_prob = attn_prob * torch.einsum(\'ijbn->bnij\', head_mask)\n\n        # attention output\n        attn_vec = torch.einsum(\'bnij,jbnd->ibnd\', attn_prob, v_head_h)\n\n        if self.output_attentions:\n            return attn_vec, torch.einsum(\'bnij->ijbn\', attn_prob)\n\n        return attn_vec\n\n    def post_attention(self, h, attn_vec, residual=True):\n        """"""Post-attention processing.""""""\n        # post-attention projection (back to `d_model`)\n        attn_out = torch.einsum(\'ibnd,hnd->ibh\', attn_vec, self.o)\n\n        attn_out = self.dropout(attn_out)\n        if residual:\n            attn_out = attn_out + h\n        output = self.layer_norm(attn_out)\n\n        return output\n\n    def forward(self, h, g,\n                      attn_mask_h, attn_mask_g,\n                      r, seg_mat,\n                      mems=None, target_mapping=None, head_mask=None):\n        if g is not None:\n            ###### Two-stream attention with relative positional encoding.\n            # content based attention score\n            if mems is not None and mems.dim() > 1:\n                cat = torch.cat([mems, h], dim=0)\n            else:\n                cat = h\n\n            # content-based key head\n            k_head_h = torch.einsum(\'ibh,hnd->ibnd\', cat, self.k)\n\n            # content-based value head\n            v_head_h = torch.einsum(\'ibh,hnd->ibnd\', cat, self.v)\n\n            # position-based key head\n            k_head_r = torch.einsum(\'ibh,hnd->ibnd\', r, self.r)\n\n            ##### h-stream\n            # content-stream query head\n            q_head_h = torch.einsum(\'ibh,hnd->ibnd\', h, self.q)\n\n            # core attention ops\n            attn_vec_h = self.rel_attn_core(\n                q_head_h, k_head_h, v_head_h, k_head_r, seg_mat=seg_mat, attn_mask=attn_mask_h, head_mask=head_mask)\n\n            if self.output_attentions:\n                attn_vec_h, attn_prob_h = attn_vec_h\n\n            # post processing\n            output_h = self.post_attention(h, attn_vec_h)\n\n            ##### g-stream\n            # query-stream query head\n            q_head_g = torch.einsum(\'ibh,hnd->ibnd\', g, self.q)\n\n            # core attention ops\n            if target_mapping is not None:\n                q_head_g = torch.einsum(\'mbnd,mlb->lbnd\', q_head_g, target_mapping)\n                attn_vec_g = self.rel_attn_core(\n                    q_head_g, k_head_h, v_head_h, k_head_r, seg_mat=seg_mat, attn_mask=attn_mask_g, head_mask=head_mask)\n\n                if self.output_attentions:\n                    attn_vec_g, attn_prob_g = attn_vec_g\n\n                attn_vec_g = torch.einsum(\'lbnd,mlb->mbnd\', attn_vec_g, target_mapping)\n            else:\n                attn_vec_g = self.rel_attn_core(\n                    q_head_g, k_head_h, v_head_h, k_head_r, seg_mat=seg_mat, attn_mask=attn_mask_g, head_mask=head_mask)\n\n                if self.output_attentions:\n                    attn_vec_g, attn_prob_g = attn_vec_g\n\n            # post processing\n            output_g = self.post_attention(g, attn_vec_g)\n\n            if self.output_attentions:\n                attn_prob = attn_prob_h, attn_prob_g\n\n        else:\n            ###### Multi-head attention with relative positional encoding\n            if mems is not None and mems.dim() > 1:\n                cat = torch.cat([mems, h], dim=0)\n            else:\n                cat = h\n\n            # content heads\n            q_head_h = torch.einsum(\'ibh,hnd->ibnd\', h, self.q)\n            k_head_h = torch.einsum(\'ibh,hnd->ibnd\', cat, self.k)\n            v_head_h = torch.einsum(\'ibh,hnd->ibnd\', cat, self.v)\n\n            # positional heads\n            k_head_r = torch.einsum(\'ibh,hnd->ibnd\', r, self.r)\n\n            # core attention ops\n            attn_vec = self.rel_attn_core(\n                q_head_h, k_head_h, v_head_h, k_head_r, seg_mat=seg_mat, attn_mask=attn_mask_h, head_mask=head_mask)\n\n            if self.output_attentions:\n                attn_vec, attn_prob = attn_vec\n\n            # post processing\n            output_h = self.post_attention(h, attn_vec)\n            output_g = None\n\n        outputs = (output_h, output_g)\n        if self.output_attentions:\n            outputs = outputs + (attn_prob,)\n        return outputs\n\nclass XLNetFeedForward(nn.Module):\n    def __init__(self, config):\n        super(XLNetFeedForward, self).__init__()\n        self.layer_norm = XLNetLayerNorm(config.d_model, eps=config.layer_norm_eps)\n        self.layer_1 = nn.Linear(config.d_model, config.d_inner)\n        self.layer_2 = nn.Linear(config.d_inner, config.d_model)\n        self.dropout = nn.Dropout(config.dropout)\n        if isinstance(config.ff_activation, str) or \\\n                (sys.version_info[0] == 2 and isinstance(config.ff_activation, unicode)):\n            self.activation_function = ACT2FN[config.ff_activation]\n        else:\n            self.activation_function = config.ff_activation\n\n    def forward(self, inp):\n        output = inp\n        output = self.layer_1(output)\n        output = self.activation_function(output)\n        output = self.dropout(output)\n        output = self.layer_2(output)\n        output = self.dropout(output)\n        output = self.layer_norm(output + inp)\n        return output\n\nclass XLNetLayer(nn.Module):\n    def __init__(self, config):\n        super(XLNetLayer, self).__init__()\n        self.rel_attn = XLNetRelativeAttention(config)\n        self.ff = XLNetFeedForward(config)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, output_h, output_g,\n                attn_mask_h, attn_mask_g,\n                r, seg_mat, mems=None, target_mapping=None, head_mask=None):\n        outputs = self.rel_attn(output_h, output_g, attn_mask_h, attn_mask_g,\n                                r, seg_mat, mems=mems, target_mapping=target_mapping,\n                                head_mask=head_mask)\n        output_h, output_g = outputs[:2]\n\n        if output_g is not None:\n            output_g = self.ff(output_g)\n        output_h = self.ff(output_h)\n\n        outputs = (output_h, output_g) + outputs[2:]  # Add again attentions if there are there\n        return outputs\n\n\nclass XLNetPreTrainedModel(PreTrainedModel):\n    """""" An abstract class to handle weights initialization and\n        a simple interface for dowloading and loading pretrained models.\n    """"""\n    config_class = XLNetConfig\n    pretrained_model_archive_map = XLNET_PRETRAINED_MODEL_ARCHIVE_MAP\n    load_tf_weights = load_tf_weights_in_xlnet\n    base_model_prefix = ""transformer""\n\n    def _init_weights(self, module):\n        """""" Initialize the weights.\n        """"""\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if isinstance(module, nn.Linear) and module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, XLNetLayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        elif isinstance(module, XLNetRelativeAttention):\n            for param in [module.q, module.k, module.v, module.o, module.r,\n                          module.r_r_bias, module.r_s_bias, module.r_w_bias,\n                          module.seg_embed]:\n                param.data.normal_(mean=0.0, std=self.config.initializer_range)\n        elif isinstance(module, XLNetModel):\n                module.mask_emb.data.normal_(mean=0.0, std=self.config.initializer_range)\n\n\nXLNET_START_DOCSTRING = r""""""    The XLNet model was proposed in\n    `XLNet: Generalized Autoregressive Pretraining for Language Understanding`_\n    by Zhilin Yang*, Zihang Dai*, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le.\n    XLnet is an extension of the Transformer-XL model pre-trained using an autoregressive method\n    to learn bidirectional contexts by maximizing the expected likelihood over all permutations\n    of the input sequence factorization order.\n\n    The specific attention pattern can be controlled at training and test time using the `perm_mask` input.\n\n    Do to the difficulty of training a fully auto-regressive model over various factorization order,\n    XLNet is pretrained using only a sub-set of the output tokens as target which are selected\n    with the `target_mapping` input.\n\n    To use XLNet for sequential decoding (i.e. not in fully bi-directional setting), use the `perm_mask` and\n    `target_mapping` inputs to control the attention span and outputs (see examples in `examples/run_generation.py`)\n\n    This model is a PyTorch `torch.nn.Module`_ sub-class. Use it as a regular PyTorch Module and\n    refer to the PyTorch documentation for all matter related to general usage and behavior.\n\n    .. _`XLNet: Generalized Autoregressive Pretraining for Language Understanding`:\n        http://arxiv.org/abs/1906.08237\n\n    .. _`torch.nn.Module`:\n        https://pytorch.org/docs/stable/nn.html#module\n\n    Parameters:\n        config (:class:`~transformers.XLNetConfig`): Model configuration class with all the parameters of the model.\n            Initializing with a config file does not load the weights associated with the model, only the configuration.\n            Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model weights.\n""""""\n\nXLNET_INPUTS_DOCSTRING = r""""""\n    Inputs:\n        **input_ids**: ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Indices of input sequence tokens in the vocabulary.\n            XLNet is a model with relative position embeddings so you can either pad the inputs on\n            the right or on the left.\n            Indices can be obtained using :class:`transformers.XLNetTokenizer`.\n            See :func:`transformers.PreTrainedTokenizer.encode` and\n            :func:`transformers.PreTrainedTokenizer.convert_tokens_to_ids` for details.\n        **token_type_ids**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            A parallel sequence of tokens (can be used to indicate various portions of the inputs).\n            The type indices in XLNet are NOT selected in the vocabulary, they can be arbitrary numbers and\n            the important thing is that they should be different for tokens which belong to different segments.\n            The model will compute relative segment differences from the given type indices:\n            0 if the segment id of two tokens are the same, 1 if not.\n        **attention_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, sequence_length)``:\n            Mask to avoid performing attention on padding token indices.\n            Mask values selected in ``[0, 1]``:\n            ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n        **mems**: (`optional`)\n            list of ``torch.FloatTensor`` (one for each layer):\n            that contains pre-computed hidden-states (key and values in the attention blocks) as output by the model\n            (see `mems` output below). Can be used to speed up sequential decoding and attend to longer context.\n            To activate mems you need to set up config.mem_len to a positive value which will be the max number of tokens in\n            the memory output by the model. E.g. `model = XLNetModel.from_pretrained(\'xlnet-base-case, mem_len=1024)` will\n            instantiate a model which can use up to 1024 tokens of memory (in addition to the input it self).\n        **perm_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, sequence_length)``:\n            Mask to indicate the attention pattern for each input token with values selected in ``[0, 1]``:\n            If ``perm_mask[k, i, j] = 0``, i attend to j in batch k;\n            if ``perm_mask[k, i, j] = 1``, i does not attend to j in batch k.\n            If None, each token attends to all the others (full bidirectional attention).\n            Only used during pretraining (to define factorization order) or for sequential decoding (generation).\n        **target_mapping**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, num_predict, sequence_length)``:\n            Mask to indicate the output tokens to use.\n            If ``target_mapping[k, i, j] = 1``, the i-th predict in batch k is on the j-th token.\n            Only used during pretraining for partial prediction or for sequential decoding (generation).\n        **token_type_ids**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            A parallel sequence of tokens (can be used to indicate various portions of the inputs).\n            The type indices in XLNet are NOT selected in the vocabulary, they can be arbitrary numbers and\n            the important thing is that they should be different for tokens which belong to different segments.\n            The model will compute relative segment differences from the given type indices:\n            0 if the segment id of two tokens are the same, 1 if not.\n        **input_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, sequence_length)``:\n            Mask to avoid performing attention on padding token indices.\n            Negative of `attention_mask`, i.e. with 0 for real tokens and 1 for padding.\n            Kept for compatibility with the original code base.\n            You can only uses one of `input_mask` and `attention_mask`\n            Mask values selected in ``[0, 1]``:\n            ``1`` for tokens that are MASKED, ``0`` for tokens that are NOT MASKED.\n        **head_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(num_heads,)`` or ``(num_layers, num_heads)``:\n            Mask to nullify selected heads of the self-attention modules.\n            Mask values selected in ``[0, 1]``:\n            ``1`` indicates the head is **not masked**, ``0`` indicates the head is **masked**.\n""""""\n\n@add_start_docstrings(""The bare XLNet Model transformer outputting raw hidden-states without any specific head on top."",\n                      XLNET_START_DOCSTRING, XLNET_INPUTS_DOCSTRING)\nclass XLNetModel(XLNetPreTrainedModel):\n    r""""""\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **last_hidden_state**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, hidden_size)``\n            Sequence of hidden-states at the last layer of the model.\n        **mems**: (`optional`, returned when ``config.mem_len > 0``)\n            list of ``torch.FloatTensor`` (one for each layer):\n            that contains pre-computed hidden-states (key and values in the attention blocks) as computed by the model\n            if config.mem_len > 0 else tuple of None. Can be used to speed up sequential decoding and attend to longer context.\n            See details in the docstring of the `mems` input above.\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = XLNetTokenizer.from_pretrained(\'xlnet-large-cased\')\n        model = XLNetModel.from_pretrained(\'xlnet-large-cased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids)\n        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n\n    """"""\n    def __init__(self, config):\n        super(XLNetModel, self).__init__(config)\n        self.output_attentions = config.output_attentions\n        self.output_hidden_states = config.output_hidden_states\n        self.output_past = config.output_past\n\n        self.mem_len = config.mem_len\n        self.reuse_len = config.reuse_len\n        self.d_model = config.d_model\n        self.same_length = config.same_length\n        self.attn_type = config.attn_type\n        self.bi_data = config.bi_data\n        self.clamp_len = config.clamp_len\n        self.n_layer = config.n_layer\n\n        self.word_embedding = nn.Embedding(config.n_token, config.d_model)\n        self.mask_emb = nn.Parameter(torch.FloatTensor(1, 1, config.d_model))\n        self.layer = nn.ModuleList([XLNetLayer(config) for _ in range(config.n_layer)])\n        self.dropout = nn.Dropout(config.dropout)\n\n        self.init_weights()\n\n    def _resize_token_embeddings(self, new_num_tokens):\n        self.word_embedding = self._get_resized_embeddings(self.word_embedding, new_num_tokens)\n        return self.word_embedding\n\n    def _prune_heads(self, heads_to_prune):\n        raise NotImplementedError\n\n    def create_mask(self, qlen, mlen):\n        """"""\n        Creates causal attention mask. Float mask where 1.0 indicates masked, 0.0 indicates not-masked.\n\n        Args:\n            qlen: TODO Lysandre didn\'t fill\n            mlen: TODO Lysandre didn\'t fill\n\n        ::\n\n                  same_length=False:      same_length=True:\n                  <mlen > <  qlen >       <mlen > <  qlen >\n               ^ [0 0 0 0 0 1 1 1 1]     [0 0 0 0 0 1 1 1 1]\n                 [0 0 0 0 0 0 1 1 1]     [1 0 0 0 0 0 1 1 1]\n            qlen [0 0 0 0 0 0 0 1 1]     [1 1 0 0 0 0 0 1 1]\n                 [0 0 0 0 0 0 0 0 1]     [1 1 1 0 0 0 0 0 1]\n               v [0 0 0 0 0 0 0 0 0]     [1 1 1 1 0 0 0 0 0]\n\n        """"""\n        attn_mask = torch.ones([qlen, qlen])\n        mask_up = torch.triu(attn_mask, diagonal=1)\n        attn_mask_pad = torch.zeros([qlen, mlen])\n        ret = torch.cat([attn_mask_pad, mask_up], dim=1)\n        if self.same_length:\n            mask_lo = torch.tril(attn_mask, diagonal=-1)\n            ret = torch.cat([ret[:, :qlen] + mask_lo, ret[:, qlen:]], dim=1)\n\n        ret = ret.to(next(self.parameters()))\n        return ret\n\n    def cache_mem(self, curr_out, prev_mem):\n        """"""cache hidden states into memory.""""""\n        if self.reuse_len is not None and self.reuse_len > 0:\n            curr_out = curr_out[:self.reuse_len]\n\n        if prev_mem is None:\n            new_mem = curr_out[-self.mem_len:]\n        else:\n            new_mem = torch.cat([prev_mem, curr_out], dim=0)[-self.mem_len:]\n\n        return new_mem.detach()\n\n    @staticmethod\n    def positional_embedding(pos_seq, inv_freq, bsz=None):\n        sinusoid_inp = torch.einsum(\'i,d->id\', pos_seq, inv_freq)\n        pos_emb = torch.cat([torch.sin(sinusoid_inp), torch.cos(sinusoid_inp)], dim=-1)\n        pos_emb = pos_emb[:, None, :]\n\n        if bsz is not None:\n            pos_emb = pos_emb.expand(-1, bsz, -1)\n\n        return pos_emb\n\n    def relative_positional_encoding(self, qlen, klen, bsz=None):\n        """"""create relative positional encoding.""""""\n        freq_seq = torch.arange(0, self.d_model, 2.0, dtype=torch.float)\n        inv_freq = 1 / torch.pow(10000, (freq_seq / self.d_model))\n\n        if self.attn_type == \'bi\':\n            # beg, end = klen - 1, -qlen\n            beg, end = klen, -qlen\n        elif self.attn_type == \'uni\':\n            # beg, end = klen - 1, -1\n            beg, end = klen, -1\n        else:\n            raise ValueError(\'Unknown `attn_type` {}.\'.format(self.attn_type))\n\n        if self.bi_data:\n            fwd_pos_seq = torch.arange(beg, end, -1.0, dtype=torch.float)\n            bwd_pos_seq = torch.arange(-beg, -end, 1.0, dtype=torch.float)\n\n            if self.clamp_len > 0:\n                fwd_pos_seq = fwd_pos_seq.clamp(-self.clamp_len, self.clamp_len)\n                bwd_pos_seq = bwd_pos_seq.clamp(-self.clamp_len, self.clamp_len)\n\n            if bsz is not None:\n                fwd_pos_emb = self.positional_embedding(fwd_pos_seq, inv_freq, bsz//2)\n                bwd_pos_emb = self.positional_embedding(bwd_pos_seq, inv_freq, bsz//2)\n            else:\n                fwd_pos_emb = self.positional_embedding(fwd_pos_seq, inv_freq)\n                bwd_pos_emb = self.positional_embedding(bwd_pos_seq, inv_freq)\n\n            pos_emb = torch.cat([fwd_pos_emb, bwd_pos_emb], dim=1)\n        else:\n            fwd_pos_seq = torch.arange(beg, end, -1.0)\n            if self.clamp_len > 0:\n                fwd_pos_seq = fwd_pos_seq.clamp(-self.clamp_len, self.clamp_len)\n            pos_emb = self.positional_embedding(fwd_pos_seq, inv_freq, bsz)\n\n        pos_emb = pos_emb.to(next(self.parameters()))\n        return pos_emb\n\n    def forward(self, input_ids, attention_mask=None, mems=None, perm_mask=None, target_mapping=None,\n                token_type_ids=None, input_mask=None, head_mask=None):\n        # the original code for XLNet uses shapes [len, bsz] with the batch dimension at the end\n        # but we want a unified interface in the library with the batch size on the first dimension\n        # so we move here the first dimension (batch) to the end\n        input_ids = input_ids.transpose(0, 1).contiguous()\n        token_type_ids = token_type_ids.transpose(0, 1).contiguous() if token_type_ids is not None else None\n        input_mask = input_mask.transpose(0, 1).contiguous() if input_mask is not None else None\n        attention_mask = attention_mask.transpose(0, 1).contiguous() if attention_mask is not None else None\n        perm_mask = perm_mask.permute(1, 2, 0).contiguous() if perm_mask is not None else None\n        target_mapping = target_mapping.permute(1, 2, 0).contiguous() if target_mapping is not None else None\n\n        qlen, bsz = input_ids.shape[0], input_ids.shape[1]\n        mlen = mems[0].shape[0] if mems is not None and mems[0] is not None else 0\n        klen = mlen + qlen\n\n        dtype_float = next(self.parameters()).dtype\n        device = next(self.parameters()).device\n\n        ##### Attention mask\n        # causal attention mask\n        if self.attn_type == \'uni\':\n            attn_mask = self.create_mask(qlen, mlen)\n            attn_mask = attn_mask[:, :, None, None]\n        elif self.attn_type == \'bi\':\n            attn_mask = None\n        else:\n            raise ValueError(\'Unsupported attention type: {}\'.format(self.attn_type))\n\n        # data mask: input mask & perm mask\n        assert input_mask is None or attention_mask is None, ""You can only use one of input_mask (uses 1 for padding) ""\n        ""or attention_mask (uses 0 for padding, added for compatbility with BERT). Please choose one.""\n        if input_mask is None and attention_mask is not None:\n            input_mask = 1.0 - attention_mask\n        if input_mask is not None and perm_mask is not None:\n            data_mask = input_mask[None] + perm_mask\n        elif input_mask is not None and perm_mask is None:\n            data_mask = input_mask[None]\n        elif input_mask is None and perm_mask is not None:\n            data_mask = perm_mask\n        else:\n            data_mask = None\n\n        if data_mask is not None:\n            # all mems can be attended to\n            if mlen > 0:\n                mems_mask = torch.zeros([data_mask.shape[0], mlen, bsz]).to(data_mask)\n                data_mask = torch.cat([mems_mask, data_mask], dim=1)\n            if attn_mask is None:\n                attn_mask = data_mask[:, :, :, None]\n            else:\n                attn_mask += data_mask[:, :, :, None]\n\n        if attn_mask is not None:\n            attn_mask = (attn_mask > 0).to(dtype_float)\n\n        if attn_mask is not None:\n            non_tgt_mask = -torch.eye(qlen).to(attn_mask)\n            if mlen > 0:\n                non_tgt_mask = torch.cat([torch.zeros([qlen, mlen]).to(attn_mask), non_tgt_mask], dim=-1)\n            non_tgt_mask = ((attn_mask + non_tgt_mask[:, :, None, None]) > 0).to(attn_mask)\n        else:\n            non_tgt_mask = None\n\n        ##### Word embeddings and prepare h & g hidden states\n        word_emb_k = self.word_embedding(input_ids)\n        output_h = self.dropout(word_emb_k)\n        if target_mapping is not None:\n            word_emb_q = self.mask_emb.expand(target_mapping.shape[0], bsz, -1)\n        # else:  # We removed the inp_q input which was same as target mapping\n        #     inp_q_ext = inp_q[:, :, None]\n        #     word_emb_q = inp_q_ext * self.mask_emb + (1 - inp_q_ext) * word_emb_k\n            output_g = self.dropout(word_emb_q)\n        else:\n            output_g = None\n\n        ##### Segment embedding\n        if token_type_ids is not None:\n            # Convert `token_type_ids` to one-hot `seg_mat`\n            if mlen > 0:\n                mem_pad = torch.zeros([mlen, bsz], dtype=torch.long, device=device)\n                cat_ids = torch.cat([mem_pad, token_type_ids], dim=0)\n            else:\n                cat_ids = token_type_ids\n\n            # `1` indicates not in the same segment [qlen x klen x bsz]\n            seg_mat = (token_type_ids[:, None] != cat_ids[None, :]).long()\n            seg_mat = F.one_hot(seg_mat, num_classes=2).to(dtype_float)\n        else:\n            seg_mat = None\n\n        ##### Positional encoding\n        pos_emb = self.relative_positional_encoding(qlen, klen, bsz=bsz)\n        pos_emb = self.dropout(pos_emb)\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] (a head_mask for each layer)\n        # and head_mask is converted to shape [num_hidden_layers x qlen x klen x bsz x n_head]\n        if head_mask is not None:\n            if head_mask.dim() == 1:\n                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(0).unsqueeze(0)\n                head_mask = head_mask.expand(self.n_layer, -1, -1, -1, -1)\n            elif head_mask.dim() == 2:\n                head_mask = head_mask.unsqueeze(1).unsqueeze(1).unsqueeze(1)\n            head_mask = head_mask.to(dtype=next(self.parameters()).dtype) # switch to fload if need + fp16 compatibility\n        else:\n            head_mask = [None] * self.n_layer\n\n        new_mems = ()\n        if mems is None:\n            mems = [None] * len(self.layer)\n\n        attentions = []\n        hidden_states = []\n        for i, layer_module in enumerate(self.layer):\n            if self.mem_len is not None and self.mem_len > 0 and self.output_past:\n                # cache new mems\n                new_mems = new_mems + (self.cache_mem(output_h, mems[i]),)\n            if self.output_hidden_states:\n                hidden_states.append((output_h, output_g) if output_g is not None else output_h)\n\n            outputs = layer_module(output_h, output_g, attn_mask_h=non_tgt_mask, attn_mask_g=attn_mask,\n                                   r=pos_emb, seg_mat=seg_mat, mems=mems[i], target_mapping=target_mapping,\n                                   head_mask=head_mask[i])\n            output_h, output_g = outputs[:2]\n            if self.output_attentions:\n                attentions.append(outputs[2])\n\n        # Add last hidden state\n        if self.output_hidden_states:\n            hidden_states.append((output_h, output_g) if output_g is not None else output_h)\n\n        output = self.dropout(output_g if output_g is not None else output_h)\n\n        # Prepare outputs, we transpose back here to shape [bsz, len, hidden_dim] (cf. beginning of forward() method)\n        outputs = (output.permute(1, 0, 2).contiguous(),)\n\n        if self.mem_len is not None and self.mem_len > 0 and self.output_past:\n            outputs = outputs + (new_mems,)\n\n        if self.output_hidden_states:\n            if output_g is not None:\n                hidden_states = tuple(h.permute(1, 0, 2).contiguous() for hs in hidden_states for h in hs)\n            else:\n                hidden_states = tuple(hs.permute(1, 0, 2).contiguous() for hs in hidden_states)\n            outputs = outputs + (hidden_states,)\n        if self.output_attentions:\n            attentions = tuple(t.permute(2, 3, 0, 1).contiguous() for t in attentions)\n            outputs = outputs + (attentions,)\n\n        return outputs  # outputs, (new_mems), (hidden_states), (attentions)\n\n\n@add_start_docstrings(""""""XLNet Model with a language modeling head on top\n    (linear layer with weights tied to the input embeddings). """""",\n    XLNET_START_DOCSTRING, XLNET_INPUTS_DOCSTRING)\nclass XLNetLMHeadModel(XLNetPreTrainedModel):\n    r""""""\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Labels for language modeling.\n            Note that the labels **are shifted** inside the model, i.e. you can set ``lm_labels = input_ids``\n            Indices are selected in ``[-1, 0, ..., config.vocab_size]``\n            All labels set to ``-1`` are ignored (masked), the loss is only\n            computed for labels in ``[0, ..., config.vocab_size]``\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Language modeling loss.\n        **prediction_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.vocab_size)``\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n        **mems**: (`optional`, returned when ``config.mem_len > 0``)\n            list of ``torch.FloatTensor`` (one for each layer):\n            that contains pre-computed hidden-states (key and values in the attention blocks) as computed by the model\n            if config.mem_len > 0 else tuple of None. Can be used to speed up sequential decoding and attend to longer context.\n            See details in the docstring of the `mems` input above.\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = XLNetTokenizer.from_pretrained(\'xlnet-large-cased\')\n        model = XLNetLMHeadModel.from_pretrained(\'xlnet-large-cased\')\n        # We show how to setup inputs to predict a next token using a bi-directional context.\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is very <mask>"")).unsqueeze(0)  # We will predict the masked token\n        perm_mask = torch.zeros((1, input_ids.shape[1], input_ids.shape[1]), dtype=torch.float)\n        perm_mask[:, :, -1] = 1.0  # Previous tokens don\'t see last token\n        target_mapping = torch.zeros((1, 1, input_ids.shape[1]), dtype=torch.float)  # Shape [1, 1, seq_length] => let\'s predict one token\n        target_mapping[0, 0, -1] = 1.0  # Our first (and only) prediction will be the last token of the sequence (the masked token)\n        outputs = model(input_ids, perm_mask=perm_mask, target_mapping=target_mapping)\n        next_token_logits = outputs[0]  # Output has shape [target_mapping.size(0), target_mapping.size(1), config.vocab_size]\n\n    """"""\n    def __init__(self, config):\n        super(XLNetLMHeadModel, self).__init__(config)\n        self.attn_type = config.attn_type\n        self.same_length = config.same_length\n\n        self.transformer = XLNetModel(config)\n        self.lm_loss = nn.Linear(config.d_model, config.n_token, bias=True)\n\n        self.init_weights()\n        self.tie_weights()\n\n    def tie_weights(self):\n        """""" Make sure we are sharing the embeddings\n        """"""\n        self._tie_or_clone_weights(self.lm_loss, self.transformer.word_embedding)\n\n    def forward(self, input_ids, attention_mask=None, mems=None, perm_mask=None, target_mapping=None,\n                token_type_ids=None, input_mask=None, head_mask=None, labels=None):\n        transformer_outputs = self.transformer(input_ids,\n                                               attention_mask=attention_mask,\n                                               mems=mems,\n                                               perm_mask=perm_mask,\n                                               target_mapping=target_mapping,\n                                               token_type_ids=token_type_ids,\n                                               input_mask=input_mask,\n                                               head_mask=head_mask)\n\n        logits = self.lm_loss(transformer_outputs[0])\n\n        outputs = (logits,) + transformer_outputs[1:]  # Keep mems, hidden states, attentions if there are in it\n\n        if labels is not None:\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            loss = loss_fct(logits.view(-1, logits.size(-1)),\n                            labels.view(-1))\n            outputs = (loss,) + outputs\n\n        return outputs  # return (loss), logits, (mems), (hidden states), (attentions)\n\n\n@add_start_docstrings(""""""XLNet Model with a sequence classification/regression head on top (a linear layer on top of\n    the pooled output) e.g. for GLUE tasks. """""",\n    XLNET_START_DOCSTRING, XLNET_INPUTS_DOCSTRING)\nclass XLNetForSequenceClassification(XLNetPreTrainedModel):\n    r""""""\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the sequence classification/regression loss.\n            Indices should be in ``[0, ..., config.num_labels - 1]``.\n            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Classification (or regression if config.num_labels==1) loss.\n        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n        **mems**: (`optional`, returned when ``config.mem_len > 0``)\n            list of ``torch.FloatTensor`` (one for each layer):\n            that contains pre-computed hidden-states (key and values in the attention blocks) as computed by the model\n            if config.mem_len > 0 else tuple of None. Can be used to speed up sequential decoding and attend to longer context.\n            See details in the docstring of the `mems` input above.\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = XLNetTokenizer.from_pretrained(\'xlnet-large-cased\')\n        model = XLNetForSequenceClassification.from_pretrained(\'xlnet-large-cased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, labels=labels)\n        loss, logits = outputs[:2]\n\n    """"""\n    def __init__(self, config):\n        super(XLNetForSequenceClassification, self).__init__(config)\n        self.num_labels = config.num_labels\n\n        self.transformer = XLNetModel(config)\n        self.sequence_summary = SequenceSummary(config)\n        self.logits_proj = nn.Linear(config.d_model, config.num_labels)\n\n        self.init_weights()\n\n    def forward(self, input_ids, attention_mask=None, mems=None, perm_mask=None, target_mapping=None,\n                token_type_ids=None, input_mask=None, head_mask=None, labels=None):\n        transformer_outputs = self.transformer(input_ids,\n                                               attention_mask=attention_mask,\n                                               mems=mems,\n                                               perm_mask=perm_mask,\n                                               target_mapping=target_mapping,\n                                               token_type_ids=token_type_ids,\n                                               input_mask=input_mask,\n                                               head_mask=head_mask)\n        output = transformer_outputs[0]\n\n        output = self.sequence_summary(output)\n        logits = self.logits_proj(output)\n\n        outputs = (logits,) + transformer_outputs[1:]  # Keep mems, hidden states, attentions if there are in it\n\n        if labels is not None:\n            if self.num_labels == 1:\n                #  We are doing regression\n                loss_fct = MSELoss()\n                loss = loss_fct(logits.view(-1), labels.view(-1))\n            else:\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            outputs = (loss,) + outputs\n\n        return outputs  # return (loss), logits, (mems), (hidden states), (attentions)\n\n@add_start_docstrings(""""""XLNet Model with a multiple choice classification head on top (a linear layer on top of\n    the pooled output and a softmax) e.g. for RACE/SWAG tasks. """""",\n    XLNET_START_DOCSTRING, XLNET_INPUTS_DOCSTRING)\nclass XLNetForMultipleChoice(XLNetPreTrainedModel):\n    r""""""\n    Inputs:\n        **input_ids**: ``torch.LongTensor`` of shape ``(batch_size, num_choices, sequence_length)``:\n            Indices of input sequence tokens in the vocabulary.\n            The second dimension of the input (`num_choices`) indicates the number of choices to scores.\n        **token_type_ids**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, num_choices, sequence_length)``:\n            Segment token indices to indicate first and second portions of the inputs.\n            The second dimension of the input (`num_choices`) indicates the number of choices to score.\n            Indices are selected in ``[0, 1]``: ``0`` corresponds to a `sentence A` token, ``1``\n        **attention_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, num_choices, sequence_length)``:\n            Mask to avoid performing attention on padding token indices.\n            The second dimension of the input (`num_choices`) indicates the number of choices to score.\n            Mask values selected in ``[0, 1]``:\n            ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n        **head_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(num_heads,)`` or ``(num_layers, num_heads)``:\n            Mask to nullify selected heads of the self-attention modules.\n            Mask values selected in ``[0, 1]``:\n            ``1`` indicates the head is **not masked**, ``0`` indicates the head is **masked**.\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the multiple choice classification loss.\n            Indices should be in ``[0, ..., num_choices]`` where `num_choices` is the size of the second dimension\n            of the input tensors. (see `input_ids` above)\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Classification loss.\n        **classification_scores**: ``torch.FloatTensor`` of shape ``(batch_size, num_choices)`` where `num_choices` is the size of the second dimension\n            of the input tensors. (see `input_ids` above).\n            Classification scores (before SoftMax).\n        **mems**: (`optional`, returned when ``config.mem_len > 0``)\n            list of ``torch.FloatTensor`` (one for each layer):\n            that contains pre-computed hidden-states (key and values in the attention blocks) as computed by the model\n            if config.mem_len > 0 else tuple of None. Can be used to speed up sequential decoding and attend to longer context.\n            See details in the docstring of the `mems` input above.\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = XLNetTokenizer.from_pretrained(\'xlnet-base-cased\')\n        model = XLNetForMultipleChoice.from_pretrained(\'xlnet-base-cased\')\n        choices = [""Hello, my dog is cute"", ""Hello, my cat is amazing""]\n        input_ids = torch.tensor([tokenizer.encode(s) for s in choices]).unsqueeze(0)  # Batch size 1, 2 choices\n        labels = torch.tensor(1).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, labels=labels)\n        loss, classification_scores = outputs[:2]\n\n    """"""\n    def __init__(self, config):\n        super(XLNetForMultipleChoice, self).__init__(config)\n\n        self.transformer = XLNetModel(config)\n        self.sequence_summary = SequenceSummary(config)\n        self.logits_proj = nn.Linear(config.d_model, 1)\n\n        self.init_weights()\n\n    def forward(self, input_ids, token_type_ids=None, input_mask=None, attention_mask=None,\n                mems=None, perm_mask=None, target_mapping=None,\n                labels=None, head_mask=None):\n        num_choices = input_ids.shape[1]\n\n        flat_input_ids = input_ids.view(-1, input_ids.size(-1))\n        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n        flat_input_mask = input_mask.view(-1, input_mask.size(-1)) if input_mask is not None else None\n\n        transformer_outputs = self.transformer(flat_input_ids, token_type_ids=flat_token_type_ids,\n                                               input_mask=flat_input_mask, attention_mask=flat_attention_mask,\n                                               mems=mems, perm_mask=perm_mask, target_mapping=target_mapping,\n                                               head_mask=head_mask)\n\n\n        output = transformer_outputs[0]\n\n        output = self.sequence_summary(output)\n        logits = self.logits_proj(output)\n        reshaped_logits = logits.view(-1, num_choices)\n        outputs = (reshaped_logits,) + transformer_outputs[1:]  # Keep mems, hidden states, attentions if there are in it\n\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(reshaped_logits, labels.view(-1))\n            outputs = (loss,) + outputs\n\n        return outputs  # return (loss), logits, (mems), (hidden states), (attentions)\n\n\n@add_start_docstrings(""""""XLNet Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear layers on top of\n    the hidden-states output to compute `span start logits` and `span end logits`). """""",\n    XLNET_START_DOCSTRING, XLNET_INPUTS_DOCSTRING)\nclass XLNetForQuestionAnsweringSimple(XLNetPreTrainedModel):\n    r""""""\n        **start_positions**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`).\n            Position outside of the sequence are not taken into account for computing the loss.\n        **end_positions**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`).\n            Position outside of the sequence are not taken into account for computing the loss.\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned if both ``start_positions`` and ``end_positions`` are provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Classification loss as the sum of start token, end token (and is_impossible if provided) classification losses.\n        **start_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length,)``\n            Span-start scores (before SoftMax).\n        **end_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length,)``\n            Span-end scores (before SoftMax).\n        **mems**: (`optional`, returned when ``config.mem_len > 0``)\n            list of ``torch.FloatTensor`` (one for each layer):\n            that contains pre-computed hidden-states (key and values in the attention blocks) as computed by the model\n            if config.mem_len > 0 else tuple of None. Can be used to speed up sequential decoding and attend to longer context.\n            See details in the docstring of the `mems` input above.\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = XLMTokenizer.from_pretrained(\'xlm-mlm-en-2048\')\n        model = XLMForQuestionAnswering.from_pretrained(\'xlnet-large-cased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        start_positions = torch.tensor([1])\n        end_positions = torch.tensor([3])\n        outputs = model(input_ids, start_positions=start_positions, end_positions=end_positions)\n        loss, start_scores, end_scores = outputs[:2]\n\n    """"""\n    def __init__(self, config):\n        super(XLNetForQuestionAnsweringSimple, self).__init__(config)\n        self.num_labels = config.num_labels\n\n        self.transformer = XLNetModel(config)\n        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    def forward(self, input_ids, attention_mask=None, mems=None, perm_mask=None, target_mapping=None,\n                token_type_ids=None, input_mask=None, head_mask=None,\n                start_positions=None, end_positions=None):\n\n        outputs = self.transformer(input_ids,\n                                    attention_mask=attention_mask,\n                                    mems=mems,\n                                    perm_mask=perm_mask,\n                                    target_mapping=target_mapping,\n                                    token_type_ids=token_type_ids,\n                                    input_mask=input_mask,\n                                    head_mask=head_mask)\n\n        sequence_output = outputs[0]\n\n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        outputs = (start_logits, end_logits,) + outputs[2:]\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions.clamp_(0, ignored_index)\n            end_positions.clamp_(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n            outputs = (total_loss,) + outputs\n\n        return outputs  # (loss), start_logits, end_logits, (mems), (hidden_states), (attentions)\n\n\n@add_start_docstrings(""""""XLNet Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear layers on top of\n    the hidden-states output to compute `span start logits` and `span end logits`). """""",\n    XLNET_START_DOCSTRING, XLNET_INPUTS_DOCSTRING)\nclass XLNetForQuestionAnswering(XLNetPreTrainedModel):\n    r""""""\n        **start_positions**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`).\n            Position outside of the sequence are not taken into account for computing the loss.\n        **end_positions**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`).\n            Position outside of the sequence are not taken into account for computing the loss.\n        **is_impossible**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels whether a question has an answer or no answer (SQuAD 2.0)\n        **cls_index**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for position (index) of the classification token to use as input for computing plausibility of the answer.\n        **p_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, sequence_length)``:\n            Optional mask of tokens which can\'t be in answers (e.g. [CLS], [PAD], ...).\n            1.0 means token should be masked. 0.0 mean token is not masked.\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned if both ``start_positions`` and ``end_positions`` are provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Classification loss as the sum of start token, end token (and is_impossible if provided) classification losses.\n        **start_top_log_probs**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)\n            ``torch.FloatTensor`` of shape ``(batch_size, config.start_n_top)``\n            Log probabilities for the top config.start_n_top start token possibilities (beam-search).\n        **start_top_index**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)\n            ``torch.LongTensor`` of shape ``(batch_size, config.start_n_top)``\n            Indices for the top config.start_n_top start token possibilities (beam-search).\n        **end_top_log_probs**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)\n            ``torch.FloatTensor`` of shape ``(batch_size, config.start_n_top * config.end_n_top)``\n            Log probabilities for the top ``config.start_n_top * config.end_n_top`` end token possibilities (beam-search).\n        **end_top_index**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)\n            ``torch.LongTensor`` of shape ``(batch_size, config.start_n_top * config.end_n_top)``\n            Indices for the top ``config.start_n_top * config.end_n_top`` end token possibilities (beam-search).\n        **cls_logits**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)\n            ``torch.FloatTensor`` of shape ``(batch_size,)``\n            Log probabilities for the ``is_impossible`` label of the answers.\n        **mems**: (`optional`, returned when ``config.mem_len > 0``)\n            list of ``torch.FloatTensor`` (one for each layer):\n            that contains pre-computed hidden-states (key and values in the attention blocks) as computed by the model\n            if config.mem_len > 0 else tuple of None. Can be used to speed up sequential decoding and attend to longer context.\n            See details in the docstring of the `mems` input above.\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer =  XLNetTokenizer.from_pretrained(\'xlnet-large-cased\')\n        model = XLMForQuestionAnswering.from_pretrained(\'xlnet-large-cased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        start_positions = torch.tensor([1])\n        end_positions = torch.tensor([3])\n        outputs = model(input_ids, start_positions=start_positions, end_positions=end_positions)\n        loss, start_scores, end_scores = outputs[:2]\n\n    """"""\n    def __init__(self, config):\n        super(XLNetForQuestionAnswering, self).__init__(config)\n        self.start_n_top = config.start_n_top\n        self.end_n_top = config.end_n_top\n\n        self.transformer = XLNetModel(config)\n        self.start_logits = PoolerStartLogits(config)\n        self.end_logits = PoolerEndLogits(config)\n        self.answer_class = PoolerAnswerClass(config)\n\n        self.init_weights()\n\n    def forward(self, input_ids, attention_mask=None, mems=None, perm_mask=None, target_mapping=None,\n                token_type_ids=None, input_mask=None, head_mask=None,\n                start_positions=None, end_positions=None, is_impossible=None, cls_index=None, p_mask=None,):\n        transformer_outputs = self.transformer(input_ids,\n                                               attention_mask=attention_mask,\n                                               mems=mems,\n                                               perm_mask=perm_mask,\n                                               target_mapping=target_mapping,\n                                               token_type_ids=token_type_ids,\n                                               input_mask=input_mask,\n                                               head_mask=head_mask)\n        hidden_states = transformer_outputs[0]\n        start_logits = self.start_logits(hidden_states, p_mask=p_mask)\n\n        outputs = transformer_outputs[1:]  # Keep mems, hidden states, attentions if there are in it\n\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, let\'s remove the dimension added by batch splitting\n            for x in (start_positions, end_positions, cls_index, is_impossible):\n                if x is not None and x.dim() > 1:\n                    x.squeeze_(-1)\n\n            # during training, compute the end logits based on the ground truth of the start position\n            end_logits = self.end_logits(hidden_states, start_positions=start_positions, p_mask=p_mask)\n\n            loss_fct = CrossEntropyLoss()\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n\n            if cls_index is not None and is_impossible is not None:\n                # Predict answerability from the representation of CLS and START\n                cls_logits = self.answer_class(hidden_states, start_positions=start_positions, cls_index=cls_index)\n                loss_fct_cls = nn.BCEWithLogitsLoss()\n                cls_loss = loss_fct_cls(cls_logits, is_impossible)\n\n                # note(zhiliny): by default multiply the loss by 0.5 so that the scale is comparable to start_loss and end_loss\n                total_loss += cls_loss * 0.5\n\n            outputs = (total_loss,) + outputs\n\n        else:\n            # during inference, compute the end logits based on beam search\n            bsz, slen, hsz = hidden_states.size()\n            start_log_probs = F.softmax(start_logits, dim=-1) # shape (bsz, slen)\n\n            start_top_log_probs, start_top_index = torch.topk(start_log_probs, self.start_n_top, dim=-1) # shape (bsz, start_n_top)\n            start_top_index_exp = start_top_index.unsqueeze(-1).expand(-1, -1, hsz) # shape (bsz, start_n_top, hsz)\n            start_states = torch.gather(hidden_states, -2, start_top_index_exp) # shape (bsz, start_n_top, hsz)\n            start_states = start_states.unsqueeze(1).expand(-1, slen, -1, -1) # shape (bsz, slen, start_n_top, hsz)\n\n            hidden_states_expanded = hidden_states.unsqueeze(2).expand_as(start_states) # shape (bsz, slen, start_n_top, hsz)\n            p_mask = p_mask.unsqueeze(-1) if p_mask is not None else None\n            end_logits = self.end_logits(hidden_states_expanded, start_states=start_states, p_mask=p_mask)\n            end_log_probs = F.softmax(end_logits, dim=1) # shape (bsz, slen, start_n_top)\n\n            end_top_log_probs, end_top_index = torch.topk(end_log_probs, self.end_n_top, dim=1) # shape (bsz, end_n_top, start_n_top)\n            end_top_log_probs = end_top_log_probs.view(-1, self.start_n_top * self.end_n_top)\n            end_top_index = end_top_index.view(-1, self.start_n_top * self.end_n_top)\n\n            start_states = torch.einsum(""blh,bl->bh"", hidden_states, start_log_probs)  # get the representation of START as weighted sum of hidden states\n            cls_logits = self.answer_class(hidden_states, start_states=start_states, cls_index=cls_index)  # Shape (batch size,): one single `cls_logits` for each sample\n\n            outputs = (start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits) + outputs\n\n        # return start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits\n        # or (if labels are provided) (total_loss,)\n        return outputs\n'"
models/transformers/tokenization_albert.py,0,"b'""""""Tokenization classes.""""""\n\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\nimport collections\nimport unicodedata\nimport six\nimport logging\nimport sentencepiece as spm\n\nlogger = logging.getLogger(__name__)\nSPIECE_UNDERLINE = u""\xe2\x96\x81""\n\ndef preprocess_text(inputs,remove_space=True,do_lower_case=True):\n  if remove_space:\n    outputs = \' \'.join(inputs.strip().split())\n  else:\n    outputs = inputs\n  outputs = outputs.replace(""``"", \'""\').replace(""\'\'"", \'""\')\n  if six.PY2 and isinstance(outputs, str):\n    outputs = outputs.decode(\'utf-8\')\n  outputs = unicodedata.normalize(""NFKD"", outputs)\n  outputs = """".join([c for c in outputs if not unicodedata.combining(c)])\n  if do_lower_case:\n    outputs = outputs.lower()\n  return outputs\n\ndef encode_pieces(sp_model, text, return_unicode=True, sample=False):\n  """"""turn sentences into word pieces.""""""\n  text = preprocess_text(text,)\n  if six.PY2 and isinstance(text, unicode):\n    text = text.encode(\'utf-8\')\n  if not sample:\n    pieces = sp_model.EncodeAsPieces(text)\n  else:\n    pieces = sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n  new_pieces = []\n  for piece in pieces:\n    if len(piece) > 1 and piece[-1] == \',\' and piece[-2].isdigit():\n      cur_pieces = sp_model.EncodeAsPieces(\n        piece[:-1].replace(SPIECE_UNDERLINE, \'\'))\n      if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n        if len(cur_pieces[0]) == 1:\n          cur_pieces = cur_pieces[1:]\n        else:\n          cur_pieces[0] = cur_pieces[0][1:]\n      cur_pieces.append(piece[-1])\n      new_pieces.extend(cur_pieces)\n    else:\n      new_pieces.append(piece)\n\n  # note(zhiliny): convert back to unicode for py2\n  if six.PY2 and return_unicode:\n    ret_pieces = []\n    for piece in new_pieces:\n      if isinstance(piece, str):\n        piece = piece.decode(piece, ""utf-8"")\n      ret_pieces.append(piece)\n    new_pieces = ret_pieces\n\n  return new_pieces\n\ndef encode_ids(sp_model, text, sample=False):\n  pieces = encode_pieces(sp_model, text, return_unicode=False, sample=sample)\n  ids = [sp_model.PieceToId(piece) for piece in pieces]\n  return ids\n\n\ndef load_vocab(vocab_file):\n  """"""Loads a vocabulary file into a dictionary.""""""\n  vocab = collections.OrderedDict()\n  with open(vocab_file, ""r"", encoding=""utf-8"") as reader:\n    tokens = reader.readlines()\n  for index, token in enumerate(tokens):\n    token = token.rstrip(\'\\n\')\n    vocab[token] = index\n  return vocab\n\ndef convert_by_vocab(vocab, items):\n  """"""Converts a sequence of [tokens|ids] using the vocab.""""""\n  output = []\n  for item in items:\n    output.append(vocab[item])\n  return output\n\ndef convert_tokens_to_ids(vocab, tokens):\n  return convert_by_vocab(vocab, tokens)\n\ndef convert_ids_to_tokens(inv_vocab, ids):\n  return convert_by_vocab(inv_vocab, ids)\n\ndef whitespace_tokenize(text):\n  """"""Runs basic whitespace cleaning and splitting on a piece of text.""""""\n  text = text.strip()\n  if not text:\n    return []\n  tokens = text.split()\n  return tokens\n\nclass FullTokenizer(object):\n  """"""Runs end-to-end tokenziation.""""""\n\n  def __init__(self, vocab_file, do_lower_case=True, spm_model_file=None):\n    self.vocab = None\n    self.sp_model = None\n    if spm_model_file:\n      self.sp_model = spm.SentencePieceProcessor()\n      logger.info(""loading sentence piece model"")\n      self.sp_model.Load(spm_model_file)\n      # # Note(mingdachen): For the purpose of consisent API, we are\n      # # generating a vocabulary for the sentence piece tokenizer.\n      self.vocab = {self.sp_model.IdToPiece(i): i for i\n                    in range(self.sp_model.GetPieceSize())}\n    else:\n      print(""load vocab"")\n      self.vocab = load_vocab(vocab_file)\n      print(""load token"")\n      self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n      self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab,unk_token=""[UNK]"", max_input_chars_per_word=100)\n    self.inv_vocab = {v: k for k, v in self.vocab.items()}\n\n  def tokenize(self, text):\n    if self.sp_model:\n      split_tokens = encode_pieces(self.sp_model, text, return_unicode=False)\n    else:\n      split_tokens = []\n      for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n          split_tokens.append(sub_token)\n\n    return split_tokens\n\n  def convert_tokens_to_ids(self, tokens):\n    if self.sp_model:\n      return [self.sp_model.PieceToId(token) for token in tokens]\n    else:\n      return convert_by_vocab(self.vocab, tokens)\n\n  def convert_ids_to_tokens(self, ids):\n    if self.sp_model:\n      logger.info(""using sentence piece tokenzier."")\n      return [self.sp_model.IdToPiece(id_) for id_ in ids]\n    else:\n      return convert_by_vocab(self.inv_vocab, ids)\n\nclass BasicTokenizer(object):\n  """"""Runs basic tokenization (punctuation splitting, lower casing, etc.).""""""\n\n  def __init__(self, do_lower_case=True):\n    """"""Constructs a BasicTokenizer.\n\n    Args:\n      do_lower_case: Whether to lower case the input.\n    """"""\n    self.do_lower_case = do_lower_case\n\n  def tokenize(self, text):\n    """"""Tokenizes a piece of text.""""""\n    text = self._clean_text(text)\n\n    # This was added on November 1st, 2018 for the multilingual and Chinese\n    # models. This is also applied to the English models now, but it doesn\'t\n    # matter since the English models were not trained on any Chinese data\n    # and generally don\'t have any Chinese data in them (there are Chinese\n    # characters in the vocabulary because Wikipedia does have some Chinese\n    # words in the English Wikipedia.).\n    text = self._tokenize_chinese_chars(text)\n    orig_tokens = whitespace_tokenize(text)\n    split_tokens = []\n    for token in orig_tokens:\n      if self.do_lower_case:\n        token = token.lower()\n        token = self._run_strip_accents(token)\n      split_tokens.extend(self._run_split_on_punc(token))\n    output_tokens = whitespace_tokenize("" "".join(split_tokens))\n    return output_tokens\n\n  def _run_strip_accents(self, text):\n    """"""Strips accents from a piece of text.""""""\n    text = unicodedata.normalize(""NFD"", text)\n    output = []\n    for char in text:\n      cat = unicodedata.category(char)\n      if cat == ""Mn"":\n        continue\n      output.append(char)\n    return """".join(output)\n\n  def _run_split_on_punc(self, text):\n    """"""Splits punctuation on a piece of text.""""""\n    chars = list(text)\n    i = 0\n    start_new_word = True\n    output = []\n    while i < len(chars):\n      char = chars[i]\n      if _is_punctuation(char):\n        output.append([char])\n        start_new_word = True\n      else:\n        if start_new_word:\n          output.append([])\n        start_new_word = False\n        output[-1].append(char)\n      i += 1\n\n    return ["""".join(x) for x in output]\n\n  def _tokenize_chinese_chars(self, text):\n    """"""Adds whitespace around any CJK character.""""""\n    output = []\n    for char in text:\n      cp = ord(char)\n      if self._is_chinese_char(cp):\n        output.append("" "")\n        output.append(char)\n        output.append("" "")\n      else:\n        output.append(char)\n    return """".join(output)\n\n  def _is_chinese_char(self, cp):\n    """"""Checks whether CP is the codepoint of a CJK character.""""""\n    # This defines a ""chinese character"" as anything in the CJK Unicode block:\n    #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n    #\n    # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n    # despite its name. The modern Korean Hangul alphabet is a different block,\n    # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n    # space-separated words, so they are not treated specially and handled\n    # like the all of the other languages.\n    if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n        (cp >= 0x3400 and cp <= 0x4DBF) or  #\n        (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n        (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n        (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n        (cp >= 0x2B820 and cp <= 0x2CEAF) or\n        (cp >= 0xF900 and cp <= 0xFAFF) or  #\n        (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n      return True\n\n    return False\n\n  def _clean_text(self, text):\n    """"""Performs invalid character removal and whitespace cleanup on text.""""""\n    output = []\n    for char in text:\n      cp = ord(char)\n      if cp == 0 or cp == 0xfffd or _is_control(char):\n        continue\n      if _is_whitespace(char):\n        output.append("" "")\n      else:\n        output.append(char)\n    return """".join(output)\n\nclass WordpieceTokenizer(object):\n    """"""Runs WordPiece tokenization.""""""\n\n    def __init__(self, vocab, unk_token, max_input_chars_per_word=100):\n      self.vocab = vocab\n      self.unk_token = unk_token\n      self.max_input_chars_per_word = max_input_chars_per_word\n\n    def tokenize(self, text):\n      """"""Tokenizes a piece of text into its word pieces.\n\n      This uses a greedy longest-match-first algorithm to perform tokenization\n      using the given vocabulary.\n\n      For example:\n        input = ""unaffable""\n        output = [""un"", ""##aff"", ""##able""]\n\n      Args:\n        text: A single token or whitespace separated tokens. This should have\n          already been passed through `BasicTokenizer`.\n\n      Returns:\n        A list of wordpiece tokens.\n      """"""\n\n      output_tokens = []\n      for token in whitespace_tokenize(text):\n        chars = list(token)\n        if len(chars) > self.max_input_chars_per_word:\n          output_tokens.append(self.unk_token)\n          continue\n\n        is_bad = False\n        start = 0\n        sub_tokens = []\n        while start < len(chars):\n          end = len(chars)\n          cur_substr = None\n          while start < end:\n            substr = """".join(chars[start:end])\n            if start > 0:\n              substr = ""##"" + substr\n            if substr in self.vocab:\n              cur_substr = substr\n              break\n            end -= 1\n          if cur_substr is None:\n            is_bad = True\n            break\n          sub_tokens.append(cur_substr)\n          start = end\n\n        if is_bad:\n          output_tokens.append(self.unk_token)\n        else:\n          output_tokens.extend(sub_tokens)\n      return output_tokens\n\ndef _is_whitespace(char):\n  """"""Checks whether `chars` is a whitespace character.""""""\n  # \\t, \\n, and \\r are technically control characters but we treat them\n  # as whitespace since they are generally considered as such.\n  if char == "" "" or char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n    return True\n  cat = unicodedata.category(char)\n  if cat == ""Zs"":\n    return True\n  return False\n\n\ndef _is_control(char):\n  """"""Checks whether `chars` is a control character.""""""\n  # These are technically control characters but we count them as whitespace\n  # characters.\n  if char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n    return False\n  cat = unicodedata.category(char)\n  if cat in (""Cc"", ""Cf""):\n    return True\n  return False\n\ndef _is_punctuation(char):\n  """"""Checks whether `chars` is a punctuation character.""""""\n  cp = ord(char)\n  # We treat all non-letter/number ASCII as punctuation.\n  # Characters such as ""^"", ""$"", and ""`"" are not in the Unicode\n  # Punctuation class but we treat them as punctuation anyways, for\n  # consistency.\n  if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n      (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n    return True\n  cat = unicodedata.category(char)\n  if cat.startswith(""P""):\n    return True\n  return False\n'"
models/transformers/tokenization_auto.py,0,"b'# coding=utf-8\n# Copyright 2018 The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" Auto Model class. """"""\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport logging\n\nfrom .tokenization_bert import BertTokenizer\nfrom .tokenization_openai import OpenAIGPTTokenizer\nfrom .tokenization_gpt2 import GPT2Tokenizer\nfrom .tokenization_ctrl import CTRLTokenizer\nfrom .tokenization_transfo_xl import TransfoXLTokenizer\nfrom .tokenization_xlnet import XLNetTokenizer\nfrom .tokenization_xlm import XLMTokenizer\nfrom .tokenization_roberta import RobertaTokenizer\nfrom .tokenization_distilbert import DistilBertTokenizer\n\nlogger = logging.getLogger(__name__)\n\nclass AutoTokenizer(object):\n    r"""""":class:`~transformers.AutoTokenizer` is a generic tokenizer class\n        that will be instantiated as one of the tokenizer classes of the library\n        when created with the `AutoTokenizer.from_pretrained(pretrained_model_name_or_path)`\n        class method.\n\n        The `from_pretrained()` method take care of returning the correct tokenizer class instance\n        using pattern matching on the `pretrained_model_name_or_path` string.\n\n        The tokenizer class to instantiate is selected as the first pattern matching\n        in the `pretrained_model_name_or_path` string (in the following order):\n            - contains `distilbert`: DistilBertTokenizer (DistilBert model)\n            - contains `roberta`: RobertaTokenizer (RoBERTa model)\n            - contains `bert`: BertTokenizer (Bert model)\n            - contains `openai-gpt`: OpenAIGPTTokenizer (OpenAI GPT model)\n            - contains `gpt2`: GPT2Tokenizer (OpenAI GPT-2 model)\n            - contains `ctrl`: CTRLTokenizer (Salesforce CTRL model)\n            - contains `transfo-xl`: TransfoXLTokenizer (Transformer-XL model)\n            - contains `xlnet`: XLNetTokenizer (XLNet model)\n            - contains `xlm`: XLMTokenizer (XLM model)\n\n        This class cannot be instantiated using `__init__()` (throw an error).\n    """"""\n    def __init__(self):\n        raise EnvironmentError(""AutoTokenizer is designed to be instantiated ""\n            ""using the `AutoTokenizer.from_pretrained(pretrained_model_name_or_path)` method."")\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n        r"""""" Instantiate a one of the tokenizer classes of the library\n        from a pre-trained model vocabulary.\n\n        The tokenizer class to instantiate is selected as the first pattern matching\n        in the `pretrained_model_name_or_path` string (in the following order):\n            - contains `distilbert`: DistilBertTokenizer (DistilBert model)\n            - contains `roberta`: RobertaTokenizer (XLM model)\n            - contains `bert`: BertTokenizer (Bert model)\n            - contains `openai-gpt`: OpenAIGPTTokenizer (OpenAI GPT model)\n            - contains `gpt2`: GPT2Tokenizer (OpenAI GPT-2 model)\n            - contains `ctrl`: CTRLTokenizer (Salesforce CTRL model)\n            - contains `transfo-xl`: TransfoXLTokenizer (Transformer-XL model)\n            - contains `xlnet`: XLNetTokenizer (XLNet model)\n            - contains `xlm`: XLMTokenizer (XLM model)\n\n        Params:\n            pretrained_model_name_or_path: either:\n\n                - a string with the `shortcut name` of a predefined tokenizer to load from cache or download, e.g.: ``bert-base-uncased``.\n                - a path to a `directory` containing vocabulary files required by the tokenizer, for instance saved using the :func:`~transformers.PreTrainedTokenizer.save_pretrained` method, e.g.: ``./my_model_directory/``.\n                - (not applicable to all derived classes) a path or url to a single saved vocabulary file if and only if the tokenizer only requires a single vocabulary file (e.g. Bert, XLNet), e.g.: ``./my_model_directory/vocab.txt``.\n\n            cache_dir: (`optional`) string:\n                Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the standard cache should not be used.\n\n            force_download: (`optional`) boolean, default False:\n                Force to (re-)download the vocabulary files and override the cached versions if they exists.\n\n            proxies: (`optional`) dict, default None:\n                A dictionary of proxy servers to use by protocol or endpoint, e.g.: {\'http\': \'foo.bar:3128\', \'http://hostname\': \'foo.bar:4012\'}.\n                The proxies are used on each request.\n\n            inputs: (`optional`) positional arguments: will be passed to the Tokenizer ``__init__`` method.\n\n            kwargs: (`optional`) keyword arguments: will be passed to the Tokenizer ``__init__`` method. Can be used to set special tokens like ``bos_token``, ``eos_token``, ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``, ``additional_special_tokens``. See parameters in the doc string of :class:`~transformers.PreTrainedTokenizer` for details.\n\n        Examples::\n\n            tokenizer = AutoTokenizer.from_pretrained(\'bert-base-uncased\')    # Download vocabulary from S3 and cache.\n            tokenizer = AutoTokenizer.from_pretrained(\'./test/bert_saved_model/\')  # E.g. tokenizer was saved using `save_pretrained(\'./test/saved_model/\')`\n\n        """"""\n        if \'distilbert\' in pretrained_model_name_or_path:\n            return DistilBertTokenizer.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n        elif \'roberta\' in pretrained_model_name_or_path:\n            return RobertaTokenizer.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n        elif \'bert\' in pretrained_model_name_or_path:\n            return BertTokenizer.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n        elif \'openai-gpt\' in pretrained_model_name_or_path:\n            return OpenAIGPTTokenizer.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n        elif \'gpt2\' in pretrained_model_name_or_path:\n            return GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n        elif \'transfo-xl\' in pretrained_model_name_or_path:\n            return TransfoXLTokenizer.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n        elif \'xlnet\' in pretrained_model_name_or_path:\n            return XLNetTokenizer.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n        elif \'xlm\' in pretrained_model_name_or_path:\n            return XLMTokenizer.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n        elif \'ctrl\' in pretrained_model_name_or_path:\n            return CTRLTokenizer.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n        raise ValueError(""Unrecognized model identifier in {}. Should contains one of ""\n                         ""\'bert\', \'openai-gpt\', \'gpt2\', \'transfo-xl\', \'xlnet\', ""\n                         ""\'xlm\', \'roberta\', \'ctrl\'"".format(pretrained_model_name_or_path))\n'"
models/transformers/tokenization_bert.py,0,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tokenization classes.""""""\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport collections\nimport logging\nimport os\nimport unicodedata\nfrom io import open\n\nfrom .tokenization_utils import PreTrainedTokenizer\n\nlogger = logging.getLogger(__name__)\n\nVOCAB_FILES_NAMES = {\'vocab_file\': \'vocab.txt\'}\n\nPRETRAINED_VOCAB_FILES_MAP = {\n    \'vocab_file\':\n    {\n        \'bert-base-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt"",\n        \'bert-large-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt"",\n        \'bert-base-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt"",\n        \'bert-large-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt"",\n        \'bert-base-multilingual-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt"",\n        \'bert-base-multilingual-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-vocab.txt"",\n        \'bert-base-chinese\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt"",\n        \'bert-base-german-cased\': ""https://int-deepset-models-bert.s3.eu-central-1.amazonaws.com/pytorch/bert-base-german-cased-vocab.txt"",\n        \'bert-large-uncased-whole-word-masking\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-vocab.txt"",\n        \'bert-large-cased-whole-word-masking\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-vocab.txt"",\n        \'bert-large-uncased-whole-word-masking-finetuned-squad\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-vocab.txt"",\n        \'bert-large-cased-whole-word-masking-finetuned-squad\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-finetuned-squad-vocab.txt"",\n        \'bert-base-cased-finetuned-mrpc\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-mrpc-vocab.txt"",\n        \'bert-base-german-dbmdz-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-vocab.txt"",\n        \'bert-base-german-dbmdz-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-uncased-vocab.txt"",\n    }\n}\n\nPRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n    \'bert-base-uncased\': 512,\n    \'bert-large-uncased\': 512,\n    \'bert-base-cased\': 512,\n    \'bert-large-cased\': 512,\n    \'bert-base-multilingual-uncased\': 512,\n    \'bert-base-multilingual-cased\': 512,\n    \'bert-base-chinese\': 512,\n    \'bert-base-german-cased\': 512,\n    \'bert-large-uncased-whole-word-masking\': 512,\n    \'bert-large-cased-whole-word-masking\': 512,\n    \'bert-large-uncased-whole-word-masking-finetuned-squad\': 512,\n    \'bert-large-cased-whole-word-masking-finetuned-squad\': 512,\n    \'bert-base-cased-finetuned-mrpc\': 512,\n    \'bert-base-german-dbmdz-cased\': 512,\n    \'bert-base-german-dbmdz-uncased\': 512,\n}\n\nPRETRAINED_INIT_CONFIGURATION = {\n    \'bert-base-uncased\': {\'do_lower_case\': True},\n    \'bert-large-uncased\': {\'do_lower_case\': True},\n    \'bert-base-cased\': {\'do_lower_case\': False},\n    \'bert-large-cased\': {\'do_lower_case\': False},\n    \'bert-base-multilingual-uncased\': {\'do_lower_case\': True},\n    \'bert-base-multilingual-cased\': {\'do_lower_case\': False},\n    \'bert-base-chinese\': {\'do_lower_case\': False},\n    \'bert-base-german-cased\': {\'do_lower_case\': False},\n    \'bert-large-uncased-whole-word-masking\': {\'do_lower_case\': True},\n    \'bert-large-cased-whole-word-masking\': {\'do_lower_case\': False},\n    \'bert-large-uncased-whole-word-masking-finetuned-squad\': {\'do_lower_case\': True},\n    \'bert-large-cased-whole-word-masking-finetuned-squad\': {\'do_lower_case\': False},\n    \'bert-base-cased-finetuned-mrpc\': {\'do_lower_case\': False},\n    \'bert-base-german-dbmdz-cased\': {\'do_lower_case\': False},\n    \'bert-base-german-dbmdz-uncased\': {\'do_lower_case\': True},\n}\n\n\ndef load_vocab(vocab_file):\n    """"""Loads a vocabulary file into a dictionary.""""""\n    vocab = collections.OrderedDict()\n    with open(vocab_file, ""r"", encoding=""utf-8"") as reader:\n        tokens = reader.readlines()\n    for index, token in enumerate(tokens):\n        token = token.rstrip(\'\\n\')\n        vocab[token] = index\n    return vocab\n\n\ndef whitespace_tokenize(text):\n    """"""Runs basic whitespace cleaning and splitting on a piece of text.""""""\n    text = text.strip()\n    if not text:\n        return []\n    tokens = text.split()\n    return tokens\n\n\nclass BertTokenizer(PreTrainedTokenizer):\n    r""""""\n    Constructs a BertTokenizer.\n    :class:`~transformers.BertTokenizer` runs end-to-end tokenization: punctuation splitting + wordpiece\n\n    Args:\n        vocab_file: Path to a one-wordpiece-per-line vocabulary file\n        do_lower_case: Whether to lower case the input. Only has an effect when do_wordpiece_only=False\n        do_basic_tokenize: Whether to do basic tokenization before wordpiece.\n        max_len: An artificial maximum length to truncate tokenized sequences to; Effective maximum length is always the\n            minimum of this value (if specified) and the underlying BERT model\'s sequence length.\n        never_split: List of tokens which will never be split during tokenization. Only has an effect when\n            do_wordpiece_only=False\n    """"""\n\n    vocab_files_names = VOCAB_FILES_NAMES\n    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n\n    def __init__(self, vocab_file, do_lower_case=True, do_basic_tokenize=True, never_split=None,\n                 unk_token=""[UNK]"", sep_token=""[SEP]"", pad_token=""[PAD]"", cls_token=""[CLS]"",\n                 mask_token=""[MASK]"", tokenize_chinese_chars=True, **kwargs):\n        """"""Constructs a BertTokenizer.\n\n        Args:\n            **vocab_file**: Path to a one-wordpiece-per-line vocabulary file\n            **do_lower_case**: (`optional`) boolean (default True)\n                Whether to lower case the input\n                Only has an effect when do_basic_tokenize=True\n            **do_basic_tokenize**: (`optional`) boolean (default True)\n                Whether to do basic tokenization before wordpiece.\n            **never_split**: (`optional`) list of string\n                List of tokens which will never be split during tokenization.\n                Only has an effect when do_basic_tokenize=True\n            **tokenize_chinese_chars**: (`optional`) boolean (default True)\n                Whether to tokenize Chinese characters.\n                This should likely be deactivated for Japanese:\n                see: https://github.com/huggingface/pytorch-pretrained-BERT/issues/328\n        """"""\n        super(BertTokenizer, self).__init__(unk_token=unk_token, sep_token=sep_token,\n                                            pad_token=pad_token, cls_token=cls_token,\n                                            mask_token=mask_token, **kwargs)\n        self.max_len_single_sentence = self.max_len - 2  # take into account special tokens\n        self.max_len_sentences_pair = self.max_len - 3  # take into account special tokens\n\n        if not os.path.isfile(vocab_file):\n            raise ValueError(\n                ""Can\'t find a vocabulary file at path \'{}\'. To load the vocabulary from a Google pretrained ""\n                ""model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`"".format(vocab_file))\n        self.vocab = load_vocab(vocab_file)\n        self.ids_to_tokens = collections.OrderedDict(\n            [(ids, tok) for tok, ids in self.vocab.items()])\n        self.do_basic_tokenize = do_basic_tokenize\n        if do_basic_tokenize:\n            self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case,\n                                                  never_split=never_split,\n                                                  tokenize_chinese_chars=tokenize_chinese_chars)\n        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab, unk_token=self.unk_token)\n\n    @property\n    def vocab_size(self):\n        return len(self.vocab)\n\n    def _tokenize(self, text):\n        split_tokens = []\n        if self.do_basic_tokenize:\n            for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n                for sub_token in self.wordpiece_tokenizer.tokenize(token):\n                    split_tokens.append(sub_token)\n        else:\n            split_tokens = self.wordpiece_tokenizer.tokenize(text)\n        return split_tokens\n\n    def _convert_token_to_id(self, token):\n        """""" Converts a token (str/unicode) in an id using the vocab. """"""\n        return self.vocab.get(token, self.vocab.get(self.unk_token))\n\n    def _convert_id_to_token(self, index):\n        """"""Converts an index (integer) in a token (string/unicode) using the vocab.""""""\n        return self.ids_to_tokens.get(index, self.unk_token)\n\n    def convert_tokens_to_string(self, tokens):\n        """""" Converts a sequence of tokens (string) in a single string. """"""\n        out_string = \' \'.join(tokens).replace(\' ##\', \'\').strip()\n        return out_string\n\n    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n        """"""\n        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n        by concatenating and adding special tokens.\n        A BERT sequence has the following format:\n            single sequence: [CLS] X [SEP]\n            pair of sequences: [CLS] A [SEP] B [SEP]\n        """"""\n        if token_ids_1 is None:\n            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n        cls = [self.cls_token_id]\n        sep = [self.sep_token_id]\n        return cls + token_ids_0 + sep + token_ids_1 + sep\n\n    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n        """"""\n        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n\n        Args:\n            token_ids_0: list of ids (must not contain special tokens)\n            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n                for sequence pairs\n            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n                special tokens for the model\n\n        Returns:\n            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n        """"""\n\n        if already_has_special_tokens:\n            if token_ids_1 is not None:\n                raise ValueError(""You should not supply a second sequence if the provided sequence of ""\n                                 ""ids is already formated with special tokens for the model."")\n            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n\n        if token_ids_1 is not None:\n            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n        return [1] + ([0] * len(token_ids_0)) + [1]\n\n    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n        """"""\n        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n        A BERT sequence pair mask has the following format:\n        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n        | first sequence    | second sequence\n\n        if token_ids_1 is None, only returns the first portion of the mask (0\'s).\n        """"""\n        sep = [self.sep_token_id]\n        cls = [self.cls_token_id]\n        if token_ids_1 is None:\n            return len(cls + token_ids_0 + sep) * [0]\n        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n\n    def save_vocabulary(self, vocab_path):\n        """"""Save the tokenizer vocabulary to a directory or file.""""""\n        index = 0\n        if os.path.isdir(vocab_path):\n            vocab_file = os.path.join(vocab_path, VOCAB_FILES_NAMES[\'vocab_file\'])\n        else:\n            vocab_file = vocab_path\n        with open(vocab_file, ""w"", encoding=""utf-8"") as writer:\n            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):\n                if index != token_index:\n                    logger.warning(""Saving vocabulary to {}: vocabulary indices are not consecutive.""\n                                   "" Please check that the vocabulary is not corrupted!"".format(vocab_file))\n                    index = token_index\n                writer.write(token + u\'\\n\')\n                index += 1\n        return (vocab_file,)\n\n\nclass BasicTokenizer(object):\n    """"""Runs basic tokenization (punctuation splitting, lower casing, etc.).""""""\n\n    def __init__(self, do_lower_case=True, never_split=None, tokenize_chinese_chars=True):\n        """""" Constructs a BasicTokenizer.\n\n        Args:\n            **do_lower_case**: Whether to lower case the input.\n            **never_split**: (`optional`) list of str\n                Kept for backward compatibility purposes.\n                Now implemented directly at the base class level (see :func:`PreTrainedTokenizer.tokenize`)\n                List of token not to split.\n            **tokenize_chinese_chars**: (`optional`) boolean (default True)\n                Whether to tokenize Chinese characters.\n                This should likely be deactivated for Japanese:\n                see: https://github.com/huggingface/pytorch-pretrained-BERT/issues/328\n        """"""\n        if never_split is None:\n            never_split = []\n        self.do_lower_case = do_lower_case\n        self.never_split = never_split\n        self.tokenize_chinese_chars = tokenize_chinese_chars\n\n    def tokenize(self, text, never_split=None):\n        """""" Basic Tokenization of a piece of text.\n            Split on ""white spaces"" only, for sub-word tokenization, see WordPieceTokenizer.\n\n        Args:\n            **never_split**: (`optional`) list of str\n                Kept for backward compatibility purposes.\n                Now implemented directly at the base class level (see :func:`PreTrainedTokenizer.tokenize`)\n                List of token not to split.\n        """"""\n        never_split = self.never_split + (never_split if never_split is not None else [])\n        text = self._clean_text(text)\n        # This was added on November 1st, 2018 for the multilingual and Chinese\n        # models. This is also applied to the English models now, but it doesn\'t\n        # matter since the English models were not trained on any Chinese data\n        # and generally don\'t have any Chinese data in them (there are Chinese\n        # characters in the vocabulary because Wikipedia does have some Chinese\n        # words in the English Wikipedia.).\n        if self.tokenize_chinese_chars:\n            text = self._tokenize_chinese_chars(text)\n        orig_tokens = whitespace_tokenize(text)\n        split_tokens = []\n        for token in orig_tokens:\n            if self.do_lower_case and token not in never_split:\n                token = token.lower()\n                token = self._run_strip_accents(token)\n            split_tokens.extend(self._run_split_on_punc(token))\n\n        output_tokens = whitespace_tokenize("" "".join(split_tokens))\n        return output_tokens\n\n    def _run_strip_accents(self, text):\n        """"""Strips accents from a piece of text.""""""\n        text = unicodedata.normalize(""NFD"", text)\n        output = []\n        for char in text:\n            cat = unicodedata.category(char)\n            if cat == ""Mn"":\n                continue\n            output.append(char)\n        return """".join(output)\n\n    def _run_split_on_punc(self, text, never_split=None):\n        """"""Splits punctuation on a piece of text.""""""\n        if never_split is not None and text in never_split:\n            return [text]\n        chars = list(text)\n        i = 0\n        start_new_word = True\n        output = []\n        while i < len(chars):\n            char = chars[i]\n            if _is_punctuation(char):\n                output.append([char])\n                start_new_word = True\n            else:\n                if start_new_word:\n                    output.append([])\n                start_new_word = False\n                output[-1].append(char)\n            i += 1\n\n        return ["""".join(x) for x in output]\n\n    def _tokenize_chinese_chars(self, text):\n        """"""Adds whitespace around any CJK character.""""""\n        output = []\n        for char in text:\n            cp = ord(char)\n            if self._is_chinese_char(cp):\n                output.append("" "")\n                output.append(char)\n                output.append("" "")\n            else:\n                output.append(char)\n        return """".join(output)\n\n    def _is_chinese_char(self, cp):\n        """"""Checks whether CP is the codepoint of a CJK character.""""""\n        # This defines a ""chinese character"" as anything in the CJK Unicode block:\n        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n        #\n        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n        # despite its name. The modern Korean Hangul alphabet is a different block,\n        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n        # space-separated words, so they are not treated specially and handled\n        # like the all of the other languages.\n        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n                (cp >= 0x3400 and cp <= 0x4DBF) or  #\n                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n                (cp >= 0xF900 and cp <= 0xFAFF) or  #\n                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n            return True\n\n        return False\n\n    def _clean_text(self, text):\n        """"""Performs invalid character removal and whitespace cleanup on text.""""""\n        output = []\n        for char in text:\n            cp = ord(char)\n            if cp == 0 or cp == 0xfffd or _is_control(char):\n                continue\n            if _is_whitespace(char):\n                output.append("" "")\n            else:\n                output.append(char)\n        return """".join(output)\n\n\nclass WordpieceTokenizer(object):\n    """"""Runs WordPiece tokenization.""""""\n\n    def __init__(self, vocab, unk_token, max_input_chars_per_word=100):\n        self.vocab = vocab\n        self.unk_token = unk_token\n        self.max_input_chars_per_word = max_input_chars_per_word\n\n    def tokenize(self, text):\n        """"""Tokenizes a piece of text into its word pieces.\n\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n\n        For example:\n          input = ""unaffable""\n          output = [""un"", ""##aff"", ""##able""]\n\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer`.\n\n        Returns:\n          A list of wordpiece tokens.\n        """"""\n\n        output_tokens = []\n        for token in whitespace_tokenize(text):\n            chars = list(token)\n            if len(chars) > self.max_input_chars_per_word:\n                output_tokens.append(self.unk_token)\n                continue\n\n            is_bad = False\n            start = 0\n            sub_tokens = []\n            while start < len(chars):\n                end = len(chars)\n                cur_substr = None\n                while start < end:\n                    substr = """".join(chars[start:end])\n                    if start > 0:\n                        substr = ""##"" + substr\n                    if substr in self.vocab:\n                        cur_substr = substr\n                        break\n                    end -= 1\n                if cur_substr is None:\n                    is_bad = True\n                    break\n                sub_tokens.append(cur_substr)\n                start = end\n\n            if is_bad:\n                output_tokens.append(self.unk_token)\n            else:\n                output_tokens.extend(sub_tokens)\n        return output_tokens\n\n\ndef _is_whitespace(char):\n    """"""Checks whether `chars` is a whitespace character.""""""\n    # \\t, \\n, and \\r are technically contorl characters but we treat them\n    # as whitespace since they are generally considered as such.\n    if char == "" "" or char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n        return True\n    cat = unicodedata.category(char)\n    if cat == ""Zs"":\n        return True\n    return False\n\n\ndef _is_control(char):\n    """"""Checks whether `chars` is a control character.""""""\n    # These are technically control characters but we count them as whitespace\n    # characters.\n    if char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n        return False\n    cat = unicodedata.category(char)\n    if cat.startswith(""C""):\n        return True\n    return False\n\n\ndef _is_punctuation(char):\n    """"""Checks whether `chars` is a punctuation character.""""""\n    cp = ord(char)\n    # We treat all non-letter/number ASCII as punctuation.\n    # Characters such as ""^"", ""$"", and ""`"" are not in the Unicode\n    # Punctuation class but we treat them as punctuation anyways, for\n    # consistency.\n    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n        return True\n    cat = unicodedata.category(char)\n    if cat.startswith(""P""):\n        return True\n    return False\n'"
models/transformers/tokenization_ctrl.py,0,"b'# coding=utf-8\n# Copyright 2018 Salesforce and The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tokenization classes for Salesforce CTRL.""""""\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport json\nimport logging\nimport os\nimport regex as re\nfrom io import open\n\nfrom .tokenization_utils import PreTrainedTokenizer\n\nlogger = logging.getLogger(__name__)\n\nVOCAB_FILES_NAMES = {\n    \'vocab_file\': \'vocab.json\',\n    \'merges_file\': \'merges.txt\',\n}\n\nPRETRAINED_VOCAB_FILES_MAP = {\n    \'vocab_file\':\n    {\n        \'ctrl\': ""https://raw.githubusercontent.com/salesforce/ctrl/master/ctrl-vocab.json"",\n    },\n    \'merges_file\':\n    {\n        \'ctrl\': ""https://raw.githubusercontent.com/salesforce/ctrl/master/ctrl-merges.txt"",\n    },\n}\n\nPRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n    \'ctrl\': 256,\n}\n\ndef get_pairs(word):\n    """"""Return set of symbol pairs in a word.\n\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    """"""\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n\n    pairs = set(pairs)\n    return pairs\n\nclass CTRLTokenizer(PreTrainedTokenizer):\n    """"""\n    CTRL BPE tokenizer. Peculiarities:\n        - Byte-level Byte-Pair-Encoding\n        - Requires a space to start the input string => the encoding methods should be called with the\n          ``add_prefix_space`` flag set to ``True``.\n          Otherwise, this tokenizer ``encode`` and ``decode`` method will not conserve\n          the absence of a space at the beginning of a string: `tokenizer.decode(tokenizer.encode(""Hello"")) = "" Hello""`\n    """"""\n    vocab_files_names = VOCAB_FILES_NAMES\n    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n\n    def __init__(self, vocab_file, merges_file, unk_token=""<unk>"", **kwargs):\n        super(CTRLTokenizer, self).__init__(unk_token=unk_token, **kwargs)\n        self.max_len_single_sentence = self.max_len # no default special tokens - you can update this value if you add special tokens\n        self.max_len_sentences_pair = self.max_len # no default special tokens - you can update this value if you add special tokens\n\n        self.encoder = json.load(open(vocab_file, encoding=""utf-8""))\n        self.decoder = {v:k for k,v in self.encoder.items()}\n        merges = open(merges_file, encoding=\'utf-8\').read().split(\'\\n\')[1:-1]\n        merges = [tuple(merge.split()) for merge in merges]\n        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n        self.cache = {}\n\n    @property\n    def vocab_size(self):\n        return len(self.encoder)\n\n    def bpe(self, token):\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token)\n        word = tuple(list(word[:-1]) + [word[-1]+\'</w>\'])\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token\n\n        while True:\n            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float(\'inf\')))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n                    new_word.append(first+second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = \'@@ \'.join(word)\n        word = word[:-4]\n        self.cache[token] = word\n        return word\n\n    def _tokenize(self, text):\n        """""" Tokenize a string.\n        """"""\n        split_tokens = []\n\n        text = text.split(\' \')\n\n        for token in text:\n            split_tokens.extend([t for t in self.bpe(token).split(\' \')])\n        return split_tokens\n\n    def _convert_token_to_id(self, token):\n        """""" Converts a token (str/unicode) in an id using the vocab. """"""\n        return self.encoder.get(token, self.encoder.get(self.unk_token))\n\n    def _convert_id_to_token(self, index):\n        """"""Converts an index (integer) in a token (string/unicode) using the vocab.""""""\n        return self.decoder.get(index, self.unk_token)\n\n    def convert_tokens_to_string(self, tokens):\n        """""" Converts a sequence of tokens (string) in a single string. """"""\n        out_string = \' \'.join(tokens).replace(\'@@ \', \'\').strip()\n        return out_string\n\n    def save_vocabulary(self, save_directory):\n        """"""Save the tokenizer vocabulary and merge files to a directory.""""""\n        if not os.path.isdir(save_directory):\n            logger.error(""Vocabulary path ({}) should be a directory"".format(save_directory))\n            return\n        vocab_file = os.path.join(save_directory, VOCAB_FILES_NAMES[\'vocab_file\'])\n        merge_file = os.path.join(save_directory, VOCAB_FILES_NAMES[\'merges_file\'])\n\n        with open(vocab_file, \'w\', encoding=\'utf-8\') as f:\n            f.write(json.dumps(self.encoder, ensure_ascii=False))\n\n        index = 0\n        with open(merge_file, ""w"", encoding=""utf-8"") as writer:\n            writer.write(u\'#version: 0.2\\n\')\n            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):\n                if index != token_index:\n                    logger.warning(""Saving vocabulary to {}: BPE merge indices are not consecutive.""\n                                   "" Please check that the tokenizer is not corrupted!"".format(merge_file))\n                    index = token_index\n                writer.write(\' \'.join(bpe_tokens) + u\'\\n\')\n                index += 1\n\n        return vocab_file, merge_file\n\n    # def decode(self, token_ids, skip_special_tokens=False, clean_up_tokenization_spaces=True):\n    #     filtered_tokens = \' \'.join(self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens))\n    #     tokens_generated_so_far = re.sub(\'(@@ )\', \'\', string=filtered_tokens)\n    #     tokens_generated_so_far = re.sub(\'(@@ ?$)\', \'\', string=tokens_generated_so_far)\n    #     return \'\'.join(tokens_generated_so_far)\n'"
models/transformers/tokenization_distilbert.py,0,"b'# coding=utf-8\n# Copyright 2018 The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tokenization classes for DistilBERT.""""""\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport collections\nimport logging\nimport os\nimport unicodedata\nfrom io import open\n\nfrom .tokenization_bert import BertTokenizer\n\nlogger = logging.getLogger(__name__)\n\nVOCAB_FILES_NAMES = {\'vocab_file\': \'vocab.txt\'}\n\nPRETRAINED_VOCAB_FILES_MAP = {\n    \'vocab_file\':\n    {\n        \'distilbert-base-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt"",\n        \'distilbert-base-uncased-distilled-squad\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-vocab.txt"",\n    }\n}\n\nPRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n    \'distilbert-base-uncased\': 512,\n    \'distilbert-base-uncased-distilled-squad\': 512,\n}\n\n\nclass DistilBertTokenizer(BertTokenizer):\n    r""""""\n    Constructs a DistilBertTokenizer.\n    :class:`~transformers.DistilBertTokenizer` is identical to BertTokenizer and runs end-to-end tokenization: punctuation splitting + wordpiece\n\n    Args:\n        vocab_file: Path to a one-wordpiece-per-line vocabulary file\n        do_lower_case: Whether to lower case the input. Only has an effect when do_wordpiece_only=False\n        do_basic_tokenize: Whether to do basic tokenization before wordpiece.\n        max_len: An artificial maximum length to truncate tokenized sequences to; Effective maximum length is always the\n            minimum of this value (if specified) and the underlying BERT model\'s sequence length.\n        never_split: List of tokens which will never be split during tokenization. Only has an effect when\n            do_wordpiece_only=False\n    """"""\n\n    vocab_files_names = VOCAB_FILES_NAMES\n    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n'"
models/transformers/tokenization_gpt2.py,0,"b'# coding=utf-8\n# Copyright 2018 The Open AI Team Authors and The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tokenization classes for OpenAI GPT.""""""\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport sys\nimport json\nimport logging\nimport os\nimport regex as re\nfrom io import open\n\ntry:\n    from functools import lru_cache\nexcept ImportError:\n    # Just a dummy decorator to get the checks to run on python2\n    # because honestly I don\'t want to support a byte-level unicode BPE tokenizer on python 2 right now.\n    def lru_cache():\n        return lambda func: func\n\nfrom .tokenization_utils import PreTrainedTokenizer\n\nlogger = logging.getLogger(__name__)\n\nVOCAB_FILES_NAMES = {\n    \'vocab_file\': \'vocab.json\',\n    \'merges_file\': \'merges.txt\',\n}\n\nPRETRAINED_VOCAB_FILES_MAP = {\n    \'vocab_file\':\n    {\n        \'gpt2\': ""https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json"",\n        \'gpt2-medium\': ""https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-vocab.json"",\n        \'gpt2-large\': ""https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-vocab.json"",\n        \'distilgpt2\': ""https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-vocab.json"",\n    },\n    \'merges_file\':\n    {\n        \'gpt2\': ""https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt"",\n        \'gpt2-medium\': ""https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-merges.txt"",\n        \'gpt2-large\': ""https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-merges.txt"",\n        \'distilgpt2\': ""https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-merges.txt"",\n    },\n}\n\nPRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n    \'gpt2\': 1024,\n    \'gpt2-medium\': 1024,\n    \'gpt2-large\': 1024,\n    \'distilgpt2\': 1024,\n}\n\n@lru_cache()\ndef bytes_to_unicode():\n    """"""\n    Returns list of utf-8 byte and a mapping to unicode strings.\n    We specifically avoids mapping to whitespace/control characters the bpe code barfs on.\n    \n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you\'re at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    """"""\n    _chr = unichr if sys.version_info[0] == 2 else chr\n    bs = list(range(ord(""!""), ord(""~"")+1))+list(range(ord(""\xc2\xa1""), ord(""\xc2\xac"")+1))+list(range(ord(""\xc2\xae""), ord(""\xc3\xbf"")+1))\n    cs = bs[:]\n    n = 0\n    for b in range(2**8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2**8+n)\n            n += 1\n    cs = [_chr(n) for n in cs]\n    return dict(zip(bs, cs))\n\ndef get_pairs(word):\n    """"""Return set of symbol pairs in a word.\n\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    """"""\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\nclass GPT2Tokenizer(PreTrainedTokenizer):\n    """"""\n    GPT-2 BPE tokenizer. Peculiarities:\n        - Byte-level Byte-Pair-Encoding\n        - Requires a space to start the input string => the encoding methods should be called with the\n          ``add_prefix_space`` flag set to ``True``.\n          Otherwise, this tokenizer ``encode`` and ``decode`` method will not conserve\n          the absence of a space at the beginning of a string: `tokenizer.decode(tokenizer.encode(""Hello"")) = "" Hello""`\n    """"""\n    vocab_files_names = VOCAB_FILES_NAMES\n    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n\n    def __init__(self, vocab_file, merges_file, errors=\'replace\', unk_token=""<|endoftext|>"",\n                 bos_token=""<|endoftext|>"", eos_token=""<|endoftext|>"", **kwargs):\n        super(GPT2Tokenizer, self).__init__(bos_token=bos_token, eos_token=eos_token, unk_token=unk_token, **kwargs)\n        self.max_len_single_sentence = self.max_len # no default special tokens - you can update this value if you add special tokens\n        self.max_len_sentences_pair = self.max_len # no default special tokens - you can update this value if you add special tokens\n\n        self.encoder = json.load(open(vocab_file, encoding=""utf-8""))\n        self.decoder = {v: k for k, v in self.encoder.items()}\n        self.errors = errors  # how to handle errors in decoding\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n        bpe_data = open(merges_file, encoding=\'utf-8\').read().split(\'\\n\')[1:-1]\n        bpe_merges = [tuple(merge.split()) for merge in bpe_data]\n        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n        self.cache = {}\n\n        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n        self.pat = re.compile(r""""""\'s|\'t|\'re|\'ve|\'m|\'ll|\'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+"""""")\n\n    @property\n    def vocab_size(self):\n        return len(self.encoder)\n\n    def bpe(self, token):\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token)\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token\n\n        while True:\n            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float(\'inf\')))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n                    new_word.append(first+second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = \' \'.join(word)\n        self.cache[token] = word\n        return word\n\n    def _tokenize(self, text, add_prefix_space=False):\n        """""" Tokenize a string.\n            Args:\n                - add_prefix_space (boolean, default False):\n                    Begin the sentence with at least one space toto get invariance to word order in GPT-2 (and RoBERTa) tokenizers.\n        """"""\n        if add_prefix_space:\n            text = \' \' + text\n\n        bpe_tokens = []\n        for token in re.findall(self.pat, text):\n            if sys.version_info[0] == 2:\n                token = \'\'.join(self.byte_encoder[ord(b)] for b in token) # Maps all our bytes to unicode strings, avoiding controle tokens of the BPE (spaces in our case)\n            else:\n                token = \'\'.join(self.byte_encoder[b] for b in token.encode(\'utf-8\')) # Maps all our bytes to unicode strings, avoiding controle tokens of the BPE (spaces in our case)\n            bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(\' \'))\n        return bpe_tokens\n\n    def _convert_token_to_id(self, token):\n        """""" Converts a token (str/unicode) in an id using the vocab. """"""\n        return self.encoder.get(token, self.encoder.get(self.unk_token))\n\n    def _convert_id_to_token(self, index):\n        """"""Converts an index (integer) in a token (string/unicode) using the vocab.""""""\n        return self.decoder.get(index)\n\n    def convert_tokens_to_string(self, tokens):\n        """""" Converts a sequence of tokens (string) in a single string. """"""\n        text = \'\'.join(tokens)\n        text = bytearray([self.byte_decoder[c] for c in text]).decode(\'utf-8\', errors=self.errors)\n        return text\n\n    def save_vocabulary(self, save_directory):\n        """"""Save the tokenizer vocabulary and merge files to a directory.""""""\n        if not os.path.isdir(save_directory):\n            logger.error(""Vocabulary path ({}) should be a directory"".format(save_directory))\n            return\n        vocab_file = os.path.join(save_directory, VOCAB_FILES_NAMES[\'vocab_file\'])\n        merge_file = os.path.join(save_directory, VOCAB_FILES_NAMES[\'merges_file\'])\n\n        with open(vocab_file, \'w\', encoding=\'utf-8\') as f:\n            f.write(json.dumps(self.encoder, ensure_ascii=False))\n\n        index = 0\n        with open(merge_file, ""w"", encoding=""utf-8"") as writer:\n            writer.write(u\'#version: 0.2\\n\')\n            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):\n                if index != token_index:\n                    logger.warning(""Saving vocabulary to {}: BPE merge indices are not consecutive.""\n                                   "" Please check that the tokenizer is not corrupted!"".format(merge_file))\n                    index = token_index\n                writer.write(\' \'.join(bpe_tokens) + u\'\\n\')\n                index += 1\n\n        return vocab_file, merge_file'"
models/transformers/tokenization_openai.py,0,"b'# coding=utf-8\n# Copyright 2018 The Open AI Team Authors and The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tokenization classes for OpenAI GPT.""""""\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport json\nimport logging\nimport os\nimport re\nfrom io import open\n\nfrom .tokenization_utils import PreTrainedTokenizer\nfrom .tokenization_bert import BasicTokenizer\n\nlogger = logging.getLogger(__name__)\n\nVOCAB_FILES_NAMES = {\n    \'vocab_file\': \'vocab.json\',\n    \'merges_file\': \'merges.txt\',\n}\n\nPRETRAINED_VOCAB_FILES_MAP = {\n    \'vocab_file\':\n    {\n        \'openai-gpt\': ""https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-vocab.json"",\n    },\n    \'merges_file\':\n    {\n        \'openai-gpt\': ""https://s3.amazonaws.com/models.huggingface.co/bert/openai-gpt-merges.txt"",\n    },\n}\n\nPRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n    \'openai-gpt\': 512,\n}\n\ndef get_pairs(word):\n    """"""\n    Return set of symbol pairs in a word.\n    word is represented as tuple of symbols (symbols being variable-length strings)\n    """"""\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\ndef text_standardize(text):\n    """"""\n    fixes some issues the spacy tokenizer had on books corpus\n    also does some whitespace standardization\n    """"""\n    text = text.replace(\'\xe2\x80\x94\', \'-\')\n    text = text.replace(\'\xe2\x80\x93\', \'-\')\n    text = text.replace(\'\xe2\x80\x95\', \'-\')\n    text = text.replace(\'\xe2\x80\xa6\', \'...\')\n    text = text.replace(\'\xc2\xb4\', ""\'"")\n    text = re.sub(r\'\'\'(-+|~+|!+|""+|;+|\\?+|\\++|,+|\\)+|\\(+|\\\\+|\\/+|\\*+|\\[+|\\]+|}+|{+|\\|+|_+)\'\'\', r\' \\1 \', text)\n    text = re.sub(r\'\\s*\\n\\s*\', \' \\n \', text)\n    text = re.sub(r\'[^\\S\\n]+\', \' \', text)\n    return text.strip()\n\nclass OpenAIGPTTokenizer(PreTrainedTokenizer):\n    """"""\n    BPE tokenizer. Peculiarities:\n        - lower case all inputs\n        - uses SpaCy tokenizer and ftfy for pre-BPE tokenization if they are installed, fallback to BERT\'s BasicTokenizer if not.\n    """"""\n    vocab_files_names = VOCAB_FILES_NAMES\n    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n\n    def __init__(self, vocab_file, merges_file, unk_token=""<unk>"", **kwargs):\n        super(OpenAIGPTTokenizer, self).__init__(unk_token=unk_token, **kwargs)\n\n        self.max_len_single_sentence = self.max_len # no default special tokens - you can update this value if you add special tokens\n        self.max_len_sentences_pair = self.max_len # no default special tokens - you can update this value if you add special tokens\n\n        try:\n            import ftfy\n            from spacy.lang.en import English\n            _nlp = English()\n            self.nlp = _nlp.Defaults.create_tokenizer(_nlp)\n            self.fix_text = ftfy.fix_text\n        except ImportError:\n            logger.warning(""ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy."")\n            self.nlp = BasicTokenizer(do_lower_case=True)\n            self.fix_text = None\n\n        self.encoder = json.load(open(vocab_file, encoding=""utf-8""))\n        self.decoder = {v:k for k,v in self.encoder.items()}\n        merges = open(merges_file, encoding=\'utf-8\').read().split(\'\\n\')[1:-1]\n        merges = [tuple(merge.split()) for merge in merges]\n        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n        self.cache = {}\n\n    @property\n    def vocab_size(self):\n        return len(self.encoder)\n\n    def bpe(self, token):\n        word = tuple(token[:-1]) + (token[-1] + \'</w>\',)\n        if token in self.cache:\n            return self.cache[token]\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token+\'</w>\'\n\n        while True:\n            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\'inf\')))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n                    new_word.append(first+second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = \' \'.join(word)\n        if word == \'\\n  </w>\':\n            word = \'\\n</w>\'\n        self.cache[token] = word\n        return word\n\n    def _tokenize(self, text):\n        """""" Tokenize a string. """"""\n        split_tokens = []\n        if self.fix_text is None:\n            # Using BERT\'s BasicTokenizer\n            text = self.nlp.tokenize(text)\n            for token in text:\n                split_tokens.extend([t for t in self.bpe(token).split(\' \')])\n        else:\n            # Using SpaCy & ftfy (original tokenization process of OpenAI GPT)\n            text = self.nlp(text_standardize(self.fix_text(text)))\n            for token in text:\n                split_tokens.extend([t for t in self.bpe(token.text.lower()).split(\' \')])\n        return split_tokens\n\n    def _convert_token_to_id(self, token):\n        """""" Converts a token (str/unicode) in an id using the vocab. """"""\n        return self.encoder.get(token, self.encoder.get(self.unk_token))\n\n    def _convert_id_to_token(self, index):\n        """"""Converts an id in a token (BPE) using the vocab.""""""\n        return self.decoder.get(index, self.unk_token)\n\n    def convert_tokens_to_string(self, tokens):\n        """""" Converts a sequence of tokens (string) in a single string. """"""\n        out_string = \'\'.join(tokens).replace(\'</w>\', \' \').strip()\n        return out_string\n\n    def save_vocabulary(self, save_directory):\n        """"""Save the tokenizer vocabulary and merge files to a directory.""""""\n        if not os.path.isdir(save_directory):\n            logger.error(""Vocabulary path ({}) should be a directory"".format(save_directory))\n            return\n        vocab_file = os.path.join(save_directory, VOCAB_FILES_NAMES[\'vocab_file\'])\n        merge_file = os.path.join(save_directory, VOCAB_FILES_NAMES[\'merges_file\'])\n\n        with open(vocab_file, \'w\', encoding=\'utf-8\') as f:\n            f.write(json.dumps(self.encoder, ensure_ascii=False))\n\n        index = 0\n        with open(merge_file, ""w"", encoding=""utf-8"") as writer:\n            writer.write(u\'#version: 0.2\\n\')\n            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):\n                if index != token_index:\n                    logger.warning(""Saving vocabulary to {}: BPE merge indices are not consecutive.""\n                                   "" Please check that the tokenizer is not corrupted!"".format(merge_file))\n                    index = token_index\n                writer.write(\' \'.join(bpe_tokens) + u\'\\n\')\n                index += 1\n\n        return vocab_file, merge_file\n'"
models/transformers/tokenization_roberta.py,0,"b'# coding=utf-8\n# Copyright 2018 The Open AI Team Authors and The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tokenization classes for RoBERTa.""""""\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport sys\nimport json\nimport logging\nimport os\nimport regex as re\nfrom io import open\n\nfrom .tokenization_gpt2 import GPT2Tokenizer\n\ntry:\n    from functools import lru_cache\nexcept ImportError:\n    # Just a dummy decorator to get the checks to run on python2\n    # because honestly I don\'t want to support a byte-level unicode BPE tokenizer on python 2 right now.\n    def lru_cache():\n        return lambda func: func\n\nlogger = logging.getLogger(__name__)\n\nVOCAB_FILES_NAMES = {\n    \'vocab_file\': \'vocab.json\',\n    \'merges_file\': \'merges.txt\',\n}\n\nPRETRAINED_VOCAB_FILES_MAP = {\n    \'vocab_file\':\n    {\n        \'roberta-base\': ""https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json"",\n        \'roberta-large\': ""https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json"",\n        \'roberta-large-mnli\': ""https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-mnli-vocab.json"",\n    },\n    \'merges_file\':\n    {\n        \'roberta-base\': ""https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt"",\n        \'roberta-large\': ""https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt"",\n        \'roberta-large-mnli\': ""https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-mnli-merges.txt"",\n    },\n}\n\nPRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n    \'roberta-base\': 512,\n    \'roberta-large\': 512,\n    \'roberta-large-mnli\': 512,\n}\n\n\nclass RobertaTokenizer(GPT2Tokenizer):\n    """"""\n    RoBERTa BPE tokenizer, derived from the GPT-2 tokenizer. Peculiarities:\n        - Byte-level Byte-Pair-Encoding\n        - Requires a space to start the input string => the encoding methods should be called with the\n          ``add_prefix_space`` flag set to ``True``.\n          Otherwise, this tokenizer ``encode`` and ``decode`` method will not conserve\n          the absence of a space at the beginning of a string: `tokenizer.decode(tokenizer.encode(""Hello"")) = "" Hello""`\n    """"""\n    vocab_files_names = VOCAB_FILES_NAMES\n    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n\n    def __init__(self, vocab_file, merges_file, errors=\'replace\', bos_token=""<s>"", eos_token=""</s>"", sep_token=""</s>"",\n                 cls_token=""<s>"", unk_token=""<unk>"", pad_token=\'<pad>\', mask_token=\'<mask>\', **kwargs):\n        super(RobertaTokenizer, self).__init__(vocab_file=vocab_file, merges_file=merges_file, errors=errors,\n                                               bos_token=bos_token, eos_token=eos_token, unk_token=unk_token,\n                                               sep_token=sep_token, cls_token=cls_token, pad_token=pad_token,\n                                               mask_token=mask_token, **kwargs)\n        self.max_len_single_sentence = self.max_len - 2  # take into account special tokens\n        self.max_len_sentences_pair = self.max_len - 4  # take into account special tokens\n\n    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n        """"""\n        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n        by concatenating and adding special tokens.\n        A RoBERTa sequence has the following format:\n            single sequence: <s> X </s>\n            pair of sequences: <s> A </s></s> B </s>\n        """"""\n        if token_ids_1 is None:\n            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n        cls = [self.cls_token_id]\n        sep = [self.sep_token_id]\n        return cls + token_ids_0 + sep + sep + token_ids_1 + sep\n\n    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n        """"""\n        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n\n        Args:\n            token_ids_0: list of ids (must not contain special tokens)\n            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n                for sequence pairs\n            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n                special tokens for the model\n\n        Returns:\n            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n        """"""\n        if already_has_special_tokens:\n            if token_ids_1 is not None:\n                raise ValueError(""You should not supply a second sequence if the provided sequence of ""\n                                 ""ids is already formated with special tokens for the model."")\n            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n\n        if token_ids_1 is None:\n            return [1] + ([0] * len(token_ids_0)) + [1]\n        return [1] + ([0] * len(token_ids_0)) + [1, 1] + ([0] * len(token_ids_1)) + [1]\n\n    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n        """"""\n        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n        A RoBERTa sequence pair mask has the following format:\n        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n        | first sequence    | second sequence\n\n        if token_ids_1 is None, only returns the first portion of the mask (0\'s).\n        """"""\n        sep = [self.sep_token_id]\n        cls = [self.cls_token_id]\n\n        if token_ids_1 is None:\n            return len(cls + token_ids_0 + sep) * [0]\n        return len(cls + token_ids_0 + sep + sep) * [0] + len(token_ids_1 + sep) * [1]\n'"
models/transformers/tokenization_transfo_xl.py,13,"b'# coding=utf-8\n# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" Tokenization classes for Transformer XL model.\n    Adapted from https://github.com/kimiyoung/transformer-xl.\n""""""\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport glob\nimport logging\nimport os\nimport sys\nfrom collections import Counter, OrderedDict\nfrom io import open\n\nimport numpy as np\n\nfrom .file_utils import cached_path\nfrom .tokenization_utils import PreTrainedTokenizer\n\ntry:\n    import torch\nexcept ImportError:\n    pass\n\n# if sys.version_info[0] == 2:\n#     import cPickle as pickle\n# else:\n#     import pickle\n\n\nlogger = logging.getLogger(__name__)\n\nVOCAB_FILES_NAMES = {\'pretrained_vocab_file\': \'vocab.bin\', \'vocab_file\': \'vocab.txt\'}\n\nPRETRAINED_VOCAB_FILES_MAP = {\n    \'pretrained_vocab_file\':\n    {\n        \'transfo-xl-wt103\': ""https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-vocab.bin"",\n    }\n}\n\nPRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n    \'transfo-xl-wt103\': None,\n}\n\nPRETRAINED_CORPUS_ARCHIVE_MAP = {\n    \'transfo-xl-wt103\': ""https://s3.amazonaws.com/models.huggingface.co/bert/transfo-xl-wt103-corpus.bin"",\n}\nCORPUS_NAME = \'corpus.bin\'\n\nclass TransfoXLTokenizer(PreTrainedTokenizer):\n    """"""\n    Transformer-XL tokenizer adapted from Vocab class in https://github.com/kimiyoung/transformer-xl\n    """"""\n    vocab_files_names = VOCAB_FILES_NAMES\n    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n\n    def __init__(self, special=None, min_freq=0, max_size=None, lower_case=False,\n                 delimiter=None, vocab_file=None, pretrained_vocab_file=None,\n                 never_split=None, unk_token=""<unk>"", eos_token=""<eos>"",\n                 additional_special_tokens=[""<formula>""], **kwargs):\n        super(TransfoXLTokenizer, self).__init__(unk_token=unk_token, eos_token=eos_token,\n                                                 additional_special_tokens=additional_special_tokens,\n                                                 **kwargs)\n\n        self.max_len_single_sentence = self.max_len # no default special tokens - you can update this value if you add special tokens\n        self.max_len_sentences_pair = self.max_len # no default special tokens - you can update this value if you add special tokens\n\n        if never_split is None:\n            never_split = self.all_special_tokens\n        if special is None:\n            special = []\n        self.counter = Counter()\n        self.special = special\n        self.min_freq = min_freq\n        self.max_size = max_size\n        self.lower_case = lower_case\n        self.delimiter = delimiter\n        self.vocab_file = vocab_file\n        self.never_split = never_split\n\n        if pretrained_vocab_file is not None:\n            # Hack because, honestly this tokenizer was not made to be used\n            # in a library like ours, at all.\n            vocab_dict = torch.load(pretrained_vocab_file)\n            for key, value in vocab_dict.items():\n                if key not in self.__dict__:\n                    self.__dict__[key] = value\n\n        if vocab_file is not None:\n            self.build_vocab()\n\n    def count_file(self, path, verbose=False, add_eos=False):\n        if verbose: logger.info(\'counting file {} ...\'.format(path))\n        assert os.path.exists(path)\n\n        sents = []\n        with open(path, \'r\', encoding=\'utf-8\') as f:\n            for idx, line in enumerate(f):\n                if verbose and idx > 0 and idx % 500000 == 0:\n                    logger.info(\'    line {}\'.format(idx))\n                symbols = self.tokenize(line, add_eos=add_eos)\n                self.counter.update(symbols)\n                sents.append(symbols)\n\n        return sents\n\n    def count_sents(self, sents, verbose=False):\n        """"""\n            sents : a list of sentences, each a list of tokenized symbols\n        """"""\n        if verbose: logger.info(\'counting {} sents ...\'.format(len(sents)))\n        for idx, symbols in enumerate(sents):\n            if verbose and idx > 0 and idx % 500000 == 0:\n                logger.info(\'    line {}\'.format(idx))\n            self.counter.update(symbols)\n\n    def _build_from_file(self, vocab_file):\n        self.idx2sym = []\n        self.sym2idx = OrderedDict()\n\n        with open(vocab_file, \'r\', encoding=\'utf-8\') as f:\n            for line in f:\n                symb = line.strip().split()[0]\n                self.add_symbol(symb)\n        if \'<UNK>\' in self.sym2idx:\n            self.unk_idx = self.sym2idx[\'<UNK>\']\n        elif \'<unk>\' in self.sym2idx:\n            self.unk_idx = self.sym2idx[\'<unk>\']\n        else:\n            raise ValueError(\'No <unkown> token in vocabulary\')\n\n    def save_vocabulary(self, vocab_path):\n        """"""Save the tokenizer vocabulary to a directory or file.""""""\n        if os.path.isdir(vocab_path):\n            vocab_file = os.path.join(vocab_path, VOCAB_FILES_NAMES[\'pretrained_vocab_file\'])\n        torch.save(self.__dict__, vocab_file)\n        return (vocab_file,)\n\n    def build_vocab(self):\n        if self.vocab_file:\n            logger.info(\'building vocab from {}\'.format(self.vocab_file))\n            self._build_from_file(self.vocab_file)\n            logger.info(\'final vocab size {}\'.format(len(self)))\n        else:\n            logger.info(\'building vocab with min_freq={}, max_size={}\'.format(\n                self.min_freq, self.max_size))\n            self.idx2sym = []\n            self.sym2idx = OrderedDict()\n\n            for sym in self.special:\n                self.add_special(sym)\n\n            for sym, cnt in self.counter.most_common(self.max_size):\n                if cnt < self.min_freq: break\n                self.add_symbol(sym)\n\n            logger.info(\'final vocab size {} from {} unique tokens\'.format(\n                len(self), len(self.counter)))\n\n    def encode_file(self, path, ordered=False, verbose=False, add_eos=True,\n            add_double_eos=False):\n        if verbose: logger.info(\'encoding file {} ...\'.format(path))\n        assert os.path.exists(path)\n        encoded = []\n        with open(path, \'r\', encoding=\'utf-8\') as f:\n            for idx, line in enumerate(f):\n                if verbose and idx > 0 and idx % 500000 == 0:\n                    logger.info(\'    line {}\'.format(idx))\n                symbols = self.tokenize(line, add_eos=add_eos,\n                    add_double_eos=add_double_eos)\n                encoded.append(self.convert_to_tensor(symbols))\n\n        if ordered:\n            encoded = torch.cat(encoded)\n\n        return encoded\n\n    def encode_sents(self, sents, ordered=False, verbose=False):\n        if verbose: logger.info(\'encoding {} sents ...\'.format(len(sents)))\n        encoded = []\n        for idx, symbols in enumerate(sents):\n            if verbose and idx > 0 and idx % 500000 == 0:\n                logger.info(\'    line {}\'.format(idx))\n            encoded.append(self.convert_to_tensor(symbols))\n\n        if ordered:\n            encoded = torch.cat(encoded)\n\n        return encoded\n\n    def add_special(self, sym):\n        if sym not in self.sym2idx:\n            self.idx2sym.append(sym)\n            self.sym2idx[sym] = len(self.idx2sym) - 1\n            setattr(self, \'{}_idx\'.format(sym.strip(\'<>\')), self.sym2idx[sym])\n\n    def add_symbol(self, sym):\n        if sym not in self.sym2idx:\n            self.idx2sym.append(sym)\n            self.sym2idx[sym] = len(self.idx2sym) - 1\n\n    def _convert_id_to_token(self, idx):\n        """"""Converts an id in a token (BPE) using the vocab.""""""\n        assert 0 <= idx < len(self), \'Index {} out of vocabulary range\'.format(idx)\n        return self.idx2sym[idx]\n\n    def _convert_token_to_id(self, sym):\n        """""" Converts a token (str/unicode) in an id using the vocab. """"""\n        if sym in self.sym2idx:\n            return self.sym2idx[sym]\n        else:\n            # logger.info(\'encounter unk {}\'.format(sym))\n            # assert \'<eos>\' not in sym\n            if hasattr(self, \'unk_idx\'):\n                return self.sym2idx.get(sym, self.unk_idx)\n            # Backward compatibility with pre-trained models\n            elif \'<unk>\' in self.sym2idx:\n                return self.sym2idx[\'<unk>\']\n            elif \'<UNK>\' in self.sym2idx:\n                return self.sym2idx[\'<UNK>\']\n            else:\n                raise ValueError(\'Token not in vocabulary and no <unk> token in vocabulary for replacement\')\n\n    def convert_tokens_to_string(self, tokens):\n        """""" Converts a sequence of tokens (string) in a single string. """"""\n        out_string = \' \'.join(tokens).strip()\n        return out_string\n\n    def convert_to_tensor(self, symbols):\n        return torch.LongTensor(self.convert_tokens_to_ids(symbols))\n\n    @property\n    def vocab_size(self):\n        return len(self.idx2sym)\n\n    def _tokenize(self, line, add_eos=False, add_double_eos=False):\n        line = line.strip()\n        # convert to lower case\n        if self.lower_case:\n            line = line.lower()\n\n        # empty delimiter \'\' will evaluate False\n        if self.delimiter == \'\':\n            symbols = line\n        else:\n            symbols = line.split(self.delimiter)\n\n        if add_double_eos: # lm1b\n            return [\'<S>\'] + symbols + [\'<S>\']\n        elif add_eos:\n            return symbols + [\'<eos>\']\n        else:\n            return symbols\n\n\nclass LMOrderedIterator(object):\n    def __init__(self, data, bsz, bptt, device=\'cpu\', ext_len=None):\n        """"""\n            data -- LongTensor -- the LongTensor is strictly ordered\n        """"""\n        self.bsz = bsz\n        self.bptt = bptt\n        self.ext_len = ext_len if ext_len is not None else 0\n\n        self.device = device\n\n        # Work out how cleanly we can divide the dataset into bsz parts.\n        self.n_step = data.size(0) // bsz\n\n        # Trim off any extra elements that wouldn\'t cleanly fit (remainders).\n        data = data.narrow(0, 0, self.n_step * bsz)\n\n        # Evenly divide the data across the bsz batches.\n        self.data = data.view(bsz, -1).t().contiguous().to(device)\n\n        # Number of mini-batches\n        self.n_batch = (self.n_step + self.bptt - 1) // self.bptt\n\n    def get_batch(self, i, bptt=None):\n        if bptt is None: bptt = self.bptt\n        seq_len = min(bptt, self.data.size(0) - 1 - i)\n\n        end_idx = i + seq_len\n        beg_idx = max(0, i - self.ext_len)\n\n        data = self.data[beg_idx:end_idx]\n        target = self.data[i+1:i+1+seq_len]\n\n        data_out = data.transpose(0, 1).contiguous().to(self.device)\n        target_out = target.transpose(0, 1).contiguous().to(self.device)\n\n        return data_out, target_out, seq_len\n\n    def get_fixlen_iter(self, start=0):\n        for i in range(start, self.data.size(0) - 1, self.bptt):\n            yield self.get_batch(i)\n\n    def get_varlen_iter(self, start=0, std=5, min_len=5, max_deviation=3):\n        max_len = self.bptt + max_deviation * std\n        i = start\n        while True:\n            bptt = self.bptt if np.random.random() < 0.95 else self.bptt / 2.\n            bptt = min(max_len, max(min_len, int(np.random.normal(bptt, std))))\n            data, target, seq_len = self.get_batch(i, bptt)\n            i += seq_len\n            yield data, target, seq_len\n            if i >= self.data.size(0) - 2:\n                break\n\n    def __iter__(self):\n        return self.get_fixlen_iter()\n\n\nclass LMShuffledIterator(object):\n    def __init__(self, data, bsz, bptt, device=\'cpu\', ext_len=None, shuffle=False):\n        """"""\n            data -- list[LongTensor] -- there is no order among the LongTensors\n        """"""\n        self.data = data\n\n        self.bsz = bsz\n        self.bptt = bptt\n        self.ext_len = ext_len if ext_len is not None else 0\n\n        self.device = device\n        self.shuffle = shuffle\n\n    def get_sent_stream(self):\n        # index iterator\n        epoch_indices = np.random.permutation(len(self.data)) if self.shuffle \\\n            else np.array(range(len(self.data)))\n\n        # sentence iterator\n        for idx in epoch_indices:\n            yield self.data[idx]\n\n    def stream_iterator(self, sent_stream):\n        # streams for each data in the batch\n        streams = [None] * self.bsz\n\n        data = torch.LongTensor(self.bptt, self.bsz)\n        target = torch.LongTensor(self.bptt, self.bsz)\n\n        n_retain = 0\n\n        while True:\n            # data   : [n_retain+bptt x bsz]\n            # target : [bptt x bsz]\n            data[n_retain:].fill_(-1)\n            target.fill_(-1)\n\n            valid_batch = True\n\n            for i in range(self.bsz):\n                n_filled = 0\n                try:\n                    while n_filled < self.bptt:\n                        if streams[i] is None or len(streams[i]) <= 1:\n                            streams[i] = next(sent_stream)\n                        # number of new tokens to fill in\n                        n_new = min(len(streams[i]) - 1, self.bptt - n_filled)\n                        # first n_retain tokens are retained from last batch\n                        data[n_retain+n_filled:n_retain+n_filled+n_new, i] = \\\n                            streams[i][:n_new]\n                        target[n_filled:n_filled+n_new, i] = \\\n                            streams[i][1:n_new+1]\n                        streams[i] = streams[i][n_new:]\n                        n_filled += n_new\n                except StopIteration:\n                    valid_batch = False\n                    break\n\n            if not valid_batch:\n                return\n\n            data_out = data.transpose(0, 1).contiguous().to(self.device)\n            target_out = target.transpose(0, 1).contiguous().to(self.device)\n\n            yield data_out, target_out, self.bptt\n\n            n_retain = min(data.size(0), self.ext_len)\n            if n_retain > 0:\n                data[:n_retain] = data[-n_retain:]\n            data.resize_(n_retain + self.bptt, data.size(1))\n\n    def __iter__(self):\n        # sent_stream is an iterator\n        sent_stream = self.get_sent_stream()\n\n        for batch in self.stream_iterator(sent_stream):\n            yield batch\n\n\nclass LMMultiFileIterator(LMShuffledIterator):\n    def __init__(self, paths, vocab, bsz, bptt, device=\'cpu\', ext_len=None,\n        shuffle=False):\n\n        self.paths = paths\n        self.vocab = vocab\n\n        self.bsz = bsz\n        self.bptt = bptt\n        self.ext_len = ext_len if ext_len is not None else 0\n\n        self.device = device\n        self.shuffle = shuffle\n\n    def get_sent_stream(self, path):\n        sents = self.vocab.encode_file(path, add_double_eos=True)\n        if self.shuffle:\n            np.random.shuffle(sents)\n        sent_stream = iter(sents)\n\n        return sent_stream\n\n    def __iter__(self):\n        if self.shuffle:\n            np.random.shuffle(self.paths)\n\n        for path in self.paths:\n            # sent_stream is an iterator\n            sent_stream = self.get_sent_stream(path)\n            for batch in self.stream_iterator(sent_stream):\n                yield batch\n\n\nclass TransfoXLCorpus(object):\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, cache_dir=None, *inputs, **kwargs):\n        """"""\n        Instantiate a pre-processed corpus.\n        """"""\n        vocab = TransfoXLTokenizer.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n        if pretrained_model_name_or_path in PRETRAINED_CORPUS_ARCHIVE_MAP:\n            corpus_file = PRETRAINED_CORPUS_ARCHIVE_MAP[pretrained_model_name_or_path]\n        else:\n            corpus_file = os.path.join(pretrained_model_name_or_path, CORPUS_NAME)\n        # redirect to the cache, if necessary\n        try:\n            resolved_corpus_file = cached_path(corpus_file, cache_dir=cache_dir)\n        except EnvironmentError:\n            logger.error(\n                ""Corpus \'{}\' was not found in corpus list ({}). ""\n                ""We assumed \'{}\' was a path or url but couldn\'t find files {} ""\n                ""at this path or url."".format(\n                    pretrained_model_name_or_path,\n                    \', \'.join(PRETRAINED_CORPUS_ARCHIVE_MAP.keys()),\n                    pretrained_model_name_or_path,\n                    corpus_file))\n            return None\n        if resolved_corpus_file == corpus_file:\n            logger.info(""loading corpus file {}"".format(corpus_file))\n        else:\n            logger.info(""loading corpus file {} from cache at {}"".format(\n                corpus_file, resolved_corpus_file))\n\n        # Instantiate tokenizer.\n        corpus = cls(*inputs, **kwargs)\n        corpus_dict = torch.load(resolved_corpus_file)\n        for key, value in corpus_dict.items():\n            corpus.__dict__[key] = value\n        corpus.vocab = vocab\n        if corpus.train is not None:\n            corpus.train = torch.tensor(corpus.train, dtype=torch.long)\n        if corpus.valid is not None:\n            corpus.valid = torch.tensor(corpus.valid, dtype=torch.long)\n        if corpus.test is not None:\n            corpus.test = torch.tensor(corpus.test, dtype=torch.long)\n        return corpus\n\n    def __init__(self, *args, **kwargs):\n        self.vocab = TransfoXLTokenizer(*args, **kwargs)\n        self.dataset = None\n        self.train = None\n        self.valid = None\n        self.test = None\n\n    def build_corpus(self, path, dataset):\n        self.dataset = dataset\n\n        if self.dataset in [\'ptb\', \'wt2\', \'enwik8\', \'text8\']:\n            self.vocab.count_file(os.path.join(path, \'train.txt\'))\n            self.vocab.count_file(os.path.join(path, \'valid.txt\'))\n            self.vocab.count_file(os.path.join(path, \'test.txt\'))\n        elif self.dataset == \'wt103\':\n            self.vocab.count_file(os.path.join(path, \'train.txt\'))\n        elif self.dataset == \'lm1b\':\n            train_path_pattern = os.path.join(\n                path, \'1-billion-word-language-modeling-benchmark-r13output\',\n                \'training-monolingual.tokenized.shuffled\', \'news.en-*\')\n            train_paths = glob.glob(train_path_pattern)\n            # the vocab will load from file when build_vocab() is called\n\n        self.vocab.build_vocab()\n\n        if self.dataset in [\'ptb\', \'wt2\', \'wt103\']:\n            self.train = self.vocab.encode_file(\n                os.path.join(path, \'train.txt\'), ordered=True)\n            self.valid = self.vocab.encode_file(\n                os.path.join(path, \'valid.txt\'), ordered=True)\n            self.test = self.vocab.encode_file(\n                os.path.join(path, \'test.txt\'), ordered=True)\n        elif self.dataset in [\'enwik8\', \'text8\']:\n            self.train = self.vocab.encode_file(\n                os.path.join(path, \'train.txt\'), ordered=True, add_eos=False)\n            self.valid = self.vocab.encode_file(\n                os.path.join(path, \'valid.txt\'), ordered=True, add_eos=False)\n            self.test = self.vocab.encode_file(\n                os.path.join(path, \'test.txt\'), ordered=True, add_eos=False)\n        elif self.dataset == \'lm1b\':\n            self.train = train_paths\n            self.valid = self.vocab.encode_file(\n                os.path.join(path, \'valid.txt\'), ordered=False, add_double_eos=True)\n            self.test = self.vocab.encode_file(\n                os.path.join(path, \'test.txt\'), ordered=False, add_double_eos=True)\n\n    def get_iterator(self, split, *args, **kwargs):\n        if split == \'train\':\n            if self.dataset in [\'ptb\', \'wt2\', \'wt103\', \'enwik8\', \'text8\']:\n                data_iter = LMOrderedIterator(self.train, *args, **kwargs)\n            elif self.dataset == \'lm1b\':\n                kwargs[\'shuffle\'] = True\n                data_iter = LMMultiFileIterator(self.train, self.vocab, *args, **kwargs)\n        elif split in [\'valid\', \'test\']:\n            data = self.valid if split == \'valid\' else self.test\n            if self.dataset in [\'ptb\', \'wt2\', \'wt103\', \'enwik8\', \'text8\']:\n                data_iter = LMOrderedIterator(data, *args, **kwargs)\n            elif self.dataset == \'lm1b\':\n                data_iter = LMShuffledIterator(data, *args, **kwargs)\n\n        return data_iter\n\n\ndef get_lm_corpus(datadir, dataset):\n    fn = os.path.join(datadir, \'cache.pt\')\n    fn_pickle = os.path.join(datadir, \'cache.pkl\')\n    if os.path.exists(fn):\n        logger.info(\'Loading cached dataset...\')\n        corpus = torch.load(fn_pickle)\n    elif os.path.exists(fn):\n        logger.info(\'Loading cached dataset from pickle...\')\n        with open(fn, ""rb"") as fp:\n            corpus = pickle.load(fp)\n    else:\n        logger.info(\'Producing dataset {}...\'.format(dataset))\n        kwargs = {}\n        if dataset in [\'wt103\', \'wt2\']:\n            kwargs[\'special\'] = [\'<eos>\']\n            kwargs[\'lower_case\'] = False\n        elif dataset == \'ptb\':\n            kwargs[\'special\'] = [\'<eos>\']\n            kwargs[\'lower_case\'] = True\n        elif dataset == \'lm1b\':\n            kwargs[\'special\'] = []\n            kwargs[\'lower_case\'] = False\n            kwargs[\'vocab_file\'] = os.path.join(datadir, \'1b_word_vocab.txt\')\n        elif dataset in [\'enwik8\', \'text8\']:\n            pass\n\n        corpus = TransfoXLCorpus(datadir, dataset, **kwargs)\n        torch.save(corpus, fn)\n\n    return corpus\n'"
models/transformers/tokenization_utils.py,5,"b'# coding=utf-8\n# Copyright 2018 The Open AI Team Authors and The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tokenization classes for OpenAI GPT.""""""\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport logging\nimport os\nimport json\nimport six\nimport copy\nfrom io import open\n\nfrom .file_utils import cached_path, is_tf_available, is_torch_available\n\nif is_tf_available():\n    import tensorflow as tf\nif is_torch_available():\n    import torch\n\nlogger = logging.getLogger(__name__)\n\nSPECIAL_TOKENS_MAP_FILE = \'special_tokens_map.json\'\nADDED_TOKENS_FILE = \'added_tokens.json\'\nTOKENIZER_CONFIG_FILE = \'tokenizer_config.json\'\n\nclass PreTrainedTokenizer(object):\n    """""" Base class for all tokenizers.\n    Handle all the shared methods for tokenization and special tokens as well as methods dowloading/caching/loading pretrained tokenizers as well as adding tokens to the vocabulary.\n\n    This class also contain the added tokens in a unified way on top of all tokenizers so we don\'t have to handle the specific vocabulary augmentation methods of the various underlying dictionary structures (BPE, sentencepiece...).\n\n    Class attributes (overridden by derived classes):\n\n        - ``vocab_files_names``: a python ``dict`` with, as keys, the ``__init__`` keyword name of each vocabulary file required by the model, and as associated values, the filename for saving the associated file (string).\n        - ``pretrained_vocab_files_map``: a python ``dict of dict`` the high-level keys being the ``__init__`` keyword name of each vocabulary file required by the model, the low-level being the `short-cut-names` (string) of the pretrained models with, as associated values, the `url` (string) to the associated pretrained vocabulary file.\n        - ``max_model_input_sizes``: a python ``dict`` with, as keys, the `short-cut-names` (string) of the pretrained models, and as associated values, the maximum length of the sequence inputs of this model, or None if the model has no maximum input size.\n        - ``pretrained_init_configuration``: a python ``dict`` with, as keys, the `short-cut-names` (string) of the pretrained models, and as associated values, a dictionnary of specific arguments to pass to the ``__init__``method of the tokenizer class for this pretrained model when loading the tokenizer with the ``from_pretrained()`` method.\n\n    Parameters:\n\n        - ``bos_token``: (`Optional`) string: a beginning of sentence token. Will be associated to ``self.bos_token`` and ``self.bos_token_id``\n\n        - ``eos_token``: (`Optional`) string: an end of sentence token. Will be associated to ``self.eos_token`` and ``self.eos_token_id``\n\n        - ``unk_token``: (`Optional`) string: an unknown token. Will be associated to ``self.unk_token`` and ``self.unk_token_id``\n\n        - ``sep_token``: (`Optional`) string: a separation token (e.g. to separate context and query in an input sequence). Will be associated to ``self.sep_token`` and ``self.sep_token_id``\n\n        - ``pad_token``: (`Optional`) string: a padding token. Will be associated to ``self.pad_token`` and ``self.pad_token_id``\n\n        - ``cls_token``: (`Optional`) string: a classification token (e.g. to extract a summary of an input sequence leveraging self-attention along the full depth of the model). Will be associated to ``self.cls_token`` and ``self.cls_token_id``\n\n        - ``mask_token``: (`Optional`) string: a masking token (e.g. when training a model with masked-language modeling). Will be associated to ``self.mask_token`` and ``self.mask_token_id``\n\n        - ``additional_special_tokens``: (`Optional`) list: a list of additional special tokens. Adding all special tokens here ensure they won\'t be split by the tokenization process. Will be associated to ``self.additional_special_tokens`` and ``self.additional_special_tokens_ids``\n    """"""\n    vocab_files_names = {}\n    pretrained_vocab_files_map = {}\n    pretrained_init_configuration = {}\n    max_model_input_sizes = {}\n\n    SPECIAL_TOKENS_ATTRIBUTES = [""bos_token"", ""eos_token"", ""unk_token"", ""sep_token"",\n                                 ""pad_token"", ""cls_token"", ""mask_token"",\n                                 ""additional_special_tokens""]\n\n    @property\n    def bos_token(self):\n        """""" Beginning of sentence token (string). Log an error if used while not having been set. """"""\n        if self._bos_token is None:\n            logger.error(""Using bos_token, but it is not set yet."")\n        return self._bos_token\n\n    @property\n    def eos_token(self):\n        """""" End of sentence token (string). Log an error if used while not having been set. """"""\n        if self._eos_token is None:\n            logger.error(""Using eos_token, but it is not set yet."")\n        return self._eos_token\n\n    @property\n    def unk_token(self):\n        """""" Unknown token (string). Log an error if used while not having been set. """"""\n        if self._unk_token is None:\n            logger.error(""Using unk_token, but it is not set yet."")\n        return self._unk_token\n\n    @property\n    def sep_token(self):\n        """""" Separation token (string). E.g. separate context and query in an input sequence. Log an error if used while not having been set. """"""\n        if self._sep_token is None:\n            logger.error(""Using sep_token, but it is not set yet."")\n        return self._sep_token\n\n    @property\n    def pad_token(self):\n        """""" Padding token (string). Log an error if used while not having been set. """"""\n        if self._pad_token is None:\n            logger.error(""Using pad_token, but it is not set yet."")\n        return self._pad_token\n\n    @property\n    def cls_token(self):\n        """""" Classification token (string). E.g. to extract a summary of an input sequence leveraging self-attention along the full depth of the model. Log an error if used while not having been set. """"""\n        if self._cls_token is None:\n            logger.error(""Using cls_token, but it is not set yet."")\n        return self._cls_token\n\n    @property\n    def mask_token(self):\n        """""" Mask token (string). E.g. when training a model with masked-language modeling. Log an error if used while not having been set. """"""\n        if self._mask_token is None:\n            logger.error(""Using mask_token, but it is not set yet."")\n        return self._mask_token\n\n    @property\n    def additional_special_tokens(self):\n        """""" All the additional special tokens you may want to use (list of strings). Log an error if used while not having been set. """"""\n        if self._additional_special_tokens is None:\n            logger.error(""Using additional_special_tokens, but it is not set yet."")\n        return self._additional_special_tokens\n\n    @bos_token.setter\n    def bos_token(self, value):\n        self._bos_token = value\n\n    @eos_token.setter\n    def eos_token(self, value):\n        self._eos_token = value\n\n    @unk_token.setter\n    def unk_token(self, value):\n        self._unk_token = value\n\n    @sep_token.setter\n    def sep_token(self, value):\n        self._sep_token = value\n\n    @pad_token.setter\n    def pad_token(self, value):\n        self._pad_token = value\n\n    @cls_token.setter\n    def cls_token(self, value):\n        self._cls_token = value\n\n    @mask_token.setter\n    def mask_token(self, value):\n        self._mask_token = value\n\n    @additional_special_tokens.setter\n    def additional_special_tokens(self, value):\n        self._additional_special_tokens = value\n\n    @property\n    def bos_token_id(self):\n        """""" Id of the beginning of sentence token in the vocabulary. Log an error if used while not having been set. """"""\n        return self.convert_tokens_to_ids(self.bos_token)\n\n    @property\n    def eos_token_id(self):\n        """""" Id of the end of sentence token in the vocabulary. Log an error if used while not having been set. """"""\n        return self.convert_tokens_to_ids(self.eos_token)\n\n    @property\n    def unk_token_id(self):\n        """""" Id of the unknown token in the vocabulary. Log an error if used while not having been set. """"""\n        return self.convert_tokens_to_ids(self.unk_token)\n\n    @property\n    def sep_token_id(self):\n        """""" Id of the separation token in the vocabulary. E.g. separate context and query in an input sequence. Log an error if used while not having been set. """"""\n        return self.convert_tokens_to_ids(self.sep_token)\n\n    @property\n    def pad_token_id(self):\n        """""" Id of the padding token in the vocabulary. Log an error if used while not having been set. """"""\n        return self.convert_tokens_to_ids(self.pad_token)\n\n    @property\n    def cls_token_id(self):\n        """""" Id of the classification token in the vocabulary. E.g. to extract a summary of an input sequence leveraging self-attention along the full depth of the model. Log an error if used while not having been set. """"""\n        return self.convert_tokens_to_ids(self.cls_token)\n\n    @property\n    def mask_token_id(self):\n        """""" Id of the mask token in the vocabulary. E.g. when training a model with masked-language modeling. Log an error if used while not having been set. """"""\n        return self.convert_tokens_to_ids(self.mask_token)\n\n    @property\n    def additional_special_tokens_ids(self):\n        """""" Ids of all the additional special tokens in the vocabulary (list of integers). Log an error if used while not having been set. """"""\n        return self.convert_tokens_to_ids(self.additional_special_tokens)\n\n    def __init__(self, max_len=None, **kwargs):\n        self._bos_token = None\n        self._eos_token = None\n        self._unk_token = None\n        self._sep_token = None\n        self._pad_token = None\n        self._cls_token = None\n        self._mask_token = None\n        self._additional_special_tokens = []\n\n        self.max_len = max_len if max_len is not None else int(1e12)\n\n        # Added tokens\n        self.added_tokens_encoder = {}\n        self.added_tokens_decoder = {}\n\n        # inputs and kwargs for saving and re-loading (see ``from_pretrained`` and ``save_pretrained``)\n        self.init_inputs = ()\n        self.init_kwargs = {}\n\n        for key, value in kwargs.items():\n            if key in self.SPECIAL_TOKENS_ATTRIBUTES:\n                if key == \'additional_special_tokens\':\n                    assert isinstance(value, (list, tuple)) and all(isinstance(t, str) or (six.PY2 and isinstance(t, unicode)) for t in value)\n                else:\n                    assert isinstance(value, str) or (six.PY2 and isinstance(value, unicode))\n                setattr(self, key, value)\n\n\n    @classmethod\n    def from_pretrained(cls, *inputs, **kwargs):\n        r""""""\n        Instantiate a :class:`~transformers.PreTrainedTokenizer` (or a derived class) from a predefined tokenizer.\n\n        Args:\n            pretrained_model_name_or_path: either:\n\n                - a string with the `shortcut name` of a predefined tokenizer to load from cache or download, e.g.: ``bert-base-uncased``.\n                - a path to a `directory` containing vocabulary files required by the tokenizer, for instance saved using the :func:`~transformers.PreTrainedTokenizer.save_pretrained` method, e.g.: ``./my_model_directory/``.\n                - (not applicable to all derived classes) a path or url to a single saved vocabulary file if and only if the tokenizer only requires a single vocabulary file (e.g. Bert, XLNet), e.g.: ``./my_model_directory/vocab.txt``.\n\n            cache_dir: (`optional`) string:\n                Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the standard cache should not be used.\n\n            force_download: (`optional`) boolean, default False:\n                Force to (re-)download the vocabulary files and override the cached versions if they exists.\n\n            proxies: (`optional`) dict, default None:\n                A dictionary of proxy servers to use by protocol or endpoint, e.g.: {\'http\': \'foo.bar:3128\', \'http://hostname\': \'foo.bar:4012\'}.\n                The proxies are used on each request.\n\n            inputs: (`optional`) positional arguments: will be passed to the Tokenizer ``__init__`` method.\n\n            kwargs: (`optional`) keyword arguments: will be passed to the Tokenizer ``__init__`` method. Can be used to set special tokens like ``bos_token``, ``eos_token``, ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``, ``additional_special_tokens``. See parameters in the doc string of :class:`~transformers.PreTrainedTokenizer` for details.\n\n        Examples::\n\n            # We can\'t instantiate directly the base class `PreTrainedTokenizer` so let\'s show our examples on a derived class: BertTokenizer\n\n            # Download vocabulary from S3 and cache.\n            tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n\n            # If vocabulary files are in a directory (e.g. tokenizer was saved using `save_pretrained(\'./test/saved_model/\')`)\n            tokenizer = BertTokenizer.from_pretrained(\'./test/saved_model/\')\n\n            # If the tokenizer uses a single vocabulary file, you can point directly to this file\n            tokenizer = BertTokenizer.from_pretrained(\'./test/saved_model/my_vocab.txt\')\n\n            # You can link tokens to special vocabulary when instantiating\n            tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\', unk_token=\'<unk>\')\n            # You should be sure \'<unk>\' is in the vocabulary when doing that.\n            # Otherwise use tokenizer.add_special_tokens({\'unk_token\': \'<unk>\'}) instead)\n            assert tokenizer.unk_token == \'<unk>\'\n\n        """"""\n        return cls._from_pretrained(*inputs, **kwargs)\n\n\n    @classmethod\n    def _from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs):\n        cache_dir = kwargs.pop(\'cache_dir\', None)\n        force_download = kwargs.pop(\'force_download\', False)\n        proxies = kwargs.pop(\'proxies\', None)\n\n        s3_models = list(cls.max_model_input_sizes.keys())\n        vocab_files = {}\n        init_configuration = {}\n        if pretrained_model_name_or_path in s3_models:\n            # Get the vocabulary from AWS S3 bucket\n            for file_id, map_list in cls.pretrained_vocab_files_map.items():\n                vocab_files[file_id] = map_list[pretrained_model_name_or_path]\n            if cls.pretrained_init_configuration and pretrained_model_name_or_path in cls.pretrained_init_configuration:\n                init_configuration = cls.pretrained_init_configuration[pretrained_model_name_or_path]\n        else:\n            # Get the vocabulary from local files\n            logger.info(\n                ""Model name \'{}\' not found in model shortcut name list ({}). ""\n                ""Assuming \'{}\' is a path or url to a directory containing tokenizer files."".format(\n                    pretrained_model_name_or_path, \', \'.join(s3_models),\n                    pretrained_model_name_or_path))\n\n            # Look for the tokenizer main vocabulary files\n            for file_id, file_name in cls.vocab_files_names.items():\n                if os.path.isdir(pretrained_model_name_or_path):\n                    # If a directory is provided we look for the standard filenames\n                    full_file_name = os.path.join(pretrained_model_name_or_path, file_name)\n                else:\n                    # If a path to a file is provided we use it (will only work for non-BPE tokenizer using a single vocabulary file)\n                    full_file_name = pretrained_model_name_or_path\n                if not os.path.exists(full_file_name):\n                    logger.info(""Didn\'t find file {}. We won\'t load it."".format(full_file_name))\n                    full_file_name = None\n                vocab_files[file_id] = full_file_name\n\n            # Look for the additional tokens files\n            additional_files_names = {\'added_tokens_file\': ADDED_TOKENS_FILE,\n                                      \'special_tokens_map_file\': SPECIAL_TOKENS_MAP_FILE,\n                                      \'tokenizer_config_file\': TOKENIZER_CONFIG_FILE,\n                                      }\n\n            # If a path to a file was provided, get the parent directory\n            saved_directory = pretrained_model_name_or_path\n            if os.path.exists(saved_directory) and not os.path.isdir(saved_directory):\n                saved_directory = os.path.dirname(saved_directory)\n\n            for file_id, file_name in additional_files_names.items():\n                full_file_name = os.path.join(saved_directory, file_name)\n                if not os.path.exists(full_file_name):\n                    logger.info(""Didn\'t find file {}. We won\'t load it."".format(full_file_name))\n                    full_file_name = None\n                vocab_files[file_id] = full_file_name\n\n            if all(full_file_name is None for full_file_name in vocab_files.values()):\n                raise EnvironmentError(\n                    ""Model name \'{}\' was not found in tokenizers model name list ({}). ""\n                    ""We assumed \'{}\' was a path or url to a directory containing vocabulary files ""\n                    ""named {} but couldn\'t find such vocabulary files at this path or url."".format(\n                        pretrained_model_name_or_path, \', \'.join(s3_models),\n                        pretrained_model_name_or_path, \n                        list(cls.vocab_files_names.values())))\n\n        # Get files from url, cache, or disk depending on the case\n        try:\n            resolved_vocab_files = {}\n            for file_id, file_path in vocab_files.items():\n                if file_path is None:\n                    resolved_vocab_files[file_id] = None\n                else:\n                    resolved_vocab_files[file_id] = cached_path(file_path, cache_dir=cache_dir, force_download=force_download, proxies=proxies)\n        except EnvironmentError:\n            if pretrained_model_name_or_path in s3_models:\n                msg = ""Couldn\'t reach server at \'{}\' to download vocabulary files.""\n            else:\n                msg = ""Model name \'{}\' was not found in tokenizers model name list ({}). "" \\\n                    ""We assumed \'{}\' was a path or url to a directory containing vocabulary files "" \\\n                    ""named {}, but couldn\'t find such vocabulary files at this path or url."".format(\n                        pretrained_model_name_or_path, \', \'.join(s3_models),\n                        pretrained_model_name_or_path,\n                        list(cls.vocab_files_names.values()))\n\n            raise EnvironmentError(msg)\n\n        for file_id, file_path in vocab_files.items():\n            if file_path == resolved_vocab_files[file_id]:\n                logger.info(""loading file {}"".format(file_path))\n            else:\n                logger.info(""loading file {} from cache at {}"".format(\n                    file_path, resolved_vocab_files[file_id]))\n\n        # Prepare tokenizer initialization kwargs\n        # Did we saved some inputs and kwargs to reload ?\n        tokenizer_config_file = resolved_vocab_files.pop(\'tokenizer_config_file\', None)\n        if tokenizer_config_file is not None:\n            init_kwargs = json.load(open(tokenizer_config_file, encoding=""utf-8""))\n            saved_init_inputs = init_kwargs.pop(\'init_inputs\', ())\n            if not init_inputs:\n                init_inputs = saved_init_inputs\n        else:\n            init_kwargs = init_configuration\n\n        # Update with newly provided kwargs\n        init_kwargs.update(kwargs)\n\n        # Set max length if needed\n        if pretrained_model_name_or_path in cls.max_model_input_sizes:\n            # if we\'re using a pretrained model, ensure the tokenizer\n            # wont index sequences longer than the number of positional embeddings\n            max_len = cls.max_model_input_sizes[pretrained_model_name_or_path]\n            if max_len is not None and isinstance(max_len, (int, float)):\n                init_kwargs[\'max_len\'] = min(init_kwargs.get(\'max_len\', int(1e12)), max_len)\n\n        # Merge resolved_vocab_files arguments in init_kwargs.\n        added_tokens_file = resolved_vocab_files.pop(\'added_tokens_file\', None)\n        special_tokens_map_file = resolved_vocab_files.pop(\'special_tokens_map_file\', None)\n        for args_name, file_path in resolved_vocab_files.items():\n            if args_name not in init_kwargs:\n                init_kwargs[args_name] = file_path\n        if special_tokens_map_file is not None:\n            special_tokens_map = json.load(open(special_tokens_map_file, encoding=""utf-8""))\n            for key, value in special_tokens_map.items():\n                if key not in init_kwargs:\n                    init_kwargs[key] = value\n\n        # Instantiate tokenizer.\n        tokenizer = cls(*init_inputs, **init_kwargs)\n\n        # Save inputs and kwargs for saving and re-loading with ``save_pretrained``\n        tokenizer.init_inputs = init_inputs\n        tokenizer.init_kwargs = init_kwargs\n\n        # Add supplementary tokens.\n        if added_tokens_file is not None:\n            added_tok_encoder = json.load(open(added_tokens_file, encoding=""utf-8""))\n            added_tok_decoder = {v:k for k, v in added_tok_encoder.items()}\n            tokenizer.added_tokens_encoder.update(added_tok_encoder)\n            tokenizer.added_tokens_decoder.update(added_tok_decoder)\n\n        return tokenizer\n\n\n    def save_pretrained(self, save_directory):\n        """""" Save the tokenizer vocabulary files together with:\n                - added tokens,\n                - special-tokens-to-class-attributes-mapping,\n                - tokenizer instantiation positional and keywords inputs (e.g. do_lower_case for Bert).\n\n            This won\'t save modifications other than (added tokens and special token mapping) you may have\n            applied to the tokenizer after the instantiation (e.g. modifying tokenizer.do_lower_case after creation).\n\n            This method make sure the full tokenizer can then be re-loaded using the :func:`~transformers.PreTrainedTokenizer.from_pretrained` class method.\n        """"""\n        if not os.path.isdir(save_directory):\n            logger.error(""Saving directory ({}) should be a directory"".format(save_directory))\n            return\n\n        special_tokens_map_file = os.path.join(save_directory, SPECIAL_TOKENS_MAP_FILE)\n        added_tokens_file = os.path.join(save_directory, ADDED_TOKENS_FILE)\n        tokenizer_config_file = os.path.join(save_directory, TOKENIZER_CONFIG_FILE)\n\n        tokenizer_config = copy.deepcopy(self.init_kwargs)\n        tokenizer_config[\'init_inputs\'] = copy.deepcopy(self.init_inputs)\n        for file_id in self.vocab_files_names.keys():\n            tokenizer_config.pop(file_id, None)\n\n        with open(tokenizer_config_file, \'w\', encoding=\'utf-8\') as f:\n            f.write(json.dumps(tokenizer_config, ensure_ascii=False))\n\n        with open(special_tokens_map_file, \'w\', encoding=\'utf-8\') as f:\n            f.write(json.dumps(self.special_tokens_map, ensure_ascii=False))\n\n        with open(added_tokens_file, \'w\', encoding=\'utf-8\') as f:\n            if self.added_tokens_encoder:\n                out_str = json.dumps(self.added_tokens_encoder, ensure_ascii=False)\n            else:\n                out_str = u""{}""\n            f.write(out_str)\n\n        vocab_files = self.save_vocabulary(save_directory)\n\n        return vocab_files + (special_tokens_map_file, added_tokens_file)\n\n\n    def save_vocabulary(self, save_directory):\n        """""" Save the tokenizer vocabulary to a directory. This method does *NOT* save added tokens\n            and special token mappings.\n\n            Please use :func:`~transformers.PreTrainedTokenizer.save_pretrained` `()` to save the full Tokenizer state if you want to reload it using the :func:`~transformers.PreTrainedTokenizer.from_pretrained` class method.\n        """"""\n        raise NotImplementedError\n\n\n    def vocab_size(self):\n        """""" Size of the base vocabulary (without the added tokens) """"""\n        raise NotImplementedError\n\n\n    def __len__(self):\n        """""" Size of the full vocabulary with the added tokens """"""\n        return self.vocab_size + len(self.added_tokens_encoder)\n\n\n    def add_tokens(self, new_tokens):\n        """"""\n        Add a list of new tokens to the tokenizer class. If the new tokens are not in the\n        vocabulary, they are added to it with indices starting from length of the current vocabulary.\n\n        Args:\n            new_tokens: list of string. Each string is a token to add. Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer assign the index of the ``unk_token`` to them).\n\n        Returns:\n            Number of tokens added to the vocabulary.\n\n        Examples::\n\n            # Let\'s see how to increase the vocabulary of Bert model and tokenizer\n            tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n            model = BertModel.from_pretrained(\'bert-base-uncased\')\n\n            num_added_toks = tokenizer.add_tokens([\'new_tok1\', \'my_new-tok2\'])\n            print(\'We have added\', num_added_toks, \'tokens\')\n            model.resize_token_embeddings(len(tokenizer))  # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e. the length of the tokenizer.\n        """"""\n        if not new_tokens:\n            return 0\n\n        to_add_tokens = []\n        for token in new_tokens:\n            assert isinstance(token, str) or (six.PY2 and isinstance(token, unicode))\n            if token != self.unk_token and \\\n                    self.convert_tokens_to_ids(token) == self.convert_tokens_to_ids(self.unk_token) and \\\n                    token not in to_add_tokens:\n                to_add_tokens.append(token)\n                logger.info(""Adding %s to the vocabulary"", token)\n\n        added_tok_encoder = dict((tok, len(self) + i) for i, tok in enumerate(to_add_tokens))\n        added_tok_decoder = {v:k for k, v in added_tok_encoder.items()}\n        self.added_tokens_encoder.update(added_tok_encoder)\n        self.added_tokens_decoder.update(added_tok_decoder)\n\n        return len(to_add_tokens)\n\n    def num_added_tokens(self, pair=False):\n        """"""\n        Returns the number of added tokens when encoding a sequence with special tokens.\n\n        Note:\n            This encodes inputs and checks the number of added tokens, and is therefore not efficient. Do not put this\n            inside your training loop.\n\n        Args:\n            pair: Returns the number of added tokens in the case of a sequence pair if set to True, returns the\n                number of added tokens in the case of a single sequence if set to False.\n\n        Returns:\n            Number of tokens added to sequences\n        """"""\n        token_ids_0 = []\n        token_ids_1 = []\n        return len(self.build_inputs_with_special_tokens(token_ids_0, token_ids_1 if pair else None))\n\n    def add_special_tokens(self, special_tokens_dict):\n        """"""\n        Add a dictionary of special tokens (eos, pad, cls...) to the encoder and link them\n        to class attributes. If special tokens are NOT in the vocabulary, they are added\n        to it (indexed starting from the last index of the current vocabulary).\n\n        Using `add_special_tokens` will ensure your special tokens can be used in several ways:\n\n        - special tokens are carefully handled by the tokenizer (they are never split)\n        - you can easily refer to special tokens using tokenizer class attributes like `tokenizer.cls_token`. This makes it easy to develop model-agnostic training and fine-tuning scripts.\n\n        When possible, special tokens are already registered for provided pretrained models (ex: BertTokenizer cls_token is already registered to be \'[CLS]\' and XLM\'s one is also registered to be \'</s>\')\n\n        Args:\n            special_tokens_dict: dict of string. Keys should be in the list of predefined special attributes:\n                [``bos_token``, ``eos_token``, ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``,\n                ``additional_special_tokens``].\n\n                Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer assign the index of the ``unk_token`` to them).\n\n        Returns:\n            Number of tokens added to the vocabulary.\n\n        Examples::\n\n            # Let\'s see how to add a new classification token to GPT-2\n            tokenizer = GPT2Tokenizer.from_pretrained(\'gpt2\')\n            model = GPT2Model.from_pretrained(\'gpt2\')\n\n            special_tokens_dict = {\'cls_token\': \'<CLS>\'}\n\n            num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n            print(\'We have added\', num_added_toks, \'tokens\')\n            model.resize_token_embeddings(len(tokenizer))  # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e. the length of the tokenizer.\n\n            assert tokenizer.cls_token == \'<CLS>\'\n        """"""\n        if not special_tokens_dict:\n            return 0\n\n        added_tokens = 0\n        for key, value in special_tokens_dict.items():\n            assert key in self.SPECIAL_TOKENS_ATTRIBUTES\n            if key == \'additional_special_tokens\':\n                assert isinstance(value, (list, tuple)) and all(isinstance(t, str) or (six.PY2 and isinstance(t, unicode)) for t in value)\n                added_tokens += self.add_tokens(value)\n            else:\n                assert isinstance(value, str) or (six.PY2 and isinstance(value, unicode))\n                added_tokens += self.add_tokens([value])\n            logger.info(""Assigning %s to the %s key of the tokenizer"", value, key)\n            setattr(self, key, value)\n\n        return added_tokens\n\n    def tokenize(self, text, **kwargs):\n        """""" Converts a string in a sequence of tokens (string), using the tokenizer.\n            Split in words for word-based vocabulary or sub-words for sub-word-based\n            vocabularies (BPE/SentencePieces/WordPieces).\n\n            Take care of added tokens.\n        """"""\n        def split_on_token(tok, text):\n            result = []\n            split_text = text.split(tok)\n            for i, sub_text in enumerate(split_text):\n                sub_text = sub_text.strip()\n                if i == 0 and not sub_text:\n                    result += [tok]\n                elif i == len(split_text) - 1:\n                    if sub_text:\n                        result += [sub_text]\n                    else:\n                        pass\n                else:\n                    if sub_text:\n                        result += [sub_text]\n                    result += [tok]\n            return result\n\n        def split_on_tokens(tok_list, text):\n            if not text:\n                return []\n            if not tok_list:\n                return self._tokenize(text, **kwargs)\n\n            tokenized_text = []\n            text_list = [text]\n            for tok in tok_list:\n                tokenized_text = []\n                for sub_text in text_list:\n                    if sub_text not in self.added_tokens_encoder \\\n                            and sub_text not in self.all_special_tokens:\n                        tokenized_text += split_on_token(tok, sub_text)\n                    else:\n                        tokenized_text += [sub_text]\n                text_list = tokenized_text\n\n            return sum((self._tokenize(token, **kwargs) if token not \\\n                    in self.added_tokens_encoder and token not in self.all_special_tokens \\\n                    else [token] for token in tokenized_text), [])\n\n        added_tokens = list(self.added_tokens_encoder.keys()) + self.all_special_tokens\n        tokenized_text = split_on_tokens(added_tokens, text)\n        return tokenized_text\n\n    def _tokenize(self, text, **kwargs):\n        """""" Converts a string in a sequence of tokens (string), using the tokenizer.\n            Split in words for word-based vocabulary or sub-words for sub-word-based\n            vocabularies (BPE/SentencePieces/WordPieces).\n\n            Do NOT take care of added tokens.\n        """"""\n        raise NotImplementedError\n\n    def convert_tokens_to_ids(self, tokens):\n        """""" Converts a single token, or a sequence of tokens, (str/unicode) in a single integer id\n            (resp. a sequence of ids), using the vocabulary.\n        """"""\n        if tokens is None:\n            return None\n\n        if isinstance(tokens, str) or (six.PY2 and isinstance(tokens, unicode)):\n            return self._convert_token_to_id_with_added_voc(tokens)\n\n        ids = []\n        for token in tokens:\n            ids.append(self._convert_token_to_id_with_added_voc(token))\n        if len(ids) > self.max_len:\n            logger.warning(""Token indices sequence length is longer than the specified maximum sequence length ""\n                           ""for this model ({} > {}). Running this sequence through the model will result in ""\n                           ""indexing errors"".format(len(ids), self.max_len))\n        return ids\n\n    def _convert_token_to_id_with_added_voc(self, token):\n        if token is None:\n            return None\n\n        if token in self.added_tokens_encoder:\n            return self.added_tokens_encoder[token]\n        return self._convert_token_to_id(token)\n\n    def _convert_token_to_id(self, token):\n        raise NotImplementedError\n\n    def encode(self,\n                text,\n                text_pair=None,\n                add_special_tokens=False,\n                max_length=None,\n                stride=0,\n                truncation_strategy=\'longest_first\',\n                return_tensors=None,\n                **kwargs):\n        """"""\n        Converts a string in a sequence of ids (integer), using the tokenizer and vocabulary.\n\n        Same as doing ``self.convert_tokens_to_ids(self.tokenize(text))``.\n\n        Args:\n            text: The first sequence to be encoded. This can be a string, a list of strings (tokenized string using\n                the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n                method)\n            text_pair: Optional second sequence to be encoded. This can be a string, a list of strings (tokenized\n                string using the `tokenize` method) or a list of integers (tokenized string ids using the\n                `convert_tokens_to_ids` method)\n            add_special_tokens: if set to ``True``, the sequences will be encoded with the special tokens relative\n                to their model.\n            max_length: if set to a number, will limit the total sequence returned so that it has a maximum length.\n                If there are overflowing tokens, those will be added to the returned dictionary\n            stride: if set to a number along with max_length, the overflowing tokens returned will contain some tokens\n                from the main sequence returned. The value of this argument defines the number of additional tokens.\n            truncation_strategy: string selected in the following options:\n                - \'longest_first\' (default) Iteratively reduce the inputs sequence until the input is under max_length\n                    starting from the longest one at each token (when there is a pair of input sequences)\n                - \'only_first\': Only truncate the first sequence\n                - \'only_second\': Only truncate the second sequence\n                - \'do_not_truncate\': Does not truncate (raise an error if the input sequence is longer than max_length)\n            return_tensors: (optional) can be set to \'tf\' or \'pt\' to return respectively TensorFlow tf.constant\n                or PyTorch torch.Tensor instead of a list of python integers.\n            **kwargs: passed to the `self.tokenize()` method\n        """"""\n        encoded_inputs = self.encode_plus(text,\n                                          text_pair=text_pair,\n                                          max_length=max_length,\n                                          add_special_tokens=add_special_tokens,\n                                          stride=stride,\n                                          truncation_strategy=truncation_strategy,\n                                          return_tensors=return_tensors,\n                                          **kwargs)\n\n        return encoded_inputs[""input_ids""]\n\n    def encode_plus(self,\n                    text,\n                    text_pair=None,\n                    add_special_tokens=False,\n                    max_length=None,\n                    stride=0,\n                    truncation_strategy=\'longest_first\',\n                    return_tensors=None,\n                    **kwargs):\n        """"""\n        Returns a dictionary containing the encoded sequence or sequence pair and additional informations:\n        the mask for sequence classification and the overflowing elements if a ``max_length`` is specified.\n\n        Args:\n            text: The first sequence to be encoded. This can be a string, a list of strings (tokenized string using\n                the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n                method)\n            text_pair: Optional second sequence to be encoded. This can be a string, a list of strings (tokenized\n                string using the `tokenize` method) or a list of integers (tokenized string ids using the\n                `convert_tokens_to_ids` method)\n            add_special_tokens: if set to ``True``, the sequences will be encoded with the special tokens relative\n                to their model.\n            max_length: if set to a number, will limit the total sequence returned so that it has a maximum length.\n                If there are overflowing tokens, those will be added to the returned dictionary\n            stride: if set to a number along with max_length, the overflowing tokens returned will contain some tokens\n                from the main sequence returned. The value of this argument defines the number of additional tokens.\n            truncation_strategy: string selected in the following options:\n                - \'longest_first\' (default) Iteratively reduce the inputs sequence until the input is under max_length\n                    starting from the longest one at each token (when there is a pair of input sequences)\n                - \'only_first\': Only truncate the first sequence\n                - \'only_second\': Only truncate the second sequence\n                - \'do_not_truncate\': Does not truncate (raise an error if the input sequence is longer than max_length)\n            return_tensors: (optional) can be set to \'tf\' or \'pt\' to return respectively TensorFlow tf.constant\n                or PyTorch torch.Tensor instead of a list of python integers.\n            **kwargs: passed to the `self.tokenize()` method\n        """"""\n\n        def get_input_ids(text):\n            if isinstance(text, six.string_types):\n                return self.convert_tokens_to_ids(self.tokenize(text, **kwargs))\n            elif isinstance(text, (list, tuple)) and len(text) > 0 and isinstance(text[0], six.string_types):\n                return self.convert_tokens_to_ids(text)\n            elif isinstance(text, (list, tuple)) and len(text) > 0 and isinstance(text[0], int):\n                return text\n            else:\n                raise ValueError(""Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."")\n\n        first_ids = get_input_ids(text)\n        second_ids = get_input_ids(text_pair) if text_pair is not None else None\n\n        return self.prepare_for_model(first_ids,\n                                      pair_ids=second_ids,\n                                      max_length=max_length,\n                                      add_special_tokens=add_special_tokens,\n                                      stride=stride,\n                                      truncation_strategy=truncation_strategy,\n                                      return_tensors=return_tensors)\n\n    def prepare_for_model(self, ids, pair_ids=None, max_length=None, add_special_tokens=False, stride=0,\n                          truncation_strategy=\'longest_first\', return_tensors=None):\n        """"""\n        Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model.\n        It adds special tokens, truncates\n        sequences if overflowing while taking into account the special tokens and manages a window stride for\n        overflowing tokens\n\n        Args:\n            ids: list of tokenized input ids. Can be obtained from a string by chaining the\n                `tokenize` and `convert_tokens_to_ids` methods.\n            pair_ids: Optional second list of input ids. Can be obtained from a string by chaining the\n                `tokenize` and `convert_tokens_to_ids` methods.\n            max_length: maximum length of the returned list. Will truncate by taking into account the special tokens.\n            add_special_tokens: if set to ``True``, the sequences will be encoded with the special tokens relative\n                to their model.\n            stride: window stride for overflowing tokens. Can be useful for edge effect removal when using sequential\n                list of inputs.\n            truncation_strategy: string selected in the following options:\n                - \'longest_first\' (default) Iteratively reduce the inputs sequence until the input is under max_length\n                    starting from the longest one at each token (when there is a pair of input sequences)\n                - \'only_first\': Only truncate the first sequence\n                - \'only_second\': Only truncate the second sequence\n                - \'do_not_truncate\': Does not truncate (raise an error if the input sequence is longer than max_length)\n            return_tensors: (optional) can be set to \'tf\' or \'pt\' to return respectively TensorFlow tf.constant\n                or PyTorch torch.Tensor instead of a list of python integers.\n\n        Return:\n            A Dictionary of shape::\n\n                {\n                    input_ids: list[int],\n                    overflowing_tokens: list[int] if a ``max_length`` is specified, else None\n                    special_tokens_mask: list[int] if ``add_special_tokens`` if set to ``True``\n                }\n\n            With the fields:\n                ``input_ids``: list of tokens to be fed to a model\n\n                ``overflowing_tokens``: list of overflowing tokens if a max length is specified.\n\n                ``special_tokens_mask``: if adding special tokens, this is a list of [0, 1], with 0 specifying special added\n                tokens and 1 specifying sequence tokens.\n        """"""\n        pair = bool(pair_ids is not None)\n        len_ids = len(ids)\n        len_pair_ids = len(pair_ids) if pair else 0\n\n        encoded_inputs = {}\n        total_len = len_ids + len_pair_ids + (self.num_added_tokens(pair=pair) if add_special_tokens else 0)\n        if max_length and total_len > max_length:\n            ids, pair_ids, overflowing_tokens = self.truncate_sequences(ids, pair_ids=pair_ids,\n                                                                        num_tokens_to_remove=total_len-max_length,\n                                                                        truncation_strategy=truncation_strategy,\n                                                                        stride=stride)\n            encoded_inputs[""overflowing_tokens""] = overflowing_tokens\n            encoded_inputs[""num_truncated_tokens""] = total_len - max_length\n\n        if add_special_tokens:\n            sequence = self.build_inputs_with_special_tokens(ids, pair_ids)\n            token_type_ids = self.create_token_type_ids_from_sequences(ids, pair_ids)\n            encoded_inputs[""special_tokens_mask""] = self.get_special_tokens_mask(ids, pair_ids)\n        else:\n            sequence = ids + pair_ids if pair else ids\n            token_type_ids = [0] * len(ids) + ([1] * len(pair_ids) if pair else [])\n\n        if return_tensors == \'tf\' and is_tf_available():\n            sequence = tf.constant([sequence])\n            token_type_ids = tf.constant([token_type_ids])\n        elif return_tensors == \'pt\' and is_torch_available():\n            sequence = torch.tensor([sequence])\n            token_type_ids = torch.tensor([token_type_ids])\n        elif return_tensors is not None:\n            logger.warning(""Unable to convert output to tensors format {}, PyTorch or TensorFlow is not available."".format(return_tensors))\n\n        encoded_inputs[""input_ids""] = sequence\n        encoded_inputs[""token_type_ids""] = token_type_ids\n\n        if max_length and len(encoded_inputs[""input_ids""]) > max_length:\n            encoded_inputs[""input_ids""] = encoded_inputs[""input_ids""][:max_length]\n            encoded_inputs[""token_type_ids""] = encoded_inputs[""token_type_ids""][:max_length]\n            encoded_inputs[""special_tokens_mask""] = encoded_inputs[""special_tokens_mask""][:max_length]\n\n        return encoded_inputs\n\n    def truncate_sequences(self, ids, pair_ids=None, num_tokens_to_remove=0, truncation_strategy=\'longest_first\', stride=0):\n        """"""Truncates a sequence pair in place to the maximum length.\n            truncation_strategy: string selected in the following options:\n                - \'longest_first\' (default) Iteratively reduce the inputs sequence until the input is under max_length\n                    starting from the longest one at each token (when there is a pair of input sequences).\n                    Overflowing tokens only contains overflow from the first sequence.\n                - \'only_first\': Only truncate the first sequence. raise an error if the first sequence is shorter or equal to than num_tokens_to_remove.\n                - \'only_second\': Only truncate the second sequence\n                - \'do_not_truncate\': Does not truncate (raise an error if the input sequence is longer than max_length)\n        """"""\n        if num_tokens_to_remove <= 0:\n            return ids, pair_ids, []\n\n        if truncation_strategy == \'longest_first\':\n            overflowing_tokens = []\n            for _ in range(num_tokens_to_remove):\n                if pair_ids is None or len(ids) > len(pair_ids):\n                    overflowing_tokens = [ids[-1]] + overflowing_tokens\n                    ids = ids[:-1]\n                else:\n                    pair_ids = pair_ids[:-1]\n            window_len = min(len(ids), stride)\n            if window_len > 0:\n                overflowing_tokens = ids[-window_len:] + overflowing_tokens\n        elif truncation_strategy == \'only_first\':\n            assert len(ids) > num_tokens_to_remove\n            window_len = min(len(ids), stride + num_tokens_to_remove)\n            overflowing_tokens = ids[-window_len:]\n            ids = ids[:-num_tokens_to_remove]\n        elif truncation_strategy == \'only_second\':\n            assert pair_ids is not None and len(pair_ids) > num_tokens_to_remove\n            window_len = min(len(pair_ids), stride + num_tokens_to_remove)\n            overflowing_tokens = pair_ids[-window_len:]\n            pair_ids = pair_ids[:-num_tokens_to_remove]\n        elif truncation_strategy == \'do_not_truncate\':\n            raise ValueError(""Input sequence are too long for max_length. Please select a truncation strategy."")\n        else:\n            raise ValueError(""Truncation_strategy should be selected in [\'longest_first\', \'only_first\', \'only_second\', \'do_not_truncate\']"")\n        return (ids, pair_ids, overflowing_tokens)\n\n    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n        logger.warning(""This tokenizer does not make use of special tokens."")\n        if token_ids_1 is None:\n            return len(token_ids_0) * [0]\n        return [0] * len(token_ids_0) + [1] * len(token_ids_1)\n\n    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n        """"""\n        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n        by concatenating and adding special tokens.\n        A RoBERTa sequence has the following format:\n            single sequence: <s> X </s>\n            pair of sequences: <s> A </s></s> B </s>\n        """"""\n        logger.warning(""This tokenizer does not make use of special tokens. Input is returned with no modification."")\n        if token_ids_1 is None:\n            return token_ids_0\n        return token_ids_0 + token_ids_1\n\n    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n        """"""\n        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n\n        Args:\n            token_ids_0: list of ids (must not contain special tokens)\n            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n                for sequence pairs\n            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n                special tokens for the model\n\n        Returns:\n            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n        """"""\n        return [0] * ((len(token_ids_1) if token_ids_1 else 0) + len(token_ids_0))\n\n    def convert_ids_to_tokens(self, ids, skip_special_tokens=False):\n        """""" Converts a single index or a sequence of indices (integers) in a token ""\n            (resp.) a sequence of tokens (str/unicode), using the vocabulary and added tokens.\n\n            Args:\n                skip_special_tokens: Don\'t decode special tokens (self.all_special_tokens). Default: False\n        """"""\n        if isinstance(ids, int):\n            if ids in self.added_tokens_decoder:\n                return self.added_tokens_decoder[ids]\n            else:\n                return self._convert_id_to_token(ids)\n        tokens = []\n        for index in ids:\n            if skip_special_tokens and index in self.all_special_ids:\n                continue\n            if index in self.added_tokens_decoder:\n                tokens.append(self.added_tokens_decoder[index])\n            else:\n                tokens.append(self._convert_id_to_token(index))\n        return tokens\n\n    def _convert_id_to_token(self, index):\n        raise NotImplementedError\n\n    def convert_tokens_to_string(self, tokens):\n        """""" Converts a sequence of tokens (string) in a single string.\n            The most simple way to do it is \' \'.join(self.convert_ids_to_tokens(token_ids))\n            but we often want to remove sub-word tokenization artifacts at the same time.\n        """"""\n        return \' \'.join(self.convert_ids_to_tokens(tokens))\n\n    def decode(self, token_ids, skip_special_tokens=False, clean_up_tokenization_spaces=True):\n        """"""\n        Converts a sequence of ids (integer) in a string, using the tokenizer and vocabulary\n        with options to remove special tokens and clean up tokenization spaces.\n        Similar to doing ``self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))``.\n\n        Args:\n            token_ids: list of tokenized input ids. Can be obtained using the `encode` or `encode_plus` methods.\n            skip_special_tokens: if set to True, will replace special tokens.\n            clean_up_tokenization_spaces: if set to True, will clean up the tokenization spaces.\n        """"""\n        filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)\n\n        # To avoid mixing byte-level and unicode for byte-level BPT\n        # we need to build string separatly for added tokens and byte-level tokens\n        # cf. https://github.com/huggingface/transformers/issues/1133\n        sub_texts = []\n        current_sub_text = []\n        for token in filtered_tokens:\n            if skip_special_tokens and token in self.all_special_ids:\n                continue\n            if token in self.added_tokens_encoder:\n                if current_sub_text:\n                    sub_texts.append(self.convert_tokens_to_string(current_sub_text))\n                    current_sub_text = []\n                sub_texts.append("" "" + token)\n            else:\n                current_sub_text.append(token)\n        if current_sub_text:\n            sub_texts.append(self.convert_tokens_to_string(current_sub_text))\n        text = \'\'.join(sub_texts)\n\n        if clean_up_tokenization_spaces:\n            clean_text = self.clean_up_tokenization(text)\n            return clean_text\n        else:\n            return text\n\n    @property\n    def special_tokens_map(self):\n        """""" A dictionary mapping special token class attribute (cls_token, unk_token...) to their\n            values (\'<unk>\', \'<cls>\'...)\n        """"""\n        set_attr = {}\n        for attr in self.SPECIAL_TOKENS_ATTRIBUTES:\n            attr_value = getattr(self, ""_"" + attr)\n            if attr_value:\n                set_attr[attr] = attr_value\n        return set_attr\n\n    @property\n    def all_special_tokens(self):\n        """""" List all the special tokens (\'<unk>\', \'<cls>\'...) mapped to class attributes\n            (cls_token, unk_token...).\n        """"""\n        all_toks = []\n        set_attr = self.special_tokens_map\n        for attr_value in set_attr.values():\n            all_toks = all_toks + (list(attr_value) if isinstance(attr_value, (list, tuple)) else [attr_value])\n        all_toks = list(set(all_toks))\n        return all_toks\n\n    @property\n    def all_special_ids(self):\n        """""" List the vocabulary indices of the special tokens (\'<unk>\', \'<cls>\'...) mapped to\n            class attributes (cls_token, unk_token...).\n        """"""\n        all_toks = self.all_special_tokens\n        all_ids = list(self._convert_token_to_id(t) for t in all_toks)\n        return all_ids\n\n    @staticmethod\n    def clean_up_tokenization(out_string):\n        """""" Clean up a list of simple English tokenization artifacts like spaces before punctuations and abreviated forms.\n        """"""\n        out_string = out_string.replace(\' .\', \'.\').replace(\' ?\', \'?\').replace(\' !\', \'!\').replace(\' ,\', \',\'\n                        ).replace("" \' "", ""\'"").replace("" n\'t"", ""n\'t"").replace("" \'m"", ""\'m"").replace("" do not"", "" don\'t""\n                        ).replace("" \'s"", ""\'s"").replace("" \'ve"", ""\'ve"").replace("" \'re"", ""\'re"")\n        return out_string\n'"
models/transformers/tokenization_xlm.py,0,"b'# coding=utf-8\n# Copyright 2019 The Open AI Team Authors and The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tokenization classes for OpenAI GPT.""""""\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport json\nimport logging\nimport os\nimport re\nimport sys\nimport unicodedata\nfrom io import open\n\nimport sacremoses as sm\n\nfrom .tokenization_utils import PreTrainedTokenizer\nfrom .tokenization_bert import BasicTokenizer\n\nlogger = logging.getLogger(__name__)\n\nVOCAB_FILES_NAMES = {\n    \'vocab_file\': \'vocab.json\',\n    \'merges_file\': \'merges.txt\',\n}\n\nPRETRAINED_VOCAB_FILES_MAP = {\n    \'vocab_file\':\n    {\n        \'xlm-mlm-en-2048\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-en-2048-vocab.json"",\n        \'xlm-mlm-ende-1024\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-ende-1024-vocab.json"",\n        \'xlm-mlm-enfr-1024\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-enfr-1024-vocab.json"",\n        \'xlm-mlm-enro-1024\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-enro-1024-vocab.json"",\n        \'xlm-mlm-tlm-xnli15-1024\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-tlm-xnli15-1024-vocab.json"",\n        \'xlm-mlm-xnli15-1024\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-xnli15-1024-vocab.json"",\n        \'xlm-clm-enfr-1024\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-clm-enfr-1024-vocab.json"",\n        \'xlm-clm-ende-1024\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-clm-ende-1024-vocab.json"",\n        \'xlm-mlm-17-1280\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-17-1280-vocab.json"",\n        \'xlm-mlm-100-1280\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-100-1280-vocab.json"",\n    },\n    \'merges_file\':\n    {\n        \'xlm-mlm-en-2048\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-en-2048-merges.txt"",\n        \'xlm-mlm-ende-1024\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-ende-1024-merges.txt"",\n        \'xlm-mlm-enfr-1024\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-enfr-1024-merges.txt"",\n        \'xlm-mlm-enro-1024\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-enro-1024-merges.txt"",\n        \'xlm-mlm-tlm-xnli15-1024\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-tlm-xnli15-1024-merges.txt"",\n        \'xlm-mlm-xnli15-1024\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-xnli15-1024-merges.txt"",\n        \'xlm-clm-enfr-1024\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-enfr-1024-merges.txt"",\n        \'xlm-clm-ende-1024\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-ende-1024-merges.txt"",\n        \'xlm-mlm-17-1280\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-17-1280-merges.txt"",\n        \'xlm-mlm-100-1280\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlm-mlm-100-1280-merges.txt"",\n    },\n}\n\nPRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n    \'xlm-mlm-en-2048\': 512,\n    \'xlm-mlm-ende-1024\': 512,\n    \'xlm-mlm-enfr-1024\': 512,\n    \'xlm-mlm-enro-1024\': 512,\n    \'xlm-mlm-tlm-xnli15-1024\': 512,\n    \'xlm-mlm-xnli15-1024\': 512,\n    \'xlm-clm-enfr-1024\': 512,\n    \'xlm-clm-ende-1024\': 512,\n    \'xlm-mlm-17-1280\': 512,\n    \'xlm-mlm-100-1280\': 512,\n}\n\nPRETRAINED_INIT_CONFIGURATION = {\n    \'xlm-mlm-en-2048\': {""do_lowercase_and_remove_accent"": True},\n    \'xlm-mlm-ende-1024\': { ""do_lowercase_and_remove_accent"": True,\n                            ""id2lang"": { ""0"": ""de"",\n                                        ""1"": ""en""},\n                           ""lang2id"": { ""de"": 0,\n                                        ""en"": 1 }},\n    \'xlm-mlm-enfr-1024\': { ""do_lowercase_and_remove_accent"": True,\n                           ""id2lang"": { ""0"": ""en"",\n                                        ""1"": ""fr""},\n                           ""lang2id"": { ""en"": 0,\n                                        ""fr"": 1 }},\n    \'xlm-mlm-enro-1024\': { ""do_lowercase_and_remove_accent"": True,\n                           ""id2lang"": { ""0"": ""en"",\n                                        ""1"": ""ro""},\n                           ""lang2id"": { ""en"": 0,\n                                        ""ro"": 1 }},\n    \'xlm-mlm-tlm-xnli15-1024\': { ""do_lowercase_and_remove_accent"": True,\n                                 ""id2lang"": {   ""0"": ""ar"",\n                                                ""1"": ""bg"",\n                                                ""2"": ""de"",\n                                                ""3"": ""el"",\n                                                ""4"": ""en"",\n                                                ""5"": ""es"",\n                                                ""6"": ""fr"",\n                                                ""7"": ""hi"",\n                                                ""8"": ""ru"",\n                                                ""9"": ""sw"",\n                                                ""10"": ""th"",\n                                                ""11"": ""tr"",\n                                                ""12"": ""ur"",\n                                                ""13"": ""vi"",\n                                                ""14"": ""zh""},\n                                 ""lang2id"": {   ""ar"": 0,\n                                                ""bg"": 1,\n                                                ""de"": 2,\n                                                ""el"": 3,\n                                                ""en"": 4,\n                                                ""es"": 5,\n                                                ""fr"": 6,\n                                                ""hi"": 7,\n                                                ""ru"": 8,\n                                                ""sw"": 9,\n                                                ""th"": 10,\n                                                ""tr"": 11,\n                                                ""ur"": 12,\n                                                ""vi"": 13,\n                                                ""zh"": 14 }},\n    \'xlm-mlm-xnli15-1024\': { ""do_lowercase_and_remove_accent"": True,\n                             ""id2lang"": {   ""0"": ""ar"",\n                                                ""1"": ""bg"",\n                                                ""2"": ""de"",\n                                                ""3"": ""el"",\n                                                ""4"": ""en"",\n                                                ""5"": ""es"",\n                                                ""6"": ""fr"",\n                                                ""7"": ""hi"",\n                                                ""8"": ""ru"",\n                                                ""9"": ""sw"",\n                                                ""10"": ""th"",\n                                                ""11"": ""tr"",\n                                                ""12"": ""ur"",\n                                                ""13"": ""vi"",\n                                                ""14"": ""zh""},\n                                 ""lang2id"": {   ""ar"": 0,\n                                                ""bg"": 1,\n                                                ""de"": 2,\n                                                ""el"": 3,\n                                                ""en"": 4,\n                                                ""es"": 5,\n                                                ""fr"": 6,\n                                                ""hi"": 7,\n                                                ""ru"": 8,\n                                                ""sw"": 9,\n                                                ""th"": 10,\n                                                ""tr"": 11,\n                                                ""ur"": 12,\n                                                ""vi"": 13,\n                                                ""zh"": 14 }},\n    \'xlm-clm-enfr-1024\': { ""do_lowercase_and_remove_accent"": True,\n                           ""id2lang"": { ""0"": ""en"",\n                                        ""1"": ""fr""},\n                           ""lang2id"": { ""en"": 0,\n                                        ""fr"": 1 }},\n    \'xlm-clm-ende-1024\': { ""do_lowercase_and_remove_accent"": True,\n                           ""id2lang"": { ""0"": ""de"",\n                                        ""1"": ""en""},\n                           ""lang2id"": { ""de"": 0,\n                                        ""en"": 1 }},\n    \'xlm-mlm-17-1280\': {""do_lowercase_and_remove_accent"": False,\n                        ""id2lang"": {\n                            ""0"": ""ar"",\n                            ""1"": ""de"",\n                            ""2"": ""en"",\n                            ""3"": ""es"",\n                            ""4"": ""fr"",\n                            ""5"": ""hi"",\n                            ""6"": ""it"",\n                            ""7"": ""ja"",\n                            ""8"": ""ko"",\n                            ""9"": ""nl"",\n                            ""10"": ""pl"",\n                            ""11"": ""pt"",\n                            ""12"": ""ru"",\n                            ""13"": ""sv"",\n                            ""14"": ""tr"",\n                            ""15"": ""vi"",\n                            ""16"": ""zh""\n                        },\n                        ""lang2id"": {\n                            ""ar"": 0,\n                            ""de"": 1,\n                            ""en"": 2,\n                            ""es"": 3,\n                            ""fr"": 4,\n                            ""hi"": 5,\n                            ""it"": 6,\n                            ""ja"": 7,\n                            ""ko"": 8,\n                            ""nl"": 9,\n                            ""pl"": 10,\n                            ""pt"": 11,\n                            ""ru"": 12,\n                            ""sv"": 13,\n                            ""tr"": 14,\n                            ""vi"": 15,\n                            ""zh"": 16}},\n    \'xlm-mlm-100-1280\': {""do_lowercase_and_remove_accent"": False,\n                        ""id2lang"": {\n                            ""0"": ""af"",\n                            ""1"": ""als"",\n                            ""2"": ""am"",\n                            ""3"": ""an"",\n                            ""4"": ""ang"",\n                            ""5"": ""ar"",\n                            ""6"": ""arz"",\n                            ""7"": ""ast"",\n                            ""8"": ""az"",\n                            ""9"": ""bar"",\n                            ""10"": ""be"",\n                            ""11"": ""bg"",\n                            ""12"": ""bn"",\n                            ""13"": ""br"",\n                            ""14"": ""bs"",\n                            ""15"": ""ca"",\n                            ""16"": ""ceb"",\n                            ""17"": ""ckb"",\n                            ""18"": ""cs"",\n                            ""19"": ""cy"",\n                            ""20"": ""da"",\n                            ""21"": ""de"",\n                            ""22"": ""el"",\n                            ""23"": ""en"",\n                            ""24"": ""eo"",\n                            ""25"": ""es"",\n                            ""26"": ""et"",\n                            ""27"": ""eu"",\n                            ""28"": ""fa"",\n                            ""29"": ""fi"",\n                            ""30"": ""fr"",\n                            ""31"": ""fy"",\n                            ""32"": ""ga"",\n                            ""33"": ""gan"",\n                            ""34"": ""gl"",\n                            ""35"": ""gu"",\n                            ""36"": ""he"",\n                            ""37"": ""hi"",\n                            ""38"": ""hr"",\n                            ""39"": ""hu"",\n                            ""40"": ""hy"",\n                            ""41"": ""ia"",\n                            ""42"": ""id"",\n                            ""43"": ""is"",\n                            ""44"": ""it"",\n                            ""45"": ""ja"",\n                            ""46"": ""jv"",\n                            ""47"": ""ka"",\n                            ""48"": ""kk"",\n                            ""49"": ""kn"",\n                            ""50"": ""ko"",\n                            ""51"": ""ku"",\n                            ""52"": ""la"",\n                            ""53"": ""lb"",\n                            ""54"": ""lt"",\n                            ""55"": ""lv"",\n                            ""56"": ""mk"",\n                            ""57"": ""ml"",\n                            ""58"": ""mn"",\n                            ""59"": ""mr"",\n                            ""60"": ""ms"",\n                            ""61"": ""my"",\n                            ""62"": ""nds"",\n                            ""63"": ""ne"",\n                            ""64"": ""nl"",\n                            ""65"": ""nn"",\n                            ""66"": ""no"",\n                            ""67"": ""oc"",\n                            ""68"": ""pl"",\n                            ""69"": ""pt"",\n                            ""70"": ""ro"",\n                            ""71"": ""ru"",\n                            ""72"": ""scn"",\n                            ""73"": ""sco"",\n                            ""74"": ""sh"",\n                            ""75"": ""si"",\n                            ""76"": ""simple"",\n                            ""77"": ""sk"",\n                            ""78"": ""sl"",\n                            ""79"": ""sq"",\n                            ""80"": ""sr"",\n                            ""81"": ""sv"",\n                            ""82"": ""sw"",\n                            ""83"": ""ta"",\n                            ""84"": ""te"",\n                            ""85"": ""th"",\n                            ""86"": ""tl"",\n                            ""87"": ""tr"",\n                            ""88"": ""tt"",\n                            ""89"": ""uk"",\n                            ""90"": ""ur"",\n                            ""91"": ""uz"",\n                            ""92"": ""vi"",\n                            ""93"": ""war"",\n                            ""94"": ""wuu"",\n                            ""95"": ""yi"",\n                            ""96"": ""zh"",\n                            ""97"": ""zh_classical"",\n                            ""98"": ""zh_min_nan"",\n                            ""99"": ""zh_yue""\n                        },\n                        ""lang2id"": {\n                            ""af"": 0,\n                            ""als"": 1,\n                            ""am"": 2,\n                            ""an"": 3,\n                            ""ang"": 4,\n                            ""ar"": 5,\n                            ""arz"": 6,\n                            ""ast"": 7,\n                            ""az"": 8,\n                            ""bar"": 9,\n                            ""be"": 10,\n                            ""bg"": 11,\n                            ""bn"": 12,\n                            ""br"": 13,\n                            ""bs"": 14,\n                            ""ca"": 15,\n                            ""ceb"": 16,\n                            ""ckb"": 17,\n                            ""cs"": 18,\n                            ""cy"": 19,\n                            ""da"": 20,\n                            ""de"": 21,\n                            ""el"": 22,\n                            ""en"": 23,\n                            ""eo"": 24,\n                            ""es"": 25,\n                            ""et"": 26,\n                            ""eu"": 27,\n                            ""fa"": 28,\n                            ""fi"": 29,\n                            ""fr"": 30,\n                            ""fy"": 31,\n                            ""ga"": 32,\n                            ""gan"": 33,\n                            ""gl"": 34,\n                            ""gu"": 35,\n                            ""he"": 36,\n                            ""hi"": 37,\n                            ""hr"": 38,\n                            ""hu"": 39,\n                            ""hy"": 40,\n                            ""ia"": 41,\n                            ""id"": 42,\n                            ""is"": 43,\n                            ""it"": 44,\n                            ""ja"": 45,\n                            ""jv"": 46,\n                            ""ka"": 47,\n                            ""kk"": 48,\n                            ""kn"": 49,\n                            ""ko"": 50,\n                            ""ku"": 51,\n                            ""la"": 52,\n                            ""lb"": 53,\n                            ""lt"": 54,\n                            ""lv"": 55,\n                            ""mk"": 56,\n                            ""ml"": 57,\n                            ""mn"": 58,\n                            ""mr"": 59,\n                            ""ms"": 60,\n                            ""my"": 61,\n                            ""nds"": 62,\n                            ""ne"": 63,\n                            ""nl"": 64,\n                            ""nn"": 65,\n                            ""no"": 66,\n                            ""oc"": 67,\n                            ""pl"": 68,\n                            ""pt"": 69,\n                            ""ro"": 70,\n                            ""ru"": 71,\n                            ""scn"": 72,\n                            ""sco"": 73,\n                            ""sh"": 74,\n                            ""si"": 75,\n                            ""simple"": 76,\n                            ""sk"": 77,\n                            ""sl"": 78,\n                            ""sq"": 79,\n                            ""sr"": 80,\n                            ""sv"": 81,\n                            ""sw"": 82,\n                            ""ta"": 83,\n                            ""te"": 84,\n                            ""th"": 85,\n                            ""tl"": 86,\n                            ""tr"": 87,\n                            ""tt"": 88,\n                            ""uk"": 89,\n                            ""ur"": 90,\n                            ""uz"": 91,\n                            ""vi"": 92,\n                            ""war"": 93,\n                            ""wuu"": 94,\n                            ""yi"": 95,\n                            ""zh"": 96,\n                            ""zh_classical"": 97,\n                            ""zh_min_nan"": 98,\n                            ""zh_yue"": 99\n                        }},\n}\n\ndef get_pairs(word):\n    """"""\n    Return set of symbol pairs in a word.\n    word is represented as tuple of symbols (symbols being variable-length strings)\n    """"""\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\n\ndef lowercase_and_remove_accent(text):\n    """"""\n    Lowercase and strips accents from a piece of text based on\n    https://github.com/facebookresearch/XLM/blob/master/tools/lowercase_and_remove_accent.py\n    """"""\n    text = \' \'.join(text)\n    text = text.lower()\n    text = unicodedata.normalize(""NFD"", text)\n    output = []\n    for char in text:\n        cat = unicodedata.category(char)\n        if cat == ""Mn"":\n            continue\n        output.append(char)\n    return """".join(output).lower().split(\' \')\n\n\ndef replace_unicode_punct(text):\n    \'\'\'\n    Port of https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/replace-unicode-punctuation.perl\n    \'\'\'\n    text = text.replace(\'\xef\xbc\x8c\', \',\')\n    text = re.sub(r\'\xe3\x80\x82\\s*\', \'. \', text)\n    text = text.replace(\'\xe3\x80\x81\', \',\')\n    text = text.replace(\'\xe2\x80\x9d\', \'""\')\n    text = text.replace(\'\xe2\x80\x9c\', \'""\')\n    text = text.replace(\'\xe2\x88\xb6\', \':\')\n    text = text.replace(\'\xef\xbc\x9a\', \':\')\n    text = text.replace(\'\xef\xbc\x9f\', \'?\')\n    text = text.replace(\'\xe3\x80\x8a\', \'""\')\n    text = text.replace(\'\xe3\x80\x8b\', \'""\')\n    text = text.replace(\'\xef\xbc\x89\', \')\')\n    text = text.replace(\'\xef\xbc\x81\', \'!\')\n    text = text.replace(\'\xef\xbc\x88\', \'(\')\n    text = text.replace(\'\xef\xbc\x9b\', \';\')\n    text = text.replace(\'\xef\xbc\x91\', \'""\')\n    text = text.replace(\'\xe3\x80\x8d\', \'""\')\n    text = text.replace(\'\xe3\x80\x8c\', \'""\')\n    text = text.replace(\'\xef\xbc\x90\', \'0\')\n    text = text.replace(\'\xef\xbc\x93\', \'3\')\n    text = text.replace(\'\xef\xbc\x92\', \'2\')\n    text = text.replace(\'\xef\xbc\x95\', \'5\')\n    text = text.replace(\'\xef\xbc\x96\', \'6\')\n    text = text.replace(\'\xef\xbc\x99\', \'9\')\n    text = text.replace(\'\xef\xbc\x97\', \'7\')\n    text = text.replace(\'\xef\xbc\x98\', \'8\')\n    text = text.replace(\'\xef\xbc\x94\', \'4\')\n    text = re.sub(r\'\xef\xbc\x8e\\s*\', \'. \', text)\n    text = text.replace(\'\xef\xbd\x9e\', \'~\')\n    text = text.replace(\'\xe2\x80\x99\', \'\\\'\')\n    text = text.replace(\'\xe2\x80\xa6\', \'...\')\n    text = text.replace(\'\xe2\x94\x81\', \'-\')\n    text = text.replace(\'\xe3\x80\x88\', \'<\')\n    text = text.replace(\'\xe3\x80\x89\', \'>\')\n    text = text.replace(\'\xe3\x80\x90\', \'[\')\n    text = text.replace(\'\xe3\x80\x91\', \']\')\n    text = text.replace(\'\xef\xbc\x85\', \'%\')\n    return text\n\n\ndef remove_non_printing_char(text):\n    \'\'\'\n    Port of https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/remove-non-printing-char.perl\n    \'\'\'\n    output = []\n    for char in text:\n        cat = unicodedata.category(char)\n        if cat.startswith(\'C\'):\n            continue\n        output.append(char)\n    return """".join(output)\n\n\ndef romanian_preprocessing(text):\n    \'\'\'Sennrich\'s WMT16 scripts for Romanian preprocessing, used by model `xlm-mlm-enro-1024`\'\'\'\n    # https://github.com/rsennrich/wmt16-scripts/blob/master/preprocess/normalise-romanian.py\n    text = text.replace(""\\u015e"", ""\\u0218"").replace(""\\u015f"", ""\\u0219"")\n    text = text.replace(""\\u0162"", ""\\u021a"").replace(""\\u0163"", ""\\u021b"")\n    # https://github.com/rsennrich/wmt16-scripts/blob/master/preprocess/remove-diacritics.py\n    text = text.replace(""\\u0218"", ""S"").replace(""\\u0219"", ""s"") #s-comma\n    text = text.replace(""\\u021a"", ""T"").replace(""\\u021b"", ""t"") #t-comma\n    text = text.replace(""\\u0102"", ""A"").replace(""\\u0103"", ""a"")\n    text = text.replace(""\\u00C2"", ""A"").replace(""\\u00E2"", ""a"")\n    text = text.replace(""\\u00CE"", ""I"").replace(""\\u00EE"", ""i"")\n    return text\n\n\nclass XLMTokenizer(PreTrainedTokenizer):\n    """"""\n    BPE tokenizer for XLM\n\n        - Moses preprocessing & tokenization for most supported languages\n\n        - Language specific tokenization for Chinese (Jieba), Japanese (KyTea) and Thai (PyThaiNLP)\n\n        - (optionally) lower case & normalize all inputs text\n\n        - argument ``special_tokens`` and function ``set_special_tokens``, can be used to add additional symbols \\\n        (ex: ""__classify__"") to a vocabulary\n        \n        - `lang2id` attribute maps the languages supported by the model with their ids if provided (automatically set for pretrained vocabularies)\n\n        - `id2lang` attributes does reverse mapping if provided (automatically set for pretrained vocabularies)\n\n        - `do_lowercase_and_remove_accent` controle lower casing and accent (automatically set for pretrained vocabularies)\n    """"""\n    vocab_files_names = VOCAB_FILES_NAMES\n    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n\n    def __init__(self, vocab_file, merges_file, unk_token=""<unk>"", bos_token=""<s>"",\n                 sep_token=""</s>"", pad_token=""<pad>"", cls_token=""</s>"",\n                 mask_token=""<special1>"", additional_special_tokens=[""<special0>"",\n                 ""<special1>"", ""<special2>"", ""<special3>"", ""<special4>"", ""<special5>"",\n                 ""<special6>"", ""<special7>"", ""<special8>"", ""<special9>""],\n                 lang2id=None, id2lang=None, do_lowercase_and_remove_accent=True,\n                 **kwargs):\n        super(XLMTokenizer, self).__init__(unk_token=unk_token, bos_token=bos_token,\n                                           sep_token=sep_token, pad_token=pad_token,\n                                           cls_token=cls_token, mask_token=mask_token,\n                                           additional_special_tokens=additional_special_tokens,\n                                           **kwargs)\n\n        # cache of sm.MosesPunctNormalizer instance\n        self.cache_moses_punct_normalizer = dict()\n        # cache of sm.MosesTokenizer instance\n        self.cache_moses_tokenizer = dict()\n        self.lang_with_custom_tokenizer = set([\'zh\', \'th\', \'ja\'])\n        # True for current supported model (v1.2.0), False for XLM-17 & 100\n        self.do_lowercase_and_remove_accent = do_lowercase_and_remove_accent\n        self.lang2id = lang2id\n        self.id2lang = id2lang\n        if lang2id is not None and id2lang is not None:\n            assert len(lang2id) == len(id2lang)\n\n        self.ja_word_tokenizer = None\n        self.zh_word_tokenizer = None\n\n        self.encoder = json.load(open(vocab_file, encoding=""utf-8""))\n        self.decoder = {v:k for k,v in self.encoder.items()}\n        merges = open(merges_file, encoding=\'utf-8\').read().split(\'\\n\')[:-1]\n        merges = [tuple(merge.split()[:2]) for merge in merges]\n        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n        self.cache = {}\n\n    def moses_punct_norm(self, text, lang):\n        if lang not in self.cache_moses_punct_normalizer:\n            punct_normalizer = sm.MosesPunctNormalizer(lang=lang)\n            self.cache_moses_punct_normalizer[lang] = punct_normalizer\n        else:\n            punct_normalizer = self.cache_moses_punct_normalizer[lang]\n        return punct_normalizer.normalize(text)\n\n    def moses_tokenize(self, text, lang):\n        if lang not in self.cache_moses_tokenizer:\n            moses_tokenizer = sm.MosesTokenizer(lang=lang)\n            self.cache_moses_tokenizer[lang] = moses_tokenizer\n        else:\n            moses_tokenizer = self.cache_moses_tokenizer[lang]\n        return moses_tokenizer.tokenize(text, return_str=False, escape=False)\n\n    def moses_pipeline(self, text, lang):\n        text = replace_unicode_punct(text)\n        text = self.moses_punct_norm(text, lang)\n        text = remove_non_printing_char(text)\n        return text\n\n    def ja_tokenize(self, text):\n        if self.ja_word_tokenizer is None:\n            try:\n                import Mykytea\n                self.ja_word_tokenizer = Mykytea.Mykytea(\'-model %s/local/share/kytea/model.bin\' % os.path.expanduser(\'~\'))\n            except (AttributeError, ImportError) as e:\n                logger.error(""Make sure you install KyTea (https://github.com/neubig/kytea) and it\'s python wrapper (https://github.com/chezou/Mykytea-python) with the following steps"")\n                logger.error(""1. git clone git@github.com:neubig/kytea.git && cd kytea"")\n                logger.error(""2. autoreconf -i"")\n                logger.error(""3. ./configure --prefix=$HOME/local"")\n                logger.error(""4. make && make install"")\n                logger.error(""5. pip install kytea"")\n                raise e\n        return list(self.ja_word_tokenizer.getWS(text))\n\n    @property\n    def vocab_size(self):\n        return len(self.encoder)\n\n    def bpe(self, token):\n        word = tuple(token[:-1]) + (token[-1] + \'</w>\',)\n        if token in self.cache:\n            return self.cache[token]\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token+\'</w>\'\n\n        while True:\n            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\'inf\')))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n                    new_word.append(first+second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = \' \'.join(word)\n        if word == \'\\n  </w>\':\n            word = \'\\n</w>\'\n        self.cache[token] = word\n        return word\n\n    def _tokenize(self, text, lang=\'en\', bypass_tokenizer=False):\n        """"""\n        Tokenize a string given language code. For Chinese, Japanese and Thai, we use a language specific tokenizerself. Otherwise, we use Moses.\n\n        Details of tokenization:\n        - [sacremoses](https://github.com/alvations/sacremoses): port of Moses\n            - Install with `pip install sacremoses`\n        - [pythainlp](https://github.com/PyThaiNLP/pythainlp): Thai tokenizer\n            - Install with `pip install pythainlp`\n        - [kytea](https://github.com/chezou/Mykytea-python): Japanese tokenizer, wrapper of [KyTea](https://github.com/neubig/kytea)\n            - Install with the following steps:\n            ```\n            git clone git@github.com:neubig/kytea.git && cd kytea\n            autoreconf -i\n            ./configure --prefix=$HOME/local\n            make && make install\n            pip install kytea\n            ```\n        - [jieba](https://github.com/fxsjy/jieba): Chinese tokenizer *\n            - Install with `pip install jieba`\n\n        \\* The original XLM used [Stanford Segmenter](https://nlp.stanford.edu/software/stanford-segmenter-2018-10-16.zip).\n        However, the wrapper (`nltk.tokenize.stanford_segmenter`) is slow due to JVM overhead, and it will be deprecated.\n        Jieba is a lot faster and pip-installable. Note there is some mismatch with the Stanford Segmenter. It should be fine\n        if you fine-tune the model with Chinese supervisionself. If you want the same exact behaviour, use the original XLM\n        [preprocessing script](https://github.com/facebookresearch/XLM/tree/master/tools) to tokenize the sentence externally,\n        and set `bypass_tokenizer=True` to bypass the tokenizer.\n\n        Args:\n            - lang: ISO language code (default = \'en\') (string). Languages should belong of the model supported languages. However, we don\'t enforce it.\n            - bypass_tokenizer: Allow users to preprocess and tokenize the sentences externally (default = False)  (bool). If True, we only apply BPE.\n\n        Returns:\n            List of tokens.\n        """"""\n        if lang and self.lang2id and lang not in self.lang2id:\n            logger.error(""Supplied language code not found in lang2id mapping. Please check that your language is supported by the loaded pretrained model."")\n        if bypass_tokenizer:\n            text = text.split()\n        elif lang not in self.lang_with_custom_tokenizer:\n            text = self.moses_pipeline(text, lang=lang)\n            # TODO: make sure we are using `xlm-mlm-enro-1024`, since XLM-100 doesn\'t have this step\n            if lang == \'ro\':\n                text = romanian_preprocessing(text)\n            text = self.moses_tokenize(text, lang=lang)\n        elif lang == \'th\':\n            text = self.moses_pipeline(text, lang=lang)\n            try:\n                if \'pythainlp\' not in sys.modules:\n                    from pythainlp.tokenize import word_tokenize as th_word_tokenize\n                else:\n                    th_word_tokenize = sys.modules[\'pythainlp\'].word_tokenize\n            except (AttributeError, ImportError) as e:\n                logger.error(""Make sure you install PyThaiNLP (https://github.com/PyThaiNLP/pythainlp) with the following steps"")\n                logger.error(""1. pip install pythainlp"")\n                raise e\n            text = th_word_tokenize(text)\n        elif lang == \'zh\':\n            try:\n                if \'jieba\' not in sys.modules:\n                    import jieba\n                else:\n                    jieba = sys.modules[\'jieba\']\n            except (AttributeError, ImportError) as e:\n                logger.error(""Make sure you install Jieba (https://github.com/fxsjy/jieba) with the following steps"")\n                logger.error(""1. pip install jieba"")\n                raise e\n            text = \' \'.join(jieba.cut(text))\n            text = self.moses_pipeline(text, lang=lang)\n            text = text.split()\n        elif lang == \'ja\':\n            text = self.moses_pipeline(text, lang=lang)\n            text = self.ja_tokenize(text)\n        else:\n            raise ValueError(\'It should not reach here\')\n\n        if self.do_lowercase_and_remove_accent and not bypass_tokenizer:\n            text = lowercase_and_remove_accent(text)\n\n        split_tokens = []\n        for token in text:\n            if token:\n                split_tokens.extend([t for t in self.bpe(token).split(\' \')])\n\n        return split_tokens\n\n    def _convert_token_to_id(self, token):\n        """""" Converts a token (str/unicode) in an id using the vocab. """"""\n        return self.encoder.get(token, self.encoder.get(self.unk_token))\n\n    def _convert_id_to_token(self, index):\n        """"""Converts an index (integer) in a token (string/unicode) using the vocab.""""""\n        return self.decoder.get(index, self.unk_token)\n\n    def convert_tokens_to_string(self, tokens):\n        """""" Converts a sequence of tokens (string) in a single string. """"""\n        out_string = \'\'.join(tokens).replace(\'</w>\', \' \').strip()\n        return out_string\n\n    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n        """"""\n        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n        by concatenating and adding special tokens.\n        A RoBERTa sequence has the following format:\n            single sequence: <s> X </s>\n            pair of sequences: <s> A </s></s> B </s>\n        """"""\n        if token_ids_1 is None:\n            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n        sep = [self.sep_token_id]\n        cls = [self.cls_token_id]\n        return cls + token_ids_0 + sep + token_ids_1 + sep\n\n    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n        """"""\n        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n\n        Args:\n            token_ids_0: list of ids (must not contain special tokens)\n            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n                for sequence pairs\n            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n                special tokens for the model\n\n        Returns:\n            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n        """"""\n\n        if already_has_special_tokens:\n            if token_ids_1 is not None:\n                raise ValueError(""You should not supply a second sequence if the provided sequence of ""\n                                 ""ids is already formated with special tokens for the model."")\n            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n\n        if token_ids_1 is not None:\n            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n        return [1] + ([0] * len(token_ids_0)) + [1]\n\n    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n        """"""\n        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n        An XLM sequence pair mask has the following format:\n        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n        | first sequence    | second sequence\n\n        if token_ids_1 is None, only returns the first portion of the mask (0\'s).\n        """"""\n        sep = [self.sep_token_id]\n        cls = [self.cls_token_id]\n        if token_ids_1 is None:\n            return len(cls + token_ids_0 + sep) * [0]\n        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n\n    def save_vocabulary(self, save_directory):\n        """"""Save the tokenizer vocabulary and merge files to a directory.""""""\n        if not os.path.isdir(save_directory):\n            logger.error(""Vocabulary path ({}) should be a directory"".format(save_directory))\n            return\n        vocab_file = os.path.join(save_directory, VOCAB_FILES_NAMES[\'vocab_file\'])\n        merge_file = os.path.join(save_directory, VOCAB_FILES_NAMES[\'merges_file\'])\n\n        with open(vocab_file, \'w\', encoding=\'utf-8\') as f:\n            f.write(json.dumps(self.encoder, ensure_ascii=False))\n\n        index = 0\n        with open(merge_file, ""w"", encoding=""utf-8"") as writer:\n            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):\n                if index != token_index:\n                    logger.warning(""Saving vocabulary to {}: BPE merge indices are not consecutive.""\n                                   "" Please check that the tokenizer is not corrupted!"".format(merge_file))\n                    index = token_index\n                writer.write(\' \'.join(bpe_tokens) + u\'\\n\')\n                index += 1\n\n        return vocab_file, merge_file\n'"
models/transformers/tokenization_xlnet.py,0,"b'# coding=utf-8\n# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" Tokenization classes for XLNet model.""""""\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport logging\nimport os\nfrom shutil import copyfile\n\nimport unicodedata\nimport six\n\nfrom .tokenization_utils import PreTrainedTokenizer\n\nlogger = logging.getLogger(__name__)\n\nVOCAB_FILES_NAMES = {\'vocab_file\': \'spiece.model\'}\n\nPRETRAINED_VOCAB_FILES_MAP = {\n    \'vocab_file\':\n    {\n    \'xlnet-base-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-spiece.model"",\n    \'xlnet-large-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-large-cased-spiece.model"",\n    }\n}\n\nPRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n    \'xlnet-base-cased\': None,\n    \'xlnet-large-cased\': None,\n}\n\nSPIECE_UNDERLINE = u\'\xe2\x96\x81\'\n\n# Segments (not really needed)\nSEG_ID_A   = 0\nSEG_ID_B   = 1\nSEG_ID_CLS = 2\nSEG_ID_SEP = 3\nSEG_ID_PAD = 4\n\nclass XLNetTokenizer(PreTrainedTokenizer):\n    """"""\n        SentencePiece based tokenizer. Peculiarities:\n\n            - requires `SentencePiece <https://github.com/google/sentencepiece>`_\n    """"""\n    vocab_files_names = VOCAB_FILES_NAMES\n    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n\n    def __init__(self, vocab_file,\n                 do_lower_case=False, remove_space=True, keep_accents=False,\n                 bos_token=""<s>"", eos_token=""</s>"", unk_token=""<unk>"", sep_token=""<sep>"",\n                 pad_token=""<pad>"", cls_token=""<cls>"", mask_token=""<mask>"",\n                 additional_special_tokens=[""<eop>"", ""<eod>""], **kwargs):\n        super(XLNetTokenizer, self).__init__(bos_token=bos_token, eos_token=eos_token,\n                                             unk_token=unk_token, sep_token=sep_token,\n                                             pad_token=pad_token, cls_token=cls_token,\n                                             mask_token=mask_token, additional_special_tokens=\n                                             additional_special_tokens, **kwargs)\n\n        self.max_len_single_sentence = self.max_len - 2  # take into account special tokens\n        self.max_len_sentences_pair = self.max_len - 3  # take into account special tokens\n\n        try:\n            import sentencepiece as spm\n        except ImportError:\n            logger.warning(""You need to install SentencePiece to use XLNetTokenizer: https://github.com/google/sentencepiece""\n                           ""pip install sentencepiece"")\n\n        self.do_lower_case = do_lower_case\n        self.remove_space = remove_space\n        self.keep_accents = keep_accents\n        self.vocab_file = vocab_file\n\n        self.sp_model = spm.SentencePieceProcessor()\n        self.sp_model.Load(vocab_file)\n\n    @property\n    def vocab_size(self):\n        return len(self.sp_model)\n\n    def __getstate__(self):\n        state = self.__dict__.copy()\n        state[""sp_model""] = None\n        return state\n\n    def __setstate__(self, d):\n        self.__dict__ = d\n        try:\n            import sentencepiece as spm\n        except ImportError:\n            logger.warning(""You need to install SentencePiece to use XLNetTokenizer: https://github.com/google/sentencepiece""\n                           ""pip install sentencepiece"")\n        self.sp_model = spm.SentencePieceProcessor()\n        self.sp_model.Load(self.vocab_file)\n\n    def preprocess_text(self, inputs):\n        if self.remove_space:\n            outputs = \' \'.join(inputs.strip().split())\n        else:\n            outputs = inputs\n        outputs = outputs.replace(""``"", \'""\').replace(""\'\'"", \'""\')\n\n        if six.PY2 and isinstance(outputs, str):\n            outputs = outputs.decode(\'utf-8\')\n\n        if not self.keep_accents:\n            outputs = unicodedata.normalize(\'NFKD\', outputs)\n            outputs = \'\'.join([c for c in outputs if not unicodedata.combining(c)])\n        if self.do_lower_case:\n            outputs = outputs.lower()\n\n        return outputs\n\n    def _tokenize(self, text, return_unicode=True, sample=False):\n        """""" Tokenize a string.\n            return_unicode is used only for py2\n        """"""\n        text = self.preprocess_text(text)\n        # note(zhiliny): in some systems, sentencepiece only accepts str for py2\n        if six.PY2 and isinstance(text, unicode):\n            text = text.encode(\'utf-8\')\n\n        if not sample:\n            pieces = self.sp_model.EncodeAsPieces(text)\n        else:\n            pieces = self.sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n        new_pieces = []\n        for piece in pieces:\n            if len(piece) > 1 and piece[-1] == \',\' and piece[-2].isdigit():\n                cur_pieces = self.sp_model.EncodeAsPieces(\n                    piece[:-1].replace(SPIECE_UNDERLINE, \'\'))\n                if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n                    if len(cur_pieces[0]) == 1:\n                        cur_pieces = cur_pieces[1:]\n                    else:\n                        cur_pieces[0] = cur_pieces[0][1:]\n                cur_pieces.append(piece[-1])\n                new_pieces.extend(cur_pieces)\n            else:\n                new_pieces.append(piece)\n\n        # note(zhiliny): convert back to unicode for py2\n        if six.PY2 and return_unicode:\n            ret_pieces = []\n            for piece in new_pieces:\n                if isinstance(piece, str):\n                    piece = piece.decode(\'utf-8\')\n                ret_pieces.append(piece)\n            new_pieces = ret_pieces\n\n        return new_pieces\n\n    def _convert_token_to_id(self, token):\n        """""" Converts a token (str/unicode) in an id using the vocab. """"""\n        return self.sp_model.PieceToId(token)\n\n    def _convert_id_to_token(self, index, return_unicode=True):\n        """"""Converts an index (integer) in a token (string/unicode) using the vocab.""""""\n        token = self.sp_model.IdToPiece(index)\n        if six.PY2 and return_unicode and isinstance(token, str):\n            token = token.decode(\'utf-8\')\n        return token\n\n    def convert_tokens_to_string(self, tokens):\n        """"""Converts a sequence of tokens (strings for sub-words) in a single string.""""""\n        out_string = \'\'.join(tokens).replace(SPIECE_UNDERLINE, \' \').strip()\n        return out_string\n\n    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n        """"""\n        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n        by concatenating and adding special tokens.\n        A RoBERTa sequence has the following format:\n            single sequence: <s> X </s>\n            pair of sequences: <s> A </s></s> B </s>\n        """"""\n        sep = [self.sep_token_id]\n        cls = [self.cls_token_id]\n        if token_ids_1 is None:\n            return token_ids_0 + sep + cls\n        return token_ids_0 + sep + token_ids_1 + sep + cls\n\n    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n        """"""\n        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n\n        Args:\n            token_ids_0: list of ids (must not contain special tokens)\n            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n                for sequence pairs\n            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n                special tokens for the model\n\n        Returns:\n            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n        """"""\n\n        if already_has_special_tokens:\n            if token_ids_1 is not None:\n                raise ValueError(""You should not supply a second sequence if the provided sequence of ""\n                                 ""ids is already formated with special tokens for the model."")\n            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n\n        if token_ids_1 is not None:\n            return ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1, 1]\n        return ([0] * len(token_ids_0)) + [1, 1]\n\n    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n        """"""\n        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n        A BERT sequence pair mask has the following format:\n        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 2\n        | first sequence    | second sequence     | CLS segment ID\n        \n        if token_ids_1 is None, only returns the first portion of the mask (0\'s).\n        """"""\n        sep = [self.sep_token_id]\n        cls = [self.cls_token_id]\n        cls_segment_id = [2]\n\n        if token_ids_1 is None:\n            return len(token_ids_0 + sep + cls) * [0]\n        return len(token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1] + cls_segment_id\n\n    def save_vocabulary(self, save_directory):\n        """""" Save the sentencepiece vocabulary (copy original file) and special tokens file\n            to a directory.\n        """"""\n        if not os.path.isdir(save_directory):\n            logger.error(""Vocabulary path ({}) should be a directory"".format(save_directory))\n            return\n        out_vocab_file = os.path.join(save_directory, VOCAB_FILES_NAMES[\'vocab_file\'])\n\n        if os.path.abspath(self.vocab_file) != os.path.abspath(out_vocab_file):\n            copyfile(self.vocab_file, out_vocab_file)\n\n        return (out_vocab_file,)\n'"
prev_trained_model/bert-base/__init__.py,0,b'\n\n'
outputs/cner_output/bert/__init__.py,0,b'\n\n'
