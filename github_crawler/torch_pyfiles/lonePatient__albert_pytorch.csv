file_path,api_count,code
__init__.py,0,b'#encoding:utf-8'
convert_albert_tf_checkpoint_to_pytorch.py,3,"b'# coding=utf-8\n# Copyright 2018 The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Convert BERT checkpoint.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport torch\nfrom model.modeling_albert import AlbertConfig, AlbertForPreTraining, load_tf_weights_in_albert\n# from model.modeling_albert_bright import AlbertConfig, AlbertForPreTraining, load_tf_weights_in_albert # chinese version\nimport logging\nlogging.basicConfig(level=logging.INFO)\n\ndef convert_tf_checkpoint_to_pytorch(tf_checkpoint_path, bert_config_file, pytorch_dump_path):\n    # Initialise PyTorch model\n    config = AlbertConfig.from_pretrained(bert_config_file)\n    # print(""Building PyTorch model from configuration: {}"".format(str(config)))\n    model = AlbertForPreTraining(config)\n    # Load weights from tf checkpoint\n    load_tf_weights_in_albert(model, config, tf_checkpoint_path)\n\n    # Save pytorch-model\n    print(""Save PyTorch model to {}"".format(pytorch_dump_path))\n    torch.save(model.state_dict(), pytorch_dump_path)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    ## Required parameters\n    parser.add_argument(""--tf_checkpoint_path"",\n                        default = None,\n                        type = str,\n                        required = True,\n                        help = ""Path to the TensorFlow checkpoint path."")\n    parser.add_argument(""--bert_config_file"",\n                        default = None,\n                        type = str,\n                        required = True,\n                        help = ""The config json file corresponding to the pre-trained BERT model. \\n""\n                            ""This specifies the model architecture."")\n    parser.add_argument(""--pytorch_dump_path"",\n                        default = None,\n                        type = str,\n                        required = True,\n                        help = ""Path to the output PyTorch model."")\n    args = parser.parse_args()\n    convert_tf_checkpoint_to_pytorch(args.tf_checkpoint_path,args.bert_config_file,\n                                     args.pytorch_dump_path)\n\n\'\'\'\npython convert_albert_tf_checkpoint_to_pytorch.py \\\n    --tf_checkpoint_path=./prev_trained_model/albert_large_zh \\\n    --bert_config_file=./prev_trained_model/albert_large_zh/config.json \\\n    --pytorch_dump_path=./prev_trained_model/albert_large_zh/pytorch_model.bin\n    \n\nfrom model.modeling_albert_bright import AlbertConfig, AlbertForPreTraining, load_tf_weights_in_albert\npython convert_albert_tf_checkpoint_to_pytorch.py \\\n    --tf_checkpoint_path=./prev_trained_model/albert_base_bright \\\n    --bert_config_file=./prev_trained_model/albert_base_bright/config.json \\\n    --pytorch_dump_path=./prev_trained_model/albert_base_bright/pytorch_model.bin\n\'\'\'\n'"
prepare_lm_data_mask.py,0,"b'import os\r\nimport json\r\nimport random\r\nimport collections\r\nfrom tools.common import logger, init_logger\r\nfrom argparse import ArgumentParser\r\nfrom tools.common import seed_everything\r\nfrom model.tokenization_bert import BertTokenizer\r\nfrom callback.progressbar import ProgressBar\r\nfrom pathlib import Path\r\n\r\nMaskedLmInstance = collections.namedtuple(""MaskedLmInstance"", [""index"", ""label""])\r\n\r\ndef truncate_seq_pair(tokens_a, tokens_b, max_num_tokens):\r\n    """"""Truncates a pair of sequences to a maximum sequence length.""""""\r\n    while True:\r\n        total_length = len(tokens_a) + len(tokens_b)\r\n        if total_length <= max_num_tokens:\r\n            break\r\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\r\n        assert len(trunc_tokens) >= 1\r\n        # We want to sometimes truncate from the front and sometimes from the\r\n        # back to add more randomness and avoid biases.\r\n        if random.random() < 0.5:\r\n            del trunc_tokens[0]\r\n        else:\r\n            trunc_tokens.pop()\r\n\r\ndef create_instances_from_document(all_documents, document_index, max_seq_length, short_seq_prob,\r\n                                   masked_lm_prob, max_predictions_per_seq, vocab_words):\r\n    """"""Creates `TrainingInstance`s for a single document.\r\n     This method is changed to create sentence-order prediction (SOP) followed by idea from paper of ALBERT, 2019-08-28, brightmart\r\n    """"""\r\n    document = all_documents[document_index]  # \xe5\xbe\x97\xe5\x88\xb0\xe4\xb8\x80\xe4\xb8\xaa\xe6\x96\x87\xe6\xa1\xa3\r\n\r\n    # Account for [CLS], [SEP], [SEP]\r\n    max_num_tokens = max_seq_length - 3\r\n\r\n    # We *usually* want to fill up the entire sequence since we are padding\r\n    # to `max_seq_length` anyways, so short sequences are generally wasted\r\n    # computation. However, we *sometimes*\r\n    # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\r\n    # sequences to minimize the mismatch between pre-training and fine-tuning.\r\n    # The `target_seq_length` is just a rough target however, whereas\r\n    # `max_seq_length` is a hard limit.\r\n    target_seq_length = max_num_tokens\r\n    if random.random() < short_seq_prob:  # \xe6\x9c\x89\xe4\xb8\x80\xe5\xae\x9a\xe7\x9a\x84\xe6\xaf\x94\xe4\xbe\x8b\xef\xbc\x8c\xe5\xa6\x8210%\xe7\x9a\x84\xe6\xa6\x82\xe7\x8e\x87\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe4\xbd\xbf\xe7\x94\xa8\xe6\xaf\x94\xe8\xbe\x83\xe7\x9f\xad\xe7\x9a\x84\xe5\xba\x8f\xe5\x88\x97\xe9\x95\xbf\xe5\xba\xa6\xef\xbc\x8c\xe4\xbb\xa5\xe7\xbc\x93\xe8\xa7\xa3\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\x8f\xe5\x88\x97\xe5\x92\x8c\xe8\xb0\x83\xe4\xbc\x98\xe9\x98\xb6\xe6\xae\xb5\xef\xbc\x88\xe5\x8f\xaf\xe8\x83\xbd\xe7\x9a\x84\xef\xbc\x89\xe7\x9f\xad\xe5\xba\x8f\xe5\x88\x97\xe7\x9a\x84\xe4\xb8\x8d\xe4\xb8\x80\xe8\x87\xb4\xe6\x83\x85\xe5\x86\xb5\r\n        target_seq_length = random.randint(2, max_num_tokens)\r\n\r\n    # We DON\'T just concatenate all of the tokens from a document into a long\r\n    # sequence and choose an arbitrary split point because this would make the\r\n    # next sentence prediction task too easy. Instead, we split the input into\r\n    # segments ""A"" and ""B"" based on the actual ""sentences"" provided by the user\r\n    # input.\r\n    # \xe8\xae\xbe\xe6\xb3\x95\xe4\xbd\xbf\xe7\x94\xa8\xe5\xae\x9e\xe9\x99\x85\xe7\x9a\x84\xe5\x8f\xa5\xe5\xad\x90\xef\xbc\x8c\xe8\x80\x8c\xe4\xb8\x8d\xe6\x98\xaf\xe4\xbb\xbb\xe6\x84\x8f\xe7\x9a\x84\xe6\x88\xaa\xe6\x96\xad\xe5\x8f\xa5\xe5\xad\x90\xef\xbc\x8c\xe4\xbb\x8e\xe8\x80\x8c\xe6\x9b\xb4\xe5\xa5\xbd\xe7\x9a\x84\xe6\x9e\x84\xe9\x80\xa0\xe5\x8f\xa5\xe5\xad\x90\xe8\xbf\x9e\xe8\xb4\xaf\xe6\x80\xa7\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe4\xbb\xbb\xe5\x8a\xa1\r\n    instances = []\r\n    current_chunk = []  # \xe5\xbd\x93\xe5\x89\x8d\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe6\x96\x87\xe6\x9c\xac\xe6\xae\xb5\xef\xbc\x8c\xe5\x8c\x85\xe5\x90\xab\xe5\xa4\x9a\xe4\xb8\xaa\xe5\x8f\xa5\xe5\xad\x90\r\n    current_length = 0\r\n    i = 0\r\n    # print(""###document:"",document) # \xe4\xb8\x80\xe4\xb8\xaadocument\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x98\xaf\xe4\xb8\x80\xe6\x95\xb4\xe7\xaf\x87\xe6\x96\x87\xe7\xab\xa0\xe3\x80\x81\xe6\x96\xb0\xe9\x97\xbb\xe3\x80\x81\xe8\xaf\x8d\xe6\x9d\xa1\xe7\xad\x89. document:[[\'\xe6\x98\xaf\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe5\xb0\xb1\', \'\xe5\xbe\x97\', \'\xe7\xbb\x99\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe5\xb9\xb8\', \'\xe7\xa6\x8f\'], [\'\xe5\x85\xb3\', \'\xe6\xb3\xa8\', \'\xe3\x80\x90\', \'\xe6\x99\xa8\', \'\xe6\x9b\xa6\', \'\xe6\x95\x99\', \'\xe8\x82\xb2\', \'\xe3\x80\x91\', \'\xef\xbc\x8c\', \'\xe8\x8e\xb7\', \'\xe5\x8f\x96\', \'\xe8\x82\xb2\', \'\xe5\x84\xbf\', \'\xe7\x9a\x84\', \'\xe6\x99\xba\', \'\xe6\x85\xa7\', \'\xef\xbc\x8c\', \'\xe4\xb8\x8e\', \'\xe5\xad\xa9\', \'\xe5\xad\x90\', \'\xe4\xb8\x80\', \'\xe5\x90\x8c\', \'\xe6\x88\x90\', \'\xe9\x95\xbf\', \'\xef\xbc\x81\'], [\'\xe6\x96\xb9\', \'\xe6\xb3\x95\', \':\', \'\xe6\x89\x93\', \'\xe5\xbc\x80\', \'\xe5\xbe\xae\', \'\xe4\xbf\xa1\', \'\xe2\x86\x92\', \'\xe6\xb7\xbb\', \'\xe5\x8a\xa0\', \'\xe6\x9c\x8b\', \'\xe5\x8f\x8b\', \'\xe2\x86\x92\', \'\xe6\x90\x9c\', \'\xe5\x8f\xb7\', \'\xe2\x86\x92\', \'##he\', \'##bc\', \'##x\', \'##jy\', \'##\xe2\x86\x92\', \'\xe5\x85\xb3\', \'\xe6\xb3\xa8\', \'!\', \'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe4\xb8\xaa\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe5\xad\x9d\', \'\xe9\xa1\xba\', \'\xe6\x98\xaf\', \'\xe5\x81\x9a\', \'\xe4\xba\xba\', \'\xe7\x9a\x84\', \'\xe7\xac\xac\', \'\xe4\xb8\x80\', \'\xe5\x87\x86\', \'\xe5\x88\x99\', \'\xe3\x80\x82\'], [\'\xe7\x94\xad\', \'\xe7\xae\xa1\', \'\xe5\xb0\x8f\', \'\xe6\x97\xb6\', \'\xe5\x80\x99\', \'\xe6\x80\x8e\', \'\xe4\xb9\x88\', \'\xe8\xb7\x9f\', \'\xe5\xae\xb6\', \'\xe9\x95\xbf\', \'\xe7\x8a\xaf\', \'\xe6\xb7\xb7\', \'\xe8\x9b\x8b\', \'\xef\xbc\x8c\', \'\xe9\x95\xbf\', \'\xe5\xa4\xa7\', \'\xe4\xba\x86\', \'\xef\xbc\x8c\', \'\xe5\xb0\xb1\', \'\xe5\xba\x95\', \'\xe6\x8a\xa5\', \'\xe7\xad\x94\', \'\xe7\x88\xb6\', \'\xe6\xaf\x8d\', \'\xef\xbc\x8c\', \'\xe4\xbb\xa5\', \'\xe5\x90\x8e\', \'\xe6\x88\x91\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe4\xb9\x9f\', \'\xe5\xbf\x85\', \'\xe9\xa1\xbb\', \'\xe5\xad\x9d\', \'\xe9\xa1\xba\', \'\xe3\x80\x82\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe4\xb8\xaa\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe5\x8f\xaf\', \'\xe4\xbb\xa5\', \'\xe8\x8a\xb1\', \'\xe5\xbf\x83\', \'\xef\xbc\x8c\', \'\xe5\x8f\xaf\', \'\xe4\xbb\xa5\', \'\xe5\xa5\xbd\', \'\xe7\x8e\xa9\', \'\xe3\x80\x82\'], [\'\xe4\xbd\x86\', \'\xe6\x88\x91\', \'\xe4\xb8\x80\', \'\xe5\xae\x9a\', \'\xe4\xbc\x9a\', \'\xe6\x89\xbe\', \'\xe4\xb8\x80\', \'\xe4\xb8\xaa\', \'\xe7\xae\xa1\', \'\xe7\x9a\x84\', \'\xe4\xbd\x8f\', \'\xe6\x88\x91\', \'\xe7\x9a\x84\', \'\xe5\xa5\xb3\', \'\xe4\xba\xba\', \'\xef\xbc\x8c\', \'\xe5\x92\x8c\', \'\xe6\x88\x91\', \'\xe4\xb8\x80\', \'\xe8\xb5\xb7\', \'\xe7\x94\x9f\', \'\xe6\xb4\xbb\', \'\xe3\x80\x82\'], [\'28\', \'\xe5\xb2\x81\', \'\xe4\xbb\xa5\', \'\xe5\x89\x8d\', \'\xe5\x9c\xa8\', \'\xe6\x80\x8e\', \'\xe4\xb9\x88\', \'\xe7\x8e\xa9\', \'\xe9\x83\xbd\', \'\xe8\xa1\x8c\', \'\xef\xbc\x8c\', \'\xe4\xbd\x86\', \'\xe6\x88\x91\', \'\xe6\x9c\x80\', \'\xe5\x90\x8e\', \'\xe4\xb8\x80\', \'\xe5\xae\x9a\', \'\xe4\xbc\x9a\', \'\xe6\x89\xbe\', \'\xe4\xb8\x80\', \'\xe4\xb8\xaa\', \'\xe5\x8b\xa4\', \'\xe4\xbf\xad\', \'\xe6\x8c\x81\', \'\xe5\xae\xb6\', \'\xe7\x9a\x84\', \'\xe5\xa5\xb3\', \'\xe4\xba\xba\', \'\xe3\x80\x82\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe6\x88\x91\', \'\xe4\xb8\x8d\', \'\xe4\xbc\x9a\', \'\xe8\xae\xa9\', \'\xe8\x87\xaa\', \'\xe5\xb7\xb1\', \'\xe7\x9a\x84\', \'\xe5\xa5\xb3\', \'\xe4\xba\xba\', \'\xe5\x8f\x97\', \'\xe4\xb8\x80\', \'\xe7\x82\xb9\', \'\xe5\xa7\x94\', \'\xe5\xb1\x88\', \'\xef\xbc\x8c\', \'\xe6\xaf\x8f\', \'\xe6\xac\xa1\', \'\xe6\x8a\x8a\', \'\xe5\xa5\xb9\', \'\xe6\x8a\xb1\', \'\xe5\x9c\xa8\', \'\xe6\x80\x80\', \'\xe9\x87\x8c\', \'\xef\xbc\x8c\', \'\xe7\x9c\x8b\', \'\xe5\xa5\xb9\', \'\xe6\xb4\x8b\', \'\xe6\xba\xa2\', \'\xe7\x9d\x80\', \'\xe5\xb9\xb8\', \'\xe7\xa6\x8f\', \'\xe7\x9a\x84\', \'\xe8\x84\xb8\', \'\xef\xbc\x8c\', \'\xe6\x88\x91\', \'\xe9\x83\xbd\', \'\xe4\xbc\x9a\', \'\xe5\xbc\x95\', \'\xe4\xbb\xa5\', \'\xe4\xb8\xba\', \'\xe5\x82\xb2\', \'\xef\xbc\x8c\', \'\xe8\xbf\x99\', \'\xe7\x89\xb9\', \'\xe4\xb9\x88\', \'\xe5\xb0\xb1\', \'\xe6\x98\xaf\', \'\xe6\x88\x91\', \'\xe7\x9a\x84\', \'\xe5\xa5\xb3\', \'\xe4\xba\xba\', \'\xe3\x80\x82\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe5\xb9\xb2\', \'\xe4\xbb\x80\', \'\xe4\xb9\x88\', \'\xe4\xb9\x9f\', \'\xe4\xb8\x8d\', \'\xe8\x83\xbd\', \'\xe5\xbf\x98\', \'\xe4\xba\x86\', \'\xe8\x87\xaa\', \'\xe5\xb7\xb1\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xef\xbc\x8c\', \'\xe5\xb0\xb1\', \'\xe7\xae\x97\', \'\xe5\x92\x8c\', \'\xe5\x93\xa5\', \'\xe4\xbb\xac\', \'\xe4\xb8\x80\', \'\xe8\xb5\xb7\', \'\xe5\x96\x9d\', \'\xe9\x85\x92\', \'\xef\xbc\x8c\', \'\xe5\x96\x9d\', \'\xe5\x88\xb0\', \'\xe5\xbe\x88\', \'\xe6\x99\x9a\', \'\xef\xbc\x8c\', \'\xe4\xb9\x9f\', \'\xe8\xa6\x81\', \'\xe6\x8f\x90\', \'\xe5\x89\x8d\', \'\xe6\x89\x93\', \'\xe7\x94\xb5\', \'\xe8\xaf\x9d\', \'\xe5\x91\x8a\', \'\xe8\xaf\x89\', \'\xe5\xa5\xb9\', \'\xef\xbc\x8c\', \'\xe8\xae\xa9\', \'\xe5\xa5\xb9\', \'\xe6\x97\xa9\', \'\xe7\x82\xb9\', \'\xe4\xbc\x91\', \'\xe6\x81\xaf\', \'\xe3\x80\x82\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe6\x88\x91\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe7\xbb\x9d\', \'\xe5\xaf\xb9\', \'\xe4\xb8\x8d\', \'\xe8\x83\xbd\', \'\xe6\x8a\xbd\', \'\xe7\x83\x9f\', \'\xef\xbc\x8c\', \'\xe5\x96\x9d\', \'\xe9\x85\x92\', \'\xe8\xbf\x98\', \'\xe5\x8b\x89\', \'\xe5\xbc\xba\', \'\xe8\xbf\x87\', \'\xe5\xbe\x97\', \'\xe5\x8e\xbb\', \'\xef\xbc\x8c\', \'\xe4\xb8\x8d\', \'\xe8\xbf\x87\', \'\xe8\xaf\xa5\', \'\xe5\x96\x9d\', \'\xe7\x9a\x84\', \'\xe6\x97\xb6\', \'\xe5\x80\x99\', \'\xe5\x96\x9d\', \'\xef\xbc\x8c\', \'\xe4\xb8\x8d\', \'\xe8\xaf\xa5\', \'\xe5\x96\x9d\', \'\xe7\x9a\x84\', \'\xe6\x97\xb6\', \'\xe5\x80\x99\', \'\xef\xbc\x8c\', \'\xe5\xb0\x91\', \'\xe6\x89\xaf\', \'\xe7\xba\xb3\', \'\xe6\x9e\x81\', \'\xe8\x96\x84\', \'\xe8\x9b\x8b\', \'\xe3\x80\x82\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe6\x88\x91\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe5\xbf\x85\', \'\xe9\xa1\xbb\', \'\xe5\x90\xac\', \'\xe6\x88\x91\', \'\xe8\xaf\x9d\', \'\xef\xbc\x8c\', \'\xe5\x9c\xa8\', \'\xe4\xba\xba\', \'\xe5\x89\x8d\', \'\xe4\xb8\x80\', \'\xe5\xae\x9a\', \'\xe8\xa6\x81\', \'\xe7\xbb\x99\', \'\xe6\x88\x91\', \'\xe9\x9d\xa2\', \'\xe5\xad\x90\', \'\xef\xbc\x8c\', \'\xe5\x9b\x9e\', \'\xe5\xae\xb6\', \'\xe4\xba\x86\', \'\xe5\x92\xb1\', \'\xe4\xbb\x80\', \'\xe4\xb9\x88\', \'\xe9\x83\xbd\', \'\xe5\xa5\xbd\', \'\xe8\xaf\xb4\', \'\xe3\x80\x82\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe5\xb0\xb1\', \'\xe7\xae\x97\', \'\xe9\x9a\xbe\', \'\xe7\x9a\x84\', \'\xe5\x90\x83\', \'\xe4\xb8\x8d\', \'\xe4\xb8\x8a\', \'\xe9\xa5\xad\', \'\xe4\xba\x86\', \'\xef\xbc\x8c\', \'\xe9\x83\xbd\', \'\xe4\xb8\x8d\', \'\xe5\xbc\xa0\', \'\xe5\x8f\xa3\', \'\xe8\xb7\x9f\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe8\xa6\x81\', \'\xe4\xb8\x80\', \'\xe5\x88\x86\', \'\xe9\x92\xb1\', \'\xe3\x80\x82\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe4\xb8\x8d\', \'\xe7\xae\xa1\', \'\xe4\xb8\x8a\', \'\xe5\xad\xa6\', \'\xe8\xbf\x98\', \'\xe6\x98\xaf\', \'\xe4\xb8\x8a\', \'\xe7\x8f\xad\', \'\xef\xbc\x8c\', \'\xe6\x88\x91\', \'\xe9\x83\xbd\', \'\xe4\xbc\x9a\', \'\xe9\x80\x81\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe5\x9b\x9e\', \'\xe5\xae\xb6\', \'\xe3\x80\x82\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe4\xba\xa4\', \'\xe5\xbe\x80\', \'\xe4\xb8\x8d\', \'\xe5\x88\xb0\', \'1\', \'\xe5\xb9\xb4\', \'\xef\xbc\x8c\', \'\xe7\xbb\x9d\', \'\xe5\xaf\xb9\', \'\xe4\xb8\x8d\', \'\xe4\xbc\x9a\', \'\xe5\x92\x8c\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe6\x8f\x90\', \'\xe8\xbf\x87\', \'\xe5\x88\x86\', \'\xe7\x9a\x84\', \'\xe8\xa6\x81\', \'\xe6\xb1\x82\', \'\xef\xbc\x8c\', \'\xe6\x88\x91\', \'\xe4\xbc\x9a\', \'\xe5\xb0\x8a\', \'\xe9\x87\x8d\', \'\xe5\xa5\xb9\', \'\xe3\x80\x82\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe6\xb8\xb8\', \'\xe6\x88\x8f\', \'\xe6\xb0\xb8\', \'\xe8\xbf\x9c\', \'\xe6\xaf\x94\', \'\xe4\xb8\x8d\', \'\xe4\xb8\x8a\', \'\xe6\x88\x91\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe9\x87\x8d\', \'\xe8\xa6\x81\', \'\xef\xbc\x8c\', \'\xe5\x8f\xaa\', \'\xe8\xa6\x81\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe5\x8f\x91\', \'\xe8\xaf\x9d\', \'\xef\xbc\x8c\', \'\xe6\x88\x91\', \'\xe7\xbb\x9d\', \'\xe5\xaf\xb9\', \'\xe5\x94\xaf\', \'\xe5\x91\xbd\', \'\xe6\x98\xaf\', \'\xe4\xbb\x8e\', \'\xe3\x80\x82\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe4\xb8\x8a\', \'q\', \'\xe7\xbb\x9d\', \'\xe5\xaf\xb9\', \'\xe6\x98\xaf\', \'\xe4\xb8\xba\', \'\xe4\xba\x86\', \'\xe7\xad\x89\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xef\xbc\x8c\', \'\xe6\x89\x80\', \'\xe6\x9c\x89\', \'\xe6\x9a\xa7\', \'\xe6\x98\xa7\', \'\xe7\x9a\x84\', \'\xe5\xbf\x83\', \'\xe6\x83\x85\', \'\xe5\x8f\xaa\', \'\xe4\xb8\xba\', \'\xe5\xa5\xb9\', \'\xe4\xb8\x80\', \'\xe4\xb8\xaa\', \'\xe5\xa5\xb3\', \'\xe4\xba\xba\', \'\xe8\x80\x8c\', \'\xe5\x86\x99\', \'\xef\xbc\x8c\', \'\xe6\x88\x91\', \'\xe4\xb8\x8d\', \'\xe4\xb8\x80\', \'\xe5\xae\x9a\', \'\xe4\xbc\x9a\', \'\xe7\xbb\x8f\', \'\xe5\xb8\xb8\', \'\xe5\x86\x99\', \'\xe6\x97\xa5\', \'\xe5\xbf\x97\', \'\xef\xbc\x8c\', \'\xe5\x8f\xaf\', \'\xe6\x98\xaf\', \'\xe6\x88\x91\', \'\xe4\xbc\x9a\', \'\xe5\x91\x8a\', \'\xe8\xaf\x89\', \'\xe5\x85\xa8\', \'\xe4\xb8\x96\', \'\xe7\x95\x8c\', \'\xef\xbc\x8c\', \'\xe6\x88\x91\', \'\xe5\xbe\x88\', \'\xe7\x88\xb1\', \'\xe5\xa5\xb9\', \'\xe3\x80\x82\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe4\xb8\x8d\', \'\xe4\xb8\x80\', \'\xe5\xae\x9a\', \'\xe8\xa6\x81\', \'\xe7\xbb\x8f\', \'\xe5\xb8\xb8\', \'\xe5\x88\xb6\', \'\xe9\x80\xa0\', \'\xe6\xb5\xaa\', \'\xe6\xbc\xab\', \'\xe3\x80\x81\', \'\xe5\x81\xb6\', \'\xe5\xb0\x94\', \'\xe8\xbf\x87\', \'\xe4\xb8\xaa\', \'\xe8\x8a\x82\', \'\xe6\x97\xa5\', \'\xe4\xb9\x9f\', \'\xe8\xa6\x81\', \'\xe9\x80\x81\', \'\xe6\x9d\x9f\', \'\xe7\x8e\xab\', \'\xe7\x91\xb0\', \'\xe8\x8a\xb1\', \'\xe7\xbb\x99\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe6\x8a\xb1\', \'\xe5\x9b\x9e\', \'\xe5\xae\xb6\', \'\xe3\x80\x82\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe6\x89\x8b\', \'\xe6\x9c\xba\', \'\xe4\xbc\x9a\', \'24\', \'\xe5\xb0\x8f\', \'\xe6\x97\xb6\', \'\xe4\xb8\xba\', \'\xe5\xa5\xb9\', \'\xe5\xbc\x80\', \'\xe6\x9c\xba\', \'\xef\xbc\x8c\', \'\xe8\xae\xa9\', \'\xe5\xa5\xb9\', \'\xe5\x8d\x8a\', \'\xe5\xa4\x9c\', \'\xe7\x97\x9b\', \'\xe7\xbb\x8f\', \'\xe7\x9a\x84\', \'\xe6\x97\xb6\', \'\xe5\x80\x99\', \'\xef\xbc\x8c\', \'\xe5\x81\x9a\', \'\xe6\x81\xb6\', \'\xe6\xa2\xa6\', \'\xe7\x9a\x84\', \'\xe6\x97\xb6\', \'\xe5\x80\x99\', \'\xef\xbc\x8c\', \'\xe9\x9a\x8f\', \'\xe6\x97\xb6\', \'\xe5\x8f\xaf\', \'\xe4\xbb\xa5\', \'\xe8\x81\x94\', \'\xe7\xb3\xbb\', \'\xe5\x88\xb0\', \'\xe6\x88\x91\', \'\xe3\x80\x82\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe6\x88\x91\', \'\xe4\xbc\x9a\', \'\xe7\xbb\x8f\', \'\xe5\xb8\xb8\', \'\xe5\xb8\xa6\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe5\x87\xba\', \'\xe5\x8e\xbb\', \'\xe7\x8e\xa9\', \'\xef\xbc\x8c\', \'\xe5\xa5\xb9\', \'\xe4\xb8\x8d\', \'\xe4\xb8\x80\', \'\xe5\xae\x9a\', \'\xe8\xa6\x81\', \'\xe5\x92\x8c\', \'\xe6\x88\x91\', \'\xe6\x89\x80\', \'\xe6\x9c\x89\', \'\xe7\x9a\x84\', \'\xe5\x93\xa5\', \'\xe4\xbb\xac\', \'\xe9\x83\xbd\', \'\xe8\xae\xa4\', \'\xe8\xaf\x86\', \'\xef\xbc\x8c\', \'\xe4\xbd\x86\', \'\xe8\xa7\x81\', \'\xe9\x9d\xa2\', \'\xe8\x83\xbd\', \'\xe8\xaf\xb4\', \'\xe7\x9a\x84\', \'\xe4\xb8\x8a\', \'\xe8\xaf\x9d\', \'\xe5\xb0\xb1\', \'\xe8\xa1\x8c\', \'\xe3\x80\x82\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe6\x88\x91\', \'\xe4\xbc\x9a\', \'\xe5\x92\x8c\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe7\x9a\x84\', \'\xe5\xa7\x90\', \'\xe5\xa6\xb9\', \'\xe5\x93\xa5\', \'\xe4\xbb\xac\', \'\xe6\x90\x9e\', \'\xe5\xa5\xbd\', \'\xe5\x85\xb3\', \'\xe7\xb3\xbb\', \'\xef\xbc\x8c\', \'\xe8\xae\xa9\', \'\xe5\xa5\xb9\', \'\xe4\xbb\xac\', \'\xe7\x9b\xb8\', \'\xe4\xbf\xa1\', \'\xe6\x88\x91\', \'\xe4\xb8\x80\', \'\xe5\xae\x9a\', \'\xe5\x8f\xaf\', \'\xe4\xbb\xa5\', \'\xe7\xbb\x99\', \'\xe6\x88\x91\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe5\xb9\xb8\', \'\xe7\xa6\x8f\', \'\xe3\x80\x82\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe5\x90\xb5\', \'\xe6\x9e\xb6\', \'\xe5\x90\x8e\', \'\xe3\x80\x81\', \'\xe4\xb9\x9f\', \'\xe8\xa6\x81\', \'\xe4\xb8\xbb\', \'\xe5\x8a\xa8\', \'\xe6\x89\x93\', \'\xe7\x94\xb5\', \'\xe8\xaf\x9d\', \'\xe5\x85\xb3\', \'\xe5\xbf\x83\', \'\xe5\xa5\xb9\', \'\xef\xbc\x8c\', \'\xe5\x92\xb1\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe7\xbb\x99\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe6\x9c\x8d\', \'\xe4\xb8\xaa\', \'\xe8\xbd\xaf\', \'\xef\xbc\x8c\', \'\xe9\x81\x93\', \'\xe4\xb8\xaa\', \'\xe6\xad\x89\', \'\xe6\x80\x8e\', \'\xe4\xb9\x88\', \'\xe4\xba\x86\', \'\xef\xbc\x9f\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe7\xbb\x9d\', \'\xe5\xaf\xb9\', \'\xe4\xb8\x8d\', \'\xe4\xbc\x9a\', \'\xe5\xab\x8c\', \'\xe5\xbc\x83\', \'\xe8\x87\xaa\', \'\xe5\xb7\xb1\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xef\xbc\x8c\', \'\xe6\x8b\xbf\', \'\xe5\xa5\xb9\', \'\xe5\x92\x8c\', \'\xe5\x88\xab\', \'\xe4\xba\xba\', \'\xe6\xaf\x94\', \'\xef\xbc\x8c\', \'\xe8\xaf\xb4\', \'\xe5\xa5\xb9\', \'\xe8\xbf\x99\', \'\xe4\xb8\x8d\', \'\xe5\xa6\x82\', \'\xe4\xba\xba\', \'\xe5\xae\xb6\', \'\xef\xbc\x8c\', \'\xe7\xba\xb3\', \'\xe4\xb8\x8d\', \'\xe5\xa6\x82\', \'\xe4\xba\xba\', \'\xe5\xae\xb6\', \'\xe7\x9a\x84\', \'\xe3\x80\x82\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe9\x99\xaa\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe9\x80\x9b\', \'\xe8\xa1\x97\', \'\xe6\x97\xb6\', \'\xef\xbc\x8c\', \'\xe7\xa2\xb0\', \'\xe8\xa7\x81\', \'\xe7\x86\x9f\', \'\xe4\xba\xba\', \'\xef\xbc\x8c\', \'\xe6\x97\xa0\', \'\xe8\xae\xba\', \'\xe6\x88\x91\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe9\x95\xbf\', \'\xe7\x9a\x84\', \'\xe5\xa5\xbd\', \'\xe7\x9c\x8b\', \'\xe4\xb8\x8e\', \'\xe5\x90\xa6\', \'\xef\xbc\x8c\', \'\xe6\x88\x91\', \'\xe9\x83\xbd\', \'\xe4\xbc\x9a\', \'\xe5\xa4\xa7\', \'\xe6\x96\xb9\', \'\xe7\x9a\x84\', \'\xe4\xbb\x8b\', \'\xe7\xbb\x8d\', \'\xe3\x80\x82\'], [\'\xe8\xb0\x81\', \'\xe8\xae\xa9\', \'\xe5\x92\xb1\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xe5\xb0\xb1\', \'\xe5\xa5\xbd\', \'\xe8\xbf\x99\', \'\xe5\x8f\xa3\', \'\xe5\x91\xa2\', \'\xe3\x80\x82\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe6\x88\x91\', \'\xe6\x83\xb3\', \'\xe6\x88\x91\', \'\xe4\xbc\x9a\', \'\xe7\xbb\x99\', \'\xe6\x88\x91\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe6\x9c\x80\', \'\xe5\xa5\xbd\', \'\xe7\x9a\x84\', \'\xe5\xb9\xb8\', \'\xe7\xa6\x8f\', \'\xe3\x80\x82\'], [\'\xe3\x80\x90\', \'\xe6\x88\x91\', \'\xe4\xbb\xac\', \'\xe9\x87\x8d\', \'\xe5\x9c\xa8\', \'\xe5\x88\x86\', \'\xe4\xba\xab\', \'\xe3\x80\x82\'], [\'\xe6\x89\x80\', \'\xe6\x9c\x89\', \'\xe6\x96\x87\', \'\xe5\xad\x97\', \'\xe5\x92\x8c\', \'\xe7\xbe\x8e\', \'\xe5\x9b\xbe\', \'\xef\xbc\x8c\', \'\xe6\x9d\xa5\', \'\xe8\x87\xaa\', \'\xe7\xbd\x91\', \'\xe7\xbb\x9c\', \'\xef\xbc\x8c\', \'\xe6\x99\xa8\', \'\xe6\xac\xa3\', \'\xe6\x95\x99\', \'\xe8\x82\xb2\', \'\xe6\x95\xb4\', \'\xe7\x90\x86\', \'\xe3\x80\x82\'], [\'\xe5\xaf\xb9\', \'\xe5\x8e\x9f\', \'\xe6\x96\x87\', \'\xe4\xbd\x9c\', \'\xe8\x80\x85\', \'\xef\xbc\x8c\', \'\xe8\xa1\xa8\', \'\xe7\xa4\xba\', \'\xe6\x95\xac\', \'\xe6\x84\x8f\', \'\xe3\x80\x82\'], [\'\xe3\x80\x91\', \'\xe5\x85\xb3\', \'\xe6\xb3\xa8\', \'\xe6\x99\xa8\', \'\xe6\x9b\xa6\', \'\xe6\x95\x99\', \'\xe8\x82\xb2\', \'[UNK]\', \'[UNK]\', \'\xe6\x99\xa8\', \'\xe6\x9b\xa6\', \'\xe6\x95\x99\', \'\xe8\x82\xb2\', \'\xef\xbc\x88\', \'\xe5\xbe\xae\', \'\xe4\xbf\xa1\', \'\xe5\x8f\xb7\', \'\xef\xbc\x9a\', \'he\', \'##bc\', \'##x\', \'##jy\', \'\xef\xbc\x89\', \'\xe3\x80\x82\'], [\'\xe6\x89\x93\', \'\xe5\xbc\x80\', \'\xe5\xbe\xae\', \'\xe4\xbf\xa1\', \'\xef\xbc\x8c\', \'\xe6\x89\xab\', \'\xe6\x8f\x8f\', \'\xe4\xba\x8c\', \'\xe7\xbb\xb4\', \'\xe7\xa0\x81\', \'\xef\xbc\x8c\', \'\xe5\x85\xb3\', \'\xe6\xb3\xa8\', \'[UNK]\', \'\xe6\x99\xa8\', \'\xe6\x9b\xa6\', \'\xe6\x95\x99\', \'\xe8\x82\xb2\', \'[UNK]\', \'\xef\xbc\x8c\', \'\xe8\x8e\xb7\', \'\xe5\x8f\x96\', \'\xe6\x9b\xb4\', \'\xe5\xa4\x9a\', \'\xe8\x82\xb2\', \'\xe5\x84\xbf\', \'\xe8\xb5\x84\', \'\xe6\xba\x90\', \'\xe3\x80\x82\'], [\'\xe7\x82\xb9\', \'\xe5\x87\xbb\', \'\xe4\xb8\x8b\', \'\xe9\x9d\xa2\', \'\xe8\xae\xa2\', \'\xe9\x98\x85\', \'\xe6\x8c\x89\', \'\xe9\x92\xae\', \'\xe8\xae\xa2\', \'\xe9\x98\x85\', \'\xef\xbc\x8c\', \'\xe4\xbc\x9a\', \'\xe6\x9c\x89\', \'\xe6\x9b\xb4\', \'\xe5\xa4\x9a\', \'\xe6\x83\x8a\', \'\xe5\x96\x9c\', \'\xe5\x93\xa6\', \'\xef\xbc\x81\']]\r\n    while i < len(document):  # \xe4\xbb\x8e\xe6\x96\x87\xe6\xa1\xa3\xe7\x9a\x84\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe4\xbd\x8d\xe7\xbd\xae\xe5\xbc\x80\xe5\xa7\x8b\xef\xbc\x8c\xe6\x8c\x89\xe4\xb8\xaa\xe5\xbe\x80\xe4\xb8\x8b\xe7\x9c\x8b\r\n        segment = document[\r\n            i]  # segment\xe6\x98\xaf\xe5\x88\x97\xe8\xa1\xa8\xef\xbc\x8c\xe4\xbb\xa3\xe8\xa1\xa8\xe7\x9a\x84\xe6\x98\xaf\xe6\x8c\x89\xe5\xad\x97\xe5\x88\x86\xe5\xbc\x80\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xe5\xae\x8c\xe6\x95\xb4\xe5\x8f\xa5\xe5\xad\x90\xef\xbc\x8c\xe5\xa6\x82 segment=[\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe6\x88\x91\', \'\xe6\x83\xb3\', \'\xe6\x88\x91\', \'\xe4\xbc\x9a\', \'\xe7\xbb\x99\', \'\xe6\x88\x91\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe6\x9c\x80\', \'\xe5\xa5\xbd\', \'\xe7\x9a\x84\', \'\xe5\xb9\xb8\', \'\xe7\xa6\x8f\', \'\xe3\x80\x82\']\r\n        # segment = get_new_segment(segment)  # whole word mask for chinese: \xe7\xbb\x93\xe5\x90\x88\xe5\x88\x86\xe8\xaf\x8d\xe7\x9a\x84\xe4\xb8\xad\xe6\x96\x87\xe7\x9a\x84whole mask\xe8\xae\xbe\xe7\xbd\xae\xe5\x8d\xb3\xe5\x9c\xa8\xe9\x9c\x80\xe8\xa6\x81\xe7\x9a\x84\xe5\x9c\xb0\xe6\x96\xb9\xe5\x8a\xa0\xe4\xb8\x8a\xe2\x80\x9c##\xe2\x80\x9d\r\n        current_chunk.append(segment)  # \xe5\xb0\x86\xe4\xb8\x80\xe4\xb8\xaa\xe7\x8b\xac\xe7\xab\x8b\xe7\x9a\x84\xe5\x8f\xa5\xe5\xad\x90\xe5\x8a\xa0\xe5\x85\xa5\xe5\x88\xb0\xe5\xbd\x93\xe5\x89\x8d\xe7\x9a\x84\xe6\x96\x87\xe6\x9c\xac\xe5\x9d\x97\xe4\xb8\xad\r\n        current_length += len(segment)  # \xe7\xb4\xaf\xe8\xae\xa1\xe5\x88\xb0\xe4\xb8\xba\xe6\xad\xa2\xe4\xbd\x8d\xe7\xbd\xae\xe6\x8e\xa5\xe8\xa7\xa6\xe5\x88\xb0\xe5\x8f\xa5\xe5\xad\x90\xe7\x9a\x84\xe6\x80\xbb\xe9\x95\xbf\xe5\xba\xa6\r\n        if i == len(document) - 1 or current_length >= target_seq_length:\r\n            # \xe5\xa6\x82\xe6\x9e\x9c\xe7\xb4\xaf\xe8\xae\xa1\xe7\x9a\x84\xe5\xba\x8f\xe5\x88\x97\xe9\x95\xbf\xe5\xba\xa6\xe8\xbe\xbe\xe5\x88\xb0\xe4\xba\x86\xe7\x9b\xae\xe6\xa0\x87\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\xef\xbc\x8c\xe6\x88\x96\xe5\xbd\x93\xe5\x89\x8d\xe8\xb5\xb0\xe5\x88\xb0\xe4\xba\x86\xe6\x96\x87\xe6\xa1\xa3\xe7\xbb\x93\xe5\xb0\xbe==>\xe6\x9e\x84\xe9\x80\xa0\xe5\xb9\xb6\xe6\xb7\xbb\xe5\x8a\xa0\xe5\x88\xb0\xe2\x80\x9cA[SEP]B\xe2\x80\x9c\xe4\xb8\xad\xe7\x9a\x84A\xe5\x92\x8cB\xe4\xb8\xad\xef\xbc\x9b\r\n            if current_chunk:  # \xe5\xa6\x82\xe6\x9e\x9c\xe5\xbd\x93\xe5\x89\x8d\xe5\x9d\x97\xe4\xb8\x8d\xe4\xb8\xba\xe7\xa9\xba\r\n                # `a_end` is how many segments from `current_chunk` go into the `A`\r\n                # (first) sentence.\r\n                a_end = 1\r\n                if len(current_chunk) >= 2:  # \xe5\xbd\x93\xe5\x89\x8d\xe5\x9d\x97\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe5\x8c\x85\xe5\x90\xab\xe8\xb6\x85\xe8\xbf\x87\xe4\xb8\xa4\xe4\xb8\xaa\xe5\x8f\xa5\xe5\xad\x90\xef\xbc\x8c\xe5\x8f\x96\xe5\xbd\x93\xe5\x89\x8d\xe5\x9d\x97\xe7\x9a\x84\xe4\xb8\x80\xe9\x83\xa8\xe5\x88\x86\xe4\xbd\x9c\xe4\xb8\xba\xe2\x80\x9cA[SEP]B\xe2\x80\x9c\xe4\xb8\xad\xe7\x9a\x84A\xe9\x83\xa8\xe5\x88\x86\r\n                    a_end = random.randint(1, len(current_chunk) - 1)\r\n                # \xe5\xb0\x86\xe5\xbd\x93\xe5\x89\x8d\xe6\x96\x87\xe6\x9c\xac\xe6\xae\xb5\xe4\xb8\xad\xe9\x80\x89\xe5\x8f\x96\xe5\x87\xba\xe6\x9d\xa5\xe7\x9a\x84\xe5\x89\x8d\xe5\x8d\x8a\xe9\x83\xa8\xe5\x88\x86\xef\xbc\x8c\xe8\xb5\x8b\xe5\x80\xbc\xe7\xbb\x99A\xe5\x8d\xb3tokens_a\r\n                tokens_a = []\r\n                for j in range(a_end):\r\n                    tokens_a.extend(current_chunk[j])\r\n\r\n                # \xe6\x9e\x84\xe9\x80\xa0\xe2\x80\x9cA[SEP]B\xe2\x80\x9c\xe4\xb8\xad\xe7\x9a\x84B\xe9\x83\xa8\xe5\x88\x86(\xe6\x9c\x89\xe4\xb8\x80\xe9\x83\xa8\xe5\x88\x86\xe6\x98\xaf\xe6\xad\xa3\xe5\xb8\xb8\xe7\x9a\x84\xe5\xbd\x93\xe5\x89\x8d\xe6\x96\x87\xe6\xa1\xa3\xe4\xb8\xad\xe7\x9a\x84\xe5\x90\x8e\xe5\x8d\x8a\xe9\x83\xa8;\xe5\x9c\xa8\xe5\x8e\x9fBERT\xe7\x9a\x84\xe5\xae\x9e\xe7\x8e\xb0\xe4\xb8\xad\xe4\xb8\x80\xe9\x83\xa8\xe5\x88\x86\xe6\x98\xaf\xe9\x9a\x8f\xe6\x9c\xba\xe7\x9a\x84\xe4\xbb\x8e\xe5\x8f\xa6\xe4\xb8\x80\xe4\xb8\xaa\xe6\x96\x87\xe6\xa1\xa3\xe4\xb8\xad\xe9\x80\x89\xe5\x8f\x96\xe7\x9a\x84\xef\xbc\x8c\xef\xbc\x89\r\n                tokens_b = []\r\n                for j in range(a_end, len(current_chunk)):\r\n                    tokens_b.extend(current_chunk[j])\r\n\r\n                # \xe6\x9c\x89\xe7\x99\xbe\xe5\x88\x86\xe4\xb9\x8b50%\xe7\x9a\x84\xe6\xa6\x82\xe7\x8e\x87\xe4\xba\xa4\xe6\x8d\xa2\xe4\xb8\x80\xe4\xb8\x8btokens_a\xe5\x92\x8ctokens_b\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\r\n                # print(""tokens_a length1:"",len(tokens_a))\r\n                # print(""tokens_b length1:"",len(tokens_b)) # len(tokens_b) = 0\r\n                if len(tokens_a) == 0 or len(tokens_b) == 0: continue\r\n                if random.random() < 0.5:  # \xe4\xba\xa4\xe6\x8d\xa2\xe4\xb8\x80\xe4\xb8\x8btokens_a\xe5\x92\x8ctokens_b\r\n                    is_random_next = True\r\n                    temp = tokens_a\r\n                    tokens_a = tokens_b\r\n                    tokens_b = temp\r\n                else:\r\n                    is_random_next = False\r\n                truncate_seq_pair(tokens_a, tokens_b, max_num_tokens)\r\n                assert len(tokens_a) >= 1\r\n                assert len(tokens_b) >= 1\r\n\r\n                # \xe6\x8a\x8atokens_a & tokens_b\xe5\x8a\xa0\xe5\x85\xa5\xe5\x88\xb0\xe6\x8c\x89\xe7\x85\xa7bert\xe7\x9a\x84\xe9\xa3\x8e\xe6\xa0\xbc\xef\xbc\x8c\xe5\x8d\xb3\xe4\xbb\xa5[CLS]tokens_a[SEP]tokens_b[SEP]\xe7\x9a\x84\xe5\xbd\xa2\xe5\xbc\x8f\xef\xbc\x8c\xe7\xbb\x93\xe5\x90\x88\xe5\x88\xb0\xe4\xb8\x80\xe8\xb5\xb7\xef\xbc\x8c\xe4\xbd\x9c\xe4\xb8\xba\xe6\x9c\x80\xe7\xbb\x88\xe7\x9a\x84tokens; \xe4\xb9\x9f\xe5\xb8\xa6\xe4\xb8\x8asegment_ids\xef\xbc\x8c\xe5\x89\x8d\xe9\x9d\xa2\xe9\x83\xa8\xe5\x88\x86segment_ids\xe7\x9a\x84\xe5\x80\xbc\xe6\x98\xaf0\xef\xbc\x8c\xe5\x90\x8e\xe9\x9d\xa2\xe9\x83\xa8\xe5\x88\x86\xe7\x9a\x84\xe5\x80\xbc\xe6\x98\xaf1.\r\n                tokens = [""[CLS]""] + tokens_a + [""[SEP]""] + tokens_b + [""[SEP]""]\r\n                # The segment IDs are 0 for the [CLS] token, the A tokens and the first [SEP]\r\n                # They are 1 for the B tokens and the final [SEP]\r\n                segment_ids = [0 for _ in range(len(tokens_a) + 2)] + [1 for _ in range(len(tokens_b) + 1)]\r\n\r\n                # \xe5\x88\x9b\xe5\xbb\xbamasked LM\xe7\x9a\x84\xe4\xbb\xbb\xe5\x8a\xa1\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae Creates the predictions for the masked LM objective\r\n                tokens, masked_lm_positions, masked_lm_labels = create_masked_lm_predictions(\r\n                    tokens, masked_lm_prob, max_predictions_per_seq, vocab_words)\r\n                instance = {\r\n                    ""tokens"": tokens,\r\n                    ""segment_ids"": segment_ids,\r\n                    ""is_random_next"": is_random_next,\r\n                    ""masked_lm_positions"": masked_lm_positions,\r\n                    ""masked_lm_labels"": masked_lm_labels}\r\n                instances.append(instance)\r\n            current_chunk = []  # \xe6\xb8\x85\xe7\xa9\xba\xe5\xbd\x93\xe5\x89\x8d\xe5\x9d\x97\r\n            current_length = 0  # \xe9\x87\x8d\xe7\xbd\xae\xe5\xbd\x93\xe5\x89\x8d\xe6\x96\x87\xe6\x9c\xac\xe5\x9d\x97\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\r\n        i += 1  # \xe6\x8e\xa5\xe7\x9d\x80\xe6\x96\x87\xe6\xa1\xa3\xe4\xb8\xad\xe7\x9a\x84\xe5\x86\x85\xe5\xae\xb9\xe5\xbe\x80\xe5\x90\x8e\xe7\x9c\x8b\r\n    return instances\r\n\r\n\r\ndef create_masked_lm_predictions(tokens, masked_lm_prob, max_predictions_per_seq, vocab_list):\r\n    """"""Creates the predictions for the masked LM objective. This is mostly copied from the Google BERT repo, but\r\n    with several refactors to clean it up and remove a lot of unnecessary variables.""""""\r\n    cand_indices = []\r\n    for (i, token) in enumerate(tokens):\r\n        if token == ""[CLS]"" or token == ""[SEP]"":\r\n            continue\r\n        # Whole Word Masking means that if we mask all of the wordpieces\r\n        # corresponding to an original word. When a word has been split into\r\n        # WordPieces, the first token does not have any marker and any subsequence\r\n        # tokens are prefixed with ##. So whenever we see the ## token, we\r\n        # append it to the previous set of word indexes.\r\n        #\r\n        # Note that Whole Word Masking does *not* change the training code\r\n        # at all -- we still predict each WordPiece independently, softmaxed\r\n        # over the entire vocabulary.\r\n        cand_indices.append(i)\r\n\r\n    num_to_mask = min(max_predictions_per_seq, max(1, int(round(len(tokens) * masked_lm_prob))))\r\n    random.shuffle(cand_indices)\r\n    mask_indices = sorted(random.sample(cand_indices, num_to_mask))\r\n    masked_token_labels = []\r\n    for index in mask_indices:\r\n        # 80% of the time, replace with [MASK]\r\n        if random.random() < 0.8:\r\n            masked_token = ""[MASK]""\r\n        else:\r\n            # 10% of the time, keep original\r\n            if random.random() < 0.5:\r\n                masked_token = tokens[index]\r\n            # 10% of the time, replace with random word\r\n            else:\r\n                masked_token = random.choice(vocab_list)\r\n        masked_token_labels.append(MaskedLmInstance(index=index, label=tokens[index]))\r\n        tokens[index] = masked_token\r\n    assert len(masked_token_labels) <= num_to_mask\r\n    masked_token_labels = sorted(masked_token_labels, key=lambda x: x.index)\r\n    mask_indices = [p.index for p in masked_token_labels]\r\n    masked_labels = [p.label for p in masked_token_labels]\r\n    return tokens, mask_indices, masked_labels\r\n\r\n\r\ndef create_training_instances(input_file, tokenizer, max_seq_len, short_seq_prob,\r\n                              masked_lm_prob, max_predictions_per_seq):\r\n    """"""Create `TrainingInstance`s from raw text.""""""\r\n    all_documents = [[]]\r\n    # Input file format:\r\n    # (1) One sentence per line. These should ideally be actual sentences, not\r\n    # entire paragraphs or arbitrary spans of text. (Because we use the\r\n    # sentence boundaries for the ""next sentence prediction"" task).\r\n    # (2) Blank lines between documents. Document boundaries are needed so\r\n    # that the ""next sentence prediction"" task doesn\'t span between documents.\r\n    f = open(input_file, \'r\')\r\n    lines = f.readlines()\r\n    pbar = ProgressBar(n_total=len(lines), desc=\'read data\')\r\n    for line_cnt, line in enumerate(lines):\r\n        line = line.strip()\r\n        # Empty lines are used as document delimiters\r\n        if not line:\r\n            all_documents.append([])\r\n        tokens = tokenizer.tokenize(line)\r\n        if tokens:\r\n            all_documents[-1].append(tokens)\r\n        pbar(step=line_cnt)\r\n    print(\' \')\r\n    # Remove empty documents\r\n    all_documents = [x for x in all_documents if x]\r\n    random.shuffle(all_documents)\r\n\r\n    vocab_words = list(tokenizer.vocab.keys())\r\n    instances = []\r\n    pbar = ProgressBar(n_total=len(all_documents), desc=\'create instances\')\r\n    for document_index in range(len(all_documents)):\r\n        instances.extend(\r\n            create_instances_from_document(\r\n                all_documents, document_index, max_seq_len, short_seq_prob,\r\n                masked_lm_prob, max_predictions_per_seq, vocab_words))\r\n        pbar(step=document_index)\r\n    print(\' \')\r\n    ex_idx = 0\r\n    while ex_idx < 5:\r\n        instance = instances[ex_idx]\r\n        logger.info(""-------------------------Example-----------------------"")\r\n        logger.info(f""id: {ex_idx}"")\r\n        logger.info(f""tokens: {\' \'.join([str(x) for x in instance[\'tokens\']])}"")\r\n        logger.info(f""masked_lm_labels: {\' \'.join([str(x) for x in instance[\'masked_lm_labels\']])}"")\r\n        logger.info(f""segment_ids: {\' \'.join([str(x) for x in instance[\'segment_ids\']])}"")\r\n        logger.info(f""masked_lm_positions: {\' \'.join([str(x) for x in instance[\'masked_lm_positions\']])}"")\r\n        logger.info(f""is_random_next : {instance[\'is_random_next\']}"")\r\n        ex_idx += 1\r\n    random.shuffle(instances)\r\n    return instances\r\n\r\n\r\ndef main():\r\n    parser = ArgumentParser()\r\n    ## Required parameters\r\n    parser.add_argument(""--data_dir"", default=None, type=str, required=True)\r\n    parser.add_argument(""--vocab_path"", default=None, type=str, required=True)\r\n    parser.add_argument(""--output_dir"", default=None, type=str, required=True)\r\n\r\n    parser.add_argument(\'--data_name\', default=\'albert\', type=str)\r\n    parser.add_argument(""--do_data"", default=False, action=\'store_true\')\r\n    parser.add_argument(""--do_split"", default=False, action=\'store_true\')\r\n    parser.add_argument(""--do_lower_case"", default=False, action=\'store_true\')\r\n    parser.add_argument(\'--seed\', default=42, type=int)\r\n    parser.add_argument(""--line_per_file"", default=1000000000, type=int)\r\n    parser.add_argument(""--file_num"", type=int, default=10,\r\n                        help=""Number of dynamic masking to pregenerate (with different masks)"")\r\n    parser.add_argument(""--max_seq_len"", type=int, default=128)\r\n    parser.add_argument(""--short_seq_prob"", type=float, default=0.1,\r\n                        help=""Probability of making a short sentence as a training example"")\r\n    parser.add_argument(""--masked_lm_prob"", type=float, default=0.15,\r\n                        help=""Probability of masking each token for the LM task"")\r\n    parser.add_argument(""--max_predictions_per_seq"", type=int, default=20,  # 128 * 0.15\r\n                        help=""Maximum number of tokens to mask in each sequence"")\r\n    args = parser.parse_args()\r\n    seed_everything(args.seed)\r\n    args.data_dir = Path(args.data_dir)\r\n    if not os.path.exists(args.output_dir):\r\n        os.mkdir(args.output_dir)\r\n    init_logger(log_file=args.output_dir +""pregenerate_training_data.log"")\r\n    logger.info(""pregenerate training data parameters:\\n %s"", args)\r\n    tokenizer = BertTokenizer(vocab_file=args.vocab_path, do_lower_case=args.do_lower_case)\r\n\r\n    # split big file\r\n    if args.do_split:\r\n        corpus_path = args.data_dir / ""corpus/corpus.txt""\r\n        split_save_path = args.data_dir / ""/corpus/train""\r\n        if not split_save_path.exists():\r\n            split_save_path.mkdir(exist_ok=True)\r\n        line_per_file = args.line_per_file\r\n        command = f\'split -a 4 -l {line_per_file} -d {corpus_path} {split_save_path}/shard_\'\r\n        os.system(f""{command}"")\r\n\r\n    # generator train data\r\n    if args.do_data:\r\n        data_path = args.data_dir / ""corpus/train""\r\n        files = sorted([f for f in data_path.parent.iterdir() if f.exists() and \'.txt\' in str(f)])\r\n        for idx in range(args.file_num):\r\n            logger.info(f""pregenetate {args.data_name}_file_{idx}.json"")\r\n            save_filename = data_path / f""{args.data_name}_file_{idx}.json""\r\n            num_instances = 0\r\n            with save_filename.open(\'w\') as fw:\r\n                for file_idx in range(len(files)):\r\n                    file_path = files[file_idx]\r\n                    file_examples = create_training_instances(input_file=file_path,\r\n                                                              tokenizer=tokenizer,\r\n                                                              max_seq_len=args.max_seq_len,\r\n                                                              short_seq_prob=args.short_seq_prob,\r\n                                                              masked_lm_prob=args.masked_lm_prob,\r\n                                                              max_predictions_per_seq=args.max_predictions_per_seq)\r\n                    file_examples = [json.dumps(instance) for instance in file_examples]\r\n                    for instance in file_examples:\r\n                        fw.write(instance + \'\\n\')\r\n                        num_instances += 1\r\n            metrics_file = data_path / f""{args.data_name}_file_{idx}_metrics.json""\r\n            print(f""num_instances: {num_instances}"")\r\n            with metrics_file.open(\'w\') as metrics_file:\r\n                metrics = {\r\n                    ""num_training_examples"": num_instances,\r\n                    ""max_seq_len"": args.max_seq_len\r\n                }\r\n                metrics_file.write(json.dumps(metrics))\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\n'"
prepare_lm_data_ngram.py,0,"b'import os\r\nimport json\r\nimport random\r\nimport numpy as np\r\nimport collections\r\nfrom pathlib import Path\r\nfrom tools.common import logger, init_logger\r\nfrom argparse import ArgumentParser\r\nfrom tools.common import seed_everything\r\nfrom model.tokenization_bert import BertTokenizer\r\nfrom callback.progressbar import ProgressBar\r\n\r\nMaskedLmInstance = collections.namedtuple(""MaskedLmInstance"", [""index"", ""label""])\r\n\r\ndef truncate_seq_pair(tokens_a, tokens_b, max_num_tokens):\r\n    """"""Truncates a pair of sequences to a maximum sequence length.""""""\r\n    while True:\r\n        total_length = len(tokens_a) + len(tokens_b)\r\n        if total_length <= max_num_tokens:\r\n            break\r\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\r\n        assert len(trunc_tokens) >= 1\r\n        # We want to sometimes truncate from the front and sometimes from the\r\n        # back to add more randomness and avoid biases.\r\n        if random.random() < 0.5:\r\n            del trunc_tokens[0]\r\n        else:\r\n            trunc_tokens.pop()\r\n\r\ndef create_instances_from_document(all_documents, document_index, max_seq_length, short_seq_prob,\r\n                                   max_ngram, masked_lm_prob, max_predictions_per_seq, vocab_words):\r\n    """"""Creates `TrainingInstance`s for a single document.\r\n     This method is changed to create sentence-order prediction (SOP) followed by idea from paper of ALBERT, 2019-08-28, brightmart\r\n    """"""\r\n    document = all_documents[document_index]  # \xe5\xbe\x97\xe5\x88\xb0\xe4\xb8\x80\xe4\xb8\xaa\xe6\x96\x87\xe6\xa1\xa3\r\n\r\n    # Account for [CLS], [SEP], [SEP]\r\n    max_num_tokens = max_seq_length - 3\r\n\r\n    # We *usually* want to fill up the entire sequence since we are padding\r\n    # to `max_seq_length` anyways, so short sequences are generally wasted\r\n    # computation. However, we *sometimes*\r\n    # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\r\n    # sequences to minimize the mismatch between pre-training and fine-tuning.\r\n    # The `target_seq_length` is just a rough target however, whereas\r\n    # `max_seq_length` is a hard limit.\r\n    target_seq_length = max_num_tokens\r\n    if random.random() < short_seq_prob:  # \xe6\x9c\x89\xe4\xb8\x80\xe5\xae\x9a\xe7\x9a\x84\xe6\xaf\x94\xe4\xbe\x8b\xef\xbc\x8c\xe5\xa6\x8210%\xe7\x9a\x84\xe6\xa6\x82\xe7\x8e\x87\xef\xbc\x8c\xe6\x88\x91\xe4\xbb\xac\xe4\xbd\xbf\xe7\x94\xa8\xe6\xaf\x94\xe8\xbe\x83\xe7\x9f\xad\xe7\x9a\x84\xe5\xba\x8f\xe5\x88\x97\xe9\x95\xbf\xe5\xba\xa6\xef\xbc\x8c\xe4\xbb\xa5\xe7\xbc\x93\xe8\xa7\xa3\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\x8f\xe5\x88\x97\xe5\x92\x8c\xe8\xb0\x83\xe4\xbc\x98\xe9\x98\xb6\xe6\xae\xb5\xef\xbc\x88\xe5\x8f\xaf\xe8\x83\xbd\xe7\x9a\x84\xef\xbc\x89\xe7\x9f\xad\xe5\xba\x8f\xe5\x88\x97\xe7\x9a\x84\xe4\xb8\x8d\xe4\xb8\x80\xe8\x87\xb4\xe6\x83\x85\xe5\x86\xb5\r\n        target_seq_length = random.randint(2, max_num_tokens)\r\n\r\n    # We DON\'T just concatenate all of the tokens from a document into a long\r\n    # sequence and choose an arbitrary split point because this would make the\r\n    # next sentence prediction task too easy. Instead, we split the input into\r\n    # segments ""A"" and ""B"" based on the actual ""sentences"" provided by the user\r\n    # input.\r\n    # \xe8\xae\xbe\xe6\xb3\x95\xe4\xbd\xbf\xe7\x94\xa8\xe5\xae\x9e\xe9\x99\x85\xe7\x9a\x84\xe5\x8f\xa5\xe5\xad\x90\xef\xbc\x8c\xe8\x80\x8c\xe4\xb8\x8d\xe6\x98\xaf\xe4\xbb\xbb\xe6\x84\x8f\xe7\x9a\x84\xe6\x88\xaa\xe6\x96\xad\xe5\x8f\xa5\xe5\xad\x90\xef\xbc\x8c\xe4\xbb\x8e\xe8\x80\x8c\xe6\x9b\xb4\xe5\xa5\xbd\xe7\x9a\x84\xe6\x9e\x84\xe9\x80\xa0\xe5\x8f\xa5\xe5\xad\x90\xe8\xbf\x9e\xe8\xb4\xaf\xe6\x80\xa7\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84\xe4\xbb\xbb\xe5\x8a\xa1\r\n    instances = []\r\n    current_chunk = []  # \xe5\xbd\x93\xe5\x89\x8d\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe6\x96\x87\xe6\x9c\xac\xe6\xae\xb5\xef\xbc\x8c\xe5\x8c\x85\xe5\x90\xab\xe5\xa4\x9a\xe4\xb8\xaa\xe5\x8f\xa5\xe5\xad\x90\r\n    current_length = 0\r\n    i = 0\r\n    # print(""###document:"",document) # \xe4\xb8\x80\xe4\xb8\xaadocument\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x98\xaf\xe4\xb8\x80\xe6\x95\xb4\xe7\xaf\x87\xe6\x96\x87\xe7\xab\xa0\xe3\x80\x81\xe6\x96\xb0\xe9\x97\xbb\xe3\x80\x81\xe8\xaf\x8d\xe6\x9d\xa1\xe7\xad\x89. document:[[\'\xe6\x98\xaf\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe5\xb0\xb1\', \'\xe5\xbe\x97\', \'\xe7\xbb\x99\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe5\xb9\xb8\', \'\xe7\xa6\x8f\'], [\'\xe5\x85\xb3\', \'\xe6\xb3\xa8\', \'\xe3\x80\x90\', \'\xe6\x99\xa8\', \'\xe6\x9b\xa6\', \'\xe6\x95\x99\', \'\xe8\x82\xb2\', \'\xe3\x80\x91\', \'\xef\xbc\x8c\', \'\xe8\x8e\xb7\', \'\xe5\x8f\x96\', \'\xe8\x82\xb2\', \'\xe5\x84\xbf\', \'\xe7\x9a\x84\', \'\xe6\x99\xba\', \'\xe6\x85\xa7\', \'\xef\xbc\x8c\', \'\xe4\xb8\x8e\', \'\xe5\xad\xa9\', \'\xe5\xad\x90\', \'\xe4\xb8\x80\', \'\xe5\x90\x8c\', \'\xe6\x88\x90\', \'\xe9\x95\xbf\', \'\xef\xbc\x81\'], [\'\xe6\x96\xb9\', \'\xe6\xb3\x95\', \':\', \'\xe6\x89\x93\', \'\xe5\xbc\x80\', \'\xe5\xbe\xae\', \'\xe4\xbf\xa1\', \'\xe2\x86\x92\', \'\xe6\xb7\xbb\', \'\xe5\x8a\xa0\', \'\xe6\x9c\x8b\', \'\xe5\x8f\x8b\', \'\xe2\x86\x92\', \'\xe6\x90\x9c\', \'\xe5\x8f\xb7\', \'\xe2\x86\x92\', \'##he\', \'##bc\', \'##x\', \'##jy\', \'##\xe2\x86\x92\', \'\xe5\x85\xb3\', \'\xe6\xb3\xa8\', \'!\', \'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe4\xb8\xaa\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe5\xad\x9d\', \'\xe9\xa1\xba\', \'\xe6\x98\xaf\', \'\xe5\x81\x9a\', \'\xe4\xba\xba\', \'\xe7\x9a\x84\', \'\xe7\xac\xac\', \'\xe4\xb8\x80\', \'\xe5\x87\x86\', \'\xe5\x88\x99\', \'\xe3\x80\x82\'], [\'\xe7\x94\xad\', \'\xe7\xae\xa1\', \'\xe5\xb0\x8f\', \'\xe6\x97\xb6\', \'\xe5\x80\x99\', \'\xe6\x80\x8e\', \'\xe4\xb9\x88\', \'\xe8\xb7\x9f\', \'\xe5\xae\xb6\', \'\xe9\x95\xbf\', \'\xe7\x8a\xaf\', \'\xe6\xb7\xb7\', \'\xe8\x9b\x8b\', \'\xef\xbc\x8c\', \'\xe9\x95\xbf\', \'\xe5\xa4\xa7\', \'\xe4\xba\x86\', \'\xef\xbc\x8c\', \'\xe5\xb0\xb1\', \'\xe5\xba\x95\', \'\xe6\x8a\xa5\', \'\xe7\xad\x94\', \'\xe7\x88\xb6\', \'\xe6\xaf\x8d\', \'\xef\xbc\x8c\', \'\xe4\xbb\xa5\', \'\xe5\x90\x8e\', \'\xe6\x88\x91\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe4\xb9\x9f\', \'\xe5\xbf\x85\', \'\xe9\xa1\xbb\', \'\xe5\xad\x9d\', \'\xe9\xa1\xba\', \'\xe3\x80\x82\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe4\xb8\xaa\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe5\x8f\xaf\', \'\xe4\xbb\xa5\', \'\xe8\x8a\xb1\', \'\xe5\xbf\x83\', \'\xef\xbc\x8c\', \'\xe5\x8f\xaf\', \'\xe4\xbb\xa5\', \'\xe5\xa5\xbd\', \'\xe7\x8e\xa9\', \'\xe3\x80\x82\'], [\'\xe4\xbd\x86\', \'\xe6\x88\x91\', \'\xe4\xb8\x80\', \'\xe5\xae\x9a\', \'\xe4\xbc\x9a\', \'\xe6\x89\xbe\', \'\xe4\xb8\x80\', \'\xe4\xb8\xaa\', \'\xe7\xae\xa1\', \'\xe7\x9a\x84\', \'\xe4\xbd\x8f\', \'\xe6\x88\x91\', \'\xe7\x9a\x84\', \'\xe5\xa5\xb3\', \'\xe4\xba\xba\', \'\xef\xbc\x8c\', \'\xe5\x92\x8c\', \'\xe6\x88\x91\', \'\xe4\xb8\x80\', \'\xe8\xb5\xb7\', \'\xe7\x94\x9f\', \'\xe6\xb4\xbb\', \'\xe3\x80\x82\'], [\'28\', \'\xe5\xb2\x81\', \'\xe4\xbb\xa5\', \'\xe5\x89\x8d\', \'\xe5\x9c\xa8\', \'\xe6\x80\x8e\', \'\xe4\xb9\x88\', \'\xe7\x8e\xa9\', \'\xe9\x83\xbd\', \'\xe8\xa1\x8c\', \'\xef\xbc\x8c\', \'\xe4\xbd\x86\', \'\xe6\x88\x91\', \'\xe6\x9c\x80\', \'\xe5\x90\x8e\', \'\xe4\xb8\x80\', \'\xe5\xae\x9a\', \'\xe4\xbc\x9a\', \'\xe6\x89\xbe\', \'\xe4\xb8\x80\', \'\xe4\xb8\xaa\', \'\xe5\x8b\xa4\', \'\xe4\xbf\xad\', \'\xe6\x8c\x81\', \'\xe5\xae\xb6\', \'\xe7\x9a\x84\', \'\xe5\xa5\xb3\', \'\xe4\xba\xba\', \'\xe3\x80\x82\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe6\x88\x91\', \'\xe4\xb8\x8d\', \'\xe4\xbc\x9a\', \'\xe8\xae\xa9\', \'\xe8\x87\xaa\', \'\xe5\xb7\xb1\', \'\xe7\x9a\x84\', \'\xe5\xa5\xb3\', \'\xe4\xba\xba\', \'\xe5\x8f\x97\', \'\xe4\xb8\x80\', \'\xe7\x82\xb9\', \'\xe5\xa7\x94\', \'\xe5\xb1\x88\', \'\xef\xbc\x8c\', \'\xe6\xaf\x8f\', \'\xe6\xac\xa1\', \'\xe6\x8a\x8a\', \'\xe5\xa5\xb9\', \'\xe6\x8a\xb1\', \'\xe5\x9c\xa8\', \'\xe6\x80\x80\', \'\xe9\x87\x8c\', \'\xef\xbc\x8c\', \'\xe7\x9c\x8b\', \'\xe5\xa5\xb9\', \'\xe6\xb4\x8b\', \'\xe6\xba\xa2\', \'\xe7\x9d\x80\', \'\xe5\xb9\xb8\', \'\xe7\xa6\x8f\', \'\xe7\x9a\x84\', \'\xe8\x84\xb8\', \'\xef\xbc\x8c\', \'\xe6\x88\x91\', \'\xe9\x83\xbd\', \'\xe4\xbc\x9a\', \'\xe5\xbc\x95\', \'\xe4\xbb\xa5\', \'\xe4\xb8\xba\', \'\xe5\x82\xb2\', \'\xef\xbc\x8c\', \'\xe8\xbf\x99\', \'\xe7\x89\xb9\', \'\xe4\xb9\x88\', \'\xe5\xb0\xb1\', \'\xe6\x98\xaf\', \'\xe6\x88\x91\', \'\xe7\x9a\x84\', \'\xe5\xa5\xb3\', \'\xe4\xba\xba\', \'\xe3\x80\x82\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe5\xb9\xb2\', \'\xe4\xbb\x80\', \'\xe4\xb9\x88\', \'\xe4\xb9\x9f\', \'\xe4\xb8\x8d\', \'\xe8\x83\xbd\', \'\xe5\xbf\x98\', \'\xe4\xba\x86\', \'\xe8\x87\xaa\', \'\xe5\xb7\xb1\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xef\xbc\x8c\', \'\xe5\xb0\xb1\', \'\xe7\xae\x97\', \'\xe5\x92\x8c\', \'\xe5\x93\xa5\', \'\xe4\xbb\xac\', \'\xe4\xb8\x80\', \'\xe8\xb5\xb7\', \'\xe5\x96\x9d\', \'\xe9\x85\x92\', \'\xef\xbc\x8c\', \'\xe5\x96\x9d\', \'\xe5\x88\xb0\', \'\xe5\xbe\x88\', \'\xe6\x99\x9a\', \'\xef\xbc\x8c\', \'\xe4\xb9\x9f\', \'\xe8\xa6\x81\', \'\xe6\x8f\x90\', \'\xe5\x89\x8d\', \'\xe6\x89\x93\', \'\xe7\x94\xb5\', \'\xe8\xaf\x9d\', \'\xe5\x91\x8a\', \'\xe8\xaf\x89\', \'\xe5\xa5\xb9\', \'\xef\xbc\x8c\', \'\xe8\xae\xa9\', \'\xe5\xa5\xb9\', \'\xe6\x97\xa9\', \'\xe7\x82\xb9\', \'\xe4\xbc\x91\', \'\xe6\x81\xaf\', \'\xe3\x80\x82\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe6\x88\x91\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe7\xbb\x9d\', \'\xe5\xaf\xb9\', \'\xe4\xb8\x8d\', \'\xe8\x83\xbd\', \'\xe6\x8a\xbd\', \'\xe7\x83\x9f\', \'\xef\xbc\x8c\', \'\xe5\x96\x9d\', \'\xe9\x85\x92\', \'\xe8\xbf\x98\', \'\xe5\x8b\x89\', \'\xe5\xbc\xba\', \'\xe8\xbf\x87\', \'\xe5\xbe\x97\', \'\xe5\x8e\xbb\', \'\xef\xbc\x8c\', \'\xe4\xb8\x8d\', \'\xe8\xbf\x87\', \'\xe8\xaf\xa5\', \'\xe5\x96\x9d\', \'\xe7\x9a\x84\', \'\xe6\x97\xb6\', \'\xe5\x80\x99\', \'\xe5\x96\x9d\', \'\xef\xbc\x8c\', \'\xe4\xb8\x8d\', \'\xe8\xaf\xa5\', \'\xe5\x96\x9d\', \'\xe7\x9a\x84\', \'\xe6\x97\xb6\', \'\xe5\x80\x99\', \'\xef\xbc\x8c\', \'\xe5\xb0\x91\', \'\xe6\x89\xaf\', \'\xe7\xba\xb3\', \'\xe6\x9e\x81\', \'\xe8\x96\x84\', \'\xe8\x9b\x8b\', \'\xe3\x80\x82\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe6\x88\x91\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe5\xbf\x85\', \'\xe9\xa1\xbb\', \'\xe5\x90\xac\', \'\xe6\x88\x91\', \'\xe8\xaf\x9d\', \'\xef\xbc\x8c\', \'\xe5\x9c\xa8\', \'\xe4\xba\xba\', \'\xe5\x89\x8d\', \'\xe4\xb8\x80\', \'\xe5\xae\x9a\', \'\xe8\xa6\x81\', \'\xe7\xbb\x99\', \'\xe6\x88\x91\', \'\xe9\x9d\xa2\', \'\xe5\xad\x90\', \'\xef\xbc\x8c\', \'\xe5\x9b\x9e\', \'\xe5\xae\xb6\', \'\xe4\xba\x86\', \'\xe5\x92\xb1\', \'\xe4\xbb\x80\', \'\xe4\xb9\x88\', \'\xe9\x83\xbd\', \'\xe5\xa5\xbd\', \'\xe8\xaf\xb4\', \'\xe3\x80\x82\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe5\xb0\xb1\', \'\xe7\xae\x97\', \'\xe9\x9a\xbe\', \'\xe7\x9a\x84\', \'\xe5\x90\x83\', \'\xe4\xb8\x8d\', \'\xe4\xb8\x8a\', \'\xe9\xa5\xad\', \'\xe4\xba\x86\', \'\xef\xbc\x8c\', \'\xe9\x83\xbd\', \'\xe4\xb8\x8d\', \'\xe5\xbc\xa0\', \'\xe5\x8f\xa3\', \'\xe8\xb7\x9f\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe8\xa6\x81\', \'\xe4\xb8\x80\', \'\xe5\x88\x86\', \'\xe9\x92\xb1\', \'\xe3\x80\x82\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe4\xb8\x8d\', \'\xe7\xae\xa1\', \'\xe4\xb8\x8a\', \'\xe5\xad\xa6\', \'\xe8\xbf\x98\', \'\xe6\x98\xaf\', \'\xe4\xb8\x8a\', \'\xe7\x8f\xad\', \'\xef\xbc\x8c\', \'\xe6\x88\x91\', \'\xe9\x83\xbd\', \'\xe4\xbc\x9a\', \'\xe9\x80\x81\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe5\x9b\x9e\', \'\xe5\xae\xb6\', \'\xe3\x80\x82\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe4\xba\xa4\', \'\xe5\xbe\x80\', \'\xe4\xb8\x8d\', \'\xe5\x88\xb0\', \'1\', \'\xe5\xb9\xb4\', \'\xef\xbc\x8c\', \'\xe7\xbb\x9d\', \'\xe5\xaf\xb9\', \'\xe4\xb8\x8d\', \'\xe4\xbc\x9a\', \'\xe5\x92\x8c\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe6\x8f\x90\', \'\xe8\xbf\x87\', \'\xe5\x88\x86\', \'\xe7\x9a\x84\', \'\xe8\xa6\x81\', \'\xe6\xb1\x82\', \'\xef\xbc\x8c\', \'\xe6\x88\x91\', \'\xe4\xbc\x9a\', \'\xe5\xb0\x8a\', \'\xe9\x87\x8d\', \'\xe5\xa5\xb9\', \'\xe3\x80\x82\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe6\xb8\xb8\', \'\xe6\x88\x8f\', \'\xe6\xb0\xb8\', \'\xe8\xbf\x9c\', \'\xe6\xaf\x94\', \'\xe4\xb8\x8d\', \'\xe4\xb8\x8a\', \'\xe6\x88\x91\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe9\x87\x8d\', \'\xe8\xa6\x81\', \'\xef\xbc\x8c\', \'\xe5\x8f\xaa\', \'\xe8\xa6\x81\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe5\x8f\x91\', \'\xe8\xaf\x9d\', \'\xef\xbc\x8c\', \'\xe6\x88\x91\', \'\xe7\xbb\x9d\', \'\xe5\xaf\xb9\', \'\xe5\x94\xaf\', \'\xe5\x91\xbd\', \'\xe6\x98\xaf\', \'\xe4\xbb\x8e\', \'\xe3\x80\x82\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe4\xb8\x8a\', \'q\', \'\xe7\xbb\x9d\', \'\xe5\xaf\xb9\', \'\xe6\x98\xaf\', \'\xe4\xb8\xba\', \'\xe4\xba\x86\', \'\xe7\xad\x89\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xef\xbc\x8c\', \'\xe6\x89\x80\', \'\xe6\x9c\x89\', \'\xe6\x9a\xa7\', \'\xe6\x98\xa7\', \'\xe7\x9a\x84\', \'\xe5\xbf\x83\', \'\xe6\x83\x85\', \'\xe5\x8f\xaa\', \'\xe4\xb8\xba\', \'\xe5\xa5\xb9\', \'\xe4\xb8\x80\', \'\xe4\xb8\xaa\', \'\xe5\xa5\xb3\', \'\xe4\xba\xba\', \'\xe8\x80\x8c\', \'\xe5\x86\x99\', \'\xef\xbc\x8c\', \'\xe6\x88\x91\', \'\xe4\xb8\x8d\', \'\xe4\xb8\x80\', \'\xe5\xae\x9a\', \'\xe4\xbc\x9a\', \'\xe7\xbb\x8f\', \'\xe5\xb8\xb8\', \'\xe5\x86\x99\', \'\xe6\x97\xa5\', \'\xe5\xbf\x97\', \'\xef\xbc\x8c\', \'\xe5\x8f\xaf\', \'\xe6\x98\xaf\', \'\xe6\x88\x91\', \'\xe4\xbc\x9a\', \'\xe5\x91\x8a\', \'\xe8\xaf\x89\', \'\xe5\x85\xa8\', \'\xe4\xb8\x96\', \'\xe7\x95\x8c\', \'\xef\xbc\x8c\', \'\xe6\x88\x91\', \'\xe5\xbe\x88\', \'\xe7\x88\xb1\', \'\xe5\xa5\xb9\', \'\xe3\x80\x82\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe4\xb8\x8d\', \'\xe4\xb8\x80\', \'\xe5\xae\x9a\', \'\xe8\xa6\x81\', \'\xe7\xbb\x8f\', \'\xe5\xb8\xb8\', \'\xe5\x88\xb6\', \'\xe9\x80\xa0\', \'\xe6\xb5\xaa\', \'\xe6\xbc\xab\', \'\xe3\x80\x81\', \'\xe5\x81\xb6\', \'\xe5\xb0\x94\', \'\xe8\xbf\x87\', \'\xe4\xb8\xaa\', \'\xe8\x8a\x82\', \'\xe6\x97\xa5\', \'\xe4\xb9\x9f\', \'\xe8\xa6\x81\', \'\xe9\x80\x81\', \'\xe6\x9d\x9f\', \'\xe7\x8e\xab\', \'\xe7\x91\xb0\', \'\xe8\x8a\xb1\', \'\xe7\xbb\x99\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe6\x8a\xb1\', \'\xe5\x9b\x9e\', \'\xe5\xae\xb6\', \'\xe3\x80\x82\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe6\x89\x8b\', \'\xe6\x9c\xba\', \'\xe4\xbc\x9a\', \'24\', \'\xe5\xb0\x8f\', \'\xe6\x97\xb6\', \'\xe4\xb8\xba\', \'\xe5\xa5\xb9\', \'\xe5\xbc\x80\', \'\xe6\x9c\xba\', \'\xef\xbc\x8c\', \'\xe8\xae\xa9\', \'\xe5\xa5\xb9\', \'\xe5\x8d\x8a\', \'\xe5\xa4\x9c\', \'\xe7\x97\x9b\', \'\xe7\xbb\x8f\', \'\xe7\x9a\x84\', \'\xe6\x97\xb6\', \'\xe5\x80\x99\', \'\xef\xbc\x8c\', \'\xe5\x81\x9a\', \'\xe6\x81\xb6\', \'\xe6\xa2\xa6\', \'\xe7\x9a\x84\', \'\xe6\x97\xb6\', \'\xe5\x80\x99\', \'\xef\xbc\x8c\', \'\xe9\x9a\x8f\', \'\xe6\x97\xb6\', \'\xe5\x8f\xaf\', \'\xe4\xbb\xa5\', \'\xe8\x81\x94\', \'\xe7\xb3\xbb\', \'\xe5\x88\xb0\', \'\xe6\x88\x91\', \'\xe3\x80\x82\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe6\x88\x91\', \'\xe4\xbc\x9a\', \'\xe7\xbb\x8f\', \'\xe5\xb8\xb8\', \'\xe5\xb8\xa6\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe5\x87\xba\', \'\xe5\x8e\xbb\', \'\xe7\x8e\xa9\', \'\xef\xbc\x8c\', \'\xe5\xa5\xb9\', \'\xe4\xb8\x8d\', \'\xe4\xb8\x80\', \'\xe5\xae\x9a\', \'\xe8\xa6\x81\', \'\xe5\x92\x8c\', \'\xe6\x88\x91\', \'\xe6\x89\x80\', \'\xe6\x9c\x89\', \'\xe7\x9a\x84\', \'\xe5\x93\xa5\', \'\xe4\xbb\xac\', \'\xe9\x83\xbd\', \'\xe8\xae\xa4\', \'\xe8\xaf\x86\', \'\xef\xbc\x8c\', \'\xe4\xbd\x86\', \'\xe8\xa7\x81\', \'\xe9\x9d\xa2\', \'\xe8\x83\xbd\', \'\xe8\xaf\xb4\', \'\xe7\x9a\x84\', \'\xe4\xb8\x8a\', \'\xe8\xaf\x9d\', \'\xe5\xb0\xb1\', \'\xe8\xa1\x8c\', \'\xe3\x80\x82\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe6\x88\x91\', \'\xe4\xbc\x9a\', \'\xe5\x92\x8c\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe7\x9a\x84\', \'\xe5\xa7\x90\', \'\xe5\xa6\xb9\', \'\xe5\x93\xa5\', \'\xe4\xbb\xac\', \'\xe6\x90\x9e\', \'\xe5\xa5\xbd\', \'\xe5\x85\xb3\', \'\xe7\xb3\xbb\', \'\xef\xbc\x8c\', \'\xe8\xae\xa9\', \'\xe5\xa5\xb9\', \'\xe4\xbb\xac\', \'\xe7\x9b\xb8\', \'\xe4\xbf\xa1\', \'\xe6\x88\x91\', \'\xe4\xb8\x80\', \'\xe5\xae\x9a\', \'\xe5\x8f\xaf\', \'\xe4\xbb\xa5\', \'\xe7\xbb\x99\', \'\xe6\x88\x91\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe5\xb9\xb8\', \'\xe7\xa6\x8f\', \'\xe3\x80\x82\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe5\x90\xb5\', \'\xe6\x9e\xb6\', \'\xe5\x90\x8e\', \'\xe3\x80\x81\', \'\xe4\xb9\x9f\', \'\xe8\xa6\x81\', \'\xe4\xb8\xbb\', \'\xe5\x8a\xa8\', \'\xe6\x89\x93\', \'\xe7\x94\xb5\', \'\xe8\xaf\x9d\', \'\xe5\x85\xb3\', \'\xe5\xbf\x83\', \'\xe5\xa5\xb9\', \'\xef\xbc\x8c\', \'\xe5\x92\xb1\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe7\xbb\x99\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe6\x9c\x8d\', \'\xe4\xb8\xaa\', \'\xe8\xbd\xaf\', \'\xef\xbc\x8c\', \'\xe9\x81\x93\', \'\xe4\xb8\xaa\', \'\xe6\xad\x89\', \'\xe6\x80\x8e\', \'\xe4\xb9\x88\', \'\xe4\xba\x86\', \'\xef\xbc\x9f\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe7\xbb\x9d\', \'\xe5\xaf\xb9\', \'\xe4\xb8\x8d\', \'\xe4\xbc\x9a\', \'\xe5\xab\x8c\', \'\xe5\xbc\x83\', \'\xe8\x87\xaa\', \'\xe5\xb7\xb1\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xef\xbc\x8c\', \'\xe6\x8b\xbf\', \'\xe5\xa5\xb9\', \'\xe5\x92\x8c\', \'\xe5\x88\xab\', \'\xe4\xba\xba\', \'\xe6\xaf\x94\', \'\xef\xbc\x8c\', \'\xe8\xaf\xb4\', \'\xe5\xa5\xb9\', \'\xe8\xbf\x99\', \'\xe4\xb8\x8d\', \'\xe5\xa6\x82\', \'\xe4\xba\xba\', \'\xe5\xae\xb6\', \'\xef\xbc\x8c\', \'\xe7\xba\xb3\', \'\xe4\xb8\x8d\', \'\xe5\xa6\x82\', \'\xe4\xba\xba\', \'\xe5\xae\xb6\', \'\xe7\x9a\x84\', \'\xe3\x80\x82\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe9\x99\xaa\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe9\x80\x9b\', \'\xe8\xa1\x97\', \'\xe6\x97\xb6\', \'\xef\xbc\x8c\', \'\xe7\xa2\xb0\', \'\xe8\xa7\x81\', \'\xe7\x86\x9f\', \'\xe4\xba\xba\', \'\xef\xbc\x8c\', \'\xe6\x97\xa0\', \'\xe8\xae\xba\', \'\xe6\x88\x91\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe9\x95\xbf\', \'\xe7\x9a\x84\', \'\xe5\xa5\xbd\', \'\xe7\x9c\x8b\', \'\xe4\xb8\x8e\', \'\xe5\x90\xa6\', \'\xef\xbc\x8c\', \'\xe6\x88\x91\', \'\xe9\x83\xbd\', \'\xe4\xbc\x9a\', \'\xe5\xa4\xa7\', \'\xe6\x96\xb9\', \'\xe7\x9a\x84\', \'\xe4\xbb\x8b\', \'\xe7\xbb\x8d\', \'\xe3\x80\x82\'], [\'\xe8\xb0\x81\', \'\xe8\xae\xa9\', \'\xe5\x92\xb1\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xe5\xb0\xb1\', \'\xe5\xa5\xbd\', \'\xe8\xbf\x99\', \'\xe5\x8f\xa3\', \'\xe5\x91\xa2\', \'\xe3\x80\x82\'], [\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe6\x88\x91\', \'\xe6\x83\xb3\', \'\xe6\x88\x91\', \'\xe4\xbc\x9a\', \'\xe7\xbb\x99\', \'\xe6\x88\x91\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe6\x9c\x80\', \'\xe5\xa5\xbd\', \'\xe7\x9a\x84\', \'\xe5\xb9\xb8\', \'\xe7\xa6\x8f\', \'\xe3\x80\x82\'], [\'\xe3\x80\x90\', \'\xe6\x88\x91\', \'\xe4\xbb\xac\', \'\xe9\x87\x8d\', \'\xe5\x9c\xa8\', \'\xe5\x88\x86\', \'\xe4\xba\xab\', \'\xe3\x80\x82\'], [\'\xe6\x89\x80\', \'\xe6\x9c\x89\', \'\xe6\x96\x87\', \'\xe5\xad\x97\', \'\xe5\x92\x8c\', \'\xe7\xbe\x8e\', \'\xe5\x9b\xbe\', \'\xef\xbc\x8c\', \'\xe6\x9d\xa5\', \'\xe8\x87\xaa\', \'\xe7\xbd\x91\', \'\xe7\xbb\x9c\', \'\xef\xbc\x8c\', \'\xe6\x99\xa8\', \'\xe6\xac\xa3\', \'\xe6\x95\x99\', \'\xe8\x82\xb2\', \'\xe6\x95\xb4\', \'\xe7\x90\x86\', \'\xe3\x80\x82\'], [\'\xe5\xaf\xb9\', \'\xe5\x8e\x9f\', \'\xe6\x96\x87\', \'\xe4\xbd\x9c\', \'\xe8\x80\x85\', \'\xef\xbc\x8c\', \'\xe8\xa1\xa8\', \'\xe7\xa4\xba\', \'\xe6\x95\xac\', \'\xe6\x84\x8f\', \'\xe3\x80\x82\'], [\'\xe3\x80\x91\', \'\xe5\x85\xb3\', \'\xe6\xb3\xa8\', \'\xe6\x99\xa8\', \'\xe6\x9b\xa6\', \'\xe6\x95\x99\', \'\xe8\x82\xb2\', \'[UNK]\', \'[UNK]\', \'\xe6\x99\xa8\', \'\xe6\x9b\xa6\', \'\xe6\x95\x99\', \'\xe8\x82\xb2\', \'\xef\xbc\x88\', \'\xe5\xbe\xae\', \'\xe4\xbf\xa1\', \'\xe5\x8f\xb7\', \'\xef\xbc\x9a\', \'he\', \'##bc\', \'##x\', \'##jy\', \'\xef\xbc\x89\', \'\xe3\x80\x82\'], [\'\xe6\x89\x93\', \'\xe5\xbc\x80\', \'\xe5\xbe\xae\', \'\xe4\xbf\xa1\', \'\xef\xbc\x8c\', \'\xe6\x89\xab\', \'\xe6\x8f\x8f\', \'\xe4\xba\x8c\', \'\xe7\xbb\xb4\', \'\xe7\xa0\x81\', \'\xef\xbc\x8c\', \'\xe5\x85\xb3\', \'\xe6\xb3\xa8\', \'[UNK]\', \'\xe6\x99\xa8\', \'\xe6\x9b\xa6\', \'\xe6\x95\x99\', \'\xe8\x82\xb2\', \'[UNK]\', \'\xef\xbc\x8c\', \'\xe8\x8e\xb7\', \'\xe5\x8f\x96\', \'\xe6\x9b\xb4\', \'\xe5\xa4\x9a\', \'\xe8\x82\xb2\', \'\xe5\x84\xbf\', \'\xe8\xb5\x84\', \'\xe6\xba\x90\', \'\xe3\x80\x82\'], [\'\xe7\x82\xb9\', \'\xe5\x87\xbb\', \'\xe4\xb8\x8b\', \'\xe9\x9d\xa2\', \'\xe8\xae\xa2\', \'\xe9\x98\x85\', \'\xe6\x8c\x89\', \'\xe9\x92\xae\', \'\xe8\xae\xa2\', \'\xe9\x98\x85\', \'\xef\xbc\x8c\', \'\xe4\xbc\x9a\', \'\xe6\x9c\x89\', \'\xe6\x9b\xb4\', \'\xe5\xa4\x9a\', \'\xe6\x83\x8a\', \'\xe5\x96\x9c\', \'\xe5\x93\xa6\', \'\xef\xbc\x81\']]\r\n    while i < len(document):  # \xe4\xbb\x8e\xe6\x96\x87\xe6\xa1\xa3\xe7\x9a\x84\xe7\xac\xac\xe4\xb8\x80\xe4\xb8\xaa\xe4\xbd\x8d\xe7\xbd\xae\xe5\xbc\x80\xe5\xa7\x8b\xef\xbc\x8c\xe6\x8c\x89\xe4\xb8\xaa\xe5\xbe\x80\xe4\xb8\x8b\xe7\x9c\x8b\r\n        segment = document[\r\n            i]  # segment\xe6\x98\xaf\xe5\x88\x97\xe8\xa1\xa8\xef\xbc\x8c\xe4\xbb\xa3\xe8\xa1\xa8\xe7\x9a\x84\xe6\x98\xaf\xe6\x8c\x89\xe5\xad\x97\xe5\x88\x86\xe5\xbc\x80\xe7\x9a\x84\xe4\xb8\x80\xe4\xb8\xaa\xe5\xae\x8c\xe6\x95\xb4\xe5\x8f\xa5\xe5\xad\x90\xef\xbc\x8c\xe5\xa6\x82 segment=[\'\xe6\x88\x91\', \'\xe6\x98\xaf\', \'\xe4\xb8\x80\', \'\xe7\x88\xb7\', \'\xe4\xbb\xac\', \'\xef\xbc\x8c\', \'\xe6\x88\x91\', \'\xe6\x83\xb3\', \'\xe6\x88\x91\', \'\xe4\xbc\x9a\', \'\xe7\xbb\x99\', \'\xe6\x88\x91\', \'\xe5\xaa\xb3\', \'\xe5\xa6\x87\', \'\xe6\x9c\x80\', \'\xe5\xa5\xbd\', \'\xe7\x9a\x84\', \'\xe5\xb9\xb8\', \'\xe7\xa6\x8f\', \'\xe3\x80\x82\']\r\n        # segment = get_new_segment(segment)  # whole word mask for chinese: \xe7\xbb\x93\xe5\x90\x88\xe5\x88\x86\xe8\xaf\x8d\xe7\x9a\x84\xe4\xb8\xad\xe6\x96\x87\xe7\x9a\x84whole mask\xe8\xae\xbe\xe7\xbd\xae\xe5\x8d\xb3\xe5\x9c\xa8\xe9\x9c\x80\xe8\xa6\x81\xe7\x9a\x84\xe5\x9c\xb0\xe6\x96\xb9\xe5\x8a\xa0\xe4\xb8\x8a\xe2\x80\x9c##\xe2\x80\x9d\r\n        current_chunk.append(segment)  # \xe5\xb0\x86\xe4\xb8\x80\xe4\xb8\xaa\xe7\x8b\xac\xe7\xab\x8b\xe7\x9a\x84\xe5\x8f\xa5\xe5\xad\x90\xe5\x8a\xa0\xe5\x85\xa5\xe5\x88\xb0\xe5\xbd\x93\xe5\x89\x8d\xe7\x9a\x84\xe6\x96\x87\xe6\x9c\xac\xe5\x9d\x97\xe4\xb8\xad\r\n        current_length += len(segment)  # \xe7\xb4\xaf\xe8\xae\xa1\xe5\x88\xb0\xe4\xb8\xba\xe6\xad\xa2\xe4\xbd\x8d\xe7\xbd\xae\xe6\x8e\xa5\xe8\xa7\xa6\xe5\x88\xb0\xe5\x8f\xa5\xe5\xad\x90\xe7\x9a\x84\xe6\x80\xbb\xe9\x95\xbf\xe5\xba\xa6\r\n        if i == len(document) - 1 or current_length >= target_seq_length:\r\n            # \xe5\xa6\x82\xe6\x9e\x9c\xe7\xb4\xaf\xe8\xae\xa1\xe7\x9a\x84\xe5\xba\x8f\xe5\x88\x97\xe9\x95\xbf\xe5\xba\xa6\xe8\xbe\xbe\xe5\x88\xb0\xe4\xba\x86\xe7\x9b\xae\xe6\xa0\x87\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\xef\xbc\x8c\xe6\x88\x96\xe5\xbd\x93\xe5\x89\x8d\xe8\xb5\xb0\xe5\x88\xb0\xe4\xba\x86\xe6\x96\x87\xe6\xa1\xa3\xe7\xbb\x93\xe5\xb0\xbe==>\xe6\x9e\x84\xe9\x80\xa0\xe5\xb9\xb6\xe6\xb7\xbb\xe5\x8a\xa0\xe5\x88\xb0\xe2\x80\x9cA[SEP]B\xe2\x80\x9c\xe4\xb8\xad\xe7\x9a\x84A\xe5\x92\x8cB\xe4\xb8\xad\xef\xbc\x9b\r\n            if current_chunk:  # \xe5\xa6\x82\xe6\x9e\x9c\xe5\xbd\x93\xe5\x89\x8d\xe5\x9d\x97\xe4\xb8\x8d\xe4\xb8\xba\xe7\xa9\xba\r\n                # `a_end` is how many segments from `current_chunk` go into the `A`\r\n                # (first) sentence.\r\n                a_end = 1\r\n                if len(current_chunk) >= 2:  # \xe5\xbd\x93\xe5\x89\x8d\xe5\x9d\x97\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe5\x8c\x85\xe5\x90\xab\xe8\xb6\x85\xe8\xbf\x87\xe4\xb8\xa4\xe4\xb8\xaa\xe5\x8f\xa5\xe5\xad\x90\xef\xbc\x8c\xe5\x8f\x96\xe5\xbd\x93\xe5\x89\x8d\xe5\x9d\x97\xe7\x9a\x84\xe4\xb8\x80\xe9\x83\xa8\xe5\x88\x86\xe4\xbd\x9c\xe4\xb8\xba\xe2\x80\x9cA[SEP]B\xe2\x80\x9c\xe4\xb8\xad\xe7\x9a\x84A\xe9\x83\xa8\xe5\x88\x86\r\n                    a_end = random.randint(1, len(current_chunk) - 1)\r\n                # \xe5\xb0\x86\xe5\xbd\x93\xe5\x89\x8d\xe6\x96\x87\xe6\x9c\xac\xe6\xae\xb5\xe4\xb8\xad\xe9\x80\x89\xe5\x8f\x96\xe5\x87\xba\xe6\x9d\xa5\xe7\x9a\x84\xe5\x89\x8d\xe5\x8d\x8a\xe9\x83\xa8\xe5\x88\x86\xef\xbc\x8c\xe8\xb5\x8b\xe5\x80\xbc\xe7\xbb\x99A\xe5\x8d\xb3tokens_a\r\n                tokens_a = []\r\n                for j in range(a_end):\r\n                    tokens_a.extend(current_chunk[j])\r\n\r\n                # \xe6\x9e\x84\xe9\x80\xa0\xe2\x80\x9cA[SEP]B\xe2\x80\x9c\xe4\xb8\xad\xe7\x9a\x84B\xe9\x83\xa8\xe5\x88\x86(\xe6\x9c\x89\xe4\xb8\x80\xe9\x83\xa8\xe5\x88\x86\xe6\x98\xaf\xe6\xad\xa3\xe5\xb8\xb8\xe7\x9a\x84\xe5\xbd\x93\xe5\x89\x8d\xe6\x96\x87\xe6\xa1\xa3\xe4\xb8\xad\xe7\x9a\x84\xe5\x90\x8e\xe5\x8d\x8a\xe9\x83\xa8;\xe5\x9c\xa8\xe5\x8e\x9fBERT\xe7\x9a\x84\xe5\xae\x9e\xe7\x8e\xb0\xe4\xb8\xad\xe4\xb8\x80\xe9\x83\xa8\xe5\x88\x86\xe6\x98\xaf\xe9\x9a\x8f\xe6\x9c\xba\xe7\x9a\x84\xe4\xbb\x8e\xe5\x8f\xa6\xe4\xb8\x80\xe4\xb8\xaa\xe6\x96\x87\xe6\xa1\xa3\xe4\xb8\xad\xe9\x80\x89\xe5\x8f\x96\xe7\x9a\x84\xef\xbc\x8c\xef\xbc\x89\r\n                tokens_b = []\r\n                for j in range(a_end, len(current_chunk)):\r\n                    tokens_b.extend(current_chunk[j])\r\n\r\n                # \xe6\x9c\x89\xe7\x99\xbe\xe5\x88\x86\xe4\xb9\x8b50%\xe7\x9a\x84\xe6\xa6\x82\xe7\x8e\x87\xe4\xba\xa4\xe6\x8d\xa2\xe4\xb8\x80\xe4\xb8\x8btokens_a\xe5\x92\x8ctokens_b\xe7\x9a\x84\xe4\xbd\x8d\xe7\xbd\xae\r\n                # print(""tokens_a length1:"",len(tokens_a))\r\n                # print(""tokens_b length1:"",len(tokens_b)) # len(tokens_b) = 0\r\n                if len(tokens_a) == 0 or len(tokens_b) == 0: continue\r\n                if random.random() < 0.5:  # \xe4\xba\xa4\xe6\x8d\xa2\xe4\xb8\x80\xe4\xb8\x8btokens_a\xe5\x92\x8ctokens_b\r\n                    is_random_next = True\r\n                    temp = tokens_a\r\n                    tokens_a = tokens_b\r\n                    tokens_b = temp\r\n                else:\r\n                    is_random_next = False\r\n                truncate_seq_pair(tokens_a, tokens_b, max_num_tokens)\r\n                assert len(tokens_a) >= 1\r\n                assert len(tokens_b) >= 1\r\n\r\n                # \xe6\x8a\x8atokens_a & tokens_b\xe5\x8a\xa0\xe5\x85\xa5\xe5\x88\xb0\xe6\x8c\x89\xe7\x85\xa7bert\xe7\x9a\x84\xe9\xa3\x8e\xe6\xa0\xbc\xef\xbc\x8c\xe5\x8d\xb3\xe4\xbb\xa5[CLS]tokens_a[SEP]tokens_b[SEP]\xe7\x9a\x84\xe5\xbd\xa2\xe5\xbc\x8f\xef\xbc\x8c\xe7\xbb\x93\xe5\x90\x88\xe5\x88\xb0\xe4\xb8\x80\xe8\xb5\xb7\xef\xbc\x8c\xe4\xbd\x9c\xe4\xb8\xba\xe6\x9c\x80\xe7\xbb\x88\xe7\x9a\x84tokens; \xe4\xb9\x9f\xe5\xb8\xa6\xe4\xb8\x8asegment_ids\xef\xbc\x8c\xe5\x89\x8d\xe9\x9d\xa2\xe9\x83\xa8\xe5\x88\x86segment_ids\xe7\x9a\x84\xe5\x80\xbc\xe6\x98\xaf0\xef\xbc\x8c\xe5\x90\x8e\xe9\x9d\xa2\xe9\x83\xa8\xe5\x88\x86\xe7\x9a\x84\xe5\x80\xbc\xe6\x98\xaf1.\r\n                tokens = [""[CLS]""] + tokens_a + [""[SEP]""] + tokens_b + [""[SEP]""]\r\n                # The segment IDs are 0 for the [CLS] token, the A tokens and the first [SEP]\r\n                # They are 1 for the B tokens and the final [SEP]\r\n                segment_ids = [0 for _ in range(len(tokens_a) + 2)] + [1 for _ in range(len(tokens_b) + 1)]\r\n\r\n                # \xe5\x88\x9b\xe5\xbb\xbamasked LM\xe7\x9a\x84\xe4\xbb\xbb\xe5\x8a\xa1\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae Creates the predictions for the masked LM objective\r\n                tokens, masked_lm_positions, masked_lm_labels = create_masked_lm_predictions(\r\n                    tokens, max_ngram, masked_lm_prob, max_predictions_per_seq, vocab_words)\r\n                instance = {\r\n                    ""tokens"": tokens,\r\n                    ""segment_ids"": segment_ids,\r\n                    ""is_random_next"": is_random_next,\r\n                    ""masked_lm_positions"": masked_lm_positions,\r\n                    ""masked_lm_labels"": masked_lm_labels}\r\n                instances.append(instance)\r\n            current_chunk = []  # \xe6\xb8\x85\xe7\xa9\xba\xe5\xbd\x93\xe5\x89\x8d\xe5\x9d\x97\r\n            current_length = 0  # \xe9\x87\x8d\xe7\xbd\xae\xe5\xbd\x93\xe5\x89\x8d\xe6\x96\x87\xe6\x9c\xac\xe5\x9d\x97\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\r\n        i += 1  # \xe6\x8e\xa5\xe7\x9d\x80\xe6\x96\x87\xe6\xa1\xa3\xe4\xb8\xad\xe7\x9a\x84\xe5\x86\x85\xe5\xae\xb9\xe5\xbe\x80\xe5\x90\x8e\xe7\x9c\x8b\r\n    return instances\r\n\r\n\r\ndef create_masked_lm_predictions(tokens, max_ngram, masked_lm_prob, max_predictions_per_seq, vocab_list):\r\n    """"""Creates the predictions for the masked LM objective. This is mostly copied from the Google BERT repo, but\r\n    with several refactors to clean it up and remove a lot of unnecessary variables.""""""\r\n\r\n    # n-gram masking Albert\r\n    ngrams = np.arange(1, max_ngram + 1, dtype=np.int64)\r\n    pvals = 1. / np.arange(1, max_ngram + 1)\r\n    pvals /= pvals.sum(keepdims=True)  # p(n) = 1/n / sigma(1/k)\r\n    cand_indices = []\r\n    for (i, token) in enumerate(tokens):\r\n        if token == ""[CLS]"" or token == ""[SEP]"":\r\n            continue\r\n        # Whole Word Masking means that if we mask all of the wordpieces\r\n        # corresponding to an original word. When a word has been split into\r\n        # WordPieces, the first token does not have any marker and any subsequence\r\n        # tokens are prefixed with ##. So whenever we see the ## token, we\r\n        # append it to the previous set of word indexes.\r\n        #\r\n        # Note that Whole Word Masking does *not* change the training code\r\n        # at all -- we still predict each WordPiece independently, softmaxed\r\n        # over the entire vocabulary.\r\n        cand_indices.append(i)\r\n    num_to_mask = min(max_predictions_per_seq, max(1, int(round(len(tokens) * masked_lm_prob))))\r\n    random.shuffle(cand_indices)\r\n    masked_token_labels = []\r\n    covered_indices = set()\r\n    for index in cand_indices:\r\n        n = np.random.choice(ngrams, p=pvals)\r\n        if len(masked_token_labels) >= num_to_mask:\r\n            break\r\n        if index in covered_indices:\r\n            continue\r\n        if index < len(cand_indices) - (n - 1):\r\n            for i in range(n):\r\n                ind = index + i\r\n                if ind in covered_indices:\r\n                    continue\r\n                covered_indices.add(ind)\r\n                # 80% of the time, replace with [MASK]\r\n                if random.random() < 0.8:\r\n                    masked_token = ""[MASK]""\r\n                else:\r\n                    # 10% of the time, keep original\r\n                    if random.random() < 0.5:\r\n                        masked_token = tokens[ind]\r\n                    # 10% of the time, replace with random word\r\n                    else:\r\n                        masked_token = random.choice(vocab_list)\r\n                masked_token_labels.append(MaskedLmInstance(index=ind, label=tokens[ind]))\r\n                tokens[ind] = masked_token\r\n\r\n    #assert len(masked_token_labels) <= num_to_mask\r\n    masked_token_labels = sorted(masked_token_labels, key=lambda x: x.index)\r\n    mask_indices = [p.index for p in masked_token_labels]\r\n    masked_labels = [p.label for p in masked_token_labels]\r\n    return tokens, mask_indices, masked_labels\r\n\r\ndef create_training_instances(input_file, tokenizer, max_seq_len, short_seq_prob,\r\n                              max_ngram, masked_lm_prob, max_predictions_per_seq):\r\n    """"""Create `TrainingInstance`s from raw text.""""""\r\n    all_documents = [[]]\r\n    # Input file format:\r\n    # (1) One sentence per line. These should ideally be actual sentences, not\r\n    # entire paragraphs or arbitrary spans of text. (Because we use the\r\n    # sentence boundaries for the ""next sentence prediction"" task).\r\n    # (2) Blank lines between documents. Document boundaries are needed so\r\n    # that the ""next sentence prediction"" task doesn\'t span between documents.\r\n    f = open(input_file, \'r\')\r\n    lines = f.readlines()\r\n    pbar = ProgressBar(n_total=len(lines), desc=\'read data\')\r\n    for line_cnt, line in enumerate(lines):\r\n        line = line.strip()\r\n        # Empty lines are used as document delimiters\r\n        if not line:\r\n            all_documents.append([])\r\n        tokens = tokenizer.tokenize(line)\r\n        if tokens:\r\n            all_documents[-1].append(tokens)\r\n        pbar(step=line_cnt)\r\n    print(\' \')\r\n    # Remove empty documents\r\n    all_documents = [x for x in all_documents if x]\r\n    random.shuffle(all_documents)\r\n\r\n    vocab_words = list(tokenizer.vocab.keys())\r\n    instances = []\r\n    pbar = ProgressBar(n_total=len(all_documents), desc=\'create instances\')\r\n    for document_index in range(len(all_documents)):\r\n        instances.extend(\r\n            create_instances_from_document(\r\n                all_documents, document_index, max_seq_len, short_seq_prob,\r\n                max_ngram, masked_lm_prob, max_predictions_per_seq, vocab_words))\r\n        pbar(step=document_index)\r\n    print(\' \')\r\n    ex_idx = 0\r\n    while ex_idx < 5:\r\n        instance = instances[ex_idx]\r\n        logger.info(""-------------------------Example-----------------------"")\r\n        logger.info(f""id: {ex_idx}"")\r\n        logger.info(f""tokens: {\' \'.join([str(x) for x in instance[\'tokens\']])}"")\r\n        logger.info(f""masked_lm_labels: {\' \'.join([str(x) for x in instance[\'masked_lm_labels\']])}"")\r\n        logger.info(f""segment_ids: {\' \'.join([str(x) for x in instance[\'segment_ids\']])}"")\r\n        logger.info(f""masked_lm_positions: {\' \'.join([str(x) for x in instance[\'masked_lm_positions\']])}"")\r\n        logger.info(f""is_random_next : {instance[\'is_random_next\']}"")\r\n        ex_idx += 1\r\n    random.shuffle(instances)\r\n    return instances\r\n\r\n\r\ndef main():\r\n    parser = ArgumentParser()\r\n    ## Required parameters\r\n    parser.add_argument(""--data_dir"", default=None, type=str, required=True)\r\n    parser.add_argument(""--vocab_path"", default=None, type=str, required=True)\r\n    parser.add_argument(""--output_dir"", default=None, type=str, required=True)\r\n\r\n    parser.add_argument(\'--data_name\', default=\'albert\', type=str)\r\n    parser.add_argument(\'--max_ngram\', default=3, type=int)\r\n    parser.add_argument(""--do_data"", default=False, action=\'store_true\')\r\n    parser.add_argument(""--do_split"", default=False, action=\'store_true\')\r\n    parser.add_argument(""--do_lower_case"", default=False, action=\'store_true\')\r\n    parser.add_argument(\'--seed\', default=42, type=int)\r\n    parser.add_argument(""--line_per_file"", default=1000000000, type=int)\r\n    parser.add_argument(""--file_num"", type=int, default=10,\r\n                        help=""Number of dynamic masking to pregenerate (with different masks)"")\r\n    parser.add_argument(""--max_seq_len"", type=int, default=128)\r\n    parser.add_argument(""--short_seq_prob"", type=float, default=0.1,\r\n                        help=""Probability of making a short sentence as a training example"")\r\n    parser.add_argument(""--masked_lm_prob"", type=float, default=0.15,\r\n                        help=""Probability of masking each token for the LM task"")\r\n    parser.add_argument(""--max_predictions_per_seq"", type=int, default=20,  # 128 * 0.15\r\n                        help=""Maximum number of tokens to mask in each sequence"")\r\n    args = parser.parse_args()\r\n    seed_everything(args.seed)\r\n    args.data_dir = Path(args.data_dir)\r\n    if not os.path.exists(args.output_dir):\r\n        os.mkdir(args.output_dir)\r\n    init_logger(log_file=args.output_dir +""pregenerate_training_data_ngram.log"")\r\n    logger.info(""pregenerate training data parameters:\\n %s"", args)\r\n    tokenizer = BertTokenizer(vocab_file=args.vocab_path, do_lower_case=args.do_lower_case)\r\n\r\n    # split big file\r\n    if args.do_split:\r\n        corpus_path =args.data_dir / ""corpus/corpus.txt""\r\n        split_save_path = args.data_dir / ""/corpus/train""\r\n        if not split_save_path.exists():\r\n            split_save_path.mkdir(exist_ok=True)\r\n        line_per_file = args.line_per_file\r\n        command = f\'split -a 4 -l {line_per_file} -d {corpus_path} {split_save_path}/shard_\'\r\n        os.system(f""{command}"")\r\n\r\n    # generator train data\r\n    if args.do_data:\r\n        data_path = args.data_dir / ""corpus/train""\r\n        files = sorted([f for f in data_path.parent.iterdir() if f.exists() and \'.txt\' in str(f)])\r\n        for idx in range(args.file_num):\r\n            logger.info(f""pregenetate {args.data_name}_file_{idx}.json"")\r\n            save_filename = data_path / f""{args.data_name}_file_{idx}.json""\r\n            num_instances = 0\r\n            with save_filename.open(\'w\') as fw:\r\n                for file_idx in range(len(files)):\r\n                    file_path = files[file_idx]\r\n                    file_examples = create_training_instances(input_file=file_path,\r\n                                                              tokenizer=tokenizer,\r\n                                                              max_seq_len=args.max_seq_len,\r\n                                                              max_ngram=args.max_ngram,\r\n                                                              short_seq_prob=args.short_seq_prob,\r\n                                                              masked_lm_prob=args.masked_lm_prob,\r\n                                                              max_predictions_per_seq=args.max_predictions_per_seq)\r\n                    file_examples = [json.dumps(instance) for instance in file_examples]\r\n                    for instance in file_examples:\r\n                        fw.write(instance + \'\\n\')\r\n                        num_instances += 1\r\n            metrics_file = data_path / f""{args.data_name}_file_{idx}_metrics.json""\r\n            print(f""num_instances: {num_instances}"")\r\n            with metrics_file.open(\'w\') as metrics_file:\r\n                metrics = {\r\n                    ""num_training_examples"": num_instances,\r\n                    ""max_seq_len"": args.max_seq_len\r\n                }\r\n                metrics_file.write(json.dumps(metrics))\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\n\r\n\'\'\'\r\npython prepare_lm_data_ngram.py \\\r\n    --data_dir=dataset/ \\\r\n    --vocab_path=vocab.txt \\\r\n    --output_dir=outputs/ \\\r\n    --data_name=albert \\\r\n    --max_ngram=3 \\\r\n    --do_data\r\n\'\'\'\r\n'"
run_classifier.py,30,"b'"""""" Finetuning the library models for sequence classification .""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport argparse\nimport os\nimport glob\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\nfrom torch.utils.data.distributed import DistributedSampler\n\nfrom model.modeling_albert import AlbertConfig, AlbertForSequenceClassification\n# from model.modeling_albert_bright import AlbertConfig, AlbertForSequenceClassification # chinese version\nfrom model import tokenization_albert\nfrom model.file_utils import WEIGHTS_NAME\nfrom callback.optimization.adamw import AdamW\nfrom callback.lr_scheduler import get_linear_schedule_with_warmup\n\nfrom metrics.glue_compute_metrics import compute_metrics\nfrom processors import glue_output_modes as output_modes\nfrom processors import glue_processors as processors\nfrom processors import glue_convert_examples_to_features as convert_examples_to_features\nfrom processors import collate_fn\nfrom tools.common import seed_everything\nfrom tools.common import init_logger, logger\nfrom callback.progressbar import ProgressBar\n\ndef train(args, train_dataset, model, tokenizer):\n    """""" Train the model """"""\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size,\n                                  collate_fn=collate_fn)\n\n    if args.max_steps > 0:\n        num_training_steps = args.max_steps\n        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n    else:\n        num_training_steps = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n    args.warmup_steps = int(num_training_steps * args.warmup_proportion)\n    # Prepare optimizer and schedule (linear warmup and decay)\n    no_decay = [\'bias\', \'LayerNorm.weight\']\n    optimizer_grouped_parameters = [\n        {\'params\': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n         \'weight_decay\': args.weight_decay},\n        {\'params\': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \'weight_decay\': 0.0}\n    ]\n    # optimizer = Lamb(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    optimizer = AdamW(params=optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=num_training_steps)\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(""Please install apex from https://www.github.com/nvidia/apex to use fp16 training."")\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n\n    # multi-gpu training (should be after apex fp16 initialization)\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n\n    # Distributed training (should be after apex fp16 initialization)\n    if args.local_rank != -1:\n        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n                                                          output_device=args.local_rank,\n                                                          find_unused_parameters=True)\n\n    # Train!\n    logger.info(""***** Running training *****"")\n    logger.info(""  Num examples = %d"", len(train_dataset))\n    logger.info(""  Num Epochs = %d"", args.num_train_epochs)\n    logger.info(""  Instantaneous batch size per GPU = %d"", args.per_gpu_train_batch_size)\n    logger.info(""  Total train batch size (w. parallel, distributed & accumulation) = %d"",\n                args.train_batch_size * args.gradient_accumulation_steps * (\n                    torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n    logger.info(""  Gradient Accumulation steps = %d"", args.gradient_accumulation_steps)\n    logger.info(""  Total optimization steps = %d"", num_training_steps)\n\n    global_step = 0\n    tr_loss, logging_loss = 0.0, 0.0\n    model.zero_grad()\n    seed_everything(args.seed)  # Added here for reproductibility (even between python 2 and 3)\n    for _ in range(int(args.num_train_epochs)):\n        pbar = ProgressBar(n_total=len(train_dataloader), desc=\'Training\')\n        for step, batch in enumerate(train_dataloader):\n            model.train()\n            batch = tuple(t.to(args.device) for t in batch)\n            inputs = {\'input_ids\': batch[0],\n                      \'attention_mask\': batch[1],\n                      \'labels\': batch[3]}\n            inputs[\'token_type_ids\'] = batch[2]\n            outputs = model(**inputs)\n            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n\n            if args.n_gpu > 1:\n                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n            else:\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                optimizer.step()\n                scheduler.step()  # Update learning rate schedule\n                model.zero_grad()\n                global_step += 1\n\n            if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n                #Log metrics\n                if args.local_rank == -1:  # Only evaluate when single GPU otherwise metrics may not average well\n                    evaluate(args, model, tokenizer)\n\n            if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n                # Save model checkpoint\n                output_dir = os.path.join(args.output_dir, \'checkpoint-{}\'.format(global_step))\n                if not os.path.exists(output_dir):\n                    os.makedirs(output_dir)\n                model_to_save = model.module if hasattr(model,\n                                                        \'module\') else model  # Take care of distributed/parallel training\n                model_to_save.save_pretrained(output_dir)\n                torch.save(args, os.path.join(output_dir, \'training_args.bin\'))\n                logger.info(""Saving model checkpoint to %s"", output_dir)\n            pbar(step, {\'loss\': loss.item()})\n        print("" "")\n        if \'cuda\' in str(args.device):\n            torch.cuda.empty_cache()\n    return global_step, tr_loss / global_step\n\ndef evaluate(args, model, tokenizer, prefix=""""):\n    # Loop to handle MNLI double evaluation (matched, mis-matched)\n    eval_task_names = (""mnli"", ""mnli-mm"") if args.task_name == ""mnli"" else (args.task_name,)\n    eval_outputs_dirs = (args.output_dir, args.output_dir + \'-MM\') if args.task_name == ""mnli"" else (args.output_dir,)\n\n    results = {}\n    for eval_task, eval_output_dir in zip(eval_task_names, eval_outputs_dirs):\n        eval_dataset = load_and_cache_examples(args, eval_task, tokenizer, data_type=\'dev\')\n        if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n            os.makedirs(eval_output_dir)\n\n        args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n        # Note that DistributedSampler samples randomly\n        eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size,\n                                     collate_fn=collate_fn)\n\n        # Eval!\n        logger.info(""***** Running evaluation {} *****"".format(prefix))\n        logger.info(""  Num examples = %d"", len(eval_dataset))\n        logger.info(""  Batch size = %d"", args.eval_batch_size)\n        eval_loss = 0.0\n        nb_eval_steps = 0\n        preds = None\n        out_label_ids = None\n        pbar = ProgressBar(n_total=len(eval_dataloader), desc=""Evaluating"")\n        for step, batch in enumerate(eval_dataloader):\n            model.eval()\n            batch = tuple(t.to(args.device) for t in batch)\n            with torch.no_grad():\n                inputs = {\'input_ids\': batch[0],\n                          \'attention_mask\': batch[1],\n                          \'labels\': batch[3]}\n                inputs[\'token_type_ids\'] = batch[2]\n                outputs = model(**inputs)\n                tmp_eval_loss, logits = outputs[:2]\n                eval_loss += tmp_eval_loss.mean().item()\n            nb_eval_steps += 1\n            if preds is None:\n                preds = logits.detach().cpu().numpy()\n                out_label_ids = inputs[\'labels\'].detach().cpu().numpy()\n            else:\n                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n                out_label_ids = np.append(out_label_ids, inputs[\'labels\'].detach().cpu().numpy(), axis=0)\n            pbar(step)\n        print(\' \')\n        if \'cuda\' in str(args.device):\n            torch.cuda.empty_cache()\n        eval_loss = eval_loss / nb_eval_steps\n        if args.output_mode == ""classification"":\n            preds = np.argmax(preds, axis=1)\n        elif args.output_mode == ""regression"":\n            preds = np.squeeze(preds)\n        result = compute_metrics(eval_task, preds, out_label_ids)\n        results.update(result)\n        logger.info(""***** Eval results {} *****"".format(prefix))\n        for key in sorted(result.keys()):\n            logger.info(""  %s = %s"", key, str(result[key]))\n    return results\n\ndef load_and_cache_examples(args, task, tokenizer, data_type=\'train\'):\n    if args.local_rank not in [-1, 0] and not evaluate:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n\n    processor = processors[task]()\n    output_mode = output_modes[task]\n    # Load data features from cache or dataset file\n    cached_features_file = os.path.join(args.data_dir, \'cached_{}_{}_{}_{}\'.format(\n        data_type,\n        list(filter(None, args.model_name_or_path.split(\'/\'))).pop(),\n        str(args.max_seq_length),\n        str(task)))\n    if os.path.exists(cached_features_file):\n        logger.info(""Loading features from cached file %s"", cached_features_file)\n        features = torch.load(cached_features_file)\n    else:\n        logger.info(""Creating features from dataset file at %s"", args.data_dir)\n        label_list = processor.get_labels()\n        if task in [\'mnli\', \'mnli-mm\'] and \'roberta\' in args.model_type:\n            # HACK(label indices are swapped in RoBERTa pretrained model)\n            label_list[1], label_list[2] = label_list[2], label_list[1]\n\n        if data_type == \'train\':\n            examples = processor.get_train_examples(args.data_dir)\n        elif data_type == \'dev\':\n            examples = processor.get_dev_examples(args.data_dir)\n        else:\n            examples = processor.get_test_examples(args.data_dir)\n\n        features = convert_examples_to_features(examples,\n                                                tokenizer,\n                                                label_list=label_list,\n                                                max_seq_length=args.max_seq_length,\n                                                output_mode = output_mode)\n        if args.local_rank in [-1, 0]:\n            logger.info(""Saving features into cached file %s"", cached_features_file)\n            torch.save(features, cached_features_file)\n\n    if args.local_rank == 0 and not evaluate:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n    # Convert to Tensors and build dataset\n    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n    all_lens = torch.tensor([f.input_len for f in features], dtype=torch.long)\n    if output_mode == ""classification"":\n        all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n    elif output_mode == ""regression"":\n        all_labels = torch.tensor([f.label for f in features], dtype=torch.float)\n    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_lens, all_labels)\n    return dataset\n\ndef main():\n    parser = argparse.ArgumentParser()\n\n    ## Required parameters\n    parser.add_argument(""--data_dir"", default=None, type=str, required=True,\n                        help=""The input data dir. Should contain the .tsv files (or other data files) for the task."")\n    parser.add_argument(""--model_type"", default=None, type=str, required=True,\n                        help=""Model type selected in the list: "")\n    parser.add_argument(""--model_name_or_path"", default=None, type=str, required=True,\n                        help=""Path to pre-trained model or shortcut name selected in the list"")\n    parser.add_argument(""--task_name"", default=None, type=str, required=True,\n                        help=""The name of the task to train selected in the list: "" + "", "".join(processors.keys()))\n    parser.add_argument(""--output_dir"", default=None, type=str, required=True,\n                        help=""The output directory where the model predictions and checkpoints will be written."")\n    parser.add_argument(""--vocab_file"",default=\'\', type=str)\n    parser.add_argument(""--spm_model_file"",default=\'\',type=str)\n\n    ## Other parameters\n    parser.add_argument(""--config_name"", default="""", type=str,\n                        help=""Pretrained config name or path if not the same as model_name"")\n    parser.add_argument(""--tokenizer_name"", default="""", type=str,\n                        help=""Pretrained tokenizer name or path if not the same as model_name"")\n    parser.add_argument(""--cache_dir"", default="""", type=str,\n                        help=""Where do you want to store the pre-trained models downloaded from s3"")\n    parser.add_argument(""--max_seq_length"", default=512, type=int,\n                        help=""The maximum total input sequence length after tokenization. Sequences longer ""\n                             ""than this will be truncated, sequences shorter will be padded."")\n    parser.add_argument(""--do_train"", action=\'store_true\',\n                        help=""Whether to run training."")\n    parser.add_argument(""--do_eval"", action=\'store_true\',\n                        help=""Whether to run eval on the dev set."")\n    parser.add_argument(""--do_predict"", action=\'store_true\',\n                        help=""Whether to run the model in inference mode on the test set."")\n    parser.add_argument(""--do_lower_case"", action=\'store_true\',\n                        help=""Set this flag if you are using an uncased model."")\n\n    parser.add_argument(""--per_gpu_train_batch_size"", default=8, type=int,\n                        help=""Batch size per GPU/CPU for training."")\n    parser.add_argument(""--per_gpu_eval_batch_size"", default=8, type=int,\n                        help=""Batch size per GPU/CPU for evaluation."")\n    parser.add_argument(\'--gradient_accumulation_steps\', type=int, default=1,\n                        help=""Number of updates steps to accumulate before performing a backward/update pass."")\n    parser.add_argument(""--learning_rate"", default=5e-5, type=float,\n                        help=""The initial learning rate for Adam."")\n    parser.add_argument(""--weight_decay"", default=0.0, type=float,\n                        help=""Weight deay if we apply some."")\n    parser.add_argument(""--adam_epsilon"", default=1e-6, type=float,\n                        help=""Epsilon for Adam optimizer."")\n    parser.add_argument(""--max_grad_norm"", default=1.0, type=float,\n                        help=""Max gradient norm."")\n    parser.add_argument(""--num_train_epochs"", default=3.0, type=float,\n                        help=""Total number of training epochs to perform."")\n    parser.add_argument(""--max_steps"", default=-1, type=int,\n                        help=""If > 0: set total number of training steps to perform. Override num_train_epochs."")\n    parser.add_argument(""--warmup_proportion"", default=0.1, type=float,\n                        help=""Proportion of training to perform linear learning rate warmup for,E.g., 0.1 = 10% of training."")\n\n    parser.add_argument(\'--logging_steps\', type=int, default=10,\n                        help=""Log every X updates steps."")\n    parser.add_argument(\'--save_steps\', type=int, default=1000,\n                        help=""Save checkpoint every X updates steps."")\n    parser.add_argument(""--eval_all_checkpoints"", action=\'store_true\',\n                        help=""Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number"")\n    parser.add_argument(""--no_cuda"", action=\'store_true\',\n                        help=""Avoid using CUDA when available"")\n    parser.add_argument(\'--overwrite_output_dir\', action=\'store_true\',\n                        help=""Overwrite the content of the output directory"")\n    parser.add_argument(\'--overwrite_cache\', action=\'store_true\',\n                        help=""Overwrite the cached training and evaluation sets"")\n    parser.add_argument(\'--seed\', type=int, default=42,\n                        help=""random seed for initialization"")\n\n    parser.add_argument(\'--fp16\', action=\'store_true\',\n                        help=""Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit"")\n    parser.add_argument(\'--fp16_opt_level\', type=str, default=\'O1\',\n                        help=""For fp16: Apex AMP optimization level selected in [\'O0\', \'O1\', \'O2\', and \'O3\'].""\n                             ""See details at https://nvidia.github.io/apex/amp.html"")\n    parser.add_argument(""--local_rank"", type=int, default=-1,\n                        help=""For distributed training: local_rank"")\n    parser.add_argument(\'--server_ip\', type=str, default=\'\', help=""For distant debugging."")\n    parser.add_argument(\'--server_port\', type=str, default=\'\', help=""For distant debugging."")\n    args = parser.parse_args()\n\n    if not os.path.exists(args.output_dir):\n        os.mkdir(args.output_dir)\n    args.output_dir = args.output_dir + \'{}\'.format(args.model_type)\n    if not os.path.exists(args.output_dir):\n        os.mkdir(args.output_dir)\n    init_logger(log_file=args.output_dir + \'/{}-{}.log\'.format(args.model_type, args.task_name))\n    if os.path.exists(args.output_dir) and os.listdir(\n            args.output_dir) and args.do_train and not args.overwrite_output_dir:\n        raise ValueError(\n            ""Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome."".format(\n                args.output_dir))\n\n    # Setup distant debugging if needed\n    if args.server_ip and args.server_port:\n        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n        import ptvsd\n        print(""Waiting for debugger attach"")\n        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n        ptvsd.wait_for_attach()\n\n    # Setup CUDA, GPU & distributed training\n    if args.local_rank == -1 or args.no_cuda:\n        device = torch.device(""cuda"" if torch.cuda.is_available() and not args.no_cuda else ""cpu"")\n        args.n_gpu = torch.cuda.device_count()\n    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n        torch.cuda.set_device(args.local_rank)\n        device = torch.device(""cuda"", args.local_rank)\n        torch.distributed.init_process_group(backend=\'nccl\')\n        args.n_gpu = 1\n    args.device = device\n    # Setup logging\n    logger.warning(""Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s"",\n                   args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n    # Set seed\n    seed_everything(args.seed)\n    # Prepare GLUE task\n    args.task_name = args.task_name.lower()\n    if args.task_name not in processors:\n        raise ValueError(""Task not found: %s"" % (args.task_name))\n    processor = processors[args.task_name]()\n    args.output_mode = output_modes[args.task_name]\n    label_list = processor.get_labels()\n    num_labels = len(label_list)\n\n    # Load pretrained model and tokenizer\n    if args.local_rank not in [-1, 0]:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n\n    args.model_type = args.model_type.lower()\n    config = AlbertConfig.from_pretrained(args.config_name if args.config_name else args.model_name_or_path,\n                                          num_labels=num_labels,\n                                          finetuning_task=args.task_name)\n    tokenizer = tokenization_albert.FullTokenizer(vocab_file=args.vocab_file, do_lower_case=args.do_lower_case,\n                                                 spm_model_file=args.spm_model_file)\n    model =AlbertForSequenceClassification.from_pretrained(args.model_name_or_path,\n                                                           from_tf=bool(\'.ckpt\' in args.model_name_or_path),\n                                                            config=config)\n    if args.local_rank == 0:\n        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n    model.to(args.device)\n    logger.info(""Training/evaluation parameters %s"", args)\n\n    # Training\n    if args.do_train:\n        train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, data_type=\'train\')\n        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n        logger.info("" global_step = %s, average loss = %s"", global_step, tr_loss)\n\n    # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n        # Create output directory if needed\n        if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n            os.makedirs(args.output_dir)\n\n        logger.info(""Saving model checkpoint to %s"", args.output_dir)\n        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n        # They can then be reloaded using `from_pretrained()`\n        model_to_save = model.module if hasattr(model,\n                                                \'module\') else model  # Take care of distributed/parallel training\n        model_to_save.save_pretrained(args.output_dir)\n        # Good practice: save your training arguments together with the trained model\n        torch.save(args, os.path.join(args.output_dir, \'training_args.bin\'))\n\n    # Evaluation\n    results = []\n    if args.do_eval and args.local_rank in [-1, 0]:\n        tokenizer = tokenization_albert.FullTokenizer(vocab_file=args.vocab_file,\n                                                      do_lower_case=args.do_lower_case,\n                                                      spm_model_file=args.spm_model_file)\n        checkpoints = [(0,args.output_dir)]\n        if args.eval_all_checkpoints:\n            checkpoints = list(\n                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \'/**/\' + WEIGHTS_NAME, recursive=True)))\n            checkpoints = [(int(checkpoint.split(\'-\')[-1]),checkpoint) for checkpoint in checkpoints if checkpoint.find(\'checkpoint\') != -1]\n            checkpoints = sorted(checkpoints,key =lambda x:x[0])\n        logger.info(""Evaluate the following checkpoints: %s"", checkpoints)\n        for _,checkpoint in checkpoints:\n            global_step = checkpoint.split(\'-\')[-1] if len(checkpoints) > 1 else """"\n            prefix = checkpoint.split(\'/\')[-1] if checkpoint.find(\'checkpoint\') != -1 else """"\n\n            model =AlbertForSequenceClassification.from_pretrained(checkpoint)\n            model.to(args.device)\n            result = evaluate(args, model, tokenizer, prefix=prefix)\n            results.extend([(k + \'_{}\'.format(global_step), v) for k, v in result.items()])\n        output_eval_file = os.path.join(args.output_dir, ""checkpoint_eval_results.txt"")\n        with open(output_eval_file, ""w"") as writer:\n            for key,value in results:\n                writer.write(""%s = %s\\n"" % (key, str(value)))\n\nif __name__ == ""__main__"":\n    main()\n'"
run_pretraining.py,21,"b'import torch\r\nimport json\r\nimport time\r\nimport numpy as np\r\nfrom pathlib import Path\r\nfrom argparse import ArgumentParser\r\nfrom collections import namedtuple\r\nfrom tempfile import TemporaryDirectory\r\nfrom tools.common import logger, init_logger\r\nfrom torch.utils.data import DataLoader, Dataset, RandomSampler\r\nfrom torch.utils.data.distributed import DistributedSampler\r\nfrom tools.common import AverageMeter\r\nfrom metrics.custom_metrics import LMAccuracy\r\nfrom torch.nn import CrossEntropyLoss\r\nfrom model.modeling_albert import AlbertForPreTraining, AlbertConfig\r\nfrom model.file_utils import CONFIG_NAME\r\nfrom model.tokenization_bert import BertTokenizer\r\nfrom callback.optimization.adamw import AdamW\r\nfrom callback.lr_scheduler import get_linear_schedule_with_warmup\r\nfrom tools.common import seed_everything\r\n\r\nInputFeatures = namedtuple(""InputFeatures"", ""input_ids input_mask segment_ids lm_label_ids is_next"")\r\n\r\ndef convert_example_to_features(example, tokenizer, max_seq_length):\r\n    tokens = example[""tokens""]\r\n    segment_ids = example[""segment_ids""]\r\n    is_random_next = example[""is_random_next""]\r\n    masked_lm_positions = example[""masked_lm_positions""]\r\n    masked_lm_labels = example[""masked_lm_labels""]\r\n\r\n    assert len(tokens) == len(segment_ids) <= max_seq_length  # The preprocessed data should be already truncated\r\n    input_ids = tokenizer.convert_tokens_to_ids(tokens)\r\n    masked_label_ids = tokenizer.convert_tokens_to_ids(masked_lm_labels)\r\n    input_array = np.zeros(max_seq_length, dtype=np.int)\r\n    input_array[:len(input_ids)] = input_ids\r\n    mask_array = np.zeros(max_seq_length, dtype=np.bool)\r\n    mask_array[:len(input_ids)] = 1\r\n    segment_array = np.zeros(max_seq_length, dtype=np.bool)\r\n    segment_array[:len(segment_ids)] = segment_ids\r\n    lm_label_array = np.full(max_seq_length, dtype=np.int, fill_value=-1)\r\n    lm_label_array[masked_lm_positions] = masked_label_ids\r\n\r\n    features = InputFeatures(input_ids=input_array,\r\n                             input_mask=mask_array,\r\n                             segment_ids=segment_array,\r\n                             lm_label_ids=lm_label_array,\r\n                             is_next=is_random_next)\r\n    return features\r\n\r\nclass PregeneratedDataset(Dataset):\r\n    def __init__(self, training_path, file_id, tokenizer, data_name, reduce_memory=False):\r\n        self.tokenizer = tokenizer\r\n        self.file_id = file_id\r\n        data_file = training_path / f""{data_name}_file_{self.file_id}.json""\r\n        metrics_file = training_path / f""{data_name}_file_{self.file_id}_metrics.json""\r\n        assert data_file.is_file() and metrics_file.is_file()\r\n        metrics = json.loads(metrics_file.read_text())\r\n        num_samples = metrics[\'num_training_examples\']\r\n        seq_len = metrics[\'max_seq_len\']\r\n        self.temp_dir = None\r\n        self.working_dir = None\r\n        if reduce_memory:\r\n            self.temp_dir = TemporaryDirectory()\r\n            self.working_dir = Path(self.temp_dir.name)\r\n            input_ids = np.memmap(filename=self.working_dir / \'input_ids.memmap\',\r\n                                  mode=\'w+\', dtype=np.int32, shape=(num_samples, seq_len))\r\n            input_masks = np.memmap(filename=self.working_dir / \'input_masks.memmap\',\r\n                                    shape=(num_samples, seq_len), mode=\'w+\', dtype=np.bool)\r\n            segment_ids = np.memmap(filename=self.working_dir / \'segment_ids.memmap\',\r\n                                    shape=(num_samples, seq_len), mode=\'w+\', dtype=np.bool)\r\n            lm_label_ids = np.memmap(filename=self.working_dir / \'lm_label_ids.memmap\',\r\n                                     shape=(num_samples, seq_len), mode=\'w+\', dtype=np.int32)\r\n            lm_label_ids[:] = -1\r\n            is_nexts = np.memmap(filename=self.working_dir / \'is_nexts.memmap\',\r\n                                 shape=(num_samples,), mode=\'w+\', dtype=np.bool)\r\n        else:\r\n            input_ids = np.zeros(shape=(num_samples, seq_len), dtype=np.int32)\r\n            input_masks = np.zeros(shape=(num_samples, seq_len), dtype=np.bool)\r\n            segment_ids = np.zeros(shape=(num_samples, seq_len), dtype=np.bool)\r\n            lm_label_ids = np.full(shape=(num_samples, seq_len), dtype=np.int32, fill_value=-1)\r\n            is_nexts = np.zeros(shape=(num_samples,), dtype=np.bool)\r\n        logger.info(f""Loading training examples for {str(data_file)}"")\r\n        with data_file.open() as f:\r\n            for i, line in enumerate(f):\r\n                line = line.strip()\r\n                example = json.loads(line)\r\n                features = convert_example_to_features(example, tokenizer, seq_len)\r\n                input_ids[i] = features.input_ids\r\n                segment_ids[i] = features.segment_ids\r\n                input_masks[i] = features.input_mask\r\n                lm_label_ids[i] = features.lm_label_ids\r\n                is_nexts[i] = features.is_next\r\n        assert i == num_samples - 1  # Assert that the sample count metric was true\r\n        logger.info(""Loading complete!"")\r\n        self.num_samples = num_samples\r\n        self.seq_len = seq_len\r\n        self.input_ids = input_ids\r\n        self.input_masks = input_masks\r\n        self.segment_ids = segment_ids\r\n        self.lm_label_ids = lm_label_ids\r\n        self.is_nexts = is_nexts\r\n\r\n    def __len__(self):\r\n        return self.num_samples\r\n\r\n    def __getitem__(self, item):\r\n        return (torch.tensor(self.input_ids[item].astype(np.int64)),\r\n                torch.tensor(self.input_masks[item].astype(np.int64)),\r\n                torch.tensor(self.segment_ids[item].astype(np.int64)),\r\n                torch.tensor(self.lm_label_ids[item].astype(np.int64)),\r\n                torch.tensor(self.is_nexts[item].astype(np.int64)))\r\ndef main():\r\n    parser = ArgumentParser()\r\n    ## Required parameters\r\n    parser.add_argument(""--data_dir"", default=None, type=str, required=True,\r\n                        help=""The input data dir. Should contain the .tsv files (or other data files) for the task."")\r\n    parser.add_argument(""--config_path"", default=None, type=str, required=True)\r\n    parser.add_argument(""--vocab_path"",default=None,type=str,required=True)\r\n    parser.add_argument(""--output_dir"", default=None, type=str, required=True,\r\n                        help=""The output directory where the model predictions and checkpoints will be written."")\r\n    parser.add_argument(""--model_path"", default=\'\', type=str)\r\n    parser.add_argument(\'--data_name\', default=\'albert\', type=str)\r\n    parser.add_argument(""--file_num"", type=int, default=10,\r\n                        help=""Number of dynamic masking to pregenerate (with different masks)"")\r\n    parser.add_argument(""--reduce_memory"", action=""store_true"",\r\n                        help=""Store training data as on-disc memmaps to massively reduce memory usage"")\r\n    parser.add_argument(""--epochs"", type=int, default=4,\r\n                        help=""Number of epochs to train for"")\r\n    parser.add_argument(""--do_lower_case"", action=\'store_true\',\r\n                        help=""Set this flag if you are using an uncased model."")\r\n\r\n    parser.add_argument(\'--num_eval_steps\', default=1000)\r\n    parser.add_argument(\'--num_save_steps\', default=2000)\r\n    parser.add_argument(""--local_rank"", type=int, default=-1,\r\n                        help=""local_rank for distributed training on gpus"")\r\n    parser.add_argument(""--weight_decay"", default=0.01, type=float,\r\n                        help=""Weight deay if we apply some."")\r\n    parser.add_argument(""--no_cuda"", action=\'store_true\',\r\n                        help=""Whether not to use CUDA when available"")\r\n    parser.add_argument(\'--gradient_accumulation_steps\', type=int, default=1,\r\n                        help=""Number of updates steps to accumulate before performing a backward/update pass."")\r\n    parser.add_argument(""--train_batch_size"", default=16, type=int,\r\n                        help=""Total batch size for training."")\r\n    parser.add_argument(\'--loss_scale\', type=float, default=0,\r\n                        help=""Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n""\r\n                             ""0 (default value): dynamic loss scaling.\\n""\r\n                             ""Positive power of 2: static loss scaling value.\\n"")\r\n    parser.add_argument(""--warmup_proportion"", default=0.1, type=float,\r\n                        help=""Linear warmup over warmup_steps."")\r\n    parser.add_argument(""--adam_epsilon"", default=1e-8, type=float,\r\n                        help=""Epsilon for Adam optimizer."")\r\n    parser.add_argument(\'--max_grad_norm\', default=1.0, type=float)\r\n    parser.add_argument(""--learning_rate"", default=0.000176, type=float,\r\n                        help=""The initial learning rate for Adam."")\r\n    parser.add_argument(\'--seed\', type=int, default=42,\r\n                        help=""random seed for initialization"")\r\n    parser.add_argument(\'--fp16_opt_level\', type=str, default=\'O2\',\r\n                        help=""For fp16: Apex AMP optimization level selected in [\'O0\', \'O1\', \'O2\', and \'O3\'].""\r\n                             ""See details at https://nvidia.github.io/apex/amp.html"")\r\n    parser.add_argument(\'--fp16\', action=\'store_true\',\r\n                        help=""Whether to use 16-bit float precision instead of 32-bit"")\r\n    args = parser.parse_args()\r\n\r\n    args.data_dir = Path(args.data_dir)\r\n    args.output_dir = Path(args.output_dir)\r\n\r\n    pregenerated_data = args.data_dir / ""corpus/train""\r\n    init_logger(log_file=str(args.output_dir/ ""train_albert_model.log""))\r\n    assert pregenerated_data.is_dir(), \\\r\n        ""--pregenerated_data should point to the folder of files made by prepare_lm_data_mask.py!""\r\n\r\n    samples_per_epoch = 0\r\n    for i in range(args.file_num):\r\n        data_file = pregenerated_data / f""{args.data_name}_file_{i}.json""\r\n        metrics_file = pregenerated_data / f""{args.data_name}_file_{i}_metrics.json""\r\n        if data_file.is_file() and metrics_file.is_file():\r\n            metrics = json.loads(metrics_file.read_text())\r\n            samples_per_epoch += metrics[\'num_training_examples\']\r\n        else:\r\n            if i == 0:\r\n                exit(""No training data was found!"")\r\n            print(f""Warning! There are fewer epochs of pregenerated data ({i}) than training epochs ({args.epochs})."")\r\n            print(""This script will loop over the available data, but training diversity may be negatively impacted."")\r\n            break\r\n    logger.info(f""samples_per_epoch: {samples_per_epoch}"")\r\n    if args.local_rank == -1 or args.no_cuda:\r\n        device = torch.device(f""cuda"" if torch.cuda.is_available() and not args.no_cuda else ""cpu"")\r\n        args.n_gpu = torch.cuda.device_count()\r\n    else:\r\n        torch.cuda.set_device(args.local_rank)\r\n        device = torch.device(""cuda"", args.local_rank)\r\n        args.n_gpu = 1\r\n        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\r\n        torch.distributed.init_process_group(backend=\'nccl\')\r\n    logger.info(\r\n        f""device: {device} , distributed training: {bool(args.local_rank != -1)}, 16-bits training: {args.fp16}"")\r\n\r\n    if args.gradient_accumulation_steps < 1:\r\n        raise ValueError(\r\n            f""Invalid gradient_accumulation_steps parameter: {args.gradient_accumulation_steps}, should be >= 1"")\r\n    args.train_batch_size = args.train_batch_size // args.gradient_accumulation_steps\r\n\r\n    seed_everything(args.seed)\r\n    tokenizer = BertTokenizer.from_pretrained(args.vocab_path, do_lower_case=args.do_lower_case)\r\n    total_train_examples = samples_per_epoch * args.epochs\r\n\r\n    num_train_optimization_steps = int(\r\n        total_train_examples / args.train_batch_size / args.gradient_accumulation_steps)\r\n    if args.local_rank != -1:\r\n        num_train_optimization_steps = num_train_optimization_steps // torch.distributed.get_world_size()\r\n    args.warmup_steps = int(num_train_optimization_steps * args.warmup_proportion)\r\n\r\n    bert_config = AlbertConfig.from_pretrained(args.config_path)\r\n    model = AlbertForPreTraining(config=bert_config)\r\n    if args.model_path:\r\n        model = AlbertForPreTraining.from_pretrained(args.model_path)\r\n    model.to(device)\r\n    # Prepare optimizer\r\n    param_optimizer = list(model.named_parameters())\r\n    no_decay = [\'bias\', \'LayerNorm.bias\', \'LayerNorm.weight\']\r\n    optimizer_grouped_parameters = [\r\n        {\'params\': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \'weight_decay\': args.weight_decay},\r\n        {\'params\': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \'weight_decay\': 0.0}\r\n    ]\r\n    optimizer = AdamW(params=optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\r\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=num_train_optimization_steps)\r\n    # optimizer = Lamb(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\r\n    if args.model_path:\r\n        optimizer.load_state_dict(torch.load(args.model_path + ""/optimizer.bin""))\r\n    if args.fp16:\r\n        try:\r\n            from apex import amp\r\n        except ImportError:\r\n            raise ImportError(""Please install apex from https://www.github.com/nvidia/apex to use fp16 training."")\r\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\r\n\r\n    if args.n_gpu > 1:\r\n        model = torch.nn.DataParallel(model)\r\n\r\n    if args.local_rank != -1:\r\n        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\r\n                                                          output_device=args.local_rank)\r\n    global_step = 0\r\n    mask_metric = LMAccuracy()\r\n    sop_metric = LMAccuracy()\r\n    tr_mask_acc = AverageMeter()\r\n    tr_sop_acc = AverageMeter()\r\n    tr_loss = AverageMeter()\r\n    tr_mask_loss = AverageMeter()\r\n    tr_sop_loss = AverageMeter()\r\n    loss_fct = CrossEntropyLoss(ignore_index=-1)\r\n\r\n    train_logs = {}\r\n    logger.info(""***** Running training *****"")\r\n    logger.info(f""  Num examples = {total_train_examples}"")\r\n    logger.info(f""  Batch size = {args.train_batch_size}"")\r\n    logger.info(f""  Num steps = {num_train_optimization_steps}"")\r\n    logger.info(f""  warmup_steps = {args.warmup_steps}"")\r\n    start_time = time.time()\r\n    seed_everything(args.seed)  # Added here for reproducibility\r\n    for epoch in range(args.epochs):\r\n        for idx in range(args.file_num):\r\n            epoch_dataset = PregeneratedDataset(file_id=idx, training_path=pregenerated_data, tokenizer=tokenizer,\r\n                                                reduce_memory=args.reduce_memory, data_name=args.data_name)\r\n            if args.local_rank == -1:\r\n                train_sampler = RandomSampler(epoch_dataset)\r\n            else:\r\n                train_sampler = DistributedSampler(epoch_dataset)\r\n            train_dataloader = DataLoader(epoch_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\r\n            model.train()\r\n            nb_tr_examples, nb_tr_steps = 0, 0\r\n            for step, batch in enumerate(train_dataloader):\r\n                batch = tuple(t.to(device) for t in batch)\r\n                input_ids, input_mask, segment_ids, lm_label_ids, is_next = batch\r\n                outputs = model(input_ids=input_ids, token_type_ids=segment_ids, attention_mask=input_mask)\r\n                prediction_scores = outputs[0]\r\n                seq_relationship_score = outputs[1]\r\n\r\n                masked_lm_loss = loss_fct(prediction_scores.view(-1, bert_config.vocab_size), lm_label_ids.view(-1))\r\n                next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), is_next.view(-1))\r\n                loss = masked_lm_loss + next_sentence_loss\r\n\r\n                mask_metric(logits=prediction_scores.view(-1, bert_config.vocab_size), target=lm_label_ids.view(-1))\r\n                sop_metric(logits=seq_relationship_score.view(-1, 2), target=is_next.view(-1))\r\n\r\n                if args.n_gpu > 1:\r\n                    loss = loss.mean()  # mean() to average on multi-gpu.\r\n                if args.gradient_accumulation_steps > 1:\r\n                    loss = loss / args.gradient_accumulation_steps\r\n                if args.fp16:\r\n                    with amp.scale_loss(loss, optimizer) as scaled_loss:\r\n                        scaled_loss.backward()\r\n                else:\r\n                    loss.backward()\r\n\r\n                nb_tr_steps += 1\r\n                tr_mask_acc.update(mask_metric.value(), n=input_ids.size(0))\r\n                tr_sop_acc.update(sop_metric.value(), n=input_ids.size(0))\r\n                tr_loss.update(loss.item(), n=1)\r\n                tr_mask_loss.update(masked_lm_loss.item(), n=1)\r\n                tr_sop_loss.update(next_sentence_loss.item(), n=1)\r\n\r\n                if (step + 1) % args.gradient_accumulation_steps == 0:\r\n                    if args.fp16:\r\n                        torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\r\n                    else:\r\n                        torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\r\n                    scheduler.step()\r\n                    optimizer.step()\r\n                    optimizer.zero_grad()\r\n                    global_step += 1\r\n\r\n                if global_step % args.num_eval_steps == 0:\r\n                    now = time.time()\r\n                    eta = now - start_time\r\n                    if eta > 3600:\r\n                        eta_format = (\'%d:%02d:%02d\' % (eta // 3600, (eta % 3600) // 60, eta % 60))\r\n                    elif eta > 60:\r\n                        eta_format = \'%d:%02d\' % (eta // 60, eta % 60)\r\n                    else:\r\n                        eta_format = \'%ds\' % eta\r\n                    train_logs[\'loss\'] = tr_loss.avg\r\n                    train_logs[\'mask_acc\'] = tr_mask_acc.avg\r\n                    train_logs[\'sop_acc\'] = tr_sop_acc.avg\r\n                    train_logs[\'mask_loss\'] = tr_mask_loss.avg\r\n                    train_logs[\'sop_loss\'] = tr_sop_loss.avg\r\n                    show_info = f\'[Training]:[{epoch}/{args.epochs}]{global_step}/{num_train_optimization_steps} \' \\\r\n                                f\'- ETA: {eta_format}\' + ""-"".join(\r\n                        [f\' {key}: {value:.4f} \' for key, value in train_logs.items()])\r\n                    logger.info(show_info)\r\n                    tr_mask_acc.reset()\r\n                    tr_sop_acc.reset()\r\n                    tr_loss.reset()\r\n                    tr_mask_loss.reset()\r\n                    tr_sop_loss.reset()\r\n                    start_time = now\r\n\r\n                if global_step % args.num_save_steps == 0:\r\n                    if args.local_rank in [-1, 0] and args.num_save_steps > 0:\r\n                        # Save model checkpoint\r\n                        output_dir = args.output_dir / f\'lm-checkpoint-{global_step}\'\r\n                        if not output_dir.exists():\r\n                            output_dir.mkdir()\r\n                        # save model\r\n                        model_to_save = model.module if hasattr(model,\r\n                                                                \'module\') else model  # Take care of distributed/parallel training\r\n                        model_to_save.save_pretrained(str(output_dir))\r\n                        torch.save(args, str(output_dir / \'training_args.bin\'))\r\n                        logger.info(""Saving model checkpoint to %s"", output_dir)\r\n\r\n                        torch.save(optimizer.state_dict(), str(output_dir / ""optimizer.bin""))\r\n                        # save config\r\n                        output_config_file = output_dir / CONFIG_NAME\r\n                        with open(str(output_config_file), \'w\') as f:\r\n                            f.write(model_to_save.config.to_json_string())\r\n                        # save vocab\r\n                        tokenizer.save_vocabulary(output_dir)\r\n'"
callback/__init__.py,0,b''
callback/lr_scheduler.py,3,"b'import math\r\nimport numpy as np\r\nimport warnings\r\nfrom torch.optim.optimizer import Optimizer\r\nfrom torch.optim.lr_scheduler import LambdaLR\r\n\r\n__all__ = [\'CustomDecayLR\',\r\n           \'BertLR\',\r\n           \'CyclicLR\',\r\n           \'ReduceLROnPlateau\',\r\n           \'ReduceLRWDOnPlateau\',\r\n           \'CosineLRWithRestarts\',\r\n           ]\r\n\r\ndef get_constant_schedule(optimizer, last_epoch=-1):\r\n    """""" Create a schedule with a constant learning rate.\r\n    """"""\r\n    return LambdaLR(optimizer, lambda _: 1, last_epoch=last_epoch)\r\n\r\n\r\ndef get_constant_schedule_with_warmup(optimizer, num_warmup_steps, last_epoch=-1):\r\n    """""" Create a schedule with a constant learning rate preceded by a warmup\r\n    period during which the learning rate increases linearly between 0 and 1.\r\n    """"""\r\n    def lr_lambda(current_step):\r\n        if current_step < num_warmup_steps:\r\n            return float(current_step) / float(max(1.0, num_warmup_steps))\r\n        return 1.\r\n\r\n    return LambdaLR(optimizer, lr_lambda, last_epoch=last_epoch)\r\n\r\n\r\ndef get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, last_epoch=-1):\r\n    """""" Create a schedule with a learning rate that decreases linearly after\r\n    linearly increasing during a warmup period.\r\n    """"""\r\n    def lr_lambda(current_step):\r\n        if current_step < num_warmup_steps:\r\n            return float(current_step) / float(max(1, num_warmup_steps))\r\n        return max(0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps)))\r\n\r\n    return LambdaLR(optimizer, lr_lambda, last_epoch)\r\n\r\n\r\ndef get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_cycles=.5, last_epoch=-1):\r\n    """""" Create a schedule with a learning rate that decreases following the\r\n    values of the cosine function between 0 and `pi * cycles` after a warmup\r\n    period during which it increases linearly between 0 and 1.\r\n    """"""\r\n    def lr_lambda(current_step):\r\n        if current_step < num_warmup_steps:\r\n            return float(current_step) / float(max(1, num_warmup_steps))\r\n        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\r\n        return max(0., 0.5 * (1. + math.cos(math.pi * float(num_cycles) * 2. * progress)))\r\n\r\n    return LambdaLR(optimizer, lr_lambda, last_epoch)\r\n\r\n\r\ndef get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, num_cycles=1., last_epoch=-1):\r\n    """""" Create a schedule with a learning rate that decreases following the\r\n    values of the cosine function with several hard restarts, after a warmup\r\n    period during which it increases linearly between 0 and 1.\r\n    """"""\r\n    def lr_lambda(current_step):\r\n        if current_step < num_warmup_steps:\r\n            return float(current_step) / float(max(1, num_warmup_steps))\r\n        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\r\n        if progress >= 1.:\r\n            return 0.\r\n        return max(0., 0.5 * (1. + math.cos(math.pi * ((float(num_cycles) * progress) % 1.))))\r\n\r\n    return LambdaLR(optimizer, lr_lambda, last_epoch)\r\n\r\n\r\nclass CustomDecayLR(object):\r\n    \'\'\'\r\n    \xe8\x87\xaa\xe5\xae\x9a\xe4\xb9\x89\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe5\x8f\x98\xe5\x8c\x96\xe6\x9c\xba\xe5\x88\xb6\r\n        Example:\r\n        >>> scheduler = CustomDecayLR(optimizer)\r\n        >>> for epoch in range(100):\r\n        >>>     scheduler.epoch_step()\r\n        >>>     train(...)\r\n        >>>         ...\r\n        >>>         optimizer.zero_grad()\r\n        >>>         loss.backward()\r\n        >>>         optimizer.step()\r\n        >>>     validate(...)\r\n    \'\'\'\r\n    def __init__(self,optimizer,lr):\r\n        self.optimizer = optimizer\r\n        self.lr = lr\r\n\r\n    def epoch_step(self,epoch):\r\n        lr = self.lr\r\n        if epoch > 12:\r\n            lr = lr / 1000\r\n        elif epoch > 8:\r\n            lr = lr / 100\r\n        elif epoch > 4:\r\n            lr = lr / 10\r\n        for param_group in self.optimizer.param_groups:\r\n            param_group[\'lr\'] = lr\r\n\r\nclass BertLR(object):\r\n    \'\'\'\r\n    Bert\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x86\x85\xe5\xae\x9a\xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xe7\x8e\x87\xe5\x8f\x98\xe5\x8c\x96\xe6\x9c\xba\xe5\x88\xb6\r\n    Example:\r\n        >>> scheduler = BertLR(optimizer)\r\n        >>> for epoch in range(100):\r\n        >>>     scheduler.step()\r\n        >>>     train(...)\r\n        >>>         ...\r\n        >>>         optimizer.zero_grad()\r\n        >>>         loss.backward()\r\n        >>>         optimizer.step()\r\n        >>>         scheduler.batch_step()\r\n        >>>     validate(...)\r\n    \'\'\'\r\n    def __init__(self,optimizer,learning_rate,t_total,warmup):\r\n        self.learning_rate = learning_rate\r\n        self.optimizer = optimizer\r\n        self.t_total = t_total\r\n        self.warmup = warmup\r\n\r\n    # \xe7\xba\xbf\xe6\x80\xa7\xe9\xa2\x84\xe7\x83\xad\xe6\x96\xb9\xe5\xbc\x8f\r\n    def warmup_linear(self,x, warmup=0.002):\r\n        if x < warmup:\r\n            return x / warmup\r\n        return 1.0 - x\r\n\r\n    def batch_step(self,training_step):\r\n        lr_this_step = self.learning_rate * self.warmup_linear(training_step / self.t_total,self.warmup)\r\n        for param_group in self.optimizer.param_groups:\r\n            param_group[\'lr\'] = lr_this_step\r\n\r\nclass CyclicLR(object):\r\n    \'\'\'\r\n    Cyclical learning rates for training neural networks\r\n    Example:\r\n        >>> scheduler = CyclicLR(optimizer)\r\n        >>> for epoch in range(100):\r\n        >>>     scheduler.step()\r\n        >>>     train(...)\r\n        >>>         ...\r\n        >>>         optimizer.zero_grad()\r\n        >>>         loss.backward()\r\n        >>>         optimizer.step()\r\n        >>>         scheduler.batch_step()\r\n        >>>     validate(...)\r\n    \'\'\'\r\n    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\r\n                 step_size=2000, mode=\'triangular\', gamma=1.,\r\n                 scale_fn=None, scale_mode=\'cycle\', last_batch_iteration=-1):\r\n\r\n        if not isinstance(optimizer, Optimizer):\r\n            raise TypeError(\'{} is not an Optimizer\'.format(\r\n                type(optimizer).__name__))\r\n\r\n        self.optimizer = optimizer\r\n\r\n        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\r\n            if len(base_lr) != len(optimizer.param_groups):\r\n                raise ValueError(""expected {} base_lr, got {}"".format(\r\n                    len(optimizer.param_groups), len(base_lr)))\r\n            self.base_lrs = list(base_lr)\r\n        else:\r\n            self.base_lrs = [base_lr] * len(optimizer.param_groups)\r\n\r\n        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\r\n            if len(max_lr) != len(optimizer.param_groups):\r\n                raise ValueError(""expected {} max_lr, got {}"".format(\r\n                    len(optimizer.param_groups), len(max_lr)))\r\n            self.max_lrs = list(max_lr)\r\n        else:\r\n            self.max_lrs = [max_lr] * len(optimizer.param_groups)\r\n\r\n        self.step_size = step_size\r\n\r\n        if mode not in [\'triangular\', \'triangular2\', \'exp_range\'] \\\r\n                and scale_fn is None:\r\n            raise ValueError(\'mode is invalid and scale_fn is None\')\r\n\r\n        self.mode = mode\r\n        self.gamma = gamma\r\n\r\n        if scale_fn is None:\r\n            if self.mode == \'triangular\':\r\n                self.scale_fn = self._triangular_scale_fn\r\n                self.scale_mode = \'cycle\'\r\n            elif self.mode == \'triangular2\':\r\n                self.scale_fn = self._triangular2_scale_fn\r\n                self.scale_mode = \'cycle\'\r\n            elif self.mode == \'exp_range\':\r\n                self.scale_fn = self._exp_range_scale_fn\r\n                self.scale_mode = \'iterations\'\r\n        else:\r\n            self.scale_fn = scale_fn\r\n            self.scale_mode = scale_mode\r\n\r\n        self.batch_step(last_batch_iteration + 1)\r\n        self.last_batch_iteration = last_batch_iteration\r\n\r\n    def _triangular_scale_fn(self, x):\r\n        return 1.\r\n\r\n    def _triangular2_scale_fn(self, x):\r\n        return 1 / (2. ** (x - 1))\r\n\r\n    def _exp_range_scale_fn(self, x):\r\n        return self.gamma**(x)\r\n\r\n    def get_lr(self):\r\n        step_size = float(self.step_size)\r\n        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\r\n        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\r\n\r\n        lrs = []\r\n        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\r\n        for param_group, base_lr, max_lr in param_lrs:\r\n            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\r\n            if self.scale_mode == \'cycle\':\r\n                lr = base_lr + base_height * self.scale_fn(cycle)\r\n            else:\r\n                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\r\n            lrs.append(lr)\r\n        return lrs\r\n\r\n    def batch_step(self, batch_iteration=None):\r\n        if batch_iteration is None:\r\n            batch_iteration = self.last_batch_iteration + 1\r\n        self.last_batch_iteration = batch_iteration\r\n        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\r\n            param_group[\'lr\'] = lr\r\n\r\nclass ReduceLROnPlateau(object):\r\n    """"""Reduce learning rate when a metric has stopped improving.\r\n    Models often benefit from reducing the learning rate by a factor\r\n    of 2-10 once learning stagnates. This scheduler reads a metrics\r\n    quantity and if no improvement is seen for a \'patience\' number\r\n    of epochs, the learning rate is reduced.\r\n\r\n    Args:\r\n        factor: factor by which the learning rate will\r\n            be reduced. new_lr = lr * factor\r\n        patience: number of epochs with no improvement\r\n            after which learning rate will be reduced.\r\n        verbose: int. 0: quiet, 1: update messages.\r\n        mode: one of {min, max}. In `min` mode,\r\n            lr will be reduced when the quantity\r\n            monitored has stopped decreasing; in `max`\r\n            mode it will be reduced when the quantity\r\n            monitored has stopped increasing.\r\n        epsilon: threshold for measuring the new optimum,\r\n            to only focus on significant changes.\r\n        cooldown: number of epochs to wait before resuming\r\n            normal operation after lr has been reduced.\r\n        min_lr: lower bound on the learning rate.\r\n\r\n\r\n    Example:\r\n        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\r\n        >>> scheduler = ReduceLROnPlateau(optimizer, \'min\')\r\n        >>> for epoch in range(10):\r\n        >>>     train(...)\r\n        >>>     val_acc, val_loss = validate(...)\r\n        >>>     scheduler.epoch_step(val_loss, epoch)\r\n    """"""\r\n\r\n    def __init__(self, optimizer, mode=\'min\', factor=0.1, patience=10,\r\n                 verbose=0, epsilon=1e-4, cooldown=0, min_lr=0,eps=1e-8):\r\n\r\n        super(ReduceLROnPlateau, self).__init__()\r\n        assert isinstance(optimizer, Optimizer)\r\n        if factor >= 1.0:\r\n            raise ValueError(\'ReduceLROnPlateau \'\r\n                             \'does not support a factor >= 1.0.\')\r\n        self.factor = factor\r\n        self.min_lr = min_lr\r\n        self.epsilon = epsilon\r\n        self.patience = patience\r\n        self.verbose = verbose\r\n        self.cooldown = cooldown\r\n        self.cooldown_counter = 0  # Cooldown counter.\r\n        self.monitor_op = None\r\n        self.wait = 0\r\n        self.best = 0\r\n        self.mode = mode\r\n        self.optimizer = optimizer\r\n        self.eps = eps\r\n        self._reset()\r\n\r\n    def _reset(self):\r\n        """"""Resets wait counter and cooldown counter.\r\n        """"""\r\n        if self.mode not in [\'min\', \'max\']:\r\n            raise RuntimeError(\'Learning Rate Plateau Reducing mode %s is unknown!\')\r\n        if self.mode == \'min\':\r\n            self.monitor_op = lambda a, b: np.less(a, b - self.epsilon)\r\n            self.best = np.Inf\r\n        else:\r\n            self.monitor_op = lambda a, b: np.greater(a, b + self.epsilon)\r\n            self.best = -np.Inf\r\n        self.cooldown_counter = 0\r\n        self.wait = 0\r\n\r\n    def reset(self):\r\n        self._reset()\r\n\r\n    def epoch_step(self, metrics, epoch):\r\n        current = metrics\r\n        if current is None:\r\n            warnings.warn(\'Learning Rate Plateau Reducing requires metrics available!\', RuntimeWarning)\r\n        else:\r\n            if self.in_cooldown():\r\n                self.cooldown_counter -= 1\r\n                self.wait = 0\r\n\r\n            if self.monitor_op(current, self.best):\r\n                self.best = current\r\n                self.wait = 0\r\n            elif not self.in_cooldown():\r\n                if self.wait >= self.patience:\r\n                    for param_group in self.optimizer.param_groups:\r\n                        old_lr = float(param_group[\'lr\'])\r\n                        if old_lr > self.min_lr + self.eps:\r\n                            new_lr = old_lr * self.factor\r\n                            new_lr = max(new_lr, self.min_lr)\r\n                            param_group[\'lr\'] = new_lr\r\n                            if self.verbose > 0:\r\n                                print(\'\\nEpoch %05d: reducing learning rate to %s.\' % (epoch, new_lr))\r\n                            self.cooldown_counter = self.cooldown\r\n                            self.wait = 0\r\n                self.wait += 1\r\n\r\n    def in_cooldown(self):\r\n        return self.cooldown_counter > 0\r\n\r\nclass ReduceLRWDOnPlateau(ReduceLROnPlateau):\r\n    """"""Reduce learning rate and weight decay when a metric has stopped\r\n    improving. Models often benefit from reducing the learning rate by\r\n    a factor of 2-10 once learning stagnates. This scheduler reads a metric\r\n    quantity and if no improvement is seen for a \'patience\' number\r\n    of epochs, the learning rate and weight decay factor is reduced for\r\n    optimizers that implement the the weight decay method from the paper\r\n    `Fixing Weight Decay Regularization in Adam`_.\r\n\r\n    .. _Fixing Weight Decay Regularization in Adam:\r\n        https://arxiv.org/abs/1711.05101\r\n    for AdamW or SGDW\r\n    Example:\r\n        >>> optimizer = AdamW(model.parameters(), lr=0.1, weight_decay=1e-3)\r\n        >>> scheduler = ReduceLRWDOnPlateau(optimizer, \'min\')\r\n        >>> for epoch in range(10):\r\n        >>>     train(...)\r\n        >>>     val_loss = validate(...)\r\n        >>>     # Note that step should be called after validate()\r\n        >>>     scheduler.epoch_step(val_loss)\r\n    """"""\r\n    def epoch_step(self, metrics, epoch):\r\n        current = metrics\r\n        if current is None:\r\n            warnings.warn(\'Learning Rate Plateau Reducing requires metrics available!\', RuntimeWarning)\r\n        else:\r\n            if self.in_cooldown():\r\n                self.cooldown_counter -= 1\r\n                self.wait = 0\r\n\r\n            if self.monitor_op(current, self.best):\r\n                self.best = current\r\n                self.wait = 0\r\n            elif not self.in_cooldown():\r\n                if self.wait >= self.patience:\r\n                    for param_group in self.optimizer.param_groups:\r\n                        old_lr = float(param_group[\'lr\'])\r\n                        if old_lr > self.min_lr + self.eps:\r\n                            new_lr = old_lr * self.factor\r\n                            new_lr = max(new_lr, self.min_lr)\r\n                            param_group[\'lr\'] = new_lr\r\n                            if self.verbose > 0:\r\n                                print(\'\\nEpoch %d: reducing learning rate to %s.\' % (epoch, new_lr))\r\n                        if param_group[\'weight_decay\'] != 0:\r\n                            old_weight_decay = float(param_group[\'weight_decay\'])\r\n                            new_weight_decay = max(old_weight_decay * self.factor, self.min_lr)\r\n                            if old_weight_decay > new_weight_decay + self.eps:\r\n                                param_group[\'weight_decay\'] = new_weight_decay\r\n                                if self.verbose:\r\n                                    print(\'\\nEpoch {epoch}: reducing weight decay factor of group {i} to {new_weight_decay:.4e}.\')\r\n                    self.cooldown_counter = self.cooldown\r\n                    self.wait = 0\r\n                self.wait += 1\r\n\r\nclass CosineLRWithRestarts(object):\r\n    """"""Decays learning rate with cosine annealing, normalizes weight decay\r\n    hyperparameter value, implements restarts.\r\n    https://arxiv.org/abs/1711.05101\r\n\r\n    Args:\r\n        optimizer (Optimizer): Wrapped optimizer.\r\n        batch_size: minibatch size\r\n        epoch_size: training samples per epoch\r\n        restart_period: epoch count in the first restart period\r\n        t_mult: multiplication factor by which the next restart period will extend/shrink\r\n\r\n    Example:\r\n        >>> scheduler = CosineLRWithRestarts(optimizer, 32, 1024, restart_period=5, t_mult=1.2)\r\n        >>> for epoch in range(100):\r\n        >>>     scheduler.step()\r\n        >>>     train(...)\r\n        >>>         ...\r\n        >>>         optimizer.zero_grad()\r\n        >>>         loss.backward()\r\n        >>>         optimizer.step()\r\n        >>>         scheduler.batch_step()\r\n        >>>     validate(...)\r\n    """"""\r\n\r\n    def __init__(self, optimizer, batch_size, epoch_size, restart_period=100,\r\n                 t_mult=2, last_epoch=-1, eta_threshold=1000, verbose=False):\r\n        if not isinstance(optimizer, Optimizer):\r\n            raise TypeError(\'{} is not an Optimizer\'.format(\r\n                type(optimizer).__name__))\r\n        self.optimizer = optimizer\r\n        if last_epoch == -1:\r\n            for group in optimizer.param_groups:\r\n                group.setdefault(\'initial_lr\', group[\'lr\'])\r\n        else:\r\n            for i, group in enumerate(optimizer.param_groups):\r\n                if \'initial_lr\' not in group:\r\n                    raise KeyError(""param \'initial_lr\' is not specified ""\r\n                                   ""in param_groups[{}] when resuming an""\r\n                                   "" optimizer"".format(i))\r\n        self.base_lrs = list(map(lambda group: group[\'initial_lr\'],\r\n                                 optimizer.param_groups))\r\n\r\n        self.last_epoch = last_epoch\r\n        self.batch_size = batch_size\r\n        self.iteration = 0\r\n        self.epoch_size = epoch_size\r\n        self.eta_threshold = eta_threshold\r\n        self.t_mult = t_mult\r\n        self.verbose = verbose\r\n        self.base_weight_decays = list(map(lambda group: group[\'weight_decay\'],\r\n                                           optimizer.param_groups))\r\n        self.restart_period = restart_period\r\n        self.restarts = 0\r\n        self.t_epoch = -1\r\n        self.batch_increments = []\r\n        self._set_batch_increment()\r\n\r\n    def _schedule_eta(self):\r\n        """"""\r\n        Threshold value could be adjusted to shrink eta_min and eta_max values.\r\n        """"""\r\n        eta_min = 0\r\n        eta_max = 1\r\n        if self.restarts <= self.eta_threshold:\r\n            return eta_min, eta_max\r\n        else:\r\n            d = self.restarts - self.eta_threshold\r\n            k = d * 0.09\r\n            return (eta_min + k, eta_max - k)\r\n\r\n    def get_lr(self, t_cur):\r\n        eta_min, eta_max = self._schedule_eta()\r\n\r\n        eta_t = (eta_min + 0.5 * (eta_max - eta_min)\r\n                 * (1. + math.cos(math.pi *\r\n                                  (t_cur / self.restart_period))))\r\n\r\n        weight_decay_norm_multi = math.sqrt(self.batch_size /\r\n                                            (self.epoch_size *\r\n                                             self.restart_period))\r\n        lrs = [base_lr * eta_t for base_lr in self.base_lrs]\r\n        weight_decays = [base_weight_decay * eta_t * weight_decay_norm_multi\r\n                         for base_weight_decay in self.base_weight_decays]\r\n\r\n        if self.t_epoch % self.restart_period < self.t_epoch:\r\n            if self.verbose:\r\n                print(""Restart at epoch {}"".format(self.last_epoch))\r\n            self.restart_period *= self.t_mult\r\n            self.restarts += 1\r\n            self.t_epoch = 0\r\n\r\n        return zip(lrs, weight_decays)\r\n\r\n    def _set_batch_increment(self):\r\n        d, r = divmod(self.epoch_size, self.batch_size)\r\n        batches_in_epoch = d + 2 if r > 0 else d + 1\r\n        self.iteration = 0\r\n        self.batch_increments = list(np.linspace(0, 1, batches_in_epoch))\r\n\r\n    def batch_step(self):\r\n        self.last_epoch += 1\r\n        self.t_epoch += 1\r\n        self._set_batch_increment()\r\n        try:\r\n            t_cur = self.t_epoch + self.batch_increments[self.iteration]\r\n            self.iteration += 1\r\n        except (IndexError):\r\n            raise RuntimeError(""Epoch size and batch size used in the ""\r\n                               ""training loop and while initializing ""\r\n                               ""scheduler should be the same."")\r\n\r\n        for param_group, (lr, weight_decay) in zip(self.optimizer.param_groups,self.get_lr(t_cur)):\r\n            param_group[\'lr\'] = lr\r\n            param_group[\'weight_decay\'] = weight_decay\r\n\r\n\r\nclass NoamLR(object):\r\n    \'\'\'\r\n    \xe4\xb8\xbb\xe8\xa6\x81\xe5\x8f\x82\xe8\x80\x83\xe8\xae\xba\xe6\x96\x87<< Attention Is All You Need>>\xe4\xb8\xad\xe7\x9a\x84\xe5\xad\xa6\xe4\xb9\xa0\xe6\x9b\xb4\xe6\x96\xb0\xe6\x96\xb9\xe5\xbc\x8f\r\n    Example:\r\n        >>> scheduler = NoamLR(d_model,factor,warm_up,optimizer)\r\n        >>> for epoch in range(100):\r\n        >>>     scheduler.step()\r\n        >>>     train(...)\r\n        >>>         ...\r\n        >>>         glopab_step += 1\r\n        >>>         optimizer.zero_grad()\r\n        >>>         loss.backward()\r\n        >>>         optimizer.step()\r\n        >>>         scheduler.batch_step(global_step)\r\n        >>>     validate(...)\r\n    \'\'\'\r\n    def __init__(self,d_model,factor,warm_up,optimizer):\r\n        self.optimizer = optimizer\r\n        self.warm_up = warm_up\r\n        self.factor = factor\r\n        self.d_model = d_model\r\n        self._lr = 0\r\n\r\n    def get_lr(self,step):\r\n        lr = self.factor * (self.d_model ** (-0.5) * min(step ** (-0.5),step * self.warm_up ** (-1.5)))\r\n        return lr\r\n\r\n    def batch_step(self,step):\r\n        \'\'\'\r\n        update parameters and rate\r\n        :return:\r\n        \'\'\'\r\n        lr = self.get_lr(step)\r\n        for p in self.optimizer.param_groups:\r\n            p[\'lr\'] = lr\r\n        self._lr = lr\r\n'"
callback/modelcheckpoint.py,4,"b'from pathlib import Path\r\nimport numpy as np\r\nimport torch\r\nfrom ..tools.common import logger\r\n\r\nclass ModelCheckpoint(object):\r\n    \'\'\'\r\n    \xe6\xa8\xa1\xe5\x9e\x8b\xe4\xbf\x9d\xe5\xad\x98\xef\xbc\x8c\xe4\xb8\xa4\xe7\xa7\x8d\xe6\xa8\xa1\xe5\xbc\x8f\xef\xbc\x9a\r\n    1. \xe7\x9b\xb4\xe6\x8e\xa5\xe4\xbf\x9d\xe5\xad\x98\xe6\x9c\x80\xe5\xa5\xbd\xe6\xa8\xa1\xe5\x9e\x8b\r\n    2. \xe6\x8c\x89\xe7\x85\xa7epoch\xe9\xa2\x91\xe7\x8e\x87\xe4\xbf\x9d\xe5\xad\x98\xe6\xa8\xa1\xe5\x9e\x8b\r\n    \'\'\'\r\n    def __init__(self, checkpoint_dir,\r\n                 monitor,\r\n                 arch,mode=\'min\',\r\n                 epoch_freq=1,\r\n                 best = None,\r\n                 save_best_only = True):\r\n        if isinstance(checkpoint_dir,Path):\r\n            checkpoint_dir = checkpoint_dir\r\n        else:\r\n            checkpoint_dir = Path(checkpoint_dir)\r\n        assert checkpoint_dir.is_dir()\r\n        checkpoint_dir.mkdir(exist_ok=True)\r\n        self.base_path = checkpoint_dir\r\n        self.arch = arch\r\n        self.monitor = monitor\r\n        self.epoch_freq = epoch_freq\r\n        self.save_best_only = save_best_only\r\n\r\n        # \xe8\xae\xa1\xe7\xae\x97\xe6\xa8\xa1\xe5\xbc\x8f\r\n        if mode == \'min\':\r\n            self.monitor_op = np.less\r\n            self.best = np.Inf\r\n\r\n        elif mode == \'max\':\r\n            self.monitor_op = np.greater\r\n            self.best = -np.Inf\r\n        # \xe8\xbf\x99\xe9\x87\x8c\xe4\xb8\xbb\xe8\xa6\x81\xe9\x87\x8d\xe6\x96\xb0\xe5\x8a\xa0\xe8\xbd\xbd\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x97\xb6\xe5\x80\x99\r\n        #\xe5\xaf\xb9best\xe9\x87\x8d\xe6\x96\xb0\xe8\xb5\x8b\xe5\x80\xbc\r\n        if best:\r\n            self.best = best\r\n\r\n        if save_best_only:\r\n            self.model_name = f""BEST_{arch}_MODEL.pth""\r\n\r\n    def epoch_step(self, state,current):\r\n        \'\'\'\r\n        \xe6\xad\xa3\xe5\xb8\xb8\xe6\xa8\xa1\xe5\x9e\x8b\r\n        :param state: \xe9\x9c\x80\xe8\xa6\x81\xe4\xbf\x9d\xe5\xad\x98\xe7\x9a\x84\xe4\xbf\xa1\xe6\x81\xaf\r\n        :param current: \xe5\xbd\x93\xe5\x89\x8d\xe5\x88\xa4\xe6\x96\xad\xe6\x8c\x87\xe6\xa0\x87\r\n        :return:\r\n        \'\'\'\r\n        # \xe6\x98\xaf\xe5\x90\xa6\xe4\xbf\x9d\xe5\xad\x98\xe6\x9c\x80\xe5\xa5\xbd\xe6\xa8\xa1\xe5\x9e\x8b\r\n        if self.save_best_only:\r\n            if self.monitor_op(current, self.best):\r\n                logger.info(f""\\nEpoch {state[\'epoch\']}: {self.monitor} improved from {self.best:.5f} to {current:.5f}"")\r\n                self.best = current\r\n                state[\'best\'] = self.best\r\n                best_path = self.base_path/ self.model_name\r\n                torch.save(state, str(best_path))\r\n        # \xe6\xaf\x8f\xe9\x9a\x94\xe5\x87\xa0\xe4\xb8\xaaepoch\xe4\xbf\x9d\xe5\xad\x98\xe4\xb8\x8b\xe6\xa8\xa1\xe5\x9e\x8b\r\n        else:\r\n            filename = self.base_path / f""EPOCH_{state[\'epoch\']}_{state[self.monitor]}_{self.arch}_MODEL.pth""\r\n            if state[\'epoch\'] % self.epoch_freq == 0:\r\n                logger.info(f""\\nEpoch {state[\'epoch\']}: save model to disk."")\r\n                torch.save(state, str(filename))\r\n\r\n    def bert_epoch_step(self, state,current):\r\n        \'\'\'\r\n        \xe9\x80\x82\xe5\x90\x88bert\xe7\xb1\xbb\xe5\x9e\x8b\xe6\xa8\xa1\xe5\x9e\x8b\xef\xbc\x8c\xe9\x80\x82\xe5\x90\x88pytorch_transformer\xe6\xa8\xa1\xe5\x9d\x97\r\n        :param state:\r\n        :param current:\r\n        :return:\r\n        \'\'\'\r\n        model_to_save = state[\'model\']\r\n        if self.save_best_only:\r\n            if self.monitor_op(current, self.best):\r\n                logger.info(f""\\nEpoch {state[\'epoch\']}: {self.monitor} improved from {self.best:.5f} to {current:.5f}"")\r\n                self.best = current\r\n                state[\'best\'] = self.best\r\n                model_to_save.save_pretrained(str(self.base_path))\r\n                output_config_file = self.base_path / \'configs.json\'\r\n                with open(str(output_config_file), \'w\') as f:\r\n                    f.write(model_to_save.config.to_json_string())\r\n                state.pop(""model"")\r\n                torch.save(state,self.base_path / \'checkpoint_info.bin\')\r\n        else:\r\n            if state[\'epoch\'] % self.epoch_freq == 0:\r\n                save_path = self.base_path / f""checkpoint-epoch-{state[\'epoch\']}""\r\n                save_path.mkdir(exist_ok=True)\r\n                logger.info(f""\\nEpoch {state[\'epoch\']}: save model to disk."")\r\n                model_to_save.save_pretrained(save_path)\r\n                output_config_file = save_path / \'configs.json\'\r\n                with open(str(output_config_file), \'w\') as f:\r\n                    f.write(model_to_save.config.to_json_string())\r\n                state.pop(""model"")\r\n                torch.save(state, save_path / \'checkpoint_info.bin\')\r\n'"
callback/progressbar.py,0,"b'import time\r\nclass ProgressBar(object):\r\n    \'\'\'\r\n    custom progress bar\r\n    Example:\r\n        >>> pbar = ProgressBar(n_total=30,desc=\'training\')\r\n        >>> step = 2\r\n        >>> pbar(step=step)\r\n    \'\'\'\r\n    def __init__(self, n_total,width=30,desc = \'Training\'):\r\n        self.width = width\r\n        self.n_total = n_total\r\n        self.start_time = time.time()\r\n        self.desc = desc\r\n\r\n    def __call__(self, step, info={}):\r\n        now = time.time()\r\n        current = step + 1\r\n        recv_per = current / self.n_total\r\n        bar = f\'[{self.desc}] {current}/{self.n_total} [\'\r\n        if recv_per >= 1:\r\n            recv_per = 1\r\n        prog_width = int(self.width * recv_per)\r\n        if prog_width > 0:\r\n            bar += \'=\' * (prog_width - 1)\r\n            if current< self.n_total:\r\n                bar += "">""\r\n            else:\r\n                bar += \'=\'\r\n        bar += \'.\' * (self.width - prog_width)\r\n        bar += \']\'\r\n        show_bar = f""\\r{bar}""\r\n        time_per_unit = (now - self.start_time) / current\r\n        if current < self.n_total:\r\n            eta = time_per_unit * (self.n_total - current)\r\n            if eta > 3600:\r\n                eta_format = (\'%d:%02d:%02d\' %\r\n                              (eta // 3600, (eta % 3600) // 60, eta % 60))\r\n            elif eta > 60:\r\n                eta_format = \'%d:%02d\' % (eta // 60, eta % 60)\r\n            else:\r\n                eta_format = \'%ds\' % eta\r\n            time_info = f\' - ETA: {eta_format}\'\r\n        else:\r\n            if time_per_unit >= 1:\r\n                time_info = f\' {time_per_unit:.1f}s/step\'\r\n            elif time_per_unit >= 1e-3:\r\n                time_info = f\' {time_per_unit * 1e3:.1f}ms/step\'\r\n            else:\r\n                time_info = f\' {time_per_unit * 1e6:.1f}us/step\'\r\n\r\n        show_bar += time_info\r\n        if len(info) != 0:\r\n            show_info = f\'{show_bar} \' + \\\r\n                        ""-"".join([f\' {key}: {value:.4f} \' for key, value in info.items()])\r\n            print(show_info, end=\'\')\r\n        else:\r\n            print(show_bar, end=\'\')\r\n'"
callback/trainingmonitor.py,0,"b'# encoding:utf-8\nimport numpy as np\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nfrom ..tools.common import load_json\nfrom ..tools.common import save_json\nplt.switch_backend(\'agg\')\n\nclass TrainingMonitor():\n    def __init__(self, file_dir, arch, add_test=False):\n        \'\'\'\n        :param startAt: \xe9\x87\x8d\xe6\x96\xb0\xe5\xbc\x80\xe5\xa7\x8b\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84epoch\xe7\x82\xb9\n        \'\'\'\n        if isinstance(file_dir, Path):\n            pass\n        else:\n            file_dir = Path(file_dir)\n        file_dir.mkdir(parents=True, exist_ok=True)\n\n        self.arch = arch\n        self.file_dir = file_dir\n        self.H = {}\n        self.add_test = add_test\n        self.json_path = file_dir / (arch + ""_training_monitor.json"")\n\n    def reset(self,start_at):\n        if start_at > 0:\n            if self.json_path is not None:\n                if self.json_path.exists():\n                    self.H = load_json(self.json_path)\n                    for k in self.H.keys():\n                        self.H[k] = self.H[k][:start_at]\n\n    def epoch_step(self, logs={}):\n        for (k, v) in logs.items():\n            l = self.H.get(k, [])\n            # np.float32\xe4\xbc\x9a\xe6\x8a\xa5\xe9\x94\x99\n            if not isinstance(v, np.float):\n                v = round(float(v), 4)\n            l.append(v)\n            self.H[k] = l\n\n        # \xe5\x86\x99\xe5\x85\xa5\xe6\x96\x87\xe4\xbb\xb6\n        if self.json_path is not None:\n            save_json(data = self.H,file_path=self.json_path)\n\n        # \xe4\xbf\x9d\xe5\xad\x98train\xe5\x9b\xbe\xe5\x83\x8f\n        if len(self.H[""loss""]) == 1:\n            self.paths = {key: self.file_dir / (self.arch + f\'_{key.upper()}\') for key in self.H.keys()}\n\n        if len(self.H[""loss""]) > 1:\n            # \xe6\x8c\x87\xe6\xa0\x87\xe5\x8f\x98\xe5\x8c\x96\n            # \xe6\x9b\xb2\xe7\xba\xbf\n            # \xe9\x9c\x80\xe8\xa6\x81\xe6\x88\x90\xe5\xaf\xb9\xe5\x87\xba\xe7\x8e\xb0\n            keys = [key for key, _ in self.H.items() if \'_\' not in key]\n            for key in keys:\n                N = np.arange(0, len(self.H[key]))\n                plt.style.use(""ggplot"")\n                plt.figure()\n                plt.plot(N, self.H[key], label=f""train_{key}"")\n                plt.plot(N, self.H[f""valid_{key}""], label=f""valid_{key}"")\n                if self.add_test:\n                    plt.plot(N, self.H[f""test_{key}""], label=f""test_{key}"")\n                plt.legend()\n                plt.xlabel(""Epoch #"")\n                plt.ylabel(key)\n                plt.title(f""Training {key} [Epoch {len(self.H[key])}]"")\n                plt.savefig(str(self.paths[key]))\n                plt.close()\n'"
metrics/__init__.py,0,b''
metrics/custom_metrics.py,2,"b'#encoding:utf-8\r\nimport torch\r\nfrom tqdm import tqdm\r\nimport numpy as np\r\nfrom collections import Counter\r\nfrom sklearn.metrics import roc_auc_score\r\nfrom sklearn.metrics import f1_score, classification_report\r\n\r\n__call__ = [\'Accuracy\',\'AUC\',\'F1Score\',\'EntityScore\',\'ClassReport\',\'MultiLabelReport\',\'AccuracyThresh\']\r\n\r\nclass Metric:\r\n    def __init__(self):\r\n        pass\r\n\r\n    def __call__(self, outputs, target):\r\n        raise NotImplementedError\r\n\r\n    def reset(self):\r\n        raise NotImplementedError\r\n\r\n    def value(self):\r\n        raise NotImplementedError\r\n\r\n    def name(self):\r\n        raise NotImplementedError\r\n\r\nclass Accuracy(Metric):\r\n    \'\'\'\r\n    \xe8\xae\xa1\xe7\xae\x97\xe5\x87\x86\xe7\xa1\xae\xe5\xba\xa6\r\n    \xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8topK\xe5\x8f\x82\xe6\x95\xb0\xe8\xae\xbe\xe5\xae\x9a\xe8\xae\xa1\xe7\xae\x97K\xe5\x87\x86\xe7\xa1\xae\xe5\xba\xa6\r\n    Example:\r\n        >>> metrics = Accuracy(**)\r\n        >>> for epoch in range(epochs):\r\n        >>>     metrics.reset()\r\n        >>>     for batch in batchs:\r\n        >>>         logits = model()\r\n        >>>         metrics(logits,target)\r\n        >>>         print(metrics.name(),metrics.value())\r\n    \'\'\'\r\n    def __init__(self,topK):\r\n        super(Accuracy,self).__init__()\r\n        self.topK = topK\r\n        self.reset()\r\n\r\n    def __call__(self, logits, target):\r\n        _, pred = logits.topk(self.topK, 1, True, True)\r\n        pred = pred.t()\r\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\r\n        self.correct_k = correct[:self.topK].view(-1).float().sum(0)\r\n        self.total = target.size(0)\r\n\r\n    def reset(self):\r\n        self.correct_k = 0\r\n        self.total = 0\r\n\r\n    def value(self):\r\n        return float(self.correct_k)  / self.total\r\n\r\n    def name(self):\r\n        return \'accuracy\'\r\n\r\n\r\nclass AccuracyThresh(Metric):\r\n    \'\'\'\r\n    \xe8\xae\xa1\xe7\xae\x97\xe5\x87\x86\xe7\xa1\xae\xe5\xba\xa6\r\n    \xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8topK\xe5\x8f\x82\xe6\x95\xb0\xe8\xae\xbe\xe5\xae\x9a\xe8\xae\xa1\xe7\xae\x97K\xe5\x87\x86\xe7\xa1\xae\xe5\xba\xa6\r\n    Example:\r\n        >>> metrics = AccuracyThresh(**)\r\n        >>> for epoch in range(epochs):\r\n        >>>     metrics.reset()\r\n        >>>     for batch in batchs:\r\n        >>>         logits = model()\r\n        >>>         metrics(logits,target)\r\n        >>>         print(metrics.name(),metrics.value())\r\n    \'\'\'\r\n    def __init__(self,thresh = 0.5):\r\n        super(AccuracyThresh,self).__init__()\r\n        self.thresh = thresh\r\n        self.reset()\r\n\r\n    def __call__(self, logits, target):\r\n        self.y_pred = logits.sigmoid()\r\n        self.y_true = target\r\n\r\n    def reset(self):\r\n        self.correct_k = 0\r\n        self.total = 0\r\n\r\n    def value(self):\r\n        data_size = self.y_pred.size(0)\r\n        acc = np.mean(((self.y_pred>self.thresh)==self.y_true.byte()).float().cpu().numpy(), axis=1).sum()\r\n        return acc / data_size\r\n\r\n    def name(self):\r\n        return \'accuracy\'\r\n\r\n\r\nclass AUC(Metric):\r\n    \'\'\'\r\n    AUC score\r\n    micro:\r\n            Calculate metrics globally by considering each element of the label\r\n            indicator matrix as a label.\r\n    macro:\r\n            Calculate metrics for each label, and find their unweighted\r\n            mean.  This does not take label imbalance into account.\r\n    weighted:\r\n            Calculate metrics for each label, and find their average, weighted\r\n            by support (the number of true instances for each label).\r\n    samples:\r\n            Calculate metrics for each instance, and find their average.\r\n    Example:\r\n        >>> metrics = AUC(**)\r\n        >>> for epoch in range(epochs):\r\n        >>>     metrics.reset()\r\n        >>>     for batch in batchs:\r\n        >>>         logits = model()\r\n        >>>         metrics(logits,target)\r\n        >>>         print(metrics.name(),metrics.value())\r\n    \'\'\'\r\n\r\n    def __init__(self,task_type = \'binary\',average = \'binary\'):\r\n        super(AUC, self).__init__()\r\n\r\n        assert task_type in [\'binary\',\'multiclass\']\r\n        assert average in [\'binary\',\'micro\', \'macro\', \'samples\', \'weighted\']\r\n\r\n        self.task_type = task_type\r\n        self.average = average\r\n\r\n    def __call__(self,logits,target):\r\n        \'\'\'\r\n        \xe8\xae\xa1\xe7\xae\x97\xe6\x95\xb4\xe4\xb8\xaa\xe7\xbb\x93\xe6\x9e\x9c\r\n        \'\'\'\r\n        if self.task_type == \'binary\':\r\n            self.y_prob = logits.sigmoid().data.cpu().numpy()\r\n        else:\r\n            self.y_prob = logits.softmax(-1).data.cpu().detach().numpy()\r\n        self.y_true = target.cpu().numpy()\r\n\r\n    def reset(self):\r\n        self.y_prob = 0\r\n        self.y_true = 0\r\n\r\n    def value(self):\r\n        \'\'\'\r\n        \xe8\xae\xa1\xe7\xae\x97\xe6\x8c\x87\xe6\xa0\x87\xe5\xbe\x97\xe5\x88\x86\r\n        \'\'\'\r\n        auc = roc_auc_score(y_score=self.y_prob, y_true=self.y_true, average=self.average)\r\n        return auc\r\n\r\n    def name(self):\r\n        return \'auc\'\r\n\r\nclass F1Score(Metric):\r\n    \'\'\'\r\n    F1 Score\r\n    binary:\r\n            Only report results for the class specified by ``pos_label``.\r\n            This is applicable only if targets (``y_{true,pred}``) are binary.\r\n    micro:\r\n            Calculate metrics globally by considering each element of the label\r\n            indicator matrix as a label.\r\n    macro:\r\n            Calculate metrics for each label, and find their unweighted\r\n            mean.  This does not take label imbalance into account.\r\n    weighted:\r\n            Calculate metrics for each label, and find their average, weighted\r\n            by support (the number of true instances for each label).\r\n    samples:\r\n            Calculate metrics for each instance, and find their average.\r\n    Example:\r\n        >>> metrics = F1Score(**)\r\n        >>> for epoch in range(epochs):\r\n        >>>     metrics.reset()\r\n        >>>     for batch in batchs:\r\n        >>>         logits = model()\r\n        >>>         metrics(logits,target)\r\n        >>>         print(metrics.name(),metrics.value())\r\n    \'\'\'\r\n    def __init__(self,thresh = 0.5, normalizate = True,task_type = \'binary\',average = \'binary\',search_thresh = False):\r\n        super(F1Score).__init__()\r\n        assert task_type in [\'binary\',\'multiclass\']\r\n        assert average in [\'binary\',\'micro\', \'macro\', \'samples\', \'weighted\']\r\n\r\n        self.thresh = thresh\r\n        self.task_type = task_type\r\n        self.normalizate  = normalizate\r\n        self.search_thresh = search_thresh\r\n        self.average = average\r\n\r\n    def thresh_search(self,y_prob):\r\n        \'\'\'\r\n        \xe5\xaf\xb9\xe4\xba\x8ef1\xe8\xaf\x84\xe5\x88\x86\xe7\x9a\x84\xe6\x8c\x87\xe6\xa0\x87\xef\xbc\x8c\xe4\xb8\x80\xe8\x88\xac\xe6\x88\x91\xe4\xbb\xac\xe9\x9c\x80\xe8\xa6\x81\xe5\xaf\xb9\xe9\x98\x88\xe5\x80\xbc\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xb0\x83\xe6\x95\xb4\xef\xbc\x8c\xe4\xb8\x80\xe8\x88\xac\xe4\xb8\x8d\xe4\xbc\x9a\xe4\xbd\xbf\xe7\x94\xa8\xe9\xbb\x98\xe8\xae\xa4\xe7\x9a\x840.5\xe5\x80\xbc\xef\xbc\x8c\xe5\x9b\xa0\xe6\xad\xa4\r\n        \xe8\xbf\x99\xe9\x87\x8c\xe6\x88\x91\xe4\xbb\xac\xe9\x98\x9fThresh\xe8\xbf\x9b\xe8\xa1\x8c\xe4\xbc\x98\xe5\x8c\x96\r\n        :return:\r\n        \'\'\'\r\n        best_threshold = 0\r\n        best_score = 0\r\n        for threshold in tqdm([i * 0.01 for i in range(100)], disable=True):\r\n            self.y_pred = y_prob > threshold\r\n            score = self.value()\r\n            if score > best_score:\r\n                best_threshold = threshold\r\n                best_score = score\r\n        return best_threshold,best_score\r\n\r\n    def __call__(self,logits,target):\r\n        \'\'\'\r\n        \xe8\xae\xa1\xe7\xae\x97\xe6\x95\xb4\xe4\xb8\xaa\xe7\xbb\x93\xe6\x9e\x9c\r\n        :return:\r\n        \'\'\'\r\n        self.y_true = target.cpu().numpy()\r\n        if self.normalizate and self.task_type == \'binary\':\r\n            y_prob = logits.sigmoid().data.cpu().numpy()\r\n        elif self.normalizate and self.task_type == \'multiclass\':\r\n            y_prob = logits.softmax(-1).data.cpu().detach().numpy()\r\n        else:\r\n            y_prob = logits.cpu().detach().numpy()\r\n\r\n        if self.task_type == \'binary\':\r\n            if self.thresh and self.search_thresh == False:\r\n                self.y_pred = (y_prob > self.thresh ).astype(int)\r\n                self.value()\r\n            else:\r\n                thresh,f1 = self.thresh_search(y_prob = y_prob)\r\n                print(f""Best thresh: {thresh:.4f} - F1 Score: {f1:.4f}"")\r\n\r\n        if self.task_type == \'multiclass\':\r\n            self.y_pred = np.argmax(self.y_pred, 1)\r\n\r\n    def reset(self):\r\n        self.y_pred = 0\r\n        self.y_true = 0\r\n\r\n    def value(self):\r\n        \'\'\'\r\n         \xe8\xae\xa1\xe7\xae\x97\xe6\x8c\x87\xe6\xa0\x87\xe5\xbe\x97\xe5\x88\x86\r\n         \'\'\'\r\n        if self.task_type == \'binary\':\r\n            f1 = f1_score(y_true=self.y_true, y_pred=self.y_pred, average=self.average)\r\n            return f1\r\n        if self.task_type == \'multiclass\':\r\n            f1 = f1_score(y_true=self.y_true, y_pred=self.y_pred, average=self.average)\r\n            return f1\r\n\r\n    def name(self):\r\n        return \'f1\'\r\n\r\nclass ClassReport(Metric):\r\n    \'\'\'\r\n    class report\r\n    \'\'\'\r\n    def __init__(self,target_names = None):\r\n        super(ClassReport).__init__()\r\n        self.target_names = target_names\r\n\r\n    def reset(self):\r\n        self.y_pred = 0\r\n        self.y_true = 0\r\n\r\n    def value(self):\r\n        \'\'\'\r\n        \xe8\xae\xa1\xe7\xae\x97\xe6\x8c\x87\xe6\xa0\x87\xe5\xbe\x97\xe5\x88\x86\r\n        \'\'\'\r\n        score = classification_report(y_true = self.y_true, y_pred = self.y_pred, target_names=self.target_names)\r\n        print(f""\\n\\n classification report: {score}"")\r\n\r\n    def __call__(self,logits,target):\r\n        _, y_pred = torch.max(logits.data, 1)\r\n        self.y_pred = y_pred.cpu().numpy()\r\n        self.y_true = target.cpu().numpy()\r\n\r\n    def name(self):\r\n        return ""class_report""\r\n\r\nclass MultiLabelReport(Metric):\r\n    \'\'\'\r\n    multi label report\r\n    \'\'\'\r\n    def __init__(self,id2label = None):\r\n        super(MultiLabelReport).__init__()\r\n        self.id2label = id2label\r\n\r\n    def reset(self):\r\n        self.y_prob = 0\r\n        self.y_true = 0\r\n\r\n    def __call__(self,logits,target):\r\n\r\n        self.y_prob = logits.sigmoid().data.cpu().detach().numpy()\r\n        self.y_true = target.cpu().numpy()\r\n\r\n    def value(self):\r\n        \'\'\'\r\n        \xe8\xae\xa1\xe7\xae\x97\xe6\x8c\x87\xe6\xa0\x87\xe5\xbe\x97\xe5\x88\x86\r\n        \'\'\'\r\n        for i, label in self.id2label.items():\r\n            auc = roc_auc_score(y_score=self.y_prob[:, i], y_true=self.y_true[:, i])\r\n            print(f""label:{label} - auc: {auc:.4f}"")\r\n\r\n    def name(self):\r\n        return ""multilabel_report""\r\n\r\n\r\nclass LMAccuracy(Metric):\r\n    def __init__(self,topK =1):\r\n        super(LMAccuracy).__init__()\r\n        self.topK = topK\r\n        self.reset()\r\n\r\n    def __call__(self,logits,target):\r\n        pred = torch.argmax(logits, 1)\r\n        active_acc = target.view(-1) != -1\r\n        active_pred = pred[active_acc]\r\n        active_labels = target[active_acc]\r\n\r\n        correct = active_pred.eq(active_labels)\r\n        self.correct_k = correct.float().sum(0)\r\n        self.total = active_labels.size(0)\r\n\r\n    def reset(self):\r\n        self.correct_k = 0\r\n        self.total = 0\r\n\r\n    def value(self):\r\n        return float(self.correct_k) / self.total\r\n\r\n    def name(self):\r\n        return \'accuracy\'\r\n\r\n\r\n'"
metrics/glue_compute_metrics.py,0,"b'# coding=utf-8\r\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\r\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the ""License"");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an ""AS IS"" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n\r\nimport csv\r\nimport sys\r\nimport logging\r\n\r\nlogger = logging.getLogger(__name__)\r\ntry:\r\n    from scipy.stats import pearsonr, spearmanr\r\n    from sklearn.metrics import matthews_corrcoef, f1_score\r\n    _has_sklearn = True\r\nexcept (AttributeError, ImportError) as e:\r\n    logger.warning(""To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html"")\r\n    _has_sklearn = False\r\n\r\ndef simple_accuracy(preds, labels):\r\n    return (preds == labels).mean()\r\n\r\n\r\ndef acc_and_f1(preds, labels):\r\n    acc = simple_accuracy(preds, labels)\r\n    f1 = f1_score(y_true=labels, y_pred=preds)\r\n    return {\r\n        ""acc"": acc,\r\n        ""f1"": f1,\r\n        ""acc_and_f1"": (acc + f1) / 2,\r\n    }\r\n\r\ndef pearson_and_spearman(preds, labels):\r\n    pearson_corr = pearsonr(preds, labels)[0]\r\n    spearman_corr = spearmanr(preds, labels)[0]\r\n    return {\r\n        ""pearson"": pearson_corr,\r\n        ""spearmanr"": spearman_corr,\r\n        ""corr"": (pearson_corr + spearman_corr) / 2,\r\n    }\r\n\r\ndef compute_metrics(task_name, preds, labels):\r\n    assert len(preds) == len(labels)\r\n    if task_name == ""cola"":\r\n        return {""mcc"": matthews_corrcoef(labels, preds)}\r\n    elif task_name == ""sst-2"":\r\n        return {""acc"": simple_accuracy(preds, labels)}\r\n    elif task_name == ""mrpc"":\r\n        return acc_and_f1(preds, labels)\r\n    elif task_name == ""sts-b"":\r\n        return pearson_and_spearman(preds, labels)\r\n    elif task_name == ""qqp"":\r\n        return acc_and_f1(preds, labels)\r\n    elif task_name == ""mnli"":\r\n        return {""acc"": simple_accuracy(preds, labels)}\r\n    elif task_name == ""mnli-mm"":\r\n        return {""acc"": simple_accuracy(preds, labels)}\r\n    elif task_name == ""qnli"":\r\n        return {""acc"": simple_accuracy(preds, labels)}\r\n    elif task_name == ""rte"":\r\n        return {""acc"": simple_accuracy(preds, labels)}\r\n    elif task_name == ""wnli"":\r\n        return {""acc"": simple_accuracy(preds, labels)}\r\n    elif task_name == ""lcqmc"":\r\n        return {""acc"": simple_accuracy(preds, labels)}\r\n    else:\r\n        raise KeyError(task_name)\r\n'"
model/__init__.py,0,b'#encoding:utf-8'
model/configuration_albert.py,0,"b'"""""" BERT model configuration """"""\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport json\nimport logging\nimport sys\nfrom io import open\n\nfrom .configuration_utils import PretrainedConfig\nlogger = logging.getLogger(__name__)\n\nclass AlbertConfig(PretrainedConfig):\n    r""""""\n        Arguments:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n            hidden_size: Size of the encoder layers and the pooler layer.\n            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n            num_attention_heads: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            intermediate_size: The size of the ""intermediate"" (i.e., feed-forward)\n                layer in the Transformer encoder.\n            hidden_act: The non-linear activation function (function or string) in the\n                encoder and pooler. If string, ""gelu"", ""relu"" and ""swish"" are supported.\n            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            attention_probs_dropout_prob: The dropout ratio for the attention\n                probabilities.\n            max_position_embeddings: The maximum sequence length that this model might\n                ever be used with. Typically set this to something large just in case\n                (e.g., 512 or 1024 or 2048).\n            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n                `BertModel`.\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n            layer_norm_eps: The epsilon used by LayerNorm.\n    """"""\n    def __init__(self,\n                 vocab_size_or_config_json_file=30000,\n                 embedding_size=128,\n                 hidden_size=4096,\n                 num_hidden_layers=12,\n                 num_hidden_groups=1,\n                 num_attention_heads=64,\n                 intermediate_size=16384,\n                 inner_group_num=1,\n                 hidden_act=""gelu_new"",\n                 hidden_dropout_prob=0,\n                 attention_probs_dropout_prob=0,\n                 max_position_embeddings=512,\n                 type_vocab_size=2,\n                 initializer_range=0.02,\n                 layer_norm_eps=1e-12,\n                 **kwargs):\n        super(AlbertConfig, self).__init__(**kwargs)\n        if isinstance(vocab_size_or_config_json_file, str) or (sys.version_info[0] == 2\n                        and isinstance(vocab_size_or_config_json_file, unicode)):\n            with open(vocab_size_or_config_json_file, ""r"", encoding=\'utf-8\') as reader:\n                json_config = json.loads(reader.read())\n            for key, value in json_config.items():\n                self.__dict__[key] = value\n        elif isinstance(vocab_size_or_config_json_file, int):\n            self.vocab_size = vocab_size_or_config_json_file\n            self.hidden_size = hidden_size\n            self.num_hidden_layers = num_hidden_layers\n            self.num_attention_heads = num_attention_heads\n            self.hidden_act = hidden_act\n            self.intermediate_size = intermediate_size\n            self.hidden_dropout_prob = hidden_dropout_prob\n            self.attention_probs_dropout_prob = attention_probs_dropout_prob\n            self.max_position_embeddings = max_position_embeddings\n            self.type_vocab_size = type_vocab_size\n            self.initializer_range = initializer_range\n            self.layer_norm_eps = layer_norm_eps\n            self.embedding_size = embedding_size\n            self.inner_group_num = inner_group_num\n            self.num_hidden_groups = num_hidden_groups\n        else:\n            raise ValueError(""First argument must be either a vocabulary size (int)""\n                             "" or the path to a pretrained model config file (str)"")\n'"
model/configuration_bert.py,0,"b'\n"""""" BERT model configuration """"""\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport json\nimport logging\nimport sys\nfrom io import open\n\nfrom .configuration_utils import PretrainedConfig\n\nlogger = logging.getLogger(__name__)\n\nBERT_PRETRAINED_CONFIG_ARCHIVE_MAP = {}\nclass BertConfig(PretrainedConfig):\n    r""""""\n        :class:`~pytorch_transformers.BertConfig` is the configuration class to store the configuration of a\n        `BertModel`.\n\n\n        Arguments:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n            hidden_size: Size of the encoder layers and the pooler layer.\n            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n            num_attention_heads: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            intermediate_size: The size of the ""intermediate"" (i.e., feed-forward)\n                layer in the Transformer encoder.\n            hidden_act: The non-linear activation function (function or string) in the\n                encoder and pooler. If string, ""gelu"", ""relu"" and ""swish"" are supported.\n            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            attention_probs_dropout_prob: The dropout ratio for the attention\n                probabilities.\n            max_position_embeddings: The maximum sequence length that this model might\n                ever be used with. Typically set this to something large just in case\n                (e.g., 512 or 1024 or 2048).\n            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n                `BertModel`.\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n            layer_norm_eps: The epsilon used by LayerNorm.\n    """"""\n    pretrained_config_archive_map = BERT_PRETRAINED_CONFIG_ARCHIVE_MAP\n\n    def __init__(self,\n                 vocab_size_or_config_json_file=30522,\n                 hidden_size=768,\n                 num_hidden_layers=12,\n                 num_attention_heads=12,\n                 intermediate_size=3072,\n                 hidden_act=""gelu"",\n                 hidden_dropout_prob=0.1,\n                 attention_probs_dropout_prob=0.1,\n                 max_position_embeddings=512,\n                 type_vocab_size=2,\n                 initializer_range=0.02,\n                 layer_norm_eps=1e-12,\n                 **kwargs):\n        super(BertConfig, self).__init__(**kwargs)\n        if isinstance(vocab_size_or_config_json_file, str) or (sys.version_info[0] == 2\n                        and isinstance(vocab_size_or_config_json_file, unicode)):\n            with open(vocab_size_or_config_json_file, ""r"", encoding=\'utf-8\') as reader:\n                json_config = json.loads(reader.read())\n            for key, value in json_config.items():\n                self.__dict__[key] = value\n        elif isinstance(vocab_size_or_config_json_file, int):\n            self.vocab_size = vocab_size_or_config_json_file\n            self.hidden_size = hidden_size\n            self.num_hidden_layers = num_hidden_layers\n            self.num_attention_heads = num_attention_heads\n            self.hidden_act = hidden_act\n            self.intermediate_size = intermediate_size\n            self.hidden_dropout_prob = hidden_dropout_prob\n            self.attention_probs_dropout_prob = attention_probs_dropout_prob\n            self.max_position_embeddings = max_position_embeddings\n            self.type_vocab_size = type_vocab_size\n            self.initializer_range = initializer_range\n            self.layer_norm_eps = layer_norm_eps\n        else:\n            raise ValueError(""First argument must be either a vocabulary size (int)""\n                             "" or the path to a pretrained model config file (str)"")\n'"
model/configuration_utils.py,0,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" Configuration base class and utilities.""""""\n\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport copy\nimport json\nimport logging\nimport os\nfrom io import open\n\nfrom model.file_utils import cached_path, CONFIG_NAME\n\nlogger = logging.getLogger(__name__)\n\nclass PretrainedConfig(object):\n    r"""""" Base class for all configuration classes.\n        Handles a few parameters tools to all models\' configurations as well as methods for loading/downloading/saving configurations.\n\n        Note:\n            A configuration file can be loaded and saved to disk. Loading the configuration file and using this file to initialize a model does **not** load the model weights.\n            It only affects the model\'s configuration.\n\n        Class attributes (overridden by derived classes):\n            - ``pretrained_config_archive_map``: a python ``dict`` of with `short-cut-names` (string) as keys and `url` (string) of associated pretrained model configurations as values.\n\n        Parameters:\n            ``finetuning_task``: string, default `None`. Name of the task used to fine-tune the model. This can be used when converting from an original (TensorFlow or PyTorch) checkpoint.\n            ``num_labels``: integer, default `2`. Number of classes to use when the model is a classification model (sequences/tokens)\n            ``output_attentions``: boolean, default `False`. Should the model returns attentions weights.\n            ``output_hidden_states``: string, default `False`. Should the model returns all hidden-states.\n            ``torchscript``: string, default `False`. Is the model used with Torchscript.\n    """"""\n    pretrained_config_archive_map = {}\n\n    def __init__(self, **kwargs):\n        self.finetuning_task = kwargs.pop(\'finetuning_task\', None)\n        self.num_labels = kwargs.pop(\'num_labels\', 2)\n        self.output_attentions = kwargs.pop(\'output_attentions\', False)\n        self.output_hidden_states = kwargs.pop(\'output_hidden_states\', False)\n        self.torchscript = kwargs.pop(\'torchscript\', False)\n        self.pruned_heads = kwargs.pop(\'pruned_heads\', {})\n\n    def save_pretrained(self, save_directory):\n        """""" Save a configuration object to the directory `save_directory`, so that it\n            can be re-loaded using the :func:`~pytorch_transformers.PretrainedConfig.from_pretrained` class method.\n        """"""\n        assert os.path.isdir(save_directory), ""Saving path should be a directory where the model and configuration can be saved""\n\n        # If we save using the predefined names, we can load using `from_pretrained`\n        output_config_file = os.path.join(save_directory, CONFIG_NAME)\n\n        self.to_json_file(output_config_file)\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n        r"""""" Instantiate a :class:`~pytorch_transformers.PretrainedConfig` (or a derived class) from a pre-trained model configuration.\n\n        Parameters:\n            pretrained_model_name_or_path: either:\n\n                - a string with the `shortcut name` of a pre-trained model configuration to load from cache or download, e.g.: ``bert-base-uncased``.\n                - a path to a `directory` containing a configuration file saved using the :func:`~pytorch_transformers.PretrainedConfig.save_pretrained` method, e.g.: ``./my_model_directory/``.\n                - a path or url to a saved configuration JSON `file`, e.g.: ``./my_model_directory/configuration.json``.\n\n            cache_dir: (`optional`) string:\n                Path to a directory in which a downloaded pre-trained model\n                configuration should be cached if the standard cache should not be used.\n\n            kwargs: (`optional`) dict: key/value pairs with which to update the configuration object after loading.\n\n                - The values in kwargs of any keys which are configuration attributes will be used to override the loaded values.\n                - Behavior concerning key/value pairs whose keys are *not* configuration attributes is controlled by the `return_unused_kwargs` keyword parameter.\n\n            force_download: (`optional`) boolean, default False:\n                Force to (re-)download the model weights and configuration files and override the cached versions if they exists.\n\n            proxies: (`optional`) dict, default None:\n                A dictionary of proxy servers to use by protocol or endpoint, e.g.: {\'http\': \'foo.bar:3128\', \'http://hostname\': \'foo.bar:4012\'}.\n                The proxies are used on each request.\n\n            return_unused_kwargs: (`optional`) bool:\n\n                - If False, then this function returns just the final configuration object.\n                - If True, then this functions returns a tuple `(config, unused_kwargs)` where `unused_kwargs` is a dictionary consisting of the key/value pairs whose keys are not configuration attributes: ie the part of kwargs which has not been used to update `config` and is otherwise ignored.\n\n        Examples::\n\n            # We can\'t instantiate directly the base class `PretrainedConfig` so let\'s show the examples on a\n            # derived class: BertConfig\n            config = BertConfig.from_pretrained(\'bert-base-uncased\')    # Download configuration from S3 and cache.\n            config = BertConfig.from_pretrained(\'./test/saved_model/\')  # E.g. config (or model) was saved using `save_pretrained(\'./test/saved_model/\')`\n            config = BertConfig.from_pretrained(\'./test/saved_model/my_configuration.json\')\n            config = BertConfig.from_pretrained(\'bert-base-uncased\', output_attention=True, foo=False)\n            assert config.output_attention == True\n            config, unused_kwargs = BertConfig.from_pretrained(\'bert-base-uncased\', output_attention=True,\n                                                               foo=False, return_unused_kwargs=True)\n            assert config.output_attention == True\n            assert unused_kwargs == {\'foo\': False}\n\n        """"""\n        cache_dir = kwargs.pop(\'cache_dir\', None)\n        force_download = kwargs.pop(\'force_download\', False)\n        proxies = kwargs.pop(\'proxies\', None)\n        return_unused_kwargs = kwargs.pop(\'return_unused_kwargs\', False)\n\n        if pretrained_model_name_or_path in cls.pretrained_config_archive_map:\n            config_file = cls.pretrained_config_archive_map[pretrained_model_name_or_path]\n        elif os.path.isdir(pretrained_model_name_or_path):\n            config_file = os.path.join(pretrained_model_name_or_path, CONFIG_NAME)\n        else:\n            config_file = pretrained_model_name_or_path\n        # redirect to the cache, if necessary\n        try:\n            resolved_config_file = cached_path(config_file, cache_dir=cache_dir, force_download=force_download, proxies=proxies)\n        except EnvironmentError as e:\n            if pretrained_model_name_or_path in cls.pretrained_config_archive_map:\n                logger.error(\n                    ""Couldn\'t reach server at \'{}\' to download pretrained model configuration file."".format(\n                        config_file))\n            else:\n                logger.error(\n                    ""Model name \'{}\' was not found in model name list ({}). ""\n                    ""We assumed \'{}\' was a path or url but couldn\'t find any file ""\n                    ""associated to this path or url."".format(\n                        pretrained_model_name_or_path,\n                        \', \'.join(cls.pretrained_config_archive_map.keys()),\n                        config_file))\n            raise e\n        if resolved_config_file == config_file:\n            logger.info(""loading configuration file {}"".format(config_file))\n        else:\n            logger.info(""loading configuration file {} from cache at {}"".format(\n                config_file, resolved_config_file))\n\n        # Load config\n        config = cls.from_json_file(resolved_config_file)\n\n        if hasattr(config, \'pruned_heads\'):\n            config.pruned_heads = dict((int(key), set(value)) for key, value in config.pruned_heads.items())\n\n        # Update config with kwargs if needed\n        to_remove = []\n        for key, value in kwargs.items():\n            if hasattr(config, key):\n                setattr(config, key, value)\n                to_remove.append(key)\n            else:\n                setattr(config,key,value)\n        for key in to_remove:\n            kwargs.pop(key, None)\n\n        logger.info(""Model config %s"", config)\n        if return_unused_kwargs:\n            return config, kwargs\n        else:\n            return config\n\n    @classmethod\n    def from_dict(cls, json_object):\n        """"""Constructs a `Config` from a Python dictionary of parameters.""""""\n        config = cls(vocab_size_or_config_json_file=-1)\n        for key, value in json_object.items():\n            config.__dict__[key] = value\n        return config\n\n    @classmethod\n    def from_json_file(cls, json_file):\n        """"""Constructs a `BertConfig` from a json file of parameters.""""""\n        with open(json_file, ""r"", encoding=\'utf-8\') as reader:\n            text = reader.read()\n        return cls.from_dict(json.loads(text))\n\n    def __eq__(self, other):\n        return self.__dict__ == other.__dict__\n\n    def __repr__(self):\n        return str(self.to_json_string())\n\n    def to_dict(self):\n        """"""Serializes this instance to a Python dictionary.""""""\n        output = copy.deepcopy(self.__dict__)\n        return output\n\n    def to_json_string(self):\n        """"""Serializes this instance to a JSON string.""""""\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\n\n    def to_json_file(self, json_file_path):\n        """""" Save this instance to a json file.""""""\n        with open(json_file_path, ""w"", encoding=\'utf-8\') as writer:\n            writer.write(self.to_json_string())\n'"
model/file_utils.py,1,"b'""""""\nUtilities for working with the local dataset cache.\nThis file is adapted from the AllenNLP library at https://github.com/allenai/allennlp\nCopyright by the AllenNLP authors.\n""""""\nfrom __future__ import (absolute_import, division, print_function, unicode_literals)\n\nimport sys\nimport json\nimport logging\nimport os\nimport six\nimport shutil\nimport tempfile\nimport fnmatch\nfrom functools import wraps\nfrom hashlib import sha256\nfrom io import open\n\nimport boto3\nfrom botocore.config import Config\nfrom botocore.exceptions import ClientError\nimport requests\nfrom tqdm import tqdm\n\ntry:\n    from torch.hub import _get_torch_home\n    torch_cache_home = _get_torch_home()\nexcept ImportError:\n    torch_cache_home = os.path.expanduser(\n        os.getenv(\'TORCH_HOME\', os.path.join(\n            os.getenv(\'XDG_CACHE_HOME\', \'~/.cache\'), \'torch\')))\ndefault_cache_path = os.path.join(torch_cache_home, \'pytorch_transformers\')\n\ntry:\n    from urllib.parse import urlparse\nexcept ImportError:\n    from urlparse import urlparse\n\ntry:\n    from pathlib import Path\n    PYTORCH_PRETRAINED_BERT_CACHE = Path(\n        os.getenv(\'PYTORCH_TRANSFORMERS_CACHE\', os.getenv(\'PYTORCH_PRETRAINED_BERT_CACHE\', default_cache_path)))\nexcept (AttributeError, ImportError):\n    PYTORCH_PRETRAINED_BERT_CACHE = os.getenv(\'PYTORCH_TRANSFORMERS_CACHE\',\n                                              os.getenv(\'PYTORCH_PRETRAINED_BERT_CACHE\',\n                                                        default_cache_path))\n\nPYTORCH_TRANSFORMERS_CACHE = PYTORCH_PRETRAINED_BERT_CACHE  # Kept for backward compatibility\n\nWEIGHTS_NAME = ""pytorch_model.bin""\nTF_WEIGHTS_NAME = \'model.ckpt\'\nCONFIG_NAME = ""config.json""\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\nif not six.PY2:\n    def add_start_docstrings(*docstr):\n        def docstring_decorator(fn):\n            fn.__doc__ = \'\'.join(docstr) + fn.__doc__\n            return fn\n        return docstring_decorator\n\n    def add_end_docstrings(*docstr):\n        def docstring_decorator(fn):\n            fn.__doc__ = fn.__doc__ + \'\'.join(docstr)\n            return fn\n        return docstring_decorator\nelse:\n    # Not possible to update class docstrings on python2\n    def add_start_docstrings(*docstr):\n        def docstring_decorator(fn):\n            return fn\n        return docstring_decorator\n\n    def add_end_docstrings(*docstr):\n        def docstring_decorator(fn):\n            return fn\n        return docstring_decorator\n\ndef url_to_filename(url, etag=None):\n    """"""\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the url\'s, delimited\n    by a period.\n    """"""\n    url_bytes = url.encode(\'utf-8\')\n    url_hash = sha256(url_bytes)\n    filename = url_hash.hexdigest()\n\n    if etag:\n        etag_bytes = etag.encode(\'utf-8\')\n        etag_hash = sha256(etag_bytes)\n        filename += \'.\' + etag_hash.hexdigest()\n\n    return filename\n\n\ndef filename_to_url(filename, cache_dir=None):\n    """"""\n    Return the url and etag (which may be ``None``) stored for `filename`.\n    Raise ``EnvironmentError`` if `filename` or its stored metadata do not exist.\n    """"""\n    if cache_dir is None:\n        cache_dir = PYTORCH_TRANSFORMERS_CACHE\n    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n        cache_dir = str(cache_dir)\n\n    cache_path = os.path.join(cache_dir, filename)\n    if not os.path.exists(cache_path):\n        raise EnvironmentError(""file {} not found"".format(cache_path))\n\n    meta_path = cache_path + \'.json\'\n    if not os.path.exists(meta_path):\n        raise EnvironmentError(""file {} not found"".format(meta_path))\n\n    with open(meta_path, encoding=""utf-8"") as meta_file:\n        metadata = json.load(meta_file)\n    url = metadata[\'url\']\n    etag = metadata[\'etag\']\n\n    return url, etag\n\n\ndef cached_path(url_or_filename, cache_dir=None, force_download=False, proxies=None):\n    """"""\n    Given something that might be a URL (or might be a local path),\n    determine which. If it\'s a URL, download the file and cache it, and\n    return the path to the cached file. If it\'s already a local path,\n    make sure the file exists and then return the path.\n    Args:\n        cache_dir: specify a cache directory to save the file to (overwrite the default cache dir).\n        force_download: if True, re-dowload the file even if it\'s already cached in the cache dir.\n    """"""\n    if cache_dir is None:\n        cache_dir = PYTORCH_TRANSFORMERS_CACHE\n    if sys.version_info[0] == 3 and isinstance(url_or_filename, Path):\n        url_or_filename = str(url_or_filename)\n    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n        cache_dir = str(cache_dir)\n\n    parsed = urlparse(url_or_filename)\n\n    if parsed.scheme in (\'http\', \'https\', \'s3\'):\n        # URL, so get it from the cache (downloading if necessary)\n        return get_from_cache(url_or_filename, cache_dir=cache_dir, force_download=force_download, proxies=proxies)\n    elif os.path.exists(url_or_filename):\n        # File, and it exists.\n        return url_or_filename\n    elif parsed.scheme == \'\':\n        # File, but it doesn\'t exist.\n        raise EnvironmentError(""file {} not found"".format(url_or_filename))\n    else:\n        # Something unknown\n        raise ValueError(""unable to parse {} as a URL or as a local path"".format(url_or_filename))\n\n\ndef split_s3_path(url):\n    """"""Split a full s3 path into the bucket name and path.""""""\n    parsed = urlparse(url)\n    if not parsed.netloc or not parsed.path:\n        raise ValueError(""bad s3 path {}"".format(url))\n    bucket_name = parsed.netloc\n    s3_path = parsed.path\n    # Remove \'/\' at beginning of path.\n    if s3_path.startswith(""/""):\n        s3_path = s3_path[1:]\n    return bucket_name, s3_path\n\n\ndef s3_request(func):\n    """"""\n    Wrapper function for s3 requests in order to create more helpful error\n    messages.\n    """"""\n\n    @wraps(func)\n    def wrapper(url, *args, **kwargs):\n        try:\n            return func(url, *args, **kwargs)\n        except ClientError as exc:\n            if int(exc.response[""Error""][""Code""]) == 404:\n                raise EnvironmentError(""file {} not found"".format(url))\n            else:\n                raise\n\n    return wrapper\n\n\n@s3_request\ndef s3_etag(url, proxies=None):\n    """"""Check ETag on S3 object.""""""\n    s3_resource = boto3.resource(""s3"", config=Config(proxies=proxies))\n    bucket_name, s3_path = split_s3_path(url)\n    s3_object = s3_resource.Object(bucket_name, s3_path)\n    return s3_object.e_tag\n\n\n@s3_request\ndef s3_get(url, temp_file, proxies=None):\n    """"""Pull a file directly from S3.""""""\n    s3_resource = boto3.resource(""s3"", config=Config(proxies=proxies))\n    bucket_name, s3_path = split_s3_path(url)\n    s3_resource.Bucket(bucket_name).download_fileobj(s3_path, temp_file)\n\n\ndef http_get(url, temp_file, proxies=None):\n    req = requests.get(url, stream=True, proxies=proxies)\n    content_length = req.headers.get(\'Content-Length\')\n    total = int(content_length) if content_length is not None else None\n    progress = tqdm(unit=""B"", total=total)\n    for chunk in req.iter_content(chunk_size=1024):\n        if chunk: # filter out keep-alive new chunks\n            progress.update(len(chunk))\n            temp_file.write(chunk)\n    progress.close()\n\n\ndef get_from_cache(url, cache_dir=None, force_download=False, proxies=None):\n    """"""\n    Given a URL, look for the corresponding dataset in the local cache.\n    If it\'s not there, download it. Then return the path to the cached file.\n    """"""\n    if cache_dir is None:\n        cache_dir = PYTORCH_TRANSFORMERS_CACHE\n    if sys.version_info[0] == 3 and isinstance(cache_dir, Path):\n        cache_dir = str(cache_dir)\n    if sys.version_info[0] == 2 and not isinstance(cache_dir, str):\n        cache_dir = str(cache_dir)\n\n    if not os.path.exists(cache_dir):\n        os.makedirs(cache_dir)\n\n    # Get eTag to add to filename, if it exists.\n    if url.startswith(""s3://""):\n        etag = s3_etag(url, proxies=proxies)\n    else:\n        try:\n            response = requests.head(url, allow_redirects=True, proxies=proxies)\n            if response.status_code != 200:\n                etag = None\n            else:\n                etag = response.headers.get(""ETag"")\n        except EnvironmentError:\n            etag = None\n\n    if sys.version_info[0] == 2 and etag is not None:\n        etag = etag.decode(\'utf-8\')\n    filename = url_to_filename(url, etag)\n\n    # get cache path to put the file\n    cache_path = os.path.join(cache_dir, filename)\n\n    # If we don\'t have a connection (etag is None) and can\'t identify the file\n    # try to get the last downloaded one\n    if not os.path.exists(cache_path) and etag is None:\n        matching_files = fnmatch.filter(os.listdir(cache_dir), filename + \'.*\')\n        matching_files = list(filter(lambda s: not s.endswith(\'.json\'), matching_files))\n        if matching_files:\n            cache_path = os.path.join(cache_dir, matching_files[-1])\n\n    if not os.path.exists(cache_path) or force_download:\n        # Download to temporary file, then copy to cache dir once finished.\n        # Otherwise you get corrupt cache entries if the download gets interrupted.\n        with tempfile.NamedTemporaryFile() as temp_file:\n            logger.info(""%s not found in cache or force_download set to True, downloading to %s"", url, temp_file.name)\n\n            # GET file object\n            if url.startswith(""s3://""):\n                s3_get(url, temp_file, proxies=proxies)\n            else:\n                http_get(url, temp_file, proxies=proxies)\n\n            # we are copying the file before closing it, so flush to avoid truncation\n            temp_file.flush()\n            # shutil.copyfileobj() starts at the current position, so go to the start\n            temp_file.seek(0)\n\n            logger.info(""copying %s to cache at %s"", temp_file.name, cache_path)\n            with open(cache_path, \'wb\') as cache_file:\n                shutil.copyfileobj(temp_file, cache_file)\n\n            logger.info(""creating metadata file for %s"", cache_path)\n            meta = {\'url\': url, \'etag\': etag}\n            meta_path = cache_path + \'.json\'\n            with open(meta_path, \'w\') as meta_file:\n                output_string = json.dumps(meta)\n                if sys.version_info[0] == 2 and isinstance(output_string, str):\n                    output_string = unicode(output_string, \'utf-8\')  # The beauty of python 2\n                meta_file.write(output_string)\n\n            logger.info(""removing temp file %s"", temp_file.name)\n\n    return cache_path\n'"
model/modeling_albert.py,81,"b'""""""PyTorch ALBERT model. """"""\nfrom __future__ import absolute_import, division, print_function, unicode_literals\nimport logging\nimport math\nimport os\nimport sys\nimport torch\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss, MSELoss\nfrom .modeling_utils import PreTrainedModel, prune_linear_layer\nfrom .configuration_albert import AlbertConfig\nfrom .file_utils import add_start_docstrings\nlogger = logging.getLogger(__name__)\n\nALBERT_PRETRAINED_MODEL_ARCHIVE_MAP = {\n    \'albert-base\': """",\n    \'albert-large\': """",\n    \'albert-xlarge\': """",\n    \'albert-xxlarge\': """",\n}\ndef load_tf_weights_in_albert(model, config, tf_checkpoint_path):\n    """""" Load tf checkpoints in a pytorch model.\n    """"""\n    try:\n        import re\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        logger.error(""Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see ""\n                     ""https://www.tensorflow.org/install/ for installation instructions."")\n        raise\n    tf_path = os.path.abspath(tf_checkpoint_path)\n    logger.info(""Converting TensorFlow checkpoint from {}"".format(tf_path))\n    if not os.path.exists(tf_path+\'/checkpoint\'):\n        tf_path = tf_path + ""/variables/variables""\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    names = []\n    arrays = []\n    for name, shape in init_vars:\n        logger.info(""Loading TF weight {} with shape {}"".format(name, shape))\n        array = tf.train.load_variable(tf_path, name)\n        names.append(name)\n        arrays.append(array)\n    for name, array in zip(names, arrays):\n        name = name.replace(""attention_1"",""attention"")\n        name = name.replace(""ffn_1"",""ffn"")\n        name = name.split(\'/\')\n        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n        # which are not required for using pretrained model\n        if any(n in [""adam_v"", ""adam_m"", ""global_step""] for n in name):\n            logger.info(""Skipping {}"".format(""/"".join(name)))\n            continue\n        pointer = model\n        for m_name in name:\n            if re.fullmatch(r\'[A-Za-z]+_\\d+\', m_name):\n                l = re.split(r\'_(\\d+)\', m_name)\n            elif re.fullmatch(r\'[A-Za-z]+_+[A-Za-z]+_\\d+\', m_name):\n                l = re.split(r\'_(\\d+)\', m_name)\n            else:\n                l = [m_name]\n            if l[0] in [\'LayerNorm\', \'attention\', \'ffn\'] and len(l) >= 2:\n                l = [""_"".join(l[:-1])]\n            if l[0] == \'kernel\' or l[0] == \'gamma\':\n                pointer = getattr(pointer, \'weight\')\n            elif l[0] == \'output_bias\' or l[0] == \'beta\':\n                pointer = getattr(pointer, \'bias\')\n            elif l[0] == \'output_weights\':\n                pointer = getattr(pointer, \'weight\')\n            elif l[0] == \'squad\':\n                pointer = getattr(pointer, \'classifier\')\n            else:\n                try:\n                    pointer = getattr(pointer, l[0])\n                except AttributeError:\n                    logger.info(""Skipping {}"".format(""/"".join(name)))\n                    continue\n            if len(l) >= 2:\n                num = int(l[1])\n                pointer = pointer[num]\n\n        if m_name[-11:] == \'_embeddings\':\n            pointer = getattr(pointer, \'weight\')\n        elif m_name == \'kernel\':\n            array = np.transpose(array)\n        try:\n            assert pointer.shape == array.shape\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        logger.info(""Initialize PyTorch weight {}"".format(name))\n        pointer.data = torch.from_numpy(array)\n    return model\n\ndef gelu(x):\n    """""" Original Implementation of the gelu activation function in Google Bert repo when initially created.\n        For information: OpenAI GPT\'s gelu is slightly different (and gives slightly different results):\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n        Also see https://arxiv.org/abs/1606.08415\n    """"""\n    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n\ndef gelu_new(x):\n    """""" Implementation of the gelu activation function currently in Google Bert repo (identical to OpenAI GPT).\n        Also see https://arxiv.org/abs/1606.08415\n    """"""\n    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n\ndef swish(x):\n    return x * torch.sigmoid(x)\n\nACT2FN = {""gelu"": gelu, ""relu"": torch.nn.functional.relu, ""swish"": swish, ""gelu_new"": gelu_new}\nAlbertLayerNorm = torch.nn.LayerNorm\n\nclass AlbertEmbeddings(nn.Module):\n    """"""Construct the embeddings from word, position and token_type embeddings.\n    """"""\n    def __init__(self, config):\n        super(AlbertEmbeddings, self).__init__()\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.embedding_size, padding_idx=0)\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.embedding_size)\n        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.embedding_size)\n        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n        self.LayerNorm = AlbertLayerNorm(config.embedding_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, input_ids, token_type_ids=None, position_ids=None):\n        seq_length = input_ids.size(1)\n        if position_ids is None:\n            position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n        words_embeddings = self.word_embeddings(input_ids)\n        position_embeddings = self.position_embeddings(position_ids)\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings\n\nclass AlbertSelfAttention(nn.Module):\n    def __init__(self, config):\n        super(AlbertSelfAttention, self).__init__()\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                ""The hidden size (%d) is not a multiple of the number of attention ""\n                ""heads (%d)"" % (config.hidden_size, config.num_attention_heads))\n        self.output_attentions = config.output_attentions\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(self, hidden_states, attention_mask=None, head_mask=None):\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n\n        # Take the dot product between ""query"" and ""key"" to get the raw attention scores.\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n            attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout(attention_probs)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attention_probs = attention_probs * head_mask\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n        outputs = (context_layer, attention_probs) if self.output_attentions else (context_layer,)\n        return outputs\n\nclass AlbertSelfOutput(nn.Module):\n    def __init__(self, config):\n        super(AlbertSelfOutput, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        return hidden_states\n\nclass AlbertAttention(nn.Module):\n    def __init__(self, config):\n        super(AlbertAttention, self).__init__()\n        self.self = AlbertSelfAttention(config)\n        self.output = AlbertSelfOutput(config)\n        self.pruned_heads = set()\n\n    def prune_heads(self, heads):\n        if len(heads) == 0:\n            return\n        mask = torch.ones(self.self.num_attention_heads, self.self.attention_head_size)\n        heads = set(heads) - self.pruned_heads  # Convert to set and emove already pruned heads\n        for head in heads:\n            # Compute how many pruned heads are before the head and move the index accordingly\n            head = head - sum(1 if h < head else 0 for h in self.pruned_heads)\n            mask[head] = 0\n        mask = mask.view(-1).contiguous().eq(1)\n        index = torch.arange(len(mask))[mask].long()\n\n        # Prune linear layers\n        self.self.query = prune_linear_layer(self.self.query, index)\n        self.self.key = prune_linear_layer(self.self.key, index)\n        self.self.value = prune_linear_layer(self.self.value, index)\n        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n\n        # Update hyper params and store pruned heads\n        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n        self.pruned_heads = self.pruned_heads.union(heads)\n\n    def forward(self, input_tensor, attention_mask=None, head_mask=None):\n        self_outputs = self.self(input_tensor, attention_mask, head_mask)\n        attention_output = self.output(self_outputs[0], input_tensor)\n        outputs = (attention_output,self_outputs)\n        return outputs\n\nclass AlbertOutput(nn.Module):\n    def __init__(self, config):\n        super(AlbertOutput, self).__init__()\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        return hidden_states\n\nclass AlbertIntermediate(nn.Module):\n    def __init__(self, config):\n        super(AlbertIntermediate, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n        self.output = AlbertOutput(config)\n        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.intermediate_act_fn = config.hidden_act\n\n    def forward(self, hidden_states):\n        intermediate_output = self.dense(hidden_states)\n        intermediate_output = self.intermediate_act_fn(intermediate_output)\n        output = self.output(intermediate_output)\n        return output\n\nclass AlbertFFN(nn.Module):\n    def __init__(self, config):\n        super(AlbertFFN, self).__init__()\n        self.intermediate = AlbertIntermediate(config)\n\n    def forward(self, attention_output):\n        output = self.intermediate(attention_output)\n        return output\n\nclass AlbertLayer(nn.Module):\n    def __init__(self, config):\n        super(AlbertLayer, self).__init__()\n        self.attention = AlbertAttention(config)\n        self.ffn = AlbertFFN(config)\n        self.LayerNorm = AlbertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.LayerNorm_1 = AlbertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n\n    def forward(self, hidden_states, attention_mask=None, head_mask=None):\n        attention_outputs = self.attention(hidden_states, attention_mask, head_mask)\n        attention_output = self.LayerNorm(attention_outputs[0] + hidden_states)\n        ffn_output = self.ffn(attention_output)\n        ffn_output = self.LayerNorm_1(ffn_output+attention_output)\n        outputs = (ffn_output,) + attention_outputs[1:] # add attentions if we output them\n        return outputs\n\nclass AlbertGroup(nn.Module):\n    def __init__(self, config):\n        super(AlbertGroup, self).__init__()\n        self.inner_group_num = config.inner_group_num\n        self.inner_group = nn.ModuleList([AlbertLayer(config) for _ in range(config.inner_group_num)])\n\n    def forward(self, hidden_states, attention_mask, head_mask):\n        layer_attentions = ()\n        layer_hidden_states = ()\n        for inner_group_idx in range(self.inner_group_num):\n            layer_module = self.inner_group[inner_group_idx]\n            layer_outputs = layer_module(hidden_states, attention_mask, head_mask)\n            hidden_states = layer_outputs[0]\n            layer_attentions = layer_attentions + (layer_outputs[1],)\n            layer_hidden_states = layer_hidden_states + (hidden_states,)\n        return (layer_hidden_states, layer_attentions)\n\nclass AlbertTransformer(nn.Module):\n    def __init__(self, config):\n        super(AlbertTransformer, self).__init__()\n        self.output_attentions = config.output_attentions\n        self.output_hidden_states = config.output_hidden_states\n        self.num_hidden_layers = config.num_hidden_layers\n        self.num_hidden_groups = config.num_hidden_groups\n        self.group = nn.ModuleList([AlbertGroup(config) for _ in range(config.num_hidden_groups)])\n\n    def forward(self, hidden_states, attention_mask, head_mask):\n        all_hidden_states = ()\n        all_attentions = ()\n        for layer_idx in range(self.num_hidden_layers):\n            if self.output_hidden_states and layer_idx == 0:\n                all_hidden_states = all_hidden_states + (hidden_states,)\n            group_idx = int(layer_idx / self.num_hidden_layers * self.num_hidden_groups)\n            layer_module = self.group[group_idx]\n            layer_outputs = layer_module(hidden_states, attention_mask, head_mask[layer_idx])\n            hidden_states = layer_outputs[0][-1]\n            if self.output_attentions:\n                all_attentions = all_attentions + layer_outputs[1]\n            if self.output_hidden_states:\n                all_hidden_states = all_hidden_states + layer_outputs[0]\n        outputs = (hidden_states,)\n        if self.output_hidden_states:\n            outputs = outputs + (all_hidden_states,)\n        if self.output_attentions:\n            outputs = outputs + (all_attentions,)\n        return outputs  # last-layer hidden state, (all hidden states), (all attentions)\n\nclass AlbertEncoder(nn.Module):\n    def __init__(self, config):\n        super(AlbertEncoder, self).__init__()\n        self.hidden_size = config.hidden_size\n        self.embedding_size = config.embedding_size\n        self.embedding_hidden_mapping_in = nn.Linear(self.embedding_size, self.hidden_size)\n        self.transformer = AlbertTransformer(config)\n\n    def forward(self, hidden_states, attention_mask=None, head_mask=None):\n        if self.embedding_size != self.hidden_size:\n            prev_output = self.embedding_hidden_mapping_in(hidden_states)\n        else:\n            prev_output = hidden_states\n        outputs = self.transformer(prev_output, attention_mask, head_mask)\n        return outputs  # last-layer hidden state, (all hidden states), (all attentions)\n\nclass AlbertPooler(nn.Module):\n    def __init__(self, config):\n        super(AlbertPooler, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.activation = nn.Tanh()\n\n    def forward(self, hidden_states):\n        # We ""pool"" the model by simply taking the hidden state corresponding\n        # to the first token.\n        first_token_tensor = hidden_states[:, 0]\n        pooled_output = self.dense(first_token_tensor)\n        pooled_output = self.activation(pooled_output)\n        return pooled_output\n\nclass AlbertPredictionHeadTransform(nn.Module):\n    def __init__(self, config):\n        super(AlbertPredictionHeadTransform, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.embedding_size)\n        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n            self.transform_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.transform_act_fn = config.hidden_act\n        self.LayerNorm = AlbertLayerNorm(config.embedding_size, eps=config.layer_norm_eps)\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.transform_act_fn(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states)\n        return hidden_states\n\nclass AlbertLMPredictionHead(nn.Module):\n    def __init__(self, config):\n        super(AlbertLMPredictionHead, self).__init__()\n        self.transform = AlbertPredictionHeadTransform(config)\n        # The output weights are the same as the input embeddings, but there is\n        # an output-only bias for each token.\n        self.decoder = nn.Linear(config.embedding_size,config.vocab_size,bias=False)\n        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n\n    def forward(self, hidden_states):\n        hidden_states = self.transform(hidden_states)\n        hidden_states = self.decoder(hidden_states) + self.bias\n        return hidden_states\n\nclass AlbertOnlyMLMHead(nn.Module):\n    def __init__(self, config):\n        super(AlbertOnlyMLMHead, self).__init__()\n        self.predictions = AlbertLMPredictionHead(config)\n\n    def forward(self, sequence_output):\n        prediction_scores = self.predictions(sequence_output)\n        return prediction_scores\n\nclass AlbertOnlyNSPHead(nn.Module):\n    def __init__(self, config):\n        super(AlbertOnlyNSPHead, self).__init__()\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n\n    def forward(self, pooled_output):\n        seq_relationship_score = self.seq_relationship(pooled_output)\n        return seq_relationship_score\n\nclass AlbertPreTrainingHeads(nn.Module):\n    def __init__(self, config):\n        super(AlbertPreTrainingHeads, self).__init__()\n        self.predictions = AlbertLMPredictionHead(config)\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n\n    def forward(self, sequence_output, pooled_output):\n        prediction_scores = self.predictions(sequence_output)\n        seq_relationship_score = self.seq_relationship(pooled_output)\n        return prediction_scores, seq_relationship_score\n\nclass AlbertPreTrainedModel(PreTrainedModel):\n    """""" An abstract class to handle weights initialization and\n        a simple interface for dowloading and loading pretrained models.\n    """"""\n    config_class = AlbertConfig\n    pretrained_model_archive_map = ALBERT_PRETRAINED_MODEL_ARCHIVE_MAP\n    load_tf_weights = load_tf_weights_in_albert\n    base_model_prefix = ""bert""\n\n    def _init_weights(self, module):\n        """""" Initialize the weights """"""\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n        elif isinstance(module, AlbertLayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n\n\nALBERT_START_DOCSTRING = r""""""    The ALBERT model was proposed in\n    `ALBERT: A Lite BERT for Self-supervised Learning of Language Representations`_\n    by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut. \n    This model is a PyTorch `torch.nn.Module`_ sub-class. Use it as a regular PyTorch Module and\n    refer to the PyTorch documentation for all matter related to general usage and behavior.\n    .. _`BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`:\n        https://arxiv.org/abs/1909.11942\n    .. _`torch.nn.Module`:\n        https://pytorch.org/docs/stable/nn.html#module\n    Parameters:\n        config (:class:`~transformers.ALbertConfig`): Model configuration class with all the parameters of the model. \n            Initializing with a config file does not load the weights associated with the model, only the configuration.\n            Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model weights.\n""""""\n\nALBERT_INPUTS_DOCSTRING = r""""""\n    Inputs:\n        **input_ids**: ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Indices of input sequence tokens in the vocabulary.\n            To match pre-training, ALBERT input sequence should be formatted with [CLS] and [SEP] tokens as follows:\n            (a) For sequence pairs:\n                ``tokens:         [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]``\n                ``token_type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1``\n            (b) For single sequences:\n                ``tokens:         [CLS] the dog is hairy . [SEP]``\n                ``token_type_ids:   0   0   0   0  0     0   0``\n            ALBert is a model with absolute position embeddings so it\'s usually advised to pad the inputs on\n            the right rather than the left.\n            Indices can be obtained using :class:`transformers.BertTokenizer`.\n            See :func:`transformers.PreTrainedTokenizer.encode` and\n            :func:`transformers.PreTrainedTokenizer.convert_tokens_to_ids` for details.\n        **attention_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, sequence_length)``:\n            Mask to avoid performing attention on padding token indices.\n            Mask values selected in ``[0, 1]``:\n            ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n        **token_type_ids**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Segment token indices to indicate first and second portions of the inputs.\n            Indices are selected in ``[0, 1]``: ``0`` corresponds to a `sentence A` token, ``1``\n            corresponds to a `sentence B` token\n            (see `BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`_ for more details).\n        **position_ids**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Indices of positions of each input sequence tokens in the position embeddings.\n            Selected in the range ``[0, config.max_position_embeddings - 1]``.\n        **head_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(num_heads,)`` or ``(num_layers, num_heads)``:\n            Mask to nullify selected heads of the self-attention modules.\n            Mask values selected in ``[0, 1]``:\n            ``1`` indicates the head is **not masked**, ``0`` indicates the head is **masked**.\n""""""\n\n@add_start_docstrings(""The bare Albert Model transformer outputting raw hidden-states without any specific head on top."",\n                      ALBERT_START_DOCSTRING, ALBERT_INPUTS_DOCSTRING)\nclass AlbertModel(AlbertPreTrainedModel):\n    r""""""\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **last_hidden_state**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, hidden_size)``\n            Sequence of hidden-states at the output of the last layer of the model.\n        **pooler_output**: ``torch.FloatTensor`` of shape ``(batch_size, hidden_size)``\n            Last layer hidden-state of the first token of the sequence (classification token)\n            further processed by a Linear layer and a Tanh activation function. The Linear\n            layer weights are trained from the next sentence prediction (classification)\n            objective during Bert pretraining. This output is usually *not* a good summary\n            of the semantic content of the input, you\'re often better with averaging or pooling\n            the sequence of hidden-states for the whole input sequence.\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n    Examples::\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n        model = BertModel.from_pretrained(\'bert-base-uncased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids)\n        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n    """"""\n\n    def __init__(self, config):\n        super(AlbertModel, self).__init__(config)\n\n        self.embeddings = AlbertEmbeddings(config)\n        self.encoder = AlbertEncoder(config)\n        self.pooler = AlbertPooler(config)\n\n        self.init_weights()\n\n    def _resize_token_embeddings(self, new_num_tokens):\n        old_embeddings = self.embeddings.word_embeddings\n        new_embeddings = self._get_resized_embeddings(old_embeddings, new_num_tokens)\n        self.embeddings.word_embeddings = new_embeddings\n        return self.embeddings.word_embeddings\n\n    def _prune_heads(self, heads_to_prune):\n        """""" Prunes heads of the model.\n            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n            See base class PreTrainedModel\n        """"""\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n        if attention_mask is None:\n            attention_mask = torch.ones_like(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        # We create a 3D attention mask from a 2D tensor mask.\n        # Sizes are [batch_size, 1, 1, to_seq_length]\n        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n        # this attention mask is more simple than the triangular masking of causal attention\n        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        if head_mask is not None:\n            if head_mask.dim() == 1:\n                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n                head_mask = head_mask.expand(self.config.num_hidden_layers, -1, -1, -1, -1)\n            elif head_mask.dim() == 2:\n                head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(\n                    -1)  # We can specify head_mask for each layer\n            head_mask = head_mask.to(\n                dtype=next(self.parameters()).dtype)  # switch to fload if need + fp16 compatibility\n        else:\n            head_mask = [None] * self.config.num_hidden_layers\n\n        embedding_output = self.embeddings(input_ids, position_ids=position_ids, token_type_ids=token_type_ids)\n        encoder_outputs = self.encoder(embedding_output,\n                                       extended_attention_mask,\n                                       head_mask=head_mask)\n        sequence_output = encoder_outputs[0]\n        pooled_output = self.pooler(sequence_output)\n\n        outputs = (sequence_output, pooled_output,) + encoder_outputs[\n                                                      1:]  # add hidden_states and attentions if they are here\n        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\n\n@add_start_docstrings(""""""Bert Model with two heads on top as done during the pre-training:\n    a `masked language modeling` head and a `next sentence prediction (classification)` head. """""",\n                      ALBERT_START_DOCSTRING, ALBERT_INPUTS_DOCSTRING)\nclass AlbertForPreTraining(AlbertPreTrainedModel):\n    r""""""\n        **masked_lm_labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Labels for computing the masked language modeling loss.\n            Indices should be in ``[-1, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring)\n            Tokens with indices set to ``-1`` are ignored (masked), the loss is only computed for the tokens with labels\n            in ``[0, ..., config.vocab_size]``\n        **next_sentence_label**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair (see ``input_ids`` docstring)\n            Indices should be in ``[0, 1]``.\n            ``0`` indicates sequence B is a continuation of sequence A,\n            ``1`` indicates sequence B is a random sequence.\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when both ``masked_lm_labels`` and ``next_sentence_label`` are provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Total loss as the sum of the masked language modeling loss and the next sequence prediction (classification) loss.\n        **prediction_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.vocab_size)``\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n        **seq_relationship_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, 2)``\n            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n    Examples::\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n        model = BertForPreTraining.from_pretrained(\'bert-base-uncased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids)\n        prediction_scores, seq_relationship_scores = outputs[:2]\n    """"""\n\n    def __init__(self, config):\n        super(AlbertForPreTraining, self).__init__(config)\n        self.bert = AlbertModel(config)\n        self.cls = AlbertPreTrainingHeads(config)\n\n        self.init_weights()\n        self.tie_weights()\n\n    def tie_weights(self):\n        """""" Make sure we are sharing the input and output embeddings.\n            Export to TorchScript can\'t handle parameter sharing so we are cloning them instead.\n        """"""\n        self._tie_or_clone_weights(self.cls.predictions.decoder,\n                                   self.bert.embeddings.word_embeddings)\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n                masked_lm_labels=None, next_sentence_label=None):\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids,\n                            head_mask=head_mask)\n\n        sequence_output, pooled_output = outputs[:2]\n        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n\n        outputs = (prediction_scores, seq_relationship_score,) + outputs[\n                                                                 2:]  # add hidden states and attention if they are here\n\n        if masked_lm_labels is not None and next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n            total_loss = masked_lm_loss + next_sentence_loss\n            outputs = (total_loss,) + outputs\n        return outputs  # (loss), prediction_scores, seq_relationship_score, (hidden_states), (attentions)\n\n@add_start_docstrings(""""""Bert Model with a `language modeling` head on top. """""",\n                      ALBERT_START_DOCSTRING, ALBERT_INPUTS_DOCSTRING)\nclass AlbertForMaskedLM(AlbertPreTrainedModel):\n    r""""""\n        **masked_lm_labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Labels for computing the masked language modeling loss.\n            Indices should be in ``[-1, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring)\n            Tokens with indices set to ``-1`` are ignored (masked), the loss is only computed for the tokens with labels\n            in ``[0, ..., config.vocab_size]``\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``masked_lm_labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Masked language modeling loss.\n        **prediction_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.vocab_size)``\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n    Examples::\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n        model = BertForMaskedLM.from_pretrained(\'bert-base-uncased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, masked_lm_labels=input_ids)\n        loss, prediction_scores = outputs[:2]\n    """"""\n\n    def __init__(self, config):\n        super(AlbertForMaskedLM, self).__init__(config)\n\n        self.bert = AlbertModel(config)\n        self.cls = AlbertOnlyMLMHead(config)\n\n        self.init_weights()\n        self.tie_weights()\n\n    def tie_weights(self):\n        """""" Make sure we are sharing the input and output embeddings.\n            Export to TorchScript can\'t handle parameter sharing so we are cloning them instead.\n        """"""\n        self._tie_or_clone_weights(self.cls.predictions.decoder,\n                                   self.bert.embeddings.word_embeddings)\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n                masked_lm_labels=None):\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids,\n                            head_mask=head_mask)\n\n        sequence_output = outputs[0]\n        prediction_scores = self.cls(sequence_output)\n\n        outputs = (prediction_scores,) + outputs[2:]  # Add hidden states and attention if they are here\n        if masked_lm_labels is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n            outputs = (masked_lm_loss,) + outputs\n\n        return outputs  # (masked_lm_loss), prediction_scores, (hidden_states), (attentions)\n\n\n@add_start_docstrings(""""""Bert Model with a `next sentence prediction (classification)` head on top. """""",\n                      ALBERT_START_DOCSTRING, ALBERT_INPUTS_DOCSTRING)\nclass AlbertForNextSentencePrediction(AlbertPreTrainedModel):\n    r""""""\n        **next_sentence_label**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair (see ``input_ids`` docstring)\n            Indices should be in ``[0, 1]``.\n            ``0`` indicates sequence B is a continuation of sequence A,\n            ``1`` indicates sequence B is a random sequence.\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``next_sentence_label`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Next sequence prediction (classification) loss.\n        **seq_relationship_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, 2)``\n            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n    Examples::\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n        model = BertForNextSentencePrediction.from_pretrained(\'bert-base-uncased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids)\n        seq_relationship_scores = outputs[0]\n    """"""\n\n    def __init__(self, config):\n        super(AlbertForNextSentencePrediction, self).__init__(config)\n\n        self.bert = AlbertModel(config)\n        self.cls = AlbertOnlyNSPHead(config)\n\n        self.init_weights()\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n                next_sentence_label=None):\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids,\n                            head_mask=head_mask)\n\n        pooled_output = outputs[1]\n\n        seq_relationship_score = self.cls(pooled_output)\n\n        outputs = (seq_relationship_score,) + outputs[2:]  # add hidden states and attention if they are here\n        if next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n            outputs = (next_sentence_loss,) + outputs\n\n        return outputs  # (next_sentence_loss), seq_relationship_score, (hidden_states), (attentions)\n\n\n@add_start_docstrings(""""""Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of\n    the pooled output) e.g. for GLUE tasks. """""",\n                      ALBERT_START_DOCSTRING, ALBERT_INPUTS_DOCSTRING)\nclass AlbertForSequenceClassification(AlbertPreTrainedModel):\n    r""""""\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the sequence classification/regression loss.\n            Indices should be in ``[0, ..., config.num_labels - 1]``.\n            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Classification (or regression if config.num_labels==1) loss.\n        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n    Examples::\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n        model = BertForSequenceClassification.from_pretrained(\'bert-base-uncased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, labels=labels)\n        loss, logits = outputs[:2]\n    """"""\n\n    def __init__(self, config):\n        super(AlbertForSequenceClassification, self).__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = AlbertModel(config)\n        self.dropout = nn.Dropout(0.1 if config.hidden_dropout_prob == 0 else config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n\n        self.init_weights()\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None,\n                position_ids=None, head_mask=None, labels=None):\n\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids,\n                            head_mask=head_mask)\n\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output+0.1)\n        logits = self.classifier(pooled_output)\n\n        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n\n        if labels is not None:\n            if self.num_labels == 1:\n                #  We are doing regression\n                loss_fct = MSELoss()\n                loss = loss_fct(logits.view(-1), labels.view(-1))\n            else:\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            outputs = (loss,) + outputs\n\n        return outputs  # (loss), logits, (hidden_states), (attentions)\n\n\n@add_start_docstrings(""""""Bert Model with a multiple choice classification head on top (a linear layer on top of\n    the pooled output and a softmax) e.g. for RocStories/SWAG tasks. """""",\n                      ALBERT_START_DOCSTRING, ALBERT_INPUTS_DOCSTRING)\nclass AlbertForMultipleChoice(AlbertPreTrainedModel):\n    r""""""\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the multiple choice classification loss.\n            Indices should be in ``[0, ..., num_choices]`` where `num_choices` is the size of the second dimension\n            of the input tensors. (see `input_ids` above)\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Classification loss.\n        **classification_scores**: ``torch.FloatTensor`` of shape ``(batch_size, num_choices)`` where `num_choices` is the size of the second dimension\n            of the input tensors. (see `input_ids` above).\n            Classification scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n    Examples::\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n        model = BertForMultipleChoice.from_pretrained(\'bert-base-uncased\')\n        choices = [""Hello, my dog is cute"", ""Hello, my cat is amazing""]\n        input_ids = torch.tensor([tokenizer.encode(s) for s in choices]).unsqueeze(0)  # Batch size 1, 2 choices\n        labels = torch.tensor(1).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, labels=labels)\n        loss, classification_scores = outputs[:2]\n    """"""\n\n    def __init__(self, config):\n        super(AlbertForMultipleChoice, self).__init__(config)\n\n        self.bert = AlbertModel(config)\n        self.dropout = nn.Dropout(0.1 if config.hidden_dropout_prob == 0 else config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n\n        self.init_weights()\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None,\n                position_ids=None, head_mask=None, labels=None):\n        num_choices = input_ids.shape[1]\n\n        input_ids = input_ids.view(-1, input_ids.size(-1))\n        attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n        position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids,\n                            head_mask=head_mask)\n        pooled_output = outputs[1]\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        reshaped_logits = logits.view(-1, num_choices)\n        outputs = (reshaped_logits,) + outputs[2:]  # add hidden states and attention if they are here\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(reshaped_logits, labels)\n            outputs = (loss,) + outputs\n\n        return outputs  # (loss), reshaped_logits, (hidden_states), (attentions)\n\n\n@add_start_docstrings(""""""Bert Model with a token classification head on top (a linear layer on top of\n    the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. """""",\n                      ALBERT_START_DOCSTRING, ALBERT_INPUTS_DOCSTRING)\n\nclass AlbertForTokenClassification(AlbertPreTrainedModel):\n    r""""""\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Labels for computing the token classification loss.\n            Indices should be in ``[0, ..., config.num_labels - 1]``.\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Classification loss.\n        **scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.num_labels)``\n            Classification scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n    Examples::\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n        model = BertForTokenClassification.from_pretrained(\'bert-base-uncased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        labels = torch.tensor([1] * input_ids.size(1)).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, labels=labels)\n        loss, scores = outputs[:2]\n    """"""\n\n    def __init__(self, config):\n        super(AlbertForTokenClassification, self).__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = AlbertModel(config)\n        self.dropout = nn.Dropout(0.1 if config.hidden_dropout_prob == 0 else config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None,\n                position_ids=None, head_mask=None, labels=None):\n\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids,\n                            head_mask=head_mask)\n\n        sequence_output = outputs[0]\n\n        sequence_output = self.dropout(sequence_output)\n        logits = self.classifier(sequence_output)\n\n        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            # Only keep active parts of the loss\n            if attention_mask is not None:\n                active_loss = attention_mask.view(-1) == 1\n                active_logits = logits.view(-1, self.num_labels)[active_loss]\n                active_labels = labels.view(-1)[active_loss]\n                loss = loss_fct(active_logits, active_labels)\n            else:\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            outputs = (loss,) + outputs\n\n        return outputs  # (loss), scores, (hidden_states), (attentions)\n\n\n@add_start_docstrings(""""""Bert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear layers on top of\n    the hidden-states output to compute `span start logits` and `span end logits`). """""",\n                      ALBERT_START_DOCSTRING, ALBERT_INPUTS_DOCSTRING)\nclass AlbertForQuestionAnswering(AlbertPreTrainedModel):\n    r""""""\n        **start_positions**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`).\n            Position outside of the sequence are not taken into account for computing the loss.\n        **end_positions**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`).\n            Position outside of the sequence are not taken into account for computing the loss.\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.\n        **start_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length,)``\n            Span-start scores (before SoftMax).\n        **end_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length,)``\n            Span-end scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n    Examples::\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n        model = BertForQuestionAnswering.from_pretrained(\'bert-base-uncased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        start_positions = torch.tensor([1])\n        end_positions = torch.tensor([3])\n        outputs = model(input_ids, start_positions=start_positions, end_positions=end_positions)\n        loss, start_scores, end_scores = outputs[:2]\n    """"""\n\n    def __init__(self, config):\n        super(AlbertForQuestionAnswering, self).__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = AlbertModel(config)\n        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n                start_positions=None, end_positions=None):\n\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids,\n                            head_mask=head_mask)\n\n        sequence_output = outputs[0]\n\n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        outputs = (start_logits, end_logits,) + outputs[2:]\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions.clamp_(0, ignored_index)\n            end_positions.clamp_(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n            outputs = (total_loss,) + outputs\n\n        return outputs  # (loss), start_logits, end_logits, (hidden_states), (attentions)\n'"
model/modeling_albert_bright.py,74,"b'""""""PyTorch brightmart version  ALBERT model. """"""\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\nimport logging\r\nimport os\r\n\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import CrossEntropyLoss, MSELoss\r\n\r\nfrom .modeling_utils import PreTrainedModel, prune_linear_layer\r\nfrom .configuration_albert import AlbertConfig\r\nfrom .file_utils import add_start_docstrings\r\nfrom .modeling_bert import (ACT2FN,\r\n                            BertSelfAttention,\r\n                            BertIntermediate,\r\n                            BertPooler,\r\n                            BertPredictionHeadTransform)\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\nALBERT_PRETRAINED_MODEL_ARCHIVE_MAP = {\r\n    \'albert-base\': """",\r\n    \'albert-large\': """",\r\n    \'albert-xlarge\': """",\r\n    \'albert-xxlarge\': """",\r\n}\r\ndef load_tf_weights_in_albert(model, config, tf_checkpoint_path):\r\n    """""" Load tf checkpoints in a pytorch model.\r\n    """"""\r\n    try:\r\n        import re\r\n        import numpy as np\r\n        import tensorflow as tf\r\n    except ImportError:\r\n        logger.error(""Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see ""\r\n                     ""https://www.tensorflow.org/install/ for installation instructions."")\r\n        raise\r\n    tf_path = os.path.abspath(tf_checkpoint_path)\r\n    logger.info(""Converting TensorFlow checkpoint from {}"".format(tf_path))\r\n    # Load weights from TF model\r\n    init_vars = tf.train.list_variables(tf_path)\r\n    names = []\r\n    arrays = []\r\n    for name, shape in init_vars:\r\n        logger.info(""Loading TF weight {} with shape {}"".format(name, shape))\r\n        array = tf.train.load_variable(tf_path, name)\r\n        names.append(name)\r\n        arrays.append(array)\r\n    for name, array in zip(names, arrays):\r\n        name = name.split(\'/\')\r\n        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\r\n        # which are not required for using pretrained model\r\n        if any(n in [""adam_v"", ""adam_m"", ""global_step""] for n in name):\r\n            logger.info(""Skipping {}"".format(""/"".join(name)))\r\n            continue\r\n        pointer = model\r\n        for m_name in name:\r\n            if re.fullmatch(r\'[A-Za-z]+_\\d+\', m_name):\r\n                l = re.split(r\'_(\\d+)\', m_name)\r\n            else:\r\n                l = [m_name]\r\n            if l[0] == \'kernel\' or l[0] == \'gamma\':\r\n                pointer = getattr(pointer, \'weight\')\r\n            elif l[0] == \'output_bias\' or l[0] == \'beta\':\r\n                pointer = getattr(pointer, \'bias\')\r\n            elif l[0] == \'output_weights\':\r\n                pointer = getattr(pointer, \'weight\')\r\n            elif l[0] == \'squad\':\r\n                pointer = getattr(pointer, \'classifier\')\r\n            else:\r\n                try:\r\n                    pointer = getattr(pointer, l[0])\r\n                except AttributeError:\r\n                    logger.info(""Skipping {}"".format(""/"".join(name)))\r\n                    continue\r\n            if len(l) >= 2:\r\n                num = int(l[1])\r\n                pointer = pointer[num]\r\n        if m_name[-11:] == \'_embeddings\':\r\n            pointer = getattr(pointer, \'weight\')\r\n        elif m_name[-13:] == \'_embeddings_2\':\r\n            pointer = getattr(pointer, \'weight\')\r\n            array = np.transpose(array)\r\n        elif m_name == \'kernel\':\r\n            array = np.transpose(array)\r\n        try:\r\n            assert pointer.shape == array.shape\r\n        except AssertionError as e:\r\n            e.args += (pointer.shape, array.shape)\r\n            raise\r\n        logger.info(""Initialize PyTorch weight {}"".format(name))\r\n        pointer.data = torch.from_numpy(array)\r\n    return model\r\n\r\nAlbertLayerNorm = torch.nn.LayerNorm\r\nclass AlbertEmbeddings(nn.Module):\r\n    """"""Construct the embeddings from word, position and token_type embeddings.\r\n    """"""\r\n    def __init__(self, config):\r\n        super(AlbertEmbeddings, self).__init__()\r\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.embedding_size, padding_idx=0)\r\n        # project layer\r\n        self.word_embeddings_2 = nn.Linear(config.embedding_size, config.hidden_size, bias=False)\r\n\r\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\r\n        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\r\n\r\n        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\r\n        # any TensorFlow checkpoint file\r\n        self.LayerNorm =AlbertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\r\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\r\n\r\n    def forward(self, input_ids, token_type_ids=None, position_ids=None):\r\n        seq_length = input_ids.size(1)\r\n        if position_ids is None:\r\n            position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\r\n            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\r\n        if token_type_ids is None:\r\n            token_type_ids = torch.zeros_like(input_ids)\r\n\r\n        words_embeddings = self.word_embeddings(input_ids)\r\n        # project transform\r\n        words_embeddings = self.word_embeddings_2(words_embeddings)\r\n        position_embeddings = self.position_embeddings(position_ids)\r\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\r\n\r\n        embeddings = words_embeddings + position_embeddings + token_type_embeddings\r\n        embeddings = self.LayerNorm(embeddings)\r\n        embeddings = self.dropout(embeddings)\r\n        return embeddings\r\n\r\nclass AlbertSelfOutput(nn.Module):\r\n    def __init__(self, config):\r\n        super(AlbertSelfOutput, self).__init__()\r\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\r\n        self.LayerNorm = AlbertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\r\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\r\n\r\n    def forward(self, hidden_states, input_tensor):\r\n        hidden_states = self.dense(hidden_states)\r\n        hidden_states = self.dropout(hidden_states)\r\n        # postln\r\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\r\n        return hidden_states\r\n\r\n\r\nclass AlbertAttention(nn.Module):\r\n    def __init__(self, config):\r\n        super(AlbertAttention, self).__init__()\r\n        self.self = BertSelfAttention(config)\r\n        self.output = AlbertSelfOutput(config)\r\n        self.pruned_heads = set()\r\n\r\n    def prune_heads(self, heads):\r\n        if len(heads) == 0:\r\n            return\r\n        mask = torch.ones(self.self.num_attention_heads, self.self.attention_head_size)\r\n        heads = set(heads) - self.pruned_heads  # Convert to set and emove already pruned heads\r\n        for head in heads:\r\n            # Compute how many pruned heads are before the head and move the index accordingly\r\n            head = head - sum(1 if h < head else 0 for h in self.pruned_heads)\r\n            mask[head] = 0\r\n        mask = mask.view(-1).contiguous().eq(1)\r\n        index = torch.arange(len(mask))[mask].long()\r\n\r\n        # Prune linear layers\r\n        self.self.query = prune_linear_layer(self.self.query, index)\r\n        self.self.key = prune_linear_layer(self.self.key, index)\r\n        self.self.value = prune_linear_layer(self.self.value, index)\r\n        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\r\n\r\n        # Update hyper params and store pruned heads\r\n        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\r\n        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\r\n        self.pruned_heads = self.pruned_heads.union(heads)\r\n\r\n    def forward(self, input_tensor, attention_mask=None, head_mask=None):\r\n        # postln\r\n        self_outputs = self.self(input_tensor, attention_mask, head_mask)\r\n        attention_output = self.output(self_outputs[0], input_tensor)\r\n        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\r\n        return outputs\r\n\r\nclass AlbertOutput(nn.Module):\r\n    def __init__(self, config):\r\n        super(AlbertOutput, self).__init__()\r\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\r\n        self.LayerNorm = AlbertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\r\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\r\n\r\n    def forward(self, hidden_states, input_tensor):\r\n        hidden_states = self.dense(hidden_states)\r\n        hidden_states = self.dropout(hidden_states)\r\n        # postln\r\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\r\n        return hidden_states\r\n\r\nclass BertLayer(nn.Module):\r\n    def __init__(self, config):\r\n        super(BertLayer, self).__init__()\r\n        self.attention = AlbertAttention(config)\r\n        self.intermediate = BertIntermediate(config)\r\n        self.output = AlbertOutput(config)\r\n\r\n    def forward(self, hidden_states, attention_mask=None, head_mask=None):\r\n        attention_outputs = self.attention(hidden_states, attention_mask, head_mask)\r\n        attention_output = attention_outputs[0]\r\n        # postln\r\n        attention_output_pre = attention_output\r\n        intermediate_output = self.intermediate(attention_output_pre)\r\n        layer_output = self.output(intermediate_output, attention_output)\r\n        outputs = (layer_output,) + attention_outputs[1:]  # add attentions if we output them\r\n        return outputs\r\n\r\nclass AlbertEncoder(nn.Module):\r\n    def __init__(self, config):\r\n        super(AlbertEncoder, self).__init__()\r\n        self.output_attentions = config.output_attentions\r\n        self.output_hidden_states = config.output_hidden_states\r\n        self.num_hidden_layers = config.num_hidden_layers\r\n        self.layer_shared = BertLayer(config)\r\n\r\n    def forward(self, hidden_states, attention_mask=None, head_mask=None):\r\n        all_hidden_states = ()\r\n        all_attentions = ()\r\n        for i in range(self.num_hidden_layers):\r\n            layer_module = self.layer_shared\r\n            if self.output_hidden_states:\r\n                all_hidden_states = all_hidden_states + (hidden_states,)\r\n            layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i])\r\n            hidden_states = layer_outputs[0]\r\n\r\n            if self.output_attentions:\r\n                all_attentions = all_attentions + (layer_outputs[1],)\r\n        # Add last layer\r\n        if self.output_hidden_states:\r\n            all_hidden_states = all_hidden_states + (hidden_states,)\r\n        outputs = (hidden_states,)\r\n        if self.output_hidden_states:\r\n            outputs = outputs + (all_hidden_states,)\r\n        if self.output_attentions:\r\n            outputs = outputs + (all_attentions,)\r\n        return outputs  # last-layer hidden state, (all hidden states), (all attentions)\r\n\r\nclass AlbertLMPredictionHead(nn.Module):\r\n    def __init__(self, config):\r\n        super(AlbertLMPredictionHead, self).__init__()\r\n        self.transform = BertPredictionHeadTransform(config)\r\n        # The output weights are the same as the input embeddings, but there is\r\n        # an output-only bias for each token.\r\n        self.project_layer = nn.Linear(config.hidden_size, config.embedding_size, bias=False)\r\n        self.decoder = nn.Linear(config.embedding_size,\r\n                                 config.vocab_size,\r\n                                 bias=False)\r\n        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\r\n\r\n    def forward(self, hidden_states):\r\n        hidden_states = self.transform(hidden_states)\r\n        hidden_states = self.project_layer(hidden_states)\r\n        hidden_states = self.decoder(hidden_states) + self.bias\r\n        return hidden_states\r\n\r\nclass AlbertOnlyMLMHead(nn.Module):\r\n    def __init__(self, config):\r\n        super(AlbertOnlyMLMHead, self).__init__()\r\n        self.predictions = AlbertLMPredictionHead(config)\r\n\r\n    def forward(self, sequence_output):\r\n        prediction_scores = self.predictions(sequence_output)\r\n        return prediction_scores\r\n\r\nclass AlbertOnlyNSPHead(nn.Module):\r\n    def __init__(self, config):\r\n        super(AlbertOnlyNSPHead, self).__init__()\r\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\r\n\r\n    def forward(self, pooled_output):\r\n        seq_relationship_score = self.seq_relationship(pooled_output)\r\n        return seq_relationship_score\r\n\r\nclass AlbertPreTrainingHeads(nn.Module):\r\n    def __init__(self, config):\r\n        super(AlbertPreTrainingHeads, self).__init__()\r\n        self.predictions = AlbertLMPredictionHead(config)\r\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\r\n\r\n    def forward(self, sequence_output, pooled_output):\r\n        prediction_scores = self.predictions(sequence_output)\r\n        seq_relationship_score = self.seq_relationship(pooled_output)\r\n        return prediction_scores, seq_relationship_score\r\n\r\nclass AlbertPreTrainedModel(PreTrainedModel):\r\n    """""" An abstract class to handle weights initialization and\r\n        a simple interface for dowloading and loading pretrained models.\r\n    """"""\r\n    config_class = AlbertConfig\r\n    pretrained_model_archive_map = ALBERT_PRETRAINED_MODEL_ARCHIVE_MAP\r\n    load_tf_weights = load_tf_weights_in_albert\r\n    base_model_prefix = ""bert""\r\n\r\n    def _init_weights(self, module):\r\n        """""" Initialize the weights """"""\r\n        if isinstance(module, (nn.Linear, nn.Embedding)):\r\n            # Slightly different from the TF version which uses truncated_normal for initialization\r\n            # cf https://github.com/pytorch/pytorch/pull/5617\r\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\r\n        elif isinstance(module, AlbertLayerNorm):\r\n            module.bias.data.zero_()\r\n            module.weight.data.fill_(1.0)\r\n        if isinstance(module, nn.Linear) and module.bias is not None:\r\n            module.bias.data.zero_()\r\n\r\nBERT_START_DOCSTRING = r""""""    The BERT model was proposed in\r\n    `BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`_\r\n    by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It\'s a bidirectional transformer\r\n    pre-trained using a combination of masked language modeling objective and next sentence prediction\r\n    on a large corpus comprising the Toronto Book Corpus and Wikipedia.\r\n\r\n    This model is a PyTorch `torch.nn.Module`_ sub-class. Use it as a regular PyTorch Module and\r\n    refer to the PyTorch documentation for all matter related to general usage and behavior.\r\n\r\n    .. _`BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`:\r\n        https://arxiv.org/abs/1810.04805\r\n\r\n    .. _`torch.nn.Module`:\r\n        https://pytorch.org/docs/stable/nn.html#module\r\n\r\n    Parameters:\r\n        config (:class:`~transformers.BertConfig`): Model configuration class with all the parameters of the model. \r\n            Initializing with a config file does not load the weights associated with the model, only the configuration.\r\n            Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model weights.\r\n""""""\r\n\r\nBERT_INPUTS_DOCSTRING = r""""""\r\n    Inputs:\r\n        **input_ids**: ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\r\n            Indices of input sequence tokens in the vocabulary.\r\n            To match pre-training, BERT input sequence should be formatted with [CLS] and [SEP] tokens as follows:\r\n\r\n            (a) For sequence pairs:\r\n\r\n                ``tokens:         [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]``\r\n\r\n                ``token_type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1``\r\n\r\n            (b) For single sequences:\r\n\r\n                ``tokens:         [CLS] the dog is hairy . [SEP]``\r\n\r\n                ``token_type_ids:   0   0   0   0  0     0   0``\r\n\r\n            Bert is a model with absolute position embeddings so it\'s usually advised to pad the inputs on\r\n            the right rather than the left.\r\n\r\n            Indices can be obtained using :class:`transformers.BertTokenizer`.\r\n            See :func:`transformers.PreTrainedTokenizer.encode` and\r\n            :func:`transformers.PreTrainedTokenizer.convert_tokens_to_ids` for details.\r\n        **attention_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, sequence_length)``:\r\n            Mask to avoid performing attention on padding token indices.\r\n            Mask values selected in ``[0, 1]``:\r\n            ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\r\n        **token_type_ids**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\r\n            Segment token indices to indicate first and second portions of the inputs.\r\n            Indices are selected in ``[0, 1]``: ``0`` corresponds to a `sentence A` token, ``1``\r\n            corresponds to a `sentence B` token\r\n            (see `BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`_ for more details).\r\n        **position_ids**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\r\n            Indices of positions of each input sequence tokens in the position embeddings.\r\n            Selected in the range ``[0, config.max_position_embeddings - 1]``.\r\n        **head_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(num_heads,)`` or ``(num_layers, num_heads)``:\r\n            Mask to nullify selected heads of the self-attention modules.\r\n            Mask values selected in ``[0, 1]``:\r\n            ``1`` indicates the head is **not masked**, ``0`` indicates the head is **masked**.\r\n""""""\r\n\r\n\r\n@add_start_docstrings(""The bare Bert Model transformer outputting raw hidden-states without any specific head on top."",\r\n                      BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\r\nclass AlbertModel(AlbertPreTrainedModel):\r\n    r""""""\r\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\r\n        **last_hidden_state**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, hidden_size)``\r\n            Sequence of hidden-states at the output of the last layer of the model.\r\n        **pooler_output**: ``torch.FloatTensor`` of shape ``(batch_size, hidden_size)``\r\n            Last layer hidden-state of the first token of the sequence (classification token)\r\n            further processed by a Linear layer and a Tanh activation function. The Linear\r\n            layer weights are trained from the next sentence prediction (classification)\r\n            objective during Bert pretraining. This output is usually *not* a good summary\r\n            of the semantic content of the input, you\'re often better with averaging or pooling\r\n            the sequence of hidden-states for the whole input sequence.\r\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\r\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\r\n            of shape ``(batch_size, sequence_length, hidden_size)``:\r\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\r\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\r\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\r\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\r\n\r\n    Examples::\r\n\r\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\r\n        model = BertModel.from_pretrained(\'bert-base-uncased\')\r\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\r\n        outputs = model(input_ids)\r\n        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\r\n\r\n    """"""\r\n\r\n    def __init__(self, config):\r\n        super(AlbertModel, self).__init__(config)\r\n\r\n        self.embeddings = AlbertEmbeddings(config)\r\n        self.encoder = AlbertEncoder(config)\r\n        self.pooler = BertPooler(config)\r\n\r\n        self.init_weights()\r\n\r\n    def _resize_token_embeddings(self, new_num_tokens):\r\n        old_embeddings = self.embeddings.word_embeddings\r\n        new_embeddings = self._get_resized_embeddings(old_embeddings, new_num_tokens)\r\n        self.embeddings.word_embeddings = new_embeddings\r\n        return self.embeddings.word_embeddings\r\n\r\n    def _prune_heads(self, heads_to_prune):\r\n        """""" Prunes heads of the model.\r\n            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\r\n            See base class PreTrainedModel\r\n        """"""\r\n        for layer, heads in heads_to_prune.items():\r\n            self.encoder.layer[layer].attention.prune_heads(heads)\r\n\r\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\r\n        if attention_mask is None:\r\n            attention_mask = torch.ones_like(input_ids)\r\n        if token_type_ids is None:\r\n            token_type_ids = torch.zeros_like(input_ids)\r\n\r\n        # We create a 3D attention mask from a 2D tensor mask.\r\n        # Sizes are [batch_size, 1, 1, to_seq_length]\r\n        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\r\n        # this attention mask is more simple than the triangular masking of causal attention\r\n        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\r\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\r\n\r\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\r\n        # masked positions, this operation will create a tensor which is 0.0 for\r\n        # positions we want to attend and -10000.0 for masked positions.\r\n        # Since we are adding it to the raw scores before the softmax, this is\r\n        # effectively the same as removing these entirely.\r\n        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype)  # fp16 compatibility\r\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\r\n\r\n        # Prepare head mask if needed\r\n        # 1.0 in head_mask indicate we keep the head\r\n        # attention_probs has shape bsz x n_heads x N x N\r\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\r\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\r\n        if head_mask is not None:\r\n            if head_mask.dim() == 1:\r\n                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\r\n                head_mask = head_mask.expand(self.config.num_hidden_layers, -1, -1, -1, -1)\r\n            elif head_mask.dim() == 2:\r\n                head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(\r\n                    -1)  # We can specify head_mask for each layer\r\n            head_mask = head_mask.to(\r\n                dtype=next(self.parameters()).dtype)  # switch to fload if need + fp16 compatibility\r\n        else:\r\n            head_mask = [None] * self.config.num_hidden_layers\r\n\r\n        embedding_output = self.embeddings(input_ids, position_ids=position_ids, token_type_ids=token_type_ids)\r\n        encoder_outputs = self.encoder(embedding_output,\r\n                                       extended_attention_mask,\r\n                                       head_mask=head_mask)\r\n        sequence_output = encoder_outputs[0]\r\n        pooled_output = self.pooler(sequence_output)\r\n\r\n        outputs = (sequence_output, pooled_output,) + encoder_outputs[\r\n                                                      1:]  # add hidden_states and attentions if they are here\r\n        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\r\n\r\n\r\n@add_start_docstrings(""""""Bert Model with two heads on top as done during the pre-training:\r\n    a `masked language modeling` head and a `next sentence prediction (classification)` head. """""",\r\n                      BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\r\nclass AlbertForPreTraining(AlbertPreTrainedModel):\r\n    r""""""\r\n        **masked_lm_labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\r\n            Labels for computing the masked language modeling loss.\r\n            Indices should be in ``[-1, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring)\r\n            Tokens with indices set to ``-1`` are ignored (masked), the loss is only computed for the tokens with labels\r\n            in ``[0, ..., config.vocab_size]``\r\n        **next_sentence_label**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\r\n            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair (see ``input_ids`` docstring)\r\n            Indices should be in ``[0, 1]``.\r\n            ``0`` indicates sequence B is a continuation of sequence A,\r\n            ``1`` indicates sequence B is a random sequence.\r\n\r\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\r\n        **loss**: (`optional`, returned when both ``masked_lm_labels`` and ``next_sentence_label`` are provided) ``torch.FloatTensor`` of shape ``(1,)``:\r\n            Total loss as the sum of the masked language modeling loss and the next sequence prediction (classification) loss.\r\n        **prediction_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.vocab_size)``\r\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\r\n        **seq_relationship_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, 2)``\r\n            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation before SoftMax).\r\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\r\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\r\n            of shape ``(batch_size, sequence_length, hidden_size)``:\r\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\r\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\r\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\r\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\r\n\r\n    Examples::\r\n\r\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\r\n        model = BertForPreTraining.from_pretrained(\'bert-base-uncased\')\r\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\r\n        outputs = model(input_ids)\r\n        prediction_scores, seq_relationship_scores = outputs[:2]\r\n\r\n    """"""\r\n\r\n    def __init__(self, config):\r\n        super(AlbertForPreTraining, self).__init__(config)\r\n\r\n        self.bert = AlbertModel(config)\r\n        self.cls = AlbertPreTrainingHeads(config)\r\n\r\n        self.init_weights()\r\n        self.tie_weights()\r\n\r\n    def tie_weights(self):\r\n        """""" Make sure we are sharing the input and output embeddings.\r\n            Export to TorchScript can\'t handle parameter sharing so we are cloning them instead.\r\n        """"""\r\n        self._tie_or_clone_weights(self.cls.predictions.decoder,\r\n                                       self.bert.embeddings.word_embeddings)\r\n\r\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\r\n                masked_lm_labels=None, next_sentence_label=None):\r\n        outputs = self.bert(input_ids,\r\n                            attention_mask=attention_mask,\r\n                            token_type_ids=token_type_ids,\r\n                            position_ids=position_ids,\r\n                            head_mask=head_mask)\r\n\r\n        sequence_output, pooled_output = outputs[:2]\r\n        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\r\n\r\n        outputs = (prediction_scores, seq_relationship_score,) + outputs[\r\n                                                                 2:]  # add hidden states and attention if they are here\r\n\r\n        if masked_lm_labels is not None and next_sentence_label is not None:\r\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\r\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\r\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\r\n            total_loss = masked_lm_loss + next_sentence_loss\r\n            outputs = (total_loss,) + outputs\r\n\r\n        return outputs  # (loss), prediction_scores, seq_relationship_score, (hidden_states), (attentions)\r\n\r\n\r\n@add_start_docstrings(""""""Bert Model with a `language modeling` head on top. """""",\r\n                      BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\r\nclass AlbertForMaskedLM(AlbertPreTrainedModel):\r\n    r""""""\r\n        **masked_lm_labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\r\n            Labels for computing the masked language modeling loss.\r\n            Indices should be in ``[-1, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring)\r\n            Tokens with indices set to ``-1`` are ignored (masked), the loss is only computed for the tokens with labels\r\n            in ``[0, ..., config.vocab_size]``\r\n\r\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\r\n        **loss**: (`optional`, returned when ``masked_lm_labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\r\n            Masked language modeling loss.\r\n        **prediction_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.vocab_size)``\r\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\r\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\r\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\r\n            of shape ``(batch_size, sequence_length, hidden_size)``:\r\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\r\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\r\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\r\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\r\n\r\n    Examples::\r\n\r\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\r\n        model = BertForMaskedLM.from_pretrained(\'bert-base-uncased\')\r\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\r\n        outputs = model(input_ids, masked_lm_labels=input_ids)\r\n        loss, prediction_scores = outputs[:2]\r\n\r\n    """"""\r\n\r\n    def __init__(self, config):\r\n        super(AlbertForMaskedLM, self).__init__(config)\r\n\r\n        self.bert = AlbertModel(config)\r\n        self.cls = AlbertOnlyMLMHead(config)\r\n\r\n        self.init_weights()\r\n        self.tie_weights()\r\n\r\n    def tie_weights(self):\r\n        """""" Make sure we are sharing the input and output embeddings.\r\n            Export to TorchScript can\'t handle parameter sharing so we are cloning them instead.\r\n        """"""\r\n        self._tie_or_clone_weights(self.cls.predictions.decoder,\r\n                                   self.bert.embeddings.word_embeddings)\r\n\r\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\r\n                masked_lm_labels=None):\r\n        outputs = self.bert(input_ids,\r\n                            attention_mask=attention_mask,\r\n                            token_type_ids=token_type_ids,\r\n                            position_ids=position_ids,\r\n                            head_mask=head_mask)\r\n\r\n        sequence_output = outputs[0]\r\n        prediction_scores = self.cls(sequence_output)\r\n\r\n        outputs = (prediction_scores,) + outputs[2:]  # Add hidden states and attention if they are here\r\n        if masked_lm_labels is not None:\r\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\r\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\r\n            outputs = (masked_lm_loss,) + outputs\r\n\r\n        return outputs  # (masked_lm_loss), prediction_scores, (hidden_states), (attentions)\r\n\r\n\r\n@add_start_docstrings(""""""Bert Model with a `next sentence prediction (classification)` head on top. """""",\r\n                      BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\r\nclass AlbertForNextSentencePrediction(AlbertPreTrainedModel):\r\n    r""""""\r\n        **next_sentence_label**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\r\n            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair (see ``input_ids`` docstring)\r\n            Indices should be in ``[0, 1]``.\r\n            ``0`` indicates sequence B is a continuation of sequence A,\r\n            ``1`` indicates sequence B is a random sequence.\r\n\r\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\r\n        **loss**: (`optional`, returned when ``next_sentence_label`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\r\n            Next sequence prediction (classification) loss.\r\n        **seq_relationship_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, 2)``\r\n            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation before SoftMax).\r\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\r\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\r\n            of shape ``(batch_size, sequence_length, hidden_size)``:\r\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\r\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\r\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\r\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\r\n\r\n    Examples::\r\n\r\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\r\n        model = BertForNextSentencePrediction.from_pretrained(\'bert-base-uncased\')\r\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\r\n        outputs = model(input_ids)\r\n        seq_relationship_scores = outputs[0]\r\n\r\n    """"""\r\n\r\n    def __init__(self, config):\r\n        super(AlbertForNextSentencePrediction, self).__init__(config)\r\n\r\n        self.bert = AlbertModel(config)\r\n        self.cls = AlbertOnlyNSPHead(config)\r\n\r\n        self.init_weights()\r\n\r\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\r\n                next_sentence_label=None):\r\n        outputs = self.bert(input_ids,\r\n                            attention_mask=attention_mask,\r\n                            token_type_ids=token_type_ids,\r\n                            position_ids=position_ids,\r\n                            head_mask=head_mask)\r\n\r\n        pooled_output = outputs[1]\r\n\r\n        seq_relationship_score = self.cls(pooled_output)\r\n\r\n        outputs = (seq_relationship_score,) + outputs[2:]  # add hidden states and attention if they are here\r\n        if next_sentence_label is not None:\r\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\r\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\r\n            outputs = (next_sentence_loss,) + outputs\r\n\r\n        return outputs  # (next_sentence_loss), seq_relationship_score, (hidden_states), (attentions)\r\n\r\n\r\n@add_start_docstrings(""""""Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of\r\n    the pooled output) e.g. for GLUE tasks. """""",\r\n                      BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\r\nclass AlbertForSequenceClassification(AlbertPreTrainedModel):\r\n    r""""""\r\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\r\n            Labels for computing the sequence classification/regression loss.\r\n            Indices should be in ``[0, ..., config.num_labels - 1]``.\r\n            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\r\n            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\r\n\r\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\r\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\r\n            Classification (or regression if config.num_labels==1) loss.\r\n        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\r\n            Classification (or regression if config.num_labels==1) scores (before SoftMax).\r\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\r\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\r\n            of shape ``(batch_size, sequence_length, hidden_size)``:\r\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\r\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\r\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\r\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\r\n\r\n    Examples::\r\n\r\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\r\n        model = BertForSequenceClassification.from_pretrained(\'bert-base-uncased\')\r\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\r\n        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\r\n        outputs = model(input_ids, labels=labels)\r\n        loss, logits = outputs[:2]\r\n\r\n    """"""\r\n\r\n    def __init__(self, config):\r\n        super(AlbertForSequenceClassification, self).__init__(config)\r\n        self.num_labels = config.num_labels\r\n\r\n        self.bert = AlbertModel(config)\r\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\r\n        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\r\n\r\n        self.init_weights()\r\n\r\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None,\r\n                position_ids=None, head_mask=None, labels=None):\r\n\r\n        outputs = self.bert(input_ids,\r\n                            attention_mask=attention_mask,\r\n                            token_type_ids=token_type_ids,\r\n                            position_ids=position_ids,\r\n                            head_mask=head_mask)\r\n\r\n        pooled_output = outputs[1]\r\n\r\n        pooled_output = self.dropout(pooled_output)\r\n        logits = self.classifier(pooled_output)\r\n\r\n        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\r\n\r\n        if labels is not None:\r\n            if self.num_labels == 1:\r\n                #  We are doing regression\r\n                loss_fct = MSELoss()\r\n                loss = loss_fct(logits.view(-1), labels.view(-1))\r\n            else:\r\n                loss_fct = CrossEntropyLoss()\r\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\r\n            outputs = (loss,) + outputs\r\n\r\n        return outputs  # (loss), logits, (hidden_states), (attentions)\r\n\r\n\r\n@add_start_docstrings(""""""Bert Model with a multiple choice classification head on top (a linear layer on top of\r\n    the pooled output and a softmax) e.g. for RocStories/SWAG tasks. """""",\r\n                      BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\r\nclass AlbertForMultipleChoice(AlbertPreTrainedModel):\r\n    r""""""\r\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\r\n            Labels for computing the multiple choice classification loss.\r\n            Indices should be in ``[0, ..., num_choices]`` where `num_choices` is the size of the second dimension\r\n            of the input tensors. (see `input_ids` above)\r\n\r\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\r\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\r\n            Classification loss.\r\n        **classification_scores**: ``torch.FloatTensor`` of shape ``(batch_size, num_choices)`` where `num_choices` is the size of the second dimension\r\n            of the input tensors. (see `input_ids` above).\r\n            Classification scores (before SoftMax).\r\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\r\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\r\n            of shape ``(batch_size, sequence_length, hidden_size)``:\r\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\r\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\r\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\r\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\r\n\r\n    Examples::\r\n\r\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\r\n        model = BertForMultipleChoice.from_pretrained(\'bert-base-uncased\')\r\n        choices = [""Hello, my dog is cute"", ""Hello, my cat is amazing""]\r\n        input_ids = torch.tensor([tokenizer.encode(s) for s in choices]).unsqueeze(0)  # Batch size 1, 2 choices\r\n        labels = torch.tensor(1).unsqueeze(0)  # Batch size 1\r\n        outputs = model(input_ids, labels=labels)\r\n        loss, classification_scores = outputs[:2]\r\n\r\n    """"""\r\n\r\n    def __init__(self, config):\r\n        super(AlbertForMultipleChoice, self).__init__(config)\r\n\r\n        self.bert = AlbertModel(config)\r\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\r\n        self.classifier = nn.Linear(config.hidden_size, 1)\r\n\r\n        self.init_weights()\r\n\r\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None,\r\n                position_ids=None, head_mask=None, labels=None):\r\n        num_choices = input_ids.shape[1]\r\n\r\n        input_ids = input_ids.view(-1, input_ids.size(-1))\r\n        attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\r\n        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\r\n        position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\r\n\r\n        outputs = self.bert(input_ids,\r\n                            attention_mask=attention_mask,\r\n                            token_type_ids=token_type_ids,\r\n                            position_ids=position_ids,\r\n                            head_mask=head_mask)\r\n\r\n        pooled_output = outputs[1]\r\n\r\n        pooled_output = self.dropout(pooled_output)\r\n        logits = self.classifier(pooled_output)\r\n        reshaped_logits = logits.view(-1, num_choices)\r\n\r\n        outputs = (reshaped_logits,) + outputs[2:]  # add hidden states and attention if they are here\r\n\r\n        if labels is not None:\r\n            loss_fct = CrossEntropyLoss()\r\n            loss = loss_fct(reshaped_logits, labels)\r\n            outputs = (loss,) + outputs\r\n\r\n        return outputs  # (loss), reshaped_logits, (hidden_states), (attentions)\r\n\r\n\r\n@add_start_docstrings(""""""Bert Model with a token classification head on top (a linear layer on top of\r\n    the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. """""",\r\n                      BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\r\nclass AlbertForTokenClassification(AlbertPreTrainedModel):\r\n    r""""""\r\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\r\n            Labels for computing the token classification loss.\r\n            Indices should be in ``[0, ..., config.num_labels - 1]``.\r\n\r\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\r\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\r\n            Classification loss.\r\n        **scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.num_labels)``\r\n            Classification scores (before SoftMax).\r\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\r\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\r\n            of shape ``(batch_size, sequence_length, hidden_size)``:\r\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\r\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\r\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\r\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\r\n\r\n    Examples::\r\n\r\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\r\n        model = BertForTokenClassification.from_pretrained(\'bert-base-uncased\')\r\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\r\n        labels = torch.tensor([1] * input_ids.size(1)).unsqueeze(0)  # Batch size 1\r\n        outputs = model(input_ids, labels=labels)\r\n        loss, scores = outputs[:2]\r\n\r\n    """"""\r\n\r\n    def __init__(self, config):\r\n        super(AlbertForTokenClassification, self).__init__(config)\r\n        self.num_labels = config.num_labels\r\n\r\n        self.bert = AlbertModel(config)\r\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\r\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\r\n\r\n        self.init_weights()\r\n\r\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None,\r\n                position_ids=None, head_mask=None, labels=None):\r\n\r\n        outputs = self.bert(input_ids,\r\n                            attention_mask=attention_mask,\r\n                            token_type_ids=token_type_ids,\r\n                            position_ids=position_ids,\r\n                            head_mask=head_mask)\r\n\r\n        sequence_output = outputs[0]\r\n\r\n        sequence_output = self.dropout(sequence_output)\r\n        logits = self.classifier(sequence_output)\r\n\r\n        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\r\n        if labels is not None:\r\n            loss_fct = CrossEntropyLoss()\r\n            # Only keep active parts of the loss\r\n            if attention_mask is not None:\r\n                active_loss = attention_mask.view(-1) == 1\r\n                active_logits = logits.view(-1, self.num_labels)[active_loss]\r\n                active_labels = labels.view(-1)[active_loss]\r\n                loss = loss_fct(active_logits, active_labels)\r\n            else:\r\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\r\n            outputs = (loss,) + outputs\r\n\r\n        return outputs  # (loss), scores, (hidden_states), (attentions)\r\n\r\n\r\n@add_start_docstrings(""""""Bert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear layers on top of\r\n    the hidden-states output to compute `span start logits` and `span end logits`). """""",\r\n                      BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\r\nclass AlbertForQuestionAnswering(AlbertPreTrainedModel):\r\n    r""""""\r\n        **start_positions**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\r\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\r\n            Positions are clamped to the length of the sequence (`sequence_length`).\r\n            Position outside of the sequence are not taken into account for computing the loss.\r\n        **end_positions**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\r\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\r\n            Positions are clamped to the length of the sequence (`sequence_length`).\r\n            Position outside of the sequence are not taken into account for computing the loss.\r\n\r\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\r\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\r\n            Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.\r\n        **start_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length,)``\r\n            Span-start scores (before SoftMax).\r\n        **end_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length,)``\r\n            Span-end scores (before SoftMax).\r\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\r\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\r\n            of shape ``(batch_size, sequence_length, hidden_size)``:\r\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\r\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\r\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\r\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\r\n\r\n    Examples::\r\n\r\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\r\n        model = BertForQuestionAnswering.from_pretrained(\'bert-base-uncased\')\r\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\r\n        start_positions = torch.tensor([1])\r\n        end_positions = torch.tensor([3])\r\n        outputs = model(input_ids, start_positions=start_positions, end_positions=end_positions)\r\n        loss, start_scores, end_scores = outputs[:2]\r\n\r\n    """"""\r\n\r\n    def __init__(self, config):\r\n        super(AlbertForQuestionAnswering, self).__init__(config)\r\n        self.num_labels = config.num_labels\r\n\r\n        self.bert = AlbertModel(config)\r\n        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\r\n\r\n        self.init_weights()\r\n\r\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\r\n                start_positions=None, end_positions=None):\r\n\r\n        outputs = self.bert(input_ids,\r\n                            attention_mask=attention_mask,\r\n                            token_type_ids=token_type_ids,\r\n                            position_ids=position_ids,\r\n                            head_mask=head_mask)\r\n\r\n        sequence_output = outputs[0]\r\n\r\n        logits = self.qa_outputs(sequence_output)\r\n        start_logits, end_logits = logits.split(1, dim=-1)\r\n        start_logits = start_logits.squeeze(-1)\r\n        end_logits = end_logits.squeeze(-1)\r\n\r\n        outputs = (start_logits, end_logits,) + outputs[2:]\r\n        if start_positions is not None and end_positions is not None:\r\n            # If we are on multi-GPU, split add a dimension\r\n            if len(start_positions.size()) > 1:\r\n                start_positions = start_positions.squeeze(-1)\r\n            if len(end_positions.size()) > 1:\r\n                end_positions = end_positions.squeeze(-1)\r\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\r\n            ignored_index = start_logits.size(1)\r\n            start_positions.clamp_(0, ignored_index)\r\n            end_positions.clamp_(0, ignored_index)\r\n\r\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\r\n            start_loss = loss_fct(start_logits, start_positions)\r\n            end_loss = loss_fct(end_logits, end_positions)\r\n            total_loss = (start_loss + end_loss) / 2\r\n            outputs = (total_loss,) + outputs\r\n\r\n        return outputs  # (loss), start_logits, end_logits, (hidden_states), (attentions)\r\n'"
model/modeling_bert.py,81,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""PyTorch BERT model. """"""\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport json\nimport logging\nimport math\nimport os\nimport sys\nfrom io import open\n\nimport torch\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss, MSELoss\n\nfrom .modeling_utils import PreTrainedModel, prune_linear_layer\nfrom .configuration_bert import BertConfig\nfrom .file_utils import add_start_docstrings\n\nlogger = logging.getLogger(__name__)\n\nBERT_PRETRAINED_MODEL_ARCHIVE_MAP = {\n    \'bert-base-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin"",\n    \'bert-large-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-pytorch_model.bin"",\n    \'bert-base-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin"",\n    \'bert-large-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-pytorch_model.bin"",\n    \'bert-base-multilingual-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-pytorch_model.bin"",\n    \'bert-base-multilingual-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased-pytorch_model.bin"",\n    \'bert-base-chinese\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-pytorch_model.bin"",\n    \'bert-base-german-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-cased-pytorch_model.bin"",\n    \'bert-large-uncased-whole-word-masking\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-pytorch_model.bin"",\n    \'bert-large-cased-whole-word-masking\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-pytorch_model.bin"",\n    \'bert-large-uncased-whole-word-masking-finetuned-squad\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased-whole-word-masking-finetuned-squad-pytorch_model.bin"",\n    \'bert-large-cased-whole-word-masking-finetuned-squad\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-whole-word-masking-finetuned-squad-pytorch_model.bin"",\n    \'bert-base-cased-finetuned-mrpc\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-finetuned-mrpc-pytorch_model.bin"",\n    \'bert-base-german-dbmdz-cased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-cased-pytorch_model.bin"",\n    \'bert-base-german-dbmdz-uncased\': ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-german-dbmdz-uncased-pytorch_model.bin"",\n}\n\ndef load_tf_weights_in_bert(model, config, tf_checkpoint_path):\n    """""" Load tf checkpoints in a pytorch model.\n    """"""\n    try:\n        import re\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        logger.error(""Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see ""\n            ""https://www.tensorflow.org/install/ for installation instructions."")\n        raise\n    tf_path = os.path.abspath(tf_checkpoint_path)\n    logger.info(""Converting TensorFlow checkpoint from {}"".format(tf_path))\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    names = []\n    arrays = []\n    for name, shape in init_vars:\n        logger.info(""Loading TF weight {} with shape {}"".format(name, shape))\n        array = tf.train.load_variable(tf_path, name)\n        names.append(name)\n        arrays.append(array)\n\n    for name, array in zip(names, arrays):\n        name = name.split(\'/\')\n        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n        # which are not required for using pretrained model\n        if any(n in [""adam_v"", ""adam_m"", ""global_step""] for n in name):\n            logger.info(""Skipping {}"".format(""/"".join(name)))\n            continue\n        pointer = model\n        for m_name in name:\n            if re.fullmatch(r\'[A-Za-z]+_\\d+\', m_name):\n                l = re.split(r\'_(\\d+)\', m_name)\n            else:\n                l = [m_name]\n            if l[0] == \'kernel\' or l[0] == \'gamma\':\n                pointer = getattr(pointer, \'weight\')\n            elif l[0] == \'output_bias\' or l[0] == \'beta\':\n                pointer = getattr(pointer, \'bias\')\n            elif l[0] == \'output_weights\':\n                pointer = getattr(pointer, \'weight\')\n            elif l[0] == \'squad\':\n                pointer = getattr(pointer, \'classifier\')\n            else:\n                try:\n                    pointer = getattr(pointer, l[0])\n                except AttributeError:\n                    logger.info(""Skipping {}"".format(""/"".join(name)))\n                    continue\n            if len(l) >= 2:\n                num = int(l[1])\n                pointer = pointer[num]\n        if m_name[-11:] == \'_embeddings\':\n            pointer = getattr(pointer, \'weight\')\n        elif m_name == \'kernel\':\n            array = np.transpose(array)\n        try:\n            assert pointer.shape == array.shape\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        logger.info(""Initialize PyTorch weight {}"".format(name))\n        pointer.data = torch.from_numpy(array)\n    return model\n\n\ndef gelu(x):\n    """""" Original Implementation of the gelu activation function in Google Bert repo when initially created.\n        For information: OpenAI GPT\'s gelu is slightly different (and gives slightly different results):\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n        Also see https://arxiv.org/abs/1606.08415\n    """"""\n    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n\ndef gelu_new(x):\n    """""" Implementation of the gelu activation function currently in Google Bert repo (identical to OpenAI GPT).\n        Also see https://arxiv.org/abs/1606.08415\n    """"""\n    return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n\ndef swish(x):\n    return x * torch.sigmoid(x)\n\n\nACT2FN = {""gelu"": gelu, ""relu"": torch.nn.functional.relu, ""swish"": swish, ""gelu_new"": gelu_new}\n\n\nBertLayerNorm = torch.nn.LayerNorm\n\nclass BertEmbeddings(nn.Module):\n    """"""Construct the embeddings from word, position and token_type embeddings.\n    """"""\n    def __init__(self, config):\n        super(BertEmbeddings, self).__init__()\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n\n        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n        # any TensorFlow checkpoint file\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, input_ids, token_type_ids=None, position_ids=None):\n        seq_length = input_ids.size(1)\n        if position_ids is None:\n            position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        words_embeddings = self.word_embeddings(input_ids)\n        position_embeddings = self.position_embeddings(position_ids)\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n\n        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings\n\n\nclass BertSelfAttention(nn.Module):\n    def __init__(self, config):\n        super(BertSelfAttention, self).__init__()\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                ""The hidden size (%d) is not a multiple of the number of attention ""\n                ""heads (%d)"" % (config.hidden_size, config.num_attention_heads))\n        self.output_attentions = config.output_attentions\n\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def forward(self, hidden_states, attention_mask=None, head_mask=None):\n        mixed_query_layer = self.query(hidden_states)\n        mixed_key_layer = self.key(hidden_states)\n        mixed_value_layer = self.value(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n        key_layer = self.transpose_for_scores(mixed_key_layer)\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n\n        # Take the dot product between ""query"" and ""key"" to get the raw attention scores.\n        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        if attention_mask is not None:\n            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n            attention_scores = attention_scores + attention_mask\n\n        # Normalize the attention scores to probabilities.\n        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout(attention_probs)\n\n        # Mask heads if we want to\n        if head_mask is not None:\n            attention_probs = attention_probs * head_mask\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n\n        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n        context_layer = context_layer.view(*new_context_layer_shape)\n\n        outputs = (context_layer, attention_probs) if self.output_attentions else (context_layer,)\n        return outputs\n\n\nclass BertSelfOutput(nn.Module):\n    def __init__(self, config):\n        super(BertSelfOutput, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass BertAttention(nn.Module):\n    def __init__(self, config):\n        super(BertAttention, self).__init__()\n        self.self = BertSelfAttention(config)\n        self.output = BertSelfOutput(config)\n        self.pruned_heads = set()\n\n    def prune_heads(self, heads):\n        if len(heads) == 0:\n            return\n        mask = torch.ones(self.self.num_attention_heads, self.self.attention_head_size)\n        heads = set(heads) - self.pruned_heads  # Convert to set and emove already pruned heads\n        for head in heads:\n            # Compute how many pruned heads are before the head and move the index accordingly\n            head = head - sum(1 if h < head else 0 for h in self.pruned_heads)\n            mask[head] = 0\n        mask = mask.view(-1).contiguous().eq(1)\n        index = torch.arange(len(mask))[mask].long()\n\n        # Prune linear layers\n        self.self.query = prune_linear_layer(self.self.query, index)\n        self.self.key = prune_linear_layer(self.self.key, index)\n        self.self.value = prune_linear_layer(self.self.value, index)\n        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n\n        # Update hyper params and store pruned heads\n        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n        self.pruned_heads = self.pruned_heads.union(heads)\n\n    def forward(self, input_tensor, attention_mask=None, head_mask=None):\n        self_outputs = self.self(input_tensor, attention_mask, head_mask)\n        attention_output = self.output(self_outputs[0], input_tensor)\n        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n        return outputs\n\n\nclass BertIntermediate(nn.Module):\n    def __init__(self, config):\n        super(BertIntermediate, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.intermediate_act_fn = config.hidden_act\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.intermediate_act_fn(hidden_states)\n        return hidden_states\n\n\nclass BertOutput(nn.Module):\n    def __init__(self, config):\n        super(BertOutput, self).__init__()\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass BertLayer(nn.Module):\n    def __init__(self, config):\n        super(BertLayer, self).__init__()\n        self.attention = BertAttention(config)\n        self.intermediate = BertIntermediate(config)\n        self.output = BertOutput(config)\n\n    def forward(self, hidden_states, attention_mask=None, head_mask=None):\n        attention_outputs = self.attention(hidden_states, attention_mask, head_mask)\n        attention_output = attention_outputs[0]\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n        outputs = (layer_output,) + attention_outputs[1:]  # add attentions if we output them\n        return outputs\n\n\nclass BertEncoder(nn.Module):\n    def __init__(self, config):\n        super(BertEncoder, self).__init__()\n        self.output_attentions = config.output_attentions\n        self.output_hidden_states = config.output_hidden_states\n        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n\n    def forward(self, hidden_states, attention_mask=None, head_mask=None):\n        all_hidden_states = ()\n        all_attentions = ()\n        for i, layer_module in enumerate(self.layer):\n            if self.output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states,)\n\n            layer_outputs = layer_module(hidden_states, attention_mask, head_mask[i])\n            hidden_states = layer_outputs[0]\n\n            if self.output_attentions:\n                all_attentions = all_attentions + (layer_outputs[1],)\n\n        # Add last layer\n        if self.output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_states,)\n\n        outputs = (hidden_states,)\n        if self.output_hidden_states:\n            outputs = outputs + (all_hidden_states,)\n        if self.output_attentions:\n            outputs = outputs + (all_attentions,)\n        return outputs  # last-layer hidden state, (all hidden states), (all attentions)\n\n\nclass BertPooler(nn.Module):\n    def __init__(self, config):\n        super(BertPooler, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.activation = nn.Tanh()\n\n    def forward(self, hidden_states):\n        # We ""pool"" the model by simply taking the hidden state corresponding\n        # to the first token.\n        first_token_tensor = hidden_states[:, 0]\n        pooled_output = self.dense(first_token_tensor)\n        pooled_output = self.activation(pooled_output)\n        return pooled_output\n\n\nclass BertPredictionHeadTransform(nn.Module):\n    def __init__(self, config):\n        super(BertPredictionHeadTransform, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n            self.transform_act_fn = ACT2FN[config.hidden_act]\n        else:\n            self.transform_act_fn = config.hidden_act\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.transform_act_fn(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states)\n        return hidden_states\n\n\nclass BertLMPredictionHead(nn.Module):\n    def __init__(self, config):\n        super(BertLMPredictionHead, self).__init__()\n        self.transform = BertPredictionHeadTransform(config)\n\n        # The output weights are the same as the input embeddings, but there is\n        # an output-only bias for each token.\n        self.decoder = nn.Linear(config.hidden_size,\n                                 config.vocab_size,\n                                 bias=False)\n\n        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n\n    def forward(self, hidden_states):\n        hidden_states = self.transform(hidden_states)\n        hidden_states = self.decoder(hidden_states) + self.bias\n        return hidden_states\n\n\nclass BertOnlyMLMHead(nn.Module):\n    def __init__(self, config):\n        super(BertOnlyMLMHead, self).__init__()\n        self.predictions = BertLMPredictionHead(config)\n\n    def forward(self, sequence_output):\n        prediction_scores = self.predictions(sequence_output)\n        return prediction_scores\n\n\nclass BertOnlyNSPHead(nn.Module):\n    def __init__(self, config):\n        super(BertOnlyNSPHead, self).__init__()\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n\n    def forward(self, pooled_output):\n        seq_relationship_score = self.seq_relationship(pooled_output)\n        return seq_relationship_score\n\n\nclass BertPreTrainingHeads(nn.Module):\n    def __init__(self, config):\n        super(BertPreTrainingHeads, self).__init__()\n        self.predictions = BertLMPredictionHead(config)\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n\n    def forward(self, sequence_output, pooled_output):\n        prediction_scores = self.predictions(sequence_output)\n        seq_relationship_score = self.seq_relationship(pooled_output)\n        return prediction_scores, seq_relationship_score\n\n\nclass BertPreTrainedModel(PreTrainedModel):\n    """""" An abstract class to handle weights initialization and\n        a simple interface for dowloading and loading pretrained models.\n    """"""\n    config_class = BertConfig\n    pretrained_model_archive_map = BERT_PRETRAINED_MODEL_ARCHIVE_MAP\n    load_tf_weights = load_tf_weights_in_bert\n    base_model_prefix = ""bert""\n\n    def _init_weights(self, module):\n        """""" Initialize the weights """"""\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n        elif isinstance(module, BertLayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n\n\nBERT_START_DOCSTRING = r""""""    The BERT model was proposed in\n    `BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`_\n    by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. It\'s a bidirectional transformer\n    pre-trained using a combination of masked language modeling objective and next sentence prediction\n    on a large corpus comprising the Toronto Book Corpus and Wikipedia.\n\n    This model is a PyTorch `torch.nn.Module`_ sub-class. Use it as a regular PyTorch Module and\n    refer to the PyTorch documentation for all matter related to general usage and behavior.\n\n    .. _`BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`:\n        https://arxiv.org/abs/1810.04805\n\n    .. _`torch.nn.Module`:\n        https://pytorch.org/docs/stable/nn.html#module\n\n    Parameters:\n        config (:class:`~transformers.BertConfig`): Model configuration class with all the parameters of the model. \n            Initializing with a config file does not load the weights associated with the model, only the configuration.\n            Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model weights.\n""""""\n\nBERT_INPUTS_DOCSTRING = r""""""\n    Inputs:\n        **input_ids**: ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Indices of input sequence tokens in the vocabulary.\n            To match pre-training, BERT input sequence should be formatted with [CLS] and [SEP] tokens as follows:\n\n            (a) For sequence pairs:\n\n                ``tokens:         [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]``\n                \n                ``token_type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1``\n\n            (b) For single sequences:\n\n                ``tokens:         [CLS] the dog is hairy . [SEP]``\n                \n                ``token_type_ids:   0   0   0   0  0     0   0``\n\n            Bert is a model with absolute position embeddings so it\'s usually advised to pad the inputs on\n            the right rather than the left.\n\n            Indices can be obtained using :class:`transformers.BertTokenizer`.\n            See :func:`transformers.PreTrainedTokenizer.encode` and\n            :func:`transformers.PreTrainedTokenizer.convert_tokens_to_ids` for details.\n        **attention_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, sequence_length)``:\n            Mask to avoid performing attention on padding token indices.\n            Mask values selected in ``[0, 1]``:\n            ``1`` for tokens that are NOT MASKED, ``0`` for MASKED tokens.\n        **token_type_ids**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Segment token indices to indicate first and second portions of the inputs.\n            Indices are selected in ``[0, 1]``: ``0`` corresponds to a `sentence A` token, ``1``\n            corresponds to a `sentence B` token\n            (see `BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding`_ for more details).\n        **position_ids**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Indices of positions of each input sequence tokens in the position embeddings.\n            Selected in the range ``[0, config.max_position_embeddings - 1]``.\n        **head_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(num_heads,)`` or ``(num_layers, num_heads)``:\n            Mask to nullify selected heads of the self-attention modules.\n            Mask values selected in ``[0, 1]``:\n            ``1`` indicates the head is **not masked**, ``0`` indicates the head is **masked**.\n""""""\n\n@add_start_docstrings(""The bare Bert Model transformer outputting raw hidden-states without any specific head on top."",\n                      BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\nclass BertModel(BertPreTrainedModel):\n    r""""""\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **last_hidden_state**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, hidden_size)``\n            Sequence of hidden-states at the output of the last layer of the model.\n        **pooler_output**: ``torch.FloatTensor`` of shape ``(batch_size, hidden_size)``\n            Last layer hidden-state of the first token of the sequence (classification token)\n            further processed by a Linear layer and a Tanh activation function. The Linear\n            layer weights are trained from the next sentence prediction (classification)\n            objective during Bert pretraining. This output is usually *not* a good summary\n            of the semantic content of the input, you\'re often better with averaging or pooling\n            the sequence of hidden-states for the whole input sequence.\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n        model = BertModel.from_pretrained(\'bert-base-uncased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids)\n        last_hidden_states = outputs[0]  # The last hidden-state is the first element of the output tuple\n\n    """"""\n    def __init__(self, config):\n        super(BertModel, self).__init__(config)\n\n        self.embeddings = BertEmbeddings(config)\n        self.encoder = BertEncoder(config)\n        self.pooler = BertPooler(config)\n\n        self.init_weights()\n\n    def _resize_token_embeddings(self, new_num_tokens):\n        old_embeddings = self.embeddings.word_embeddings\n        new_embeddings = self._get_resized_embeddings(old_embeddings, new_num_tokens)\n        self.embeddings.word_embeddings = new_embeddings\n        return self.embeddings.word_embeddings\n\n    def _prune_heads(self, heads_to_prune):\n        """""" Prunes heads of the model.\n            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n            See base class PreTrainedModel\n        """"""\n        for layer, heads in heads_to_prune.items():\n            self.encoder.layer[layer].attention.prune_heads(heads)\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n        if attention_mask is None:\n            attention_mask = torch.ones_like(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        # We create a 3D attention mask from a 2D tensor mask.\n        # Sizes are [batch_size, 1, 1, to_seq_length]\n        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n        # this attention mask is more simple than the triangular masking of causal attention\n        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n\n        # Prepare head mask if needed\n        # 1.0 in head_mask indicate we keep the head\n        # attention_probs has shape bsz x n_heads x N x N\n        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n        if head_mask is not None:\n            if head_mask.dim() == 1:\n                head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n                head_mask = head_mask.expand(self.config.num_hidden_layers, -1, -1, -1, -1)\n            elif head_mask.dim() == 2:\n                head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)  # We can specify head_mask for each layer\n            head_mask = head_mask.to(dtype=next(self.parameters()).dtype) # switch to fload if need + fp16 compatibility\n        else:\n            head_mask = [None] * self.config.num_hidden_layers\n\n        embedding_output = self.embeddings(input_ids, position_ids=position_ids, token_type_ids=token_type_ids)\n        encoder_outputs = self.encoder(embedding_output,\n                                       extended_attention_mask,\n                                       head_mask=head_mask)\n        sequence_output = encoder_outputs[0]\n        pooled_output = self.pooler(sequence_output)\n\n        outputs = (sequence_output, pooled_output,) + encoder_outputs[1:]  # add hidden_states and attentions if they are here\n        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\n\n\n@add_start_docstrings(""""""Bert Model with two heads on top as done during the pre-training:\n    a `masked language modeling` head and a `next sentence prediction (classification)` head. """""",\n    BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\nclass BertForPreTraining(BertPreTrainedModel):\n    r""""""\n        **masked_lm_labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Labels for computing the masked language modeling loss.\n            Indices should be in ``[-1, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring)\n            Tokens with indices set to ``-1`` are ignored (masked), the loss is only computed for the tokens with labels\n            in ``[0, ..., config.vocab_size]``\n        **next_sentence_label**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair (see ``input_ids`` docstring)\n            Indices should be in ``[0, 1]``.\n            ``0`` indicates sequence B is a continuation of sequence A,\n            ``1`` indicates sequence B is a random sequence.\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when both ``masked_lm_labels`` and ``next_sentence_label`` are provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Total loss as the sum of the masked language modeling loss and the next sequence prediction (classification) loss.\n        **prediction_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.vocab_size)``\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n        **seq_relationship_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, 2)``\n            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n        model = BertForPreTraining.from_pretrained(\'bert-base-uncased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids)\n        prediction_scores, seq_relationship_scores = outputs[:2]\n\n    """"""\n    def __init__(self, config):\n        super(BertForPreTraining, self).__init__(config)\n\n        self.bert = BertModel(config)\n        self.cls = BertPreTrainingHeads(config)\n\n        self.init_weights()\n        self.tie_weights()\n\n    def tie_weights(self):\n        """""" Make sure we are sharing the input and output embeddings.\n            Export to TorchScript can\'t handle parameter sharing so we are cloning them instead.\n        """"""\n        self._tie_or_clone_weights(self.cls.predictions.decoder,\n                                   self.bert.embeddings.word_embeddings)\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n                masked_lm_labels=None, next_sentence_label=None):\n\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids, \n                            head_mask=head_mask)\n\n        sequence_output, pooled_output = outputs[:2]\n        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n\n        outputs = (prediction_scores, seq_relationship_score,) + outputs[2:]  # add hidden states and attention if they are here\n\n        if masked_lm_labels is not None and next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n            total_loss = masked_lm_loss + next_sentence_loss\n            outputs = (total_loss,) + outputs\n\n        return outputs  # (loss), prediction_scores, seq_relationship_score, (hidden_states), (attentions)\n\n\n@add_start_docstrings(""""""Bert Model with a `language modeling` head on top. """""",\n    BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\nclass BertForMaskedLM(BertPreTrainedModel):\n    r""""""\n        **masked_lm_labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Labels for computing the masked language modeling loss.\n            Indices should be in ``[-1, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring)\n            Tokens with indices set to ``-1`` are ignored (masked), the loss is only computed for the tokens with labels\n            in ``[0, ..., config.vocab_size]``\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``masked_lm_labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Masked language modeling loss.\n        **prediction_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.vocab_size)``\n            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n        model = BertForMaskedLM.from_pretrained(\'bert-base-uncased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, masked_lm_labels=input_ids)\n        loss, prediction_scores = outputs[:2]\n\n    """"""\n    def __init__(self, config):\n        super(BertForMaskedLM, self).__init__(config)\n\n        self.bert = BertModel(config)\n        self.cls = BertOnlyMLMHead(config)\n\n        self.init_weights()\n        self.tie_weights()\n\n    def tie_weights(self):\n        """""" Make sure we are sharing the input and output embeddings.\n            Export to TorchScript can\'t handle parameter sharing so we are cloning them instead.\n        """"""\n        self._tie_or_clone_weights(self.cls.predictions.decoder,\n                                   self.bert.embeddings.word_embeddings)\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n                masked_lm_labels=None):\n\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids, \n                            head_mask=head_mask)\n\n        sequence_output = outputs[0]\n        prediction_scores = self.cls(sequence_output)\n\n        outputs = (prediction_scores,) + outputs[2:]  # Add hidden states and attention if they are here\n        if masked_lm_labels is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n            outputs = (masked_lm_loss,) + outputs\n\n        return outputs  # (masked_lm_loss), prediction_scores, (hidden_states), (attentions)\n\n\n@add_start_docstrings(""""""Bert Model with a `next sentence prediction (classification)` head on top. """""",\n    BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\nclass BertForNextSentencePrediction(BertPreTrainedModel):\n    r""""""\n        **next_sentence_label**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair (see ``input_ids`` docstring)\n            Indices should be in ``[0, 1]``.\n            ``0`` indicates sequence B is a continuation of sequence A,\n            ``1`` indicates sequence B is a random sequence.\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``next_sentence_label`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Next sequence prediction (classification) loss.\n        **seq_relationship_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, 2)``\n            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n        model = BertForNextSentencePrediction.from_pretrained(\'bert-base-uncased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids)\n        seq_relationship_scores = outputs[0]\n\n    """"""\n    def __init__(self, config):\n        super(BertForNextSentencePrediction, self).__init__(config)\n\n        self.bert = BertModel(config)\n        self.cls = BertOnlyNSPHead(config)\n\n        self.init_weights()\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n                next_sentence_label=None):\n\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids, \n                            head_mask=head_mask)\n\n        pooled_output = outputs[1]\n\n        seq_relationship_score = self.cls(pooled_output)\n\n        outputs = (seq_relationship_score,) + outputs[2:]  # add hidden states and attention if they are here\n        if next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n            outputs = (next_sentence_loss,) + outputs\n\n        return outputs  # (next_sentence_loss), seq_relationship_score, (hidden_states), (attentions)\n\n\n@add_start_docstrings(""""""Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of\n    the pooled output) e.g. for GLUE tasks. """""",\n    BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\nclass BertForSequenceClassification(BertPreTrainedModel):\n    r""""""\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the sequence classification/regression loss.\n            Indices should be in ``[0, ..., config.num_labels - 1]``.\n            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Classification (or regression if config.num_labels==1) loss.\n        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n        model = BertForSequenceClassification.from_pretrained(\'bert-base-uncased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, labels=labels)\n        loss, logits = outputs[:2]\n\n    """"""\n    def __init__(self, config):\n        super(BertForSequenceClassification, self).__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n\n        self.init_weights()\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None,\n                position_ids=None, head_mask=None, labels=None):\n\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids, \n                            head_mask=head_mask)\n\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n\n        if labels is not None:\n            if self.num_labels == 1:\n                #  We are doing regression\n                loss_fct = MSELoss()\n                loss = loss_fct(logits.view(-1), labels.view(-1))\n            else:\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            outputs = (loss,) + outputs\n\n        return outputs  # (loss), logits, (hidden_states), (attentions)\n\n\n@add_start_docstrings(""""""Bert Model with a multiple choice classification head on top (a linear layer on top of\n    the pooled output and a softmax) e.g. for RocStories/SWAG tasks. """""",\n    BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\nclass BertForMultipleChoice(BertPreTrainedModel):\n    r""""""\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the multiple choice classification loss.\n            Indices should be in ``[0, ..., num_choices]`` where `num_choices` is the size of the second dimension\n            of the input tensors. (see `input_ids` above)\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Classification loss.\n        **classification_scores**: ``torch.FloatTensor`` of shape ``(batch_size, num_choices)`` where `num_choices` is the size of the second dimension\n            of the input tensors. (see `input_ids` above).\n            Classification scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n        model = BertForMultipleChoice.from_pretrained(\'bert-base-uncased\')\n        choices = [""Hello, my dog is cute"", ""Hello, my cat is amazing""]\n        input_ids = torch.tensor([tokenizer.encode(s) for s in choices]).unsqueeze(0)  # Batch size 1, 2 choices\n        labels = torch.tensor(1).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, labels=labels)\n        loss, classification_scores = outputs[:2]\n\n    """"""\n    def __init__(self, config):\n        super(BertForMultipleChoice, self).__init__(config)\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n\n        self.init_weights()\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None,\n                position_ids=None, head_mask=None, labels=None):\n        num_choices = input_ids.shape[1]\n\n        input_ids = input_ids.view(-1, input_ids.size(-1))\n        attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n        position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids,\n                            head_mask=head_mask)\n\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        reshaped_logits = logits.view(-1, num_choices)\n\n        outputs = (reshaped_logits,) + outputs[2:]  # add hidden states and attention if they are here\n\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(reshaped_logits, labels)\n            outputs = (loss,) + outputs\n\n        return outputs  # (loss), reshaped_logits, (hidden_states), (attentions)\n\n\n@add_start_docstrings(""""""Bert Model with a token classification head on top (a linear layer on top of\n    the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. """""",\n    BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\nclass BertForTokenClassification(BertPreTrainedModel):\n    r""""""\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size, sequence_length)``:\n            Labels for computing the token classification loss.\n            Indices should be in ``[0, ..., config.num_labels - 1]``.\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Classification loss.\n        **scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length, config.num_labels)``\n            Classification scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n        model = BertForTokenClassification.from_pretrained(\'bert-base-uncased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        labels = torch.tensor([1] * input_ids.size(1)).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, labels=labels)\n        loss, scores = outputs[:2]\n\n    """"""\n    def __init__(self, config):\n        super(BertForTokenClassification, self).__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None,\n                position_ids=None, head_mask=None, labels=None):\n\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids, \n                            head_mask=head_mask)\n\n        sequence_output = outputs[0]\n\n        sequence_output = self.dropout(sequence_output)\n        logits = self.classifier(sequence_output)\n\n        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            # Only keep active parts of the loss\n            if attention_mask is not None:\n                active_loss = attention_mask.view(-1) == 1\n                active_logits = logits.view(-1, self.num_labels)[active_loss]\n                active_labels = labels.view(-1)[active_loss]\n                loss = loss_fct(active_logits, active_labels)\n            else:\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            outputs = (loss,) + outputs\n\n        return outputs  # (loss), scores, (hidden_states), (attentions)\n\n\n@add_start_docstrings(""""""Bert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear layers on top of\n    the hidden-states output to compute `span start logits` and `span end logits`). """""",\n    BERT_START_DOCSTRING, BERT_INPUTS_DOCSTRING)\nclass BertForQuestionAnswering(BertPreTrainedModel):\n    r""""""\n        **start_positions**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`).\n            Position outside of the sequence are not taken into account for computing the loss.\n        **end_positions**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (`sequence_length`).\n            Position outside of the sequence are not taken into account for computing the loss.\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.\n        **start_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length,)``\n            Span-start scores (before SoftMax).\n        **end_scores**: ``torch.FloatTensor`` of shape ``(batch_size, sequence_length,)``\n            Span-end scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n        model = BertForQuestionAnswering.from_pretrained(\'bert-base-uncased\')\n        input_ids = torch.tensor(tokenizer.encode(""Hello, my dog is cute"")).unsqueeze(0)  # Batch size 1\n        start_positions = torch.tensor([1])\n        end_positions = torch.tensor([3])\n        outputs = model(input_ids, start_positions=start_positions, end_positions=end_positions)\n        loss, start_scores, end_scores = outputs[:2]\n\n    """"""\n    def __init__(self, config):\n        super(BertForQuestionAnswering, self).__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config)\n        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n\n        self.init_weights()\n\n    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None,\n                start_positions=None, end_positions=None):\n\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids, \n                            head_mask=head_mask)\n\n        sequence_output = outputs[0]\n\n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        outputs = (start_logits, end_logits,) + outputs[2:]\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions.clamp_(0, ignored_index)\n            end_positions.clamp_(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n            outputs = (total_loss,) + outputs\n\n        return outputs  # (loss), start_logits, end_logits, (hidden_states), (attentions)\n'"
model/modeling_utils.py,38,"b'""""""PyTorch BERT model.""""""\n\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport logging\nimport os\n\nimport torch\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss\nfrom torch.nn import functional as F\n\nfrom model.configuration_utils import PretrainedConfig\nfrom model.file_utils import cached_path, WEIGHTS_NAME, TF_WEIGHTS_NAME\n\nlogger = logging.getLogger(__name__)\n\n\ntry:\n    from torch.nn import Identity\nexcept ImportError:\n    # Older PyTorch compatibility\n    class Identity(nn.Module):\n        r""""""A placeholder identity operator that is argument-insensitive.\n        """"""\n        def __init__(self, *args, **kwargs):\n            super(Identity, self).__init__()\n\n        def forward(self, input):\n            return input\n\nclass PreTrainedModel(nn.Module):\n    r"""""" Base class for all models.\n\n        :class:`~pytorch_transformers.PreTrainedModel` takes care of storing the configuration of the models and handles methods for loading/downloading/saving models\n        as well as a few methods commons to all models to (i) resize the input embeddings and (ii) prune heads in the self-attention heads.\n\n        Class attributes (overridden by derived classes):\n            - ``config_class``: a class derived from :class:`~pytorch_transformers.PretrainedConfig` to use as configuration class for this model architecture.\n            - ``pretrained_model_archive_map``: a python ``dict`` of with `short-cut-names` (string) as keys and `url` (string) of associated pretrained weights as values.\n            - ``load_tf_weights``: a python ``method`` for loading a TensorFlow checkpoint in a PyTorch model, taking as arguments:\n\n                - ``model``: an instance of the relevant subclass of :class:`~pytorch_transformers.PreTrainedModel`,\n                - ``config``: an instance of the relevant subclass of :class:`~pytorch_transformers.PretrainedConfig`,\n                - ``path``: a path (string) to the TensorFlow checkpoint.\n\n            - ``base_model_prefix``: a string indicating the attribute associated to the base model in derived classes of the same architecture adding modules on top of the base model.\n    """"""\n    config_class = None\n    pretrained_model_archive_map = {}\n    load_tf_weights = lambda model, config, path: None\n    base_model_prefix = """"\n\n    def __init__(self, config, *inputs, **kwargs):\n        super(PreTrainedModel, self).__init__()\n        if not isinstance(config, PretrainedConfig):\n            raise ValueError(\n                ""Parameter config in `{}(config)` should be an instance of class `PretrainedConfig`. ""\n                ""To create a model from a pretrained model use ""\n                ""`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`"".format(\n                    self.__class__.__name__, self.__class__.__name__\n                ))\n        # Save config in model\n        self.config = config\n\n    def _get_resized_embeddings(self, old_embeddings, new_num_tokens=None):\n        """""" Build a resized Embedding Module from a provided token Embedding Module.\n            Increasing the size will add newly initialized vectors at the end\n            Reducing the size will remove vectors from the end\n\n        Args:\n            new_num_tokens: (`optional`) int\n                New number of tokens in the embedding matrix.\n                Increasing the size will add newly initialized vectors at the end\n                Reducing the size will remove vectors from the end\n                If not provided or None: return the provided token Embedding Module.\n        Return: ``torch.nn.Embeddings``\n            Pointer to the resized Embedding Module or the old Embedding Module if new_num_tokens is None\n        """"""\n        if new_num_tokens is None:\n            return old_embeddings\n\n        old_num_tokens, old_embedding_dim = old_embeddings.weight.size()\n        if old_num_tokens == new_num_tokens:\n            return old_embeddings\n\n        # Build new embeddings\n        new_embeddings = nn.Embedding(new_num_tokens, old_embedding_dim)\n        new_embeddings.to(old_embeddings.weight.device)\n\n        # initialize all new embeddings (in particular added tokens)\n        self._init_weights(new_embeddings)\n\n        # Copy word embeddings from the previous weights\n        num_tokens_to_copy = min(old_num_tokens, new_num_tokens)\n        new_embeddings.weight.data[:num_tokens_to_copy, :] = old_embeddings.weight.data[:num_tokens_to_copy, :]\n\n        return new_embeddings\n\n    def _tie_or_clone_weights(self, first_module, second_module):\n        """""" Tie or clone module weights depending of weither we are using TorchScript or not\n        """"""\n\n        if self.config.torchscript:\n            first_module.weight = nn.Parameter(second_module.weight.clone())\n        else:\n            first_module.weight = second_module.weight\n\n\n        if hasattr(first_module, \'bias\') and first_module.bias is not None:\n            first_module.bias.data = torch.nn.functional.pad(\n                first_module.bias.data,\n                (0, first_module.weight.shape[0] - first_module.bias.shape[0]),\n                \'constant\',\n                0\n            )\n\n    def resize_token_embeddings(self, new_num_tokens=None):\n        """""" Resize input token embeddings matrix of the model if new_num_tokens != config.vocab_size.\n        Take care of tying weights embeddings afterwards if the model class has a `tie_weights()` method.\n\n        Arguments:\n\n            new_num_tokens: (`optional`) int:\n                New number of tokens in the embedding matrix. Increasing the size will add newly initialized vectors at the end. Reducing the size will remove vectors from the end. \n                If not provided or None: does nothing and just returns a pointer to the input tokens ``torch.nn.Embeddings`` Module of the model.\n\n        Return: ``torch.nn.Embeddings``\n            Pointer to the input tokens Embeddings Module of the model\n        """"""\n        base_model = getattr(self, self.base_model_prefix, self)  # get the base model if needed\n        model_embeds = base_model._resize_token_embeddings(new_num_tokens)\n        if new_num_tokens is None:\n            return model_embeds\n\n        # Update base model and current model config\n        self.config.vocab_size = new_num_tokens\n        base_model.vocab_size = new_num_tokens\n\n        # Tie weights again if needed\n        if hasattr(self, \'tie_weights\'):\n            self.tie_weights()\n\n        return model_embeds\n\n    def init_weights(self):\n        """""" Initialize and prunes weights if needed. """"""\n        # Initialize weights\n        self.apply(self._init_weights)\n\n        # Prune heads if needed\n        if self.config.pruned_heads:\n            self.prune_heads(self.config.pruned_heads)\n\n    def prune_heads(self, heads_to_prune):\n        """""" Prunes heads of the base model.\n\n            Arguments:\n\n                heads_to_prune: dict with keys being selected layer indices (`int`) and associated values being the list of heads to prune in said layer (list of `int`).\n                E.g. {1: [0, 2], 2: [2, 3]} will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.\n        """"""\n        base_model = getattr(self, self.base_model_prefix, self)  # get the base model if needed\n\n        # save new sets of pruned heads as union of previously stored pruned heads and newly pruned heads\n        for layer, heads in heads_to_prune.items():\n            union_heads = set(self.config.pruned_heads.get(layer, [])) | set(heads)\n            self.config.pruned_heads[layer] = list(union_heads)  # Unfortunately we have to store it as list for JSON\n\n        base_model._prune_heads(heads_to_prune)\n\n    def save_pretrained(self, save_directory):\n        """""" Save a model and its configuration file to a directory, so that it\n            can be re-loaded using the `:func:`~pytorch_transformers.PreTrainedModel.from_pretrained`` class method.\n        """"""\n        assert os.path.isdir(save_directory), ""Saving path should be a directory where the model and configuration can be saved""\n\n        # Only save the model it-self if we are using distributed training\n        model_to_save = self.module if hasattr(self, \'module\') else self\n\n        # Save configuration file\n        model_to_save.config.save_pretrained(save_directory)\n\n        # If we save using the predefined names, we can load using `from_pretrained`\n        output_model_file = os.path.join(save_directory, WEIGHTS_NAME)\n\n        torch.save(model_to_save.state_dict(), output_model_file)\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n        r""""""Instantiate a pretrained pytorch model from a pre-trained model configuration.\n\n        The model is set in evaluation mode by default using ``model.eval()`` (Dropout modules are deactivated)\n        To train the model, you should first set it back in training mode with ``model.train()``\n\n        The warning ``Weights from XXX not initialized from pretrained model`` means that the weights of XXX do not come pre-trained with the rest of the model.\n        It is up to you to train those weights with a downstream fine-tuning task.\n\n        The warning ``Weights from XXX not used in YYY`` means that the layer XXX is not used by YYY, therefore those weights are discarded.\n\n        Parameters:\n            pretrained_model_name_or_path: either:\n\n                - a string with the `shortcut name` of a pre-trained model to load from cache or download, e.g.: ``bert-base-uncased``.\n                - a path to a `directory` containing model weights saved using :func:`~pytorch_transformers.PreTrainedModel.save_pretrained`, e.g.: ``./my_model_directory/``.\n                - a path or url to a `tensorflow index checkpoint file` (e.g. `./tf_model/model.ckpt.index`). In this case, ``from_tf`` should be set to True and a configuration object should be provided as ``config`` argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n\n            model_args: (`optional`) Sequence of positional arguments:\n                All remaning positional arguments will be passed to the underlying model\'s ``__init__`` method\n\n            config: (`optional`) instance of a class derived from :class:`~pytorch_transformers.PretrainedConfig`:\n                Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:\n\n                - the model is a model provided by the library (loaded with the ``shortcut-name`` string of a pretrained model), or\n                - the model was saved using :func:`~pytorch_transformers.PreTrainedModel.save_pretrained` and is reloaded by suppling the save directory.\n                - the model is loaded by suppling a local directory as ``pretrained_model_name_or_path`` and a configuration JSON file named `config.json` is found in the directory.\n\n            state_dict: (`optional`) dict:\n                an optional state dictionnary for the model to use instead of a state dictionary loaded from saved weights file.\n                This option can be used if you want to create a model from a pretrained configuration but load your own weights.\n                In this case though, you should check if using :func:`~pytorch_transformers.PreTrainedModel.save_pretrained` and :func:`~pytorch_transformers.PreTrainedModel.from_pretrained` is not a simpler option.\n\n            cache_dir: (`optional`) string:\n                Path to a directory in which a downloaded pre-trained model\n                configuration should be cached if the standard cache should not be used.\n\n            force_download: (`optional`) boolean, default False:\n                Force to (re-)download the model weights and configuration files and override the cached versions if they exists.\n\n            proxies: (`optional`) dict, default None:\n                A dictionary of proxy servers to use by protocol or endpoint, e.g.: {\'http\': \'foo.bar:3128\', \'http://hostname\': \'foo.bar:4012\'}.\n                The proxies are used on each request.\n\n            output_loading_info: (`optional`) boolean:\n                Set to ``True`` to also return a dictionnary containing missing keys, unexpected keys and error messages.\n\n            kwargs: (`optional`) Remaining dictionary of keyword arguments:\n                Can be used to update the configuration object (after it being loaded) and initiate the model. (e.g. ``output_attention=True``). Behave differently depending on whether a `config` is provided or automatically loaded:\n\n                - If a configuration is provided with ``config``, ``**kwargs`` will be directly passed to the underlying model\'s ``__init__`` method (we assume all relevant updates to the configuration have already been done)\n                - If a configuration is not provided, ``kwargs`` will be first passed to the configuration class initialization function (:func:`~pytorch_transformers.PretrainedConfig.from_pretrained`). Each key of ``kwargs`` that corresponds to a configuration attribute will be used to override said attribute with the supplied ``kwargs`` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model\'s ``__init__`` function.\n\n        Examples::\n\n            model = BertModel.from_pretrained(\'bert-base-uncased\')    # Download model and configuration from S3 and cache.\n            model = BertModel.from_pretrained(\'./test/saved_model/\')  # E.g. model was saved using `save_pretrained(\'./test/saved_model/\')`\n            model = BertModel.from_pretrained(\'bert-base-uncased\', output_attention=True)  # Update configuration during loading\n            assert model.config.output_attention == True\n            # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n            config = BertConfig.from_json_file(\'./tf_model/my_tf_model_config.json\')\n            model = BertModel.from_pretrained(\'./tf_model/my_tf_checkpoint.ckpt.index\', from_tf=True, config=config)\n\n        """"""\n        config = kwargs.pop(\'config\', None)\n        state_dict = kwargs.pop(\'state_dict\', None)\n        cache_dir = kwargs.pop(\'cache_dir\', None)\n        from_tf = kwargs.pop(\'from_tf\', False)\n        force_download = kwargs.pop(\'force_download\', False)\n        proxies = kwargs.pop(\'proxies\', None)\n        output_loading_info = kwargs.pop(\'output_loading_info\', False)\n\n        # Load config\n        if config is None:\n            config, model_kwargs = cls.config_class.from_pretrained(\n                pretrained_model_name_or_path, *model_args,\n                cache_dir=cache_dir, return_unused_kwargs=True,\n                force_download=force_download,\n                **kwargs\n            )\n        else:\n            model_kwargs = kwargs\n\n        # Load model\n        if pretrained_model_name_or_path in cls.pretrained_model_archive_map:\n            archive_file = cls.pretrained_model_archive_map[pretrained_model_name_or_path]\n        elif os.path.isdir(pretrained_model_name_or_path):\n            if from_tf:\n                # Directly load from a TensorFlow checkpoint\n                archive_file = os.path.join(pretrained_model_name_or_path, TF_WEIGHTS_NAME + "".index"")\n            else:\n                archive_file = os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)\n        else:\n            if from_tf:\n                # Directly load from a TensorFlow checkpoint\n                archive_file = pretrained_model_name_or_path + "".index""\n            else:\n                archive_file = pretrained_model_name_or_path\n        # redirect to the cache, if necessary\n        try:\n            resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir, force_download=force_download, proxies=proxies)\n        except EnvironmentError as e:\n            if pretrained_model_name_or_path in cls.pretrained_model_archive_map:\n                logger.error(\n                    ""Couldn\'t reach server at \'{}\' to download pretrained weights."".format(\n                        archive_file))\n            else:\n                logger.error(\n                    ""Model name \'{}\' was not found in model name list ({}). ""\n                    ""We assumed \'{}\' was a path or url but couldn\'t find any file ""\n                    ""associated to this path or url."".format(\n                        pretrained_model_name_or_path,\n                        \', \'.join(cls.pretrained_model_archive_map.keys()),\n                        archive_file))\n            raise e\n        if resolved_archive_file == archive_file:\n            logger.info(""loading weights file {}"".format(archive_file))\n        else:\n            logger.info(""loading weights file {} from cache at {}"".format(\n                archive_file, resolved_archive_file))\n\n        # Instantiate model.\n        model = cls(config, *model_args, **model_kwargs)\n\n        if state_dict is None and not from_tf:\n            state_dict = torch.load(resolved_archive_file, map_location=\'cpu\')\n        if from_tf:\n            # Directly load from a TensorFlow checkpoint\n            return cls.load_tf_weights(model, config, resolved_archive_file[:-6])  # Remove the \'.index\'\n\n        # Convert old format to new format if needed from a PyTorch state_dict\n        old_keys = []\n        new_keys = []\n        for key in state_dict.keys():\n            new_key = None\n            if \'gamma\' in key:\n                new_key = key.replace(\'gamma\', \'weight\')\n            if \'beta\' in key:\n                new_key = key.replace(\'beta\', \'bias\')\n            if new_key:\n                old_keys.append(key)\n                new_keys.append(new_key)\n        for old_key, new_key in zip(old_keys, new_keys):\n            state_dict[new_key] = state_dict.pop(old_key)\n\n        # Load from a PyTorch state_dict\n        missing_keys = []\n        unexpected_keys = []\n        error_msgs = []\n        # copy state_dict so _load_from_state_dict can modify it\n        metadata = getattr(state_dict, \'_metadata\', None)\n        state_dict = state_dict.copy()\n        if metadata is not None:\n            state_dict._metadata = metadata\n\n        def load(module, prefix=\'\'):\n            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n            module._load_from_state_dict(\n                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n            for name, child in module._modules.items():\n                if child is not None:\n                    load(child, prefix + name + \'.\')\n\n        # Make sure we are able to load base models as well as derived models (with heads)\n        start_prefix = \'\'\n        model_to_load = model\n        if not hasattr(model, cls.base_model_prefix) and any(s.startswith(cls.base_model_prefix) for s in state_dict.keys()):\n            start_prefix = cls.base_model_prefix + \'.\'\n        if hasattr(model, cls.base_model_prefix) and not any(s.startswith(cls.base_model_prefix) for s in state_dict.keys()):\n            model_to_load = getattr(model, cls.base_model_prefix)\n\n        load(model_to_load, prefix=start_prefix)\n        if len(missing_keys) > 0:\n            logger.info(""Weights of {} not initialized from pretrained model: {}"".format(\n                model.__class__.__name__, missing_keys))\n        if len(unexpected_keys) > 0:\n            logger.info(""Weights from pretrained model not used in {}: {}"".format(\n                model.__class__.__name__, unexpected_keys))\n        if len(error_msgs) > 0:\n            raise RuntimeError(\'Error(s) in loading state_dict for {}:\\n\\t{}\'.format(\n                               model.__class__.__name__, ""\\n\\t"".join(error_msgs)))\n\n        if hasattr(model, \'tie_weights\'):\n            model.tie_weights()  # make sure word embedding weights are still tied\n\n        # Set model in evaluation mode to desactivate DropOut modules by default\n        model.eval()\n\n        if output_loading_info:\n            loading_info = {""missing_keys"": missing_keys, ""unexpected_keys"": unexpected_keys, ""error_msgs"": error_msgs}\n            return model, loading_info\n\n        return model\n\n\nclass Conv1D(nn.Module):\n    def __init__(self, nf, nx):\n        """""" Conv1D layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2)\n            Basically works like a Linear layer but the weights are transposed\n        """"""\n        super(Conv1D, self).__init__()\n        self.nf = nf\n        w = torch.empty(nx, nf)\n        nn.init.normal_(w, std=0.02)\n        self.weight = nn.Parameter(w)\n        self.bias = nn.Parameter(torch.zeros(nf))\n\n    def forward(self, x):\n        size_out = x.size()[:-1] + (self.nf,)\n        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)\n        x = x.view(*size_out)\n        return x\n\n\nclass PoolerStartLogits(nn.Module):\n    """""" Compute SQuAD start_logits from sequence hidden states. """"""\n    def __init__(self, config):\n        super(PoolerStartLogits, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, 1)\n\n    def forward(self, hidden_states, p_mask=None):\n        """""" Args:\n            **p_mask**: (`optional`) ``torch.FloatTensor`` of shape `(batch_size, seq_len)`\n                invalid position mask such as query and special symbols (PAD, SEP, CLS)\n                1.0 means token should be masked.\n        """"""\n        x = self.dense(hidden_states).squeeze(-1)\n\n        if p_mask is not None:\n            x = x * (1 - p_mask) - 1e30 * p_mask\n\n        return x\n\n\nclass PoolerEndLogits(nn.Module):\n    """""" Compute SQuAD end_logits from sequence hidden states and start token hidden state.\n    """"""\n    def __init__(self, config):\n        super(PoolerEndLogits, self).__init__()\n        self.dense_0 = nn.Linear(config.hidden_size * 2, config.hidden_size)\n        self.activation = nn.Tanh()\n        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.dense_1 = nn.Linear(config.hidden_size, 1)\n\n    def forward(self, hidden_states, start_states=None, start_positions=None, p_mask=None):\n        """""" Args:\n            One of ``start_states``, ``start_positions`` should be not None.\n            If both are set, ``start_positions`` overrides ``start_states``.\n\n            **start_states**: ``torch.LongTensor`` of shape identical to hidden_states\n                hidden states of the first tokens for the labeled span.\n            **start_positions**: ``torch.LongTensor`` of shape ``(batch_size,)``\n                position of the first token for the labeled span:\n            **p_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, seq_len)``\n                Mask of invalid position such as query and special symbols (PAD, SEP, CLS)\n                1.0 means token should be masked.\n        """"""\n        assert start_states is not None or start_positions is not None, ""One of start_states, start_positions should be not None""\n        if start_positions is not None:\n            slen, hsz = hidden_states.shape[-2:]\n            start_positions = start_positions[:, None, None].expand(-1, -1, hsz) # shape (bsz, 1, hsz)\n            start_states = hidden_states.gather(-2, start_positions) # shape (bsz, 1, hsz)\n            start_states = start_states.expand(-1, slen, -1) # shape (bsz, slen, hsz)\n\n        x = self.dense_0(torch.cat([hidden_states, start_states], dim=-1))\n        x = self.activation(x)\n        x = self.LayerNorm(x)\n        x = self.dense_1(x).squeeze(-1)\n\n        if p_mask is not None:\n            x = x * (1 - p_mask) - 1e30 * p_mask\n\n        return x\n\n\nclass PoolerAnswerClass(nn.Module):\n    """""" Compute SQuAD 2.0 answer class from classification and start tokens hidden states. """"""\n    def __init__(self, config):\n        super(PoolerAnswerClass, self).__init__()\n        self.dense_0 = nn.Linear(config.hidden_size * 2, config.hidden_size)\n        self.activation = nn.Tanh()\n        self.dense_1 = nn.Linear(config.hidden_size, 1, bias=False)\n\n    def forward(self, hidden_states, start_states=None, start_positions=None, cls_index=None):\n        """"""\n        Args:\n            One of ``start_states``, ``start_positions`` should be not None.\n            If both are set, ``start_positions`` overrides ``start_states``.\n\n            **start_states**: ``torch.LongTensor`` of shape identical to ``hidden_states``.\n                hidden states of the first tokens for the labeled span.\n            **start_positions**: ``torch.LongTensor`` of shape ``(batch_size,)``\n                position of the first token for the labeled span.\n            **cls_index**: torch.LongTensor of shape ``(batch_size,)``\n                position of the CLS token. If None, take the last token.\n\n            note(Original repo):\n                no dependency on end_feature so that we can obtain one single `cls_logits`\n                for each sample\n        """"""\n        hsz = hidden_states.shape[-1]\n        assert start_states is not None or start_positions is not None, ""One of start_states, start_positions should be not None""\n        if start_positions is not None:\n            start_positions = start_positions[:, None, None].expand(-1, -1, hsz) # shape (bsz, 1, hsz)\n            start_states = hidden_states.gather(-2, start_positions).squeeze(-2) # shape (bsz, hsz)\n\n        if cls_index is not None:\n            cls_index = cls_index[:, None, None].expand(-1, -1, hsz) # shape (bsz, 1, hsz)\n            cls_token_state = hidden_states.gather(-2, cls_index).squeeze(-2) # shape (bsz, hsz)\n        else:\n            cls_token_state = hidden_states[:, -1, :] # shape (bsz, hsz)\n\n        x = self.dense_0(torch.cat([start_states, cls_token_state], dim=-1))\n        x = self.activation(x)\n        x = self.dense_1(x).squeeze(-1)\n\n        return x\n\n\nclass SQuADHead(nn.Module):\n    r"""""" A SQuAD head inspired by XLNet.\n\n    Parameters:\n        config (:class:`~pytorch_transformers.XLNetConfig`): Model configuration class with all the parameters of the model.\n\n    Inputs:\n        **hidden_states**: ``torch.FloatTensor`` of shape ``(batch_size, seq_len, hidden_size)``\n            hidden states of sequence tokens\n        **start_positions**: ``torch.LongTensor`` of shape ``(batch_size,)``\n            position of the first token for the labeled span.\n        **end_positions**: ``torch.LongTensor`` of shape ``(batch_size,)``\n            position of the last token for the labeled span.\n        **cls_index**: torch.LongTensor of shape ``(batch_size,)``\n            position of the CLS token. If None, take the last token.\n        **is_impossible**: ``torch.LongTensor`` of shape ``(batch_size,)``\n            Whether the question has a possible answer in the paragraph or not.\n        **p_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, seq_len)``\n            Mask of invalid position such as query and special symbols (PAD, SEP, CLS)\n            1.0 means token should be masked.\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned if both ``start_positions`` and ``end_positions`` are provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Classification loss as the sum of start token, end token (and is_impossible if provided) classification losses.\n        **start_top_log_probs**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)\n            ``torch.FloatTensor`` of shape ``(batch_size, config.start_n_top)``\n            Log probabilities for the top config.start_n_top start token possibilities (beam-search).\n        **start_top_index**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)\n            ``torch.LongTensor`` of shape ``(batch_size, config.start_n_top)``\n            Indices for the top config.start_n_top start token possibilities (beam-search).\n        **end_top_log_probs**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)\n            ``torch.FloatTensor`` of shape ``(batch_size, config.start_n_top * config.end_n_top)``\n            Log probabilities for the top ``config.start_n_top * config.end_n_top`` end token possibilities (beam-search).\n        **end_top_index**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)\n            ``torch.LongTensor`` of shape ``(batch_size, config.start_n_top * config.end_n_top)``\n            Indices for the top ``config.start_n_top * config.end_n_top`` end token possibilities (beam-search).\n        **cls_logits**: (`optional`, returned if ``start_positions`` or ``end_positions`` is not provided)\n            ``torch.FloatTensor`` of shape ``(batch_size,)``\n            Log probabilities for the ``is_impossible`` label of the answers.\n    """"""\n    def __init__(self, config):\n        super(SQuADHead, self).__init__()\n        self.start_n_top = config.start_n_top\n        self.end_n_top = config.end_n_top\n\n        self.start_logits = PoolerStartLogits(config)\n        self.end_logits = PoolerEndLogits(config)\n        self.answer_class = PoolerAnswerClass(config)\n\n    def forward(self, hidden_states, start_positions=None, end_positions=None,\n                cls_index=None, is_impossible=None, p_mask=None):\n        outputs = ()\n\n        start_logits = self.start_logits(hidden_states, p_mask=p_mask)\n\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, let\'s remove the dimension added by batch splitting\n            for x in (start_positions, end_positions, cls_index, is_impossible):\n                if x is not None and x.dim() > 1:\n                    x.squeeze_(-1)\n\n            # during training, compute the end logits based on the ground truth of the start position\n            end_logits = self.end_logits(hidden_states, start_positions=start_positions, p_mask=p_mask)\n\n            loss_fct = CrossEntropyLoss()\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n\n            if cls_index is not None and is_impossible is not None:\n                # Predict answerability from the representation of CLS and START\n                cls_logits = self.answer_class(hidden_states, start_positions=start_positions, cls_index=cls_index)\n                loss_fct_cls = nn.BCEWithLogitsLoss()\n                cls_loss = loss_fct_cls(cls_logits, is_impossible)\n\n                # note(zhiliny): by default multiply the loss by 0.5 so that the scale is comparable to start_loss and end_loss\n                total_loss += cls_loss * 0.5\n\n            outputs = (total_loss,) + outputs\n\n        else:\n            # during inference, compute the end logits based on beam search\n            bsz, slen, hsz = hidden_states.size()\n            start_log_probs = F.softmax(start_logits, dim=-1) # shape (bsz, slen)\n\n            start_top_log_probs, start_top_index = torch.topk(start_log_probs, self.start_n_top, dim=-1) # shape (bsz, start_n_top)\n            start_top_index_exp = start_top_index.unsqueeze(-1).expand(-1, -1, hsz) # shape (bsz, start_n_top, hsz)\n            start_states = torch.gather(hidden_states, -2, start_top_index_exp) # shape (bsz, start_n_top, hsz)\n            start_states = start_states.unsqueeze(1).expand(-1, slen, -1, -1) # shape (bsz, slen, start_n_top, hsz)\n\n            hidden_states_expanded = hidden_states.unsqueeze(2).expand_as(start_states) # shape (bsz, slen, start_n_top, hsz)\n            p_mask = p_mask.unsqueeze(-1) if p_mask is not None else None\n            end_logits = self.end_logits(hidden_states_expanded, start_states=start_states, p_mask=p_mask)\n            end_log_probs = F.softmax(end_logits, dim=1) # shape (bsz, slen, start_n_top)\n\n            end_top_log_probs, end_top_index = torch.topk(end_log_probs, self.end_n_top, dim=1) # shape (bsz, end_n_top, start_n_top)\n            end_top_log_probs = end_top_log_probs.view(-1, self.start_n_top * self.end_n_top)\n            end_top_index = end_top_index.view(-1, self.start_n_top * self.end_n_top)\n\n            start_states = torch.einsum(""blh,bl->bh"", hidden_states, start_log_probs)\n            cls_logits = self.answer_class(hidden_states, start_states=start_states, cls_index=cls_index)\n\n            outputs = (start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits) + outputs\n\n        # return start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits\n        # or (if labels are provided) (total_loss,)\n        return outputs\n\n\nclass SequenceSummary(nn.Module):\n    r"""""" Compute a single vector summary of a sequence hidden states according to various possibilities:\n        Args of the config class:\n            summary_type:\n                - \'last\' => [default] take the last token hidden state (like XLNet)\n                - \'first\' => take the first token hidden state (like Bert)\n                - \'mean\' => take the mean of all tokens hidden states\n                - \'cls_index\' => supply a Tensor of classification token position (GPT/GPT-2)\n                - \'attn\' => Not implemented now, use multi-head attention\n            summary_use_proj: Add a projection after the vector extraction\n            summary_proj_to_labels: If True, the projection outputs to config.num_labels classes (otherwise to hidden_size). Default: False.\n            summary_activation: \'tanh\' => add a tanh activation to the output, Other => no activation. Default\n            summary_first_dropout: Add a dropout before the projection and activation\n            summary_last_dropout: Add a dropout after the projection and activation\n    """"""\n    def __init__(self, config):\n        super(SequenceSummary, self).__init__()\n\n        self.summary_type = config.summary_type if hasattr(config, \'summary_use_proj\') else \'last\'\n        if self.summary_type == \'attn\':\n            # We should use a standard multi-head attention module with absolute positional embedding for that.\n            # Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276\n            # We can probably just use the multi-head attention module of PyTorch >=1.1.0\n            raise NotImplementedError\n\n        self.summary = Identity()\n        if hasattr(config, \'summary_use_proj\') and config.summary_use_proj:\n            if hasattr(config, \'summary_proj_to_labels\') and config.summary_proj_to_labels and config.num_labels > 0:\n                num_classes = config.num_labels\n            else:\n                num_classes = config.hidden_size\n            self.summary = nn.Linear(config.hidden_size, num_classes)\n\n        self.activation = Identity()\n        if hasattr(config, \'summary_activation\') and config.summary_activation == \'tanh\':\n            self.activation = nn.Tanh()\n\n        self.first_dropout = Identity()\n        if hasattr(config, \'summary_first_dropout\') and config.summary_first_dropout > 0:\n            self.first_dropout = nn.Dropout(config.summary_first_dropout)\n\n        self.last_dropout = Identity()\n        if hasattr(config, \'summary_last_dropout\') and config.summary_last_dropout > 0:\n            self.last_dropout = nn.Dropout(config.summary_last_dropout)\n\n    def forward(self, hidden_states, cls_index=None):\n        """""" hidden_states: float Tensor in shape [bsz, seq_len, hidden_size], the hidden-states of the last layer.\n            cls_index: [optional] position of the classification token if summary_type == \'cls_index\',\n                shape (bsz,) or more generally (bsz, ...) where ... are optional leading dimensions of hidden_states.\n                if summary_type == \'cls_index\' and cls_index is None:\n                    we take the last token of the sequence as classification token\n        """"""\n        if self.summary_type == \'last\':\n            output = hidden_states[:, -1]\n        elif self.summary_type == \'first\':\n            output = hidden_states[:, 0]\n        elif self.summary_type == \'mean\':\n            output = hidden_states.mean(dim=1)\n        elif self.summary_type == \'cls_index\':\n            if cls_index is None:\n                cls_index = torch.full_like(hidden_states[..., :1, :], hidden_states.shape[-2]-1, dtype=torch.long)\n            else:\n                cls_index = cls_index.unsqueeze(-1).unsqueeze(-1)\n                cls_index = cls_index.expand((-1,) * (cls_index.dim()-1) + (hidden_states.size(-1),))\n            # shape of cls_index: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states\n            output = hidden_states.gather(-2, cls_index).squeeze(-2) # shape (bsz, XX, hidden_size)\n        elif self.summary_type == \'attn\':\n            raise NotImplementedError\n\n        output = self.first_dropout(output)\n        output = self.summary(output)\n        output = self.activation(output)\n        output = self.last_dropout(output)\n\n        return output\n\n\ndef prune_linear_layer(layer, index, dim=0):\n    """""" Prune a linear layer (a model parameters) to keep only entries in index.\n        Return the pruned layer as a new layer with requires_grad=True.\n        Used to remove heads.\n    """"""\n    index = index.to(layer.weight.device)\n    W = layer.weight.index_select(dim, index).clone().detach()\n    if layer.bias is not None:\n        if dim == 1:\n            b = layer.bias.clone().detach()\n        else:\n            b = layer.bias[index].clone().detach()\n    new_size = list(layer.weight.size())\n    new_size[dim] = len(index)\n    new_layer = nn.Linear(new_size[1], new_size[0], bias=layer.bias is not None).to(layer.weight.device)\n    new_layer.weight.requires_grad = False\n    new_layer.weight.copy_(W.contiguous())\n    new_layer.weight.requires_grad = True\n    if layer.bias is not None:\n        new_layer.bias.requires_grad = False\n        new_layer.bias.copy_(b.contiguous())\n        new_layer.bias.requires_grad = True\n    return new_layer\n\n\ndef prune_conv1d_layer(layer, index, dim=1):\n    """""" Prune a Conv1D layer (a model parameters) to keep only entries in index.\n        A Conv1D work as a Linear layer (see e.g. BERT) but the weights are transposed.\n        Return the pruned layer as a new layer with requires_grad=True.\n        Used to remove heads.\n    """"""\n    index = index.to(layer.weight.device)\n    W = layer.weight.index_select(dim, index).clone().detach()\n    if dim == 0:\n        b = layer.bias.clone().detach()\n    else:\n        b = layer.bias[index].clone().detach()\n    new_size = list(layer.weight.size())\n    new_size[dim] = len(index)\n    new_layer = Conv1D(new_size[1], new_size[0]).to(layer.weight.device)\n    new_layer.weight.requires_grad = False\n    new_layer.weight.copy_(W.contiguous())\n    new_layer.weight.requires_grad = True\n    new_layer.bias.requires_grad = False\n    new_layer.bias.copy_(b.contiguous())\n    new_layer.bias.requires_grad = True\n    return new_layer\n\n\ndef prune_layer(layer, index, dim=None):\n    """""" Prune a Conv1D or nn.Linear layer (a model parameters) to keep only entries in index.\n        Return the pruned layer as a new layer with requires_grad=True.\n        Used to remove heads.\n    """"""\n    if isinstance(layer, nn.Linear):\n        return prune_linear_layer(layer, index, dim=0 if dim is None else dim)\n    elif isinstance(layer, Conv1D):\n        return prune_conv1d_layer(layer, index, dim=1 if dim is None else dim)\n    else:\n        raise ValueError(""Can\'t prune layer of class {}"".format(layer.__class__))\n'"
model/tokenization_albert.py,0,"b'""""""Tokenization classes.""""""\n\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\nimport collections\nimport unicodedata\nimport six\nimport logging\nimport sentencepiece as spm\n\nlogger = logging.getLogger(__name__)\nSPIECE_UNDERLINE = u""\xe2\x96\x81""\n\ndef preprocess_text(inputs,remove_space=True,do_lower_case=True):\n  if remove_space:\n    outputs = \' \'.join(inputs.strip().split())\n  else:\n    outputs = inputs\n  outputs = outputs.replace(""``"", \'""\').replace(""\'\'"", \'""\')\n  if six.PY2 and isinstance(outputs, str):\n    outputs = outputs.decode(\'utf-8\')\n  outputs = unicodedata.normalize(""NFKD"", outputs)\n  outputs = """".join([c for c in outputs if not unicodedata.combining(c)])\n  if do_lower_case:\n    outputs = outputs.lower()\n  return outputs\n\ndef encode_pieces(sp_model, text, return_unicode=True, sample=False):\n  """"""turn sentences into word pieces.""""""\n  text = preprocess_text(text,)\n  if six.PY2 and isinstance(text, unicode):\n    text = text.encode(\'utf-8\')\n  if not sample:\n    pieces = sp_model.EncodeAsPieces(text)\n  else:\n    pieces = sp_model.SampleEncodeAsPieces(text, 64, 0.1)\n  new_pieces = []\n  for piece in pieces:\n    if len(piece) > 1 and piece[-1] == \',\' and piece[-2].isdigit():\n      cur_pieces = sp_model.EncodeAsPieces(\n        piece[:-1].replace(SPIECE_UNDERLINE, \'\'))\n      if piece[0] != SPIECE_UNDERLINE and cur_pieces[0][0] == SPIECE_UNDERLINE:\n        if len(cur_pieces[0]) == 1:\n          cur_pieces = cur_pieces[1:]\n        else:\n          cur_pieces[0] = cur_pieces[0][1:]\n      cur_pieces.append(piece[-1])\n      new_pieces.extend(cur_pieces)\n    else:\n      new_pieces.append(piece)\n\n  # note(zhiliny): convert back to unicode for py2\n  if six.PY2 and return_unicode:\n    ret_pieces = []\n    for piece in new_pieces:\n      if isinstance(piece, str):\n        piece = piece.decode(piece, ""utf-8"")\n      ret_pieces.append(piece)\n    new_pieces = ret_pieces\n\n  return new_pieces\n\ndef encode_ids(sp_model, text, sample=False):\n  pieces = encode_pieces(sp_model, text, return_unicode=False, sample=sample)\n  ids = [sp_model.PieceToId(piece) for piece in pieces]\n  return ids\n\n\ndef load_vocab(vocab_file):\n  """"""Loads a vocabulary file into a dictionary.""""""\n  vocab = collections.OrderedDict()\n  with open(vocab_file, ""r"", encoding=""utf-8"") as reader:\n    tokens = reader.readlines()\n  for index, token in enumerate(tokens):\n    token = token.rstrip(\'\\n\')\n    vocab[token] = index\n  return vocab\n\ndef convert_by_vocab(vocab, items):\n  """"""Converts a sequence of [tokens|ids] using the vocab.""""""\n  output = []\n  for item in items:\n    output.append(vocab[item])\n  return output\n\ndef convert_tokens_to_ids(vocab, tokens):\n  return convert_by_vocab(vocab, tokens)\n\ndef convert_ids_to_tokens(inv_vocab, ids):\n  return convert_by_vocab(inv_vocab, ids)\n\ndef whitespace_tokenize(text):\n  """"""Runs basic whitespace cleaning and splitting on a piece of text.""""""\n  text = text.strip()\n  if not text:\n    return []\n  tokens = text.split()\n  return tokens\n\nclass FullTokenizer(object):\n  """"""Runs end-to-end tokenziation.""""""\n\n  def __init__(self, vocab_file, do_lower_case=True, spm_model_file=None):\n    self.vocab = None\n    self.sp_model = None\n    if spm_model_file:\n      self.sp_model = spm.SentencePieceProcessor()\n      logger.info(""loading sentence piece model"")\n      self.sp_model.Load(spm_model_file)\n      # # Note(mingdachen): For the purpose of consisent API, we are\n      # # generating a vocabulary for the sentence piece tokenizer.\n      self.vocab = {self.sp_model.IdToPiece(i): i for i\n                    in range(self.sp_model.GetPieceSize())}\n    else:\n      print(""load vocab"")\n      self.vocab = load_vocab(vocab_file)\n      print(""load token"")\n      self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n      self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab,unk_token=""[UNK]"", max_input_chars_per_word=100)\n    self.inv_vocab = {v: k for k, v in self.vocab.items()}\n\n  def tokenize(self, text):\n    if self.sp_model:\n      split_tokens = encode_pieces(self.sp_model, text, return_unicode=False)\n    else:\n      split_tokens = []\n      for token in self.basic_tokenizer.tokenize(text):\n        for sub_token in self.wordpiece_tokenizer.tokenize(token):\n          split_tokens.append(sub_token)\n\n    return split_tokens\n\n  def convert_tokens_to_ids(self, tokens):\n    if self.sp_model:\n      return [self.sp_model.PieceToId(token) for token in tokens]\n    else:\n      return convert_by_vocab(self.vocab, tokens)\n\n  def convert_ids_to_tokens(self, ids):\n    if self.sp_model:\n      logger.info(""using sentence piece tokenzier."")\n      return [self.sp_model.IdToPiece(id_) for id_ in ids]\n    else:\n      return convert_by_vocab(self.inv_vocab, ids)\n\nclass BasicTokenizer(object):\n  """"""Runs basic tokenization (punctuation splitting, lower casing, etc.).""""""\n\n  def __init__(self, do_lower_case=True):\n    """"""Constructs a BasicTokenizer.\n\n    Args:\n      do_lower_case: Whether to lower case the input.\n    """"""\n    self.do_lower_case = do_lower_case\n\n  def tokenize(self, text):\n    """"""Tokenizes a piece of text.""""""\n    text = self._clean_text(text)\n\n    # This was added on November 1st, 2018 for the multilingual and Chinese\n    # models. This is also applied to the English models now, but it doesn\'t\n    # matter since the English models were not trained on any Chinese data\n    # and generally don\'t have any Chinese data in them (there are Chinese\n    # characters in the vocabulary because Wikipedia does have some Chinese\n    # words in the English Wikipedia.).\n    text = self._tokenize_chinese_chars(text)\n    orig_tokens = whitespace_tokenize(text)\n    split_tokens = []\n    for token in orig_tokens:\n      if self.do_lower_case:\n        token = token.lower()\n        token = self._run_strip_accents(token)\n      split_tokens.extend(self._run_split_on_punc(token))\n    output_tokens = whitespace_tokenize("" "".join(split_tokens))\n    return output_tokens\n\n  def _run_strip_accents(self, text):\n    """"""Strips accents from a piece of text.""""""\n    text = unicodedata.normalize(""NFD"", text)\n    output = []\n    for char in text:\n      cat = unicodedata.category(char)\n      if cat == ""Mn"":\n        continue\n      output.append(char)\n    return """".join(output)\n\n  def _run_split_on_punc(self, text):\n    """"""Splits punctuation on a piece of text.""""""\n    chars = list(text)\n    i = 0\n    start_new_word = True\n    output = []\n    while i < len(chars):\n      char = chars[i]\n      if _is_punctuation(char):\n        output.append([char])\n        start_new_word = True\n      else:\n        if start_new_word:\n          output.append([])\n        start_new_word = False\n        output[-1].append(char)\n      i += 1\n\n    return ["""".join(x) for x in output]\n\n  def _tokenize_chinese_chars(self, text):\n    """"""Adds whitespace around any CJK character.""""""\n    output = []\n    for char in text:\n      cp = ord(char)\n      if self._is_chinese_char(cp):\n        output.append("" "")\n        output.append(char)\n        output.append("" "")\n      else:\n        output.append(char)\n    return """".join(output)\n\n  def _is_chinese_char(self, cp):\n    """"""Checks whether CP is the codepoint of a CJK character.""""""\n    # This defines a ""chinese character"" as anything in the CJK Unicode block:\n    #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n    #\n    # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n    # despite its name. The modern Korean Hangul alphabet is a different block,\n    # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n    # space-separated words, so they are not treated specially and handled\n    # like the all of the other languages.\n    if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n        (cp >= 0x3400 and cp <= 0x4DBF) or  #\n        (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n        (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n        (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n        (cp >= 0x2B820 and cp <= 0x2CEAF) or\n        (cp >= 0xF900 and cp <= 0xFAFF) or  #\n        (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n      return True\n\n    return False\n\n  def _clean_text(self, text):\n    """"""Performs invalid character removal and whitespace cleanup on text.""""""\n    output = []\n    for char in text:\n      cp = ord(char)\n      if cp == 0 or cp == 0xfffd or _is_control(char):\n        continue\n      if _is_whitespace(char):\n        output.append("" "")\n      else:\n        output.append(char)\n    return """".join(output)\n\nclass WordpieceTokenizer(object):\n    """"""Runs WordPiece tokenization.""""""\n\n    def __init__(self, vocab, unk_token, max_input_chars_per_word=100):\n      self.vocab = vocab\n      self.unk_token = unk_token\n      self.max_input_chars_per_word = max_input_chars_per_word\n\n    def tokenize(self, text):\n      """"""Tokenizes a piece of text into its word pieces.\n\n      This uses a greedy longest-match-first algorithm to perform tokenization\n      using the given vocabulary.\n\n      For example:\n        input = ""unaffable""\n        output = [""un"", ""##aff"", ""##able""]\n\n      Args:\n        text: A single token or whitespace separated tokens. This should have\n          already been passed through `BasicTokenizer`.\n\n      Returns:\n        A list of wordpiece tokens.\n      """"""\n\n      output_tokens = []\n      for token in whitespace_tokenize(text):\n        chars = list(token)\n        if len(chars) > self.max_input_chars_per_word:\n          output_tokens.append(self.unk_token)\n          continue\n\n        is_bad = False\n        start = 0\n        sub_tokens = []\n        while start < len(chars):\n          end = len(chars)\n          cur_substr = None\n          while start < end:\n            substr = """".join(chars[start:end])\n            if start > 0:\n              substr = ""##"" + substr\n            if substr in self.vocab:\n              cur_substr = substr\n              break\n            end -= 1\n          if cur_substr is None:\n            is_bad = True\n            break\n          sub_tokens.append(cur_substr)\n          start = end\n\n        if is_bad:\n          output_tokens.append(self.unk_token)\n        else:\n          output_tokens.extend(sub_tokens)\n      return output_tokens\n\ndef _is_whitespace(char):\n  """"""Checks whether `chars` is a whitespace character.""""""\n  # \\t, \\n, and \\r are technically control characters but we treat them\n  # as whitespace since they are generally considered as such.\n  if char == "" "" or char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n    return True\n  cat = unicodedata.category(char)\n  if cat == ""Zs"":\n    return True\n  return False\n\n\ndef _is_control(char):\n  """"""Checks whether `chars` is a control character.""""""\n  # These are technically control characters but we count them as whitespace\n  # characters.\n  if char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n    return False\n  cat = unicodedata.category(char)\n  if cat in (""Cc"", ""Cf""):\n    return True\n  return False\n\ndef _is_punctuation(char):\n  """"""Checks whether `chars` is a punctuation character.""""""\n  cp = ord(char)\n  # We treat all non-letter/number ASCII as punctuation.\n  # Characters such as ""^"", ""$"", and ""`"" are not in the Unicode\n  # Punctuation class but we treat them as punctuation anyways, for\n  # consistency.\n  if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n      (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n    return True\n  cat = unicodedata.category(char)\n  if cat.startswith(""P""):\n    return True\n  return False\n'"
model/tokenization_bert.py,0,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tokenization classes.""""""\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport collections\nimport logging\nimport os\nimport unicodedata\nfrom io import open\n\nfrom .tokenization_utils import PreTrainedTokenizer\n\nlogger = logging.getLogger(__name__)\n\nVOCAB_FILES_NAMES = {\'vocab_file\': \'vocab.txt\'}\n\ndef load_vocab(vocab_file):\n    """"""Loads a vocabulary file into a dictionary.""""""\n    vocab = collections.OrderedDict()\n    with open(vocab_file, ""r"", encoding=""utf-8"") as reader:\n        tokens = reader.readlines()\n    for index, token in enumerate(tokens):\n        token = token.rstrip(\'\\n\')\n        vocab[token] = index\n    return vocab\n\n\ndef whitespace_tokenize(text):\n    """"""Runs basic whitespace cleaning and splitting on a piece of text.""""""\n    text = text.strip()\n    if not text:\n        return []\n    tokens = text.split()\n    return tokens\n\n\nclass BertTokenizer(PreTrainedTokenizer):\n    r""""""\n    Constructs a BertTokenizer.\n    :class:`~transformers.BertTokenizer` runs end-to-end tokenization: punctuation splitting + wordpiece\n\n    Args:\n        vocab_file: Path to a one-wordpiece-per-line vocabulary file\n        do_lower_case: Whether to lower case the input. Only has an effect when do_wordpiece_only=False\n        do_basic_tokenize: Whether to do basic tokenization before wordpiece.\n        max_len: An artificial maximum length to truncate tokenized sequences to; Effective maximum length is always the\n            minimum of this value (if specified) and the underlying BERT model\'s sequence length.\n        never_split: List of tokens which will never be split during tokenization. Only has an effect when\n            do_wordpiece_only=False\n    """"""\n\n    vocab_files_names = VOCAB_FILES_NAMES\n\n    def __init__(self, vocab_file, do_lower_case=True, do_basic_tokenize=True, never_split=None,\n                 unk_token=""[UNK]"", sep_token=""[SEP]"", pad_token=""[PAD]"", cls_token=""[CLS]"",\n                 mask_token=""[MASK]"", tokenize_chinese_chars=True, **kwargs):\n        """"""Constructs a BertTokenizer.\n\n        Args:\n            **vocab_file**: Path to a one-wordpiece-per-line vocabulary file\n            **do_lower_case**: (`optional`) boolean (default True)\n                Whether to lower case the input\n                Only has an effect when do_basic_tokenize=True\n            **do_basic_tokenize**: (`optional`) boolean (default True)\n                Whether to do basic tokenization before wordpiece.\n            **never_split**: (`optional`) list of string\n                List of tokens which will never be split during tokenization.\n                Only has an effect when do_basic_tokenize=True\n            **tokenize_chinese_chars**: (`optional`) boolean (default True)\n                Whether to tokenize Chinese characters.\n                This should likely be deactivated for Japanese:\n                see: https://github.com/huggingface/pytorch-pretrained-BERT/issues/328\n        """"""\n        super(BertTokenizer, self).__init__(unk_token=unk_token, sep_token=sep_token,\n                                            pad_token=pad_token, cls_token=cls_token,\n                                            mask_token=mask_token, **kwargs)\n        self.max_len_single_sentence = self.max_len - 2  # take into account special tokens\n        self.max_len_sentences_pair = self.max_len - 3  # take into account special tokens\n\n        if not os.path.isfile(vocab_file):\n            raise ValueError(\n                ""Can\'t find a vocabulary file at path \'{}\'. To load the vocabulary from a Google pretrained ""\n                ""model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`"".format(vocab_file))\n        self.vocab = load_vocab(vocab_file)\n        self.ids_to_tokens = collections.OrderedDict(\n            [(ids, tok) for tok, ids in self.vocab.items()])\n        self.do_basic_tokenize = do_basic_tokenize\n        if do_basic_tokenize:\n            self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case,\n                                                  never_split=never_split,\n                                                  tokenize_chinese_chars=tokenize_chinese_chars)\n        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab, unk_token=self.unk_token)\n\n    @property\n    def vocab_size(self):\n        return len(self.vocab)\n\n    def _tokenize(self, text):\n        split_tokens = []\n        if self.do_basic_tokenize:\n            for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n                for sub_token in self.wordpiece_tokenizer.tokenize(token):\n                    split_tokens.append(sub_token)\n        else:\n            split_tokens = self.wordpiece_tokenizer.tokenize(text)\n        return split_tokens\n\n    def _convert_token_to_id(self, token):\n        """""" Converts a token (str/unicode) in an id using the vocab. """"""\n        return self.vocab.get(token, self.vocab.get(self.unk_token))\n\n    def _convert_id_to_token(self, index):\n        """"""Converts an index (integer) in a token (string/unicode) using the vocab.""""""\n        return self.ids_to_tokens.get(index, self.unk_token)\n\n    def convert_tokens_to_string(self, tokens):\n        """""" Converts a sequence of tokens (string) in a single string. """"""\n        out_string = \' \'.join(tokens).replace(\' ##\', \'\').strip()\n        return out_string\n\n    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n        """"""\n        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n        by concatenating and adding special tokens.\n        A BERT sequence has the following format:\n            single sequence: [CLS] X [SEP]\n            pair of sequences: [CLS] A [SEP] B [SEP]\n        """"""\n        if token_ids_1 is None:\n            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n        cls = [self.cls_token_id]\n        sep = [self.sep_token_id]\n        return cls + token_ids_0 + sep + token_ids_1 + sep\n\n    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n        """"""\n        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n\n        Args:\n            token_ids_0: list of ids (must not contain special tokens)\n            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n                for sequence pairs\n            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n                special tokens for the model\n\n        Returns:\n            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n        """"""\n\n        if already_has_special_tokens:\n            if token_ids_1 is not None:\n                raise ValueError(""You should not supply a second sequence if the provided sequence of ""\n                                 ""ids is already formated with special tokens for the model."")\n            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n\n        if token_ids_1 is not None:\n            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n        return [1] + ([0] * len(token_ids_0)) + [1]\n\n    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n        """"""\n        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n        A BERT sequence pair mask has the following format:\n        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n        | first sequence    | second sequence\n\n        if token_ids_1 is None, only returns the first portion of the mask (0\'s).\n        """"""\n        sep = [self.sep_token_id]\n        cls = [self.cls_token_id]\n        if token_ids_1 is None:\n            return len(cls + token_ids_0 + sep) * [0]\n        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n\n    def save_vocabulary(self, vocab_path):\n        """"""Save the tokenizer vocabulary to a directory or file.""""""\n        index = 0\n        if os.path.isdir(vocab_path):\n            vocab_file = os.path.join(vocab_path, VOCAB_FILES_NAMES[\'vocab_file\'])\n        else:\n            vocab_file = vocab_path\n        with open(vocab_file, ""w"", encoding=""utf-8"") as writer:\n            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):\n                if index != token_index:\n                    logger.warning(""Saving vocabulary to {}: vocabulary indices are not consecutive.""\n                                   "" Please check that the vocabulary is not corrupted!"".format(vocab_file))\n                    index = token_index\n                writer.write(token + u\'\\n\')\n                index += 1\n        return (vocab_file,)\n\n\nclass BasicTokenizer(object):\n    """"""Runs basic tokenization (punctuation splitting, lower casing, etc.).""""""\n\n    def __init__(self, do_lower_case=True, never_split=None, tokenize_chinese_chars=True):\n        """""" Constructs a BasicTokenizer.\n\n        Args:\n            **do_lower_case**: Whether to lower case the input.\n            **never_split**: (`optional`) list of str\n                Kept for backward compatibility purposes.\n                Now implemented directly at the base class level (see :func:`PreTrainedTokenizer.tokenize`)\n                List of token not to split.\n            **tokenize_chinese_chars**: (`optional`) boolean (default True)\n                Whether to tokenize Chinese characters.\n                This should likely be deactivated for Japanese:\n                see: https://github.com/huggingface/pytorch-pretrained-BERT/issues/328\n        """"""\n        if never_split is None:\n            never_split = []\n        self.do_lower_case = do_lower_case\n        self.never_split = never_split\n        self.tokenize_chinese_chars = tokenize_chinese_chars\n\n    def tokenize(self, text, never_split=None):\n        """""" Basic Tokenization of a piece of text.\n            Split on ""white spaces"" only, for sub-word tokenization, see WordPieceTokenizer.\n\n        Args:\n            **never_split**: (`optional`) list of str\n                Kept for backward compatibility purposes.\n                Now implemented directly at the base class level (see :func:`PreTrainedTokenizer.tokenize`)\n                List of token not to split.\n        """"""\n        never_split = self.never_split + (never_split if never_split is not None else [])\n        text = self._clean_text(text)\n        # This was added on November 1st, 2018 for the multilingual and Chinese\n        # models. This is also applied to the English models now, but it doesn\'t\n        # matter since the English models were not trained on any Chinese data\n        # and generally don\'t have any Chinese data in them (there are Chinese\n        # characters in the vocabulary because Wikipedia does have some Chinese\n        # words in the English Wikipedia.).\n        if self.tokenize_chinese_chars:\n            text = self._tokenize_chinese_chars(text)\n        orig_tokens = whitespace_tokenize(text)\n        split_tokens = []\n        for token in orig_tokens:\n            if self.do_lower_case and token not in never_split:\n                token = token.lower()\n                token = self._run_strip_accents(token)\n            split_tokens.extend(self._run_split_on_punc(token))\n\n        output_tokens = whitespace_tokenize("" "".join(split_tokens))\n        return output_tokens\n\n    def _run_strip_accents(self, text):\n        """"""Strips accents from a piece of text.""""""\n        text = unicodedata.normalize(""NFD"", text)\n        output = []\n        for char in text:\n            cat = unicodedata.category(char)\n            if cat == ""Mn"":\n                continue\n            output.append(char)\n        return """".join(output)\n\n    def _run_split_on_punc(self, text, never_split=None):\n        """"""Splits punctuation on a piece of text.""""""\n        if never_split is not None and text in never_split:\n            return [text]\n        chars = list(text)\n        i = 0\n        start_new_word = True\n        output = []\n        while i < len(chars):\n            char = chars[i]\n            if _is_punctuation(char):\n                output.append([char])\n                start_new_word = True\n            else:\n                if start_new_word:\n                    output.append([])\n                start_new_word = False\n                output[-1].append(char)\n            i += 1\n\n        return ["""".join(x) for x in output]\n\n    def _tokenize_chinese_chars(self, text):\n        """"""Adds whitespace around any CJK character.""""""\n        output = []\n        for char in text:\n            cp = ord(char)\n            if self._is_chinese_char(cp):\n                output.append("" "")\n                output.append(char)\n                output.append("" "")\n            else:\n                output.append(char)\n        return """".join(output)\n\n    def _is_chinese_char(self, cp):\n        """"""Checks whether CP is the codepoint of a CJK character.""""""\n        # This defines a ""chinese character"" as anything in the CJK Unicode block:\n        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n        #\n        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n        # despite its name. The modern Korean Hangul alphabet is a different block,\n        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n        # space-separated words, so they are not treated specially and handled\n        # like the all of the other languages.\n        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n                (cp >= 0x3400 and cp <= 0x4DBF) or  #\n                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n                (cp >= 0xF900 and cp <= 0xFAFF) or  #\n                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n            return True\n\n        return False\n\n    def _clean_text(self, text):\n        """"""Performs invalid character removal and whitespace cleanup on text.""""""\n        output = []\n        for char in text:\n            cp = ord(char)\n            if cp == 0 or cp == 0xfffd or _is_control(char):\n                continue\n            if _is_whitespace(char):\n                output.append("" "")\n            else:\n                output.append(char)\n        return """".join(output)\n\n\nclass WordpieceTokenizer(object):\n    """"""Runs WordPiece tokenization.""""""\n\n    def __init__(self, vocab, unk_token, max_input_chars_per_word=100):\n        self.vocab = vocab\n        self.unk_token = unk_token\n        self.max_input_chars_per_word = max_input_chars_per_word\n\n    def tokenize(self, text):\n        """"""Tokenizes a piece of text into its word pieces.\n\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n\n        For example:\n          input = ""unaffable""\n          output = [""un"", ""##aff"", ""##able""]\n\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer`.\n\n        Returns:\n          A list of wordpiece tokens.\n        """"""\n\n        output_tokens = []\n        for token in whitespace_tokenize(text):\n            chars = list(token)\n            if len(chars) > self.max_input_chars_per_word:\n                output_tokens.append(self.unk_token)\n                continue\n\n            is_bad = False\n            start = 0\n            sub_tokens = []\n            while start < len(chars):\n                end = len(chars)\n                cur_substr = None\n                while start < end:\n                    substr = """".join(chars[start:end])\n                    if start > 0:\n                        substr = ""##"" + substr\n                    if substr in self.vocab:\n                        cur_substr = substr\n                        break\n                    end -= 1\n                if cur_substr is None:\n                    is_bad = True\n                    break\n                sub_tokens.append(cur_substr)\n                start = end\n\n            if is_bad:\n                output_tokens.append(self.unk_token)\n            else:\n                output_tokens.extend(sub_tokens)\n        return output_tokens\n\n\ndef _is_whitespace(char):\n    """"""Checks whether `chars` is a whitespace character.""""""\n    # \\t, \\n, and \\r are technically contorl characters but we treat them\n    # as whitespace since they are generally considered as such.\n    if char == "" "" or char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n        return True\n    cat = unicodedata.category(char)\n    if cat == ""Zs"":\n        return True\n    return False\n\n\ndef _is_control(char):\n    """"""Checks whether `chars` is a control character.""""""\n    # These are technically control characters but we count them as whitespace\n    # characters.\n    if char == ""\\t"" or char == ""\\n"" or char == ""\\r"":\n        return False\n    cat = unicodedata.category(char)\n    if cat.startswith(""C""):\n        return True\n    return False\n\n\ndef _is_punctuation(char):\n    """"""Checks whether `chars` is a punctuation character.""""""\n    cp = ord(char)\n    # We treat all non-letter/number ASCII as punctuation.\n    # Characters such as ""^"", ""$"", and ""`"" are not in the Unicode\n    # Punctuation class but we treat them as punctuation anyways, for\n    # consistency.\n    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n        return True\n    cat = unicodedata.category(char)\n    if cat.startswith(""P""):\n        return True\n    return False\n'"
model/tokenization_utils.py,5,"b'# coding=utf-8\n# Copyright 2018 The Open AI Team Authors and The HuggingFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Tokenization classes for OpenAI GPT.""""""\nfrom __future__ import (absolute_import, division, print_function,\n                        unicode_literals)\n\nimport logging\nimport os\nimport json\nimport six\nimport copy\nfrom io import open\n\nfrom .file_utils import cached_path\n\nimport torch\n\nlogger = logging.getLogger(__name__)\n\nSPECIAL_TOKENS_MAP_FILE = \'special_tokens_map.json\'\nADDED_TOKENS_FILE = \'added_tokens.json\'\nTOKENIZER_CONFIG_FILE = \'tokenizer_config.json\'\n\nclass PreTrainedTokenizer(object):\n    """""" Base class for all tokenizers.\n    Handle all the shared methods for tokenization and special tokens as well as methods dowloading/caching/loading pretrained tokenizers as well as adding tokens to the vocabulary.\n\n    This class also contain the added tokens in a unified way on top of all tokenizers so we don\'t have to handle the specific vocabulary augmentation methods of the various underlying dictionary structures (BPE, sentencepiece...).\n\n    Class attributes (overridden by derived classes):\n\n        - ``vocab_files_names``: a python ``dict`` with, as keys, the ``__init__`` keyword name of each vocabulary file required by the model, and as associated values, the filename for saving the associated file (string).\n        - ``pretrained_vocab_files_map``: a python ``dict of dict`` the high-level keys being the ``__init__`` keyword name of each vocabulary file required by the model, the low-level being the `short-cut-names` (string) of the pretrained models with, as associated values, the `url` (string) to the associated pretrained vocabulary file.\n        - ``max_model_input_sizes``: a python ``dict`` with, as keys, the `short-cut-names` (string) of the pretrained models, and as associated values, the maximum length of the sequence inputs of this model, or None if the model has no maximum input size.\n        - ``pretrained_init_configuration``: a python ``dict`` with, as keys, the `short-cut-names` (string) of the pretrained models, and as associated values, a dictionnary of specific arguments to pass to the ``__init__``method of the tokenizer class for this pretrained model when loading the tokenizer with the ``from_pretrained()`` method.\n\n    Parameters:\n\n        - ``bos_token``: (`Optional`) string: a beginning of sentence token. Will be associated to ``self.bos_token`` and ``self.bos_token_id``\n\n        - ``eos_token``: (`Optional`) string: an end of sentence token. Will be associated to ``self.eos_token`` and ``self.eos_token_id``\n\n        - ``unk_token``: (`Optional`) string: an unknown token. Will be associated to ``self.unk_token`` and ``self.unk_token_id``\n\n        - ``sep_token``: (`Optional`) string: a separation token (e.g. to separate context and query in an input sequence). Will be associated to ``self.sep_token`` and ``self.sep_token_id``\n\n        - ``pad_token``: (`Optional`) string: a padding token. Will be associated to ``self.pad_token`` and ``self.pad_token_id``\n\n        - ``cls_token``: (`Optional`) string: a classification token (e.g. to extract a summary of an input sequence leveraging self-attention along the full depth of the model). Will be associated to ``self.cls_token`` and ``self.cls_token_id``\n\n        - ``mask_token``: (`Optional`) string: a masking token (e.g. when training a model with masked-language modeling). Will be associated to ``self.mask_token`` and ``self.mask_token_id``\n\n        - ``additional_special_tokens``: (`Optional`) list: a list of additional special tokens. Adding all special tokens here ensure they won\'t be split by the tokenization process. Will be associated to ``self.additional_special_tokens`` and ``self.additional_special_tokens_ids``\n    """"""\n    vocab_files_names = {}\n    pretrained_vocab_files_map = {}\n    pretrained_init_configuration = {}\n    max_model_input_sizes = {}\n\n    SPECIAL_TOKENS_ATTRIBUTES = [""bos_token"", ""eos_token"", ""unk_token"", ""sep_token"",\n                                 ""pad_token"", ""cls_token"", ""mask_token"",\n                                 ""additional_special_tokens""]\n\n    @property\n    def bos_token(self):\n        """""" Beginning of sentence token (string). Log an error if used while not having been set. """"""\n        if self._bos_token is None:\n            logger.error(""Using bos_token, but it is not set yet."")\n        return self._bos_token\n\n    @property\n    def eos_token(self):\n        """""" End of sentence token (string). Log an error if used while not having been set. """"""\n        if self._eos_token is None:\n            logger.error(""Using eos_token, but it is not set yet."")\n        return self._eos_token\n\n    @property\n    def unk_token(self):\n        """""" Unknown token (string). Log an error if used while not having been set. """"""\n        if self._unk_token is None:\n            logger.error(""Using unk_token, but it is not set yet."")\n        return self._unk_token\n\n    @property\n    def sep_token(self):\n        """""" Separation token (string). E.g. separate context and query in an input sequence. Log an error if used while not having been set. """"""\n        if self._sep_token is None:\n            logger.error(""Using sep_token, but it is not set yet."")\n        return self._sep_token\n\n    @property\n    def pad_token(self):\n        """""" Padding token (string). Log an error if used while not having been set. """"""\n        if self._pad_token is None:\n            logger.error(""Using pad_token, but it is not set yet."")\n        return self._pad_token\n\n    @property\n    def cls_token(self):\n        """""" Classification token (string). E.g. to extract a summary of an input sequence leveraging self-attention along the full depth of the model. Log an error if used while not having been set. """"""\n        if self._cls_token is None:\n            logger.error(""Using cls_token, but it is not set yet."")\n        return self._cls_token\n\n    @property\n    def mask_token(self):\n        """""" Mask token (string). E.g. when training a model with masked-language modeling. Log an error if used while not having been set. """"""\n        if self._mask_token is None:\n            logger.error(""Using mask_token, but it is not set yet."")\n        return self._mask_token\n\n    @property\n    def additional_special_tokens(self):\n        """""" All the additional special tokens you may want to use (list of strings). Log an error if used while not having been set. """"""\n        if self._additional_special_tokens is None:\n            logger.error(""Using additional_special_tokens, but it is not set yet."")\n        return self._additional_special_tokens\n\n    @bos_token.setter\n    def bos_token(self, value):\n        self._bos_token = value\n\n    @eos_token.setter\n    def eos_token(self, value):\n        self._eos_token = value\n\n    @unk_token.setter\n    def unk_token(self, value):\n        self._unk_token = value\n\n    @sep_token.setter\n    def sep_token(self, value):\n        self._sep_token = value\n\n    @pad_token.setter\n    def pad_token(self, value):\n        self._pad_token = value\n\n    @cls_token.setter\n    def cls_token(self, value):\n        self._cls_token = value\n\n    @mask_token.setter\n    def mask_token(self, value):\n        self._mask_token = value\n\n    @additional_special_tokens.setter\n    def additional_special_tokens(self, value):\n        self._additional_special_tokens = value\n\n    @property\n    def bos_token_id(self):\n        """""" Id of the beginning of sentence token in the vocabulary. Log an error if used while not having been set. """"""\n        return self.convert_tokens_to_ids(self.bos_token)\n\n    @property\n    def eos_token_id(self):\n        """""" Id of the end of sentence token in the vocabulary. Log an error if used while not having been set. """"""\n        return self.convert_tokens_to_ids(self.eos_token)\n\n    @property\n    def unk_token_id(self):\n        """""" Id of the unknown token in the vocabulary. Log an error if used while not having been set. """"""\n        return self.convert_tokens_to_ids(self.unk_token)\n\n    @property\n    def sep_token_id(self):\n        """""" Id of the separation token in the vocabulary. E.g. separate context and query in an input sequence. Log an error if used while not having been set. """"""\n        return self.convert_tokens_to_ids(self.sep_token)\n\n    @property\n    def pad_token_id(self):\n        """""" Id of the padding token in the vocabulary. Log an error if used while not having been set. """"""\n        return self.convert_tokens_to_ids(self.pad_token)\n\n    @property\n    def cls_token_id(self):\n        """""" Id of the classification token in the vocabulary. E.g. to extract a summary of an input sequence leveraging self-attention along the full depth of the model. Log an error if used while not having been set. """"""\n        return self.convert_tokens_to_ids(self.cls_token)\n\n    @property\n    def mask_token_id(self):\n        """""" Id of the mask token in the vocabulary. E.g. when training a model with masked-language modeling. Log an error if used while not having been set. """"""\n        return self.convert_tokens_to_ids(self.mask_token)\n\n    @property\n    def additional_special_tokens_ids(self):\n        """""" Ids of all the additional special tokens in the vocabulary (list of integers). Log an error if used while not having been set. """"""\n        return self.convert_tokens_to_ids(self.additional_special_tokens)\n\n    def __init__(self, max_len=None, **kwargs):\n        self._bos_token = None\n        self._eos_token = None\n        self._unk_token = None\n        self._sep_token = None\n        self._pad_token = None\n        self._cls_token = None\n        self._mask_token = None\n        self._additional_special_tokens = []\n\n        self.max_len = max_len if max_len is not None else int(1e12)\n\n        # Added tokens\n        self.added_tokens_encoder = {}\n        self.added_tokens_decoder = {}\n\n        # inputs and kwargs for saving and re-loading (see ``from_pretrained`` and ``save_pretrained``)\n        self.init_inputs = ()\n        self.init_kwargs = {}\n\n        for key, value in kwargs.items():\n            if key in self.SPECIAL_TOKENS_ATTRIBUTES:\n                if key == \'additional_special_tokens\':\n                    assert isinstance(value, (list, tuple)) and all(isinstance(t, str) or (six.PY2 and isinstance(t, unicode)) for t in value)\n                else:\n                    assert isinstance(value, str) or (six.PY2 and isinstance(value, unicode))\n                setattr(self, key, value)\n\n\n    @classmethod\n    def from_pretrained(cls, *inputs, **kwargs):\n        r""""""\n        Instantiate a :class:`~transformers.PreTrainedTokenizer` (or a derived class) from a predefined tokenizer.\n\n        Args:\n            pretrained_model_name_or_path: either:\n\n                - a string with the `shortcut name` of a predefined tokenizer to load from cache or download, e.g.: ``bert-base-uncased``.\n                - a path to a `directory` containing vocabulary files required by the tokenizer, for instance saved using the :func:`~transformers.PreTrainedTokenizer.save_pretrained` method, e.g.: ``./my_model_directory/``.\n                - (not applicable to all derived classes) a path or url to a single saved vocabulary file if and only if the tokenizer only requires a single vocabulary file (e.g. Bert, XLNet), e.g.: ``./my_model_directory/vocab.txt``.\n\n            cache_dir: (`optional`) string:\n                Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the standard cache should not be used.\n\n            force_download: (`optional`) boolean, default False:\n                Force to (re-)download the vocabulary files and override the cached versions if they exists.\n\n            proxies: (`optional`) dict, default None:\n                A dictionary of proxy servers to use by protocol or endpoint, e.g.: {\'http\': \'foo.bar:3128\', \'http://hostname\': \'foo.bar:4012\'}.\n                The proxies are used on each request.\n\n            inputs: (`optional`) positional arguments: will be passed to the Tokenizer ``__init__`` method.\n\n            kwargs: (`optional`) keyword arguments: will be passed to the Tokenizer ``__init__`` method. Can be used to set special tokens like ``bos_token``, ``eos_token``, ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``, ``additional_special_tokens``. See parameters in the doc string of :class:`~transformers.PreTrainedTokenizer` for details.\n\n        Examples::\n\n            # We can\'t instantiate directly the base class `PreTrainedTokenizer` so let\'s show our examples on a derived class: BertTokenizer\n\n            # Download vocabulary from S3 and cache.\n            tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n\n            # If vocabulary files are in a directory (e.g. tokenizer was saved using `save_pretrained(\'./test/saved_model/\')`)\n            tokenizer = BertTokenizer.from_pretrained(\'./test/saved_model/\')\n\n            # If the tokenizer uses a single vocabulary file, you can point directly to this file\n            tokenizer = BertTokenizer.from_pretrained(\'./test/saved_model/my_vocab.txt\')\n\n            # You can link tokens to special vocabulary when instantiating\n            tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\', unk_token=\'<unk>\')\n            # You should be sure \'<unk>\' is in the vocabulary when doing that.\n            # Otherwise use tokenizer.add_special_tokens({\'unk_token\': \'<unk>\'}) instead)\n            assert tokenizer.unk_token == \'<unk>\'\n\n        """"""\n        return cls._from_pretrained(*inputs, **kwargs)\n\n\n    @classmethod\n    def _from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs):\n        cache_dir = kwargs.pop(\'cache_dir\', None)\n        force_download = kwargs.pop(\'force_download\', False)\n        proxies = kwargs.pop(\'proxies\', None)\n\n        s3_models = list(cls.max_model_input_sizes.keys())\n        vocab_files = {}\n        init_configuration = {}\n        if pretrained_model_name_or_path in s3_models:\n            # Get the vocabulary from AWS S3 bucket\n            for file_id, map_list in cls.pretrained_vocab_files_map.items():\n                vocab_files[file_id] = map_list[pretrained_model_name_or_path]\n            if cls.pretrained_init_configuration and pretrained_model_name_or_path in cls.pretrained_init_configuration:\n                init_configuration = cls.pretrained_init_configuration[pretrained_model_name_or_path]\n        else:\n            # Get the vocabulary from local files\n            logger.info(\n                ""Model name \'{}\' not found in model shortcut name list ({}). ""\n                ""Assuming \'{}\' is a path or url to a directory containing tokenizer files."".format(\n                    pretrained_model_name_or_path, \', \'.join(s3_models),\n                    pretrained_model_name_or_path))\n\n            # Look for the tokenizer main vocabulary files\n            for file_id, file_name in cls.vocab_files_names.items():\n                if os.path.isdir(pretrained_model_name_or_path):\n                    # If a directory is provided we look for the standard filenames\n                    full_file_name = os.path.join(pretrained_model_name_or_path, file_name)\n                else:\n                    # If a path to a file is provided we use it (will only work for non-BPE tokenizer using a single vocabulary file)\n                    full_file_name = pretrained_model_name_or_path\n                if not os.path.exists(full_file_name):\n                    logger.info(""Didn\'t find file {}. We won\'t load it."".format(full_file_name))\n                    full_file_name = None\n                vocab_files[file_id] = full_file_name\n\n            # Look for the additional tokens files\n            additional_files_names = {\'added_tokens_file\': ADDED_TOKENS_FILE,\n                                      \'special_tokens_map_file\': SPECIAL_TOKENS_MAP_FILE,\n                                      \'tokenizer_config_file\': TOKENIZER_CONFIG_FILE,\n                                      }\n\n            # If a path to a file was provided, get the parent directory\n            saved_directory = pretrained_model_name_or_path\n            if os.path.exists(saved_directory) and not os.path.isdir(saved_directory):\n                saved_directory = os.path.dirname(saved_directory)\n\n            for file_id, file_name in additional_files_names.items():\n                full_file_name = os.path.join(saved_directory, file_name)\n                if not os.path.exists(full_file_name):\n                    logger.info(""Didn\'t find file {}. We won\'t load it."".format(full_file_name))\n                    full_file_name = None\n                vocab_files[file_id] = full_file_name\n\n            if all(full_file_name is None for full_file_name in vocab_files.values()):\n                raise EnvironmentError(\n                    ""Model name \'{}\' was not found in tokenizers model name list ({}). ""\n                    ""We assumed \'{}\' was a path or url to a directory containing vocabulary files ""\n                    ""named {} but couldn\'t find such vocabulary files at this path or url."".format(\n                        pretrained_model_name_or_path, \', \'.join(s3_models),\n                        pretrained_model_name_or_path, \n                        list(cls.vocab_files_names.values())))\n\n        # Get files from url, cache, or disk depending on the case\n        try:\n            resolved_vocab_files = {}\n            for file_id, file_path in vocab_files.items():\n                if file_path is None:\n                    resolved_vocab_files[file_id] = None\n                else:\n                    resolved_vocab_files[file_id] = cached_path(file_path, cache_dir=cache_dir, force_download=force_download, proxies=proxies)\n        except EnvironmentError:\n            if pretrained_model_name_or_path in s3_models:\n                msg = ""Couldn\'t reach server at \'{}\' to download vocabulary files.""\n            else:\n                msg = ""Model name \'{}\' was not found in tokenizers model name list ({}). "" \\\n                    ""We assumed \'{}\' was a path or url to a directory containing vocabulary files "" \\\n                    ""named {}, but couldn\'t find such vocabulary files at this path or url."".format(\n                        pretrained_model_name_or_path, \', \'.join(s3_models),\n                        pretrained_model_name_or_path,\n                        list(cls.vocab_files_names.values()))\n\n            raise EnvironmentError(msg)\n\n        for file_id, file_path in vocab_files.items():\n            if file_path == resolved_vocab_files[file_id]:\n                logger.info(""loading file {}"".format(file_path))\n            else:\n                logger.info(""loading file {} from cache at {}"".format(\n                    file_path, resolved_vocab_files[file_id]))\n\n        # Prepare tokenizer initialization kwargs\n        # Did we saved some inputs and kwargs to reload ?\n        tokenizer_config_file = resolved_vocab_files.pop(\'tokenizer_config_file\', None)\n        if tokenizer_config_file is not None:\n            init_kwargs = json.load(open(tokenizer_config_file, encoding=""utf-8""))\n            saved_init_inputs = init_kwargs.pop(\'init_inputs\', ())\n            if not init_inputs:\n                init_inputs = saved_init_inputs\n        else:\n            init_kwargs = init_configuration\n\n        # Update with newly provided kwargs\n        init_kwargs.update(kwargs)\n\n        # Set max length if needed\n        if pretrained_model_name_or_path in cls.max_model_input_sizes:\n            # if we\'re using a pretrained model, ensure the tokenizer\n            # wont index sequences longer than the number of positional embeddings\n            max_len = cls.max_model_input_sizes[pretrained_model_name_or_path]\n            if max_len is not None and isinstance(max_len, (int, float)):\n                init_kwargs[\'max_len\'] = min(init_kwargs.get(\'max_len\', int(1e12)), max_len)\n\n        # Merge resolved_vocab_files arguments in init_kwargs.\n        added_tokens_file = resolved_vocab_files.pop(\'added_tokens_file\', None)\n        special_tokens_map_file = resolved_vocab_files.pop(\'special_tokens_map_file\', None)\n        for args_name, file_path in resolved_vocab_files.items():\n            if args_name not in init_kwargs:\n                init_kwargs[args_name] = file_path\n        if special_tokens_map_file is not None:\n            special_tokens_map = json.load(open(special_tokens_map_file, encoding=""utf-8""))\n            for key, value in special_tokens_map.items():\n                if key not in init_kwargs:\n                    init_kwargs[key] = value\n\n        # Instantiate tokenizer.\n        tokenizer = cls(*init_inputs, **init_kwargs)\n\n        # Save inputs and kwargs for saving and re-loading with ``save_pretrained``\n        tokenizer.init_inputs = init_inputs\n        tokenizer.init_kwargs = init_kwargs\n\n        # Add supplementary tokens.\n        if added_tokens_file is not None:\n            added_tok_encoder = json.load(open(added_tokens_file, encoding=""utf-8""))\n            added_tok_decoder = {v:k for k, v in added_tok_encoder.items()}\n            tokenizer.added_tokens_encoder.update(added_tok_encoder)\n            tokenizer.added_tokens_decoder.update(added_tok_decoder)\n\n        return tokenizer\n\n\n    def save_pretrained(self, save_directory):\n        """""" Save the tokenizer vocabulary files together with:\n                - added tokens,\n                - special-tokens-to-class-attributes-mapping,\n                - tokenizer instantiation positional and keywords inputs (e.g. do_lower_case for Bert).\n\n            This won\'t save modifications other than (added tokens and special token mapping) you may have\n            applied to the tokenizer after the instantiation (e.g. modifying tokenizer.do_lower_case after creation).\n\n            This method make sure the full tokenizer can then be re-loaded using the :func:`~transformers.PreTrainedTokenizer.from_pretrained` class method.\n        """"""\n        if not os.path.isdir(save_directory):\n            logger.error(""Saving directory ({}) should be a directory"".format(save_directory))\n            return\n\n        special_tokens_map_file = os.path.join(save_directory, SPECIAL_TOKENS_MAP_FILE)\n        added_tokens_file = os.path.join(save_directory, ADDED_TOKENS_FILE)\n        tokenizer_config_file = os.path.join(save_directory, TOKENIZER_CONFIG_FILE)\n\n        tokenizer_config = copy.deepcopy(self.init_kwargs)\n        tokenizer_config[\'init_inputs\'] = copy.deepcopy(self.init_inputs)\n        for file_id in self.vocab_files_names.keys():\n            tokenizer_config.pop(file_id, None)\n\n        with open(tokenizer_config_file, \'w\', encoding=\'utf-8\') as f:\n            f.write(json.dumps(tokenizer_config, ensure_ascii=False))\n\n        with open(special_tokens_map_file, \'w\', encoding=\'utf-8\') as f:\n            f.write(json.dumps(self.special_tokens_map, ensure_ascii=False))\n\n        with open(added_tokens_file, \'w\', encoding=\'utf-8\') as f:\n            if self.added_tokens_encoder:\n                out_str = json.dumps(self.added_tokens_encoder, ensure_ascii=False)\n            else:\n                out_str = u""{}""\n            f.write(out_str)\n\n        vocab_files = self.save_vocabulary(save_directory)\n\n        return vocab_files + (special_tokens_map_file, added_tokens_file)\n\n\n    def save_vocabulary(self, save_directory):\n        """""" Save the tokenizer vocabulary to a directory. This method does *NOT* save added tokens\n            and special token mappings.\n\n            Please use :func:`~transformers.PreTrainedTokenizer.save_pretrained` `()` to save the full Tokenizer state if you want to reload it using the :func:`~transformers.PreTrainedTokenizer.from_pretrained` class method.\n        """"""\n        raise NotImplementedError\n\n\n    def vocab_size(self):\n        """""" Size of the base vocabulary (without the added tokens) """"""\n        raise NotImplementedError\n\n\n    def __len__(self):\n        """""" Size of the full vocabulary with the added tokens """"""\n        return self.vocab_size + len(self.added_tokens_encoder)\n\n\n    def add_tokens(self, new_tokens):\n        """"""\n        Add a list of new tokens to the tokenizer class. If the new tokens are not in the\n        vocabulary, they are added to it with indices starting from length of the current vocabulary.\n\n        Args:\n            new_tokens: list of string. Each string is a token to add. Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer assign the index of the ``unk_token`` to them).\n\n        Returns:\n            Number of tokens added to the vocabulary.\n\n        Examples::\n\n            # Let\'s see how to increase the vocabulary of Bert model and tokenizer\n            tokenizer = BertTokenizer.from_pretrained(\'bert-base-uncased\')\n            model = BertModel.from_pretrained(\'bert-base-uncased\')\n\n            num_added_toks = tokenizer.add_tokens([\'new_tok1\', \'my_new-tok2\'])\n            print(\'We have added\', num_added_toks, \'tokens\')\n            model.resize_token_embeddings(len(tokenizer))  # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e. the length of the tokenizer.\n        """"""\n        if not new_tokens:\n            return 0\n\n        to_add_tokens = []\n        for token in new_tokens:\n            assert isinstance(token, str) or (six.PY2 and isinstance(token, unicode))\n            if token != self.unk_token and \\\n                    self.convert_tokens_to_ids(token) == self.convert_tokens_to_ids(self.unk_token) and \\\n                    token not in to_add_tokens:\n                to_add_tokens.append(token)\n                logger.info(""Adding %s to the vocabulary"", token)\n\n        added_tok_encoder = dict((tok, len(self) + i) for i, tok in enumerate(to_add_tokens))\n        added_tok_decoder = {v:k for k, v in added_tok_encoder.items()}\n        self.added_tokens_encoder.update(added_tok_encoder)\n        self.added_tokens_decoder.update(added_tok_decoder)\n\n        return len(to_add_tokens)\n\n    def num_added_tokens(self, pair=False):\n        """"""\n        Returns the number of added tokens when encoding a sequence with special tokens.\n\n        Note:\n            This encodes inputs and checks the number of added tokens, and is therefore not efficient. Do not put this\n            inside your training loop.\n\n        Args:\n            pair: Returns the number of added tokens in the case of a sequence pair if set to True, returns the\n                number of added tokens in the case of a single sequence if set to False.\n\n        Returns:\n            Number of tokens added to sequences\n        """"""\n        token_ids_0 = []\n        token_ids_1 = []\n        return len(self.build_inputs_with_special_tokens(token_ids_0, token_ids_1 if pair else None))\n\n    def add_special_tokens(self, special_tokens_dict):\n        """"""\n        Add a dictionary of special tokens (eos, pad, cls...) to the encoder and link them\n        to class attributes. If special tokens are NOT in the vocabulary, they are added\n        to it (indexed starting from the last index of the current vocabulary).\n\n        Using `add_special_tokens` will ensure your special tokens can be used in several ways:\n\n        - special tokens are carefully handled by the tokenizer (they are never split)\n        - you can easily refer to special tokens using tokenizer class attributes like `tokenizer.cls_token`. This makes it easy to develop model-agnostic training and fine-tuning scripts.\n\n        When possible, special tokens are already registered for provided pretrained models (ex: BertTokenizer cls_token is already registered to be \'[CLS]\' and XLM\'s one is also registered to be \'</s>\')\n\n        Args:\n            special_tokens_dict: dict of string. Keys should be in the list of predefined special attributes:\n                [``bos_token``, ``eos_token``, ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``,\n                ``additional_special_tokens``].\n\n                Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer assign the index of the ``unk_token`` to them).\n\n        Returns:\n            Number of tokens added to the vocabulary.\n\n        Examples::\n\n            # Let\'s see how to add a new classification token to GPT-2\n            tokenizer = GPT2Tokenizer.from_pretrained(\'gpt2\')\n            model = GPT2Model.from_pretrained(\'gpt2\')\n\n            special_tokens_dict = {\'cls_token\': \'<CLS>\'}\n\n            num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n            print(\'We have added\', num_added_toks, \'tokens\')\n            model.resize_token_embeddings(len(tokenizer))  # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e. the length of the tokenizer.\n\n            assert tokenizer.cls_token == \'<CLS>\'\n        """"""\n        if not special_tokens_dict:\n            return 0\n\n        added_tokens = 0\n        for key, value in special_tokens_dict.items():\n            assert key in self.SPECIAL_TOKENS_ATTRIBUTES\n            if key == \'additional_special_tokens\':\n                assert isinstance(value, (list, tuple)) and all(isinstance(t, str) or (six.PY2 and isinstance(t, unicode)) for t in value)\n                added_tokens += self.add_tokens(value)\n            else:\n                assert isinstance(value, str) or (six.PY2 and isinstance(value, unicode))\n                added_tokens += self.add_tokens([value])\n            logger.info(""Assigning %s to the %s key of the tokenizer"", value, key)\n            setattr(self, key, value)\n\n        return added_tokens\n\n    def tokenize(self, text, **kwargs):\n        """""" Converts a string in a sequence of tokens (string), using the tokenizer.\n            Split in words for word-based vocabulary or sub-words for sub-word-based\n            vocabularies (BPE/SentencePieces/WordPieces).\n\n            Take care of added tokens.\n        """"""\n        def split_on_token(tok, text):\n            result = []\n            split_text = text.split(tok)\n            for i, sub_text in enumerate(split_text):\n                sub_text = sub_text.strip()\n                if i == 0 and not sub_text:\n                    result += [tok]\n                elif i == len(split_text) - 1:\n                    if sub_text:\n                        result += [sub_text]\n                    else:\n                        pass\n                else:\n                    if sub_text:\n                        result += [sub_text]\n                    result += [tok]\n            return result\n\n        def split_on_tokens(tok_list, text):\n            if not text:\n                return []\n            if not tok_list:\n                return self._tokenize(text, **kwargs)\n\n            tokenized_text = []\n            text_list = [text]\n            for tok in tok_list:\n                tokenized_text = []\n                for sub_text in text_list:\n                    if sub_text not in self.added_tokens_encoder \\\n                            and sub_text not in self.all_special_tokens:\n                        tokenized_text += split_on_token(tok, sub_text)\n                    else:\n                        tokenized_text += [sub_text]\n                text_list = tokenized_text\n\n            return sum((self._tokenize(token, **kwargs) if token not \\\n                    in self.added_tokens_encoder and token not in self.all_special_tokens \\\n                    else [token] for token in tokenized_text), [])\n\n        added_tokens = list(self.added_tokens_encoder.keys()) + self.all_special_tokens\n        tokenized_text = split_on_tokens(added_tokens, text)\n        return tokenized_text\n\n    def _tokenize(self, text, **kwargs):\n        """""" Converts a string in a sequence of tokens (string), using the tokenizer.\n            Split in words for word-based vocabulary or sub-words for sub-word-based\n            vocabularies (BPE/SentencePieces/WordPieces).\n\n            Do NOT take care of added tokens.\n        """"""\n        raise NotImplementedError\n\n    def convert_tokens_to_ids(self, tokens):\n        """""" Converts a single token, or a sequence of tokens, (str/unicode) in a single integer id\n            (resp. a sequence of ids), using the vocabulary.\n        """"""\n        if tokens is None:\n            return None\n\n        if isinstance(tokens, str) or (six.PY2 and isinstance(tokens, unicode)):\n            return self._convert_token_to_id_with_added_voc(tokens)\n\n        ids = []\n        for token in tokens:\n            ids.append(self._convert_token_to_id_with_added_voc(token))\n        if len(ids) > self.max_len:\n            logger.warning(""Token indices sequence length is longer than the specified maximum sequence length ""\n                           ""for this model ({} > {}). Running this sequence through the model will result in ""\n                           ""indexing errors"".format(len(ids), self.max_len))\n        return ids\n\n    def _convert_token_to_id_with_added_voc(self, token):\n        if token is None:\n            return None\n\n        if token in self.added_tokens_encoder:\n            return self.added_tokens_encoder[token]\n        return self._convert_token_to_id(token)\n\n    def _convert_token_to_id(self, token):\n        raise NotImplementedError\n\n    def encode(self,\n                text,\n                text_pair=None,\n                add_special_tokens=False,\n                max_length=None,\n                stride=0,\n                truncation_strategy=\'longest_first\',\n                return_tensors=None,\n                **kwargs):\n        """"""\n        Converts a string in a sequence of ids (integer), using the tokenizer and vocabulary.\n\n        Same as doing ``self.convert_tokens_to_ids(self.tokenize(text))``.\n\n        Args:\n            text: The first sequence to be encoded. This can be a string, a list of strings (tokenized string using\n                the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n                method)\n            text_pair: Optional second sequence to be encoded. This can be a string, a list of strings (tokenized\n                string using the `tokenize` method) or a list of integers (tokenized string ids using the\n                `convert_tokens_to_ids` method)\n            add_special_tokens: if set to ``True``, the sequences will be encoded with the special tokens relative\n                to their model.\n            max_length: if set to a number, will limit the total sequence returned so that it has a maximum length.\n                If there are overflowing tokens, those will be added to the returned dictionary\n            stride: if set to a number along with max_length, the overflowing tokens returned will contain some tokens\n                from the main sequence returned. The value of this argument defines the number of additional tokens.\n            truncation_strategy: string selected in the following options:\n                - \'longest_first\' (default) Iteratively reduce the inputs sequence until the input is under max_length\n                    starting from the longest one at each token (when there is a pair of input sequences)\n                - \'only_first\': Only truncate the first sequence\n                - \'only_second\': Only truncate the second sequence\n                - \'do_not_truncate\': Does not truncate (raise an error if the input sequence is longer than max_length)\n            return_tensors: (optional) can be set to \'tf\' or \'pt\' to return respectively TensorFlow tf.constant\n                or PyTorch torch.Tensor instead of a list of python integers.\n            **kwargs: passed to the `self.tokenize()` method\n        """"""\n        encoded_inputs = self.encode_plus(text,\n                                          text_pair=text_pair,\n                                          max_length=max_length,\n                                          add_special_tokens=add_special_tokens,\n                                          stride=stride,\n                                          truncation_strategy=truncation_strategy,\n                                          return_tensors=return_tensors,\n                                          **kwargs)\n\n        return encoded_inputs[""input_ids""]\n\n    def encode_plus(self,\n                    text,\n                    text_pair=None,\n                    add_special_tokens=False,\n                    max_length=None,\n                    stride=0,\n                    truncation_strategy=\'longest_first\',\n                    return_tensors=None,\n                    **kwargs):\n        """"""\n        Returns a dictionary containing the encoded sequence or sequence pair and additional informations:\n        the mask for sequence classification and the overflowing elements if a ``max_length`` is specified.\n\n        Args:\n            text: The first sequence to be encoded. This can be a string, a list of strings (tokenized string using\n                the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n                method)\n            text_pair: Optional second sequence to be encoded. This can be a string, a list of strings (tokenized\n                string using the `tokenize` method) or a list of integers (tokenized string ids using the\n                `convert_tokens_to_ids` method)\n            add_special_tokens: if set to ``True``, the sequences will be encoded with the special tokens relative\n                to their model.\n            max_length: if set to a number, will limit the total sequence returned so that it has a maximum length.\n                If there are overflowing tokens, those will be added to the returned dictionary\n            stride: if set to a number along with max_length, the overflowing tokens returned will contain some tokens\n                from the main sequence returned. The value of this argument defines the number of additional tokens.\n            truncation_strategy: string selected in the following options:\n                - \'longest_first\' (default) Iteratively reduce the inputs sequence until the input is under max_length\n                    starting from the longest one at each token (when there is a pair of input sequences)\n                - \'only_first\': Only truncate the first sequence\n                - \'only_second\': Only truncate the second sequence\n                - \'do_not_truncate\': Does not truncate (raise an error if the input sequence is longer than max_length)\n            return_tensors: (optional) can be set to \'tf\' or \'pt\' to return respectively TensorFlow tf.constant\n                or PyTorch torch.Tensor instead of a list of python integers.\n            **kwargs: passed to the `self.tokenize()` method\n        """"""\n\n        def get_input_ids(text):\n            if isinstance(text, six.string_types):\n                return self.convert_tokens_to_ids(self.tokenize(text, **kwargs))\n            elif isinstance(text, (list, tuple)) and len(text) > 0 and isinstance(text[0], six.string_types):\n                return self.convert_tokens_to_ids(text)\n            elif isinstance(text, (list, tuple)) and len(text) > 0 and isinstance(text[0], int):\n                return text\n            else:\n                raise ValueError(""Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."")\n\n        first_ids = get_input_ids(text)\n        second_ids = get_input_ids(text_pair) if text_pair is not None else None\n\n        return self.prepare_for_model(first_ids,\n                                      pair_ids=second_ids,\n                                      max_length=max_length,\n                                      add_special_tokens=add_special_tokens,\n                                      stride=stride,\n                                      truncation_strategy=truncation_strategy,\n                                      return_tensors=return_tensors)\n\n    def prepare_for_model(self, ids, pair_ids=None, max_length=None, add_special_tokens=False, stride=0,\n                          truncation_strategy=\'longest_first\', return_tensors=None):\n        """"""\n        Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model.\n        It adds special tokens, truncates\n        sequences if overflowing while taking into account the special tokens and manages a window stride for\n        overflowing tokens\n\n        Args:\n            ids: list of tokenized input ids. Can be obtained from a string by chaining the\n                `tokenize` and `convert_tokens_to_ids` methods.\n            pair_ids: Optional second list of input ids. Can be obtained from a string by chaining the\n                `tokenize` and `convert_tokens_to_ids` methods.\n            max_length: maximum length of the returned list. Will truncate by taking into account the special tokens.\n            add_special_tokens: if set to ``True``, the sequences will be encoded with the special tokens relative\n                to their model.\n            stride: window stride for overflowing tokens. Can be useful for edge effect removal when using sequential\n                list of inputs.\n            truncation_strategy: string selected in the following options:\n                - \'longest_first\' (default) Iteratively reduce the inputs sequence until the input is under max_length\n                    starting from the longest one at each token (when there is a pair of input sequences)\n                - \'only_first\': Only truncate the first sequence\n                - \'only_second\': Only truncate the second sequence\n                - \'do_not_truncate\': Does not truncate (raise an error if the input sequence is longer than max_length)\n            return_tensors: (optional) can be set to \'tf\' or \'pt\' to return respectively TensorFlow tf.constant\n                or PyTorch torch.Tensor instead of a list of python integers.\n\n        Return:\n            A Dictionary of shape::\n\n                {\n                    input_ids: list[int],\n                    overflowing_tokens: list[int] if a ``max_length`` is specified, else None\n                    special_tokens_mask: list[int] if ``add_special_tokens`` if set to ``True``\n                }\n\n            With the fields:\n                ``input_ids``: list of tokens to be fed to a model\n\n                ``overflowing_tokens``: list of overflowing tokens if a max length is specified.\n\n                ``special_tokens_mask``: if adding special tokens, this is a list of [0, 1], with 0 specifying special added\n                tokens and 1 specifying sequence tokens.\n        """"""\n        pair = bool(pair_ids is not None)\n        len_ids = len(ids)\n        len_pair_ids = len(pair_ids) if pair else 0\n\n        encoded_inputs = {}\n        total_len = len_ids + len_pair_ids + (self.num_added_tokens(pair=pair) if add_special_tokens else 0)\n        if max_length and total_len > max_length:\n            ids, pair_ids, overflowing_tokens = self.truncate_sequences(ids, pair_ids=pair_ids,\n                                                                        num_tokens_to_remove=total_len-max_length,\n                                                                        truncation_strategy=truncation_strategy,\n                                                                        stride=stride)\n            encoded_inputs[""overflowing_tokens""] = overflowing_tokens\n            encoded_inputs[""num_truncated_tokens""] = total_len - max_length\n\n        if add_special_tokens:\n            sequence = self.build_inputs_with_special_tokens(ids, pair_ids)\n            token_type_ids = self.create_token_type_ids_from_sequences(ids, pair_ids)\n            encoded_inputs[""special_tokens_mask""] = self.get_special_tokens_mask(ids, pair_ids)\n        else:\n            sequence = ids + pair_ids if pair else ids\n            token_type_ids = [0] * len(ids) + ([1] * len(pair_ids) if pair else [])\n\n        if return_tensors == \'tf\' and is_tf_available():\n            sequence = tf.constant([sequence])\n            token_type_ids = tf.constant([token_type_ids])\n        elif return_tensors == \'pt\' and is_torch_available():\n            sequence = torch.tensor([sequence])\n            token_type_ids = torch.tensor([token_type_ids])\n        elif return_tensors is not None:\n            logger.warning(""Unable to convert output to tensors format {}, PyTorch or TensorFlow is not available."".format(return_tensors))\n\n        encoded_inputs[""input_ids""] = sequence\n        encoded_inputs[""token_type_ids""] = token_type_ids\n\n        if max_length and len(encoded_inputs[""input_ids""]) > max_length:\n            encoded_inputs[""input_ids""] = encoded_inputs[""input_ids""][:max_length]\n            encoded_inputs[""token_type_ids""] = encoded_inputs[""token_type_ids""][:max_length]\n            encoded_inputs[""special_tokens_mask""] = encoded_inputs[""special_tokens_mask""][:max_length]\n\n        return encoded_inputs\n\n    def truncate_sequences(self, ids, pair_ids=None, num_tokens_to_remove=0, truncation_strategy=\'longest_first\', stride=0):\n        """"""Truncates a sequence pair in place to the maximum length.\n            truncation_strategy: string selected in the following options:\n                - \'longest_first\' (default) Iteratively reduce the inputs sequence until the input is under max_length\n                    starting from the longest one at each token (when there is a pair of input sequences).\n                    Overflowing tokens only contains overflow from the first sequence.\n                - \'only_first\': Only truncate the first sequence. raise an error if the first sequence is shorter or equal to than num_tokens_to_remove.\n                - \'only_second\': Only truncate the second sequence\n                - \'do_not_truncate\': Does not truncate (raise an error if the input sequence is longer than max_length)\n        """"""\n        if num_tokens_to_remove <= 0:\n            return ids, pair_ids, []\n\n        if truncation_strategy == \'longest_first\':\n            overflowing_tokens = []\n            for _ in range(num_tokens_to_remove):\n                if pair_ids is None or len(ids) > len(pair_ids):\n                    overflowing_tokens = [ids[-1]] + overflowing_tokens\n                    ids = ids[:-1]\n                else:\n                    pair_ids = pair_ids[:-1]\n            window_len = min(len(ids), stride)\n            if window_len > 0:\n                overflowing_tokens = ids[-window_len:] + overflowing_tokens\n        elif truncation_strategy == \'only_first\':\n            assert len(ids) > num_tokens_to_remove\n            window_len = min(len(ids), stride + num_tokens_to_remove)\n            overflowing_tokens = ids[-window_len:]\n            ids = ids[:-num_tokens_to_remove]\n        elif truncation_strategy == \'only_second\':\n            assert pair_ids is not None and len(pair_ids) > num_tokens_to_remove\n            window_len = min(len(pair_ids), stride + num_tokens_to_remove)\n            overflowing_tokens = pair_ids[-window_len:]\n            pair_ids = pair_ids[:-num_tokens_to_remove]\n        elif truncation_strategy == \'do_not_truncate\':\n            raise ValueError(""Input sequence are too long for max_length. Please select a truncation strategy."")\n        else:\n            raise ValueError(""Truncation_strategy should be selected in [\'longest_first\', \'only_first\', \'only_second\', \'do_not_truncate\']"")\n        return (ids, pair_ids, overflowing_tokens)\n\n    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n        logger.warning(""This tokenizer does not make use of special tokens."")\n        if token_ids_1 is None:\n            return len(token_ids_0) * [0]\n        return [0] * len(token_ids_0) + [1] * len(token_ids_1)\n\n    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n        """"""\n        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n        by concatenating and adding special tokens.\n        A RoBERTa sequence has the following format:\n            single sequence: <s> X </s>\n            pair of sequences: <s> A </s></s> B </s>\n        """"""\n        logger.warning(""This tokenizer does not make use of special tokens. Input is returned with no modification."")\n        if token_ids_1 is None:\n            return token_ids_0\n        return token_ids_0 + token_ids_1\n\n    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n        """"""\n        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n\n        Args:\n            token_ids_0: list of ids (must not contain special tokens)\n            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n                for sequence pairs\n            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n                special tokens for the model\n\n        Returns:\n            A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n        """"""\n        return [0] * ((len(token_ids_1) if token_ids_1 else 0) + len(token_ids_0))\n\n    def convert_ids_to_tokens(self, ids, skip_special_tokens=False):\n        """""" Converts a single index or a sequence of indices (integers) in a token ""\n            (resp.) a sequence of tokens (str/unicode), using the vocabulary and added tokens.\n\n            Args:\n                skip_special_tokens: Don\'t decode special tokens (self.all_special_tokens). Default: False\n        """"""\n        if isinstance(ids, int):\n            if ids in self.added_tokens_decoder:\n                return self.added_tokens_decoder[ids]\n            else:\n                return self._convert_id_to_token(ids)\n        tokens = []\n        for index in ids:\n            if skip_special_tokens and index in self.all_special_ids:\n                continue\n            if index in self.added_tokens_decoder:\n                tokens.append(self.added_tokens_decoder[index])\n            else:\n                tokens.append(self._convert_id_to_token(index))\n        return tokens\n\n    def _convert_id_to_token(self, index):\n        raise NotImplementedError\n\n    def convert_tokens_to_string(self, tokens):\n        """""" Converts a sequence of tokens (string) in a single string.\n            The most simple way to do it is \' \'.join(self.convert_ids_to_tokens(token_ids))\n            but we often want to remove sub-word tokenization artifacts at the same time.\n        """"""\n        return \' \'.join(self.convert_ids_to_tokens(tokens))\n\n    def decode(self, token_ids, skip_special_tokens=False, clean_up_tokenization_spaces=True):\n        """"""\n        Converts a sequence of ids (integer) in a string, using the tokenizer and vocabulary\n        with options to remove special tokens and clean up tokenization spaces.\n        Similar to doing ``self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))``.\n\n        Args:\n            token_ids: list of tokenized input ids. Can be obtained using the `encode` or `encode_plus` methods.\n            skip_special_tokens: if set to True, will replace special tokens.\n            clean_up_tokenization_spaces: if set to True, will clean up the tokenization spaces.\n        """"""\n        filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)\n\n        # To avoid mixing byte-level and unicode for byte-level BPT\n        # we need to build string separatly for added tokens and byte-level tokens\n        # cf. https://github.com/huggingface/transformers/issues/1133\n        sub_texts = []\n        current_sub_text = []\n        for token in filtered_tokens:\n            if skip_special_tokens and token in self.all_special_ids:\n                continue\n            if token in self.added_tokens_encoder:\n                if current_sub_text:\n                    sub_texts.append(self.convert_tokens_to_string(current_sub_text))\n                    current_sub_text = []\n                sub_texts.append("" "" + token)\n            else:\n                current_sub_text.append(token)\n        if current_sub_text:\n            sub_texts.append(self.convert_tokens_to_string(current_sub_text))\n        text = \'\'.join(sub_texts)\n\n        if clean_up_tokenization_spaces:\n            clean_text = self.clean_up_tokenization(text)\n            return clean_text\n        else:\n            return text\n\n    @property\n    def special_tokens_map(self):\n        """""" A dictionary mapping special token class attribute (cls_token, unk_token...) to their\n            values (\'<unk>\', \'<cls>\'...)\n        """"""\n        set_attr = {}\n        for attr in self.SPECIAL_TOKENS_ATTRIBUTES:\n            attr_value = getattr(self, ""_"" + attr)\n            if attr_value:\n                set_attr[attr] = attr_value\n        return set_attr\n\n    @property\n    def all_special_tokens(self):\n        """""" List all the special tokens (\'<unk>\', \'<cls>\'...) mapped to class attributes\n            (cls_token, unk_token...).\n        """"""\n        all_toks = []\n        set_attr = self.special_tokens_map\n        for attr_value in set_attr.values():\n            all_toks = all_toks + (list(attr_value) if isinstance(attr_value, (list, tuple)) else [attr_value])\n        all_toks = list(set(all_toks))\n        return all_toks\n\n    @property\n    def all_special_ids(self):\n        """""" List the vocabulary indices of the special tokens (\'<unk>\', \'<cls>\'...) mapped to\n            class attributes (cls_token, unk_token...).\n        """"""\n        all_toks = self.all_special_tokens\n        all_ids = list(self._convert_token_to_id(t) for t in all_toks)\n        return all_ids\n\n    @staticmethod\n    def clean_up_tokenization(out_string):\n        """""" Clean up a list of simple English tokenization artifacts like spaces before punctuations and abreviated forms.\n        """"""\n        out_string = out_string.replace(\' .\', \'.\').replace(\' ?\', \'?\').replace(\' !\', \'!\').replace(\' ,\', \',\'\n                        ).replace("" \' "", ""\'"").replace("" n\'t"", ""n\'t"").replace("" \'m"", ""\'m"").replace("" do not"", "" don\'t""\n                        ).replace("" \'s"", ""\'s"").replace("" \'ve"", ""\'ve"").replace("" \'re"", ""\'re"")\n        return out_string\n'"
processors/__init__.py,0,"b'from .utils import InputExample, InputFeatures, DataProcessor\nfrom .glue import (glue_output_modes, glue_processors, glue_tasks_num_labels,\n                   glue_convert_examples_to_features,collate_fn)\n\n\n'"
processors/glue.py,1,"b'"""""" GLUE processors and helpers """"""\n\nimport logging\nimport os\nimport torch\nfrom .utils import DataProcessor, InputExample, InputFeatures\n\nlogger = logging.getLogger(__name__)\n\n\ndef collate_fn(batch):\n    """"""\n    batch should be a list of (sequence, target, length) tuples...\n    Returns a padded tensor of sequences sorted from longest to shortest,\n    """"""\n    all_input_ids, all_attention_mask, all_token_type_ids, all_lens, all_labels = map(torch.stack, zip(*batch))\n    max_len = max(all_lens).item()\n    all_input_ids = all_input_ids[:, :max_len]\n    all_attention_mask = all_attention_mask[:, :max_len]\n    all_token_type_ids = all_token_type_ids[:, :max_len]\n    return all_input_ids, all_attention_mask, all_token_type_ids, all_labels\n\n\ndef glue_convert_examples_to_features(examples, tokenizer,\n                                      max_seq_length=512,\n                                      task=None,\n                                      label_list=None,\n                                      output_mode=None):\n    """"""\n    Loads a data file into a list of ``InputFeatures``\n    Args:\n        examples: List of ``InputExamples`` or ``tf.data.Dataset`` containing the examples.\n        tokenizer: Instance of a tokenizer that will tokenize the examples\n        max_length: Maximum example length\n        task: GLUE task\n        label_list: List of labels. Can be obtained from the processor using the ``processor.get_labels()`` method\n        output_mode: String indicating the output mode. Either ``regression`` or ``classification``\n        pad_on_left: If set to ``True``, the examples will be padded on the left rather than on the right (default)\n        pad_token: Padding token\n        pad_token_segment_id: The segment ID for the padding token (It is usually 0, but can vary such as for XLNet where it is 4)\n        mask_padding_with_zero: If set to ``True``, the attention mask will be filled by ``1`` for actual values\n            and by ``0`` for padded values. If set to ``False``, inverts it (``1`` for padded values, ``0`` for\n            actual values)\n\n    Returns:\n        If the ``examples`` input is a ``tf.data.Dataset``, will return a ``tf.data.Dataset``\n        containing the task-specific features. If the input is a list of ``InputExamples``, will return\n        a list of task-specific ``InputFeatures`` which can be fed to the model.\n\n    """"""\n    if task is not None:\n        processor = glue_processors[task]()\n        if label_list is None:\n            label_list = processor.get_labels()\n            logger.info(""Using label list %s for task %s"" % (label_list, task))\n        if output_mode is None:\n            output_mode = glue_output_modes[task]\n            logger.info(""Using output mode %s for task %s"" % (output_mode, task))\n\n    label_map = {label: i for i, label in enumerate(label_list)}\n\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            logger.info(""Writing example %d"" % (ex_index))\n\n        tokens_a = tokenizer.tokenize(example.text_a)\n        tokens_b  =None\n        if example.text_b:\n            tokens_b = tokenizer.tokenize(example.text_b)\n        if tokens_b:\n            # Modifies `tokens_a` and `tokens_b` in place so that the total\n            # length is less than the specified length.\n            # Account for [CLS], [SEP], [SEP] with ""- 3""\n            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n        else:\n            # Account for [CLS] and [SEP] with ""- 2""\n            if len(tokens_a) > max_seq_length - 2:\n                tokens_a = tokens_a[0:(max_seq_length - 2)]\n\n        tokens = []\n        token_type_ids = []\n        tokens.append(""[CLS]"")\n        token_type_ids.append(0)\n        for token in tokens_a:\n            tokens.append(token)\n            token_type_ids.append(0)\n        tokens.append(""[SEP]"")\n        token_type_ids.append(0)\n\n        if tokens_b:\n            for token in tokens_b:\n                tokens.append(token)\n                token_type_ids.append(1)\n            tokens.append(""[SEP]"")\n            token_type_ids.append(1)\n\n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n        # tokens are attended to.\n        attention_mask = [1] * len(input_ids)\n        input_len = len(input_ids)\n\n        # Zero-pad up to the sequence length.\n        while len(input_ids) < max_seq_length:\n            input_ids.append(0)\n            attention_mask.append(0)\n            token_type_ids.append(0)\n\n        assert len(input_ids) == max_seq_length\n        assert len(attention_mask) == max_seq_length\n        assert len(token_type_ids) == max_seq_length\n        if output_mode == ""classification"":\n            label_id = label_map[example.label]\n        elif output_mode == ""regression"":\n            label_id = float(example.label)\n        else:\n            raise KeyError(output_mode)\n        if ex_index < 5:\n            logger.info(""*** Example ***"")\n            logger.info(""guid: %s"" % (example.guid))\n            logger.info(""input_ids: %s"" % "" "".join([str(x) for x in input_ids]))\n            logger.info(""attention_mask: %s"" % "" "".join([str(x) for x in attention_mask]))\n            logger.info(""token_type_ids: %s"" % "" "".join([str(x) for x in token_type_ids]))\n            logger.info(""label: %s (id = %d)"" % (example.label, label_id))\n            logger.info(""input length: %d"" % (input_len))\n\n        features.append(\n            InputFeatures(input_ids=input_ids,\n                          attention_mask=attention_mask,\n                          token_type_ids=token_type_ids,\n                          label=label_id,\n                          input_len=input_len))\n    return features\n\ndef _truncate_seq_pair(tokens_a, tokens_b, max_length):\n  """"""Truncates a sequence pair in place to the maximum length.""""""\n\n  # This is a simple heuristic which will always truncate the longer sequence\n  # one token at a time. This makes more sense than truncating an equal percent\n  # of tokens from each, since if one sequence is very short then each token\n  # that\'s truncated likely contains more information than a longer sequence.\n  while True:\n    total_length = len(tokens_a) + len(tokens_b)\n    if total_length <= max_length:\n      break\n    if len(tokens_a) > len(tokens_b):\n      tokens_a.pop()\n    else:\n      tokens_b.pop()\n\nclass MrpcProcessor(DataProcessor):\n    """"""Processor for the MRPC data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        logger.info(""LOOKING AT {}"".format(os.path.join(data_dir, ""train.tsv"")))\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""0"", ""1""]\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, i)\n            text_a = line[3]\n            text_b = line[4]\n            label = line[0]\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n        return examples\n\n\nclass MnliProcessor(DataProcessor):\n    """"""Processor for the MultiNLI data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev_matched.tsv"")),\n            ""dev_matched"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""contradiction"", ""entailment"", ""neutral""]\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, line[0])\n            text_a = line[8]\n            text_b = line[9]\n            label = line[-1]\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n        return examples\n\nclass MnliMismatchedProcessor(MnliProcessor):\n    """"""Processor for the MultiNLI Mismatched data set (GLUE version).""""""\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev_mismatched.tsv"")),\n            ""dev_matched"")\n\n\nclass ColaProcessor(DataProcessor):\n    """"""Processor for the cola data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""0"", ""1""]\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            guid = ""%s-%s"" % (set_type, i)\n            text_a = line[3]\n            label = line[1]\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n        return examples\n\nclass Sst2Processor(DataProcessor):\n    """"""Processor for the SST-2 data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""0"", ""1""]\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, i)\n            text_a = line[0]\n            label = line[1]\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n        return examples\n\nclass StsbProcessor(DataProcessor):\n    """"""Processor for the sts-b data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [None]\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, line[0])\n            text_a = line[7]\n            text_b = line[8]\n            label = line[-1]\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n        return examples\n\n\nclass QqpProcessor(DataProcessor):\n    """"""Processor for the QQP data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""0"", ""1""]\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, line[0])\n            try:\n                text_a = line[3]\n                text_b = line[4]\n                label = line[5]\n            except IndexError:\n                continue\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n        return examples\n\n\nclass QnliProcessor(DataProcessor):\n    """"""Processor for the QNLI data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev.tsv"")),\n            ""dev_matched"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""entailment"", ""not_entailment""]\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, line[0])\n            text_a = line[1]\n            text_b = line[2]\n            label = line[-1]\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n        return examples\n\n\nclass RteProcessor(DataProcessor):\n    """"""Processor for the RTE data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""entailment"", ""not_entailment""]\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, line[0])\n            text_a = line[1]\n            text_b = line[2]\n            label = line[-1]\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n        return examples\n\nclass LcqmcProcessor(DataProcessor):\n    """"""Processor for the LCQMC data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.txt"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev.txt"")), ""dev"")\n\n    def get_test_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""test.txt"")), ""test"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""0"", ""1""]\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            guid = ""%s-%s"" % (set_type, i)\n            text_a = line[0]\n            text_b = line[1]\n            if set_type == \'test222\':\n                label = \'0\'\n            else:\n                label = line[2]\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n        return examples\n\n\nclass WnliProcessor(DataProcessor):\n    """"""Processor for the WNLI data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""0"", ""1""]\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, line[0])\n            text_a = line[1]\n            text_b = line[2]\n            label = line[-1]\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n        return examples\n\n\nglue_tasks_num_labels = {\n    ""mnli"": 3,\n    ""mrpc"": 2,\n    ""sst-2"": 2,\n    ""sts-b"": 1,\n    ""qqp"": 2,\n    ""qnli"": 2,\n    ""rte"": 2,\n    \'lcqmc\': 2,\n    ""xnli"": 3,\n}\n\nglue_processors = {\n    ""cola"": ColaProcessor,\n    ""mnli"": MnliProcessor,\n    ""mnli-mm"": MnliMismatchedProcessor,\n    ""mrpc"": MrpcProcessor,\n    ""sst-2"": Sst2Processor,\n    ""sts-b"": StsbProcessor,\n    ""qqp"": QqpProcessor,\n    ""qnli"": QnliProcessor,\n    ""rte"": RteProcessor,\n    \'lcqmc\': LcqmcProcessor,\n    ""wnli"": WnliProcessor,\n}\n\nglue_output_modes = {\n    ""cola"": ""classification"",\n    ""mnli"": ""classification"",\n    ""mnli-mm"": ""classification"",\n    ""mrpc"": ""classification"",\n    ""sst-2"": ""classification"",\n    ""sts-b"": ""regression"",\n    ""qqp"": ""classification"",\n    ""qnli"": ""classification"",\n    ""rte"": ""classification"",\n    ""wnli"": ""classification"",\n    \'lcqmc\': ""classification"",\n}\n'"
processors/utils.py,0,"b'import csv\nimport sys\nimport copy\nimport json\n\nclass InputExample(object):\n    """"""\n    A single training/test example for simple sequence classification.\n\n    Args:\n        guid: Unique id for the example.\n        text_a: string. The untokenized text of the first sequence. For single\n        sequence tasks, only this sequence must be specified.\n        text_b: (Optional) string. The untokenized text of the second sequence.\n        Only must be specified for sequence pair tasks.\n        label: (Optional) string. The label of the example. This should be\n        specified for train and dev examples, but not for test examples.\n    """"""\n    def __init__(self, guid, text_a, text_b=None, label=None):\n        self.guid = guid\n        self.text_a = text_a\n        self.text_b = text_b\n        self.label = label\n\n    def __repr__(self):\n        return str(self.to_json_string())\n\n    def to_dict(self):\n        """"""Serializes this instance to a Python dictionary.""""""\n        output = copy.deepcopy(self.__dict__)\n        return output\n\n    def to_json_string(self):\n        """"""Serializes this instance to a JSON string.""""""\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\n\n\nclass InputFeatures(object):\n    """"""\n    A single set of features of data.\n\n    Args:\n        input_ids: Indices of input sequence tokens in the vocabulary.\n        attention_mask: Mask to avoid performing attention on padding token indices.\n            Mask values selected in ``[0, 1]``:\n            Usually  ``1`` for tokens that are NOT MASKED, ``0`` for MASKED (padded) tokens.\n        token_type_ids: Segment token indices to indicate first and second portions of the inputs.\n        label: Label corresponding to the input\n    """"""\n\n    def __init__(self, input_ids, attention_mask, token_type_ids, label,input_len):\n        self.input_ids = input_ids\n        self.attention_mask = attention_mask\n        self.token_type_ids = token_type_ids\n        self.input_len = input_len\n        self.label = label\n\n    def __repr__(self):\n        return str(self.to_json_string())\n\n    def to_dict(self):\n        """"""Serializes this instance to a Python dictionary.""""""\n        output = copy.deepcopy(self.__dict__)\n        return output\n\n    def to_json_string(self):\n        """"""Serializes this instance to a JSON string.""""""\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\n\n\nclass DataProcessor(object):\n    """"""Base class for data converters for sequence classification data sets.""""""\n\n    def get_train_examples(self, data_dir):\n        """"""Gets a collection of `InputExample`s for the train set.""""""\n        raise NotImplementedError()\n\n    def get_dev_examples(self, data_dir):\n        """"""Gets a collection of `InputExample`s for the dev set.""""""\n        raise NotImplementedError()\n\n    def get_labels(self):\n        """"""Gets the list of labels for this data set.""""""\n        raise NotImplementedError()\n\n    @classmethod\n    def _read_tsv(cls, input_file, quotechar=None):\n        """"""Reads a tab separated value file.""""""\n        with open(input_file, ""r"", encoding=""utf-8-sig"") as f:\n            reader = csv.reader(f, delimiter=""\\t"", quotechar=quotechar)\n            lines = []\n            for line in reader:\n                lines.append(line)\n            return lines\n\n    @classmethod\n    def _read_txt(cls, input_file):\n        """"""Reads a tab separated value file.""""""\n        with open(input_file, ""r"") as f:\n            reader = f.readlines()\n            lines = []\n            for line in reader:\n                lines.append(line.strip().split(""_!_""))\n            return lines\n'"
tools/common.py,14,"b'import os\r\nimport random\r\nimport torch\r\nimport numpy as np\r\nimport json\r\nimport pickle\r\nimport torch.nn as nn\r\nfrom collections import OrderedDict\r\nfrom pathlib import Path\r\nimport logging\r\n\r\nlogger = logging.getLogger()\r\n\r\ndef init_logger(log_file=None, log_file_level=logging.NOTSET):\r\n    \'\'\'\r\n    Example:\r\n        >>> init_logger(log_file)\r\n        >>> logger.info(""abc\'"")\r\n    \'\'\'\r\n    if isinstance(log_file,Path):\r\n        log_file = str(log_file)\r\n\r\n    log_format = logging.Formatter(fmt=\'%(asctime)s - %(levelname)s - %(name)s -   %(message)s\',\r\n                                   datefmt=\'%m/%d/%Y %H:%M:%S\')\r\n    logger = logging.getLogger()\r\n    logger.setLevel(logging.INFO)\r\n    console_handler = logging.StreamHandler()\r\n    console_handler.setFormatter(log_format)\r\n    logger.handlers = [console_handler]\r\n    if log_file and log_file != \'\':\r\n        file_handler = logging.FileHandler(log_file)\r\n        file_handler.setLevel(log_file_level)\r\n        file_handler.setFormatter(log_format)\r\n        logger.addHandler(file_handler)\r\n    return logger\r\n\r\ndef seed_everything(seed=1029):\r\n    \'\'\'\r\n    \xe8\xae\xbe\xe7\xbd\xae\xe6\x95\xb4\xe4\xb8\xaa\xe5\xbc\x80\xe5\x8f\x91\xe7\x8e\xaf\xe5\xa2\x83\xe7\x9a\x84seed\r\n    :param seed:\r\n    :param device:\r\n    :return:\r\n    \'\'\'\r\n    random.seed(seed)\r\n    os.environ[\'PYTHONHASHSEED\'] = str(seed)\r\n    np.random.seed(seed)\r\n    torch.manual_seed(seed)\r\n    torch.cuda.manual_seed(seed)\r\n    torch.cuda.manual_seed_all(seed)\r\n    # some cudnn methods can be random even after fixing the seed\r\n    # unless you tell it to be deterministic\r\n    torch.backends.cudnn.deterministic = True\r\n\r\n\r\ndef prepare_device(n_gpu_use):\r\n    """"""\r\n    setup GPU device if available, move model into configured device\r\n    # \xe5\xa6\x82\xe6\x9e\x9cn_gpu_use\xe4\xb8\xba\xe6\x95\xb0\xe5\xad\x97\xef\xbc\x8c\xe5\x88\x99\xe4\xbd\xbf\xe7\x94\xa8range\xe7\x94\x9f\xe6\x88\x90list\r\n    # \xe5\xa6\x82\xe6\x9e\x9c\xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaalist\xef\xbc\x8c\xe5\x88\x99\xe9\xbb\x98\xe8\xae\xa4\xe4\xbd\xbf\xe7\x94\xa8list[0]\xe4\xbd\x9c\xe4\xb8\xbacontroller\r\n     """"""\r\n    if not n_gpu_use:\r\n        device_type = \'cpu\'\r\n    else:\r\n        n_gpu_use = n_gpu_use.split("","")\r\n        device_type = f""cuda:{n_gpu_use[0]}""\r\n    n_gpu = torch.cuda.device_count()\r\n    if len(n_gpu_use) > 0 and n_gpu == 0:\r\n        logger.warning(""Warning: There\\\'s no GPU available on this machine, training will be performed on CPU."")\r\n        device_type = \'cpu\'\r\n    if len(n_gpu_use) > n_gpu:\r\n        msg = f""Warning: The number of GPU\\\'s configured to use is {n_gpu_use}, but only {n_gpu} are available on this machine.""\r\n        logger.warning(msg)\r\n        n_gpu_use = range(n_gpu)\r\n    device = torch.device(device_type)\r\n    list_ids = n_gpu_use\r\n    return device, list_ids\r\n\r\n\r\ndef model_device(n_gpu, model):\r\n    \'\'\'\r\n    \xe5\x88\xa4\xe6\x96\xad\xe7\x8e\xaf\xe5\xa2\x83 cpu\xe8\xbf\x98\xe6\x98\xafgpu\r\n    \xe6\x94\xaf\xe6\x8c\x81\xe5\x8d\x95\xe6\x9c\xba\xe5\xa4\x9a\xe5\x8d\xa1\r\n    :param n_gpu:\r\n    :param model:\r\n    :return:\r\n    \'\'\'\r\n    device, device_ids = prepare_device(n_gpu)\r\n    if len(device_ids) > 1:\r\n        logger.info(f""current {len(device_ids)} GPUs"")\r\n        model = torch.nn.DataParallel(model, device_ids=device_ids)\r\n    if len(device_ids) == 1:\r\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = str(device_ids[0])\r\n    model = model.to(device)\r\n    return model, device\r\n\r\n\r\ndef restore_checkpoint(resume_path, model=None):\r\n    \'\'\'\r\n    \xe5\x8a\xa0\xe8\xbd\xbd\xe6\xa8\xa1\xe5\x9e\x8b\r\n    :param resume_path:\r\n    :param model:\r\n    :param optimizer:\r\n    :return:\r\n    \xe6\xb3\xa8\xe6\x84\x8f\xef\xbc\x9a \xe5\xa6\x82\xe6\x9e\x9c\xe6\x98\xaf\xe5\x8a\xa0\xe8\xbd\xbdBert\xe6\xa8\xa1\xe5\x9e\x8b\xe7\x9a\x84\xe8\xaf\x9d\xef\xbc\x8c\xe9\x9c\x80\xe8\xa6\x81\xe8\xb0\x83\xe6\x95\xb4\xef\xbc\x8c\xe4\xb8\x8d\xe8\x83\xbd\xe4\xbd\xbf\xe7\x94\xa8\xe8\xaf\xa5\xe6\xa8\xa1\xe5\xbc\x8f\r\n    \xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8\xe6\xa8\xa1\xe5\x9d\x97\xe8\x87\xaa\xe5\xb8\xa6\xe7\x9a\x84Bert_model.from_pretrained(state_dict = your save state_dict)\r\n    \'\'\'\r\n    if isinstance(resume_path, Path):\r\n        resume_path = str(resume_path)\r\n    checkpoint = torch.load(resume_path)\r\n    best = checkpoint[\'best\']\r\n    start_epoch = checkpoint[\'epoch\'] + 1\r\n    states = checkpoint[\'state_dict\']\r\n    if isinstance(model, nn.DataParallel):\r\n        model.module.load_state_dict(states)\r\n    else:\r\n        model.load_state_dict(states)\r\n    return [model,best,start_epoch]\r\n\r\n\r\ndef save_pickle(data, file_path):\r\n    \'\'\'\r\n    \xe4\xbf\x9d\xe5\xad\x98\xe6\x88\x90pickle\xe6\x96\x87\xe4\xbb\xb6\r\n    :param data:\r\n    :param file_name:\r\n    :param pickle_path:\r\n    :return:\r\n    \'\'\'\r\n    if isinstance(file_path, Path):\r\n        file_path = str(file_path)\r\n    with open(file_path, \'wb\') as f:\r\n        pickle.dump(data, f)\r\n\r\n\r\ndef load_pickle(input_file):\r\n    \'\'\'\r\n    \xe8\xaf\xbb\xe5\x8f\x96pickle\xe6\x96\x87\xe4\xbb\xb6\r\n    :param pickle_path:\r\n    :param file_name:\r\n    :return:\r\n    \'\'\'\r\n    with open(str(input_file), \'rb\') as f:\r\n        data = pickle.load(f)\r\n    return data\r\n\r\n\r\ndef save_json(data, file_path):\r\n    \'\'\'\r\n    \xe4\xbf\x9d\xe5\xad\x98\xe6\x88\x90json\xe6\x96\x87\xe4\xbb\xb6\r\n    :param data:\r\n    :param json_path:\r\n    :param file_name:\r\n    :return:\r\n    \'\'\'\r\n    if not isinstance(file_path, Path):\r\n        file_path = Path(file_path)\r\n    # if isinstance(data,dict):\r\n    #     data = json.dumps(data)\r\n    with open(str(file_path), \'w\') as f:\r\n        json.dump(data, f)\r\n\r\n\r\ndef load_json(file_path):\r\n    \'\'\'\r\n    \xe5\x8a\xa0\xe8\xbd\xbdjson\xe6\x96\x87\xe4\xbb\xb6\r\n    :param json_path:\r\n    :param file_name:\r\n    :return:\r\n    \'\'\'\r\n    if not isinstance(file_path, Path):\r\n        file_path = Path(file_path)\r\n    with open(str(file_path), \'r\') as f:\r\n        data = json.load(f)\r\n    return data\r\n\r\ndef save_model(model, model_path):\r\n    """""" \xe5\xad\x98\xe5\x82\xa8\xe4\xb8\x8d\xe5\x90\xab\xe6\x9c\x89\xe6\x98\xbe\xe5\x8d\xa1\xe4\xbf\xa1\xe6\x81\xaf\xe7\x9a\x84state_dict\xe6\x88\x96model\r\n    :param model:\r\n    :param model_name:\r\n    :param only_param:\r\n    :return:\r\n    """"""\r\n    if isinstance(model_path, Path):\r\n        model_path = str(model_path)\r\n    if isinstance(model, nn.DataParallel):\r\n        model = model.module\r\n    state_dict = model.state_dict()\r\n    for key in state_dict:\r\n        state_dict[key] = state_dict[key].cpu()\r\n    torch.save(state_dict, model_path)\r\n\r\ndef load_model(model, model_path):\r\n    \'\'\'\r\n    \xe5\x8a\xa0\xe8\xbd\xbd\xe6\xa8\xa1\xe5\x9e\x8b\r\n    :param model:\r\n    :param model_name:\r\n    :param model_path:\r\n    :param only_param:\r\n    :return:\r\n    \'\'\'\r\n    if isinstance(model_path, Path):\r\n        model_path = str(model_path)\r\n    logging.info(f""loading model from {str(model_path)} ."")\r\n    states = torch.load(model_path)\r\n    state = states[\'state_dict\']\r\n    if isinstance(model, nn.DataParallel):\r\n        model.module.load_state_dict(state)\r\n    else:\r\n        model.load_state_dict(state)\r\n    return model\r\n\r\n\r\nclass AverageMeter(object):\r\n    \'\'\'\r\n    computes and stores the average and current value\r\n    Example:\r\n        >>> loss = AverageMeter()\r\n        >>> for step,batch in enumerate(train_data):\r\n        >>>     pred = self.model(batch)\r\n        >>>     raw_loss = self.metrics(pred,target)\r\n        >>>     loss.update(raw_loss.item(),n = 1)\r\n        >>> cur_loss = loss.avg\r\n    \'\'\'\r\n\r\n    def __init__(self):\r\n        self.reset()\r\n\r\n    def reset(self):\r\n        self.val = 0\r\n        self.avg = 0\r\n        self.sum = 0\r\n        self.count = 0\r\n\r\n    def update(self, val, n=1):\r\n        self.val = val\r\n        self.sum += val * n\r\n        self.count += n\r\n        self.avg = self.sum / self.count\r\n\r\n\r\ndef summary(model, *inputs, batch_size=-1, show_input=True):\r\n    \'\'\'\r\n    \xe6\x89\x93\xe5\x8d\xb0\xe6\xa8\xa1\xe5\x9e\x8b\xe7\xbb\x93\xe6\x9e\x84\xe4\xbf\xa1\xe6\x81\xaf\r\n    :param model:\r\n    :param inputs:\r\n    :param batch_size:\r\n    :param show_input:\r\n    :return:\r\n    Example:\r\n        >>> print(""model summary info: "")\r\n        >>> for step,batch in enumerate(train_data):\r\n        >>>     summary(self.model,*batch,show_input=True)\r\n        >>>     break\r\n    \'\'\'\r\n\r\n    def register_hook(module):\r\n        def hook(module, input, output=None):\r\n            class_name = str(module.__class__).split(""."")[-1].split(""\'"")[0]\r\n            module_idx = len(summary)\r\n\r\n            m_key = f""{class_name}-{module_idx + 1}""\r\n            summary[m_key] = OrderedDict()\r\n            summary[m_key][""input_shape""] = list(input[0].size())\r\n            summary[m_key][""input_shape""][0] = batch_size\r\n\r\n            if show_input is False and output is not None:\r\n                if isinstance(output, (list, tuple)):\r\n                    for out in output:\r\n                        if isinstance(out, torch.Tensor):\r\n                            summary[m_key][""output_shape""] = [\r\n                                [-1] + list(out.size())[1:]\r\n                            ][0]\r\n                        else:\r\n                            summary[m_key][""output_shape""] = [\r\n                                [-1] + list(out[0].size())[1:]\r\n                            ][0]\r\n                else:\r\n                    summary[m_key][""output_shape""] = list(output.size())\r\n                    summary[m_key][""output_shape""][0] = batch_size\r\n\r\n            params = 0\r\n            if hasattr(module, ""weight"") and hasattr(module.weight, ""size""):\r\n                params += torch.prod(torch.LongTensor(list(module.weight.size())))\r\n                summary[m_key][""trainable""] = module.weight.requires_grad\r\n            if hasattr(module, ""bias"") and hasattr(module.bias, ""size""):\r\n                params += torch.prod(torch.LongTensor(list(module.bias.size())))\r\n            summary[m_key][""nb_params""] = params\r\n\r\n        if (not isinstance(module, nn.Sequential) and not isinstance(module, nn.ModuleList) and not (module == model)):\r\n            if show_input is True:\r\n                hooks.append(module.register_forward_pre_hook(hook))\r\n            else:\r\n                hooks.append(module.register_forward_hook(hook))\r\n\r\n    # create properties\r\n    summary = OrderedDict()\r\n    hooks = []\r\n\r\n    # register hook\r\n    model.apply(register_hook)\r\n    model(*inputs)\r\n\r\n    # remove these hooks\r\n    for h in hooks:\r\n        h.remove()\r\n\r\n    print(""-----------------------------------------------------------------------"")\r\n    if show_input is True:\r\n        line_new = f""{\'Layer (type)\':>25}  {\'Input Shape\':>25} {\'Param #\':>15}""\r\n    else:\r\n        line_new = f""{\'Layer (type)\':>25}  {\'Output Shape\':>25} {\'Param #\':>15}""\r\n    print(line_new)\r\n    print(""======================================================================="")\r\n\r\n    total_params = 0\r\n    total_output = 0\r\n    trainable_params = 0\r\n    for layer in summary:\r\n        # input_shape, output_shape, trainable, nb_params\r\n        if show_input is True:\r\n            line_new = ""{:>25}  {:>25} {:>15}"".format(\r\n                layer,\r\n                str(summary[layer][""input_shape""]),\r\n                ""{0:,}"".format(summary[layer][""nb_params""]),\r\n            )\r\n        else:\r\n            line_new = ""{:>25}  {:>25} {:>15}"".format(\r\n                layer,\r\n                str(summary[layer][""output_shape""]),\r\n                ""{0:,}"".format(summary[layer][""nb_params""]),\r\n            )\r\n\r\n        total_params += summary[layer][""nb_params""]\r\n        if show_input is True:\r\n            total_output += np.prod(summary[layer][""input_shape""])\r\n        else:\r\n            total_output += np.prod(summary[layer][""output_shape""])\r\n        if ""trainable"" in summary[layer]:\r\n            if summary[layer][""trainable""] == True:\r\n                trainable_params += summary[layer][""nb_params""]\r\n\r\n        print(line_new)\r\n\r\n    print(""======================================================================="")\r\n    print(f""Total params: {total_params:0,}"")\r\n    print(f""Trainable params: {trainable_params:0,}"")\r\n    print(f""Non-trainable params: {(total_params - trainable_params):0,}"")\r\n    print(""-----------------------------------------------------------------------"")\r\n'"
callback/optimization/__init__.py,0,b''
callback/optimization/adabound.py,6,"b'import torch\r\nimport math\r\nfrom torch.optim.optimizer import Optimizer\r\n\r\nclass AdaBound(Optimizer):\r\n    """"""Implements AdaBound algorithm.\r\n    It has been proposed in `Adaptive Gradient Methods with Dynamic Bound of Learning Rate`_.\r\n    Arguments:\r\n        params (iterable): iterable of parameters to optimize or dicts defining\r\n            parameter groups\r\n        lr (float, optional): Adam learning rate (default: 1e-3)\r\n        betas (Tuple[float, float], optional): coefficients used for computing\r\n            running averages of gradient and its square (default: (0.9, 0.999))\r\n        final_lr (float, optional): final (SGD) learning rate (default: 0.1)\r\n        gamma (float, optional): convergence speed of the bound functions (default: 1e-3)\r\n        eps (float, optional): term added to the denominator to improve\r\n            numerical stability (default: 1e-8)\r\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\r\n        amsbound (boolean, optional): whether to use the AMSBound variant of this algorithm\r\n    .. Adaptive Gradient Methods with Dynamic Bound of Learning Rate:\r\n        https://openreview.net/forum?id=Bkg3g2R9FX\r\n    Example:\r\n        >>> model = LSTM()\r\n        >>> optimizer = AdaBound(model.parameters())\r\n    """"""\r\n\r\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), final_lr=0.1, gamma=1e-3,\r\n                 eps=1e-8, weight_decay=0, amsbound=False):\r\n        if not 0.0 <= lr:\r\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\r\n        if not 0.0 <= eps:\r\n            raise ValueError(""Invalid epsilon value: {}"".format(eps))\r\n        if not 0.0 <= betas[0] < 1.0:\r\n            raise ValueError(""Invalid beta parameter at index 0: {}"".format(betas[0]))\r\n        if not 0.0 <= betas[1] < 1.0:\r\n            raise ValueError(""Invalid beta parameter at index 1: {}"".format(betas[1]))\r\n        if not 0.0 <= final_lr:\r\n            raise ValueError(""Invalid final learning rate: {}"".format(final_lr))\r\n        if not 0.0 <= gamma < 1.0:\r\n            raise ValueError(""Invalid gamma parameter: {}"".format(gamma))\r\n        defaults = dict(lr=lr, betas=betas, final_lr=final_lr, gamma=gamma, eps=eps,\r\n                        weight_decay=weight_decay, amsbound=amsbound)\r\n        super(AdaBound, self).__init__(params, defaults)\r\n\r\n        self.base_lrs = list(map(lambda group: group[\'lr\'], self.param_groups))\r\n\r\n    def __setstate__(self, state):\r\n        super(AdaBound, self).__setstate__(state)\r\n        for group in self.param_groups:\r\n            group.setdefault(\'amsbound\', False)\r\n\r\n    def step(self, closure=None):\r\n        """"""Performs a single optimization step.\r\n        Arguments:\r\n            closure (callable, optional): A closure that reevaluates the model\r\n                and returns the loss.\r\n        """"""\r\n        loss = None\r\n        if closure is not None:\r\n            loss = closure()\r\n        for group, base_lr in zip(self.param_groups, self.base_lrs):\r\n            for p in group[\'params\']:\r\n                if p.grad is None:\r\n                    continue\r\n                grad = p.grad.data\r\n                if grad.is_sparse:\r\n                    raise RuntimeError(\r\n                        \'Adam does not support sparse gradients, please consider SparseAdam instead\')\r\n                amsbound = group[\'amsbound\']\r\n                state = self.state[p]\r\n                # State initialization\r\n                if len(state) == 0:\r\n                    state[\'step\'] = 0\r\n                    # Exponential moving average of gradient values\r\n                    state[\'exp_avg\'] = torch.zeros_like(p.data)\r\n                    # Exponential moving average of squared gradient values\r\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\r\n                    if amsbound:\r\n                        # Maintains max of all exp. moving avg. of sq. grad. values\r\n                        state[\'max_exp_avg_sq\'] = torch.zeros_like(p.data)\r\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\r\n                if amsbound:\r\n                    max_exp_avg_sq = state[\'max_exp_avg_sq\']\r\n                beta1, beta2 = group[\'betas\']\r\n                state[\'step\'] += 1\r\n                if group[\'weight_decay\'] != 0:\r\n                    grad = grad.add(group[\'weight_decay\'], p.data)\r\n                # Decay the first and second moment running average coefficient\r\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n                if amsbound:\r\n                    # Maintains the maximum of all 2nd moment running avg. till now\r\n                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\r\n                    # Use the max. for normalizing running avg. of gradient\r\n                    denom = max_exp_avg_sq.sqrt().add_(group[\'eps\'])\r\n                else:\r\n                    denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\r\n\r\n                bias_correction1 = 1 - beta1 ** state[\'step\']\r\n                bias_correction2 = 1 - beta2 ** state[\'step\']\r\n                step_size = group[\'lr\'] * math.sqrt(bias_correction2) / bias_correction1\r\n\r\n                # Applies bounds on actual learning rate\r\n                # lr_scheduler cannot affect final_lr, this is a workaround to apply lr decay\r\n                final_lr = group[\'final_lr\'] * group[\'lr\'] / base_lr\r\n                lower_bound = final_lr * (1 - 1 / (group[\'gamma\'] * state[\'step\'] + 1))\r\n                upper_bound = final_lr * (1 + 1 / (group[\'gamma\'] * state[\'step\']))\r\n                step_size = torch.full_like(denom, step_size)\r\n                step_size.div_(denom).clamp_(lower_bound, upper_bound).mul_(exp_avg)\r\n                p.data.add_(-step_size)\r\n        return loss'"
callback/optimization/adafactor.py,19,"b""import operator\r\nimport torch\r\nfrom copy import copy\r\nimport functools\r\nfrom math import sqrt\r\nfrom torch.optim.optimizer import Optimizer\r\n\r\n\r\nclass AdaFactor(Optimizer):\r\n    '''\r\n    # Code below is an implementation of https://arxiv.org/pdf/1804.04235.pdf\r\n    # inspired but modified from https://github.com/DeadAt0m/adafactor-pytorch\r\n    Example:\r\n        >>> model = LSTM()\r\n        >>> optimizer = AdaFactor(model.parameters(),lr= lr)\r\n    '''\r\n\r\n    def __init__(self, params, lr=None, beta1=0.9, beta2=0.999, eps1=1e-30,\r\n                 eps2=1e-3, cliping_threshold=1, non_constant_decay=True,\r\n                 enable_factorization=True, ams_grad=True, weight_decay=0):\r\n\r\n        enable_momentum = beta1 != 0\r\n        if non_constant_decay:\r\n            ams_grad = False\r\n\r\n        defaults = dict(lr=lr, beta1=beta1, beta2=beta2, eps1=eps1,\r\n                        eps2=eps2, cliping_threshold=cliping_threshold,\r\n                        weight_decay=weight_decay, ams_grad=ams_grad,\r\n                        enable_factorization=enable_factorization,\r\n                        enable_momentum=enable_momentum,\r\n                        non_constant_decay=non_constant_decay)\r\n\r\n        super(AdaFactor, self).__init__(params, defaults)\r\n\r\n    def __setstate__(self, state):\r\n        super(AdaFactor, self).__setstate__(state)\r\n\r\n    def _experimental_reshape(self, shape):\r\n        temp_shape = shape[2:]\r\n        if len(temp_shape) == 1:\r\n            new_shape = (shape[0], shape[1] * shape[2])\r\n        else:\r\n            tmp_div = len(temp_shape) // 2 + len(temp_shape) % 2\r\n            new_shape = (shape[0] * functools.reduce(operator.mul,\r\n                                                     temp_shape[tmp_div:], 1),\r\n                         shape[1] * functools.reduce(operator.mul,\r\n                                                     temp_shape[:tmp_div], 1))\r\n        return new_shape, copy(shape)\r\n\r\n    def _check_shape(self, shape):\r\n        '''\r\n        output1 - True - algorithm for matrix, False - vector;\r\n        output2 - need reshape\r\n        '''\r\n        if len(shape) > 2:\r\n            return True, True\r\n        elif len(shape) == 2:\r\n            return True, False\r\n        elif len(shape) == 2 and (shape[0] == 1 or shape[1] == 1):\r\n            return False, False\r\n        else:\r\n            return False, False\r\n\r\n    def _rms(self, x):\r\n        return sqrt(torch.mean(x.pow(2)))\r\n\r\n    def step(self, closure=None):\r\n        loss = None\r\n        if closure is not None:\r\n            loss = closure()\r\n        for group in self.param_groups:\r\n            for p in group['params']:\r\n                if p.grad is None:\r\n                    continue\r\n                grad = p.grad.data\r\n\r\n                if grad.is_sparse:\r\n                    raise RuntimeError('Adam does not support sparse \\\r\n                                       gradients, use SparseAdam instead')\r\n\r\n                is_matrix, is_need_reshape = self._check_shape(grad.size())\r\n                new_shape = p.data.size()\r\n                if is_need_reshape and group['enable_factorization']:\r\n                    new_shape, old_shape = \\\r\n                        self._experimental_reshape(p.data.size())\r\n                    grad = grad.view(new_shape)\r\n\r\n                state = self.state[p]\r\n                if len(state) == 0:\r\n                    state['step'] = 0\r\n                    if group['enable_momentum']:\r\n                        state['exp_avg'] = torch.zeros(new_shape,\r\n                                                       dtype=torch.float32,\r\n                                                       device=p.grad.device)\r\n\r\n                    if is_matrix and group['enable_factorization']:\r\n                        state['exp_avg_sq_R'] = \\\r\n                            torch.zeros((1, new_shape[1]),\r\n                                        dtype=torch.float32,\r\n                                        device=p.grad.device)\r\n                        state['exp_avg_sq_C'] = \\\r\n                            torch.zeros((new_shape[0], 1),\r\n                                        dtype=torch.float32,\r\n                                        device=p.grad.device)\r\n                    else:\r\n                        state['exp_avg_sq'] = torch.zeros(new_shape,\r\n                                                          dtype=torch.float32,\r\n                                                          device=p.grad.device)\r\n                    if group['ams_grad']:\r\n                        state['exp_avg_sq_hat'] = \\\r\n                            torch.zeros(new_shape, dtype=torch.float32,\r\n                                        device=p.grad.device)\r\n\r\n                if group['enable_momentum']:\r\n                    exp_avg = state['exp_avg']\r\n\r\n                if is_matrix and group['enable_factorization']:\r\n                    exp_avg_sq_r = state['exp_avg_sq_R']\r\n                    exp_avg_sq_c = state['exp_avg_sq_C']\r\n                else:\r\n                    exp_avg_sq = state['exp_avg_sq']\r\n\r\n                if group['ams_grad']:\r\n                    exp_avg_sq_hat = state['exp_avg_sq_hat']\r\n\r\n                state['step'] += 1\r\n                lr_t = group['lr']\r\n                lr_t *= max(group['eps2'], self._rms(p.data))\r\n\r\n                if group['enable_momentum']:\r\n                    if group['non_constant_decay']:\r\n                        beta1_t = group['beta1'] * \\\r\n                                  (1 - group['beta1'] ** (state['step'] - 1)) \\\r\n                                  / (1 - group['beta1'] ** state['step'])\r\n                    else:\r\n                        beta1_t = group['beta1']\r\n                    exp_avg.mul_(beta1_t).add_(1 - beta1_t, grad)\r\n\r\n                if group['non_constant_decay']:\r\n                    beta2_t = group['beta2'] * \\\r\n                              (1 - group['beta2'] ** (state['step'] - 1)) / \\\r\n                              (1 - group['beta2'] ** state['step'])\r\n                else:\r\n                    beta2_t = group['beta2']\r\n\r\n                if is_matrix and group['enable_factorization']:\r\n                    exp_avg_sq_r.mul_(beta2_t). \\\r\n                        add_(1 - beta2_t, torch.sum(torch.mul(grad, grad).\r\n                                                    add_(group['eps1']),\r\n                                                    dim=0, keepdim=True))\r\n                    exp_avg_sq_c.mul_(beta2_t). \\\r\n                        add_(1 - beta2_t, torch.sum(torch.mul(grad, grad).\r\n                                                    add_(group['eps1']),\r\n                                                    dim=1, keepdim=True))\r\n                    v = torch.mul(exp_avg_sq_c,\r\n                                  exp_avg_sq_r).div_(torch.sum(exp_avg_sq_r))\r\n                else:\r\n                    exp_avg_sq.mul_(beta2_t). \\\r\n                        addcmul_(1 - beta2_t, grad, grad). \\\r\n                        add_((1 - beta2_t) * group['eps1'])\r\n                    v = exp_avg_sq\r\n                g = grad\r\n                if group['enable_momentum']:\r\n                    g = torch.div(exp_avg, 1 - beta1_t ** state['step'])\r\n                if group['ams_grad']:\r\n                    torch.max(exp_avg_sq_hat, v, out=exp_avg_sq_hat)\r\n                    v = exp_avg_sq_hat\r\n                    u = torch.div(g, (torch.div(v, 1 - beta2_t **\r\n                                                state['step'])).sqrt().add_(group['eps1']))\r\n                else:\r\n                    u = torch.div(g, v.sqrt())\r\n                u.div_(max(1, self._rms(u) / group['cliping_threshold']))\r\n                p.data.add_(-lr_t * (u.view(old_shape) if is_need_reshape and\r\n                                                          group['enable_factorization'] else u))\r\n                if group['weight_decay'] != 0:\r\n                    p.data.add_(-group['weight_decay'] * lr_t, p.data)\r\n        return loss\r\n"""
callback/optimization/adamw.py,3,"b'import torch\r\nimport math\r\nfrom torch.optim.optimizer import Optimizer\r\n\r\nclass AdamW(Optimizer):\r\n    """""" Implements Adam algorithm with weight decay fix.\r\n\r\n    Parameters:\r\n        lr (float): learning rate. Default 1e-3.\r\n        betas (tuple of 2 floats): Adams beta parameters (b1, b2). Default: (0.9, 0.999)\r\n        eps (float): Adams epsilon. Default: 1e-6\r\n        weight_decay (float): Weight decay. Default: 0.0\r\n        correct_bias (bool): can be set to False to avoid correcting bias in Adam (e.g. like in Bert TF repository). Default True.\r\n    Example:\r\n        >>> model = LSTM()\r\n        >>> optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\r\n    """"""\r\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6, weight_decay=0.0, correct_bias=True):\r\n        if lr < 0.0:\r\n            raise ValueError(""Invalid learning rate: {} - should be >= 0.0"".format(lr))\r\n        if not 0.0 <= betas[0] < 1.0:\r\n            raise ValueError(""Invalid beta parameter: {} - should be in [0.0, 1.0["".format(betas[0]))\r\n        if not 0.0 <= betas[1]  < 1.0:\r\n            raise ValueError(""Invalid beta parameter: {} - should be in [0.0, 1.0["".format(betas[1]))\r\n        if not 0.0 <= eps:\r\n            raise ValueError(""Invalid epsilon value: {} - should be >= 0.0"".format(eps))\r\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay,\r\n                        correct_bias=correct_bias)\r\n        super(AdamW, self).__init__(params, defaults)\r\n\r\n    def step(self, closure=None):\r\n        """"""Performs a single optimization step.\r\n\r\n        Arguments:\r\n            closure (callable, optional): A closure that reevaluates the model\r\n                and returns the loss.\r\n        """"""\r\n        loss = None\r\n        if closure is not None:\r\n            loss = closure()\r\n\r\n        for group in self.param_groups:\r\n            for p in group[\'params\']:\r\n                if p.grad is None:\r\n                    continue\r\n                grad = p.grad.data\r\n                if grad.is_sparse:\r\n                    raise RuntimeError(\'Adam does not support sparse gradients, please consider SparseAdam instead\')\r\n\r\n                state = self.state[p]\r\n\r\n                # State initialization\r\n                if len(state) == 0:\r\n                    state[\'step\'] = 0\r\n                    # Exponential moving average of gradient values\r\n                    state[\'exp_avg\'] = torch.zeros_like(p.data)\r\n                    # Exponential moving average of squared gradient values\r\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\r\n\r\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\r\n                beta1, beta2 = group[\'betas\']\r\n\r\n                state[\'step\'] += 1\r\n\r\n                # Decay the first and second moment running average coefficient\r\n                # In-place operations to update the averages at the same time\r\n                exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\r\n                exp_avg_sq.mul_(beta2).addcmul_(1.0 - beta2, grad, grad)\r\n                denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\r\n\r\n                step_size = group[\'lr\']\r\n                if group[\'correct_bias\']:  # No bias correction for Bert\r\n                    bias_correction1 = 1.0 - beta1 ** state[\'step\']\r\n                    bias_correction2 = 1.0 - beta2 ** state[\'step\']\r\n                    step_size = step_size * math.sqrt(bias_correction2) / bias_correction1\r\n\r\n                p.data.addcdiv_(-step_size, exp_avg, denom)\r\n\r\n                # Just adding the square of the weights to the loss function is *not*\r\n                # the correct way of using L2 regularization/weight decay with Adam,\r\n                # since that will interact with the m and v parameters in strange ways.\r\n                #\r\n                # Instead we want to decay the weights in a manner that doesn\'t interact\r\n                # with the m/v parameters. This is equivalent to adding the square\r\n                # of the weights to the loss with plain (non-momentum) SGD.\r\n                # Add weight decay at the end (fixed version)\r\n                if group[\'weight_decay\'] > 0.0:\r\n                    p.data.add_(-group[\'lr\'] * group[\'weight_decay\'], p.data)\r\n\r\n        return loss\r\n'"
callback/optimization/lamb.py,3,"b'import torch\r\nfrom torch.optim.optimizer import Optimizer\r\n\r\n\r\nclass Lamb(Optimizer):\r\n    r""""""Implements Lamb algorithm.\r\n    It has been proposed in `Large Batch Optimization for Deep Learning: Training BERT in 76 minutes`_.\r\n    Arguments:\r\n        params (iterable): iterable of parameters to optimize or dicts defining\r\n            parameter groups\r\n        lr (float, optional): learning rate (default: 1e-3)\r\n        betas (Tuple[float, float], optional): coefficients used for computing\r\n            running averages of gradient and its square (default: (0.9, 0.999))\r\n        eps (float, optional): term added to the denominator to improve\r\n            numerical stability (default: 1e-8)\r\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\r\n        adam (bool, optional): always use trust ratio = 1, which turns this into\r\n            Adam. Useful for comparison purposes.\r\n    .. _Large Batch Optimization for Deep Learning: Training BERT in 76 minutes:\r\n        https://arxiv.org/abs/1904.00962\r\n    Example:\r\n        >>> model = ResNet()\r\n        >>> optimizer = Lamb(model.parameters(), lr=1e-2, weight_decay=1e-5)\r\n    """"""\r\n\r\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-6,\r\n                 weight_decay=0, adam=False):\r\n        if not 0.0 <= lr:\r\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\r\n        if not 0.0 <= eps:\r\n            raise ValueError(""Invalid epsilon value: {}"".format(eps))\r\n        if not 0.0 <= betas[0] < 1.0:\r\n            raise ValueError(""Invalid beta parameter at index 0: {}"".format(betas[0]))\r\n        if not 0.0 <= betas[1] < 1.0:\r\n            raise ValueError(""Invalid beta parameter at index 1: {}"".format(betas[1]))\r\n        defaults = dict(lr=lr, betas=betas, eps=eps,\r\n                        weight_decay=weight_decay)\r\n        self.adam = adam\r\n        super(Lamb, self).__init__(params, defaults)\r\n\r\n    def step(self, closure=None):\r\n        """"""Performs a single optimization step.\r\n        Arguments:\r\n            closure (callable, optional): A closure that reevaluates the model\r\n                and returns the loss.\r\n        """"""\r\n        loss = None\r\n        if closure is not None:\r\n            loss = closure()\r\n\r\n        for group in self.param_groups:\r\n            for p in group[\'params\']:\r\n                if p.grad is None:\r\n                    continue\r\n                grad = p.grad.data\r\n                if grad.is_sparse:\r\n                    raise RuntimeError(\'Lamb does not support sparse gradients, consider SparseAdam instad.\')\r\n\r\n                state = self.state[p]\r\n\r\n                # State initialization\r\n                if len(state) == 0:\r\n                    state[\'step\'] = 0\r\n                    # Exponential moving average of gradient values\r\n                    state[\'exp_avg\'] = torch.zeros_like(p.data)\r\n                    # Exponential moving average of squared gradient values\r\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\r\n\r\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\r\n                beta1, beta2 = group[\'betas\']\r\n\r\n                state[\'step\'] += 1\r\n\r\n                # Decay the first and second moment running average coefficient\r\n                # m_t\r\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n                # v_t\r\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n\r\n                # Paper v3 does not use debiasing.\r\n                # bias_correction1 = 1 - beta1 ** state[\'step\']\r\n                # bias_correction2 = 1 - beta2 ** state[\'step\']\r\n                # Apply bias to lr to avoid broadcast.\r\n                step_size = group[\'lr\'] # * math.sqrt(bias_correction2) / bias_correction1\r\n\r\n                weight_norm = p.data.pow(2).sum().sqrt().clamp(0, 10)\r\n\r\n                adam_step = exp_avg / exp_avg_sq.sqrt().add(group[\'eps\'])\r\n                if group[\'weight_decay\'] != 0:\r\n                    adam_step.add_(group[\'weight_decay\'], p.data)\r\n\r\n                adam_norm = adam_step.pow(2).sum().sqrt()\r\n                if weight_norm == 0 or adam_norm == 0:\r\n                    trust_ratio = 1\r\n                else:\r\n                    trust_ratio = weight_norm / adam_norm\r\n                state[\'weight_norm\'] = weight_norm\r\n                state[\'adam_norm\'] = adam_norm\r\n                state[\'trust_ratio\'] = trust_ratio\r\n                if self.adam:\r\n                    trust_ratio = 1\r\n\r\n                p.data.add_(-step_size * trust_ratio, adam_step)\r\n\r\n        return loss'"
callback/optimization/lars.py,2,"b'import torch\r\nfrom torch.optim.optimizer import Optimizer\r\n\r\nclass Lars(Optimizer):\r\n    r""""""Implements the LARS optimizer from https://arxiv.org/pdf/1708.03888.pdf\r\n\r\n    Args:\r\n        params (iterable): iterable of parameters to optimize or dicts defining\r\n            parameter groups\r\n        lr (float): learning rate\r\n        momentum (float, optional): momentum factor (default: 0)\r\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\r\n        dampening (float, optional): dampening for momentum (default: 0)\r\n        nesterov (bool, optional): enables Nesterov momentum (default: False)\r\n        scale_clip (tuple, optional): the lower and upper bounds for the weight norm in local LR of LARS\r\n    Example:\r\n        >>> model = ResNet()\r\n        >>> optimizer = Lars(model.parameters(), lr=1e-2, weight_decay=1e-5)\r\n    """"""\r\n\r\n    def __init__(self, params, lr, momentum=0, dampening=0,\r\n                 weight_decay=0, nesterov=False, scale_clip=None):\r\n        if lr < 0.0:\r\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\r\n        if momentum < 0.0:\r\n            raise ValueError(""Invalid momentum value: {}"".format(momentum))\r\n        if weight_decay < 0.0:\r\n            raise ValueError(""Invalid weight_decay value: {}"".format(weight_decay))\r\n\r\n        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\r\n                        weight_decay=weight_decay, nesterov=nesterov)\r\n        if nesterov and (momentum <= 0 or dampening != 0):\r\n            raise ValueError(""Nesterov momentum requires a momentum and zero dampening"")\r\n        super(Lars, self).__init__(params, defaults)\r\n        # LARS arguments\r\n        self.scale_clip = scale_clip\r\n        if self.scale_clip is None:\r\n            self.scale_clip = (0, 10)\r\n\r\n    def __setstate__(self, state):\r\n        super(Lars, self).__setstate__(state)\r\n        for group in self.param_groups:\r\n            group.setdefault(\'nesterov\', False)\r\n\r\n    def step(self, closure=None):\r\n        """"""Performs a single optimization step.\r\n\r\n        Arguments:\r\n            closure (callable, optional): A closure that reevaluates the model\r\n                and returns the loss.\r\n        """"""\r\n        loss = None\r\n        if closure is not None:\r\n            loss = closure()\r\n\r\n        for group in self.param_groups:\r\n            weight_decay = group[\'weight_decay\']\r\n            momentum = group[\'momentum\']\r\n            dampening = group[\'dampening\']\r\n            nesterov = group[\'nesterov\']\r\n\r\n            for p in group[\'params\']:\r\n                if p.grad is None:\r\n                    continue\r\n                d_p = p.grad.data\r\n                if weight_decay != 0:\r\n                    d_p.add_(weight_decay, p.data)\r\n                if momentum != 0:\r\n                    param_state = self.state[p]\r\n                    if \'momentum_buffer\' not in param_state:\r\n                        buf = param_state[\'momentum_buffer\'] = torch.clone(d_p).detach()\r\n                    else:\r\n                        buf = param_state[\'momentum_buffer\']\r\n                        buf.mul_(momentum).add_(1 - dampening, d_p)\r\n                    if nesterov:\r\n                        d_p = d_p.add(momentum, buf)\r\n                    else:\r\n                        d_p = buf\r\n\r\n                # LARS\r\n                p_norm = p.data.pow(2).sum().sqrt()\r\n                update_norm = d_p.pow(2).sum().sqrt()\r\n                # Compute the local LR\r\n                if p_norm == 0 or update_norm == 0:\r\n                    local_lr = 1\r\n                else:\r\n                    local_lr = p_norm / update_norm\r\n\r\n                p.data.add_(-group[\'lr\'] * local_lr, d_p)\r\n\r\n        return loss'"
callback/optimization/lookahead.py,4,"b'import torch\r\nfrom torch.optim import Optimizer\r\nfrom collections import defaultdict\r\n\r\nclass Lookahead(Optimizer):\r\n    \'\'\'\r\n    PyTorch implementation of the lookahead wrapper.\r\n    Lookahead Optimizer: https://arxiv.org/abs/1907.08610\r\n\r\n    We found that evaluation performance is typically better using the slow weights.\r\n    This can be done in PyTorch with something like this in your eval loop:\r\n    if args.lookahead:\r\n        optimizer._backup_and_load_cache()\r\n        val_loss = eval_func(model)\r\n        optimizer._clear_and_load_backup()\r\n    \'\'\'\r\n    def __init__(self, optimizer,alpha=0.5, k=6,pullback_momentum=""none""):\r\n        \'\'\'\r\n        :param optimizer:inner optimizer\r\n        :param k (int): number of lookahead steps\r\n        :param alpha(float): linear interpolation factor. 1.0 recovers the inner optimizer.\r\n        :param pullback_momentum (str): change to inner optimizer momentum on interpolation update\r\n        \'\'\'\r\n        if not 0.0 <= alpha <= 1.0:\r\n            raise ValueError(f\'Invalid slow update rate: {alpha}\')\r\n        if not 1 <= k:\r\n            raise ValueError(f\'Invalid lookahead steps: {k}\')\r\n        self.optimizer = optimizer\r\n        self.param_groups = self.optimizer.param_groups\r\n        self.alpha = alpha\r\n        self.k = k\r\n        self.step_counter = 0\r\n        assert pullback_momentum in [""reset"", ""pullback"", ""none""]\r\n        self.pullback_momentum = pullback_momentum\r\n        self.state = defaultdict(dict)\r\n\r\n        # Cache the current optimizer parameters\r\n        for group in self.optimizer.param_groups:\r\n            for p in group[\'params\']:\r\n                param_state = self.state[p]\r\n                param_state[\'cached_params\'] = torch.zeros_like(p.data)\r\n                param_state[\'cached_params\'].copy_(p.data)\r\n\r\n    def __getstate__(self):\r\n        return {\r\n            \'state\': self.state,\r\n            \'optimizer\': self.optimizer,\r\n            \'alpha\': self.alpha,\r\n            \'step_counter\': self.step_counter,\r\n            \'k\':self.k,\r\n            \'pullback_momentum\': self.pullback_momentum\r\n        }\r\n\r\n    def zero_grad(self):\r\n        self.optimizer.zero_grad()\r\n\r\n    def state_dict(self):\r\n        return self.optimizer.state_dict()\r\n\r\n    def load_state_dict(self, state_dict):\r\n        self.optimizer.load_state_dict(state_dict)\r\n\r\n    def _backup_and_load_cache(self):\r\n        """"""Useful for performing evaluation on the slow weights (which typically generalize better)\r\n        """"""\r\n        for group in self.optimizer.param_groups:\r\n            for p in group[\'params\']:\r\n                param_state = self.state[p]\r\n                param_state[\'backup_params\'] = torch.zeros_like(p.data)\r\n                param_state[\'backup_params\'].copy_(p.data)\r\n                p.data.copy_(param_state[\'cached_params\'])\r\n\r\n    def _clear_and_load_backup(self):\r\n        for group in self.optimizer.param_groups:\r\n            for p in group[\'params\']:\r\n                param_state = self.state[p]\r\n                p.data.copy_(param_state[\'backup_params\'])\r\n                del param_state[\'backup_params\']\r\n\r\n    def step(self, closure=None):\r\n        """"""Performs a single Lookahead optimization step.\r\n        Arguments:\r\n            closure (callable, optional): A closure that reevaluates the model\r\n                and returns the loss.\r\n        """"""\r\n        loss = self.optimizer.step(closure)\r\n        self.step_counter += 1\r\n\r\n        if self.step_counter >= self.k:\r\n            self.step_counter = 0\r\n            # Lookahead and cache the current optimizer parameters\r\n            for group in self.optimizer.param_groups:\r\n                for p in group[\'params\']:\r\n                    param_state = self.state[p]\r\n                    p.data.mul_(self.alpha).add_(1.0 - self.alpha, param_state[\'cached_params\'])  # crucial line\r\n                    param_state[\'cached_params\'].copy_(p.data)\r\n                    if self.pullback_momentum == ""pullback"":\r\n                        internal_momentum = self.optimizer.state[p][""momentum_buffer""]\r\n                        self.optimizer.state[p][""momentum_buffer""] = internal_momentum.mul_(self.alpha).add_(\r\n                            1.0 - self.alpha, param_state[""cached_mom""])\r\n                        param_state[""cached_mom""] = self.optimizer.state[p][""momentum_buffer""]\r\n                    elif self.pullback_momentum == ""reset"":\r\n                        self.optimizer.state[p][""momentum_buffer""] = torch.zeros_like(p.data)\r\n\r\n        return loss\r\n'"
callback/optimization/nadam.py,1,"b'import torch\r\nimport math\r\nfrom torch.optim.optimizer import Optimizer\r\n\r\nclass Nadam(Optimizer):\r\n    """"""Implements Nadam algorithm (a variant of Adam based on Nesterov momentum).\r\n\r\n    It has been proposed in `Incorporating Nesterov Momentum into Adam`__.\r\n\r\n    Arguments:\r\n        params (iterable): iterable of parameters to optimize or dicts defining\r\n            parameter groups\r\n        lr (float, optional): learning rate (default: 2e-3)\r\n        betas (Tuple[float, float], optional): coefficients used for computing\r\n            running averages of gradient and its square\r\n        eps (float, optional): term added to the denominator to improve\r\n            numerical stability (default: 1e-8)\r\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\r\n        schedule_decay (float, optional): momentum schedule decay (default: 4e-3)\r\n\r\n    __ http://cs229.stanford.edu/proj2015/054_report.pdf\r\n    __ http://www.cs.toronto.edu/~fritz/absps/momentum.pdf\r\n\r\n        Originally taken from: https://github.com/pytorch/pytorch/pull/1408\r\n        NOTE: Has potential issues but does work well on some problems.\r\n    Example:\r\n        >>> model = LSTM()\r\n        >>> optimizer = Nadam(model.parameters())\r\n    """"""\r\n\r\n    def __init__(self, params, lr=2e-3, betas=(0.9, 0.999), eps=1e-8,\r\n                 weight_decay=0, schedule_decay=4e-3):\r\n        defaults = dict(lr=lr, betas=betas, eps=eps,\r\n                        weight_decay=weight_decay, schedule_decay=schedule_decay)\r\n        super(Nadam, self).__init__(params, defaults)\r\n\r\n    def step(self, closure=None):\r\n        """"""Performs a single optimization step.\r\n\r\n        Arguments:\r\n            closure (callable, optional): A closure that reevaluates the model\r\n                and returns the loss.\r\n        """"""\r\n        loss = None\r\n        if closure is not None:\r\n            loss = closure()\r\n\r\n        for group in self.param_groups:\r\n            for p in group[\'params\']:\r\n                if p.grad is None:\r\n                    continue\r\n                grad = p.grad.data\r\n                state = self.state[p]\r\n\r\n                # State initialization\r\n                if len(state) == 0:\r\n                    state[\'step\'] = 0\r\n                    state[\'m_schedule\'] = 1.\r\n                    state[\'exp_avg\'] = grad.new().resize_as_(grad).zero_()\r\n                    state[\'exp_avg_sq\'] = grad.new().resize_as_(grad).zero_()\r\n\r\n                # Warming momentum schedule\r\n                m_schedule = state[\'m_schedule\']\r\n                schedule_decay = group[\'schedule_decay\']\r\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\r\n                beta1, beta2 = group[\'betas\']\r\n                eps = group[\'eps\']\r\n                state[\'step\'] += 1\r\n                t = state[\'step\']\r\n\r\n                if group[\'weight_decay\'] != 0:\r\n                    grad = grad.add(group[\'weight_decay\'], p.data)\r\n\r\n                momentum_cache_t = beta1 * \\\r\n                    (1. - 0.5 * (0.96 ** (t * schedule_decay)))\r\n                momentum_cache_t_1 = beta1 * \\\r\n                    (1. - 0.5 * (0.96 ** ((t + 1) * schedule_decay)))\r\n                m_schedule_new = m_schedule * momentum_cache_t\r\n                m_schedule_next = m_schedule * momentum_cache_t * momentum_cache_t_1\r\n                state[\'m_schedule\'] = m_schedule_new\r\n\r\n                # Decay the first and second moment running average coefficient\r\n                exp_avg.mul_(beta1).add_(1. - beta1, grad)\r\n                exp_avg_sq.mul_(beta2).addcmul_(1. - beta2, grad, grad)\r\n                exp_avg_sq_prime = exp_avg_sq / (1. - beta2 ** t)\r\n                denom = exp_avg_sq_prime.sqrt_().add_(eps)\r\n\r\n                p.data.addcdiv_(-group[\'lr\'] * (1. - momentum_cache_t) / (1. - m_schedule_new), grad, denom)\r\n                p.data.addcdiv_(-group[\'lr\'] * momentum_cache_t_1 / (1. - m_schedule_next), exp_avg, denom)\r\n\r\n        return loss'"
callback/optimization/novograd.py,3,"b'import torch\r\nimport math\r\nfrom torch.optim.optimizer import Optimizer\r\n\r\n\r\nclass NovoGrad(Optimizer):\r\n    """"""Implements NovoGrad algorithm.\r\n    Arguments:\r\n        params (iterable): iterable of parameters to optimize or dicts defining\r\n            parameter groups\r\n        lr (float, optional): learning rate (default: 1e-2)\r\n        betas (Tuple[float, float], optional): coefficients used for computing\r\n            running averages of gradient and its square (default: (0.95, 0.98))\r\n        eps (float, optional): term added to the denominator to improve\r\n            numerical stability (default: 1e-8)\r\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\r\n    Example:\r\n        >>> model = ResNet()\r\n        >>> optimizer = NovoGrad(model.parameters(), lr=1e-2, weight_decay=1e-5)\r\n    """"""\r\n\r\n    def __init__(self, params, lr=0.01, betas=(0.95, 0.98), eps=1e-8,\r\n                 weight_decay=0, grad_averaging=False):\r\n        if lr < 0.0:\r\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\r\n        if not 0.0 <= betas[0] < 1.0:\r\n            raise ValueError(""Invalid beta parameter at index 0: {}"".format(betas[0]))\r\n        if not 0.0 <= betas[1] < 1.0:\r\n            raise ValueError(""Invalid beta parameter at index 1: {}"".format(betas[1]))\r\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, grad_averaging=grad_averaging)\r\n        super().__init__(params, defaults)\r\n\r\n    def step(self, closure=None):\r\n        loss = None\r\n        if closure is not None:\r\n            loss = closure()\r\n        for group in self.param_groups:\r\n            for p in group[\'params\']:\r\n                if p.grad is None:\r\n                    continue\r\n                grad = p.grad.data\r\n                if grad.is_sparse:\r\n                    raise RuntimeError(\'NovoGrad does not support sparse gradients\')\r\n                state = self.state[p]\r\n                g_2 = torch.sum(grad ** 2)\r\n                if len(state) == 0:\r\n                    state[\'step\'] = 0\r\n                    state[\'moments\'] = grad.div(g_2.sqrt() + group[\'eps\']) + \\\r\n                                       group[\'weight_decay\'] * p.data\r\n                    state[\'grads_ema\'] = g_2\r\n                moments = state[\'moments\']\r\n                grads_ema = state[\'grads_ema\']\r\n                beta1, beta2 = group[\'betas\']\r\n                state[\'step\'] += 1\r\n                grads_ema.mul_(beta2).add_(1 - beta2, g_2)\r\n\r\n                denom = grads_ema.sqrt().add_(group[\'eps\'])\r\n                grad.div_(denom)\r\n                # weight decay\r\n                if group[\'weight_decay\'] != 0:\r\n                    decayed_weights = torch.mul(p.data, group[\'weight_decay\'])\r\n                    grad.add_(decayed_weights)\r\n\r\n                # Momentum --> SAG\r\n                if group[\'grad_averaging\']:\r\n                    grad.mul_(1.0 - beta1)\r\n\r\n                moments.mul_(beta1).add_(grad)  # velocity\r\n\r\n                bias_correction1 = 1 - beta1 ** state[\'step\']\r\n                bias_correction2 = 1 - beta2 ** state[\'step\']\r\n                step_size = group[\'lr\'] * math.sqrt(bias_correction2) / bias_correction1\r\n                p.data.add_(-step_size, moments)\r\n\r\n        return loss\r\n'"
callback/optimization/planradam.py,3,"b""import torch\r\nimport math\r\nfrom torch.optim.optimizer import Optimizer\r\nclass PlainRAdam(Optimizer):\r\n\r\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\r\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\r\n\r\n        super(PlainRAdam, self).__init__(params, defaults)\r\n\r\n    def __setstate__(self, state):\r\n        super(PlainRAdam, self).__setstate__(state)\r\n\r\n    def step(self, closure=None):\r\n\r\n        loss = None\r\n        if closure is not None:\r\n            loss = closure()\r\n\r\n        for group in self.param_groups:\r\n\r\n            for p in group['params']:\r\n                if p.grad is None:\r\n                    continue\r\n                grad = p.grad.data.float()\r\n                if grad.is_sparse:\r\n                    raise RuntimeError('RAdam does not support sparse gradients')\r\n\r\n                p_data_fp32 = p.data.float()\r\n\r\n                state = self.state[p]\r\n\r\n                if len(state) == 0:\r\n                    state['step'] = 0\r\n                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\r\n                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\r\n                else:\r\n                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\r\n                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\r\n\r\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\r\n                beta1, beta2 = group['betas']\r\n\r\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n\r\n                state['step'] += 1\r\n                beta2_t = beta2 ** state['step']\r\n                N_sma_max = 2 / (1 - beta2) - 1\r\n                N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\r\n\r\n                if group['weight_decay'] != 0:\r\n                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\r\n\r\n                # more conservative since it's an approximated value\r\n                if N_sma >= 5:\r\n                    step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\r\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\r\n                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\r\n                else:\r\n                    step_size = group['lr'] / (1 - beta1 ** state['step'])\r\n                    p_data_fp32.add_(-step_size, exp_avg)\r\n\r\n                p.data.copy_(p_data_fp32)\r\n\r\n        return loss"""
callback/optimization/radam.py,3,"b'import torch\r\nimport math\r\nfrom torch.optim.optimizer import Optimizer\r\nclass RAdam(Optimizer):\r\n    """"""Implements the RAdam optimizer from https://arxiv.org/pdf/1908.03265.pdf\r\n    Args:\r\n        params (iterable): iterable of parameters to optimize or dicts defining parameter groups\r\n        lr (float, optional): learning rate\r\n        betas (Tuple[float, float], optional): coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999))\r\n        eps (float, optional): term added to the denominator to improve numerical stability (default: 1e-8)\r\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\r\n    Example:\r\n        >>> model = ResNet()\r\n        >>> optimizer = RAdam(model.parameters(), lr=0.001)\r\n    """"""\r\n\r\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\r\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\r\n        self.buffer = [[None, None, None] for ind in range(10)]\r\n        super(RAdam, self).__init__(params, defaults)\r\n\r\n    def __setstate__(self, state):\r\n        super(RAdam, self).__setstate__(state)\r\n\r\n    def step(self, closure=None):\r\n\r\n        loss = None\r\n        if closure is not None:\r\n            loss = closure()\r\n\r\n        for group in self.param_groups:\r\n\r\n            for p in group[\'params\']:\r\n                if p.grad is None:\r\n                    continue\r\n                grad = p.grad.data.float()\r\n                if grad.is_sparse:\r\n                    raise RuntimeError(\'RAdam does not support sparse gradients\')\r\n\r\n                p_data_fp32 = p.data.float()\r\n\r\n                state = self.state[p]\r\n\r\n                if len(state) == 0:\r\n                    state[\'step\'] = 0\r\n                    state[\'exp_avg\'] = torch.zeros_like(p_data_fp32)\r\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p_data_fp32)\r\n                else:\r\n                    state[\'exp_avg\'] = state[\'exp_avg\'].type_as(p_data_fp32)\r\n                    state[\'exp_avg_sq\'] = state[\'exp_avg_sq\'].type_as(p_data_fp32)\r\n\r\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\r\n                beta1, beta2 = group[\'betas\']\r\n\r\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n\r\n                state[\'step\'] += 1\r\n                buffered = self.buffer[int(state[\'step\'] % 10)]\r\n                if state[\'step\'] == buffered[0]:\r\n                    N_sma, step_size = buffered[1], buffered[2]\r\n                else:\r\n                    buffered[0] = state[\'step\']\r\n                    beta2_t = beta2 ** state[\'step\']\r\n                    N_sma_max = 2 / (1 - beta2) - 1\r\n                    N_sma = N_sma_max - 2 * state[\'step\'] * beta2_t / (1 - beta2_t)\r\n                    buffered[1] = N_sma\r\n\r\n                    # more conservative since it\'s an approximated value\r\n                    if N_sma >= 5:\r\n                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state[\'step\'])\r\n                    else:\r\n                        step_size = 1.0 / (1 - beta1 ** state[\'step\'])\r\n                    buffered[2] = step_size\r\n\r\n                if group[\'weight_decay\'] != 0:\r\n                    p_data_fp32.add_(-group[\'weight_decay\'] * group[\'lr\'], p_data_fp32)\r\n\r\n                # more conservative since it\'s an approximated value\r\n                if N_sma >= 5:\r\n                    denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\r\n                    p_data_fp32.addcdiv_(-step_size * group[\'lr\'], exp_avg, denom)\r\n                else:\r\n                    p_data_fp32.add_(-step_size * group[\'lr\'], exp_avg)\r\n\r\n                p.data.copy_(p_data_fp32)\r\n\r\n        return loss'"
callback/optimization/ralamb.py,3,"b""import math\r\nimport torch\r\nfrom torch.optim.optimizer import Optimizer\r\n\r\nclass Ralamb(Optimizer):\r\n    '''\r\n    RAdam + LARS\r\n    Example:\r\n        >>> model = ResNet()\r\n        >>> optimizer = Ralamb(model.parameters(), lr=0.001)\r\n    '''\r\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\r\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\r\n        self.buffer = [[None, None, None] for ind in range(10)]\r\n        super(Ralamb, self).__init__(params, defaults)\r\n\r\n    def __setstate__(self, state):\r\n        super(Ralamb, self).__setstate__(state)\r\n\r\n    def step(self, closure=None):\r\n\r\n        loss = None\r\n        if closure is not None:\r\n            loss = closure()\r\n\r\n        for group in self.param_groups:\r\n\r\n            for p in group['params']:\r\n                if p.grad is None:\r\n                    continue\r\n                grad = p.grad.data.float()\r\n                if grad.is_sparse:\r\n                    raise RuntimeError('Ralamb does not support sparse gradients')\r\n\r\n                p_data_fp32 = p.data.float()\r\n\r\n                state = self.state[p]\r\n\r\n                if len(state) == 0:\r\n                    state['step'] = 0\r\n                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\r\n                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\r\n                else:\r\n                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\r\n                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\r\n\r\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\r\n                beta1, beta2 = group['betas']\r\n\r\n                # Decay the first and second moment running average coefficient\r\n                # m_t\r\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n                # v_t\r\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n\r\n                state['step'] += 1\r\n                buffered = self.buffer[int(state['step'] % 10)]\r\n\r\n                if state['step'] == buffered[0]:\r\n                    N_sma, radam_step_size = buffered[1], buffered[2]\r\n                else:\r\n                    buffered[0] = state['step']\r\n                    beta2_t = beta2 ** state['step']\r\n                    N_sma_max = 2 / (1 - beta2) - 1\r\n                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\r\n                    buffered[1] = N_sma\r\n\r\n                    # more conservative since it's an approximated value\r\n                    if N_sma >= 5:\r\n                        radam_step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\r\n                    else:\r\n                        radam_step_size = 1.0 / (1 - beta1 ** state['step'])\r\n                    buffered[2] = radam_step_size\r\n\r\n                if group['weight_decay'] != 0:\r\n                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\r\n\r\n                # more conservative since it's an approximated value\r\n                radam_step = p_data_fp32.clone()\r\n                if N_sma >= 5:\r\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\r\n                    radam_step.addcdiv_(-radam_step_size * group['lr'], exp_avg, denom)\r\n                else:\r\n                    radam_step.add_(-radam_step_size * group['lr'], exp_avg)\r\n\r\n                radam_norm = radam_step.pow(2).sum().sqrt()\r\n                weight_norm = p.data.pow(2).sum().sqrt().clamp(0, 10)\r\n                if weight_norm == 0 or radam_norm == 0:\r\n                    trust_ratio = 1\r\n                else:\r\n                    trust_ratio = weight_norm / radam_norm\r\n\r\n                state['weight_norm'] = weight_norm\r\n                state['adam_norm'] = radam_norm\r\n                state['trust_ratio'] = trust_ratio\r\n\r\n                if N_sma >= 5:\r\n                    p_data_fp32.addcdiv_(-radam_step_size * group['lr'] * trust_ratio, exp_avg, denom)\r\n                else:\r\n                    p_data_fp32.add_(-radam_step_size * group['lr'] * trust_ratio, exp_avg)\r\n\r\n                p.data.copy_(p_data_fp32)\r\n\r\n        return loss"""
callback/optimization/ralars.py,4,"b'import math\r\nimport torch\r\nfrom torch.optim.optimizer import Optimizer\r\n\r\n\r\nclass RaLars(Optimizer):\r\n    """"""Implements the RAdam optimizer from https://arxiv.org/pdf/1908.03265.pdf\r\n    with optional Layer-wise adaptive Scaling from https://arxiv.org/pdf/1708.03888.pdf\r\n\r\n    Args:\r\n        params (iterable): iterable of parameters to optimize or dicts defining parameter groups\r\n        lr (float, optional): learning rate\r\n        betas (Tuple[float, float], optional): coefficients used for computing running averages of gradient and its square (default: (0.9, 0.999))\r\n        eps (float, optional): term added to the denominator to improve numerical stability (default: 1e-8)\r\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\r\n        scale_clip (float, optional): the maximal upper bound for the scale factor of LARS\r\n    Example:\r\n        >>> model = ResNet()\r\n        >>> optimizer = RaLars(model.parameters(), lr=0.001)\r\n    """"""\r\n\r\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0,\r\n                 scale_clip=None):\r\n        if not 0.0 <= lr:\r\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\r\n        if not 0.0 <= eps:\r\n            raise ValueError(""Invalid epsilon value: {}"".format(eps))\r\n        if not 0.0 <= betas[0] < 1.0:\r\n            raise ValueError(""Invalid beta parameter at index 0: {}"".format(betas[0]))\r\n        if not 0.0 <= betas[1] < 1.0:\r\n            raise ValueError(""Invalid beta parameter at index 1: {}"".format(betas[1]))\r\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\r\n        super(RaLars, self).__init__(params, defaults)\r\n        # LARS arguments\r\n        self.scale_clip = scale_clip\r\n        if self.scale_clip is None:\r\n            self.scale_clip = (0, 10)\r\n\r\n    def step(self, closure=None):\r\n        """"""Performs a single optimization step.\r\n        Arguments:\r\n            closure (callable, optional): A closure that reevaluates the model\r\n                and returns the loss.\r\n        """"""\r\n        loss = None\r\n        if closure is not None:\r\n            loss = closure()\r\n\r\n        for group in self.param_groups:\r\n\r\n            # Get group-shared variables\r\n            beta1, beta2 = group[\'betas\']\r\n            sma_inf = group.get(\'sma_inf\')\r\n            # Compute max length of SMA on first step\r\n            if not isinstance(sma_inf, float):\r\n                group[\'sma_inf\'] = 2 / (1 - beta2) - 1\r\n                sma_inf = group.get(\'sma_inf\')\r\n\r\n            for p in group[\'params\']:\r\n                if p.grad is None:\r\n                    continue\r\n                grad = p.grad.data\r\n                if grad.is_sparse:\r\n                    raise RuntimeError(\'RAdam does not support sparse gradients\')\r\n\r\n                state = self.state[p]\r\n\r\n                # State initialization\r\n                if len(state) == 0:\r\n                    state[\'step\'] = 0\r\n                    # Exponential moving average of gradient values\r\n                    state[\'exp_avg\'] = torch.zeros_like(p.data)\r\n                    # Exponential moving average of squared gradient values\r\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\r\n\r\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\r\n\r\n                state[\'step\'] += 1\r\n\r\n                # Decay the first and second moment running average coefficient\r\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\r\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\r\n\r\n                # Bias correction\r\n                bias_correction1 = 1 - beta1 ** state[\'step\']\r\n                bias_correction2 = 1 - beta2 ** state[\'step\']\r\n\r\n                # Compute length of SMA\r\n                sma_t = sma_inf - 2 * state[\'step\'] * (1 - bias_correction2) / bias_correction2\r\n\r\n                update = torch.zeros_like(p.data)\r\n                if sma_t > 4:\r\n                    # \xc2\xa0Variance rectification term\r\n                    r_t = math.sqrt((sma_t - 4) * (sma_t - 2) * sma_inf / ((sma_inf - 4) * (sma_inf - 2) * sma_t))\r\n                    # \xc2\xa0Adaptive momentum\r\n                    update.addcdiv_(r_t, exp_avg / bias_correction1,\r\n                                    (exp_avg_sq / bias_correction2).sqrt().add_(group[\'eps\']))\r\n                else:\r\n                    # Unadapted momentum\r\n                    update.add_(exp_avg / bias_correction1)\r\n\r\n                # Weight decay\r\n                if group[\'weight_decay\'] != 0:\r\n                    update.add_(group[\'weight_decay\'], p.data)\r\n\r\n                # LARS\r\n                p_norm = p.data.pow(2).sum().sqrt()\r\n                update_norm = update.pow(2).sum().sqrt()\r\n                phi_p = p_norm.clamp(*self.scale_clip)\r\n                # Compute the local LR\r\n                if phi_p == 0 or update_norm == 0:\r\n                    local_lr = 1\r\n                else:\r\n                    local_lr = phi_p / update_norm\r\n\r\n                state[\'local_lr\'] = local_lr\r\n\r\n                p.data.add_(-group[\'lr\'] * local_lr, update)\r\n\r\n        return loss\r\n'"
callback/optimization/sgdw.py,2,"b'import torch\r\nfrom torch.optim.optimizer import Optimizer\r\n\r\nclass SGDW(Optimizer):\r\n    r""""""Implements stochastic gradient descent (optionally with momentum) with\r\n    weight decay from the paper `Fixing Weight Decay Regularization in Adam`_.\r\n\r\n    Nesterov momentum is based on the formula from\r\n    `On the importance of initialization and momentum in deep learning`__.\r\n\r\n    Args:\r\n        params (iterable): iterable of parameters to optimize or dicts defining\r\n            parameter groups\r\n        lr (float): learning rate\r\n        momentum (float, optional): momentum factor (default: 0)\r\n        weight_decay (float, optional): weight decay factor (default: 0)\r\n        dampening (float, optional): dampening for momentum (default: 0)\r\n        nesterov (bool, optional): enables Nesterov momentum (default: False)\r\n\r\n    .. _Fixing Weight Decay Regularization in Adam:\r\n        https://arxiv.org/abs/1711.05101\r\n\r\n    Example:\r\n        >>> model = LSTM()\r\n        >>> optimizer = SGDW(model.parameters(), lr=0.1, momentum=0.9,weight_decay=1e-5)\r\n    """"""\r\n    def __init__(self, params, lr=0.1, momentum=0, dampening=0,\r\n                 weight_decay=0, nesterov=False):\r\n        if lr < 0.0:\r\n            raise ValueError(f""Invalid learning rate: {lr}"")\r\n        if momentum < 0.0:\r\n            raise ValueError(f""Invalid momentum value: {momentum}"")\r\n        if weight_decay < 0.0:\r\n            raise ValueError(f""Invalid weight_decay value: {weight_decay}"")\r\n\r\n        defaults = dict(lr=lr, momentum=momentum, dampening=dampening,\r\n                        weight_decay=weight_decay, nesterov=nesterov)\r\n        if nesterov and (momentum <= 0 or dampening != 0):\r\n            raise ValueError(""Nesterov momentum requires a momentum and zero dampening"")\r\n        super(SGDW, self).__init__(params, defaults)\r\n\r\n    def __setstate__(self, state):\r\n        super(SGDW, self).__setstate__(state)\r\n        for group in self.param_groups:\r\n            group.setdefault(\'nesterov\', False)\r\n\r\n    def step(self, closure=None):\r\n        """"""Performs a single optimization step.\r\n\r\n        Arguments:\r\n            closure (callable, optional): A closure that reevaluates the model\r\n                and returns the loss.\r\n        """"""\r\n        loss = None\r\n        if closure is not None:\r\n            loss = closure()\r\n\r\n        for group in self.param_groups:\r\n            weight_decay = group[\'weight_decay\']\r\n            momentum = group[\'momentum\']\r\n            dampening = group[\'dampening\']\r\n            nesterov = group[\'nesterov\']\r\n            for p in group[\'params\']:\r\n                if p.grad is None:\r\n                    continue\r\n                d_p = p.grad.data\r\n                if momentum != 0:\r\n                    param_state = self.state[p]\r\n                    if \'momentum_buffer\' not in param_state:\r\n                        buf = param_state[\'momentum_buffer\'] = torch.zeros_like(p.data)\r\n                        buf.mul_(momentum).add_(d_p)\r\n                    else:\r\n                        buf = param_state[\'momentum_buffer\']\r\n                        buf.mul_(momentum).add_(1 - dampening, d_p)\r\n                    if nesterov:\r\n                        d_p = d_p.add(momentum, buf)\r\n                    else:\r\n                        d_p = buf\r\n                if weight_decay != 0:\r\n                    p.data.add_(-weight_decay, p.data)\r\n                p.data.add_(-group[\'lr\'], d_p)\r\n        return loss'"
