file_path,api_count,code
__init__.py,0,b''
demo.py,0,"b""import cv2\nimport numpy as np\nfrom faster_rcnn import network\nfrom faster_rcnn.faster_rcnn import FasterRCNN\nfrom faster_rcnn.utils.timer import Timer\n\n\ndef test():\n    import os\n    im_file = 'demo/004545.jpg'\n    # im_file = 'data/VOCdevkit2007/VOC2007/JPEGImages/009036.jpg'\n    # im_file = '/media/longc/Data/data/2DMOT2015/test/ETH-Crossing/img1/000100.jpg'\n    image = cv2.imread(im_file)\n\n    model_file = '/media/longc/Data/models/VGGnet_fast_rcnn_iter_70000.h5'\n    # model_file = '/media/longc/Data/models/faster_rcnn_pytorch3/faster_rcnn_100000.h5'\n    # model_file = '/media/longc/Data/models/faster_rcnn_pytorch2/faster_rcnn_2000.h5'\n    detector = FasterRCNN()\n    network.load_net(model_file, detector)\n    detector.cuda()\n    detector.eval()\n    print('load model successfully!')\n\n    # network.save_net(r'/media/longc/Data/models/VGGnet_fast_rcnn_iter_70000.h5', detector)\n    # print('save model succ')\n\n    t = Timer()\n    t.tic()\n    # image = np.zeros(shape=[600, 800, 3], dtype=np.uint8) + 255\n    dets, scores, classes = detector.detect(image, 0.7)\n    runtime = t.toc()\n    print('total spend: {}s'.format(runtime))\n\n    im2show = np.copy(image)\n    for i, det in enumerate(dets):\n        det = tuple(int(x) for x in det)\n        cv2.rectangle(im2show, det[0:2], det[2:4], (255, 205, 51), 2)\n        cv2.putText(im2show, '%s: %.3f' % (classes[i], scores[i]), (det[0], det[1] + 15), cv2.FONT_HERSHEY_PLAIN,\n                    1.0, (0, 0, 255), thickness=1)\n    cv2.imwrite(os.path.join('demo', 'out.jpg'), im2show)\n    cv2.imshow('demo', im2show)\n    cv2.waitKey(0)\n\n\nif __name__ == '__main__':\n    test()"""
test.py,0,"b'import os\nimport torch\nimport cv2\nimport cPickle\nimport numpy as np\n\nfrom faster_rcnn import network\nfrom faster_rcnn.faster_rcnn import FasterRCNN, RPN\nfrom faster_rcnn.utils.timer import Timer\nfrom faster_rcnn.fast_rcnn.nms_wrapper import nms\n\nfrom faster_rcnn.fast_rcnn.bbox_transform import bbox_transform_inv, clip_boxes\nfrom faster_rcnn.datasets.factory import get_imdb\nfrom faster_rcnn.fast_rcnn.config import cfg, cfg_from_file, get_output_dir\n\n\n# hyper-parameters\n# ------------\nimdb_name = \'voc_2007_test\'\ncfg_file = \'experiments/cfgs/faster_rcnn_end2end.yml\'\n# trained_model = \'/media/longc/Data/models/VGGnet_fast_rcnn_iter_70000.h5\'\ntrained_model = \'models/saved_model3/faster_rcnn_90000.h5\'\n\nrand_seed = 1024\n\nsave_name = \'faster_rcnn_100000\'\nmax_per_image = 300\nthresh = 0.05\nvis = False\n\n# ------------\n\nif rand_seed is not None:\n    np.random.seed(rand_seed)\n\nif rand_seed is not None:\n    np.random.seed(rand_seed)\n\n# load config\ncfg_from_file(cfg_file)\n\n\ndef vis_detections(im, class_name, dets, thresh=0.8):\n    """"""Visual debugging of detections.""""""\n    for i in range(np.minimum(10, dets.shape[0])):\n        bbox = tuple(int(np.round(x)) for x in dets[i, :4])\n        score = dets[i, -1]\n        if score > thresh:\n            cv2.rectangle(im, bbox[0:2], bbox[2:4], (0, 204, 0), 2)\n            cv2.putText(im, \'%s: %.3f\' % (class_name, score), (bbox[0], bbox[1] + 15), cv2.FONT_HERSHEY_PLAIN,\n                        1.0, (0, 0, 255), thickness=1)\n    return im\n\n\ndef im_detect(net, image):\n    """"""Detect object classes in an image given object proposals.\n    Returns:\n        scores (ndarray): R x K array of object class scores (K includes\n            background as object category 0)\n        boxes (ndarray): R x (4*K) array of predicted bounding boxes\n    """"""\n\n    im_data, im_scales = net.get_image_blob(image)\n    im_info = np.array(\n        [[im_data.shape[1], im_data.shape[2], im_scales[0]]],\n        dtype=np.float32)\n\n    cls_prob, bbox_pred, rois = net(im_data, im_info)\n    scores = cls_prob.data.cpu().numpy()\n    boxes = rois.data.cpu().numpy()[:, 1:5] / im_info[0][2]\n\n    if cfg.TEST.BBOX_REG:\n        # Apply bounding-box regression deltas\n        box_deltas = bbox_pred.data.cpu().numpy()\n        pred_boxes = bbox_transform_inv(boxes, box_deltas)\n        pred_boxes = clip_boxes(pred_boxes, image.shape)\n    else:\n        # Simply repeat the boxes, once for each class\n        pred_boxes = np.tile(boxes, (1, scores.shape[1]))\n\n    return scores, pred_boxes\n\n\ndef test_net(name, net, imdb, max_per_image=300, thresh=0.05, vis=False):\n    """"""Test a Fast R-CNN network on an image database.""""""\n    num_images = len(imdb.image_index)\n    # all detections are collected into:\n    #    all_boxes[cls][image] = N x 5 array of detections in\n    #    (x1, y1, x2, y2, score)\n    all_boxes = [[[] for _ in xrange(num_images)]\n                 for _ in xrange(imdb.num_classes)]\n\n    output_dir = get_output_dir(imdb, name)\n\n    # timers\n    _t = {\'im_detect\': Timer(), \'misc\': Timer()}\n    det_file = os.path.join(output_dir, \'detections.pkl\')\n\n    for i in range(num_images):\n\n        im = cv2.imread(imdb.image_path_at(i))\n        _t[\'im_detect\'].tic()\n        scores, boxes = im_detect(net, im)\n        detect_time = _t[\'im_detect\'].toc(average=False)\n\n        _t[\'misc\'].tic()\n        if vis:\n            # im2show = np.copy(im[:, :, (2, 1, 0)])\n            im2show = np.copy(im)\n\n        # skip j = 0, because it\'s the background class\n        for j in xrange(1, imdb.num_classes):\n            inds = np.where(scores[:, j] > thresh)[0]\n            cls_scores = scores[inds, j]\n            cls_boxes = boxes[inds, j * 4:(j + 1) * 4]\n            cls_dets = np.hstack((cls_boxes, cls_scores[:, np.newaxis])) \\\n                .astype(np.float32, copy=False)\n            keep = nms(cls_dets, cfg.TEST.NMS)\n            cls_dets = cls_dets[keep, :]\n            if vis:\n                im2show = vis_detections(im2show, imdb.classes[j], cls_dets)\n            all_boxes[j][i] = cls_dets\n\n        # Limit to max_per_image detections *over all classes*\n        if max_per_image > 0:\n            image_scores = np.hstack([all_boxes[j][i][:, -1]\n                                      for j in xrange(1, imdb.num_classes)])\n            if len(image_scores) > max_per_image:\n                image_thresh = np.sort(image_scores)[-max_per_image]\n                for j in xrange(1, imdb.num_classes):\n                    keep = np.where(all_boxes[j][i][:, -1] >= image_thresh)[0]\n                    all_boxes[j][i] = all_boxes[j][i][keep, :]\n        nms_time = _t[\'misc\'].toc(average=False)\n\n        print \'im_detect: {:d}/{:d} {:.3f}s {:.3f}s\' \\\n            .format(i + 1, num_images, detect_time, nms_time)\n\n        if vis:\n            cv2.imshow(\'test\', im2show)\n            cv2.waitKey(1)\n\n    with open(det_file, \'wb\') as f:\n        cPickle.dump(all_boxes, f, cPickle.HIGHEST_PROTOCOL)\n\n    print \'Evaluating detections\'\n    imdb.evaluate_detections(all_boxes, output_dir)\n\n\nif __name__ == \'__main__\':\n    # load data\n    imdb = get_imdb(imdb_name)\n    imdb.competition_mode(on=True)\n\n    # load net\n    net = FasterRCNN(classes=imdb.classes, debug=False)\n    network.load_net(trained_model, net)\n    print(\'load model successfully!\')\n\n    net.cuda()\n    net.eval()\n\n    # evaluation\n    test_net(save_name, net, imdb, max_per_image, thresh=thresh, vis=vis)\n'"
train.py,3,"b""import os\nimport torch\nimport numpy as np\nfrom datetime import datetime\n\nfrom faster_rcnn import network\nfrom faster_rcnn.faster_rcnn import FasterRCNN, RPN\nfrom faster_rcnn.utils.timer import Timer\n\nimport faster_rcnn.roi_data_layer.roidb as rdl_roidb\nfrom faster_rcnn.roi_data_layer.layer import RoIDataLayer\nfrom faster_rcnn.datasets.factory import get_imdb\nfrom faster_rcnn.fast_rcnn.config import cfg, cfg_from_file\n\ntry:\n    from termcolor import cprint\nexcept ImportError:\n    cprint = None\n\ntry:\n    from pycrayon import CrayonClient\nexcept ImportError:\n    CrayonClient = None\n\n\ndef log_print(text, color=None, on_color=None, attrs=None):\n    if cprint is not None:\n        cprint(text, color=color, on_color=on_color, attrs=attrs)\n    else:\n        print(text)\n\n\n\n# hyper-parameters\n# ------------\nimdb_name = 'voc_2007_trainval'\ncfg_file = 'experiments/cfgs/faster_rcnn_end2end.yml'\npretrained_model = 'data/pretrained_model/VGG_imagenet.npy'\noutput_dir = 'models/saved_model3'\n\nstart_step = 0\nend_step = 100000\nlr_decay_steps = {60000, 80000}\nlr_decay = 1./10\n\nrand_seed = 1024\n_DEBUG = True\nuse_tensorboard = True\nremove_all_log = False   # remove all historical experiments in TensorBoard\nexp_name = None # the previous experiment name in TensorBoard\n\n# ------------\n\nif rand_seed is not None:\n    np.random.seed(rand_seed)\n\n# load config\ncfg_from_file(cfg_file)\nlr = cfg.TRAIN.LEARNING_RATE\nmomentum = cfg.TRAIN.MOMENTUM\nweight_decay = cfg.TRAIN.WEIGHT_DECAY\ndisp_interval = cfg.TRAIN.DISPLAY\nlog_interval = cfg.TRAIN.LOG_IMAGE_ITERS\n\n# load data\nimdb = get_imdb(imdb_name)\nrdl_roidb.prepare_roidb(imdb)\nroidb = imdb.roidb\ndata_layer = RoIDataLayer(roidb, imdb.num_classes)\n\n# load net\nnet = FasterRCNN(classes=imdb.classes, debug=_DEBUG)\nnetwork.weights_normal_init(net, dev=0.01)\nnetwork.load_pretrained_npy(net, pretrained_model)\n# model_file = '/media/longc/Data/models/VGGnet_fast_rcnn_iter_70000.h5'\n# model_file = 'models/saved_model3/faster_rcnn_60000.h5'\n# network.load_net(model_file, net)\n# exp_name = 'vgg16_02-19_13-24'\n# start_step = 60001\n# lr /= 10.\n# network.weights_normal_init([net.bbox_fc, net.score_fc, net.fc6, net.fc7], dev=0.01)\n\nnet.cuda()\nnet.train()\n\nparams = list(net.parameters())\n# optimizer = torch.optim.Adam(params[-8:], lr=lr)\noptimizer = torch.optim.SGD(params[8:], lr=lr, momentum=momentum, weight_decay=weight_decay)\n\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\n# tensorboad\nuse_tensorboard = use_tensorboard and CrayonClient is not None\nif use_tensorboard:\n    cc = CrayonClient(hostname='127.0.0.1')\n    if remove_all_log:\n        cc.remove_all_experiments()\n    if exp_name is None:\n        exp_name = datetime.now().strftime('vgg16_%m-%d_%H-%M')\n        exp = cc.create_experiment(exp_name)\n    else:\n        exp = cc.open_experiment(exp_name)\n\n# training\ntrain_loss = 0\ntp, tf, fg, bg = 0., 0., 0, 0\nstep_cnt = 0\nre_cnt = False\nt = Timer()\nt.tic()\nfor step in range(start_step, end_step+1):\n\n    # get one batch\n    blobs = data_layer.forward()\n    im_data = blobs['data']\n    im_info = blobs['im_info']\n    gt_boxes = blobs['gt_boxes']\n    gt_ishard = blobs['gt_ishard']\n    dontcare_areas = blobs['dontcare_areas']\n\n    # forward\n    net(im_data, im_info, gt_boxes, gt_ishard, dontcare_areas)\n    loss = net.loss + net.rpn.loss\n\n    if _DEBUG:\n        tp += float(net.tp)\n        tf += float(net.tf)\n        fg += net.fg_cnt\n        bg += net.bg_cnt\n\n    train_loss += loss.data[0]\n    step_cnt += 1\n\n    # backward\n    optimizer.zero_grad()\n    loss.backward()\n    network.clip_gradient(net, 10.)\n    optimizer.step()\n\n    if step % disp_interval == 0:\n        duration = t.toc(average=False)\n        fps = step_cnt / duration\n\n        log_text = 'step %d, image: %s, loss: %.4f, fps: %.2f (%.2fs per batch)' % (\n            step, blobs['im_name'], train_loss / step_cnt, fps, 1./fps)\n        log_print(log_text, color='green', attrs=['bold'])\n\n        if _DEBUG:\n            log_print('\\tTP: %.2f%%, TF: %.2f%%, fg/bg=(%d/%d)' % (tp/fg*100., tf/bg*100., fg/step_cnt, bg/step_cnt))\n            log_print('\\trpn_cls: %.4f, rpn_box: %.4f, rcnn_cls: %.4f, rcnn_box: %.4f' % (\n                net.rpn.cross_entropy.data.cpu().numpy()[0], net.rpn.loss_box.data.cpu().numpy()[0],\n                net.cross_entropy.data.cpu().numpy()[0], net.loss_box.data.cpu().numpy()[0])\n            )\n        re_cnt = True\n\n    if use_tensorboard and step % log_interval == 0:\n        exp.add_scalar_value('train_loss', train_loss / step_cnt, step=step)\n        exp.add_scalar_value('learning_rate', lr, step=step)\n        if _DEBUG:\n            exp.add_scalar_value('true_positive', tp/fg*100., step=step)\n            exp.add_scalar_value('true_negative', tf/bg*100., step=step)\n            losses = {'rpn_cls': float(net.rpn.cross_entropy.data.cpu().numpy()[0]),\n                      'rpn_box': float(net.rpn.loss_box.data.cpu().numpy()[0]),\n                      'rcnn_cls': float(net.cross_entropy.data.cpu().numpy()[0]),\n                      'rcnn_box': float(net.loss_box.data.cpu().numpy()[0])}\n            exp.add_scalar_dict(losses, step=step)\n\n    if (step % 10000 == 0) and step > 0:\n        save_name = os.path.join(output_dir, 'faster_rcnn_{}.h5'.format(step))\n        network.save_net(save_name, net)\n        print('save model: {}'.format(save_name))\n    if step in lr_decay_steps:\n        lr *= lr_decay\n        optimizer = torch.optim.SGD(params[8:], lr=lr, momentum=momentum, weight_decay=weight_decay)\n\n    if re_cnt:\n        tp, tf, fg, bg = 0., 0., 0, 0\n        train_loss = 0\n        step_cnt = 0\n        t.tic()\n        re_cnt = False\n\n"""
faster_rcnn/__init__.py,0,b''
faster_rcnn/faster_rcnn.py,20,"b'import cv2\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nfrom utils.timer import Timer\nfrom utils.blob import im_list_to_blob\nfrom fast_rcnn.nms_wrapper import nms\nfrom rpn_msr.proposal_layer import proposal_layer as proposal_layer_py\nfrom rpn_msr.anchor_target_layer import anchor_target_layer as anchor_target_layer_py\nfrom rpn_msr.proposal_target_layer import proposal_target_layer as proposal_target_layer_py\nfrom fast_rcnn.bbox_transform import bbox_transform_inv, clip_boxes\n\nimport network\nfrom network import Conv2d, FC\n# from roi_pooling.modules.roi_pool_py import RoIPool\nfrom roi_pooling.modules.roi_pool import RoIPool\nfrom vgg16 import VGG16\n\n\ndef nms_detections(pred_boxes, scores, nms_thresh, inds=None):\n    dets = np.hstack((pred_boxes,\n                      scores[:, np.newaxis])).astype(np.float32)\n    keep = nms(dets, nms_thresh)\n    if inds is None:\n        return pred_boxes[keep], scores[keep]\n    return pred_boxes[keep], scores[keep], inds[keep]\n\n\nclass RPN(nn.Module):\n    _feat_stride = [16, ]\n    anchor_scales = [8, 16, 32]\n\n    def __init__(self):\n        super(RPN, self).__init__()\n\n        self.features = VGG16(bn=False)\n        self.conv1 = Conv2d(512, 512, 3, same_padding=True)\n        self.score_conv = Conv2d(512, len(self.anchor_scales) * 3 * 2, 1, relu=False, same_padding=False)\n        self.bbox_conv = Conv2d(512, len(self.anchor_scales) * 3 * 4, 1, relu=False, same_padding=False)\n\n        # loss\n        self.cross_entropy = None\n        self.los_box = None\n\n    @property\n    def loss(self):\n        return self.cross_entropy + self.loss_box * 10\n\n    def forward(self, im_data, im_info, gt_boxes=None, gt_ishard=None, dontcare_areas=None):\n        im_data = network.np_to_variable(im_data, is_cuda=True)\n        im_data = im_data.permute(0, 3, 1, 2)\n        features = self.features(im_data)\n\n        rpn_conv1 = self.conv1(features)\n\n        # rpn score\n        rpn_cls_score = self.score_conv(rpn_conv1)\n        rpn_cls_score_reshape = self.reshape_layer(rpn_cls_score, 2)\n        rpn_cls_prob = F.softmax(rpn_cls_score_reshape)\n        rpn_cls_prob_reshape = self.reshape_layer(rpn_cls_prob, len(self.anchor_scales)*3*2)\n\n        # rpn boxes\n        rpn_bbox_pred = self.bbox_conv(rpn_conv1)\n\n        # proposal layer\n        cfg_key = \'TRAIN\' if self.training else \'TEST\'\n        rois = self.proposal_layer(rpn_cls_prob_reshape, rpn_bbox_pred, im_info,\n                                   cfg_key, self._feat_stride, self.anchor_scales)\n\n        # generating training labels and build the rpn loss\n        if self.training:\n            assert gt_boxes is not None\n            rpn_data = self.anchor_target_layer(rpn_cls_score, gt_boxes, gt_ishard, dontcare_areas,\n                                                im_info, self._feat_stride, self.anchor_scales)\n            self.cross_entropy, self.loss_box = self.build_loss(rpn_cls_score_reshape, rpn_bbox_pred, rpn_data)\n\n        return features, rois\n\n    def build_loss(self, rpn_cls_score_reshape, rpn_bbox_pred, rpn_data):\n        # classification loss\n        rpn_cls_score = rpn_cls_score_reshape.permute(0, 2, 3, 1).contiguous().view(-1, 2)\n        rpn_label = rpn_data[0].view(-1)\n\n        rpn_keep = Variable(rpn_label.data.ne(-1).nonzero().squeeze()).cuda()\n        rpn_cls_score = torch.index_select(rpn_cls_score, 0, rpn_keep)\n        rpn_label = torch.index_select(rpn_label, 0, rpn_keep)\n\n        fg_cnt = torch.sum(rpn_label.data.ne(0))\n\n        rpn_cross_entropy = F.cross_entropy(rpn_cls_score, rpn_label)\n\n        # box loss\n        rpn_bbox_targets, rpn_bbox_inside_weights, rpn_bbox_outside_weights = rpn_data[1:]\n        rpn_bbox_targets = torch.mul(rpn_bbox_targets, rpn_bbox_inside_weights)\n        rpn_bbox_pred = torch.mul(rpn_bbox_pred, rpn_bbox_inside_weights)\n\n        rpn_loss_box = F.smooth_l1_loss(rpn_bbox_pred, rpn_bbox_targets, size_average=False) / (fg_cnt + 1e-4)\n\n        return rpn_cross_entropy, rpn_loss_box\n\n    @staticmethod\n    def reshape_layer(x, d):\n        input_shape = x.size()\n        # x = x.permute(0, 3, 1, 2)\n        # b c w h\n        x = x.view(\n            input_shape[0],\n            int(d),\n            int(float(input_shape[1] * input_shape[2]) / float(d)),\n            input_shape[3]\n        )\n        # x = x.permute(0, 2, 3, 1)\n        return x\n\n    @staticmethod\n    def proposal_layer(rpn_cls_prob_reshape, rpn_bbox_pred, im_info, cfg_key, _feat_stride, anchor_scales):\n        rpn_cls_prob_reshape = rpn_cls_prob_reshape.data.cpu().numpy()\n        rpn_bbox_pred = rpn_bbox_pred.data.cpu().numpy()\n        x = proposal_layer_py(rpn_cls_prob_reshape, rpn_bbox_pred, im_info, cfg_key, _feat_stride, anchor_scales)\n        x = network.np_to_variable(x, is_cuda=True)\n        return x.view(-1, 5)\n\n    @staticmethod\n    def anchor_target_layer(rpn_cls_score, gt_boxes, gt_ishard, dontcare_areas, im_info, _feat_stride, anchor_scales):\n        """"""\n        rpn_cls_score: for pytorch (1, Ax2, H, W) bg/fg scores of previous conv layer\n        gt_boxes: (G, 5) vstack of [x1, y1, x2, y2, class]\n        gt_ishard: (G, 1), 1 or 0 indicates difficult or not\n        dontcare_areas: (D, 4), some areas may contains small objs but no labelling. D may be 0\n        im_info: a list of [image_height, image_width, scale_ratios]\n        _feat_stride: the downsampling ratio of feature map to the original input image\n        anchor_scales: the scales to the basic_anchor (basic anchor is [16, 16])\n        ----------\n        Returns\n        ----------\n        rpn_labels : (1, 1, HxA, W), for each anchor, 0 denotes bg, 1 fg, -1 dontcare\n        rpn_bbox_targets: (1, 4xA, H, W), distances of the anchors to the gt_boxes(may contains some transform)\n                        that are the regression objectives\n        rpn_bbox_inside_weights: (1, 4xA, H, W) weights of each boxes, mainly accepts hyper param in cfg\n        rpn_bbox_outside_weights: (1, 4xA, H, W) used to balance the fg/bg,\n        beacuse the numbers of bgs and fgs mays significiantly different\n        """"""\n        rpn_cls_score = rpn_cls_score.data.cpu().numpy()\n        rpn_labels, rpn_bbox_targets, rpn_bbox_inside_weights, rpn_bbox_outside_weights = \\\n            anchor_target_layer_py(rpn_cls_score, gt_boxes, gt_ishard, dontcare_areas, im_info, _feat_stride, anchor_scales)\n\n        rpn_labels = network.np_to_variable(rpn_labels, is_cuda=True, dtype=torch.LongTensor)\n        rpn_bbox_targets = network.np_to_variable(rpn_bbox_targets, is_cuda=True)\n        rpn_bbox_inside_weights = network.np_to_variable(rpn_bbox_inside_weights, is_cuda=True)\n        rpn_bbox_outside_weights = network.np_to_variable(rpn_bbox_outside_weights, is_cuda=True)\n\n        return rpn_labels, rpn_bbox_targets, rpn_bbox_inside_weights, rpn_bbox_outside_weights\n\n    def load_from_npz(self, params):\n        # params = np.load(npz_file)\n        self.features.load_from_npz(params)\n\n        pairs = {\'conv1.conv\': \'rpn_conv/3x3\', \'score_conv.conv\': \'rpn_cls_score\', \'bbox_conv.conv\': \'rpn_bbox_pred\'}\n        own_dict = self.state_dict()\n        for k, v in pairs.items():\n            key = \'{}.weight\'.format(k)\n            param = torch.from_numpy(params[\'{}/weights:0\'.format(v)]).permute(3, 2, 0, 1)\n            own_dict[key].copy_(param)\n\n            key = \'{}.bias\'.format(k)\n            param = torch.from_numpy(params[\'{}/biases:0\'.format(v)])\n            own_dict[key].copy_(param)\n\n\nclass FasterRCNN(nn.Module):\n    n_classes = 21\n    classes = np.asarray([\'__background__\',\n                       \'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n                       \'bottle\', \'bus\', \'car\', \'cat\', \'chair\',\n                       \'cow\', \'diningtable\', \'dog\', \'horse\',\n                       \'motorbike\', \'person\', \'pottedplant\',\n                       \'sheep\', \'sofa\', \'train\', \'tvmonitor\'])\n    PIXEL_MEANS = np.array([[[102.9801, 115.9465, 122.7717]]])\n    SCALES = (600,)\n    MAX_SIZE = 1000\n\n    def __init__(self, classes=None, debug=False):\n        super(FasterRCNN, self).__init__()\n\n        if classes is not None:\n            self.classes = classes\n            self.n_classes = len(classes)\n\n        self.rpn = RPN()\n        self.roi_pool = RoIPool(7, 7, 1.0/16)\n        self.fc6 = FC(512 * 7 * 7, 4096)\n        self.fc7 = FC(4096, 4096)\n        self.score_fc = FC(4096, self.n_classes, relu=False)\n        self.bbox_fc = FC(4096, self.n_classes * 4, relu=False)\n\n        # loss\n        self.cross_entropy = None\n        self.loss_box = None\n\n        # for log\n        self.debug = debug\n\n    @property\n    def loss(self):\n        # print self.cross_entropy\n        # print self.loss_box\n        # print self.rpn.cross_entropy\n        # print self.rpn.loss_box\n        return self.cross_entropy + self.loss_box * 10\n\n    def forward(self, im_data, im_info, gt_boxes=None, gt_ishard=None, dontcare_areas=None):\n        features, rois = self.rpn(im_data, im_info, gt_boxes, gt_ishard, dontcare_areas)\n\n        if self.training:\n            roi_data = self.proposal_target_layer(rois, gt_boxes, gt_ishard, dontcare_areas, self.n_classes)\n            rois = roi_data[0]\n\n        # roi pool\n        pooled_features = self.roi_pool(features, rois)\n        x = pooled_features.view(pooled_features.size()[0], -1)\n        x = self.fc6(x)\n        x = F.dropout(x, training=self.training)\n        x = self.fc7(x)\n        x = F.dropout(x, training=self.training)\n\n        cls_score = self.score_fc(x)\n        cls_prob = F.softmax(cls_score)\n        bbox_pred = self.bbox_fc(x)\n\n        if self.training:\n            self.cross_entropy, self.loss_box = self.build_loss(cls_score, bbox_pred, roi_data)\n\n        return cls_prob, bbox_pred, rois\n\n    def build_loss(self, cls_score, bbox_pred, roi_data):\n        # classification loss\n        label = roi_data[1].squeeze()\n        fg_cnt = torch.sum(label.data.ne(0))\n        bg_cnt = label.data.numel() - fg_cnt\n\n        # for log\n        if self.debug:\n            maxv, predict = cls_score.data.max(1)\n            self.tp = torch.sum(predict[:fg_cnt].eq(label.data[:fg_cnt])) if fg_cnt > 0 else 0\n            self.tf = torch.sum(predict[fg_cnt:].eq(label.data[fg_cnt:]))\n            self.fg_cnt = fg_cnt\n            self.bg_cnt = bg_cnt\n\n        ce_weights = torch.ones(cls_score.size()[1])\n        ce_weights[0] = float(fg_cnt) / bg_cnt\n        ce_weights = ce_weights.cuda()\n        cross_entropy = F.cross_entropy(cls_score, label, weight=ce_weights)\n\n        # bounding box regression L1 loss\n        bbox_targets, bbox_inside_weights, bbox_outside_weights = roi_data[2:]\n        bbox_targets = torch.mul(bbox_targets, bbox_inside_weights)\n        bbox_pred = torch.mul(bbox_pred, bbox_inside_weights)\n\n        loss_box = F.smooth_l1_loss(bbox_pred, bbox_targets, size_average=False) / (fg_cnt + 1e-4)\n\n        return cross_entropy, loss_box\n\n    @staticmethod\n    def proposal_target_layer(rpn_rois, gt_boxes, gt_ishard, dontcare_areas, num_classes):\n        """"""\n        ----------\n        rpn_rois:  (1 x H x W x A, 5) [0, x1, y1, x2, y2]\n        gt_boxes: (G, 5) [x1 ,y1 ,x2, y2, class] int\n        # gt_ishard: (G, 1) {0 | 1} 1 indicates hard\n        dontcare_areas: (D, 4) [ x1, y1, x2, y2]\n        num_classes\n        ----------\n        Returns\n        ----------\n        rois: (1 x H x W x A, 5) [0, x1, y1, x2, y2]\n        labels: (1 x H x W x A, 1) {0,1,...,_num_classes-1}\n        bbox_targets: (1 x H x W x A, K x4) [dx1, dy1, dx2, dy2]\n        bbox_inside_weights: (1 x H x W x A, Kx4) 0, 1 masks for the computing loss\n        bbox_outside_weights: (1 x H x W x A, Kx4) 0, 1 masks for the computing loss\n        """"""\n        rpn_rois = rpn_rois.data.cpu().numpy()\n        rois, labels, bbox_targets, bbox_inside_weights, bbox_outside_weights = \\\n            proposal_target_layer_py(rpn_rois, gt_boxes, gt_ishard, dontcare_areas, num_classes)\n        # print labels.shape, bbox_targets.shape, bbox_inside_weights.shape\n        rois = network.np_to_variable(rois, is_cuda=True)\n        labels = network.np_to_variable(labels, is_cuda=True, dtype=torch.LongTensor)\n        bbox_targets = network.np_to_variable(bbox_targets, is_cuda=True)\n        bbox_inside_weights = network.np_to_variable(bbox_inside_weights, is_cuda=True)\n        bbox_outside_weights = network.np_to_variable(bbox_outside_weights, is_cuda=True)\n\n        return rois, labels, bbox_targets, bbox_inside_weights, bbox_outside_weights\n\n    def interpret_faster_rcnn(self, cls_prob, bbox_pred, rois, im_info, im_shape, nms=True, clip=True, min_score=0.0):\n        # find class\n        scores, inds = cls_prob.data.max(1)\n        scores, inds = scores.cpu().numpy(), inds.cpu().numpy()\n\n        keep = np.where((inds > 0) & (scores >= min_score))\n        scores, inds = scores[keep], inds[keep]\n\n        # Apply bounding-box regression deltas\n        keep = keep[0]\n        box_deltas = bbox_pred.data.cpu().numpy()[keep]\n        box_deltas = np.asarray([\n            box_deltas[i, (inds[i] * 4): (inds[i] * 4 + 4)] for i in range(len(inds))\n        ], dtype=np.float)\n        boxes = rois.data.cpu().numpy()[keep, 1:5] / im_info[0][2]\n        pred_boxes = bbox_transform_inv(boxes, box_deltas)\n        if clip:\n            pred_boxes = clip_boxes(pred_boxes, im_shape)\n\n        # nms\n        if nms and pred_boxes.shape[0] > 0:\n            pred_boxes, scores, inds = nms_detections(pred_boxes, scores, 0.3, inds=inds)\n\n        return pred_boxes, scores, self.classes[inds]\n\n    def detect(self, image, thr=0.3):\n        im_data, im_scales = self.get_image_blob(image)\n        im_info = np.array(\n            [[im_data.shape[1], im_data.shape[2], im_scales[0]]],\n            dtype=np.float32)\n\n        cls_prob, bbox_pred, rois = self(im_data, im_info)\n        pred_boxes, scores, classes = \\\n            self.interpret_faster_rcnn(cls_prob, bbox_pred, rois, im_info, image.shape, min_score=thr)\n        return pred_boxes, scores, classes\n\n    def get_image_blob_noscale(self, im):\n        im_orig = im.astype(np.float32, copy=True)\n        im_orig -= self.PIXEL_MEANS\n\n        processed_ims = [im]\n        im_scale_factors = [1.0]\n\n        blob = im_list_to_blob(processed_ims)\n\n        return blob, np.array(im_scale_factors)\n\n    def get_image_blob(self, im):\n        """"""Converts an image into a network input.\n        Arguments:\n            im (ndarray): a color image in BGR order\n        Returns:\n            blob (ndarray): a data blob holding an image pyramid\n            im_scale_factors (list): list of image scales (relative to im) used\n                in the image pyramid\n        """"""\n        im_orig = im.astype(np.float32, copy=True)\n        im_orig -= self.PIXEL_MEANS\n\n        im_shape = im_orig.shape\n        im_size_min = np.min(im_shape[0:2])\n        im_size_max = np.max(im_shape[0:2])\n\n        processed_ims = []\n        im_scale_factors = []\n\n        for target_size in self.SCALES:\n            im_scale = float(target_size) / float(im_size_min)\n            # Prevent the biggest axis from being more than MAX_SIZE\n            if np.round(im_scale * im_size_max) > self.MAX_SIZE:\n                im_scale = float(self.MAX_SIZE) / float(im_size_max)\n            im = cv2.resize(im_orig, None, None, fx=im_scale, fy=im_scale,\n                            interpolation=cv2.INTER_LINEAR)\n            im_scale_factors.append(im_scale)\n            processed_ims.append(im)\n\n        # Create a blob to hold the input images\n        blob = im_list_to_blob(processed_ims)\n\n        return blob, np.array(im_scale_factors)\n\n    def load_from_npz(self, params):\n        self.rpn.load_from_npz(params)\n\n        pairs = {\'fc6.fc\': \'fc6\', \'fc7.fc\': \'fc7\', \'score_fc.fc\': \'cls_score\', \'bbox_fc.fc\': \'bbox_pred\'}\n        own_dict = self.state_dict()\n        for k, v in pairs.items():\n            key = \'{}.weight\'.format(k)\n            param = torch.from_numpy(params[\'{}/weights:0\'.format(v)]).permute(1, 0)\n            own_dict[key].copy_(param)\n\n            key = \'{}.bias\'.format(k)\n            param = torch.from_numpy(params[\'{}/biases:0\'.format(v)])\n            own_dict[key].copy_(param)\n\n'"
faster_rcnn/network.py,8,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np\n\n\nclass Conv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, relu=True, same_padding=False, bn=False):\n        super(Conv2d, self).__init__()\n        padding = int((kernel_size - 1) / 2) if same_padding else 0\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=padding)\n        self.bn = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0, affine=True) if bn else None\n        self.relu = nn.ReLU(inplace=True) if relu else None\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.bn is not None:\n            x = self.bn(x)\n        if self.relu is not None:\n            x = self.relu(x)\n        return x\n\n\nclass FC(nn.Module):\n    def __init__(self, in_features, out_features, relu=True):\n        super(FC, self).__init__()\n        self.fc = nn.Linear(in_features, out_features)\n        self.relu = nn.ReLU(inplace=True) if relu else None\n\n    def forward(self, x):\n        x = self.fc(x)\n        if self.relu is not None:\n            x = self.relu(x)\n        return x\n\n\ndef save_net(fname, net):\n    import h5py\n    h5f = h5py.File(fname, mode=\'w\')\n    for k, v in net.state_dict().items():\n        h5f.create_dataset(k, data=v.cpu().numpy())\n\n\ndef load_net(fname, net):\n    import h5py\n    h5f = h5py.File(fname, mode=\'r\')\n    for k, v in net.state_dict().items():\n        param = torch.from_numpy(np.asarray(h5f[k]))\n        v.copy_(param)\n\n\ndef load_pretrained_npy(faster_rcnn_model, fname):\n    params = np.load(fname).item()\n    # vgg16\n    vgg16_dict = faster_rcnn_model.rpn.features.state_dict()\n    for name, val in vgg16_dict.items():\n        # # print name\n        # # print val.size()\n        # # print param.size()\n        if name.find(\'bn.\') >= 0:\n            continue\n        i, j = int(name[4]), int(name[6]) + 1\n        ptype = \'weights\' if name[-1] == \'t\' else \'biases\'\n        key = \'conv{}_{}\'.format(i, j)\n        param = torch.from_numpy(params[key][ptype])\n\n        if ptype == \'weights\':\n            param = param.permute(3, 2, 0, 1)\n\n        val.copy_(param)\n\n    # fc6 fc7\n    frcnn_dict = faster_rcnn_model.state_dict()\n    pairs = {\'fc6.fc\': \'fc6\', \'fc7.fc\': \'fc7\'}\n    for k, v in pairs.items():\n        key = \'{}.weight\'.format(k)\n        param = torch.from_numpy(params[v][\'weights\']).permute(1, 0)\n        frcnn_dict[key].copy_(param)\n\n        key = \'{}.bias\'.format(k)\n        param = torch.from_numpy(params[v][\'biases\'])\n        frcnn_dict[key].copy_(param)\n\n\ndef np_to_variable(x, is_cuda=True, dtype=torch.FloatTensor):\n    v = Variable(torch.from_numpy(x).type(dtype))\n    if is_cuda:\n        v = v.cuda()\n    return v\n\n\ndef set_trainable(model, requires_grad):\n    for param in model.parameters():\n        param.requires_grad = requires_grad\n\n\ndef weights_normal_init(model, dev=0.01):\n    if isinstance(model, list):\n        for m in model:\n            weights_normal_init(m, dev)\n    else:\n        for m in model.modules():\n            if isinstance(m, nn.Conv2d):\n                m.weight.data.normal_(0.0, dev)\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0.0, dev)\n\n\ndef clip_gradient(model, clip_norm):\n    """"""Computes a gradient clipping coefficient based on gradient norm.""""""\n    totalnorm = 0\n    for p in model.parameters():\n        if p.requires_grad:\n            modulenorm = p.grad.data.norm()\n            totalnorm += modulenorm ** 2\n    totalnorm = np.sqrt(totalnorm)\n\n    norm = clip_norm / max(totalnorm, clip_norm)\n    for p in model.parameters():\n        if p.requires_grad:\n            p.grad.mul_(norm)\n'"
faster_rcnn/setup.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport os\nfrom os.path import join as pjoin\nimport numpy as np\nfrom distutils.core import setup\nfrom distutils.extension import Extension\nfrom Cython.Distutils import build_ext\n\n\ndef find_in_path(name, path):\n    ""Find a file in a search path""\n    # adapted fom http://code.activestate.com/recipes/52224-find-a-file-given-a-search-path/\n    for dir in path.split(os.pathsep):\n        binpath = pjoin(dir, name)\n        if os.path.exists(binpath):\n            return os.path.abspath(binpath)\n    return None\n\n\ndef locate_cuda():\n    """"""Locate the CUDA environment on the system\n\n    Returns a dict with keys \'home\', \'nvcc\', \'include\', and \'lib64\'\n    and values giving the absolute path to each directory.\n\n    Starts by looking for the CUDAHOME env variable. If not found, everything\n    is based on finding \'nvcc\' in the PATH.\n    """"""\n\n    # first check if the CUDAHOME env variable is in use\n    if \'CUDAHOME\' in os.environ:\n        home = os.environ[\'CUDAHOME\']\n        nvcc = pjoin(home, \'bin\', \'nvcc\')\n    else:\n        # otherwise, search the PATH for NVCC\n        default_path = pjoin(os.sep, \'usr\', \'local\', \'cuda\', \'bin\')\n        nvcc = find_in_path(\'nvcc\', os.environ[\'PATH\'] + os.pathsep + default_path)\n        if nvcc is None:\n            raise EnvironmentError(\'The nvcc binary could not be \'\n                                   \'located in your $PATH. Either add it to your path, or set $CUDAHOME\')\n        home = os.path.dirname(os.path.dirname(nvcc))\n\n    cudaconfig = {\'home\': home, \'nvcc\': nvcc,\n                  \'include\': pjoin(home, \'include\'),\n                  \'lib64\': pjoin(home, \'lib64\')}\n    for k, v in cudaconfig.iteritems():\n        if not os.path.exists(v):\n            raise EnvironmentError(\'The CUDA %s path could not be located in %s\' % (k, v))\n\n    return cudaconfig\n\n\nCUDA = locate_cuda()\n\n# Obtain the numpy include directory.  This logic works across numpy versions.\ntry:\n    numpy_include = np.get_include()\nexcept AttributeError:\n    numpy_include = np.get_numpy_include()\n\n\ndef customize_compiler_for_nvcc(self):\n    """"""inject deep into distutils to customize how the dispatch\n    to gcc/nvcc works.\n\n    If you subclass UnixCCompiler, it\'s not trivial to get your subclass\n    injected in, and still have the right customizations (i.e.\n    distutils.sysconfig.customize_compiler) run on it. So instead of going\n    the OO route, I have this. Note, it\'s kindof like a wierd functional\n    subclassing going on.""""""\n\n    # tell the compiler it can processes .cu\n    self.src_extensions.append(\'.cu\')\n\n    # save references to the default compiler_so and _comple methods\n    default_compiler_so = self.compiler_so\n    super = self._compile\n\n    # now redefine the _compile method. This gets executed for each\n    # object but distutils doesn\'t have the ability to change compilers\n    # based on source extension: we add it.\n    def _compile(obj, src, ext, cc_args, extra_postargs, pp_opts):\n        print (extra_postargs)\n        if os.path.splitext(src)[1] == \'.cu\':\n            # use the cuda for .cu files\n            self.set_executable(\'compiler_so\', CUDA[\'nvcc\'])\n            # use only a subset of the extra_postargs, which are 1-1 translated\n            # from the extra_compile_args in the Extension class\n            postargs = extra_postargs[\'nvcc\']\n        else:\n            postargs = extra_postargs[\'gcc\']\n\n        super(obj, src, ext, cc_args, postargs, pp_opts)\n        # reset the default compiler_so, which we might have changed for cuda\n        self.compiler_so = default_compiler_so\n\n    # inject our redefined _compile method into the class\n    self._compile = _compile\n\n\n# run the customize_compiler\nclass custom_build_ext(build_ext):\n    def build_extensions(self):\n        customize_compiler_for_nvcc(self.compiler)\n        build_ext.build_extensions(self)\n\n\next_modules = [\n    Extension(\n        ""utils.cython_bbox"",\n        [""utils/bbox.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n        include_dirs=[numpy_include]\n    ),\n    Extension(\n        ""utils.cython_nms"",\n        [""utils/nms.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n        include_dirs=[numpy_include]\n    ),\n    Extension(\n        ""nms.cpu_nms"",\n        [""nms/cpu_nms.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n        include_dirs=[numpy_include]\n    ),\n    Extension(\'nms.gpu_nms\',\n              [\'nms/nms_kernel.cu\', \'nms/gpu_nms.pyx\'],\n              library_dirs=[CUDA[\'lib64\']],\n              libraries=[\'cudart\'],\n              language=\'c++\',\n              runtime_library_dirs=[CUDA[\'lib64\']],\n              # this syntax is specific to this build system\n              # we\'re only going to use certain compiler args with nvcc and not with gcc\n              # the implementation of this trick is in customize_compiler() below\n              extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n                                  \'nvcc\': [\'-arch=sm_35\',\n                                           \'--ptxas-options=-v\',\n                                           \'-c\',\n                                           \'--compiler-options\',\n                                           ""\'-fPIC\'""]},\n              include_dirs=[numpy_include, CUDA[\'include\']]\n              ),\n    Extension(\n        \'pycocotools._mask\',\n        sources=[\'pycocotools/maskApi.c\', \'pycocotools/_mask.pyx\'],\n        include_dirs=[numpy_include, \'pycocotools\'],\n        extra_compile_args={\n            \'gcc\': [\'-Wno-cpp\', \'-Wno-unused-function\', \'-std=c99\']},\n    ),\n]\n\nsetup(\n    name=\'fast_rcnn\',\n    ext_modules=ext_modules,\n    # inject our custom trigger\n    cmdclass={\'build_ext\': custom_build_ext},\n)\n'"
faster_rcnn/vgg16.py,5,"b""import cv2\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nfrom utils.blob import im_list_to_blob\nfrom network import Conv2d\nimport network\n\n\nclass VGG16(nn.Module):\n    def __init__(self, bn=False):\n        super(VGG16, self).__init__()\n\n        self.conv1 = nn.Sequential(Conv2d(3, 64, 3, same_padding=True, bn=bn),\n                                   Conv2d(64, 64, 3, same_padding=True, bn=bn),\n                                   nn.MaxPool2d(2))\n        self.conv2 = nn.Sequential(Conv2d(64, 128, 3, same_padding=True, bn=bn),\n                                   Conv2d(128, 128, 3, same_padding=True, bn=bn),\n                                   nn.MaxPool2d(2))\n        network.set_trainable(self.conv1, requires_grad=False)\n        network.set_trainable(self.conv2, requires_grad=False)\n\n        self.conv3 = nn.Sequential(Conv2d(128, 256, 3, same_padding=True, bn=bn),\n                                   Conv2d(256, 256, 3, same_padding=True, bn=bn),\n                                   Conv2d(256, 256, 3, same_padding=True, bn=bn),\n                                   nn.MaxPool2d(2))\n        self.conv4 = nn.Sequential(Conv2d(256, 512, 3, same_padding=True, bn=bn),\n                                   Conv2d(512, 512, 3, same_padding=True, bn=bn),\n                                   Conv2d(512, 512, 3, same_padding=True, bn=bn),\n                                   nn.MaxPool2d(2))\n        self.conv5 = nn.Sequential(Conv2d(512, 512, 3, same_padding=True, bn=bn),\n                                   Conv2d(512, 512, 3, same_padding=True, bn=bn),\n                                   Conv2d(512, 512, 3, same_padding=True, bn=bn))\n\n    def forward(self, im_data):\n        # im_data, im_scales = get_blobs(image)\n        # im_info = np.array(\n        #     [[im_data.shape[1], im_data.shape[2], im_scales[0]]],\n        #     dtype=np.float32)\n        # data = Variable(torch.from_numpy(im_data)).cuda()\n        # x = data.permute(0, 3, 1, 2)\n\n        x = self.conv1(im_data)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.conv5(x)\n        return x\n\n    def load_from_npz(self, params):\n        # params = np.load(npz_file)\n        own_dict = self.state_dict()\n        for name, val in own_dict.items():\n            i, j = int(name[4]), int(name[6]) + 1\n            ptype = 'weights' if name[-1] == 't' else 'biases'\n            key = 'conv{}_{}/{}:0'.format(i, j, ptype)\n            param = torch.from_numpy(params[key])\n            if ptype == 'weights':\n                param = param.permute(3, 2, 0, 1)\n            val.copy_(param)\n\n    # def load_from_npy_file(self, fname):\n    #     own_dict = self.state_dict()\n    #     params = np.load(fname).item()\n    #     for name, val in own_dict.items():\n    #         # # print name\n    #         # # print val.size()\n    #         # # print param.size()\n    #         # if name.find('bn.') >= 0:\n    #         #     continue\n    #\n    #         i, j = int(name[4]), int(name[6]) + 1\n    #         ptype = 'weights' if name[-1] == 't' else 'biases'\n    #         key = 'conv{}_{}'.format(i, j)\n    #         param = torch.from_numpy(params[key][ptype])\n    #\n    #         if ptype == 'weights':\n    #             param = param.permute(3, 2, 0, 1)\n    #\n    #         val.copy_(param)\n\n\nif __name__ == '__main__':\n    vgg = VGG16()\n    vgg.load_from_npy_file('/media/longc/Data/models/VGG_imagenet.npy')"""
faster_rcnn/datasets/__init__.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n# TODO: make this fold self-contained, only depends on utils package\n\nfrom .imdb import imdb\nfrom .pascal_voc import pascal_voc\nfrom .pascal3d import pascal3d\nfrom .imagenet3d import imagenet3d\nfrom .kitti import kitti\nfrom .kitti_tracking import kitti_tracking\nfrom .nissan import nissan\nfrom .nthu import nthu\nfrom . import factory\n\n## NOTE: obsolete\nimport os.path as osp\nfrom .imdb import ROOT_DIR\nfrom .imdb import MATLAB\n\n# http://stackoverflow.com/questions/377017/test-if-executable-exists-in-python\ndef _which(program):\n    import os\n    def is_exe(fpath):\n        return os.path.isfile(fpath) and os.access(fpath, os.X_OK)\n\n    fpath, fname = os.path.split(program)\n    if fpath:\n        if is_exe(program):\n            return program\n    else:\n        for path in os.environ[""PATH""].split(os.pathsep):\n            path = path.strip(\'""\')\n            exe_file = os.path.join(path, program)\n            if is_exe(exe_file):\n                return exe_file\n\n    return None\n""""""\nif _which(MATLAB) is None:\n    msg = (""MATLAB command \'{}\' not found. ""\n           ""Please add \'{}\' to your PATH."").format(MATLAB, MATLAB)\n    raise EnvironmentError(msg)\n""""""\n'"
faster_rcnn/datasets/coco.py,0,"b'# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n# from datasets.imdb import imdb\n# import datasets.ds_utils as ds_utils\n# from fast_rcnn.config import cfg\nimport os.path as osp\nimport sys\nimport os\nimport numpy as np\nimport scipy.sparse\nimport scipy.io as sio\nimport cPickle\nimport json\nimport uuid\n# COCO API\n# TODO: add this part into this project\nfrom ..pycocotools.coco import COCO\nfrom ..pycocotools.cocoeval import COCOeval\nfrom ..pycocotools import mask as COCOmask\n\nfrom .imdb import imdb\nimport ds_utils\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\n# <<<< obsolete\n\ndef _filter_crowd_proposals(roidb, crowd_thresh):\n    """"""\n    Finds proposals that are inside crowd regions and marks them with\n    overlap = -1 (for all gt rois), which means they will be excluded from\n    training.\n    """"""\n    for ix, entry in enumerate(roidb):\n        overlaps = entry[\'gt_overlaps\'].toarray()\n        crowd_inds = np.where(overlaps.max(axis=1) == -1)[0]\n        non_gt_inds = np.where(entry[\'gt_classes\'] == 0)[0]\n        if len(crowd_inds) == 0 or len(non_gt_inds) == 0:\n            continue\n        iscrowd = [int(True) for _ in xrange(len(crowd_inds))]\n        crowd_boxes = ds_utils.xyxy_to_xywh(entry[\'boxes\'][crowd_inds, :])\n        non_gt_boxes = ds_utils.xyxy_to_xywh(entry[\'boxes\'][non_gt_inds, :])\n        ious = COCOmask.iou(non_gt_boxes, crowd_boxes, iscrowd)\n        bad_inds = np.where(ious.max(axis=1) > crowd_thresh)[0]\n        overlaps[non_gt_inds[bad_inds], :] = -1\n        roidb[ix][\'gt_overlaps\'] = scipy.sparse.csr_matrix(overlaps)\n    return roidb\n\nclass coco(imdb):\n    def __init__(self, image_set, year):\n        imdb.__init__(self, \'coco_\' + year + \'_\' + image_set)\n        # COCO specific config options\n        self.config = {\'top_k\' : 2000,\n                       \'use_salt\' : True,\n                       \'cleanup\' : True,\n                       \'crowd_thresh\' : 0.7,\n                       \'min_size\' : 2}\n        # name, paths\n        self._year = year\n        self._image_set = image_set\n        self._data_path = osp.join(cfg.DATA_DIR, \'coco\')\n        # load COCO API, classes, class <-> id mappings\n        self._COCO = COCO(self._get_ann_file())\n        cats = self._COCO.loadCats(self._COCO.getCatIds())\n        self._classes = tuple([\'__background__\'] + [c[\'name\'] for c in cats])\n        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))\n        self._class_to_coco_cat_id = dict(zip([c[\'name\'] for c in cats],\n                                              self._COCO.getCatIds()))\n        self._image_index = self._load_image_set_index()\n        # Default to roidb handler\n        self.set_proposal_method(\'selective_search\')\n        self.competition_mode(False)\n\n        # Some image sets are ""views"" (i.e. subsets) into others.\n        # For example, minival2014 is a random 5000 image subset of val2014.\n        # This mapping tells us where the view\'s images and proposals come from.\n        self._view_map = {\n            \'minival2014\' : \'val2014\',          # 5k val2014 subset\n            \'valminusminival2014\' : \'val2014\',  # val2014 \\setminus minival2014\n        }\n        coco_name = image_set + year  # e.g., ""val2014""\n        self._data_name = (self._view_map[coco_name]\n                           if self._view_map.has_key(coco_name)\n                           else coco_name)\n        # Dataset splits that have ground-truth annotations (test splits\n        # do not have gt annotations)\n        self._gt_splits = (\'train\', \'val\', \'minival\')\n\n    def _get_ann_file(self):\n        prefix = \'instances\' if self._image_set.find(\'test\') == -1 \\\n                             else \'image_info\'\n        return osp.join(self._data_path, \'annotations\',\n                        prefix + \'_\' + self._image_set + self._year + \'.json\')\n\n    def _load_image_set_index(self):\n        """"""\n        Load image ids.\n        """"""\n        image_ids = self._COCO.getImgIds()\n        return image_ids\n\n    def _get_widths(self):\n        anns = self._COCO.loadImgs(self._image_index)\n        widths = [ann[\'width\'] for ann in anns]\n        return widths\n\n    def image_path_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return self.image_path_from_index(self._image_index[i])\n\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n        # Example image path for index=119993:\n        #   images/train2014/COCO_train2014_000000119993.jpg\n        file_name = (\'COCO_\' + self._data_name + \'_\' +\n                     str(index).zfill(12) + \'.jpg\')\n        image_path = osp.join(self._data_path, \'images\',\n                              self._data_name, file_name)\n        assert osp.exists(image_path), \\\n                \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def selective_search_roidb(self):\n        return self._roidb_from_proposals(\'selective_search\')\n\n    def edge_boxes_roidb(self):\n        return self._roidb_from_proposals(\'edge_boxes_AR\')\n\n    def mcg_roidb(self):\n        return self._roidb_from_proposals(\'MCG\')\n\n    def _roidb_from_proposals(self, method):\n        """"""\n        Creates a roidb from pre-computed proposals of a particular methods.\n        """"""\n        top_k = self.config[\'top_k\']\n        cache_file = osp.join(self.cache_path, self.name +\n                              \'_{:s}_top{:d}\'.format(method, top_k) +\n                              \'_roidb.pkl\')\n\n        if osp.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{:s} {:s} roidb loaded from {:s}\'.format(self.name, method,\n                                                            cache_file)\n            return roidb\n\n        if self._image_set in self._gt_splits:\n            gt_roidb = self.gt_roidb()\n            method_roidb = self._load_proposals(method, gt_roidb)\n            roidb = imdb.merge_roidbs(gt_roidb, method_roidb)\n            # Make sure we don\'t use proposals that are contained in crowds\n            roidb = _filter_crowd_proposals(roidb, self.config[\'crowd_thresh\'])\n        else:\n            roidb = self._load_proposals(method, None)\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote {:s} roidb to {:s}\'.format(method, cache_file)\n        return roidb\n\n    def _load_proposals(self, method, gt_roidb):\n        """"""\n        Load pre-computed proposals in the format provided by Jan Hosang:\n        http://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-\n          computing/research/object-recognition-and-scene-understanding/how-\n          good-are-detection-proposals-really/\n        For MCG, use boxes from http://www.eecs.berkeley.edu/Research/Projects/\n          CS/vision/grouping/mcg/ and convert the file layout using\n        lib/datasets/tools/mcg_munge.py.\n        """"""\n        box_list = []\n        top_k = self.config[\'top_k\']\n        valid_methods = [\n            \'MCG\',\n            \'selective_search\',\n            \'edge_boxes_AR\',\n            \'edge_boxes_70\']\n        assert method in valid_methods\n\n        print \'Loading {} boxes\'.format(method)\n        for i, index in enumerate(self._image_index):\n            if i % 1000 == 0:\n                print \'{:d} / {:d}\'.format(i + 1, len(self._image_index))\n\n            box_file = osp.join(\n                cfg.DATA_DIR, \'coco_proposals\', method, \'mat\',\n                self._get_box_file(index))\n\n            raw_data = sio.loadmat(box_file)[\'boxes\']\n            boxes = np.maximum(raw_data - 1, 0).astype(np.uint16)\n            if method == \'MCG\':\n                # Boxes from the MCG website are in (y1, x1, y2, x2) order\n                boxes = boxes[:, (1, 0, 3, 2)]\n            # Remove duplicate boxes and very small boxes and then take top k\n            keep = ds_utils.unique_boxes(boxes)\n            boxes = boxes[keep, :]\n            keep = ds_utils.filter_small_boxes(boxes, self.config[\'min_size\'])\n            boxes = boxes[keep, :]\n            boxes = boxes[:top_k, :]\n            box_list.append(boxes)\n            # Sanity check\n            im_ann = self._COCO.loadImgs(index)[0]\n            width = im_ann[\'width\']\n            height = im_ann[\'height\']\n            ds_utils.validate_boxes(boxes, width=width, height=height)\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def gt_roidb(self):\n        """"""\n        Return the database of ground-truth regions of interest.\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = osp.join(self.cache_path, self.name + \'_gt_roidb.pkl\')\n        if osp.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} gt roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        gt_roidb = [self._load_coco_annotation(index)\n                    for index in self._image_index]\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(gt_roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote gt roidb to {}\'.format(cache_file)\n        return gt_roidb\n\n    def _load_coco_annotation(self, index):\n        """"""\n        Loads COCO bounding-box instance annotations. Crowd instances are\n        handled by marking their overlaps (with all categories) to -1. This\n        overlap value means that crowd ""instances"" are excluded from training.\n        """"""\n        im_ann = self._COCO.loadImgs(index)[0]\n        width = im_ann[\'width\']\n        height = im_ann[\'height\']\n\n        annIds = self._COCO.getAnnIds(imgIds=index, iscrowd=None)\n        objs = self._COCO.loadAnns(annIds)\n        # Sanitize bboxes -- some are invalid\n        valid_objs = []\n        for obj in objs:\n            x1 = np.max((0, obj[\'bbox\'][0]))\n            y1 = np.max((0, obj[\'bbox\'][1]))\n            x2 = np.min((width - 1, x1 + np.max((0, obj[\'bbox\'][2] - 1))))\n            y2 = np.min((height - 1, y1 + np.max((0, obj[\'bbox\'][3] - 1))))\n            if obj[\'area\'] > 0 and x2 >= x1 and y2 >= y1:\n                obj[\'clean_bbox\'] = [x1, y1, x2, y2]\n                valid_objs.append(obj)\n        objs = valid_objs\n        num_objs = len(objs)\n\n        boxes = np.zeros((num_objs, 4), dtype=np.uint16)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        seg_areas = np.zeros((num_objs), dtype=np.float32)\n\n        # Lookup table to map from COCO category ids to our internal class\n        # indices\n        coco_cat_id_to_class_ind = dict([(self._class_to_coco_cat_id[cls],\n                                          self._class_to_ind[cls])\n                                         for cls in self._classes[1:]])\n\n        for ix, obj in enumerate(objs):\n            cls = coco_cat_id_to_class_ind[obj[\'category_id\']]\n            boxes[ix, :] = obj[\'clean_bbox\']\n            gt_classes[ix] = cls\n            seg_areas[ix] = obj[\'area\']\n            if obj[\'iscrowd\']:\n                # Set overlap to -1 for all classes for crowd objects\n                # so they will be excluded during training\n                overlaps[ix, :] = -1.0\n            else:\n                overlaps[ix, cls] = 1.0\n\n        ds_utils.validate_boxes(boxes, width=width, height=height)\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n        return {\'boxes\' : boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_overlaps\' : overlaps,\n                \'flipped\' : False,\n                \'seg_areas\' : seg_areas}\n\n    def _get_box_file(self, index):\n        # first 14 chars / first 22 chars / all chars + .mat\n        # COCO_val2014_0/COCO_val2014_000000447/COCO_val2014_000000447991.mat\n        file_name = (\'COCO_\' + self._data_name +\n                     \'_\' + str(index).zfill(12) + \'.mat\')\n        return osp.join(file_name[:14], file_name[:22], file_name)\n\n    def _print_detection_eval_metrics(self, coco_eval):\n        IoU_lo_thresh = 0.5\n        IoU_hi_thresh = 0.95\n        def _get_thr_ind(coco_eval, thr):\n            ind = np.where((coco_eval.params.iouThrs > thr - 1e-5) &\n                           (coco_eval.params.iouThrs < thr + 1e-5))[0][0]\n            iou_thr = coco_eval.params.iouThrs[ind]\n            assert np.isclose(iou_thr, thr)\n            return ind\n\n        ind_lo = _get_thr_ind(coco_eval, IoU_lo_thresh)\n        ind_hi = _get_thr_ind(coco_eval, IoU_hi_thresh)\n        # precision has dims (iou, recall, cls, area range, max dets)\n        # area range index 0: all area ranges\n        # max dets index 2: 100 per image\n        precision = \\\n            coco_eval.eval[\'precision\'][ind_lo:(ind_hi + 1), :, :, 0, 2]\n        ap_default = np.mean(precision[precision > -1])\n        print (\'~~~~ Mean and per-category AP @ IoU=[{:.2f},{:.2f}] \'\n               \'~~~~\').format(IoU_lo_thresh, IoU_hi_thresh)\n        print \'{:.1f}\'.format(100 * ap_default)\n        for cls_ind, cls in enumerate(self.classes):\n            if cls == \'__background__\':\n                continue\n            # minus 1 because of __background__\n            precision = coco_eval.eval[\'precision\'][ind_lo:(ind_hi + 1), :, cls_ind - 1, 0, 2]\n            ap = np.mean(precision[precision > -1])\n            print \'{:.1f}\'.format(100 * ap)\n\n        print \'~~~~ Summary metrics ~~~~\'\n        coco_eval.summarize()\n\n    def _do_detection_eval(self, res_file, output_dir):\n        ann_type = \'bbox\'\n        coco_dt = self._COCO.loadRes(res_file)\n        coco_eval = COCOeval(self._COCO, coco_dt)\n        coco_eval.params.useSegm = (ann_type == \'segm\')\n        coco_eval.evaluate()\n        coco_eval.accumulate()\n        self._print_detection_eval_metrics(coco_eval)\n        eval_file = osp.join(output_dir, \'detection_results.pkl\')\n        with open(eval_file, \'wb\') as fid:\n            cPickle.dump(coco_eval, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'Wrote COCO eval results to: {}\'.format(eval_file)\n\n    def _coco_results_one_category(self, boxes, cat_id):\n        results = []\n        for im_ind, index in enumerate(self.image_index):\n            dets = boxes[im_ind].astype(np.float)\n            if dets == []:\n                continue\n            scores = dets[:, -1]\n            xs = dets[:, 0]\n            ys = dets[:, 1]\n            ws = dets[:, 2] - xs + 1\n            hs = dets[:, 3] - ys + 1\n            results.extend(\n              [{\'image_id\' : index,\n                \'category_id\' : cat_id,\n                \'bbox\' : [xs[k], ys[k], ws[k], hs[k]],\n                \'score\' : scores[k]} for k in xrange(dets.shape[0])])\n        return results\n\n    def _write_coco_results_file(self, all_boxes, res_file):\n        # [{""image_id"": 42,\n        #   ""category_id"": 18,\n        #   ""bbox"": [258.15,41.29,348.26,243.78],\n        #   ""score"": 0.236}, ...]\n        results = []\n        for cls_ind, cls in enumerate(self.classes):\n            if cls == \'__background__\':\n                continue\n            print \'Collecting {} results ({:d}/{:d})\'.format(cls, cls_ind,\n                                                          self.num_classes - 1)\n            coco_cat_id = self._class_to_coco_cat_id[cls]\n            results.extend(self._coco_results_one_category(all_boxes[cls_ind],\n                                                           coco_cat_id))\n        print \'Writing results json to {}\'.format(res_file)\n        with open(res_file, \'w\') as fid:\n            json.dump(results, fid)\n\n    def evaluate_detections(self, all_boxes, output_dir):\n        res_file = osp.join(output_dir, (\'detections_\' +\n                                         self._image_set +\n                                         self._year +\n                                         \'_results\'))\n        if self.config[\'use_salt\']:\n            res_file += \'_{}\'.format(str(uuid.uuid4()))\n        res_file += \'.json\'\n        self._write_coco_results_file(all_boxes, res_file)\n        # Only do evaluation on non-test sets\n        if self._image_set.find(\'test\') == -1:\n            self._do_detection_eval(res_file, output_dir)\n        # Optionally cleanup results json file\n        if self.config[\'cleanup\']:\n            os.remove(res_file)\n\n    def competition_mode(self, on):\n        if on:\n            self.config[\'use_salt\'] = False\n            self.config[\'cleanup\'] = False\n        else:\n            self.config[\'use_salt\'] = True\n            self.config[\'cleanup\'] = True\n'"
faster_rcnn/datasets/ds_utils.py,0,"b'# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport numpy as np\n\ndef unique_boxes(boxes, scale=1.0):\n    """"""Return indices of unique boxes.""""""\n    v = np.array([1, 1e3, 1e6, 1e9])\n    hashes = np.round(boxes * scale).dot(v)\n    _, index = np.unique(hashes, return_index=True)\n    return np.sort(index)\n\ndef xywh_to_xyxy(boxes):\n    """"""Convert [x y w h] box format to [x1 y1 x2 y2] format.""""""\n    return np.hstack((boxes[:, 0:2], boxes[:, 0:2] + boxes[:, 2:4] - 1))\n\ndef xyxy_to_xywh(boxes):\n    """"""Convert [x1 y1 x2 y2] box format to [x y w h] format.""""""\n    return np.hstack((boxes[:, 0:2], boxes[:, 2:4] - boxes[:, 0:2] + 1))\n\ndef validate_boxes(boxes, width=0, height=0):\n    """"""Check that a set of boxes are valid.""""""\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n    assert (x1 >= 0).all()\n    assert (y1 >= 0).all()\n    assert (x2 >= x1).all()\n    assert (y2 >= y1).all()\n    assert (x2 < width).all()\n    assert (y2 < height).all()\n\ndef filter_small_boxes(boxes, min_size):\n    w = boxes[:, 2] - boxes[:, 0]\n    h = boxes[:, 3] - boxes[:, 1]\n    keep = np.where((w >= min_size) & (h > min_size))[0]\n    return keep\n'"
faster_rcnn/datasets/factory.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Factory method for easily getting imdbs by name.""""""\n\n__sets = {}\n\nimport numpy as np\n\nfrom .pascal_voc import pascal_voc\nfrom .imagenet3d import imagenet3d\nfrom .kitti import kitti\nfrom .kitti_tracking import kitti_tracking\nfrom .nthu import nthu\nfrom .coco import coco\nfrom .kittivoc import kittivoc\n\n\ndef _selective_search_IJCV_top_k(split, year, top_k):\n    """"""Return an imdb that uses the top k proposals from the selective search\n    IJCV code.\n    """"""\n    imdb = pascal_voc(split, year)\n    imdb.roidb_handler = imdb.selective_search_IJCV_roidb\n    imdb.config[\'top_k\'] = top_k\n    return imdb\n\n\n# Set up voc_<year>_<split> using selective search ""fast"" mode\nfor year in [\'2007\', \'2012\', \'0712\']:\n    for split in [\'train\', \'val\', \'trainval\', \'test\']:\n        name = \'voc_{}_{}\'.format(year, split)\n        __sets[name] = (lambda split=split, year=year:\n                        pascal_voc(split, year))\n\n\n    # Set up kittivoc\n    for split in [\'train\', \'val\', \'trainval\', \'test\']:\n        name = \'kittivoc_{}\'.format(split)\n        # print name\n        __sets[name] = (lambda split=split: kittivoc(split))\n\n# # KITTI dataset\nfor split in [\'train\', \'val\', \'trainval\', \'test\']:\n    name = \'kitti_{}\'.format(split)\n    # print name\n    __sets[name] = (lambda split=split: kitti(split))\n\n# Set up coco_2014_<split>\nfor year in [\'2014\']:\n    for split in [\'train\', \'val\', \'minival\', \'valminusminival\']:\n        name = \'coco_{}_{}\'.format(year, split)\n        __sets[name] = (lambda split=split, year=year: coco(split, year))\n\n# Set up coco_2015_<split>\nfor year in [\'2015\']:\n    for split in [\'test\', \'test-dev\']:\n        name = \'coco_{}_{}\'.format(year, split)\n        __sets[name] = (lambda split=split, year=year: coco(split, year))\n\n# NTHU dataset\nfor split in [\'71\', \'370\']:\n    name = \'nthu_{}\'.format(split)\n    # print name\n    __sets[name] = (lambda split=split: nthu(split))\n\n\ndef get_imdb(name):\n    """"""Get an imdb (image database) by name.""""""\n    if not __sets.has_key(name):\n        # print (list_imdbs())\n        raise KeyError(\'Unknown dataset: {}\'.format(name))\n    return __sets[name]()\n\n\ndef list_imdbs():\n    """"""List all registered imdbs.""""""\n    return __sets.keys()\n'"
faster_rcnn/datasets/imagenet3d.py,0,"b'import imagenet3d\nimport os\nimport PIL\nimport numpy as np\nimport scipy.sparse\nimport subprocess\nimport cPickle\nimport math\nimport sys\n\nfrom .imdb import imdb\nfrom .imdb import ROOT_DIR\nfrom ..utils.cython_bbox import bbox_overlaps\nfrom ..utils.boxes_grid import get_boxes_grid\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\nfrom ..rpn_msr.generate_anchors import generate_anchors\n# <<<< obsolete\n\n\nclass imagenet3d(imdb):\n    def __init__(self, image_set, imagenet3d_path=None):\n        imdb.__init__(self, \'imagenet3d_\' + image_set)\n        self._image_set = image_set\n        self._imagenet3d_path = self._get_default_path() if imagenet3d_path is None \\\n                            else imagenet3d_path\n        self._data_path = os.path.join(self._imagenet3d_path, \'Images\')\n        self._classes = (\'__background__\', \'aeroplane\', \'ashtray\', \'backpack\', \'basket\', \\\n             \'bed\', \'bench\', \'bicycle\', \'blackboard\', \'boat\', \'bookshelf\', \'bottle\', \'bucket\', \\\n             \'bus\', \'cabinet\', \'calculator\', \'camera\', \'can\', \'cap\', \'car\', \'cellphone\', \'chair\', \\\n             \'clock\', \'coffee_maker\', \'comb\', \'computer\', \'cup\', \'desk_lamp\', \'diningtable\', \\\n             \'dishwasher\', \'door\', \'eraser\', \'eyeglasses\', \'fan\', \'faucet\', \'filing_cabinet\', \\\n             \'fire_extinguisher\', \'fish_tank\', \'flashlight\', \'fork\', \'guitar\', \'hair_dryer\', \\\n             \'hammer\', \'headphone\', \'helmet\', \'iron\', \'jar\', \'kettle\', \'key\', \'keyboard\', \'knife\', \\\n             \'laptop\', \'lighter\', \'mailbox\', \'microphone\', \'microwave\', \'motorbike\', \'mouse\', \\\n             \'paintbrush\', \'pan\', \'pen\', \'pencil\', \'piano\', \'pillow\', \'plate\', \'pot\', \'printer\', \\\n             \'racket\', \'refrigerator\', \'remote_control\', \'rifle\', \'road_pole\', \'satellite_dish\', \\\n             \'scissors\', \'screwdriver\', \'shoe\', \'shovel\', \'sign\', \'skate\', \'skateboard\', \'slipper\', \\\n             \'sofa\', \'speaker\', \'spoon\', \'stapler\', \'stove\', \'suitcase\', \'teapot\', \'telephone\', \\\n             \'toaster\', \'toilet\', \'toothbrush\', \'train\', \'trash_bin\', \'trophy\', \'tub\', \'tvmonitor\', \\\n             \'vending_machine\', \'washing_machine\', \'watch\', \'wheelchair\')\n        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))\n        self._image_ext = \'.JPEG\'\n        self._image_index = self._load_image_set_index()\n        # Default to roidb handler\n        if cfg.IS_RPN:\n            self._roidb_handler = self.gt_roidb\n        else:\n            self._roidb_handler = self.region_proposal_roidb\n\n        self.config = {\'top_k\': 100000}\n\n        # statistics for computing recall\n        self._num_boxes_all = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_covered = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_proposal = 0\n\n        assert os.path.exists(self._imagenet3d_path), \\\n                \'imagenet3d path does not exist: {}\'.format(self._imagenet3d_path)\n        assert os.path.exists(self._data_path), \\\n                \'Path does not exist: {}\'.format(self._data_path)\n\n    def image_path_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return self.image_path_from_index(self.image_index[i])\n\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n\n        image_path = os.path.join(self._data_path, index + self._image_ext)\n        assert os.path.exists(image_path), \\\n                \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def _load_image_set_index(self):\n        """"""\n        Load the indexes listed in this dataset\'s image set file.\n        """"""\n        image_set_file = os.path.join(self._imagenet3d_path, \'Image_sets\', self._image_set + \'.txt\')\n        assert os.path.exists(image_set_file), \\\n                \'Path does not exist: {}\'.format(image_set_file)\n\n        with open(image_set_file) as f:\n            image_index = [x.rstrip(\'\\n\') for x in f.readlines()]\n        return image_index\n\n    def _get_default_path(self):\n        """"""\n        Return the default path where imagenet3d is expected to be installed.\n        """"""\n        return os.path.join(ROOT_DIR, \'data\', \'ImageNet3D\')\n\n\n    def gt_roidb(self):\n        """"""\n        Return the database of ground-truth regions of interest.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n\n        cache_file = os.path.join(self.cache_path, self.name + \'_\' + cfg.SUBCLS_NAME + \'_gt_roidb.pkl\')\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} gt roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        gt_roidb = [self._load_imagenet3d_annotation(index)\n                    for index in self.image_index]\n\n        if cfg.IS_RPN:\n            # print out recall\n            for i in xrange(1, self.num_classes):\n                print \'{}: Total number of boxes {:d}\'.format(self.classes[i], self._num_boxes_all[i])\n                print \'{}: Number of boxes covered {:d}\'.format(self.classes[i], self._num_boxes_covered[i])\n                print \'{}: Recall {:f}\'.format(self.classes[i], float(self._num_boxes_covered[i]) / float(self._num_boxes_all[i]))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(gt_roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote gt roidb to {}\'.format(cache_file)\n\n        return gt_roidb\n\n\n    def _load_imagenet3d_annotation(self, index):\n        """"""\n        Load image and bounding boxes info from txt file in the imagenet3d format.\n        """"""\n\n        if self._image_set == \'test\' or self._image_set == \'test_1\' or self._image_set == \'test_2\':\n            lines = []\n        else:\n            filename = os.path.join(self._imagenet3d_path, \'Labels\', index + \'.txt\')\n            lines = []\n            with open(filename) as f:\n                for line in f:\n                    lines.append(line)\n\n        num_objs = len(lines)\n\n        boxes = np.zeros((num_objs, 4), dtype=np.float32)\n        viewpoints = np.zeros((num_objs, 3), dtype=np.float32)          # azimuth, elevation, in-plane rotation\n        viewpoints_flipped = np.zeros((num_objs, 3), dtype=np.float32)  # azimuth, elevation, in-plane rotation\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n\n        for ix, line in enumerate(lines):\n            words = line.split()\n            assert len(words) == 5 or len(words) == 8, \'Wrong label format: {}\'.format(index)\n            cls = self._class_to_ind[words[0]]\n            boxes[ix, :] = [float(n) for n in words[1:5]]\n            gt_classes[ix] = cls\n            overlaps[ix, cls] = 1.0\n            if len(words) == 8:\n                viewpoints[ix, :] = [float(n) for n in words[5:8]]\n                # flip the viewpoint\n                viewpoints_flipped[ix, 0] = -viewpoints[ix, 0]  # azimuth\n                viewpoints_flipped[ix, 1] = viewpoints[ix, 1]   # elevation\n                viewpoints_flipped[ix, 2] = -viewpoints[ix, 2]  # in-plane rotation\n            else:\n                viewpoints[ix, :] = np.inf\n                viewpoints_flipped[ix, :] = np.inf\n\n        gt_subclasses = np.zeros((num_objs), dtype=np.int32)\n        gt_subclasses_flipped = np.zeros((num_objs), dtype=np.int32)\n        subindexes = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n        subindexes_flipped = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n        viewindexes_azimuth = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        viewindexes_azimuth_flipped = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        viewindexes_elevation = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        viewindexes_elevation_flipped = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        viewindexes_rotation = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        viewindexes_rotation_flipped = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n        subindexes = scipy.sparse.csr_matrix(subindexes)\n        subindexes_flipped = scipy.sparse.csr_matrix(subindexes_flipped)\n        viewindexes_azimuth = scipy.sparse.csr_matrix(viewindexes_azimuth)\n        viewindexes_azimuth_flipped = scipy.sparse.csr_matrix(viewindexes_azimuth_flipped)\n        viewindexes_elevation = scipy.sparse.csr_matrix(viewindexes_elevation)\n        viewindexes_elevation_flipped = scipy.sparse.csr_matrix(viewindexes_elevation_flipped)\n        viewindexes_rotation = scipy.sparse.csr_matrix(viewindexes_rotation)\n        viewindexes_rotation_flipped = scipy.sparse.csr_matrix(viewindexes_rotation_flipped)\n\n        if cfg.IS_RPN:\n            if cfg.IS_MULTISCALE:\n                # compute overlaps between grid boxes and gt boxes in multi-scales\n                # rescale the gt boxes\n                boxes_all = np.zeros((0, 4), dtype=np.float32)\n                for scale in cfg.TRAIN.SCALES:\n                    boxes_all = np.vstack((boxes_all, boxes * scale))\n                gt_classes_all = np.tile(gt_classes, len(cfg.TRAIN.SCALES))\n\n                # compute grid boxes\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n                boxes_grid, _, _ = get_boxes_grid(image_height, image_width)\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(boxes_grid.astype(np.float), boxes_all.astype(np.float))\n        \n                # check how many gt boxes are covered by grids\n                if num_objs != 0:\n                    index = np.tile(range(num_objs), len(cfg.TRAIN.SCALES))\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes_all == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n                    index_covered = np.unique(index[fg_inds])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[index_covered] == i)[0])\n            else:\n                assert len(cfg.TRAIN.SCALES_BASE) == 1\n                scale = cfg.TRAIN.SCALES_BASE[0]\n                feat_stride = 16\n                # faster rcnn region proposal\n                base_size = 16\n                ratios = cfg.TRAIN.RPN_ASPECTS\n                scales = cfg.TRAIN.RPN_SCALES\n                anchors = generate_anchors(base_size, ratios, scales)\n                num_anchors = anchors.shape[0]\n\n                # image size\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n\n                # height and width of the heatmap\n                height = np.round((image_height * scale - 1) / 4.0 + 1)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n\n                width = np.round((image_width * scale - 1) / 4.0 + 1)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n\n                # gt boxes\n                gt_boxes = boxes * scale\n\n                # 1. Generate proposals from bbox deltas and shifted anchors\n                shift_x = np.arange(0, width) * feat_stride\n                shift_y = np.arange(0, height) * feat_stride\n                shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n                shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                            shift_x.ravel(), shift_y.ravel())).transpose()\n                # add A anchors (1, A, 4) to\n                # cell K shifts (K, 1, 4) to get\n                # shift anchors (K, A, 4)\n                # reshape to (K*A, 4) shifted anchors\n                A = num_anchors\n                K = shifts.shape[0]\n                all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n                all_anchors = all_anchors.reshape((K * A, 4))\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(all_anchors.astype(np.float), gt_boxes.astype(np.float))\n        \n                # check how many gt boxes are covered by anchors\n                if num_objs != 0:\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[fg_inds] == i)[0])\n\n        return {\'boxes\' : boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_viewpoints\': viewpoints,\n                \'gt_viewpoints_flipped\': viewpoints_flipped,\n                \'gt_viewindexes_azimuth\': viewindexes_azimuth,\n                \'gt_viewindexes_azimuth_flipped\': viewindexes_azimuth_flipped,\n                \'gt_viewindexes_elevation\': viewindexes_elevation,\n                \'gt_viewindexes_elevation_flipped\': viewindexes_elevation_flipped,\n                \'gt_viewindexes_rotation\': viewindexes_rotation,\n                \'gt_viewindexes_rotation_flipped\': viewindexes_rotation_flipped,\n                \'gt_subclasses\': gt_subclasses,\n                \'gt_subclasses_flipped\': gt_subclasses_flipped,\n                \'gt_overlaps\' : overlaps,\n                \'gt_subindexes\': subindexes,\n                \'gt_subindexes_flipped\': subindexes_flipped,\n                \'flipped\' : False}\n\n\n    def region_proposal_roidb(self):\n        """"""\n        Return the database of regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_\' + cfg.REGION_PROPOSAL + \'_region_proposal_roidb.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        if self._image_set != \'test\':\n            gt_roidb = self.gt_roidb()\n\n            print \'Loading region proposal network boxes...\'\n            model = cfg.REGION_PROPOSAL\n            rpn_roidb = self._load_rpn_roidb(gt_roidb, model)\n            print \'Region proposal network boxes loaded\'\n            roidb = imdb.merge_roidbs(rpn_roidb, gt_roidb)\n        else:\n            print \'Loading region proposal network boxes...\'\n            model = cfg.REGION_PROPOSAL\n            roidb = self._load_rpn_roidb(None, model)\n            print \'Region proposal network boxes loaded\'\n\n        print \'{} region proposals per image\'.format(self._num_boxes_proposal / len(self.image_index))\n\n        # print out recall\n        if self._image_set != \'test\':\n            for i in xrange(1, self.num_classes):\n                print \'{}: Total number of boxes {:d}\'.format(self.classes[i], self._num_boxes_all[i])\n                print \'{}: Number of boxes covered {:d}\'.format(self.classes[i], self._num_boxes_covered[i])\n                if self._num_boxes_all[i] > 0:\n                    print \'{}: Recall {:f}\'.format(self.classes[i], float(self._num_boxes_covered[i]) / float(self._num_boxes_all[i]))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def _load_rpn_roidb(self, gt_roidb, model):\n\n        box_list = []\n        for ix, index in enumerate(self.image_index):\n            filename = os.path.join(self._imagenet3d_path, \'region_proposals\', model, index + \'.txt\')\n            assert os.path.exists(filename), \\\n                \'{} data not found at: {}\'.format(model, filename)\n            raw_data = np.loadtxt(filename, dtype=float)\n            if len(raw_data.shape) == 1:\n                if raw_data.size == 0:\n                    raw_data = raw_data.reshape((0, 5))\n                else:\n                    raw_data = raw_data.reshape((1, 5))\n\n            if model == \'selective_search\' or model == \'mcg\':\n                x1 = raw_data[:, 1].copy()\n                y1 = raw_data[:, 0].copy()\n                x2 = raw_data[:, 3].copy()\n                y2 = raw_data[:, 2].copy()\n            elif model == \'edge_boxes\':\n                x1 = raw_data[:, 0].copy()\n                y1 = raw_data[:, 1].copy()\n                x2 = raw_data[:, 2].copy() + raw_data[:, 0].copy()\n                y2 = raw_data[:, 3].copy() + raw_data[:, 1].copy()\n            elif model == \'rpn_caffenet\' or model == \'rpn_vgg16\':\n                x1 = raw_data[:, 0].copy()\n                y1 = raw_data[:, 1].copy()\n                x2 = raw_data[:, 2].copy()\n                y2 = raw_data[:, 3].copy()\n            else:\n                assert 1, \'region proposal not supported: {}\'.format(model)\n\n            inds = np.where((x2 > x1) & (y2 > y1))[0]\n            raw_data[:, 0] = x1\n            raw_data[:, 1] = y1\n            raw_data[:, 2] = x2\n            raw_data[:, 3] = y2\n            raw_data = raw_data[inds,:4]\n\n            self._num_boxes_proposal += raw_data.shape[0]\n            box_list.append(raw_data)\n            print \'load {}: {}\'.format(model, index)\n\n            if gt_roidb is not None:\n                # compute overlaps between region proposals and gt boxes\n                boxes = gt_roidb[ix][\'boxes\'].copy()\n                gt_classes = gt_roidb[ix][\'gt_classes\'].copy()\n                # compute overlap\n                overlaps = bbox_overlaps(raw_data.astype(np.float), boxes.astype(np.float))\n                # check how many gt boxes are covered by anchors\n                if raw_data.shape[0] != 0:\n                    max_overlaps = overlaps.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[fg_inds] == i)[0])\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n\n    def evaluate_detections(self, all_boxes, output_dir):\n\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing imagenet3d results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    # detection and viewpoint\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:s} {:f} {:f} {:f} {:f} {:.32f} {:f} {:f} {:f}\\n\'.format(\\\n                                 cls, dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4], dets[k, 6], dets[k, 7], dets[k, 8]))\n\n    # write detection results into one file\n    def evaluate_detections_one_file(self, all_boxes, output_dir):\n\n        # for each class\n        for cls_ind, cls in enumerate(self.classes):\n            if cls == \'__background__\':\n                continue\n            # open results file\n            filename = os.path.join(output_dir, \'detections_{}.txt\'.format(cls))\n            print \'Writing imagenet3d results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each image\n                for im_ind, index in enumerate(self.image_index):\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    # detection and viewpoint\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:s} {:f} {:f} {:f} {:f} {:.32f} {:f} {:f} {:f}\\n\'.format(\\\n                                 index, dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4], dets[k, 6], dets[k, 7], dets[k, 8]))\n\n    def evaluate_proposals(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing imagenet3d results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(\\\n                                 dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    def evaluate_proposals_msr(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing imagenet3d results to file \' + filename\n            with open(filename, \'wt\') as f:\n                dets = all_boxes[im_ind]\n                if dets == []:\n                    continue\n                for k in xrange(dets.shape[0]):\n                    f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n\nif __name__ == \'__main__\':\n    d = imagenet3d(\'trainval\')\n    res = d.roidb\n    from IPython import embed; embed()\n'"
faster_rcnn/datasets/imdb.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport os\nimport os.path as osp\nimport PIL\nimport numpy as np\nimport scipy.sparse\n\nfrom ..utils.cython_bbox import bbox_overlaps\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\n\n# <<<< obsolete\n\nROOT_DIR = osp.join(osp.dirname(__file__), \'..\', \'..\')\nMATLAB = \'matlab_r2013b\'\n\n\nclass imdb(object):\n    """"""Image database.""""""\n\n    def __init__(self, name):\n        self._name = name\n        self._num_classes = 0\n        self._classes = []\n        self._image_index = []\n        self._obj_proposer = \'selective_search\'\n        self._roidb = None\n        print self.default_roidb\n        self._roidb_handler = self.default_roidb\n        # Use this dict for storing dataset specific config options\n        self.config = {}\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def num_classes(self):\n        return len(self._classes)\n\n    @property\n    def classes(self):\n        return self._classes\n\n    @property\n    def image_index(self):\n        return self._image_index\n\n    @property\n    def roidb_handler(self):\n        return self._roidb_handler\n\n    @roidb_handler.setter\n    def roidb_handler(self, val):\n        self._roidb_handler = val\n\n    def set_proposal_method(self, method):\n        method = eval(\'self.\' + method + \'_roidb\')\n        self.roidb_handler = method\n\n    @property\n    def roidb(self):\n        # A roidb is a list of dictionaries, each with the following keys:\n        #   boxes\n        #   gt_overlaps\n        #   gt_classes\n        #   flipped\n        if self._roidb is not None:\n            return self._roidb\n        self._roidb = self.roidb_handler()\n        return self._roidb\n\n    @property\n    def cache_path(self):\n        cache_path = osp.abspath(osp.join(cfg.DATA_DIR, \'cache\'))\n        if not os.path.exists(cache_path):\n            os.makedirs(cache_path)\n        return cache_path\n\n    @property\n    def num_images(self):\n        return len(self.image_index)\n\n    def image_path_at(self, i):\n        raise NotImplementedError\n\n    def default_roidb(self):\n        raise NotImplementedError\n\n    def evaluate_detections(self, all_boxes, output_dir=None):\n        """"""\n        all_boxes is a list of length number-of-classes.\n        Each list element is a list of length number-of-images.\n        Each of those list elements is either an empty list []\n        or a numpy array of detection.\n\n        all_boxes[class][image] = [] or np.array of shape #dets x 5\n        """"""\n        raise NotImplementedError\n\n    def _get_widths(self):\n        return [PIL.Image.open(self.image_path_at(i)).size[0]\n                for i in xrange(self.num_images)]\n\n    def append_flipped_images(self):\n        num_images = self.num_images\n        widths = self._get_widths()\n        for i in xrange(num_images):\n            boxes = self.roidb[i][\'boxes\'].copy()\n            oldx1 = boxes[:, 0].copy()\n            oldx2 = boxes[:, 2].copy()\n            boxes[:, 0] = widths[i] - oldx2 - 1\n            boxes[:, 2] = widths[i] - oldx1 - 1\n            assert (boxes[:, 2] >= boxes[:, 0]).all()\n            entry = {\'boxes\': boxes,\n                     \'gt_overlaps\': self.roidb[i][\'gt_overlaps\'],\n                     \'gt_classes\': self.roidb[i][\'gt_classes\'],\n                     \'flipped\': True}\n\n            if \'gt_ishard\' in self.roidb[i] and \'dontcare_areas\' in self.roidb[i]:\n                entry[\'gt_ishard\'] = self.roidb[i][\'gt_ishard\'].copy()\n                dontcare_areas = self.roidb[i][\'dontcare_areas\'].copy()\n                oldx1 = dontcare_areas[:, 0].copy()\n                oldx2 = dontcare_areas[:, 2].copy()\n                dontcare_areas[:, 0] = widths[i] - oldx2 - 1\n                dontcare_areas[:, 2] = widths[i] - oldx1 - 1\n                entry[\'dontcare_areas\'] = dontcare_areas\n\n            self.roidb.append(entry)\n\n        self._image_index = self._image_index * 2\n\n    def evaluate_recall(self, candidate_boxes=None, thresholds=None,\n                        area=\'all\', limit=None):\n        """"""Evaluate detection proposal recall metrics.\n\n        Returns:\n            results: dictionary of results with keys\n                \'ar\': average recall\n                \'recalls\': vector recalls at each IoU overlap threshold\n                \'thresholds\': vector of IoU overlap thresholds\n                \'gt_overlaps\': vector of all ground-truth overlaps\n        """"""\n        # Record max overlap value for each gt box\n        # Return vector of overlap values\n        areas = {\'all\': 0, \'small\': 1, \'medium\': 2, \'large\': 3,\n                 \'96-128\': 4, \'128-256\': 5, \'256-512\': 6, \'512-inf\': 7}\n        area_ranges = [[0 ** 2, 1e5 ** 2],  # all\n                       [0 ** 2, 32 ** 2],  # small\n                       [32 ** 2, 96 ** 2],  # medium\n                       [96 ** 2, 1e5 ** 2],  # large\n                       [96 ** 2, 128 ** 2],  # 96-128\n                       [128 ** 2, 256 ** 2],  # 128-256\n                       [256 ** 2, 512 ** 2],  # 256-512\n                       [512 ** 2, 1e5 ** 2],  # 512-inf\n                       ]\n        assert areas.has_key(area), \'unknown area range: {}\'.format(area)\n        area_range = area_ranges[areas[area]]\n        gt_overlaps = np.zeros(0)\n        num_pos = 0\n        for i in xrange(self.num_images):\n            # Checking for max_overlaps == 1 avoids including crowd annotations\n            # (...pretty hacking :/)\n            max_gt_overlaps = self.roidb[i][\'gt_overlaps\'].toarray().max(axis=1)\n            gt_inds = np.where((self.roidb[i][\'gt_classes\'] > 0) &\n                               (max_gt_overlaps == 1))[0]\n            gt_boxes = self.roidb[i][\'boxes\'][gt_inds, :]\n            gt_areas = self.roidb[i][\'seg_areas\'][gt_inds]\n            valid_gt_inds = np.where((gt_areas >= area_range[0]) &\n                                     (gt_areas <= area_range[1]))[0]\n            gt_boxes = gt_boxes[valid_gt_inds, :]\n            num_pos += len(valid_gt_inds)\n\n            if candidate_boxes is None:\n                # If candidate_boxes is not supplied, the default is to use the\n                # non-ground-truth boxes from this roidb\n                non_gt_inds = np.where(self.roidb[i][\'gt_classes\'] == 0)[0]\n                boxes = self.roidb[i][\'boxes\'][non_gt_inds, :]\n            else:\n                boxes = candidate_boxes[i]\n            if boxes.shape[0] == 0:\n                continue\n            if limit is not None and boxes.shape[0] > limit:\n                boxes = boxes[:limit, :]\n\n            overlaps = bbox_overlaps(boxes.astype(np.float),\n                                     gt_boxes.astype(np.float))\n\n            _gt_overlaps = np.zeros((gt_boxes.shape[0]))\n            for j in xrange(gt_boxes.shape[0]):\n                # find which proposal box maximally covers each gt box\n                argmax_overlaps = overlaps.argmax(axis=0)\n                # and get the iou amount of coverage for each gt box\n                max_overlaps = overlaps.max(axis=0)\n                # find which gt box is \'best\' covered (i.e. \'best\' = most iou)\n                gt_ind = max_overlaps.argmax()\n                gt_ovr = max_overlaps.max()\n                assert (gt_ovr >= 0)\n                # find the proposal box that covers the best covered gt box\n                box_ind = argmax_overlaps[gt_ind]\n                # record the iou coverage of this gt box\n                _gt_overlaps[j] = overlaps[box_ind, gt_ind]\n                assert (_gt_overlaps[j] == gt_ovr)\n                # mark the proposal box and the gt box as used\n                overlaps[box_ind, :] = -1\n                overlaps[:, gt_ind] = -1\n            # append recorded iou coverage level\n            gt_overlaps = np.hstack((gt_overlaps, _gt_overlaps))\n\n        gt_overlaps = np.sort(gt_overlaps)\n        if thresholds is None:\n            step = 0.05\n            thresholds = np.arange(0.5, 0.95 + 1e-5, step)\n        recalls = np.zeros_like(thresholds)\n        # compute recall for each iou threshold\n        for i, t in enumerate(thresholds):\n            recalls[i] = (gt_overlaps >= t).sum() / float(num_pos)\n        # ar = 2 * np.trapz(recalls, thresholds)\n        ar = recalls.mean()\n        return {\'ar\': ar, \'recalls\': recalls, \'thresholds\': thresholds,\n                \'gt_overlaps\': gt_overlaps}\n\n    def create_roidb_from_box_list(self, box_list, gt_roidb):\n        assert len(box_list) == self.num_images, \\\n            \'Number of boxes must match number of ground-truth images\'\n        roidb = []\n        for i in xrange(self.num_images):\n            boxes = box_list[i]\n            num_boxes = boxes.shape[0]\n            overlaps = np.zeros((num_boxes, self.num_classes), dtype=np.float32)\n\n            if gt_roidb is not None and gt_roidb[i][\'boxes\'].size > 0:\n                gt_boxes = gt_roidb[i][\'boxes\']\n                gt_classes = gt_roidb[i][\'gt_classes\']\n                gt_overlaps = bbox_overlaps(boxes.astype(np.float),\n                                            gt_boxes.astype(np.float))\n                argmaxes = gt_overlaps.argmax(axis=1)\n                maxes = gt_overlaps.max(axis=1)\n                I = np.where(maxes > 0)[0]\n                overlaps[I, gt_classes[argmaxes[I]]] = maxes[I]\n\n            overlaps = scipy.sparse.csr_matrix(overlaps)\n            roidb.append({\n                \'boxes\': boxes,\n                \'gt_classes\': np.zeros((num_boxes,), dtype=np.int32),\n                \'gt_overlaps\': overlaps,\n                \'flipped\': False,\n                \'seg_areas\': np.zeros((num_boxes,), dtype=np.float32),\n            })\n        return roidb\n\n    @staticmethod\n    def merge_roidbs(a, b):\n        assert len(a) == len(b)\n        for i in xrange(len(a)):\n            a[i][\'boxes\'] = np.vstack((a[i][\'boxes\'], b[i][\'boxes\']))\n            a[i][\'gt_classes\'] = np.hstack((a[i][\'gt_classes\'],\n                                            b[i][\'gt_classes\']))\n            a[i][\'gt_overlaps\'] = scipy.sparse.vstack([a[i][\'gt_overlaps\'],\n                                                       b[i][\'gt_overlaps\']])\n            a[i][\'seg_areas\'] = np.hstack((a[i][\'seg_areas\'],\n                                           b[i][\'seg_areas\']))\n        return a\n\n    def competition_mode(self, on):\n        """"""Turn competition mode on or off.""""""\n        pass\n'"
faster_rcnn/datasets/imdb2.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport os\nimport os.path as osp\nimport PIL\nimport numpy as np\nimport scipy.sparse\n\nfrom . import ROOT_DIR\nfrom ..utils.cython_bbox import bbox_overlaps\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\n# <<<< obsolete\n\nclass imdb(object):\n    """"""Image database.""""""\n\n    def __init__(self, name):\n        self._name = name\n        self._num_classes = 0\n        self._num_subclasses = 0\n        self._subclass_mapping = []\n        self._classes = []\n        self._image_index = []\n        self._obj_proposer = \'selective_search\'\n        self._roidb = None\n        self._roidb_handler = self.default_roidb\n        # Use this dict for storing dataset specific config options\n        self.config = {}\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def num_classes(self):\n        return len(self._classes)\n\n    @property\n    def num_subclasses(self):\n        return self._num_subclasses\n\n    @property\n    def subclass_mapping(self):\n        return self._subclass_mapping\n\n    @property\n    def classes(self):\n        return self._classes\n\n    @property\n    def image_index(self):\n        return self._image_index\n\n    @property\n    def roidb_handler(self):\n        return self._roidb_handler\n\n    @roidb_handler.setter\n    def roidb_handler(self, val):\n        self._roidb_handler = val\n\n    @property\n    def roidb(self):\n        # A roidb is a list of dictionaries, each with the following keys:\n        #   boxes\n        #   gt_overlaps\n        #   gt_classes\n        #   flipped\n        if self._roidb is not None:\n            return self._roidb\n        self._roidb = self.roidb_handler()\n        return self._roidb\n\n    @property\n    def cache_path(self):\n        cache_path = osp.abspath(osp.join(ROOT_DIR, \'data\', \'cache\'))\n        if not os.path.exists(cache_path):\n            os.makedirs(cache_path)\n        return cache_path\n\n    @property\n    def num_images(self):\n      return len(self.image_index)\n\n    def image_path_at(self, i):\n        raise NotImplementedError\n\n    def default_roidb(self):\n        raise NotImplementedError\n\n    def evaluate_detections(self, all_boxes, output_dir=None):\n        """"""\n        all_boxes is a list of length number-of-classes.\n        Each list element is a list of length number-of-images.\n        Each of those list elements is either an empty list []\n        or a numpy array of detection.\n\n        all_boxes[class][image] = [] or np.array of shape #dets x 5\n        """"""\n        raise NotImplementedError\n\n    def evaluate_proposals(self, all_boxes, output_dir=None):\n        """"""\n        all_boxes is a list of length number-of-classes.\n        Each list element is a list of length number-of-images.\n        Each of those list elements is either an empty list []\n        or a numpy array of detection.\n\n        all_boxes[class][image] = [] or np.array of shape #dets x 5\n        """"""\n        raise NotImplementedError\n\n    def append_flipped_images(self):\n        num_images = self.num_images\n        widths = [PIL.Image.open(self.image_path_at(i)).size[0]\n                  for i in xrange(num_images)]\n        for i in xrange(num_images):\n            boxes = self.roidb[i][\'boxes\'].copy()\n            oldx1 = boxes[:, 0].copy()\n            oldx2 = boxes[:, 2].copy()\n            boxes[:, 0] = widths[i] - oldx2 - 1\n            boxes[:, 2] = widths[i] - oldx1 - 1\n            assert (boxes[:, 2] >= boxes[:, 0]).all()\n            assert (self.roidb[i][\'gt_subindexes\'].shape == self.roidb[i][\'gt_subindexes_flipped\'].shape), \\\n                \'gt_subindexes {}, gt_subindexes_flip {}\'.format(self.roidb[i][\'gt_subindexes\'].shape, \n                                                                 self.roidb[i][\'gt_subindexes_flipped\'].shape)\n\n            if cfg.TRAIN.VIEWPOINT == True or cfg.TEST.VIEWPOINT == True:\n                entry = {\'boxes\' : boxes,\n                         \'gt_overlaps\' : self.roidb[i][\'gt_overlaps\'],\n                         \'gt_classes\' : self.roidb[i][\'gt_classes\'],\n                         \'gt_viewpoints\' : self.roidb[i][\'gt_viewpoints_flipped\'],\n                         \'gt_viewpoints_flipped\' : self.roidb[i][\'gt_viewpoints\'],\n                         \'gt_viewindexes_azimuth\' : self.roidb[i][\'gt_viewindexes_azimuth_flipped\'],\n                         \'gt_viewindexes_azimuth_flipped\' : self.roidb[i][\'gt_viewindexes_azimuth\'],\n                         \'gt_viewindexes_elevation\' : self.roidb[i][\'gt_viewindexes_elevation_flipped\'],\n                         \'gt_viewindexes_elevation_flipped\' : self.roidb[i][\'gt_viewindexes_elevation\'],\n                         \'gt_viewindexes_rotation\' : self.roidb[i][\'gt_viewindexes_rotation_flipped\'],\n                         \'gt_viewindexes_rotation_flipped\' : self.roidb[i][\'gt_viewindexes_rotation\'],\n                         \'gt_subclasses\' : self.roidb[i][\'gt_subclasses_flipped\'],\n                         \'gt_subclasses_flipped\' : self.roidb[i][\'gt_subclasses\'],\n                         \'gt_subindexes\' : self.roidb[i][\'gt_subindexes_flipped\'],\n                         \'gt_subindexes_flipped\' : self.roidb[i][\'gt_subindexes\'],\n                         \'flipped\' : True}\n            else:\n                entry = {\'boxes\' : boxes,\n                         \'gt_overlaps\' : self.roidb[i][\'gt_overlaps\'],\n                         \'gt_classes\' : self.roidb[i][\'gt_classes\'],\n                         \'gt_subclasses\' : self.roidb[i][\'gt_subclasses_flipped\'],\n                         \'gt_subclasses_flipped\' : self.roidb[i][\'gt_subclasses\'],\n                         \'gt_subindexes\' : self.roidb[i][\'gt_subindexes_flipped\'],\n                         \'gt_subindexes_flipped\' : self.roidb[i][\'gt_subindexes\'],\n                         \'flipped\' : True}\n            self.roidb.append(entry)\n        self._image_index = self._image_index * 2\n        print \'finish appending flipped images\'\n\n    def evaluate_recall(self, candidate_boxes, ar_thresh=0.5):\n        # Record max overlap value for each gt box\n        # Return vector of overlap values\n        gt_overlaps = np.zeros(0)\n        for i in xrange(self.num_images):\n            gt_inds = np.where(self.roidb[i][\'gt_classes\'] > 0)[0]\n            gt_boxes = self.roidb[i][\'boxes\'][gt_inds, :]\n\n            boxes = candidate_boxes[i]\n            if boxes.shape[0] == 0:\n                continue\n            overlaps = bbox_overlaps(boxes.astype(np.float),\n                                     gt_boxes.astype(np.float))\n\n            # gt_overlaps = np.hstack((gt_overlaps, overlaps.max(axis=0)))\n            _gt_overlaps = np.zeros((gt_boxes.shape[0]))\n            for j in xrange(gt_boxes.shape[0]):\n                argmax_overlaps = overlaps.argmax(axis=0)\n                max_overlaps = overlaps.max(axis=0)\n                gt_ind = max_overlaps.argmax()\n                gt_ovr = max_overlaps.max()\n                assert(gt_ovr >= 0)\n                box_ind = argmax_overlaps[gt_ind]\n                _gt_overlaps[j] = overlaps[box_ind, gt_ind]\n                assert(_gt_overlaps[j] == gt_ovr)\n                overlaps[box_ind, :] = -1\n                overlaps[:, gt_ind] = -1\n\n            gt_overlaps = np.hstack((gt_overlaps, _gt_overlaps))\n\n        num_pos = gt_overlaps.size\n        gt_overlaps = np.sort(gt_overlaps)\n        step = 0.001\n        thresholds = np.minimum(np.arange(0.5, 1.0 + step, step), 1.0)\n        recalls = np.zeros_like(thresholds)\n        for i, t in enumerate(thresholds):\n            recalls[i] = (gt_overlaps >= t).sum() / float(num_pos)\n        ar = 2 * np.trapz(recalls, thresholds)\n\n        return ar, gt_overlaps, recalls, thresholds\n\n    def create_roidb_from_box_list(self, box_list, gt_roidb):\n        assert len(box_list) == self.num_images, \\\n                \'Number of boxes must match number of ground-truth images\'\n        roidb = []\n        for i in xrange(self.num_images):\n            boxes = box_list[i]\n            num_boxes = boxes.shape[0]\n            overlaps = np.zeros((num_boxes, self.num_classes), dtype=np.float32)\n            subindexes = np.zeros((num_boxes, self.num_classes), dtype=np.int32)\n            subindexes_flipped = np.zeros((num_boxes, self.num_classes), dtype=np.int32)\n            if cfg.TRAIN.VIEWPOINT == True or cfg.TEST.VIEWPOINT == True:\n                viewindexes_azimuth = np.zeros((num_boxes, self.num_classes), dtype=np.float32)\n                viewindexes_azimuth_flipped = np.zeros((num_boxes, self.num_classes), dtype=np.float32)\n                viewindexes_elevation = np.zeros((num_boxes, self.num_classes), dtype=np.float32)\n                viewindexes_elevation_flipped = np.zeros((num_boxes, self.num_classes), dtype=np.float32)\n                viewindexes_rotation = np.zeros((num_boxes, self.num_classes), dtype=np.float32)\n                viewindexes_rotation_flipped = np.zeros((num_boxes, self.num_classes), dtype=np.float32)\n\n            if gt_roidb is not None:\n                gt_boxes = gt_roidb[i][\'boxes\']\n                if gt_boxes.shape[0] != 0 and num_boxes != 0:\n                    gt_classes = gt_roidb[i][\'gt_classes\']\n                    gt_subclasses = gt_roidb[i][\'gt_subclasses\']\n                    gt_subclasses_flipped = gt_roidb[i][\'gt_subclasses_flipped\']\n                    if cfg.TRAIN.VIEWPOINT == True or cfg.TEST.VIEWPOINT == True:\n                        gt_viewpoints = gt_roidb[i][\'gt_viewpoints\']\n                        gt_viewpoints_flipped = gt_roidb[i][\'gt_viewpoints_flipped\']\n                    gt_overlaps = bbox_overlaps(boxes.astype(np.float),\n                                            gt_boxes.astype(np.float))\n                    argmaxes = gt_overlaps.argmax(axis=1)\n                    maxes = gt_overlaps.max(axis=1)\n                    I = np.where(maxes > 0)[0]\n                    overlaps[I, gt_classes[argmaxes[I]]] = maxes[I]\n                    subindexes[I, gt_classes[argmaxes[I]]] = gt_subclasses[argmaxes[I]]\n                    subindexes_flipped[I, gt_classes[argmaxes[I]]] = gt_subclasses_flipped[argmaxes[I]]\n                    if cfg.TRAIN.VIEWPOINT == True or cfg.TEST.VIEWPOINT == True:\n                        viewindexes_azimuth[I, gt_classes[argmaxes[I]]] = gt_viewpoints[argmaxes[I], 0]\n                        viewindexes_azimuth_flipped[I, gt_classes[argmaxes[I]]] = gt_viewpoints_flipped[argmaxes[I], 0]\n                        viewindexes_elevation[I, gt_classes[argmaxes[I]]] = gt_viewpoints[argmaxes[I], 1]\n                        viewindexes_elevation_flipped[I, gt_classes[argmaxes[I]]] = gt_viewpoints_flipped[argmaxes[I], 1]\n                        viewindexes_rotation[I, gt_classes[argmaxes[I]]] = gt_viewpoints[argmaxes[I], 2]\n                        viewindexes_rotation_flipped[I, gt_classes[argmaxes[I]]] = gt_viewpoints_flipped[argmaxes[I], 2]\n\n            overlaps = scipy.sparse.csr_matrix(overlaps)\n            subindexes = scipy.sparse.csr_matrix(subindexes)\n            subindexes_flipped = scipy.sparse.csr_matrix(subindexes_flipped)\n\n            if cfg.TRAIN.VIEWPOINT == True or cfg.TEST.VIEWPOINT == True:\n                viewindexes_azimuth = scipy.sparse.csr_matrix(viewindexes_azimuth)\n                viewindexes_azimuth_flipped = scipy.sparse.csr_matrix(viewindexes_azimuth_flipped)\n                viewindexes_elevation = scipy.sparse.csr_matrix(viewindexes_elevation)\n                viewindexes_elevation_flipped = scipy.sparse.csr_matrix(viewindexes_elevation_flipped)\n                viewindexes_rotation = scipy.sparse.csr_matrix(viewindexes_rotation)\n                viewindexes_rotation_flipped = scipy.sparse.csr_matrix(viewindexes_rotation_flipped)\n                roidb.append({\'boxes\' : boxes,\n                              \'gt_classes\' : np.zeros((num_boxes,), dtype=np.int32),\n                              \'gt_viewpoints\': np.zeros((num_boxes, 3), dtype=np.float32),\n                              \'gt_viewpoints_flipped\': np.zeros((num_boxes, 3), dtype=np.float32),\n                              \'gt_viewindexes_azimuth\': viewindexes_azimuth,\n                              \'gt_viewindexes_azimuth_flipped\': viewindexes_azimuth_flipped,\n                              \'gt_viewindexes_elevation\': viewindexes_elevation,\n                              \'gt_viewindexes_elevation_flipped\': viewindexes_elevation_flipped,\n                              \'gt_viewindexes_rotation\': viewindexes_rotation,\n                              \'gt_viewindexes_rotation_flipped\': viewindexes_rotation_flipped,\n                              \'gt_subclasses\' : np.zeros((num_boxes,), dtype=np.int32),\n                              \'gt_subclasses_flipped\' : np.zeros((num_boxes,), dtype=np.int32),\n                              \'gt_overlaps\' : overlaps,\n                              \'gt_subindexes\': subindexes,\n                              \'gt_subindexes_flipped\': subindexes_flipped,\n                              \'flipped\' : False})\n            else:\n                roidb.append({\'boxes\' : boxes,\n                              \'gt_classes\' : np.zeros((num_boxes,), dtype=np.int32),\n                              \'gt_subclasses\' : np.zeros((num_boxes,), dtype=np.int32),\n                              \'gt_subclasses_flipped\' : np.zeros((num_boxes,), dtype=np.int32),\n                              \'gt_overlaps\' : overlaps,\n                              \'gt_subindexes\': subindexes,\n                              \'gt_subindexes_flipped\': subindexes_flipped,\n                              \'flipped\' : False})\n        return roidb\n\n    @staticmethod\n    def merge_roidbs(a, b):\n        assert len(a) == len(b)\n        for i in xrange(len(a)):\n            a[i][\'boxes\'] = np.vstack((a[i][\'boxes\'], b[i][\'boxes\']))\n            a[i][\'gt_classes\'] = np.hstack((a[i][\'gt_classes\'],\n                                            b[i][\'gt_classes\']))\n            if cfg.TRAIN.VIEWPOINT == True or cfg.TEST.VIEWPOINT == True:\n                a[i][\'gt_viewpoints\'] = np.vstack((a[i][\'gt_viewpoints\'],\n                                                b[i][\'gt_viewpoints\']))\n                a[i][\'gt_viewpoints_flipped\'] = np.vstack((a[i][\'gt_viewpoints_flipped\'],\n                                                b[i][\'gt_viewpoints_flipped\']))\n                a[i][\'gt_viewindexes_azimuth\'] = scipy.sparse.vstack((a[i][\'gt_viewindexes_azimuth\'],\n                                                b[i][\'gt_viewindexes_azimuth\']))\n                a[i][\'gt_viewindexes_azimuth_flipped\'] = scipy.sparse.vstack((a[i][\'gt_viewindexes_azimuth_flipped\'],\n                                                b[i][\'gt_viewindexes_azimuth_flipped\']))\n                a[i][\'gt_viewindexes_elevation\'] = scipy.sparse.vstack((a[i][\'gt_viewindexes_elevation\'],\n                                                b[i][\'gt_viewindexes_elevation\']))\n                a[i][\'gt_viewindexes_elevation_flipped\'] = scipy.sparse.vstack((a[i][\'gt_viewindexes_elevation_flipped\'],\n                                                b[i][\'gt_viewindexes_elevation_flipped\']))\n                a[i][\'gt_viewindexes_rotation\'] = scipy.sparse.vstack((a[i][\'gt_viewindexes_rotation\'],\n                                                b[i][\'gt_viewindexes_rotation\']))\n                a[i][\'gt_viewindexes_rotation_flipped\'] = scipy.sparse.vstack((a[i][\'gt_viewindexes_rotation_flipped\'],\n                                                b[i][\'gt_viewindexes_rotation_flipped\']))\n\n            a[i][\'gt_subclasses\'] = np.hstack((a[i][\'gt_subclasses\'],\n                                            b[i][\'gt_subclasses\']))\n            a[i][\'gt_subclasses_flipped\'] = np.hstack((a[i][\'gt_subclasses_flipped\'],\n                                            b[i][\'gt_subclasses_flipped\']))\n            a[i][\'gt_overlaps\'] = scipy.sparse.vstack([a[i][\'gt_overlaps\'],\n                                                       b[i][\'gt_overlaps\']])\n            a[i][\'gt_subindexes\'] = scipy.sparse.vstack((a[i][\'gt_subindexes\'],\n                                            b[i][\'gt_subindexes\']))\n            a[i][\'gt_subindexes_flipped\'] = scipy.sparse.vstack((a[i][\'gt_subindexes_flipped\'],\n                                            b[i][\'gt_subindexes_flipped\']))\n        return a\n\n    def competition_mode(self, on):\n        """"""Turn competition mode on or off.""""""\n        pass\n'"
faster_rcnn/datasets/kitti.py,0,"b'import os\nimport PIL\nimport numpy as np\nimport scipy.sparse\nimport subprocess\nimport cPickle\nimport math\nimport glob\n\nfrom .imdb import imdb\nfrom .imdb import ROOT_DIR\n\nfrom ..utils.cython_bbox import bbox_overlaps\nfrom ..utils.boxes_grid import get_boxes_grid\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\nfrom ..rpn_msr.generate_anchors import generate_anchors\n# <<<< obsolete\n\nclass kitti(imdb):\n    def __init__(self, image_set, kitti_path=None):\n\n        imdb.__init__(self, \'kitti_\' + image_set)\n        self._image_set = image_set\n        self._kitti_path = self._get_default_path() if kitti_path is None \\\n                            else kitti_path\n        self._data_path = os.path.join(self._kitti_path, \'data_object_image_2\')\n        self._classes = (\'__background__\', \'Car\', \'Pedestrian\', \'Cyclist\')\n        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))\n        self._image_ext = \'.png\'\n        self._image_index = self._load_image_set_index_new()\n        # Default to roidb handler\n        if cfg.IS_RPN:\n            self._roidb_handler = self.gt_roidb\n        else:\n            self._roidb_handler = self.region_proposal_roidb\n\n        # num of subclasses\n        if image_set == \'train\' or image_set == \'val\':\n            self._num_subclasses = 125 + 24 + 24 + 1\n            prefix = \'validation\'\n        else:\n            self._num_subclasses = 227 + 36 + 36 + 1\n            prefix = \'test\'\n\n        self.config = {\'top_k\': 100000}\n\n        # statistics for computing recall\n        self._num_boxes_all = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_covered = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_proposal = 0\n\n        assert os.path.exists(self._kitti_path), \\\n                \'KITTI path does not exist: {}\'.format(self._kitti_path)\n        assert os.path.exists(self._data_path), \\\n                \'Path does not exist: {}\'.format(self._data_path)\n\n    def image_path_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return self.image_path_from_index(self.image_index[i])\n\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n        # set the prefix\n        if self._image_set == \'test\':\n            prefix = \'testing/image_2\'\n        else:\n            prefix = \'training/image_2\'\n\n        image_path = os.path.join(self._data_path, prefix, index + self._image_ext)\n        assert os.path.exists(image_path), \\\n                \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def _load_image_set_index(self):\n        """"""\n        obsolete, using _load_image_set_index_new instead\n        Load the indexes listed in this dataset\'s image set file.\n        """"""\n        image_set_file = os.path.join(self._kitti_path, self._image_set + \'.txt\')\n        assert os.path.exists(image_set_file), \\\n                \'Path does not exist: {}\'.format(image_set_file)\n\n        with open(image_set_file) as f:\n            image_index = [x.rstrip(\'\\n\') for x in f.readlines()]\n        return image_index\n\n    def _load_image_set_index_new(self):\n        """"""\n        Load the indexes listed in this dataset\'s image set file.\n        """"""\n        image_set_file = os.path.join(self._kitti_path, \'training/image_2/\')\n        assert os.path.exists(image_set_file), \\\n                \'Path does not exist: {}\'.format(image_set_file)\n\n        image_index = os.listdir(image_set_file)\n        image_set_file = image_set_file + \'*.png\'\n        image_index = glob.glob(image_set_file)\n\n        return image_index\n\n    def _get_default_path(self):\n        """"""\n        Return the default path where KITTI is expected to be installed.\n        """"""\n        return os.path.join(cfg.DATA_DIR, \'KITTI\')\n\n\n    def gt_roidb(self):\n        """"""\n        Return the database of ground-truth regions of interest.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n\n        cache_file = os.path.join(self.cache_path, self.name + \'_\' + cfg.SUBCLS_NAME + \'_gt_roidb.pkl\')\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} gt roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        gt_roidb = [self._load_kitti_voxel_exemplar_annotation(index)\n                    for index in self.image_index]\n\n        if cfg.IS_RPN:\n            # print out recall\n            for i in xrange(1, self.num_classes):\n                print \'{}: Total number of boxes {:d}\'.format(self.classes[i], self._num_boxes_all[i])\n                print \'{}: Number of boxes covered {:d}\'.format(self.classes[i], self._num_boxes_covered[i])\n                print \'{}: Recall {:f}\'.format(self.classes[i], float(self._num_boxes_covered[i]) / float(self._num_boxes_all[i]))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(gt_roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote gt roidb to {}\'.format(cache_file)\n\n        return gt_roidb\n\n\n    def _load_kitti_annotation(self, index):\n        """"""\n        Load image and bounding boxes info from txt file in the KITTI format.\n        """"""\n\n        if self._image_set == \'test\':\n            lines = []\n        else:\n            filename = os.path.join(self._data_path, \'training\', \'label_2\', index + \'.txt\')\n            lines = []\n            with open(filename) as f:\n                for line in f:\n                    line = line.replace(\'Van\', \'Car\')\n                    words = line.split()\n                    cls = words[0]\n                    truncation = float(words[1])\n                    occlusion = int(words[2])\n                    height = float(words[7]) - float(words[5])\n                    if cls in self._class_to_ind and truncation < 0.5 and occlusion < 3 and height > 25:\n                        lines.append(line)\n\n        num_objs = len(lines)\n\n        boxes = np.zeros((num_objs, 4), dtype=np.float32)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n\n        for ix, line in enumerate(lines):\n            words = line.split()\n            cls = self._class_to_ind[words[0]]\n            boxes[ix, :] = [float(n) for n in words[4:8]]\n            gt_classes[ix] = cls\n            overlaps[ix, cls] = 1.0\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n        gt_subclasses = np.zeros((num_objs), dtype=np.int32)\n        gt_subclasses_flipped = np.zeros((num_objs), dtype=np.int32)\n        subindexes = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n        subindexes_flipped = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n        subindexes = scipy.sparse.csr_matrix(subindexes)\n        subindexes_flipped = scipy.sparse.csr_matrix(subindexes_flipped)\n\n        if cfg.IS_RPN:\n            if cfg.IS_MULTISCALE:\n                # compute overlaps between grid boxes and gt boxes in multi-scales\n                # rescale the gt boxes\n                boxes_all = np.zeros((0, 4), dtype=np.float32)\n                for scale in cfg.TRAIN.SCALES:\n                    boxes_all = np.vstack((boxes_all, boxes * scale))\n                gt_classes_all = np.tile(gt_classes, len(cfg.TRAIN.SCALES))\n\n                # compute grid boxes\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n                boxes_grid, _, _ = get_boxes_grid(image_height, image_width)\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(boxes_grid.astype(np.float), boxes_all.astype(np.float))\n        \n                # check how many gt boxes are covered by grids\n                if num_objs != 0:\n                    index = np.tile(range(num_objs), len(cfg.TRAIN.SCALES))\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes_all == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n                    index_covered = np.unique(index[fg_inds])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[index_covered] == i)[0])\n            else:\n                assert len(cfg.TRAIN.SCALES_BASE) == 1\n                scale = cfg.TRAIN.SCALES_BASE[0]\n                feat_stride = 16\n                # faster rcnn region proposal\n                anchors = generate_anchors()\n                num_anchors = anchors.shape[0]\n\n                # image size\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n\n                # height and width of the heatmap\n                height = np.round((image_height * scale - 1) / 4.0 + 1)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n\n                width = np.round((image_width * scale - 1) / 4.0 + 1)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n\n                # gt boxes\n                gt_boxes = boxes * scale\n\n                # 1. Generate proposals from bbox deltas and shifted anchors\n                shift_x = np.arange(0, width) * feat_stride\n                shift_y = np.arange(0, height) * feat_stride\n                shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n                shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                            shift_x.ravel(), shift_y.ravel())).transpose()\n                # add A anchors (1, A, 4) to\n                # cell K shifts (K, 1, 4) to get\n                # shift anchors (K, A, 4)\n                # reshape to (K*A, 4) shifted anchors\n                A = num_anchors\n                K = shifts.shape[0]\n                all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n                all_anchors = all_anchors.reshape((K * A, 4))\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(all_anchors.astype(np.float), gt_boxes.astype(np.float))\n        \n                # check how many gt boxes are covered by anchors\n                if num_objs != 0:\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[fg_inds] == i)[0])\n\n        return {\'boxes\' : boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_subclasses\': gt_subclasses,\n                \'gt_subclasses_flipped\': gt_subclasses_flipped,\n                \'gt_overlaps\' : overlaps,\n                \'gt_subindexes\': subindexes,\n                \'gt_subindexes_flipped\': subindexes_flipped,\n                \'flipped\' : False}\n\n    def _load_kitti_voxel_exemplar_annotation(self, index):\n        """"""\n        Load image and bounding boxes info from txt file in the KITTI voxel exemplar format.\n        """"""\n        if self._image_set == \'train\':\n            prefix = \'validation\'\n        elif self._image_set == \'trainval\':\n            prefix = \'test\'\n        else:\n            return self._load_kitti_annotation(index)\n\n        filename = os.path.join(self._kitti_path, cfg.SUBCLS_NAME, prefix, index + \'.txt\')\n        assert os.path.exists(filename), \\\n                \'Path does not exist: {}\'.format(filename)\n\n        # the annotation file contains flipped objects    \n        lines = []\n        lines_flipped = []\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[1])\n                is_flip = int(words[2])\n                if subcls != -1:\n                    if is_flip == 0:\n                        lines.append(line)\n                    else:\n                        lines_flipped.append(line)\n        \n        num_objs = len(lines)\n\n        # store information of flipped objects\n        assert (num_objs == len(lines_flipped)), \'The number of flipped objects is not the same!\'\n        gt_subclasses_flipped = np.zeros((num_objs), dtype=np.int32)\n        \n        for ix, line in enumerate(lines_flipped):\n            words = line.split()\n            subcls = int(words[1])\n            gt_subclasses_flipped[ix] = subcls\n\n        boxes = np.zeros((num_objs, 4), dtype=np.float32)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        gt_subclasses = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        subindexes = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n        subindexes_flipped = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n\n        for ix, line in enumerate(lines):\n            words = line.split()\n            cls = self._class_to_ind[words[0]]\n            subcls = int(words[1])\n            boxes[ix, :] = [float(n) for n in words[3:7]]\n            gt_classes[ix] = cls\n            gt_subclasses[ix] = subcls\n            overlaps[ix, cls] = 1.0\n            subindexes[ix, cls] = subcls\n            subindexes_flipped[ix, cls] = gt_subclasses_flipped[ix]\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n        subindexes = scipy.sparse.csr_matrix(subindexes)\n        subindexes_flipped = scipy.sparse.csr_matrix(subindexes_flipped)\n\n        if cfg.IS_RPN:\n            if cfg.IS_MULTISCALE:\n                # compute overlaps between grid boxes and gt boxes in multi-scales\n                # rescale the gt boxes\n                boxes_all = np.zeros((0, 4), dtype=np.float32)\n                for scale in cfg.TRAIN.SCALES:\n                    boxes_all = np.vstack((boxes_all, boxes * scale))\n                gt_classes_all = np.tile(gt_classes, len(cfg.TRAIN.SCALES))\n\n                # compute grid boxes\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n                boxes_grid, _, _ = get_boxes_grid(image_height, image_width)\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(boxes_grid.astype(np.float), boxes_all.astype(np.float))\n        \n                # check how many gt boxes are covered by grids\n                if num_objs != 0:\n                    index = np.tile(range(num_objs), len(cfg.TRAIN.SCALES))\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes_all == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n                    index_covered = np.unique(index[fg_inds])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[index_covered] == i)[0])\n            else:\n                assert len(cfg.TRAIN.SCALES_BASE) == 1\n                scale = cfg.TRAIN.SCALES_BASE[0]\n                feat_stride = 16\n                # faster rcnn region proposal\n                base_size = 16\n                ratios = [3.0, 2.0, 1.5, 1.0, 0.75, 0.5, 0.25]\n                scales = 2**np.arange(1, 6, 0.5)\n                anchors = generate_anchors(base_size, ratios, scales)\n                num_anchors = anchors.shape[0]\n\n                # image size\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n\n                # height and width of the heatmap\n                height = np.round((image_height * scale - 1) / 4.0 + 1)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n\n                width = np.round((image_width * scale - 1) / 4.0 + 1)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n\n                # gt boxes\n                gt_boxes = boxes * scale\n\n                # 1. Generate proposals from bbox deltas and shifted anchors\n                shift_x = np.arange(0, width) * feat_stride\n                shift_y = np.arange(0, height) * feat_stride\n                shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n                shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                            shift_x.ravel(), shift_y.ravel())).transpose()\n                # add A anchors (1, A, 4) to\n                # cell K shifts (K, 1, 4) to get\n                # shift anchors (K, A, 4)\n                # reshape to (K*A, 4) shifted anchors\n                A = num_anchors\n                K = shifts.shape[0]\n                all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n                all_anchors = all_anchors.reshape((K * A, 4))\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(all_anchors.astype(np.float), gt_boxes.astype(np.float))\n        \n                # check how many gt boxes are covered by anchors\n                if num_objs != 0:\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[fg_inds] == i)[0])\n\n        return {\'boxes\' : boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_subclasses\': gt_subclasses,\n                \'gt_subclasses_flipped\': gt_subclasses_flipped,\n                \'gt_overlaps\': overlaps,\n                \'gt_subindexes\': subindexes, \n                \'gt_subindexes_flipped\': subindexes_flipped, \n                \'flipped\' : False}\n\n    def region_proposal_roidb(self):\n        """"""\n        Return the database of regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_\' + cfg.SUBCLS_NAME + \'_\' + cfg.REGION_PROPOSAL + \'_region_proposal_roidb.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        if self._image_set != \'test\':\n            gt_roidb = self.gt_roidb()\n\n            print \'Loading region proposal network boxes...\'\n            if self._image_set == \'trainval\':\n                model = cfg.REGION_PROPOSAL + \'_227/\'\n            else:\n                model = cfg.REGION_PROPOSAL + \'_125/\'\n            rpn_roidb = self._load_rpn_roidb(gt_roidb, model)\n            print \'Region proposal network boxes loaded\'\n            roidb = imdb.merge_roidbs(rpn_roidb, gt_roidb)\n\n            # print \'Loading voxel pattern boxes...\'\n            # if self._image_set == \'trainval\':\n            #    model = \'3DVP_227\'\n            # else:\n            #    model = \'3DVP_125/\'\n            # vp_roidb = self._load_voxel_pattern_roidb(gt_roidb, model)\n            # print \'Voxel pattern boxes loaded\'\n            # roidb = imdb.merge_roidbs(vp_roidb, gt_roidb)\n\n            # print \'Loading selective search boxes...\'\n            # ss_roidb = self._load_selective_search_roidb(gt_roidb)\n            # print \'Selective search boxes loaded\'\n\n            # print \'Loading ACF boxes...\'\n            # acf_roidb = self._load_acf_roidb(gt_roidb)\n            # print \'ACF boxes loaded\'\n\n            # roidb = imdb.merge_roidbs(ss_roidb, gt_roidb)\n            # roidb = imdb.merge_roidbs(roidb, acf_roidb)\n        else:\n            print \'Loading region proposal network boxes...\'\n            model = cfg.REGION_PROPOSAL + \'_227/\'\n            roidb = self._load_rpn_roidb(None, model)\n            print \'Region proposal network boxes loaded\'\n\n            # print \'Loading voxel pattern boxes...\'\n            # model = \'3DVP_227/\'\n            # roidb = self._load_voxel_pattern_roidb(None, model)\n            # print \'Voxel pattern boxes loaded\'\n\n            # print \'Loading selective search boxes...\'\n            # roidb = self._load_selective_search_roidb(None)\n            # print \'Selective search boxes loaded\'\n\n            # print \'Loading ACF boxes...\'\n            # acf_roidb = self._load_acf_roidb(None)\n            # print \'ACF boxes loaded\'\n\n            # roidb = imdb.merge_roidbs(roidb, acf_roidb)\n        print \'{} region proposals per image\'.format(self._num_boxes_proposal / len(self.image_index))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def _load_rpn_roidb(self, gt_roidb, model):\n        # set the prefix\n        if self._image_set == \'test\':\n            prefix = model + \'testing\'\n        else:\n            prefix = model + \'training\'\n\n        box_list = []\n        for index in self.image_index:\n            filename = os.path.join(self._kitti_path, \'region_proposals\',  prefix, index + \'.txt\')\n            assert os.path.exists(filename), \\\n                \'RPN data not found at: {}\'.format(filename)\n            raw_data = np.loadtxt(filename, dtype=float)\n            if len(raw_data.shape) == 1:\n                if raw_data.size == 0:\n                    raw_data = raw_data.reshape((0, 5))\n                else:\n                    raw_data = raw_data.reshape((1, 5))\n\n            x1 = raw_data[:, 0]\n            y1 = raw_data[:, 1]\n            x2 = raw_data[:, 2]\n            y2 = raw_data[:, 3]\n            score = raw_data[:, 4]\n            inds = np.where((x2 > x1) & (y2 > y1))[0]\n            raw_data = raw_data[inds,:4]\n            self._num_boxes_proposal += raw_data.shape[0]\n            box_list.append(raw_data)\n            print \'load {}: {}\'.format(model, index)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def _load_voxel_pattern_roidb(self, gt_roidb, model):\n        # set the prefix\n        if self._image_set == \'test\':\n            prefix = model + \'testing\'\n        else:\n            prefix = model + \'training\'\n\n        box_list = []\n        for index in self.image_index:\n            filename = os.path.join(self._kitti_path, \'region_proposals\',  prefix, index + \'.txt\')\n            assert os.path.exists(filename), \\\n                \'Voxel pattern data not found at: {}\'.format(filename)\n            raw_data = np.loadtxt(filename, dtype=float)\n            if len(raw_data.shape) == 1:\n                if raw_data.size == 0:\n                    raw_data = raw_data.reshape((0, 4))\n                else:\n                    raw_data = raw_data.reshape((1, 4))\n            self._num_boxes_proposal += raw_data.shape[0]\n            box_list.append(raw_data)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def _load_selective_search_roidb(self, gt_roidb):\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_selective_search_box_list.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                box_list = cPickle.load(fid)\n            print \'{} boxes loaded from {}\'.format(self.name, cache_file)\n        else:\n            # set the prefix\n            model = \'selective_search/\'\n            if self._image_set == \'test\':\n                prefix = model + \'testing\'\n            else:\n                prefix = model + \'training\'\n\n            box_list = []\n            for index in self.image_index:\n                filename = os.path.join(self._kitti_path, \'region_proposals\', prefix, index + \'.txt\')\n                assert os.path.exists(filename), \\\n                    \'Selective search data not found at: {}\'.format(filename)\n                raw_data = np.loadtxt(filename, dtype=float)\n                box_list.append(raw_data[:min(self.config[\'top_k\'], raw_data.shape[0]), 1:])\n\n            with open(cache_file, \'wb\') as fid:\n                cPickle.dump(box_list, fid, cPickle.HIGHEST_PROTOCOL)\n            print \'wrote selective search boxes to {}\'.format(cache_file)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def _load_acf_roidb(self, gt_roidb):\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_acf_box_list.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                box_list = cPickle.load(fid)\n            print \'{} boxes loaded from {}\'.format(self.name, cache_file)\n        else:\n            # set the prefix\n            model = \'ACF/\'\n            if self._image_set == \'test\':\n                prefix = model + \'testing\'\n            else:\n                prefix = model + \'training\'\n\n            box_list = []\n            for index in self.image_index:\n                filename = os.path.join(self._kitti_path, \'region_proposals\', prefix, index + \'.txt\')\n                assert os.path.exists(filename), \\\n                    \'ACF data not found at: {}\'.format(filename)\n                raw_data = np.loadtxt(filename, usecols=(2,3,4,5), dtype=float)\n                box_list.append(raw_data[:min(self.config[\'top_k\'], raw_data.shape[0]), :])\n\n            with open(cache_file, \'wb\') as fid:\n                cPickle.dump(box_list, fid, cPickle.HIGHEST_PROTOCOL)\n            print \'wrote ACF boxes to {}\'.format(cache_file)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def evaluate_detections(self, all_boxes, output_dir):\n        # load the mapping for subcalss the alpha (viewpoint)\n        if self._image_set == \'val\':\n            prefix = \'validation\'\n        elif self._image_set == \'test\':\n            prefix = \'test\'\n        else:\n            prefix = \'\'\n\n        filename = os.path.join(self._kitti_path, cfg.SUBCLS_NAME, prefix, \'mapping.txt\')\n        assert os.path.exists(filename), \\\n                \'Path does not exist: {}\'.format(filename)\n\n        mapping = np.zeros(self._num_subclasses, dtype=np.float)\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[0])\n                mapping[subcls] = float(words[3])\n\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing KITTI results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        if cfg.TEST.SUBCLS:\n                            subcls = int(dets[k, 5])\n                            cls_name = self.classes[self.subclass_mapping[subcls]]\n                            assert (cls_name == cls), \'subclass not in class\'\n                            alpha = mapping[subcls]\n                        else:\n                            alpha = -10\n                        f.write(\'{:s} -1 -1 {:f} {:f} {:f} {:f} {:f} -1 -1 -1 -1 -1 -1 -1 {:.32f}\\n\'.format(\\\n                                 cls, alpha, dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    # write detection results into one file\n    def evaluate_detections_one_file(self, all_boxes, output_dir):\n        # open results file\n        filename = os.path.join(output_dir, \'detections.txt\')\n        print \'Writing all KITTI results to file \' + filename\n        with open(filename, \'wt\') as f:\n            # for each image\n            for im_ind, index in enumerate(self.image_index):\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        if cfg.TEST.SUBCLS:\n                            subcls = int(dets[k, 5])\n                            cls_name = self.classes[self.subclass_mapping[subcls]]\n                            assert (cls_name == cls), \'subclass not in class\'\n                        else:\n                            subcls = -1\n                        f.write(\'{:s} {:s} {:f} {:f} {:f} {:f} {:d} {:f}\\n\'.format(\\\n                                 index, cls, dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], subcls, dets[k, 4]))\n\n    def evaluate_proposals(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing KITTI results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(\\\n                                 dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    def evaluate_proposals_msr(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing KITTI results to file \' + filename\n            with open(filename, \'wt\') as f:\n                dets = all_boxes[im_ind]\n                if dets == []:\n                    continue\n                for k in xrange(dets.shape[0]):\n                    f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n\nif __name__ == \'__main__\':\n    d = kitti(\'train\')\n    res = d.roidb\n    from IPython import embed; embed()\n'"
faster_rcnn/datasets/kitti_tracking.py,0,"b'import os\nimport PIL\nimport numpy as np\nimport scipy.sparse\nimport subprocess\nimport cPickle\nimport math\n\nfrom .imdb import imdb\nfrom .imdb import ROOT_DIR\n\nfrom ..utils.cython_bbox import bbox_overlaps\nfrom ..utils.boxes_grid import get_boxes_grid\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\nfrom ..rpn_msr.generate_anchors import generate_anchors\n\n\n# <<<< obsolete\n\nclass kitti_tracking(imdb):\n    def __init__(self, image_set, seq_name, kitti_tracking_path=None):\n        imdb.__init__(self, \'kitti_tracking_\' + image_set + \'_\' + seq_name)\n        self._image_set = image_set\n        self._seq_name = seq_name\n        self._kitti_tracking_path = self._get_default_path() if kitti_tracking_path is None \\\n            else kitti_tracking_path\n        self._data_path = os.path.join(self._kitti_tracking_path, image_set, \'image_02\')\n        self._classes = (\'__background__\', \'Car\', \'Pedestrian\', \'Cyclist\')\n        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))\n        self._image_ext = \'.png\'\n        self._image_index = self._load_image_set_index()\n        # Default to roidb handler\n        if cfg.IS_RPN:\n            self._roidb_handler = self.gt_roidb\n        else:\n            self._roidb_handler = self.region_proposal_roidb\n\n        # num of subclasses\n        if image_set == \'training\' and seq_name != \'trainval\':\n            self._num_subclasses = 220 + 1\n        else:\n            self._num_subclasses = 472 + 1\n\n        # load the mapping for subcalss to class\n        if image_set == \'training\' and seq_name != \'trainval\':\n            filename = os.path.join(self._kitti_tracking_path, \'voxel_exemplars\', \'train\', \'mapping.txt\')\n        else:\n            filename = os.path.join(self._kitti_tracking_path, \'voxel_exemplars\', \'trainval\', \'mapping.txt\')\n        assert os.path.exists(filename), \'Path does not exist: {}\'.format(filename)\n\n        mapping = np.zeros(self._num_subclasses, dtype=np.int)\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[0])\n                mapping[subcls] = self._class_to_ind[words[1]]\n        self._subclass_mapping = mapping\n\n        self.config = {\'top_k\': 100000}\n\n        # statistics for computing recall\n        self._num_boxes_all = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_covered = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_proposal = 0\n\n        assert os.path.exists(self._kitti_tracking_path), \\\n            \'kitti_tracking path does not exist: {}\'.format(self._kitti_tracking_path)\n        assert os.path.exists(self._data_path), \\\n            \'Path does not exist: {}\'.format(self._data_path)\n\n    def image_path_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return self.image_path_from_index(self.image_index[i])\n\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n\n        image_path = os.path.join(self._data_path, index + self._image_ext)\n        assert os.path.exists(image_path), \\\n            \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def _load_image_set_index(self):\n        """"""\n        Load the indexes listed in this dataset\'s image set file.\n        """"""\n\n        kitti_train_nums = [154, 447, 233, 144, 314, 297, 270, 800, 390, 803, 294, \\\n                            373, 78, 340, 106, 376, 209, 145, 339, 1059, 837]\n\n        kitti_test_nums = [465, 147, 243, 257, 421, 809, 114, 215, 165, 349, 1176, \\\n                           774, 694, 152, 850, 701, 510, 305, 180, 404, 173, 203, \\\n                           436, 430, 316, 176, 170, 85, 175]\n\n        if self._seq_name == \'train\' or self._seq_name == \'trainval\':\n\n            assert self._image_set == \'training\', \'Use train set or trainval set in testing\'\n\n            if self._seq_name == \'train\':\n                seq_index = [0, 1, 2, 3, 4, 5, 12, 13, 14, 15, 16]\n            else:\n                seq_index = range(0, 21)\n\n            # for each sequence\n            image_index = []\n            for i in xrange(len(seq_index)):\n                seq_idx = seq_index[i]\n                num = kitti_train_nums[seq_idx]\n                for j in xrange(num):\n                    image_index.append(\'{:04d}/{:06d}\'.format(seq_idx, j))\n        else:\n            # a single sequence\n            seq_num = int(self._seq_name)\n            if self._image_set == \'training\':\n                num = kitti_train_nums[seq_num]\n            else:\n                num = kitti_test_nums[seq_num]\n            image_index = []\n            for i in xrange(num):\n                image_index.append(\'{:04d}/{:06d}\'.format(seq_num, i))\n\n        return image_index\n\n    def _get_default_path(self):\n        """"""\n        Return the default path where kitti_tracking is expected to be installed.\n        """"""\n        return os.path.join(ROOT_DIR, \'data\', \'KITTI_Tracking\')\n\n    def gt_roidb(self):\n        """"""\n        Return the database of ground-truth regions of interest.\n        """"""\n\n        cache_file = os.path.join(self.cache_path, self.name + \'_\' + cfg.SUBCLS_NAME + \'_gt_roidb.pkl\')\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} gt roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        gt_roidb = [self._load_kitti_voxel_exemplar_annotation(index)\n                    for index in self.image_index]\n\n        if cfg.IS_RPN:\n            # print out recall\n            for i in xrange(1, self.num_classes):\n                print \'{}: Total number of boxes {:d}\'.format(self.classes[i], self._num_boxes_all[i])\n                print \'{}: Number of boxes covered {:d}\'.format(self.classes[i], self._num_boxes_covered[i])\n                print \'{}: Recall {:f}\'.format(self.classes[i],\n                                               float(self._num_boxes_covered[i]) / float(self._num_boxes_all[i]))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(gt_roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote gt roidb to {}\'.format(cache_file)\n\n        return gt_roidb\n\n    def _load_kitti_voxel_exemplar_annotation(self, index):\n        """"""\n        Load image and bounding boxes info from txt file in the KITTI voxel exemplar format.\n        """"""\n        if self._image_set == \'training\' and self._seq_name != \'trainval\':\n            prefix = \'train\'\n        elif self._image_set == \'training\':\n            prefix = \'trainval\'\n        else:\n            prefix = \'\'\n\n        if prefix == \'\':\n            lines = []\n            lines_flipped = []\n        else:\n            filename = os.path.join(self._kitti_tracking_path, cfg.SUBCLS_NAME, prefix, index + \'.txt\')\n            if os.path.exists(filename):\n                print filename\n\n                # the annotation file contains flipped objects    \n                lines = []\n                lines_flipped = []\n                with open(filename) as f:\n                    for line in f:\n                        words = line.split()\n                        subcls = int(words[1])\n                        is_flip = int(words[2])\n                        if subcls != -1:\n                            if is_flip == 0:\n                                lines.append(line)\n                            else:\n                                lines_flipped.append(line)\n            else:\n                lines = []\n                lines_flipped = []\n\n        num_objs = len(lines)\n\n        # store information of flipped objects\n        assert (num_objs == len(lines_flipped)), \'The number of flipped objects is not the same!\'\n        gt_subclasses_flipped = np.zeros((num_objs), dtype=np.int32)\n\n        for ix, line in enumerate(lines_flipped):\n            words = line.split()\n            subcls = int(words[1])\n            gt_subclasses_flipped[ix] = subcls\n\n        boxes = np.zeros((num_objs, 4), dtype=np.float32)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        gt_subclasses = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        subindexes = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n        subindexes_flipped = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n\n        for ix, line in enumerate(lines):\n            words = line.split()\n            cls = self._class_to_ind[words[0]]\n            subcls = int(words[1])\n            boxes[ix, :] = [float(n) for n in words[3:7]]\n            gt_classes[ix] = cls\n            gt_subclasses[ix] = subcls\n            overlaps[ix, cls] = 1.0\n            subindexes[ix, cls] = subcls\n            subindexes_flipped[ix, cls] = gt_subclasses_flipped[ix]\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n        subindexes = scipy.sparse.csr_matrix(subindexes)\n        subindexes_flipped = scipy.sparse.csr_matrix(subindexes_flipped)\n\n        if cfg.IS_RPN:\n            if cfg.IS_MULTISCALE:\n                # compute overlaps between grid boxes and gt boxes in multi-scales\n                # rescale the gt boxes\n                boxes_all = np.zeros((0, 4), dtype=np.float32)\n                for scale in cfg.TRAIN.SCALES:\n                    boxes_all = np.vstack((boxes_all, boxes * scale))\n                gt_classes_all = np.tile(gt_classes, len(cfg.TRAIN.SCALES))\n\n                # compute grid boxes\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n                boxes_grid, _, _ = get_boxes_grid(image_height, image_width)\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(boxes_grid.astype(np.float), boxes_all.astype(np.float))\n\n                # check how many gt boxes are covered by grids\n                if num_objs != 0:\n                    index = np.tile(range(num_objs), len(cfg.TRAIN.SCALES))\n                    max_overlaps = overlaps_grid.max(axis=0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(\n                            np.where((gt_classes_all == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k - 1]))[0])\n                    index_covered = np.unique(index[fg_inds])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[index_covered] == i)[0])\n            else:\n                assert len(cfg.TRAIN.SCALES_BASE) == 1\n                scale = cfg.TRAIN.SCALES_BASE[0]\n                feat_stride = 16\n                # faster rcnn region proposal\n                base_size = 16\n                ratios = [3.0, 2.0, 1.5, 1.0, 0.75, 0.5, 0.25]\n                scales = 2 ** np.arange(1, 6, 0.5)\n                anchors = generate_anchors(base_size, ratios, scales)\n                num_anchors = anchors.shape[0]\n\n                # image size\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n\n                # height and width of the heatmap\n                height = np.round((image_height * scale - 1) / 4.0 + 1)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n\n                width = np.round((image_width * scale - 1) / 4.0 + 1)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n\n                # gt boxes\n                gt_boxes = boxes * scale\n\n                # 1. Generate proposals from bbox deltas and shifted anchors\n                shift_x = np.arange(0, width) * feat_stride\n                shift_y = np.arange(0, height) * feat_stride\n                shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n                shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                                    shift_x.ravel(), shift_y.ravel())).transpose()\n                # add A anchors (1, A, 4) to\n                # cell K shifts (K, 1, 4) to get\n                # shift anchors (K, A, 4)\n                # reshape to (K*A, 4) shifted anchors\n                A = num_anchors\n                K = shifts.shape[0]\n                all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n                all_anchors = all_anchors.reshape((K * A, 4))\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(all_anchors.astype(np.float), gt_boxes.astype(np.float))\n\n                # check how many gt boxes are covered by anchors\n                if num_objs != 0:\n                    max_overlaps = overlaps_grid.max(axis=0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k - 1]))[0])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[fg_inds] == i)[0])\n\n        return {\'boxes\': boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_subclasses\': gt_subclasses,\n                \'gt_subclasses_flipped\': gt_subclasses_flipped,\n                \'gt_overlaps\': overlaps,\n                \'gt_subindexes\': subindexes,\n                \'gt_subindexes_flipped\': subindexes_flipped,\n                \'flipped\': False}\n\n    def region_proposal_roidb(self):\n        """"""\n        Return the database of regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_\' + cfg.SUBCLS_NAME + \'_\' + cfg.REGION_PROPOSAL + \'_region_proposal_roidb.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        if self._image_set != \'testing\':\n            gt_roidb = self.gt_roidb()\n\n            print \'Loading region proposal network boxes...\'\n            if self._image_set == \'trainval\':\n                model = cfg.REGION_PROPOSAL + \'_trainval/\'\n            else:\n                model = cfg.REGION_PROPOSAL + \'_train/\'\n            rpn_roidb = self._load_rpn_roidb(gt_roidb, model)\n            print \'Region proposal network boxes loaded\'\n            roidb = imdb.merge_roidbs(rpn_roidb, gt_roidb)\n        else:\n            print \'Loading region proposal network boxes...\'\n            model = cfg.REGION_PROPOSAL + \'_trainval/\'\n            roidb = self._load_rpn_roidb(None, model)\n            print \'Region proposal network boxes loaded\'\n\n        print \'{} region proposals per image\'.format(self._num_boxes_proposal / len(self.image_index))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def _load_rpn_roidb(self, gt_roidb, model):\n        # set the prefix\n        prefix = model\n\n        box_list = []\n        for index in self.image_index:\n            filename = os.path.join(self._kitti_tracking_path, \'region_proposals\', prefix, self._image_set,\n                                    index + \'.txt\')\n            assert os.path.exists(filename), \\\n                \'RPN data not found at: {}\'.format(filename)\n            print filename\n            raw_data = np.loadtxt(filename, dtype=float)\n            if len(raw_data.shape) == 1:\n                if raw_data.size == 0:\n                    raw_data = raw_data.reshape((0, 5))\n                else:\n                    raw_data = raw_data.reshape((1, 5))\n\n            x1 = raw_data[:, 0]\n            y1 = raw_data[:, 1]\n            x2 = raw_data[:, 2]\n            y2 = raw_data[:, 3]\n            score = raw_data[:, 4]\n            inds = np.where((x2 > x1) & (y2 > y1))[0]\n            raw_data = raw_data[inds, :4]\n            self._num_boxes_proposal += raw_data.shape[0]\n            box_list.append(raw_data)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def evaluate_detections(self, all_boxes, output_dir):\n        # load the mapping for subcalss the alpha (viewpoint)\n        if self._image_set == \'training\' and self._seq_name != \'trainval\':\n            filename = os.path.join(self._kitti_tracking_path, \'voxel_exemplars\', \'train\', \'mapping.txt\')\n        else:\n            filename = os.path.join(self._kitti_tracking_path, \'voxel_exemplars\', \'trainval\', \'mapping.txt\')\n        assert os.path.exists(filename), \'Path does not exist: {}\'.format(filename)\n\n        mapping = np.zeros(self._num_subclasses, dtype=np.float)\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[0])\n                mapping[subcls] = float(words[3])\n\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index[5:] + \'.txt\')\n            print \'Writing kitti_tracking results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        subcls = int(dets[k, 5])\n                        cls_name = self.classes[self.subclass_mapping[subcls]]\n                        assert (cls_name == cls), \'subclass not in class\'\n                        alpha = mapping[subcls]\n                        f.write(\'{:s} -1 -1 {:f} {:f} {:f} {:f} {:f} -1 -1 -1 -1 -1 -1 -1 {:.32f}\\n\'.format( \\\n                            cls, alpha, dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    # write detection results into one file\n    def evaluate_detections_one_file(self, all_boxes, output_dir):\n        # load the mapping for subcalss the alpha (viewpoint)\n        if self._image_set == \'training\' and self._seq_name != \'trainval\':\n            filename = os.path.join(self._kitti_tracking_path, \'voxel_exemplars\', \'train\', \'mapping.txt\')\n        else:\n            filename = os.path.join(self._kitti_tracking_path, \'voxel_exemplars\', \'trainval\', \'mapping.txt\')\n        assert os.path.exists(filename), \'Path does not exist: {}\'.format(filename)\n\n        mapping = np.zeros(self._num_subclasses, dtype=np.float)\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[0])\n                mapping[subcls] = float(words[3])\n\n        # open results file\n        filename = os.path.join(output_dir, self._seq_name + \'.txt\')\n        print \'Writing all kitti_tracking results to file \' + filename\n        with open(filename, \'wt\') as f:\n            # for each image\n            for im_ind, index in enumerate(self.image_index):\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        subcls = int(dets[k, 5])\n                        cls_name = self.classes[self.subclass_mapping[subcls]]\n                        assert (cls_name == cls), \'subclass not in class\'\n                        alpha = mapping[subcls]\n                        f.write(\n                            \'{:d} -1 {:s} -1 -1 {:f} {:f} {:f} {:f} {:f} -1 -1 -1 -1000 -1000 -1000 -10 {:f}\\n\'.format( \\\n                                im_ind, cls, alpha, dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    def evaluate_proposals(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index[5:] + \'.txt\')\n            print \'Writing kitti_tracking results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format( \\\n                            dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    def evaluate_proposals_msr(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing kitti_tracking results to file \' + filename\n            with open(filename, \'wt\') as f:\n                dets = all_boxes[im_ind]\n                if dets == []:\n                    continue\n                for k in xrange(dets.shape[0]):\n                    f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3],\n                                                                   dets[k, 4]))\n\n\nif __name__ == \'__main__\':\n    d = kitti_tracking(\'training\', \'0000\')\n    res = d.roidb\n    from IPython import embed;\n\n    embed()\n'"
faster_rcnn/datasets/kittivoc.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport xml.dom.minidom as minidom\n\nimport os\n# import PIL\nimport numpy as np\nimport scipy.sparse\nimport subprocess\nimport cPickle\nimport math\nimport glob\nimport uuid\nimport scipy.io as sio\nimport xml.etree.ElementTree as ET\n\nfrom .imdb import imdb\nfrom .imdb import ROOT_DIR\nimport ds_utils\nfrom .voc_eval import voc_eval\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\n# <<<< obsolete\n\n\nclass kittivoc(imdb):\n    def __init__(self, image_set, devkit_path=None):\n        imdb.__init__(self, \'kittivoc_\' + image_set)\n        self._image_set = image_set\n        self._devkit_path = self._get_default_path() if devkit_path is None \\\n                            else devkit_path\n        self._data_path = self._devkit_path\n        self._classes = (\'__background__\', # always index 0\n                         \'pedestrian\', \'car\', \'cyclist\')\n        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))\n        self._image_ext = \'.jpg\'\n        self._image_index = self._load_image_set_index()\n        self._remove_empty_samples()\n        # Default to roidb handler\n        #self._roidb_handler = self.selective_search_roidb\n        self._roidb_handler = self.gt_roidb\n        self._salt = str(uuid.uuid4())\n        self._comp_id = \'comp4\'\n        self._year = \'\'\n        # PASCAL specific config options\n        self.config = {\'cleanup\'     : True,\n                       \'use_salt\'    : True,\n                       \'use_diff\'    : False, # using difficult samples\n                       \'matlab_eval\' : False,\n                       \'rpn_file\'    : None,\n                       \'min_size\'    : 2}\n\n        assert os.path.exists(self._devkit_path), \\\n                \'VOCdevkit path does not exist: {}\'.format(self._devkit_path)\n        assert os.path.exists(self._data_path), \\\n                \'Path does not exist: {}\'.format(self._data_path)\n\n    def image_path_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return self.image_path_from_index(self._image_index[i])\n\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier\n        :param index filename stem e.g. 000000\n        :return filepath\n        """"""\n        image_path = os.path.join(self._data_path, \'JPEGImages\',\n                                  index + self._image_ext)\n        assert os.path.exists(image_path), \\\n                \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def _load_image_set_index(self):\n        """"""\n        Load the indexes listed in this dataset\'s image set file.\n        """"""\n        # Example path to image set file:\n        image_set_file = os.path.join(self._data_path, \'ImageSets\', \'Main\',\n                                      self._image_set + \'.txt\')\n        assert os.path.exists(image_set_file), \\\n                \'Path does not exist: {}\'.format(image_set_file)\n        with open(image_set_file) as f:\n            image_index = [x.strip() for x in f.readlines()]\n        return image_index\n\n    def _get_default_path(self):\n        """"""\n        Return the default path where PASCAL VOC is expected to be installed.\n        """"""\n        return os.path.join(cfg.DATA_DIR, \'KITTIVOC\')\n\n    def gt_roidb(self):\n        """"""\n        Return the database of ground-truth regions of interest, aka, the annotations.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path, self.name + \'_gt_roidb.pkl\')\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} gt roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        gt_roidb = [self._load_pascal_annotation(index)\n                    for index in self.image_index]\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(gt_roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote gt roidb to {}\'.format(cache_file)\n\n        return gt_roidb\n\n    def selective_search_roidb(self):\n        """"""\n        Return the database of selective search regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_selective_search_roidb.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} ss roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        if self._image_set != \'test\':\n            gt_roidb = self.gt_roidb()\n            ss_roidb = self._load_selective_search_roidb(gt_roidb)\n            roidb = imdb.merge_roidbs(gt_roidb, ss_roidb)\n        else:\n            roidb = self._load_selective_search_roidb(None)\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote ss roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def rpn_roidb(self):\n        if self._image_set != \'test\':\n            gt_roidb = self.gt_roidb()\n            rpn_roidb = self._load_rpn_roidb(gt_roidb)\n            roidb = imdb.merge_roidbs(gt_roidb, rpn_roidb)\n        else:\n            roidb = self._load_rpn_roidb(None)\n\n        return roidb\n\n    def _load_rpn_roidb(self, gt_roidb):\n        filename = self.config[\'rpn_file\']\n        print \'loading {}\'.format(filename)\n        assert os.path.exists(filename), \\\n               \'rpn data not found at: {}\'.format(filename)\n        with open(filename, \'rb\') as f:\n            box_list = cPickle.load(f)\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def _load_selective_search_roidb(self, gt_roidb):\n        filename = os.path.abspath(os.path.join(self._data_path,\n                                                \'selective_search_data\',\n                                                self.name + \'.mat\'))\n        assert os.path.exists(filename), \\\n               \'Selective search data not found at: {}\'.format(filename)\n        raw_data = sio.loadmat(filename)[\'boxes\'].ravel()\n\n        box_list = []\n        for i in xrange(raw_data.shape[0]):\n            boxes = raw_data[i][:, (1, 0, 3, 2)] - 1\n            keep = ds_utils.unique_boxes(boxes)\n            boxes = boxes[keep, :]\n            keep = ds_utils.filter_small_boxes(boxes, self.config[\'min_size\'])\n            boxes = boxes[keep, :]\n            box_list.append(boxes)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def _remove_empty_samples(self):\n        """"""\n        Remove images with zero annotation ()\n        """"""\n        print \'Remove empty annotations: \',\n        for i in range(len(self._image_index)-1, -1, -1):\n            index = self._image_index[i]\n            filename = os.path.join(self._data_path, \'Annotations\', index + \'.xml\')\n            tree = ET.parse(filename)\n            objs = tree.findall(\'object\')\n            non_diff_objs = [\n                obj for obj in objs if \\\n                    int(obj.find(\'difficult\').text) == 0 and obj.find(\'name\').text.lower().strip() != \'dontcare\']\n            num_objs = len(non_diff_objs)\n            if num_objs == 0:\n                print index,\n                self._image_index.pop(i)\n        print \'Done. \'\n\n    def _load_pascal_annotation(self, index):\n        """"""\n        Load image and bounding boxes info from XML file in the PASCAL VOC\n        format.\n        """"""\n        filename = os.path.join(self._data_path, \'Annotations\', index + \'.xml\')\n        tree = ET.parse(filename)\n        objs = tree.findall(\'object\')\n        # if not self.config[\'use_diff\']:\n        #     # Exclude the samples labeled as difficult\n        #     non_diff_objs = [\n        #         obj for obj in objs if int(obj.find(\'difficult\').text) == 0]\n        #     objs = non_diff_objs\n        num_objs = len(objs)\n\n        boxes = np.zeros((num_objs, 4), dtype=np.int32)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        # just the same as gt_classes\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        # ""Seg"" area for pascal is just the box area\n        seg_areas = np.zeros((num_objs), dtype=np.float32)\n\n        ishards = np.zeros((num_objs), dtype=np.int32)\n        care_inds = np.empty((0), dtype=np.int32)\n        dontcare_inds = np.empty((0), dtype=np.int32)\n\n        # Load object bounding boxes into a data frame.\n        for ix, obj in enumerate(objs):\n            bbox = obj.find(\'bndbox\')\n            # Make pixel indexes 0-based\n            x1 = max(float(bbox.find(\'xmin\').text) - 1, 0)\n            y1 = max(float(bbox.find(\'ymin\').text) - 1, 0)\n            x2 = float(bbox.find(\'xmax\').text) - 1\n            y2 = float(bbox.find(\'ymax\').text) - 1\n\n            diffc = obj.find(\'difficult\')\n            difficult = 0 if diffc == None else int(diffc.text)\n            ishards[ix] = difficult\n\n            class_name = obj.find(\'name\').text.lower().strip()\n            if class_name != \'dontcare\':\n                care_inds = np.append(care_inds, np.asarray([ix], dtype=np.int32))\n            if class_name == \'dontcare\':\n                dontcare_inds = np.append(dontcare_inds, np.asarray([ix], dtype=np.int32))\n                boxes[ix, :] = [x1, y1, x2, y2]\n                continue\n            cls = self._class_to_ind[class_name]\n            boxes[ix, :] = [x1, y1, x2, y2]\n            gt_classes[ix] = cls\n            overlaps[ix, cls] = 1.0\n            seg_areas[ix] = (x2 - x1 + 1) * (y2 - y1 + 1)\n\n        # deal with dontcare areas\n        dontcare_areas = boxes[dontcare_inds, :]\n        boxes = boxes[care_inds, :]\n        gt_classes = gt_classes[care_inds]\n        overlaps = overlaps[care_inds, :]\n        seg_areas = seg_areas[care_inds]\n        ishards = ishards[care_inds]\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n\n        return {\'boxes\' : boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_ishard\' : ishards,\n                \'dontcare_areas\' : dontcare_areas,\n                \'gt_overlaps\' : overlaps,\n                \'flipped\' : False,\n                \'seg_areas\' : seg_areas}\n\n    def _get_comp_id(self):\n        comp_id = (self._comp_id + \'_\' + self._salt if self.config[\'use_salt\']\n            else self._comp_id)\n        return comp_id\n\n    def _get_voc_results_file_template(self):\n        # VOCdevkit/results/VOC2007/Main/<comp_id>_det_test_aeroplane.txt\n        filename = self._get_comp_id() + \'_det_\' + self._image_set + \'_{:s}.txt\'\n        filedir = os.path.join(self._devkit_path, \'results\', \'KITTI\', \'Main\')\n        if not os.path.exists(filedir):\n            os.makedirs(filedir)\n        path = os.path.join(filedir, filename)\n        return path\n\n    def _write_voc_results_file(self, all_boxes):\n        for cls_ind, cls in enumerate(self.classes):\n            if cls == \'__background__\':\n                continue\n            print \'Writing {} VOC results file\'.format(cls)\n            filename = self._get_voc_results_file_template().format(cls)\n            with open(filename, \'wt\') as f:\n                for im_ind, index in enumerate(self.image_index):\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    # the VOCdevkit expects 1-based indices\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\\n\'.\n                            format(index, dets[k, -1],              # filename(stem), score\n                                   dets[k, 0] + 1, dets[k, 1] + 1,  # x1, y1, x2, y2\n                                   dets[k, 2] + 1, dets[k, 3] + 1))\n\n    def _do_python_eval(self, output_dir = \'output\'):\n        annopath = os.path.join(\n            self._devkit_path,\n            \'Annotations\', \'{:s}.xml\')\n        imagesetfile = os.path.join(\n            self._devkit_path,\n            \'ImageSets\', \'Main\',\n            self._image_set + \'.txt\')\n        cachedir = os.path.join(self._devkit_path, \'annotations_cache\')\n        aps = []\n        # The PASCAL VOC metric changed in 2010\n        use_07_metric = False\n        print \'VOC07 metric? \' + (\'Yes\' if use_07_metric else \'No\')\n        if not os.path.isdir(output_dir):\n            os.mkdir(output_dir)\n        for i, cls in enumerate(self._classes):\n            if cls == \'__background__\':\n                continue\n            filename = self._get_voc_results_file_template().format(cls)\n            rec, prec, ap = voc_eval(filename, annopath, imagesetfile, cls, cachedir,\n                                     ovthresh=0.5, use_07_metric = use_07_metric)\n            aps += [ap]\n            print(\'AP for {} = {:.4f}\'.format(cls, ap))\n            with open(os.path.join(output_dir, cls + \'_pr.pkl\'), \'w\') as f:\n                cPickle.dump({\'rec\': rec, \'prec\': prec, \'ap\': ap}, f)\n        print(\'Mean AP = {:.4f}\'.format(np.mean(aps)))\n        print(\'~~~~~~~~\')\n        print(\'Results:\')\n        for ap in aps:\n            print(\'{:.3f}\'.format(ap))\n        print(\'{:.3f}\'.format(np.mean(aps)))\n        print(\'~~~~~~~~\')\n        print(\'\')\n        print(\'--------------------------------------------------------------\')\n        print(\'Results computed with the **unofficial** Python eval code.\')\n        print(\'Results should be very close to the official MATLAB eval code.\')\n        print(\'Recompute with `./tools/reval.py --matlab ...` for your paper.\')\n        print(\'-- Thanks, The Management\')\n        print(\'--------------------------------------------------------------\')\n\n    def _do_matlab_eval(self, output_dir=\'output\'):\n        print \'-----------------------------------------------------\'\n        print \'Computing results with the official MATLAB eval code.\'\n        print \'-----------------------------------------------------\'\n        path = os.path.join(cfg.ROOT_DIR, \'lib\', \'datasets\',\n                            \'VOCdevkit-matlab-wrapper\')\n        cmd = \'cd {} && \'.format(path)\n        cmd += \'{:s} -nodisplay -nodesktop \'.format(cfg.MATLAB)\n        cmd += \'-r ""dbstop if error; \'\n        cmd += \'voc_eval(\\\'{:s}\\\',\\\'{:s}\\\',\\\'{:s}\\\',\\\'{:s}\\\'); quit;""\' \\\n               .format(self._devkit_path, self._get_comp_id(),\n                       self._image_set, output_dir)\n        print(\'Running:\\n{}\'.format(cmd))\n        status = subprocess.call(cmd, shell=True)\n\n\n    def evaluate_detections(self, all_boxes, output_dir):\n        self._write_voc_results_file(all_boxes)\n        self._do_python_eval(output_dir)\n        if self.config[\'matlab_eval\']:\n            self._do_matlab_eval(output_dir)\n        if self.config[\'cleanup\']:\n            for cls in self._classes:\n                if cls == \'__background__\':\n                    continue\n                filename = self._get_voc_results_file_template().format(cls)\n                os.remove(filename)\n\n    def competition_mode(self, on):\n        if on:\n            self.config[\'use_salt\'] = False\n            self.config[\'cleanup\'] = False\n        else:\n            self.config[\'use_salt\'] = True\n            self.config[\'cleanup\'] = True\n\nif __name__ == \'__main__\':\n    d = kittivoc(\'trainval\')\n    res = d.roidb\n    from IPython import embed; embed()\n'"
faster_rcnn/datasets/nissan.py,0,"b'import os\nimport PIL\nimport numpy as np\nimport scipy.sparse\nimport subprocess\nimport cPickle\nimport math\nimport glob\n\nfrom .imdb import imdb\nfrom .imdb import ROOT_DIR\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\n# <<<< obsolete\n\nclass nissan(imdb):\n    def __init__(self, image_set, nissan_path=None):\n        imdb.__init__(self, \'nissan_\' + image_set)\n        self._image_set = image_set\n        self._nissan_path = self._get_default_path() if nissan_path is None \\\n                            else nissan_path\n        self._data_path = os.path.join(self._nissan_path, \'Images\')\n        self._classes = (\'__background__\', \'Car\', \'Pedestrian\', \'Cyclist\')\n        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))\n        self._image_ext = \'.png\'\n        self._image_index = self._load_image_set_index()\n        # Default to roidb handler\n        if cfg.IS_RPN:\n            self._roidb_handler = self.gt_roidb\n        else:\n            self._roidb_handler = self.region_proposal_roidb\n\n        # num of subclasses\n        self._num_subclasses = 227 + 36 + 36 + 1\n\n        # load the mapping for subcalss to class\n        filename = os.path.join(self._nissan_path, \'mapping.txt\')\n        assert os.path.exists(filename), \'Path does not exist: {}\'.format(filename)\n        \n        mapping = np.zeros(self._num_subclasses, dtype=np.int)\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[0])\n                mapping[subcls] = self._class_to_ind[words[1]]\n        self._subclass_mapping = mapping\n\n        self.config = {\'top_k\': 100000}\n\n        # statistics for computing recall\n        self._num_boxes_all = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_covered = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_proposal = 0\n\n        assert os.path.exists(self._nissan_path), \\\n                \'Nissan path does not exist: {}\'.format(self._nissan_path)\n        assert os.path.exists(self._data_path), \\\n                \'Path does not exist: {}\'.format(self._data_path)\n\n    def image_path_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return self.image_path_from_index(self.image_index[i])\n\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n        # set the prefix\n        prefix = self._image_set\n\n        image_path = os.path.join(self._data_path, prefix, index + self._image_ext)\n        assert os.path.exists(image_path), \\\n                \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def _load_image_set_index(self):\n        """"""\n        Load the indexes listed in this dataset\'s image set file.\n        """"""\n        image_set_file = os.path.join(self._data_path, self._image_set + \'.txt\')\n        assert os.path.exists(image_set_file), \\\n                \'Path does not exist: {}\'.format(image_set_file)\n\n        with open(image_set_file) as f:\n            image_index = [x.rstrip(\'\\n\') for x in f.readlines()]\n        return image_index\n\n    def _get_default_path(self):\n        """"""\n        Return the default path where NISSAN is expected to be installed.\n        """"""\n        return os.path.join(ROOT_DIR, \'data\', \'NISSAN\')\n\n\n    def gt_roidb(self):\n        """"""\n        Return the database of ground-truth regions of interest.\n        No implementation.\n        """"""\n\n        gt_roidb = []\n        return gt_roidb\n\n    def region_proposal_roidb(self):\n        """"""\n        Return the database of regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_\' + cfg.REGION_PROPOSAL + \'_region_proposal_roidb.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        print \'Loading region proposal network boxes...\'\n        model = cfg.REGION_PROPOSAL\n        roidb = self._load_rpn_roidb(None, model)\n        print \'Region proposal network boxes loaded\'\n        print \'{} region proposals per image\'.format(self._num_boxes_proposal / len(self.image_index))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def _load_rpn_roidb(self, gt_roidb, model):\n        # set the prefix\n        prefix = model\n\n        box_list = []\n        for index in self.image_index:\n            filename = os.path.join(self._nissan_path, \'region_proposals\',  prefix, self._image_set, index + \'.txt\')\n            assert os.path.exists(filename), \\\n                \'RPN data not found at: {}\'.format(filename)\n            raw_data = np.loadtxt(filename, dtype=float)\n            if len(raw_data.shape) == 1:\n                if raw_data.size == 0:\n                    raw_data = raw_data.reshape((0, 5))\n                else:\n                    raw_data = raw_data.reshape((1, 5))\n\n            x1 = raw_data[:, 0]\n            y1 = raw_data[:, 1]\n            x2 = raw_data[:, 2]\n            y2 = raw_data[:, 3]\n            score = raw_data[:, 4]\n            inds = np.where((x2 > x1) & (y2 > y1))[0]\n            raw_data = raw_data[inds,:4]\n            self._num_boxes_proposal += raw_data.shape[0]\n            box_list.append(raw_data)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def evaluate_detections(self, all_boxes, output_dir):\n        # load the mapping for subcalss the alpha (viewpoint)\n        filename = os.path.join(self._nissan_path, \'mapping.txt\')\n        assert os.path.exists(filename), \\\n                \'Path does not exist: {}\'.format(filename)\n\n        mapping = np.zeros(self._num_subclasses, dtype=np.float)\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[0])\n                mapping[subcls] = float(words[3])\n\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing NISSAN results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        subcls = int(dets[k, 5])\n                        cls_name = self.classes[self.subclass_mapping[subcls]]\n                        assert (cls_name == cls), \'subclass not in class\'\n                        alpha = mapping[subcls]\n                        f.write(\'{:s} -1 -1 {:f} {:f} {:f} {:f} {:f} -1 -1 -1 -1 -1 -1 -1 {:.32f}\\n\'.format(\\\n                                 cls, alpha, dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    # write detection results into one file\n    def evaluate_detections_one_file(self, all_boxes, output_dir):\n        # open results file\n        filename = os.path.join(output_dir, \'detections.txt\')\n        print \'Writing all NISSAN results to file \' + filename\n        with open(filename, \'wt\') as f:\n            # for each image\n            for im_ind, index in enumerate(self.image_index):\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        subcls = int(dets[k, 5])\n                        cls_name = self.classes[self.subclass_mapping[subcls]]\n                        assert (cls_name == cls), \'subclass not in class\'\n                        f.write(\'{:s} {:s} {:f} {:f} {:f} {:f} {:d} {:f}\\n\'.format(\\\n                                 index, cls, dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], subcls, dets[k, 4]))\n\n    def evaluate_proposals(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing NISSAN results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(\\\n                                 dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    def evaluate_proposals_msr(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing NISSAN results to file \' + filename\n            with open(filename, \'wt\') as f:\n                dets = all_boxes[im_ind]\n                if dets == []:\n                    continue\n                for k in xrange(dets.shape[0]):\n                    f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n\nif __name__ == \'__main__\':\n    d = nissan(\'2015-10-21-16-25-12\')\n    res = d.roidb\n    from IPython import embed; embed()\n'"
faster_rcnn/datasets/nthu.py,0,"b'import os\nimport PIL\nimport numpy as np\nimport scipy.sparse\nimport subprocess\nimport cPickle\nimport math\nimport glob\n\nfrom .imdb import imdb\nfrom .imdb import ROOT_DIR\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\n# <<<< obsolete\n\nclass nthu(imdb):\n    def __init__(self, image_set, nthu_path=None):\n        imdb.__init__(self, \'nthu_\' + image_set)\n        self._image_set = image_set\n        self._nthu_path = self._get_default_path() if nthu_path is None \\\n                            else nthu_path\n        self._data_path = os.path.join(self._nthu_path, \'data\')\n        self._classes = (\'__background__\', \'Car\', \'Pedestrian\', \'Cyclist\')\n        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))\n        self._image_ext = \'.jpg\'\n        self._image_index = self._load_image_set_index()\n        # Default to roidb handler\n        if cfg.IS_RPN:\n            self._roidb_handler = self.gt_roidb\n        else:\n            self._roidb_handler = self.region_proposal_roidb\n\n        # num of subclasses\n        self._num_subclasses = 227 + 36 + 36 + 1\n\n        # load the mapping for subcalss to class\n        filename = os.path.join(self._nthu_path, \'mapping.txt\')\n        assert os.path.exists(filename), \'Path does not exist: {}\'.format(filename)\n        \n        mapping = np.zeros(self._num_subclasses, dtype=np.int)\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[0])\n                mapping[subcls] = self._class_to_ind[words[1]]\n        self._subclass_mapping = mapping\n\n        self.config = {\'top_k\': 100000}\n\n        # statistics for computing recall\n        self._num_boxes_all = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_covered = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_proposal = 0\n\n        assert os.path.exists(self._nthu_path), \\\n                \'NTHU path does not exist: {}\'.format(self._nthu_path)\n        assert os.path.exists(self._data_path), \\\n                \'Path does not exist: {}\'.format(self._data_path)\n\n    def image_path_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return self.image_path_from_index(self.image_index[i])\n\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n        # set the prefix\n        prefix = self._image_set\n\n        image_path = os.path.join(self._data_path, prefix, index + self._image_ext)\n        assert os.path.exists(image_path), \\\n                \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def _load_image_set_index(self):\n        """"""\n        Load the indexes listed in this dataset\'s image set file.\n        """"""\n        image_set_file = os.path.join(self._data_path, self._image_set + \'.txt\')\n        assert os.path.exists(image_set_file), \\\n                \'Path does not exist: {}\'.format(image_set_file)\n\n        with open(image_set_file) as f:\n            image_index = [x.rstrip(\'\\n\') for x in f.readlines()]\n        return image_index\n\n    def _get_default_path(self):\n        """"""\n        Return the default path where nthu is expected to be installed.\n        """"""\n        return os.path.join(ROOT_DIR, \'data\', \'NTHU\')\n\n\n    def gt_roidb(self):\n        """"""\n        Return the database of ground-truth regions of interest.\n        No implementation.\n        """"""\n\n        gt_roidb = []\n        return gt_roidb\n\n    def region_proposal_roidb(self):\n        """"""\n        Return the database of regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_\' + cfg.REGION_PROPOSAL + \'_region_proposal_roidb.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        print \'Loading region proposal network boxes...\'\n        model = cfg.REGION_PROPOSAL\n        roidb = self._load_rpn_roidb(None, model)\n        print \'Region proposal network boxes loaded\'\n        print \'{} region proposals per image\'.format(self._num_boxes_proposal / len(self.image_index))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def _load_rpn_roidb(self, gt_roidb, model):\n        # set the prefix\n        prefix = model\n\n        box_list = []\n        for index in self.image_index:\n            filename = os.path.join(self._nthu_path, \'region_proposals\',  prefix, self._image_set, index + \'.txt\')\n            assert os.path.exists(filename), \\\n                \'RPN data not found at: {}\'.format(filename)\n            raw_data = np.loadtxt(filename, dtype=float)\n            if len(raw_data.shape) == 1:\n                if raw_data.size == 0:\n                    raw_data = raw_data.reshape((0, 5))\n                else:\n                    raw_data = raw_data.reshape((1, 5))\n\n            x1 = raw_data[:, 0]\n            y1 = raw_data[:, 1]\n            x2 = raw_data[:, 2]\n            y2 = raw_data[:, 3]\n            score = raw_data[:, 4]\n            inds = np.where((x2 > x1) & (y2 > y1))[0]\n            raw_data = raw_data[inds,:4]\n            self._num_boxes_proposal += raw_data.shape[0]\n            box_list.append(raw_data)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def evaluate_detections(self, all_boxes, output_dir):\n        # load the mapping for subcalss the alpha (viewpoint)\n        filename = os.path.join(self._nthu_path, \'mapping.txt\')\n        assert os.path.exists(filename), \\\n                \'Path does not exist: {}\'.format(filename)\n\n        mapping = np.zeros(self._num_subclasses, dtype=np.float)\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[0])\n                mapping[subcls] = float(words[3])\n\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing nthu results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        subcls = int(dets[k, 5])\n                        cls_name = self.classes[self.subclass_mapping[subcls]]\n                        assert (cls_name == cls), \'subclass not in class\'\n                        alpha = mapping[subcls]\n                        f.write(\'{:s} -1 -1 {:f} {:f} {:f} {:f} {:f} -1 -1 -1 -1 -1 -1 -1 {:.32f}\\n\'.format(\\\n                                 cls, alpha, dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    # write detection results into one file\n    def evaluate_detections_one_file(self, all_boxes, output_dir):\n        # open results file\n        filename = os.path.join(output_dir, \'detections.txt\')\n        print \'Writing all nthu results to file \' + filename\n        with open(filename, \'wt\') as f:\n            # for each image\n            for im_ind, index in enumerate(self.image_index):\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        subcls = int(dets[k, 5])\n                        cls_name = self.classes[self.subclass_mapping[subcls]]\n                        assert (cls_name == cls), \'subclass not in class\'\n                        f.write(\'{:s} {:s} {:f} {:f} {:f} {:f} {:d} {:f}\\n\'.format(\\\n                                 index, cls, dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], subcls, dets[k, 4]))\n\n    def evaluate_proposals(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing nthu results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(\\\n                                 dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    def evaluate_proposals_msr(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing nthu results to file \' + filename\n            with open(filename, \'wt\') as f:\n                dets = all_boxes[im_ind]\n                if dets == []:\n                    continue\n                for k in xrange(dets.shape[0]):\n                    f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n\nif __name__ == \'__main__\':\n    d = nthu(\'71\')\n    res = d.roidb\n    from IPython import embed; embed()\n'"
faster_rcnn/datasets/pascal3d.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport xml.dom.minidom as minidom\n\nimport os\nimport PIL\nimport numpy as np\nimport scipy.sparse\nimport subprocess\nimport cPickle\nimport math\nimport glob\nimport scipy.io as sio\n\nfrom .imdb import imdb\nfrom .imdb import ROOT_DIR\n\nfrom ..utils.cython_bbox import bbox_overlaps\nfrom ..utils.boxes_grid import get_boxes_grid\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\nfrom ..rpn_msr.generate_anchors import generate_anchors\n# <<<< obsolete\n\nclass pascal3d(imdb):\n    def __init__(self, image_set, pascal3d_path = None):\n        imdb.__init__(self, \'pascal3d_\' + image_set)\n        self._year = \'2012\'\n        self._image_set = image_set\n        self._pascal3d_path = self._get_default_path() if pascal3d_path is None \\\n                            else pascal3d_path\n        self._data_path = os.path.join(self._pascal3d_path, \'VOCdevkit\' + self._year, \'VOC\' + self._year)\n        self._classes = (\'__background__\', # always index 0\n                         \'aeroplane\', \'bicycle\', \'boat\',\n                         \'bottle\', \'bus\', \'car\', \'chair\',\n                         \'diningtable\', \'motorbike\',\n                         \'sofa\', \'train\', \'tvmonitor\')\n        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))\n        self._image_ext = \'.jpg\'\n        self._image_index = self._load_image_set_index()\n        # Default to roidb handler\n        if cfg.IS_RPN:\n            self._roidb_handler = self.gt_roidb\n        else:\n            self._roidb_handler = self.region_proposal_roidb\n\n        # num of subclasses\n        if cfg.SUBCLS_NAME == \'voxel_exemplars\':\n            self._num_subclasses = 337 + 1\n        elif cfg.SUBCLS_NAME == \'pose_exemplars\':\n            self._num_subclasses = 260 + 1\n        else:\n            assert (1), \'cfg.SUBCLS_NAME not supported!\'\n\n        # load the mapping for subcalss to class\n        filename = os.path.join(self._pascal3d_path, cfg.SUBCLS_NAME, \'mapping.txt\')\n        assert os.path.exists(filename), \'Path does not exist: {}\'.format(filename)\n        \n        mapping = np.zeros(self._num_subclasses, dtype=np.int)\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[0])\n                mapping[subcls] = self._class_to_ind[words[1]]\n        self._subclass_mapping = mapping\n\n        # PASCAL specific config options\n        self.config = {\'cleanup\'  : True,\n                       \'use_salt\' : True,\n                       \'top_k\'    : 2000}\n\n        # statistics for computing recall\n        self._num_boxes_all = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_covered = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_proposal = 0\n\n        assert os.path.exists(self._pascal3d_path), \\\n                \'PASCAL3D path does not exist: {}\'.format(self._pascal3d_path)\n        assert os.path.exists(self._data_path), \\\n                \'Path does not exist: {}\'.format(self._data_path)\n\n    def image_path_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return self.image_path_from_index(self._image_index[i])\n\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n        image_path = os.path.join(self._data_path, \'JPEGImages\',\n                                  index + self._image_ext)\n        assert os.path.exists(image_path), \\\n                \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def _load_image_set_index(self):\n        """"""\n        Load the indexes listed in this dataset\'s image set file.\n        """"""\n        # Example path to image set file:\n        # self._pascal3d_path + /VOCdevkit2012/VOC2012/ImageSets/Main/val.txt\n        image_set_file = os.path.join(self._data_path, \'ImageSets\', \'Main\',\n                                      self._image_set + \'.txt\')\n        assert os.path.exists(image_set_file), \\\n                \'Path does not exist: {}\'.format(image_set_file)\n        with open(image_set_file) as f:\n            image_index = [x.strip() for x in f.readlines()]\n        return image_index\n\n    def _get_default_path(self):\n        """"""\n        Return the default path where PASCAL3D is expected to be installed.\n        """"""\n        return os.path.join(ROOT_DIR, \'data\', \'PASCAL3D\')\n\n    def gt_roidb(self):\n        """"""\n        Return the database of ground-truth regions of interest.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path, self.name + \'_\' + cfg.SUBCLS_NAME + \'_gt_roidb.pkl\')\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} gt roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        gt_roidb = [self._load_pascal3d_voxel_exemplar_annotation(index)\n                    for index in self.image_index]\n\n        if cfg.IS_RPN:\n            # print out recall\n            for i in xrange(1, self.num_classes):\n                print \'{}: Total number of boxes {:d}\'.format(self.classes[i], self._num_boxes_all[i])\n                print \'{}: Number of boxes covered {:d}\'.format(self.classes[i], self._num_boxes_covered[i])\n                print \'{}: Recall {:f}\'.format(self.classes[i], float(self._num_boxes_covered[i]) / float(self._num_boxes_all[i]))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(gt_roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote gt roidb to {}\'.format(cache_file)\n\n        return gt_roidb\n\n    def _load_pascal_annotation(self, index):\n        """"""\n        Load image and bounding boxes info from XML file in the PASCAL VOC\n        format.\n        """"""\n        filename = os.path.join(self._data_path, \'Annotations\', index + \'.xml\')\n        # print \'Loading: {}\'.format(filename)\n        def get_data_from_tag(node, tag):\n            return node.getElementsByTagName(tag)[0].childNodes[0].data\n\n        with open(filename) as f:\n            data = minidom.parseString(f.read())\n\n        objs = data.getElementsByTagName(\'object\')\n        num_objs = len(objs)\n\n        boxes = np.zeros((num_objs, 4), dtype=np.uint16)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n\n        # Load object bounding boxes into a data frame.\n        for ix, obj in enumerate(objs):\n            # Make pixel indexes 0-based\n            x1 = float(get_data_from_tag(obj, \'xmin\')) - 1\n            y1 = float(get_data_from_tag(obj, \'ymin\')) - 1\n            x2 = float(get_data_from_tag(obj, \'xmax\')) - 1\n            y2 = float(get_data_from_tag(obj, \'ymax\')) - 1\n            name =  str(get_data_from_tag(obj, ""name"")).lower().strip()\n            if name in self._classes:\n                cls = self._class_to_ind[name]\n            else:\n                cls = 0\n            boxes[ix, :] = [x1, y1, x2, y2]\n            gt_classes[ix] = cls\n            overlaps[ix, cls] = 1.0\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n        gt_subclasses = np.zeros((num_objs), dtype=np.int32)\n        gt_subclasses_flipped = np.zeros((num_objs), dtype=np.int32)\n        subindexes = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n        subindexes_flipped = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n        subindexes = scipy.sparse.csr_matrix(subindexes)\n        subindexes_flipped = scipy.sparse.csr_matrix(subindexes_flipped)\n\n        if cfg.IS_RPN:\n            if cfg.IS_MULTISCALE:\n                # compute overlaps between grid boxes and gt boxes in multi-scales\n                # rescale the gt boxes\n                boxes_all = np.zeros((0, 4), dtype=np.float32)\n                for scale in cfg.TRAIN.SCALES:\n                    boxes_all = np.vstack((boxes_all, boxes * scale))\n                gt_classes_all = np.tile(gt_classes, len(cfg.TRAIN.SCALES))\n\n                # compute grid boxes\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n                boxes_grid, _, _ = get_boxes_grid(image_height, image_width)\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(boxes_grid.astype(np.float), boxes_all.astype(np.float))\n        \n                # check how many gt boxes are covered by grids\n                if num_objs != 0:\n                    index = np.tile(range(num_objs), len(cfg.TRAIN.SCALES))\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes_all == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n                    index_covered = np.unique(index[fg_inds])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[index_covered] == i)[0])\n            else:\n                assert len(cfg.TRAIN.SCALES_BASE) == 1\n                scale = cfg.TRAIN.SCALES_BASE[0]\n                feat_stride = 16\n                # faster rcnn region proposal\n                anchors = generate_anchors()\n                num_anchors = anchors.shape[0]\n\n                # image size\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n\n                # height and width of the heatmap\n                height = np.round((image_height * scale - 1) / 4.0 + 1)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n\n                width = np.round((image_width * scale - 1) / 4.0 + 1)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n\n                # gt boxes\n                gt_boxes = boxes * scale\n\n                # 1. Generate proposals from bbox deltas and shifted anchors\n                shift_x = np.arange(0, width) * feat_stride\n                shift_y = np.arange(0, height) * feat_stride\n                shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n                shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                            shift_x.ravel(), shift_y.ravel())).transpose()\n                # add A anchors (1, A, 4) to\n                # cell K shifts (K, 1, 4) to get\n                # shift anchors (K, A, 4)\n                # reshape to (K*A, 4) shifted anchors\n                A = num_anchors\n                K = shifts.shape[0]\n                all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n                all_anchors = all_anchors.reshape((K * A, 4))\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(all_anchors.astype(np.float), gt_boxes.astype(np.float))\n        \n                # check how many gt boxes are covered by anchors\n                if num_objs != 0:\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[fg_inds] == i)[0])\n\n        return {\'boxes\' : boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_subclasses\': gt_subclasses,\n                \'gt_subclasses_flipped\': gt_subclasses_flipped,\n                \'gt_overlaps\' : overlaps,\n                \'gt_subindexes\': subindexes,\n                \'gt_subindexes_flipped\': subindexes_flipped,\n                \'flipped\' : False}\n\n\n    def _load_pascal3d_voxel_exemplar_annotation(self, index):\n        """"""\n        Load image and bounding boxes info from txt file in the pascal subcategory exemplar format.\n        """"""\n\n        if self._image_set == \'val\':\n            return self._load_pascal_annotation(index)\n\n        filename = os.path.join(self._pascal3d_path, cfg.SUBCLS_NAME, index + \'.txt\')\n        assert os.path.exists(filename), \\\n                \'Path does not exist: {}\'.format(filename)\n\n        # the annotation file contains flipped objects    \n        lines = []\n        lines_flipped = []\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[1])\n                is_flip = int(words[2])\n                if subcls != -1:\n                    if is_flip == 0:\n                        lines.append(line)\n                    else:\n                        lines_flipped.append(line)\n        \n        num_objs = len(lines)\n\n        # store information of flipped objects\n        assert (num_objs == len(lines_flipped)), \'The number of flipped objects is not the same!\'\n        gt_subclasses_flipped = np.zeros((num_objs), dtype=np.int32)\n        \n        for ix, line in enumerate(lines_flipped):\n            words = line.split()\n            subcls = int(words[1])\n            gt_subclasses_flipped[ix] = subcls\n\n        boxes = np.zeros((num_objs, 4), dtype=np.float32)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        gt_subclasses = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        subindexes = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n        subindexes_flipped = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n\n        for ix, line in enumerate(lines):\n            words = line.split()\n            cls = self._class_to_ind[words[0]]\n            subcls = int(words[1])\n            # Make pixel indexes 0-based\n            boxes[ix, :] = [float(n)-1 for n in words[3:7]]\n            gt_classes[ix] = cls\n            gt_subclasses[ix] = subcls\n            overlaps[ix, cls] = 1.0\n            subindexes[ix, cls] = subcls\n            subindexes_flipped[ix, cls] = gt_subclasses_flipped[ix]\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n        subindexes = scipy.sparse.csr_matrix(subindexes)\n        subindexes_flipped = scipy.sparse.csr_matrix(subindexes_flipped)\n\n        if cfg.IS_RPN:\n            if cfg.IS_MULTISCALE:\n                # compute overlaps between grid boxes and gt boxes in multi-scales\n                # rescale the gt boxes\n                boxes_all = np.zeros((0, 4), dtype=np.float32)\n                for scale in cfg.TRAIN.SCALES:\n                    boxes_all = np.vstack((boxes_all, boxes * scale))\n                gt_classes_all = np.tile(gt_classes, len(cfg.TRAIN.SCALES))\n\n                # compute grid boxes\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n                boxes_grid, _, _ = get_boxes_grid(image_height, image_width)\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(boxes_grid.astype(np.float), boxes_all.astype(np.float))\n        \n                # check how many gt boxes are covered by grids\n                if num_objs != 0:\n                    index = np.tile(range(num_objs), len(cfg.TRAIN.SCALES))\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes_all == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n                    index_covered = np.unique(index[fg_inds])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[index_covered] == i)[0])\n            else:\n                assert len(cfg.TRAIN.SCALES_BASE) == 1\n                scale = cfg.TRAIN.SCALES_BASE[0]\n                feat_stride = 16\n                # faster rcnn region proposal\n                base_size = 16\n                ratios = [3.0, 2.0, 1.5, 1.0, 0.75, 0.5, 0.25]\n                scales = 2**np.arange(1, 6, 0.5)\n                anchors = generate_anchors(base_size, ratios, scales)\n                num_anchors = anchors.shape[0]\n\n                # image size\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n\n                # height and width of the heatmap\n                height = np.round((image_height * scale - 1) / 4.0 + 1)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n\n                width = np.round((image_width * scale - 1) / 4.0 + 1)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n\n                # gt boxes\n                gt_boxes = boxes * scale\n\n                # 1. Generate proposals from bbox deltas and shifted anchors\n                shift_x = np.arange(0, width) * feat_stride\n                shift_y = np.arange(0, height) * feat_stride\n                shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n                shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                            shift_x.ravel(), shift_y.ravel())).transpose()\n                # add A anchors (1, A, 4) to\n                # cell K shifts (K, 1, 4) to get\n                # shift anchors (K, A, 4)\n                # reshape to (K*A, 4) shifted anchors\n                A = num_anchors\n                K = shifts.shape[0]\n                all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n                all_anchors = all_anchors.reshape((K * A, 4))\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(all_anchors.astype(np.float), gt_boxes.astype(np.float))\n        \n                # check how many gt boxes are covered by anchors\n                if num_objs != 0:\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[fg_inds] == i)[0])\n\n        return {\'boxes\' : boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_subclasses\': gt_subclasses,\n                \'gt_subclasses_flipped\': gt_subclasses_flipped,\n                \'gt_overlaps\': overlaps,\n                \'gt_subindexes\': subindexes, \n                \'gt_subindexes_flipped\': subindexes_flipped, \n                \'flipped\' : False}\n\n    def region_proposal_roidb(self):\n        """"""\n        Return the database of regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_\' + cfg.SUBCLS_NAME + \'_\' + cfg.REGION_PROPOSAL + \'_region_proposal_roidb.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        if self._image_set != \'test\':\n            gt_roidb = self.gt_roidb()\n\n            print \'Loading region proposal network boxes...\'\n            model = cfg.REGION_PROPOSAL\n            rpn_roidb = self._load_rpn_roidb(gt_roidb, model)\n            print \'Region proposal network boxes loaded\'\n            roidb = imdb.merge_roidbs(rpn_roidb, gt_roidb)\n        else:\n            print \'Loading region proposal network boxes...\'\n            model = cfg.REGION_PROPOSAL\n            roidb = self._load_rpn_roidb(None, model)\n            print \'Region proposal network boxes loaded\'\n\n        print \'{} region proposals per image\'.format(self._num_boxes_proposal / len(self.image_index))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def _load_rpn_roidb(self, gt_roidb, model):\n        # set the prefix\n        if self._image_set == \'val\':\n            prefix = model + \'/validation\'\n        elif self._image_set == \'train\':\n            prefix = model + \'/training\'\n        else:\n            predix = \'\'\n\n        box_list = []\n        for index in self.image_index:\n            filename = os.path.join(self._pascal3d_path, \'region_proposals\',  prefix, index + \'.txt\')\n            assert os.path.exists(filename), \\\n                \'RPN data not found at: {}\'.format(filename)\n            raw_data = np.loadtxt(filename, dtype=float)\n            if len(raw_data.shape) == 1:\n                if raw_data.size == 0:\n                    raw_data = raw_data.reshape((0, 5))\n                else:\n                    raw_data = raw_data.reshape((1, 5))\n\n            x1 = raw_data[:, 0]\n            y1 = raw_data[:, 1]\n            x2 = raw_data[:, 2]\n            y2 = raw_data[:, 3]\n            score = raw_data[:, 4]\n            inds = np.where((x2 > x1) & (y2 > y1))[0]\n            raw_data = raw_data[inds,:4]\n            self._num_boxes_proposal += raw_data.shape[0]\n            box_list.append(raw_data)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n\n    def selective_search_roidb(self):\n        """"""\n        Return the database of selective search regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_selective_search_roidb.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} ss roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        if int(self._year) == 2007 or self._image_set != \'test\':\n            gt_roidb = self.gt_roidb()\n            ss_roidb = self._load_selective_search_roidb(gt_roidb)\n            roidb = imdb.merge_roidbs(gt_roidb, ss_roidb)\n        else:\n            roidb = self._load_selective_search_roidb(None)\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote ss roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def _load_selective_search_roidb(self, gt_roidb):\n        filename = os.path.abspath(os.path.join(self.cache_path, \'..\',\n                                                \'selective_search_data\',\n                                                self.name + \'.mat\'))\n        assert os.path.exists(filename), \\\n               \'Selective search data not found at: {}\'.format(filename)\n        raw_data = sio.loadmat(filename)[\'boxes\'].ravel()\n\n        box_list = []\n        for i in xrange(raw_data.shape[0]):\n            box_list.append(raw_data[i][:, (1, 0, 3, 2)] - 1)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def selective_search_IJCV_roidb(self):\n        """"""\n        Return the database of selective search regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                \'{:s}_selective_search_IJCV_top_{:d}_roidb.pkl\'.\n                format(self.name, self.config[\'top_k\']))\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} ss roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        gt_roidb = self.gt_roidb()\n        ss_roidb = self._load_selective_search_IJCV_roidb(gt_roidb)\n        roidb = imdb.merge_roidbs(gt_roidb, ss_roidb)\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote ss roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def _load_selective_search_IJCV_roidb(self, gt_roidb):\n        IJCV_path = os.path.abspath(os.path.join(self.cache_path, \'..\',\n                                                 \'selective_search_IJCV_data\',\n                                                 \'voc_\' + self._year))\n        assert os.path.exists(IJCV_path), \\\n               \'Selective search IJCV data not found at: {}\'.format(IJCV_path)\n\n        top_k = self.config[\'top_k\']\n        box_list = []\n        for i in xrange(self.num_images):\n            filename = os.path.join(IJCV_path, self.image_index[i] + \'.mat\')\n            raw_data = sio.loadmat(filename)\n            box_list.append((raw_data[\'boxes\'][:top_k, :]-1).astype(np.uint16))\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    # evaluate detection results\n    def evaluate_detections(self, all_boxes, output_dir):\n        # load the mapping for subcalss the azimuth (viewpoint)\n        filename = os.path.join(self._pascal3d_path, cfg.SUBCLS_NAME, \'mapping.txt\')\n        assert os.path.exists(filename), \\\n                \'Path does not exist: {}\'.format(filename)\n\n        mapping = np.zeros(self._num_subclasses, dtype=np.float)\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[0])\n                mapping[subcls] = float(words[2])\n\n        for cls_ind, cls in enumerate(self.classes):\n            if cls == \'__background__\':\n                continue\n            print \'Writing {} VOC results file\'.format(cls)\n            filename = os.path.join(output_dir, \'det_\' + self._image_set + \'_\' + cls + \'.txt\')\n            print filename\n\n            with open(filename, \'wt\') as f:\n                for im_ind, index in enumerate(self.image_index):\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    # the VOCdevkit expects 1-based indices\n                    for k in xrange(dets.shape[0]):\n                        subcls = int(dets[k, 5])\n                        cls_name = self.classes[self.subclass_mapping[subcls]]\n                        assert (cls_name == cls), \'subclass not in class\'\n                        azimuth = mapping[subcls]\n                        f.write(\'{:s} {:.3f} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\\n\'.\n                                format(index, dets[k, 4], azimuth,\n                                       dets[k, 0] + 1, dets[k, 1] + 1,\n                                       dets[k, 2] + 1, dets[k, 3] + 1))\n\n    # evaluate detection results\n    def evaluate_detections_one_file(self, all_boxes, output_dir):\n        # open results file\n        filename = os.path.join(output_dir, \'detections.txt\')\n        print \'Writing all PASCAL3D results to file \' + filename\n        with open(filename, \'wt\') as f:\n            for im_ind, index in enumerate(self.image_index):\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    # the VOCdevkit expects 1-based indices\n                    for k in xrange(dets.shape[0]):\n                        subcls = int(dets[k, 5])\n                        cls_name = self.classes[self.subclass_mapping[subcls]]\n                        assert (cls_name == cls), \'subclass not in class\'\n                        f.write(\'{:s} {:s} {:f} {:f} {:f} {:f} {:d} {:f}\\n\'.\n                                format(index, cls, dets[k, 0] + 1, dets[k, 1] + 1,\n                                       dets[k, 2] + 1, dets[k, 3] + 1, subcls, dets[k, 4]))\n\n\n    def evaluate_proposals(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing PASCAL results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(\\\n                                 dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    def evaluate_proposals_msr(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing PASCAL results to file \' + filename\n            with open(filename, \'wt\') as f:\n                dets = all_boxes[im_ind]\n                if dets == []:\n                    continue\n                for k in xrange(dets.shape[0]):\n                    f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n\n    def competition_mode(self, on):\n        if on:\n            self.config[\'use_salt\'] = False\n            self.config[\'cleanup\'] = False\n        else:\n            self.config[\'use_salt\'] = True\n            self.config[\'cleanup\'] = True\n\nif __name__ == \'__main__\':\n    d = pascal3d(\'train\')\n    res = d.roidb\n    from IPython import embed; embed()\n'"
faster_rcnn/datasets/pascal_voc.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport xml.dom.minidom as minidom\n\nimport os\n# import PIL\nimport numpy as np\nimport scipy.sparse\nimport subprocess\nimport cPickle\nimport math\nimport glob\nimport uuid\nimport scipy.io as sio\nimport xml.etree.ElementTree as ET\n\nfrom .imdb import imdb\nfrom .imdb import ROOT_DIR\nimport ds_utils\nfrom .voc_eval import voc_eval\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\n\n\n# <<<< obsolete\n\n\nclass pascal_voc(imdb):\n    def __init__(self, image_set, year, devkit_path=None):\n        imdb.__init__(self, \'voc_\' + year + \'_\' + image_set)\n        self._year = year\n        self._image_set = image_set\n        self._devkit_path = self._get_default_path() if devkit_path is None \\\n            else devkit_path\n        self._data_path = os.path.join(self._devkit_path, \'VOC\' + self._year)\n        self._classes = (\'__background__\',  # always index 0\n                         \'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n                         \'bottle\', \'bus\', \'car\', \'cat\', \'chair\',\n                         \'cow\', \'diningtable\', \'dog\', \'horse\',\n                         \'motorbike\', \'person\', \'pottedplant\',\n                         \'sheep\', \'sofa\', \'train\', \'tvmonitor\')\n        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))\n        self._image_ext = \'.jpg\'\n        self._image_index = self._load_image_set_index()\n        # Default to roidb handler\n        # self._roidb_handler = self.selective_search_roidb\n        self._roidb_handler = self.gt_roidb\n        self._salt = str(uuid.uuid4())\n        self._comp_id = \'comp4\'\n\n        # PASCAL specific config options\n        self.config = {\'cleanup\': True,\n                       \'use_salt\': True,\n                       \'use_diff\': False,\n                       \'matlab_eval\': False,\n                       \'rpn_file\': None,\n                       \'min_size\': 2}\n\n        assert os.path.exists(self._devkit_path), \\\n            \'VOCdevkit path does not exist: {}\'.format(self._devkit_path)\n        assert os.path.exists(self._data_path), \\\n            \'Path does not exist: {}\'.format(self._data_path)\n\n    def image_path_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return self.image_path_from_index(self._image_index[i])\n\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n        image_path = os.path.join(self._data_path, \'JPEGImages\',\n                                  index + self._image_ext)\n        assert os.path.exists(image_path), \\\n            \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def _load_image_set_index(self):\n        """"""\n        Load the indexes listed in this dataset\'s image set file.\n        """"""\n        # Example path to image set file:\n        # self._devkit_path + /VOCdevkit2007/VOC2007/ImageSets/Main/val.txt\n        image_set_file = os.path.join(self._data_path, \'ImageSets\', \'Main\',\n                                      self._image_set + \'.txt\')\n        assert os.path.exists(image_set_file), \\\n            \'Path does not exist: {}\'.format(image_set_file)\n        with open(image_set_file) as f:\n            image_index = [x.strip() for x in f.readlines()]\n        return image_index\n\n    def _get_default_path(self):\n        """"""\n        Return the default path where PASCAL VOC is expected to be installed.\n        """"""\n        return os.path.join(cfg.DATA_DIR, \'VOCdevkit\' + self._year)\n\n    def gt_roidb(self):\n        """"""\n        Return the database of ground-truth regions of interest.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path, self.name + \'_gt_roidb.pkl\')\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} gt roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        gt_roidb = [self._load_pascal_annotation(index)\n                    for index in self.image_index]\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(gt_roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote gt roidb to {}\'.format(cache_file)\n\n        return gt_roidb\n\n    def selective_search_roidb(self):\n        """"""\n        Return the database of selective search regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_selective_search_roidb.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} ss roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        if int(self._year) == 2007 or self._image_set != \'test\':\n            gt_roidb = self.gt_roidb()\n            ss_roidb = self._load_selective_search_roidb(gt_roidb)\n            roidb = imdb.merge_roidbs(gt_roidb, ss_roidb)\n        else:\n            roidb = self._load_selective_search_roidb(None)\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote ss roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def rpn_roidb(self):\n        if int(self._year) == 2007 or self._image_set != \'test\':\n            gt_roidb = self.gt_roidb()\n            rpn_roidb = self._load_rpn_roidb(gt_roidb)\n            roidb = imdb.merge_roidbs(gt_roidb, rpn_roidb)\n        else:\n            roidb = self._load_rpn_roidb(None)\n\n        return roidb\n\n    def _load_rpn_roidb(self, gt_roidb):\n        filename = self.config[\'rpn_file\']\n        print \'loading {}\'.format(filename)\n        assert os.path.exists(filename), \\\n            \'rpn data not found at: {}\'.format(filename)\n        with open(filename, \'rb\') as f:\n            box_list = cPickle.load(f)\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def _load_selective_search_roidb(self, gt_roidb):\n        filename = os.path.abspath(os.path.join(cfg.DATA_DIR,\n                                                \'selective_search_data\',\n                                                self.name + \'.mat\'))\n        assert os.path.exists(filename), \\\n            \'Selective search data not found at: {}\'.format(filename)\n        raw_data = sio.loadmat(filename)[\'boxes\'].ravel()\n\n        box_list = []\n        for i in xrange(raw_data.shape[0]):\n            boxes = raw_data[i][:, (1, 0, 3, 2)] - 1\n            keep = ds_utils.unique_boxes(boxes)\n            boxes = boxes[keep, :]\n            keep = ds_utils.filter_small_boxes(boxes, self.config[\'min_size\'])\n            boxes = boxes[keep, :]\n            box_list.append(boxes)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def _load_pascal_annotation(self, index):\n        """"""\n        Load image and bounding boxes info from XML file in the PASCAL VOC\n        format.\n        """"""\n        filename = os.path.join(self._data_path, \'Annotations\', index + \'.xml\')\n        tree = ET.parse(filename)\n        objs = tree.findall(\'object\')\n        # if not self.config[\'use_diff\']:\n        #     # Exclude the samples labeled as difficult\n        #     non_diff_objs = [\n        #         obj for obj in objs if int(obj.find(\'difficult\').text) == 0]\n        #     # if len(non_diff_objs) != len(objs):\n        #     #     print \'Removed {} difficult objects\'.format(\n        #     #         len(objs) - len(non_diff_objs))\n        #     objs = non_diff_objs\n        num_objs = len(objs)\n\n        boxes = np.zeros((num_objs, 4), dtype=np.uint16)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        # ""Seg"" area for pascal is just the box area\n        seg_areas = np.zeros((num_objs), dtype=np.float32)\n        ishards = np.zeros((num_objs), dtype=np.int32)\n\n        # Load object bounding boxes into a data frame.\n        for ix, obj in enumerate(objs):\n            bbox = obj.find(\'bndbox\')\n            # Make pixel indexes 0-based\n            x1 = float(bbox.find(\'xmin\').text) - 1\n            y1 = float(bbox.find(\'ymin\').text) - 1\n            x2 = float(bbox.find(\'xmax\').text) - 1\n            y2 = float(bbox.find(\'ymax\').text) - 1\n\n            diffc = obj.find(\'difficult\')\n            difficult = 0 if diffc == None else int(diffc.text)\n            ishards[ix] = difficult\n\n            cls = self._class_to_ind[obj.find(\'name\').text.lower().strip()]\n            boxes[ix, :] = [x1, y1, x2, y2]\n            gt_classes[ix] = cls\n            overlaps[ix, cls] = 1.0\n            seg_areas[ix] = (x2 - x1 + 1) * (y2 - y1 + 1)\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n\n        return {\'boxes\': boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_ishard\': ishards,\n                \'gt_overlaps\': overlaps,\n                \'flipped\': False,\n                \'seg_areas\': seg_areas}\n\n    def _get_comp_id(self):\n        comp_id = (self._comp_id + \'_\' + self._salt if self.config[\'use_salt\']\n                   else self._comp_id)\n        return comp_id\n\n    def _get_voc_results_file_template(self):\n        # VOCdevkit/results/VOC2007/Main/<comp_id>_det_test_aeroplane.txt\n        filename = self._get_comp_id() + \'_det_\' + self._image_set + \'_{:s}.txt\'\n        filedir = os.path.join(self._devkit_path, \'results\', \'VOC\' + self._year, \'Main\')\n        if not os.path.exists(filedir):\n            os.makedirs(filedir)\n        path = os.path.join(filedir, filename)\n        return path\n\n    def _write_voc_results_file(self, all_boxes):\n        for cls_ind, cls in enumerate(self.classes):\n            if cls == \'__background__\':\n                continue\n            print \'Writing {} VOC results file\'.format(cls)\n            filename = self._get_voc_results_file_template().format(cls)\n            with open(filename, \'wt\') as f:\n                for im_ind, index in enumerate(self.image_index):\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    # the VOCdevkit expects 1-based indices\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\\n\'.\n                                format(index, dets[k, -1],\n                                       dets[k, 0] + 1, dets[k, 1] + 1,\n                                       dets[k, 2] + 1, dets[k, 3] + 1))\n\n    def _do_python_eval(self, output_dir=\'output\'):\n        annopath = os.path.join(\n            self._devkit_path,\n            \'VOC\' + self._year,\n            \'Annotations\',\n            \'{:s}.xml\')\n        imagesetfile = os.path.join(\n            self._devkit_path,\n            \'VOC\' + self._year,\n            \'ImageSets\',\n            \'Main\',\n            self._image_set + \'.txt\')\n        cachedir = os.path.join(self._devkit_path, \'annotations_cache\')\n        aps = []\n        # The PASCAL VOC metric changed in 2010\n        use_07_metric = True if int(self._year) < 2010 else False\n        print \'VOC07 metric? \' + (\'Yes\' if use_07_metric else \'No\')\n        if not os.path.isdir(output_dir):\n            os.mkdir(output_dir)\n        for i, cls in enumerate(self._classes):\n            if cls == \'__background__\':\n                continue\n            filename = self._get_voc_results_file_template().format(cls)\n            rec, prec, ap = voc_eval(\n                filename, annopath, imagesetfile, cls, cachedir, ovthresh=0.5,\n                use_07_metric=use_07_metric)\n            aps += [ap]\n            print(\'AP for {} = {:.4f}\'.format(cls, ap))\n            with open(os.path.join(output_dir, cls + \'_pr.pkl\'), \'w\') as f:\n                cPickle.dump({\'rec\': rec, \'prec\': prec, \'ap\': ap}, f)\n        print(\'Mean AP = {:.4f}\'.format(np.mean(aps)))\n        print(\'~~~~~~~~\')\n        print(\'Results:\')\n        for ap in aps:\n            print(\'{:.3f}\'.format(ap))\n        print(\'{:.3f}\'.format(np.mean(aps)))\n        print(\'~~~~~~~~\')\n        print(\'\')\n        print(\'--------------------------------------------------------------\')\n        print(\'Results computed with the **unofficial** Python eval code.\')\n        print(\'Results should be very close to the official MATLAB eval code.\')\n        print(\'Recompute with `./tools/reval.py --matlab ...` for your paper.\')\n        print(\'-- Thanks, The Management\')\n        print(\'--------------------------------------------------------------\')\n\n    def _do_matlab_eval(self, output_dir=\'output\'):\n        print \'-----------------------------------------------------\'\n        print \'Computing results with the official MATLAB eval code.\'\n        print \'-----------------------------------------------------\'\n        path = os.path.join(cfg.ROOT_DIR, \'lib\', \'datasets\',\n                            \'VOCdevkit-matlab-wrapper\')\n        cmd = \'cd {} && \'.format(path)\n        cmd += \'{:s} -nodisplay -nodesktop \'.format(cfg.MATLAB)\n        cmd += \'-r ""dbstop if error; \'\n        cmd += \'voc_eval(\\\'{:s}\\\',\\\'{:s}\\\',\\\'{:s}\\\',\\\'{:s}\\\'); quit;""\' \\\n            .format(self._devkit_path, self._get_comp_id(),\n                    self._image_set, output_dir)\n        print(\'Running:\\n{}\'.format(cmd))\n        status = subprocess.call(cmd, shell=True)\n\n    def evaluate_detections(self, all_boxes, output_dir):\n        self._write_voc_results_file(all_boxes)\n        self._do_python_eval(output_dir)\n        if self.config[\'matlab_eval\']:\n            self._do_matlab_eval(output_dir)\n        if self.config[\'cleanup\']:\n            for cls in self._classes:\n                if cls == \'__background__\':\n                    continue\n                filename = self._get_voc_results_file_template().format(cls)\n                os.remove(filename)\n\n    def competition_mode(self, on):\n        if on:\n            self.config[\'use_salt\'] = False\n            self.config[\'cleanup\'] = False\n        else:\n            self.config[\'use_salt\'] = True\n            self.config[\'cleanup\'] = True\n\n\nif __name__ == \'__main__\':\n    d = pascal_voc(\'trainval\', \'2007\')\n    res = d.roidb\n    from IPython import embed;\n\n    embed()\n'"
faster_rcnn/datasets/pascal_voc2.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport xml.dom.minidom as minidom\n\nimport os\nimport PIL\nimport numpy as np\nimport scipy.sparse\nimport subprocess\nimport cPickle\nimport math\nimport glob\nimport uuid\nimport scipy.io as sio\nimport xml.etree.ElementTree as ET\n\nfrom .imdb import imdb\nfrom .imdb import ROOT_DIR\nfrom .imdb import MATLAB\n\nfrom ..utils.cython_bbox import bbox_overlaps\nfrom ..utils.boxes_grid import get_boxes_grid\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\nfrom ..rpn_msr.generate_anchors import generate_anchors\n# <<<< obsolete\n\nclass pascal_voc(imdb):\n    def __init__(self, image_set, year, pascal_path=None):\n        imdb.__init__(self, \'voc_\' + year + \'_\' + image_set)\n        self._year = year\n        self._image_set = image_set\n        self._pascal_path = self._get_default_path() if pascal_path is None \\\n                            else pascal_path\n        self._data_path = os.path.join(self._pascal_path, \'VOCdevkit\' + self._year, \'VOC\' + self._year)\n        self._classes = (\'__background__\', # always index 0\n                         \'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n                         \'bottle\', \'bus\', \'car\', \'cat\', \'chair\',\n                         \'cow\', \'diningtable\', \'dog\', \'horse\',\n                         \'motorbike\', \'person\', \'pottedplant\',\n                         \'sheep\', \'sofa\', \'train\', \'tvmonitor\')\n        self._class_to_ind = dict(zip(self.classes, xrange(self.num_classes)))\n        self._image_ext = \'.jpg\'\n        self._image_index = self._load_image_set_index()\n        # Default to roidb handler\n        if cfg.IS_RPN:\n            self._roidb_handler = self.gt_roidb\n        else:\n            self._roidb_handler = self.region_proposal_roidb\n\n        # num of subclasses\n        self._num_subclasses = 240 + 1\n\n        # load the mapping for subcalss to class\n        filename = os.path.join(self._pascal_path, \'subcategory_exemplars\', \'mapping.txt\')\n        assert os.path.exists(filename), \'Path does not exist: {}\'.format(filename)\n        \n        mapping = np.zeros(self._num_subclasses, dtype=np.int)\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[0])\n                mapping[subcls] = self._class_to_ind[words[1]]\n        self._subclass_mapping = mapping\n\n        # PASCAL specific config options\n        self.config = {\'cleanup\'  : True,\n                       \'use_salt\' : True,\n                       \'top_k\'    : 2000}\n\n        # statistics for computing recall\n        self._num_boxes_all = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_covered = np.zeros(self.num_classes, dtype=np.int)\n        self._num_boxes_proposal = 0\n\n        assert os.path.exists(self._pascal_path), \\\n                \'PASCAL path does not exist: {}\'.format(self._pascal_path)\n        assert os.path.exists(self._data_path), \\\n                \'Path does not exist: {}\'.format(self._data_path)\n\n    def image_path_at(self, i):\n        """"""\n        Return the absolute path to image i in the image sequence.\n        """"""\n        return self.image_path_from_index(self._image_index[i])\n\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n        image_path = os.path.join(self._data_path, \'JPEGImages\',\n                                  index + self._image_ext)\n        assert os.path.exists(image_path), \\\n                \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def _load_image_set_index(self):\n        """"""\n        Load the indexes listed in this dataset\'s image set file.\n        """"""\n        # Example path to image set file:\n        # self._pascal_path + /VOCdevkit2007/VOC2007/ImageSets/Main/val.txt\n        image_set_file = os.path.join(self._data_path, \'ImageSets\', \'Main\',\n                                      self._image_set + \'.txt\')\n        assert os.path.exists(image_set_file), \\\n                \'Path does not exist: {}\'.format(image_set_file)\n        with open(image_set_file) as f:\n            image_index = [x.strip() for x in f.readlines()]\n        return image_index\n\n    def _get_default_path(self):\n        """"""\n        Return the default path where PASCAL VOC is expected to be installed.\n        """"""\n        return os.path.join(ROOT_DIR, \'data\', \'PASCAL\')\n\n    def gt_roidb(self):\n        """"""\n        Return the database of ground-truth regions of interest.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path, self.name + \'_gt_roidb.pkl\')\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} gt roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        gt_roidb = [self._load_pascal_subcategory_exemplar_annotation(index)\n                    for index in self.image_index]\n\n        if cfg.IS_RPN:\n            # print out recall\n            for i in xrange(1, self.num_classes):\n                print \'{}: Total number of boxes {:d}\'.format(self.classes[i], self._num_boxes_all[i])\n                print \'{}: Number of boxes covered {:d}\'.format(self.classes[i], self._num_boxes_covered[i])\n                print \'{}: Recall {:f}\'.format(self.classes[i], float(self._num_boxes_covered[i]) / float(self._num_boxes_all[i]))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(gt_roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote gt roidb to {}\'.format(cache_file)\n\n        return gt_roidb\n\n\n    def _load_pascal_annotation(self, index):\n        """"""\n        Load image and bounding boxes info from XML file in the PASCAL VOC\n        format.\n        """"""\n        filename = os.path.join(self._data_path, \'Annotations\', index + \'.xml\')\n        # print \'Loading: {}\'.format(filename)\n        def get_data_from_tag(node, tag):\n            return node.getElementsByTagName(tag)[0].childNodes[0].data\n\n        with open(filename) as f:\n            data = minidom.parseString(f.read())\n\n        objs = data.getElementsByTagName(\'object\')\n        num_objs = len(objs)\n\n        boxes = np.zeros((num_objs, 4), dtype=np.uint16)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n\n        # Load object bounding boxes into a data frame.\n        for ix, obj in enumerate(objs):\n            # Make pixel indexes 0-based\n            x1 = float(get_data_from_tag(obj, \'xmin\')) - 1\n            y1 = float(get_data_from_tag(obj, \'ymin\')) - 1\n            x2 = float(get_data_from_tag(obj, \'xmax\')) - 1\n            y2 = float(get_data_from_tag(obj, \'ymax\')) - 1\n            cls = self._class_to_ind[\n                    str(get_data_from_tag(obj, ""name"")).lower().strip()]\n            boxes[ix, :] = [x1, y1, x2, y2]\n            gt_classes[ix] = cls\n            overlaps[ix, cls] = 1.0\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n        gt_subclasses = np.zeros((num_objs), dtype=np.int32)\n        gt_subclasses_flipped = np.zeros((num_objs), dtype=np.int32)\n        subindexes = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n        subindexes_flipped = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n\n        if cfg.IS_RPN:\n            if cfg.IS_MULTISCALE:\n                # compute overlaps between grid boxes and gt boxes in multi-scales\n                # rescale the gt boxes\n                boxes_all = np.zeros((0, 4), dtype=np.float32)\n                for scale in cfg.TRAIN.SCALES:\n                    boxes_all = np.vstack((boxes_all, boxes * scale))\n                gt_classes_all = np.tile(gt_classes, len(cfg.TRAIN.SCALES))\n\n                # compute grid boxes\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n                boxes_grid, _, _ = get_boxes_grid(image_height, image_width)\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(boxes_grid.astype(np.float), boxes_all.astype(np.float))\n        \n                # check how many gt boxes are covered by grids\n                if num_objs != 0:\n                    index = np.tile(range(num_objs), len(cfg.TRAIN.SCALES))\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes_all == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n                    index_covered = np.unique(index[fg_inds])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[index_covered] == i)[0])\n            else:\n                assert len(cfg.TRAIN.SCALES_BASE) == 1\n                scale = cfg.TRAIN.SCALES_BASE[0]\n                feat_stride = 16\n                # faster rcnn region proposal\n                anchors = generate_anchors()\n                num_anchors = anchors.shape[0]\n\n                # image size\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n\n                # height and width of the heatmap\n                height = np.round((image_height * scale - 1) / 4.0 + 1)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n\n                width = np.round((image_width * scale - 1) / 4.0 + 1)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n\n                # gt boxes\n                gt_boxes = boxes * scale\n\n                # 1. Generate proposals from bbox deltas and shifted anchors\n                shift_x = np.arange(0, width) * feat_stride\n                shift_y = np.arange(0, height) * feat_stride\n                shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n                shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                            shift_x.ravel(), shift_y.ravel())).transpose()\n                # add A anchors (1, A, 4) to\n                # cell K shifts (K, 1, 4) to get\n                # shift anchors (K, A, 4)\n                # reshape to (K*A, 4) shifted anchors\n                A = num_anchors\n                K = shifts.shape[0]\n                all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n                all_anchors = all_anchors.reshape((K * A, 4))\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(all_anchors.astype(np.float), gt_boxes.astype(np.float))\n        \n                # check how many gt boxes are covered by anchors\n                if num_objs != 0:\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[fg_inds] == i)[0])\n\n        return {\'boxes\' : boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_subclasses\': gt_subclasses,\n                \'gt_subclasses_flipped\': gt_subclasses_flipped,\n                \'gt_overlaps\' : overlaps,\n                \'gt_subindexes\': subindexes,\n                \'gt_subindexes_flipped\': subindexes_flipped,\n                \'flipped\' : False}\n\n\n    def _load_pascal_subcategory_exemplar_annotation(self, index):\n        """"""\n        Load image and bounding boxes info from txt file in the pascal subcategory exemplar format.\n        """"""\n        if self._image_set == \'test\':\n            return self._load_pascal_annotation(index)\n\n        filename = os.path.join(self._pascal_path, \'subcategory_exemplars\', index + \'.txt\')\n        assert os.path.exists(filename), \\\n                \'Path does not exist: {}\'.format(filename)\n\n        # the annotation file contains flipped objects    \n        lines = []\n        lines_flipped = []\n        with open(filename) as f:\n            for line in f:\n                words = line.split()\n                subcls = int(words[1])\n                is_flip = int(words[2])\n                if subcls != -1:\n                    if is_flip == 0:\n                        lines.append(line)\n                    else:\n                        lines_flipped.append(line)\n        \n        num_objs = len(lines)\n\n        # store information of flipped objects\n        assert (num_objs == len(lines_flipped)), \'The number of flipped objects is not the same!\'\n        gt_subclasses_flipped = np.zeros((num_objs), dtype=np.int32)\n        \n        for ix, line in enumerate(lines_flipped):\n            words = line.split()\n            subcls = int(words[1])\n            gt_subclasses_flipped[ix] = subcls\n\n        boxes = np.zeros((num_objs, 4), dtype=np.float32)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        gt_subclasses = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        subindexes = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n        subindexes_flipped = np.zeros((num_objs, self.num_classes), dtype=np.int32)\n\n        for ix, line in enumerate(lines):\n            words = line.split()\n            cls = self._class_to_ind[words[0]]\n            subcls = int(words[1])\n            # Make pixel indexes 0-based\n            boxes[ix, :] = [float(n)-1 for n in words[3:7]]\n            gt_classes[ix] = cls\n            gt_subclasses[ix] = subcls\n            overlaps[ix, cls] = 1.0\n            subindexes[ix, cls] = subcls\n            subindexes_flipped[ix, cls] = gt_subclasses_flipped[ix]\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n\n        if cfg.IS_RPN:\n            if cfg.IS_MULTISCALE:\n                # compute overlaps between grid boxes and gt boxes in multi-scales\n                # rescale the gt boxes\n                boxes_all = np.zeros((0, 4), dtype=np.float32)\n                for scale in cfg.TRAIN.SCALES:\n                    boxes_all = np.vstack((boxes_all, boxes * scale))\n                gt_classes_all = np.tile(gt_classes, len(cfg.TRAIN.SCALES))\n\n                # compute grid boxes\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n                boxes_grid, _, _ = get_boxes_grid(image_height, image_width)\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(boxes_grid.astype(np.float), boxes_all.astype(np.float))\n        \n                # check how many gt boxes are covered by grids\n                if num_objs != 0:\n                    index = np.tile(range(num_objs), len(cfg.TRAIN.SCALES))\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes_all == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n                    index_covered = np.unique(index[fg_inds])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[index_covered] == i)[0])\n            else:\n                assert len(cfg.TRAIN.SCALES_BASE) == 1\n                scale = cfg.TRAIN.SCALES_BASE[0]\n                feat_stride = 16\n                # faster rcnn region proposal\n                base_size = 16\n                ratios = [3.0, 2.0, 1.5, 1.0, 0.75, 0.5, 0.25]\n                scales = 2**np.arange(1, 6, 0.5)\n                anchors = generate_anchors(base_size, ratios, scales)\n                num_anchors = anchors.shape[0]\n\n                # image size\n                s = PIL.Image.open(self.image_path_from_index(index)).size\n                image_height = s[1]\n                image_width = s[0]\n\n                # height and width of the heatmap\n                height = np.round((image_height * scale - 1) / 4.0 + 1)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n                height = np.floor((height - 1) / 2 + 1 + 0.5)\n\n                width = np.round((image_width * scale - 1) / 4.0 + 1)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n                width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n\n                # gt boxes\n                gt_boxes = boxes * scale\n\n                # 1. Generate proposals from bbox deltas and shifted anchors\n                shift_x = np.arange(0, width) * feat_stride\n                shift_y = np.arange(0, height) * feat_stride\n                shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n                shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                            shift_x.ravel(), shift_y.ravel())).transpose()\n                # add A anchors (1, A, 4) to\n                # cell K shifts (K, 1, 4) to get\n                # shift anchors (K, A, 4)\n                # reshape to (K*A, 4) shifted anchors\n                A = num_anchors\n                K = shifts.shape[0]\n                all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n                all_anchors = all_anchors.reshape((K * A, 4))\n\n                # compute overlap\n                overlaps_grid = bbox_overlaps(all_anchors.astype(np.float), gt_boxes.astype(np.float))\n        \n                # check how many gt boxes are covered by anchors\n                if num_objs != 0:\n                    max_overlaps = overlaps_grid.max(axis = 0)\n                    fg_inds = []\n                    for k in xrange(1, self.num_classes):\n                        fg_inds.extend(np.where((gt_classes == k) & (max_overlaps >= cfg.TRAIN.FG_THRESH[k-1]))[0])\n\n                    for i in xrange(self.num_classes):\n                        self._num_boxes_all[i] += len(np.where(gt_classes == i)[0])\n                        self._num_boxes_covered[i] += len(np.where(gt_classes[fg_inds] == i)[0])\n\n        return {\'boxes\' : boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_subclasses\': gt_subclasses,\n                \'gt_subclasses_flipped\': gt_subclasses_flipped,\n                \'gt_overlaps\': overlaps,\n                \'gt_subindexes\': subindexes, \n                \'gt_subindexes_flipped\': subindexes_flipped, \n                \'flipped\' : False}\n\n    def region_proposal_roidb(self):\n        """"""\n        Return the database of regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_\' + cfg.REGION_PROPOSAL + \'_region_proposal_roidb.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        if self._image_set != \'test\':\n            gt_roidb = self.gt_roidb()\n\n            print \'Loading region proposal network boxes...\'\n            model = cfg.REGION_PROPOSAL\n            rpn_roidb = self._load_rpn_roidb(gt_roidb, model)\n            print \'Region proposal network boxes loaded\'\n            roidb = imdb.merge_roidbs(rpn_roidb, gt_roidb)\n        else:\n            print \'Loading region proposal network boxes...\'\n            model = cfg.REGION_PROPOSAL\n            roidb = self._load_rpn_roidb(None, model)\n            print \'Region proposal network boxes loaded\'\n\n        print \'{} region proposals per image\'.format(self._num_boxes_proposal / len(self.image_index))\n\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def _load_rpn_roidb(self, gt_roidb, model):\n        # set the prefix\n        if self._image_set == \'test\':\n            prefix = model + \'/testing\'\n        else:\n            prefix = model + \'/training\'\n\n        box_list = []\n        for index in self.image_index:\n            filename = os.path.join(self._pascal_path, \'region_proposals\',  prefix, index + \'.txt\')\n            assert os.path.exists(filename), \\\n                \'RPN data not found at: {}\'.format(filename)\n            raw_data = np.loadtxt(filename, dtype=float)\n            if len(raw_data.shape) == 1:\n                if raw_data.size == 0:\n                    raw_data = raw_data.reshape((0, 5))\n                else:\n                    raw_data = raw_data.reshape((1, 5))\n\n            x1 = raw_data[:, 0]\n            y1 = raw_data[:, 1]\n            x2 = raw_data[:, 2]\n            y2 = raw_data[:, 3]\n            score = raw_data[:, 4]\n            inds = np.where((x2 > x1) & (y2 > y1))[0]\n            raw_data = raw_data[inds,:4]\n            self._num_boxes_proposal += raw_data.shape[0]\n            box_list.append(raw_data)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n\n    def selective_search_roidb(self):\n        """"""\n        Return the database of selective search regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                                  self.name + \'_selective_search_roidb.pkl\')\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} ss roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        if int(self._year) == 2007 or self._image_set != \'test\':\n            gt_roidb = self.gt_roidb()\n            ss_roidb = self._load_selective_search_roidb(gt_roidb)\n            roidb = imdb.merge_roidbs(gt_roidb, ss_roidb)\n        else:\n            roidb = self._load_selective_search_roidb(None)\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote ss roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def _load_selective_search_roidb(self, gt_roidb):\n        filename = os.path.abspath(os.path.join(self.cache_path, \'..\',\n                                                \'selective_search_data\',\n                                                self.name + \'.mat\'))\n        assert os.path.exists(filename), \\\n               \'Selective search data not found at: {}\'.format(filename)\n        raw_data = sio.loadmat(filename)[\'boxes\'].ravel()\n\n        box_list = []\n        for i in xrange(raw_data.shape[0]):\n            box_list.append(raw_data[i][:, (1, 0, 3, 2)] - 1)\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n    def selective_search_IJCV_roidb(self):\n        """"""\n        Return the database of selective search regions of interest.\n        Ground-truth ROIs are also included.\n\n        This function loads/saves from/to a cache file to speed up future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path,\n                \'{:s}_selective_search_IJCV_top_{:d}_roidb.pkl\'.\n                format(self.name, self.config[\'top_k\']))\n\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = cPickle.load(fid)\n            print \'{} ss roidb loaded from {}\'.format(self.name, cache_file)\n            return roidb\n\n        gt_roidb = self.gt_roidb()\n        ss_roidb = self._load_selective_search_IJCV_roidb(gt_roidb)\n        roidb = imdb.merge_roidbs(gt_roidb, ss_roidb)\n        with open(cache_file, \'wb\') as fid:\n            cPickle.dump(roidb, fid, cPickle.HIGHEST_PROTOCOL)\n        print \'wrote ss roidb to {}\'.format(cache_file)\n\n        return roidb\n\n    def _load_selective_search_IJCV_roidb(self, gt_roidb):\n        IJCV_path = os.path.abspath(os.path.join(self.cache_path, \'..\',\n                                                 \'selective_search_IJCV_data\',\n                                                 \'voc_\' + self._year))\n        assert os.path.exists(IJCV_path), \\\n               \'Selective search IJCV data not found at: {}\'.format(IJCV_path)\n\n        top_k = self.config[\'top_k\']\n        box_list = []\n        for i in xrange(self.num_images):\n            filename = os.path.join(IJCV_path, self.image_index[i] + \'.mat\')\n            raw_data = sio.loadmat(filename)\n            box_list.append((raw_data[\'boxes\'][:top_k, :]-1).astype(np.uint16))\n\n        return self.create_roidb_from_box_list(box_list, gt_roidb)\n\n\n    def _write_voc_results_file(self, all_boxes):\n        use_salt = self.config[\'use_salt\']\n        comp_id = \'comp4\'\n        if use_salt:\n            comp_id += \'-{}\'.format(os.getpid())\n\n        # VOCdevkit/results/VOC2007/Main/comp4-44503_det_test_aeroplane.txt\n        path = os.path.join(self._pascal_path, \'VOCdevkit\' + self._year, \'results\', \'VOC\' + self._year,\n                            \'Main\', comp_id + \'_\')\n        for cls_ind, cls in enumerate(self.classes):\n            if cls == \'__background__\':\n                continue\n            print \'Writing {} VOC results file\'.format(cls)\n            filename = path + \'det_\' + self._image_set + \'_\' + cls + \'.txt\'\n            print filename\n            with open(filename, \'wt\') as f:\n                for im_ind, index in enumerate(self.image_index):\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    # the VOCdevkit expects 1-based indices\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\\n\'.\n                                format(index, dets[k, 4],\n                                       dets[k, 0] + 1, dets[k, 1] + 1,\n                                       dets[k, 2] + 1, dets[k, 3] + 1))\n        return comp_id\n\n    def _do_matlab_eval(self, comp_id, output_dir=\'output\'):\n        rm_results = self.config[\'cleanup\']\n\n        path = os.path.join(os.path.dirname(__file__),\n                            \'VOCdevkit-matlab-wrapper\')\n        cmd = \'cd {} && \'.format(path)\n        cmd += \'{:s} -nodisplay -nodesktop \'.format(MATLAB)\n        cmd += \'-r ""dbstop if error; \'\n        cmd += \'voc_eval(\\\'{:s}\\\',\\\'{:s}\\\',\\\'{:s}\\\',\\\'{:s}\\\',{:d}); quit;""\' \\\n               .format(self._pascal_path + \'/VOCdevkit\' + self._year, comp_id,\n                       self._image_set, output_dir, int(rm_results))\n        print(\'Running:\\n{}\'.format(cmd))\n        status = subprocess.call(cmd, shell=True)\n\n    # evaluate detection results\n    def evaluate_detections(self, all_boxes, output_dir):\n        comp_id = self._write_voc_results_file(all_boxes)\n        self._do_matlab_eval(comp_id, output_dir)\n\n    def evaluate_proposals(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing PASCAL results to file \' + filename\n            with open(filename, \'wt\') as f:\n                # for each class\n                for cls_ind, cls in enumerate(self.classes):\n                    if cls == \'__background__\':\n                        continue\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in xrange(dets.shape[0]):\n                        f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(\\\n                                 dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n    def evaluate_proposals_msr(self, all_boxes, output_dir):\n        # for each image\n        for im_ind, index in enumerate(self.image_index):\n            filename = os.path.join(output_dir, index + \'.txt\')\n            print \'Writing PASCAL results to file \' + filename\n            with open(filename, \'wt\') as f:\n                dets = all_boxes[im_ind]\n                if dets == []:\n                    continue\n                for k in xrange(dets.shape[0]):\n                    f.write(\'{:f} {:f} {:f} {:f} {:.32f}\\n\'.format(dets[k, 0], dets[k, 1], dets[k, 2], dets[k, 3], dets[k, 4]))\n\n\n    def competition_mode(self, on):\n        if on:\n            self.config[\'use_salt\'] = False\n            self.config[\'cleanup\'] = False\n        else:\n            self.config[\'use_salt\'] = True\n            self.config[\'cleanup\'] = True\n\nif __name__ == \'__main__\':\n    d = pascal_voc(\'trainval\', \'2007\')\n    res = d.roidb\n    from IPython import embed; embed()\n'"
faster_rcnn/datasets/voc_eval.py,0,"b'# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Bharath Hariharan\n# --------------------------------------------------------\n\nimport xml.etree.ElementTree as ET\nimport os\nimport cPickle\nimport numpy as np\nimport pdb\ndef parse_rec(filename):\n    """""" Parse a PASCAL VOC xml file """"""\n    tree = ET.parse(filename)\n    objects = []\n    for obj in tree.findall(\'object\'):\n        obj_struct = {}\n        obj_struct[\'name\'] = obj.find(\'name\').text\n        obj_struct[\'pose\'] = obj.find(\'pose\').text\n        obj_struct[\'truncated\'] = int(obj.find(\'truncated\').text)\n        obj_struct[\'difficult\'] = int(obj.find(\'difficult\').text)\n        bbox = obj.find(\'bndbox\')\n        obj_struct[\'bbox\'] = [int(bbox.find(\'xmin\').text),\n                              int(bbox.find(\'ymin\').text),\n                              int(bbox.find(\'xmax\').text),\n                              int(bbox.find(\'ymax\').text)]\n        objects.append(obj_struct)\n\n    return objects\n\ndef voc_ap(rec, prec, use_07_metric=False):\n    """""" ap = voc_ap(rec, prec, [use_07_metric])\n    Compute VOC AP given precision and recall.\n    If use_07_metric is true, uses the\n    VOC 07 11 point method (default:False).\n    """"""\n    if use_07_metric:\n        # 11 point metric\n        ap = 0.\n        for t in np.arange(0., 1.1, 0.1):\n            if np.sum(rec >= t) == 0:\n                p = 0\n            else:\n                p = np.max(prec[rec >= t])\n            ap = ap + p / 11.\n    else:\n        # correct AP calculation\n        # first append sentinel values at the end\n        mrec = np.concatenate(([0.], rec, [1.]))\n        mpre = np.concatenate(([0.], prec, [0.]))\n\n        # compute the precision envelope\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n        # to calculate area under PR curve, look for points\n        # where X axis (recall) changes value\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        # and sum (\\Delta recall) * prec\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap\n\ndef voc_eval(detpath,\n             annopath,\n             imagesetfile,\n             classname,\n             cachedir,\n             ovthresh=0.5,\n             use_07_metric=False):\n    """"""rec, prec, ap = voc_eval(detpath,\n                                annopath,\n                                imagesetfile,\n                                classname,\n                                [ovthresh],\n                                [use_07_metric])\n\n    Top level function that does the PASCAL VOC evaluation.\n\n    detpath: Path to detections\n        detpath.format(classname) should produce the detection results file.\n    annopath: Path to annotations\n        annopath.format(imagename) should be the xml annotations file.\n    imagesetfile: Text file containing the list of images, one image per line.\n    classname: Category name (duh)\n    cachedir: Directory for caching the annotations\n    [ovthresh]: Overlap threshold (default = 0.5)\n    [use_07_metric]: Whether to use VOC07\'s 11 point AP computation\n        (default False)\n    """"""\n    # assumes detections are in detpath.format(classname)\n    # assumes annotations are in annopath.format(imagename)\n    # assumes imagesetfile is a text file with each line an image name\n    # cachedir caches the annotations in a pickle file\n\n    # first load gt\n    if not os.path.isdir(cachedir):\n        os.mkdir(cachedir)\n    cachefile = os.path.join(cachedir, \'annots.pkl\')\n    # read list of images\n    with open(imagesetfile, \'r\') as f:\n        lines = f.readlines()\n    imagenames = [x.strip() for x in lines]\n\n    if not os.path.isfile(cachefile):\n        # load annots\n        recs = {}\n        for i, imagename in enumerate(imagenames):\n            recs[imagename] = parse_rec(annopath.format(imagename))\n            if i % 100 == 0:\n                print \'Reading annotation for {:d}/{:d}\'.format(\n                    i + 1, len(imagenames))\n        # save\n        print \'Saving cached annotations to {:s}\'.format(cachefile)\n        with open(cachefile, \'w\') as f:\n            cPickle.dump(recs, f)\n    else:\n        # load\n        with open(cachefile, \'r\') as f:\n            recs = cPickle.load(f)\n\n    # extract gt objects for this class\n    class_recs = {}\n    npos = 0\n    for imagename in imagenames:\n        R = [obj for obj in recs[imagename] if obj[\'name\'] == classname]\n        bbox = np.array([x[\'bbox\'] for x in R])\n        difficult = np.array([x[\'difficult\'] for x in R]).astype(np.bool)\n        det = [False] * len(R)\n        npos = npos + sum(~difficult)\n        class_recs[imagename] = {\'bbox\': bbox,\n                                 \'difficult\': difficult,\n                                 \'det\': det}\n\n    # read dets\n    detfile = detpath.format(classname)\n    with open(detfile, \'r\') as f:\n        lines = f.readlines()\n    if any(lines) == 1:\n\n        splitlines = [x.strip().split(\' \') for x in lines]\n        image_ids = [x[0] for x in splitlines]\n        confidence = np.array([float(x[1]) for x in splitlines])\n        BB = np.array([[float(z) for z in x[2:]] for x in splitlines])\n\n        # sort by confidence\n        sorted_ind = np.argsort(-confidence)\n        sorted_scores = np.sort(-confidence)\n        BB = BB[sorted_ind, :]\n        image_ids = [image_ids[x] for x in sorted_ind]\n\n        # go down dets and mark TPs and FPs\n        nd = len(image_ids)\n        tp = np.zeros(nd)\n        fp = np.zeros(nd)\n        for d in range(nd):\n            R = class_recs[image_ids[d]]\n            bb = BB[d, :].astype(float)\n            ovmax = -np.inf\n            BBGT = R[\'bbox\'].astype(float)\n\n            if BBGT.size > 0:\n                # compute overlaps\n                # intersection\n                ixmin = np.maximum(BBGT[:, 0], bb[0])\n                iymin = np.maximum(BBGT[:, 1], bb[1])\n                ixmax = np.minimum(BBGT[:, 2], bb[2])\n                iymax = np.minimum(BBGT[:, 3], bb[3])\n                iw = np.maximum(ixmax - ixmin + 1., 0.)\n                ih = np.maximum(iymax - iymin + 1., 0.)\n                inters = iw * ih\n\n                # union\n                uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +\n                       (BBGT[:, 2] - BBGT[:, 0] + 1.) *\n                       (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)\n\n                overlaps = inters / uni\n                ovmax = np.max(overlaps)\n                jmax = np.argmax(overlaps)\n\n            if ovmax > ovthresh:\n                if not R[\'difficult\'][jmax]:\n                    if not R[\'det\'][jmax]:\n                        tp[d] = 1.\n                        R[\'det\'][jmax] = 1\n                    else:\n                        fp[d] = 1.\n            else:\n                fp[d] = 1.\n\n        # compute precision recall\n        fp = np.cumsum(fp)\n        tp = np.cumsum(tp)\n        rec = tp / float(npos)\n        # avoid divide by zero in case the first detection matches a difficult\n        # ground truth\n        prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n        ap = voc_ap(rec, prec, use_07_metric)\n    else:\n         rec = -1\n         prec = -1\n         ap = -1\n\n    return rec, prec, ap\n'"
faster_rcnn/fast_rcnn/__init__.py,0,b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nfrom . import config\nfrom . import nms_wrapper\n# from nms_wrapper import nms'
faster_rcnn/fast_rcnn/bbox_transform.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport numpy as np\nfrom sympy.physics.paulialgebra import delta\n\n\ndef bbox_transform(ex_rois, gt_rois):\n    """"""\n    computes the distance from ground-truth boxes to the given boxes, normed by their size\n    :param ex_rois: n * 4 numpy array, given boxes\n    :param gt_rois: n * 4 numpy array, ground-truth boxes\n    :return: deltas: n * 4 numpy array, ground-truth boxes\n    """"""\n    ex_widths = ex_rois[:, 2] - ex_rois[:, 0] + 1.0\n    ex_heights = ex_rois[:, 3] - ex_rois[:, 1] + 1.0\n    ex_ctr_x = ex_rois[:, 0] + 0.5 * ex_widths\n    ex_ctr_y = ex_rois[:, 1] + 0.5 * ex_heights\n\n    # assert np.min(ex_widths) > 0.1 and np.min(ex_heights) > 0.1, \\\n    #     \'Invalid boxes found: {} {}\'. \\\n    #         format(ex_rois[np.argmin(ex_widths), :], ex_rois[np.argmin(ex_heights), :])\n\n    gt_widths = gt_rois[:, 2] - gt_rois[:, 0] + 1.0\n    gt_heights = gt_rois[:, 3] - gt_rois[:, 1] + 1.0\n    gt_ctr_x = gt_rois[:, 0] + 0.5 * gt_widths\n    gt_ctr_y = gt_rois[:, 1] + 0.5 * gt_heights\n\n    targets_dx = (gt_ctr_x - ex_ctr_x) / ex_widths\n    targets_dy = (gt_ctr_y - ex_ctr_y) / ex_heights\n    targets_dw = np.log(gt_widths / ex_widths)\n    targets_dh = np.log(gt_heights / ex_heights)\n\n    targets = np.vstack(\n        (targets_dx, targets_dy, targets_dw, targets_dh)).transpose()\n    return targets\n\n\ndef bbox_transform_inv(boxes, deltas):\n    if boxes.shape[0] == 0:\n        return np.zeros((0,), dtype=deltas.dtype)\n\n    boxes = boxes.astype(deltas.dtype, copy=False)\n\n    widths = boxes[:, 2] - boxes[:, 0] + 1.0\n    heights = boxes[:, 3] - boxes[:, 1] + 1.0\n    ctr_x = boxes[:, 0] + 0.5 * widths\n    ctr_y = boxes[:, 1] + 0.5 * heights\n\n    dx = deltas[:, 0::4]\n    dy = deltas[:, 1::4]\n    dw = deltas[:, 2::4]\n    dh = deltas[:, 3::4]\n\n    pred_ctr_x = dx * widths[:, np.newaxis] + ctr_x[:, np.newaxis]\n    pred_ctr_y = dy * heights[:, np.newaxis] + ctr_y[:, np.newaxis]\n    pred_w = np.exp(dw) * widths[:, np.newaxis]\n    pred_h = np.exp(dh) * heights[:, np.newaxis]\n\n    pred_boxes = np.zeros(deltas.shape, dtype=deltas.dtype)\n    # x1\n    pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n    # y1\n    pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n    # x2\n    pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w\n    # y2\n    pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h\n\n    return pred_boxes\n\n\ndef clip_boxes(boxes, im_shape):\n    """"""\n    Clip boxes to image boundaries.\n    """"""\n    if boxes.shape[0] == 0:\n        return boxes\n\n    # x1 >= 0\n    boxes[:, 0::4] = np.maximum(np.minimum(boxes[:, 0::4], im_shape[1] - 1), 0)\n    # y1 >= 0\n    boxes[:, 1::4] = np.maximum(np.minimum(boxes[:, 1::4], im_shape[0] - 1), 0)\n    # x2 < im_shape[1]\n    boxes[:, 2::4] = np.maximum(np.minimum(boxes[:, 2::4], im_shape[1] - 1), 0)\n    # y2 < im_shape[0]\n    boxes[:, 3::4] = np.maximum(np.minimum(boxes[:, 3::4], im_shape[0] - 1), 0)\n    return boxes\n'"
faster_rcnn/fast_rcnn/config.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Fast R-CNN config system.\n\nThis file specifies default config options for Fast R-CNN. You should not\nchange values in this file. Instead, you should write a config file (in yaml)\nand use cfg_from_file(yaml_file) to load it and override the default options.\n\nMost tools in $ROOT/tools take a --cfg option to specify an override file.\n    - See tools/{train,test}_net.py for example code that uses cfg_from_file()\n    - See experiments/cfgs/*.yml for example YAML config override files\n""""""\n\nimport os\nimport os.path as osp\nimport numpy as np\nfrom time import strftime, localtime\nfrom easydict import EasyDict as edict\n\n__C = edict()\n# Consumers can get config by:\n#   from fast_rcnn_config import cfg\ncfg = __C\n\n#\n# Training options\n#\n\n# region proposal network (RPN) or not\n__C.IS_RPN = True\n__C.ANCHOR_SCALES = [8, 16, 32]\n__C.NCLASSES = 21\n\n# multiscale training and testing\n__C.IS_MULTISCALE = False\n__C.IS_EXTRAPOLATING = True\n\n__C.REGION_PROPOSAL = \'RPN\'\n\n__C.NET_NAME = \'VGGnet\'\n__C.SUBCLS_NAME = \'voxel_exemplars\'\n\n__C.TRAIN = edict()\n# Adam, Momentum, RMS\n__C.TRAIN.SOLVER = \'Momentum\'\n# learning rate\n__C.TRAIN.WEIGHT_DECAY = 0.0005\n__C.TRAIN.LEARNING_RATE = 0.001\n__C.TRAIN.MOMENTUM = 0.9\n__C.TRAIN.GAMMA = 0.1\n__C.TRAIN.STEPSIZE = 50000\n__C.TRAIN.DISPLAY = 10\n__C.TRAIN.LOG_IMAGE_ITERS = 100\n__C.TRAIN.OHEM = False\n\n# Scales to compute real features\n__C.TRAIN.SCALES_BASE = (0.25, 0.5, 1.0, 2.0, 3.0)\n# __C.TRAIN.SCALES_BASE = (1.0,)\n\n# parameters for ROI generating\n# __C.TRAIN.SPATIAL_SCALE = 0.0625\n__C.TRAIN.KERNEL_SIZE = 5\n\n# Aspect ratio to use during training\n# __C.TRAIN.ASPECTS = (1, 0.75, 0.5, 0.25)\n__C.TRAIN.ASPECTS = (1,)\n\n# Scales to use during training (can list multiple scales)\n# Each scale is the pixel size of an image\'s shortest side\n__C.TRAIN.SCALES = (600,)\n\n# Max pixel size of the longest side of a scaled input image\n__C.TRAIN.MAX_SIZE = 1000\n\n# Images to use per minibatch\n__C.TRAIN.IMS_PER_BATCH = 2\n\n# Minibatch size (number of regions of interest [ROIs])\n__C.TRAIN.BATCH_SIZE = 128\n\n# Fraction of minibatch that is labeled foreground (i.e. class > 0)\n__C.TRAIN.FG_FRACTION = 0.25\n\n# Overlap threshold for a ROI to be considered foreground (if >= FG_THRESH)\n__C.TRAIN.FG_THRESH = 0.5\n\n# Overlap threshold for a ROI to be considered background (class = 0 if\n# overlap in [LO, HI))\n__C.TRAIN.BG_THRESH_HI = 0.5\n__C.TRAIN.BG_THRESH_LO = 0.1\n\n# Use horizontally-flipped images during training?\n__C.TRAIN.USE_FLIPPED = True\n\n# Train bounding-box regressors\n__C.TRAIN.BBOX_REG = True\n\n# Overlap required between a ROI and ground-truth box in order for that ROI to\n# be used as a bounding-box regression training example\n__C.TRAIN.BBOX_THRESH = 0.5\n\n# Iterations between snapshots\n__C.TRAIN.SNAPSHOT_ITERS = 5000\n\n# solver.prototxt specifies the snapshot path prefix, this adds an optional\n# infix to yield the path: <prefix>[_<infix>]_iters_XYZ.caffemodel\n__C.TRAIN.SNAPSHOT_PREFIX = \'VGGnet_fast_rcnn\'\n__C.TRAIN.SNAPSHOT_INFIX = \'\'\n\n# Use a prefetch thread in roi_data_layer.layer\n# So far I haven\'t found this useful; likely more engineering work is required\n__C.TRAIN.USE_PREFETCH = False\n\n# Normalize the targets (subtract empirical mean, divide by empirical stddev)\n__C.TRAIN.BBOX_NORMALIZE_TARGETS = True\n# Deprecated (inside weights)\n# used for assigning weights for each coords (x1, y1, w, h)\n__C.TRAIN.BBOX_INSIDE_WEIGHTS = (1.0, 1.0, 1.0, 1.0)\n# Normalize the targets using ""precomputed"" (or made up) means and stdevs\n# (BBOX_NORMALIZE_TARGETS must also be True)\n__C.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED = True\n__C.TRAIN.BBOX_NORMALIZE_MEANS = (0.0, 0.0, 0.0, 0.0)\n__C.TRAIN.BBOX_NORMALIZE_STDS = (0.1, 0.1, 0.2, 0.2)\n# faster rcnn dont use pre-generated rois by selective search\n# __C.TRAIN.BBOX_NORMALIZE_STDS = (1, 1, 1, 1)\n\n# Train using these proposals\n__C.TRAIN.PROPOSAL_METHOD = \'selective_search\'\n\n# Make minibatches from images that have similar aspect ratios (i.e. both\n# tall and thin or both short and wide) in order to avoid wasting computation\n# on zero-padding.\n__C.TRAIN.ASPECT_GROUPING = True\n# preclude rois intersected with dontcare areas above the value\n__C.TRAIN.DONTCARE_AREA_INTERSECTION_HI = 0.5\n__C.TRAIN.PRECLUDE_HARD_SAMPLES = True\n# Use RPN to detect objects\n__C.TRAIN.HAS_RPN = True\n# IOU >= thresh: positive example\n__C.TRAIN.RPN_POSITIVE_OVERLAP = 0.7\n# IOU < thresh: negative example\n__C.TRAIN.RPN_NEGATIVE_OVERLAP = 0.3\n# If an anchor statisfied by positive and negative conditions set to negative\n__C.TRAIN.RPN_CLOBBER_POSITIVES = False\n# Max number of foreground examples\n__C.TRAIN.RPN_FG_FRACTION = 0.5\n# Total number of examples\n__C.TRAIN.RPN_BATCHSIZE = 256\n# NMS threshold used on RPN proposals\n__C.TRAIN.RPN_NMS_THRESH = 0.7\n# Number of top scoring boxes to keep before apply NMS to RPN proposals\n__C.TRAIN.RPN_PRE_NMS_TOP_N = 12000\n# Number of top scoring boxes to keep after applying NMS to RPN proposals\n__C.TRAIN.RPN_POST_NMS_TOP_N = 2000\n# Proposal height and width both need to be greater than RPN_MIN_SIZE (at orig image scale)\n__C.TRAIN.RPN_MIN_SIZE = 16\n# Deprecated (outside weights)\n__C.TRAIN.RPN_BBOX_INSIDE_WEIGHTS = (1.0, 1.0, 1.0, 1.0)\n# Give the positive RPN examples weight of p * 1 / {num positives}\n# and give negatives a weight of (1 - p)\n# Set to -1.0 to use uniform example weighting\n__C.TRAIN.RPN_POSITIVE_WEIGHT = -1.0\n# __C.TRAIN.RPN_POSITIVE_WEIGHT = 0.5\n\n\n#\n# Testing options\n#\n\n__C.TEST = edict()\n\n# Scales to use during testing (can list multiple scales)\n# Each scale is the pixel size of an image\'s shortest side\n__C.TEST.SCALES = (600,)\n\n# Max pixel size of the longest side of a scaled input image\n__C.TEST.MAX_SIZE = 1000\n\n# Overlap threshold used for non-maximum suppression (suppress boxes with\n# IoU >= this threshold)\n__C.TEST.NMS = 0.3\n\n# Experimental: treat the (K+1) units in the cls_score layer as linear\n# predictors (trained, eg, with one-vs-rest SVMs).\n__C.TEST.SVM = False\n\n# Test using bounding-box regressors\n__C.TEST.BBOX_REG = True\n\n# Propose boxes\n__C.TEST.HAS_RPN = True\n\n# Test using these proposals\n__C.TEST.PROPOSAL_METHOD = \'selective_search\'\n\n## NMS threshold used on RPN proposals\n__C.TEST.RPN_NMS_THRESH = 0.7\n## Number of top scoring boxes to keep before apply NMS to RPN proposals\n__C.TEST.RPN_PRE_NMS_TOP_N = 6000\n# __C.TEST.RPN_PRE_NMS_TOP_N = 12000\n## Number of top scoring boxes to keep after applying NMS to RPN proposals\n__C.TEST.RPN_POST_NMS_TOP_N = 300\n# __C.TEST.RPN_POST_NMS_TOP_N = 2000\n# Proposal height and width both need to be greater than RPN_MIN_SIZE (at orig image scale)\n__C.TEST.RPN_MIN_SIZE = 16\n\n#\n# MISC\n#\n\n# The mapping from image coordinates to feature map coordinates might cause\n# some boxes that are distinct in image space to become identical in feature\n# coordinates. If DEDUP_BOXES > 0, then DEDUP_BOXES is used as the scale factor\n# for identifying duplicate boxes.\n# 1/16 is correct for {Alex,Caffe}Net, VGG_CNN_M_1024, and VGG16\n__C.DEDUP_BOXES = 1. / 16.\n\n# Pixel mean values (BGR order) as a (1, 1, 3) array\n# We use the same pixel mean for all networks even though it\'s not exactly what\n# they were trained with\n__C.PIXEL_MEANS = np.array([[[102.9801, 115.9465, 122.7717]]])\n\n# For reproducibility\n__C.RNG_SEED = 3\n\n# A small number that\'s used many times\n__C.EPS = 1e-14\n\n# Root directory of project\n__C.ROOT_DIR = osp.abspath(osp.join(osp.dirname(__file__), \'..\', \'..\'))\n\n# Data directory\n__C.DATA_DIR = osp.abspath(osp.join(__C.ROOT_DIR, \'data\'))\n\n# Model directory\n__C.MODELS_DIR = osp.abspath(osp.join(__C.ROOT_DIR, \'models\', \'pascal_voc\'))\n\n# Name (or path to) the matlab executable\n__C.MATLAB = \'matlab\'\n\n# Place outputs under an experiments directory\n__C.EXP_DIR = \'default\'\n__C.LOG_DIR = \'default\'\n\n# Use GPU implementation of non-maximum suppression\n__C.USE_GPU_NMS = True\n\n# Default GPU device id\n__C.GPU_ID = 0\n\n\ndef get_output_dir(imdb, weights_filename):\n    """"""Return the directory where experimental artifacts are placed.\n    If the directory does not exist, it is created.\n\n    A canonical path is built using the name from an imdb and a network\n    (if not None).\n    """"""\n    outdir = osp.abspath(osp.join(__C.ROOT_DIR, \'output\', __C.EXP_DIR, imdb.name))\n    if weights_filename is not None:\n        outdir = osp.join(outdir, weights_filename)\n    if not os.path.exists(outdir):\n        os.makedirs(outdir)\n    return outdir\n\n\ndef get_log_dir(imdb):\n    """"""Return the directory where experimental artifacts are placed.\n    If the directory does not exist, it is created.\n    A canonical path is built using the name from an imdb and a network\n    (if not None).\n    """"""\n    log_dir = osp.abspath( \\\n        osp.join(__C.ROOT_DIR, \'logs\', __C.LOG_DIR, imdb.name, strftime(""%Y-%m-%d-%H-%M-%S"", localtime())))\n    if not os.path.exists(log_dir):\n        os.makedirs(log_dir)\n    return log_dir\n\n\ndef _merge_a_into_b(a, b):\n    """"""Merge config dictionary a into config dictionary b, clobbering the\n    options in b whenever they are also specified in a.\n    """"""\n    if type(a) is not edict:\n        return\n\n    for k, v in a.iteritems():\n        # a must specify keys that are in b\n        if not b.has_key(k):\n            raise KeyError(\'{} is not a valid config key\'.format(k))\n\n        # the types must match, too\n        old_type = type(b[k])\n        if old_type is not type(v):\n            if isinstance(b[k], np.ndarray):\n                v = np.array(v, dtype=b[k].dtype)\n            else:\n                raise ValueError((\'Type mismatch ({} vs. {}) \'\n                                  \'for config key: {}\').format(type(b[k]),\n                                                               type(v), k))\n\n        # recursively merge dicts\n        if type(v) is edict:\n            try:\n                _merge_a_into_b(a[k], b[k])\n            except:\n                print(\'Error under config key: {}\'.format(k))\n                raise\n        else:\n            b[k] = v\n\n\ndef cfg_from_file(filename):\n    """"""Load a config file and merge it into the default options.""""""\n    import yaml\n    with open(filename, \'r\') as f:\n        yaml_cfg = edict(yaml.load(f))\n\n    _merge_a_into_b(yaml_cfg, __C)\n\n\ndef cfg_from_list(cfg_list):\n    """"""Set config keys via list (e.g., from command line).""""""\n    from ast import literal_eval\n    assert len(cfg_list) % 2 == 0\n    for k, v in zip(cfg_list[0::2], cfg_list[1::2]):\n        key_list = k.split(\'.\')\n        d = __C\n        for subkey in key_list[:-1]:\n            assert d.has_key(subkey)\n            d = d[subkey]\n        subkey = key_list[-1]\n        assert d.has_key(subkey)\n        try:\n            value = literal_eval(v)\n        except:\n            # handle the case when v is a string literal\n            value = v\n        assert type(value) == type(d[subkey]), \\\n            \'type {} does not match original type {}\'.format(\n                type(value), type(d[subkey]))\n        d[subkey] = value\n'"
faster_rcnn/fast_rcnn/config2.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Fast R-CNN config system.\n\nThis file specifies default config options for Fast R-CNN. You should not\nchange values in this file. Instead, you should write a config file (in yaml)\nand use cfg_from_file(yaml_file) to load it and override the default options.\n\nMost tools in $ROOT/tools take a --cfg option to specify an override file.\n    - See tools/{train,test}_net.py for example code that uses cfg_from_file()\n    - See experiments/cfgs/*.yml for example YAML config override files\n""""""\n\nimport os\nimport os.path as osp\nimport numpy as np\nimport math\n# `pip install easydict` if you don\'t have it\nfrom easydict import EasyDict as edict\n\n__C = edict()\n# Consumers can get config by:\n#   from fast_rcnn_config import cfg\ncfg = __C\n\n# region proposal network (RPN) or not\n__C.IS_RPN = True\n\n# multiscale training and testing\n__C.IS_MULTISCALE = False\n__C.IS_EXTRAPOLATING = True\n\n#\n__C.REGION_PROPOSAL = \'RPN\'\n\n__C.NET_NAME = \'CaffeNet\'\n__C.SUBCLS_NAME = \'voxel_exemplars\'\n\n#\n# Training options\n#\n\n__C.TRAIN = edict()\n\n# learning rate\n__C.TRAIN.LEARNING_RATE = 0.001\n__C.TRAIN.MOMENTUM = 0.9\n__C.TRAIN.GAMMA = 0.1\n__C.TRAIN.STEPSIZE = 30000\n\n# Scales to compute real features\n__C.TRAIN.SCALES_BASE = (0.25, 0.5, 1.0, 2.0, 3.0)\n\n# The number of scales per octave in the image pyramid\n# An octave is the set of scales up to half of the initial scale\n__C.TRAIN.NUM_PER_OCTAVE = 4\n\n# parameters for ROI generating\n__C.TRAIN.SPATIAL_SCALE = 0.0625\n__C.TRAIN.KERNEL_SIZE = 5\n\n# Aspect ratio to use during training\n__C.TRAIN.ASPECTS = (1, 0.75, 0.5, 0.25)\n\n# Images to use per minibatch\n__C.TRAIN.IMS_PER_BATCH = 2\n\n# Minibatch size (number of regions of interest [ROIs])\n__C.TRAIN.BATCH_SIZE = 128\n\n# Fraction of minibatch that is labeled foreground (i.e. class > 0)\n__C.TRAIN.FG_FRACTION = 0.25\n\n# Overlap threshold for a ROI to be considered foreground (if >= FG_THRESH)\n__C.TRAIN.FG_THRESH = (0.5,)\n\n# Overlap threshold for a ROI to be considered background (class = 0 if\n# overlap in [LO, HI))\n__C.TRAIN.BG_THRESH_HI = (0.5,)\n__C.TRAIN.BG_THRESH_LO = (0.1,)\n\n# Use horizontally-flipped images during training?\n__C.TRAIN.USE_FLIPPED = True\n\n# Train bounding-box regressors\n__C.TRAIN.BBOX_REG = True\n\n# Overlap required between a ROI and ground-truth box in order for that ROI to\n# be used as a bounding-box regression training example\n__C.TRAIN.BBOX_THRESH = (0.5,)\n\n# Iterations between snapshots\n__C.TRAIN.SNAPSHOT_ITERS = 10000\n\n# solver.prototxt specifies the snapshot path prefix, this adds an optional\n# infix to yield the path: <prefix>[_<infix>]_iters_XYZ.caffemodel\n__C.TRAIN.SNAPSHOT_PREFIX = \'caffenet_fast_rcnn\'\n__C.TRAIN.SNAPSHOT_INFIX = \'\'\n\n# Use a prefetch thread in roi_data_layer.layer\n# So far I haven\'t found this useful; likely more engineering work is required\n__C.TRAIN.USE_PREFETCH = False\n\n# Train using subclasses\n__C.TRAIN.SUBCLS = True\n\n# Train using viewpoint\n__C.TRAIN.VIEWPOINT = False\n\n# Threshold of ROIs in training RCNN\n__C.TRAIN.ROI_THRESHOLD = 0.1\n\n__C.TRAIN.DISPLAY = 20\n\n\n# IOU >= thresh: positive example\n__C.TRAIN.RPN_POSITIVE_OVERLAP = 0.7\n# IOU < thresh: negative example\n__C.TRAIN.RPN_NEGATIVE_OVERLAP = 0.3\n# If an anchor statisfied by positive and negative conditions set to negative\n__C.TRAIN.RPN_CLOBBER_POSITIVES = False\n# Max number of foreground examples\n__C.TRAIN.RPN_FG_FRACTION = 0.5\n# Total number of examples\n__C.TRAIN.RPN_BATCHSIZE = 256\n# NMS threshold used on RPN proposals\n__C.TRAIN.RPN_NMS_THRESH = 0.7\n# Number of top scoring boxes to keep before apply NMS to RPN proposals\n__C.TRAIN.RPN_PRE_NMS_TOP_N = 12000\n# Number of top scoring boxes to keep after applying NMS to RPN proposals\n__C.TRAIN.RPN_POST_NMS_TOP_N = 2000\n# Proposal height and width both need to be greater than RPN_MIN_SIZE (at orig image scale)\n__C.TRAIN.RPN_MIN_SIZE = 16\n# Deprecated (outside weights)\n__C.TRAIN.RPN_BBOX_INSIDE_WEIGHTS = (1.0, 1.0, 1.0, 1.0)\n# Give the positive RPN examples weight of p * 1 / {num positives}\n# and give negatives a weight of (1 - p)\n# Set to -1.0 to use uniform example weighting\n__C.TRAIN.RPN_POSITIVE_WEIGHT = -1.0\n\n__C.TRAIN.RPN_BASE_SIZE = 16\n__C.TRAIN.RPN_ASPECTS = [0.25, 0.5, 0.75, 1, 1.5, 2, 3]  # 7 aspects\n__C.TRAIN.RPN_SCALES = [2, 2.82842712, 4, 5.65685425, 8, 11.3137085, 16, 22.627417, 32, 45.254834] # 2**np.arange(1, 6, 0.5), 10 scales\n\n#\n# Testing options\n#\n\n__C.TEST = edict()\n\n# Scales to compute real features\n#__C.TEST.SCALES_BASE = (0.25, 0.5, 1.0, 2.0, 3.0)\n__C.TEST.SCALES_BASE = (1.0,)\n\n# The number of scales per octave in the image pyramid\n# An octave is the set of scales up to half of the initial scale\n__C.TEST.NUM_PER_OCTAVE = 4\n\n# Aspect ratio to use during testing\n__C.TEST.ASPECTS = (1, 0.75, 0.5, 0.25)\n\n# parameters for ROI generating\n__C.TEST.SPATIAL_SCALE = 0.0625\n__C.TEST.KERNEL_SIZE = 5\n\n# Overlap threshold used for non-maximum suppression (suppress boxes with\n# IoU >= this threshold)\n__C.TEST.NMS = 0.5\n\n# Experimental: treat the (K+1) units in the cls_score layer as linear\n# predictors (trained, eg, with one-vs-rest SVMs).\n__C.TEST.SVM = False\n\n# Test using bounding-box regressors\n__C.TEST.BBOX_REG = True\n\n# Test using subclass\n__C.TEST.SUBCLS = True\n\n# Train using viewpoint\n__C.TEST.VIEWPOINT = False\n\n# Threshold of ROIs in testing\n__C.TEST.ROI_THRESHOLD = 0.1\n__C.TEST.ROI_NUM = 2000\n__C.TEST.DET_THRESHOLD = 0.0001\n\n## NMS threshold used on RPN proposals\n__C.TEST.RPN_NMS_THRESH = 0.7\n## Number of top scoring boxes to keep before apply NMS to RPN proposals\n__C.TEST.RPN_PRE_NMS_TOP_N = 6000\n## Number of top scoring boxes to keep after applying NMS to RPN proposals\n__C.TEST.RPN_POST_NMS_TOP_N = 300\n# Proposal height and width both need to be greater than RPN_MIN_SIZE (at orig image scale)\n__C.TEST.RPN_MIN_SIZE = 16\n\n#\n# MISC\n#\n\n# The mapping from image coordinates to feature map coordinates might cause\n# some boxes that are distinct in image space to become identical in feature\n# coordinates. If DEDUP_BOXES > 0, then DEDUP_BOXES is used as the scale factor\n# for identifying duplicate boxes.\n# 1/16 is correct for {Alex,Caffe}Net, VGG_CNN_M_1024, and VGG16\n__C.DEDUP_BOXES = 1./16.\n\n# Pixel mean values (BGR order) as a (1, 1, 3) array\n# These are the values originally used for training VGG16\n__C.PIXEL_MEANS = np.array([[[102.9801, 115.9465, 122.7717]]])\n\n# For reproducibility\n__C.RNG_SEED = 3\n\n# A small number that\'s used many times\n__C.EPS = 1e-14\n\n# Root directory of project\n__C.ROOT_DIR = osp.abspath(osp.join(osp.dirname(__file__), \'..\', \'..\'))\n\n# Place outputs under an experiments directory\n__C.EXP_DIR = \'default\'\n\n# Use GPU implementation of non-maximum suppression\n__C.USE_GPU_NMS = True\n\n# Default GPU device id\n__C.GPU_ID = 0\n\ndef get_output_dir(imdb, net):\n    """"""Return the directory where experimental artifacts are placed.\n\n    A canonical path is built using the name from an imdb and a network\n    (if not None).\n    """"""\n    path = osp.abspath(osp.join(__C.ROOT_DIR, \'output\', __C.EXP_DIR, imdb.name))\n    if net is None:\n        return path\n    else:\n        return osp.join(path, net)\n\ndef _add_more_info(is_train):\n    # compute all the scales\n    if is_train:\n        scales_base = __C.TRAIN.SCALES_BASE\n        num_per_octave = __C.TRAIN.NUM_PER_OCTAVE\n    else:\n        scales_base = __C.TEST.SCALES_BASE\n        num_per_octave = __C.TEST.NUM_PER_OCTAVE\n\n    num_scale_base = len(scales_base)\n    num = (num_scale_base - 1) * num_per_octave + 1\n    scales = []\n    for i in xrange(num):\n        index_scale_base = i / num_per_octave\n        sbase = scales_base[index_scale_base]\n        j = i % num_per_octave\n        if j == 0:\n            scales.append(sbase)\n        else:\n            sbase_next = scales_base[index_scale_base+1]\n            step = (sbase_next - sbase) / num_per_octave\n            scales.append(sbase + j * step)\n\n    if is_train:\n        __C.TRAIN.SCALES = scales\n    else:\n        __C.TEST.SCALES = scales\n    print scales\n\n\n    # map the scales to scales for RoI pooling of classification\n    if is_train:\n        kernel_size = __C.TRAIN.KERNEL_SIZE / __C.TRAIN.SPATIAL_SCALE\n    else:\n        kernel_size = __C.TEST.KERNEL_SIZE / __C.TEST.SPATIAL_SCALE\n\n    area = kernel_size * kernel_size\n    scales = np.array(scales)\n    areas = np.repeat(area, num) / (scales ** 2)\n    scaled_areas = areas[:, np.newaxis] * (scales[np.newaxis, :] ** 2)\n    diff_areas = np.abs(scaled_areas - 224 * 224)\n    levels = diff_areas.argmin(axis=1)\n\n    if is_train:\n        __C.TRAIN.SCALE_MAPPING = levels\n    else:\n        __C.TEST.SCALE_MAPPING = levels\n\n    # compute width and height of grid box\n    if is_train:\n        area = __C.TRAIN.KERNEL_SIZE * __C.TRAIN.KERNEL_SIZE\n        aspect = __C.TRAIN.ASPECTS  # height / width\n    else:\n        area = __C.TEST.KERNEL_SIZE * __C.TEST.KERNEL_SIZE\n        aspect = __C.TEST.ASPECTS  # height / width\n\n    num_aspect = len(aspect)\n    widths = np.zeros((num_aspect), dtype=np.float32)\n    heights = np.zeros((num_aspect), dtype=np.float32)\n    for i in xrange(num_aspect):\n        widths[i] = math.sqrt(area / aspect[i])\n        heights[i] = widths[i] * aspect[i]\n\n    if is_train:\n        __C.TRAIN.ASPECT_WIDTHS = widths\n        __C.TRAIN.ASPECT_HEIGHTS = heights\n        __C.TRAIN.RPN_SCALES = np.array(__C.TRAIN.RPN_SCALES)\n    else:\n        __C.TEST.ASPECT_WIDTHS = widths\n        __C.TEST.ASPECT_HEIGHTS = heights\n\ndef _merge_a_into_b(a, b):\n    """"""Merge config dictionary a into config dictionary b, clobbering the\n    options in b whenever they are also specified in a.\n    """"""\n    if type(a) is not edict:\n        return\n\n    for k, v in a.iteritems():\n        # a must specify keys that are in b\n        if not b.has_key(k):\n            raise KeyError(\'{} is not a valid config key\'.format(k))\n\n        # the types must match, too\n        if type(b[k]) is not type(v):\n            raise ValueError((\'Type mismatch ({} vs. {}) \'\n                              \'for config key: {}\').format(type(b[k]),\n                                                           type(v), k))\n\n        # recursively merge dicts\n        if type(v) is edict:\n            try:\n                _merge_a_into_b(a[k], b[k])\n            except:\n                print(\'Error under config key: {}\'.format(k))\n                raise\n        else:\n            b[k] = v\n\ndef cfg_from_file(filename):\n    """"""Load a config file and merge it into the default options.""""""\n    import yaml\n    with open(filename, \'r\') as f:\n        yaml_cfg = edict(yaml.load(f))\n\n    _merge_a_into_b(yaml_cfg, __C)\n    _add_more_info(1)\n    _add_more_info(0)\n\ndef cfg_from_list(cfg_list):\n    """"""Set config keys via list (e.g., from command line).""""""\n    from ast import literal_eval\n    assert len(cfg_list) % 2 == 0\n    for k, v in zip(cfg_list[0::2], cfg_list[1::2]):\n        key_list = k.split(\'.\')\n        d = __C\n        for subkey in key_list[:-1]:\n            assert d.has_key(subkey)\n            d = d[subkey]\n        subkey = key_list[-1]\n        assert d.has_key(subkey)\n        try:\n            value = literal_eval(v)\n        except:\n            # handle the case when v is a string literal\n            value = v\n        assert type(value) == type(d[subkey]), \\\n            \'type {} does not match original type {}\'.format(\n            type(value), type(d[subkey]))\n        d[subkey] = value'"
faster_rcnn/fast_rcnn/nms_wrapper.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nfrom faster_rcnn.nms.cpu_nms import cpu_nms\nfrom faster_rcnn.nms.gpu_nms import gpu_nms\n# from ..nms import cpu_nms\n# from ..nms import gpu_nms\nfrom .config import cfg\n\n\ndef nms(dets, thresh, force_cpu=False):\n    """"""Dispatch to either CPU or GPU NMS implementations.""""""\n\n    if dets.shape[0] == 0:\n        return []\n    if cfg.USE_GPU_NMS and not force_cpu:\n        return gpu_nms(dets, thresh, device_id=cfg.GPU_ID)\n    else:\n        return cpu_nms(dets, thresh)\n'"
faster_rcnn/nms/__init__.py,0,b''
faster_rcnn/nms/py_cpu_nms.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport numpy as np\n\ndef py_cpu_nms(dets, thresh):\n    """"""Pure Python NMS baseline.""""""\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    x2 = dets[:, 2]\n    y2 = dets[:, 3]\n    scores = dets[:, 4]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr <= thresh)[0]\n        order = order[inds + 1]\n\n    return keep\n'"
faster_rcnn/pycocotools/__init__.py,0,"b""__author__ = 'tylin'\n"""
faster_rcnn/pycocotools/coco.py,0,"b'__author__ = \'tylin\'\n__version__ = \'1.0.1\'\n# Interface for accessing the Microsoft COCO dataset.\n\n# Microsoft COCO is a large image dataset designed for object detection,\n# segmentation, and caption generation. pycocotools is a Python API that\n# assists in loading, parsing and visualizing the annotations in COCO.\n# Please visit http://mscoco.org/ for more information on COCO, including\n# for the data, paper, and tutorials. The exact format of the annotations\n# is also described on the COCO website. For example usage of the pycocotools\n# please see pycocotools_demo.ipynb. In addition to this API, please download both\n# the COCO images and annotations in order to run the demo.\n\n# An alternative to using the API is to load the annotations directly\n# into Python dictionary\n# Using the API provides additional utility functions. Note that this API\n# supports both *instance* and *caption* annotations. In the case of\n# captions not all functions are defined (e.g. categories are undefined).\n\n# The following API functions are defined:\n#  COCO       - COCO api class that loads COCO annotation file and prepare data structures.\n#  decodeMask - Decode binary mask M encoded via run-length encoding.\n#  encodeMask - Encode binary mask M using run-length encoding.\n#  getAnnIds  - Get ann ids that satisfy given filter conditions.\n#  getCatIds  - Get cat ids that satisfy given filter conditions.\n#  getImgIds  - Get img ids that satisfy given filter conditions.\n#  loadAnns   - Load anns with the specified ids.\n#  loadCats   - Load cats with the specified ids.\n#  loadImgs   - Load imgs with the specified ids.\n#  segToMask  - Convert polygon segmentation to binary mask.\n#  showAnns   - Display the specified annotations.\n#  loadRes    - Load algorithm results and create API for accessing them.\n#  download   - Download COCO images from mscoco.org server.\n# Throughout the API ""ann""=annotation, ""cat""=category, and ""img""=image.\n# Help on each functions can be accessed by: ""help COCO>function"".\n\n# See also COCO>decodeMask,\n# COCO>encodeMask, COCO>getAnnIds, COCO>getCatIds,\n# COCO>getImgIds, COCO>loadAnns, COCO>loadCats,\n# COCO>loadImgs, COCO>segToMask, COCO>showAnns\n\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2014.\n# Licensed under the Simplified BSD License [see bsd.txt]\n\nimport json\nimport datetime\nimport time\n# import matplotlib.pyplot as plt\n# from matplotlib.collections import PatchCollection\n# from matplotlib.patches import Polygon\nimport numpy as np\n# from skimage.draw import polygon\nimport urllib\nimport copy\nimport itertools\nimport mask\nimport os\n\nclass COCO:\n    def __init__(self, annotation_file=None):\n        """"""\n        Constructor of Microsoft COCO helper class for reading and visualizing annotations.\n        :param annotation_file (str): location of annotation file\n        :param image_folder (str): location to the folder that hosts images.\n        :return:\n        """"""\n        # load dataset\n        self.dataset = {}\n        self.anns = []\n        self.imgToAnns = {}\n        self.catToImgs = {}\n        self.imgs = {}\n        self.cats = {}\n        if not annotation_file == None:\n            print \'loading annotations into memory...\'\n            tic = time.time()\n            dataset = json.load(open(annotation_file, \'r\'))\n            print \'Done (t=%0.2fs)\'%(time.time()- tic)\n            self.dataset = dataset\n            self.createIndex()\n\n    def createIndex(self):\n        # create index\n        print \'creating index...\'\n        anns = {}\n        imgToAnns = {}\n        catToImgs = {}\n        cats = {}\n        imgs = {}\n        if \'annotations\' in self.dataset:\n            imgToAnns = {ann[\'image_id\']: [] for ann in self.dataset[\'annotations\']}\n            anns =      {ann[\'id\']:       [] for ann in self.dataset[\'annotations\']}\n            for ann in self.dataset[\'annotations\']:\n                imgToAnns[ann[\'image_id\']] += [ann]\n                anns[ann[\'id\']] = ann\n\n        if \'images\' in self.dataset:\n            imgs      = {im[\'id\']: {} for im in self.dataset[\'images\']}\n            for img in self.dataset[\'images\']:\n                imgs[img[\'id\']] = img\n\n        if \'categories\' in self.dataset:\n            cats = {cat[\'id\']: [] for cat in self.dataset[\'categories\']}\n            for cat in self.dataset[\'categories\']:\n                cats[cat[\'id\']] = cat\n            catToImgs = {cat[\'id\']: [] for cat in self.dataset[\'categories\']}\n            if \'annotations\' in self.dataset:\n                for ann in self.dataset[\'annotations\']:\n                    catToImgs[ann[\'category_id\']] += [ann[\'image_id\']]\n\n        print \'index created!\'\n\n        # create class members\n        self.anns = anns\n        self.imgToAnns = imgToAnns\n        self.catToImgs = catToImgs\n        self.imgs = imgs\n        self.cats = cats\n\n    def info(self):\n        """"""\n        Print information about the annotation file.\n        :return:\n        """"""\n        for key, value in self.dataset[\'info\'].items():\n            print \'%s: %s\'%(key, value)\n\n    def getAnnIds(self, imgIds=[], catIds=[], areaRng=[], iscrowd=None):\n        """"""\n        Get ann ids that satisfy given filter conditions. default skips that filter\n        :param imgIds  (int array)     : get anns for given imgs\n               catIds  (int array)     : get anns for given cats\n               areaRng (float array)   : get anns for given area range (e.g. [0 inf])\n               iscrowd (boolean)       : get anns for given crowd label (False or True)\n        :return: ids (int array)       : integer array of ann ids\n        """"""\n        imgIds = imgIds if type(imgIds) == list else [imgIds]\n        catIds = catIds if type(catIds) == list else [catIds]\n\n        if len(imgIds) == len(catIds) == len(areaRng) == 0:\n            anns = self.dataset[\'annotations\']\n        else:\n            if not len(imgIds) == 0:\n                # this can be changed by defaultdict\n                lists = [self.imgToAnns[imgId] for imgId in imgIds if imgId in self.imgToAnns]\n                anns = list(itertools.chain.from_iterable(lists))\n            else:\n                anns = self.dataset[\'annotations\']\n            anns = anns if len(catIds)  == 0 else [ann for ann in anns if ann[\'category_id\'] in catIds]\n            anns = anns if len(areaRng) == 0 else [ann for ann in anns if ann[\'area\'] > areaRng[0] and ann[\'area\'] < areaRng[1]]\n        if not iscrowd == None:\n            ids = [ann[\'id\'] for ann in anns if ann[\'iscrowd\'] == iscrowd]\n        else:\n            ids = [ann[\'id\'] for ann in anns]\n        return ids\n\n    def getCatIds(self, catNms=[], supNms=[], catIds=[]):\n        """"""\n        filtering parameters. default skips that filter.\n        :param catNms (str array)  : get cats for given cat names\n        :param supNms (str array)  : get cats for given supercategory names\n        :param catIds (int array)  : get cats for given cat ids\n        :return: ids (int array)   : integer array of cat ids\n        """"""\n        catNms = catNms if type(catNms) == list else [catNms]\n        supNms = supNms if type(supNms) == list else [supNms]\n        catIds = catIds if type(catIds) == list else [catIds]\n\n        if len(catNms) == len(supNms) == len(catIds) == 0:\n            cats = self.dataset[\'categories\']\n        else:\n            cats = self.dataset[\'categories\']\n            cats = cats if len(catNms) == 0 else [cat for cat in cats if cat[\'name\']          in catNms]\n            cats = cats if len(supNms) == 0 else [cat for cat in cats if cat[\'supercategory\'] in supNms]\n            cats = cats if len(catIds) == 0 else [cat for cat in cats if cat[\'id\']            in catIds]\n        ids = [cat[\'id\'] for cat in cats]\n        return ids\n\n    def getImgIds(self, imgIds=[], catIds=[]):\n        \'\'\'\n        Get img ids that satisfy given filter conditions.\n        :param imgIds (int array) : get imgs for given ids\n        :param catIds (int array) : get imgs with all given cats\n        :return: ids (int array)  : integer array of img ids\n        \'\'\'\n        imgIds = imgIds if type(imgIds) == list else [imgIds]\n        catIds = catIds if type(catIds) == list else [catIds]\n\n        if len(imgIds) == len(catIds) == 0:\n            ids = self.imgs.keys()\n        else:\n            ids = set(imgIds)\n            for i, catId in enumerate(catIds):\n                if i == 0 and len(ids) == 0:\n                    ids = set(self.catToImgs[catId])\n                else:\n                    ids &= set(self.catToImgs[catId])\n        return list(ids)\n\n    def loadAnns(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying anns\n        :return: anns (object array) : loaded ann objects\n        """"""\n        if type(ids) == list:\n            return [self.anns[id] for id in ids]\n        elif type(ids) == int:\n            return [self.anns[ids]]\n\n    def loadCats(self, ids=[]):\n        """"""\n        Load cats with the specified ids.\n        :param ids (int array)       : integer ids specifying cats\n        :return: cats (object array) : loaded cat objects\n        """"""\n        if type(ids) == list:\n            return [self.cats[id] for id in ids]\n        elif type(ids) == int:\n            return [self.cats[ids]]\n\n    def loadImgs(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying img\n        :return: imgs (object array) : loaded img objects\n        """"""\n        if type(ids) == list:\n            return [self.imgs[id] for id in ids]\n        elif type(ids) == int:\n            return [self.imgs[ids]]\n\n    def showAnns(self, anns):\n        """"""\n        Display the specified annotations.\n        :param anns (array of object): annotations to display\n        :return: None\n        """"""\n        if len(anns) == 0:\n            return 0\n        if \'segmentation\' in anns[0]:\n            datasetType = \'instances\'\n        elif \'caption\' in anns[0]:\n            datasetType = \'captions\'\n        if datasetType == \'instances\':\n            ax = plt.gca()\n            polygons = []\n            color = []\n            for ann in anns:\n                c = np.random.random((1, 3)).tolist()[0]\n                if type(ann[\'segmentation\']) == list:\n                    # polygon\n                    for seg in ann[\'segmentation\']:\n                        poly = np.array(seg).reshape((len(seg)/2, 2))\n                        polygons.append(Polygon(poly, True,alpha=0.4))\n                        color.append(c)\n                else:\n                    # mask\n                    t = self.imgs[ann[\'image_id\']]\n                    if type(ann[\'segmentation\'][\'counts\']) == list:\n                        rle = mask.frPyObjects([ann[\'segmentation\']], t[\'height\'], t[\'width\'])\n                    else:\n                        rle = [ann[\'segmentation\']]\n                    m = mask.decode(rle)\n                    img = np.ones( (m.shape[0], m.shape[1], 3) )\n                    if ann[\'iscrowd\'] == 1:\n                        color_mask = np.array([2.0,166.0,101.0])/255\n                    if ann[\'iscrowd\'] == 0:\n                        color_mask = np.random.random((1, 3)).tolist()[0]\n                    for i in range(3):\n                        img[:,:,i] = color_mask[i]\n                    ax.imshow(np.dstack( (img, m*0.5) ))\n            p = PatchCollection(polygons, facecolors=color, edgecolors=(0,0,0,1), linewidths=3, alpha=0.4)\n            ax.add_collection(p)\n        elif datasetType == \'captions\':\n            for ann in anns:\n                print ann[\'caption\']\n\n    def loadRes(self, resFile):\n        """"""\n        Load result file and return a result api object.\n        :param   resFile (str)     : file name of result file\n        :return: res (obj)         : result api object\n        """"""\n        res = COCO()\n        res.dataset[\'images\'] = [img for img in self.dataset[\'images\']]\n        # res.dataset[\'info\'] = copy.deepcopy(self.dataset[\'info\'])\n        # res.dataset[\'licenses\'] = copy.deepcopy(self.dataset[\'licenses\'])\n\n        print \'Loading and preparing results...     \'\n        tic = time.time()\n        anns    = json.load(open(resFile))\n        assert type(anns) == list, \'results in not an array of objects\'\n        annsImgIds = [ann[\'image_id\'] for ann in anns]\n        assert set(annsImgIds) == (set(annsImgIds) & set(self.getImgIds())), \\\n               \'Results do not correspond to current coco set\'\n        if \'caption\' in anns[0]:\n            imgIds = set([img[\'id\'] for img in res.dataset[\'images\']]) & set([ann[\'image_id\'] for ann in anns])\n            res.dataset[\'images\'] = [img for img in res.dataset[\'images\'] if img[\'id\'] in imgIds]\n            for id, ann in enumerate(anns):\n                ann[\'id\'] = id+1\n        elif \'bbox\' in anns[0] and not anns[0][\'bbox\'] == []:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                bb = ann[\'bbox\']\n                x1, x2, y1, y2 = [bb[0], bb[0]+bb[2], bb[1], bb[1]+bb[3]]\n                if not \'segmentation\' in ann:\n                    ann[\'segmentation\'] = [[x1, y1, x1, y2, x2, y2, x2, y1]]\n                ann[\'area\'] = bb[2]*bb[3]\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        elif \'segmentation\' in anns[0]:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                # now only support compressed RLE format as segmentation results\n                ann[\'area\'] = mask.area([ann[\'segmentation\']])[0]\n                if not \'bbox\' in ann:\n                    ann[\'bbox\'] = mask.toBbox([ann[\'segmentation\']])[0]\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        print \'DONE (t=%0.2fs)\'%(time.time()- tic)\n\n        res.dataset[\'annotations\'] = anns\n        res.createIndex()\n        return res\n\n    def download( self, tarDir = None, imgIds = [] ):\n        \'\'\'\n        Download COCO images from mscoco.org server.\n        :param tarDir (str): COCO results directory name\n               imgIds (list): images to be downloaded\n        :return:\n        \'\'\'\n        if tarDir is None:\n            print \'Please specify target directory\'\n            return -1\n        if len(imgIds) == 0:\n            imgs = self.imgs.values()\n        else:\n            imgs = self.loadImgs(imgIds)\n        N = len(imgs)\n        if not os.path.exists(tarDir):\n            os.makedirs(tarDir)\n        for i, img in enumerate(imgs):\n            tic = time.time()\n            fname = os.path.join(tarDir, img[\'file_name\'])\n            if not os.path.exists(fname):\n                urllib.urlretrieve(img[\'coco_url\'], fname)\n            print \'downloaded %d/%d images (t=%.1fs)\'%(i, N, time.time()- tic)\n'"
faster_rcnn/pycocotools/cocoeval.py,0,"b'__author__ = \'tsungyi\'\n\nimport numpy as np\nimport datetime\nimport time\nfrom collections import defaultdict\nimport mask\nimport copy\n\nclass COCOeval:\n    # Interface for evaluating detection on the Microsoft COCO dataset.\n    #\n    # The usage for CocoEval is as follows:\n    #  cocoGt=..., cocoDt=...       # load dataset and results\n    #  E = CocoEval(cocoGt,cocoDt); # initialize CocoEval object\n    #  E.params.recThrs = ...;      # set parameters as desired\n    #  E.evaluate();                # run per image evaluation\n    #  E.accumulate();              # accumulate per image results\n    #  E.summarize();               # display summary metrics of results\n    # For example usage see evalDemo.m and http://mscoco.org/.\n    #\n    # The evaluation parameters are as follows (defaults in brackets):\n    #  imgIds     - [all] N img ids to use for evaluation\n    #  catIds     - [all] K cat ids to use for evaluation\n    #  iouThrs    - [.5:.05:.95] T=10 IoU thresholds for evaluation\n    #  recThrs    - [0:.01:1] R=101 recall thresholds for evaluation\n    #  areaRng    - [...] A=4 object area ranges for evaluation\n    #  maxDets    - [1 10 100] M=3 thresholds on max detections per image\n    #  useSegm    - [1] if true evaluate against ground-truth segments\n    #  useCats    - [1] if true use category labels for evaluation    # Note: if useSegm=0 the evaluation is run on bounding boxes.\n    # Note: if useCats=0 category labels are ignored as in proposal scoring.\n    # Note: multiple areaRngs [Ax2] and maxDets [Mx1] can be specified.\n    #\n    # evaluate(): evaluates detections on every image and every category and\n    # concats the results into the ""evalImgs"" with fields:\n    #  dtIds      - [1xD] id for each of the D detections (dt)\n    #  gtIds      - [1xG] id for each of the G ground truths (gt)\n    #  dtMatches  - [TxD] matching gt id at each IoU or 0\n    #  gtMatches  - [TxG] matching dt id at each IoU or 0\n    #  dtScores   - [1xD] confidence of each dt\n    #  gtIgnore   - [1xG] ignore flag for each gt\n    #  dtIgnore   - [TxD] ignore flag for each dt at each IoU\n    #\n    # accumulate(): accumulates the per-image, per-category evaluation\n    # results in ""evalImgs"" into the dictionary ""eval"" with fields:\n    #  params     - parameters used for evaluation\n    #  date       - date evaluation was performed\n    #  counts     - [T,R,K,A,M] parameter dimensions (see above)\n    #  precision  - [TxRxKxAxM] precision for every evaluation setting\n    #  recall     - [TxKxAxM] max recall for every evaluation setting\n    # Note: precision and recall==-1 for settings with no gt objects.\n    #\n    # See also coco, mask, pycocoDemo, pycocoEvalDemo\n    #\n    # Microsoft COCO Toolbox.      version 2.0\n    # Data, paper, and tutorials available at:  http://mscoco.org/\n    # Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n    # Licensed under the Simplified BSD License [see coco/license.txt]\n    def __init__(self, cocoGt=None, cocoDt=None):\n        \'\'\'\n        Initialize CocoEval using coco APIs for gt and dt\n        :param cocoGt: coco object with ground truth annotations\n        :param cocoDt: coco object with detection results\n        :return: None\n        \'\'\'\n        self.cocoGt   = cocoGt              # ground truth COCO API\n        self.cocoDt   = cocoDt              # detections COCO API\n        self.params   = {}                  # evaluation parameters\n        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results [KxAxI] elements\n        self.eval     = {}                  # accumulated evaluation results\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        self.params = Params()              # parameters\n        self._paramsEval = {}               # parameters for evaluation\n        self.stats = []                     # result summarization\n        self.ious = {}                      # ious between all gts and dts\n        if not cocoGt is None:\n            self.params.imgIds = sorted(cocoGt.getImgIds())\n            self.params.catIds = sorted(cocoGt.getCatIds())\n\n\n    def _prepare(self):\n        \'\'\'\n        Prepare ._gts and ._dts for evaluation based on params\n        :return: None\n        \'\'\'\n        #\n        def _toMask(objs, coco):\n            # modify segmentation by reference\n            for obj in objs:\n                t = coco.imgs[obj[\'image_id\']]\n                if type(obj[\'segmentation\']) == list:\n                    if type(obj[\'segmentation\'][0]) == dict:\n                        print \'debug\'\n                    obj[\'segmentation\'] = mask.frPyObjects(obj[\'segmentation\'],t[\'height\'],t[\'width\'])\n                    if len(obj[\'segmentation\']) == 1:\n                        obj[\'segmentation\'] = obj[\'segmentation\'][0]\n                    else:\n                        # an object can have multiple polygon regions\n                        # merge them into one RLE mask\n                        obj[\'segmentation\'] = mask.merge(obj[\'segmentation\'])\n                elif type(obj[\'segmentation\']) == dict and type(obj[\'segmentation\'][\'counts\']) == list:\n                    obj[\'segmentation\'] = mask.frPyObjects([obj[\'segmentation\']],t[\'height\'],t[\'width\'])[0]\n                elif type(obj[\'segmentation\']) == dict and \\\n                     type(obj[\'segmentation\'][\'counts\'] == unicode or type(obj[\'segmentation\'][\'counts\']) == str):\n                    pass\n                else:\n                    raise Exception(\'segmentation format not supported.\')\n        p = self.params\n        if p.useCats:\n            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n        else:\n            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds))\n            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds))\n\n        if p.useSegm:\n            _toMask(gts, self.cocoGt)\n            _toMask(dts, self.cocoDt)\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        for gt in gts:\n            self._gts[gt[\'image_id\'], gt[\'category_id\']].append(gt)\n        for dt in dts:\n            self._dts[dt[\'image_id\'], dt[\'category_id\']].append(dt)\n        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results\n        self.eval     = {}                  # accumulated evaluation results\n\n    def evaluate(self):\n        \'\'\'\n        Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n        :return: None\n        \'\'\'\n        tic = time.time()\n        print \'Running per image evaluation...      \'\n        p = self.params\n        p.imgIds = list(np.unique(p.imgIds))\n        if p.useCats:\n            p.catIds = list(np.unique(p.catIds))\n        p.maxDets = sorted(p.maxDets)\n        self.params=p\n\n        self._prepare()\n        # loop through images, area range, max detection number\n        catIds = p.catIds if p.useCats else [-1]\n\n        computeIoU = self.computeIoU\n        self.ious = {(imgId, catId): computeIoU(imgId, catId) \\\n                        for imgId in p.imgIds\n                        for catId in catIds}\n\n        evaluateImg = self.evaluateImg\n        maxDet = p.maxDets[-1]\n        self.evalImgs = [evaluateImg(imgId, catId, areaRng, maxDet)\n                 for catId in catIds\n                 for areaRng in p.areaRng\n                 for imgId in p.imgIds\n             ]\n        self._paramsEval = copy.deepcopy(self.params)\n        toc = time.time()\n        print \'DONE (t=%0.2fs).\'%(toc-tic)\n\n    def computeIoU(self, imgId, catId):\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId,catId]\n            dt = self._dts[imgId,catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n        if len(gt) == 0 and len(dt) ==0:\n            return []\n        dt = sorted(dt, key=lambda x: -x[\'score\'])\n        if len(dt) > p.maxDets[-1]:\n            dt=dt[0:p.maxDets[-1]]\n\n        if p.useSegm:\n            g = [g[\'segmentation\'] for g in gt]\n            d = [d[\'segmentation\'] for d in dt]\n        else:\n            g = [g[\'bbox\'] for g in gt]\n            d = [d[\'bbox\'] for d in dt]\n\n        # compute iou between each dt and gt region\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        ious = mask.iou(d,g,iscrowd)\n        return ious\n\n    def evaluateImg(self, imgId, catId, aRng, maxDet):\n        \'\'\'\n        perform evaluation for single category and image\n        :return: dict (single image results)\n        \'\'\'\n        #\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId,catId]\n            dt = self._dts[imgId,catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n        if len(gt) == 0 and len(dt) ==0:\n            return None\n\n        for g in gt:\n            if \'ignore\' not in g:\n                g[\'ignore\'] = 0\n            if g[\'iscrowd\'] == 1 or g[\'ignore\'] or (g[\'area\']<aRng[0] or g[\'area\']>aRng[1]):\n                g[\'_ignore\'] = 1\n            else:\n                g[\'_ignore\'] = 0\n\n        # sort dt highest score first, sort gt ignore last\n        # gt = sorted(gt, key=lambda x: x[\'_ignore\'])\n        gtind = [ind for (ind, g) in sorted(enumerate(gt), key=lambda (ind, g): g[\'_ignore\']) ]\n\n        gt = [gt[ind] for ind in gtind]\n        dt = sorted(dt, key=lambda x: -x[\'score\'])[0:maxDet]\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        # load computed ious\n        N_iou = len(self.ious[imgId, catId])\n        ious = self.ious[imgId, catId][0:maxDet, np.array(gtind)] if N_iou >0 else self.ious[imgId, catId]\n\n        T = len(p.iouThrs)\n        G = len(gt)\n        D = len(dt)\n        gtm  = np.zeros((T,G))\n        dtm  = np.zeros((T,D))\n        gtIg = np.array([g[\'_ignore\'] for g in gt])\n        dtIg = np.zeros((T,D))\n        if not len(ious)==0:\n            for tind, t in enumerate(p.iouThrs):\n                for dind, d in enumerate(dt):\n                    # information about best match so far (m=-1 -> unmatched)\n                    iou = min([t,1-1e-10])\n                    m   = -1\n                    for gind, g in enumerate(gt):\n                        # if this gt already matched, and not a crowd, continue\n                        if gtm[tind,gind]>0 and not iscrowd[gind]:\n                            continue\n                        # if dt matched to reg gt, and on ignore gt, stop\n                        if m>-1 and gtIg[m]==0 and gtIg[gind]==1:\n                            break\n                        # continue to next gt unless better match made\n                        if ious[dind,gind] < iou:\n                            continue\n                        # match successful and best so far, store appropriately\n                        iou=ious[dind,gind]\n                        m=gind\n                    # if match made store id of match for both dt and gt\n                    if m ==-1:\n                        continue\n                    dtIg[tind,dind] = gtIg[m]\n                    dtm[tind,dind]  = gt[m][\'id\']\n                    gtm[tind,m]     = d[\'id\']\n        # set unmatched detections outside of area range to ignore\n        a = np.array([d[\'area\']<aRng[0] or d[\'area\']>aRng[1] for d in dt]).reshape((1, len(dt)))\n        dtIg = np.logical_or(dtIg, np.logical_and(dtm==0, np.repeat(a,T,0)))\n        # store results for given image and category\n        return {\n                \'image_id\':     imgId,\n                \'category_id\':  catId,\n                \'aRng\':         aRng,\n                \'maxDet\':       maxDet,\n                \'dtIds\':        [d[\'id\'] for d in dt],\n                \'gtIds\':        [g[\'id\'] for g in gt],\n                \'dtMatches\':    dtm,\n                \'gtMatches\':    gtm,\n                \'dtScores\':     [d[\'score\'] for d in dt],\n                \'gtIgnore\':     gtIg,\n                \'dtIgnore\':     dtIg,\n            }\n\n    def accumulate(self, p = None):\n        \'\'\'\n        Accumulate per image evaluation results and store the result in self.eval\n        :param p: input params for evaluation\n        :return: None\n        \'\'\'\n        print \'Accumulating evaluation results...   \'\n        tic = time.time()\n        if not self.evalImgs:\n            print \'Please run evaluate() first\'\n        # allows input customized parameters\n        if p is None:\n            p = self.params\n        p.catIds = p.catIds if p.useCats == 1 else [-1]\n        T           = len(p.iouThrs)\n        R           = len(p.recThrs)\n        K           = len(p.catIds) if p.useCats else 1\n        A           = len(p.areaRng)\n        M           = len(p.maxDets)\n        precision   = -np.ones((T,R,K,A,M)) # -1 for the precision of absent categories\n        recall      = -np.ones((T,K,A,M))\n\n        # create dictionary for future indexing\n        _pe = self._paramsEval\n        catIds = _pe.catIds if _pe.useCats else [-1]\n        setK = set(catIds)\n        setA = set(map(tuple, _pe.areaRng))\n        setM = set(_pe.maxDets)\n        setI = set(_pe.imgIds)\n        # get inds to evaluate\n        k_list = [n for n, k in enumerate(p.catIds)  if k in setK]\n        m_list = [m for n, m in enumerate(p.maxDets) if m in setM]\n        a_list = [n for n, a in enumerate(map(lambda x: tuple(x), p.areaRng)) if a in setA]\n        i_list = [n for n, i in enumerate(p.imgIds)  if i in setI]\n        # K0 = len(_pe.catIds)\n        I0 = len(_pe.imgIds)\n        A0 = len(_pe.areaRng)\n        # retrieve E at each category, area range, and max number of detections\n        for k, k0 in enumerate(k_list):\n            Nk = k0*A0*I0\n            for a, a0 in enumerate(a_list):\n                Na = a0*I0\n                for m, maxDet in enumerate(m_list):\n                    E = [self.evalImgs[Nk+Na+i] for i in i_list]\n                    E = filter(None, E)\n                    if len(E) == 0:\n                        continue\n                    dtScores = np.concatenate([e[\'dtScores\'][0:maxDet] for e in E])\n\n                    # different sorting method generates slightly different results.\n                    # mergesort is used to be consistent as Matlab implementation.\n                    inds = np.argsort(-dtScores, kind=\'mergesort\')\n\n                    dtm  = np.concatenate([e[\'dtMatches\'][:,0:maxDet] for e in E], axis=1)[:,inds]\n                    dtIg = np.concatenate([e[\'dtIgnore\'][:,0:maxDet]  for e in E], axis=1)[:,inds]\n                    gtIg = np.concatenate([e[\'gtIgnore\']  for e in E])\n                    npig = len([ig for ig in gtIg if ig == 0])\n                    if npig == 0:\n                        continue\n                    tps = np.logical_and(               dtm,  np.logical_not(dtIg) )\n                    fps = np.logical_and(np.logical_not(dtm), np.logical_not(dtIg) )\n\n                    tp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)\n                    fp_sum = np.cumsum(fps, axis=1).astype(dtype=np.float)\n                    for t, (tp, fp) in enumerate(zip(tp_sum, fp_sum)):\n                        tp = np.array(tp)\n                        fp = np.array(fp)\n                        nd = len(tp)\n                        rc = tp / npig\n                        pr = tp / (fp+tp+np.spacing(1))\n                        q  = np.zeros((R,))\n\n                        if nd:\n                            recall[t,k,a,m] = rc[-1]\n                        else:\n                            recall[t,k,a,m] = 0\n\n                        # numpy is slow without cython optimization for accessing elements\n                        # use python array gets significant speed improvement\n                        pr = pr.tolist(); q = q.tolist()\n\n                        for i in range(nd-1, 0, -1):\n                            if pr[i] > pr[i-1]:\n                                pr[i-1] = pr[i]\n\n                        inds = np.searchsorted(rc, p.recThrs)\n                        try:\n                            for ri, pi in enumerate(inds):\n                                q[ri] = pr[pi]\n                        except:\n                            pass\n                        precision[t,:,k,a,m] = np.array(q)\n        self.eval = {\n            \'params\': p,\n            \'counts\': [T, R, K, A, M],\n            \'date\': datetime.datetime.now().strftime(""%Y-%m-%d %H:%M:%S""),\n            \'precision\': precision,\n            \'recall\':   recall,\n        }\n        toc = time.time()\n        print \'DONE (t=%0.2fs).\'%( toc-tic )\n\n    def summarize(self):\n        \'\'\'\n        Compute and display summary metrics for evaluation results.\n        Note this functin can *only* be applied on the default parameter setting\n        \'\'\'\n        def _summarize( ap=1, iouThr=None, areaRng=\'all\', maxDets=100 ):\n            p = self.params\n            iStr        = \' {:<18} {} @[ IoU={:<9} | area={:>6} | maxDets={:>3} ] = {}\'\n            titleStr    = \'Average Precision\' if ap == 1 else \'Average Recall\'\n            typeStr     = \'(AP)\' if ap==1 else \'(AR)\'\n            iouStr      = \'%0.2f:%0.2f\'%(p.iouThrs[0], p.iouThrs[-1]) if iouThr is None else \'%0.2f\'%(iouThr)\n            areaStr     = areaRng\n            maxDetsStr  = \'%d\'%(maxDets)\n\n            aind = [i for i, aRng in enumerate([\'all\', \'small\', \'medium\', \'large\']) if aRng == areaRng]\n            mind = [i for i, mDet in enumerate([1, 10, 100]) if mDet == maxDets]\n            if ap == 1:\n                # dimension of precision: [TxRxKxAxM]\n                s = self.eval[\'precision\']\n                # IoU\n                if iouThr is not None:\n                    t = np.where(iouThr == p.iouThrs)[0]\n                    s = s[t]\n                # areaRng\n                s = s[:,:,:,aind,mind]\n            else:\n                # dimension of recall: [TxKxAxM]\n                s = self.eval[\'recall\']\n                s = s[:,:,aind,mind]\n            if len(s[s>-1])==0:\n                mean_s = -1\n            else:\n                mean_s = np.mean(s[s>-1])\n            print iStr.format(titleStr, typeStr, iouStr, areaStr, maxDetsStr, \'%.3f\'%(float(mean_s)))\n            return mean_s\n\n        if not self.eval:\n            raise Exception(\'Please run accumulate() first\')\n        self.stats = np.zeros((12,))\n        self.stats[0] = _summarize(1)\n        self.stats[1] = _summarize(1,iouThr=.5)\n        self.stats[2] = _summarize(1,iouThr=.75)\n        self.stats[3] = _summarize(1,areaRng=\'small\')\n        self.stats[4] = _summarize(1,areaRng=\'medium\')\n        self.stats[5] = _summarize(1,areaRng=\'large\')\n        self.stats[6] = _summarize(0,maxDets=1)\n        self.stats[7] = _summarize(0,maxDets=10)\n        self.stats[8] = _summarize(0,maxDets=100)\n        self.stats[9]  = _summarize(0,areaRng=\'small\')\n        self.stats[10] = _summarize(0,areaRng=\'medium\')\n        self.stats[11] = _summarize(0,areaRng=\'large\')\n\n    def __str__(self):\n        self.summarize()\n\nclass Params:\n    \'\'\'\n    Params for coco evaluation api\n    \'\'\'\n    def __init__(self):\n        self.imgIds = []\n        self.catIds = []\n        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n        self.iouThrs = np.linspace(.5, 0.95, np.round((0.95-.5)/.05)+1, endpoint=True)\n        self.recThrs = np.linspace(.0, 1.00, np.round((1.00-.0)/.01)+1, endpoint=True)\n        self.maxDets = [1,10,100]\n        self.areaRng = [ [0**2,1e5**2], [0**2, 32**2], [32**2, 96**2], [96**2, 1e5**2] ]\n        self.useSegm = 0\n        self.useCats = 1'"
faster_rcnn/pycocotools/mask.py,0,"b'__author__ = \'tsungyi\'\n\nfrom . import _mask\n\n# Interface for manipulating masks stored in RLE format.\n#\n# RLE is a simple yet efficient format for storing binary masks. RLE\n# first divides a vector (or vectorized image) into a series of piecewise\n# constant regions and then for each piece simply stores the length of\n# that piece. For example, given M=[0 0 1 1 1 0 1] the RLE counts would\n# be [2 3 1 1], or for M=[1 1 1 1 1 1 0] the counts would be [0 6 1]\n# (note that the odd counts are always the numbers of zeros). Instead of\n# storing the counts directly, additional compression is achieved with a\n# variable bitrate representation based on a common scheme called LEB128.\n#\n# Compression is greatest given large piecewise constant regions.\n# Specifically, the size of the RLE is proportional to the number of\n# *boundaries* in M (or for an image the number of boundaries in the y\n# direction). Assuming fairly simple shapes, the RLE representation is\n# O(sqrt(n)) where n is number of pixels in the object. Hence space usage\n# is substantially lower, especially for large simple objects (large n).\n#\n# Many common operations on masks can be computed directly using the RLE\n# (without need for decoding). This includes computations such as area,\n# union, intersection, etc. All of these operations are linear in the\n# size of the RLE, in other words they are O(sqrt(n)) where n is the area\n# of the object. Computing these operations on the original mask is O(n).\n# Thus, using the RLE can result in substantial computational savings.\n#\n# The following API functions are defined:\n#  encode         - Encode binary masks using RLE.\n#  decode         - Decode binary masks encoded via RLE.\n#  merge          - Compute union or intersection of encoded masks.\n#  iou            - Compute intersection over union between masks.\n#  area           - Compute area of encoded masks.\n#  toBbox         - Get bounding boxes surrounding encoded masks.\n#  frPyObjects    - Convert polygon, bbox, and uncompressed RLE to encoded RLE mask.\n#\n# Usage:\n#  Rs     = encode( masks )\n#  masks  = decode( Rs )\n#  R      = merge( Rs, intersect=false )\n#  o      = iou( dt, gt, iscrowd )\n#  a      = area( Rs )\n#  bbs    = toBbox( Rs )\n#  Rs     = frPyObjects( [pyObjects], h, w )\n#\n# In the API the following formats are used:\n#  Rs      - [dict] Run-length encoding of binary masks\n#  R       - dict Run-length encoding of binary mask\n#  masks   - [hxwxn] Binary mask(s) (must have type np.ndarray(dtype=uint8) in column-major order)\n#  iscrowd - [nx1] list of np.ndarray. 1 indicates corresponding gt image has crowd region to ignore\n#  bbs     - [nx4] Bounding box(es) stored as [x y w h]\n#  poly    - Polygon stored as [[x1 y1 x2 y2...],[x1 y1 ...],...] (2D list)\n#  dt,gt   - May be either bounding boxes or encoded masks\n# Both poly and bbs are 0-indexed (bbox=[0 0 1 1] encloses first pixel).\n#\n# Finally, a note about the intersection over union (iou) computation.\n# The standard iou of a ground truth (gt) and detected (dt) object is\n#  iou(gt,dt) = area(intersect(gt,dt)) / area(union(gt,dt))\n# For ""crowd"" regions, we use a modified criteria. If a gt object is\n# marked as ""iscrowd"", we allow a dt to match any subregion of the gt.\n# Choosing gt\' in the crowd gt that best matches the dt can be done using\n# gt\'=intersect(dt,gt). Since by definition union(gt\',dt)=dt, computing\n#  iou(gt,dt,iscrowd) = iou(gt\',dt) = area(intersect(gt,dt)) / area(dt)\n# For crowd gt regions we use this modified criteria above for the iou.\n#\n# To compile run ""python setup.py build_ext --inplace""\n# Please do not contact us for help with compiling.\n#\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n# Licensed under the Simplified BSD License [see coco/license.txt]\n\nencode      = _mask.encode\ndecode      = _mask.decode\niou         = _mask.iou\nmerge       = _mask.merge\narea        = _mask.area\ntoBbox      = _mask.toBbox\nfrPyObjects = _mask.frPyObjects'"
faster_rcnn/roi_data_layer/__init__.py,0,b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\nimport roidb'
faster_rcnn/roi_data_layer/layer.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""The data layer used during training to train a Fast R-CNN network.\n\nRoIDataLayer implements a Caffe Python layer.\n""""""\n\nimport numpy as np\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\n# <<<< obsolete\nfrom ..roi_data_layer.minibatch import get_minibatch\n\nclass RoIDataLayer(object):\n    """"""Fast R-CNN data layer used for training.""""""\n\n    def __init__(self, roidb, num_classes):\n        """"""Set the roidb to be used by this layer during training.""""""\n        self._roidb = roidb\n        self._num_classes = num_classes\n        self._shuffle_roidb_inds()\n\n    def _shuffle_roidb_inds(self):\n        """"""Randomly permute the training roidb.""""""\n        self._perm = np.random.permutation(np.arange(len(self._roidb)))\n        # self._perm = np.arange(len(self._roidb))\n        self._cur = 0\n\n    def _get_next_minibatch_inds(self):\n        """"""Return the roidb indices for the next minibatch.""""""\n        \n        if cfg.TRAIN.HAS_RPN:\n            if self._cur + cfg.TRAIN.IMS_PER_BATCH >= len(self._roidb):\n                self._shuffle_roidb_inds()\n\n            db_inds = self._perm[self._cur:self._cur + cfg.TRAIN.IMS_PER_BATCH]\n            self._cur += cfg.TRAIN.IMS_PER_BATCH\n        else:\n            # sample images\n            db_inds = np.zeros((cfg.TRAIN.IMS_PER_BATCH), dtype=np.int32)\n            i = 0\n            while (i < cfg.TRAIN.IMS_PER_BATCH):\n                ind = self._perm[self._cur]\n                num_objs = self._roidb[ind][\'boxes\'].shape[0]\n                if num_objs != 0:\n                    db_inds[i] = ind\n                    i += 1\n\n                self._cur += 1\n                if self._cur >= len(self._roidb):\n                    self._shuffle_roidb_inds()\n\n        return db_inds\n\n    def _get_next_minibatch(self):\n        """"""Return the blobs to be used for the next minibatch.\n\n        If cfg.TRAIN.USE_PREFETCH is True, then blobs will be computed in a\n        separate process and made available through self._blob_queue.\n        """"""\n        db_inds = self._get_next_minibatch_inds()\n        minibatch_db = [self._roidb[i] for i in db_inds]\n        return get_minibatch(minibatch_db, self._num_classes)\n            \n    def forward(self):\n        """"""Get blobs and copy them into this layer\'s top blob vector.""""""\n        blobs = self._get_next_minibatch()\n        return blobs\n'"
faster_rcnn/roi_data_layer/minibatch.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Compute minibatch blobs for training a Fast R-CNN network.""""""\n\nimport numpy as np\nimport numpy.random as npr\nimport cv2\nimport os\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\n# <<<< obsolete\nfrom ..utils.blob import prep_im_for_blob, im_list_to_blob\n\ndef get_minibatch(roidb, num_classes):\n    """"""Given a roidb, construct a minibatch sampled from it.""""""\n    num_images = len(roidb)\n    # Sample random scales to use for each image in this batch\n    random_scale_inds = npr.randint(0, high=len(cfg.TRAIN.SCALES),\n                                    size=num_images)\n    assert(cfg.TRAIN.BATCH_SIZE % num_images == 0), \\\n        \'num_images ({}) must divide BATCH_SIZE ({})\'. \\\n        format(num_images, cfg.TRAIN.BATCH_SIZE)\n    rois_per_image = cfg.TRAIN.BATCH_SIZE / num_images\n    fg_rois_per_image = np.round(cfg.TRAIN.FG_FRACTION * rois_per_image)\n\n    # Get the input image blob, formatted for caffe\n    im_blob, im_scales = _get_image_blob(roidb, random_scale_inds)\n\n    blobs = {\'data\': im_blob}\n\n    if cfg.TRAIN.HAS_RPN:\n        assert len(im_scales) == 1, ""Single batch only""\n        assert len(roidb) == 1, ""Single batch only""\n        # gt boxes: (x1, y1, x2, y2, cls)\n        gt_inds = np.where(roidb[0][\'gt_classes\'] != 0)[0]\n        gt_boxes = np.empty((len(gt_inds), 5), dtype=np.float32)\n        gt_boxes[:, 0:4] = roidb[0][\'boxes\'][gt_inds, :] * im_scales[0]\n        gt_boxes[:, 4] = roidb[0][\'gt_classes\'][gt_inds]\n        blobs[\'gt_boxes\'] = gt_boxes\n        blobs[\'gt_ishard\'] = roidb[0][\'gt_ishard\'][gt_inds]  \\\n            if \'gt_ishard\' in roidb[0] else np.zeros(gt_inds.size, dtype=int)\n        # blobs[\'gt_ishard\'] = roidb[0][\'gt_ishard\'][gt_inds]\n        blobs[\'dontcare_areas\'] = roidb[0][\'dontcare_areas\'] * im_scales[0] \\\n            if \'dontcare_areas\' in roidb[0] else np.zeros([0, 4], dtype=float)\n        blobs[\'im_info\'] = np.array(\n            [[im_blob.shape[1], im_blob.shape[2], im_scales[0]]],\n            dtype=np.float32)\n        blobs[\'im_name\'] = os.path.basename(roidb[0][\'image\'])\n\n    else: # not using RPN\n        # Now, build the region of interest and label blobs\n        rois_blob = np.zeros((0, 5), dtype=np.float32)\n        labels_blob = np.zeros((0), dtype=np.float32)\n        bbox_targets_blob = np.zeros((0, 4 * num_classes), dtype=np.float32)\n        bbox_inside_blob = np.zeros(bbox_targets_blob.shape, dtype=np.float32)\n        # all_overlaps = []\n        for im_i in xrange(num_images):\n            labels, overlaps, im_rois, bbox_targets, bbox_inside_weights \\\n                = _sample_rois(roidb[im_i], fg_rois_per_image, rois_per_image,\n                               num_classes)\n\n            # Add to RoIs blob\n            rois = _project_im_rois(im_rois, im_scales[im_i])\n            batch_ind = im_i * np.ones((rois.shape[0], 1))\n            rois_blob_this_image = np.hstack((batch_ind, rois))\n            rois_blob = np.vstack((rois_blob, rois_blob_this_image))\n\n            # Add to labels, bbox targets, and bbox loss blobs\n            labels_blob = np.hstack((labels_blob, labels))\n            bbox_targets_blob = np.vstack((bbox_targets_blob, bbox_targets))\n            bbox_inside_blob = np.vstack((bbox_inside_blob, bbox_inside_weights))\n            # all_overlaps = np.hstack((all_overlaps, overlaps))\n\n        # For debug visualizations\n        # _vis_minibatch(im_blob, rois_blob, labels_blob, all_overlaps)\n\n        blobs[\'rois\'] = rois_blob\n        blobs[\'labels\'] = labels_blob\n\n        if cfg.TRAIN.BBOX_REG:\n            blobs[\'bbox_targets\'] = bbox_targets_blob\n            blobs[\'bbox_inside_weights\'] = bbox_inside_blob\n            blobs[\'bbox_outside_weights\'] = \\\n                np.array(bbox_inside_blob > 0).astype(np.float32)\n\n    return blobs\n\ndef _sample_rois(roidb, fg_rois_per_image, rois_per_image, num_classes):\n    """"""Generate a random sample of RoIs comprising foreground and background\n    examples.\n    """"""\n    # label = class RoI has max overlap with\n    labels = roidb[\'max_classes\']\n    overlaps = roidb[\'max_overlaps\']\n    rois = roidb[\'boxes\']\n\n    # Select foreground RoIs as those with >= FG_THRESH overlap\n    fg_inds = np.where(overlaps >= cfg.TRAIN.FG_THRESH)[0]\n    # Guard against the case when an image has fewer than fg_rois_per_image\n    # foreground RoIs\n    fg_rois_per_this_image = np.minimum(fg_rois_per_image, fg_inds.size)\n    # Sample foreground regions without replacement\n    if fg_inds.size > 0:\n        fg_inds = npr.choice(\n                fg_inds, size=fg_rois_per_this_image, replace=False)\n\n    # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n    bg_inds = np.where((overlaps < cfg.TRAIN.BG_THRESH_HI) &\n                       (overlaps >= cfg.TRAIN.BG_THRESH_LO))[0]\n    # Compute number of background RoIs to take from this image (guarding\n    # against there being fewer than desired)\n    bg_rois_per_this_image = rois_per_image - fg_rois_per_this_image\n    bg_rois_per_this_image = np.minimum(bg_rois_per_this_image,\n                                        bg_inds.size)\n    # Sample foreground regions without replacement\n    if bg_inds.size > 0:\n        bg_inds = npr.choice(\n                bg_inds, size=bg_rois_per_this_image, replace=False)\n\n    # The indices that we\'re selecting (both fg and bg)\n    keep_inds = np.append(fg_inds, bg_inds)\n    # Select sampled values from various arrays:\n    labels = labels[keep_inds]\n    # Clamp labels for the background RoIs to 0\n    labels[fg_rois_per_this_image:] = 0\n    overlaps = overlaps[keep_inds]\n    rois = rois[keep_inds]\n\n    bbox_targets, bbox_inside_weights = _get_bbox_regression_labels(\n            roidb[\'bbox_targets\'][keep_inds, :], num_classes)\n\n    return labels, overlaps, rois, bbox_targets, bbox_inside_weights\n\ndef _get_image_blob(roidb, scale_inds):\n    """"""Builds an input blob from the images in the roidb at the specified\n    scales.\n    """"""\n    num_images = len(roidb)\n    processed_ims = []\n    im_scales = []\n    for i in xrange(num_images):\n        im = cv2.imread(roidb[i][\'image\'])\n        if roidb[i][\'flipped\']:\n            im = im[:, ::-1, :]\n        target_size = cfg.TRAIN.SCALES[scale_inds[i]]\n        im, im_scale = prep_im_for_blob(im, cfg.PIXEL_MEANS, target_size,\n                                        cfg.TRAIN.MAX_SIZE)\n        im_scales.append(im_scale)\n        processed_ims.append(im)\n\n    # Create a blob to hold the input images\n    blob = im_list_to_blob(processed_ims)\n\n    return blob, im_scales\n\ndef _project_im_rois(im_rois, im_scale_factor):\n    """"""Project image RoIs into the rescaled training image.""""""\n    rois = im_rois * im_scale_factor\n    return rois\n\ndef _get_bbox_regression_labels(bbox_target_data, num_classes):\n    """"""Bounding-box regression targets are stored in a compact form in the\n    roidb.\n\n    This function expands those targets into the 4-of-4*K representation used\n    by the network (i.e. only one class has non-zero targets). The loss weights\n    are similarly expanded.\n\n    Returns:\n        bbox_target_data (ndarray): N x 4K blob of regression targets\n        bbox_inside_weights (ndarray): N x 4K blob of loss weights\n    """"""\n    clss = bbox_target_data[:, 0]\n    bbox_targets = np.zeros((clss.size, 4 * num_classes), dtype=np.float32)\n    bbox_inside_weights = np.zeros(bbox_targets.shape, dtype=np.float32)\n    inds = np.where(clss > 0)[0]\n    for ind in inds:\n        cls = clss[ind]\n        start = 4 * cls\n        end = start + 4\n        bbox_targets[ind, start:end] = bbox_target_data[ind, 1:]\n        bbox_inside_weights[ind, start:end] = cfg.TRAIN.BBOX_INSIDE_WEIGHTS\n    return bbox_targets, bbox_inside_weights\n\ndef _vis_minibatch(im_blob, rois_blob, labels_blob, overlaps):\n    """"""Visualize a mini-batch for debugging.""""""\n    import matplotlib.pyplot as plt\n    for i in xrange(rois_blob.shape[0]):\n        rois = rois_blob[i, :]\n        im_ind = rois[0]\n        roi = rois[1:]\n        im = im_blob[im_ind, :, :, :].transpose((1, 2, 0)).copy()\n        im += cfg.PIXEL_MEANS\n        im = im[:, :, (2, 1, 0)]\n        im = im.astype(np.uint8)\n        cls = labels_blob[i]\n        plt.imshow(im)\n        print \'class: \', cls, \' overlap: \', overlaps[i]\n        plt.gca().add_patch(\n            plt.Rectangle((roi[0], roi[1]), roi[2] - roi[0],\n                          roi[3] - roi[1], fill=False,\n                          edgecolor=\'r\', linewidth=3)\n            )\n        plt.show()\n'"
faster_rcnn/roi_data_layer/minibatch2.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Compute minibatch blobs for training a Fast R-CNN network.""""""\n\nimport numpy as np\nimport numpy.random as npr\nimport cv2\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\n# <<<< obsolete\nfrom ..utils.blob import prep_im_for_blob, im_list_to_blob\n\ndef get_minibatch(roidb, num_classes):\n    """"""Given a roidb, construct a minibatch sampled from it.""""""\n    num_images = len(roidb)\n\n    assert(cfg.TRAIN.BATCH_SIZE % num_images == 0), \\\n        \'num_images ({}) must divide BATCH_SIZE ({})\'. \\\n        format(num_images, cfg.TRAIN.BATCH_SIZE)\n    rois_per_image = cfg.TRAIN.BATCH_SIZE / num_images\n    fg_rois_per_image = np.round(cfg.TRAIN.FG_FRACTION * rois_per_image)\n\n    if cfg.IS_MULTISCALE:\n        im_blob, im_scales = _get_image_blob_multiscale(roidb)\n    else:\n        # Get the input image blob, formatted for caffe\n        # Sample random scales to use for each image in this batch\n        random_scale_inds = npr.randint(0, high=len(cfg.TRAIN.SCALES_BASE), size=num_images)\n        im_blob, im_scales = _get_image_blob(roidb, random_scale_inds)\n\n    blobs = {\'data\': im_blob}\n\n    if cfg.TRAIN.HAS_RPN:\n        assert len(im_scales) == 1, ""Single batch only""\n        assert len(roidb) == 1, ""Single batch only""\n        # gt boxes: (x1, y1, x2, y2, cls)\n        gt_inds = np.where(roidb[0][\'gt_classes\'] != 0)[0]\n        gt_boxes = np.empty((len(gt_inds), 5), dtype=np.float32)\n        gt_boxes[:, 0:4] = roidb[0][\'boxes\'][gt_inds, :] * im_scales[0]\n        gt_boxes[:, 4] = roidb[0][\'gt_classes\'][gt_inds]\n        blobs[\'gt_boxes\'] = gt_boxes\n        blobs[\'im_info\'] = np.array(\n            [[im_blob.shape[1], im_blob.shape[2], im_scales[0]]],\n            dtype=np.float32)\n\n\n    else:\n        # Now, build the region of interest and label blobs\n        rois_blob = np.zeros((0, 5), dtype=np.float32)\n        labels_blob = np.zeros((0), dtype=np.float32)\n        bbox_targets_blob = np.zeros((0, 4 * num_classes), dtype=np.float32)\n        bbox_inside_blob = np.zeros(bbox_targets_blob.shape, dtype=np.float32)\n\n        # all_overlaps = []\n        for im_i in xrange(num_images):\n            labels, overlaps, im_rois, bbox_targets, bbox_inside_weights, sublabels \\\n                    = _sample_rois(roidb[im_i], fg_rois_per_image, rois_per_image, num_classes)\n\n            # Add to RoIs blob\n            if cfg.IS_MULTISCALE:\n                if cfg.IS_EXTRAPOLATING:\n                    rois, levels = _project_im_rois_multiscale(im_rois, cfg.TRAIN.SCALES)\n                    batch_ind = im_i * len(cfg.TRAIN.SCALES) + levels\n                else:\n                    rois, levels = _project_im_rois_multiscale(im_rois, cfg.TRAIN.SCALES_BASE)\n                    batch_ind = im_i * len(cfg.TRAIN.SCALES_BASE) + levels\n            else:\n                rois = _project_im_rois(im_rois, im_scales[im_i])\n                batch_ind = im_i * np.ones((rois.shape[0], 1))\n\n            rois_blob_this_image = np.hstack((batch_ind, rois))\n            rois_blob = np.vstack((rois_blob, rois_blob_this_image))\n\n            # Add to labels, bbox targets, and bbox loss blobs\n            labels_blob = np.hstack((labels_blob, labels))\n            bbox_targets_blob = np.vstack((bbox_targets_blob, bbox_targets))\n            bbox_inside_blob = np.vstack((bbox_inside_blob, bbox_inside_weights))\n\n            # all_overlaps = np.hstack((all_overlaps, overlaps))\n\n        # For debug visualizations\n        # _vis_minibatch(im_blob, rois_blob, labels_blob, all_overlaps, sublabels_blob, view_targets_blob, view_inside_blob)\n        # _vis_minibatch(im_blob, rois_blob, labels_blob, all_overlaps, sublabels_blob)\n\n        blobs[\'rois\'] = rois_blob\n        blobs[\'labels\'] = labels_blob\n\n        if cfg.TRAIN.BBOX_REG:\n            blobs[\'bbox_targets\'] = bbox_targets_blob\n            blobs[\'bbox_inside_weights\'] = bbox_inside_blob\n            blobs[\'bbox_outside_weights\'] = np.array(bbox_inside_blob > 0).astype(np.float32)\n\n    return blobs\n\ndef _sample_rois(roidb, fg_rois_per_image, rois_per_image, num_classes):\n    """"""Generate a random sample of RoIs comprising foreground and background\n    examples.\n    """"""\n    # label = class RoI has max overlap with\n    labels = roidb[\'max_classes\']\n    overlaps = roidb[\'max_overlaps\']\n    rois = roidb[\'boxes\']\n\n    # Select foreground RoIs as those with >= FG_THRESH overlap\n    fg_inds = []\n    for i in xrange(1, num_classes):\n        fg_inds.extend(np.where((labels == i) & (overlaps >= cfg.TRAIN.FG_THRESH))[0])\n    fg_inds = np.array(fg_inds)\n\n    # Guard against the case when an image has fewer than fg_rois_per_image\n    # foreground RoIs\n    fg_rois_per_this_image = np.minimum(fg_rois_per_image, fg_inds.size)\n    # Sample foreground regions without replacement\n    if fg_inds.size > 0:\n        fg_inds = npr.choice(fg_inds, size=fg_rois_per_this_image,\n                             replace=False)\n\n    bg_rois_per_this_image = rois_per_image - fg_rois_per_this_image\n    # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n    bg_inds = []\n    for i in xrange(1, num_classes):\n        bg_inds.extend( np.where((labels == i) & (overlaps < cfg.TRAIN.BG_THRESH_HI) &\n                        (overlaps >= cfg.TRAIN.BG_THRESH_LO))[0] )\n\n    if len(bg_inds) < bg_rois_per_this_image:\n        for i in xrange(1, num_classes):\n            bg_inds.extend( np.where((labels == i) & (overlaps < cfg.TRAIN.BG_THRESH_HI))[0] )\n\n    if len(bg_inds) < bg_rois_per_this_image:\n        bg_inds.extend( np.where(overlaps < cfg.TRAIN.BG_THRESH_HI)[0] )\n    bg_inds = np.array(bg_inds, dtype=np.int32)\n\n    # Compute number of background RoIs to take from this image (guarding\n    # against there being fewer than desired)\n    bg_rois_per_this_image = np.minimum(bg_rois_per_this_image,\n                                        bg_inds.size)\n    # Sample foreground regions without replacement\n    if bg_inds.size > 0:\n        bg_inds = npr.choice(bg_inds, size=bg_rois_per_this_image,\n                             replace=False)\n\n    # The indices that we\'re selecting (both fg and bg)\n    keep_inds = np.append(fg_inds, bg_inds).astype(int)\n    # print \'{} foregrounds and {} backgrounds\'.format(fg_inds.size, bg_inds.size)\n    # Select sampled values from various arrays:\n    labels = labels[keep_inds]\n    # Clamp labels for the background RoIs to 0\n    labels[fg_rois_per_this_image:] = 0\n    overlaps = overlaps[keep_inds]\n    rois = rois[keep_inds]\n    sublabels = sublabels[keep_inds]\n    sublabels[fg_rois_per_this_image:] = 0\n\n    bbox_targets, bbox_loss_weights = \\\n            _get_bbox_regression_labels(roidb[\'bbox_targets\'][keep_inds, :],\n                                        num_classes)\n\n    if cfg.TRAIN.VIEWPOINT or cfg.TEST.VIEWPOINT:\n        viewpoints = viewpoints[keep_inds]\n        view_targets, view_loss_weights = \\\n                _get_viewpoint_estimation_labels(viewpoints, labels, num_classes)\n        return labels, overlaps, rois, bbox_targets, bbox_loss_weights, sublabels, view_targets, view_loss_weights\n\n    return labels, overlaps, rois, bbox_targets, bbox_loss_weights, sublabels\n\ndef _get_image_blob(roidb, scale_inds):\n    """"""Builds an input blob from the images in the roidb at the specified\n    scales.\n    """"""\n    num_images = len(roidb)\n    processed_ims = []\n    im_scales = []\n    for i in xrange(num_images):\n        im = cv2.imread(roidb[i][\'image\'])\n        if roidb[i][\'flipped\']:\n            im = im[:, ::-1, :]\n\n        im_orig = im.astype(np.float32, copy=True)\n        im_orig -= cfg.PIXEL_MEANS\n\n        im_scale = cfg.TRAIN.SCALES_BASE[scale_inds[i]]\n        im = cv2.resize(im_orig, None, None, fx=im_scale, fy=im_scale, interpolation=cv2.INTER_LINEAR)\n\n        im_scales.append(im_scale)\n        processed_ims.append(im)\n\n    # Create a blob to hold the input images\n    blob = im_list_to_blob(processed_ims)\n\n    return blob, im_scales\n\n\ndef _get_image_blob_multiscale(roidb):\n    """"""Builds an input blob from the images in the roidb at multiscales.\n    """"""\n    num_images = len(roidb)\n    processed_ims = []\n    im_scales = []\n    scales = cfg.TRAIN.SCALES_BASE\n    for i in xrange(num_images):\n        im = cv2.imread(roidb[i][\'image\'])\n        if roidb[i][\'flipped\']:\n            im = im[:, ::-1, :]\n\n        im_orig = im.astype(np.float32, copy=True)\n        im_orig -= cfg.PIXEL_MEANS\n\n        for im_scale in scales:\n            im = cv2.resize(im_orig, None, None, fx=im_scale, fy=im_scale, interpolation=cv2.INTER_LINEAR)\n            im_scales.append(im_scale)\n            processed_ims.append(im)\n\n    # Create a blob to hold the input images\n    blob = im_list_to_blob(processed_ims)\n\n    return blob, im_scales\n\n\ndef _project_im_rois(im_rois, im_scale_factor):\n    """"""Project image RoIs into the rescaled training image.""""""\n    rois = im_rois * im_scale_factor\n    return rois\n\n\ndef _project_im_rois_multiscale(im_rois, scales):\n    """"""Project image RoIs into the image pyramid built by _get_image_blob.\n\n    Arguments:\n        im_rois (ndarray): R x 4 matrix of RoIs in original image coordinates\n        scales (list): scale factors as returned by _get_image_blob\n\n    Returns:\n        rois (ndarray): R x 4 matrix of projected RoI coordinates\n        levels (list): image pyramid levels used by each projected RoI\n    """"""\n    im_rois = im_rois.astype(np.float, copy=False)\n    scales = np.array(scales)\n\n    if len(scales) > 1:\n        widths = im_rois[:, 2] - im_rois[:, 0] + 1\n        heights = im_rois[:, 3] - im_rois[:, 1] + 1\n\n        areas = widths * heights\n        scaled_areas = areas[:, np.newaxis] * (scales[np.newaxis, :] ** 2)\n        diff_areas = np.abs(scaled_areas - 224 * 224)\n        levels = diff_areas.argmin(axis=1)[:, np.newaxis]\n    else:\n        levels = np.zeros((im_rois.shape[0], 1), dtype=np.int)\n\n    rois = im_rois * scales[levels]\n\n    return rois, levels\n\n\ndef _get_bbox_regression_labels(bbox_target_data, num_classes):\n    """"""Bounding-box regression targets are stored in a compact form in the\n    roidb.\n\n    This function expands those targets into the 4-of-4*K representation used\n    by the network (i.e. only one class has non-zero targets). The loss weights\n    are similarly expanded.\n\n    Returns:\n        bbox_target_data (ndarray): N x 4K blob of regression targets\n        bbox_loss_weights (ndarray): N x 4K blob of loss weights\n    """"""\n    clss = bbox_target_data[:, 0]\n    bbox_targets = np.zeros((clss.size, 4 * num_classes), dtype=np.float32)\n    bbox_loss_weights = np.zeros(bbox_targets.shape, dtype=np.float32)\n    inds = np.where(clss > 0)[0]\n    for ind in inds:\n        cls = clss[ind]\n        start = 4 * cls\n        end = start + 4\n        bbox_targets[ind, start:end] = bbox_target_data[ind, 1:]\n        bbox_loss_weights[ind, start:end] = [1., 1., 1., 1.]\n    return bbox_targets, bbox_loss_weights\n\n\ndef _get_viewpoint_estimation_labels(viewpoint_data, clss, num_classes):\n    """"""Bounding-box regression targets are stored in a compact form in the\n    roidb.\n\n    This function expands those targets into the 4-of-4*K representation used\n    by the network (i.e. only one class has non-zero targets). The loss weights\n    are similarly expanded.\n\n    Returns:\n        view_target_data (ndarray): N x 3K blob of regression targets\n        view_loss_weights (ndarray): N x 3K blob of loss weights\n    """"""\n    view_targets = np.zeros((clss.size, 3 * num_classes), dtype=np.float32)\n    view_loss_weights = np.zeros(view_targets.shape, dtype=np.float32)\n    inds = np.where( (clss > 0) & np.isfinite(viewpoint_data[:,0]) & np.isfinite(viewpoint_data[:,1]) & np.isfinite(viewpoint_data[:,2]) )[0]\n    for ind in inds:\n        cls = clss[ind]\n        start = 3 * cls\n        end = start + 3\n        view_targets[ind, start:end] = viewpoint_data[ind, :]\n        view_loss_weights[ind, start:end] = [1., 1., 1.]\n\n    assert not np.isinf(view_targets).any(), \'viewpoint undefined\'\n    return view_targets, view_loss_weights\n\n\ndef _vis_minibatch(im_blob, rois_blob, labels_blob, overlaps, sublabels_blob, view_targets_blob=None, view_inside_blob=None):\n    """"""Visualize a mini-batch for debugging.""""""\n    import matplotlib.pyplot as plt\n    import math\n    for i in xrange(min(rois_blob.shape[0], 10)):\n        rois = rois_blob[i, :]\n        im_ind = rois[0]\n        roi = rois[1:]\n        im = im_blob[im_ind, :, :, :].transpose((1, 2, 0)).copy()\n        im += cfg.PIXEL_MEANS\n        im = im[:, :, (2, 1, 0)]\n        im = im.astype(np.uint8)\n        cls = labels_blob[i]\n        subcls = sublabels_blob[i]\n        plt.imshow(im)\n        print \'class: \', cls, \' subclass: \', subcls, \' overlap: \', overlaps[i]\n\n        start = 3 * cls\n        end = start + 3\n        # print \'view: \', view_targets_blob[i, start:end] * 180 / math.pi\n        # print \'view weights: \', view_inside_blob[i, start:end]\n\n        plt.gca().add_patch(\n            plt.Rectangle((roi[0], roi[1]), roi[2] - roi[0],\n                          roi[3] - roi[1], fill=False,\n                          edgecolor=\'r\', linewidth=3)\n            )\n        plt.show()\n'"
faster_rcnn/roi_data_layer/roidb.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Transform a roidb into a trainable roidb by adding a bunch of metadata.""""""\n\nimport numpy as np\n\nimport PIL\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\nfrom ..fast_rcnn.bbox_transform import bbox_transform\n# <<<< obsolete\nfrom ..utils.cython_bbox import bbox_overlaps\n\ndef prepare_roidb(imdb):\n    """"""Enrich the imdb\'s roidb by adding some derived quantities that\n    are useful for training. This function precomputes the maximum\n    overlap, taken over ground-truth boxes, between each ROI and\n    each ground-truth box. The class with maximum overlap is also\n    recorded.\n    """"""\n    sizes = [PIL.Image.open(imdb.image_path_at(i)).size\n             for i in xrange(imdb.num_images)]\n    roidb = imdb.roidb\n    for i in xrange(len(imdb.image_index)):\n        roidb[i][\'image\'] = imdb.image_path_at(i)\n        roidb[i][\'width\'] = sizes[i][0]\n        roidb[i][\'height\'] = sizes[i][1]\n        # need gt_overlaps as a dense array for argmax\n        gt_overlaps = roidb[i][\'gt_overlaps\'].toarray()\n        # max overlap with gt over classes (columns)\n        max_overlaps = gt_overlaps.max(axis=1)\n        # gt class that had the max overlap\n        max_classes = gt_overlaps.argmax(axis=1)\n        roidb[i][\'max_classes\'] = max_classes\n        roidb[i][\'max_overlaps\'] = max_overlaps\n        # sanity checks\n        # max overlap of 0 => class should be zero (background)\n        zero_inds = np.where(max_overlaps == 0)[0]\n        assert all(max_classes[zero_inds] == 0)\n        # max overlap > 0 => class should not be zero (must be a fg class)\n        nonzero_inds = np.where(max_overlaps > 0)[0]\n        assert all(max_classes[nonzero_inds] != 0)\n\ndef add_bbox_regression_targets(roidb):\n    """"""\n    Add information needed to train bounding-box regressors.\n    For each roi find the corresponding gt box, and compute the distance.\n    then normalize the distance into Gaussian by minus mean and divided by std\n    """"""\n    assert len(roidb) > 0\n    assert \'max_classes\' in roidb[0], \'Did you call prepare_roidb first?\'\n\n    num_images = len(roidb)\n    # Infer number of classes from the number of columns in gt_overlaps\n    num_classes = roidb[0][\'gt_overlaps\'].shape[1]\n    for im_i in xrange(num_images):\n        rois = roidb[im_i][\'boxes\']\n        max_overlaps = roidb[im_i][\'max_overlaps\']\n        max_classes = roidb[im_i][\'max_classes\']\n        roidb[im_i][\'bbox_targets\'] = \\\n                _compute_targets(rois, max_overlaps, max_classes)\n\n    if cfg.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED:\n        # Use fixed / precomputed ""means"" and ""stds"" instead of empirical values\n        means = np.tile(\n                np.array(cfg.TRAIN.BBOX_NORMALIZE_MEANS), (num_classes, 1))\n        stds = np.tile(\n                np.array(cfg.TRAIN.BBOX_NORMALIZE_STDS), (num_classes, 1))\n    else:\n        # Compute values needed for means and stds\n        # var(x) = E(x^2) - E(x)^2\n        class_counts = np.zeros((num_classes, 1)) + cfg.EPS\n        sums = np.zeros((num_classes, 4))\n        squared_sums = np.zeros((num_classes, 4))\n        for im_i in xrange(num_images):\n            targets = roidb[im_i][\'bbox_targets\']\n            for cls in xrange(1, num_classes):\n                cls_inds = np.where(targets[:, 0] == cls)[0]\n                if cls_inds.size > 0:\n                    class_counts[cls] += cls_inds.size\n                    sums[cls, :] += targets[cls_inds, 1:].sum(axis=0)\n                    squared_sums[cls, :] += \\\n                            (targets[cls_inds, 1:] ** 2).sum(axis=0)\n\n        means = sums / class_counts\n        stds = np.sqrt(squared_sums / class_counts - means ** 2)\n        # too small number will cause nan error\n        assert np.min(stds) < 0.01, \\\n            \'Boxes std is too small, std:{}\'.format(stds)\n\n    print \'bbox target means:\'\n    print means\n    print means[1:, :].mean(axis=0) # ignore bg class\n    print \'bbox target stdevs:\'\n    print stds\n    print stds[1:, :].mean(axis=0) # ignore bg class\n\n    # Normalize targets\n    if cfg.TRAIN.BBOX_NORMALIZE_TARGETS:\n        print ""Normalizing targets""\n        for im_i in xrange(num_images):\n            targets = roidb[im_i][\'bbox_targets\']\n            for cls in xrange(1, num_classes):\n                cls_inds = np.where(targets[:, 0] == cls)[0]\n                roidb[im_i][\'bbox_targets\'][cls_inds, 1:] -= means[cls, :]\n                roidb[im_i][\'bbox_targets\'][cls_inds, 1:] /= stds[cls, :]\n    else:\n        print ""NOT normalizing targets""\n\n    # These values will be needed for making predictions\n    # (the predicts will need to be unnormalized and uncentered)\n    return means.ravel(), stds.ravel()\n\ndef _compute_targets(rois, overlaps, labels):\n    """"""\n    Compute bounding-box regression targets for an image.\n    for each roi find the corresponding gt_box, then compute the distance.\n    """"""\n    # Indices of ground-truth ROIs\n    gt_inds = np.where(overlaps == 1)[0]\n    if len(gt_inds) == 0:\n        # Bail if the image has no ground-truth ROIs\n        return np.zeros((rois.shape[0], 5), dtype=np.float32)\n    # Indices of examples for which we try to make predictions\n    ex_inds = np.where(overlaps >= cfg.TRAIN.BBOX_THRESH)[0]\n\n    # Get IoU overlap between each ex ROI and gt ROI\n    ex_gt_overlaps = bbox_overlaps(\n        np.ascontiguousarray(rois[ex_inds, :], dtype=np.float),\n        np.ascontiguousarray(rois[gt_inds, :], dtype=np.float))\n\n    # Find which gt ROI each ex ROI has max overlap with:\n    # this will be the ex ROI\'s gt target\n    gt_assignment = ex_gt_overlaps.argmax(axis=1)\n    gt_rois = rois[gt_inds[gt_assignment], :]\n    ex_rois = rois[ex_inds, :]\n\n    targets = np.zeros((rois.shape[0], 5), dtype=np.float32)\n    targets[ex_inds, 0] = labels[ex_inds]\n    targets[ex_inds, 1:] = bbox_transform(ex_rois, gt_rois)\n    return targets\n'"
faster_rcnn/roi_data_layer/roidb2.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Transform a roidb into a trainable roidb by adding a bunch of metadata.""""""\n\nimport numpy as np\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\nfrom ..fast_rcnn.bbox_transform import bbox_transform\n# <<<< obsolete\nfrom ..utils.cython_bbox import bbox_overlaps\n\ndef prepare_roidb(imdb):\n    """"""Enrich the imdb\'s roidb by adding some derived quantities that\n    are useful for training. This function precomputes the maximum\n    overlap, taken over ground-truth boxes, between each ROI and\n    each ground-truth box. The class with maximum overlap is also\n    recorded.\n    """"""\n    roidb = imdb.roidb\n    for i in xrange(len(imdb.image_index)):\n        roidb[i][\'image\'] = imdb.image_path_at(i)\n        # need gt_overlaps as a dense array for argmax\n        gt_overlaps = roidb[i][\'gt_overlaps\'].toarray()\n        # max overlap with gt over classes (columns)\n        max_overlaps = gt_overlaps.max(axis=1)\n        # gt class that had the max overlap\n        max_classes = gt_overlaps.argmax(axis=1)\n\n        roidb[i][\'max_classes\'] = max_classes\n        roidb[i][\'max_overlaps\'] = max_overlaps\n\n        # sanity checks\n        # max overlap of 0 => class should be zero (background)\n        zero_inds = np.where(max_overlaps == 0)[0]\n        assert all(max_classes[zero_inds] == 0)\n        # max overlap > 0 => class should not be zero (must be a fg class)\n        nonzero_inds = np.where(max_overlaps > 0)[0]\n        assert all(max_classes[nonzero_inds] != 0)\n\ndef add_bbox_regression_targets(roidb):\n    """"""Add information needed to train bounding-box regressors.""""""\n    assert len(roidb) > 0\n    assert \'max_classes\' in roidb[0], \'Did you call prepare_roidb first?\'\n\n    num_images = len(roidb)\n    # Infer number of classes from the number of columns in gt_overlaps\n    num_classes = roidb[0][\'gt_overlaps\'].shape[1]\n    for im_i in xrange(num_images):\n        rois = roidb[im_i][\'boxes\']\n        max_overlaps = roidb[im_i][\'max_overlaps\']\n        max_classes = roidb[im_i][\'max_classes\']\n        roidb[im_i][\'bbox_targets\'] = \\\n                _compute_targets(rois, max_overlaps, max_classes, num_classes)\n\n    # Compute values needed for means and stds\n    # var(x) = E(x^2) - E(x)^2\n    class_counts = np.zeros((num_classes, 1)) + cfg.EPS\n    sums = np.zeros((num_classes, 4))\n    squared_sums = np.zeros((num_classes, 4))\n    for im_i in xrange(num_images):\n        targets = roidb[im_i][\'bbox_targets\']\n        for cls in xrange(1, num_classes):\n            cls_inds = np.where(targets[:, 0] == cls)[0]\n            if cls_inds.size > 0:\n                class_counts[cls] += cls_inds.size\n                sums[cls, :] += targets[cls_inds, 1:].sum(axis=0)\n                squared_sums[cls, :] += (targets[cls_inds, 1:] ** 2).sum(axis=0)\n\n    means = sums / class_counts\n    stds = np.sqrt(squared_sums / class_counts - means ** 2)\n\n    # Normalize targets\n    for im_i in xrange(num_images):\n        targets = roidb[im_i][\'bbox_targets\']\n        for cls in xrange(1, num_classes):\n            cls_inds = np.where(targets[:, 0] == cls)[0]\n            roidb[im_i][\'bbox_targets\'][cls_inds, 1:] -= means[cls, :]\n            if stds[cls, 0] != 0:\n                roidb[im_i][\'bbox_targets\'][cls_inds, 1:] /= stds[cls, :]\n\n    # These values will be needed for making predictions\n    # (the predicts will need to be unnormalized and uncentered)\n    return means.ravel(), stds.ravel()\n\ndef _compute_targets(rois, overlaps, labels, num_classes):\n    """"""Compute bounding-box regression targets for an image.""""""\n    # Ensure ROIs are floats\n    rois = rois.astype(np.float, copy=False)\n\n    # Indices of ground-truth ROIs\n    gt_inds = np.where(overlaps == 1)[0]\n    # Indices of examples for which we try to make predictions\n    ex_inds = []\n    for i in xrange(1, num_classes):\n        ex_inds.extend( np.where((labels == i) & (overlaps >= cfg.TRAIN.BBOX_THRESH))[0] )\n\n    # Get IoU overlap between each ex ROI and gt ROI\n    ex_gt_overlaps = utils.cython_bbox.bbox_overlaps(rois[ex_inds, :],\n                                                     rois[gt_inds, :])\n\n    # Find which gt ROI each ex ROI has max overlap with:\n    # this will be the ex ROI\'s gt target\n    if ex_gt_overlaps.shape[0] != 0:\n        gt_assignment = ex_gt_overlaps.argmax(axis=1)\n    else:\n        gt_assignment = []\n    gt_rois = rois[gt_inds[gt_assignment], :]\n    ex_rois = rois[ex_inds, :]\n\n    ex_widths = ex_rois[:, 2] - ex_rois[:, 0] + cfg.EPS\n    ex_heights = ex_rois[:, 3] - ex_rois[:, 1] + cfg.EPS\n    ex_ctr_x = ex_rois[:, 0] + 0.5 * ex_widths\n    ex_ctr_y = ex_rois[:, 1] + 0.5 * ex_heights\n\n    gt_widths = gt_rois[:, 2] - gt_rois[:, 0] + cfg.EPS\n    gt_heights = gt_rois[:, 3] - gt_rois[:, 1] + cfg.EPS\n    gt_ctr_x = gt_rois[:, 0] + 0.5 * gt_widths\n    gt_ctr_y = gt_rois[:, 1] + 0.5 * gt_heights\n\n    targets_dx = (gt_ctr_x - ex_ctr_x) / ex_widths\n    targets_dy = (gt_ctr_y - ex_ctr_y) / ex_heights\n    targets_dw = np.log(gt_widths / ex_widths)\n    targets_dh = np.log(gt_heights / ex_heights)\n\n    targets = np.zeros((rois.shape[0], 5), dtype=np.float32)\n    targets[ex_inds, 0] = labels[ex_inds]\n    targets[ex_inds, 1] = targets_dx\n    targets[ex_inds, 2] = targets_dy\n    targets[ex_inds, 3] = targets_dw\n    targets[ex_inds, 4] = targets_dh\n    return targets\n'"
faster_rcnn/roi_pooling/__init__.py,0,b''
faster_rcnn/roi_pooling/build.py,2,"b""import os\nimport torch\nfrom torch.utils.ffi import create_extension\n\n\nsources = ['src/roi_pooling.c']\nheaders = ['src/roi_pooling.h']\ndefines = []\nwith_cuda = False\n\nif torch.cuda.is_available():\n    print('Including CUDA code.')\n    sources += ['src/roi_pooling_cuda.c']\n    headers += ['src/roi_pooling_cuda.h']\n    defines += [('WITH_CUDA', None)]\n    with_cuda = True\n\nthis_file = os.path.dirname(os.path.realpath(__file__))\nprint(this_file)\nextra_objects = ['src/cuda/roi_pooling.cu.o']\nextra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\n\nffi = create_extension(\n    '_ext.roi_pooling',\n    headers=headers,\n    sources=sources,\n    define_macros=defines,\n    relative_to=__file__,\n    with_cuda=with_cuda,\n    extra_objects=extra_objects\n)\n\nif __name__ == '__main__':\n    ffi.build()\n"""
faster_rcnn/rpn_msr/__init__.py,0,b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Sean Bell\n# --------------------------------------------------------\n'
faster_rcnn/rpn_msr/anchor_target_layer.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Sean Bell\n# --------------------------------------------------------\n\nimport os\nimport yaml\nimport numpy as np\nimport numpy.random as npr\n\nfrom .generate_anchors import generate_anchors\nfrom ..utils.cython_bbox import bbox_overlaps, bbox_intersections\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\nfrom ..fast_rcnn.bbox_transform import bbox_transform\n\n# <<<< obsolete\n\nDEBUG = False\n\n\ndef anchor_target_layer(rpn_cls_score, gt_boxes, gt_ishard, dontcare_areas, im_info, _feat_stride=[16, ],\n                        anchor_scales=[4, 8, 16, 32]):\n    """"""\n    Assign anchors to ground-truth targets. Produces anchor classification\n    labels and bounding-box regression targets.\n    Parameters\n    ----------\n    rpn_cls_score: for pytorch (1, Ax2, H, W) bg/fg scores of previous conv layer\n    gt_boxes: (G, 5) vstack of [x1, y1, x2, y2, class]\n    gt_ishard: (G, 1), 1 or 0 indicates difficult or not\n    dontcare_areas: (D, 4), some areas may contains small objs but no labelling. D may be 0\n    im_info: a list of [image_height, image_width, scale_ratios]\n    _feat_stride: the downsampling ratio of feature map to the original input image\n    anchor_scales: the scales to the basic_anchor (basic anchor is [16, 16])\n    ----------\n    Returns\n    ----------\n    rpn_labels : (HxWxA, 1), for each anchor, 0 denotes bg, 1 fg, -1 dontcare\n    rpn_bbox_targets: (HxWxA, 4), distances of the anchors to the gt_boxes(may contains some transform)\n                            that are the regression objectives\n    rpn_bbox_inside_weights: (HxWxA, 4) weights of each boxes, mainly accepts hyper param in cfg\n    rpn_bbox_outside_weights: (HxWxA, 4) used to balance the fg/bg,\n                            beacuse the numbers of bgs and fgs mays significiantly different\n    """"""\n    _anchors = generate_anchors(scales=np.array(anchor_scales))\n    _num_anchors = _anchors.shape[0]\n\n    if DEBUG:\n        print \'anchors:\'\n        print _anchors\n        print \'anchor shapes:\'\n        print np.hstack((\n            _anchors[:, 2::4] - _anchors[:, 0::4],\n            _anchors[:, 3::4] - _anchors[:, 1::4],\n        ))\n        _counts = cfg.EPS\n        _sums = np.zeros((1, 4))\n        _squared_sums = np.zeros((1, 4))\n        _fg_sum = 0\n        _bg_sum = 0\n        _count = 0\n\n    # allow boxes to sit over the edge by a small amount\n    _allowed_border = 0\n    # map of shape (..., H, W)\n    # height, width = rpn_cls_score.shape[1:3]\n\n    im_info = im_info[0]\n\n    # Algorithm:\n    #\n    # for each (H, W) location i\n    #   generate 9 anchor boxes centered on cell i\n    #   apply predicted bbox deltas at cell i to each of the 9 anchors\n    # filter out-of-image anchors\n    # measure GT overlap\n\n    assert rpn_cls_score.shape[0] == 1, \\\n        \'Only single item batches are supported\'\n\n    # map of shape (..., H, W)\n    # pytorch (bs, c, h, w)\n    height, width = rpn_cls_score.shape[2:4]\n\n    if DEBUG:\n        print \'AnchorTargetLayer: height\', height, \'width\', width\n        print \'\'\n        print \'im_size: ({}, {})\'.format(im_info[0], im_info[1])\n        print \'scale: {}\'.format(im_info[2])\n        print \'height, width: ({}, {})\'.format(height, width)\n        print \'rpn: gt_boxes.shape\', gt_boxes.shape\n        print \'rpn: gt_boxes\', gt_boxes\n\n    # 1. Generate proposals from bbox deltas and shifted anchors\n    shift_x = np.arange(0, width) * _feat_stride\n    shift_y = np.arange(0, height) * _feat_stride\n    shift_x, shift_y = np.meshgrid(shift_x, shift_y)  # in W H order\n    # K is H x W\n    shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                        shift_x.ravel(), shift_y.ravel())).transpose()\n    # add A anchors (1, A, 4) to\n    # cell K shifts (K, 1, 4) to get\n    # shift anchors (K, A, 4)\n    # reshape to (K*A, 4) shifted anchors\n    A = _num_anchors\n    K = shifts.shape[0]\n    all_anchors = (_anchors.reshape((1, A, 4)) +\n                   shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n    all_anchors = all_anchors.reshape((K * A, 4))\n    total_anchors = int(K * A)\n\n    # only keep anchors inside the image\n    inds_inside = np.where(\n        (all_anchors[:, 0] >= -_allowed_border) &\n        (all_anchors[:, 1] >= -_allowed_border) &\n        (all_anchors[:, 2] < im_info[1] + _allowed_border) &  # width\n        (all_anchors[:, 3] < im_info[0] + _allowed_border)  # height\n    )[0]\n\n    if DEBUG:\n        print \'total_anchors\', total_anchors\n        print \'inds_inside\', len(inds_inside)\n\n    # keep only inside anchors\n    anchors = all_anchors[inds_inside, :]\n    if DEBUG:\n        print \'anchors.shape\', anchors.shape\n\n    # label: 1 is positive, 0 is negative, -1 is dont care\n    # (A)\n    labels = np.empty((len(inds_inside),), dtype=np.float32)\n    labels.fill(-1)\n\n    # overlaps between the anchors and the gt boxes\n    # overlaps (ex, gt), shape is A x G\n    overlaps = bbox_overlaps(\n        np.ascontiguousarray(anchors, dtype=np.float),\n        np.ascontiguousarray(gt_boxes, dtype=np.float))\n    argmax_overlaps = overlaps.argmax(axis=1)  # (A)\n    max_overlaps = overlaps[np.arange(len(inds_inside)), argmax_overlaps]\n    gt_argmax_overlaps = overlaps.argmax(axis=0)  # G\n    gt_max_overlaps = overlaps[gt_argmax_overlaps,\n                               np.arange(overlaps.shape[1])]\n    gt_argmax_overlaps = np.where(overlaps == gt_max_overlaps)[0]\n\n    if not cfg.TRAIN.RPN_CLOBBER_POSITIVES:\n        # assign bg labels first so that positive labels can clobber them\n        labels[max_overlaps < cfg.TRAIN.RPN_NEGATIVE_OVERLAP] = 0\n\n    # fg label: for each gt, anchor with highest overlap\n    labels[gt_argmax_overlaps] = 1\n    # fg label: above threshold IOU\n    labels[max_overlaps >= cfg.TRAIN.RPN_POSITIVE_OVERLAP] = 1\n\n    if cfg.TRAIN.RPN_CLOBBER_POSITIVES:\n        # assign bg labels last so that negative labels can clobber positives\n        labels[max_overlaps < cfg.TRAIN.RPN_NEGATIVE_OVERLAP] = 0\n\n    # preclude dontcare areas\n    if dontcare_areas is not None and dontcare_areas.shape[0] > 0:\n        # intersec shape is D x A\n        intersecs = bbox_intersections(\n            np.ascontiguousarray(dontcare_areas, dtype=np.float),  # D x 4\n            np.ascontiguousarray(anchors, dtype=np.float)  # A x 4\n        )\n        intersecs_ = intersecs.sum(axis=0)  # A x 1\n        labels[intersecs_ > cfg.TRAIN.DONTCARE_AREA_INTERSECTION_HI] = -1\n\n    # preclude hard samples that are highly occlusioned, truncated or difficult to see\n    if cfg.TRAIN.PRECLUDE_HARD_SAMPLES and gt_ishard is not None and gt_ishard.shape[0] > 0:\n        assert gt_ishard.shape[0] == gt_boxes.shape[0]\n        gt_ishard = gt_ishard.astype(int)\n        gt_hardboxes = gt_boxes[gt_ishard == 1, :]\n        if gt_hardboxes.shape[0] > 0:\n            # H x A\n            hard_overlaps = bbox_overlaps(\n                np.ascontiguousarray(gt_hardboxes, dtype=np.float),  # H x 4\n                np.ascontiguousarray(anchors, dtype=np.float))  # A x 4\n            hard_max_overlaps = hard_overlaps.max(axis=0)  # (A)\n            labels[hard_max_overlaps >= cfg.TRAIN.RPN_POSITIVE_OVERLAP] = -1\n            max_intersec_label_inds = hard_overlaps.argmax(axis=1)  # H x 1\n            labels[max_intersec_label_inds] = -1  #\n\n    # subsample positive labels if we have too many\n    num_fg = int(cfg.TRAIN.RPN_FG_FRACTION * cfg.TRAIN.RPN_BATCHSIZE)\n    fg_inds = np.where(labels == 1)[0]\n    if len(fg_inds) > num_fg:\n        disable_inds = npr.choice(\n            fg_inds, size=(len(fg_inds) - num_fg), replace=False)\n        labels[disable_inds] = -1\n\n    # subsample negative labels if we have too many\n    num_bg = cfg.TRAIN.RPN_BATCHSIZE - np.sum(labels == 1)\n    bg_inds = np.where(labels == 0)[0]\n    if len(bg_inds) > num_bg:\n        disable_inds = npr.choice(\n            bg_inds, size=(len(bg_inds) - num_bg), replace=False)\n        labels[disable_inds] = -1\n        # print ""was %s inds, disabling %s, now %s inds"" % (\n        # len(bg_inds), len(disable_inds), np.sum(labels == 0))\n\n    # bbox_targets = np.zeros((len(inds_inside), 4), dtype=np.float32)\n    bbox_targets = _compute_targets(anchors, gt_boxes[argmax_overlaps, :])\n\n    bbox_inside_weights = np.zeros((len(inds_inside), 4), dtype=np.float32)\n    bbox_inside_weights[labels == 1, :] = np.array(cfg.TRAIN.RPN_BBOX_INSIDE_WEIGHTS)\n\n    bbox_outside_weights = np.zeros((len(inds_inside), 4), dtype=np.float32)\n    if cfg.TRAIN.RPN_POSITIVE_WEIGHT < 0:\n        # uniform weighting of examples (given non-uniform sampling)\n        # num_examples = np.sum(labels >= 0) + 1\n        # positive_weights = np.ones((1, 4)) * 1.0 / num_examples\n        # negative_weights = np.ones((1, 4)) * 1.0 / num_examples\n        positive_weights = np.ones((1, 4))\n        negative_weights = np.zeros((1, 4))\n    else:\n        assert ((cfg.TRAIN.RPN_POSITIVE_WEIGHT > 0) &\n                (cfg.TRAIN.RPN_POSITIVE_WEIGHT < 1))\n        positive_weights = (cfg.TRAIN.RPN_POSITIVE_WEIGHT /\n                            (np.sum(labels == 1)) + 1)\n        negative_weights = ((1.0 - cfg.TRAIN.RPN_POSITIVE_WEIGHT) /\n                            (np.sum(labels == 0)) + 1)\n    bbox_outside_weights[labels == 1, :] = positive_weights\n    bbox_outside_weights[labels == 0, :] = negative_weights\n\n    if DEBUG:\n        _sums += bbox_targets[labels == 1, :].sum(axis=0)\n        _squared_sums += (bbox_targets[labels == 1, :] ** 2).sum(axis=0)\n        _counts += np.sum(labels == 1)\n        means = _sums / _counts\n        stds = np.sqrt(_squared_sums / _counts - means ** 2)\n        print \'means:\'\n        print means\n        print \'stdevs:\'\n        print stds\n\n    # map up to original set of anchors\n    labels = _unmap(labels, total_anchors, inds_inside, fill=-1)\n    bbox_targets = _unmap(bbox_targets, total_anchors, inds_inside, fill=0)\n    bbox_inside_weights = _unmap(bbox_inside_weights, total_anchors, inds_inside, fill=0)\n    bbox_outside_weights = _unmap(bbox_outside_weights, total_anchors, inds_inside, fill=0)\n\n    if DEBUG:\n        print \'rpn: max max_overlap\', np.max(max_overlaps)\n        print \'rpn: num_positive\', np.sum(labels == 1)\n        print \'rpn: num_negative\', np.sum(labels == 0)\n        _fg_sum += np.sum(labels == 1)\n        _bg_sum += np.sum(labels == 0)\n        _count += 1\n        print \'rpn: num_positive avg\', _fg_sum / _count\n        print \'rpn: num_negative avg\', _bg_sum / _count\n\n    # labels\n    # pdb.set_trace()\n    labels = labels.reshape((1, height, width, A))\n    labels = labels.transpose(0, 3, 1, 2)\n    rpn_labels = labels.reshape((1, 1, A * height, width)).transpose(0, 2, 3, 1)\n\n    # bbox_targets\n    bbox_targets = bbox_targets \\\n        .reshape((1, height, width, A * 4)).transpose(0, 3, 1, 2)\n\n    rpn_bbox_targets = bbox_targets\n    # bbox_inside_weights\n    bbox_inside_weights = bbox_inside_weights \\\n        .reshape((1, height, width, A * 4)).transpose(0, 3, 1, 2)\n    # assert bbox_inside_weights.shape[2] == height\n    # assert bbox_inside_weights.shape[3] == width\n\n    rpn_bbox_inside_weights = bbox_inside_weights\n\n    # bbox_outside_weights\n    bbox_outside_weights = bbox_outside_weights \\\n        .reshape((1, height, width, A * 4)).transpose(0, 3, 1, 2)\n    # assert bbox_outside_weights.shape[2] == height\n    # assert bbox_outside_weights.shape[3] == width\n\n    rpn_bbox_outside_weights = bbox_outside_weights\n\n    return rpn_labels, rpn_bbox_targets, rpn_bbox_inside_weights, rpn_bbox_outside_weights\n\n\ndef _unmap(data, count, inds, fill=0):\n    """""" Unmap a subset of item (data) back to the original set of items (of\n    size count) """"""\n    if len(data.shape) == 1:\n        ret = np.empty((count,), dtype=np.float32)\n        ret.fill(fill)\n        ret[inds] = data\n    else:\n        ret = np.empty((count,) + data.shape[1:], dtype=np.float32)\n        ret.fill(fill)\n        ret[inds, :] = data\n    return ret\n\n\ndef _compute_targets(ex_rois, gt_rois):\n    """"""Compute bounding-box regression targets for an image.""""""\n\n    assert ex_rois.shape[0] == gt_rois.shape[0]\n    assert ex_rois.shape[1] == 4\n    assert gt_rois.shape[1] == 5\n\n    return bbox_transform(ex_rois, gt_rois[:, :4]).astype(np.float32, copy=False)\n'"
faster_rcnn/rpn_msr/generate.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n\nfrom ..utils.blob import im_list_to_blob\nfrom ..utils.timer import Timer\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\n# <<<< obsolete\n\n\ndef _vis_proposals(im, dets, thresh=0.5):\n    """"""Draw detected bounding boxes.""""""\n    inds = np.where(dets[:, -1] >= thresh)[0]\n    if len(inds) == 0:\n        return\n\n    class_name = \'obj\'\n    im = im[:, :, (2, 1, 0)]\n    fig, ax = plt.subplots(figsize=(12, 12))\n    ax.imshow(im, aspect=\'equal\')\n    for i in inds:\n        bbox = dets[i, :4]\n        score = dets[i, -1]\n\n        ax.add_patch(\n            plt.Rectangle((bbox[0], bbox[1]),\n                          bbox[2] - bbox[0],\n                          bbox[3] - bbox[1], fill=False,\n                          edgecolor=\'red\', linewidth=3.5)\n            )\n        ax.text(bbox[0], bbox[1] - 2,\n                \'{:s} {:.3f}\'.format(class_name, score),\n                bbox=dict(facecolor=\'blue\', alpha=0.5),\n                fontsize=14, color=\'white\')\n\n    ax.set_title((\'{} detections with \'\n                  \'p({} | box) >= {:.1f}\').format(class_name, class_name,\n                                                  thresh),\n                  fontsize=14)\n    plt.axis(\'off\')\n    plt.tight_layout()\n    plt.draw()\n\ndef _get_image_blob(im):\n    """"""Converts an image into a network input.\n\n    Arguments:\n        im (ndarray): a color image in BGR order\n\n    Returns:\n        blob (ndarray): a data blob holding an image pyramid\n        im_scale_factors (list): list of image scales (relative to im) used\n            in the image pyramid\n    """"""\n    im_orig = im.astype(np.float32, copy=True)\n    im_orig -= cfg.PIXEL_MEANS\n\n    processed_ims = []\n\n    assert len(cfg.TEST.SCALES_BASE) == 1\n    im_scale = cfg.TRAIN.SCALES_BASE[0]\n\n    im = cv2.resize(im_orig, None, None, fx=im_scale, fy=im_scale,\n                    interpolation=cv2.INTER_LINEAR)\n    im_info = np.hstack((im.shape[:2], im_scale))[np.newaxis, :]\n    processed_ims.append(im)\n\n    # Create a blob to hold the input images\n    blob = im_list_to_blob(processed_ims)\n\n    return blob, im_info\n\ndef im_proposals(net, im):\n    """"""Generate RPN proposals on a single image.""""""\n    blobs = {}\n    blobs[\'data\'], blobs[\'im_info\'] = _get_image_blob(im)\n    net.blobs[\'data\'].reshape(*(blobs[\'data\'].shape))\n    net.blobs[\'im_info\'].reshape(*(blobs[\'im_info\'].shape))\n    blobs_out = net.forward(\n            data=blobs[\'data\'].astype(np.float32, copy=False),\n            im_info=blobs[\'im_info\'].astype(np.float32, copy=False))\n\n    scale = blobs[\'im_info\'][0, 2]\n    boxes = blobs_out[\'rois\'][:, 1:].copy() / scale\n    scores = blobs_out[\'scores\'].copy()\n    return boxes, scores\n\ndef imdb_proposals(net, imdb):\n    """"""Generate RPN proposals on all images in an imdb.""""""\n\n    _t = Timer()\n    imdb_boxes = [[] for _ in xrange(imdb.num_images)]\n    for i in xrange(imdb.num_images):\n        im = cv2.imread(imdb.image_path_at(i))\n        _t.tic()\n        imdb_boxes[i], scores = im_proposals(net, im)\n        _t.toc()\n        print \'im_proposals: {:d}/{:d} {:.3f}s\' \\\n              .format(i + 1, imdb.num_images, _t.average_time)\n        if 0:\n            dets = np.hstack((imdb_boxes[i], scores))\n            # from IPython import embed; embed()\n            _vis_proposals(im, dets[:3, :], thresh=0.9)\n            plt.show()\n\n    return imdb_boxes\n\ndef imdb_proposals_det(net, imdb):\n    """"""Generate RPN proposals on all images in an imdb.""""""\n\n    _t = Timer()\n    imdb_boxes = [[] for _ in xrange(imdb.num_images)]\n    for i in xrange(imdb.num_images):\n        im = cv2.imread(imdb.image_path_at(i))\n        _t.tic()\n        boxes, scores = im_proposals(net, im)\n        _t.toc()\n        print \'im_proposals: {:d}/{:d} {:.3f}s\' \\\n              .format(i + 1, imdb.num_images, _t.average_time)\n        dets = np.hstack((boxes, scores))\n        imdb_boxes[i] = dets\n\n        if 0:            \n            # from IPython import embed; embed()\n            _vis_proposals(im, dets[:3, :], thresh=0.9)\n            plt.show()\n\n    return imdb_boxes\n'"
faster_rcnn/rpn_msr/generate_anchors.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Sean Bell\n# --------------------------------------------------------\n\nimport numpy as np\n\n# Verify that we compute the same anchors as Shaoqing\'s matlab implementation:\n#\n#    >> load output/rpn_cachedir/faster_rcnn_VOC2007_ZF_stage1_rpn/anchors.mat\n#    >> anchors\n#\n#    anchors =\n#\n#       -83   -39   100    56\n#      -175   -87   192   104\n#      -359  -183   376   200\n#       -55   -55    72    72\n#      -119  -119   136   136\n#      -247  -247   264   264\n#       -35   -79    52    96\n#       -79  -167    96   184\n#      -167  -343   184   360\n\n#array([[ -83.,  -39.,  100.,   56.],\n#       [-175.,  -87.,  192.,  104.],\n#       [-359., -183.,  376.,  200.],\n#       [ -55.,  -55.,   72.,   72.],\n#       [-119., -119.,  136.,  136.],\n#       [-247., -247.,  264.,  264.],\n#       [ -35.,  -79.,   52.,   96.],\n#       [ -79., -167.,   96.,  184.],\n#       [-167., -343.,  184.,  360.]])\n\ndef generate_anchors(base_size=16, ratios=[0.5, 1, 2],\n                     scales=2**np.arange(3, 6)):\n    """"""\n    Generate anchor (reference) windows by enumerating aspect ratios X\n    scales wrt a reference (0, 0, 15, 15) window.\n    """"""\n\n    base_anchor = np.array([1, 1, base_size, base_size]) - 1\n    ratio_anchors = _ratio_enum(base_anchor, ratios)\n    anchors = np.vstack([_scale_enum(ratio_anchors[i, :], scales)\n                         for i in xrange(ratio_anchors.shape[0])])\n    return anchors\n\ndef _whctrs(anchor):\n    """"""\n    Return width, height, x center, and y center for an anchor (window).\n    """"""\n\n    w = anchor[2] - anchor[0] + 1\n    h = anchor[3] - anchor[1] + 1\n    x_ctr = anchor[0] + 0.5 * (w - 1)\n    y_ctr = anchor[1] + 0.5 * (h - 1)\n    return w, h, x_ctr, y_ctr\n\ndef _mkanchors(ws, hs, x_ctr, y_ctr):\n    """"""\n    Given a vector of widths (ws) and heights (hs) around a center\n    (x_ctr, y_ctr), output a set of anchors (windows).\n    """"""\n\n    ws = ws[:, np.newaxis]\n    hs = hs[:, np.newaxis]\n    anchors = np.hstack((x_ctr - 0.5 * (ws - 1),\n                         y_ctr - 0.5 * (hs - 1),\n                         x_ctr + 0.5 * (ws - 1),\n                         y_ctr + 0.5 * (hs - 1)))\n    return anchors\n\ndef _ratio_enum(anchor, ratios):\n    """"""\n    Enumerate a set of anchors for each aspect ratio wrt an anchor.\n    """"""\n\n    w, h, x_ctr, y_ctr = _whctrs(anchor)\n    size = w * h\n    size_ratios = size / ratios\n    ws = np.round(np.sqrt(size_ratios))\n    hs = np.round(ws * ratios)\n    anchors = _mkanchors(ws, hs, x_ctr, y_ctr)\n    return anchors\n\ndef _scale_enum(anchor, scales):\n    """"""\n    Enumerate a set of anchors for each scale wrt an anchor.\n    """"""\n\n    w, h, x_ctr, y_ctr = _whctrs(anchor)\n    ws = w * scales\n    hs = h * scales\n    anchors = _mkanchors(ws, hs, x_ctr, y_ctr)\n    return anchors\n\nif __name__ == \'__main__\':\n    import time\n    t = time.time()\n    a = generate_anchors()\n    print time.time() - t\n    print a\n    from IPython import embed; embed()\n'"
faster_rcnn/rpn_msr/proposal_layer.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Sean Bell\n# --------------------------------------------------------\n\nimport numpy as np\nimport yaml\n\nfrom .generate_anchors import generate_anchors\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\nfrom ..fast_rcnn.bbox_transform import bbox_transform_inv, clip_boxes\nfrom ..fast_rcnn.nms_wrapper import nms\n\n# <<<< obsolete\n\n\nDEBUG = False\n""""""\nOutputs object detection proposals by applying estimated bounding-box\ntransformations to a set of regular boxes (called ""anchors"").\n""""""\n\n\ndef proposal_layer(rpn_cls_prob_reshape, rpn_bbox_pred, im_info, cfg_key, _feat_stride=[16, ],\n                   anchor_scales=[8, 16, 32]):\n    """"""\n    Parameters\n    ----------\n    rpn_cls_prob_reshape: (1 , H , W , Ax2) outputs of RPN, prob of bg or fg\n                         NOTICE: the old version is ordered by (1, H, W, 2, A) !!!!\n    rpn_bbox_pred: (1 , H , W , Ax4), rgs boxes output of RPN\n    im_info: a list of [image_height, image_width, scale_ratios]\n    cfg_key: \'TRAIN\' or \'TEST\'\n    _feat_stride: the downsampling ratio of feature map to the original input image\n    anchor_scales: the scales to the basic_anchor (basic anchor is [16, 16])\n    ----------\n    Returns\n    ----------\n    rpn_rois : (1 x H x W x A, 5) e.g. [0, x1, y1, x2, y2]\n\n    # Algorithm:\n    #\n    # for each (H, W) location i\n    #   generate A anchor boxes centered on cell i\n    #   apply predicted bbox deltas at cell i to each of the A anchors\n    # clip predicted boxes to image\n    # remove predicted boxes with either height or width < threshold\n    # sort all (proposal, score) pairs by score from highest to lowest\n    # take top pre_nms_topN proposals before NMS\n    # apply NMS with threshold 0.7 to remaining proposals\n    # take after_nms_topN proposals after NMS\n    # return the top proposals (-> RoIs top, scores top)\n    #layer_params = yaml.load(self.param_str_)\n\n    """"""\n    _anchors = generate_anchors(scales=np.array(anchor_scales))\n    _num_anchors = _anchors.shape[0]\n    # rpn_cls_prob_reshape = np.transpose(rpn_cls_prob_reshape,[0,3,1,2]) #-> (1 , 2xA, H , W)\n    # rpn_bbox_pred = np.transpose(rpn_bbox_pred,[0,3,1,2])              # -> (1 , Ax4, H , W)\n\n    # rpn_cls_prob_reshape = np.transpose(np.reshape(rpn_cls_prob_reshape,[1,rpn_cls_prob_reshape.shape[0],rpn_cls_prob_reshape.shape[1],rpn_cls_prob_reshape.shape[2]]),[0,3,2,1])\n    # rpn_bbox_pred = np.transpose(rpn_bbox_pred,[0,3,2,1])\n    im_info = im_info[0]\n\n    assert rpn_cls_prob_reshape.shape[0] == 1, \\\n        \'Only single item batches are supported\'\n    # cfg_key = str(self.phase) # either \'TRAIN\' or \'TEST\'\n    # cfg_key = \'TEST\'\n    pre_nms_topN = cfg[cfg_key].RPN_PRE_NMS_TOP_N\n    post_nms_topN = cfg[cfg_key].RPN_POST_NMS_TOP_N\n    nms_thresh = cfg[cfg_key].RPN_NMS_THRESH\n    min_size = cfg[cfg_key].RPN_MIN_SIZE\n\n    # the first set of _num_anchors channels are bg probs\n    # the second set are the fg probs, which we want\n    scores = rpn_cls_prob_reshape[:, _num_anchors:, :, :]\n    bbox_deltas = rpn_bbox_pred\n    # im_info = bottom[2].data[0, :]\n\n    if DEBUG:\n        print \'im_size: ({}, {})\'.format(im_info[0], im_info[1])\n        print \'scale: {}\'.format(im_info[2])\n\n    # 1. Generate proposals from bbox deltas and shifted anchors\n    height, width = scores.shape[-2:]\n\n    if DEBUG:\n        print \'score map size: {}\'.format(scores.shape)\n\n    # Enumerate all shifts\n    shift_x = np.arange(0, width) * _feat_stride\n    shift_y = np.arange(0, height) * _feat_stride\n    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n    shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),\n                        shift_x.ravel(), shift_y.ravel())).transpose()\n\n    # Enumerate all shifted anchors:\n    #\n    # add A anchors (1, A, 4) to\n    # cell K shifts (K, 1, 4) to get\n    # shift anchors (K, A, 4)\n    # reshape to (K*A, 4) shifted anchors\n    A = _num_anchors\n    K = shifts.shape[0]\n    anchors = _anchors.reshape((1, A, 4)) + \\\n              shifts.reshape((1, K, 4)).transpose((1, 0, 2))\n    anchors = anchors.reshape((K * A, 4))\n\n    # Transpose and reshape predicted bbox transformations to get them\n    # into the same order as the anchors:\n    #\n    # bbox deltas will be (1, 4 * A, H, W) format\n    # transpose to (1, H, W, 4 * A)\n    # reshape to (1 * H * W * A, 4) where rows are ordered by (h, w, a)\n    # in slowest to fastest order\n    bbox_deltas = bbox_deltas.transpose((0, 2, 3, 1)).reshape((-1, 4))\n\n    # Same story for the scores:\n    #\n    # scores are (1, A, H, W) format\n    # transpose to (1, H, W, A)\n    # reshape to (1 * H * W * A, 1) where rows are ordered by (h, w, a)\n    scores = scores.transpose((0, 2, 3, 1)).reshape((-1, 1))\n\n    # Convert anchors into proposals via bbox transformations\n    proposals = bbox_transform_inv(anchors, bbox_deltas)\n\n    # 2. clip predicted boxes to image\n    proposals = clip_boxes(proposals, im_info[:2])\n\n    # 3. remove predicted boxes with either height or width < threshold\n    # (NOTE: convert min_size to input image scale stored in im_info[2])\n    keep = _filter_boxes(proposals, min_size * im_info[2])\n    proposals = proposals[keep, :]\n    scores = scores[keep]\n\n    # # remove irregular boxes, too fat too tall\n    # keep = _filter_irregular_boxes(proposals)\n    # proposals = proposals[keep, :]\n    # scores = scores[keep]\n\n    # 4. sort all (proposal, score) pairs by score from highest to lowest\n    # 5. take top pre_nms_topN (e.g. 6000)\n    order = scores.ravel().argsort()[::-1]\n    if pre_nms_topN > 0:\n        order = order[:pre_nms_topN]\n    proposals = proposals[order, :]\n    scores = scores[order]\n\n    # 6. apply nms (e.g. threshold = 0.7)\n    # 7. take after_nms_topN (e.g. 300)\n    # 8. return the top proposals (-> RoIs top)\n    keep = nms(np.hstack((proposals, scores)), nms_thresh)\n    if post_nms_topN > 0:\n        keep = keep[:post_nms_topN]\n    proposals = proposals[keep, :]\n    scores = scores[keep]\n    # Output rois blob\n    # Our RPN implementation only supports a single input image, so all\n    # batch inds are 0\n    batch_inds = np.zeros((proposals.shape[0], 1), dtype=np.float32)\n    blob = np.hstack((batch_inds, proposals.astype(np.float32, copy=False)))\n    return blob\n    # top[0].reshape(*(blob.shape))\n    # top[0].data[...] = blob\n\n    # [Optional] output scores blob\n    # if len(top) > 1:\n    #    top[1].reshape(*(scores.shape))\n    #    top[1].data[...] = scores\n\n\ndef _filter_boxes(boxes, min_size):\n    """"""Remove all boxes with any side smaller than min_size.""""""\n    ws = boxes[:, 2] - boxes[:, 0] + 1\n    hs = boxes[:, 3] - boxes[:, 1] + 1\n    keep = np.where((ws >= min_size) & (hs >= min_size))[0]\n    return keep\n\n\ndef _filter_irregular_boxes(boxes, min_ratio=0.2, max_ratio=5):\n    """"""Remove all boxes with any side smaller than min_size.""""""\n    ws = boxes[:, 2] - boxes[:, 0] + 1\n    hs = boxes[:, 3] - boxes[:, 1] + 1\n    rs = ws / hs\n    keep = np.where((rs <= max_ratio) & (rs >= min_ratio))[0]\n    return keep\n'"
faster_rcnn/rpn_msr/proposal_target_layer.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Sean Bell\n# --------------------------------------------------------\n\nimport yaml\nimport numpy as np\nimport numpy.random as npr\nimport pdb\n\nfrom ..utils.cython_bbox import bbox_overlaps, bbox_intersections\n\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\nfrom ..fast_rcnn.bbox_transform import bbox_transform\n\n# <<<< obsolete\n\nDEBUG = False\n\n\ndef proposal_target_layer(rpn_rois, gt_boxes, gt_ishard, dontcare_areas, _num_classes):\n    """"""\n    Assign object detection proposals to ground-truth targets. Produces proposal\n    classification labels and bounding-box regression targets.\n    Parameters\n    ----------\n    rpn_rois:  (1 x H x W x A, 5) [0, x1, y1, x2, y2]\n    gt_boxes: (G, 5) [x1 ,y1 ,x2, y2, class] int\n    gt_ishard: (G, 1) {0 | 1} 1 indicates hard\n    dontcare_areas: (D, 4) [ x1, y1, x2, y2]\n    _num_classes\n    ----------\n    Returns\n    ----------\n    rois: (1 x H x W x A, 5) [0, x1, y1, x2, y2]\n    labels: (1 x H x W x A, 1) {0,1,...,_num_classes-1}\n    bbox_targets: (1 x H x W x A, K x4) [dx1, dy1, dx2, dy2]\n    bbox_inside_weights: (1 x H x W x A, Kx4) 0, 1 masks for the computing loss\n    bbox_outside_weights: (1 x H x W x A, Kx4) 0, 1 masks for the computing loss\n    """"""\n\n    # Proposal ROIs (0, x1, y1, x2, y2) coming from RPN\n    # (i.e., rpn.proposal_layer.ProposalLayer), or any other source\n    all_rois = rpn_rois\n    # TODO(rbg): it\'s annoying that sometimes I have extra info before\n    # and other times after box coordinates -- normalize to one format\n\n    # Include ground-truth boxes in the set of candidate rois\n    if cfg.TRAIN.PRECLUDE_HARD_SAMPLES and gt_ishard is not None and gt_ishard.shape[0] > 0:\n        assert gt_ishard.shape[0] == gt_boxes.shape[0]\n        gt_ishard = gt_ishard.astype(int)\n        gt_easyboxes = gt_boxes[gt_ishard != 1, :]\n    else:\n        gt_easyboxes = gt_boxes\n\n    """"""\n    add the ground-truth to rois will cause zero loss! not good for visuallization\n    """"""\n    jittered_gt_boxes = _jitter_gt_boxes(gt_easyboxes)\n    zeros = np.zeros((gt_easyboxes.shape[0] * 2, 1), dtype=gt_easyboxes.dtype)\n    all_rois = np.vstack((all_rois, \\\n                          np.hstack((zeros, np.vstack((gt_easyboxes[:, :-1], jittered_gt_boxes[:, :-1]))))))\n\n    # Sanity check: single batch only\n    assert np.all(all_rois[:, 0] == 0), \\\n        \'Only single item batches are supported\'\n\n    num_images = 1\n    rois_per_image = cfg.TRAIN.BATCH_SIZE / num_images\n    fg_rois_per_image = int(np.round(cfg.TRAIN.FG_FRACTION * rois_per_image))\n\n    # Sample rois with classification labels and bounding box regression\n    # targets\n    labels, rois, bbox_targets, bbox_inside_weights = _sample_rois(\n        all_rois, gt_boxes, gt_ishard, dontcare_areas, fg_rois_per_image,\n        rois_per_image, _num_classes)\n\n    _count = 1\n    if DEBUG:\n        if _count == 1:\n            _fg_num, _bg_num = 0, 0\n        print \'num fg: {}\'.format((labels > 0).sum())\n        print \'num bg: {}\'.format((labels == 0).sum())\n        _count += 1\n        _fg_num += (labels > 0).sum()\n        _bg_num += (labels == 0).sum()\n        print \'num fg avg: {}\'.format(_fg_num / _count)\n        print \'num bg avg: {}\'.format(_bg_num / _count)\n        print \'ratio: {:.3f}\'.format(float(_fg_num) / float(_bg_num))\n\n    rois = rois.reshape(-1, 5)\n    labels = labels.reshape(-1, 1)\n    bbox_targets = bbox_targets.reshape(-1, _num_classes * 4)\n    bbox_inside_weights = bbox_inside_weights.reshape(-1, _num_classes * 4)\n\n    bbox_outside_weights = np.array(bbox_inside_weights > 0).astype(np.float32)\n\n    return rois, labels, bbox_targets, bbox_inside_weights, bbox_outside_weights\n\n\ndef _sample_rois(all_rois, gt_boxes, gt_ishard, dontcare_areas, fg_rois_per_image, rois_per_image, num_classes):\n    """"""Generate a random sample of RoIs comprising foreground and background\n    examples.\n    """"""\n    # overlaps: R x G\n    overlaps = bbox_overlaps(\n        np.ascontiguousarray(all_rois[:, 1:5], dtype=np.float),\n        np.ascontiguousarray(gt_boxes[:, :4], dtype=np.float))\n    gt_assignment = overlaps.argmax(axis=1)  # R\n    max_overlaps = overlaps.max(axis=1)  # R\n    labels = gt_boxes[gt_assignment, 4]\n\n    # preclude hard samples\n    ignore_inds = np.empty(shape=(0), dtype=int)\n    if cfg.TRAIN.PRECLUDE_HARD_SAMPLES and gt_ishard is not None and gt_ishard.shape[0] > 0:\n        gt_ishard = gt_ishard.astype(int)\n        gt_hardboxes = gt_boxes[gt_ishard == 1, :]\n        if gt_hardboxes.shape[0] > 0:\n            # R x H\n            hard_overlaps = bbox_overlaps(\n                np.ascontiguousarray(all_rois[:, 1:5], dtype=np.float),\n                np.ascontiguousarray(gt_hardboxes[:, :4], dtype=np.float))\n            hard_max_overlaps = hard_overlaps.max(axis=1)  # R x 1\n            # hard_gt_assignment = hard_overlaps.argmax(axis=0)  # H\n            ignore_inds = np.append(ignore_inds,\n                                    np.where(hard_max_overlaps >= cfg.TRAIN.FG_THRESH)[0])\n            if DEBUG:\n                if ignore_inds.size > 1:\n                    print \'num hard: {:d}:\'.format(ignore_inds.size)\n                    print \'hard box:\', gt_hardboxes\n                    print \'rois: \'\n                    print all_rois[ignore_inds]\n\n    # preclude dontcare areas\n    if dontcare_areas is not None and dontcare_areas.shape[0] > 0:\n        # intersec shape is D x R\n        intersecs = bbox_intersections(\n            np.ascontiguousarray(dontcare_areas, dtype=np.float),  # D x 4\n            np.ascontiguousarray(all_rois[:, 1:5], dtype=np.float)  # R x 4\n        )\n        intersecs_sum = intersecs.sum(axis=0)  # R x 1\n        ignore_inds = np.append(ignore_inds,\n                                np.where(intersecs_sum > cfg.TRAIN.DONTCARE_AREA_INTERSECTION_HI)[0])\n        # if ignore_inds.size >= 1:\n        #     print \'num dontcare: {:d}:\'.format(ignore_inds.size)\n        #     print \'dontcare box:\', dontcare_areas.astype(int)\n        #     print \'rois: \'\n        #     print all_rois[ignore_inds].astype(int)\n\n    # Select foreground RoIs as those with >= FG_THRESH overlap\n    fg_inds = np.where(max_overlaps >= cfg.TRAIN.FG_THRESH)[0]\n    fg_inds = np.setdiff1d(fg_inds, ignore_inds)\n    # Guard against the case when an image has fewer than fg_rois_per_image\n    # foreground RoIs\n    fg_rois_per_this_image = min(fg_rois_per_image, fg_inds.size)\n    # Sample foreground regions without replacement\n    if fg_inds.size > 0:\n        fg_inds = npr.choice(fg_inds, size=fg_rois_per_this_image, replace=False)\n\n    # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n    bg_inds = np.where((max_overlaps < cfg.TRAIN.BG_THRESH_HI) &\n                       (max_overlaps >= cfg.TRAIN.BG_THRESH_LO))[0]\n    bg_inds = np.setdiff1d(bg_inds, ignore_inds)\n    # Compute number of background RoIs to take from this image (guarding\n    # against there being fewer than desired)\n    bg_rois_per_this_image = rois_per_image - fg_rois_per_this_image\n    bg_rois_per_this_image = min(bg_rois_per_this_image, bg_inds.size)\n    # Sample background regions without replacement\n    if bg_inds.size > 0:\n        bg_inds = npr.choice(bg_inds, size=bg_rois_per_this_image, replace=False)\n\n    # The indices that we\'re selecting (both fg and bg)\n    keep_inds = np.append(fg_inds, bg_inds)\n    # Select sampled values from various arrays:\n    labels = labels[keep_inds]\n    # Clamp labels for the background RoIs to 0\n    labels[fg_rois_per_this_image:] = 0\n    rois = all_rois[keep_inds]\n\n    bbox_target_data = _compute_targets(\n        rois[:, 1:5], gt_boxes[gt_assignment[keep_inds], :4], labels)\n\n    # bbox_target_data (1 x H x W x A, 5)\n    # bbox_targets <- (1 x H x W x A, K x 4)\n    # bbox_inside_weights <- (1 x H x W x A, K x 4)\n    bbox_targets, bbox_inside_weights = \\\n        _get_bbox_regression_labels(bbox_target_data, num_classes)\n\n    return labels, rois, bbox_targets, bbox_inside_weights\n\n\ndef _get_bbox_regression_labels(bbox_target_data, num_classes):\n    """"""Bounding-box regression targets (bbox_target_data) are stored in a\n    compact form N x (class, tx, ty, tw, th)\n\n    This function expands those targets into the 4-of-4*K representation used\n    by the network (i.e. only one class has non-zero targets).\n\n    Returns:\n        bbox_target (ndarray): N x 4K blob of regression targets\n        bbox_inside_weights (ndarray): N x 4K blob of loss weights\n    """"""\n\n    clss = bbox_target_data[:, 0]\n    bbox_targets = np.zeros((clss.size, 4 * num_classes), dtype=np.float32)\n    bbox_inside_weights = np.zeros(bbox_targets.shape, dtype=np.float32)\n    inds = np.where(clss > 0)[0]\n    for ind in inds:\n        cls = int(clss[ind])\n        start = 4 * cls\n        end = start + 4\n        bbox_targets[ind, start:end] = bbox_target_data[ind, 1:]\n        bbox_inside_weights[ind, start:end] = cfg.TRAIN.BBOX_INSIDE_WEIGHTS\n    return bbox_targets, bbox_inside_weights\n\n\ndef _compute_targets(ex_rois, gt_rois, labels):\n    """"""Compute bounding-box regression targets for an image.""""""\n\n    assert ex_rois.shape[0] == gt_rois.shape[0]\n    assert ex_rois.shape[1] == 4\n    assert gt_rois.shape[1] == 4\n\n    targets = bbox_transform(ex_rois, gt_rois)\n    if cfg.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED:\n        # Optionally normalize targets by a precomputed mean and stdev\n        targets = ((targets - np.array(cfg.TRAIN.BBOX_NORMALIZE_MEANS))\n                   / np.array(cfg.TRAIN.BBOX_NORMALIZE_STDS))\n    return np.hstack(\n        (labels[:, np.newaxis], targets)).astype(np.float32, copy=False)\n\n\ndef _jitter_gt_boxes(gt_boxes, jitter=0.05):\n    """""" jitter the gtboxes, before adding them into rois, to be more robust for cls and rgs\n    gt_boxes: (G, 5) [x1 ,y1 ,x2, y2, class] int\n    """"""\n    jittered_boxes = gt_boxes.copy()\n    ws = jittered_boxes[:, 2] - jittered_boxes[:, 0] + 1.0\n    hs = jittered_boxes[:, 3] - jittered_boxes[:, 1] + 1.0\n    width_offset = (np.random.rand(jittered_boxes.shape[0]) - 0.5) * jitter * ws\n    height_offset = (np.random.rand(jittered_boxes.shape[0]) - 0.5) * jitter * hs\n    jittered_boxes[:, 0] += width_offset\n    jittered_boxes[:, 2] += width_offset\n    jittered_boxes[:, 1] += height_offset\n    jittered_boxes[:, 3] += height_offset\n\n    return jittered_boxes\n'"
faster_rcnn/utils/__init__.py,0,b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\nfrom . import cython_nms\nfrom . import cython_bbox\nimport blob\nimport nms\nimport timer'
faster_rcnn/utils/blob.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Blob helper functions.""""""\n\nimport numpy as np\nimport cv2\n\ndef im_list_to_blob(ims):\n    """"""Convert a list of images into a network input.\n\n    Assumes images are already prepared (means subtracted, BGR order, ...).\n    """"""\n    max_shape = np.array([im.shape for im in ims]).max(axis=0)\n    num_images = len(ims)\n    blob = np.zeros((num_images, max_shape[0], max_shape[1], 3),\n                    dtype=np.float32)\n    for i in xrange(num_images):\n        im = ims[i]\n        blob[i, 0:im.shape[0], 0:im.shape[1], :] = im\n\n    return blob\n\ndef prep_im_for_blob(im, pixel_means, target_size, max_size):\n    """"""Mean subtract and scale an image for use in a blob.""""""\n    im = im.astype(np.float32, copy=False)\n    im -= pixel_means\n    im_shape = im.shape\n    im_size_min = np.min(im_shape[0:2])\n    im_size_max = np.max(im_shape[0:2])\n    im_scale = float(target_size) / float(im_size_min)\n    # Prevent the biggest axis from being more than MAX_SIZE\n    if np.round(im_scale * im_size_max) > max_size:\n        im_scale = float(max_size) / float(im_size_max)\n    im = cv2.resize(im, None, None, fx=im_scale, fy=im_scale,\n                    interpolation=cv2.INTER_LINEAR)\n\n    return im, im_scale\n'"
faster_rcnn/utils/boxes_grid.py,0,"b'# --------------------------------------------------------\n# Subcategory CNN\n# Copyright (c) 2015 CVGL Stanford\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Yu Xiang\n# --------------------------------------------------------\n\nimport numpy as np\nimport math\n# TODO: make fast_rcnn irrelevant\n# >>>> obsolete, because it depends on sth outside of this project\nfrom ..fast_rcnn.config import cfg\n# <<<< obsolete\n\ndef get_boxes_grid(image_height, image_width):\n    """"""\n    Return the boxes on image grid.\n    calling this function when cfg.IS_MULTISCALE is True, otherwise, calling rdl_roidb.prepare_roidb(imdb) instead.\n    """"""\n\n    # fixed a bug, change cfg.TRAIN.SCALES to cfg.TRAIN.SCALES_BASE\n    # coz, here needs a ratio around 1.0, not the accutual size.\n    # height and width of the feature map\n    if cfg.NET_NAME == \'CaffeNet\':\n        height = np.floor((image_height * max(cfg.TRAIN.SCALES_BASE) - 1) / 4.0 + 1)\n        height = np.floor((height - 1) / 2.0 + 1 + 0.5)\n        height = np.floor((height - 1) / 2.0 + 1 + 0.5)\n\n        width = np.floor((image_width * max(cfg.TRAIN.SCALES_BASE) - 1) / 4.0 + 1)\n        width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n        width = np.floor((width - 1) / 2.0 + 1 + 0.5)\n    elif cfg.NET_NAME == \'VGGnet\':\n        height = np.floor(image_height * max(cfg.TRAIN.SCALES_BASE) / 2.0 + 0.5)\n        height = np.floor(height / 2.0 + 0.5)\n        height = np.floor(height / 2.0 + 0.5)\n        height = np.floor(height / 2.0 + 0.5)\n\n        width = np.floor(image_width * max(cfg.TRAIN.SCALES_BASE) / 2.0 + 0.5)\n        width = np.floor(width / 2.0 + 0.5)\n        width = np.floor(width / 2.0 + 0.5)\n        width = np.floor(width / 2.0 + 0.5)\n    else:\n        assert (1), \'The network architecture is not supported in utils.get_boxes_grid!\'\n\n    # compute the grid box centers\n    h = np.arange(height)\n    w = np.arange(width)\n    y, x = np.meshgrid(h, w, indexing=\'ij\') \n    centers = np.dstack((x, y))\n    centers = np.reshape(centers, (-1, 2))\n    num = centers.shape[0]\n\n    # compute width and height of grid box\n    area = cfg.TRAIN.KERNEL_SIZE * cfg.TRAIN.KERNEL_SIZE\n    aspect = cfg.TRAIN.ASPECTS  # height / width\n    num_aspect = len(aspect)\n    widths = np.zeros((1, num_aspect), dtype=np.float32)\n    heights = np.zeros((1, num_aspect), dtype=np.float32)\n    for i in xrange(num_aspect):\n        widths[0,i] = math.sqrt(area / aspect[i])\n        heights[0,i] = widths[0,i] * aspect[i]\n\n    # construct grid boxes\n    centers = np.repeat(centers, num_aspect, axis=0)\n    widths = np.tile(widths, num).transpose()\n    heights = np.tile(heights, num).transpose()\n\n    x1 = np.reshape(centers[:,0], (-1, 1)) - widths * 0.5\n    x2 = np.reshape(centers[:,0], (-1, 1)) + widths * 0.5\n    y1 = np.reshape(centers[:,1], (-1, 1)) - heights * 0.5\n    y2 = np.reshape(centers[:,1], (-1, 1)) + heights * 0.5\n    \n    boxes_grid = np.hstack((x1, y1, x2, y2)) / cfg.TRAIN.SPATIAL_SCALE\n\n    return boxes_grid, centers[:,0], centers[:,1]\n'"
faster_rcnn/utils/nms.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport numpy as np\n\ndef nms(dets, thresh):\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    x2 = dets[:, 2]\n    y2 = dets[:, 3]\n    scores = dets[:, 4]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr <= thresh)[0]\n        order = order[inds + 1]\n\n    return keep\n'"
faster_rcnn/utils/timer.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport time\n\nclass Timer(object):\n    """"""A simple timer.""""""\n    def __init__(self):\n        self.total_time = 0.\n        self.calls = 0\n        self.start_time = 0.\n        self.diff = 0.\n        self.average_time = 0.\n\n    def tic(self):\n        # using time.time instead of time.clock because time time.clock\n        # does not normalize for multithreading\n        self.start_time = time.time()\n\n    def toc(self, average=True):\n        self.diff = time.time() - self.start_time\n        self.total_time += self.diff\n        self.calls += 1\n        self.average_time = self.total_time / self.calls\n        if average:\n            return self.average_time\n        else:\n            return self.diff\n'"
faster_rcnn/roi_pooling/_ext/__init__.py,0,b''
faster_rcnn/roi_pooling/functions/__init__.py,0,b''
faster_rcnn/roi_pooling/functions/roi_pool.py,4,"b'import torch\nfrom torch.autograd import Function\nfrom .._ext import roi_pooling\n\n\nclass RoIPoolFunction(Function):\n    def __init__(self, pooled_height, pooled_width, spatial_scale):\n        self.pooled_width = int(pooled_width)\n        self.pooled_height = int(pooled_height)\n        self.spatial_scale = float(spatial_scale)\n        self.output = None\n        self.argmax = None\n        self.rois = None\n        self.feature_size = None\n\n    def forward(self, features, rois):\n        batch_size, num_channels, data_height, data_width = features.size()\n        num_rois = rois.size()[0]\n        output = torch.zeros(num_rois, num_channels, self.pooled_height, self.pooled_width)\n        argmax = torch.IntTensor(num_rois, num_channels, self.pooled_height, self.pooled_width).zero_()\n\n        if not features.is_cuda:\n            _features = features.permute(0, 2, 3, 1)\n            roi_pooling.roi_pooling_forward(self.pooled_height, self.pooled_width, self.spatial_scale,\n                                            _features, rois, output)\n            # output = output.cuda()\n        else:\n            output = output.cuda()\n            argmax = argmax.cuda()\n            roi_pooling.roi_pooling_forward_cuda(self.pooled_height, self.pooled_width, self.spatial_scale,\n                                                 features, rois, output, argmax)\n            self.output = output\n            self.argmax = argmax\n            self.rois = rois\n            self.feature_size = features.size()\n\n        return output\n\n    def backward(self, grad_output):\n        assert(self.feature_size is not None and grad_output.is_cuda)\n\n        batch_size, num_channels, data_height, data_width = self.feature_size\n\n        grad_input = torch.zeros(batch_size, num_channels, data_height, data_width).cuda()\n        roi_pooling.roi_pooling_backward_cuda(self.pooled_height, self.pooled_width, self.spatial_scale,\n                                              grad_output, self.rois, grad_input, self.argmax)\n\n        # print grad_input\n\n        return grad_input, None\n'"
faster_rcnn/roi_pooling/modules/__init__.py,0,b''
faster_rcnn/roi_pooling/modules/roi_pool.py,1,"b'from torch.nn.modules.module import Module\nfrom ..functions.roi_pool import RoIPoolFunction\n\n\nclass RoIPool(Module):\n    def __init__(self, pooled_height, pooled_width, spatial_scale):\n        super(RoIPool, self).__init__()\n\n        self.pooled_width = int(pooled_width)\n        self.pooled_height = int(pooled_height)\n        self.spatial_scale = float(spatial_scale)\n\n    def forward(self, features, rois):\n        return RoIPoolFunction(self.pooled_height, self.pooled_width, self.spatial_scale)(features, rois)\n'"
faster_rcnn/roi_pooling/modules/roi_pool_py.py,5,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np\n\n\nclass RoIPool(nn.Module):\n    def __init__(self, pooled_height, pooled_width, spatial_scale):\n        super(RoIPool, self).__init__()\n        self.pooled_width = int(pooled_width)\n        self.pooled_height = int(pooled_height)\n        self.spatial_scale = float(spatial_scale)\n\n    def forward(self, features, rois):\n        batch_size, num_channels, data_height, data_width = features.size()\n        num_rois = rois.size()[0]\n        outputs = Variable(torch.zeros(num_rois, num_channels, self.pooled_height, self.pooled_width)).cuda()\n\n        for roi_ind, roi in enumerate(rois):\n            batch_ind = int(roi[0].data[0])\n            roi_start_w, roi_start_h, roi_end_w, roi_end_h = np.round(\n                roi[1:].data.cpu().numpy() * self.spatial_scale).astype(int)\n            roi_width = max(roi_end_w - roi_start_w + 1, 1)\n            roi_height = max(roi_end_h - roi_start_h + 1, 1)\n            bin_size_w = float(roi_width) / float(self.pooled_width)\n            bin_size_h = float(roi_height) / float(self.pooled_height)\n\n            for ph in range(self.pooled_height):\n                hstart = int(np.floor(ph * bin_size_h))\n                hend = int(np.ceil((ph + 1) * bin_size_h))\n                hstart = min(data_height, max(0, hstart + roi_start_h))\n                hend = min(data_height, max(0, hend + roi_start_h))\n                for pw in range(self.pooled_width):\n                    wstart = int(np.floor(pw * bin_size_w))\n                    wend = int(np.ceil((pw + 1) * bin_size_w))\n                    wstart = min(data_width, max(0, wstart + roi_start_w))\n                    wend = min(data_width, max(0, wend + roi_start_w))\n\n                    is_empty = (hend <= hstart) or(wend <= wstart)\n                    if is_empty:\n                        outputs[roi_ind, :, ph, pw] = 0\n                    else:\n                        data = features[batch_ind]\n                        outputs[roi_ind, :, ph, pw] = torch.max(\n                            torch.max(data[:, hstart:hend, wstart:wend], 1)[0], 2)[0].view(-1)\n\n        return outputs\n\n'"
faster_rcnn/roi_pooling/_ext/roi_pooling/__init__.py,1,"b'\nfrom torch.utils.ffi import _wrap_function\nfrom ._roi_pooling import lib as _lib, ffi as _ffi\n\n__all__ = []\ndef _import_symbols(locals):\n    for symbol in dir(_lib):\n        fn = getattr(_lib, symbol)\n        locals[symbol] = _wrap_function(fn, _ffi)\n        __all__.append(symbol)\n\n_import_symbols(locals())\n'"
