file_path,api_count,code
darknet.py,10,"b'import torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport utils.network as net_utils\nimport cfgs.config as cfg\nfrom layers.reorg.reorg_layer import ReorgLayer\nfrom utils.cython_bbox import bbox_ious, anchor_intersections\nfrom utils.cython_yolo import yolo_to_bbox\nfrom functools import partial\n\nfrom multiprocessing import Pool\n\n\ndef _make_layers(in_channels, net_cfg):\n    layers = []\n\n    if len(net_cfg) > 0 and isinstance(net_cfg[0], list):\n        for sub_cfg in net_cfg:\n            layer, in_channels = _make_layers(in_channels, sub_cfg)\n            layers.append(layer)\n    else:\n        for item in net_cfg:\n            if item == \'M\':\n                layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n            else:\n                out_channels, ksize = item\n                layers.append(net_utils.Conv2d_BatchNorm(in_channels,\n                                                         out_channels,\n                                                         ksize,\n                                                         same_padding=True))\n                # layers.append(net_utils.Conv2d(in_channels, out_channels,\n                #     ksize, same_padding=True))\n                in_channels = out_channels\n\n    return nn.Sequential(*layers), in_channels\n\n\ndef _process_batch(data, size_index):\n    W, H = cfg.multi_scale_out_size[size_index]\n    inp_size = cfg.multi_scale_inp_size[size_index]\n    out_size = cfg.multi_scale_out_size[size_index]\n\n    bbox_pred_np, gt_boxes, gt_classes, dontcares, iou_pred_np = data\n\n    # net output\n    hw, num_anchors, _ = bbox_pred_np.shape\n\n    # gt\n    _classes = np.zeros([hw, num_anchors, cfg.num_classes], dtype=np.float)\n    _class_mask = np.zeros([hw, num_anchors, 1], dtype=np.float)\n\n    _ious = np.zeros([hw, num_anchors, 1], dtype=np.float)\n    _iou_mask = np.zeros([hw, num_anchors, 1], dtype=np.float)\n\n    _boxes = np.zeros([hw, num_anchors, 4], dtype=np.float)\n    _boxes[:, :, 0:2] = 0.5\n    _boxes[:, :, 2:4] = 1.0\n    _box_mask = np.zeros([hw, num_anchors, 1], dtype=np.float) + 0.01\n\n    # scale pred_bbox\n    anchors = np.ascontiguousarray(cfg.anchors, dtype=np.float)\n    bbox_pred_np = np.expand_dims(bbox_pred_np, 0)\n    bbox_np = yolo_to_bbox(\n        np.ascontiguousarray(bbox_pred_np, dtype=np.float),\n        anchors,\n        H, W)\n    # bbox_np = (hw, num_anchors, (x1, y1, x2, y2))   range: 0 ~ 1\n    bbox_np = bbox_np[0]\n    bbox_np[:, :, 0::2] *= float(inp_size[0])  # rescale x\n    bbox_np[:, :, 1::2] *= float(inp_size[1])  # rescale y\n\n    # gt_boxes_b = np.asarray(gt_boxes[b], dtype=np.float)\n    gt_boxes_b = np.asarray(gt_boxes, dtype=np.float)\n\n    # for each cell, compare predicted_bbox and gt_bbox\n    bbox_np_b = np.reshape(bbox_np, [-1, 4])\n    ious = bbox_ious(\n        np.ascontiguousarray(bbox_np_b, dtype=np.float),\n        np.ascontiguousarray(gt_boxes_b, dtype=np.float)\n    )\n    best_ious = np.max(ious, axis=1).reshape(_iou_mask.shape)\n    iou_penalty = 0 - iou_pred_np[best_ious < cfg.iou_thresh]\n    _iou_mask[best_ious <= cfg.iou_thresh] = cfg.noobject_scale * iou_penalty\n\n    # locate the cell of each gt_boxe\n    cell_w = float(inp_size[0]) / W\n    cell_h = float(inp_size[1]) / H\n    cx = (gt_boxes_b[:, 0] + gt_boxes_b[:, 2]) * 0.5 / cell_w\n    cy = (gt_boxes_b[:, 1] + gt_boxes_b[:, 3]) * 0.5 / cell_h\n    cell_inds = np.floor(cy) * W + np.floor(cx)\n    cell_inds = cell_inds.astype(np.int)\n\n    target_boxes = np.empty(gt_boxes_b.shape, dtype=np.float)\n    target_boxes[:, 0] = cx - np.floor(cx)  # cx\n    target_boxes[:, 1] = cy - np.floor(cy)  # cy\n    target_boxes[:, 2] = \\\n        (gt_boxes_b[:, 2] - gt_boxes_b[:, 0]) / inp_size[0] * out_size[0]  # tw\n    target_boxes[:, 3] = \\\n        (gt_boxes_b[:, 3] - gt_boxes_b[:, 1]) / inp_size[1] * out_size[1]  # th\n\n    # for each gt boxes, match the best anchor\n    gt_boxes_resize = np.copy(gt_boxes_b)\n    gt_boxes_resize[:, 0::2] *= (out_size[0] / float(inp_size[0]))\n    gt_boxes_resize[:, 1::2] *= (out_size[1] / float(inp_size[1]))\n    anchor_ious = anchor_intersections(\n        anchors,\n        np.ascontiguousarray(gt_boxes_resize, dtype=np.float)\n    )\n    anchor_inds = np.argmax(anchor_ious, axis=0)\n\n    ious_reshaped = np.reshape(ious, [hw, num_anchors, len(cell_inds)])\n    for i, cell_ind in enumerate(cell_inds):\n        if cell_ind >= hw or cell_ind < 0:\n            print(\'cell inds size {}\'.format(len(cell_inds)))\n            print(\'cell over {} hw {}\'.format(cell_ind, hw))\n            continue\n        a = anchor_inds[i]\n\n        # 0 ~ 1, should be close to 1\n        iou_pred_cell_anchor = iou_pred_np[cell_ind, a, :]\n        _iou_mask[cell_ind, a, :] = cfg.object_scale * (1 - iou_pred_cell_anchor)  # noqa\n        # _ious[cell_ind, a, :] = anchor_ious[a, i]\n        _ious[cell_ind, a, :] = ious_reshaped[cell_ind, a, i]\n\n        _box_mask[cell_ind, a, :] = cfg.coord_scale\n        target_boxes[i, 2:4] /= anchors[a]\n        _boxes[cell_ind, a, :] = target_boxes[i]\n\n        _class_mask[cell_ind, a, :] = cfg.class_scale\n        _classes[cell_ind, a, gt_classes[i]] = 1.\n\n    # _boxes[:, :, 2:4] = np.maximum(_boxes[:, :, 2:4], 0.001)\n    # _boxes[:, :, 2:4] = np.log(_boxes[:, :, 2:4])\n\n    return _boxes, _ious, _classes, _box_mask, _iou_mask, _class_mask\n\n\nclass Darknet19(nn.Module):\n    def __init__(self):\n        super(Darknet19, self).__init__()\n\n        net_cfgs = [\n            # conv1s\n            [(32, 3)],\n            [\'M\', (64, 3)],\n            [\'M\', (128, 3), (64, 1), (128, 3)],\n            [\'M\', (256, 3), (128, 1), (256, 3)],\n            [\'M\', (512, 3), (256, 1), (512, 3), (256, 1), (512, 3)],\n            # conv2\n            [\'M\', (1024, 3), (512, 1), (1024, 3), (512, 1), (1024, 3)],\n            # ------------\n            # conv3\n            [(1024, 3), (1024, 3)],\n            # conv4\n            [(1024, 3)]\n        ]\n\n        # darknet\n        self.conv1s, c1 = _make_layers(3, net_cfgs[0:5])\n        self.conv2, c2 = _make_layers(c1, net_cfgs[5])\n        # ---\n        self.conv3, c3 = _make_layers(c2, net_cfgs[6])\n\n        stride = 2\n        # stride*stride times the channels of conv1s\n        self.reorg = ReorgLayer(stride=2)\n        # cat [conv1s, conv3]\n        self.conv4, c4 = _make_layers((c1*(stride*stride) + c3), net_cfgs[7])\n\n        # linear\n        out_channels = cfg.num_anchors * (cfg.num_classes + 5)\n        self.conv5 = net_utils.Conv2d(c4, out_channels, 1, 1, relu=False)\n        self.global_average_pool = nn.AvgPool2d((1, 1))\n\n        # train\n        self.bbox_loss = None\n        self.iou_loss = None\n        self.cls_loss = None\n        self.pool = Pool(processes=10)\n\n    @property\n    def loss(self):\n        return self.bbox_loss + self.iou_loss + self.cls_loss\n\n    def forward(self, im_data, gt_boxes=None, gt_classes=None, dontcare=None,\n                size_index=0):\n        conv1s = self.conv1s(im_data)\n        conv2 = self.conv2(conv1s)\n        conv3 = self.conv3(conv2)\n        conv1s_reorg = self.reorg(conv1s)\n        cat_1_3 = torch.cat([conv1s_reorg, conv3], 1)\n        conv4 = self.conv4(cat_1_3)\n        conv5 = self.conv5(conv4)   # batch_size, out_channels, h, w\n        global_average_pool = self.global_average_pool(conv5)\n\n        # for detection\n        # bsize, c, h, w -> bsize, h, w, c ->\n        #                   bsize, h x w, num_anchors, 5+num_classes\n        bsize, _, h, w = global_average_pool.size()\n        # assert bsize == 1, \'detection only support one image per batch\'\n        global_average_pool_reshaped = \\\n            global_average_pool.permute(0, 2, 3, 1).contiguous().view(bsize,\n                                                                      -1, cfg.num_anchors, cfg.num_classes + 5)  # noqa\n\n        # tx, ty, tw, th, to -> sig(tx), sig(ty), exp(tw), exp(th), sig(to)\n        xy_pred = F.sigmoid(global_average_pool_reshaped[:, :, :, 0:2])\n        wh_pred = torch.exp(global_average_pool_reshaped[:, :, :, 2:4])\n        bbox_pred = torch.cat([xy_pred, wh_pred], 3)\n        iou_pred = F.sigmoid(global_average_pool_reshaped[:, :, :, 4:5])\n\n        score_pred = global_average_pool_reshaped[:, :, :, 5:].contiguous()\n        prob_pred = F.softmax(score_pred.view(-1, score_pred.size()[-1])).view_as(score_pred)  # noqa\n\n        # for training\n        if self.training:\n            bbox_pred_np = bbox_pred.data.cpu().numpy()\n            iou_pred_np = iou_pred.data.cpu().numpy()\n            _boxes, _ious, _classes, _box_mask, _iou_mask, _class_mask = \\\n                self._build_target(bbox_pred_np,\n                                   gt_boxes,\n                                   gt_classes,\n                                   dontcare,\n                                   iou_pred_np,\n                                   size_index)\n\n            _boxes = net_utils.np_to_variable(_boxes)\n            _ious = net_utils.np_to_variable(_ious)\n            _classes = net_utils.np_to_variable(_classes)\n            box_mask = net_utils.np_to_variable(_box_mask,\n                                                dtype=torch.FloatTensor)\n            iou_mask = net_utils.np_to_variable(_iou_mask,\n                                                dtype=torch.FloatTensor)\n            class_mask = net_utils.np_to_variable(_class_mask,\n                                                  dtype=torch.FloatTensor)\n\n            num_boxes = sum((len(boxes) for boxes in gt_boxes))\n\n            # _boxes[:, :, :, 2:4] = torch.log(_boxes[:, :, :, 2:4])\n            box_mask = box_mask.expand_as(_boxes)\n\n            self.bbox_loss = nn.MSELoss(size_average=False)(bbox_pred * box_mask, _boxes * box_mask) / num_boxes  # noqa\n            self.iou_loss = nn.MSELoss(size_average=False)(iou_pred * iou_mask, _ious * iou_mask) / num_boxes  # noqa\n\n            class_mask = class_mask.expand_as(prob_pred)\n            self.cls_loss = nn.MSELoss(size_average=False)(prob_pred * class_mask, _classes * class_mask) / num_boxes  # noqa\n\n        return bbox_pred, iou_pred, prob_pred\n\n    def _build_target(self, bbox_pred_np, gt_boxes, gt_classes, dontcare,\n                      iou_pred_np, size_index):\n        """"""\n        :param bbox_pred: shape: (bsize, h x w, num_anchors, 4) :\n                          (sig(tx), sig(ty), exp(tw), exp(th))\n        """"""\n\n        bsize = bbox_pred_np.shape[0]\n\n        targets = self.pool.map(partial(_process_batch, size_index=size_index),\n                                ((bbox_pred_np[b], gt_boxes[b],\n                                  gt_classes[b], dontcare[b], iou_pred_np[b])\n                                 for b in range(bsize)))\n\n        _boxes = np.stack(tuple((row[0] for row in targets)))\n        _ious = np.stack(tuple((row[1] for row in targets)))\n        _classes = np.stack(tuple((row[2] for row in targets)))\n        _box_mask = np.stack(tuple((row[3] for row in targets)))\n        _iou_mask = np.stack(tuple((row[4] for row in targets)))\n        _class_mask = np.stack(tuple((row[5] for row in targets)))\n\n        return _boxes, _ious, _classes, _box_mask, _iou_mask, _class_mask\n\n    def load_from_npz(self, fname, num_conv=None):\n        dest_src = {\'conv.weight\': \'kernel\', \'conv.bias\': \'biases\',\n                    \'bn.weight\': \'gamma\', \'bn.bias\': \'biases\',\n                    \'bn.running_mean\': \'moving_mean\',\n                    \'bn.running_var\': \'moving_variance\'}\n        params = np.load(fname)\n        own_dict = self.state_dict()\n        keys = list(own_dict.keys())\n\n        for i, start in enumerate(range(0, len(keys), 5)):\n            if num_conv is not None and i >= num_conv:\n                break\n            end = min(start+5, len(keys))\n            for key in keys[start:end]:\n                list_key = key.split(\'.\')\n                ptype = dest_src[\'{}.{}\'.format(list_key[-2], list_key[-1])]\n                src_key = \'{}-convolutional/{}:0\'.format(i, ptype)\n                print((src_key, own_dict[key].size(), params[src_key].shape))\n                param = torch.from_numpy(params[src_key])\n                if ptype == \'kernel\':\n                    param = param.permute(3, 2, 0, 1)\n                own_dict[key].copy_(param)\n\n\nif __name__ == \'__main__\':\n    net = Darknet19()\n    # net.load_from_npz(\'models/yolo-voc.weights.npz\')\n    net.load_from_npz(\'models/darknet19.weights.npz\', num_conv=18)\n'"
demo.py,1,"b""import os\nimport cv2\nimport numpy as np\nfrom torch.multiprocessing import Pool\n\nfrom darknet import Darknet19\nimport utils.yolo as yolo_utils\nimport utils.network as net_utils\nfrom utils.timer import Timer\nimport cfgs.config as cfg\n\n# This prevents deadlocks in the data loader, caused by\n# some incompatibility between pytorch and cv2 multiprocessing.\n# See https://github.com/pytorch/pytorch/issues/1355.\ncv2.setNumThreads(0)\n\n\ndef preprocess(fname):\n    # return fname\n    image = cv2.imread(fname)\n    im_data = np.expand_dims(\n        yolo_utils.preprocess_test((image, None, cfg.multi_scale_inp_size), 0)[0], 0)\n    return image, im_data\n\n\n# hyper-parameters\n# npz_fname = 'models/yolo-voc.weights.npz'\n# h5_fname = 'models/yolo-voc.weights.h5'\ntrained_model = cfg.trained_model\n# trained_model = os.path.join(\n#     cfg.train_output_dir, 'darknet19_voc07trainval_exp3_158.h5')\nthresh = 0.5\nim_path = 'demo'\n# ---\n\nnet = Darknet19()\nnet_utils.load_net(trained_model, net)\n# net.load_from_npz(npz_fname)\n# net_utils.save_net(h5_fname, net)\nnet.cuda()\nnet.eval()\nprint('load model succ...')\n\nt_det = Timer()\nt_total = Timer()\nim_fnames = sorted((fname\n                    for fname in os.listdir(im_path)\n                    if os.path.splitext(fname)[-1] == '.jpg'))\nim_fnames = (os.path.join(im_path, fname) for fname in im_fnames)\npool = Pool(processes=1)\n\nfor i, (image, im_data) in enumerate(pool.imap(\n        preprocess, im_fnames, chunksize=1)):\n    t_total.tic()\n    im_data = net_utils.np_to_variable(\n        im_data, is_cuda=True, volatile=True).permute(0, 3, 1, 2)\n    t_det.tic()\n    bbox_pred, iou_pred, prob_pred = net(im_data)\n    det_time = t_det.toc()\n    # to numpy\n    bbox_pred = bbox_pred.data.cpu().numpy()\n    iou_pred = iou_pred.data.cpu().numpy()\n    prob_pred = prob_pred.data.cpu().numpy()\n\n    # print bbox_pred.shape, iou_pred.shape, prob_pred.shape\n\n    bboxes, scores, cls_inds = yolo_utils.postprocess(\n        bbox_pred, iou_pred, prob_pred, image.shape, cfg, thresh)\n\n    im2show = yolo_utils.draw_detection(image, bboxes, scores, cls_inds, cfg)\n\n    if im2show.shape[0] > 1100:\n        im2show = cv2.resize(im2show,\n                             (int(1000. *\n                                  float(im2show.shape[1]) / im2show.shape[0]),\n                              1000))\n    cv2.imshow('test', im2show)\n\n    total_time = t_total.toc()\n    # wait_time = max(int(60 - total_time * 1000), 1)\n    cv2.waitKey(0)\n\n    if i % 1 == 0:\n        format_str = 'frame: %d, ' \\\n                     '(detection: %.1f Hz, %.1f ms) ' \\\n                     '(total: %.1f Hz, %.1f ms)'\n        print((format_str % (\n            i,\n            1. / det_time, det_time * 1000,\n            1. / total_time, total_time * 1000)))\n\n        t_total.clear()\n        t_det.clear()\n"""
test.py,0,"b""import os\nimport cv2\nimport numpy as np\nimport pickle\nimport argparse\n\nfrom darknet import Darknet19\nimport utils.yolo as yolo_utils\nimport utils.network as net_utils\nfrom utils.timer import Timer\nfrom datasets.pascal_voc import VOCDataset\nimport cfgs.config as cfg\n\n\nparser = argparse.ArgumentParser(description='PyTorch Yolo')\nparser.add_argument('--image_size_index', type=int, default=0,\n                    metavar='image_size_index',\n                    help='setting images size index 0:320, 1:352, 2:384, 3:416, 4:448, 5:480, 6:512, 7:544, 8:576')\nargs = parser.parse_args()\n\n\n# hyper-parameters\n# ------------\nimdb_name = cfg.imdb_test\n# trained_model = cfg.trained_model\ntrained_model = os.path.join(cfg.train_output_dir,\n                             'darknet19_voc07trainval_exp3_73.h5')\noutput_dir = cfg.test_output_dir\n\nmax_per_image = 300\nthresh = 0.01\nvis = False\n# ------------\n\n\ndef test_net(net, imdb, max_per_image=300, thresh=0.5, vis=False):\n    num_images = imdb.num_images\n\n    # all detections are collected into:\n    #    all_boxes[cls][image] = N x 5 array of detections in\n    #    (x1, y1, x2, y2, score)\n    all_boxes = [[[] for _ in range(num_images)]\n                 for _ in range(imdb.num_classes)]\n\n    # timers\n    _t = {'im_detect': Timer(), 'misc': Timer()}\n    det_file = os.path.join(output_dir, 'detections.pkl')\n    size_index = args.image_size_index\n\n    for i in range(num_images):\n\n        batch = imdb.next_batch(size_index=size_index)\n        ori_im = batch['origin_im'][0]\n        im_data = net_utils.np_to_variable(batch['images'], is_cuda=True,\n                                           volatile=True).permute(0, 3, 1, 2)\n\n        _t['im_detect'].tic()\n        bbox_pred, iou_pred, prob_pred = net(im_data)\n\n        # to numpy\n        bbox_pred = bbox_pred.data.cpu().numpy()\n        iou_pred = iou_pred.data.cpu().numpy()\n        prob_pred = prob_pred.data.cpu().numpy()\n\n        bboxes, scores, cls_inds = yolo_utils.postprocess(bbox_pred,\n                                                          iou_pred,\n                                                          prob_pred,\n                                                          ori_im.shape,\n                                                          cfg,\n                                                          thresh,\n                                                          size_index\n                                                          )\n        detect_time = _t['im_detect'].toc()\n\n        _t['misc'].tic()\n\n        for j in range(imdb.num_classes):\n            inds = np.where(cls_inds == j)[0]\n            if len(inds) == 0:\n                all_boxes[j][i] = np.empty([0, 5], dtype=np.float32)\n                continue\n            c_bboxes = bboxes[inds]\n            c_scores = scores[inds]\n            c_dets = np.hstack((c_bboxes,\n                                c_scores[:, np.newaxis])).astype(np.float32,\n                                                                 copy=False)\n            all_boxes[j][i] = c_dets\n\n        # Limit to max_per_image detections *over all classes*\n        if max_per_image > 0:\n            image_scores = np.hstack([all_boxes[j][i][:, -1]\n                                      for j in range(imdb.num_classes)])\n            if len(image_scores) > max_per_image:\n                image_thresh = np.sort(image_scores)[-max_per_image]\n                for j in range(1, imdb.num_classes):\n                    keep = np.where(all_boxes[j][i][:, -1] >= image_thresh)[0]\n                    all_boxes[j][i] = all_boxes[j][i][keep, :]\n        nms_time = _t['misc'].toc()\n\n        if i % 20 == 0:\n            print('im_detect: {:d}/{:d} {:.3f}s {:.3f}s'.format(i + 1, num_images, detect_time, nms_time))  # noqa\n            _t['im_detect'].clear()\n            _t['misc'].clear()\n\n        if vis:\n            im2show = yolo_utils.draw_detection(ori_im,\n                                                bboxes,\n                                                scores,\n                                                cls_inds,\n                                                cfg,\n                                                thr=0.1)\n            if im2show.shape[0] > 1100:\n                im2show = cv2.resize(im2show,\n                                     (int(1000. * float(im2show.shape[1]) / im2show.shape[0]), 1000))  # noqa\n            cv2.imshow('test', im2show)\n            cv2.waitKey(0)\n\n    with open(det_file, 'wb') as f:\n        pickle.dump(all_boxes, f, pickle.HIGHEST_PROTOCOL)\n\n    print('Evaluating detections')\n    imdb.evaluate_detections(all_boxes, output_dir)\n\n\nif __name__ == '__main__':\n    # data loader\n    imdb = VOCDataset(imdb_name, cfg.DATA_DIR, cfg.batch_size,\n                      yolo_utils.preprocess_test,\n                      processes=1, shuffle=False, dst_size=cfg.multi_scale_inp_size)\n\n    net = Darknet19()\n    net_utils.load_net(trained_model, net)\n\n    net.cuda()\n    net.eval()\n\n    test_net(net, imdb, max_per_image, thresh, vis)\n\n    imdb.close()\n"""
train.py,2,"b'import os\nimport torch\nimport datetime\n\nfrom darknet import Darknet19\n\nfrom datasets.pascal_voc import VOCDataset\nimport utils.yolo as yolo_utils\nimport utils.network as net_utils\nfrom utils.timer import Timer\nimport cfgs.config as cfg\nfrom random import randint\n\ntry:\n    from tensorboardX import SummaryWriter\nexcept ImportError:\n    SummaryWriter = None\n\n\n# data loader\nimdb = VOCDataset(cfg.imdb_train, cfg.DATA_DIR, cfg.train_batch_size,\n                  yolo_utils.preprocess_train, processes=2, shuffle=True,\n                  dst_size=cfg.multi_scale_inp_size)\n# dst_size=cfg.inp_size)\nprint(\'load data succ...\')\n\nnet = Darknet19()\n# net_utils.load_net(cfg.trained_model, net)\n# pretrained_model = os.path.join(cfg.train_output_dir,\n#     \'darknet19_voc07trainval_exp1_63.h5\')\n# pretrained_model = cfg.trained_model\n# net_utils.load_net(pretrained_model, net)\nnet.load_from_npz(cfg.pretrained_model, num_conv=18)\nnet.cuda()\nnet.train()\nprint(\'load net succ...\')\n\n# optimizer\nstart_epoch = 0\nlr = cfg.init_learning_rate\noptimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=cfg.momentum,\n                            weight_decay=cfg.weight_decay)\n\n# tensorboad\nuse_tensorboard = cfg.use_tensorboard and SummaryWriter is not None\n# use_tensorboard = False\nif use_tensorboard:\n    summary_writer = SummaryWriter(os.path.join(cfg.TRAIN_DIR, \'runs\', cfg.exp_name))\nelse:\n    summary_writer = None\n\nbatch_per_epoch = imdb.batch_per_epoch\ntrain_loss = 0\nbbox_loss, iou_loss, cls_loss = 0., 0., 0.\ncnt = 0\nt = Timer()\nstep_cnt = 0\nsize_index = 0\nfor step in range(start_epoch * imdb.batch_per_epoch,\n                  cfg.max_epoch * imdb.batch_per_epoch):\n    t.tic()\n    # batch\n    batch = imdb.next_batch(size_index)\n    im = batch[\'images\']\n    gt_boxes = batch[\'gt_boxes\']\n    gt_classes = batch[\'gt_classes\']\n    dontcare = batch[\'dontcare\']\n    orgin_im = batch[\'origin_im\']\n\n    # forward\n    im_data = net_utils.np_to_variable(im,\n                                       is_cuda=True,\n                                       volatile=False).permute(0, 3, 1, 2)\n    bbox_pred, iou_pred, prob_pred = net(im_data, gt_boxes, gt_classes, dontcare, size_index)\n\n    # backward\n    loss = net.loss\n    bbox_loss += net.bbox_loss.data.cpu().numpy()[0]\n    iou_loss += net.iou_loss.data.cpu().numpy()[0]\n    cls_loss += net.cls_loss.data.cpu().numpy()[0]\n    train_loss += loss.data.cpu().numpy()[0]\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    cnt += 1\n    step_cnt += 1\n    duration = t.toc()\n    if step % cfg.disp_interval == 0:\n        train_loss /= cnt\n        bbox_loss /= cnt\n        iou_loss /= cnt\n        cls_loss /= cnt\n        print((\'epoch %d[%d/%d], loss: %.3f, bbox_loss: %.3f, iou_loss: %.3f, \'\n               \'cls_loss: %.3f (%.2f s/batch, rest:%s)\' %\n               (imdb.epoch, step_cnt, batch_per_epoch, train_loss, bbox_loss,\n                iou_loss, cls_loss, duration,\n                str(datetime.timedelta(seconds=int((batch_per_epoch - step_cnt) * duration))))))  # noqa\n\n        if summary_writer and step % cfg.log_interval == 0:\n            summary_writer.add_scalar(\'loss_train\', train_loss, step)\n            summary_writer.add_scalar(\'loss_bbox\', bbox_loss, step)\n            summary_writer.add_scalar(\'loss_iou\', iou_loss, step)\n            summary_writer.add_scalar(\'loss_cls\', cls_loss, step)\n            summary_writer.add_scalar(\'learning_rate\', lr, step)\n\n            # plot results\n            bbox_pred = bbox_pred.data[0:1].cpu().numpy()\n            iou_pred = iou_pred.data[0:1].cpu().numpy()\n            prob_pred = prob_pred.data[0:1].cpu().numpy()\n            image = im[0]\n            bboxes, scores, cls_inds = yolo_utils.postprocess(\n                bbox_pred, iou_pred, prob_pred, image.shape, cfg, thresh=0.3, size_index=size_index)\n            im2show = yolo_utils.draw_detection(image, bboxes, scores, cls_inds, cfg)\n            summary_writer.add_image(\'predict\', im2show, step)\n\n        train_loss = 0\n        bbox_loss, iou_loss, cls_loss = 0., 0., 0.\n        cnt = 0\n        t.clear()\n        size_index = randint(0, len(cfg.multi_scale_inp_size) - 1)\n        print(""image_size {}"".format(cfg.multi_scale_inp_size[size_index]))\n\n    if step > 0 and (step % imdb.batch_per_epoch == 0):\n        if imdb.epoch in cfg.lr_decay_epochs:\n            lr *= cfg.lr_decay\n            optimizer = torch.optim.SGD(net.parameters(), lr=lr,\n                                        momentum=cfg.momentum,\n                                        weight_decay=cfg.weight_decay)\n\n        save_name = os.path.join(cfg.train_output_dir,\n                                 \'{}_{}.h5\'.format(cfg.exp_name, imdb.epoch))\n        net_utils.save_net(save_name, net)\n        print((\'save model: {}\'.format(save_name)))\n        step_cnt = 0\n\nimdb.close()\n'"
cfgs/__init__.py,0,b''
cfgs/config.py,0,"b'import os\nfrom .config_voc import *  # noqa\nfrom .exps.darknet19_exp1 import *  # noqa\n\n\ndef mkdir(path, max_depth=3):\n    parent, child = os.path.split(path)\n    if not os.path.exists(parent) and max_depth > 1:\n        mkdir(parent, max_depth-1)\n\n    if not os.path.exists(path):\n        os.mkdir(path)\n\n\n# input and output size\n############################\nmulti_scale_inp_size = [np.array([320, 320], dtype=np.int),\n                        np.array([352, 352], dtype=np.int),\n                        np.array([384, 384], dtype=np.int),\n                        np.array([416, 416], dtype=np.int),\n                        np.array([448, 448], dtype=np.int),\n                        np.array([480, 480], dtype=np.int),\n                        np.array([512, 512], dtype=np.int),\n                        np.array([544, 544], dtype=np.int),\n                        np.array([576, 576], dtype=np.int),\n                        # np.array([608, 608], dtype=np.int),\n                        ]   # w, h\nmulti_scale_out_size = [multi_scale_inp_size[0] / 32,\n                        multi_scale_inp_size[1] / 32,\n                        multi_scale_inp_size[2] / 32,\n                        multi_scale_inp_size[3] / 32,\n                        multi_scale_inp_size[4] / 32,\n                        multi_scale_inp_size[5] / 32,\n                        multi_scale_inp_size[6] / 32,\n                        multi_scale_inp_size[7] / 32,\n                        multi_scale_inp_size[8] / 32,\n                        # multi_scale_inp_size[9] / 32,\n                        ]   # w, h\ninp_size = np.array([416, 416], dtype=np.int)   # w, h\nout_size = inp_size / 32\n\n\n# for display\n############################\ndef _to_color(indx, base):\n    """""" return (b, r, g) tuple""""""\n    base2 = base * base\n    b = 2 - indx / base2\n    r = 2 - (indx % base2) / base\n    g = 2 - (indx % base2) % base\n    return b * 127, r * 127, g * 127\n\n\nbase = int(np.ceil(pow(num_classes, 1. / 3)))\ncolors = [_to_color(x, base) for x in range(num_classes)]\n\n\n# detection config\n############################\nthresh = 0.3\n\n\n# dir config\n############################\nROOT_DIR = os.path.abspath(os.path.join(os.path.dirname(__file__), \'..\'))\nDATA_DIR = os.path.join(ROOT_DIR, \'data\')\nMODEL_DIR = os.path.join(ROOT_DIR, \'models\')\nTRAIN_DIR = os.path.join(MODEL_DIR, \'training\')\nTEST_DIR = os.path.join(MODEL_DIR, \'testing\')\n\ntrained_model = os.path.join(MODEL_DIR, h5_fname)\npretrained_model = os.path.join(MODEL_DIR, pretrained_fname)\ntrain_output_dir = os.path.join(TRAIN_DIR, exp_name)\ntest_output_dir = os.path.join(TEST_DIR, imdb_test, h5_fname)\nmkdir(train_output_dir, max_depth=3)\nmkdir(test_output_dir, max_depth=4)\n\nrand_seed = 1024\nuse_tensorboard = True\n\nlog_interval = 50\ndisp_interval = 10\n'"
cfgs/config_voc.py,0,"b""import numpy as np\n\n\n# trained model\nh5_fname = 'yolo-voc.weights.h5'\n\n# VOC\nlabel_names = ('aeroplane', 'bicycle', 'bird', 'boat',\n               'bottle', 'bus', 'car', 'cat', 'chair',\n               'cow', 'diningtable', 'dog', 'horse',\n               'motorbike', 'person', 'pottedplant',\n               'sheep', 'sofa', 'train', 'tvmonitor')\nnum_classes = len(label_names)\n\nanchors = np.asarray([(1.08, 1.19), (3.42, 4.41),\n                      (6.63, 11.38), (9.42, 5.11), (16.62, 10.52)],\n                     dtype=np.float)\nnum_anchors = len(anchors)\n"""
datasets/__init__.py,0,b''
datasets/imdb.py,0,"b'import os\nimport numpy as np\nfrom multiprocessing import Pool\nfrom functools import partial\nimport cfgs.config as cfg\nimport cv2\n\n\ndef mkdir(path, max_depth=3):\n    parent, child = os.path.split(path)\n    if not os.path.exists(parent) and max_depth > 1:\n        mkdir(parent, max_depth-1)\n\n    if not os.path.exists(path):\n        os.mkdir(path)\n\n\nclass ImageDataset(object):\n    def __init__(self, name, datadir, batch_size, im_processor,\n                 processes=3, shuffle=True, dst_size=None):\n        self._name = name\n        self._data_dir = datadir\n        self._batch_size = batch_size\n        self.dst_size = dst_size\n\n        self._epoch = -1\n        self._num_classes = 0\n        self._classes = []\n\n        # load by self.load_dataset()\n        self._image_indexes = []\n        self._image_names = []\n        self._annotations = []\n        # Use this dict for storing dataset specific config options\n        self.config = {}\n\n        # Pool\n        self._shuffle = shuffle\n        self._pool_processes = processes\n        self.pool = Pool(self._pool_processes)\n        self.gen = None\n        self._im_processor = im_processor\n\n    def next_batch(self, size_index):\n        batch = {\'images\': [], \'gt_boxes\': [], \'gt_classes\': [],\n                 \'dontcare\': [], \'origin_im\': []}\n        i = 0\n        if self.gen is None:\n            indexes = np.arange(len(self.image_names), dtype=np.int)\n            if self._shuffle:\n                np.random.shuffle(indexes)\n            self.gen = self.pool.imap(partial(self._im_processor,\n                                              size_index=None),\n                                      ([self.image_names[i],\n                                        self.get_annotation(i),\n                                        self.dst_size] for i in indexes),\n                                      chunksize=self.batch_size)\n            self._epoch += 1\n            print((\'epoch {} start...\'.format(self._epoch)))\n\n        while i < self.batch_size:\n            try:\n                images, gt_boxes, classes, dontcare, origin_im = next(self.gen)\n\n                # multi-scale\n                w, h = cfg.multi_scale_inp_size[size_index]\n                gt_boxes = np.asarray(gt_boxes, dtype=np.float)\n                if len(gt_boxes) > 0:\n                    gt_boxes[:, 0::2] *= float(w) / images.shape[1]\n                    gt_boxes[:, 1::2] *= float(h) / images.shape[0]\n                images = cv2.resize(images, (w, h))\n\n                batch[\'images\'].append(images)\n                batch[\'gt_boxes\'].append(gt_boxes)\n                batch[\'gt_classes\'].append(classes)\n                batch[\'dontcare\'].append(dontcare)\n                batch[\'origin_im\'].append(origin_im)\n                i += 1\n            except (StopIteration,):\n                indexes = np.arange(len(self.image_names), dtype=np.int)\n                if self._shuffle:\n                    np.random.shuffle(indexes)\n                self.gen = self.pool.imap(partial(self._im_processor,\n                                                  size_index=None),\n                                          ([self.image_names[i],\n                                            self.get_annotation(i),\n                                            self.dst_size] for i in indexes),\n                                          chunksize=self.batch_size)\n                self._epoch += 1\n                print((\'epoch {} start...\'.format(self._epoch)))\n        batch[\'images\'] = np.asarray(batch[\'images\'])\n        return batch\n\n    def close(self):\n        self.pool.terminate()\n        self.pool.join()\n        self.gen = None\n\n    def load_dataset(self):\n        raise NotImplementedError\n\n    def evaluate_detections(self, all_boxes, output_dir=None):\n        """"""\n        all_boxes is a list of length number-of-classes.\n        Each list element is a list of length number-of-images.\n        Each of those list elements is either an empty list []\n        or a numpy array of detection.\n\n        all_boxes[class][image] = [] or np.array of shape #dets x 5\n        """"""\n        raise NotImplementedError\n\n    def get_annotation(self, i):\n        if self.annotations is None:\n            return None\n        return self.annotations[i]\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def num_classes(self):\n        return len(self._classes)\n\n    @property\n    def classes(self):\n        return self._classes\n\n    @property\n    def image_names(self):\n        return self._image_names\n\n    @property\n    def image_indexes(self):\n        return self._image_indexes\n\n    @property\n    def annotations(self):\n        return self._annotations\n\n    @property\n    def cache_path(self):\n        cache_path = os.path.join(self._data_dir, \'cache\')\n        mkdir(cache_path)\n        return cache_path\n\n    @property\n    def num_images(self):\n        return len(self.image_names)\n\n    @property\n    def epoch(self):\n        return self._epoch\n\n    @property\n    def batch_size(self):\n        return self._batch_size\n\n    @property\n    def batch_per_epoch(self):\n        return self.num_images // self.batch_size\n'"
datasets/pascal_voc.py,0,"b'import pickle\nimport os\nimport uuid\nimport xml.etree.ElementTree as ET\n\nimport numpy as np\nimport scipy.sparse\n\n# from functools import partial\n\nfrom .imdb import ImageDataset\nfrom .voc_eval import voc_eval\n# from utils.yolo import preprocess_train\n\n\nclass VOCDataset(ImageDataset):\n    def __init__(self, imdb_name, datadir, batch_size, im_processor,\n                 processes=3, shuffle=True, dst_size=None):\n        super(VOCDataset, self).__init__(imdb_name, datadir, batch_size,\n                                         im_processor, processes,\n                                         shuffle, dst_size)\n        meta = imdb_name.split(\'_\')\n        self._year = meta[1]\n        self._image_set = meta[2]\n        self._devkit_path = os.path.join(datadir,\n                                         \'VOCdevkit{}\'.format(self._year))\n        self._data_path = os.path.join(self._devkit_path,\n                                       \'VOC{}\'.format(self._year))\n        assert os.path.exists(self._devkit_path), \\\n            \'VOCdevkit path does not exist: {}\'.format(self._devkit_path)\n        assert os.path.exists(self._data_path), \\\n            \'Path does not exist: {}\'.format(self._data_path)\n\n        self._classes = (\'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n                         \'bottle\', \'bus\', \'car\', \'cat\', \'chair\',\n                         \'cow\', \'diningtable\', \'dog\', \'horse\',\n                         \'motorbike\', \'person\', \'pottedplant\',\n                         \'sheep\', \'sofa\', \'train\', \'tvmonitor\')\n        self._class_to_ind = dict(list(zip(self.classes,\n                                           list(range(self.num_classes)))))\n        self._image_ext = \'.jpg\'\n\n        self._salt = str(uuid.uuid4())\n        self._comp_id = \'comp4\'\n\n        # PASCAL specific config options\n        self.config = {\'cleanup\': True,\n                       \'use_salt\': True}\n\n        self.load_dataset()\n        # self.im_processor = partial(process_im,\n        #     image_names=self._image_names, annotations=self._annotations)\n        # self.im_processor = preprocess_train\n\n    def load_dataset(self):\n        # set self._image_index and self._annotations\n        self._image_indexes = self._load_image_set_index()\n        self._image_names = [self.image_path_from_index(index)\n                             for index in self.image_indexes]\n        self._annotations = self._load_pascal_annotations()\n\n    def evaluate_detections(self, all_boxes, output_dir=None):\n        """"""\n        all_boxes is a list of length number-of-classes.\n        Each list element is a list of length number-of-images.\n        Each of those list elements is either an empty list []\n        or a numpy array of detection.\n\n        all_boxes[class][image] = [] or np.array of shape #dets x 5\n        """"""\n        self._write_voc_results_file(all_boxes)\n        self._do_python_eval(output_dir)\n        if self.config[\'cleanup\']:\n            for cls in self._classes:\n                if cls == \'__background__\':\n                    continue\n                filename = self._get_voc_results_file_template().format(cls)\n                os.remove(filename)\n\n    # -------------------------------------------------------------\n    def image_path_from_index(self, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n        image_path = os.path.join(self._data_path, \'JPEGImages\',\n                                  index + self._image_ext)\n        assert os.path.exists(image_path), \\\n            \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n    def _load_image_set_index(self):\n        """"""\n        Load the indexes listed in this dataset\'s image set file.\n        """"""\n        # Example path to image set file:\n        # self._devkit_path + /VOCdevkit2007/VOC2007/ImageSets/Main/val.txt\n        image_set_file = os.path.join(self._data_path, \'ImageSets\', \'Main\',\n                                      self._image_set + \'.txt\')\n        assert os.path.exists(image_set_file), \\\n            \'Path does not exist: {}\'.format(image_set_file)\n        with open(image_set_file) as f:\n            image_index = [x.strip() for x in f.readlines()]\n        return image_index\n\n    def _load_pascal_annotations(self):\n        """"""\n        Return the database of ground-truth regions of interest.\n\n        This function loads/saves from/to a cache file to speed up\n        future calls.\n        """"""\n        cache_file = os.path.join(self.cache_path, self.name + \'_gt_roidb.pkl\')\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = pickle.load(fid)\n            print(\'{} gt roidb loaded from {}\'.format(self.name, cache_file))\n            return roidb\n\n        gt_roidb = [self._annotation_from_index(index)\n                    for index in self.image_indexes]\n        with open(cache_file, \'wb\') as fid:\n            pickle.dump(gt_roidb, fid, pickle.HIGHEST_PROTOCOL)\n        print(\'wrote gt roidb to {}\'.format(cache_file))\n\n        return gt_roidb\n\n    def _annotation_from_index(self, index):\n        """"""\n        Load image and bounding boxes info from XML file in the PASCAL VOC\n        format.\n        """"""\n        filename = os.path.join(self._data_path, \'Annotations\', index + \'.xml\')\n        tree = ET.parse(filename)\n        objs = tree.findall(\'object\')\n        # if not self.config[\'use_diff\']:\n        #     # Exclude the samples labeled as difficult\n        #     non_diff_objs = [\n        #         obj for obj in objs if int(obj.find(\'difficult\').text) == 0]\n        #     # if len(non_diff_objs) != len(objs):\n        #     #     print \'Removed {} difficult objects\'.format(\n        #     #         len(objs) - len(non_diff_objs))\n        #     objs = non_diff_objs\n        num_objs = len(objs)\n\n        boxes = np.zeros((num_objs, 4), dtype=np.uint16)\n        gt_classes = np.zeros((num_objs), dtype=np.int32)\n        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)\n        # ""Seg"" area for pascal is just the box area\n        seg_areas = np.zeros((num_objs), dtype=np.float32)\n        ishards = np.zeros((num_objs), dtype=np.int32)\n\n        # Load object bounding boxes into a data frame.\n        for ix, obj in enumerate(objs):\n            bbox = obj.find(\'bndbox\')\n            # Make pixel indexes 0-based\n            x1 = float(bbox.find(\'xmin\').text) - 1\n            y1 = float(bbox.find(\'ymin\').text) - 1\n            x2 = float(bbox.find(\'xmax\').text) - 1\n            y2 = float(bbox.find(\'ymax\').text) - 1\n\n            diffc = obj.find(\'difficult\')\n            difficult = 0 if diffc is None else int(diffc.text)\n            ishards[ix] = difficult\n\n            cls = self._class_to_ind[obj.find(\'name\').text.lower().strip()]\n            boxes[ix, :] = [x1, y1, x2, y2]\n            gt_classes[ix] = cls\n            overlaps[ix, cls] = 1.0\n            seg_areas[ix] = (x2 - x1 + 1) * (y2 - y1 + 1)\n\n        overlaps = scipy.sparse.csr_matrix(overlaps)\n\n        return {\'boxes\': boxes,\n                \'gt_classes\': gt_classes,\n                \'gt_ishard\': ishards,\n                \'gt_overlaps\': overlaps,\n                \'flipped\': False,\n                \'seg_areas\': seg_areas}\n\n    def _get_voc_results_file_template(self):\n        # VOCdevkit/results/VOC2007/Main/<comp_id>_det_test_aeroplane.txt\n        filename = self._get_comp_id() + \'_det_\' + self._image_set + \\\n                   \'_{:s}.txt\'\n        filedir = os.path.join(self._devkit_path,\n                               \'results\', \'VOC\' + self._year, \'Main\')\n        if not os.path.exists(filedir):\n            os.makedirs(filedir)\n        path = os.path.join(filedir, filename)\n        return path\n\n    def _write_voc_results_file(self, all_boxes):\n        for cls_ind, cls in enumerate(self.classes):\n            if cls == \'__background__\':\n                continue\n            print(\'Writing {} VOC results file\'.format(cls))\n            filename = self._get_voc_results_file_template().format(cls)\n            with open(filename, \'wt\') as f:\n                for im_ind, index in enumerate(self.image_indexes):\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    # the VOCdevkit expects 1-based indices\n                    for k in range(dets.shape[0]):\n                        f.write(\'{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\\n\'.\n                                format(index, dets[k, -1],\n                                       dets[k, 0] + 1, dets[k, 1] + 1,\n                                       dets[k, 2] + 1, dets[k, 3] + 1))\n\n    def _do_python_eval(self, output_dir=\'output\'):\n        annopath = os.path.join(\n            self._devkit_path,\n            \'VOC\' + self._year,\n            \'Annotations\',\n            \'{:s}.xml\')\n        imagesetfile = os.path.join(\n            self._devkit_path,\n            \'VOC\' + self._year,\n            \'ImageSets\',\n            \'Main\',\n            self._image_set + \'.txt\')\n        cachedir = os.path.join(self._devkit_path, \'annotations_cache\')\n        aps = []\n        # The PASCAL VOC metric changed in 2010\n        use_07_metric = True if int(self._year) < 2010 else False\n        print(\'VOC07 metric? \' + (\'Yes\' if use_07_metric else \'No\'))\n        if output_dir is not None and not os.path.isdir(output_dir):\n            os.mkdir(output_dir)\n        for i, cls in enumerate(self._classes):\n            if cls == \'__background__\':\n                continue\n            filename = self._get_voc_results_file_template().format(cls)\n            rec, prec, ap = voc_eval(\n                filename, annopath, imagesetfile, cls, cachedir, ovthresh=0.5,\n                use_07_metric=use_07_metric)\n            aps += [ap]\n            print((\'AP for {} = {:.4f}\'.format(cls, ap)))\n            if output_dir is not None:\n                with open(os.path.join(output_dir, cls + \'_pr.pkl\'), \'wb\') as f:\n                    pickle.dump({\'rec\': rec, \'prec\': prec, \'ap\': ap}, f)\n        print((\'Mean AP = {:.4f}\'.format(np.mean(aps))))\n        print(\'~~~~~~~~\')\n        print(\'Results:\')\n        for ap in aps:\n            print((\'{:.3f}\'.format(ap)))\n        print((\'{:.3f}\'.format(np.mean(aps))))\n        print(\'~~~~~~~~\')\n        print(\'\')\n        print(\'--------------------------------------------------------------\')\n        print(\'Results computed with the **unofficial** Python eval code.\')\n        print(\'Results should be very close to the official MATLAB eval code.\')\n        print(\'Recompute with `./tools/reval.py --matlab ...` for your paper.\')\n        print(\'-- Thanks, The Management\')\n        print(\'--------------------------------------------------------------\')\n\n    def _get_comp_id(self):\n        comp_id = (self._comp_id + \'_\' + self._salt if self.config[\'use_salt\']\n                   else self._comp_id)\n        return comp_id\n'"
datasets/voc_eval.py,0,"b'# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Bharath Hariharan\n# --------------------------------------------------------\n\nimport xml.etree.ElementTree as ET\nimport os\nimport pickle\nimport numpy as np\n\n\ndef parse_rec(filename):\n    """""" Parse a PASCAL VOC xml file """"""\n    tree = ET.parse(filename)\n    objects = []\n    for obj in tree.findall(\'object\'):\n        obj_struct = {}\n        obj_struct[\'name\'] = obj.find(\'name\').text\n        obj_struct[\'pose\'] = obj.find(\'pose\').text\n        obj_struct[\'truncated\'] = int(obj.find(\'truncated\').text)\n        obj_struct[\'difficult\'] = int(obj.find(\'difficult\').text)\n        bbox = obj.find(\'bndbox\')\n        obj_struct[\'bbox\'] = [int(bbox.find(\'xmin\').text),\n                              int(bbox.find(\'ymin\').text),\n                              int(bbox.find(\'xmax\').text),\n                              int(bbox.find(\'ymax\').text)]\n        objects.append(obj_struct)\n\n    return objects\n\n\ndef voc_ap(rec, prec, use_07_metric=False):\n    """""" ap = voc_ap(rec, prec, [use_07_metric])\n    Compute VOC AP given precision and recall.\n    If use_07_metric is true, uses the\n    VOC 07 11 point method (default:False).\n    """"""\n    if use_07_metric:\n        # 11 point metric\n        ap = 0.\n        for t in np.arange(0., 1.1, 0.1):\n            if np.sum(rec >= t) == 0:\n                p = 0\n            else:\n                p = np.max(prec[rec >= t])\n            ap = ap + p / 11.\n    else:\n        # correct AP calculation\n        # first append sentinel values at the end\n        mrec = np.concatenate(([0.], rec, [1.]))\n        mpre = np.concatenate(([0.], prec, [0.]))\n\n        # compute the precision envelope\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n        # to calculate area under PR curve, look for points\n        # where X axis (recall) changes value\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        # and sum (\\Delta recall) * prec\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap\n\n\ndef voc_eval(detpath,\n             annopath,\n             imagesetfile,\n             classname,\n             cachedir,\n             ovthresh=0.5,\n             use_07_metric=False):\n    """"""rec, prec, ap = voc_eval(detpath,\n                                annopath,\n                                imagesetfile,\n                                classname,\n                                [ovthresh],\n                                [use_07_metric])\n\n    Top level function that does the PASCAL VOC evaluation.\n\n    detpath: Path to detections\n        detpath.format(classname) should produce the detection results file.\n    annopath: Path to annotations\n        annopath.format(imagename) should be the xml annotations file.\n    imagesetfile: Text file containing the list of images, one image per line.\n    classname: Category name (duh)\n    cachedir: Directory for caching the annotations\n    [ovthresh]: Overlap threshold (default = 0.5)\n    [use_07_metric]: Whether to use VOC07\'s 11 point AP computation\n        (default False)\n    """"""\n    # assumes detections are in detpath.format(classname)\n    # assumes annotations are in annopath.format(imagename)\n    # assumes imagesetfile is a text file with each line an image name\n    # cachedir caches the annotations in a pickle file\n\n    # first load gt\n    if not os.path.isdir(cachedir):\n        os.mkdir(cachedir)\n    cachefile = os.path.join(cachedir, \'annots.pkl\')\n    # read list of images\n    with open(imagesetfile, \'r\') as f:\n        lines = f.readlines()\n    imagenames = [x.strip() for x in lines]\n\n    if not os.path.isfile(cachefile):\n        # load annots\n        recs = {}\n        for i, imagename in enumerate(imagenames):\n            recs[imagename] = parse_rec(annopath.format(imagename))\n            if i % 100 == 0:\n                print(\'Reading annotation for {:d}/{:d}\'.format(\n                    i + 1, len(imagenames)))\n        # save\n        print(\'Saving cached annotations to {:s}\'.format(cachefile))\n        with open(cachefile, \'wb\') as f:\n            pickle.dump(recs, f)\n    else:\n        # load\n        with open(cachefile, \'rb\') as f:\n            recs = pickle.load(f)\n\n    # extract gt objects for this class\n    class_recs = {}\n    npos = 0\n    for imagename in imagenames:\n        R = [obj for obj in recs[imagename] if obj[\'name\'] == classname]\n        bbox = np.array([x[\'bbox\'] for x in R])\n        difficult = np.array([x[\'difficult\'] for x in R]).astype(np.bool)\n        det = [False] * len(R)\n        npos = npos + sum(~difficult)\n        class_recs[imagename] = {\'bbox\': bbox,\n                                 \'difficult\': difficult,\n                                 \'det\': det}\n\n    # read dets\n    detfile = detpath.format(classname)\n    with open(detfile, \'r\') as f:\n        lines = f.readlines()\n    if any(lines) == 1:\n\n        splitlines = [x.strip().split(\' \') for x in lines]\n        image_ids = [x[0] for x in splitlines]\n        confidence = np.array([float(x[1]) for x in splitlines])\n        BB = np.array([[float(z) for z in x[2:]] for x in splitlines])\n\n        # sort by confidence\n        sorted_ind = np.argsort(-confidence)\n        BB = BB[sorted_ind, :]\n        image_ids = [image_ids[x] for x in sorted_ind]\n\n        # go down dets and mark TPs and FPs\n        nd = len(image_ids)\n        tp = np.zeros(nd)\n        fp = np.zeros(nd)\n        for d in range(nd):\n            R = class_recs[image_ids[d]]\n            bb = BB[d, :].astype(float)\n            ovmax = -np.inf\n            BBGT = R[\'bbox\'].astype(float)\n\n            if BBGT.size > 0:\n                # compute overlaps\n                # intersection\n                ixmin = np.maximum(BBGT[:, 0], bb[0])\n                iymin = np.maximum(BBGT[:, 1], bb[1])\n                ixmax = np.minimum(BBGT[:, 2], bb[2])\n                iymax = np.minimum(BBGT[:, 3], bb[3])\n                iw = np.maximum(ixmax - ixmin + 1., 0.)\n                ih = np.maximum(iymax - iymin + 1., 0.)\n                inters = iw * ih\n\n                # union\n                uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +\n                       (BBGT[:, 2] - BBGT[:, 0] + 1.) *\n                       (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)\n\n                overlaps = inters / uni\n                ovmax = np.max(overlaps)\n                jmax = np.argmax(overlaps)\n\n            if ovmax > ovthresh:\n                if not R[\'difficult\'][jmax]:\n                    if not R[\'det\'][jmax]:\n                        tp[d] = 1.\n                        R[\'det\'][jmax] = 1\n                    else:\n                        fp[d] = 1.\n            else:\n                fp[d] = 1.\n\n        # compute precision recall\n        fp = np.cumsum(fp)\n        tp = np.cumsum(tp)\n        rec = tp / float(npos)\n        # avoid divide by zero in case the first detection matches a difficult\n        # ground truth\n        prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n        ap = voc_ap(rec, prec, use_07_metric)\n    else:\n        rec = -1\n        prec = -1\n        ap = -1\n\n    return rec, prec, ap\n'"
layers/__init__.py,0,b''
utils/__init__.py,0,b''
utils/build.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport os\nfrom os.path import join as pjoin\nimport numpy as np\nfrom distutils.core import setup\nfrom distutils.extension import Extension\nfrom Cython.Distutils import build_ext\n\n\ndef find_in_path(name, path):\n    """"""\n    Find a file in a search path\n    adapted fom\n    http://code.activestate.com/recipes/52224-find-a-file-given-a-search-path/\n    """"""\n    for dir in path.split(os.pathsep):\n        binpath = pjoin(dir, name)\n        if os.path.exists(binpath):\n            return os.path.abspath(binpath)\n    return None\n\n\ndef locate_cuda():\n    """"""Locate the CUDA environment on the system\n\n    Returns a dict with keys \'home\', \'nvcc\', \'include\', and \'lib64\'\n    and values giving the absolute path to each directory.\n\n    Starts by looking for the CUDAHOME env variable. If not found, everything\n    is based on finding \'nvcc\' in the PATH.\n    """"""\n\n    # first check if the CUDAHOME env variable is in use\n    if \'CUDAHOME\' in os.environ:\n        home = os.environ[\'CUDAHOME\']\n        nvcc = pjoin(home, \'bin\', \'nvcc\')\n    else:\n        # otherwise, search the PATH for NVCC\n        default_path = pjoin(os.sep, \'usr\', \'local\', \'cuda\', \'bin\')\n        nvcc = find_in_path(\'nvcc\',\n                            os.environ[\'PATH\'] + os.pathsep + default_path)\n        if nvcc is None:\n            raise EnvironmentError(\'The nvcc binary could not be \'\n                                   \'located in your $PATH. \'\n                                   \'Either add it to your path, \'\n                                   \'or set $CUDAHOME\')\n        home = os.path.dirname(os.path.dirname(nvcc))\n\n    cudaconfig = {\'home\': home, \'nvcc\': nvcc,\n                  \'include\': pjoin(home, \'include\'),\n                  \'lib64\': pjoin(home, \'lib64\')}\n    for k, v in cudaconfig.items():\n        if not os.path.exists(v):\n            raise EnvironmentError(\'The CUDA %s path could not \'\n                                   \'be located in %s\' % (k, v))\n\n    return cudaconfig\n\n\nCUDA = locate_cuda()\n\n# Obtain the numpy include directory.  This logic works across numpy versions.\ntry:\n    numpy_include = np.get_include()\nexcept AttributeError:\n    numpy_include = np.get_numpy_include()\n\n\ndef customize_compiler_for_nvcc(self):\n    """"""inject deep into distutils to customize how the dispatch\n    to gcc/nvcc works.\n\n    If you subclass UnixCCompiler, it\'s not trivial to get your subclass\n    injected in, and still have the right customizations (i.e.\n    distutils.sysconfig.customize_compiler) run on it. So instead of going\n    the OO route, I have this. Note, it\'s kindof like a wierd functional\n    subclassing going on.""""""\n\n    # tell the compiler it can processes .cu\n    self.src_extensions.append(\'.cu\')\n\n    # save references to the default compiler_so and _comple methods\n    default_compiler_so = self.compiler_so\n    super = self._compile\n\n    # now redefine the _compile method. This gets executed for each\n    # object but distutils doesn\'t have the ability to change compilers\n    # based on source extension: we add it.\n    def _compile(obj, src, ext, cc_args, extra_postargs, pp_opts):\n        print(extra_postargs)\n        if os.path.splitext(src)[1] == \'.cu\':\n            # use the cuda for .cu files\n            self.set_executable(\'compiler_so\', CUDA[\'nvcc\'])\n            # use only a subset of the extra_postargs, which are 1-1 translated\n            # from the extra_compile_args in the Extension class\n            postargs = extra_postargs[\'nvcc\']\n        else:\n            postargs = extra_postargs[\'gcc\']\n\n        super(obj, src, ext, cc_args, postargs, pp_opts)\n        # reset the default compiler_so, which we might have changed for cuda\n        self.compiler_so = default_compiler_so\n\n    # inject our redefined _compile method into the class\n    self._compile = _compile\n\n\n# run the customize_compiler\nclass custom_build_ext(build_ext):\n    def build_extensions(self):\n        customize_compiler_for_nvcc(self.compiler)\n        build_ext.build_extensions(self)\n\n\next_modules = [\n    Extension(\n        ""cython_bbox"",\n        [""bbox.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n        include_dirs=[numpy_include]\n    ),\n    Extension(\n        ""cython_yolo"",\n        [""yolo.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n        include_dirs=[numpy_include]\n    ),\n    Extension(\n        ""nms.cpu_nms"",\n        [""nms/cpu_nms.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n        include_dirs=[numpy_include]\n    ),\n    Extension(\'nms.gpu_nms\',\n              [\'nms/nms_kernel.cu\', \'nms/gpu_nms.pyx\'],\n              library_dirs=[CUDA[\'lib64\']],\n              libraries=[\'cudart\'],\n              language=\'c++\',\n              runtime_library_dirs=[CUDA[\'lib64\']],\n              # this syntax is specific to this build system\n              # we\'re only going to use certain compiler args with\n              # nvcc and not with gcc\n              # the implementation of this trick is in\n              # customize_compiler() below\n              extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n                                  \'nvcc\': [\'-arch=sm_35\',\n                                           \'--ptxas-options=-v\',\n                                           \'-c\',\n                                           \'--compiler-options\',\n                                           ""\'-fPIC\'""]},\n              include_dirs=[numpy_include, CUDA[\'include\']]\n              ),\n    Extension(\n        \'pycocotools._mask\',\n        sources=[\'pycocotools/maskApi.c\', \'pycocotools/_mask.pyx\'],\n        include_dirs=[numpy_include, \'pycocotools\'],\n        extra_compile_args={\n            \'gcc\': [\'-Wno-cpp\', \'-Wno-unused-function\', \'-std=c99\']},\n    ),\n]\n\nsetup(\n    name=\'mot_utils\',\n    ext_modules=ext_modules,\n    # inject our custom trigger\n    cmdclass={\'build_ext\': custom_build_ext},\n)\n'"
utils/im_transform.py,0,"b'import numpy as np\nimport cv2\n\n\ndef imcv2_recolor(im, a=.1):\n    # t = [np.random.uniform()]\n    # t += [np.random.uniform()]\n    # t += [np.random.uniform()]\n    # t = np.array(t) * 2. - 1.\n    t = np.random.uniform(-1, 1, 3)\n\n    # random amplify each channel\n    im = im.astype(np.float)\n    im *= (1 + t * a)\n    mx = 255. * (1 + a)\n    up = np.random.uniform(-1, 1)\n    im = np.power(im / mx, 1. + up * .5)\n    # return np.array(im * 255., np.uint8)\n    return im\n\n\ndef imcv2_affine_trans(im):\n    # Scale and translate\n    h, w, c = im.shape\n    scale = np.random.uniform() / 10. + 1.\n    max_offx = (scale - 1.) * w\n    max_offy = (scale - 1.) * h\n    offx = int(np.random.uniform() * max_offx)\n    offy = int(np.random.uniform() * max_offy)\n\n    im = cv2.resize(im, (0, 0), fx=scale, fy=scale)\n    im = im[offy: (offy + h), offx: (offx + w)]\n    flip = np.random.uniform() > 0.5\n    if flip:\n        im = cv2.flip(im, 1)\n\n    return im, [scale, [offx, offy], flip]\n'"
utils/network.py,8,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np\n\n\nclass Conv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n                 relu=True, same_padding=False):\n        super(Conv2d, self).__init__()\n        padding = int((kernel_size - 1) / 2) if same_padding else 0\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size,\n                              stride, padding=padding)\n        self.relu = nn.LeakyReLU(0.1, inplace=True) if relu else None\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.relu is not None:\n            x = self.relu(x)\n        return x\n\n\nclass Conv2d_BatchNorm(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n                 relu=True, same_padding=False):\n        super(Conv2d_BatchNorm, self).__init__()\n        padding = int((kernel_size - 1) / 2) if same_padding else 0\n\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size,\n                              stride, padding=padding, bias=False)\n        self.bn = nn.BatchNorm2d(out_channels, momentum=0.01)\n        self.relu = nn.LeakyReLU(0.1, inplace=True) if relu else None\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        if self.relu is not None:\n            x = self.relu(x)\n        return x\n\n\nclass FC(nn.Module):\n    def __init__(self, in_features, out_features, relu=True):\n        super(FC, self).__init__()\n        self.fc = nn.Linear(in_features, out_features)\n        self.relu = nn.ReLU(inplace=True) if relu else None\n\n    def forward(self, x):\n        x = self.fc(x)\n        if self.relu is not None:\n            x = self.relu(x)\n        return x\n\n\ndef save_net(fname, net):\n    import h5py\n    h5f = h5py.File(fname, mode=\'w\')\n    for k, v in list(net.state_dict().items()):\n        h5f.create_dataset(k, data=v.cpu().numpy())\n\n\ndef load_net(fname, net):\n    import h5py\n    h5f = h5py.File(fname, mode=\'r\')\n    for k, v in list(net.state_dict().items()):\n        param = torch.from_numpy(np.asarray(h5f[k]))\n        v.copy_(param)\n\n\ndef load_pretrained_npy(faster_rcnn_model, fname):\n    params = np.load(fname).item()\n    # vgg16\n    vgg16_dict = faster_rcnn_model.rpn.features.state_dict()\n    for name, val in list(vgg16_dict.items()):\n        # # print name\n        # # print val.size()\n        # # print param.size()\n        if name.find(\'bn.\') >= 0:\n            continue\n        i, j = int(name[4]), int(name[6]) + 1\n        ptype = \'weights\' if name[-1] == \'t\' else \'biases\'\n        key = \'conv{}_{}\'.format(i, j)\n        param = torch.from_numpy(params[key][ptype])\n\n        if ptype == \'weights\':\n            param = param.permute(3, 2, 0, 1)\n\n        val.copy_(param)\n\n    # fc6 fc7\n    frcnn_dict = faster_rcnn_model.state_dict()\n    pairs = {\'fc6.fc\': \'fc6\', \'fc7.fc\': \'fc7\'}\n    for k, v in list(pairs.items()):\n        key = \'{}.weight\'.format(k)\n        param = torch.from_numpy(params[v][\'weights\']).permute(1, 0)\n        frcnn_dict[key].copy_(param)\n\n        key = \'{}.bias\'.format(k)\n        param = torch.from_numpy(params[v][\'biases\'])\n        frcnn_dict[key].copy_(param)\n\n\ndef np_to_variable(x, is_cuda=True, dtype=torch.FloatTensor, volatile=False):\n    v = Variable(torch.from_numpy(x).type(dtype), volatile=volatile)\n    if is_cuda:\n        v = v.cuda()\n    return v\n\n\ndef variable_to_np_tf(x):\n    return x.data.cpu().numpy().transpose([0, 2, 3, 1])\n\n\ndef set_trainable(model, requires_grad):\n    for param in model.parameters():\n        param.requires_grad = requires_grad\n\n\ndef weights_normal_init(model, dev=0.01):\n    if isinstance(model, list):\n        for m in model:\n            weights_normal_init(m, dev)\n    else:\n        for m in model.modules():\n            if isinstance(m, nn.Conv2d):\n                m.weight.data.normal_(0.0, dev)\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0.0, dev)\n\n\ndef clip_gradient(model, clip_norm):\n    """"""Computes a gradient clipping coefficient based on gradient norm.""""""\n    totalnorm = 0\n    for p in model.parameters():\n        if p.requires_grad:\n            modulenorm = p.grad.data.norm()\n            totalnorm += modulenorm ** 2\n    totalnorm = np.sqrt(totalnorm)\n\n    norm = clip_norm / max(totalnorm, clip_norm)\n    for p in model.parameters():\n        if p.requires_grad:\n            p.grad.mul_(norm)\n'"
utils/nms_wrapper.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nfrom .nms.cpu_nms import cpu_nms\nfrom .nms.gpu_nms import gpu_nms\n\n\n# def nms(dets, thresh, force_cpu=False):\n#     """"""Dispatch to either CPU or GPU NMS implementations.""""""\n#\n#     if dets.shape[0] == 0:\n#         return []\n#     if cfg.USE_GPU_NMS and not force_cpu:\n#         return gpu_nms(dets, thresh, device_id=cfg.GPU_ID)\n#     else:\n#         return cpu_nms(dets, thresh)\n\n\ndef nms(dets, thresh, force_cpu=False):\n    """"""Dispatch to either CPU or GPU NMS implementations.""""""\n\n    if dets.shape[0] == 0:\n        return []\n    if force_cpu:\n        return cpu_nms(dets, thresh)\n    return gpu_nms(dets, thresh)\n'"
utils/timer.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport time\n\n\nclass Timer(object):\n    """"""A simple timer.""""""\n    def __init__(self):\n        self.total_time = 0.\n        self.calls = 0\n        self.start_time = 0.\n        self.diff = 0.\n        self.average_time = 0.\n\n    def tic(self):\n        # using time.time instead of time.clock because time time.clock\n        # does not normalize for multithreading\n        self.start_time = time.time()\n\n    def toc(self, average=True):\n        self.diff = time.time() - self.start_time\n        self.total_time += self.diff\n        self.calls += 1\n        self.average_time = self.total_time / self.calls\n        if average:\n            return self.average_time\n        else:\n            return self.diff\n\n    def clear(self):\n        self.total_time = 0.\n        self.calls = 0\n        self.start_time = 0.\n        self.diff = 0.\n        self.average_time = 0.\n'"
utils/yolo.py,0,"b'import cv2\nimport numpy as np\nfrom .im_transform import imcv2_affine_trans, imcv2_recolor\n# from box import BoundBox, box_iou, prob_compare\nfrom utils.nms_wrapper import nms\nfrom utils.cython_yolo import yolo_to_bbox\n\n\n# This prevents deadlocks in the data loader, caused by\n# some incompatibility between pytorch and cv2 multiprocessing.\n# See https://github.com/pytorch/pytorch/issues/1355.\ncv2.setNumThreads(0)\n\n\ndef clip_boxes(boxes, im_shape):\n    """"""\n    Clip boxes to image boundaries.\n    """"""\n    if boxes.shape[0] == 0:\n        return boxes\n\n    # x1 >= 0\n    boxes[:, 0::4] = np.maximum(np.minimum(boxes[:, 0::4], im_shape[1] - 1), 0)\n    # y1 >= 0\n    boxes[:, 1::4] = np.maximum(np.minimum(boxes[:, 1::4], im_shape[0] - 1), 0)\n    # x2 < im_shape[1]\n    boxes[:, 2::4] = np.maximum(np.minimum(boxes[:, 2::4], im_shape[1] - 1), 0)\n    # y2 < im_shape[0]\n    boxes[:, 3::4] = np.maximum(np.minimum(boxes[:, 3::4], im_shape[0] - 1), 0)\n    return boxes\n\n\ndef nms_detections(pred_boxes, scores, nms_thresh):\n    dets = np.hstack((pred_boxes,\n                      scores[:, np.newaxis])).astype(np.float32)\n    keep = nms(dets, nms_thresh)\n    return keep\n\n\ndef _offset_boxes(boxes, im_shape, scale, offs, flip):\n    if len(boxes) == 0:\n        return boxes\n    boxes = np.asarray(boxes, dtype=np.float)\n    boxes *= scale\n    boxes[:, 0::2] -= offs[0]\n    boxes[:, 1::2] -= offs[1]\n    boxes = clip_boxes(boxes, im_shape)\n\n    if flip:\n        boxes_x = np.copy(boxes[:, 0])\n        boxes[:, 0] = im_shape[1] - boxes[:, 2]\n        boxes[:, 2] = im_shape[1] - boxes_x\n\n    return boxes\n\n\ndef preprocess_train(data, size_index):\n    im_path, blob, inp_size = data\n\n    boxes, gt_classes = blob[\'boxes\'], blob[\'gt_classes\']\n\n    im = cv2.imread(im_path)\n    ori_im = np.copy(im)\n\n    im, trans_param = imcv2_affine_trans(im)\n    scale, offs, flip = trans_param\n    boxes = _offset_boxes(boxes, im.shape, scale, offs, flip)\n\n    if inp_size is not None and size_index is not None:\n        inp_size = inp_size[size_index]\n        w, h = inp_size\n        boxes[:, 0::2] *= float(w) / im.shape[1]\n        boxes[:, 1::2] *= float(h) / im.shape[0]\n        im = cv2.resize(im, (w, h))\n    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n    im = imcv2_recolor(im)\n    # im /= 255.\n\n    # im = imcv2_recolor(im)\n    # h, w = inp_size\n    # im = cv2.resize(im, (w, h))\n    # im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n    # im /= 255\n    boxes = np.asarray(boxes, dtype=np.int)\n    return im, boxes, gt_classes, [], ori_im\n\n\ndef preprocess_test(data, size_index):\n\n    im, _, inp_size = data\n\n    if isinstance(im, str):\n        im = cv2.imread(im)\n    ori_im = np.copy(im)\n\n    if inp_size is not None and size_index is not None:\n        inp_size = inp_size[size_index]\n        w, h = inp_size\n        im = cv2.resize(im, (w, h))\n    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n    im = im / 255.\n\n    return im, [], [], [], ori_im\n\n\ndef postprocess(bbox_pred, iou_pred, prob_pred, im_shape, cfg, thresh=0.05,\n                size_index=0):\n    """"""\n    bbox_pred: (bsize, HxW, num_anchors, 4)\n               ndarray of float (sig(tx), sig(ty), exp(tw), exp(th))\n    iou_pred: (bsize, HxW, num_anchors, 1)\n    prob_pred: (bsize, HxW, num_anchors, num_classes)\n    """"""\n\n    # num_classes, num_anchors = cfg.num_classes, cfg.num_anchors\n    num_classes = cfg.num_classes\n    anchors = cfg.anchors\n    W, H = cfg.multi_scale_out_size[size_index]\n    assert bbox_pred.shape[0] == 1, \'postprocess only support one image per batch\'  # noqa\n\n    bbox_pred = yolo_to_bbox(\n        np.ascontiguousarray(bbox_pred, dtype=np.float),\n        np.ascontiguousarray(anchors, dtype=np.float),\n        H, W)\n    bbox_pred = np.reshape(bbox_pred, [-1, 4])\n    bbox_pred[:, 0::2] *= float(im_shape[1])\n    bbox_pred[:, 1::2] *= float(im_shape[0])\n    bbox_pred = bbox_pred.astype(np.int)\n\n    iou_pred = np.reshape(iou_pred, [-1])\n    prob_pred = np.reshape(prob_pred, [-1, num_classes])\n\n    cls_inds = np.argmax(prob_pred, axis=1)\n    prob_pred = prob_pred[(np.arange(prob_pred.shape[0]), cls_inds)]\n    scores = iou_pred * prob_pred\n    # scores = iou_pred\n    assert len(scores) == len(bbox_pred), \'{}, {}\'.format(scores.shape, bbox_pred.shape)\n    # threshold\n    keep = np.where(scores >= thresh)\n    bbox_pred = bbox_pred[keep]\n    scores = scores[keep]\n    cls_inds = cls_inds[keep]\n\n    # NMS\n    keep = np.zeros(len(bbox_pred), dtype=np.int)\n    for i in range(num_classes):\n        inds = np.where(cls_inds == i)[0]\n        if len(inds) == 0:\n            continue\n        c_bboxes = bbox_pred[inds]\n        c_scores = scores[inds]\n        c_keep = nms_detections(c_bboxes, c_scores, 0.3)\n        keep[inds[c_keep]] = 1\n\n    keep = np.where(keep > 0)\n    # keep = nms_detections(bbox_pred, scores, 0.3)\n    bbox_pred = bbox_pred[keep]\n    scores = scores[keep]\n    cls_inds = cls_inds[keep]\n\n    # clip\n    bbox_pred = clip_boxes(bbox_pred, im_shape)\n\n    return bbox_pred, scores, cls_inds\n\n\ndef _bbox_targets_perimage(im_shape, gt_boxes, cls_inds, dontcare_areas, cfg):\n    # num_classes, num_anchors = cfg.num_classes, cfg.num_anchors\n    # anchors = cfg.anchors\n    H, W = cfg.out_size\n    gt_boxes = np.asarray(gt_boxes, dtype=np.float)\n    # TODO: dontcare areas\n    dontcare_areas = np.asarray(dontcare_areas, dtype=np.float)\n\n    # locate the cell of each gt_boxe\n    cell_w = float(im_shape[1]) / W\n    cell_h = float(im_shape[0]) / H\n    cx = (gt_boxes[:, 0] + gt_boxes[:, 2]) * 0.5 / cell_w\n    cy = (gt_boxes[:, 1] + gt_boxes[:, 3]) * 0.5 / cell_h\n    cell_inds = np.floor(cy) * W + np.floor(cx)\n    cell_inds = cell_inds.astype(np.int)\n\n    # [x1, y1, x2, y2],  [class]\n    # gt_boxes[:, 0::2] /= im_shape[1]\n    # gt_boxes[:, 1::2] /= im_shape[0]\n    # gt_boxes[:, 0] = cx - np.floor(cx)\n    # gt_boxes[:, 1] = cy - np.floor(cy)\n    # gt_boxes[:, 2] = (gt_boxes[:, 2] - gt_boxes[:, 0]) / im_shape[1]\n    # gt_boxes[:, 3] = (gt_boxes[:, 3] - gt_boxes[:, 1]) / im_shape[0]\n\n    bbox_target = [[] for _ in range(H*W)]\n    cls_target = [[] for _ in range(H*W)]\n    for i, ind in enumerate(cell_inds):\n        bbox_target[ind].append(gt_boxes[i])\n        cls_target[ind].append(cls_inds[i])\n    return bbox_target, cls_target\n\n\ndef get_bbox_targets(images, gt_boxes, cls_inds, dontcares, cfg):\n    bbox_targets = []\n    cls_targets = []\n    for i, im in enumerate(images):\n        bbox_target, cls_target = _bbox_targets_perimage(im.shape,\n                                                         gt_boxes[i],\n                                                         cls_inds[i],\n                                                         dontcares[i],\n                                                         cfg)\n        bbox_targets.append(bbox_target)\n        cls_targets.append(cls_target)\n    return bbox_targets, cls_targets\n\n\ndef draw_detection(im, bboxes, scores, cls_inds, cfg, thr=0.3):\n    # draw image\n    colors = cfg.colors\n    labels = cfg.label_names\n\n    imgcv = np.copy(im)\n    h, w, _ = imgcv.shape\n    for i, box in enumerate(bboxes):\n        if scores[i] < thr:\n            continue\n        cls_indx = cls_inds[i]\n\n        thick = int((h + w) / 300)\n        cv2.rectangle(imgcv,\n                      (box[0], box[1]), (box[2], box[3]),\n                      colors[cls_indx], thick)\n        mess = \'%s: %.3f\' % (labels[cls_indx], scores[i])\n        cv2.putText(imgcv, mess, (box[0], box[1] - 12),\n                    0, 1e-3 * h, colors[cls_indx], thick // 3)\n\n    return imgcv\n'"
cfgs/exps/__init__.py,0,b''
cfgs/exps/darknet19_exp1.py,0,"b""exp_name = 'darknet19_voc07trainval_exp3'\n\npretrained_fname = 'darknet19.weights.npz'\n\nstart_step = 0\nlr_decay_epochs = {60, 90}\nlr_decay = 1./10\n\nmax_epoch = 160\n\nweight_decay = 0.0005\nmomentum = 0.9\ninit_learning_rate = 1e-3\n\n# for training yolo2\nobject_scale = 5.\nnoobject_scale = 1.\nclass_scale = 1.\ncoord_scale = 1.\niou_thresh = 0.6\n\n# dataset\nimdb_train = 'voc_2007_trainval'\nimdb_test = 'voc_2007_test'\nbatch_size = 1\ntrain_batch_size = 16\n"""
cfgs/exps/darknet19_exp2.py,0,"b""exp_name = 'darknet19_voc12trainval_exp1'\n\npretrained_fname = 'darknet19.weights.npz'\n\nstart_step = 0\nlr_decay_epochs = {60, 90}\nlr_decay = 1./10\n\nmax_epoch = 160\n\nweight_decay = 0.0005\nmomentum = 0.9\ninit_learning_rate = 1e-3\n\n# for training yolo2\nobject_scale = 5.\nnoobject_scale = 1.\nclass_scale = 1.\ncoord_scale = 1.\niou_thresh = 0.6\n\n# dataset\nimdb_train = 'voc_2012_trainval'\nimdb_test = 'voc_2012_test'\nbatch_size = 1\ntrain_batch_size = 16\n"""
layers/reorg/__init__.py,0,b''
layers/reorg/build.py,2,"b""import os\nimport torch\nfrom torch.utils.ffi import create_extension\n\n\nsources = ['src/reorg_cpu.c']\nheaders = ['src/reorg_cpu.h']\ndefines = []\nwith_cuda = False\n\nif torch.cuda.is_available():\n    print('Including CUDA code.')\n    sources += ['src/reorg_cuda.c']\n    headers += ['src/reorg_cuda.h']\n    defines += [('WITH_CUDA', None)]\n    with_cuda = True\n\nthis_file = os.path.dirname(os.path.realpath(__file__))\n# print(this_file)\nextra_objects = ['src/reorg_cuda_kernel.cu.o']\nextra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\n\nffi = create_extension(\n    '_ext.reorg_layer',\n    headers=headers,\n    sources=sources,\n    define_macros=defines,\n    relative_to=__file__,\n    with_cuda=with_cuda,\n    extra_objects=extra_objects\n)\n\nif __name__ == '__main__':\n    ffi.build()\n"""
layers/reorg/reorg_layer.py,4,"b'import torch\nfrom torch.autograd import Function\nfrom ._ext import reorg_layer\n\n\nclass ReorgFunction(Function):\n    def __init__(self, stride=2):\n        self.stride = stride\n\n    def forward(self, x):\n        stride = self.stride\n\n        bsize, c, h, w = x.size()\n        out_w, out_h, out_c = int(w / stride), int(h / stride), c * (stride * stride)  # noqa\n        out = torch.FloatTensor(bsize, out_c, out_h, out_w)\n\n        if x.is_cuda:\n            out = out.cuda()\n            reorg_layer.reorg_cuda(x, out_w, out_h, out_c, bsize,\n                                   stride, 0, out)\n        else:\n            reorg_layer.reorg_cpu(x, out_w, out_h, out_c, bsize,\n                                  stride, 0, out)\n\n        return out\n\n    def backward(self, grad_top):\n        stride = self.stride\n        bsize, c, h, w = grad_top.size()\n\n        out_w, out_h, out_c = w * stride, h * stride, c / (stride * stride)\n        grad_bottom = torch.FloatTensor(bsize, int(out_c), out_h, out_w)\n\n        # rev_stride = 1. / stride    # reverse\n        if grad_top.is_cuda:\n            grad_bottom = grad_bottom.cuda()\n            reorg_layer.reorg_cuda(grad_top, w, h, c, bsize,\n                                   stride, 1, grad_bottom)\n        else:\n            reorg_layer.reorg_cpu(grad_top, w, h, c, bsize,\n                                  stride, 1, grad_bottom)\n\n        return grad_bottom\n\n\nclass ReorgLayer(torch.nn.Module):\n    def __init__(self, stride):\n        super(ReorgLayer, self).__init__()\n\n        self.stride = stride\n\n    def forward(self, x):\n        x = ReorgFunction(self.stride)(x)\n        return x\n'"
layers/roi_pooling/__init__.py,0,b''
layers/roi_pooling/build.py,2,"b""import os\nimport torch\nfrom torch.utils.ffi import create_extension\n\n\nsources = ['src/roi_pooling.c']\nheaders = ['src/roi_pooling.h']\ndefines = []\nwith_cuda = False\n\nif torch.cuda.is_available():\n    print('Including CUDA code.')\n    sources += ['src/roi_pooling_cuda.c']\n    headers += ['src/roi_pooling_cuda.h']\n    defines += [('WITH_CUDA', None)]\n    with_cuda = True\n\nthis_file = os.path.dirname(os.path.realpath(__file__))\nprint(this_file)\nextra_objects = ['src/cuda/roi_pooling_kernel.cu.o']\nextra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\n\nffi = create_extension(\n    '_ext.roi_pooling',\n    headers=headers,\n    sources=sources,\n    define_macros=defines,\n    relative_to=__file__,\n    with_cuda=with_cuda,\n    extra_objects=extra_objects\n)\n\nif __name__ == '__main__':\n    ffi.build()\n"""
layers/roi_pooling/roi_pool.py,5,"b'import torch\nfrom torch.autograd import Function\nfrom ._ext import roi_pooling\n\n\nclass RoIPoolFunction(Function):\n    def __init__(self, pooled_height, pooled_width, spatial_scale):\n        self.pooled_width = int(pooled_width)\n        self.pooled_height = int(pooled_height)\n        self.spatial_scale = float(spatial_scale)\n        self.output = None\n        self.argmax = None\n        self.rois = None\n        self.feature_size = None\n\n    def forward(self, features, rois):\n        batch_size, num_channels, data_height, data_width = features.size()\n        num_rois = rois.size()[0]\n        output = torch.zeros(num_rois, num_channels,\n                             self.pooled_height, self.pooled_width)\n        argmax = torch.IntTensor(num_rois, num_channels,\n                                 self.pooled_height, self.pooled_width).zero_()\n\n        if not features.is_cuda:\n            _features = features.permute(0, 2, 3, 1)\n            roi_pooling.roi_pooling_forward(self.pooled_height,\n                                            self.pooled_width,\n                                            self.spatial_scale,\n                                            _features,\n                                            rois,\n                                            output)\n            # output = output.cuda()\n        else:\n            output = output.cuda()\n            argmax = argmax.cuda()\n            roi_pooling.roi_pooling_forward_cuda(self.pooled_height,\n                                                 self.pooled_width,\n                                                 self.spatial_scale,\n                                                 features,\n                                                 rois,\n                                                 output,\n                                                 argmax)\n            self.output = output\n            self.argmax = argmax\n            self.rois = rois\n            self.feature_size = features.size()\n\n        return output\n\n    def backward(self, grad_output):\n        assert(self.feature_size is not None and grad_output.is_cuda)\n\n        batch_size, num_channels, data_height, data_width = self.feature_size\n\n        grad_input = torch.zeros(batch_size, num_channels,\n                                 data_height, data_width).cuda()\n        roi_pooling.roi_pooling_backward_cuda(self.pooled_height,\n                                              self.pooled_width,\n                                              self.spatial_scale,\n                                              grad_output,\n                                              self.rois,\n                                              grad_input,\n                                              self.argmax)\n\n        # print grad_input\n\n        return grad_input, None\n\n\nclass RoIPool(torch.nn.Module):\n    def __init__(self, pooled_height, pooled_width, spatial_scale):\n        super(RoIPool, self).__init__()\n\n        self.pooled_width = int(pooled_width)\n        self.pooled_height = int(pooled_height)\n        self.spatial_scale = float(spatial_scale)\n\n    def forward(self, features, rois):\n        return RoIPoolFunction(self.pooled_height, self.pooled_width, self.spatial_scale)(features, rois)  # noqa\n'"
layers/roi_pooling/roi_pool_py.py,5,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np\n\n\nclass RoIPool(nn.Module):\n    def __init__(self, pooled_height, pooled_width, spatial_scale):\n        super(RoIPool, self).__init__()\n        self.pooled_width = int(pooled_width)\n        self.pooled_height = int(pooled_height)\n        self.spatial_scale = float(spatial_scale)\n\n    def forward(self, features, rois):\n        batch_size, num_channels, data_height, data_width = features.size()\n        num_rois = rois.size()[0]\n        outputs = Variable(torch.zeros(num_rois, num_channels,\n                                       self.pooled_height,\n                                       self.pooled_width)).cuda()\n\n        for roi_ind, roi in enumerate(rois):\n            batch_ind = int(roi[0].data[0])\n            roi_start_w, roi_start_h, roi_end_w, roi_end_h = np.round(\n                roi[1:].data.cpu().numpy() * self.spatial_scale).astype(int)\n            roi_width = max(roi_end_w - roi_start_w + 1, 1)\n            roi_height = max(roi_end_h - roi_start_h + 1, 1)\n            bin_size_w = float(roi_width) / float(self.pooled_width)\n            bin_size_h = float(roi_height) / float(self.pooled_height)\n\n            for ph in range(self.pooled_height):\n                hstart = int(np.floor(ph * bin_size_h))\n                hend = int(np.ceil((ph + 1) * bin_size_h))\n                hstart = min(data_height, max(0, hstart + roi_start_h))\n                hend = min(data_height, max(0, hend + roi_start_h))\n                for pw in range(self.pooled_width):\n                    wstart = int(np.floor(pw * bin_size_w))\n                    wend = int(np.ceil((pw + 1) * bin_size_w))\n                    wstart = min(data_width, max(0, wstart + roi_start_w))\n                    wend = min(data_width, max(0, wend + roi_start_w))\n\n                    is_empty = (hend <= hstart) or(wend <= wstart)\n                    if is_empty:\n                        outputs[roi_ind, :, ph, pw] = 0\n                    else:\n                        data = features[batch_ind]\n                        outputs[roi_ind, :, ph, pw] = torch.max(\n                            torch.max(data[:, hstart:hend, wstart:wend], 1)[0], 2)[0].view(-1)  # noqa\n\n        return outputs\n'"
utils/nms/__init__.py,0,b''
utils/nms/py_cpu_nms.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport numpy as np\n\n\ndef py_cpu_nms(dets, thresh):\n    """"""Pure Python NMS baseline.""""""\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    x2 = dets[:, 2]\n    y2 = dets[:, 3]\n    scores = dets[:, 4]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr <= thresh)[0]\n        order = order[inds + 1]\n\n    return keep\n'"
utils/pycocotools/__init__.py,0,"b""__author__ = 'tylin'\n"""
utils/pycocotools/coco.py,0,"b'__author__ = \'tylin\'\n__version__ = \'1.0.1\'\n# Interface for accessing the Microsoft COCO dataset.\n\n# Microsoft COCO is a large image dataset designed for object detection,\n# segmentation, and caption generation. pycocotools is a Python API that\n# assists in loading, parsing and visualizing the annotations in COCO.\n# Please visit http://mscoco.org/ for more information on COCO, including\n# for the data, paper, and tutorials. The exact format of the annotations\n# is also described on the COCO website. For example usage of the pycocotools\n# please see pycocotools_demo.ipynb. In addition to this API,\n# please download both\n# the COCO images and annotations in order to run the demo.\n\n# An alternative to using the API is to load the annotations directly\n# into Python dictionary\n# Using the API provides additional utility functions. Note that this API\n# supports both *instance* and *caption* annotations. In the case of\n# captions not all functions are defined (e.g. categories are undefined).\n\n# The following API functions are defined:\n#  COCO       - COCO api class that loads COCO annotation file\n#               and prepare data structures.\n#  decodeMask - Decode binary mask M encoded via run-length encoding.\n#  encodeMask - Encode binary mask M using run-length encoding.\n#  getAnnIds  - Get ann ids that satisfy given filter conditions.\n#  getCatIds  - Get cat ids that satisfy given filter conditions.\n#  getImgIds  - Get img ids that satisfy given filter conditions.\n#  loadAnns   - Load anns with the specified ids.\n#  loadCats   - Load cats with the specified ids.\n#  loadImgs   - Load imgs with the specified ids.\n#  segToMask  - Convert polygon segmentation to binary mask.\n#  showAnns   - Display the specified annotations.\n#  loadRes    - Load algorithm results and create API for accessing them.\n#  download   - Download COCO images from mscoco.org server.\n# Throughout the API ""ann""=annotation, ""cat""=category, and ""img""=image.\n# Help on each functions can be accessed by: ""help COCO>function"".\n\n# See also COCO>decodeMask,\n# COCO>encodeMask, COCO>getAnnIds, COCO>getCatIds,\n# COCO>getImgIds, COCO>loadAnns, COCO>loadCats,\n# COCO>loadImgs, COCO>segToMask, COCO>showAnns\n\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2014.\n# Licensed under the Simplified BSD License [see bsd.txt]\n\nimport json\nimport time\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import PatchCollection\nfrom matplotlib.patches import Polygon\nimport numpy as np\n# from skimage.draw import polygon\nimport urllib.request\nimport urllib.parse\nimport urllib.error\nimport copy\nimport itertools\nfrom . import mask\nimport os\n\n\nclass COCO:\n\n    def __init__(self, annotation_file=None):\n        """"""\n        Constructor of Microsoft COCO helper class for\n        reading and visualizing annotations.\n        :param annotation_file (str): location of annotation file\n        :param image_folder (str): location to the folder that hosts images.\n        :return:\n        """"""\n        # load dataset\n        self.dataset = {}\n        self.anns = []\n        self.imgToAnns = {}\n        self.catToImgs = {}\n        self.imgs = {}\n        self.cats = {}\n        if annotation_file is not None:\n            print(\'loading annotations into memory...\')\n            tic = time.time()\n            dataset = json.load(open(annotation_file, \'r\'))\n            print(\'Done (t=%0.2fs)\' % (time.time() - tic))\n            self.dataset = dataset\n            self.createIndex()\n\n    def createIndex(self):\n        # create index\n        print(\'creating index...\')\n        anns = {}\n        imgToAnns = {}\n        catToImgs = {}\n        cats = {}\n        imgs = {}\n        if \'annotations\' in self.dataset:\n            imgToAnns = {ann[\'image_id\']: [] for ann in self.dataset[\'annotations\']}  # noqa\n            anns = {ann[\'id\']:       [] for ann in self.dataset[\'annotations\']}\n            for ann in self.dataset[\'annotations\']:\n                imgToAnns[ann[\'image_id\']] += [ann]\n                anns[ann[\'id\']] = ann\n\n        if \'images\' in self.dataset:\n            imgs = {im[\'id\']: {} for im in self.dataset[\'images\']}\n            for img in self.dataset[\'images\']:\n                imgs[img[\'id\']] = img\n\n        if \'categories\' in self.dataset:\n            cats = {cat[\'id\']: [] for cat in self.dataset[\'categories\']}\n            for cat in self.dataset[\'categories\']:\n                cats[cat[\'id\']] = cat\n            catToImgs = {cat[\'id\']: [] for cat in self.dataset[\'categories\']}\n            if \'annotations\' in self.dataset:\n                for ann in self.dataset[\'annotations\']:\n                    catToImgs[ann[\'category_id\']] += [ann[\'image_id\']]\n\n        print(\'index created!\')\n\n        # create class members\n        self.anns = anns\n        self.imgToAnns = imgToAnns\n        self.catToImgs = catToImgs\n        self.imgs = imgs\n        self.cats = cats\n\n    def info(self):\n        """"""\n        Print information about the annotation file.\n        :return:\n        """"""\n        for key, value in list(self.dataset[\'info\'].items()):\n            print(\'%s: %s\' % (key, value))\n\n    def getAnnIds(self, imgIds=[], catIds=[], areaRng=[], iscrowd=None):\n        """"""\n        Get ann ids that satisfy given filter conditions.\n        default skips that filter\n        :param imgIds  (int array)     : get anns for given imgs\n               catIds  (int array)     : get anns for given cats\n               areaRng (float array)   : get anns for given\n                                         area range (e.g. [0 inf])\n               iscrowd (boolean)       : get anns for given\n                                         crowd label (False or True)\n        :return: ids (int array)       : integer array of ann ids\n        """"""\n        imgIds = imgIds if type(imgIds) == list else [imgIds]\n        catIds = catIds if type(catIds) == list else [catIds]\n\n        if len(imgIds) == len(catIds) == len(areaRng) == 0:\n            anns = self.dataset[\'annotations\']\n        else:\n            if not len(imgIds) == 0:\n                # this can be changed by defaultdict\n                lists = [self.imgToAnns[imgId] for imgId in imgIds\n                         if imgId in self.imgToAnns]\n                anns = list(itertools.chain.from_iterable(lists))\n            else:\n                anns = self.dataset[\'annotations\']\n            anns = anns if len(catIds)  == 0 else [ann for ann in anns if ann[\'category_id\'] in catIds]  # noqa\n            anns = anns if len(areaRng) == 0 \\\n                else [ann for ann in anns if ann[\'area\'] > areaRng[0] and ann[\'area\'] < areaRng[1]]  # noqa\n        if iscrowd is not None:\n            ids = [ann[\'id\'] for ann in anns if ann[\'iscrowd\'] == iscrowd]\n        else:\n            ids = [ann[\'id\'] for ann in anns]\n        return ids\n\n    def getCatIds(self, catNms=[], supNms=[], catIds=[]):\n        """"""\n        filtering parameters. default skips that filter.\n        :param catNms (str array)  : get cats for given cat names\n        :param supNms (str array)  : get cats for given supercategory names\n        :param catIds (int array)  : get cats for given cat ids\n        :return: ids (int array)   : integer array of cat ids\n        """"""\n        catNms = catNms if type(catNms) == list else [catNms]\n        supNms = supNms if type(supNms) == list else [supNms]\n        catIds = catIds if type(catIds) == list else [catIds]\n\n        if len(catNms) == len(supNms) == len(catIds) == 0:\n            cats = self.dataset[\'categories\']\n        else:\n            cats = self.dataset[\'categories\']\n            cats = cats if len(catNms) == 0 else [cat for cat in cats if cat[\'name\'] in catNms]  # noqa\n            cats = cats if len(supNms) == 0 else [cat for cat in cats if cat[\'supercategory\'] in supNms]  # noqa\n            cats = cats if len(catIds) == 0 else [cat for cat in cats if cat[\'id\'] in catIds]  # noqa\n        ids = [cat[\'id\'] for cat in cats]\n        return ids\n\n    def getImgIds(self, imgIds=[], catIds=[]):\n        \'\'\'\n        Get img ids that satisfy given filter conditions.\n        :param imgIds (int array) : get imgs for given ids\n        :param catIds (int array) : get imgs with all given cats\n        :return: ids (int array)  : integer array of img ids\n        \'\'\'\n        imgIds = imgIds if type(imgIds) == list else [imgIds]\n        catIds = catIds if type(catIds) == list else [catIds]\n\n        if len(imgIds) == len(catIds) == 0:\n            ids = list(self.imgs.keys())\n        else:\n            ids = set(imgIds)\n            for i, catId in enumerate(catIds):\n                if i == 0 and len(ids) == 0:\n                    ids = set(self.catToImgs[catId])\n                else:\n                    ids &= set(self.catToImgs[catId])\n        return list(ids)\n\n    def loadAnns(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying anns\n        :return: anns (object array) : loaded ann objects\n        """"""\n        if type(ids) == list:\n            return [self.anns[id] for id in ids]\n        elif type(ids) == int:\n            return [self.anns[ids]]\n\n    def loadCats(self, ids=[]):\n        """"""\n        Load cats with the specified ids.\n        :param ids (int array)       : integer ids specifying cats\n        :return: cats (object array) : loaded cat objects\n        """"""\n        if type(ids) == list:\n            return [self.cats[id] for id in ids]\n        elif type(ids) == int:\n            return [self.cats[ids]]\n\n    def loadImgs(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying img\n        :return: imgs (object array) : loaded img objects\n        """"""\n        if type(ids) == list:\n            return [self.imgs[id] for id in ids]\n        elif type(ids) == int:\n            return [self.imgs[ids]]\n\n    def showAnns(self, anns):\n        """"""\n        Display the specified annotations.\n        :param anns (array of object): annotations to display\n        :return: None\n        """"""\n        if len(anns) == 0:\n            return 0\n        if \'segmentation\' in anns[0]:\n            datasetType = \'instances\'\n        elif \'caption\' in anns[0]:\n            datasetType = \'captions\'\n        if datasetType == \'instances\':\n            ax = plt.gca()\n            polygons = []\n            color = []\n            for ann in anns:\n                c = np.random.random((1, 3)).tolist()[0]\n                if type(ann[\'segmentation\']) == list:\n                    # polygon\n                    for seg in ann[\'segmentation\']:\n                        poly = np.array(seg).reshape((len(seg)/2, 2))\n                        polygons.append(Polygon(poly, True, alpha=0.4))\n                        color.append(c)\n                else:\n                    # mask\n                    t = self.imgs[ann[\'image_id\']]\n                    if type(ann[\'segmentation\'][\'counts\']) == list:\n                        rle = mask.frPyObjects([ann[\'segmentation\']],\n                                               t[\'height\'], t[\'width\'])\n                    else:\n                        rle = [ann[\'segmentation\']]\n                    m = mask.decode(rle)\n                    img = np.ones((m.shape[0], m.shape[1], 3))\n                    if ann[\'iscrowd\'] == 1:\n                        color_mask = np.array([2.0, 166.0, 101.0])/255\n                    if ann[\'iscrowd\'] == 0:\n                        color_mask = np.random.random((1, 3)).tolist()[0]\n                    for i in range(3):\n                        img[:, :, i] = color_mask[i]\n                    ax.imshow(np.dstack((img, m*0.5)))\n            p = PatchCollection(polygons,\n                                facecolors=color,\n                                edgecolors=(0, 0, 0, 1),\n                                linewidths=3, alpha=0.4)\n            ax.add_collection(p)\n        elif datasetType == \'captions\':\n            for ann in anns:\n                print(ann[\'caption\'])\n\n    def loadRes(self, resFile):\n        """"""\n        Load result file and return a result api object.\n        :param   resFile (str)     : file name of result file\n        :return: res (obj)         : result api object\n        """"""\n        res = COCO()\n        res.dataset[\'images\'] = [img for img in self.dataset[\'images\']]\n        # res.dataset[\'info\'] = copy.deepcopy(self.dataset[\'info\'])\n        # res.dataset[\'licenses\'] = copy.deepcopy(self.dataset[\'licenses\'])\n\n        print(\'Loading and preparing results...     \')\n        tic = time.time()\n        anns = json.load(open(resFile))\n        assert type(anns) == list, \'results in not an array of objects\'\n        annsImgIds = [ann[\'image_id\'] for ann in anns]\n        assert set(annsImgIds) == (set(annsImgIds) &\n                                   set(self.getImgIds())), \'Results do not correspond to current coco set\'  # noqa\n        if \'caption\' in anns[0]:\n            imgIds = set([img[\'id\'] for img in res.dataset[\'images\']]) & \\\n                     set([ann[\'image_id\'] for ann in anns])\n            res.dataset[\'images\'] = [img for img in res.dataset[\'images\']\n                                     if img[\'id\'] in imgIds]\n            for id, ann in enumerate(anns):\n                ann[\'id\'] = id + 1\n        elif \'bbox\' in anns[0] and not anns[0][\'bbox\'] == []:\n            res.dataset[\'categories\'] = \\\n                copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                bb = ann[\'bbox\']\n                x1, x2, y1, y2 = [bb[0], bb[0]+bb[2], bb[1], bb[1]+bb[3]]\n                if \'segmentation\' not in ann:\n                    ann[\'segmentation\'] = [[x1, y1, x1, y2, x2, y2, x2, y1]]\n                ann[\'area\'] = bb[2]*bb[3]\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        elif \'segmentation\' in anns[0]:\n            res.dataset[\'categories\'] = \\\n                copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                # now only support compressed RLE format as\n                # segmentation results\n                ann[\'area\'] = mask.area([ann[\'segmentation\']])[0]\n                if \'bbox\' not in ann:\n                    ann[\'bbox\'] = mask.toBbox([ann[\'segmentation\']])[0]\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        print(\'DONE (t=%0.2fs)\' % (time.time() - tic))\n\n        res.dataset[\'annotations\'] = anns\n        res.createIndex()\n        return res\n\n    def download(self, tarDir=None, imgIds=[]):\n        \'\'\'\n        Download COCO images from mscoco.org server.\n        :param tarDir (str): COCO results directory name\n               imgIds (list): images to be downloaded\n        :return:\n        \'\'\'\n        if tarDir is None:\n            print(\'Please specify target directory\')\n            return -1\n        if len(imgIds) == 0:\n            imgs = list(self.imgs.values())\n        else:\n            imgs = self.loadImgs(imgIds)\n        N = len(imgs)\n        if not os.path.exists(tarDir):\n            os.makedirs(tarDir)\n        for i, img in enumerate(imgs):\n            tic = time.time()\n            fname = os.path.join(tarDir, img[\'file_name\'])\n            if not os.path.exists(fname):\n                urllib.request.urlretrieve(img[\'coco_url\'], fname)\n            print(\'downloaded %d/%d images (t=%.1fs)\' %\n                  (i, N, time.time() - tic))\n'"
utils/pycocotools/cocoeval.py,0,"b'__author__ = \'tsungyi\'\n\nimport numpy as np\nimport datetime\nimport time\nfrom collections import defaultdict\nfrom . import mask\nimport copy\n\n\nclass COCOeval:\n    # Interface for evaluating detection on the Microsoft COCO dataset.\n    #\n    # The usage for CocoEval is as follows:\n    #  cocoGt=..., cocoDt=...       # load dataset and results\n    #  E = CocoEval(cocoGt,cocoDt); # initialize CocoEval object\n    #  E.params.recThrs = ...;      # set parameters as desired\n    #  E.evaluate();                # run per image evaluation\n    #  E.accumulate();              # accumulate per image results\n    #  E.summarize();               # display summary metrics of results\n    # For example usage see evalDemo.m and http://mscoco.org/.\n    #\n    # The evaluation parameters are as follows (defaults in brackets):\n    #  imgIds     - [all] N img ids to use for evaluation\n    #  catIds     - [all] K cat ids to use for evaluation\n    #  iouThrs    - [.5:.05:.95] T=10 IoU thresholds for evaluation\n    #  recThrs    - [0:.01:1] R=101 recall thresholds for evaluation\n    #  areaRng    - [...] A=4 object area ranges for evaluation\n    #  maxDets    - [1 10 100] M=3 thresholds on max detections per image\n    #  useSegm    - [1] if true evaluate against ground-truth segments\n    #  useCats    - [1] if true use category labels for evaluation\n    #     Note: if useSegm=0 the evaluation is run on bounding boxes.\n    # Note: if useCats=0 category labels are ignored as in proposal scoring.\n    # Note: multiple areaRngs [Ax2] and maxDets [Mx1] can be specified.\n    #\n    # evaluate(): evaluates detections on every image and every category and\n    # concats the results into the ""evalImgs"" with fields:\n    #  dtIds      - [1xD] id for each of the D detections (dt)\n    #  gtIds      - [1xG] id for each of the G ground truths (gt)\n    #  dtMatches  - [TxD] matching gt id at each IoU or 0\n    #  gtMatches  - [TxG] matching dt id at each IoU or 0\n    #  dtScores   - [1xD] confidence of each dt\n    #  gtIgnore   - [1xG] ignore flag for each gt\n    #  dtIgnore   - [TxD] ignore flag for each dt at each IoU\n    #\n    # accumulate(): accumulates the per-image, per-category evaluation\n    # results in ""evalImgs"" into the dictionary ""eval"" with fields:\n    #  params     - parameters used for evaluation\n    #  date       - date evaluation was performed\n    #  counts     - [T,R,K,A,M] parameter dimensions (see above)\n    #  precision  - [TxRxKxAxM] precision for every evaluation setting\n    #  recall     - [TxKxAxM] max recall for every evaluation setting\n    # Note: precision and recall==-1 for settings with no gt objects.\n    #\n    # See also coco, mask, pycocoDemo, pycocoEvalDemo\n    #\n    # Microsoft COCO Toolbox.      version 2.0\n    # Data, paper, and tutorials available at:  http://mscoco.org/\n    # Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n    # Licensed under the Simplified BSD License [see coco/license.txt]\n    def __init__(self, cocoGt=None, cocoDt=None):\n        \'\'\'\n        Initialize CocoEval using coco APIs for gt and dt\n        :param cocoGt: coco object with ground truth annotations\n        :param cocoDt: coco object with detection results\n        :return: None\n        \'\'\'\n        self.cocoGt = cocoGt              # ground truth COCO API\n        self.cocoDt = cocoDt              # detections COCO API\n        self.params = {}                  # evaluation parameters\n        # per-image per-category evaluation results [KxAxI] elements\n        self.evalImgs = defaultdict(list)\n        self.eval = {}                  # accumulated evaluation results\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        self.params = Params()              # parameters\n        self._paramsEval = {}               # parameters for evaluation\n        self.stats = []                     # result summarization\n        self.ious = {}                      # ious between all gts and dts\n        if cocoGt is not None:\n            self.params.imgIds = sorted(cocoGt.getImgIds())\n            self.params.catIds = sorted(cocoGt.getCatIds())\n\n    def _prepare(self):\n        \'\'\'\n        Prepare ._gts and ._dts for evaluation based on params\n        :return: None\n        \'\'\'\n        #\n        def _toMask(objs, coco):\n            # modify segmentation by reference\n            for obj in objs:\n                t = coco.imgs[obj[\'image_id\']]\n                if type(obj[\'segmentation\']) == list:\n                    if type(obj[\'segmentation\'][0]) == dict:\n                        print(\'debug\')\n                    obj[\'segmentation\'] = mask.frPyObjects(obj[\'segmentation\'],\n                                                           t[\'height\'],\n                                                           t[\'width\'])\n                    if len(obj[\'segmentation\']) == 1:\n                        obj[\'segmentation\'] = obj[\'segmentation\'][0]\n                    else:\n                        # an object can have multiple polygon regions\n                        # merge them into one RLE mask\n                        obj[\'segmentation\'] = mask.merge(obj[\'segmentation\'])\n                elif type(obj[\'segmentation\']) == dict and type(obj[\'segmentation\'][\'counts\']) == list:  # noqa\n                    obj[\'segmentation\'] = \\\n                        mask.frPyObjects([obj[\'segmentation\']], t[\'height\'], t[\'width\'])[0]  # noqa\n                elif type(obj[\'segmentation\']) == dict and \\\n                     type(obj[\'segmentation\'][\'counts\'] == str or\n                                          type(obj[\'segmentation\'][\'counts\']) == str):  # noqa\n                    pass\n                else:\n                    raise Exception(\'segmentation format not supported.\')\n        p = self.params\n        if p.useCats:\n            gts = self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds,\n                                                             catIds=p.catIds))\n            dts = self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds,\n                                                             catIds=p.catIds))\n        else:\n            gts = self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds))\n            dts = self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds))\n\n        if p.useSegm:\n            _toMask(gts, self.cocoGt)\n            _toMask(dts, self.cocoDt)\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        for gt in gts:\n            self._gts[gt[\'image_id\'], gt[\'category_id\']].append(gt)\n        for dt in dts:\n            self._dts[dt[\'image_id\'], dt[\'category_id\']].append(dt)\n        # per-image per-category evaluation results\n        self.evalImgs = defaultdict(list)\n        self.eval = {}                  # accumulated evaluation results\n\n    def evaluate(self):\n        \'\'\'\n        Run per image evaluation on given images\n        and store results (a list of dict) in self.evalImgs\n        :return: None\n        \'\'\'\n        tic = time.time()\n        print(\'Running per image evaluation...      \')\n        p = self.params\n        p.imgIds = list(np.unique(p.imgIds))\n        if p.useCats:\n            p.catIds = list(np.unique(p.catIds))\n        p.maxDets = sorted(p.maxDets)\n        self.params = p\n\n        self._prepare()\n        # loop through images, area range, max detection number\n        catIds = p.catIds if p.useCats else [-1]\n\n        computeIoU = self.computeIoU\n        self.ious = {(imgId, catId): computeIoU(imgId, catId)\n                     for imgId in p.imgIds\n                     for catId in catIds}\n\n        evaluateImg = self.evaluateImg\n        maxDet = p.maxDets[-1]\n        self.evalImgs = [evaluateImg(imgId, catId, areaRng, maxDet)\n                         for catId in catIds\n                         for areaRng in p.areaRng\n                         for imgId in p.imgIds]\n        self._paramsEval = copy.deepcopy(self.params)\n        toc = time.time()\n        print(\'DONE (t=%0.2fs).\' % (toc-tic))\n\n    def computeIoU(self, imgId, catId):\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId, catId]\n            dt = self._dts[imgId, catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId, cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId, cId]]\n        if len(gt) == 0 and len(dt) == 0:\n            return []\n        dt = sorted(dt, key=lambda x: -x[\'score\'])\n        if len(dt) > p.maxDets[-1]:\n            dt = dt[0:p.maxDets[-1]]\n\n        if p.useSegm:\n            g = [g[\'segmentation\'] for g in gt]\n            d = [d[\'segmentation\'] for d in dt]\n        else:\n            g = [g[\'bbox\'] for g in gt]\n            d = [d[\'bbox\'] for d in dt]\n\n        # compute iou between each dt and gt region\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        ious = mask.iou(d, g, iscrowd)\n        return ious\n\n    def evaluateImg(self, imgId, catId, aRng, maxDet):\n        \'\'\'\n        perform evaluation for single category and image\n        :return: dict (single image results)\n        \'\'\'\n        #\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId, catId]\n            dt = self._dts[imgId, catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId, cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId, cId]]\n        if len(gt) == 0 and len(dt) == 0:\n            return None\n\n        for g in gt:\n            if \'ignore\' not in g:\n                g[\'ignore\'] = 0\n            if g[\'iscrowd\'] == 1 or g[\'ignore\'] or \\\n                    (g[\'area\'] < aRng[0] or g[\'area\'] > aRng[1]):\n                g[\'_ignore\'] = 1\n            else:\n                g[\'_ignore\'] = 0\n\n        # sort dt highest score first, sort gt ignore last\n        # gt = sorted(gt, key=lambda x: x[\'_ignore\'])\n        gtind = [ind for (ind, g) in sorted(\n            enumerate(gt), key=lambda ind_g: ind_g[1][\'_ignore\'])]\n\n        gt = [gt[ind] for ind in gtind]\n        dt = sorted(dt, key=lambda x: -x[\'score\'])[0:maxDet]\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        # load computed ious\n        N_iou = len(self.ious[imgId, catId])\n        ious = self.ious[imgId, catId][0:maxDet, np.array(gtind)] \\\n            if N_iou > 0 else self.ious[imgId, catId]\n\n        T = len(p.iouThrs)\n        G = len(gt)\n        D = len(dt)\n        gtm = np.zeros((T, G))\n        dtm = np.zeros((T, D))\n        gtIg = np.array([g[\'_ignore\'] for g in gt])\n        dtIg = np.zeros((T, D))\n        if not len(ious) == 0:\n            for tind, t in enumerate(p.iouThrs):\n                for dind, d in enumerate(dt):\n                    # information about best match so far (m=-1 -> unmatched)\n                    iou = min([t, 1-1e-10])\n                    m = -1\n                    for gind, g in enumerate(gt):\n                        # if this gt already matched, and not a crowd, continue\n                        if gtm[tind, gind] > 0 and not iscrowd[gind]:\n                            continue\n                        # if dt matched to reg gt, and on ignore gt, stop\n                        if m > -1 and gtIg[m] == 0 and gtIg[gind] == 1:\n                            break\n                        # continue to next gt unless better match made\n                        if ious[dind, gind] < iou:\n                            continue\n                        # match successful and best so far, store appropriately\n                        iou = ious[dind, gind]\n                        m = gind\n                    # if match made store id of match for both dt and gt\n                    if m == -1:\n                        continue\n                    dtIg[tind, dind] = gtIg[m]\n                    dtm[tind, dind] = gt[m][\'id\']\n                    gtm[tind, m] = d[\'id\']\n        # set unmatched detections outside of area range to ignore\n        a = np.array([d[\'area\'] < aRng[0] or\n                      d[\'area\'] > aRng[1] for d in dt]).reshape((1, len(dt)))\n        dtIg = np.logical_or(dtIg, np.logical_and(dtm == 0,\n                                                  np.repeat(a, T, 0)))\n        # store results for given image and category\n        return {\n                \'image_id\':     imgId,\n                \'category_id\':  catId,\n                \'aRng\':         aRng,\n                \'maxDet\':       maxDet,\n                \'dtIds\':        [d[\'id\'] for d in dt],\n                \'gtIds\':        [g[\'id\'] for g in gt],\n                \'dtMatches\':    dtm,\n                \'gtMatches\':    gtm,\n                \'dtScores\':     [d[\'score\'] for d in dt],\n                \'gtIgnore\':     gtIg,\n                \'dtIgnore\':     dtIg,\n            }\n\n    def accumulate(self, p=None):\n        \'\'\'\n        Accumulate per image evaluation results and\n        store the result in self.eval\n        :param p: input params for evaluation\n        :return: None\n        \'\'\'\n        print(\'Accumulating evaluation results...   \')\n        tic = time.time()\n        if not self.evalImgs:\n            print(\'Please run evaluate() first\')\n        # allows input customized parameters\n        if p is None:\n            p = self.params\n        p.catIds = p.catIds if p.useCats == 1 else [-1]\n        T = len(p.iouThrs)\n        R = len(p.recThrs)\n        K = len(p.catIds) if p.useCats else 1\n        A = len(p.areaRng)\n        M = len(p.maxDets)\n        # -1 for the precision of absent categories\n        precision = -np.ones((T, R, K, A, M))\n        recall = -np.ones((T, K, A, M))\n\n        # create dictionary for future indexing\n        _pe = self._paramsEval\n        catIds = _pe.catIds if _pe.useCats else [-1]\n        setK = set(catIds)\n        setA = set(map(tuple, _pe.areaRng))\n        setM = set(_pe.maxDets)\n        setI = set(_pe.imgIds)\n        # get inds to evaluate\n        k_list = [n for n, k in enumerate(p.catIds) if k in setK]\n        m_list = [m for n, m in enumerate(p.maxDets) if m in setM]\n        a_list = [n for n, a in enumerate([tuple(x) for x in p.areaRng])\n                  if a in setA]\n        i_list = [n for n, i in enumerate(p.imgIds) if i in setI]\n        # K0 = len(_pe.catIds)\n        I0 = len(_pe.imgIds)\n        A0 = len(_pe.areaRng)\n        # retrieve E at each category, area range, and max number of detections\n        for k, k0 in enumerate(k_list):\n            Nk = k0*A0*I0\n            for a, a0 in enumerate(a_list):\n                Na = a0*I0\n                for m, maxDet in enumerate(m_list):\n                    E = [self.evalImgs[Nk+Na+i] for i in i_list]\n                    E = [_f for _f in E if _f]\n                    if len(E) == 0:\n                        continue\n                    dtScores = np.concatenate([e[\'dtScores\'][0:maxDet]\n                                               for e in E])\n\n                    # different sorting method generates\n                    # slightly different results.\n                    # mergesort is used to be consistent as\n                    # Matlab implementation.\n                    inds = np.argsort(-dtScores, kind=\'mergesort\')\n\n                    dtm = np.concatenate([e[\'dtMatches\'][:, 0:maxDet]\n                                          for e in E], axis=1)[:, inds]\n                    dtIg = np.concatenate([e[\'dtIgnore\'][:, 0:maxDet]\n                                           for e in E], axis=1)[:, inds]\n                    gtIg = np.concatenate([e[\'gtIgnore\'] for e in E])\n                    npig = len([ig for ig in gtIg if ig == 0])\n                    if npig == 0:\n                        continue\n                    tps = np.logical_and(dtm, np.logical_not(dtIg))\n                    fps = np.logical_and(np.logical_not(dtm),\n                                         np.logical_not(dtIg))\n\n                    tp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)\n                    fp_sum = np.cumsum(fps, axis=1).astype(dtype=np.float)\n                    for t, (tp, fp) in enumerate(zip(tp_sum, fp_sum)):\n                        tp = np.array(tp)\n                        fp = np.array(fp)\n                        nd = len(tp)\n                        rc = tp / npig\n                        pr = tp / (fp+tp+np.spacing(1))\n                        q = np.zeros((R,))\n\n                        if nd:\n                            recall[t, k, a, m] = rc[-1]\n                        else:\n                            recall[t, k, a, m] = 0\n\n                        # numpy is slow without cython optimization\n                        # for accessing elements\n                        # use python array gets significant speed improvement\n                        pr = pr.tolist()\n                        q = q.tolist()\n\n                        for i in range(nd-1, 0, -1):\n                            if pr[i] > pr[i-1]:\n                                pr[i-1] = pr[i]\n\n                        inds = np.searchsorted(rc, p.recThrs)\n                        try:\n                            for ri, pi in enumerate(inds):\n                                q[ri] = pr[pi]\n                        except:\n                            pass\n                        precision[t, :, k, a, m] = np.array(q)\n        self.eval = {\n            \'params\': p,\n            \'counts\': [T, R, K, A, M],\n            \'date\': datetime.datetime.now().strftime(""%Y-%m-%d %H:%M:%S""),\n            \'precision\': precision,\n            \'recall\':   recall,\n        }\n        toc = time.time()\n        print(\'DONE (t=%0.2fs).\' % (toc-tic))\n\n    def summarize(self):\n        \'\'\'\n        Compute and display summary metrics for evaluation results.\n        Note this functin can *only* be applied on\n        the default parameter setting\n        \'\'\'\n        def _summarize(ap=1, iouThr=None, areaRng=\'all\', maxDets=100):\n            p = self.params\n            iStr = \' {:<18} {} @[ IoU={:<9} | \' \\\n                   \'area={:>6} | maxDets={:>3} ] = {}\'\n            titleStr = \'Average Precision\' if ap == 1 else \'Average Recall\'\n            typeStr = \'(AP)\' if ap == 1 else \'(AR)\'\n            iouStr = \'%0.2f:%0.2f\' % (p.iouThrs[0], p.iouThrs[-1]) \\\n                if iouThr is None else \'%0.2f\' % (iouThr)\n            areaStr = areaRng\n            maxDetsStr = \'%d\' % (maxDets)\n\n            aind = [i for i, aRng in\n                    enumerate([\'all\', \'small\', \'medium\', \'large\'])\n                    if aRng == areaRng]\n            mind = [i for i, mDet in enumerate([1, 10, 100])\n                    if mDet == maxDets]\n            if ap == 1:\n                # dimension of precision: [TxRxKxAxM]\n                s = self.eval[\'precision\']\n                # IoU\n                if iouThr is not None:\n                    t = np.where(iouThr == p.iouThrs)[0]\n                    s = s[t]\n                # areaRng\n                s = s[:, :, :, aind, mind]\n            else:\n                # dimension of recall: [TxKxAxM]\n                s = self.eval[\'recall\']\n                s = s[:, :, aind, mind]\n            if len(s[s > -1]) == 0:\n                mean_s = -1\n            else:\n                mean_s = np.mean(s[s > -1])\n            print(iStr.format(titleStr, typeStr, iouStr, areaStr, maxDetsStr,\n                              \'%.3f\' % (float(mean_s))))\n            return mean_s\n\n        if not self.eval:\n            raise Exception(\'Please run accumulate() first\')\n        self.stats = np.zeros((12,))\n        self.stats[0] = _summarize(1)\n        self.stats[1] = _summarize(1, iouThr=.5)\n        self.stats[2] = _summarize(1, iouThr=.75)\n        self.stats[3] = _summarize(1, areaRng=\'small\')\n        self.stats[4] = _summarize(1, areaRng=\'medium\')\n        self.stats[5] = _summarize(1, areaRng=\'large\')\n        self.stats[6] = _summarize(0, maxDets=1)\n        self.stats[7] = _summarize(0, maxDets=10)\n        self.stats[8] = _summarize(0, maxDets=100)\n        self.stats[9] = _summarize(0, areaRng=\'small\')\n        self.stats[10] = _summarize(0, areaRng=\'medium\')\n        self.stats[11] = _summarize(0, areaRng=\'large\')\n\n    def __str__(self):\n        self.summarize()\n\n\nclass Params:\n    \'\'\'\n    Params for coco evaluation api\n    \'\'\'\n    def __init__(self):\n        self.imgIds = []\n        self.catIds = []\n        # np.arange causes trouble.  the data point on arange is\n        # slightly larger than the true value\n        self.iouThrs = np.linspace(.5, 0.95, np.round((0.95-.5)/.05)+1,\n                                   endpoint=True)\n        self.recThrs = np.linspace(.0, 1.00, np.round((1.00-.0)/.01)+1,\n                                   endpoint=True)\n        self.maxDets = [1, 10, 100]\n        self.areaRng = [[0**2, 1e5**2], [0**2, 32**2], [32**2, 96**2],\n                        [96**2, 1e5**2]]\n        self.useSegm = 0\n        self.useCats = 1\n'"
utils/pycocotools/mask.py,0,"b'__author__ = \'tsungyi\'\n\nfrom . import _mask\n\n# Interface for manipulating masks stored in RLE format.\n#\n# RLE is a simple yet efficient format for storing binary masks. RLE\n# first divides a vector (or vectorized image) into a series of piecewise\n# constant regions and then for each piece simply stores the length of\n# that piece. For example, given M=[0 0 1 1 1 0 1] the RLE counts would\n# be [2 3 1 1], or for M=[1 1 1 1 1 1 0] the counts would be [0 6 1]\n# (note that the odd counts are always the numbers of zeros). Instead of\n# storing the counts directly, additional compression is achieved with a\n# variable bitrate representation based on a common scheme called LEB128.\n#\n# Compression is greatest given large piecewise constant regions.\n# Specifically, the size of the RLE is proportional to the number of\n# *boundaries* in M (or for an image the number of boundaries in the y\n# direction). Assuming fairly simple shapes, the RLE representation is\n# O(sqrt(n)) where n is number of pixels in the object. Hence space usage\n# is substantially lower, especially for large simple objects (large n).\n#\n# Many common operations on masks can be computed directly using the RLE\n# (without need for decoding). This includes computations such as area,\n# union, intersection, etc. All of these operations are linear in the\n# size of the RLE, in other words they are O(sqrt(n)) where n is the area\n# of the object. Computing these operations on the original mask is O(n).\n# Thus, using the RLE can result in substantial computational savings.\n#\n# The following API functions are defined:\n#  encode         - Encode binary masks using RLE.\n#  decode         - Decode binary masks encoded via RLE.\n#  merge          - Compute union or intersection of encoded masks.\n#  iou            - Compute intersection over union between masks.\n#  area           - Compute area of encoded masks.\n#  toBbox         - Get bounding boxes surrounding encoded masks.\n#  frPyObjects    - Convert polygon, bbox,\n#                   and uncompressed RLE to encoded RLE mask.\n#\n# Usage:\n#  Rs     = encode( masks )\n#  masks  = decode( Rs )\n#  R      = merge( Rs, intersect=false )\n#  o      = iou( dt, gt, iscrowd )\n#  a      = area( Rs )\n#  bbs    = toBbox( Rs )\n#  Rs     = frPyObjects( [pyObjects], h, w )\n#\n# In the API the following formats are used:\n#  Rs      - [dict] Run-length encoding of binary masks\n#  R       - dict Run-length encoding of binary mask\n#  masks   - [hxwxn] Binary mask(s)\n#            (must have type np.ndarray(dtype=uint8) in column-major order)\n#  iscrowd - [nx1] list of np.ndarray.\n#            1 indicates corresponding gt image has crowd region to ignore\n#  bbs     - [nx4] Bounding box(es) stored as [x y w h]\n#  poly    - Polygon stored as [[x1 y1 x2 y2...],[x1 y1 ...],...] (2D list)\n#  dt,gt   - May be either bounding boxes or encoded masks\n# Both poly and bbs are 0-indexed (bbox=[0 0 1 1] encloses first pixel).\n#\n# Finally, a note about the intersection over union (iou) computation.\n# The standard iou of a ground truth (gt) and detected (dt) object is\n#  iou(gt,dt) = area(intersect(gt,dt)) / area(union(gt,dt))\n# For ""crowd"" regions, we use a modified criteria. If a gt object is\n# marked as ""iscrowd"", we allow a dt to match any subregion of the gt.\n# Choosing gt\' in the crowd gt that best matches the dt can be done using\n# gt\'=intersect(dt,gt). Since by definition union(gt\',dt)=dt, computing\n#  iou(gt,dt,iscrowd) = iou(gt\',dt) = area(intersect(gt,dt)) / area(dt)\n# For crowd gt regions we use this modified criteria above for the iou.\n#\n# To compile run ""python setup.py build_ext --inplace""\n# Please do not contact us for help with compiling.\n#\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n# Licensed under the Simplified BSD License [see coco/license.txt]\n\nencode = _mask.encode\ndecode = _mask.decode\niou = _mask.iou\nmerge = _mask.merge\narea = _mask.area\ntoBbox = _mask.toBbox\nfrPyObjects = _mask.frPyObjects\n'"
layers/reorg/_ext/__init__.py,0,b''
layers/roi_pooling/_ext/__init__.py,0,b''
layers/reorg/_ext/reorg_layer/__init__.py,1,"b'\nfrom torch.utils.ffi import _wrap_function\nfrom ._reorg_layer import lib as _lib, ffi as _ffi\n\n__all__ = []\ndef _import_symbols(locals):\n    for symbol in dir(_lib):\n        fn = getattr(_lib, symbol)\n        if callable(fn):\n            locals[symbol] = _wrap_function(fn, _ffi)\n        else:\n            locals[symbol] = fn\n        __all__.append(symbol)\n\n_import_symbols(locals())\n'"
layers/roi_pooling/_ext/roi_pooling/__init__.py,1,"b'\nfrom torch.utils.ffi import _wrap_function\nfrom ._roi_pooling import lib as _lib, ffi as _ffi\n\n__all__ = []\ndef _import_symbols(locals):\n    for symbol in dir(_lib):\n        fn = getattr(_lib, symbol)\n        if callable(fn):\n            locals[symbol] = _wrap_function(fn, _ffi)\n        else:\n            locals[symbol] = fn\n        __all__.append(symbol)\n\n_import_symbols(locals())\n'"
