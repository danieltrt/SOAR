file_path,api_count,code
im2im_pred/create_dataset.py,7,"b'from torch.utils.data.dataset import Dataset\n\nimport os\nimport torch\nimport fnmatch\nimport numpy as np\n\n\nclass NYUv2(Dataset):\n    """"""\n    This file is directly modified from https://pytorch.org/docs/stable/torchvision/datasets.html\n    """"""\n    def __init__(self, root, train=True):\n        self.train = train\n        self.root = os.path.expanduser(root)\n\n        # R\\read the data file\n        if train:\n            self.data_path = root + \'/train\'\n        else:\n            self.data_path = root + \'/val\'\n\n        # calculate data length\n        self.data_len = len(fnmatch.filter(os.listdir(self.data_path + \'/image\'), \'*.npy\'))\n\n    def __getitem__(self, index):\n        # get image name from the pandas df\n        image = torch.from_numpy(np.moveaxis(np.load(self.data_path + \'/image/{:d}.npy\'.format(index)), -1, 0))\n        semantic = torch.from_numpy(np.load(self.data_path + \'/label/{:d}.npy\'.format(index)))\n        depth = torch.from_numpy(np.moveaxis(np.load(self.data_path + \'/depth/{:d}.npy\'.format(index)), -1, 0))\n        normal = torch.from_numpy(np.moveaxis(np.load(self.data_path + \'/normal/{:d}.npy\'.format(index)), -1, 0))\n\n        return image.type(torch.FloatTensor), semantic.type(torch.FloatTensor), depth.type(torch.FloatTensor), normal.type(torch.FloatTensor)\n\n    def __len__(self):\n        return self.data_len\n\n'"
im2im_pred/model_segnet_cross.py,36,"b'import torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport argparse\nimport torch.utils.data.sampler as sampler\n\nfrom create_dataset import *\nfrom torch.autograd import Variable\n\nparser = argparse.ArgumentParser(description=\'Multi-task: Cross\')\nparser.add_argument(\'--weight\', default=\'equal\', type=str, help=\'multi-task weighting: equal, uncert, dwa\')\nparser.add_argument(\'--dataroot\', default=\'nyuv2\', type=str, help=\'dataset root\')\nparser.add_argument(\'--temp\', default=2.0, type=float, help=\'temperature for DWA (must be positive)\')\nopt = parser.parse_args()\n\n\nclass SegNet(nn.Module):\n    def __init__(self):\n        super(SegNet, self).__init__()\n        # initialise network parameters\n        filter = [64, 128, 256, 512, 512]\n        self.class_nb = 13\n\n        # define encoder decoder layers\n        self.encoder_block_t = nn.ModuleList([nn.ModuleList([self.conv_layer([3, filter[0], filter[0]], bottle_neck=True)])])\n        self.decoder_block_t = nn.ModuleList([nn.ModuleList([self.conv_layer([filter[0], filter[0], filter[0]], bottle_neck=True)])])\n\n        for j in range(3):\n            if j < 2:\n                self.encoder_block_t.append(nn.ModuleList([self.conv_layer([3, filter[0], filter[0]], bottle_neck=True)]))\n                self.decoder_block_t.append(nn.ModuleList([self.conv_layer([filter[0], filter[0], filter[0]], bottle_neck=True)]))\n            for i in range(4):\n                if i == 0:\n                    self.encoder_block_t[j].append(self.conv_layer([filter[i], filter[i + 1], filter[i + 1]], bottle_neck=True))\n                    self.decoder_block_t[j].append(self.conv_layer([filter[i + 1], filter[i], filter[i]], bottle_neck=True))\n                else:\n                    self.encoder_block_t[j].append(self.conv_layer([filter[i], filter[i + 1], filter[i + 1]], bottle_neck=False))\n                    self.decoder_block_t[j].append(self.conv_layer([filter[i + 1], filter[i], filter[i]], bottle_neck=False))\n\n        # define cross-stitch units\n        self.cs_unit_encoder = nn.Parameter(data=torch.ones(4, 3))\n        self.cs_unit_decoder = nn.Parameter(data=torch.ones(5, 3))\n\n        # define task specific layers\n        self.pred_task1 = self.conv_layer([filter[0], self.class_nb], bottle_neck=True, pred_layer=True)\n        self.pred_task2 = self.conv_layer([filter[0], 1], bottle_neck=True, pred_layer=True)\n        self.pred_task3 = self.conv_layer([filter[0], 3], bottle_neck=True, pred_layer=True)\n\n        # define pooling and unpooling functions\n        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\n        self.logsigma = nn.Parameter(torch.FloatTensor([-0.5, -0.5, -0.5]))\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_uniform_(m.weight)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Parameter):\n                nn.init.constant(m.weight, 1)\n\n    def conv_layer(self, channel, bottle_neck, pred_layer=False):\n        if bottle_neck:\n            if not pred_layer:\n                conv_block = nn.Sequential(\n                    nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n                    nn.BatchNorm2d(channel[1]),\n                    nn.ReLU(inplace=True),\n                    nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=3, padding=1),\n                    nn.BatchNorm2d(channel[2]),\n                    nn.ReLU(inplace=True),\n                )\n            else:\n                conv_block = nn.Sequential(\n                    nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n                    nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n                )\n\n        else:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(channel[1]),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(in_channels=channel[1], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(channel[1]),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=3, padding=1),\n                nn.BatchNorm2d(channel[2]),\n                nn.ReLU(inplace=True),\n            )\n        return conv_block\n\n    def forward(self, x):\n        encoder_conv_t, decoder_conv_t, encoder_samp_t, decoder_samp_t, indices_t = ([0] * 3 for _ in range(5))\n        for i in range(3):\n            encoder_conv_t[i], decoder_conv_t[i], encoder_samp_t[i], decoder_samp_t[i], indices_t[i] = ([0] * 5 for _ in range(5))\n\n        # task branch 1\n        for i in range(5):\n            for j in range(3):\n                if i == 0:\n                    encoder_conv_t[j][i] = self.encoder_block_t[j][i](x)\n                    encoder_samp_t[j][i], indices_t[j][i] = self.down_sampling(encoder_conv_t[j][i])\n                else:\n                    encoder_cross_stitch = self.cs_unit_encoder[i - 1][0] * encoder_samp_t[0][i - 1] + \\\n                                           self.cs_unit_encoder[i - 1][1] * encoder_samp_t[1][i - 1] + \\\n                                           self.cs_unit_encoder[i - 1][2] * encoder_samp_t[2][i - 1]\n                    encoder_conv_t[j][i] = self.encoder_block_t[j][i](encoder_cross_stitch)\n                    encoder_samp_t[j][i], indices_t[j][i] = self.down_sampling(encoder_conv_t[j][i])\n\n        for i in range(5):\n            for j in range(3):\n                if i == 0:\n                    decoder_cross_stitch = self.cs_unit_decoder[i][0] * encoder_samp_t[0][-1] + \\\n                                           self.cs_unit_decoder[i][1] * encoder_samp_t[1][-1] + \\\n                                           self.cs_unit_decoder[i][2] * encoder_samp_t[2][-1]\n                    decoder_samp_t[j][i] = self.up_sampling(decoder_cross_stitch, indices_t[j][-i - 1])\n                    decoder_conv_t[j][i] = self.decoder_block_t[j][-i - 1](decoder_samp_t[j][i])\n                else:\n                    decoder_cross_stitch = self.cs_unit_decoder[i][0] * decoder_conv_t[0][i - 1] + \\\n                                           self.cs_unit_decoder[i][1] * decoder_conv_t[1][i - 1] + \\\n                                           self.cs_unit_decoder[i][2] * decoder_conv_t[2][i - 1]\n                    decoder_samp_t[j][i] = self.up_sampling(decoder_cross_stitch, indices_t[j][-i - 1])\n                    decoder_conv_t[j][i] = self.decoder_block_t[j][-i - 1](decoder_samp_t[j][i])\n\n        # define task prediction layers\n        t1_pred = F.log_softmax(self.pred_task1(decoder_conv_t[0][-1]), dim=1)\n        t2_pred = self.pred_task2(decoder_conv_t[1][-1])\n        t3_pred = self.pred_task3(decoder_conv_t[2][-1])\n        t3_pred = t3_pred / torch.norm(t3_pred, p=2, dim=1, keepdim=True)\n\n        return [t1_pred, t2_pred, t3_pred], self.logsigma\n\n    def model_fit(self, x_pred1, x_output1, x_pred2, x_output2, x_pred3, x_output3):\n        # binary mark to mask out undefined pixel space\n        binary_mask = (torch.sum(x_output2, dim=1) != 0).type(torch.FloatTensor).unsqueeze(1).to(device)\n\n        # semantic loss: depth-wise cross entropy\n        loss1 = F.nll_loss(x_pred1, x_output1, ignore_index=-1)\n\n        # depth loss: l1 norm\n        loss2 = torch.sum(torch.abs(x_pred2 - x_output2) * binary_mask) / torch.nonzero(binary_mask).size(0)\n\n        # normal loss: dot product\n        loss3 = 1 - torch.sum((x_pred3 * x_output3) * binary_mask) / torch.nonzero(binary_mask).size(0)\n\n        return [loss1, loss2, loss3]\n\n    def compute_miou(self, x_pred, x_output):\n        _, x_pred_label = torch.max(x_pred, dim=1)\n        x_output_label = x_output\n        batch_size = x_pred.size(0)\n        for i in range(batch_size):\n            true_class = 0\n            first_switch = True\n            for j in range(self.class_nb):\n                pred_mask = torch.eq(x_pred_label[i], j * torch.ones(x_pred_label[i].shape).type(torch.LongTensor).to(device))\n                true_mask = torch.eq(x_output_label[i], j * torch.ones(x_output_label[i].shape).type(torch.LongTensor).to(device))\n                mask_comb = pred_mask.type(torch.FloatTensor) + true_mask.type(torch.FloatTensor)\n                union = torch.sum((mask_comb > 0).type(torch.FloatTensor))\n                intsec = torch.sum((mask_comb > 1).type(torch.FloatTensor))\n                if union == 0:\n                    continue\n                if first_switch:\n                    class_prob = intsec / union\n                    first_switch = False\n                else:\n                    class_prob = intsec / union + class_prob\n                true_class += 1\n            if i == 0:\n                batch_avg = class_prob / true_class\n            else:\n                batch_avg = class_prob / true_class + batch_avg\n        return batch_avg / batch_size\n\n    def compute_iou(self, x_pred, x_output):\n        _, x_pred_label = torch.max(x_pred, dim=1)\n        x_output_label = x_output\n        batch_size = x_pred.size(0)\n        for i in range(batch_size):\n            if i == 0:\n                pixel_acc = torch.div(torch.sum(torch.eq(x_pred_label[i], x_output_label[i]).type(torch.FloatTensor)),\n                                      torch.sum((x_output_label[i] >= 0).type(torch.FloatTensor)))\n            else:\n                pixel_acc = pixel_acc + torch.div(torch.sum(torch.eq(x_pred_label[i], x_output_label[i]).type(torch.FloatTensor)),\n                                                  torch.sum((x_output_label[i] >= 0).type(torch.FloatTensor)))\n        return pixel_acc / batch_size\n\n    def depth_error(self, x_pred, x_output):\n        binary_mask = (torch.sum(x_output, dim=1) != 0).unsqueeze(1).to(device)\n        x_pred_true = x_pred.masked_select(binary_mask)\n        x_output_true = x_output.masked_select(binary_mask)\n        abs_err = torch.abs(x_pred_true - x_output_true)\n        rel_err = torch.abs(x_pred_true - x_output_true) / x_output_true\n        return torch.sum(abs_err) / torch.nonzero(binary_mask).size(0), torch.sum(rel_err) / torch.nonzero(binary_mask).size(0)\n\n    def normal_error(self, x_pred, x_output):\n        binary_mask = (torch.sum(x_output, dim=1) != 0)\n        error = torch.acos(torch.clamp(torch.sum(x_pred * x_output, 1).masked_select(binary_mask), -1, 1)).detach().cpu().numpy()\n        error = np.degrees(error)\n        return np.mean(error), np.median(error), np.mean(error < 11.25), np.mean(error < 22.5), np.mean(error < 30)\n\n\n# define model, optimiser and scheduler\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\nSegNet_CROSS = SegNet().to(device)\noptimizer = optim.Adam(SegNet_CROSS.parameters(), lr=1e-4)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n\n\n# compute parameter space\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\nprint(\'Parameter Space: ABS: {:.1f}, REL: {:.4f}\\n\'.format(count_parameters(SegNet_CROSS),\n                                                           count_parameters(SegNet_CROSS)/24981069))\nprint(\'LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC | DEPTH_LOSS ABS_ERR REL_ERR | NORMAL_LOSS MEAN MED <11.25 <22.5 <30\\n\')\n\n# define dataset path\ndataset_path = opt.dataroot\nnyuv2_train_set = NYUv2(root=dataset_path, train=True)\nnyuv2_test_set = NYUv2(root=dataset_path, train=False)\n\nbatch_size = 2\nnyuv2_train_loader = torch.utils.data.DataLoader(\n    dataset=nyuv2_train_set,\n    batch_size=batch_size,\n    shuffle=True)\n\nnyuv2_test_loader = torch.utils.data.DataLoader(\n    dataset=nyuv2_test_set,\n    batch_size=batch_size,\n    shuffle=True)\n\n\n# define parameters\ntotal_epoch = 200\ntrain_batch = len(nyuv2_train_loader)\ntest_batch = len(nyuv2_test_loader)\nT = opt.temp\navg_cost = np.zeros([total_epoch, 24], dtype=np.float32)\nlambda_weight = np.ones([3, total_epoch])\nfor epoch in range(total_epoch):\n    index = epoch\n    cost = np.zeros(24, dtype=np.float32)\n    scheduler.step()\n\n    # apply Dynamic Weight Average\n    if opt.weight == \'dwa\':\n        if index == 0 or index == 1:\n            lambda_weight[:, index] = 1.0\n        else:\n            w_1 = avg_cost[index - 1, 0] / avg_cost[index - 2, 0]\n            w_2 = avg_cost[index - 1, 3] / avg_cost[index - 2, 3]\n            w_3 = avg_cost[index - 1, 6] / avg_cost[index - 2, 6]\n            lambda_weight[0, index] = 3 * np.exp(w_1 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T) + np.exp(w_3 / T))\n            lambda_weight[1, index] = 3 * np.exp(w_2 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T) + np.exp(w_3 / T))\n            lambda_weight[2, index] = 3 * np.exp(w_3 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T) + np.exp(w_3 / T))\n\n    # iteration for all batches\n    SegNet_CROSS.train()\n    nyuv2_train_dataset = iter(nyuv2_train_loader)\n    for k in range(train_batch):\n        train_data, train_label, train_depth, train_normal = nyuv2_train_dataset.next()\n        train_data, train_label = train_data.to(device), train_label.type(torch.LongTensor).to(device)\n        train_depth, train_normal = train_depth.to(device), train_normal.to(device)\n\n        train_pred, logsigma = SegNet_CROSS(train_data)\n\n        optimizer.zero_grad()\n        train_loss = SegNet_CROSS.model_fit(train_pred[0], train_label, train_pred[1], train_depth, train_pred[2], train_normal)\n\n        if opt.weight == \'equal\' or opt.weight == \'dwa\':\n            loss = sum([lambda_weight[i, index] * train_loss[i] for i in range(3)])\n        else:\n            loss = sum(1 / (2 * torch.exp(logsigma[i])) * train_loss[i] + logsigma[i] / 2 for i in range(3))\n\n        loss.backward()\n        optimizer.step()\n\n        cost[0] = train_loss[0].item()\n        cost[1] = SegNet_CROSS.compute_miou(train_pred[0], train_label).item()\n        cost[2] = SegNet_CROSS.compute_iou(train_pred[0], train_label).item()\n        cost[3] = train_loss[1].item()\n        cost[4], cost[5] = SegNet_CROSS.depth_error(train_pred[1], train_depth)\n        cost[6] = train_loss[2].item()\n        cost[7], cost[8], cost[9], cost[10], cost[11] = SegNet_CROSS.normal_error(train_pred[2], train_normal)\n        avg_cost[index, :12] += cost[:12] / train_batch\n\n    # evaluating test data\n    SegNet_CROSS.eval()\n    with torch.no_grad():  # operations inside don\'t track history\n        nyuv2_test_dataset = iter(nyuv2_test_loader)\n        for k in range(test_batch):\n            test_data, test_label, test_depth, test_normal = nyuv2_test_dataset.next()\n            test_data, test_label = test_data.to(device),  test_label.type(torch.LongTensor).to(device)\n            test_depth, test_normal = test_depth.to(device), test_normal.to(device)\n\n            test_pred, _ = SegNet_CROSS(test_data)\n            test_loss = SegNet_CROSS.model_fit(test_pred[0], test_label, test_pred[1], test_depth, test_pred[2], test_normal)\n\n            cost[12] = test_loss[0].item()\n            cost[13] = SegNet_CROSS.compute_miou(test_pred[0], test_label).item()\n            cost[14] = SegNet_CROSS.compute_iou(test_pred[0], test_label).item()\n            cost[15] = test_loss[1].item()\n            cost[16], cost[17] = SegNet_CROSS.depth_error(test_pred[1], test_depth)\n            cost[18] = test_loss[2].item()\n            cost[19], cost[20], cost[21], cost[22], cost[23] = SegNet_CROSS.normal_error(test_pred[2], test_normal)\n\n            avg_cost[index, 12:] += cost[12:] / test_batch\n\n\n    print(\'Epoch: {:04d} | TRAIN: {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} \'\n          \'TEST: {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} \'\n          .format(index, avg_cost[index, 0], avg_cost[index, 1], avg_cost[index, 2], avg_cost[index, 3],\n                avg_cost[index, 4], avg_cost[index, 5], avg_cost[index, 6], avg_cost[index, 7], avg_cost[index, 8], avg_cost[index, 9],\n                avg_cost[index, 10], avg_cost[index, 11], avg_cost[index, 12], avg_cost[index, 13],\n                avg_cost[index, 14], avg_cost[index, 15], avg_cost[index, 16], avg_cost[index, 17], avg_cost[index, 18],\n                avg_cost[index, 19], avg_cost[index, 20], avg_cost[index, 21], avg_cost[index, 22], avg_cost[index, 23]))\n\n'"
im2im_pred/model_segnet_dense.py,37,"b'import torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport argparse\nimport torch.utils.data.sampler as sampler\n\nfrom create_dataset import *\nfrom torch.autograd import Variable\n\nparser = argparse.ArgumentParser(description=\'Multi-task: Dense\')\nparser.add_argument(\'--weight\', default=\'equal\', type=str, help=\'multi-task weighting: equal, uncert, dwa\')\nparser.add_argument(\'--dataroot\', default=\'nyuv2\', type=str, help=\'dataset root\')\nparser.add_argument(\'--temp\', default=2.0, type=float, help=\'temperature for DWA (must be positive)\')\nopt = parser.parse_args()\n\n\nclass SegNet(nn.Module):\n    def __init__(self):\n        super(SegNet, self).__init__()\n        # initialise network parameters\n        filter = [64, 128, 256, 512, 512]\n        self.class_nb = 13\n\n        # define encoder decoder layers\n        self.encoder_block = nn.ModuleList([self.conv_layer([3, filter[0], filter[0]], bottle_neck=True)])\n        self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0], self.class_nb], bottle_neck=True)])\n\n        self.encoder_block_t = nn.ModuleList([nn.ModuleList([self.conv_layer([3, filter[0], filter[0]], bottle_neck=True)])])\n        self.decoder_block_t = nn.ModuleList([nn.ModuleList([self.conv_layer([2 * filter[0], 2 * filter[0], filter[0]], bottle_neck=True)])])\n\n        for i in range(4):\n            if i == 0:\n                self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1], filter[i + 1]], bottle_neck=True))\n                self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i], filter[i]], bottle_neck=True))\n            else:\n                self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1], filter[i + 1]], bottle_neck=False))\n                self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i], filter[i]], bottle_neck=False))\n\n        for j in range(3):\n            if j < 2:\n                self.encoder_block_t.append(nn.ModuleList([self.conv_layer([3, filter[0], filter[0]], bottle_neck=True)]))\n                self.decoder_block_t.append(nn.ModuleList([self.conv_layer([2 * filter[0], 2 * filter[0], filter[0]], bottle_neck=True)]))\n            for i in range(4):\n                if i == 0:\n                    self.encoder_block_t[j].append(self.conv_layer([2 * filter[i], filter[i + 1], filter[i + 1]], bottle_neck=True))\n                    self.decoder_block_t[j].append(self.conv_layer([2 * filter[i + 1], filter[i], filter[i]], bottle_neck=True))\n                else:\n                    self.encoder_block_t[j].append(self.conv_layer([2 * filter[i], filter[i + 1], filter[i + 1]], bottle_neck=False))\n                    self.decoder_block_t[j].append(self.conv_layer([2 * filter[i + 1], filter[i], filter[i]], bottle_neck=False))\n\n        # define pooling and unpooling functions\n        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\n        self.pred_task1 = self.conv_layer([filter[0], self.class_nb], bottle_neck=True, pred_layer=True)\n        self.pred_task2 = self.conv_layer([filter[0], 1], bottle_neck=True, pred_layer=True)\n        self.pred_task3 = self.conv_layer([filter[0], 3], bottle_neck=True, pred_layer=True)\n\n        self.logsigma = nn.Parameter(torch.FloatTensor([-0.5, -0.5, -0.5]))\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_uniform_(m.weight)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_uniform_(m.weight)\n                nn.init.constant_(m.bias, 0)\n\n    def conv_layer(self, channel, bottle_neck, pred_layer=False):\n        if bottle_neck:\n            if not pred_layer:\n                conv_block = nn.Sequential(\n                    nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n                    nn.BatchNorm2d(channel[1]),\n                    nn.ReLU(inplace=True),\n                    nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=3, padding=1),\n                    nn.BatchNorm2d(channel[2]),\n                    nn.ReLU(inplace=True),\n                )\n            else:\n                conv_block = nn.Sequential(\n                    nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n                    nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n                )\n\n        else:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(channel[1]),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(in_channels=channel[1], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(channel[1]),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=3, padding=1),\n                nn.BatchNorm2d(channel[2]),\n                nn.ReLU(inplace=True),\n            )\n\n        return conv_block\n\n    def forward(self, x):\n        encoder_conv, decoder_conv, encoder_samp, decoder_samp, indices = ([0] * 5 for _ in range(5))\n        encoder_conv_t, decoder_conv_t, encoder_samp_t, decoder_samp_t, indices_t = ([0] * 3 for _ in range(5))\n        for i in range(3):\n            encoder_conv_t[i], decoder_conv_t[i], encoder_samp_t[i], decoder_samp_t[i], indices_t[i] = ([0] * 5 for _ in range(5))\n\n        # global shared encoder-decoder network\n        for i in range(5):\n            if i == 0:\n                encoder_conv[i] = self.encoder_block[i](x)\n                encoder_samp[i], indices[i] = self.down_sampling(encoder_conv[i])\n            else:\n                encoder_conv[i] = self.encoder_block[i](encoder_samp[i - 1])\n                encoder_samp[i], indices[i] = self.down_sampling(encoder_conv[i])\n\n        for i in range(5):\n            if i == 0:\n                decoder_samp[i] = self.up_sampling(encoder_samp[-1], indices[-1])\n                decoder_conv[i] = self.decoder_block[-i - 1](decoder_samp[i])\n            else:\n                decoder_samp[i] = self.up_sampling(decoder_conv[i - 1], indices[-i - 1])\n                decoder_conv[i] = self.decoder_block[-i - 1](decoder_samp[i])\n\n        # define task prediction layers\n        for j in range(3):\n            for i in range(5):\n                if i == 0:\n                    encoder_conv_t[j][i] = self.encoder_block_t[j][i](x)\n                    encoder_samp_t[j][i], indices_t[j][i] = self.down_sampling(encoder_conv_t[j][i])\n                else:\n                    encoder_conv_t[j][i] = self.encoder_block_t[j][i](torch.cat((encoder_samp_t[j][i - 1], encoder_samp[i - 1]), dim=1))\n                    encoder_samp_t[j][i], indices_t[j][i] = self.down_sampling(encoder_conv_t[j][i])\n\n            for i in range(5):\n                if i == 0:\n                    decoder_samp_t[j][i] = self.up_sampling(encoder_samp_t[j][-1], indices_t[j][-1])\n                    decoder_conv_t[j][i] = self.decoder_block_t[j][-i - 1](torch.cat((decoder_samp_t[j][i], decoder_samp[i]), dim=1))\n                else:\n                    decoder_samp_t[j][i] = self.up_sampling(decoder_conv_t[j][i - 1], indices_t[j][-i - 1])\n                    decoder_conv_t[j][i] = self.decoder_block_t[j][-i - 1](torch.cat((decoder_samp_t[j][i], decoder_samp[i]), dim=1))\n\n        t1_pred = F.log_softmax(self.pred_task1(decoder_conv_t[0][-1]), dim=1)\n        t2_pred = self.pred_task2(decoder_conv_t[1][-1])\n        t3_pred = self.pred_task3(decoder_conv_t[2][-1])\n        t3_pred = t3_pred / torch.norm(t3_pred, p=2, dim=1, keepdim=True)\n\n        return [t1_pred, t2_pred, t3_pred], self.logsigma\n\n    def model_fit(self, x_pred1, x_output1, x_pred2, x_output2, x_pred3, x_output3):\n        # binary mark to mask out undefined pixel space\n        binary_mask = (torch.sum(x_output2, dim=1) != 0).type(torch.FloatTensor).unsqueeze(1).to(device)\n\n        # semantic loss: depth-wise cross entropy\n        loss1 = F.nll_loss(x_pred1, x_output1, ignore_index=-1)\n\n        # depth loss: l1 norm\n        loss2 = torch.sum(torch.abs(x_pred2 - x_output2) * binary_mask) / torch.nonzero(binary_mask).size(0)\n\n        # normal loss: dot product\n        loss3 = 1 - torch.sum((x_pred3 * x_output3) * binary_mask) / torch.nonzero(binary_mask).size(0)\n\n        return [loss1, loss2, loss3]\n\n    def compute_miou(self, x_pred, x_output):\n        _, x_pred_label = torch.max(x_pred, dim=1)\n        x_output_label = x_output\n        batch_size = x_pred.size(0)\n        for i in range(batch_size):\n            true_class = 0\n            first_switch = True\n            for j in range(self.class_nb):\n                pred_mask = torch.eq(x_pred_label[i], j * torch.ones(x_pred_label[i].shape).type(torch.LongTensor).to(device))\n                true_mask = torch.eq(x_output_label[i], j * torch.ones(x_output_label[i].shape).type(torch.LongTensor).to(device))\n                mask_comb = pred_mask.type(torch.FloatTensor) + true_mask.type(torch.FloatTensor)\n                union     = torch.sum((mask_comb > 0).type(torch.FloatTensor))\n                intsec    = torch.sum((mask_comb > 1).type(torch.FloatTensor))\n                if union == 0:\n                    continue\n                if first_switch:\n                    class_prob = intsec / union\n                    first_switch = False\n                else:\n                    class_prob = intsec / union + class_prob\n                true_class += 1\n            if i == 0:\n                batch_avg = class_prob / true_class\n            else:\n                batch_avg = class_prob / true_class + batch_avg\n        return batch_avg / batch_size\n        \n    def compute_iou(self, x_pred, x_output):\n        _, x_pred_label = torch.max(x_pred, dim=1)\n        x_output_label = x_output\n        batch_size = x_pred.size(0)\n        for i in range(batch_size):\n            if i == 0:\n                pixel_acc = torch.div(torch.sum(torch.eq(x_pred_label[i], x_output_label[i]).type(torch.FloatTensor)),\n                            torch.sum((x_output_label[i] >= 0).type(torch.FloatTensor)))\n            else:\n                pixel_acc = pixel_acc + torch.div(torch.sum(torch.eq(x_pred_label[i], x_output_label[i]).type(torch.FloatTensor)),\n                            torch.sum((x_output_label[i] >= 0).type(torch.FloatTensor)))\n        return pixel_acc / batch_size\n\n    def depth_error(self, x_pred, x_output):\n        binary_mask = (torch.sum(x_output, dim=1) != 0).unsqueeze(1).to(device)\n        x_pred_true = x_pred.masked_select(binary_mask)\n        x_output_true = x_output.masked_select(binary_mask)\n        abs_err = torch.abs(x_pred_true - x_output_true)\n        rel_err = torch.abs(x_pred_true - x_output_true) / x_output_true\n        return torch.sum(abs_err) / torch.nonzero(binary_mask).size(0), torch.sum(rel_err) / torch.nonzero(binary_mask).size(0)\n\n    def normal_error(self, x_pred, x_output):\n        binary_mask = (torch.sum(x_output, dim=1) != 0)\n        error = torch.acos(torch.clamp(torch.sum(x_pred * x_output, 1).masked_select(binary_mask), -1, 1)).detach().cpu().numpy()\n        error = np.degrees(error)\n        return np.mean(error), np.median(error), np.mean(error < 11.25), np.mean(error < 22.5), np.mean(error < 30)\n\n\n# define model, optimiser and scheduler\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\nSegNet_DENSE = SegNet().to(device)\noptimizer = optim.Adam(SegNet_DENSE.parameters(), lr=1e-4)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n\n\n# compute parameter space\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\nprint(\'Parameter Space: ABS: {:.1f}, REL: {:.4f}\\n\'.format(count_parameters(SegNet_DENSE),\n                                                           count_parameters(SegNet_DENSE)/24981069))\nprint(\'LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC | DEPTH_LOSS ABS_ERR REL_ERR | NORMAL_LOSS MEAN MED <11.25 <22.5 <30\\n\')\n\n# define dataset path\ndataset_path = opt.dataroot\nnyuv2_train_set = NYUv2(root=dataset_path, train=True)\nnyuv2_test_set = NYUv2(root=dataset_path, train=False)\n\nbatch_size = 2\nnyuv2_train_loader = torch.utils.data.DataLoader(\n    dataset=nyuv2_train_set,\n    batch_size=batch_size,\n    shuffle=True)\n\nnyuv2_test_loader = torch.utils.data.DataLoader(\n    dataset=nyuv2_test_set,\n    batch_size=batch_size,\n    shuffle=True)\n\n\n# define parameters\ntotal_epoch = 200\ntrain_batch = len(nyuv2_train_loader)\ntest_batch = len(nyuv2_test_loader)\nT = opt.temp\navg_cost = np.zeros([total_epoch, 24], dtype=np.float32)\nlambda_weight = np.ones([3, total_epoch])\nfor epoch in range(total_epoch):\n    index = epoch\n    cost = np.zeros(24, dtype=np.float32)\n    scheduler.step()\n\n    # apply Dynamic Weight Average\n    if opt.weight == \'dwa\':\n        if index == 0 or index == 1:\n            lambda_weight[:, index] = 1.0\n        else:\n            w_1 = avg_cost[index - 1, 0] / avg_cost[index - 2, 0]\n            w_2 = avg_cost[index - 1, 3] / avg_cost[index - 2, 3]\n            w_3 = avg_cost[index - 1, 6] / avg_cost[index - 2, 6]\n            lambda_weight[0, index] = 3 * np.exp(w_1 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T) + np.exp(w_3 / T))\n            lambda_weight[1, index] = 3 * np.exp(w_2 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T) + np.exp(w_3 / T))\n            lambda_weight[2, index] = 3 * np.exp(w_3 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T) + np.exp(w_3 / T))\n\n    # iteration for all batches\n    SegNet_DENSE.train()\n    nyuv2_train_dataset = iter(nyuv2_train_loader)\n    for k in range(train_batch):\n        train_data, train_label, train_depth, train_normal = nyuv2_train_dataset.next()\n        train_data, train_label = train_data.to(device), train_label.type(torch.LongTensor).to(device)\n        train_depth, train_normal = train_depth.to(device), train_normal.to(device)\n\n        train_pred, logsigma = SegNet_DENSE(train_data)\n\n        optimizer.zero_grad()\n        train_loss = SegNet_DENSE.model_fit(train_pred[0], train_label, train_pred[1], train_depth, train_pred[2], train_normal)\n\n        if opt.weight == \'equal\' or opt.weight == \'dwa\':\n            loss = sum([lambda_weight[i, index] * train_loss[i] for i in range(3)])\n        else:\n            loss = sum(1 / (2 * torch.exp(logsigma[i])) * train_loss[i] + logsigma[i] / 2 for i in range(3))\n\n        loss.backward()\n        optimizer.step()\n\n        cost[0] = train_loss[0].item()\n        cost[1] = SegNet_DENSE.compute_miou(train_pred[0], train_label).item()\n        cost[2] = SegNet_DENSE.compute_iou(train_pred[0], train_label).item()\n        cost[3] = train_loss[1].item()\n        cost[4], cost[5] = SegNet_DENSE.depth_error(train_pred[1], train_depth)\n        cost[6] = train_loss[2].item()\n        cost[7], cost[8], cost[9], cost[10], cost[11] = SegNet_DENSE.normal_error(train_pred[2], train_normal)\n        avg_cost[index, :12] += cost[:12] / train_batch\n\n    # evaluating test data\n    SegNet_DENSE.eval()\n    with torch.no_grad():  # operations inside don\'t track history\n        nyuv2_test_dataset = iter(nyuv2_test_loader)\n        for k in range(test_batch):\n            test_data, test_label, test_depth, test_normal = nyuv2_test_dataset.next()\n            test_data, test_label = test_data.to(device),  test_label.type(torch.LongTensor).to(device)\n            test_depth, test_normal = test_depth.to(device), test_normal.to(device)\n\n            test_pred, _ = SegNet_DENSE(test_data)\n            test_loss = SegNet_DENSE.model_fit(test_pred[0], test_label, test_pred[1], test_depth, test_pred[2], test_normal)\n\n            cost[12] = test_loss[0].item()\n            cost[13] = SegNet_DENSE.compute_miou(test_pred[0], test_label).item()\n            cost[14] = SegNet_DENSE.compute_iou(test_pred[0], test_label).item()\n            cost[15] = test_loss[1].item()\n            cost[16], cost[17] = SegNet_DENSE.depth_error(test_pred[1], test_depth)\n            cost[18] = test_loss[2].item()\n            cost[19], cost[20], cost[21], cost[22], cost[23] = SegNet_DENSE.normal_error(test_pred[2], test_normal)\n\n            avg_cost[index, 12:] += cost[12:] / test_batch\n\n\n    print(\'Epoch: {:04d} | TRAIN: {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} \'\n          \'TEST: {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} \'\n          .format(index, avg_cost[index, 0], avg_cost[index, 1], avg_cost[index, 2], avg_cost[index, 3],\n                avg_cost[index, 4], avg_cost[index, 5], avg_cost[index, 6], avg_cost[index, 7], avg_cost[index, 8], avg_cost[index, 9],\n                avg_cost[index, 10], avg_cost[index, 11], avg_cost[index, 12], avg_cost[index, 13],\n                avg_cost[index, 14], avg_cost[index, 15], avg_cost[index, 16], avg_cost[index, 17], avg_cost[index, 18],\n                avg_cost[index, 19], avg_cost[index, 20], avg_cost[index, 21], avg_cost[index, 22], avg_cost[index, 23]))\n\n'"
im2im_pred/model_segnet_mtan.py,37,"b'import torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport argparse\nimport torch.utils.data.sampler as sampler\n\nfrom create_dataset import *\nfrom torch.autograd import Variable\n\nparser = argparse.ArgumentParser(description=\'Multi-task: Attention Network\')\nparser.add_argument(\'--weight\', default=\'equal\', type=str, help=\'multi-task weighting: equal, uncert, dwa\')\nparser.add_argument(\'--dataroot\', default=\'nyuv2\', type=str, help=\'dataset root\')\nparser.add_argument(\'--temp\', default=2.0, type=float, help=\'temperature for DWA (must be positive)\')\nopt = parser.parse_args()\n\n\nclass SegNet(nn.Module):\n    def __init__(self):\n        super(SegNet, self).__init__()\n        # initialise network parameters\n        filter = [64, 128, 256, 512, 512]\n        self.class_nb = 13\n\n        # define encoder decoder layers\n        self.encoder_block = nn.ModuleList([self.conv_layer([3, filter[0]])])\n        self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1]]))\n            self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i]]))\n\n        # define convolution layer\n        self.conv_block_enc = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        self.conv_block_dec = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            if i == 0:\n                self.conv_block_enc.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n                self.conv_block_dec.append(self.conv_layer([filter[i], filter[i]]))\n            else:\n                self.conv_block_enc.append(nn.Sequential(self.conv_layer([filter[i + 1], filter[i + 1]]),\n                                                         self.conv_layer([filter[i + 1], filter[i + 1]])))\n                self.conv_block_dec.append(nn.Sequential(self.conv_layer([filter[i], filter[i]]),\n                                                         self.conv_layer([filter[i], filter[i]])))\n\n        # define task attention layers\n        self.encoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])])])\n        self.decoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])])])\n        self.encoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[1]])])\n        self.decoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\n        for j in range(3):\n            if j < 2:\n                self.encoder_att.append(nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])]))\n                self.decoder_att.append(nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])]))\n            for i in range(4):\n                self.encoder_att[j].append(self.att_layer([2 * filter[i + 1], filter[i + 1], filter[i + 1]]))\n                self.decoder_att[j].append(self.att_layer([filter[i + 1] + filter[i], filter[i], filter[i]]))\n\n        for i in range(4):\n            if i < 3:\n                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 2]]))\n                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i]]))\n            else:\n                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\n        self.pred_task1 = self.conv_layer([filter[0], self.class_nb], pred=True)\n        self.pred_task2 = self.conv_layer([filter[0], 1], pred=True)\n        self.pred_task3 = self.conv_layer([filter[0], 3], pred=True)\n\n        # define pooling and unpooling functions\n        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\n        self.logsigma = nn.Parameter(torch.FloatTensor([-0.5, -0.5, -0.5]))\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n\n    def conv_layer(self, channel, pred=False):\n        if not pred:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(num_features=channel[1]),\n                nn.ReLU(inplace=True),\n            )\n        else:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n            )\n        return conv_block\n\n    def att_layer(self, channel):\n        att_block = nn.Sequential(\n            nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n            nn.BatchNorm2d(channel[1]),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=1, padding=0),\n            nn.BatchNorm2d(channel[2]),\n            nn.Sigmoid(),\n        )\n        return att_block\n\n    def forward(self, x):\n        g_encoder, g_decoder, g_maxpool, g_upsampl, indices = ([0] * 5 for _ in range(5))\n        for i in range(5):\n            g_encoder[i], g_decoder[-i - 1] = ([0] * 2 for _ in range(2))\n\n        # define attention list for tasks\n        atten_encoder, atten_decoder = ([0] * 3 for _ in range(2))\n        for i in range(3):\n            atten_encoder[i], atten_decoder[i] = ([0] * 5 for _ in range(2))\n        for i in range(3):\n            for j in range(5):\n                atten_encoder[i][j], atten_decoder[i][j] = ([0] * 3 for _ in range(2))\n\n        # define global shared network\n        for i in range(5):\n            if i == 0:\n                g_encoder[i][0] = self.encoder_block[i](x)\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n            else:\n                g_encoder[i][0] = self.encoder_block[i](g_maxpool[i - 1])\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\n        for i in range(5):\n            if i == 0:\n                g_upsampl[i] = self.up_sampling(g_maxpool[-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n            else:\n                g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\n        # define task dependent attention module\n        for i in range(3):\n            for j in range(5):\n                if j == 0:\n                    atten_encoder[i][j][0] = self.encoder_att[i][j](g_encoder[j][0])\n                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n                else:\n                    atten_encoder[i][j][0] = self.encoder_att[i][j](torch.cat((g_encoder[j][0], atten_encoder[i][j - 1][2]), dim=1))\n                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n\n            for j in range(5):\n                if j == 0:\n                    atten_decoder[i][j][0] = F.interpolate(atten_encoder[i][-1][-1], scale_factor=2, mode=\'bilinear\', align_corners=True)\n                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat((g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n                else:\n                    atten_decoder[i][j][0] = F.interpolate(atten_decoder[i][j - 1][2], scale_factor=2, mode=\'bilinear\', align_corners=True)\n                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat((g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n\n        # define task prediction layers\n        t1_pred = F.log_softmax(self.pred_task1(atten_decoder[0][-1][-1]), dim=1)\n        t2_pred = self.pred_task2(atten_decoder[1][-1][-1])\n        t3_pred = self.pred_task3(atten_decoder[2][-1][-1])\n        t3_pred = t3_pred / torch.norm(t3_pred, p=2, dim=1, keepdim=True)\n\n        return [t1_pred, t2_pred, t3_pred], self.logsigma\n\n    def model_fit(self, x_pred1, x_output1, x_pred2, x_output2, x_pred3, x_output3):\n        # binary mark to mask out undefined pixel space\n        binary_mask = (torch.sum(x_output2, dim=1) != 0).type(torch.FloatTensor).unsqueeze(1).to(device)\n\n        # semantic loss: depth-wise cross entropy\n        loss1 = F.nll_loss(x_pred1, x_output1, ignore_index=-1)\n\n        # depth loss: l1 norm\n        loss2 = torch.sum(torch.abs(x_pred2 - x_output2) * binary_mask) / torch.nonzero(binary_mask).size(0)\n\n        # normal loss: dot product\n        loss3 = 1 - torch.sum((x_pred3 * x_output3) * binary_mask) / torch.nonzero(binary_mask).size(0)\n\n        return [loss1, loss2, loss3]\n\n    def compute_miou(self, x_pred, x_output):\n        _, x_pred_label = torch.max(x_pred, dim=1)\n        x_output_label = x_output\n        batch_size = x_pred.size(0)\n        for i in range(batch_size):\n            true_class = 0\n            first_switch = True\n            for j in range(self.class_nb):\n                pred_mask = torch.eq(x_pred_label[i], j * torch.ones(x_pred_label[i].shape).type(torch.LongTensor).to(device))\n                true_mask = torch.eq(x_output_label[i], j * torch.ones(x_output_label[i].shape).type(torch.LongTensor).to(device))\n                mask_comb = pred_mask.type(torch.FloatTensor) + true_mask.type(torch.FloatTensor)\n                union = torch.sum((mask_comb > 0).type(torch.FloatTensor))\n                intsec = torch.sum((mask_comb > 1).type(torch.FloatTensor))\n                if union == 0:\n                    continue\n                if first_switch:\n                    class_prob = intsec / union\n                    first_switch = False\n                else:\n                    class_prob = intsec / union + class_prob\n                true_class += 1\n            if i == 0:\n                batch_avg = class_prob / true_class\n            else:\n                batch_avg = class_prob / true_class + batch_avg\n        return batch_avg / batch_size\n\n    def compute_iou(self, x_pred, x_output):\n        _, x_pred_label = torch.max(x_pred, dim=1)\n        x_output_label = x_output\n        batch_size = x_pred.size(0)\n        for i in range(batch_size):\n            if i == 0:\n                pixel_acc = torch.div(torch.sum(torch.eq(x_pred_label[i], x_output_label[i]).type(torch.FloatTensor)),\n                                      torch.sum((x_output_label[i] >= 0).type(torch.FloatTensor)))\n            else:\n                pixel_acc = pixel_acc + torch.div(torch.sum(torch.eq(x_pred_label[i], x_output_label[i]).type(torch.FloatTensor)),\n                    torch.sum((x_output_label[i] >= 0).type(torch.FloatTensor)))\n        return pixel_acc / batch_size\n\n    def depth_error(self, x_pred, x_output):\n        binary_mask = (torch.sum(x_output, dim=1) != 0).unsqueeze(1).to(device)\n        x_pred_true = x_pred.masked_select(binary_mask)\n        x_output_true = x_output.masked_select(binary_mask)\n        abs_err = torch.abs(x_pred_true - x_output_true)\n        rel_err = torch.abs(x_pred_true - x_output_true) / x_output_true\n        return torch.sum(abs_err) / torch.nonzero(binary_mask).size(0), torch.sum(rel_err) / torch.nonzero(binary_mask).size(0)\n\n    def normal_error(self, x_pred, x_output):\n        binary_mask = (torch.sum(x_output, dim=1) != 0)\n        error = torch.acos(torch.clamp(torch.sum(x_pred * x_output, 1).masked_select(binary_mask), -1, 1)).detach().cpu().numpy()\n        error = np.degrees(error)\n        return np.mean(error), np.median(error), np.mean(error < 11.25), np.mean(error < 22.5), np.mean(error < 30)\n\n\n# define model, optimiser and scheduler\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\nSegNet_MTAN = SegNet().to(device)\noptimizer = optim.Adam(SegNet_MTAN.parameters(), lr=1e-4)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n\n\n# compute parameter space\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\nprint(\'Parameter Space: ABS: {:.1f}, REL: {:.4f}\\n\'.format(count_parameters(SegNet_MTAN),\n                                                           count_parameters(SegNet_MTAN)/24981069))\nprint(\'LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC | DEPTH_LOSS ABS_ERR REL_ERR | NORMAL_LOSS MEAN MED <11.25 <22.5 <30\\n\')\n\n# define dataset path\ndataset_path = opt.dataroot\nnyuv2_train_set = NYUv2(root=dataset_path, train=True)\nnyuv2_test_set = NYUv2(root=dataset_path, train=False)\n\nbatch_size = 2\nnyuv2_train_loader = torch.utils.data.DataLoader(\n    dataset=nyuv2_train_set,\n    batch_size=batch_size,\n    shuffle=True)\n\nnyuv2_test_loader = torch.utils.data.DataLoader(\n    dataset=nyuv2_test_set,\n    batch_size=batch_size,\n    shuffle=True)\n\n\n# define parameters\ntotal_epoch = 200\ntrain_batch = len(nyuv2_train_loader)\ntest_batch = len(nyuv2_test_loader)\nT = opt.temp\navg_cost = np.zeros([total_epoch, 24], dtype=np.float32)\nlambda_weight = np.ones([3, total_epoch])\nfor epoch in range(total_epoch):\n    index = epoch\n    cost = np.zeros(24, dtype=np.float32)\n    scheduler.step()\n\n    # apply Dynamic Weight Average\n    if opt.weight == \'dwa\':\n        if index == 0 or index == 1:\n            lambda_weight[:, index] = 1.0\n        else:\n            w_1 = avg_cost[index - 1, 0] / avg_cost[index - 2, 0]\n            w_2 = avg_cost[index - 1, 3] / avg_cost[index - 2, 3]\n            w_3 = avg_cost[index - 1, 6] / avg_cost[index - 2, 6]\n            lambda_weight[0, index] = 3 * np.exp(w_1 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T) + np.exp(w_3 / T))\n            lambda_weight[1, index] = 3 * np.exp(w_2 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T) + np.exp(w_3 / T))\n            lambda_weight[2, index] = 3 * np.exp(w_3 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T) + np.exp(w_3 / T))\n\n    # iteration for all batches\n    SegNet_MTAN.train()\n    nyuv2_train_dataset = iter(nyuv2_train_loader)\n    for k in range(train_batch):\n        train_data, train_label, train_depth, train_normal = nyuv2_train_dataset.next()\n        train_data, train_label = train_data.to(device), train_label.type(torch.LongTensor).to(device)\n        train_depth, train_normal = train_depth.to(device), train_normal.to(device)\n\n        train_pred, logsigma = SegNet_MTAN(train_data)\n\n        optimizer.zero_grad()\n        train_loss = SegNet_MTAN.model_fit(train_pred[0], train_label, train_pred[1], train_depth, train_pred[2], train_normal)\n\n        if opt.weight == \'equal\' or opt.weight == \'dwa\':\n            loss = sum([lambda_weight[i, index] * train_loss[i] for i in range(3)])\n        else:\n            loss = sum(1 / (2 * torch.exp(logsigma[i])) * train_loss[i] + logsigma[i] / 2 for i in range(3))\n\n        loss.backward()\n        optimizer.step()\n\n        cost[0] = train_loss[0].item()\n        cost[1] = SegNet_MTAN.compute_miou(train_pred[0], train_label).item()\n        cost[2] = SegNet_MTAN.compute_iou(train_pred[0], train_label).item()\n        cost[3] = train_loss[1].item()\n        cost[4], cost[5] = SegNet_MTAN.depth_error(train_pred[1], train_depth)\n        cost[6] = train_loss[2].item()\n        cost[7], cost[8], cost[9], cost[10], cost[11] = SegNet_MTAN.normal_error(train_pred[2], train_normal)\n        avg_cost[index, :12] += cost[:12] / train_batch\n\n    # evaluating test data\n    SegNet_MTAN.eval()\n    with torch.no_grad():  # operations inside don\'t track history\n        nyuv2_test_dataset = iter(nyuv2_test_loader)\n        for k in range(test_batch):\n            test_data, test_label, test_depth, test_normal = nyuv2_test_dataset.next()\n            test_data, test_label = test_data.to(device),  test_label.type(torch.LongTensor).to(device)\n            test_depth, test_normal = test_depth.to(device), test_normal.to(device)\n\n            test_pred, _ = SegNet_MTAN(test_data)\n            test_loss = SegNet_MTAN.model_fit(test_pred[0], test_label, test_pred[1], test_depth, test_pred[2], test_normal)\n\n            cost[12] = test_loss[0].item()\n            cost[13] = SegNet_MTAN.compute_miou(test_pred[0], test_label).item()\n            cost[14] = SegNet_MTAN.compute_iou(test_pred[0], test_label).item()\n            cost[15] = test_loss[1].item()\n            cost[16], cost[17] = SegNet_MTAN.depth_error(test_pred[1], test_depth)\n            cost[18] = test_loss[2].item()\n            cost[19], cost[20], cost[21], cost[22], cost[23] = SegNet_MTAN.normal_error(test_pred[2], test_normal)\n\n            avg_cost[index, 12:] += cost[12:] / test_batch\n\n\n    print(\'Epoch: {:04d} | TRAIN: {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} \'\n          \'TEST: {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} \'\n          .format(index, avg_cost[index, 0], avg_cost[index, 1], avg_cost[index, 2], avg_cost[index, 3],\n                avg_cost[index, 4], avg_cost[index, 5], avg_cost[index, 6], avg_cost[index, 7], avg_cost[index, 8], avg_cost[index, 9],\n                avg_cost[index, 10], avg_cost[index, 11], avg_cost[index, 12], avg_cost[index, 13],\n                avg_cost[index, 14], avg_cost[index, 15], avg_cost[index, 16], avg_cost[index, 17], avg_cost[index, 18],\n                avg_cost[index, 19], avg_cost[index, 20], avg_cost[index, 21], avg_cost[index, 22], avg_cost[index, 23]))\n\n'"
im2im_pred/model_segnet_single.py,32,"b'import torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport argparse\n\nfrom create_dataset import *\nfrom torch.autograd import Variable\n\nparser = argparse.ArgumentParser(description=\'Single-task: One Task\')\nparser.add_argument(\'--task\', default=\'semantic\', type=str, help=\'choose task: semantic, depth, normal\')\nparser.add_argument(\'--dataroot\', default=\'nyuv2\', type=str, help=\'dataset root\')\nopt = parser.parse_args()\n\nclass SegNet(nn.Module):\n    def __init__(self):\n        super(SegNet, self).__init__()\n        # initialise network parameters\n        filter = [64, 128, 256, 512, 512]\n        self.class_nb = 13\n\n        # define encoder decoder layers\n        self.encoder_block = nn.ModuleList([self.conv_layer([3, filter[0]])])\n        self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1]]))\n            self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i]]))\n\n        # define convolution layer\n        self.conv_block_enc = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        self.conv_block_dec = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            if i == 0:\n                self.conv_block_enc.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n                self.conv_block_dec.append(self.conv_layer([filter[i], filter[i]]))\n            else:\n                self.conv_block_enc.append(nn.Sequential(self.conv_layer([filter[i + 1], filter[i + 1]]),\n                                                         self.conv_layer([filter[i + 1], filter[i + 1]])))\n                self.conv_block_dec.append(nn.Sequential(self.conv_layer([filter[i], filter[i]]),\n                                                         self.conv_layer([filter[i], filter[i]])))\n\n        if opt.task == \'semantic\':\n            self.pred_task = self.conv_layer([filter[0], self.class_nb], pred=True)\n        if opt.task == \'depth\':\n            self.pred_task = self.conv_layer([filter[0], 1], pred=True)\n        if opt.task == \'normal\':\n            self.pred_task = self.conv_layer([filter[0], 3], pred=True)\n            \n        # define pooling and unpooling functions\n        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n\n    def conv_layer(self, channel, pred=False):\n        if not pred:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(num_features=channel[1]),\n                nn.ReLU(inplace=True),\n            )\n        else:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n            )\n        return conv_block\n\n    def forward(self, x):\n        g_encoder, g_decoder, g_maxpool, g_upsampl, indices = ([0] * 5 for _ in range(5))\n        for i in range(5):\n            g_encoder[i], g_decoder[-i - 1] = ([0] * 2 for _ in range(2))\n\n        # define global shared network\n        for i in range(5):\n            if i == 0:\n                g_encoder[i][0] = self.encoder_block[i](x)\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n            else:\n                g_encoder[i][0] = self.encoder_block[i](g_maxpool[i - 1])\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\n        for i in range(5):\n            if i == 0:\n                g_upsampl[i] = self.up_sampling(g_maxpool[-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n            else:\n                g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\n        # define task prediction layers\n        if opt.task == \'semantic\':\n            pred = F.log_softmax(self.pred_task(g_decoder[-1][-1]), dim=1)\n        if opt.task == \'depth\':\n            pred = self.pred_task(g_decoder[-1][-1])\n        if opt.task == \'normal\':\n            pred = self.pred_task(g_decoder[-1][-1])\n            pred = pred / torch.norm(pred, p=2, dim=1, keepdim=True)\n        return pred\n\n    def model_fit(self, x_pred, x_output):\n        # semantic loss: depth-wise cross entropy\n        if opt.task == \'semantic\':\n            loss = F.nll_loss(x_pred, x_output, ignore_index=-1)\n\n        # depth loss: l1 norm\n        if opt.task == \'depth\':\n            # binary mark to mask out undefined pixel space\n            binary_mask = (torch.sum(x_output, dim=1) != 0).type(torch.FloatTensor).unsqueeze(1).to(device)\n            loss = torch.sum(torch.abs(x_pred - x_output) * binary_mask) / torch.nonzero(binary_mask).size(0)\n\n        # normal loss: dot product\n        if opt.task == \'normal\':\n            # binary mark to mask out undefined pixel space\n            binary_mask = (torch.sum(x_output, dim=1) != 0).type(torch.FloatTensor).unsqueeze(1).to(device)\n            loss = 1 - torch.sum((x_pred * x_output) * binary_mask) / torch.nonzero(binary_mask).size(0)\n\n        return loss\n\n    def compute_miou(self, x_pred, x_output):\n        _, x_pred_label = torch.max(x_pred, dim=1)\n        x_output_label = x_output\n        batch_size = x_pred.size(0)\n        for i in range(batch_size):\n            true_class = 0\n            first_switch = True\n            for j in range(self.class_nb):\n                pred_mask = torch.eq(x_pred_label[i], j * torch.ones(x_pred_label[i].shape).type(torch.LongTensor).to(device))\n                true_mask = torch.eq(x_output_label[i], j * torch.ones(x_output_label[i].shape).type(torch.LongTensor).to(device))\n                mask_comb = pred_mask.type(torch.FloatTensor) + true_mask.type(torch.FloatTensor)\n                union = torch.sum((mask_comb > 0).type(torch.FloatTensor))\n                intsec = torch.sum((mask_comb > 1).type(torch.FloatTensor))\n                if union == 0:\n                    continue\n                if first_switch:\n                    class_prob = intsec / union\n                    first_switch = False\n                else:\n                    class_prob = intsec / union + class_prob\n                true_class += 1\n            if i == 0:\n                batch_avg = class_prob / true_class\n            else:\n                batch_avg = class_prob / true_class + batch_avg\n        return batch_avg / batch_size\n\n    def compute_iou(self, x_pred, x_output):\n        _, x_pred_label = torch.max(x_pred, dim=1)\n        x_output_label = x_output\n        batch_size = x_pred.size(0)\n        for i in range(batch_size):\n            if i == 0:\n                pixel_acc = torch.div(torch.sum(torch.eq(x_pred_label[i], x_output_label[i]).type(torch.FloatTensor)),\n                                      torch.sum((x_output_label[i] >= 0).type(torch.FloatTensor)))\n            else:\n                pixel_acc = pixel_acc + torch.div(torch.sum(torch.eq(x_pred_label[i], x_output_label[i]).type(torch.FloatTensor)), \n                                                  torch.sum((x_output_label[i] >= 0).type(torch.FloatTensor)))\n        return pixel_acc / batch_size\n\n    def depth_error(self, x_pred, x_output):\n        binary_mask = (torch.sum(x_output, dim=1) != 0).unsqueeze(1).to(device)\n        x_pred_true = x_pred.masked_select(binary_mask)\n        x_output_true = x_output.masked_select(binary_mask)\n        abs_err = torch.abs(x_pred_true - x_output_true)\n        rel_err = torch.abs(x_pred_true - x_output_true) / x_output_true\n        return torch.sum(abs_err) / torch.nonzero(binary_mask).size(0), torch.sum(rel_err) / torch.nonzero( binary_mask).size(0)\n\n    def normal_error(self, x_pred, x_output):\n        binary_mask = (torch.sum(x_output, dim=1) != 0)\n        error = torch.acos(torch.clamp(torch.sum(x_pred * x_output, 1).masked_select(binary_mask), -1, 1)).detach().cpu().numpy()\n        error = np.degrees(error)\n        return np.mean(error), np.median(error), np.mean(error < 11.25), np.mean(error < 22.5), np.mean(error < 30)\n\n\n# define model, optimiser and scheduler\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\nSegNet = SegNet().to(device)\noptimizer = optim.Adam(SegNet.parameters(), lr=1e-4)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n\n\n# compute parameter space\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\nprint(\'Parameter Space: ABS: {:.1f}, REL: {:.4f}\\n\'.format(count_parameters(SegNet),\n                                                           count_parameters(SegNet)/24981069))\nprint(\'LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC\\n\'\n      \'DEPTH_LOSS ABS_ERR REL_ERR\\n\'\n      \'NORMAL_LOSS MEAN MED <11.25 <22.5 <30\\n\')\n\n# define dataset path\ndataset_path = opt.dataroot\nnyuv2_train_set = NYUv2(root=dataset_path, train=True)\nnyuv2_test_set = NYUv2(root=dataset_path, train=False)\n\nbatch_size = 2\nnyuv2_train_loader = torch.utils.data.DataLoader(\n    dataset=nyuv2_train_set,\n    batch_size=batch_size,\n    shuffle=True)\n\nnyuv2_test_loader = torch.utils.data.DataLoader(\n    dataset=nyuv2_test_set,\n    batch_size=batch_size,\n    shuffle=True)\n\n\n# define parameters\ntotal_epoch = 200\ntrain_batch = len(nyuv2_train_loader)\ntest_batch = len(nyuv2_test_loader)\navg_cost = np.zeros([total_epoch, 24], dtype=np.float32)\nfor epoch in range(total_epoch):\n    index = epoch\n    cost = np.zeros(24, dtype=np.float32)\n    scheduler.step()\n\n    # iteration for all batches\n    SegNet.train()\n    nyuv2_train_dataset = iter(nyuv2_train_loader)\n    for k in range(train_batch):\n        train_data, train_label, train_depth, train_normal = nyuv2_train_dataset.next()\n        train_data, train_label = train_data.to(device), train_label.type(torch.LongTensor).to(device)\n        train_depth, train_normal = train_depth.to(device), train_normal.to(device)\n\n        train_pred = SegNet(train_data)\n        optimizer.zero_grad()\n\n        if opt.task == \'semantic\':\n            train_loss = SegNet.model_fit(train_pred, train_label)\n            train_loss.backward()\n            optimizer.step()\n            cost[0] = train_loss.item()\n            cost[1] = SegNet.compute_miou(train_pred, train_label).item()\n            cost[2] = SegNet.compute_iou(train_pred, train_label).item()\n\n        if opt.task == \'depth\':\n            train_loss = SegNet.model_fit(train_pred, train_depth)\n            train_loss.backward()\n            optimizer.step()\n            cost[3] = train_loss.item()\n            cost[4], cost[5] = SegNet.depth_error(train_pred, train_depth)\n\n        if opt.task == \'normal\':\n            train_loss = SegNet.model_fit(train_pred, train_normal)\n            train_loss.backward()\n            optimizer.step()\n            cost[6] = train_loss.item()\n            cost[7], cost[8], cost[9], cost[10], cost[11] = SegNet.normal_error(train_pred, train_normal)\n\n        avg_cost[index, :12] += cost[:12] / train_batch\n\n    # evaluating test data\n    SegNet.eval()\n    with torch.no_grad():  # operations inside don\'t track history\n        nyuv2_test_dataset = iter(nyuv2_test_loader)\n        for k in range(test_batch):\n            test_data, test_label, test_depth, test_normal = nyuv2_test_dataset.next()\n            test_data, test_label = test_data.to(device),  test_label.type(torch.LongTensor).to(device)\n            test_depth, test_normal = test_depth.to(device), test_normal.to(device)\n\n            test_pred = SegNet(test_data)\n\n            if opt.task == \'semantic\':\n                test_loss = SegNet.model_fit(test_pred, test_label)\n                cost[12] = test_loss.item()\n                cost[13] = SegNet.compute_miou(test_pred, test_label).item()\n                cost[14] = SegNet.compute_iou(test_pred, test_label).item()\n\n            if opt.task == \'depth\':\n                test_loss = SegNet.model_fit(test_pred, test_depth)\n                cost[15] = test_loss.item()\n                cost[16], cost[17] = SegNet.depth_error(test_pred, test_depth)\n\n            if opt.task == \'normal\':\n                test_loss = SegNet.model_fit(test_pred, test_normal)\n                cost[18] = test_loss.item()\n                cost[19], cost[20], cost[21], cost[22], cost[23] = SegNet.normal_error(test_pred, test_normal)\n\n            avg_cost[index, 12:] += cost[12:] / test_batch\n\n    if opt.task == \'semantic\':\n        print(\'Epoch: {:04d} | TRAIN: {:.4f} {:.4f} {:.4f} TEST: {:.4f} {:.4f} {:.4f}\'\n          .format(index, avg_cost[index, 0], avg_cost[index, 1], avg_cost[index, 2], avg_cost[index, 12], avg_cost[index, 13], avg_cost[index, 14]))\n    if opt.task == \'depth\':\n        print(\'Epoch: {:04d} | TRAIN: {:.4f} {:.4f} {:.4f} TEST: {:.4f} {:.4f} {:.4f}\'\n          .format(index, avg_cost[index, 3], avg_cost[index, 4], avg_cost[index, 5], avg_cost[index, 15], avg_cost[index, 16], avg_cost[index, 17]))\n    if opt.task == \'normal\':\n        print(\'Epoch: {:04d} | TRAIN: {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} TEST: {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f}\'\n          .format(index, avg_cost[index, 6], avg_cost[index, 7], avg_cost[index, 8], avg_cost[index, 9], avg_cost[index, 10], avg_cost[index, 11],\n                  avg_cost[index, 18], avg_cost[index, 19], avg_cost[index, 20], avg_cost[index, 21], avg_cost[index, 22], avg_cost[index, 23]))\n\n'"
im2im_pred/model_segnet_split.py,34,"b'import torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport argparse\nimport torch.utils.data.sampler as sampler\n\nfrom create_dataset import *\nfrom torch.autograd import Variable\n\nparser = argparse.ArgumentParser(description=\'Multi-task: Split\')\nparser.add_argument(\'--type\', default=\'standard\', type=str, help=\'split type: standard, wide, deep\')\nparser.add_argument(\'--weight\', default=\'equal\', type=str, help=\'multi-task weighting: equal, uncert, dwa\')\nparser.add_argument(\'--dataroot\', default=\'nyuv2\', type=str, help=\'dataset root\')\nparser.add_argument(\'--temp\', default=2.0, type=float, help=\'temperature for DWA (must be positive)\')\nopt = parser.parse_args()\n\n\nclass SegNet(nn.Module):\n    def __init__(self):\n        super(SegNet, self).__init__()\n        # initialise network parameters\n        if opt.type == \'wide\':\n            filter = [64, 128, 256, 512, 1024]\n        else:\n            filter = [64, 128, 256, 512, 512]\n\n        self.class_nb = 13\n\n        # define encoder decoder layers\n        self.encoder_block = nn.ModuleList([self.conv_layer([3, filter[0]])])\n        self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1]]))\n            self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i]]))\n\n        # define convolution layer\n        self.conv_block_enc = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        self.conv_block_dec = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            if i == 0:\n                self.conv_block_enc.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n                self.conv_block_dec.append(self.conv_layer([filter[i], filter[i]]))\n            else:\n                self.conv_block_enc.append(nn.Sequential(self.conv_layer([filter[i + 1], filter[i + 1]]),\n                                                         self.conv_layer([filter[i + 1], filter[i + 1]])))\n                self.conv_block_dec.append(nn.Sequential(self.conv_layer([filter[i], filter[i]]),\n                                                         self.conv_layer([filter[i], filter[i]])))\n\n        # define task specific layers\n        self.pred_task1 = nn.Sequential(nn.Conv2d(in_channels=filter[0], out_channels=filter[0], kernel_size=3, padding=1),\n                                        nn.Conv2d(in_channels=filter[0], out_channels=self.class_nb, kernel_size=1, padding=0))\n        self.pred_task2 = nn.Sequential(nn.Conv2d(in_channels=filter[0], out_channels=filter[0], kernel_size=3, padding=1),\n                                        nn.Conv2d(in_channels=filter[0], out_channels=1, kernel_size=1, padding=0))\n        self.pred_task3 = nn.Sequential(nn.Conv2d(in_channels=filter[0], out_channels=filter[0], kernel_size=3, padding=1),\n                                        nn.Conv2d(in_channels=filter[0], out_channels=3, kernel_size=1, padding=0))\n\n        # define pooling and unpooling functions\n        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\n        self.logsigma = nn.Parameter(torch.FloatTensor([-0.5, -0.5, -0.5]))\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n\n    # define convolutional block\n    def conv_layer(self, channel):\n        if opt.type == \'deep\':\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(num_features=channel[1]),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(in_channels=channel[1], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(num_features=channel[1]),\n                nn.ReLU(inplace=True),\n            )\n        else:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(num_features=channel[1]),\n                nn.ReLU(inplace=True)\n            )\n        return conv_block\n\n    def forward(self, x):\n        g_encoder, g_decoder, g_maxpool, g_upsampl, indices = ([0] * 5 for _ in range(5))\n        for i in range(5):\n            g_encoder[i], g_decoder[-i - 1] = ([0] * 2 for _ in range(2))\n\n        # global shared encoder-decoder network\n        for i in range(5):\n            if i == 0:\n                g_encoder[i][0] = self.encoder_block[i](x)\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n            else:\n                g_encoder[i][0] = self.encoder_block[i](g_maxpool[i - 1])\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\n        for i in range(5):\n            if i == 0:\n                g_upsampl[i] = self.up_sampling(g_maxpool[-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n            else:\n                g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\n        # define task prediction layers\n        t1_pred = F.log_softmax(self.pred_task1(g_decoder[i][1]), dim=1)\n        t2_pred = self.pred_task2(g_decoder[i][1])\n        t3_pred = self.pred_task3(g_decoder[i][1])\n        t3_pred = t3_pred / torch.norm(t3_pred, p=2, dim=1, keepdim=True)\n\n        return [t1_pred, t2_pred, t3_pred], self.logsigma\n\n    def model_fit(self, x_pred1, x_output1, x_pred2, x_output2, x_pred3, x_output3):\n        # binary mark to mask out undefined pixel space\n        binary_mask = (torch.sum(x_output2, dim=1) != 0).type(torch.FloatTensor).unsqueeze(1).to(device)\n\n        # semantic loss: depth-wise cross entropy\n        loss1 = F.nll_loss(x_pred1, x_output1, ignore_index=-1)\n\n        # depth loss: l1 norm\n        loss2 = torch.sum(torch.abs(x_pred2 - x_output2) * binary_mask) / torch.nonzero(binary_mask).size(0)\n\n        # normal loss: dot product\n        loss3 = 1 - torch.sum((x_pred3 * x_output3) * binary_mask) / torch.nonzero(binary_mask).size(0)\n\n        return [loss1, loss2, loss3]\n\n    def compute_miou(self, x_pred, x_output):\n        _, x_pred_label = torch.max(x_pred, dim=1)\n        x_output_label = x_output\n        batch_size = x_pred.size(0)\n        for i in range(batch_size):\n            true_class = 0\n            first_switch = True\n            for j in range(self.class_nb):\n                pred_mask = torch.eq(x_pred_label[i], j * torch.ones(x_pred_label[i].shape).type(torch.LongTensor).to(device))\n                true_mask = torch.eq(x_output_label[i], j * torch.ones(x_output_label[i].shape).type(torch.LongTensor).to(device))\n                mask_comb = pred_mask.type(torch.FloatTensor) + true_mask.type(torch.FloatTensor)\n                union     = torch.sum((mask_comb > 0).type(torch.FloatTensor))\n                intsec    = torch.sum((mask_comb > 1).type(torch.FloatTensor))\n                if union == 0:\n                    continue\n                if first_switch:\n                    class_prob = intsec / union\n                    first_switch = False\n                else:\n                    class_prob = intsec / union + class_prob\n                true_class += 1\n            if i == 0:\n                batch_avg = class_prob / true_class\n            else:\n                batch_avg = class_prob / true_class + batch_avg\n        return batch_avg / batch_size\n\n    def compute_iou(self, x_pred, x_output):\n        _, x_pred_label = torch.max(x_pred, dim=1)\n        x_output_label = x_output\n        batch_size = x_pred.size(0)\n        for i in range(batch_size):\n            if i == 0:\n                pixel_acc = torch.div(torch.sum(torch.eq(x_pred_label[i], x_output_label[i]).type(torch.FloatTensor)),\n                            torch.sum((x_output_label[i] >= 0).type(torch.FloatTensor)))\n            else:\n                pixel_acc = pixel_acc + torch.div(torch.sum(torch.eq(x_pred_label[i], x_output_label[i]).type(torch.FloatTensor)),\n                            torch.sum((x_output_label[i] >= 0).type(torch.FloatTensor)))\n        return pixel_acc / batch_size\n\n    def depth_error(self, x_pred, x_output):\n        binary_mask = (torch.sum(x_output, dim=1) != 0).unsqueeze(1).to(device)\n        x_pred_true = x_pred.masked_select(binary_mask)\n        x_output_true = x_output.masked_select(binary_mask)\n        abs_err = torch.abs(x_pred_true - x_output_true)\n        rel_err = torch.abs(x_pred_true - x_output_true) / x_output_true\n        return torch.sum(abs_err) / torch.nonzero(binary_mask).size(0), torch.sum(rel_err) / torch.nonzero(binary_mask).size(0)\n\n    def normal_error(self, x_pred, x_output):\n        binary_mask = (torch.sum(x_output, dim=1) != 0)\n        error = torch.acos(torch.clamp(torch.sum(x_pred * x_output, 1).masked_select(binary_mask), -1, 1)).detach().cpu().numpy()\n        error = np.degrees(error)\n        return np.mean(error), np.median(error), np.mean(error < 11.25), np.mean(error < 22.5), np.mean(error < 30)\n\n\n# define model, optimiser and scheduler\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\nSegNet_SPLIT = SegNet().to(device)\noptimizer = optim.Adam(SegNet_SPLIT.parameters(), lr=1e-4)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n\n\n# compute parameter space\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\nprint(\'Parameter Space: ABS: {:.1f}, REL: {:.4f}\\n\'.format(count_parameters(SegNet_SPLIT),\n                                                           count_parameters(SegNet_SPLIT)/24981069))\nprint(\'LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC | DEPTH_LOSS ABS_ERR REL_ERR | NORMAL_LOSS MEAN MED <11.25 <22.5 <30\\n\')\n\n# define dataset path\ndataset_path = opt.dataroot\nnyuv2_train_set = NYUv2(root=dataset_path, train=True)\nnyuv2_test_set = NYUv2(root=dataset_path, train=False)\n\nbatch_size = 2\nnyuv2_train_loader = torch.utils.data.DataLoader(\n    dataset=nyuv2_train_set,\n    batch_size=batch_size,\n    shuffle=True)\n\nnyuv2_test_loader = torch.utils.data.DataLoader(\n    dataset=nyuv2_test_set,\n    batch_size=batch_size,\n    shuffle=True)\n\n\n# define parameters\ntotal_epoch = 200\ntrain_batch = len(nyuv2_train_loader)\ntest_batch = len(nyuv2_test_loader)\nT = opt.temp\navg_cost = np.zeros([total_epoch, 24], dtype=np.float32)\nlambda_weight = np.ones([3, total_epoch])\nfor epoch in range(total_epoch):\n    index = epoch\n    cost = np.zeros(24, dtype=np.float32)\n    scheduler.step()\n\n    # apply Dynamic Weight Average\n    if opt.weight == \'dwa\':\n        if index == 0 or index == 1:\n            lambda_weight[:, index] = 1.0\n        else:\n            w_1 = avg_cost[index - 1, 0] / avg_cost[index - 2, 0]\n            w_2 = avg_cost[index - 1, 3] / avg_cost[index - 2, 3]\n            w_3 = avg_cost[index - 1, 6] / avg_cost[index - 2, 6]\n            lambda_weight[0, index] = 3 * np.exp(w_1 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T) + np.exp(w_3 / T))\n            lambda_weight[1, index] = 3 * np.exp(w_2 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T) + np.exp(w_3 / T))\n            lambda_weight[2, index] = 3 * np.exp(w_3 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T) + np.exp(w_3 / T))\n\n    # iteration for all batches\n    SegNet_SPLIT.train()\n    nyuv2_train_dataset = iter(nyuv2_train_loader)\n    for k in range(train_batch):\n        train_data, train_label, train_depth, train_normal = nyuv2_train_dataset.next()\n        train_data, train_label = train_data.to(device), train_label.type(torch.LongTensor).to(device)\n        train_depth, train_normal = train_depth.to(device), train_normal.to(device)\n\n        train_pred, logsigma = SegNet_SPLIT(train_data)\n\n        optimizer.zero_grad()\n        train_loss = SegNet_SPLIT.model_fit(train_pred[0], train_label, train_pred[1], train_depth, train_pred[2], train_normal)\n\n        if opt.weight == \'equal\' or opt.weight == \'dwa\':\n            loss = sum([lambda_weight[i, index] * train_loss[i] for i in range(3)])\n        else:\n            loss = sum(1 / (2 * torch.exp(logsigma[i])) * train_loss[i] + logsigma[i] / 2 for i in range(3))\n\n        loss.backward()\n        optimizer.step()\n\n        cost[0] = train_loss[0].item()\n        cost[1] = SegNet_SPLIT.compute_miou(train_pred[0], train_label).item()\n        cost[2] = SegNet_SPLIT.compute_iou(train_pred[0], train_label).item()\n        cost[3] = train_loss[1].item()\n        cost[4], cost[5] = SegNet_SPLIT.depth_error(train_pred[1], train_depth)\n        cost[6] = train_loss[2].item()\n        cost[7], cost[8], cost[9], cost[10], cost[11] = SegNet_SPLIT.normal_error(train_pred[2], train_normal)\n        avg_cost[index, :12] += cost[:12] / train_batch\n\n    # evaluating test data\n    SegNet_SPLIT.eval()\n    with torch.no_grad():  # operations inside don\'t track history\n        nyuv2_test_dataset = iter(nyuv2_test_loader)\n        for k in range(test_batch):\n            test_data, test_label, test_depth, test_normal = nyuv2_test_dataset.next()\n            test_data, test_label = test_data.to(device),  test_label.type(torch.LongTensor).to(device)\n            test_depth, test_normal = test_depth.to(device), test_normal.to(device)\n\n            test_pred, _ = SegNet_SPLIT(test_data)\n            test_loss = SegNet_SPLIT.model_fit(test_pred[0], test_label, test_pred[1], test_depth, test_pred[2], test_normal)\n\n            cost[12] = test_loss[0].item()\n            cost[13] = SegNet_SPLIT.compute_miou(test_pred[0], test_label).item()\n            cost[14] = SegNet_SPLIT.compute_iou(test_pred[0], test_label).item()\n            cost[15] = test_loss[1].item()\n            cost[16], cost[17] = SegNet_SPLIT.depth_error(test_pred[1], test_depth)\n            cost[18] = test_loss[2].item()\n            cost[19], cost[20], cost[21], cost[22], cost[23] = SegNet_SPLIT.normal_error(test_pred[2], test_normal)\n\n            avg_cost[index, 12:] += cost[12:] / test_batch\n\n\n    print(\'Epoch: {:04d} | TRAIN: {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} \'\n          \'TEST: {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} \'\n          .format(index, avg_cost[index, 0], avg_cost[index, 1], avg_cost[index, 2], avg_cost[index, 3],\n                avg_cost[index, 4], avg_cost[index, 5], avg_cost[index, 6], avg_cost[index, 7], avg_cost[index, 8], avg_cost[index, 9],\n                avg_cost[index, 10], avg_cost[index, 11], avg_cost[index, 12], avg_cost[index, 13],\n                avg_cost[index, 14], avg_cost[index, 15], avg_cost[index, 16], avg_cost[index, 17], avg_cost[index, 18],\n                avg_cost[index, 19], avg_cost[index, 20], avg_cost[index, 21], avg_cost[index, 22], avg_cost[index, 23]))\n\n'"
im2im_pred/model_segnet_stan.py,35,"b'import torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport argparse\n\nfrom create_dataset import *\nfrom torch.autograd import Variable\n\nparser = argparse.ArgumentParser(description=\'Single-task: Attention Network\')\nparser.add_argument(\'--task\', default=\'semantic\', type=str, help=\'choose task: semantic, depth, normal\')\nparser.add_argument(\'--dataroot\', default=\'nyuv2\', type=str, help=\'dataset root\')\nopt = parser.parse_args()\n\n\nclass SegNet(nn.Module):\n    def __init__(self):\n        super(SegNet, self).__init__()\n        # initialise network parameters\n        filter = [64, 128, 256, 512, 512]\n        self.class_nb = 13\n\n        # define encoder decoder layers\n        self.encoder_block = nn.ModuleList([self.conv_layer([3, filter[0]])])\n        self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1]]))\n            self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i]]))\n\n        # define convolution layer\n        self.conv_block_enc = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        self.conv_block_dec = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n        for i in range(4):\n            if i == 0:\n                self.conv_block_enc.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n                self.conv_block_dec.append(self.conv_layer([filter[i], filter[i]]))\n            else:\n                self.conv_block_enc.append(nn.Sequential(self.conv_layer([filter[i + 1], filter[i + 1]]),\n                                                         self.conv_layer([filter[i + 1], filter[i + 1]])))\n                self.conv_block_dec.append(nn.Sequential(self.conv_layer([filter[i], filter[i]]),\n                                                         self.conv_layer([filter[i], filter[i]])))\n\n        self.encoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])])])\n        self.decoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])])])\n        self.encoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[1]])])\n        self.decoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\n        for j in range(1):\n            for i in range(4):\n                self.encoder_att[j].append(self.att_layer([2 * filter[i + 1], filter[i + 1], filter[i + 1]]))\n                self.decoder_att[j].append(self.att_layer([filter[i + 1] + filter[i], filter[i], filter[i]]))\n\n        for i in range(4):\n            if i < 3:\n                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 2]]))\n                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i]]))\n            else:\n                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\n        if opt.task == \'semantic\':\n            self.pred_task = self.conv_layer([filter[0], self.class_nb], pred=True)\n        if opt.task == \'depth\':\n            self.pred_task = self.conv_layer([filter[0], 1], pred=True)\n        if opt.task == \'normal\':\n            self.pred_task = self.conv_layer([filter[0], 3], pred=True)\n\n        # define pooling and unpooling functions\n        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n\n    def conv_layer(self, channel, pred=False):\n        if not pred:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n                nn.BatchNorm2d(num_features=channel[1]),\n                nn.ReLU(inplace=True),\n            )\n        else:\n            conv_block = nn.Sequential(\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n            )\n        return conv_block\n\n    def att_layer(self, channel):\n        att_block = nn.Sequential(\n            nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n            nn.BatchNorm2d(channel[1]),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=1, padding=0),\n            nn.BatchNorm2d(channel[2]),\n            nn.Sigmoid(),\n        )\n        return att_block\n\n    def forward(self, x):\n        g_encoder, g_decoder, g_maxpool, g_upsampl, indices = ([0] * 5 for _ in range(5))\n        for i in range(5):\n            g_encoder[i], g_decoder[-i - 1] = ([0] * 2 for _ in range(2))\n\n        # define attention list for two tasks\n        atten_encoder, atten_decoder = ([0] * 3 for _ in range(2))\n        for i in range(3):\n            atten_encoder[i], atten_decoder[i] = ([0] * 5 for _ in range(2))\n        for i in range(3):\n            for j in range(5):\n                atten_encoder[i][j], atten_decoder[i][j] = ([0] * 3 for _ in range(2))\n\n        # define global shared network\n        for i in range(5):\n            if i == 0:\n                g_encoder[i][0] = self.encoder_block[i](x)\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n            else:\n                g_encoder[i][0] = self.encoder_block[i](g_maxpool[i - 1])\n                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\n        for i in range(5):\n            if i == 0:\n                g_upsampl[i] = self.up_sampling(g_maxpool[-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n            else:\n                g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])\n                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\n        # define task dependent attention module\n        for i in range(1):\n            for j in range(5):\n                if j == 0:\n                    atten_encoder[i][j][0] = self.encoder_att[i][j](g_encoder[j][0])\n                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n                else:\n                    atten_encoder[i][j][0] = self.encoder_att[i][j](torch.cat((g_encoder[j][0], atten_encoder[i][j - 1][2]), dim=1))\n                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n\n            for j in range(5):\n                if j == 0:\n                    atten_decoder[i][j][0] = F.interpolate(atten_encoder[i][-1][-1], scale_factor=2, mode=\'bilinear\', align_corners=True)\n                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat((g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n                else:\n                    atten_decoder[i][j][0] = F.interpolate(atten_decoder[i][j - 1][2], scale_factor=2, mode=\'bilinear\', align_corners=True)\n                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat((g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n\n        # define task prediction layers\n        if opt.task == \'semantic\':\n            pred = F.log_softmax(self.pred_task(atten_decoder[0][-1][-1]), dim=1)\n        if opt.task == \'depth\':\n            pred = self.pred_task(atten_decoder[0][-1][-1])\n        if opt.task == \'normal\':\n            pred = self.pred_task(atten_decoder[0][-1][-1])\n            pred = pred / torch.norm(pred, p=2, dim=1, keepdim=True)\n        return pred\n\n    def model_fit(self, x_pred, x_output):\n        # semantic loss: depth-wise cross entropy\n        if opt.task == \'semantic\':\n            loss = F.nll_loss(x_pred, x_output, ignore_index=-1)\n\n        # depth loss: l1 norm\n        if opt.task == \'depth\':\n            # binary mark to mask out undefined pixel space\n            binary_mask = (torch.sum(x_output, dim=1) != 0).type(torch.FloatTensor).unsqueeze(1).to(device)\n            loss = torch.sum(torch.abs(x_pred - x_output) * binary_mask) / torch.nonzero(binary_mask).size(0)\n\n        # normal loss: dot product\n        if opt.task == \'normal\':\n            # binary mark to mask out undefined pixel space\n            binary_mask = (torch.sum(x_output, dim=1) != 0).type(torch.FloatTensor).unsqueeze(1).to(device)\n            loss = 1 - torch.sum((x_pred * x_output) * binary_mask) / torch.nonzero(binary_mask).size(0)\n\n        return loss\n\n    def compute_miou(self, x_pred, x_output):\n        _, x_pred_label = torch.max(x_pred, dim=1)\n        x_output_label = x_output\n        batch_size = x_pred.size(0)\n        for i in range(batch_size):\n            true_class = 0\n            first_switch = True\n            for j in range(self.class_nb):\n                pred_mask = torch.eq(x_pred_label[i], j * torch.ones(x_pred_label[i].shape).type(torch.LongTensor).to(device))\n                true_mask = torch.eq(x_output_label[i], j * torch.ones(x_output_label[i].shape).type(torch.LongTensor).to(device))\n                mask_comb = pred_mask.type(torch.FloatTensor) + true_mask.type(torch.FloatTensor)\n                union = torch.sum((mask_comb > 0).type(torch.FloatTensor))\n                intsec = torch.sum((mask_comb > 1).type(torch.FloatTensor))\n                if union == 0:\n                    continue\n                if first_switch:\n                    class_prob = intsec / union\n                    first_switch = False\n                else:\n                    class_prob = intsec / union + class_prob\n                true_class += 1\n            if i == 0:\n                batch_avg = class_prob / true_class\n            else:\n                batch_avg = class_prob / true_class + batch_avg\n        return batch_avg / batch_size\n\n    def compute_iou(self, x_pred, x_output):\n        _, x_pred_label = torch.max(x_pred, dim=1)\n        x_output_label = x_output\n        batch_size = x_pred.size(0)\n        for i in range(batch_size):\n            if i == 0:\n                pixel_acc = torch.div(torch.sum(torch.eq(x_pred_label[i], x_output_label[i]).type(torch.FloatTensor)),\n                                      torch.sum((x_output_label[i] >= 0).type(torch.FloatTensor)))\n            else:\n                pixel_acc = pixel_acc + torch.div(torch.sum(torch.eq(x_pred_label[i], x_output_label[i]).type(torch.FloatTensor)),\n                    torch.sum((x_output_label[i] >= 0).type(torch.FloatTensor)))\n        return pixel_acc / batch_size\n\n    def depth_error(self, x_pred, x_output):\n        binary_mask = (torch.sum(x_output, dim=1) != 0).unsqueeze(1).to(device)\n        x_pred_true = x_pred.masked_select(binary_mask)\n        x_output_true = x_output.masked_select(binary_mask)\n        abs_err = torch.abs(x_pred_true - x_output_true)\n        rel_err = torch.abs(x_pred_true - x_output_true) / x_output_true\n        return torch.sum(abs_err) / torch.nonzero(binary_mask).size(0), torch.sum(rel_err) / torch.nonzero( binary_mask).size(0)\n\n    def normal_error(self, x_pred, x_output):\n        binary_mask = (torch.sum(x_output, dim=1) != 0)\n        error = torch.acos(torch.clamp(torch.sum(x_pred * x_output, 1).masked_select(binary_mask), -1, 1)).detach().cpu().numpy()\n        error = np.degrees(error)\n        return np.mean(error), np.median(error), np.mean(error < 11.25), np.mean(error < 22.5), np.mean(error < 30)\n\n\n# define model, optimiser and scheduler\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\nSegNet_STAN = SegNet().to(device)\noptimizer = optim.Adam(SegNet_STAN.parameters(), lr=1e-4)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n\n\n# compute parameter space\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\nprint(\'Parameter Space: ABS: {:.1f}, REL: {:.4f}\\n\'.format(count_parameters(SegNet_STAN),\n                                                           count_parameters(SegNet_STAN)/24981069))\nprint(\'LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC\\n\'\n      \'DEPTH_LOSS ABS_ERR REL_ERR\\n\'\n      \'NORMAL_LOSS MEAN MED <11.25 <22.5 <30\\n\')\n\n# define dataset path\ndataset_path = opt.dataroot\nnyuv2_train_set = NYUv2(root=dataset_path, train=True)\nnyuv2_test_set = NYUv2(root=dataset_path, train=False)\n\nbatch_size = 2\nnyuv2_train_loader = torch.utils.data.DataLoader(\n    dataset=nyuv2_train_set,\n    batch_size=batch_size,\n    shuffle=True)\n\nnyuv2_test_loader = torch.utils.data.DataLoader(\n    dataset=nyuv2_test_set,\n    batch_size=batch_size,\n    shuffle=True)\n\n\n# define parameters\ntotal_epoch = 200\ntrain_batch = len(nyuv2_train_loader)\ntest_batch = len(nyuv2_test_loader)\navg_cost = np.zeros([total_epoch, 24], dtype=np.float32)\nfor epoch in range(total_epoch):\n    index = epoch\n    cost = np.zeros(24, dtype=np.float32)\n    scheduler.step()\n\n    # iteration for all batches\n    SegNet_STAN.train()\n    nyuv2_train_dataset = iter(nyuv2_train_loader)\n    for k in range(train_batch):\n        train_data, train_label, train_depth, train_normal = nyuv2_train_dataset.next()\n        train_data, train_label = train_data.to(device), train_label.type(torch.LongTensor).to(device)\n        train_depth, train_normal = train_depth.to(device), train_normal.to(device)\n\n        train_pred = SegNet_STAN(train_data)\n        optimizer.zero_grad()\n\n        if opt.task == \'semantic\':\n            train_loss = SegNet_STAN.model_fit(train_pred, train_label)\n            train_loss.backward()\n            optimizer.step()\n            cost[0] = train_loss.item()\n            cost[1] = SegNet_STAN.compute_miou(train_pred, train_label).item()\n            cost[2] = SegNet_STAN.compute_iou(train_pred, train_label).item()\n\n        if opt.task == \'depth\':\n            train_loss = SegNet_STAN.model_fit(train_pred, train_depth)\n            train_loss.backward()\n            optimizer.step()\n            cost[3] = train_loss.item()\n            cost[4], cost[5] = SegNet_STAN.depth_error(train_pred, train_depth)\n\n        if opt.task == \'normal\':\n            train_loss = SegNet_STAN.model_fit(train_pred, train_normal)\n            train_loss.backward()\n            optimizer.step()\n            cost[6] = train_loss.item()\n            cost[7], cost[8], cost[9], cost[10], cost[11] = SegNet_STAN.normal_error(train_pred, train_normal)\n\n        avg_cost[index, :12] += cost[:12] / train_batch\n\n    # evaluating test data\n    SegNet_STAN.eval()\n    with torch.no_grad():  # operations inside don\'t track history\n        nyuv2_test_dataset = iter(nyuv2_test_loader)\n        for k in range(test_batch):\n            test_data, test_label, test_depth, test_normal = nyuv2_test_dataset.next()\n            test_data, test_label = test_data.to(device),  test_label.type(torch.LongTensor).to(device)\n            test_depth, test_normal = test_depth.to(device), test_normal.to(device)\n\n            test_pred = SegNet_STAN(test_data)\n\n            if opt.task == \'semantic\':\n                test_loss = SegNet_STAN.model_fit(test_pred, test_label)\n                cost[12] = test_loss.item()\n                cost[13] = SegNet_STAN.compute_miou(test_pred, test_label).item()\n                cost[14] = SegNet_STAN.compute_iou(test_pred, test_label).item()\n\n            if opt.task == \'depth\':\n                test_loss = SegNet_STAN.model_fit(test_pred, test_depth)\n                cost[15] = test_loss.item()\n                cost[16], cost[17] = SegNet_STAN.depth_error(test_pred, test_depth)\n\n            if opt.task == \'normal\':\n                test_loss = SegNet_STAN.model_fit(test_pred, test_normal)\n                cost[18] = test_loss.item()\n                cost[19], cost[20], cost[21], cost[22], cost[23] = SegNet_STAN.normal_error(test_pred, test_normal)\n\n            avg_cost[index, 12:] += cost[12:] / test_batch\n\n    if opt.task == \'semantic\':\n        print(\'Epoch: {:04d} | TRAIN: {:.4f} {:.4f} {:.4f} TEST: {:.4f} {:.4f} {:.4f}\'\n          .format(index, avg_cost[index, 0], avg_cost[index, 1], avg_cost[index, 2], avg_cost[index, 12], avg_cost[index, 13], avg_cost[index, 14]))\n    if opt.task == \'depth\':\n        print(\'Epoch: {:04d} | TRAIN: {:.4f} {:.4f} {:.4f} TEST: {:.4f} {:.4f} {:.4f}\'\n          .format(index, avg_cost[index, 3], avg_cost[index, 4], avg_cost[index, 5], avg_cost[index, 15], avg_cost[index, 16], avg_cost[index, 17]))\n    if opt.task == \'normal\':\n        print(\'Epoch: {:04d} | TRAIN: {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} TEST: {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f}\'\n          .format(index, avg_cost[index, 6], avg_cost[index, 7], avg_cost[index, 8], avg_cost[index, 9], avg_cost[index, 10], avg_cost[index, 11],\n                  avg_cost[index, 18], avg_cost[index, 19], avg_cost[index, 20], avg_cost[index, 21], avg_cost[index, 22], avg_cost[index, 23]))\n\n'"
visual_decathlon/coco_results.py,0,"b'from pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\nimport json\nimport numpy as np\nimport pickle\n\npickle_in = open(""imagenet.pickle"", ""rb"")\nimagenet = pickle.load(pickle_in)\n\npickle_in = open(""ans.pickle"", ""rb"")\nans = pickle.load(pickle_in)\n\nans[\'imagenet12\'] = imagenet[\'imagenet12\']\n\nclass_name = [\'aircraft\', \'cifar100\', \'daimlerpedcls\', \'dtd\', \'gtsrb\',\n              \'imagenet12\', \'omniglot\', \'svhn\', \'ucf101\', \'vgg-flowers\']\n\ndict_key = {}\nfor i in range(10):\n    cocoGt = COCO(\'annotations/{:s}_val.json\'.format(class_name[i]))\n    imgIds = sorted(cocoGt.getImgIds())\n    cat = cocoGt.getCatIds()\n    data_key = np.zeros(len(imgIds))\n    k = 0\n    for item in imgIds:\n          data_key[k] = cocoGt.imgToAnns[item][0][\'category_id\']\n          k = k + 1\n    u, ind = np.unique(data_key, return_index=True)\n    u[np.argsort(ind)]\n    dict_key[class_name[i]] = u[np.argsort(ind)]\n\n\nres = []\nfor i in range(10):\n    cocoGt = COCO(\'annotations/{:s}_test_stripped.json\'.format(class_name[i]))\n    imgIds = sorted(cocoGt.getImgIds())\n    cat = cocoGt.getCatIds()\n    for item in imgIds:\n        res.append({""image_id"": item, ""category_id"": int(dict_key[class_name[i]][ans[class_name[i]].pop(0)])})\n\n\nwith open(\'results.json\', \'w\') as outfile:\n    json.dump(res, outfile)\n\n\nprint(\'JSON FILE HAS BEEN CREATED. :D\')\n\n\n'"
visual_decathlon/model_wrn_eval.py,21,"b'import torch\nimport torch.nn as nn\n\nimport torch.nn.init as init\nimport torch.nn.functional as F\nimport torchvision\nimport numpy as np\nimport torch.optim as optim\nimport pickle\nimport argparse\n\nfrom torchvision.transforms import transforms\n\nparser = argparse.ArgumentParser(description=\'Visual Decathlon Challenge: Evaluation\')\nparser.add_argument(\'--dataset\', default=\'imagenet\', type=str, help=\'choose dataset: imagenet, notimagenet\')\nopt = parser.parse_args()\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=True)\n\ndef conv_init(m):\n    classname = m.__class__.__name__\n    if classname.find(\'Conv\') != -1:\n        init.xavier_uniform(m.weight, gain=np.sqrt(2))\n        init.constant(m.bias, 0)\n    elif classname.find(\'BatchNorm\') != -1:\n        init.constant(m.weight, 1)\n        init.constant(m.bias, 0)\n\n\nclass wide_basic(nn.Module):\n    def __init__(self, in_planes, planes, stride=1):\n        super(wide_basic, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, padding=1, bias=True)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=True)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=True),\n            )\n\n    def forward(self, x):\n        out = self.conv1(F.relu(self.bn1(x)))\n        out = self.conv2(F.relu(self.bn2(out)))\n        out += self.shortcut(x)\n\n        return out\n\n\nclass WideResNet(nn.Module):\n    def __init__(self, depth, widen_factor, num_classes):\n        super(WideResNet, self).__init__()\n        self.in_planes = 16\n        n = int((depth - 4) / 6)\n        k = widen_factor\n        filter = [16, 16 * k, 32 * k, 64 * k]\n\n        self.conv1 = conv3x3(3, filter[0], stride=1)\n        self.layer1 = self._wide_layer(wide_basic, filter[1], n, stride=2)\n        self.layer2 = self._wide_layer(wide_basic, filter[2], n, stride=2)\n        self.layer3 = self._wide_layer(wide_basic, filter[3], n, stride=2)\n        self.bn1 = nn.BatchNorm2d(filter[3], momentum=0.9)\n\n        self.linear = nn.ModuleList([nn.Sequential(\n            nn.Linear(filter[3], num_classes[0]),\n            nn.Softmax(dim=1))])\n\n        # attention modules\n        self.encoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])])])\n        self.encoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[1]])])\n\n        for j in range(10):\n            if j < 9:\n                self.encoder_att.append(nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])]))\n                self.linear.append(nn.Sequential(nn.Linear(filter[3], num_classes[j + 1]),\n                                                 nn.Softmax(dim=1)))\n            for i in range(3):\n                self.encoder_att[j].append(self.att_layer([2 * filter[i + 1], filter[i + 1], filter[i + 1]]))\n\n        for i in range(3):\n            if i < 2:\n                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 2]]))\n            else:\n                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\n    def conv_layer(self, channel):\n        conv_block = nn.Sequential(\n            nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n            nn.BatchNorm2d(num_features=channel[1]),\n            nn.ReLU(inplace=True),\n        )\n        return conv_block\n\n    def att_layer(self, channel):\n        att_block = nn.Sequential(\n            nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n            nn.BatchNorm2d(channel[1]),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=1, padding=0),\n            nn.BatchNorm2d(channel[2]),\n            nn.Sigmoid(),\n        )\n        return att_block\n\n    def _wide_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x, k):\n        g_encoder = [0] * 4\n\n        atten_encoder = [0] * 10\n        for i in range(10):\n            atten_encoder[i] = [0] * 4\n        for i in range(10):\n            for j in range(4):\n                atten_encoder[i][j] = [0] * 3\n\n        # shared encoder\n        g_encoder[0] = self.conv1(x)\n        g_encoder[1] = self.layer1(g_encoder[0])\n        g_encoder[2] = self.layer2(g_encoder[1])\n        g_encoder[3] = F.relu(self.bn1(self.layer3(g_encoder[2])))\n\n        # apply attention modules\n        for j in range(4):\n            if j == 0:\n                atten_encoder[k][j][0] = self.encoder_att[k][j](g_encoder[0])\n                atten_encoder[k][j][1] = (atten_encoder[k][j][0]) * g_encoder[0]\n                atten_encoder[k][j][2] = self.encoder_block_att[j](atten_encoder[k][j][1])\n                atten_encoder[k][j][2] = F.max_pool2d(atten_encoder[k][j][2], kernel_size=2, stride=2)\n            else:\n                atten_encoder[k][j][0] = self.encoder_att[k][j](torch.cat((g_encoder[j], atten_encoder[k][j - 1][2]), dim=1))\n                atten_encoder[k][j][1] = (atten_encoder[k][j][0]) * g_encoder[j]\n                atten_encoder[k][j][2] = self.encoder_block_att[j](atten_encoder[k][j][1])\n                if j < 3:\n                    atten_encoder[k][j][2] = F.max_pool2d(atten_encoder[k][j][2], kernel_size=2, stride=2)\n\n        pred = F.avg_pool2d(atten_encoder[k][-1][-1], 8)\n        pred = pred.view(pred.size(0), -1)\n\n        out = self.linear[k](pred)\n        return out\n\n    def model_fit(self, x_pred, x_output, num_output):\n        # convert a single label into a one-hot vector\n        x_output_onehot = torch.zeros((len(x_output), num_output)).to(device)\n        x_output_onehot.scatter_(1, x_output.unsqueeze(1), 1)\n\n        # apply cross-entropy loss\n        loss = x_output_onehot * torch.log(x_pred + 1e-20)\n        return torch.sum(-loss, dim=1)\n\n\ndef data_transform(data_path, name, train=True):\n    with open(data_path + \'decathlon_mean_std.pickle\', \'rb\') as handle:\n        dict_mean_std = pickle._Unpickler(handle)\n        dict_mean_std.encoding = \'latin1\'\n        dict_mean_std = dict_mean_std.load()\n\n    means = dict_mean_std[name + \'mean\']\n    stds = dict_mean_std[name + \'std\']\n\n    if name in [\'gtsrb\', \'omniglot\', \'svhn\']:  # no horz flip\n        transform_train = transforms.Compose([\n            transforms.Resize(72),\n            transforms.CenterCrop(72),\n            transforms.ToTensor(),\n            transforms.Normalize(means, stds),\n        ])\n    else:\n        transform_train = transforms.Compose([\n            transforms.Resize(72),\n            transforms.RandomCrop(72),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize(means, stds),\n        ])\n    if name in [\'gtsrb\', \'omniglot\', \'svhn\']:  # no horz flip\n        transform_test = transforms.Compose([\n            transforms.Resize(72),\n            transforms.CenterCrop(72),\n            transforms.ToTensor(),\n            transforms.Normalize(means, stds),\n        ])\n    else:\n        transform_test = transforms.Compose([\n            transforms.Resize(72),\n            transforms.CenterCrop(72),\n            transforms.ToTensor(),\n            transforms.Normalize(means, stds),\n        ])\n    if train:\n        return transform_train\n    else:\n        return transform_test\n\n\nim_train_set = [0] * 10\nim_test_set = [0] * 10\nim_val_set = [0] * 10\ndata_path = \'decathlon-1.0-data/\'\ndata_name = [\'imagenet12\', \'aircraft\', \'cifar100\', \'daimlerpedcls\', \'dtd\',\n             \'gtsrb\', \'omniglot\', \'svhn\', \'ucf101\', \'vgg-flowers\']\ndata_class = [1000, 100, 100, 2, 47, 43, 1623, 10, 101, 102]\nfor i in range(10):\n    im_train_set[i] = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(data_path + data_name[i] + \'/train\',\n                                                  transform=data_transform(data_path, data_name[i])),\n                                                  batch_size=128,\n                                                  shuffle=True,\n                                                  num_workers=4, pin_memory=True)\n    im_val_set[i] = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(data_path + data_name[i] + \'/val\',\n                                                transform=data_transform(data_path,data_name[i])),\n                                                batch_size=128,\n                                                shuffle=True,\n                                                num_workers=4, pin_memory=True)\n    im_test_set[i] = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(data_path + data_name[i] + \'/test\',\n                                                 transform=data_transform(data_path, data_name[i], train=False)),\n                                                 batch_size=100,\n                                                 shuffle=False)\n\n# define WRN model\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\nWideResNet_MTAN = WideResNet(depth=28, widen_factor=4, num_classes=data_class).to(device)\nif opt.dataset == \'imagenet\':\n    optimizer = optim.SGD(WideResNet_MTAN.parameters(), lr=0.1 * (0.5 ** 6), weight_decay=5e-5, nesterov=True, momentum=0.9)\n    WideResNet_MTAN.load_state_dict(torch.load(\'model_weights/imagenet\'))\n    start_index = 0\n    end_index = 1\nif opt.dataset == \'notimagenet\':\n    optimizer = optim.SGD(WideResNet_MTAN.parameters(), lr=0.01 * (0.5 ** 2), weight_decay=5e-5, nesterov=True, momentum=0.9)\n    WideResNet_MTAN.load_state_dict(torch.load(\'model_weights/wrn_final\'))\n    start_index = 1\n    end_index = 10\n\navg_cost = np.zeros([10, 4], dtype=np.float32)\nans = {}\nfor k in range(start_index, end_index):\n    WideResNet_MTAN.train()\n    cost = np.zeros(2, dtype=np.float32)\n    train_dataset = iter(im_train_set[k])\n    train_batch = len(train_dataset)\n    # We train the same training and validation dataset for another epoch to compute the dataset specific BN statistics\n    # in shared layers before evaluating them on test dataset.\n    for i in range(train_batch):\n        train_data, train_label = train_dataset.next()\n        train_label = train_label.type(torch.LongTensor)\n        train_data, train_label = train_data.to(device), train_label.to(device)\n        train_pred1 = WideResNet_MTAN(train_data, k)\n\n        # reset optimizer with zero gradient\n        optimizer.zero_grad()\n        train_loss1 = WideResNet_MTAN.model_fit(train_pred1, train_label, num_output=data_class[k])\n        train_loss = torch.mean(train_loss1)\n        train_loss.backward()\n        optimizer.step()\n\n        # calculate training loss and accuracy\n        train_predict_label1 = train_pred1.data.max(1)[1]\n        train_acc1 = train_predict_label1.eq(train_label).sum().item() / train_data.shape[0]\n\n        cost[0] = torch.mean(train_loss1).item()\n        cost[1] = train_acc1\n        avg_cost[k][0:2] += cost / train_batch\n\n    train_dataset = iter(im_val_set[k])\n    train_batch = len(train_dataset)\n    for i in range(train_batch):\n        train_data, train_label = train_dataset.next()\n        train_label = train_label.type(torch.LongTensor)\n        train_data, train_label = train_data.to(device), train_label.to(device)\n        train_pred1 = WideResNet_MTAN(train_data, k)\n\n        # reset optimizer with zero gradient\n        optimizer.zero_grad()\n        train_loss1 = WideResNet_MTAN.model_fit(train_pred1, train_label, num_output=data_class[k])\n        train_loss = torch.mean(train_loss1)\n        train_loss.backward()\n        optimizer.step()\n\n        # calculate training loss and accuracy\n        train_predict_label1 = train_pred1.data.max(1)[1]\n        train_acc1 = train_predict_label1.eq(train_label).sum().item() / train_data.shape[0]\n\n        cost[0] = torch.mean(train_loss1).item()\n        cost[1] = train_acc1\n        avg_cost[k][2:4] += cost / train_batch\n\n    # evaluating test data\n    with torch.no_grad():\n        WideResNet_MTAN.eval()\n        test_dataset = iter(im_test_set[k])\n        test_batch = len(test_dataset)\n        test_label = []\n        for i in range(test_batch):\n            test_data, _ = test_dataset.next()\n            test_data = test_data.to(device)\n            test_pred1 = WideResNet_MTAN(test_data, k)\n\n            # calculate testing loss and accuracy\n            test_predict = test_pred1.data.max(1)[1]\n            test_pred = test_predict.cpu().numpy()\n            test_label.extend(test_pred)\n        ans[data_name[k]] = test_label\n\n        print(\'DATASET: {:s} || TRAIN {:.4f} {:.4f} | TEST {:.4f} {:.4f}\'\n              .format(data_name[k], avg_cost[k][0], avg_cost[k][1], avg_cost[k][2], avg_cost[k][3]))\n        print(\'Evaluating DATASET: {:s} ...\'.format(data_name[k]))\n\nif opt.dataset == \'notimagenet\':\n    pickle_out = open(""ans.pickle"", ""wb"")\n    pickle.dump(ans, pickle_out)\n    pickle_out.close()\nelif opt.dataset == \'imagenet\':\n    pickle_out = open(""imagenet.pickle"", ""wb"")\n    pickle.dump(ans, pickle_out)\n    pickle_out.close()\n\n'"
visual_decathlon/model_wrn_mtan.py,23,"b'import torch\nimport torch.nn as nn\nimport argparse\n\nimport torch.nn.init as init\nimport torch.nn.functional as F\nimport torchvision\nimport numpy as np\nimport torch.optim as optim\nimport pickle\n\nfrom torchvision.transforms import transforms\n\nparser = argparse.ArgumentParser(description=\'Multi-task: Attention Network on WRN\')\nparser.add_argument(\'--mode\', default=\'eval\', type=str,\n                    help=\'eval: do not train on eval datasests; all: train on eval datasets\')\nparser.add_argument(\'--gpu\', default=0, type=int, help=\'GPU id\')\nopt = parser.parse_args()\n\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=True)\n\ndef conv_init(m):\n    classname = m.__class__.__name__\n    if classname.find(\'Conv\') != -1:\n        init.xavier_uniform(m.weight, gain=np.sqrt(2))\n        init.constant(m.bias, 0)\n    elif classname.find(\'BatchNorm\') != -1:\n        init.constant(m.weight, 1)\n        init.constant(m.bias, 0)\n\n\nclass wide_basic(nn.Module):\n    def __init__(self, in_planes, planes, stride=1):\n        super(wide_basic, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, padding=1, bias=True)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=True)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=True),\n            )\n\n    def forward(self, x):\n        out = self.conv1(F.relu(self.bn1(x)))\n        out = self.conv2(F.relu(self.bn2(out)))\n        out += self.shortcut(x)\n\n        return out\n\n\nclass WideResNet(nn.Module):\n    def __init__(self, depth, widen_factor, num_classes):\n        super(WideResNet, self).__init__()\n        self.in_planes = 16\n        n = int((depth - 4) / 6)\n        k = widen_factor\n        filter = [16, 16 * k, 32 * k, 64 * k]\n\n        self.conv1 = conv3x3(3, filter[0], stride=1)\n        self.layer1 = self._wide_layer(wide_basic, filter[1], n, stride=2)\n        self.layer2 = self._wide_layer(wide_basic, filter[2], n, stride=2)\n        self.layer3 = self._wide_layer(wide_basic, filter[3], n, stride=2)\n        self.bn1 = nn.BatchNorm2d(filter[3], momentum=0.9)\n\n        self.linear = nn.ModuleList([nn.Sequential(\n            nn.Linear(filter[3], num_classes[0]),\n            nn.Softmax(dim=1))])\n\n        # attention modules\n        self.encoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])])])\n        self.encoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[1]])])\n\n        for j in range(10):\n            if j < 9:\n                self.encoder_att.append(nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])]))\n                self.linear.append(nn.Sequential(nn.Linear(filter[3], num_classes[j + 1]),\n                                                 nn.Softmax(dim=1)))\n            for i in range(3):\n                self.encoder_att[j].append(self.att_layer([2 * filter[i + 1], filter[i + 1], filter[i + 1]]))\n\n        for i in range(3):\n            if i < 2:\n                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 2]]))\n            else:\n                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\n    def conv_layer(self, channel):\n        conv_block = nn.Sequential(\n            nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n            nn.BatchNorm2d(num_features=channel[1]),\n            nn.ReLU(inplace=True),\n        )\n        return conv_block\n\n    def att_layer(self, channel):\n        att_block = nn.Sequential(\n            nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n            nn.BatchNorm2d(channel[1]),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=1, padding=0),\n            nn.BatchNorm2d(channel[2]),\n            nn.Sigmoid(),\n        )\n        return att_block\n\n    def _wide_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x, k):\n        g_encoder = [0] * 4\n\n        atten_encoder = [0] * 10\n        for i in range(10):\n            atten_encoder[i] = [0] * 4\n        for i in range(10):\n            for j in range(4):\n                atten_encoder[i][j] = [0] * 3\n\n        # shared encoder\n        g_encoder[0] = self.conv1(x)\n        g_encoder[1] = self.layer1(g_encoder[0])\n        g_encoder[2] = self.layer2(g_encoder[1])\n        g_encoder[3] = F.relu(self.bn1(self.layer3(g_encoder[2])))\n\n        # apply attention modules\n        for j in range(4):\n            if j == 0:\n                atten_encoder[k][j][0] = self.encoder_att[k][j](g_encoder[0])\n                atten_encoder[k][j][1] = (atten_encoder[k][j][0]) * g_encoder[0]\n                atten_encoder[k][j][2] = self.encoder_block_att[j](atten_encoder[k][j][1])\n                atten_encoder[k][j][2] = F.max_pool2d(atten_encoder[k][j][2], kernel_size=2, stride=2)\n            else:\n                atten_encoder[k][j][0] = self.encoder_att[k][j](torch.cat((g_encoder[j], atten_encoder[k][j - 1][2]), dim=1))\n                atten_encoder[k][j][1] = (atten_encoder[k][j][0]) * g_encoder[j]\n                atten_encoder[k][j][2] = self.encoder_block_att[j](atten_encoder[k][j][1])\n                if j < 3:\n                    atten_encoder[k][j][2] = F.max_pool2d(atten_encoder[k][j][2], kernel_size=2, stride=2)\n\n        pred = F.avg_pool2d(atten_encoder[k][-1][-1], 8)\n        pred = pred.view(pred.size(0), -1)\n\n        out = self.linear[k](pred)\n        return out\n\n    def model_fit(self, x_pred, x_output, num_output):\n        # convert a single label into a one-hot vector\n        x_output_onehot = torch.zeros((len(x_output), num_output)).to(device)\n        x_output_onehot.scatter_(1, x_output.unsqueeze(1), 1)\n\n        # apply cross-entropy loss\n        loss = x_output_onehot * torch.log(x_pred + 1e-20)\n        return torch.sum(-loss, dim=1)\n\n\n# define data transformation\ndef data_transform(data_path, name, train=True):\n    with open(data_path + \'decathlon_mean_std.pickle\', \'rb\') as handle:\n        dict_mean_std = pickle._Unpickler(handle)\n        dict_mean_std.encoding = \'latin1\'\n        dict_mean_std = dict_mean_std.load()\n\n    means = dict_mean_std[name + \'mean\']\n    stds = dict_mean_std[name + \'std\']\n\n    if name in [\'gtsrb\', \'omniglot\', \'svhn\']:  # no horz flip\n        transform_train = transforms.Compose([\n            transforms.Resize(72),\n            transforms.CenterCrop(72),\n            transforms.ToTensor(),\n            transforms.Normalize(means, stds),\n        ])\n    else:\n        transform_train = transforms.Compose([\n            transforms.Resize(72),\n            transforms.RandomCrop(72),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize(means, stds),\n        ])\n    if name in [\'gtsrb\', \'omniglot\', \'svhn\']:  # no horz flip\n        transform_test = transforms.Compose([\n            transforms.Resize(72),\n            transforms.CenterCrop(72),\n            transforms.ToTensor(),\n            transforms.Normalize(means, stds),\n        ])\n    else:\n        transform_test = transforms.Compose([\n            transforms.Resize(72),\n            transforms.CenterCrop(72),\n            transforms.ToTensor(),\n            transforms.Normalize(means, stds),\n        ])\n    if train:\n        return transform_train\n    else:\n        return transform_test\n\nim_train_set = [0] * 10\nim_test_set = [0] * 10\ndata_path = \'decathlon-1.0-data/\'\ndata_name = [\'imagenet12\', \'aircraft\', \'cifar100\', \'daimlerpedcls\', \'dtd\',\n             \'gtsrb\', \'omniglot\', \'svhn\', \'ucf101\', \'vgg-flowers\']\ndata_class = [1000, 100, 100, 2, 47, 43, 1623, 10, 101, 102]\nfor i in range(10):\n    im_train_set[i] = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(data_path + data_name[i] + \'/train\',\n                                                  transform=data_transform(data_path,data_name[i])),\n                                                  batch_size=128,\n                                                  shuffle=True,\n                                                  num_workers=4, pin_memory=True)\n    im_test_set[i] = torch.utils.data.DataLoader(torchvision.datasets.ImageFolder(data_path + data_name[i] + \'/val\',\n                                                 transform=data_transform(data_path,data_name[i], train=True)),\n                                                 batch_size=128,\n                                                 shuffle=True,\n                                                 num_workers=4, pin_memory=True)\n\n# define WRN model\ndevice = torch.device(""cuda:{}"".format(opt.gpu) if torch.cuda.is_available() else ""cpu"")\nWideResNet_MTAN = WideResNet(depth=28, widen_factor=4, num_classes=data_class).to(device)\noptimizer = optim.SGD(WideResNet_MTAN.parameters(), lr=0.1, weight_decay=5e-5, nesterov=True, momentum=0.9)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n\n# define parameters and running for 200 epochs\ntotal_epoch = 400\nfirst_run = True\navg_cost = np.zeros([total_epoch, 10, 4], dtype=np.float32)\nfor index in range(total_epoch):\n    # evaluate training data\n    if index < 300:\n        start_index = 0\n    else:\n        start_index = 1\n        if first_run:\n            optimizer = optim.SGD(WideResNet_MTAN.parameters(), lr=0.01, weight_decay=5e-5, nesterov=True, momentum=0.9)\n            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n            first_run = False\n    for k in range(start_index, 10):\n        cost = np.zeros(2, dtype=np.float32)\n        train_dataset = iter(im_train_set[k])\n        train_batch = len(train_dataset)\n        WideResNet_MTAN.train()\n        for i in range(train_batch):\n            train_data, train_label = train_dataset.next()\n            train_label = train_label.type(torch.LongTensor)\n            train_data, train_label = train_data.to(device), train_label.to(device)\n            train_pred1 = WideResNet_MTAN(train_data, k)\n\n            # reset optimizer with zero gradient\n            optimizer.zero_grad()\n            train_loss1 = WideResNet_MTAN.model_fit(train_pred1, train_label, num_output=data_class[k])\n            train_loss = torch.mean(train_loss1)\n            train_loss.backward()\n            optimizer.step()\n\n            # calculate training loss and accuracy\n            train_predict_label1 = train_pred1.data.max(1)[1]\n            train_acc1 = train_predict_label1.eq(train_label).sum().item() / train_data.shape[0]\n\n            cost[0] = torch.mean(train_loss1).item()\n            cost[1] = train_acc1\n            avg_cost[index][k][0:2] += cost / train_batch\n\n        # evaluating test data\n        test_dataset = iter(im_test_set[k])\n        test_batch = len(test_dataset)\n        if opt.mode == \'all\':\n            for i in range(test_batch):\n                test_data, test_label = test_dataset.next()\n                test_label = test_label.type(torch.LongTensor)\n                test_data, test_label = test_data.to(device), test_label.to(device)\n                test_pred1 = WideResNet_MTAN(test_data, k)\n\n                optimizer.zero_grad()\n                test_loss1 = WideResNet_MTAN.model_fit(test_pred1, test_label, num_output=data_class[k])\n                test_loss = torch.mean(test_loss1)\n                test_loss.backward()\n                optimizer.step()\n\n                # calculate testing loss and accuracy\n                test_predict_label1 = test_pred1.data.max(1)[1]\n                test_acc1 = test_predict_label1.eq(test_label).sum().item() / test_data.shape[0]\n\n                cost[0] = torch.mean(test_loss1).item()\n                cost[1] = test_acc1\n                avg_cost[index][k][2:] += cost / test_batch\n        elif opt.mode == \'eval\':\n            WideResNet_MTAN.eval()\n            with torch.no_grad():\n                for i in range(test_batch):\n                    test_data, test_label = test_dataset.next()\n                    test_label = test_label.type(torch.LongTensor)\n                    test_data, test_label = test_data.to(device), test_label.to(device)\n\n                    test_pred1 = WideResNet_MTAN(test_data, k)\n                    test_loss1 = WideResNet_MTAN.model_fit(test_pred1, test_label, num_output=data_class[k])\n                    test_loss = torch.mean(test_loss1)\n\n                    # calculate testing loss and accuracy\n                    test_predict_label1 = test_pred1.data.max(1)[1]\n                    test_acc1 = test_predict_label1.eq(test_label).sum().item() / test_data.shape[0]\n\n                    cost[0] = torch.mean(test_loss1).item()\n                    cost[1] = test_acc1\n                    avg_cost[index][k][2:] += cost / test_batch\n        print(\'EPOCH: {:04d} | DATASET: {:s} || TRAIN: {:.4f} {:.4f} || TEST: {:.4f} {:.4f}\'\n              .format(index, data_name[k], avg_cost[index][k][0], avg_cost[index][k][1], avg_cost[index][k][2], avg_cost[index][k][3]))\n    print(\'===================================================\')\n    scheduler.step()\n    if (index + 1) % 5 == 0 and index <= 300:\n        torch.save(WideResNet_MTAN.state_dict(), \'model_weights/imagenet\')\n    if (index + 1) % 5 == 0 and index > 300:\n        torch.save(WideResNet_MTAN.state_dict(), \'model_weights/wrn_final\')\n\n\n'"
im2im_pred/model_resnet_mtan/aspp.py,3,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass DeepLabHead(nn.Sequential):\n    def __init__(self, in_channels, num_classes):\n        super(DeepLabHead, self).__init__(\n            ASPP(in_channels, [12, 24, 36]),\n            nn.Conv2d(256, 256, 3, padding=1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.Conv2d(256, num_classes, 1)\n        )\n\n\nclass ASPPConv(nn.Sequential):\n    def __init__(self, in_channels, out_channels, dilation):\n        modules = [\n            nn.Conv2d(in_channels, out_channels, 3, padding=dilation, dilation=dilation, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU()\n        ]\n        super(ASPPConv, self).__init__(*modules)\n\n\nclass ASPPPooling(nn.Sequential):\n    def __init__(self, in_channels, out_channels):\n        super(ASPPPooling, self).__init__(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU())\n\n    def forward(self, x):\n        size = x.shape[-2:]\n        x = super(ASPPPooling, self).forward(x)\n        return F.interpolate(x, size=size, mode='bilinear', align_corners=False)\n\n\nclass ASPP(nn.Module):\n    def __init__(self, in_channels, atrous_rates):\n        super(ASPP, self).__init__()\n        out_channels = 256\n        modules = []\n        modules.append(nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU()))\n\n        rate1, rate2, rate3 = tuple(atrous_rates)\n        modules.append(ASPPConv(in_channels, out_channels, rate1))\n        modules.append(ASPPConv(in_channels, out_channels, rate2))\n        modules.append(ASPPConv(in_channels, out_channels, rate3))\n        modules.append(ASPPPooling(in_channels, out_channels))\n\n        self.convs = nn.ModuleList(modules)\n\n        self.project = nn.Sequential(\n            nn.Conv2d(5 * out_channels, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n            nn.Dropout(0.5))\n\n    def forward(self, x):\n        res = []\n        for conv in self.convs:\n            res.append(conv(x))\n        res = torch.cat(res, dim=1)\n        return self.project(res)\n"""
im2im_pred/model_resnet_mtan/resnet.py,11,"b'import torch\nimport torch.nn as nn\nfrom torchvision.models.utils import load_state_dict_from_url\n\n\n__all__ = [\'ResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\',\n           \'resnet152\', \'resnext50_32x4d\', \'resnext101_32x8d\',\n           \'wide_resnet50_2\', \'wide_resnet101_2\']\n\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n    \'resnext50_32x4d\': \'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\',\n    \'resnext101_32x8d\': \'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth\',\n    \'wide_resnet50_2\': \'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth\',\n    \'wide_resnet101_2\': \'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    """"""1x1 convolution""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n    __constants__ = [\'downsample\']\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(BasicBlock, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        if groups != 1 or base_width != 64:\n            raise ValueError(\'BasicBlock only supports groups=1 and base_width=64\')\n        if dilation > 1:\n            raise NotImplementedError(""Dilation > 1 not supported in BasicBlock"")\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n    __constants__ = [\'downsample\']\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(Bottleneck, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.)) * groups\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, width)\n        self.bn1 = norm_layer(width)\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n        self.bn2 = norm_layer(width)\n        self.conv3 = conv1x1(width, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n                 norm_layer=None):\n        super(ResNet, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        self._norm_layer = norm_layer\n\n        self.inplanes = 64\n        self.dilation = 1\n        if replace_stride_with_dilation is None:\n            # each element in the tuple indicates if we should replace\n            # the 2x2 stride with a dilated convolution instead\n            replace_stride_with_dilation = [False, False, False]\n        if len(replace_stride_with_dilation) != 3:\n            raise ValueError(""replace_stride_with_dilation should be None ""\n                             ""or a 3-element tuple, got {}"".format(replace_stride_with_dilation))\n        self.groups = groups\n        self.base_width = width_per_group\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n                                       dilate=replace_stride_with_dilation[0])\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n                                       dilate=replace_stride_with_dilation[1])\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n                                       dilate=replace_stride_with_dilation[2])\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n        norm_layer = self._norm_layer\n        downsample = None\n        previous_dilation = self.dilation\n        if dilate:\n            self.dilation *= stride\n            stride = 1\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n                            self.base_width, previous_dilation, norm_layer))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups=self.groups,\n                                base_width=self.base_width, dilation=self.dilation,\n                                norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        # We remove the original classifier, to attach task-specific decoders.\n        #x = self.avgpool(x)\n        #x = torch.flatten(x, 1)\n        #x = self.fc(x)\n        return x\n\n\ndef _resnet(arch, block, layers, pretrained, progress, **kwargs):\n    model = ResNet(block, layers, **kwargs)\n    if pretrained:\n        state_dict = load_state_dict_from_url(model_urls[arch],\n                                              progress=progress)\n        model.load_state_dict(state_dict)\n    return model\n\n\ndef resnet18(pretrained=False, progress=True, **kwargs):\n    r""""""ResNet-18 model from\n    `""Deep Residual Learning for Image Recognition"" <https://arxiv.org/pdf/1512.03385.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    return _resnet(\'resnet18\', BasicBlock, [2, 2, 2, 2], pretrained, progress,\n                   **kwargs)\n\n\ndef resnet34(pretrained=False, progress=True, **kwargs):\n    r""""""ResNet-34 model from\n    `""Deep Residual Learning for Image Recognition"" <https://arxiv.org/pdf/1512.03385.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    return _resnet(\'resnet34\', BasicBlock, [3, 4, 6, 3], pretrained, progress,\n                   **kwargs)\n\n\ndef resnet50(pretrained=False, progress=True, **kwargs):\n    r""""""ResNet-50 model from\n    `""Deep Residual Learning for Image Recognition"" <https://arxiv.org/pdf/1512.03385.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    return _resnet(\'resnet50\', Bottleneck, [3, 4, 6, 3], pretrained, progress,\n                   **kwargs)\n\n\ndef resnet101(pretrained=False, progress=True, **kwargs):\n    r""""""ResNet-101 model from\n    `""Deep Residual Learning for Image Recognition"" <https://arxiv.org/pdf/1512.03385.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    return _resnet(\'resnet101\', Bottleneck, [3, 4, 23, 3], pretrained, progress,\n                   **kwargs)\n\n\ndef resnet152(pretrained=False, progress=True, **kwargs):\n    r""""""ResNet-152 model from\n    `""Deep Residual Learning for Image Recognition"" <https://arxiv.org/pdf/1512.03385.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    return _resnet(\'resnet152\', Bottleneck, [3, 8, 36, 3], pretrained, progress,\n                   **kwargs)\n\n\ndef resnext50_32x4d(pretrained=False, progress=True, **kwargs):\n    r""""""ResNeXt-50 32x4d model from\n    `""Aggregated Residual Transformation for Deep Neural Networks"" <https://arxiv.org/pdf/1611.05431.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    kwargs[\'groups\'] = 32\n    kwargs[\'width_per_group\'] = 4\n    return _resnet(\'resnext50_32x4d\', Bottleneck, [3, 4, 6, 3],\n                   pretrained, progress, **kwargs)\n\n\ndef resnext101_32x8d(pretrained=False, progress=True, **kwargs):\n    r""""""ResNeXt-101 32x8d model from\n    `""Aggregated Residual Transformation for Deep Neural Networks"" <https://arxiv.org/pdf/1611.05431.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    kwargs[\'groups\'] = 32\n    kwargs[\'width_per_group\'] = 8\n    return _resnet(\'resnext101_32x8d\', Bottleneck, [3, 4, 23, 3],\n                   pretrained, progress, **kwargs)\n\n\ndef wide_resnet50_2(pretrained=False, progress=True, **kwargs):\n    r""""""Wide ResNet-50-2 model from\n    `""Wide Residual Networks"" <https://arxiv.org/pdf/1605.07146.pdf>`_\n    The model is the same as ResNet except for the bottleneck number of channels\n    which is twice larger in every block. The number of channels in outer 1x1\n    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    kwargs[\'width_per_group\'] = 64 * 2\n    return _resnet(\'wide_resnet50_2\', Bottleneck, [3, 4, 6, 3],\n                   pretrained, progress, **kwargs)\n\n\ndef wide_resnet101_2(pretrained=False, progress=True, **kwargs):\n    r""""""Wide ResNet-101-2 model from\n    `""Wide Residual Networks"" <https://arxiv.org/pdf/1605.07146.pdf>`_\n    The model is the same as ResNet except for the bottleneck number of channels\n    which is twice larger in every block. The number of channels in outer 1x1\n    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    kwargs[\'width_per_group\'] = 64 * 2\n    return _resnet(\'wide_resnet101_2\', Bottleneck, [3, 4, 23, 3],\n                   pretrained, progress, **kwargs)\n'"
im2im_pred/model_resnet_mtan/resnet_dilated.py,1,"b""import torch.nn as nn\n\n\nclass ResnetDilated(nn.Module):\n    def __init__(self, orig_resnet, dilate_scale=8):\n        super(ResnetDilated, self).__init__()\n        from functools import partial\n\n        if dilate_scale == 8:\n            orig_resnet.layer3.apply(partial(self._nostride_dilate, dilate=2))\n            orig_resnet.layer4.apply(partial(self._nostride_dilate, dilate=4))\n        elif dilate_scale == 16:\n            orig_resnet.layer4.apply(partial(self._nostride_dilate, dilate=2))\n\n        # take pre-defined ResNet, except AvgPool and FC\n        self.conv1 = orig_resnet.conv1\n        self.bn1 = orig_resnet.bn1\n        self.relu1 = orig_resnet.relu\n        \n        self.maxpool = orig_resnet.maxpool\n        self.layer1 = orig_resnet.layer1\n        self.layer2 = orig_resnet.layer2\n        self.layer3 = orig_resnet.layer3\n        self.layer4 = orig_resnet.layer4\n\n    def _nostride_dilate(self, m, dilate):\n        classname = m.__class__.__name__\n        if classname.find('Conv') != -1:\n            # the convolution with stride\n            if m.stride == (2, 2):\n                m.stride = (1, 1)\n                if m.kernel_size == (3, 3):\n                    m.dilation = (dilate//2, dilate//2)\n                    m.padding = (dilate//2, dilate//2)\n            # other convoluions\n            else:\n                if m.kernel_size == (3, 3):\n                    m.dilation = (dilate, dilate)\n                    m.padding = (dilate, dilate)\n\n    def forward(self, x):\n        x = self.relu1(self.bn1(self.conv1(x)))\n        x = self.maxpool(x)\n\n        x = self.layer1(x) \n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n"""
im2im_pred/model_resnet_mtan/resnet_mtan.py,4,"b""import torch\nimport torch.nn as nn\nimport resnet\n\nfrom resnet_dilated import ResnetDilated\nfrom aspp import DeepLabHead\n\n\nclass MTANDeepLabv3(nn.Module):\n    def __init__(self):\n        super(MTANDeepLabv3, self).__init__()\n        backbone = ResnetDilated(resnet.__dict__['resnet50'](pretrained=True))\n        ch = [256, 512, 1024, 2048]\n        \n        self.tasks = ['segmentation', 'depth'] \n        self.num_out_channels = {'segmentation': 13, 'depth': 1}\n        \n        self.shared_conv = nn.Sequential(backbone.conv1, backbone.bn1, backbone.relu1, backbone.maxpool)\n\n        # We will apply the attention over the last bottleneck layer in the ResNet. \n        self.shared_layer1_b = backbone.layer1[:-1] \n        self.shared_layer1_t = backbone.layer1[-1]\n\n        self.shared_layer2_b = backbone.layer2[:-1]\n        self.shared_layer2_t = backbone.layer2[-1]\n\n        self.shared_layer3_b = backbone.layer3[:-1]\n        self.shared_layer3_t = backbone.layer3[-1]\n\n        self.shared_layer4_b = backbone.layer4[:-1]\n        self.shared_layer4_t = backbone.layer4[-1]\n\n        # Define task specific attention modules using a similar bottleneck design in residual block\n        # (to avoid large computations)\n        self.encoder_att_1 = nn.ModuleList([self.att_layer(ch[0], ch[0] // 4, ch[0]) for _ in self.tasks])\n        self.encoder_att_2 = nn.ModuleList([self.att_layer(2 * ch[1], ch[1] // 4, ch[1]) for _ in self.tasks])\n        self.encoder_att_3 = nn.ModuleList([self.att_layer(2 * ch[2], ch[2] // 4, ch[2]) for _ in self.tasks])\n        self.encoder_att_4 = nn.ModuleList([self.att_layer(2 * ch[3], ch[3] // 4, ch[3]) for _ in self.tasks])\n\n        # Define task shared attention encoders using residual bottleneck layers\n        # We do not apply shared attention encoders at the last layer,\n        # so the attended features will be directly fed into the task-specific decoders.\n        self.encoder_block_att_1 = self.conv_layer(ch[0], ch[1] // 4)\n        self.encoder_block_att_2 = self.conv_layer(ch[1], ch[2] // 4)\n        self.encoder_block_att_3 = self.conv_layer(ch[2], ch[3] // 4)\n        \n        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        # Define task-specific decoders using ASPP modules\n        self.decoders = nn.ModuleList([DeepLabHead(2048, self.num_out_channels[t]) for t in self.tasks])\n        \n    def forward(self, x, out_size):\n        # Shared convolution\n        x = self.shared_conv(x)\n        \n        # Shared ResNet block 1\n        u_1_b = self.shared_layer1_b(x)\n        u_1_t = self.shared_layer1_t(u_1_b)\n\n        # Shared ResNet block 2\n        u_2_b = self.shared_layer2_b(u_1_t)\n        u_2_t = self.shared_layer2_t(u_2_b)\n\n        # Shared ResNet block 3\n        u_3_b = self.shared_layer3_b(u_2_t)\n        u_3_t = self.shared_layer3_t(u_3_b)\n        \n        # Shared ResNet block 4\n        u_4_b = self.shared_layer4_b(u_3_t)\n        u_4_t = self.shared_layer4_t(u_4_b)\n\n        # Attention block 1 -> Apply attention over last residual block\n        a_1_mask = [att_i(u_1_b) for att_i in self.encoder_att_1]  # Generate task specific attention map\n        a_1 = [a_1_mask_i * u_1_t for a_1_mask_i in a_1_mask]  # Apply task specific attention map to shared features\n        a_1 = [self.down_sampling(self.encoder_block_att_1(a_1_i)) for a_1_i in a_1]\n        \n        # Attention block 2 -> Apply attention over last residual block\n        a_2_mask = [att_i(torch.cat((u_2_b, a_1_i), dim=1)) for a_1_i, att_i in zip(a_1, self.encoder_att_2)]\n        a_2 = [a_2_mask_i * u_2_t for a_2_mask_i in a_2_mask]\n        a_2 = [self.encoder_block_att_2(a_2_i) for a_2_i in a_2]\n        \n        # Attention block 3 -> Apply attention over last residual block\n        a_3_mask = [att_i(torch.cat((u_3_b, a_2_i), dim=1)) for a_2_i, att_i in zip(a_2, self.encoder_att_3)]\n        a_3 = [a_3_mask_i * u_3_t for a_3_mask_i in a_3_mask]\n        a_3 = [self.encoder_block_att_3(a_3_i) for a_3_i in a_3]\n        \n        # Attention block 4 -> Apply attention over last residual block (without final encoder)\n        a_4_mask = [att_i(torch.cat((u_4_b, a_3_i), dim=1)) for a_3_i, att_i in zip(a_3, self.encoder_att_4)]\n        a_4 = [a_4_mask_i * u_4_t for a_4_mask_i in a_4_mask]\n        \n        # Task specific decoders\n        out = {}\n        for i, t in enumerate(self.tasks):\n            out[t] = nn.functional.interpolate(self.decoders[i](a_4[i]), size=out_size, mode='bilinear').squeeze()\n        return out\n    \n    def att_layer(self, in_channel, intermediate_channel, out_channel):\n        return nn.Sequential(\n            nn.Conv2d(in_channels=in_channel, out_channels=intermediate_channel, kernel_size=1, padding=0),\n            nn.BatchNorm2d(intermediate_channel),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(in_channels=intermediate_channel, out_channels=out_channel, kernel_size=1, padding=0),\n            nn.BatchNorm2d(out_channel),\n            nn.Sigmoid())\n        \n    def conv_layer(self, in_channel, out_channel):\n        from resnet import Bottleneck, conv1x1\n        downsample = nn.Sequential(conv1x1(in_channel, 4 * out_channel, stride=1),\n                                   nn.BatchNorm2d(4 * out_channel))\n        return Bottleneck(in_channel, out_channel, downsample=downsample)\n"""
