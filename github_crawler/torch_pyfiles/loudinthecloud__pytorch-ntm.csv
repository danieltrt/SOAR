file_path,api_count,code
train.py,6,"b'#!/usr/bin/env python\n# PYTHON_ARGCOMPLETE_OK\n""""""Training for the Copy Task in Neural Turing Machines.""""""\n\nimport argparse\nimport json\nimport logging\nimport time\nimport random\nimport re\nimport sys\n\nimport attr\nimport argcomplete\nimport torch\nimport numpy as np\n\n\nLOGGER = logging.getLogger(__name__)\n\n\nfrom tasks.copytask import CopyTaskModelTraining, CopyTaskParams\nfrom tasks.repeatcopytask import RepeatCopyTaskModelTraining, RepeatCopyTaskParams\n\nTASKS = {\n    \'copy\': (CopyTaskModelTraining, CopyTaskParams),\n    \'repeat-copy\': (RepeatCopyTaskModelTraining, RepeatCopyTaskParams)\n}\n\n\n# Default values for program arguments\nRANDOM_SEED = 1000\nREPORT_INTERVAL = 200\nCHECKPOINT_INTERVAL = 1000\n\n\ndef get_ms():\n    """"""Returns the current time in miliseconds.""""""\n    return time.time() * 1000\n\n\ndef init_seed(seed=None):\n    """"""Seed the RNGs for predicatability/reproduction purposes.""""""\n    if seed is None:\n        seed = int(get_ms() // 1000)\n\n    LOGGER.info(""Using seed=%d"", seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    random.seed(seed)\n\n\ndef progress_clean():\n    """"""Clean the progress bar.""""""\n    print(""\\r{}"".format("" "" * 80), end=\'\\r\')\n\n\ndef progress_bar(batch_num, report_interval, last_loss):\n    """"""Prints the progress until the next report.""""""\n    progress = (((batch_num-1) % report_interval) + 1) / report_interval\n    fill = int(progress * 40)\n    print(""\\r[{}{}]: {} (Loss: {:.4f})"".format(\n        ""="" * fill, "" "" * (40 - fill), batch_num, last_loss), end=\'\')\n\n\ndef save_checkpoint(net, name, args, batch_num, losses, costs, seq_lengths):\n    progress_clean()\n\n    basename = ""{}/{}-{}-batch-{}"".format(args.checkpoint_path, name, args.seed, batch_num)\n    model_fname = basename + "".model""\n    LOGGER.info(""Saving model checkpoint to: \'%s\'"", model_fname)\n    torch.save(net.state_dict(), model_fname)\n\n    # Save the training history\n    train_fname = basename + "".json""\n    LOGGER.info(""Saving model training history to \'%s\'"", train_fname)\n    content = {\n        ""loss"": losses,\n        ""cost"": costs,\n        ""seq_lengths"": seq_lengths\n    }\n    open(train_fname, \'wt\').write(json.dumps(content))\n\n\ndef clip_grads(net):\n    """"""Gradient clipping to the range [10, 10].""""""\n    parameters = list(filter(lambda p: p.grad is not None, net.parameters()))\n    for p in parameters:\n        p.grad.data.clamp_(-10, 10)\n\n\ndef train_batch(net, criterion, optimizer, X, Y):\n    """"""Trains a single batch.""""""\n    optimizer.zero_grad()\n    inp_seq_len = X.size(0)\n    outp_seq_len, batch_size, _ = Y.size()\n\n    # New sequence\n    net.init_sequence(batch_size)\n\n    # Feed the sequence + delimiter\n    for i in range(inp_seq_len):\n        net(X[i])\n\n    # Read the output (no input given)\n    y_out = torch.zeros(Y.size())\n    for i in range(outp_seq_len):\n        y_out[i], _ = net()\n\n    loss = criterion(y_out, Y)\n    loss.backward()\n    clip_grads(net)\n    optimizer.step()\n\n    y_out_binarized = y_out.clone().data\n    y_out_binarized.apply_(lambda x: 0 if x < 0.5 else 1)\n\n    # The cost is the number of error bits per sequence\n    cost = torch.sum(torch.abs(y_out_binarized - Y.data))\n\n    return loss.item(), cost.item() / batch_size\n\n\ndef evaluate(net, criterion, X, Y):\n    """"""Evaluate a single batch (without training).""""""\n    inp_seq_len = X.size(0)\n    outp_seq_len, batch_size, _ = Y.size()\n\n    # New sequence\n    net.init_sequence(batch_size)\n\n    # Feed the sequence + delimiter\n    states = []\n    for i in range(inp_seq_len):\n        o, state = net(X[i])\n        states += [state]\n\n    # Read the output (no input given)\n    y_out = torch.zeros(Y.size())\n    for i in range(outp_seq_len):\n        y_out[i], state = net()\n        states += [state]\n\n    loss = criterion(y_out, Y)\n\n    y_out_binarized = y_out.clone().data\n    y_out_binarized.apply_(lambda x: 0 if x < 0.5 else 1)\n\n    # The cost is the number of error bits per sequence\n    cost = torch.sum(torch.abs(y_out_binarized - Y.data))\n\n    result = {\n        \'loss\': loss.data[0],\n        \'cost\': cost / batch_size,\n        \'y_out\': y_out,\n        \'y_out_binarized\': y_out_binarized,\n        \'states\': states\n    }\n\n    return result\n\n\ndef train_model(model, args):\n    num_batches = model.params.num_batches\n    batch_size = model.params.batch_size\n\n    LOGGER.info(""Training model for %d batches (batch_size=%d)..."",\n                num_batches, batch_size)\n\n    losses = []\n    costs = []\n    seq_lengths = []\n    start_ms = get_ms()\n\n    for batch_num, x, y in model.dataloader:\n        loss, cost = train_batch(model.net, model.criterion, model.optimizer, x, y)\n        losses += [loss]\n        costs += [cost]\n        seq_lengths += [y.size(0)]\n\n        # Update the progress bar\n        progress_bar(batch_num, args.report_interval, loss)\n\n        # Report\n        if batch_num % args.report_interval == 0:\n            mean_loss = np.array(losses[-args.report_interval:]).mean()\n            mean_cost = np.array(costs[-args.report_interval:]).mean()\n            mean_time = int(((get_ms() - start_ms) / args.report_interval) / batch_size)\n            progress_clean()\n            LOGGER.info(""Batch %d Loss: %.6f Cost: %.2f Time: %d ms/sequence"",\n                        batch_num, mean_loss, mean_cost, mean_time)\n            start_ms = get_ms()\n\n        # Checkpoint\n        if (args.checkpoint_interval != 0) and (batch_num % args.checkpoint_interval == 0):\n            save_checkpoint(model.net, model.params.name, args,\n                            batch_num, losses, costs, seq_lengths)\n\n    LOGGER.info(""Done training."")\n\n\ndef init_arguments():\n    parser = argparse.ArgumentParser(prog=\'train.py\')\n    parser.add_argument(\'--seed\', type=int, default=RANDOM_SEED, help=""Seed value for RNGs"")\n    parser.add_argument(\'--task\', action=\'store\', choices=list(TASKS.keys()), default=\'copy\',\n                        help=""Choose the task to train (default: copy)"")\n    parser.add_argument(\'-p\', \'--param\', action=\'append\', default=[],\n                        help=\'Override model params. Example: ""-pbatch_size=4 -pnum_heads=2""\')\n    parser.add_argument(\'--checkpoint-interval\', type=int, default=CHECKPOINT_INTERVAL,\n                        help=""Checkpoint interval (default: {}). ""\n                             ""Use 0 to disable checkpointing"".format(CHECKPOINT_INTERVAL))\n    parser.add_argument(\'--checkpoint-path\', action=\'store\', default=\'./\',\n                        help=""Path for saving checkpoint data (default: \'./\')"")\n    parser.add_argument(\'--report-interval\', type=int, default=REPORT_INTERVAL,\n                        help=""Reporting interval"")\n\n    argcomplete.autocomplete(parser)\n\n    args = parser.parse_args()\n    args.checkpoint_path = args.checkpoint_path.rstrip(\'/\')\n\n    return args\n\n\ndef update_model_params(params, update):\n    """"""Updates the default parameters using supplied user arguments.""""""\n\n    update_dict = {}\n    for p in update:\n        m = re.match(""(.*)=(.*)"", p)\n        if not m:\n            LOGGER.error(""Unable to parse param update \'%s\'"", p)\n            sys.exit(1)\n\n        k, v = m.groups()\n        update_dict[k] = v\n\n    try:\n        params = attr.evolve(params, **update_dict)\n    except TypeError as e:\n        LOGGER.error(e)\n        LOGGER.error(""Valid parameters: %s"", list(attr.asdict(params).keys()))\n        sys.exit(1)\n\n    return params\n\ndef init_model(args):\n    LOGGER.info(""Training for the **%s** task"", args.task)\n\n    model_cls, params_cls = TASKS[args.task]\n    params = params_cls()\n    params = update_model_params(params, args.param)\n\n    LOGGER.info(params)\n\n    model = model_cls(params=params)\n    return model\n\n\ndef init_logging():\n    logging.basicConfig(format=\'[%(asctime)s] [%(levelname)s] [%(name)s]  %(message)s\',\n                        level=logging.DEBUG)\n\n\ndef main():\n    init_logging()\n\n    # Initialize arguments\n    args = init_arguments()\n\n    # Initialize random\n    init_seed(args.seed)\n\n    # Initialize the model\n    model = init_model(args)\n\n    LOGGER.info(""Total number of parameters: %d"", model.net.calculate_num_params())\n    train_model(model, args)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
ntm/__init__.py,0,"b""__all__ = ['controller', 'head', 'memory', 'ntm', 'aio']\n"""
ntm/aio.py,1,"b'""""""All in one NTM. Encapsulation of all components.""""""\nimport torch\nfrom torch import nn\nfrom .ntm import NTM\nfrom .controller import LSTMController\nfrom .head import NTMReadHead, NTMWriteHead\nfrom .memory import NTMMemory\n\n\nclass EncapsulatedNTM(nn.Module):\n\n    def __init__(self, num_inputs, num_outputs,\n                 controller_size, controller_layers, num_heads, N, M):\n        """"""Initialize an EncapsulatedNTM.\n\n        :param num_inputs: External number of inputs.\n        :param num_outputs: External number of outputs.\n        :param controller_size: The size of the internal representation.\n        :param controller_layers: Controller number of layers.\n        :param num_heads: Number of heads.\n        :param N: Number of rows in the memory bank.\n        :param M: Number of cols/features in the memory bank.\n        """"""\n        super(EncapsulatedNTM, self).__init__()\n\n        # Save args\n        self.num_inputs = num_inputs\n        self.num_outputs = num_outputs\n        self.controller_size = controller_size\n        self.controller_layers = controller_layers\n        self.num_heads = num_heads\n        self.N = N\n        self.M = M\n\n        # Create the NTM components\n        memory = NTMMemory(N, M)\n        controller = LSTMController(num_inputs + M*num_heads, controller_size, controller_layers)\n        heads = nn.ModuleList([])\n        for i in range(num_heads):\n            heads += [\n                NTMReadHead(memory, controller_size),\n                NTMWriteHead(memory, controller_size)\n            ]\n\n        self.ntm = NTM(num_inputs, num_outputs, controller, memory, heads)\n        self.memory = memory\n\n    def init_sequence(self, batch_size):\n        """"""Initializing the state.""""""\n        self.batch_size = batch_size\n        self.memory.reset(batch_size)\n        self.previous_state = self.ntm.create_new_state(batch_size)\n\n    def forward(self, x=None):\n        if x is None:\n            x = torch.zeros(self.batch_size, self.num_inputs)\n\n        o, self.previous_state = self.ntm(x, self.previous_state)\n        return o, self.previous_state\n\n    def calculate_num_params(self):\n        """"""Returns the total number of parameters.""""""\n        num_params = 0\n        for p in self.parameters():\n            num_params += p.data.view(-1).size(0)\n        return num_params\n'"
ntm/controller.py,3,"b'""""""LSTM Controller.""""""\nimport torch\nfrom torch import nn\nfrom torch.nn import Parameter\nimport numpy as np\n\n\nclass LSTMController(nn.Module):\n    """"""An NTM controller based on LSTM.""""""\n    def __init__(self, num_inputs, num_outputs, num_layers):\n        super(LSTMController, self).__init__()\n\n        self.num_inputs = num_inputs\n        self.num_outputs = num_outputs\n        self.num_layers = num_layers\n\n        self.lstm = nn.LSTM(input_size=num_inputs,\n                            hidden_size=num_outputs,\n                            num_layers=num_layers)\n\n        # The hidden state is a learned parameter\n        self.lstm_h_bias = Parameter(torch.randn(self.num_layers, 1, self.num_outputs) * 0.05)\n        self.lstm_c_bias = Parameter(torch.randn(self.num_layers, 1, self.num_outputs) * 0.05)\n\n        self.reset_parameters()\n\n    def create_new_state(self, batch_size):\n        # Dimension: (num_layers * num_directions, batch, hidden_size)\n        lstm_h = self.lstm_h_bias.clone().repeat(1, batch_size, 1)\n        lstm_c = self.lstm_c_bias.clone().repeat(1, batch_size, 1)\n        return lstm_h, lstm_c\n\n    def reset_parameters(self):\n        for p in self.lstm.parameters():\n            if p.dim() == 1:\n                nn.init.constant_(p, 0)\n            else:\n                stdev = 5 / (np.sqrt(self.num_inputs +  self.num_outputs))\n                nn.init.uniform_(p, -stdev, stdev)\n\n    def size(self):\n        return self.num_inputs, self.num_outputs\n\n    def forward(self, x, prev_state):\n        x = x.unsqueeze(0)\n        outp, state = self.lstm(x, prev_state)\n        return outp.squeeze(0), state\n'"
ntm/head.py,3,"b'""""""NTM Read and Write Heads.""""""\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport numpy as np\n\n\ndef _split_cols(mat, lengths):\n    """"""Split a 2D matrix to variable length columns.""""""\n    assert mat.size()[1] == sum(lengths), ""Lengths must be summed to num columns""\n    l = np.cumsum([0] + lengths)\n    results = []\n    for s, e in zip(l[:-1], l[1:]):\n        results += [mat[:, s:e]]\n    return results\n\n\nclass NTMHeadBase(nn.Module):\n    """"""An NTM Read/Write Head.""""""\n\n    def __init__(self, memory, controller_size):\n        """"""Initilize the read/write head.\n\n        :param memory: The :class:`NTMMemory` to be addressed by the head.\n        :param controller_size: The size of the internal representation.\n        """"""\n        super(NTMHeadBase, self).__init__()\n\n        self.memory = memory\n        self.N, self.M = memory.size()\n        self.controller_size = controller_size\n\n    def create_new_state(self, batch_size):\n        raise NotImplementedError\n\n    def register_parameters(self):\n        raise NotImplementedError\n\n    def is_read_head(self):\n        return NotImplementedError\n\n    def _address_memory(self, k, \xce\xb2, g, s, \xce\xb3, w_prev):\n        # Handle Activations\n        k = k.clone()\n        \xce\xb2 = F.softplus(\xce\xb2)\n        g = F.sigmoid(g)\n        s = F.softmax(s, dim=1)\n        \xce\xb3 = 1 + F.softplus(\xce\xb3)\n\n        w = self.memory.address(k, \xce\xb2, g, s, \xce\xb3, w_prev)\n\n        return w\n\n\nclass NTMReadHead(NTMHeadBase):\n    def __init__(self, memory, controller_size):\n        super(NTMReadHead, self).__init__(memory, controller_size)\n\n        # Corresponding to k, \xce\xb2, g, s, \xce\xb3 sizes from the paper\n        self.read_lengths = [self.M, 1, 1, 3, 1]\n        self.fc_read = nn.Linear(controller_size, sum(self.read_lengths))\n        self.reset_parameters()\n\n    def create_new_state(self, batch_size):\n        # The state holds the previous time step address weightings\n        return torch.zeros(batch_size, self.N)\n\n    def reset_parameters(self):\n        # Initialize the linear layers\n        nn.init.xavier_uniform_(self.fc_read.weight, gain=1.4)\n        nn.init.normal_(self.fc_read.bias, std=0.01)\n\n    def is_read_head(self):\n        return True\n\n    def forward(self, embeddings, w_prev):\n        """"""NTMReadHead forward function.\n\n        :param embeddings: input representation of the controller.\n        :param w_prev: previous step state\n        """"""\n        o = self.fc_read(embeddings)\n        k, \xce\xb2, g, s, \xce\xb3 = _split_cols(o, self.read_lengths)\n\n        # Read from memory\n        w = self._address_memory(k, \xce\xb2, g, s, \xce\xb3, w_prev)\n        r = self.memory.read(w)\n\n        return r, w\n\n\nclass NTMWriteHead(NTMHeadBase):\n    def __init__(self, memory, controller_size):\n        super(NTMWriteHead, self).__init__(memory, controller_size)\n\n        # Corresponding to k, \xce\xb2, g, s, \xce\xb3, e, a sizes from the paper\n        self.write_lengths = [self.M, 1, 1, 3, 1, self.M, self.M]\n        self.fc_write = nn.Linear(controller_size, sum(self.write_lengths))\n        self.reset_parameters()\n\n    def create_new_state(self, batch_size):\n        return torch.zeros(batch_size, self.N)\n\n    def reset_parameters(self):\n        # Initialize the linear layers\n        nn.init.xavier_uniform_(self.fc_write.weight, gain=1.4)\n        nn.init.normal_(self.fc_write.bias, std=0.01)\n\n    def is_read_head(self):\n        return False\n\n    def forward(self, embeddings, w_prev):\n        """"""NTMWriteHead forward function.\n\n        :param embeddings: input representation of the controller.\n        :param w_prev: previous step state\n        """"""\n        o = self.fc_write(embeddings)\n        k, \xce\xb2, g, s, \xce\xb3, e, a = _split_cols(o, self.write_lengths)\n\n        # e should be in [0, 1]\n        e = F.sigmoid(e)\n\n        # Write to memory\n        w = self._address_memory(k, \xce\xb2, g, s, \xce\xb3, w_prev)\n        self.memory.write(w, e, a)\n\n        return w\n'"
ntm/memory.py,9,"b'""""""An NTM\'s memory implementation.""""""\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nimport numpy as np\n\n\ndef _convolve(w, s):\n    """"""Circular convolution implementation.""""""\n    assert s.size(0) == 3\n    t = torch.cat([w[-1:], w, w[:1]])\n    c = F.conv1d(t.view(1, 1, -1), s.view(1, 1, -1)).view(-1)\n    return c\n\n\nclass NTMMemory(nn.Module):\n    """"""Memory bank for NTM.""""""\n    def __init__(self, N, M):\n        """"""Initialize the NTM Memory matrix.\n\n        The memory\'s dimensions are (batch_size x N x M).\n        Each batch has it\'s own memory matrix.\n\n        :param N: Number of rows in the memory.\n        :param M: Number of columns/features in the memory.\n        """"""\n        super(NTMMemory, self).__init__()\n\n        self.N = N\n        self.M = M\n\n        # The memory bias allows the heads to learn how to initially address\n        # memory locations by content\n        self.register_buffer(\'mem_bias\', torch.Tensor(N, M))\n\n        # Initialize memory bias\n        stdev = 1 / (np.sqrt(N + M))\n        nn.init.uniform_(self.mem_bias, -stdev, stdev)\n\n    def reset(self, batch_size):\n        """"""Initialize memory from bias, for start-of-sequence.""""""\n        self.batch_size = batch_size\n        self.memory = self.mem_bias.clone().repeat(batch_size, 1, 1)\n\n    def size(self):\n        return self.N, self.M\n\n    def read(self, w):\n        """"""Read from memory (according to section 3.1).""""""\n        return torch.matmul(w.unsqueeze(1), self.memory).squeeze(1)\n\n    def write(self, w, e, a):\n        """"""write to memory (according to section 3.2).""""""\n        self.prev_mem = self.memory\n        self.memory = torch.Tensor(self.batch_size, self.N, self.M)\n        erase = torch.matmul(w.unsqueeze(-1), e.unsqueeze(1))\n        add = torch.matmul(w.unsqueeze(-1), a.unsqueeze(1))\n        self.memory = self.prev_mem * (1 - erase) + add\n\n    def address(self, k, \xce\xb2, g, s, \xce\xb3, w_prev):\n        """"""NTM Addressing (according to section 3.3).\n\n        Returns a softmax weighting over the rows of the memory matrix.\n\n        :param k: The key vector.\n        :param \xce\xb2: The key strength (focus).\n        :param g: Scalar interpolation gate (with previous weighting).\n        :param s: Shift weighting.\n        :param \xce\xb3: Sharpen weighting scalar.\n        :param w_prev: The weighting produced in the previous time step.\n        """"""\n        # Content focus\n        wc = self._similarity(k, \xce\xb2)\n\n        # Location focus\n        wg = self._interpolate(w_prev, wc, g)\n        \xc5\xb5 = self._shift(wg, s)\n        w = self._sharpen(\xc5\xb5, \xce\xb3)\n\n        return w\n\n    def _similarity(self, k, \xce\xb2):\n        k = k.view(self.batch_size, 1, -1)\n        w = F.softmax(\xce\xb2 * F.cosine_similarity(self.memory + 1e-16, k + 1e-16, dim=-1), dim=1)\n        return w\n\n    def _interpolate(self, w_prev, wc, g):\n        return g * wc + (1 - g) * w_prev\n\n    def _shift(self, wg, s):\n        result = torch.zeros(wg.size())\n        for b in range(self.batch_size):\n            result[b] = _convolve(wg[b], s[b])\n        return result\n\n    def _sharpen(self, \xc5\xb5, \xce\xb3):\n        w = \xc5\xb5 ** \xce\xb3\n        w = torch.div(w, torch.sum(w, dim=1).view(-1, 1) + 1e-16)\n        return w\n'"
ntm/ntm.py,4,"b'#!/usr/bin/env python\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\n\nclass NTM(nn.Module):\n    """"""A Neural Turing Machine.""""""\n    def __init__(self, num_inputs, num_outputs, controller, memory, heads):\n        """"""Initialize the NTM.\n\n        :param num_inputs: External input size.\n        :param num_outputs: External output size.\n        :param controller: :class:`LSTMController`\n        :param memory: :class:`NTMMemory`\n        :param heads: list of :class:`NTMReadHead` or :class:`NTMWriteHead`\n\n        Note: This design allows the flexibility of using any number of read and\n              write heads independently, also, the order by which the heads are\n              called in controlled by the user (order in list)\n        """"""\n        super(NTM, self).__init__()\n\n        # Save arguments\n        self.num_inputs = num_inputs\n        self.num_outputs = num_outputs\n        self.controller = controller\n        self.memory = memory\n        self.heads = heads\n\n        self.N, self.M = memory.size()\n        _, self.controller_size = controller.size()\n\n        # Initialize the initial previous read values to random biases\n        self.num_read_heads = 0\n        self.init_r = []\n        for head in heads:\n            if head.is_read_head():\n                init_r_bias = torch.randn(1, self.M) * 0.01\n                self.register_buffer(""read{}_bias"".format(self.num_read_heads), init_r_bias.data)\n                self.init_r += [init_r_bias]\n                self.num_read_heads += 1\n\n        assert self.num_read_heads > 0, ""heads list must contain at least a single read head""\n\n        # Initialize a fully connected layer to produce the actual output:\n        #   [controller_output; previous_reads ] -> output\n        self.fc = nn.Linear(self.controller_size + self.num_read_heads * self.M, num_outputs)\n        self.reset_parameters()\n\n    def create_new_state(self, batch_size):\n        init_r = [r.clone().repeat(batch_size, 1) for r in self.init_r]\n        controller_state = self.controller.create_new_state(batch_size)\n        heads_state = [head.create_new_state(batch_size) for head in self.heads]\n\n        return init_r, controller_state, heads_state\n\n    def reset_parameters(self):\n        # Initialize the linear layer\n        nn.init.xavier_uniform_(self.fc.weight, gain=1)\n        nn.init.normal_(self.fc.bias, std=0.01)\n\n    def forward(self, x, prev_state):\n        """"""NTM forward function.\n\n        :param x: input vector (batch_size x num_inputs)\n        :param prev_state: The previous state of the NTM\n        """"""\n        # Unpack the previous state\n        prev_reads, prev_controller_state, prev_heads_states = prev_state\n\n        # Use the controller to get an embeddings\n        inp = torch.cat([x] + prev_reads, dim=1)\n        controller_outp, controller_state = self.controller(inp, prev_controller_state)\n\n        # Read/Write from the list of heads\n        reads = []\n        heads_states = []\n        for head, prev_head_state in zip(self.heads, prev_heads_states):\n            if head.is_read_head():\n                r, head_state = head(controller_outp, prev_head_state)\n                reads += [r]\n            else:\n                head_state = head(controller_outp, prev_head_state)\n            heads_states += [head_state]\n\n        # Generate Output\n        inp2 = torch.cat([controller_outp] + reads, dim=1)\n        o = F.sigmoid(self.fc(inp2))\n\n        # Pack the current state\n        state = (reads, controller_state, heads_states)\n\n        return o, state\n'"
tasks/__init__.py,0,b''
tasks/copytask.py,2,"b'""""""Copy Task NTM model.""""""\nimport random\n\nfrom attr import attrs, attrib, Factory\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport numpy as np\n\nfrom ntm.aio import EncapsulatedNTM\n\n\n# Generator of randomized test sequences\ndef dataloader(num_batches,\n               batch_size,\n               seq_width,\n               min_len,\n               max_len):\n    """"""Generator of random sequences for the copy task.\n\n    Creates random batches of ""bits"" sequences.\n\n    All the sequences within each batch have the same length.\n    The length is [`min_len`, `max_len`]\n\n    :param num_batches: Total number of batches to generate.\n    :param seq_width: The width of each item in the sequence.\n    :param batch_size: Batch size.\n    :param min_len: Sequence minimum length.\n    :param max_len: Sequence maximum length.\n\n    NOTE: The input width is `seq_width + 1`, the additional input\n    contain the delimiter.\n    """"""\n    for batch_num in range(num_batches):\n\n        # All batches have the same sequence length\n        seq_len = random.randint(min_len, max_len)\n        seq = np.random.binomial(1, 0.5, (seq_len, batch_size, seq_width))\n        seq = torch.from_numpy(seq)\n\n        # The input includes an additional channel used for the delimiter\n        inp = torch.zeros(seq_len + 1, batch_size, seq_width + 1)\n        inp[:seq_len, :, :seq_width] = seq\n        inp[seq_len, :, seq_width] = 1.0 # delimiter in our control channel\n        outp = seq.clone()\n\n        yield batch_num+1, inp.float(), outp.float()\n\n\n@attrs\nclass CopyTaskParams(object):\n    name = attrib(default=""copy-task"")\n    controller_size = attrib(default=100, convert=int)\n    controller_layers = attrib(default=1,convert=int)\n    num_heads = attrib(default=1, convert=int)\n    sequence_width = attrib(default=8, convert=int)\n    sequence_min_len = attrib(default=1,convert=int)\n    sequence_max_len = attrib(default=20, convert=int)\n    memory_n = attrib(default=128, convert=int)\n    memory_m = attrib(default=20, convert=int)\n    num_batches = attrib(default=50000, convert=int)\n    batch_size = attrib(default=1, convert=int)\n    rmsprop_lr = attrib(default=1e-4, convert=float)\n    rmsprop_momentum = attrib(default=0.9, convert=float)\n    rmsprop_alpha = attrib(default=0.95, convert=float)\n\n\n#\n# To create a network simply instantiate the `:class:CopyTaskModelTraining`,\n# all the components will be wired with the default values.\n# In case you\'d like to change any of defaults, do the following:\n#\n# > params = CopyTaskParams(batch_size=4)\n# > model = CopyTaskModelTraining(params=params)\n#\n# Then use `model.net`, `model.optimizer` and `model.criterion` to train the\n# network. Call `model.train_batch` for training and `model.evaluate`\n# for evaluating.\n#\n# You may skip this alltogether, and use `:class:CopyTaskNTM` directly.\n#\n\n@attrs\nclass CopyTaskModelTraining(object):\n    params = attrib(default=Factory(CopyTaskParams))\n    net = attrib()\n    dataloader = attrib()\n    criterion = attrib()\n    optimizer = attrib()\n\n    @net.default\n    def default_net(self):\n        # We have 1 additional input for the delimiter which is passed on a\n        # separate ""control"" channel\n        net = EncapsulatedNTM(self.params.sequence_width + 1, self.params.sequence_width,\n                              self.params.controller_size, self.params.controller_layers,\n                              self.params.num_heads,\n                              self.params.memory_n, self.params.memory_m)\n        return net\n\n    @dataloader.default\n    def default_dataloader(self):\n        return dataloader(self.params.num_batches, self.params.batch_size,\n                          self.params.sequence_width,\n                          self.params.sequence_min_len, self.params.sequence_max_len)\n\n    @criterion.default\n    def default_criterion(self):\n        return nn.BCELoss()\n\n    @optimizer.default\n    def default_optimizer(self):\n        return optim.RMSprop(self.net.parameters(),\n                             momentum=self.params.rmsprop_momentum,\n                             alpha=self.params.rmsprop_alpha,\n                             lr=self.params.rmsprop_lr)\n'"
tasks/repeatcopytask.py,3,"b'""""""Copy Task NTM model.""""""\nimport random\n\nfrom attr import attrs, attrib, Factory\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport numpy as np\n\nfrom ntm.aio import EncapsulatedNTM\n\n\n# Generator of randomized test sequences\ndef dataloader(num_batches,\n               batch_size,\n               seq_width,\n               seq_min_len,\n               seq_max_len,\n               repeat_min,\n               repeat_max):\n    """"""Generator of random sequences for the repeat copy task.\n\n    Creates random batches of ""bits"" sequences.\n\n    All the sequences within each batch have the same length.\n    The length is between `min_len` to `max_len`\n\n    :param num_batches: Total number of batches to generate.\n    :param batch_size: Batch size.\n    :param seq_width: The width of each item in the sequence.\n    :param seq_min_len: Sequence minimum length.\n    :param seq_max_len: Sequence maximum length.\n    :param repeat_min: Minimum repeatitions.\n    :param repeat_max: Maximum repeatitions.\n\n    NOTE: The input width is `seq_width + 2`. One additional input\n    is used for the delimiter, and one for the number of repetitions.\n    The output width is `seq_width` + 1, the additional input is used\n    by the network to generate an end-marker, so we can be sure the\n    network counted correctly.\n    """"""\n    # Some normalization constants\n    reps_mean = (repeat_max + repeat_min) / 2\n    reps_var = (((repeat_max - repeat_min + 1) ** 2) - 1) / 12\n    reps_std = np.sqrt(reps_var)\n    def rpt_normalize(reps):\n        return (reps - reps_mean) / reps_std\n\n    for batch_num in range(num_batches):\n\n        # All batches have the same sequence length and number of reps\n        seq_len = random.randint(seq_min_len, seq_max_len)\n        reps = random.randint(repeat_min, repeat_max)\n\n        # Generate the sequence\n        seq = np.random.binomial(1, 0.5, (seq_len, batch_size, seq_width))\n        seq = torch.from_numpy(seq)\n\n        # The input includes 2 additional channels, for end-of-sequence and num-reps\n        inp = torch.zeros(seq_len + 2, batch_size, seq_width + 2)\n        inp[:seq_len, :, :seq_width] = seq\n        inp[seq_len, :, seq_width] = 1.0\n        inp[seq_len+1, :, seq_width+1] = rpt_normalize(reps)\n\n        # The output contain the repeated sequence + end marker\n        outp = torch.zeros(seq_len * reps + 1, batch_size, seq_width + 1)\n        outp[:seq_len * reps, :, :seq_width] = seq.clone().repeat(reps, 1, 1)\n        outp[seq_len * reps, :, seq_width] = 1.0 # End marker\n\n        yield batch_num+1, inp.float(), outp.float()\n\n\n@attrs\nclass RepeatCopyTaskParams(object):\n    name = attrib(default=""repeat-copy-task"")\n    controller_size = attrib(default=100, convert=int)\n    controller_layers = attrib(default=1, convert=int)\n    num_heads = attrib(default=1, convert=int)\n    sequence_width = attrib(default=8, convert=int)\n    sequence_min_len = attrib(default=1, convert=int)\n    sequence_max_len = attrib(default=10, convert=int)\n    repeat_min = attrib(default=1, convert=int)\n    repeat_max = attrib(default=10, convert=int)\n    memory_n = attrib(default=128, convert=int)\n    memory_m = attrib(default=20, convert=int)\n    num_batches = attrib(default=250000, convert=int)\n    batch_size = attrib(default=1, convert=int)\n    rmsprop_lr = attrib(default=1e-4, convert=float)\n    rmsprop_momentum = attrib(default=0.9, convert=float)\n    rmsprop_alpha = attrib(default=0.95, convert=float)\n\n\n@attrs\nclass RepeatCopyTaskModelTraining(object):\n    params = attrib(default=Factory(RepeatCopyTaskParams))\n    net = attrib()\n    dataloader = attrib()\n    criterion = attrib()\n    optimizer = attrib()\n\n    @net.default\n    def default_net(self):\n        # See dataloader documentation\n        net = EncapsulatedNTM(self.params.sequence_width + 2, self.params.sequence_width + 1,\n                              self.params.controller_size, self.params.controller_layers,\n                              self.params.num_heads,\n                              self.params.memory_n, self.params.memory_m)\n        return net\n\n    @dataloader.default\n    def default_dataloader(self):\n        return dataloader(self.params.num_batches, self.params.batch_size,\n                          self.params.sequence_width,\n                          self.params.sequence_min_len, self.params.sequence_max_len,\n                          self.params.repeat_min, self.params.repeat_max)\n\n    @criterion.default\n    def default_criterion(self):\n        return nn.BCELoss()\n\n    @optimizer.default\n    def default_optimizer(self):\n        return optim.RMSprop(self.net.parameters(),\n                             momentum=self.params.rmsprop_momentum,\n                             alpha=self.params.rmsprop_alpha,\n                             lr=self.params.rmsprop_lr)\n'"
tests/__init__.py,0,b''
tests/test_memory.py,3,"b""import pytest\nimport torch\nfrom ntm.memory import NTMMemory\n\ndef _t(*l):\n    return torch.Tensor(l).unsqueeze(0)\n\nclass TestMemoryReadWrite:\n    N = 4\n    M = 4\n\n    def setup_class(self):\n        self.memory = NTMMemory(self.N, self.M)\n        self.memory.reset(batch_size=1)\n\n    def teardown_class(self):\n        del self.memory\n\n    def test_size(self):\n        n, m = self.memory.size()\n        assert n == self.N\n        assert m == self.M\n\n    @pytest.mark.parametrize('w, e, a, expected', [\n        (_t(1, 0, 0, 0), _t(1, 1, 1, 1), _t(1, 0, 0, 0), _t(1, 0, 0, 0)),\n        (_t(0, 1, 0, 0), _t(1, 1, 1, 1), _t(0, 1, 0, 0), _t(0, 1, 0, 0)),\n        (_t(0, 0, 1, 0), _t(1, 1, 1, 1), _t(0, 0, 1, 0), _t(0, 0, 1, 0)),\n        (_t(0, 0, 0, 1), _t(1, 1, 1, 1), _t(0, 0, 0, 1), _t(0, 0, 0, 1)),\n        (_t(1, 0, 0, 0), _t(0, 1, 1, 1), _t(0, 1, 1, 1), _t(1, 1, 1, 1)),\n        (_t(0, 1, 0, 0), _t(0, 0, 0, 0), _t(0, 0, 1, 0), _t(0, 1, 1, 0)),\n        (_t(0, 0, 1, 0), _t(0, 0, 0, 0), _t(0, 0, 0, 0), _t(0, 0, 1, 0)),\n        (_t(0, 0, 0, 1), _t(0, 0, 0, 0.5), _t(0, 0, 0, 0.2), _t(0, 0, 0, 0.7)),\n        (_t(0.5, 0.5, 0, 0), _t(1, 1, 1, 1), _t(0, 0, 0, 0), _t(0.25, 0.5, 0.5, 0.25)),\n    ])\n    def test_read_write(self, w, e, a, expected):\n        self.memory.write(w, e, a)\n        result = self.memory.read(w)\n        assert torch.equal(expected.data, result.data)\n\n\n@pytest.fixture\ndef mem():\n    mm = NTMMemory(4, 4)\n    mm.reset(batch_size=1)\n\n    # Identity-fy the memory matrix\n    mm.write(_t(1, 0, 0, 0), _t(1, 1, 1, 1), _t(1, 0, 0, 0))\n    mm.write(_t(0, 1, 0, 0), _t(1, 1, 1, 1), _t(0, 1, 0, 0))\n    mm.write(_t(0, 0, 1, 0), _t(1, 1, 1, 1), _t(0, 0, 1, 0))\n    mm.write(_t(0, 0, 0, 1), _t(1, 1, 1, 1), _t(0, 0, 0, 1))\n\n    return mm\n\n\nclass TestAddressing:\n\n    @pytest.mark.parametrize('k, beta, g, shift, gamma, w_prev, expected', [\n        (_t(1, 0, 0, 0), _t(100), _t(1), _t(0, 1, 0), _t(100), _t(0, 0, 0, 0), _t(1, 0, 0, 0)), # test similarity/interpolation\n    ])\n    def test_addressing(self, mem, k, beta, g, shift, gamma, w_prev, expected):\n        w = mem.address(k, beta, g, shift, gamma, w_prev)\n        assert torch.equal(w.data, expected.data)\n"""
