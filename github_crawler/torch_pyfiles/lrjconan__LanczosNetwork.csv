file_path,api_count,code
__init__.py,0,b''
run_exp.py,4,"b'import os\nimport sys\nimport torch\nimport logging\nimport traceback\nimport numpy as np\nfrom pprint import pprint\n\nfrom runner import *\nfrom utils.logger import setup_logging\nfrom utils.arg_helper import parse_arguments, get_config\ntorch.set_printoptions(profile=\'full\')\n\n\ndef main():\n  args = parse_arguments()\n  config = get_config(args.config_file)\n  np.random.seed(config.seed)\n  torch.manual_seed(config.seed)\n  torch.cuda.manual_seed_all(config.seed)\n  config.use_gpu = config.use_gpu and torch.cuda.is_available()\n\n  # log info\n  log_file = os.path.join(config.save_dir,\n                          ""log_exp_{}.txt"".format(config.run_id))\n  logger = setup_logging(args.log_level, log_file)\n  logger.info(""Writing log file to {}"".format(log_file))\n  logger.info(""Exp instance id = {}"".format(config.run_id))\n  logger.info(""Exp comment = {}"".format(args.comment))\n  logger.info(""Config ="")\n  print("">"" * 80)\n  pprint(config)\n  print(""<"" * 80)\n\n  # Run the experiment\n  try:\n    runner = eval(config.runner)(config)\n    if not args.test:\n      runner.train()\n    else:\n      runner.test()\n  except:\n    logger.error(traceback.format_exc())\n\n  sys.exit(0)\n\n\nif __name__ == ""__main__"":\n  main()'"
dataset/__init__.py,0,b'from dataset.qm8 import *\nfrom dataset.graph_data import *\n'
dataset/get_graph_data.py,0,"b'import os\nimport glob\nimport pickle\nimport numpy as np\nimport networkx as nx\nfrom utils.data_helper import get_multi_graph_laplacian_eigs, get_graph_laplacian_eigs, get_laplacian\n\nsave_dir = \'../data/synthetic/\'\n\nif not os.path.exists(save_dir):\n  os.mkdir(save_dir)\n  print(\'made directory {}\'.format(save_dir))\n\n\ndef gen_data(min_num_nodes=20,\n             max_num_nodes=100,\n             num_graphs=10,\n             node_emb_dim=10,\n             graph_emb_dim=2,\n             edge_prob=0.5,\n             seed=123):\n  """"""\n    Generate synthetic graph data for graph regression, i.e., given node \n    embedding and graph structure as input, predict a graph embedding \n    as output.\n\n    N.B.: modification to other tasks like node classification is straightforward\n\n    A single graph in your dataset should contin:\n      X: Node embedding, numpy array, shape N X D where N is # nodes\n      A: Graph structure, numpy array, shape N X N X E where E is # edge types\n      Y: Graph embedding, numpy array, shape N X O\n  """"""\n  npr = np.random.RandomState(seed)\n  N = npr.randint(min_num_nodes, high=max_num_nodes+1, size=num_graphs)\n\n  data = []\n  for ii in range(num_graphs):    \n    data_dict = {}\n    data_dict[\'X\'] = npr.randn(N[ii], node_emb_dim)\n    # we assume # edge type = 1, but you can easily extend it to be more than 1\n    data_dict[\'A\'] = np.expand_dims(\n        nx.to_numpy_matrix(\n            nx.fast_gnp_random_graph(N[ii], edge_prob, seed=npr.randint(1000))),\n        axis=2)\n    data_dict[\'Y\'] = npr.randn(1, graph_emb_dim)\n    data += [data_dict]\n\n  return data\n\n\ndef dump_data(data_list, tag=\'train\'):\n  count = 0\n  print(\'Dump {} data!\'.format(tag))\n  for data in data_list:\n\n    data_dict = {}\n    data_dict[\'node_feat\'] = data[\'X\']\n    adjs = data[\'A\']\n    adj_simple = np.sum(adjs, axis=2)\n    D_list, V_list, L_list = get_multi_graph_laplacian_eigs(\n        adjs, graph_laplacian_type=\'L4\', use_eigen_decomp=True, is_sym=True)\n    D, V, L4 = get_graph_laplacian_eigs(\n        adj_simple,\n        graph_laplacian_type=\'L4\',\n        use_eigen_decomp=True,\n        is_sym=True)\n\n    L6 = get_laplacian(adj_simple, graph_laplacian_type=\'L6\')\n    L7 = get_laplacian(adj_simple, graph_laplacian_type=\'L7\')\n\n    data_dict[\'L_multi\'] = np.stack(L_list, axis=2)\n    data_dict[\'L_simple_4\'] = L4\n    data_dict[\'L_simple_6\'] = L6\n    data_dict[\'L_simple_7\'] = L7\n\n    # N.B.: for some edge type, adjacency matrix may be diagonal\n    data_dict[\'D_simple\'] = D if D is not None else np.ones(adjs.shape[0])\n    data_dict[\'V_simple\'] = V if V is not None else np.eye(adjs.shape[0])\n    data_dict[\'D_multi\'] = D_list\n    data_dict[\'V_multi\'] = V_list\n    data_dict[\'label\'] = data[\'Y\']\n\n    pickle.dump(\n        data_dict,\n        open(\n            os.path.join(save_dir, \'synthetic_{}_{:07d}.p\'.format(tag, count)),\n            \'wb\'))\n\n    count += 1\n\n  print(\'100.0 %%\')\n\n\nif __name__ == \'__main__\':\n  train_dataset = gen_data(seed=123)\n  dev_dataset = gen_data(seed=456)\n  test_dataset = gen_data(seed=789)\n\n  dump_data(train_dataset, \'train\')\n  dump_data(dev_dataset, \'dev\')\n  dump_data(test_dataset, \'test\')\n'"
dataset/get_qm8_data.py,0,"b""import os\nimport glob\nimport pickle\nimport numpy as np\nimport deepchem as dc\nfrom utils.data_helper import get_multi_graph_laplacian_eigs, get_graph_laplacian_eigs, get_laplacian\n\nglobal num_nodes, num_edges, num_graphs \nnum_nodes = 0\nnum_edges = 0\nnum_graphs = 0\nn_atom_feat = 70\nn_pair_feat = 6  # edge type #\ndata_folder = '../data/QM8/'\nsave_dir = '../data/QM8/preprocess'\n\nif not os.path.exists(data_folder):\n  os.mkdir(data_folder)\n  print('made directory {}'.format(data_folder))\n\nif not os.path.exists(save_dir):\n  os.mkdir(save_dir)\n  print('made directory {}'.format(save_dir))\n\n\ndef to_graph(mol):\n  # number of atoms in each molecule\n  n_atoms = mol.get_num_atoms()\n\n  # atom features\n  atom_feat = mol.get_atom_features()\n\n  # convert 1-of-K encoding to index\n  atom_feat = np.argmax(atom_feat, axis=1)\n\n  # edge features, shape N X N X 8\n  # first 6 channels: bond-wise adjacency matrix\n  # 6-th channel: is-in-the-same-ring adjacency matrix\n  # 7-th channel: distance matrix\n  pair_feat = mol.get_pair_features()\n\n  return atom_feat, pair_feat[:, :, :6]\n\n\ndef dump_data(dataset, tag='train'):\n  count = 0\n  global num_graphs\n  print('Dump {} data!'.format(tag))\n  for (X_b, y_b, w_b, ids_b) in dataset.iterbatches(\n      batch_size=1, deterministic=True, pad_batches=False):\n    assert len(X_b) == 1\n\n    if count % 100 == 0:\n      print('{:.1f} %%\\r'.format(100 * count / float(len(dataset))), end='')\n\n    data_dict = {}\n    for im, mol in enumerate(X_b):\n      global num_nodes, num_edges\n      node_feat, adjs = to_graph(mol)\n      data_dict['node_feat'] = node_feat\n\n      adj_simple = np.sum(adjs, axis=2)\n      D_list, V_list, L_list = get_multi_graph_laplacian_eigs(\n          adjs, graph_laplacian_type='L4', use_eigen_decomp=True, is_sym=True)\n      D, V, L4 = get_graph_laplacian_eigs(\n          adj_simple,\n          graph_laplacian_type='L4',\n          use_eigen_decomp=True,\n          is_sym=True)\n\n      L6 = get_laplacian(adj_simple, graph_laplacian_type='L6')\n      L7 = get_laplacian(adj_simple, graph_laplacian_type='L7')\n\n      data_dict['L_multi'] = np.stack(L_list, axis=2)\n      data_dict['L_simple_4'] = L4\n      data_dict['L_simple_6'] = L6\n      data_dict['L_simple_7'] = L7\n\n      # N.B.: for some edge type, adjacency matrix may be diagonal\n      data_dict['D_simple'] = D if D is not None else np.ones(adjs.shape[0])\n      data_dict['V_simple'] = V if V is not None else np.eye(adjs.shape[0])\n      data_dict['D_multi'] = D_list\n      data_dict['V_multi'] = V_list\n      \n      num_nodes += node_feat.shape[0]\n      num_edges += np.sum(adj_simple) / 2.0\n    \n    num_graphs += 1.0\n    data_dict['label'] = y_b\n    data_dict['label_weight'] = w_b\n\n    pickle.dump(\n        data_dict,\n        open(\n            os.path.join(save_dir, 'QM8_preprocess_{}_{:07d}.p'.format(\n                tag, count)), 'wb'))\n\n    count += 1\n\n  print('100.0 %%')\n\n\nif __name__ == '__main__':\n  tasks, datasets, transformers = dc.molnet.load_qm8(\n      featurizer='MP', reload=False)\n  train_dataset, dev_dataset, test_dataset = datasets\n\n  dump_data(train_dataset, 'train')\n  dump_data(dev_dataset, 'dev')\n  dump_data(test_dataset, 'test')\n\n  mean_label = transformers[0].y_means\n  std_label = transformers[0].y_stds\n\n  print('mean = {}'.format(mean_label))\n  print('std = {}'.format(std_label))\n  print('average nodes per graph = {}'.format(num_nodes / num_graphs))\n  print('average edges per graph = {}'.format(num_edges / num_graphs))\n\n  meta_data = {'mean': mean_label, 'std': std_label}\n  pickle.dump(meta_data, open(os.path.join(data_folder, 'QM8_meta.p'), 'wb'))\n"""
dataset/graph_data.py,22,"b'import os\nimport glob\nimport torch\nimport pickle\nimport numpy as np\nfrom utils.spectral_graph_partition import *\n\n__all__ = [\'GraphData\']\n\n\nclass GraphData(object):\n\n  def __init__(self, config, split=\'train\'):\n    assert split == \'train\' or split == \'dev\' or split == \'test\', ""no such split""\n    self.split = split\n    self.config = config\n    self.seed = config.seed\n    self.npr = np.random.RandomState(self.seed)\n    self.data_path = config.dataset.data_path\n    self.num_edgetype = config.dataset.num_edge_type\n    self.model_name = config.model.name\n    self.use_eigs = True if hasattr(config.model, \'num_eig_vec\') else False\n    if self.use_eigs:\n      self.num_eigs = config.model.num_eig_vec\n\n    if self.model_name == \'GraphSAGE\':\n      self.num_sample_neighbors = config.model.num_sample_neighbors\n\n    self.train_data_files = glob.glob(\n        os.path.join(self.data_path, \'synthetic_train_*.p\'))\n    self.dev_data_files = glob.glob(\n        os.path.join(self.data_path, \'synthetic_dev_*.p\'))\n    self.test_data_files = glob.glob(\n        os.path.join(self.data_path, \'synthetic_test_*.p\'))\n\n    self.num_train = len(self.train_data_files)\n    self.num_dev = len(self.dev_data_files)\n    self.num_test = len(self.test_data_files)\n    self.num_graphs = self.num_train + self.num_dev + self.num_test\n\n  def __getitem__(self, index):\n    if self.split == \'train\':\n      return pickle.load(open(self.train_data_files[index], \'rb\'))\n    elif self.split == \'dev\':\n      return pickle.load(open(self.dev_data_files[index], \'rb\'))\n    else:\n      return pickle.load(open(self.test_data_files[index], \'rb\'))\n\n  def __len__(self):\n    if self.split == \'train\':\n      return self.num_train\n    elif self.split == \'dev\':\n      return self.num_dev\n    else:\n      return self.num_test\n\n  def collate_fn(self, batch):\n    """"""\n      Collate function for mini-batch\n      N.B.: we pad all samples to the maximum of the mini-batch\n    """"""\n    assert isinstance(batch, list)\n\n    data = {}\n    batch_size = len(batch)\n    node_size = [bb[\'node_feat\'].shape[0] for bb in batch]\n    batch_node_size = max(node_size)  # value -> N\n    pad_node_size = [batch_node_size - nn for nn in node_size]\n    \n    # pad feature: shape (B, N, D)\n    data[\'node_feat\'] = torch.stack([\n        torch.from_numpy(\n            np.pad(\n                bb[\'node_feat\'], ((0, pad_node_size[ii]), (0, 0)),\n                \'constant\',\n                constant_values=0.0)) for ii, bb in enumerate(batch)\n    ]).float()\n    \n    # binary mask: shape (B, N)\n    data[\'node_mask\'] = torch.stack([\n        torch.from_numpy(\n            np.pad(\n                np.ones(node_size[ii]), (0, pad_node_size[ii]),\n                \'constant\',\n                constant_values=0.0)) for ii, bb in enumerate(batch)\n    ]).byte()\n\n    # label: shape (B, O)\n    data[\'label\'] = torch.cat(\n        [torch.from_numpy(bb[\'label\']) for bb in batch], dim=0).float()\n\n    if self.model_name == \'GPNN\':\n      #########################################################################\n      # GPNN\n      # N.B.: one can perform graph partition offline to speed up\n      #########################################################################      \n      # graph Laplacian of multi-graph: shape (B, N, N, E)\n      L_multi = np.stack(\n          [\n              np.pad(\n                  bb[\'L_multi\'], ((0, pad_node_size[ii]),\n                                  (0, pad_node_size[ii]), (0, 0)),\n                  \'constant\',\n                  constant_values=0.0) for ii, bb in enumerate(batch)\n          ],\n          axis=0)\n\n      # graph Laplacian of simple graph: shape (B, N, N, 1)\n      L_simple = np.stack(\n          [\n              np.expand_dims(\n                  np.pad(\n                      bb[\'L_simple_4\'], (0, pad_node_size[ii]),\n                      \'constant\',\n                      constant_values=0.0),\n                  axis=3) for ii, bb in enumerate(batch)\n          ],\n          axis=0)\n\n      L = np.concatenate([L_simple, L_multi], axis=3)\n      data[\'L\'] = torch.from_numpy(L).float()\n\n      # graph partition\n      L_cluster, L_cut = [], []\n\n      for ii in range(batch_size):\n        node_label = spectral_clustering(L_simple[ii, :, :, 0], self.config.model.num_partition)\n        \n        # Laplacian of clusters and cut\n        L_cluster_tmp, L_cut_tmp = get_L_cluster_cut(L_simple[ii, :, :, 0], node_label)\n\n        L_cluster += [L_cluster_tmp]\n        L_cut += [L_cut_tmp]\n\n      data[\'L_cluster\'] = torch.from_numpy(np.stack(L_cluster, axis=0)).float()\n      data[\'L_cut\'] = torch.from_numpy(np.stack(L_cut, axis=0)).float()\n    elif self.model_name == \'GraphSAGE\':\n      #########################################################################\n      # GraphSAGE\n      #########################################################################\n      # N.B.: adjacency mat of GraphSAGE is asymmetric\n      nonempty_mask = np.zeros((batch_size, batch_node_size, 1))\n      nn_idx = np.zeros((batch_size, batch_node_size, self.num_sample_neighbors,\n                         self.num_edgetype + 1))\n\n      for ii in range(batch_size):\n        for jj in range(self.num_edgetype + 1):\n          if jj == 0:\n            tmp_L = batch[ii][\'L_simple_4\']\n          else:\n            tmp_L = batch[ii][\'L_multi\'][:, :, jj - 1]\n\n          for nn in range(tmp_L.shape[0]):\n            nn_list = np.nonzero(tmp_L[nn, :])[0]\n\n            if len(nn_list) >= self.num_sample_neighbors:\n              nn_idx[ii, nn, :, jj] = self.npr.choice(\n                  nn_list, size=self.num_sample_neighbors, replace=False)\n              nonempty_mask[ii, nn] = 1\n            elif len(nn_list) > 0:\n              nn_idx[ii, nn, :, jj] = self.npr.choice(\n                  nn_list, size=self.num_sample_neighbors, replace=True)\n              nonempty_mask[ii, nn] = 1\n\n      data[\'nn_idx\'] = torch.from_numpy(nn_idx).long()\n      data[\'nonempty_mask\'] = torch.from_numpy(nonempty_mask).float()\n    elif self.model_name == \'GAT\':\n      #########################################################################\n      # GAT\n      #########################################################################\n      # graph Laplacian of multi-graph: shape (B, N, N, E)\n      L_multi = np.stack(\n          [\n              np.pad(\n                  bb[\'L_multi\'], ((0, pad_node_size[ii]),\n                                  (0, pad_node_size[ii]), (0, 0)),\n                  \'constant\',\n                  constant_values=0.0) for ii, bb in enumerate(batch)\n          ],\n          axis=0)\n\n      # graph Laplacian of simple graph: shape (B, N, N, 1)\n      L_simple = np.stack(\n          [\n              np.expand_dims(\n                  np.pad(\n                      bb[\'L_simple_4\'], (0, pad_node_size[ii]),\n                      \'constant\',\n                      constant_values=0.0),\n                  axis=3) for ii, bb in enumerate(batch)\n          ],\n          axis=0)\n\n      L = np.concatenate([L_simple, L_multi], axis=3)\n\n      # trick of graph attention networks\n      def adj_to_bias(adj, sizes, nhood=1):\n        nb_graphs = adj.shape[0]\n        mt = np.empty(adj.shape)\n        for g in range(nb_graphs):\n          mt[g] = np.eye(adj.shape[1])\n          for _ in range(nhood):\n            mt[g] = np.matmul(mt[g], (adj[g] + np.eye(adj.shape[1])))\n          for i in range(sizes[g]):\n            for j in range(sizes[g]):\n              if mt[g][i][j] > 0.0:\n                mt[g][i][j] = 1.0\n        return -1e9 * (1.0 - mt)\n\n      L_new = []\n      for ii in range(batch_size):\n        L_new += [\n            np.transpose(\n                adj_to_bias(\n                    np.transpose(L[ii, :, :, :], (2, 0, 1)),\n                    [batch_node_size] * L.shape[3]), (1, 2, 0))\n        ]\n\n      data[\'L\'] = torch.from_numpy(np.stack(L_new, axis=0)).float()\n    else:\n      #########################################################################\n      # All other models\n      #########################################################################      \n      # graph Laplacian of multi-graph: shape (B, N, N, E)\n      L_multi = torch.stack([\n          torch.from_numpy(\n              np.pad(\n                  bb[\'L_multi\'], ((0, pad_node_size[ii]),\n                                  (0, pad_node_size[ii]), (0, 0)),\n                  \'constant\',\n                  constant_values=0.0)) for ii, bb in enumerate(batch)\n      ]).float()\n\n      # graph Laplacian of simple graph: shape (B, N, N, 1)\n      L_simple_key = \'L_simple_4\'\n      if self.model_name == \'DCNN\':\n        L_simple_key = \'L_simple_7\'\n      elif self.model_name in [\'ChebyNet\']:\n        L_simple_key = \'L_simple_6\'\n\n      if self.model_name == \'ChebyNet\':\n        L_simple = torch.stack([\n            torch.from_numpy(\n                np.expand_dims(\n                    np.pad(\n                        -bb[L_simple_key], (0, pad_node_size[ii]),\n                        \'constant\',\n                        constant_values=0.0),\n                    axis=3)) for ii, bb in enumerate(batch)\n        ]).float()\n      else:\n        L_simple = torch.stack([\n            torch.from_numpy(\n                np.expand_dims(\n                    np.pad(\n                        bb[L_simple_key], (0, pad_node_size[ii]),\n                        \'constant\',\n                        constant_values=0.0),\n                    axis=3)) for ii, bb in enumerate(batch)\n        ]).float()\n\n      data[\'L\'] = torch.cat([L_simple, L_multi], dim=3)\n\n      # eigenvalues & eigenvectors of simple graph\n      if self.use_eigs:\n        eigs, eig_vecs = [], []\n        for ii, bb in enumerate(batch):\n          pad_eigs_len = self.num_eigs - len(bb[\'D_simple\'])\n          eigs += [\n              bb[\'D_simple\'][:self.num_eigs] if pad_eigs_len <= 0 else np.pad(\n                  bb[\'D_simple\'], (0, pad_eigs_len),\n                  \'constant\',\n                  constant_values=0.0)\n          ]\n\n          # pad eigenvectors\n          pad_eig_vec = np.pad(\n              bb[\'V_simple\'], ((0, pad_node_size[ii]), (0, 0)),\n              \'constant\',\n              constant_values=0.0)\n\n          eig_vecs += [\n              pad_eig_vec[:, :self.num_eigs] if pad_eigs_len <= 0 else np.pad(\n                  pad_eig_vec, ((0, 0), (0, pad_eigs_len)),\n                  \'constant\',\n                  constant_values=0.0)\n          ]\n\n        data[\'D\'] = torch.stack([torch.from_numpy(ee) for ee in eigs]).float()\n        data[\'V\'] = torch.stack(\n            [torch.from_numpy(vv) for vv in eig_vecs]).float()\n\n    return data\n'"
dataset/qm8.py,22,"b'import os\nimport glob\nimport torch\nimport pickle\nimport numpy as np\nfrom utils.spectral_graph_partition import *\n\n__all__ = [\'QM8Data\']\n\n\nclass QM8Data(object):\n\n  def __init__(self, config, split=\'train\'):\n    assert split == \'train\' or split == \'dev\' or split == \'test\', ""no such split""\n    self.split = split\n    self.config = config\n    self.seed = config.seed\n    self.npr = np.random.RandomState(self.seed)\n    self.data_path = config.dataset.data_path\n    self.num_edgetype = config.dataset.num_bond_type\n    self.model_name = config.model.name\n    self.use_eigs = True if hasattr(config.model, \'num_eig_vec\') else False\n    if self.use_eigs:\n      self.num_eigs = config.model.num_eig_vec\n\n    if self.model_name == \'GraphSAGE\':\n      self.num_sample_neighbors = config.model.num_sample_neighbors\n\n    self.train_data_files = glob.glob(\n        os.path.join(self.data_path, \'QM8_preprocess_train_*.p\'))\n    self.dev_data_files = glob.glob(\n        os.path.join(self.data_path, \'QM8_preprocess_dev_*.p\'))\n    self.test_data_files = glob.glob(\n        os.path.join(self.data_path, \'QM8_preprocess_test_*.p\'))\n\n    self.num_train = len(self.train_data_files)\n    self.num_dev = len(self.dev_data_files)\n    self.num_test = len(self.test_data_files)\n    self.num_graphs = self.num_train + self.num_dev + self.num_test\n\n  def __getitem__(self, index):\n    if self.split == \'train\':\n      return pickle.load(open(self.train_data_files[index], \'rb\'))\n    elif self.split == \'dev\':\n      return pickle.load(open(self.dev_data_files[index], \'rb\'))\n    else:\n      return pickle.load(open(self.test_data_files[index], \'rb\'))\n\n  def __len__(self):\n    if self.split == \'train\':\n      return self.num_train\n    elif self.split == \'dev\':\n      return self.num_dev\n    else:\n      return self.num_test\n\n  def collate_fn(self, batch):\n    """"""\n      Collate function for mini-batch\n      N.B.: we pad all samples to the maximum of the mini-batch\n    """"""\n    assert isinstance(batch, list)\n\n    data = {}\n    batch_size = len(batch)\n    node_size = [bb[\'node_feat\'].shape[0] for bb in batch]\n    batch_node_size = max(node_size)  # value -> N\n    pad_node_size = [batch_node_size - nn for nn in node_size]\n\n    # pad feature: shape (B, N)\n    data[\'node_feat\'] = torch.stack([\n        torch.from_numpy(\n            np.pad(\n                bb[\'node_feat\'], (0, pad_node_size[ii]),\n                \'constant\',\n                constant_values=0.0)) for ii, bb in enumerate(batch)\n    ]).long()\n\n    # binary mask: shape (B, N)\n    data[\'node_mask\'] = torch.stack([\n        torch.from_numpy(\n            np.pad(\n                np.ones(node_size[ii]), (0, pad_node_size[ii]),\n                \'constant\',\n                constant_values=0.0)) for ii, bb in enumerate(batch)\n    ]).byte()\n\n    # label: shape (B, O)\n    data[\'label\'] = torch.cat(\n        [torch.from_numpy(bb[\'label\']) for bb in batch], dim=0).float()\n\n    if self.model_name == \'GPNN\':\n      #########################################################################\n      # GPNN\n      # N.B.: one can perform graph partition offline to speed up\n      #########################################################################      \n      # graph Laplacian of multi-graph: shape (B, N, N, E)\n      L_multi = np.stack(\n          [\n              np.pad(\n                  bb[\'L_multi\'], ((0, pad_node_size[ii]),\n                                  (0, pad_node_size[ii]), (0, 0)),\n                  \'constant\',\n                  constant_values=0.0) for ii, bb in enumerate(batch)\n          ],\n          axis=0)\n\n      # graph Laplacian of simple graph: shape (B, N, N, 1)\n      L_simple = np.stack(\n          [\n              np.expand_dims(\n                  np.pad(\n                      bb[\'L_simple_4\'], (0, pad_node_size[ii]),\n                      \'constant\',\n                      constant_values=0.0),\n                  axis=3) for ii, bb in enumerate(batch)\n          ],\n          axis=0)\n\n      L = np.concatenate([L_simple, L_multi], axis=3)\n      data[\'L\'] = torch.from_numpy(L).float()\n\n      # graph partition\n      L_cluster, L_cut = [], []\n\n      for ii in range(batch_size):\n        node_label = spectral_clustering(L_simple[ii, :, :, 0], self.config.model.num_partition)\n        \n        # Laplacian of clusters and cut\n        L_cluster_tmp, L_cut_tmp = get_L_cluster_cut(L_simple[ii, :, :, 0], node_label)\n\n        L_cluster += [L_cluster_tmp]\n        L_cut += [L_cut_tmp]\n\n      data[\'L_cluster\'] = torch.from_numpy(np.stack(L_cluster, axis=0)).float()\n      data[\'L_cut\'] = torch.from_numpy(np.stack(L_cut, axis=0)).float()\n    elif self.model_name == \'GraphSAGE\':\n      #########################################################################\n      # GraphSAGE\n      #########################################################################\n      # N.B.: adjacency mat of GraphSAGE is asymmetric\n      nonempty_mask = np.zeros((batch_size, batch_node_size, 1))\n      nn_idx = np.zeros((batch_size, batch_node_size, self.num_sample_neighbors,\n                         self.num_edgetype + 1))\n\n      for ii in range(batch_size):\n        for jj in range(self.num_edgetype + 1):\n          if jj == 0:\n            tmp_L = batch[ii][\'L_simple_4\']\n          else:\n            tmp_L = batch[ii][\'L_multi\'][:, :, jj - 1]\n\n          for nn in range(tmp_L.shape[0]):\n            nn_list = np.nonzero(tmp_L[nn, :])[0]\n\n            if len(nn_list) >= self.num_sample_neighbors:\n              nn_idx[ii, nn, :, jj] = self.npr.choice(\n                  nn_list, size=self.num_sample_neighbors, replace=False)\n              nonempty_mask[ii, nn] = 1\n            elif len(nn_list) > 0:\n              nn_idx[ii, nn, :, jj] = self.npr.choice(\n                  nn_list, size=self.num_sample_neighbors, replace=True)\n              nonempty_mask[ii, nn] = 1\n\n      data[\'nn_idx\'] = torch.from_numpy(nn_idx).long()\n      data[\'nonempty_mask\'] = torch.from_numpy(nonempty_mask).float()\n    elif self.model_name == \'GAT\':\n      #########################################################################\n      # GAT\n      #########################################################################\n      # graph Laplacian of multi-graph: shape (B, N, N, E)\n      L_multi = np.stack(\n          [\n              np.pad(\n                  bb[\'L_multi\'], ((0, pad_node_size[ii]),\n                                  (0, pad_node_size[ii]), (0, 0)),\n                  \'constant\',\n                  constant_values=0.0) for ii, bb in enumerate(batch)\n          ],\n          axis=0)\n\n      # graph Laplacian of simple graph: shape (B, N, N, 1)\n      L_simple = np.stack(\n          [\n              np.expand_dims(\n                  np.pad(\n                      bb[\'L_simple_4\'], (0, pad_node_size[ii]),\n                      \'constant\',\n                      constant_values=0.0),\n                  axis=3) for ii, bb in enumerate(batch)\n          ],\n          axis=0)\n\n      L = np.concatenate([L_simple, L_multi], axis=3)\n\n      # trick of graph attention networks\n      def adj_to_bias(adj, sizes, nhood=1):\n        nb_graphs = adj.shape[0]\n        mt = np.empty(adj.shape)\n        for g in range(nb_graphs):\n          mt[g] = np.eye(adj.shape[1])\n          for _ in range(nhood):\n            mt[g] = np.matmul(mt[g], (adj[g] + np.eye(adj.shape[1])))\n          for i in range(sizes[g]):\n            for j in range(sizes[g]):\n              if mt[g][i][j] > 0.0:\n                mt[g][i][j] = 1.0\n        return -1e9 * (1.0 - mt)\n\n      L_new = []\n      for ii in range(batch_size):\n        L_new += [\n            np.transpose(\n                adj_to_bias(\n                    np.transpose(L[ii, :, :, :], (2, 0, 1)),\n                    [batch_node_size] * L.shape[3]), (1, 2, 0))\n        ]\n\n      data[\'L\'] = torch.from_numpy(np.stack(L_new, axis=0)).float()\n    else:\n      #########################################################################\n      # All other models\n      #########################################################################      \n      # graph Laplacian of multi-graph: shape (B, N, N, E)\n      L_multi = torch.stack([\n          torch.from_numpy(\n              np.pad(\n                  bb[\'L_multi\'], ((0, pad_node_size[ii]),\n                                  (0, pad_node_size[ii]), (0, 0)),\n                  \'constant\',\n                  constant_values=0.0)) for ii, bb in enumerate(batch)\n      ]).float()\n\n      # graph Laplacian of simple graph: shape (B, N, N, 1)\n      L_simple_key = \'L_simple_4\'\n      if self.model_name == \'DCNN\':\n        L_simple_key = \'L_simple_7\'\n      elif self.model_name in [\'ChebyNet\']:\n        L_simple_key = \'L_simple_6\'\n\n      if self.model_name == \'ChebyNet\':\n        L_simple = torch.stack([\n            torch.from_numpy(\n                np.expand_dims(\n                    np.pad(\n                        -bb[L_simple_key], (0, pad_node_size[ii]),\n                        \'constant\',\n                        constant_values=0.0),\n                    axis=3)) for ii, bb in enumerate(batch)\n        ]).float()\n      else:\n        L_simple = torch.stack([\n            torch.from_numpy(\n                np.expand_dims(\n                    np.pad(\n                        bb[L_simple_key], (0, pad_node_size[ii]),\n                        \'constant\',\n                        constant_values=0.0),\n                    axis=3)) for ii, bb in enumerate(batch)\n        ]).float()\n\n      data[\'L\'] = torch.cat([L_simple, L_multi], dim=3)\n\n      # eigenvalues & eigenvectors of simple graph\n      if self.use_eigs:\n        eigs, eig_vecs = [], []\n        for ii, bb in enumerate(batch):\n          pad_eigs_len = self.num_eigs - len(bb[\'D_simple\'])\n          eigs += [\n              bb[\'D_simple\'][:self.num_eigs] if pad_eigs_len <= 0 else np.pad(\n                  bb[\'D_simple\'], (0, pad_eigs_len),\n                  \'constant\',\n                  constant_values=0.0)\n          ]\n\n          # pad eigenvectors\n          pad_eig_vec = np.pad(\n              bb[\'V_simple\'], ((0, pad_node_size[ii]), (0, 0)),\n              \'constant\',\n              constant_values=0.0)\n\n          eig_vecs += [\n              pad_eig_vec[:, :self.num_eigs] if pad_eigs_len <= 0 else np.pad(\n                  pad_eig_vec, ((0, 0), (0, pad_eigs_len)),\n                  \'constant\',\n                  constant_values=0.0)\n          ]\n\n        data[\'D\'] = torch.stack([torch.from_numpy(ee) for ee in eigs]).float()\n        data[\'V\'] = torch.stack(\n            [torch.from_numpy(vv) for vv in eig_vecs]).float()\n\n    return data\n'"
model/__init__.py,0,b'from model.set2set import *\nfrom model.ggnn import *\nfrom model.gpnn import *\nfrom model.gcn import *\nfrom model.gat import *\nfrom model.dcnn import *\nfrom model.mpnn import *\nfrom model.gcnfp import *\nfrom model.graph_sage import *\nfrom model.cheby_net import *\nfrom model.lanczos_net import *\nfrom model.ada_lanczos_net import *\nfrom model.lanczos_net_general import *'
model/ada_lanczos_net.py,39,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom utils.data_helper import check_dist\n\nEPS = float(np.finfo(np.float32).eps)\n__all__ = [\'AdaLanczosNet\']\n\n\nclass AdaLanczosNet(nn.Module):\n\n  def __init__(self, config):\n    super(AdaLanczosNet, self).__init__()\n    self.config = config\n    self.input_dim = config.model.input_dim    \n    self.hidden_dim = config.model.hidden_dim\n    self.output_dim = config.model.output_dim\n    self.num_layer = config.model.num_layer\n    self.num_atom = config.dataset.num_atom\n    self.num_edgetype = config.dataset.num_bond_type\n    self.dropout = config.model.dropout if hasattr(config.model,\n                                                   \'dropout\') else 0.0\n    self.short_diffusion_dist = check_dist(config.model.short_diffusion_dist)\n    self.long_diffusion_dist = check_dist(config.model.long_diffusion_dist)\n    self.max_short_diffusion_dist = max(\n        self.short_diffusion_dist) if self.short_diffusion_dist else None\n    self.max_long_diffusion_dist = max(\n        self.long_diffusion_dist) if self.long_diffusion_dist else None\n    self.num_scale_short = len(self.short_diffusion_dist)\n    self.num_scale_long = len(self.long_diffusion_dist)\n    self.num_eig_vec = config.model.num_eig_vec\n    self.spectral_filter_kind = config.model.spectral_filter_kind\n    self.use_reorthogonalization = config.model.use_reorthogonalization if hasattr(\n        config, \'use_reorthogonalization\') else True\n    self.use_power_iteration_cap = config.model.use_power_iteration_cap if hasattr(\n        config, \'use_power_iteration_cap\') else True\n\n    self.input_dim = self.num_atom\n\n    dim_list = [self.input_dim] + self.hidden_dim + [self.output_dim]\n    self.filter = nn.ModuleList([\n        nn.Linear(dim_list[tt] * (\n            self.num_scale_short + self.num_scale_long + self.num_edgetype + 1),\n                  dim_list[tt + 1]) for tt in range(self.num_layer)\n    ] + [nn.Linear(dim_list[-2], dim_list[-1])])\n\n    self.embedding = nn.Embedding(self.num_atom, self.input_dim)\n\n    # spectral filters\n    if self.spectral_filter_kind == \'MLP\' and self.num_scale_long > 0:\n      # N.B.: one can modify the filter size based on GPU memory consumption\n      self.spectral_filter = nn.ModuleList([\n          nn.Sequential(*[\n              nn.Linear(self.num_eig_vec * self.num_eig_vec * self.num_scale_long, 4096),\n              nn.ReLU(),\n              nn.Linear(4096, 4096),\n              nn.ReLU(),\n              nn.Linear(4096, 4096),              \n              nn.ReLU(),              \n              nn.Linear(4096, self.num_eig_vec * self.num_eig_vec * self.num_scale_long)\n          ]) for _ in range(self.num_layer)\n      ])\n\n    # attention\n    self.att_func = nn.Sequential(*[nn.Linear(dim_list[-2], 1), nn.Sigmoid()])\n\n    if config.model.loss == \'CrossEntropy\':\n      self.loss_func = torch.nn.CrossEntropyLoss()\n    elif config.model.loss == \'MSE\':\n      self.loss_func = torch.nn.MSELoss()\n    elif config.model.loss == \'L1\':\n      self.loss_func = torch.nn.L1Loss()\n    else:\n      raise ValueError(""Non-supported loss function!"")\n\n    self._init_param()\n\n  def _init_param(self):\n    for ff in self.filter:\n      if isinstance(ff, nn.Linear):\n        nn.init.xavier_uniform_(ff.weight.data)\n        if ff.bias is not None:\n          ff.bias.data.zero_()\n\n    for ff in self.att_func:\n      if isinstance(ff, nn.Linear):\n        nn.init.xavier_uniform_(ff.weight.data)\n        if ff.bias is not None:\n          ff.bias.data.zero_()\n\n    if self.spectral_filter_kind == \'MLP\' and self.num_scale_long > 0:\n      for f in self.spectral_filter:\n        for ff in f:\n          if isinstance(ff, nn.Linear):\n            nn.init.xavier_uniform_(ff.weight.data)\n            if ff.bias is not None:\n              ff.bias.data.zero_()\n\n  def _get_graph_laplacian(self, node_feat, adj_mask):\n    """""" Compute graph Laplacian\n\n      Args:\n        node_feat: float tensor, shape B X N X D\n        adj_mask: float tensor, shape B X N X N, binary mask, should contain self-loop\n\n      Returns:\n        L: float tensor, shape B X N X N\n    """"""\n    batch_size = node_feat.shape[0]\n    num_node = node_feat.shape[1]\n    dim_feat = node_feat.shape[2]\n\n    # compute pairwise distance\n    idx_row, idx_col = np.meshgrid(range(num_node), range(num_node))\n    idx_row, idx_col = torch.Tensor(idx_row.reshape(-1)).long().to(node_feat.device), torch.Tensor(\n        idx_col.reshape(-1)).long().to(node_feat.device)\n\n    diff = node_feat[:, idx_row, :] - node_feat[:, idx_col, :]  # shape B X N^2 X D\n    dist2 = (diff * diff).sum(dim=2)  # shape B X N^2\n    \n    # sigma2, _ = torch.median(dist2, dim=1, keepdim=True) # median is sometimes 0\n    # sigma2 = sigma2 + 1.0e-7\n\n    sigma2 = torch.mean(dist2, dim=1, keepdim=True)\n\n    A = torch.exp(-dist2 / sigma2)  # shape B X N^2\n    A = A.reshape(batch_size, num_node, num_node) * adj_mask  # shape B X N X N\n    row_sum = torch.sum(A, dim=2, keepdim=True)\n    pad_row_sum = torch.zeros_like(row_sum)\n    pad_row_sum[row_sum == 0.0] = 1.0    \n    alpha = 0.5\n    D = 1.0 / (row_sum + pad_row_sum).pow(alpha)  # shape B X N X 1\n    L = D * A * D.transpose(1, 2)  # shape B X N X N\n\n    return L\n\n  def _lanczos_layer(self, A, mask=None):\n    """""" Lanczos layer for symmetric matrix A\n    \n      Args:\n        A: float tensor, shape B X N X N\n        mask: float tensor, shape B X N\n\n      Returns:\n        T: float tensor, shape B X K X K\n        Q: float tensor, shape B X N X K\n    """"""\n    batch_size = A.shape[0]\n    num_node = A.shape[1]\n    lanczos_iter = min(num_node, self.num_eig_vec)\n\n    # initialization\n    alpha = [None] * (lanczos_iter + 1)\n    beta = [None] * (lanczos_iter + 1)\n    Q = [None] * (lanczos_iter + 2)\n\n    beta[0] = torch.zeros(batch_size, 1, 1).to(A.device)\n    Q[0] = torch.zeros(batch_size, num_node, 1).to(A.device)\n    Q[1] = torch.randn(batch_size, num_node, 1).to(A.device)\n\n    if mask is not None:\n      mask = mask.unsqueeze(dim=2).float()\n      Q[1] = Q[1] * mask\n\n    Q[1] = Q[1] / torch.norm(Q[1], 2, dim=1, keepdim=True)\n\n    # Lanczos loop\n    lb = 1.0e-4\n    valid_mask = []\n    for ii in range(1, lanczos_iter + 1):\n      z = torch.bmm(A, Q[ii])  # shape B X N X 1\n      alpha[ii] = torch.sum(Q[ii] * z, dim=1, keepdim=True)  # shape B X 1 X 1\n      z = z - alpha[ii] * Q[ii] - beta[ii - 1] * Q[ii - 1]  # shape B X N X 1\n\n      if self.use_reorthogonalization and ii > 1:\n        # N.B.: Gram Schmidt does not bring significant difference of performance\n        def _gram_schmidt(xx, tt):\n          # xx shape B X N X 1\n          for jj in range(1, tt):\n            xx = xx - torch.sum(\n                xx * Q[jj], dim=1, keepdim=True) / (\n                    torch.sum(Q[jj] * Q[jj], dim=1, keepdim=True) + EPS) * Q[jj]\n          return xx\n\n        # do Gram Schmidt process twice\n        for _ in range(2):\n          z = _gram_schmidt(z, ii)\n\n      beta[ii] = torch.norm(z, p=2, dim=1, keepdim=True)  # shape B X 1 X 1\n\n      # N.B.: once lanczos fails at ii-th iteration, all following iterations\n      # are doomed to fail\n      tmp_valid_mask = (beta[ii] >= lb).float()  # shape\n      if ii == 1:\n        valid_mask += [tmp_valid_mask]\n      else:\n        valid_mask += [valid_mask[-1] * tmp_valid_mask]\n\n      # early stop\n      Q[ii + 1] = (z * valid_mask[-1]) / (beta[ii] + EPS)\n\n    # get alpha & beta\n    alpha = torch.cat(alpha[1:], dim=1).squeeze(dim=2)  # shape B X T\n    beta = torch.cat(beta[1:-1], dim=1).squeeze(dim=2)  # shape B X (T-1)\n\n    valid_mask = torch.cat(valid_mask, dim=1).squeeze(dim=2)  # shape B X T\n    idx_mask = torch.sum(valid_mask, dim=1).long()\n    if mask is not None:\n      idx_mask = torch.min(idx_mask, torch.sum(mask, dim=1).squeeze().long())\n\n    for ii in range(batch_size):\n      if idx_mask[ii] < valid_mask.shape[1]:\n        valid_mask[ii, idx_mask[ii]:] = 0.0\n\n    # remove spurious columns\n    alpha = alpha * valid_mask\n    beta = beta * valid_mask[:, :-1]\n\n    T = []\n    for ii in range(batch_size):\n      T += [\n          torch.diag(alpha[ii]) + torch.diag(beta[ii], diagonal=1) + torch.diag(\n              beta[ii], diagonal=-1)\n      ]\n\n    T = torch.stack(T, dim=0)  # shape B X T X T\n    Q = torch.cat(Q[1:-1], dim=2)  # shape B X N X T\n    Q_mask = valid_mask.unsqueeze(dim=1).repeat(1, Q.shape[1], 1)\n\n    # remove spurious rows\n    for ii in range(batch_size):\n      if idx_mask[ii] < Q_mask.shape[1]:\n        Q_mask[ii, idx_mask[ii]:, :] = 0.0\n\n    Q = Q * Q_mask\n\n    # pad 0 when necessary\n    if lanczos_iter < self.num_eig_vec:\n      pad = (0, self.num_eig_vec - lanczos_iter, 0,\n             self.num_eig_vec - lanczos_iter)\n      T = F.pad(T, pad)\n      pad = (0, self.num_eig_vec - lanczos_iter)\n      Q = F.pad(Q, pad)\n\n    return T, Q\n\n\n  def _get_spectral_filters(self, T, Q, layer_idx):\n    """""" Construct Spectral Filters based on Lanczos Outputs\n\n      Args:\n        T: shape B X K X K, tridiagonal matrix\n        Q: shape B X N X K, orthonormal matrix\n        layer_idx: int, index of layer\n\n      Returns:\n        L: shape B X N X N X num_scale\n    """"""\n    # multi-scale diffusion\n    L = []\n    T_list = []\n    TT = T\n\n    for ii in range(1, self.max_long_diffusion_dist + 1):\n      if ii in self.long_diffusion_dist:\n        T_list += [TT]\n\n      TT = torch.bmm(TT, T)  # shape B X K X K\n\n    # spectral filter\n    if self.spectral_filter_kind == \'MLP\':\n      DD = self.spectral_filter[layer_idx](torch.cat(T_list, dim=2).view(T.shape[0], -1))\n      DD = DD.view(T.shape[0], T.shape[1], T.shape[2], self.num_scale_long) # shape: B X K X K X C\n\n      # construct symmetric output\n      DD = (DD + DD.transpose(1, 2)) * 0.5\n\n      for ii in range(self.num_scale_long):\n        L += [Q.bmm(DD[:,:,:,ii]).bmm(Q.transpose(1, 2))]\n    else:\n      for ii in range(self.num_scale_long):\n        L += [Q.bmm(T_list[ii]).bmm(Q.transpose(1, 2))]  \n    \n    return torch.stack(L, dim=3)\n\n\n  def forward(self, node_feat, L, label=None, mask=None):\n    """"""\n      shape parameters:\n        batch size = B\n        embedding dim = D\n        max number of nodes within one mini batch = N\n        number of edge types = E\n        number of predicted properties = P\n      \n      Args:\n        node_feat: long tensor, shape B X N\n        L: float tensor, shape B X N X N X (E + 1)\n        label: float tensor, shape B X P\n        mask: float tensor, shape B X N\n    """"""\n    batch_size = node_feat.shape[0]\n    num_node = node_feat.shape[1]\n    state = self.embedding(node_feat)  # shape: B X N X D\n    \n    if self.num_scale_long > 0:\n      # compute graph Laplacian for simple graph\n      adj = torch.zeros_like(L[:, :, :, 0])  # get L of the simple graph\n      adj[L[:, :, :, 0] != 0.0] = 1.0\n      Le = self._get_graph_laplacian(state, adj)\n      \n      # Lanczos Iteration\n      T, Q = self._lanczos_layer(Le, mask)\n\n    ###########################################################################\n    # Graph Convolution\n    ###########################################################################\n    # propagation    \n    for tt in range(self.num_layer):\n      msg = []\n\n      if self.num_scale_long > 0:\n        Lf = self._get_spectral_filters(T, Q, tt)\n\n      # short diffusion\n      if self.num_scale_short > 0:\n        tmp_state = state\n        for ii in range(1, self.max_short_diffusion_dist + 1):\n          tmp_state = torch.bmm(L[:, :, :, 0], tmp_state)\n          if ii in self.short_diffusion_dist:\n            msg += [tmp_state]\n\n      # long diffusion\n      if self.num_scale_long > 0:\n        for ii in range(self.num_scale_long):\n          msg += [torch.bmm(Lf[:, :, :, ii], state)]  # shape: B X N X D\n\n      # edge type\n      for ii in range(self.num_edgetype + 1):\n        msg += [torch.bmm(L[:, :, :, ii], state)]  # shape: B X N X D\n\n      # msg += [Q]\n      msg = torch.cat(msg, dim=2).view(num_node * batch_size, -1)\n      state = F.relu(self.filter[tt](msg)).view(batch_size, num_node, -1)\n      state = F.dropout(state, self.dropout, training=self.training)\n\n    # output\n    state = state.view(batch_size * num_node, -1)\n    y = self.filter[-1](state)  # shape: BN X 1\n    att_weight = self.att_func(state)  # shape: BN X 1\n    y = (att_weight * y).view(batch_size, num_node, -1)\n\n    score = []\n    if mask is not None:\n      for bb in range(batch_size):\n        score += [torch.mean(y[bb, mask[bb], :], dim=0)]\n    else:\n      for bb in range(batch_size):\n        score += [torch.mean(y[bb, :, :], dim=0)]\n\n    score = torch.stack(score)\n\n    if label is not None:\n      return score, self.loss_func(score, label)\n    else:\n      return score\n'"
model/cheby_net.py,12,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n__all__ = [\'ChebyNet\']\n\n\nclass ChebyNet(nn.Module):\n  """""" Chebyshev Network, see reference below for more information\n\n      Defferrard, M., Bresson, X. and Vandergheynst, P., 2016.\n      Convolutional neural networks on graphs with fast localized spectral\n      filtering. In NIPS.\n  """"""\n\n  def __init__(self, config):\n    super(ChebyNet, self).__init__()\n    self.config = config\n    self.input_dim = config.model.input_dim\n    self.hidden_dim = config.model.hidden_dim\n    self.output_dim = config.model.output_dim\n    self.num_layer = config.model.num_layer\n    self.polynomial_order = config.model.polynomial_order\n    self.num_atom = config.dataset.num_atom\n    self.num_edgetype = config.dataset.num_bond_type\n    self.dropout = config.model.dropout if hasattr(config.model,\n                                                   \'dropout\') else 0.0\n\n    dim_list = [self.input_dim] + self.hidden_dim + [self.output_dim]\n    self.filter = nn.ModuleList([\n        nn.Linear(dim_list[tt] *\n                  (self.polynomial_order + self.num_edgetype + 1),\n                  dim_list[tt + 1]) for tt in range(self.num_layer)\n    ] + [nn.Linear(dim_list[-2], dim_list[-1])])\n\n    self.embedding = nn.Embedding(self.num_atom, self.input_dim)\n\n    # attention\n    self.att_func = nn.Sequential(*[nn.Linear(dim_list[-2], 1), nn.Sigmoid()])\n\n    if config.model.loss == \'CrossEntropy\':\n      self.loss_func = torch.nn.CrossEntropyLoss()\n    elif config.model.loss == \'MSE\':\n      self.loss_func = torch.nn.MSELoss()\n    elif config.model.loss == \'L1\':\n      self.loss_func = torch.nn.L1Loss()\n    else:\n      raise ValueError(""Non-supported loss function!"")\n\n    self._init_param()\n\n  def _init_param(self):\n    for ff in self.filter:\n      if isinstance(ff, nn.Linear):\n        nn.init.xavier_uniform_(ff.weight.data)\n        if ff.bias is not None:\n          ff.bias.data.zero_()\n\n    for ff in self.att_func:\n      if isinstance(ff, nn.Linear):\n        nn.init.xavier_uniform_(ff.weight.data)\n        if ff.bias is not None:\n          ff.bias.data.zero_()\n\n  def forward(self, node_feat, L, label=None, mask=None):\n    """"""\n      shape parameters:\n        batch size = B\n        embedding dim = D\n        max number of nodes within one mini batch = N\n        number of edge types = E\n        number of predicted properties = P\n      \n      Args:\n        node_feat: long tensor, shape B X N\n        L: float tensor, shape B X N X N X (E + 1)\n        label: float tensor, shape B X P\n        mask: float tensor, shape B X N\n    """"""\n    batch_size = node_feat.shape[0]\n    num_node = node_feat.shape[1]\n    state = self.embedding(node_feat)  # shape: B X N X D\n\n    # propagation\n    # Note: we assume adj = 2 * L / lambda_max - I\n    for tt in range(self.num_layer):\n      state_scale = [None] * (self.polynomial_order + 1)\n      state_scale[-1] = state\n      state_scale[0] = torch.bmm(L[:, :, :, 0], state)\n      for kk in range(1, self.polynomial_order):\n        state_scale[kk] = 2.0 * torch.bmm(\n            L[:, :, :, 0], state_scale[kk - 1]) - state_scale[kk - 2]\n\n      msg = []\n      for ii in range(1, self.num_edgetype + 1):\n        msg += [torch.bmm(L[:, :, :, ii], state)]  # shape: B X N X D\n\n      msg = torch.cat(msg + state_scale, dim=2).view(num_node * batch_size, -1)\n      state = F.relu(self.filter[tt](msg)).view(batch_size, num_node, -1)\n      state = F.dropout(state, self.dropout, training=self.training)\n\n    # output\n    state = state.view(batch_size * num_node, -1)\n    y = self.filter[-1](state)  # shape: BN X 1\n    att_weight = self.att_func(state)  # shape: BN X 1\n    y = (att_weight * y).view(batch_size, num_node, -1)\n\n    score = []\n    if mask is not None:\n      for bb in range(batch_size):\n        score += [torch.mean(y[bb, mask[bb], :], dim=0)]\n    else:\n      for bb in range(batch_size):\n        score += [torch.mean(y[bb, :, :], dim=0)]\n\n    score = torch.stack(score)\n\n    if label is not None:\n      return score, self.loss_func(score, label)\n    else:\n      return score\n'"
model/dcnn.py,11,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n__all__ = [\'DCNN\']\n\n\nclass DCNN(nn.Module):\n  """""" Diffusion-convolutional neural networks,\n      see reference below for more information\n\n      Atwood, J. and Towsley, D., 2016.\n      Diffusion-convolutional neural networks. In NIPS.\n  """"""\n\n  def __init__(self, config):\n    super(DCNN, self).__init__()\n    self.config = config\n    self.input_dim = config.model.input_dim\n    self.hidden_dim = config.model.hidden_dim\n    self.output_dim = config.model.output_dim\n    self.num_layer = config.model.num_layer\n    self.diffusion_dist = config.model.diffusion_dist\n    self.num_scale = len(self.diffusion_dist)\n    self.max_dist = max(config.model.diffusion_dist)\n    self.num_atom = config.dataset.num_atom\n    self.num_edgetype = config.dataset.num_bond_type\n    self.dropout = config.model.dropout if hasattr(config.model,\n                                                   \'dropout\') else 0.0\n\n    dim_list = [self.input_dim] + self.hidden_dim + [self.output_dim]\n    self.filter = nn.ModuleList([\n        nn.Linear(dim_list[tt] * (self.num_scale + self.num_edgetype + 1),\n                  dim_list[tt + 1]) for tt in range(self.num_layer)\n    ] + [nn.Linear(dim_list[-2], dim_list[-1])])\n\n    self.embedding = nn.Embedding(self.num_atom, self.input_dim)\n\n    # attention\n    self.att_func = nn.Sequential(*[nn.Linear(dim_list[-2], 1), nn.Sigmoid()])\n\n    if config.model.loss == \'CrossEntropy\':\n      self.loss_func = torch.nn.CrossEntropyLoss()\n    elif config.model.loss == \'MSE\':\n      self.loss_func = torch.nn.MSELoss()\n    elif config.model.loss == \'L1\':\n      self.loss_func = torch.nn.L1Loss()\n    else:\n      raise ValueError(""Non-supported loss function!"")\n\n    self._init_param()\n\n  def _init_param(self):\n    for ff in self.filter:\n      if isinstance(ff, nn.Linear):\n        nn.init.xavier_uniform_(ff.weight.data)\n        if ff.bias is not None:\n          ff.bias.data.zero_()\n\n    for ff in self.att_func:\n      if isinstance(ff, nn.Linear):\n        nn.init.xavier_uniform_(ff.weight.data)\n        if ff.bias is not None:\n          ff.bias.data.zero_()\n\n  def forward(self, node_feat, L, label=None, mask=None):\n    """"""\n      shape parameters:\n        batch size = B\n        embedding dim = D\n        max number of nodes within one mini batch = N\n        number of edge types = E\n        number of predicted properties = P\n      \n      Args:\n        node_feat: long tensor, shape B X N\n        L: float tensor, shape B X N X N X (E + 1)\n        label: float tensor, shape B X P\n        mask: float tensor, shape B X N\n    """"""\n    batch_size = node_feat.shape[0]\n    num_node = node_feat.shape[1]\n    state = self.embedding(node_feat)  # shape: B X N X D\n\n    # propagation\n    for tt in range(self.num_layer):\n      state_scale = []\n      tmp_state = state\n      for ii in range(1, self.max_dist + 1):\n        tmp_state = torch.bmm(L[:, :, :, 0], tmp_state)\n        if ii in self.diffusion_dist:\n          state_scale += [tmp_state]\n\n      msg = []\n      for ii in range(self.num_edgetype + 1):\n        msg += [torch.bmm(L[:, :, :, ii], state)]  # shape: B X N X D\n\n      msg = torch.cat(msg + state_scale, dim=2).view(num_node * batch_size, -1)\n      state = F.relu(self.filter[tt](msg)).view(batch_size, num_node, -1)\n      state = F.dropout(state, self.dropout, training=self.training)\n\n    # output\n    state = state.view(batch_size * num_node, -1)\n    y = self.filter[-1](state)  # shape: BN X 1\n    att_weight = self.att_func(state)  # shape: BN X 1\n    y = (att_weight * y).view(batch_size, num_node, -1)\n\n    score = []\n    if mask is not None:\n      for bb in range(batch_size):\n        score += [torch.mean(y[bb, mask[bb], :], dim=0)]\n    else:\n      for bb in range(batch_size):\n        score += [torch.mean(y[bb, :, :], dim=0)]\n\n    score = torch.stack(score)\n\n    if label is not None:\n      return score, self.loss_func(score, label)\n    else:\n      return score\n'"
model/gat.py,14,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n__all__ = [\'GAT\']\n\n\nclass GAT(nn.Module):\n  """""" Graph Attention Networks,\n      see reference below for more information\n\n      Velickovic, P., Cucurull, G., Casanova, A., Romero, A., Lio, P.\n      and Bengio, Y., 2018. Graph attention networks. In ICLR.\n  """"""\n\n  def __init__(self, config):\n    super(GAT, self).__init__()\n    self.input_dim = config.model.input_dim\n    self.hidden_dim = config.model.hidden_dim\n    self.output_dim = config.model.output_dim\n    self.num_layer = config.model.num_layer\n    self.num_heads = config.model.num_heads\n    self.dropout = config.model.dropout if hasattr(config.model,\n                                                   \'dropout\') else 0.0\n    self.num_atom = config.dataset.num_atom\n    self.num_edgetype = config.dataset.num_bond_type\n\n    self.embedding = nn.Embedding(self.num_atom, self.input_dim)\n\n    dim_list = [self.input_dim] + self.hidden_dim + [self.output_dim]\n    self.filter = nn.ModuleList([\n        nn.ModuleList([\n            nn.ModuleList([\n                nn.Linear(\n                    dim_list[tt] *\n                    (int(tt == 0) + int(tt != 0) * self.num_heads[tt] *\n                     (self.num_edgetype + 1)),\n                    dim_list[tt + 1],\n                    bias=False) for _ in range(self.num_heads[tt])\n            ]) for _ in range(self.num_edgetype + 1)\n        ]) for tt in range(self.num_layer)\n    ])\n\n    self.att_net_1 = nn.ModuleList([\n        nn.ModuleList([\n            nn.ModuleList([\n                nn.Linear(dim_list[tt + 1], 1)\n                for _ in range(self.num_heads[tt])\n            ]) for _ in range(self.num_edgetype + 1)\n        ]) for tt in range(self.num_layer)\n    ])\n\n    self.att_net_2 = nn.ModuleList([\n        nn.ModuleList([\n            nn.ModuleList([\n                nn.Linear(dim_list[tt + 1], 1)\n                for _ in range(self.num_heads[tt])\n            ]) for _ in range(self.num_edgetype + 1)\n        ]) for tt in range(self.num_layer)\n    ])\n\n    self.state_bias = [[[None] * self.num_heads[tt]] * (self.num_edgetype + 1)\n                       for tt in range(self.num_layer)]\n    for tt in range(self.num_layer):\n      for jj in range(self.num_edgetype + 1):\n        for ii in range(self.num_heads[tt]):\n          self.state_bias[tt][jj][ii] = torch.nn.Parameter(\n              torch.zeros(dim_list[tt + 1]))\n          self.register_parameter(\'bias_{}_{}_{}\'.format(ii, jj, tt),\n                                  self.state_bias[tt][jj][ii])\n\n    # attention\n    self.att_func = nn.Sequential(*[nn.Linear(dim_list[-2], 1), nn.Sigmoid()])\n\n    self.output_func = nn.Sequential(*[nn.Linear(dim_list[-2], dim_list[-1])])\n\n    if config.model.loss == \'CrossEntropy\':\n      self.loss_func = torch.nn.CrossEntropyLoss()\n    elif config.model.loss == \'MSE\':\n      self.loss_func = torch.nn.MSELoss()\n    elif config.model.loss == \'L1\':\n      self.loss_func = torch.nn.L1Loss()\n    else:\n      raise ValueError(""Non-supported loss function!"")\n\n    self._init_param()\n\n  def _init_param(self):\n    for ff in self.att_func:\n      if isinstance(ff, nn.Linear):\n        nn.init.xavier_uniform_(ff.weight.data)\n        if ff.bias is not None:\n          ff.bias.data.zero_()\n\n    for ff in self.output_func:\n      if isinstance(ff, nn.Linear):\n        nn.init.xavier_uniform_(ff.weight.data)\n        if ff.bias is not None:\n          ff.bias.data.zero_()\n\n    for f in self.filter:\n      for ff in f:\n        for fff in ff:\n          if isinstance(fff, nn.Linear):\n            nn.init.xavier_uniform_(fff.weight.data)\n            if fff.bias is not None:\n              fff.bias.data.zero_()\n\n    for f in self.att_net_1:\n      for ff in f:\n        for fff in ff:\n          if isinstance(fff, nn.Linear):\n            nn.init.xavier_uniform_(fff.weight.data)\n            if fff.bias is not None:\n              fff.bias.data.zero_()\n\n    for f in self.att_net_2:\n      for ff in f:\n        for fff in ff:\n          if isinstance(fff, nn.Linear):\n            nn.init.xavier_uniform_(fff.weight.data)\n            if fff.bias is not None:\n              fff.bias.data.zero_()\n\n  def forward(self, node_feat, L, label=None, mask=None):\n    """"""\n      shape parameters:\n        batch size = B\n        embedding dim = D\n        max number of nodes within one mini batch = N\n        number of edge types = E\n        number of predicted properties = P\n      \n      Args:\n        node_feat: long tensor, shape B X N\n        L: float tensor, shape B X N X N X (E + 1)\n        label: float tensor, shape B X P\n        mask: float tensor, shape B X N\n    """"""\n    batch_size = node_feat.shape[0]\n    num_node = node_feat.shape[1]\n    state = self.embedding(node_feat)  # shape: B X N X D\n\n    # propagation\n    for tt in range(self.num_layer):\n      h = []\n      for jj in range(self.num_edgetype + 1):\n        for ii in range(self.num_heads[tt]):\n          state_head = F.dropout(state, self.dropout, training=self.training)\n          Wh = self.filter[tt][jj][ii](\n              state_head.view(batch_size * num_node, -1)).view(\n                  batch_size, num_node, -1)  # B X N X D\n          att_weights_1 = self.att_net_1[tt][jj][ii](Wh)  # B X N X 1\n          att_weights_2 = self.att_net_2[tt][jj][ii](Wh)  # B X N X 1\n          att_weights = att_weights_1 + att_weights_2.transpose(\n              1, 2)  # B X N X N dense matrix\n\n          att_weights = F.softmax(\n              F.leaky_relu(att_weights, negative_slope=0.2) + L[:, :, :, jj],\n              dim=1)\n          att_weights = F.dropout(\n              att_weights, self.dropout, training=self.training)  # B X N X N\n          Wh = F.dropout(Wh, self.dropout, training=self.training)  # B X N X D\n\n          if tt == self.num_layer - 1:\n            h += [\n                torch.bmm(att_weights, Wh) + self.state_bias[tt][jj][ii].view(\n                    1, 1, -1)\n            ]  # B X N X D\n          else:\n            h += [\n                F.elu(\n                    torch.bmm(att_weights, Wh) +\n                    self.state_bias[tt][jj][ii].view(1, 1, -1))\n            ]  # B X N X D\n\n      if tt == self.num_layer - 1:\n        state = torch.mean(torch.stack(h, dim=0), dim=0)  # B X N X D\n      else:\n        state = torch.cat(h, dim=2)\n\n    # output\n    state = state.view(batch_size * num_node, -1)\n    y = self.output_func(state)  # shape: BN X 1\n    att_weight = self.att_func(state)  # shape: BN X 1\n    y = (att_weight * y).view(batch_size, num_node, -1)\n\n    score = []\n    if mask is not None:\n      for bb in range(batch_size):\n        score += [torch.mean(y[bb, mask[bb], :], dim=0)]\n    else:\n      for bb in range(batch_size):\n        score += [torch.mean(y[bb, :, :], dim=0)]\n\n    score = torch.stack(score)\n\n    if label is not None:\n      return score, self.loss_func(score, label)\n    else:\n      return score\n'"
model/gcn.py,10,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n__all__ = [\'GCN\']\n\n\nclass GCN(nn.Module):\n  """""" Graph Convolutional Networks,\n      see reference below for more information\n\n      Kipf, T.N. and Welling, M., 2016.\n      Semi-supervised classification with graph convolutional networks.\n      arXiv preprint arXiv:1609.02907.\n  """"""\n\n  def __init__(self, config):\n    super(GCN, self).__init__()\n    self.config = config\n    self.input_dim = config.model.input_dim\n    self.hidden_dim = config.model.hidden_dim\n    self.output_dim = config.model.output_dim\n    self.num_layer = config.model.num_layer\n    self.num_atom = config.dataset.num_atom\n    self.num_edgetype = config.dataset.num_bond_type\n    self.dropout = config.model.dropout if hasattr(config.model,\n                                                   \'dropout\') else 0.0\n\n    dim_list = [self.input_dim] + self.hidden_dim + [self.output_dim]\n    self.filter = nn.ModuleList([\n        nn.Linear(dim_list[tt] * (self.num_edgetype + 1), dim_list[tt + 1])\n        for tt in range(self.num_layer)\n    ] + [nn.Linear(dim_list[-2], dim_list[-1])])\n\n    self.embedding = nn.Embedding(self.num_atom, self.input_dim)\n\n    # attention\n    self.att_func = nn.Sequential(*[nn.Linear(dim_list[-2], 1), nn.Sigmoid()])\n\n    if config.model.loss == \'CrossEntropy\':\n      self.loss_func = torch.nn.CrossEntropyLoss()\n    elif config.model.loss == \'MSE\':\n      self.loss_func = torch.nn.MSELoss()\n    elif config.model.loss == \'L1\':\n      self.loss_func = torch.nn.L1Loss()\n    else:\n      raise ValueError(""Non-supported loss function!"")\n\n    self._init_param()\n\n  def _init_param(self):\n    for ff in self.filter:\n      if isinstance(ff, nn.Linear):\n        nn.init.xavier_uniform_(ff.weight.data)\n        if ff.bias is not None:\n          ff.bias.data.zero_()\n\n    for ff in self.att_func:\n      if isinstance(ff, nn.Linear):\n        nn.init.xavier_uniform_(ff.weight.data)\n        if ff.bias is not None:\n          ff.bias.data.zero_()\n\n  def forward(self, node_feat, L, label=None, mask=None):\n    """"""\n      shape parameters:\n        batch size = B\n        embedding dim = D\n        max number of nodes within one mini batch = N\n        number of edge types = E\n        number of predicted properties = P\n      \n      Args:\n        node_feat: long tensor, shape B X N\n        L: float tensor, shape B X N X N X (E + 1)\n        label: float tensor, shape B X P\n        mask: float tensor, shape B X N\n    """"""\n    batch_size = node_feat.shape[0]\n    num_node = node_feat.shape[1]\n    state = self.embedding(node_feat)  # shape: B X N X D\n\n    # propagation\n    for tt in range(self.num_layer):\n      msg = []\n\n      for ii in range(self.num_edgetype + 1):\n        msg += [torch.bmm(L[:, :, :, ii], state)]  # shape: B X N X D\n\n      msg = torch.cat(msg, dim=2).view(num_node * batch_size, -1)\n      state = F.relu(self.filter[tt](msg)).view(batch_size, num_node, -1)\n      state = F.dropout(state, self.dropout, training=self.training)\n\n    # output\n    state = state.view(batch_size * num_node, -1)\n    y = self.filter[-1](state)  # shape: BN X 1\n    att_weight = self.att_func(state)  # shape: BN X 1\n    y = (att_weight * y).view(batch_size, num_node, -1)\n\n    score = []\n    if mask is not None:\n      for bb in range(batch_size):\n        score += [torch.mean(y[bb, mask[bb], :], dim=0)]\n    else:\n      for bb in range(batch_size):\n        score += [torch.mean(y[bb, :, :], dim=0)]\n\n    score = torch.stack(score)\n\n    if label is not None:\n      return score, self.loss_func(score, label)\n    else:\n      return score\n'"
model/gcnfp.py,10,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n__all__ = [\'GCNFP\']\n\n\nclass GCNFP(nn.Module):\n  """""" Graph Convolutional Networks for fingerprints,\n      see reference below for more information\n\n      Duvenaud, D.K., Maclaurin, D., Iparraguirre, J., Bombarell,\n      R., Hirzel, T., Aspuru-Guzik, A. and Adams, R.P., 2015.\n      Convolutional networks on graphs for learning molecular\n      fingerprints. In NIPS.\n\n      N.B.: the difference with GCN is, Duvenaud et. al. use\n      binary adjacency matrix rather than graph Laplacian\n  """"""\n\n  def __init__(self, config):\n    super(GCNFP, self).__init__()\n    self.config = config\n    self.input_dim = config.model.input_dim\n    self.hidden_dim = config.model.hidden_dim\n    self.output_dim = config.model.output_dim\n    self.num_layer = config.model.num_layer\n    self.num_atom = config.dataset.num_atom\n    self.num_edgetype = config.dataset.num_bond_type\n    self.dropout = config.model.dropout if hasattr(config.model,\n                                                   \'dropout\') else 0.0\n\n    dim_list = [self.input_dim] + self.hidden_dim + [self.output_dim]\n    self.filter = nn.ModuleList([\n        nn.Linear(dim_list[tt] * (self.num_edgetype + 1), dim_list[tt + 1])\n        for tt in range(self.num_layer)\n    ] + [nn.Linear(dim_list[-2], dim_list[-1])])\n\n    self.embedding = nn.Embedding(self.num_atom, self.input_dim)\n\n    # attention\n    self.att_func = nn.Sequential(*[nn.Linear(dim_list[-2], 1), nn.Sigmoid()])\n\n    if config.model.loss == \'CrossEntropy\':\n      self.loss_func = torch.nn.CrossEntropyLoss()\n    elif config.model.loss == \'MSE\':\n      self.loss_func = torch.nn.MSELoss()\n    elif config.model.loss == \'L1\':\n      self.loss_func = torch.nn.L1Loss()\n    else:\n      raise ValueError(""Non-supported loss function!"")\n\n    self._init_param()\n\n  def _init_param(self):\n    for ff in self.filter:\n      if isinstance(ff, nn.Linear):\n        nn.init.xavier_uniform_(ff.weight.data)\n        if ff.bias is not None:\n          ff.bias.data.zero_()\n\n    for ff in self.att_func:\n      if isinstance(ff, nn.Linear):\n        nn.init.xavier_uniform_(ff.weight.data)\n        if ff.bias is not None:\n          ff.bias.data.zero_()\n\n  def forward(self, node_feat, L, label=None, mask=None):\n    """"""\n      shape parameters:\n        batch size = B\n        embedding dim = D\n        max number of nodes within one mini batch = N\n        number of edge types = E\n        number of predicted properties = P\n      \n      Args:\n        node_feat: long tensor, shape B X N\n        L: float tensor, shape B X N X N X (E + 1)\n        label: float tensor, shape B X P\n        mask: float tensor, shape B X N\n    """"""\n    L[L != 0] = 1.0\n    batch_size = node_feat.shape[0]\n    num_node = node_feat.shape[1]\n    state = self.embedding(node_feat)  # shape: B X N X D\n\n    # propagation\n    for tt in range(self.num_layer):\n      msg = []\n      for ii in range(self.num_edgetype + 1):\n        msg += [torch.bmm(L[:, :, :, ii], state)]  # shape: B X N X D\n\n      msg = torch.cat(msg, dim=2).view(num_node * batch_size, -1)\n      state = F.relu(self.filter[tt](msg)).view(batch_size, num_node, -1)\n      state = F.dropout(state, self.dropout, training=self.training)\n\n    # output\n    state = state.view(batch_size * num_node, -1)\n    y = self.filter[-1](state)  # shape: BN X 1\n    att_weight = self.att_func(state)  # shape: BN X 1\n    y = (att_weight * y).view(batch_size, num_node, -1)\n\n    score = []\n    if mask is not None:\n      for bb in range(batch_size):\n        score += [torch.mean(y[bb, mask[bb], :], dim=0)]\n    else:\n      for bb in range(batch_size):\n        score += [torch.mean(y[bb, :, :], dim=0)]\n\n    score = torch.stack(score)\n\n    if label is not None:\n      return score, self.loss_func(score, label)\n    else:\n      return score\n'"
model/ggnn.py,12,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nEPS = float(np.finfo(np.float32).eps)\n__all__ = [\'GGNN\']\n\n\nclass GGNN(nn.Module):\n\n  def __init__(self, config):\n    """""" Gated Graph Neural Networks,\n        see reference below for more information\n\n        Li, Y., Tarlow, D., Brockschmidt, M. and Zemel, R., 2015. \n        Gated graph sequence neural networks. \n        arXiv preprint arXiv:1511.05493.\n    """"""\n\n    super(GGNN, self).__init__()\n    self.config = config\n    self.input_dim = config.model.input_dim\n    self.hidden_dim = config.model.hidden_dim\n    self.output_dim = config.model.output_dim\n    self.num_layer = config.model.num_layer\n    self.num_prop = config.model.num_prop\n    self.dropout = config.model.dropout if hasattr(config.model,\n                                                   \'dropout\') else 0.0\n    self.num_atom = config.dataset.num_atom\n    self.num_edgetype = config.dataset.num_bond_type\n    self.aggregate_type = config.model.aggregate_type\n    assert self.num_layer == 1, ""not implemented""\n    assert self.aggregate_type in [\'avg\', \'sum\'], \'not implemented\'\n\n    self.embedding = nn.Embedding(self.num_atom, self.input_dim)\n\n    # update function\n    if config.model.update_func == \'RNN\':\n      self.update_func = nn.RNNCell(\n          input_size=self.hidden_dim * (self.num_edgetype + 1),\n          hidden_size=self.hidden_dim,\n          nonlinearity=\'relu\')\n    elif config.model.update_func == \'GRU\':\n      self.update_func = nn.GRUCell(\n          input_size=self.hidden_dim * (self.num_edgetype + 1),\n          hidden_size=self.hidden_dim)\n    elif config.model.update_func == \'MLP\':\n      self.update_func = nn.Sequential(*[\n          nn.Linear(self.hidden_dim * (self.num_edgetype + 1),\n                    self.hidden_dim),\n          nn.Tanh()\n      ])\n\n    # message function\n    if config.model.msg_func == \'MLP\':\n      self.msg_func = nn.ModuleList([\n          nn.Sequential(*[\n              nn.Linear(self.hidden_dim, 128),\n              nn.ReLU(),\n              nn.Linear(128, self.hidden_dim)\n          ]) for _ in range((self.num_edgetype + 1))\n      ])\n    else:\n      self.msg_func = None\n\n    # attention\n    self.att_func = nn.Sequential(\n        *[nn.Linear(self.hidden_dim, 1),\n          nn.Sigmoid()])\n\n    self.input_func = nn.Sequential(\n        *[nn.Linear(self.input_dim, self.hidden_dim)])\n    self.output_func = nn.Sequential(\n        *[nn.Linear(self.hidden_dim, self.output_dim)])\n\n    if config.model.loss == \'CrossEntropy\':\n      self.loss_func = torch.nn.CrossEntropyLoss()\n    elif config.model.loss == \'MSE\':\n      self.loss_func = torch.nn.MSELoss()\n    elif config.model.loss == \'L1\':\n      self.loss_func = torch.nn.L1Loss()\n    else:\n      raise ValueError(""Non-supported loss function!"")\n\n    self._init_param()\n\n  def _init_param(self):\n    mlp_modules = [\n        xx\n        for xx in\n        [self.input_func, self.msg_func, self.att_func, self.output_func]\n        if xx is not None\n    ]\n\n    for m in mlp_modules:\n      if isinstance(m, nn.Sequential):\n        for mm in m:\n          if isinstance(mm, nn.Linear):\n            nn.init.xavier_uniform_(mm.weight.data)\n            if mm.bias is not None:\n              mm.bias.data.zero_()\n      elif isinstance(m, nn.Linear):\n        nn.init.xavier_uniform_(m.weight.data)\n        if m.bias is not None:\n          m.bias.data.zero_()\n\n    if self.config.model.update_func in [\'GRU\', \'RNN\']:\n      for m in [self.update_func]:\n        nn.init.xavier_uniform_(m.weight_hh.data)\n        nn.init.xavier_uniform_(m.weight_ih.data)\n        if m.bias:\n          m.bias_hh.data.zero_()\n          m.bias_ih.data.zero_()\n    elif self.config.model.update_func == \'MLP\':\n      for m in self.update_func:\n        if isinstance(m, nn.Linear):\n          nn.init.xavier_uniform_(m.weight.data)\n          if m.bias is not None:\n            m.bias.data.zero_()\n\n  def forward(self, node_feat, L, label=None, mask=None):\n    """"""\n      shape parameters:\n        batch size = B\n        embedding dim = D\n        max number of nodes within one mini batch = N\n        number of edge types = E\n        number of predicted properties = P\n      \n      Args:\n        node_feat: long tensor, shape B X N\n        L: float tensor, shape B X N X N X (E + 1)\n        label: float tensor, shape B X P\n        mask: float tensor, shape B X N\n    """"""    \n    L[L != 0] = 1.0\n    batch_size = node_feat.shape[0]\n    num_node = node_feat.shape[1]\n    state = self.embedding(node_feat)  # shape: B X N X D\n    state = self.input_func(state)\n\n    def _prop(state_old):\n      # gather message\n      msg = []\n      for ii in range(self.num_edgetype + 1):\n        if self.msg_func is not None:\n          tmp_msg = self.msg_func[ii](\n              state_old.view(batch_size * num_node, -1)).view(\n                  batch_size, num_node, -1)  # shape: B X N X D\n\n        # aggregate message\n        if self.aggregate_type == \'sum\':\n          tmp_msg = torch.bmm(L[:, :, :, ii], tmp_msg)\n        elif self.aggregate_type == \'avg\':\n          denom = torch.sum(L[:, :, :, ii], dim=2, keepdim=True) + EPS\n          tmp_msg = torch.bmm(L[:, :, :, ii] / denom, tmp_msg)\n        else:\n          pass\n\n        msg += [tmp_msg]  # shape B X N X D\n\n      # update state\n      msg = torch.cat(msg, dim=2).view(batch_size * num_node, -1)\n      state_old = state_old.view(batch_size * num_node, -1)\n\n      # GRU update\n      state_new = self.update_func(msg, state_old).view(batch_size, num_node,\n                                                        -1)\n\n      return state_new\n\n    # propagation\n    for tt in range(self.num_prop):\n      state = _prop(state)\n      state = F.dropout(state, self.dropout, training=self.training)\n\n    # output\n    state = state.view(batch_size * num_node, -1)\n    y = self.output_func(state)  # shape: BN X 1\n    att_weight = self.att_func(state)  # shape: BN X 1\n    y = (att_weight * y).view(batch_size, num_node, -1)\n\n    score = []\n    if mask is not None:\n      for bb in range(batch_size):\n        score += [torch.mean(y[bb, mask[bb], :], dim=0)]\n    else:\n      for bb in range(batch_size):\n        score += [torch.mean(y[bb, :, :], dim=0)]\n\n    score = torch.stack(score)\n\n    if label is not None:\n      return score, self.loss_func(score, label)\n    else:\n      return score\n'"
model/gpnn.py,16,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n__all__ = [\'GPNN\']\nEPS = float(np.finfo(np.float32).eps)\n\n\nclass GPNN(nn.Module):\n\n  def __init__(self, config):\n    """""" Graph Partition Neural Networks,\n        see reference below for more information\n\n        Liao, R., Brockschmidt, M., Tarlow, D., Gaunt, A.L., \n        Urtasun, R. and Zemel, R., 2018. \n        Graph Partition Neural Networks for Semi-Supervised \n        Classification. arXiv preprint arXiv:1803.06272.\n    """"""\n\n    super(GPNN, self).__init__()\n    self.config = config\n    self.input_dim = config.model.input_dim\n    self.hidden_dim = config.model.hidden_dim\n    self.output_dim = config.model.output_dim\n    self.num_layer = config.model.num_layer\n    self.num_prop = config.model.num_prop\n    self.num_partition = config.model.num_partition\n    self.num_prop_cluster = config.model.num_prop_cluster\n    self.num_prop_cut = config.model.num_prop_cut\n    self.dropout = config.model.dropout if hasattr(config.model,\n                                                   \'dropout\') else 0.0\n    self.num_atom = config.dataset.num_atom\n    self.num_edgetype = config.dataset.num_bond_type\n    self.aggregate_type = config.model.aggregate_type\n    assert self.num_layer == 1, ""not implemented""\n    assert self.aggregate_type in [\'avg\', \'sum\'], \'not implemented\'\n\n    self.embedding = nn.Embedding(self.num_atom, self.input_dim)\n\n    # update function\n    if config.model.update_func == \'RNN\':\n      self.update_func = nn.RNNCell(\n          input_size=self.hidden_dim * (self.num_edgetype + 1),\n          hidden_size=self.hidden_dim,\n          nonlinearity=\'relu\')\n      self.update_func_partition = nn.RNNCell(\n          input_size=self.hidden_dim,\n          hidden_size=self.hidden_dim,\n          nonlinearity=\'relu\')\n    elif config.model.update_func == \'GRU\':\n      self.update_func = nn.GRUCell(\n          input_size=self.hidden_dim * (self.num_edgetype + 1),\n          hidden_size=self.hidden_dim)\n      self.update_func_partition = nn.GRUCell(\n          input_size=self.hidden_dim, hidden_size=self.hidden_dim)\n    elif config.model.update_func == \'MLP\':\n      self.update_func = nn.Sequential(*[\n          nn.Linear(self.hidden_dim * (self.num_edgetype + 1), self.hidden_dim),\n          nn.Tanh()\n      ])\n      self.update_func_partition = nn.Sequential(\n          *[nn.Linear(self.hidden_dim, self.hidden_dim),\n            nn.Tanh()])\n\n    # state function\n    self.state_func = nn.Sequential(*[\n        nn.Linear(3 * self.hidden_dim, 512),\n        nn.ReLU(),\n        nn.Linear(512, self.hidden_dim)\n    ])\n\n    # message function\n    if config.model.msg_func == \'MLP\':\n      self.msg_func = nn.ModuleList([\n          nn.Sequential(*[\n              nn.Linear(self.hidden_dim, 128),\n              nn.ReLU(),\n              nn.Linear(128, self.hidden_dim)\n          ]) for _ in range((self.num_edgetype + 1))\n      ])\n    else:\n      self.msg_func = None\n\n    # attention\n    self.att_func = nn.Sequential(\n        *[nn.Linear(self.hidden_dim, 1),\n          nn.Sigmoid()])\n\n    self.input_func = nn.Sequential(\n        *[nn.Linear(self.input_dim, self.hidden_dim)])\n    self.output_func = nn.Sequential(\n        *[nn.Linear(self.hidden_dim, self.output_dim)])\n\n    if config.model.loss == \'CrossEntropy\':\n      self.loss_func = torch.nn.CrossEntropyLoss()\n    elif config.model.loss == \'MSE\':\n      self.loss_func = torch.nn.MSELoss()\n    elif config.model.loss == \'L1\':\n      self.loss_func = torch.nn.L1Loss()\n    else:\n      raise ValueError(""Non-supported loss function!"")\n\n    self._init_param()\n\n  def _init_param(self):\n    mlp_modules = [\n        xx for xx in [\n            self.input_func, self.state_func, self.msg_func, self.att_func,\n            self.output_func\n        ] if xx is not None\n    ]\n\n    for m in mlp_modules:\n      if isinstance(m, nn.Sequential):\n        for mm in m:\n          if isinstance(mm, nn.Linear):\n            nn.init.xavier_uniform_(mm.weight.data)\n            if mm.bias is not None:\n              mm.bias.data.zero_()\n      elif isinstance(m, nn.Linear):\n        nn.init.xavier_uniform_(m.weight.data)\n        if m.bias is not None:\n          m.bias.data.zero_()\n\n    if self.config.model.update_func in [\'GRU\', \'RNN\']:\n      for m in [self.update_func, self.update_func_partition]:\n        nn.init.xavier_uniform_(m.weight_hh.data)\n        nn.init.xavier_uniform_(m.weight_ih.data)\n        if m.bias:\n          m.bias_hh.data.zero_()\n          m.bias_ih.data.zero_()\n    elif self.config.model.update_func == \'MLP\':\n      for m in [self.update_func, self.update_func_partition]:\n        if isinstance(m, nn.Linear):\n          nn.init.xavier_uniform_(m.weight.data)\n          if m.bias is not None:\n            m.bias.data.zero_()\n\n  def forward(self, node_feat, L, L_cluster, L_cut, label=None, mask=None):\n    """"""\n      shape parameters:\n        batch size = B\n        embedding dim = D\n        max number of nodes within one mini batch = N\n        number of edge types = E\n        number of predicted properties = P\n\n      Args:\n        node_feat: long tensor, shape B X N\n        L: float tensor, shape B X N X N X (E + 1)\n        L_cluster: float tensor, shape B X N X N\n        L_cut: float tensor, shape B X N X N\n        label: float tensor, shape B X P\n        mask: float tensor, shape B X N\n    """"""\n    L[L != 0] = 1.0\n    batch_size = node_feat.shape[0]\n    num_node = node_feat.shape[1]\n    state = self.embedding(node_feat)  # shape: B X N X D\n    state = self.input_func(state)\n\n    def _prop(state_old):\n      # gather message\n      msg = []\n      for ii in range(self.num_edgetype + 1):\n        if self.msg_func is not None:\n          tmp_msg = self.msg_func[ii](\n              state_old.view(batch_size * num_node, -1)).view(\n                  batch_size, num_node, -1)  # shape: B X N X D\n\n        # aggregate message\n        if self.aggregate_type == \'sum\':\n          tmp_msg = torch.bmm(L[:, :, :, ii], tmp_msg)\n        elif self.aggregate_type == \'avg\':\n          denom = torch.sum(L[:, :, :, ii], dim=2, keepdim=True) + EPS\n          tmp_msg = torch.bmm(L[:, :, :, ii] / denom, tmp_msg)\n        else:\n          pass\n\n        msg += [tmp_msg]  # shape B X N X D\n\n      # update state\n      msg = torch.cat(msg, dim=2).view(batch_size * num_node, -1)\n      state_old = state_old.view(batch_size * num_node, -1)\n      state_new = self.update_func(msg, state_old).view(batch_size, num_node,\n                                                        -1)\n\n      return state_new\n\n    def _prop_partition(state_old, L_step):\n      if self.msg_func is not None:\n        # 0-th edge type corresponds to simple graph\n        msg = self.msg_func[0](state_old.view(batch_size * num_node, -1)).view(\n            batch_size, num_node, -1)  # shape: B X N X D\n\n      # aggregate message\n      if self.aggregate_type == \'sum\':\n        msg = torch.bmm(L_step, msg)\n      elif self.aggregate_type == \'avg\':\n        denom = torch.sum(L_step, dim=2, keepdim=True) + EPS\n        msg = torch.bmm(L_step / denom, msg)\n      else:\n        pass\n\n      # update state\n      msg = msg.view(batch_size * num_node, -1)\n      state_old = state_old.view(batch_size * num_node, -1)\n      state_new = self.update_func_partition(msg, state_old).view(\n          batch_size, num_node, -1)\n\n      return state_new\n\n    # propagation\n    for tt in range(self.num_prop):\n      # propagate within clusters\n      state_cluster = state\n      for ii in range(self.num_prop_cluster):\n        state_cluster = _prop_partition(state_cluster, L_cluster)\n\n      # propagate between clusters\n      state_cut = state\n      for ii in range(self.num_prop_cut):\n        state_cut = _prop_partition(state_cut, L_cut)\n\n      state = self.state_func(\n          torch.cat([state, state_cluster, state_cut], dim=2))\n      state = _prop(state)\n      state = F.dropout(state, self.dropout, training=self.training)\n\n    # output\n    state = state.view(batch_size * num_node, -1)\n    y = self.output_func(state)  # shape: BN X 1\n    att_weight = self.att_func(state)  # shape: BN X 1\n    y = (att_weight * y).view(batch_size, num_node, -1)\n\n    score = []\n    if mask is not None:\n      for bb in range(batch_size):\n        score += [torch.mean(y[bb, mask[bb], :], dim=0)]\n    else:\n      for bb in range(batch_size):\n        score += [torch.mean(y[bb, :, :], dim=0)]\n\n    score = torch.stack(score)\n\n    if label is not None:\n      return score, self.loss_func(score, label)\n    else:\n      return score\n'"
model/graph_sage.py,15,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nEPS = float(np.finfo(np.float32).eps)\n__all__ = [\'GraphSAGE\']\n\n\nclass GraphSAGE(nn.Module):\n\n  def __init__(self, config):\n    """""" GraphSAGE,\n        see reference below for more information\n\n        Hamilton, W., Ying, Z. and Leskovec, J., 2017. Inductive\n        representation learning on large graphs. In NIPS.\n    """"""\n    super(GraphSAGE, self).__init__()\n    self.config = config\n    self.input_dim = config.model.input_dim\n    self.hidden_dim = config.model.hidden_dim\n    self.output_dim = config.model.output_dim\n    self.num_layer = config.model.num_layer\n    self.dropout = config.model.dropout if hasattr(config.model,\n                                                   \'dropout\') else 0.0\n    self.num_sample_neighbors = config.model.num_sample_neighbors\n    self.num_atom = config.dataset.num_atom\n    self.num_edgetype = config.dataset.num_bond_type\n    assert self.num_layer == len(self.hidden_dim)\n    dim_list = [self.input_dim] + self.hidden_dim + [self.output_dim]\n\n    self.embedding = nn.Embedding(self.num_atom, self.input_dim)\n\n    # aggregate function\n    self.agg_func_name = config.model.agg_func\n    if self.agg_func_name == \'LSTM\':\n      self.agg_func = nn.ModuleList([\n          nn.LSTMCell(input_size=dim_list[tt], hidden_size=dim_list[tt])\n          for tt in range(self.num_layer - 1)\n      ])\n    elif self.agg_func_name == \'Mean\':\n      self.agg_func = torch.mean\n    elif self.agg_func_name == \'Max\':\n      self.agg_func = torch.max\n    else:\n      self.agg_func = None\n\n    # attention\n    self.att_func = nn.Sequential(*[nn.Linear(dim_list[-2], 1), nn.Sigmoid()])\n\n    # update function\n    self.filter = nn.ModuleList([\n        nn.Linear(dim_list[tt] * (self.num_edgetype + 1), dim_list[tt + 1])\n        for tt in range(self.num_layer)\n    ] + [nn.Linear(dim_list[-2], dim_list[-1])])\n\n    if config.model.loss == \'CrossEntropy\':\n      self.loss_func = torch.nn.CrossEntropyLoss()\n    elif config.model.loss == \'MSE\':\n      self.loss_func = torch.nn.MSELoss()\n    elif config.model.loss == \'L1\':\n      self.loss_func = torch.nn.L1Loss()\n    else:\n      raise ValueError(""Non-supported loss function!"")\n\n    self._init_param()\n\n  def _init_param(self):\n    mlp_modules = [xx for xx in [self.att_func] if xx is not None]\n\n    for m in mlp_modules:\n      if isinstance(m, nn.Sequential):\n        for mm in m:\n          if isinstance(mm, nn.Linear):\n            nn.init.xavier_uniform_(mm.weight.data)\n            if mm.bias is not None:\n              mm.bias.data.zero_()\n      elif isinstance(m, nn.Linear):\n        nn.init.xavier_uniform_(m.weight.data)\n        if m.bias is not None:\n          m.bias.data.zero_()\n\n    if self.config.model.agg_func == \'LSTM\':\n      for m in self.agg_func:\n        nn.init.xavier_uniform_(m.weight_hh.data)\n        nn.init.xavier_uniform_(m.weight_ih.data)\n        if m.bias:\n          m.bias_hh.data.zero_()\n          m.bias_ih.data.zero_()\n\n    for ff in self.filter:\n      if isinstance(ff, nn.Linear):\n        nn.init.xavier_uniform_(ff.weight.data)\n        if ff.bias is not None:\n          ff.bias.data.zero_()\n\n  def forward(self, node_feat, nn_idx, nonempty_mask, label=None, mask=None):\n    """"""\n      shape parameters:\n        batch size = B\n        embedding dim = D\n        max number of nodes within one mini batch = N\n        neighborhood size = K\n        number of edge types = E\n        number of predicted properties = P\n      \n      Args:\n        node_feat: float tensor, shape B X N X D\n        nn_idx: float tensor, shape B X N X K X E\n        nonempty_mask: float tensor, shape B X N X 1\n        label: float tensor, shape B X P\n        mask: float tensor, shape B X N\n    """"""\n    batch_size = node_feat.shape[0]\n    num_node = node_feat.shape[1]\n    state = self.embedding(node_feat)  # shape B X N X D\n\n    # propagation\n    for ii in range(self.num_layer - 1):\n      msg = []\n      for jj in range(self.num_edgetype + 1):\n        # gather message\n        nn_state = []\n        for bb in range(batch_size):\n          nn_state += [state[bb, nn_idx[bb, :, :, jj], :]]  # shape N X K X D\n\n        nn_state = torch.stack(nn_state, dim=0)  # shape B X N X K X D\n\n        # aggregate message\n        if self.agg_func_name == \'LSTM\':\n          cx = torch.zeros_like(state).view(batch_size * num_node,\n                                            -1)  # shape: B X N X D\n          hx = torch.zeros_like(state).view(batch_size * num_node, -1)\n          for tt in range(self.num_sample_neighbors):\n            ix = nn_state[:, :, tt, :]\n            hx, cx = self.agg_func[ii](ix.view(batch_size * num_node, -1),\n                                       (hx, cx))\n\n          agg_state = hx.view(batch_size, num_node, -1)\n        elif self.agg_func_name == \'Max\':\n          agg_state, _ = self.agg_func(nn_state, dim=2)\n        else:\n          agg_state = self.agg_func(nn_state, dim=2)\n\n        msg += [agg_state * nonempty_mask]\n\n      # update state\n      # import pdb; pdb.set_trace()\n      state = F.relu(self.filter[ii](torch.cat(msg, dim=2).view(\n          batch_size * num_node, -1)))\n      state = (state / (torch.norm(state, 2, dim=1, keepdim=True) + EPS)).view(\n          batch_size, num_node, -1)\n      state = F.dropout(state, self.dropout, training=self.training)\n\n    # output\n    state = state.view(batch_size * num_node, -1)\n    y = self.filter[-1](state)  # shape: BN X 1\n    att_weight = self.att_func(state)  # shape: BN X 1\n    y = (att_weight * y).view(batch_size, num_node, -1)\n\n    score = []\n    if mask is not None:\n      for bb in range(batch_size):\n        score += [torch.mean(y[bb, mask[bb], :], dim=0)]\n    else:\n      for bb in range(batch_size):\n        score += [torch.mean(y[bb, :, :], dim=0)]\n\n    score = torch.stack(score)\n\n    if label is not None:\n      return score, self.loss_func(score, label)\n    else:\n      return score\n'"
model/lanczos_net.py,14,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom utils.data_helper import check_dist\n\n\nEPS = float(np.finfo(np.float32).eps)\n__all__ = [\'LanczosNet\']\n\n\nclass LanczosNet(nn.Module):\n\n  def __init__(self, config):\n    super(LanczosNet, self).__init__()\n    self.config = config\n    self.input_dim = config.model.input_dim\n    self.hidden_dim = config.model.hidden_dim\n    self.output_dim = config.model.output_dim\n    self.num_layer = config.model.num_layer\n    self.num_atom = config.dataset.num_atom\n    self.num_edgetype = config.dataset.num_bond_type\n    self.dropout = config.model.dropout if hasattr(config.model,\n                                                   \'dropout\') else 0.0\n    self.short_diffusion_dist = check_dist(config.model.short_diffusion_dist)\n    self.long_diffusion_dist = check_dist(config.model.long_diffusion_dist)\n    self.max_short_diffusion_dist = max(\n        self.short_diffusion_dist) if self.short_diffusion_dist else None\n    self.max_long_diffusion_dist = max(\n        self.long_diffusion_dist) if self.long_diffusion_dist else None\n    self.num_scale_short = len(self.short_diffusion_dist)\n    self.num_scale_long = len(self.long_diffusion_dist)\n    self.num_eig_vec = config.model.num_eig_vec\n    self.spectral_filter_kind = config.model.spectral_filter_kind\n\n    dim_list = [self.input_dim] + self.hidden_dim + [self.output_dim]\n    self.filter = nn.ModuleList([\n        nn.Linear(dim_list[tt] * (\n            self.num_scale_short + self.num_scale_long + self.num_edgetype + 1),\n                  dim_list[tt + 1]) for tt in range(self.num_layer)\n    ] + [nn.Linear(dim_list[-2], dim_list[-1])])\n\n    self.embedding = nn.Embedding(self.num_atom, self.input_dim)\n\n    # spectral filters\n    if self.spectral_filter_kind == \'MLP\' and self.num_scale_long > 0:\n      self.spectral_filter = nn.ModuleList([\n          nn.Sequential(*[\n              nn.Linear(self.num_scale_long, 128),\n              nn.ReLU(),\n              nn.Linear(128, 128),\n              nn.ReLU(),\n              nn.Linear(128, 128),\n              nn.ReLU(),\n              nn.Linear(128, self.num_scale_long)\n          ]) for _ in range(self.num_layer)\n      ])\n\n    # attention\n    self.att_func = nn.Sequential(*[nn.Linear(dim_list[-2], 1), nn.Sigmoid()])\n\n    if config.model.loss == \'CrossEntropy\':\n      self.loss_func = torch.nn.CrossEntropyLoss()\n    elif config.model.loss == \'MSE\':\n      self.loss_func = torch.nn.MSELoss()\n    elif config.model.loss == \'L1\':\n      self.loss_func = torch.nn.L1Loss()\n    else:\n      raise ValueError(""Non-supported loss function!"")\n\n    self._init_param()\n\n  def _init_param(self):\n    for ff in self.filter:\n      if isinstance(ff, nn.Linear):\n        nn.init.xavier_uniform_(ff.weight.data)\n        if ff.bias is not None:\n          ff.bias.data.zero_()\n\n    for ff in self.att_func:\n      if isinstance(ff, nn.Linear):\n        nn.init.xavier_uniform_(ff.weight.data)\n        if ff.bias is not None:\n          ff.bias.data.zero_()\n\n    if self.spectral_filter_kind == \'MLP\' and self.num_scale_long > 0:\n      for ff in self.spectral_filter:\n        for f in ff:\n          if isinstance(f, nn.Linear):\n            nn.init.xavier_uniform_(f.weight.data)\n            if f.bias is not None:\n              f.bias.data.zero_()\n\n  def _get_spectral_filters(self, T_list, Q, layer_idx):\n    """""" Construct Spectral Filters based on Lanczos Outputs\n\n      Args:\n        T_list: each element is of shape B X K\n        Q: shape B X N X K\n\n      Returns:\n        L: shape B X N X N X num_scale\n    """"""\n    # multi-scale diffusion\n    L = []\n\n    # spectral filter\n    if self.spectral_filter_kind == \'MLP\':\n      DD = torch.stack(\n          T_list, dim=2).view(Q.shape[0] * Q.shape[2], -1)  # shape BK X D\n      DD = self.spectral_filter[layer_idx](DD).view(Q.shape[0], Q.shape[2],\n                                                    -1)  # shape B X K X D\n      for ii in range(self.num_scale_long):\n        tmp_DD = DD[:, :, ii].unsqueeze(1).repeat(1, Q.shape[1],\n                                                  1)  # shape B X N X K\n        L += [(Q * tmp_DD).bmm(Q.transpose(1, 2))]\n    else:\n      for ii in range(self.num_scale_long):\n        DD = T_list[ii].unsqueeze(1).repeat(1, Q.shape[1], 1)  # shape B X N X K\n        L += [(Q * DD).bmm(Q.transpose(1, 2))]\n\n    return torch.stack(L, dim=3)\n\n  def forward(self, node_feat, L, D, V, label=None, mask=None):\n    """"""\n      shape parameters:\n        batch size = B\n        embedding dim = D\n        max number of nodes within one mini batch = N\n        number of edge types = E\n        number of predicted properties = P\n        number of approximated eigenvalues, i.e., Ritz values = K\n      \n      Args:\n        node_feat: long tensor, shape B X N\n        L: float tensor, shape B X N X N X (E + 1)\n        D: float tensor, Ritz values, shape B X K\n        V: float tensor, Ritz vectors, shape B X N X K\n        label: float tensor, shape B X P\n        mask: float tensor, shape B X N\n    """"""\n    batch_size = node_feat.shape[0]\n    num_node = node_feat.shape[1]\n\n    D_pow_list = []\n\n    for ii in self.long_diffusion_dist:\n      D_pow_list += [torch.pow(D, ii)]  # shape B X K\n\n    ###########################################################################\n    # Graph Convolution\n    ###########################################################################\n    state = self.embedding(node_feat)  # shape: B X N X D\n    \n    # propagation\n    for tt in range(self.num_layer):\n      msg = []\n\n      if self.num_scale_long > 0:\n        Lf = self._get_spectral_filters(D_pow_list, V, tt)\n\n      # short diffusion\n      if self.num_scale_short > 0:\n        tmp_state = state\n        for ii in range(1, self.max_short_diffusion_dist + 1):\n          tmp_state = torch.bmm(L[:, :, :, 0], tmp_state)\n          if ii in self.short_diffusion_dist:\n            msg += [tmp_state]\n\n      # long diffusion\n      if self.num_scale_long > 0:\n        for ii in range(self.num_scale_long):\n          msg += [torch.bmm(Lf[:, :, :, ii], state)]  # shape: B X N X D\n\n      # edge type\n      for ii in range(self.num_edgetype + 1):\n        msg += [torch.bmm(L[:, :, :, ii], state)]  # shape: B X N X D\n\n      msg = torch.cat(msg, dim=2).view(num_node * batch_size, -1)\n      state = F.relu(self.filter[tt](msg)).view(batch_size, num_node, -1)\n      state = F.dropout(state, self.dropout, training=self.training)\n\n    # output\n    state = state.view(batch_size * num_node, -1)\n    y = self.filter[-1](state)  # shape: BN X 1\n    att_weight = self.att_func(state)  # shape: BN X 1\n    y = (att_weight * y).view(batch_size, num_node, -1)\n\n    score = []\n    for bb in range(batch_size):\n      score += [torch.mean(y[bb, mask[bb], :], dim=0)]\n\n    score = torch.stack(score)\n\n    if label is not None:\n      return score, self.loss_func(score, label)\n    else:\n      return score\n'"
model/lanczos_net_general.py,14,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom utils.data_helper import check_dist\n\n\nEPS = float(np.finfo(np.float32).eps)\n__all__ = [\'LanczosNetGeneral\']\n\n\nclass LanczosNetGeneral(nn.Module):\n\n  def __init__(self, config):\n    super(LanczosNetGeneral, self).__init__()\n    self.config = config\n    self.input_dim = config.model.input_dim\n    self.hidden_dim = config.model.hidden_dim\n    self.output_dim = config.model.output_dim\n    self.num_layer = config.model.num_layer\n    self.node_emb_dim = config.dataset.node_emb_dim\n    self.graph_emb_dim = config.dataset.graph_emb_dim\n    self.num_edgetype = config.dataset.num_edge_type\n    self.dropout = config.model.dropout if hasattr(config.model,\n                                                   \'dropout\') else 0.0\n    self.short_diffusion_dist = check_dist(config.model.short_diffusion_dist)\n    self.long_diffusion_dist = check_dist(config.model.long_diffusion_dist)\n    self.max_short_diffusion_dist = max(\n        self.short_diffusion_dist) if self.short_diffusion_dist else None\n    self.max_long_diffusion_dist = max(\n        self.long_diffusion_dist) if self.long_diffusion_dist else None\n    self.num_scale_short = len(self.short_diffusion_dist)\n    self.num_scale_long = len(self.long_diffusion_dist)\n    self.num_eig_vec = config.model.num_eig_vec\n    self.spectral_filter_kind = config.model.spectral_filter_kind\n\n    dim_list = [self.input_dim] + self.hidden_dim + [self.output_dim]\n    self.filter = nn.ModuleList([\n        nn.Linear(dim_list[tt] * (\n            self.num_scale_short + self.num_scale_long + self.num_edgetype + 1),\n                  dim_list[tt + 1]) for tt in range(self.num_layer)\n    ] + [nn.Linear(dim_list[-2], dim_list[-1])])\n\n    assert self.input_dim == self.node_emb_dim\n    assert self.output_dim == self.graph_emb_dim\n\n    # spectral filters\n    if self.spectral_filter_kind == \'MLP\' and self.num_scale_long > 0:\n      self.spectral_filter = nn.ModuleList([\n          nn.Sequential(*[\n              nn.Linear(self.num_scale_long, 128),\n              nn.ReLU(),\n              nn.Linear(128, 128),\n              nn.ReLU(),\n              nn.Linear(128, 128),\n              nn.ReLU(),\n              nn.Linear(128, self.num_scale_long)\n          ]) for _ in range(self.num_layer)\n      ])\n\n    # attention\n    self.att_func = nn.Sequential(*[nn.Linear(dim_list[-2], 1), nn.Sigmoid()])\n\n    if config.model.loss == \'CrossEntropy\':\n      self.loss_func = torch.nn.CrossEntropyLoss()\n    elif config.model.loss == \'MSE\':\n      self.loss_func = torch.nn.MSELoss()\n    elif config.model.loss == \'L1\':\n      self.loss_func = torch.nn.L1Loss()\n    else:\n      raise ValueError(""Non-supported loss function!"")\n\n    self._init_param()\n\n  def _init_param(self):\n    for ff in self.filter:\n      if isinstance(ff, nn.Linear):\n        nn.init.xavier_uniform_(ff.weight.data)\n        if ff.bias is not None:\n          ff.bias.data.zero_()\n\n    for ff in self.att_func:\n      if isinstance(ff, nn.Linear):\n        nn.init.xavier_uniform_(ff.weight.data)\n        if ff.bias is not None:\n          ff.bias.data.zero_()\n\n    if self.spectral_filter_kind == \'MLP\' and self.num_scale_long > 0:\n      for ff in self.spectral_filter:\n        for f in ff:\n          if isinstance(f, nn.Linear):\n            nn.init.xavier_uniform_(f.weight.data)\n            if f.bias is not None:\n              f.bias.data.zero_()\n\n  def _get_spectral_filters(self, T_list, Q, layer_idx):\n    """""" Construct Spectral Filters based on Lanczos Outputs\n\n      Args:\n        T_list: each element is of shape B X K\n        Q: shape B X N X K\n\n      Returns:\n        L: shape B X N X N X num_scale\n    """"""\n    # multi-scale diffusion\n    L = []\n\n    # spectral filter\n    if self.spectral_filter_kind == \'MLP\':\n      DD = torch.stack(\n          T_list, dim=2).view(Q.shape[0] * Q.shape[2], -1)  # shape BK X D\n      DD = self.spectral_filter[layer_idx](DD).view(Q.shape[0], Q.shape[2],\n                                                    -1)  # shape B X K X D\n      for ii in range(self.num_scale_long):\n        tmp_DD = DD[:, :, ii].unsqueeze(1).repeat(1, Q.shape[1],\n                                                  1)  # shape B X N X K\n        L += [(Q * tmp_DD).bmm(Q.transpose(1, 2))]\n    else:\n      for ii in range(self.num_scale_long):\n        DD = T_list[ii].unsqueeze(1).repeat(1, Q.shape[1], 1)  # shape B X N X K\n        L += [(Q * DD).bmm(Q.transpose(1, 2))]\n\n    return torch.stack(L, dim=3)\n\n  def forward(self, node_feat, L, D, V, label=None, mask=None):\n    """"""\n      shape parameters:\n        batch size = B\n        embedding dim = D\n        max number of nodes within one mini batch = N\n        number of edge types = E\n        number of predicted properties = P\n        number of approximated eigenvalues, i.e., Ritz values = K\n      \n      Args:\n        node_feat: long tensor, shape B X N X D\n        L: float tensor, shape B X N X N X (E + 1)\n        D: float tensor, Ritz values, shape B X K\n        V: float tensor, Ritz vectors, shape B X N X K\n        label: float tensor, shape B X P\n        mask: float tensor, shape B X N\n    """"""\n    batch_size = node_feat.shape[0]\n    num_node = node_feat.shape[1]\n\n    D_pow_list = []\n\n    for ii in self.long_diffusion_dist:\n      D_pow_list += [torch.pow(D, ii)]  # shape B X K\n\n    ###########################################################################\n    # Graph Convolution\n    ###########################################################################\n    state = node_feat  # shape: B X N X D\n    \n    # propagation\n    for tt in range(self.num_layer):\n      msg = []\n\n      if self.num_scale_long > 0:\n        Lf = self._get_spectral_filters(D_pow_list, V, tt)\n\n      # short diffusion\n      if self.num_scale_short > 0:\n        tmp_state = state\n        for ii in range(1, self.max_short_diffusion_dist + 1):\n          tmp_state = torch.bmm(L[:, :, :, 0], tmp_state)\n          if ii in self.short_diffusion_dist:\n            msg += [tmp_state]\n\n      # long diffusion\n      if self.num_scale_long > 0:\n        for ii in range(self.num_scale_long):\n          msg += [torch.bmm(Lf[:, :, :, ii], state)]  # shape: B X N X D\n\n      # edge type\n      for ii in range(self.num_edgetype + 1):\n        msg += [torch.bmm(L[:, :, :, ii], state)]  # shape: B X N X D\n\n      msg = torch.cat(msg, dim=2).view(num_node * batch_size, -1)\n      state = F.relu(self.filter[tt](msg)).view(batch_size, num_node, -1)\n      state = F.dropout(state, self.dropout, training=self.training)\n\n    # output\n    state = state.view(batch_size * num_node, -1)\n    y = self.filter[-1](state)  # shape: BN X D\'\n    att_weight = self.att_func(state)  # shape: BN X 1\n    y = (att_weight * y).view(batch_size, num_node, -1)\n\n    score = []    \n    for bb in range(batch_size):      \n      score += [torch.mean(y[bb, mask[bb], :], dim=0)]\n\n    score = torch.stack(score)\n\n    if label is not None:\n      return score, self.loss_func(score, label)\n    else:\n      return score\n'"
model/mpnn.py,15,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model.set2set import Set2Vec\nfrom operators.functions.unsorted_segment_sum import UnsortedSegmentSumFunction\n\nEPS = float(np.finfo(np.float32).eps)\nunsorted_segment_sum = UnsortedSegmentSumFunction.apply\n\n__all__ = [\'MPNN\']\n\n\nclass MPNN(nn.Module):\n\n  def __init__(self, config):\n    """""" Message Passing Neural Networks,\n        see reference below for more information\n\n        Gilmer, J., Schoenholz, S.S., Riley, P.F., Vinyals, O. and Dahl,\n        G.E., 2017. Neural message passing for quantum chemistry. In ICML.\n    """"""\n    super(MPNN, self).__init__()\n    self.config = config\n    self.input_dim = config.model.input_dim\n    self.hidden_dim = config.model.hidden_dim\n    self.output_dim = config.model.output_dim\n    self.num_layer = config.model.num_layer\n    self.num_prop = config.model.num_prop\n    self.msg_func_name = config.model.msg_func\n    self.num_step_set2vec = config.model.num_step_set2vec\n    self.dropout = config.model.dropout if hasattr(config.model,\n                                                   \'dropout\') else 0.0\n    self.num_atom = config.dataset.num_atom\n    self.num_edgetype = config.dataset.num_bond_type\n    self.aggregate_type = config.model.aggregate_type\n    assert self.num_layer == 1, \'not implemented\'\n    assert self.aggregate_type in [\'avg\', \'sum\'], \'not implemented\'\n\n    self.node_embedding = nn.Embedding(self.num_atom, self.input_dim)\n\n    # input function\n    self.input_func = nn.Sequential(\n        *[nn.Linear(self.input_dim, self.hidden_dim)])\n\n    # update function\n    self.update_func = nn.GRUCell(\n        input_size=self.hidden_dim * (self.num_edgetype + 1),\n        hidden_size=self.hidden_dim)\n\n    # message function\n    # N.B.: if there is no edge feature, the edge network in the paper degenerates\n    # to multiple edge embedding matrices, each corresponds to one edge type\n    if config.model.msg_func == \'embedding\':\n      self.edge_embedding = nn.Embedding(self.num_edgetype + 1, self.hidden_dim\n                                         **2)\n    elif config.model.msg_func == \'MLP\':\n      self.edge_func = nn.ModuleList([\n          nn.Sequential(*[\n              nn.Linear(self.hidden_dim * 2, 64),\n              nn.ReLU(),\n              nn.Linear(64, self.hidden_dim)\n          ]) for _ in range((self.num_edgetype + 1))\n      ])\n    else:\n      raise ValueError(\'Non-supported message function\')\n\n    self.att_func = Set2Vec(self.hidden_dim, self.num_step_set2vec)\n\n    # output function\n    self.output_func = nn.Sequential(\n        *[nn.Linear(2 * self.hidden_dim, self.output_dim)])\n\n    if config.model.loss == \'CrossEntropy\':\n      self.loss_func = torch.nn.CrossEntropyLoss()\n    elif config.model.loss == \'MSE\':\n      self.loss_func = torch.nn.MSELoss()\n    elif config.model.loss == \'L1\':\n      self.loss_func = torch.nn.L1Loss()\n    else:\n      raise ValueError(""Non-supported loss function!"")\n\n    self._init_param()\n\n  def _init_param(self):\n    mlp_modules = [\n        xx for xx in [self.input_func, self.output_func, self.att_func]\n        if xx is not None\n    ]\n\n    for m in mlp_modules:\n      if isinstance(m, nn.Sequential):\n        for mm in m:\n          if isinstance(mm, nn.Linear):\n            nn.init.xavier_uniform_(mm.weight.data)\n            if mm.bias is not None:\n              mm.bias.data.zero_()\n      elif isinstance(m, nn.Linear):\n        nn.init.xavier_uniform_(m.weight.data)\n        if m.bias is not None:\n          m.bias.data.zero_()\n\n    for m in [self.update_func]:\n      nn.init.xavier_uniform_(m.weight_hh.data)\n      nn.init.xavier_uniform_(m.weight_ih.data)\n      if m.bias:\n        m.bias_hh.data.zero_()\n        m.bias_ih.data.zero_()\n\n  def forward(self, node_feat, L, label=None, mask=None):\n    """"""\n      shape parameters:\n        batch size = B\n        embedding dim = D\n        max number of nodes within one mini batch = N\n        number of edge types = E\n        number of predicted properties = P\n      \n      Args:\n        node_feat: long tensor, shape B X N\n        L: float tensor, shape B X N X N X (E + 1)\n        label: float tensor, shape B X P\n        mask: float tensor, shape B X N\n    """"""    \n    L[L != 0] = 1.0\n    batch_size = node_feat.shape[0]\n    num_node = node_feat.shape[1]\n    state = self.node_embedding(node_feat)  # shape: B X N X D\n    state = self.input_func(state)\n\n    if self.msg_func_name == \'MLP\':\n      idx_row, idx_col = np.meshgrid(range(num_node), range(num_node))\n      idx_row, idx_col = idx_row.flatten().astype(\n          np.int64), idx_col.flatten().astype(np.int64)\n\n    def _prop(state_old):\n      state_dim = state_old.shape[2]\n\n      msg = []\n      for ii in range(self.num_edgetype + 1):\n        if self.msg_func_name == \'embedding\':\n          idx_edgetype = torch.Tensor([ii]).long().to(node_feat.device)\n          edge_em = self.edge_embedding(idx_edgetype).view(state_dim, state_dim)\n          node_state = state_old.view(batch_size * num_node,\n                                      -1)  # shape: BN X D\n          tmp_msg = node_state.mm(edge_em).view(batch_size, num_node,\n                                                -1)  # shape: B X N X D\n          # aggregate message\n          if self.aggregate_type == \'sum\':\n            tmp_msg = torch.bmm(L[:, :, :, ii], tmp_msg)\n          elif self.aggregate_type == \'avg\':\n            denom = torch.sum(L[:, :, :, ii], dim=2, keepdim=True) + EPS\n            tmp_msg = torch.bmm(L[:, :, :, ii] / denom, tmp_msg)\n          else:\n            pass\n\n        elif self.msg_func_name == \'MLP\':\n          state_in = state_old[:, idx_col, :]  # shape: B X N X D\n          state_out = state_old[:, idx_row, :]  # shape: B X N X D\n          tmp_msg = self.edge_func[ii](torch.cat(\n              [state_out, state_in], dim=2).view(\n                  batch_size * num_node * num_node, -1)).view(\n                      batch_size, num_node, num_node,\n                      -1)  # shape: B X N X N X D\n\n          # aggregate message\n          if self.aggregate_type == \'sum\':\n            tmp_msg = torch.matmul(\n                tmp_msg.permute(0, 1, 3, 2),\n                L[:, :, :, ii].unsqueeze(dim=3)).squeeze()  # B X N X D\n          elif self.aggregate_type == \'avg\':\n            denom = torch.sum(\n                L[:, :, :, ii], dim=2, keepdim=True) + EPS  # B X N X 1\n            tmp_msg = torch.matmul(\n                tmp_msg.permute(0, 1, 3, 2),\n                L[:, :, :, ii].unsqueeze(dim=3)).squeeze()  # B X N X D\n            tmp_msg = tmp_msg / denom\n          else:\n            pass\n\n        msg += [tmp_msg]  # shape B X N X D\n\n      # update state\n      msg = torch.cat(msg, dim=2).view(batch_size * num_node, -1)\n      state_old = state_old.view(batch_size * num_node, -1)\n\n      # GRU update\n      state_new = self.update_func(msg, state_old).view(batch_size, num_node,\n                                                        -1)\n\n      return state_new\n\n    # propagation\n    for tt in range(self.num_prop):\n      state = _prop(state)\n      state = F.dropout(state, self.dropout, training=self.training)\n\n    # output\n    y = []\n    if mask is not None:\n      for bb in range(batch_size):\n        y += [self.att_func(state[bb, mask[bb], :])]\n    else:\n      for bb in range(batch_size):\n        y += [self.att_func(state[bb, :, :])]\n\n    score = self.output_func(torch.cat(y, dim=0))\n\n    if label is not None:\n      return score, self.loss_func(score, label)\n    else:\n      return score\n'"
model/set2set.py,26,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n__all__ = [\'Set2Set\', \'Set2Vec\']\n\n\nclass Set2SetLSTM(nn.Module):\n\n  def __init__(self, hidden_dim):\n    """""" Implementation of customized LSTM for set2set """"""\n    super(Set2SetLSTM, self).__init__()\n    self.hidden_dim = hidden_dim\n    self.forget_gate = nn.Sequential(\n        *[nn.Linear(2 * self.hidden_dim, self.hidden_dim),\n          nn.Sigmoid()])\n    self.input_gate = nn.Sequential(\n        *[nn.Linear(2 * self.hidden_dim, self.hidden_dim),\n          nn.Sigmoid()])\n    self.output_gate = nn.Sequential(\n        *[nn.Linear(2 * self.hidden_dim, self.hidden_dim),\n          nn.Sigmoid()])\n    self.memory_gate = nn.Sequential(\n        *[nn.Linear(2 * self.hidden_dim, self.hidden_dim),\n          nn.Tanh()])\n\n    self._init_param()\n\n  def _init_param(self):\n    for m in [\n        self.forget_gate, self.input_gate, self.output_gate, self.memory_gate\n    ]:\n      for mm in m:\n        if isinstance(mm, nn.Linear):\n          nn.init.xavier_uniform_(mm.weight.data)\n          if mm.bias is not None:\n            mm.bias.data.zero_()\n\n  def forward(self, hidden, memory):\n    """"""\n      Args:\n        hidden: shape N X 2D\n        memory: shape N X D\n\n      Returns:\n        hidden: shape N X D\n        memory: shape N X D\n    """"""\n    ft = self.forget_gate(hidden)\n    it = self.input_gate(hidden)\n    ot = self.output_gate(hidden)\n    ct = self.memory_gate(hidden)\n\n    memory = ft * memory + it * ct\n    hidden = ot * torch.tanh(memory)\n\n    return hidden, memory\n\n\nclass Set2Vec(nn.Module):\n\n  def __init__(self, element_dim, num_step_encoder):\n    """""" Implementation of Set2Vec """"""\n    super(Set2Vec, self).__init__()\n    self.element_dim = element_dim\n    self.num_step_encoder = num_step_encoder\n    self.LSTM = Set2SetLSTM(element_dim)\n    self.W_1 = nn.Parameter(torch.ones(self.element_dim, self.element_dim))\n    self.W_2 = nn.Parameter(torch.ones(self.element_dim, 1))\n    self.register_parameter(\'W_1\', self.W_1)\n    self.register_parameter(\'W_2\', self.W_2)\n\n    self._init_param()\n\n  def _init_param(self):\n    nn.init.xavier_uniform_(self.W_1.data)\n    nn.init.xavier_uniform_(self.W_2.data)\n\n  def forward(self, input_set):\n    """"""\n      Args:\n        input_set: shape N X D\n\n      Returns:\n        output_vec: shape 1 X 2D\n    """"""\n    num_element = input_set.shape[0]\n    element_dim = input_set.shape[1]\n    assert element_dim == self.element_dim\n    hidden = torch.zeros(1, 2 * self.element_dim).to(input_set.device)\n    memory = torch.zeros(1, self.element_dim).to(input_set.device)\n\n    for tt in range(self.num_step_encoder):\n      hidden, memory = self.LSTM(hidden, memory)\n      energy = torch.tanh(torch.mm(hidden, self.W_1) + input_set).mm(self.W_2)\n      att_weight = F.softmax(energy, dim=0)\n      read = (input_set * att_weight).sum(dim=0, keepdim=True)\n      hidden = torch.cat([hidden, read], dim=1)\n\n    return hidden\n\n\nclass Set2Set(nn.Module):\n\n  def __init__(self, element_dim, num_step_encoder):\n    """""" Implementation of Set2Set """"""\n    super(Set2Set, self).__init__()\n    self.element_dim = element_dim\n    self.num_step_encoder = num_step_encoder\n    self.LSTM_encoder = Set2SetLSTM(element_dim)\n    self.LSTM_decoder = Set2SetLSTM(element_dim)\n    self.W_1 = nn.Parameter(torch.ones(self.element_dim, self.element_dim))\n    self.W_2 = nn.Parameter(torch.ones(self.element_dim, 1))\n    self.W_3 = nn.Parameter(torch.ones(self.element_dim, self.element_dim))\n    self.W_4 = nn.Parameter(torch.ones(self.element_dim, 1))\n    self.W_5 = nn.Parameter(torch.ones(self.element_dim, self.element_dim))\n    self.W_6 = nn.Parameter(torch.ones(self.element_dim, self.element_dim))\n    self.W_7 = nn.Parameter(torch.ones(self.element_dim, 1))\n    self.register_parameter(\'W_1\', self.W_1)\n    self.register_parameter(\'W_2\', self.W_2)\n    self.register_parameter(\'W_3\', self.W_3)\n    self.register_parameter(\'W_4\', self.W_4)\n    self.register_parameter(\'W_5\', self.W_5)\n    self.register_parameter(\'W_6\', self.W_6)\n    self.register_parameter(\'W_7\', self.W_7)\n\n    self._init_param()\n\n  def _init_param(self):\n    for xx in [\n        self.W_1, self.W_2, self.W_3, self.W_4, self.W_5, self.W_6, self.W_7\n    ]:\n      nn.init.xavier_uniform_(xx.data)\n\n  def forward(self, input_set):\n    """"""\n      Args:\n        input_set: shape N X D\n\n      Returns:\n        output_set: shape N X 1\n    """"""\n    num_element = input_set.shape[0]\n    element_dim = input_set.shape[1]\n    assert element_dim == self.element_dim\n    hidden = torch.zeros(1, 2 * self.element_dim).to(input_set.device)\n    memory = torch.zeros(1, self.element_dim).to(input_set.device)\n\n    # encoding\n    for tt in range(self.num_step_encoder):\n      hidden, memory = self.LSTM_encoder(hidden, memory)\n      energy = torch.tanh(torch.mm(hidden, self.W_1) + input_set).mm(self.W_2)\n      att_weight = F.softmax(energy, dim=0)\n      read = (input_set * att_weight).sum(dim=0, keepdim=True)\n      hidden = torch.cat([hidden, read], dim=1)\n\n    # decoding\n    memory = torch.zeros_like(memory)\n    output_set = []\n    for tt in range(num_element):\n      hidden, memory = self.LSTM_decoder(hidden, memory)\n      energy = torch.tanh(torch.mm(hidden, self.W_3) + input_set).mm(self.W_4)\n      att_weight = F.softmax(energy, dim=0)\n      read = (input_set * att_weight).sum(dim=0, keepdim=True)\n      hidden = torch.cat([hidden, read], dim=1)\n      energy = torch.tanh(torch.mm(read, self.W_5) + torch.mm(\n          input_set, self.W_6)).mm(self.W_7)\n      output_set += [torch.argmax(energy)]\n\n    return torch.stack(output_set)\n'"
operators/__init__.py,0,b''
operators/build_segment_reduction.py,5,"b'import os\nimport torch\nfrom subprocess import call\n\nif torch.__version__[0] == \'1\':\n  from setuptools import setup\n  from torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\n  this_file = os.path.dirname(os.path.realpath(__file__))\n  print(this_file)\n  os.makedirs(\'_ext/segment_reduction/\')\n\n  sources = [\'src/segment_reduction.cpp\']\n  headers = [\'src/segment_reduction.h\']\n  defines = []\n  with_cuda = False\n  extra_objects = []\n\n  if torch.cuda.is_available():\n    print(\'Including CUDA code.\')\n    sources += [\'src/segment_reduction_cuda.cpp\']\n    headers += [\'src/segment_reduction_cuda.h\']\n    defines += [(\'WITH_CUDA\', None)]\n    with_cuda = True\n\n    extra_objects = [\'src/cuda/segment_reduction.cu.o\']\n    extra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\n\n  setup(\n      name=\'segment_reduction\',\n      ext_modules=[\n          CUDAExtension(\n              name=\'segment_reduction\',\n              sources=sources)\n      ],\n      cmdclass={\'build_ext\': BuildExtension})\n\n  call(\'mv segment_reduction*.so _ext/segment_reduction/\', shell=True)\n\nelse:\n  from torch.utils.ffi import create_extension  \n\n  this_file = os.path.dirname(os.path.realpath(__file__))\n  print(this_file)\n\n  sources = [\'src/segment_reduction.c\']\n  headers = [\'src/segment_reduction.h\']\n  defines = []\n  with_cuda = False\n  extra_objects = []\n\n  if torch.cuda.is_available():\n    print(\'Including CUDA code.\')\n    sources += [\'src/segment_reduction_cuda.c\']\n    headers += [\'src/segment_reduction_cuda.h\']\n    defines += [(\'WITH_CUDA\', None)]\n    with_cuda = True\n\n    extra_objects = [\'src/cuda/segment_reduction.cu.o\']\n    extra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\n\n  ffi = create_extension(\n      \'_ext.segment_reduction\',\n      headers=headers,\n      sources=sources,\n      define_macros=defines,\n      relative_to=__file__,\n      with_cuda=with_cuda,\n      extra_objects=extra_objects,\n      extra_compile_args=[""-std=c99""])\n\n  if with_cuda:\n    call(\'echo ""Compiling segment reduction kernels by nvcc...""\', shell=True)\n    call(\n        \'nvcc -std=c++11 -c -o src/cuda/segment_reduction.cu.o src/cuda/segment_reduction.cu -x cu -Xcompiler -fPIC \\\n              -gencode arch=compute_35,code=sm_35 -gencode arch=compute_52,code=sm_52 -gencode arch=compute_60,code=sm_60 -gencode arch=compute_61,code=sm_61\',\n        shell=True)\n  ffi.build()\n'"
runner/__init__.py,0,b'from runner.qm8_runner import *\nfrom runner.graph_runner import *\n'
runner/graph_runner.py,9,"b'from __future__ import (division, print_function)\nimport os\nimport numpy as np\nimport pickle\nfrom collections import defaultdict\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom tensorboardX import SummaryWriter\n\nfrom model import *\nfrom dataset import *\nfrom utils.logger import get_logger\nfrom utils.train_helper import data_to_gpu, snapshot, load_model, EarlyStopper\n\nlogger = get_logger(\'exp_logger\')\n__all__ = [\'GraphRunner\']\n\n\nclass GraphRunner(object):\n\n  def __init__(self, config):\n    self.config = config\n    self.dataset_conf = config.dataset\n    self.model_conf = config.model\n    self.train_conf = config.train\n    self.test_conf = config.test\n    self.use_gpu = config.use_gpu\n    self.gpus = config.gpus\n    self.writer = SummaryWriter(config.save_dir)\n\n  def train(self):\n    # create data loader\n    train_dataset = eval(self.dataset_conf.loader_name)(\n        self.config, split=\'train\')\n    dev_dataset = eval(self.dataset_conf.loader_name)(self.config, split=\'dev\')\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=self.train_conf.batch_size,\n        shuffle=self.train_conf.shuffle,\n        num_workers=self.train_conf.num_workers,\n        collate_fn=train_dataset.collate_fn,\n        drop_last=False)\n    dev_loader = torch.utils.data.DataLoader(\n        dev_dataset,\n        batch_size=self.train_conf.batch_size,\n        shuffle=False,\n        num_workers=self.train_conf.num_workers,\n        collate_fn=dev_dataset.collate_fn,\n        drop_last=False)\n\n    # create models\n    model = eval(self.model_conf.name)(self.config)\n\n    if self.use_gpu:\n      model = nn.DataParallel(model, device_ids=self.gpus).cuda()\n\n    # create optimizer\n    params = filter(lambda p: p.requires_grad, model.parameters())\n    if self.train_conf.optimizer == \'SGD\':\n      optimizer = optim.SGD(\n          params,\n          lr=self.train_conf.lr,\n          momentum=self.train_conf.momentum,\n          weight_decay=self.train_conf.wd)\n    elif self.train_conf.optimizer == \'Adam\':\n      optimizer = optim.Adam(\n          params, lr=self.train_conf.lr, weight_decay=self.train_conf.wd)\n    else:\n      raise ValueError(""Non-supported optimizer!"")\n\n    early_stop = EarlyStopper([0.0], win_size=10, is_decrease=False)\n\n    lr_scheduler = optim.lr_scheduler.MultiStepLR(\n        optimizer,\n        milestones=self.train_conf.lr_decay_steps,\n        gamma=self.train_conf.lr_decay)\n\n    # reset gradient\n    optimizer.zero_grad()\n\n    # resume training\n    if self.train_conf.is_resume:\n      load_model(model, self.train_conf.resume_model, optimizer=optimizer)\n\n    # Training Loop\n    iter_count = 0\n    best_val_loss = np.inf\n    results = defaultdict(list)\n    for epoch in range(self.train_conf.max_epoch):\n      # validation\n      if (epoch + 1) % self.train_conf.valid_epoch == 0 or epoch == 0:\n        model.eval()\n        val_loss = []\n\n        for data in tqdm(dev_loader):\n          if self.use_gpu:\n            data[\'node_feat\'], data[\'node_mask\'], data[\'label\'] = data_to_gpu(\n                data[\'node_feat\'], data[\'node_mask\'], data[\'label\'])\n\n            if self.model_conf.name == \'LanczosNetGeneral\':\n              data[\'L\'], data[\'D\'], data[\'V\'] = data_to_gpu(\n                  data[\'L\'], data[\'D\'], data[\'V\'])\n            elif self.model_conf.name == \'GraphSAGE\':\n              data[\'nn_idx\'], data[\'nonempty_mask\'] = data_to_gpu(\n                  data[\'nn_idx\'], data[\'nonempty_mask\'])\n            elif self.model_conf.name == \'GPNN\':\n              data[\'L\'], data[\'L_cluster\'], data[\'L_cut\'] = data_to_gpu(\n                  data[\'L\'], data[\'L_cluster\'], data[\'L_cut\'])\n            else:\n              data[\'L\'] = data_to_gpu(data[\'L\'])[0]\n\n          with torch.no_grad():\n            if self.model_conf.name == \'AdaLanczosNet\':\n              pred, _ = model(\n                  data[\'node_feat\'],\n                  data[\'L\'],\n                  label=data[\'label\'],\n                  mask=data[\'node_mask\'])\n            elif self.model_conf.name == \'LanczosNetGeneral\':\n              pred, _ = model(\n                  data[\'node_feat\'],\n                  data[\'L\'],\n                  data[\'D\'],\n                  data[\'V\'],\n                  label=data[\'label\'],\n                  mask=data[\'node_mask\'])\n            elif self.model_conf.name == \'GraphSAGE\':\n              pred, _ = model(\n                  data[\'node_feat\'],\n                  data[\'nn_idx\'],\n                  data[\'nonempty_mask\'],\n                  label=data[\'label\'],\n                  mask=data[\'node_mask\'])\n            elif self.model_conf.name == \'GPNN\':\n              pred, _ = model(\n                  data[\'node_feat\'],\n                  data[\'L\'],\n                  data[\'L_cluster\'],\n                  data[\'L_cut\'],\n                  label=data[\'label\'],\n                  mask=data[\'node_mask\'])\n            else:\n              pred, _ = model(\n                  data[\'node_feat\'],\n                  data[\'L\'],\n                  label=data[\'label\'],\n                  mask=data[\'node_mask\'])\n\n          curr_loss = (pred - data[\'label\']).pow(2).cpu().numpy()\n          val_loss += [curr_loss]\n\n        val_loss = float(np.mean(np.concatenate(val_loss)))\n        logger.info(""Avg. Validation MSE = {}"".format(val_loss))\n        self.writer.add_scalar(\'val_loss\', val_loss, iter_count)\n        results[\'val_loss\'] += [val_loss]\n\n        # save best model\n        if val_loss < best_val_loss:\n          best_val_loss = val_loss\n          snapshot(\n              model.module if self.use_gpu else model,\n              optimizer,\n              self.config,\n              epoch + 1,\n              tag=\'best\')\n\n        logger.info(""Current Best Validation MSE = {}"".format(best_val_loss))\n\n        # check early stop\n        if early_stop.tick([val_loss]):\n          snapshot(\n              model.module if self.use_gpu else model,\n              optimizer,\n              self.config,\n              epoch + 1,\n              tag=\'last\')\n          self.writer.close()\n          break\n\n      # training\n      model.train()\n      lr_scheduler.step()\n      for data in train_loader:\n        optimizer.zero_grad()\n\n        if self.use_gpu:\n          data[\'node_feat\'], data[\'node_mask\'], data[\'label\'] = data_to_gpu(\n              data[\'node_feat\'], data[\'node_mask\'], data[\'label\'])\n\n          if self.model_conf.name == \'LanczosNetGeneral\':\n            data[\'L\'], data[\'D\'], data[\'V\'] = data_to_gpu(\n                data[\'L\'], data[\'D\'], data[\'V\'])\n          elif self.model_conf.name == \'GraphSAGE\':\n            data[\'nn_idx\'], data[\'nonempty_mask\'] = data_to_gpu(\n                data[\'nn_idx\'], data[\'nonempty_mask\'])\n          elif self.model_conf.name == \'GPNN\':\n            data[\'L\'], data[\'L_cluster\'], data[\'L_cut\'] = data_to_gpu(\n                data[\'L\'], data[\'L_cluster\'], data[\'L_cut\'])\n          else:\n            data[\'L\'] = data_to_gpu(data[\'L\'])[0]\n\n        if self.model_conf.name == \'AdaLanczosNet\':\n          _, train_loss = model(\n              data[\'node_feat\'],\n              data[\'L\'],\n              label=data[\'label\'],\n              mask=data[\'node_mask\'])\n        elif self.model_conf.name == \'LanczosNetGeneral\':\n          _, train_loss = model(\n              data[\'node_feat\'],\n              data[\'L\'],\n              data[\'D\'],\n              data[\'V\'],\n              label=data[\'label\'],\n              mask=data[\'node_mask\'])\n        elif self.model_conf.name == \'GraphSAGE\':\n          _, train_loss = model(\n              data[\'node_feat\'],\n              data[\'nn_idx\'],\n              data[\'nonempty_mask\'],\n              label=data[\'label\'],\n              mask=data[\'node_mask\'])\n        elif self.model_conf.name == \'GPNN\':\n          _, train_loss = model(\n              data[\'node_feat\'],\n              data[\'L\'],\n              data[\'L_cluster\'],\n              data[\'L_cut\'],\n              label=data[\'label\'],\n              mask=data[\'node_mask\'])\n        else:\n          _, train_loss = model(\n              data[\'node_feat\'],\n              data[\'L\'],\n              label=data[\'label\'],\n              mask=data[\'node_mask\'])\n\n        # assign gradient\n        train_loss.backward()\n        optimizer.step()\n        train_loss = float(train_loss.data.cpu().numpy())\n        self.writer.add_scalar(\'train_loss\', train_loss, iter_count)\n        results[\'train_loss\'] += [train_loss]\n        results[\'train_step\'] += [iter_count]\n\n        # display loss\n        if (iter_count + 1) % self.train_conf.display_iter == 0:\n          logger.info(""Loss @ epoch {:04d} iteration {:08d} = {}"".format(\n              epoch + 1, iter_count + 1, train_loss))\n\n        iter_count += 1\n\n      # snapshot model\n      if (epoch + 1) % self.train_conf.snapshot_epoch == 0:\n        logger.info(""Saving Snapshot @ epoch {:04d}"".format(epoch + 1))\n        snapshot(model.module\n                 if self.use_gpu else model, optimizer, self.config, epoch + 1)\n\n    results[\'best_val_loss\'] += [best_val_loss]\n    pickle.dump(results,\n                open(os.path.join(self.config.save_dir, \'train_stats.p\'), \'wb\'))\n    self.writer.close()\n    logger.info(""Best Validation MSE = {}"".format(best_val_loss))\n\n    return best_val_loss\n\n  def test(self):\n    test_dataset = eval(self.dataset_conf.loader_name)(\n        self.config, split=\'test\')\n    # create data loader\n    test_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=self.test_conf.batch_size,\n        shuffle=False,\n        num_workers=self.test_conf.num_workers,\n        collate_fn=test_dataset.collate_fn,\n        drop_last=False)\n\n    # create models\n    model = eval(self.model_conf.name)(self.config)\n    load_model(model, self.test_conf.test_model)\n\n    if self.use_gpu:\n      model = nn.DataParallel(model, device_ids=self.gpus).cuda()\n\n    model.eval()\n    test_loss = []\n    for data in tqdm(test_loader):\n      if self.use_gpu:\n        data[\'node_feat\'], data[\'node_mask\'], data[\'label\'] = data_to_gpu(\n            data[\'node_feat\'], data[\'node_mask\'], data[\'label\'])\n\n        if self.model_conf.name == \'LanczosNetGeneral\':\n          data[\'D\'], data[\'V\'] = data_to_gpu(data[\'D\'], data[\'V\'])\n        elif self.model_conf.name == \'GraphSAGE\':\n          data[\'nn_idx\'], data[\'nonempty_mask\'] = data_to_gpu(\n              data[\'nn_idx\'], data[\'nonempty_mask\'])\n        elif self.model_conf.name == \'GPNN\':\n          data[\'L\'], data[\'L_cluster\'], data[\'L_cut\'] = data_to_gpu(\n              data[\'L\'], data[\'L_cluster\'], data[\'L_cut\'])\n        else:\n          data[\'L\'] = data_to_gpu(data[\'L\'])[0]\n\n      with torch.no_grad():\n        if self.model_conf.name == \'AdaLanczosNet\':\n          pred, _ = model(\n              data[\'node_feat\'],\n              data[\'L\'],\n              label=data[\'label\'],\n              mask=data[\'node_mask\'])\n        elif self.model_conf.name == \'LanczosNetGeneral\':\n          pred, _ = model(\n              data[\'node_feat\'],\n              data[\'L\'],\n              data[\'D\'],\n              data[\'V\'],\n              label=data[\'label\'],\n              mask=data[\'node_mask\'])\n        elif self.model_conf.name == \'GraphSAGE\':\n          pred, _ = model(\n              data[\'node_feat\'],\n              data[\'nn_idx\'],\n              data[\'nonempty_mask\'],\n              label=data[\'label\'],\n              mask=data[\'node_mask\'])\n        elif self.model_conf.name == \'GPNN\':\n          pred, _ = model(\n              data[\'node_feat\'],\n              data[\'L\'],\n              data[\'L_cluster\'],\n              data[\'L_cut\'],\n              label=data[\'label\'],\n              mask=data[\'node_mask\'])\n        else:\n          pred, _ = model(\n              data[\'node_feat\'],\n              data[\'L\'],\n              label=data[\'label\'],\n              mask=data[\'node_mask\'])\n\n        curr_loss = (\n            pred - data[\'label\']).pow(2).cpu().numpy() * self.const_factor\n        test_loss += [curr_loss]\n\n    test_loss = float(np.mean(np.concatenate(test_loss)))\n    logger.info(""Test MSE = {}"".format(test_loss))\n\n    return test_loss\n'"
runner/qm8_runner.py,9,"b'from __future__ import (division, print_function)\nimport os\nimport numpy as np\nimport pickle\nfrom collections import defaultdict\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom tensorboardX import SummaryWriter\n\nfrom model import *\nfrom dataset import *\nfrom utils.logger import get_logger\nfrom utils.train_helper import data_to_gpu, snapshot, load_model, EarlyStopper\n\nlogger = get_logger(\'exp_logger\')\n__all__ = [\'QM8Runner\']\n\n\nclass QM8Runner(object):\n\n  def __init__(self, config):\n    self.config = config\n    self.dataset_conf = config.dataset\n    self.model_conf = config.model\n    self.train_conf = config.train\n    self.test_conf = config.test\n    self.use_gpu = config.use_gpu\n    self.gpus = config.gpus\n    self.writer = SummaryWriter(config.save_dir)\n    self.meta_data = pickle.load(open(self.dataset_conf.meta_data_path, \'rb\'))\n    self.const_factor = self.meta_data[\'std\'].reshape(1, -1)\n\n  def train(self):\n    # create data loader\n    train_dataset = eval(self.dataset_conf.loader_name)(\n        self.config, split=\'train\')\n    dev_dataset = eval(self.dataset_conf.loader_name)(self.config, split=\'dev\')\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=self.train_conf.batch_size,\n        shuffle=self.train_conf.shuffle,\n        num_workers=self.train_conf.num_workers,\n        collate_fn=train_dataset.collate_fn,\n        drop_last=False)\n    dev_loader = torch.utils.data.DataLoader(\n        dev_dataset,\n        batch_size=self.train_conf.batch_size,\n        shuffle=False,\n        num_workers=self.train_conf.num_workers,\n        collate_fn=dev_dataset.collate_fn,\n        drop_last=False)\n\n    # create models\n    model = eval(self.model_conf.name)(self.config)\n\n    if self.use_gpu:\n      model = nn.DataParallel(model, device_ids=self.gpus).cuda()\n\n    # create optimizer\n    params = filter(lambda p: p.requires_grad, model.parameters())\n    if self.train_conf.optimizer == \'SGD\':\n      optimizer = optim.SGD(\n          params,\n          lr=self.train_conf.lr,\n          momentum=self.train_conf.momentum,\n          weight_decay=self.train_conf.wd)\n    elif self.train_conf.optimizer == \'Adam\':\n      optimizer = optim.Adam(\n          params, lr=self.train_conf.lr, weight_decay=self.train_conf.wd)\n    else:\n      raise ValueError(""Non-supported optimizer!"")\n\n    early_stop = EarlyStopper([0.0], win_size=10, is_decrease=False)\n\n    lr_scheduler = optim.lr_scheduler.MultiStepLR(\n        optimizer,\n        milestones=self.train_conf.lr_decay_steps,\n        gamma=self.train_conf.lr_decay)\n\n    # reset gradient\n    optimizer.zero_grad()\n\n    # resume training\n    if self.train_conf.is_resume:\n      load_model(model, self.train_conf.resume_model, optimizer=optimizer)\n\n    # Training Loop\n    iter_count = 0\n    best_val_loss = np.inf\n    results = defaultdict(list)\n    for epoch in range(self.train_conf.max_epoch):\n      # validation\n      if (epoch + 1) % self.train_conf.valid_epoch == 0 or epoch == 0:\n        model.eval()\n        val_loss = []\n\n        for data in tqdm(dev_loader):\n          if self.use_gpu:\n            data[\'node_feat\'], data[\'node_mask\'], data[\'label\'] = data_to_gpu(\n                data[\'node_feat\'], data[\'node_mask\'], data[\'label\'])\n\n            if self.model_conf.name == \'LanczosNet\':\n              data[\'L\'], data[\'D\'], data[\'V\'] = data_to_gpu(\n                  data[\'L\'], data[\'D\'], data[\'V\'])\n            elif self.model_conf.name == \'GraphSAGE\':\n              data[\'nn_idx\'], data[\'nonempty_mask\'] = data_to_gpu(\n                  data[\'nn_idx\'], data[\'nonempty_mask\'])\n            elif self.model_conf.name == \'GPNN\':\n              data[\'L\'], data[\'L_cluster\'], data[\'L_cut\'] = data_to_gpu(\n                  data[\'L\'], data[\'L_cluster\'], data[\'L_cut\'])\n            else:\n              data[\'L\'] = data_to_gpu(data[\'L\'])[0]\n\n          with torch.no_grad():\n            if self.model_conf.name == \'AdaLanczosNet\':\n              pred, _ = model(\n                  data[\'node_feat\'],\n                  data[\'L\'],\n                  label=data[\'label\'],\n                  mask=data[\'node_mask\'])\n            elif self.model_conf.name == \'LanczosNet\':\n              pred, _ = model(\n                  data[\'node_feat\'],\n                  data[\'L\'],\n                  data[\'D\'],\n                  data[\'V\'],\n                  label=data[\'label\'],\n                  mask=data[\'node_mask\'])\n            elif self.model_conf.name == \'GraphSAGE\':\n              pred, _ = model(\n                  data[\'node_feat\'],\n                  data[\'nn_idx\'],\n                  data[\'nonempty_mask\'],\n                  label=data[\'label\'],\n                  mask=data[\'node_mask\'])\n            elif self.model_conf.name == \'GPNN\':\n              pred, _ = model(\n                  data[\'node_feat\'],\n                  data[\'L\'],\n                  data[\'L_cluster\'],\n                  data[\'L_cut\'],\n                  label=data[\'label\'],\n                  mask=data[\'node_mask\'])\n            else:\n              pred, _ = model(\n                  data[\'node_feat\'],\n                  data[\'L\'],\n                  label=data[\'label\'],\n                  mask=data[\'node_mask\'])\n\n          curr_loss = (\n              pred - data[\'label\']).abs().cpu().numpy() * self.const_factor\n          val_loss += [curr_loss]\n\n        val_loss = float(np.mean(np.concatenate(val_loss)))\n        logger.info(""Avg. Validation MAE = {}"".format(val_loss))\n        self.writer.add_scalar(\'val_loss\', val_loss, iter_count)\n        results[\'val_loss\'] += [val_loss]\n\n        # save best model\n        if val_loss < best_val_loss:\n          best_val_loss = val_loss\n          snapshot(\n              model.module if self.use_gpu else model,\n              optimizer,\n              self.config,\n              epoch + 1,\n              tag=\'best\')\n\n        logger.info(""Current Best Validation MAE = {}"".format(best_val_loss))\n\n        # check early stop\n        if early_stop.tick([val_loss]):\n          snapshot(\n              model.module if self.use_gpu else model,\n              optimizer,\n              self.config,\n              epoch + 1,\n              tag=\'last\')\n          self.writer.close()\n          break\n\n      # training\n      model.train()\n      lr_scheduler.step()\n      for data in train_loader:\n        optimizer.zero_grad()\n\n        if self.use_gpu:\n          data[\'node_feat\'], data[\'node_mask\'], data[\'label\'] = data_to_gpu(\n              data[\'node_feat\'], data[\'node_mask\'], data[\'label\'])\n\n          if self.model_conf.name == \'LanczosNet\':\n            data[\'L\'], data[\'D\'], data[\'V\'] = data_to_gpu(\n                data[\'L\'], data[\'D\'], data[\'V\'])\n          elif self.model_conf.name == \'GraphSAGE\':\n            data[\'nn_idx\'], data[\'nonempty_mask\'] = data_to_gpu(\n                data[\'nn_idx\'], data[\'nonempty_mask\'])\n          elif self.model_conf.name == \'GPNN\':\n            data[\'L\'], data[\'L_cluster\'], data[\'L_cut\'] = data_to_gpu(\n                data[\'L\'], data[\'L_cluster\'], data[\'L_cut\'])\n          else:\n            data[\'L\'] = data_to_gpu(data[\'L\'])[0]\n\n        if self.model_conf.name == \'AdaLanczosNet\':\n          _, train_loss = model(\n              data[\'node_feat\'],\n              data[\'L\'],\n              label=data[\'label\'],\n              mask=data[\'node_mask\'])\n        elif self.model_conf.name == \'LanczosNet\':\n          _, train_loss = model(\n              data[\'node_feat\'],\n              data[\'L\'],\n              data[\'D\'],\n              data[\'V\'],\n              label=data[\'label\'],\n              mask=data[\'node_mask\'])\n        elif self.model_conf.name == \'GraphSAGE\':\n          _, train_loss = model(\n              data[\'node_feat\'],\n              data[\'nn_idx\'],\n              data[\'nonempty_mask\'],\n              label=data[\'label\'],\n              mask=data[\'node_mask\'])\n        elif self.model_conf.name == \'GPNN\':\n          _, train_loss = model(\n              data[\'node_feat\'],\n              data[\'L\'],\n              data[\'L_cluster\'],\n              data[\'L_cut\'],\n              label=data[\'label\'],\n              mask=data[\'node_mask\'])\n        else:\n          _, train_loss = model(\n              data[\'node_feat\'],\n              data[\'L\'],\n              label=data[\'label\'],\n              mask=data[\'node_mask\'])\n\n        # assign gradient\n        train_loss.backward()\n        optimizer.step()\n        train_loss = float(train_loss.data.cpu().numpy())\n        self.writer.add_scalar(\'train_loss\', train_loss, iter_count)\n        results[\'train_loss\'] += [train_loss]\n        results[\'train_step\'] += [iter_count]\n\n        # display loss\n        if (iter_count + 1) % self.train_conf.display_iter == 0:\n          logger.info(""Loss @ epoch {:04d} iteration {:08d} = {}"".format(\n              epoch + 1, iter_count + 1, train_loss))\n\n        iter_count += 1\n\n      # snapshot model\n      if (epoch + 1) % self.train_conf.snapshot_epoch == 0:\n        logger.info(""Saving Snapshot @ epoch {:04d}"".format(epoch + 1))\n        snapshot(model.module\n                 if self.use_gpu else model, optimizer, self.config, epoch + 1)\n\n    results[\'best_val_loss\'] += [best_val_loss]\n    pickle.dump(results,\n                open(os.path.join(self.config.save_dir, \'train_stats.p\'), \'wb\'))\n    self.writer.close()\n    logger.info(""Best Validation MAE = {}"".format(best_val_loss))\n\n    return best_val_loss\n\n  def test(self):\n    test_dataset = eval(self.dataset_conf.loader_name)(\n        self.config, split=\'test\')\n    # create data loader\n    test_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=self.test_conf.batch_size,\n        shuffle=False,\n        num_workers=self.test_conf.num_workers,\n        collate_fn=test_dataset.collate_fn,\n        drop_last=False)\n\n    # create models\n    model = eval(self.model_conf.name)(self.config)\n    load_model(model, self.test_conf.test_model)\n\n    if self.use_gpu:\n      model = nn.DataParallel(model, device_ids=self.gpus).cuda()\n\n    model.eval()\n    test_loss = []\n    for data in tqdm(test_loader):\n      if self.use_gpu:\n        data[\'node_feat\'], data[\'node_mask\'], data[\'label\'] = data_to_gpu(\n            data[\'node_feat\'], data[\'node_mask\'], data[\'label\'])\n\n        if self.model_conf.name == \'LanczosNet\':\n          data[\'D\'], data[\'V\'] = data_to_gpu(data[\'D\'], data[\'V\'])\n        elif self.model_conf.name == \'GraphSAGE\':\n          data[\'nn_idx\'], data[\'nonempty_mask\'] = data_to_gpu(\n              data[\'nn_idx\'], data[\'nonempty_mask\'])\n        elif self.model_conf.name == \'GPNN\':\n          data[\'L\'], data[\'L_cluster\'], data[\'L_cut\'] = data_to_gpu(\n              data[\'L\'], data[\'L_cluster\'], data[\'L_cut\'])\n        else:\n          data[\'L\'] = data_to_gpu(data[\'L\'])[0]\n\n      with torch.no_grad():\n        if self.model_conf.name == \'AdaLanczosNet\':\n          pred, _ = model(\n              data[\'node_feat\'],\n              data[\'L\'],\n              label=data[\'label\'],\n              mask=data[\'node_mask\'])\n        elif self.model_conf.name == \'LanczosNet\':\n          pred, _ = model(\n              data[\'node_feat\'],\n              data[\'L\'],\n              data[\'D\'],\n              data[\'V\'],\n              label=data[\'label\'],\n              mask=data[\'node_mask\'])\n        elif self.model_conf.name == \'GraphSAGE\':\n          pred, _ = model(\n              data[\'node_feat\'],\n              data[\'nn_idx\'],\n              data[\'nonempty_mask\'],\n              label=data[\'label\'],\n              mask=data[\'node_mask\'])\n        elif self.model_conf.name == \'GPNN\':\n          pred, _ = model(\n              data[\'node_feat\'],\n              data[\'L\'],\n              data[\'L_cluster\'],\n              data[\'L_cut\'],\n              label=data[\'label\'],\n              mask=data[\'node_mask\'])\n        else:\n          pred, _ = model(\n              data[\'node_feat\'],\n              data[\'L\'],\n              label=data[\'label\'],\n              mask=data[\'node_mask\'])\n\n        curr_loss = (\n            pred - data[\'label\']).abs().cpu().numpy() * self.const_factor\n        test_loss += [curr_loss]\n\n    test_loss = float(np.mean(np.concatenate(test_loss)))\n    logger.info(""Test MAE = {}"".format(test_loss))\n\n    return test_loss\n'"
utils/__init__.py,0,b''
utils/arg_helper.py,0,"b'import os\nimport yaml\nimport time\nimport argparse\nfrom easydict import EasyDict as edict\n\n\ndef parse_arguments():\n  parser = argparse.ArgumentParser(\n      description=""Running Experiments of Deep Prediction"")\n  parser.add_argument(\n      \'-c\',\n      \'--config_file\',\n      type=str,\n      default=""config/resnet101_cifar.json"",\n      required=True,\n      help=""Path of config file"")\n  parser.add_argument(\n      \'-l\',\n      \'--log_level\',\n      type=str,\n      default=\'INFO\',\n      help=""Logging Level, \\\n        DEBUG, \\\n        INFO, \\\n        WARNING, \\\n        ERROR, \\\n        CRITICAL"")\n  parser.add_argument(\'-m\', \'--comment\', help=""Experiment comment"")\n  parser.add_argument(\'-t\', \'--test\', help=""Test model"", action=\'store_true\')\n  args = parser.parse_args()\n\n  return args\n\n\ndef get_config(config_file, exp_dir=None):\n  """""" Construct and snapshot hyper parameters """"""\n  config = edict(yaml.load(open(config_file, \'r\')))\n\n  # create hyper parameters\n  config.run_id = str(os.getpid())\n  config.exp_name = \'_\'.join([\n      config.model.name, config.dataset.name,\n      time.strftime(\'%Y-%b-%d-%H-%M-%S\'), config.run_id\n  ])\n\n  if exp_dir is not None:\n    config.exp_dir = exp_dir\n\n  config.save_dir = os.path.join(config.exp_dir, config.exp_name)\n\n  # snapshot hyperparameters\n  mkdir(config.exp_dir)\n  mkdir(config.save_dir)\n\n  save_name = os.path.join(config.save_dir, \'config.yaml\')\n  yaml.dump(edict2dict(config), open(save_name, \'w\'), default_flow_style=False)\n\n  return config\n\n\ndef edict2dict(edict_obj):\n  dict_obj = {}\n\n  for key, vals in edict_obj.items():\n    if isinstance(vals, edict):\n      dict_obj[key] = edict2dict(vals)\n    else:\n      dict_obj[key] = vals\n\n  return dict_obj\n\n\ndef mkdir(folder):\n  if not os.path.isdir(folder):\n    os.makedirs(folder)\n'"
utils/data_helper.py,0,"b'import numpy as np\nfrom scipy import sparse as sp\n\nEPS = float(np.finfo(np.float32).eps)\nDRAW_HISTOGRAM = False\nDRAW_APPROXIMATION = False\n\n\ndef check_dist(dist):\n  for dd in dist:\n    if not isinstance(dd, int) and dd != \'inf\':\n      raise ValueError(""Non-supported value of diffusion distance"")\n\n  return dist\n\n\ndef read_idx_file(file_name):\n  idx = []\n  with open(file_name) as f:\n    for line in f:\n      idx += [int(line)]\n\n  return idx\n\n\ndef normalize_features(mx):\n  """"""Row-normalize sparse matrix""""""\n  rowsum = np.array(mx.sum(1))\n  r_inv = np.power(rowsum, -1).flatten()\n  r_inv[np.isinf(r_inv)] = 0.\n  r_mat_inv = sp.diags(r_inv)\n  mx = r_mat_inv.dot(mx)\n  return mx\n\n\ndef check_symmetric(m, tol=1e-8):\n\n  if sp.issparse(m):\n    if m.shape[0] != m.shape[1]:\n      raise ValueError(\'m must be a square matrix\')\n\n    if not isinstance(m, sp.coo_matrix):\n      m = sp.coo_matrix(m)\n\n    r, c, v = m.row, m.col, m.data\n    tril_no_diag = r > c\n    triu_no_diag = c > r\n\n    if triu_no_diag.sum() != tril_no_diag.sum():\n      return False\n\n    rl = r[tril_no_diag]\n    cl = c[tril_no_diag]\n    vl = v[tril_no_diag]\n    ru = r[triu_no_diag]\n    cu = c[triu_no_diag]\n    vu = v[triu_no_diag]\n\n    sortl = np.lexsort((cl, rl))\n    sortu = np.lexsort((ru, cu))\n    vl = vl[sortl]\n    vu = vu[sortu]\n\n    return np.allclose(vl, vu, atol=tol)\n  else:\n    return np.allclose(m, m.T, atol=tol)\n\n\ndef preprocess_feature(feature, norm_method=None):\n  """""" Normalize feature matrix """"""\n\n  if norm_method == \'L1\':\n    # L1 norm\n    feature /= (feature.sum(1, keepdims=True) + EPS)\n\n  elif norm_method == \'L2\':\n    # L2 norm\n    feature /= (np.sqrt(np.square(feature).sum(1, keepdims=True)) + EPS)\n\n  elif norm_method == \'std\':\n    # Standardize\n    std = np.std(feature, axis=0, keepdims=True)\n    feature -= np.mean(feature, 0, keepdims=True)\n    feature /= (std + EPS)\n  else:\n    # nothing\n    pass\n\n  return feature\n\n\ndef normalize_adj(A, is_sym=True, exponent=0.5):\n  """"""\n    Normalize adjacency matrix\n\n    is_sym=True: D^{-1/2} A D^{-1/2}\n    is_sym=False: D^{-1} A\n  """"""\n  rowsum = np.array(A.sum(1))\n\n  if is_sym:\n    r_inv = np.power(rowsum, -exponent).flatten()\n  else:\n    r_inv = np.power(rowsum, -1.0).flatten()\n\n  r_inv[np.isinf(r_inv)] = 0.\n\n  if sp.isspmatrix(A):\n    r_mat_inv = sp.diags(r_inv.squeeze())\n  else:\n    r_mat_inv = np.diag(r_inv)\n\n  if is_sym:\n    return r_mat_inv.dot(A).dot(r_mat_inv)\n  else:\n    return r_mat_inv.dot(A)\n\n\ndef get_laplacian(adj, graph_laplacian_type=\'L1\', alpha=0.5):\n  """"""\n    Compute Graph Laplacian\n\n    Args:\n      adj: shape N X N, adjacency matrix, could be numpy or scipy sparse array\n      graph_laplacian_type:\n        -L1: use combinatorial graph Laplacian, L = D - A\n        -L2: use symmetric graph Laplacian, L = I - D^{-1/2} A D^{-1/2}\n        -L3: use asymmetric graph Laplacian, L = I - D^{-1} A\n        -L4: use symmetric GCN renormalization trick, L = D^{-1/2} ( I + A ) D^{-1/2}\n        -L5: use asymmetric GCN renormalization trick, L = D^{-1} ( I + A )\n        -L6: use symmetric diffusion map, L = D^{-alpha} A D^{-alpha},\n              where e.g., A_{i,j} = k(x_i, x_j), alpha is typically from [0, 1]\n        -L7: use asymmetric diffusion map, L = D^{-1} A, A as L5\n\n    Returns:\n      L: shape N X N, graph Laplacian matrix\n  """"""\n\n  assert len(adj.shape) == 2 and adj.shape[0] == adj.shape[1]\n\n  if sp.isspmatrix(adj):\n    identity_mat = sp.eye(adj.shape[0])\n  else:\n    identity_mat = np.eye(adj.shape[0])\n\n  if graph_laplacian_type == \'L1\':\n    if sp.isspmatrix(adj):\n      L = sp.diags(np.array(adj.sum(axis=1)).squeeze()) - adj\n    else:\n      L = np.diag(adj.sum(axis=1).squeeze()) - adj\n  elif graph_laplacian_type == \'L2\':\n    L = identity_mat - normalize_adj(adj, is_sym=True)\n  elif graph_laplacian_type == \'L3\':\n    L = identity_mat - normalize_adj(adj, is_sym=False)\n  elif graph_laplacian_type == \'L4\':\n    L = normalize_adj(identity_mat + adj, is_sym=True)\n  elif graph_laplacian_type == \'L5\':\n    L = normalize_adj(identity_mat + adj, is_sym=False)\n  elif graph_laplacian_type == \'L6\':\n    L = normalize_adj(adj, is_sym=True, exponent=alpha)\n  elif graph_laplacian_type == \'L7\':\n    L = normalize_adj(adj, is_sym=False)\n  else:\n    raise ValueError(\'Unsupported Graph Laplacian!\')\n\n  return L\n\n\ndef get_graph_laplacian_eigs(adj,\n                             k=100,\n                             graph_laplacian_type=\'L1\',\n                             alpha=0.5,\n                             use_eigen_decomp=False,\n                             is_sym=True):\n  """"""\n    Compute first k largest eigenvalues and eigenvectors of graph Laplacian\n\n    Args:\n      adj: shape N X N, adjacency matrix, could be numpy or scipy sparse array\n      k: int, number of eigenvalues and eigenvectors\n      graph_laplacian_type: see arguments of get_laplacian for explanation\n      alpha: float, scale parameter of diffusion map\n      use_eigen_decomp: bool, indicates whether use eigen decomposition, note\n                        it is computationally heavy for large size adj\n      is_sym: bool, indicates whether adj is symmetric\n\n    Returns:\n      eig: shape K X 1, eigenvalues\n      V: shape N X K, eigenvectors, each column is an eigenvector\n      L: shape N X N, graph Laplacian, has the same type as adj\n  """"""\n\n  # compute symmetric graph Laplacian\n  L = get_laplacian(adj, graph_laplacian_type=graph_laplacian_type, alpha=alpha)\n  assert is_sym == check_symmetric(L)\n\n  try:\n    # apply Eigen-Decomposition methods\n    if use_eigen_decomp:\n      if is_sym:\n        eigs, V = np.linalg.eigh(L)\n      else:\n        eigs, V = np.linalg.eig(L)\n    else:\n      if is_sym:\n        # apply Lanczos\n        # N.B.: it can be very slow if k >= 2000\n        eigs, V = sp.linalg.eigsh(L, k=k, which=\'LM\')\n      else:\n        # apply Arnoldi\n        eigs, V = sp.linalg.eigs(L, k=k, which=\'LM\')\n\n    ### TODO: handle cases where eigs are complex\n    if np.any(np.iscomplex(eigs)):\n      print(\'Warning: there are complex eigenvalues!\')\n\n    # magnitude\n    eigs_M = np.abs(eigs)\n\n    # sort it following descending order\n    idx = np.argsort(-eigs_M, kind=\'mergesort\')\n    eigs = eigs[idx[:k]]\n    V = V[:, idx[:k]]\n  except:\n    print(\'Warning: computing eigenvalues failed!\')\n    eigs, V = None, None\n\n  # draw some statistics\n  if DRAW_HISTOGRAM:\n    import matplotlib\n    matplotlib.use(\'Agg\')\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    sns.set(color_codes=True)\n    plt.figure()\n    sns.distplot(eigs, bins=100, kde=False, rug=False)\n    plt.savefig(\'hist_eigs.png\', bbox_inches=\'tight\')\n\n  if DRAW_APPROXIMATION:\n    import matplotlib\n    matplotlib.use(\'Agg\')\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n    rank_set = [5, 10, 20, 50, 100, 200, 300]\n    loss = []\n    L1 = V.dot(np.diag(eigs)).dot(V.T)\n\n    for k in rank_set:\n      eigk, Vk = sp.linalg.eigsh(L, k=k)\n      Lk = Vk.dot(np.diag(eigk)).dot(Vk.T)\n      loss += [np.linalg.norm(L1 - Lk)]\n\n    plt.figure()\n    sns.tsplot(loss)\n    plt.xticks(np.arange(len(loss)), rank_set)\n    plt.savefig(\'loss_eigs.png\', bbox_inches=\'tight\')\n\n  return eigs, V, L\n\n\ndef get_multi_graph_laplacian_eigs(adjs,\n                                   k=100,\n                                   graph_laplacian_type=\'L1\',\n                                   alpha=0.5,\n                                   use_eigen_decomp=False,\n                                   is_sym=True):\n  """"""\n    See comments of get_graph_laplacian_eigs for more information\n\n    Args:\n      adjs: shape N X N X K, K is # edge types\n\n    Returns:\n      eigs_list: list of eigenvalues for each edge type\n      V_list: list of eigenvectors for each edge type\n      L_list: list of graph Laplacian for each edge type\n  """"""\n  V_list = []\n  L_list = []\n  eigs_list = []\n  for ii in range(adjs.shape[2]):\n    eigs, V, L = get_graph_laplacian_eigs(\n        adjs[:, :, ii],\n        k=k,\n        graph_laplacian_type=graph_laplacian_type,\n        alpha=alpha,\n        use_eigen_decomp=use_eigen_decomp,\n        is_sym=is_sym)\n    V_list += [V]\n    L_list += [L]\n    eigs_list += [eigs]\n\n  return eigs_list, V_list, L_list\n\n\ndef test_laplacian():\n  adj = np.array([[0, 1, 0, 0, 1, 0], [1, 0, 1, 0, 1, 0], [0, 1, 0, 1, 0, 0],\n                  [0, 0, 1, 0, 1, 1], [1, 1, 0, 1, 0, 0],\n                  [0, 0, 0, 1, 0, 0]]).astype(np.float32)\n  print(\'=\' * 80)\n  print(\'L1 = {}\'.format(\n      get_laplacian(adj, graph_laplacian_type=\'L1\', alpha=0.5)))\n  print(\'=\' * 80)\n  print(\'L2 = {}\'.format(\n      get_laplacian(adj, graph_laplacian_type=\'L2\', alpha=0.5)))\n  print(\'=\' * 80)\n  print(\'L3 = {}\'.format(\n      get_laplacian(adj, graph_laplacian_type=\'L3\', alpha=0.5)))\n  print(\'=\' * 80)\n  print(\'L4 = {}\'.format(\n      get_laplacian(adj, graph_laplacian_type=\'L4\', alpha=0.5)))\n  print(\'=\' * 80)\n  print(\'L5 = {}\'.format(\n      get_laplacian(adj, graph_laplacian_type=\'L5\', alpha=0.5)))\n  print(\'=\' * 80)\n  print(\'L6 = {}\'.format(\n      get_laplacian(adj, graph_laplacian_type=\'L6\', alpha=0.5)))\n  print(\'=\' * 80)\n  print(\'L7 = {}\'.format(\n      get_laplacian(adj, graph_laplacian_type=\'L7\', alpha=0.5)))\n\n  adj_sp = sp.coo_matrix(adj)\n  print(\'=\' * 80)\n  print(\'L1_sp = {}\'.format(\n      get_laplacian(adj_sp, graph_laplacian_type=\'L1\', alpha=0.5)))\n  print(\'=\' * 80)\n  print(\'L2_sp = {}\'.format(\n      get_laplacian(adj_sp, graph_laplacian_type=\'L2\', alpha=0.5)))\n  print(\'=\' * 80)\n  print(\'L3_sp = {}\'.format(\n      get_laplacian(adj_sp, graph_laplacian_type=\'L3\', alpha=0.5)))\n  print(\'=\' * 80)\n  print(\'L4_sp = {}\'.format(\n      get_laplacian(adj_sp, graph_laplacian_type=\'L4\', alpha=0.5)))\n  print(\'=\' * 80)\n  print(\'L5_sp = {}\'.format(\n      get_laplacian(adj_sp, graph_laplacian_type=\'L5\', alpha=0.5)))\n  print(\'=\' * 80)\n  print(\'L6_sp = {}\'.format(\n      get_laplacian(adj_sp, graph_laplacian_type=\'L6\', alpha=0.5)))\n  print(\'=\' * 80)\n  print(\'L7_sp = {}\'.format(\n      get_laplacian(adj_sp, graph_laplacian_type=\'L7\', alpha=0.5)))\n\n\ndef test_eigs():\n  adj = np.array([[0, 1, 0, 0, 1, 0], [1, 0, 1, 0, 1, 0], [0, 1, 0, 1, 0, 0],\n                  [0, 0, 1, 0, 1, 1], [1, 1, 0, 1, 0, 0],\n                  [0, 0, 0, 1, 0, 0]]).astype(np.float32)\n\n  D, V, L = get_graph_laplacian_eigs(\n      adj,\n      k=100,\n      graph_laplacian_type=\'L1\',\n      alpha=0.5,\n      use_eigen_decomp=True,\n      is_sym=True)\n  print(\'=\' * 80)\n  print(\'Eig values = {}\'.format(D))\n  print(\'=\' * 80)\n  print(\'Eig vectors = {}\'.format(V))\n  print(\'=\' * 80)\n  print(\'Graph Laplacian = {}\'.format(L))\n\n  adj_sp = sp.coo_matrix(adj)\n  D_sp, V_sp, L_sp = get_graph_laplacian_eigs(\n      adj_sp,\n      k=100,\n      graph_laplacian_type=\'L1\',\n      alpha=0.5,\n      use_eigen_decomp=True,\n      is_sym=True)\n  print(\'=\' * 80)\n  print(\'Eig values = {}\'.format(D_sp))\n  print(\'=\' * 80)\n  print(\'Eig vectors = {}\'.format(V_sp))\n  print(\'=\' * 80)\n  print(\'Graph Laplacian = {}\'.format(L_sp))\n\n\nif __name__ == \'__main__\':\n  # test_laplacian()\n  test_eigs()\n'"
utils/logger.py,0,"b'import logging\n\n\ndef setup_logging(log_level, log_file, logger_name=""exp_logger""):\n  """""" Setup logging """"""\n  numeric_level = getattr(logging, log_level.upper(), None)\n  if not isinstance(numeric_level, int):\n    raise ValueError(""Invalid log level: %s"" % log_level)\n\n  logging.basicConfig(\n      filename=log_file,\n      filemode=""w"",\n      format=\n      ""%(levelname)-5s | %(asctime)s | File %(filename)-20s | Line %(lineno)-5d | %(message)s"",\n      datefmt=""%m/%d/%Y %I:%M:%S %p"",\n      level=numeric_level)\n\n  # define a Handler which writes messages to the sys.stderr\n  console = logging.StreamHandler()\n  console.setLevel(numeric_level)\n  # set a format which is simpler for console use\n  formatter = logging.Formatter(\n      ""%(levelname)-5s | %(asctime)s | %(filename)-25s | line %(lineno)-5d: %(message)s""\n  )\n  # tell the handler to use this format\n  console.setFormatter(formatter)\n  # add the handler to the root logger\n  logging.getLogger(logger_name).addHandler(console)\n\n  return get_logger(logger_name)\n\n\ndef get_logger(logger_name=""exp_logger""):\n  return logging.getLogger(logger_name)\n'"
utils/spectral_graph_partition.py,0,"b'import scipy\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\nfrom utils.data_helper import get_laplacian\n\n__all__ = [\'spectral_clustering\', \'get_L_cluster_cut\']\n\n\ndef spectral_clustering(L, K, seed=1234):\n  """"""\n  Implement paper ""Shi, J. and Malik, J., 2000. Normalized cuts and image \n  segmentation. IEEE Transactions on pattern analysis and machine intelligence, \n  22(8), pp.888-905.""\n\n  Args:\n    L: graph Laplacian, numpy or scipy matrix\n    K: int, number of clusters\n\n  Returns:\n    node_label: list\n\n  N.B.: for simplicity, we only consider simple and undirected graph\n  """"""\n  num_nodes = L.shape[0]\n  assert (K < num_nodes - 1)\n\n  eig, eig_vec = scipy.sparse.linalg.eigsh(\n      L, k=K, which=\'LM\', maxiter=num_nodes * 10000, tol=0, mode=\'normal\')\n  kmeans = KMeans(n_clusters=K, random_state=seed).fit(eig_vec.real)\n\n  return kmeans.labels_\n\n\ndef get_L_cluster_cut(L, node_label):\n  adj = L - np.diag(np.diag(L))\n  adj[adj != 0] = 1.0\n  num_nodes = adj.shape[0]\n  idx_row, idx_col = np.meshgrid(range(num_nodes), range(num_nodes))\n  idx_row, idx_col = idx_row.flatten().astype(\n      np.int64), idx_col.flatten().astype(np.int64)\n  mask = (node_label[idx_row] == node_label[idx_col]).reshape(\n      num_nodes, num_nodes).astype(np.float)\n\n  adj_cluster = adj * mask\n  adj_cut = adj - adj_cluster\n  L_cut = get_laplacian(adj_cut, graph_laplacian_type=\'L4\')\n  L_cluster = get_laplacian(adj_cluster, graph_laplacian_type=\'L4\')\n\n  return L_cluster, L_cut'"
utils/train_helper.py,2,"b'import os\nimport torch\n\n\ndef data_to_gpu(*input_data):\n  return_data = []\n  for dd in input_data:\n    if type(dd).__name__ == \'Tensor\':\n      return_data += [dd.cuda()]\n\n  return tuple(return_data)\n\n\ndef snapshot(model, optimizer, config, step, gpus=[0], tag=None):\n  model_snapshot = {\n      ""model"": model.state_dict(),\n      ""optimizer"": optimizer.state_dict(),\n      ""step"": step\n  }\n\n  torch.save(\n      model_snapshot,\n      os.path.join(\n          config.save_dir, ""model_snapshot_{}.pth"".format(tag)\n          if tag is not None else ""model_snapshot_{:07d}.pth"".format(step)))\n\n\ndef load_model(model, file_name, optimizer=None):\n  model_snapshot = torch.load(file_name)\n  model.load_state_dict(model_snapshot[""model""])\n  if optimizer is not None:\n    optimizer.load_state_dict(model_snapshot[""optimizer""])\n\n\nclass EarlyStopper(object):\n  """""" \n    Check whether the early stop condition (always \n    observing decrease in a window of time steps) is met.\n\n    Usage:\n      my_stopper = EarlyStopper([0, 0], 1)\n      is_stop = my_stopper.tick([-1,-1]) # returns True\n  """"""\n\n  def __init__(self, init_val, win_size=10, is_decrease=True):\n    if not isinstance(init_val, list):\n      raise ValueError(""EarlyStopper only takes list of int/floats"")\n\n    self._win_size = win_size\n    self._num_val = len(init_val)\n    self._val = [[False] * win_size for _ in range(self._num_val)]\n    self._last_val = init_val[:]\n    self._comp_func = (lambda x, y: x < y) if is_decrease else (\n        lambda x, y: x >= y)\n\n  def tick(self, val):\n    if not isinstance(val, list):\n      raise ValueError(""EarlyStopper only takes list of int/floats"")\n\n    assert len(val) == self._num_val\n\n    for ii in range(self._num_val):\n      self._val[ii].pop(0)\n\n      if self._comp_func(val[ii], self._last_val[ii]):\n        self._val[ii].append(True)\n      else:\n        self._val[ii].append(False)\n\n      self._last_val[ii] = val[ii]\n\n    is_stop = all([all(xx) for xx in self._val])\n\n    return is_stop\n'"
operators/functions/__init__.py,0,b''
operators/functions/unsorted_segment_sum.py,3,"b""import math\nimport numpy as np\nimport torch\nfrom torch.autograd import Function, Variable\nfrom operators._ext import segment_reduction\n\n\nclass UnsortedSegmentSumFunction(Function):\n\n  @staticmethod\n  def forward(ctx, data, segment_index, num_segments):\n\n    # data's shape should be (batch, dim1, dim2), and segment reduction will be performed over dim1\n\n    ctx.save_for_backward(data, segment_index)\n    # data = data.contiguous()\n    # segment_index = segment_index.contiguous()\n\n    if not data.is_cuda:\n      output = torch.FloatTensor(data.size(0), num_segments,\n                                 data.size(2)).zero_()\n      segment_reduction.unsorted_segment_sum_forward(data, segment_index,\n                                                     data.size(), output)\n    else:\n      output = torch.cuda.FloatTensor(data.size(0), num_segments,\n                                      data.size(2)).zero_()\n      segment_reduction.unsorted_segment_sum_forward_gpu(data, segment_index,\n                                                         data.size(), output)\n\n    return output\n\n  @staticmethod\n  def backward(ctx, grad_output):\n    data, segment_index = ctx.saved_tensors\n    grad_data = data.new().resize_as_(data).zero_()\n\n    if not data.is_cuda:\n      segment_reduction.unsorted_segment_sum_backward(\n          grad_output.data, segment_index, grad_data.size(), grad_data)\n    else:\n      segment_reduction.unsorted_segment_sum_backward_gpu(\n          grad_output.data, segment_index, grad_data.size(), grad_data)\n\n    return Variable(grad_data), None, None\n"""
operators/modules/__init__.py,0,b''
operators/modules/unsorted_segment_sum.py,2,"b'import torch\nimport torch.nn as nn\nfrom torch.nn.parameter import Parameter\nfrom operators.functions.unsorted_segment_sum import UnsortedSegmentSumFunction\n\n\nclass UnsortedSegmentSum(nn.Module):\n\n  def __init__(self, num_segments):\n    super(UnsortedSegmentSum, self).__init__()\n    self.num_segments = num_segments\n\n  def forward(self, data, segment_index):\n    return UnsortedSegmentSumFunction.apply(data, segment_index,\n                                            self.num_segments)\n'"
