file_path,api_count,code
setup.py,0,"b""from setuptools import setup, find_packages\n\nsetup(\n  name = 'reformer_pytorch',\n  packages = find_packages(exclude=['examples', 'pretraining']),\n  version = '1.0.2',\n  license='MIT',\n  description = 'Reformer, the Efficient Transformer, Pytorch',\n  author = 'Phil Wang',\n  author_email = 'lucidrains@gmail.com',\n  url = 'https://github.com/lucidrains/reformer-pytorch',\n  keywords = ['transformers', 'attention', 'artificial intelligence'],\n  install_requires=[\n      'torch',\n      'product-key-memory',\n      'axial-positional-embedding>=0.1.0'\n  ],\n  classifiers=[\n      'Development Status :: 4 - Beta',\n      'Intended Audience :: Developers',\n      'Topic :: Scientific/Engineering :: Artificial Intelligence',\n      'License :: OSI Approved :: MIT License',\n      'Programming Language :: Python :: 3.6',\n  ],\n)"""
pretraining/self-supervised.py,29,"b'import re\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, random_split\n\nfrom tqdm import tqdm\n\nfrom reformer_pytorch import Reformer, ReformerLM\nfrom transformers import BertTokenizer, PreTrainedTokenizer\nfrom fairseq.optim.adafactor import Adafactor\nimport os\nimport json\nimport logging\nfrom datetime import datetime\n\n\nclass WikiDataset(Dataset):\n\n    def __init__(self, path="""", prefix=""train""):\n\n        assert os.path.isdir(path)\n\n        self.documents = []\n        filename_list = os.listdir(path)\n        for file in filename_list:\n            path_to_file = os.path.join(path, file)\n            if not os.path.isfile(path_to_file):\n                continue\n            self.documents.append(path_to_file)\n\n    def __len__(self):\n        """""" Returns the number of documents. """"""\n        return len(self.documents)\n\n    def __getitem__(self, idx):\n        document_path = self.documents[idx]\n        document_name = document_path.split(""/"")[-1]\n\n        items = []\n\n        with open(document_path, encoding=""utf-8"") as source:\n            raw_text = source.readlines()\n            for obj in raw_text:\n                text = json.loads(obj)[\'text\']\n                text = re.sub(\'\\\\n\', \' \', text)\n                text = re.sub(\'\\\\s+\', \' \', text)\n                items.append(text)\n\n        return items\n\n\nclass ReformerTrainer(object):\n\n    def __init__(self,\n                 dataset,\n                 model,\n                 tokenizer,\n                 device=None,\n                 train_batch_size=8,\n                 eval_batch_size=None,\n                 tb_writer=True,\n                 tb_dir=\'./tb_logs\',\n                 log_dir=\'./logs\'):\n        """"""\n        Provides an easy to use class for pretraining and evaluating a Reformer Model.\n\n        :param dataset: (torch.utils.data.Dataset) containing all of the data you wish to utilize during training.\n        :param model: (reformer_pytorch.Reformer)\n        :param tokenizer: (transformers.PreTrainedTokenizer) defaults to BertTokenizer (\'bert-base-case\')\n        :param device: provide manual device placement. If None, will default to cuda:0 if available.\n        :param tb_writer: (bool) Whether to write to tensorboard or not.\n        :param tb_dir: (str) Where to write TB logs to.\n        :param log_dir: (str) Where to write generic logs to.\n        """"""\n\n        self.dataset = dataset\n        self.model = model\n        self.tokenizer = tokenizer\n        self.device = device\n        self.n_gpu = torch.cuda.device_count() if torch.cuda.is_available() else 0\n        self.train_batch_size = train_batch_size\n        self.eval_batch_size = eval_batch_size\n        self.tb_writer = tb_writer\n        self.log_dir = log_dir\n\n        if tokenizer is None:\n            self.tokenizer = BertTokenizer.from_pretrained(\'bert-base-cased\')\n\n        if device is None:\n            self.device = \'cuda:0\' if torch.cuda.is_available() else \'cpu\'\n\n        if eval_batch_size is None:\n            self.eval_batch_size = train_batch_size\n\n        if tb_writer:\n            from torch.utils.tensorboard import SummaryWriter\n            self.writer = SummaryWriter(log_dir=tb_dir)\n\n        logging.basicConfig(filename=f\'{log_dir}/{datetime.now().date()}.log\', level=logging.INFO)\n\n    def build_dataloaders(self, train_test_split=0.1, train_shuffle=True, eval_shuffle=True):\n        """"""\n        Builds the Training and Eval DataLoaders\n\n        :param train_test_split: The ratio split of test to train data.\n        :param train_shuffle: (bool) True if you wish to shuffle the train_dataset.\n        :param eval_shuffle: (bool) True if you wish to shuffle the eval_dataset.\n        :return: train dataloader and evaluation dataloader.\n        """"""\n        dataset_len = len(self.dataset)\n        eval_len = int(dataset_len * train_test_split)\n        train_len = dataset_len - eval_len\n        train_dataset, eval_dataset = random_split(self.dataset, (train_len, eval_len))\n        train_loader = DataLoader(train_dataset, batch_size=self.train_batch_size, shuffle=train_shuffle)\n        eval_loader = DataLoader(eval_dataset, batch_size=self.eval_batch_size, shuffle=eval_shuffle)\n        logging.info(f\'\'\'train_dataloader size: {len(train_loader.dataset)} | shuffle: {train_shuffle}\n                         eval_dataloader size: {len(eval_loader.dataset)} | shuffle: {eval_shuffle}\'\'\')\n        return train_loader, eval_loader\n\n    def mask_tokens(self, inputs: torch.Tensor, mlm_probability=0.15, pad=True):\n        """""" Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. """"""\n        labels = inputs.clone()\n        # mlm_probability defaults to 0.15 in Bert\n        probability_matrix = torch.full(labels.shape, mlm_probability)\n        special_tokens_mask = [\n            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n        ]\n        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n        if self.tokenizer._pad_token is not None:\n            padding_mask = labels.eq(self.tokenizer.pad_token_id)\n            probability_matrix.masked_fill_(padding_mask, value=0.0)\n        masked_indices = torch.bernoulli(probability_matrix).bool()\n        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n\n        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n\n        # 10% of the time, we replace masked input tokens with random word\n        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n        inputs[indices_random] = random_words[indices_random]\n\n        if pad:\n            input_pads = self.tokenizer.max_len - inputs.shape[-1]\n            label_pads = self.tokenizer.max_len - labels.shape[-1]\n\n            inputs = F.pad(inputs, pad=(0, input_pads), value=self.tokenizer.pad_token_id)\n            labels = F.pad(labels, pad=(0, label_pads), value=self.tokenizer.pad_token_id)\n\n        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n        return inputs, labels\n\n    def _tokenize_input_ids(self, input_ids: list, pad_to_max_length: bool = True):\n        """"""\n        Helper function to clean up the train and eval functions\n        :param input_ids: inputs to tokenize.\n        :param pad_to_max_length: Whether you want to pad the inputs to the tokenizer.max_len\n        :return: Tensor containing training data.\n        """"""\n        inputs = torch.cat(\n            [\n                self.tokenizer.encode(\n                    input_ids[i],\n                    add_special_tokens=True,\n                    max_length=self.tokenizer.max_len,\n                    pad_to_max_length=pad_to_max_length,\n                    return_tensors=\'pt\'\n                ) \\\n                for i in range(len(input_ids))\n            ]\n        )\n        return inputs\n\n    def train(self,\n              epochs,\n              train_dataloader,\n              eval_dataloader,\n              log_steps,\n              ckpt_steps,\n              ckpt_dir=None,\n              gradient_accumulation_steps=1):\n        """"""\n        Trains the Reformer Model\n        :param epochs: The number of times you wish to loop through the dataset.\n        :param train_dataloader: (torch.utils.data.DataLoader) The data to train on.\n        :param eval_dataloader: (torch.utils.data.DataLoader) The data to evaluate on.\n        :param log_steps: The number of steps to iterate before logging.\n        :param ckpt_steps: The number of steps to iterate before checkpointing.\n        :param ckpt_dir: The directory to save the checkpoints to.\n        :param gradient_accumulation_steps: Optional gradient accumulation.\n        :return: Total number of steps, total loss, model\n        """"""\n\n        optimizer = Adafactor(self.model.parameters())\n        loss_fn = nn.CrossEntropyLoss()\n        losses = {}\n        global_steps = 0\n        local_steps = 0\n        step_loss = 0.0\n\n        if ckpt_dir is not None:\n            assert os.path.isdir(ckpt_dir)\n            try:\n                logging.info(f\'{datetime.now()} | Continuing from checkpoint...\')\n                self.model.load_state_dict(torch.load(f\'{ckpt_dir}/model_state_dict.pt\', map_location=self.device))\n                optimizer.load_state_dict(torch.load(f\'{ckpt_dir}/optimizer_state_dict.pt\'))\n\n            except Exception as e:\n                logging.info(f\'{datetime.now()} | No checkpoint was found | {e}\')\n\n        self.model.train()\n\n        if self.n_gpu > 1:\n            self.model = nn.DataParallel(self.model)\n            logging.info(f\'{datetime.now()} | Utilizing {self.n_gpu} GPUs\')\n\n        self.model.to(self.device)\n        logging.info(f\'{datetime.now()} | Moved model to: {self.device}\')\n        logging.info(\n            f\'{datetime.now()} | train_batch_size: {self.train_batch_size} | eval_batch_size: {self.eval_batch_size}\')\n        logging.info(f\'{datetime.now()} | Epochs: {epochs} | log_steps: {log_steps} | ckpt_steps: {ckpt_steps}\')\n        logging.info(f\'{datetime.now()} | gradient_accumulation_steps: {gradient_accumulation_steps}\')\n\n        for epoch in tqdm(range(epochs), desc=\'Epochs\', position=0):\n            logging.info(f\'{datetime.now()} | Epoch: {epoch}\')\n            for step, batch in tqdm(enumerate(train_dataloader),\n                                    desc=\'Epoch Iterator\',\n                                    position=1,\n                                    leave=True,\n                                    total=len(train_dataloader)):\n                for data in batch:\n                    inputs = self._tokenize_input_ids(data, pad_to_max_length=True)\n                    inputs, labels = self.mask_tokens(inputs)\n                    inputs, labels = inputs.to(self.device), labels.to(self.device)\n                    output = self.model(inputs)\n\n                    # only calculating loss on masked tokens\n                    loss_mx = labels != -100\n                    output = output[loss_mx].view(-1, self.tokenizer.vocab_size)\n                    labels = labels[loss_mx].view(-1)\n\n                    loss = loss_fn(output, labels)\n\n                    if gradient_accumulation_steps > 1:\n                        loss /= gradient_accumulation_steps\n\n                    loss.backward()\n                    optimizer.step()\n                    self.model.zero_grad()\n\n                    step_loss += loss.item()\n                    losses[global_steps] = loss.item()\n                    local_steps += 1\n                    global_steps += 1\n\n                    if global_steps % log_steps == 0:\n                        if self.tb_writer:\n                            self.writer.add_scalar(\'Train/Loss\', step_loss / local_steps, global_steps)\n                            self.writer.close()\n                        logging.info(\n                            f\'\'\'{datetime.now()} | Train Loss: {step_loss / local_steps} | Steps: {global_steps}\'\'\')\n\n                        with open(f\'{self.log_dir}/train_results.json\', \'w\') as results_file:\n                            json.dump(losses, results_file)\n                            results_file.close()\n                        step_loss = 0.0\n                        local_steps = 0\n\n                    if global_steps % ckpt_steps == 0:\n                        # evaluating before every checkpoint\n                        self.evaluate(eval_dataloader)\n                        model_to_save = self.model.module if hasattr(self.model, \'module\') else self.model\n                        torch.save(model_to_save.state_dict(), f\'{ckpt_dir}/model_state_dict.pt\')\n                        torch.save(optimizer.state_dict(), f\'{ckpt_dir}/optimizer_state_dict.pt\')\n\n                        logging.info(f\'{datetime.now()} | Saved checkpoint to: {ckpt_dir}\')\n\n        model_to_save = self.model.module if hasattr(self.model, \'module\') else self.model\n        torch.save(model_to_save.state_dict(), f\'{ckpt_dir}/model_state_dict.pt\')\n        torch.save(optimizer.state_dict(), f\'{ckpt_dir}/optimizer_state_dict.pt\')\n\n        return self.model\n\n    def evaluate(self, dataloader):\n        """"""\n        Runs through the provided dataloader with torch.no_grad()\n        :param dataloader: (torch.utils.data.DataLoader) Evaluation DataLoader\n        :return: None\n        """"""\n        loss_fn = nn.CrossEntropyLoss()\n\n        if self.n_gpu > 1 and not isinstance(self.model, nn.DataParallel):\n            self.model = nn.DataParallel(self.model)\n\n        self.model.eval()\n        eval_loss = 0.0\n        perplexity = 0.0\n        eval_steps = 0\n\n        logging.info(f\'{datetime.now()} | Evaluating...\')\n        for step, batch in tqdm(enumerate(dataloader), desc=\'Evaluating\', leave=True, total=len(dataloader)):\n            for data in batch:\n                inputs = self._tokenize_input_ids(data, pad_to_max_length=True)\n                inputs, labels = self.mask_tokens(inputs)\n                inputs, labels = inputs.to(self.device), labels.to(self.device)\n\n                with torch.no_grad():\n                    output = self.model(inputs)\n\n                loss_mx = labels != -100\n                output_ids = output[loss_mx].view(-1, self.tokenizer.vocab_size)\n                labels = labels[loss_mx].view(-1)\n                tmp_eval_loss = loss_fn(output_ids, labels)\n                tmp_perplexity = torch.exp(tmp_eval_loss)\n\n                if self.n_gpu > 1:\n                    tmp_eval_loss = tmp_eval_loss.mean()\n\n                eval_loss += tmp_eval_loss.item()\n                perplexity += tmp_perplexity.item()\n                eval_steps += 1\n\n            eval_loss /= eval_steps\n            perplexity /= eval_steps\n\n            if self.tb_writer:\n                self.writer.add_scalar(\'Eval/Loss\', eval_loss, eval_steps)\n                self.writer.close()\n                self.writer.add_scalar(\'Perplexity\', perplexity, eval_steps)\n                self.writer.close()\n            logging.info(f\'{datetime.now()} | Step: {step} | Eval Loss: {eval_loss} | Perplexity: {perplexity}\')\n\n        return None\n\n\nif __name__ == \'__main__\':\n    dataset = WikiDataset(path=\'D:/data/enwiki\')\n    tokenizer = BertTokenizer.from_pretrained(\'bert-base-cased\')\n    tokenizer.max_len = 128\n    model = ReformerLM(\n        num_tokens=tokenizer.vocab_size,\n        dim=512,\n        depth=6,\n        heads=8,\n        max_seq_len=tokenizer.max_len,\n        causal=True\n    )\n    trainer = ReformerTrainer(dataset, model, tokenizer, train_batch_size=32, eval_batch_size=32)\n    train_dataloader, eval_dataloader = trainer.build_dataloaders(train_test_split=0.90)\n    model = trainer.train(epochs=3,\n                          train_dataloader=train_dataloader,\n                          eval_dataloader=eval_dataloader,\n                          log_steps=10,\n                          ckpt_steps=100,\n                          ckpt_dir=\'./ckpts\',\n                          gradient_accumulation_steps=1)\n    torch.save(model, \'./ckpts/model.bin\')\n'"
reformer_pytorch/__init__.py,4,"b'from reformer_pytorch.reformer_pytorch import LSHAttention, LSHSelfAttention, Reformer, ReformerLM\nfrom reformer_pytorch.reformer_enc_dec import ReformerEncDec\nfrom reformer_pytorch.recorder import Recorder\nfrom reformer_pytorch.autopadder import Autopadder\n'"
reformer_pytorch/autopadder.py,3,"b""import math\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom reformer_pytorch.reformer_pytorch import Reformer, ReformerLM, LSHSelfAttention\n\ndef pad_to_multiple(tensor, seqlen, multiple, dim=-1):\n    m = seqlen / multiple\n    if m.is_integer():\n        return tensor\n    remainder = math.ceil(m) * multiple - seqlen\n    pad_offset = (0,) * (-1 - dim) * 2\n    return F.pad(tensor, (*pad_offset, 0, remainder), value=0)\n\nclass Autopadder(nn.Module):\n    def __init__(self, net):\n        super().__init__()\n        assert isinstance(net, (LSHSelfAttention, Reformer, ReformerLM)), 'only modules LSHSelfAttention, Reformer, ReformerLM accepted'\n        self.net = net\n\n        reformer = net.reformer if isinstance(net, ReformerLM) else net\n        self.pad_dim = -1 if isinstance(net, ReformerLM) else -2\n\n        self.bucket_size = reformer.bucket_size\n        self.num_mem_kv = reformer.num_mem_kv\n        self.full_attn_thres = reformer.full_attn_thres\n\n    def forward(self, x, **kwargs):\n        b, t, m, device = *x.shape[:2], self.num_mem_kv, x.device\n\n        keys = kwargs.get('keys')\n        input_mask = kwargs.get('input_mask')\n        input_attn_mask = kwargs.get('input_attn_mask')\n\n        k_len = 0 if keys is None else keys.shape[1]\n        seqlen = t + m + k_len\n\n        if seqlen > self.full_attn_thres:\n            if input_mask is None:\n                input_mask = torch.full_like(x, True, device=x.device, dtype=torch.bool)\n\n            x = pad_to_multiple(x, seqlen, self.bucket_size * 2, dim=self.pad_dim)\n\n            if input_mask is not None:\n                new_mask = F.pad(input_mask, (0, x.shape[1] - input_mask.shape[1]), value=False)\n                kwargs.update(input_mask=new_mask)\n\n            if input_attn_mask is not None:\n                offset = x.shape[1] - input_attn_mask.shape[1]\n                new_mask = F.pad(input_attn_mask, (0, offset, 0, offset), value=False)\n                kwargs.update(input_attn_mask=new_mask)\n\n        out = self.net(x, **kwargs)\n        return out[:, 0:t]\n"""
reformer_pytorch/generative_tools.py,14,"b""from functools import partial\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pad_sequence\nfrom reformer_pytorch.reformer_pytorch import ReformerLM\nfrom reformer_pytorch.autopadder import Autopadder\n\ndef top_p(logits, thres = 0.9):\n    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n    cum_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n\n    sorted_indices_to_remove = cum_probs > (1 - thres)\n    sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n    sorted_indices_to_remove[:, 0] = 0\n\n    sorted_logits[sorted_indices_to_remove] = float('-inf')\n    return sorted_logits.scatter(1, sorted_indices, sorted_logits)\n\ndef top_k(logits, thres = 0.9):\n    k = int((1 - thres) * logits.shape[-1])\n    val, ind = torch.topk(logits, k)\n    probs = torch.full_like(logits, float('-inf'))\n    probs.scatter_(1, ind, val)\n    return probs\n\nclass TrainingWrapper(nn.Module):\n    def __init__(self, net, ignore_index = -100, pad_value = 0):\n        super().__init__()\n        assert isinstance(net, ReformerLM), 'generative trainer wrapper can only accept ReformerLM class'\n        self.pad_value = pad_value\n        self.ignore_index = ignore_index\n\n        self.net = Autopadder(net)\n        self.max_seq_len = net.max_seq_len\n\n    @torch.no_grad()\n    def generate(self, start_tokens, seq_len, eos_token = None, temperature = 1., filter_logits_fn = top_k, filter_thres = 0.9, **kwargs):\n        was_training = self.net.training\n        num_dims = len(start_tokens.shape)\n\n        if num_dims == 1:\n            start_tokens = start_tokens[None, :]\n\n        b, t = start_tokens.shape\n\n        self.net.eval()\n        out = start_tokens\n        input_mask = kwargs.pop('input_mask', None)\n\n        if input_mask is None:\n            input_mask = torch.full_like(out, True, dtype=torch.bool, device=out.device)\n\n        for _ in range(seq_len):\n            x = out[:, -self.max_seq_len:]\n            input_mask = input_mask[:, -self.max_seq_len:]\n\n            logits = self.net(x, input_mask=input_mask, **kwargs)[:, -1, :]\n            filtered_logits = filter_logits_fn(logits, thres = filter_thres)\n            probs = F.softmax(filtered_logits / temperature, dim=-1)\n            sample = torch.multinomial(probs, 1)\n\n            out = torch.cat((out, sample), dim=-1)\n            input_mask = F.pad(input_mask, (0, 1), value=True)\n\n            if eos_token is not None and (sample == eos_token).all():\n                break\n\n        out = out[:, t:]\n\n        if num_dims == 1:\n            out = out.squeeze(0)\n\n        self.net.train(was_training)\n        return out\n\n    def forward(self, x, return_loss = False, **kwargs):\n        pad = partial(pad_sequence, batch_first = True, padding_value = self.pad_value)\n\n        if not return_loss:\n            if not isinstance(x, torch.Tensor):\n                x = pad(x)\n            return self.net(x, **kwargs)\n\n        if isinstance(x, torch.Tensor):\n            xi = x[:, :-1]\n            xo = x[:, 1:]\n        else:\n            xi = pad(list(map(lambda t: t[:-1], x)))\n            xo = pad(list(map(lambda t: t[1:], x)))\n\n        out = self.net(xi, **kwargs)\n\n        loss = F.cross_entropy(out.transpose(1, 2), xo, ignore_index = self.ignore_index)\n        return loss\n"""
reformer_pytorch/recorder.py,1,"b""from torch import nn\nfrom reformer_pytorch.reformer_pytorch import LSHAttention, LSHSelfAttention\nfrom collections import defaultdict\n\nclass Recorder(nn.Module):\n    def __init__(self, net):\n        super().__init__()\n        self.iter = 0\n        self.recordings = defaultdict(list)\n        self.net = net\n        self.on = True\n        self.ejected = False\n\n    def eject(self):\n        self.ejected = True\n        self.clear()\n        self.unwire()\n        return self.net\n\n    def wire(self):\n        for module in self.net.modules():\n            if isinstance(module, LSHAttention):\n                module._return_attn = True\n            if isinstance(module, LSHSelfAttention):\n                module.callback = self.record\n\n    def unwire(self):\n        for module in self.net.modules():\n            if isinstance(module, LSHAttention):\n                module._return_attn = False\n            if isinstance(module, LSHSelfAttention):\n                module.callback = None\n\n    def turn_on(self):\n        self.on = True\n\n    def turn_off(self):\n        self.on = False\n\n    def clear(self):\n        del self.recordings\n        self.recordings = defaultdict(list)\n        self.iter = 0        \n\n    def record(self, attn, buckets):\n        if not self.on: return\n        data = {'attn': attn.detach().cpu(), 'buckets': buckets.detach().cpu()}\n        self.recordings[self.iter].append(data)\n\n    def forward(self, x, **kwargs):\n        assert not self.ejected, 'Recorder has already been ejected and disposed'\n        if self.on:\n            self.wire()\n\n        out = self.net(x, **kwargs)\n\n        self.iter += 1\n        self.unwire()\n        return out\n"""
reformer_pytorch/reformer_enc_dec.py,2,"b""import re\nfrom torch import nn\nfrom reformer_pytorch.reformer_pytorch import ReformerLM\nfrom reformer_pytorch.generative_tools import TrainingWrapper\n\nENC_PREFIX = 'enc_'\nDEC_PREFIX = 'dec_'\n\ndef group_dict_by_key(cond, d):\n    return_val = [dict(),dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\n\ndef string_begins_with(prefix, str):\n    return bool(re.match(f'^{prefix}', str))\n\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(lambda x: string_begins_with(prefix, x), d)\n\ndef group_by_key_prefix_and_remove_prefix(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(lambda x: string_begins_with(prefix, x), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix):], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n\ndef extract_enc_dec_kwargs(kwargs):\n    enc_kwargs, kwargs = group_by_key_prefix_and_remove_prefix(ENC_PREFIX, kwargs)\n    dec_kwargs, kwargs = group_by_key_prefix_and_remove_prefix(DEC_PREFIX, kwargs)\n    return enc_kwargs, dec_kwargs, kwargs\n\ndef extract_and_set_enc_dec_kwargs(kwargs):\n    enc_kwargs, dec_kwargs, kwargs = extract_enc_dec_kwargs(kwargs)\n    if 'input_mask' in enc_kwargs:\n        dec_kwargs.setdefault('context_mask', enc_kwargs['input_mask'])\n    return enc_kwargs, dec_kwargs, kwargs\n\nclass ReformerEncDec(nn.Module):\n    def __init__(self, dim, ignore_index = -100, pad_value = 0, **kwargs):\n        super().__init__()\n        enc_kwargs, dec_kwargs, _ = extract_enc_dec_kwargs(kwargs)\n        \n        assert 'return_embedding' not in enc_kwargs, 'you cannot manually set the return embeddings flag for the encoder'\n        assert 'dim' not in dec_kwargs and 'dim' not in enc_kwargs, 'you must set the dim for both encoder and decoder'\n\n        enc_kwargs['dim'] = dec_kwargs['dim'] = dim\n        enc_kwargs['return_embeddings'] = True\n        dec_kwargs['causal'] = True\n\n        enc_kwargs.setdefault('bucket_size', 64)\n        dec_kwargs.setdefault('bucket_size', enc_kwargs['bucket_size'] * 2)\n\n        enc = ReformerLM(**enc_kwargs)\n        dec = ReformerLM(**dec_kwargs)\n\n        self.enc = TrainingWrapper(enc, ignore_index = ignore_index, pad_value = pad_value)\n        self.dec = TrainingWrapper(dec, ignore_index = ignore_index, pad_value = pad_value)\n\n    def generate(self, seq_in, seq_out_start, seq_len, **kwargs):\n        enc_kwargs, dec_kwargs, kwargs = extract_and_set_enc_dec_kwargs(kwargs)\n        enc_keys = self.enc(seq_in, **enc_kwargs)\n        return self.dec.generate(seq_out_start, seq_len, keys = enc_keys, **{**dec_kwargs, **kwargs})\n\n    def forward(self, seq_in, seq_out, return_loss = False, **kwargs):\n        enc_kwargs, dec_kwargs, kwargs = extract_and_set_enc_dec_kwargs(kwargs)\n        enc_keys = self.enc(seq_in, **enc_kwargs)\n        return self.dec(seq_out, return_loss = return_loss, keys = enc_keys, **dec_kwargs)\n"""
reformer_pytorch/reformer_pytorch.py,74,"b'import math\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Identity\nimport torch.nn.functional as F\nfrom torch.autograd import Function\nfrom functools import partial, reduce, wraps\nfrom itertools import chain\nfrom operator import mul\n\nfrom axial_positional_embedding import AxialPositionalEmbedding\nfrom product_key_memory import PKM\nfrom reformer_pytorch.reversible import ReversibleSequence\n\n#constants\n\nTOKEN_SELF_ATTN_VALUE = -5e4 # carefully set for half precision to work\n\n# helper fns\n\ndef sort_key_val(t1, t2, dim=-1):\n    values, indices = t1.sort(dim=dim)\n    t2 = t2.expand_as(t1)\n    return values, t2.gather(dim, indices)\n\ndef batched_index_select(values, indices):\n    last_dim = values.shape[-1]\n    return values.gather(1, indices[:, :, None].expand(-1, -1, last_dim))\n\ndef process_inputs_chunk(fn, chunks=1, dim=0):\n    def inner_fn(*args, **kwargs):\n        keys, values, len_args = kwargs.keys(), kwargs.values(), len(args)\n        chunked_args = list(zip(*map(lambda x: x.chunk(chunks, dim=dim), list(args) + list(values))))\n        all_args = map(lambda x: (x[:len_args], dict(zip(keys, x[len_args:]))), chunked_args)\n        outputs = [fn(*c_args, **c_kwargs) for c_args, c_kwargs in all_args]\n        return tuple(map(lambda x: torch.cat(x, dim=dim), zip(*outputs)))\n    return inner_fn\n\ndef chunked_sum(tensor, chunks=1):\n    *orig_size, last_dim = tensor.shape\n    tensor = tensor.reshape(-1, last_dim)\n    summed_tensors = [c.sum(dim=-1) for c in tensor.chunk(chunks, dim=0)]\n    return torch.cat(summed_tensors, dim=0).reshape(orig_size)\n\ndef default(val, default_val):\n    return default_val if val is None else val\n\ndef cast_tuple(x):\n    return x if isinstance(x, tuple) else (x,)\n\ndef max_neg_value(tensor):\n    return -torch.finfo(tensor.dtype).max\n\ndef cache_fn(f):\n    cache = None\n    @wraps(f)\n    def cached_fn(*args, **kwargs):\n        nonlocal cache\n        if cache is not None:\n            return cache\n        cache = f(*args, **kwargs)\n        return cache\n    return cached_fn\n\ndef cache_method_decorator(cache_attr, cache_namespace, reexecute = False):\n    def inner_fn(fn):\n        @wraps(fn)\n        def wrapper(self, *args, key_namespace=None, fetch=False, set_cache=True, **kwargs):\n            namespace_str = str(default(key_namespace, \'\'))\n            _cache = getattr(self, cache_attr)\n            _keyname = f\'{cache_namespace}:{namespace_str}\'\n\n            if fetch:\n                val = _cache[_keyname]\n                if reexecute:\n                    fn(self, *args, **kwargs)\n            else:\n                val = fn(self, *args, **kwargs)\n                if set_cache:\n                    setattr(self, cache_attr, {**_cache, **{_keyname: val}})\n            return val\n        return wrapper\n    return inner_fn\n\ndef look_around(x, backward = 1, forward = 0, pad_value = -1, dim = 2):\n    t = x.shape[1]\n    dims = (len(x.shape) - dim) * (0, 0)\n    padded_x = F.pad(x, (*dims, backward, forward), value= pad_value)\n    tensors = [padded_x[:, ind:(ind + t), ...] for ind in range(forward + backward + 1)]\n    return torch.cat(tensors, dim=dim)\n\ndef expand_dim(dim, k, t):\n    t = t.unsqueeze(dim)\n    expand_shape = [-1] * len(t.shape)\n    expand_shape[dim] = k\n    return t.expand(*expand_shape)\n\ndef merge_dims(ind_from, ind_to, tensor):\n    shape = list(tensor.shape)\n    arr_slice = slice(ind_from, ind_to + 1)\n    shape[arr_slice] = [reduce(mul, shape[arr_slice])]\n    return tensor.reshape(*shape)\n\ndef split_at_index(dim, index, t):\n    pre_slices = (slice(None),) * dim\n    l = (*pre_slices, slice(None, index))\n    r = (*pre_slices, slice(index, None))\n    return t[l], t[r]\n\n# helper classes\n\nclass MatrixMultiply(nn.Module):\n    def __init__(self, tensor, transpose = False, normalize = False):\n        super().__init__()\n        self.tensor = tensor\n        self.transpose = transpose\n        self.normalize = normalize\n\n    def forward(self, x):\n        tensor = self.tensor\n        if self.normalize:\n            tensor = F.normalize(tensor, dim=-1)\n        if self.transpose:\n            tensor = tensor.t()\n        return x @ tensor\n\nclass ReZero(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.g = nn.Parameter(torch.zeros(1))\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) * self.g\n\nclass ScaleNorm(nn.Module):\n    def __init__(self, dim, eps=1e-5):\n        super().__init__()\n        self.g = nn.Parameter(torch.ones(1))\n        self.eps = eps\n\n    def forward(self, x):\n        n = torch.norm(x, dim=-1, keepdim=True).clamp(min=self.eps)\n        return x / n * self.g\n\nclass PreNorm(nn.Module):\n    def __init__(self, norm_class, dim, fn):\n        super().__init__()\n        self.norm = norm_class(dim)\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        x = self.norm(x)\n        return self.fn(x, **kwargs)\n\nclass Chunk(nn.Module):\n    def __init__(self, chunks, fn, along_dim = -1):\n        super().__init__()\n        self.dim = along_dim\n        self.chunks = chunks\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        if self.chunks == 1:\n            return self.fn(x, **kwargs)\n        chunks = x.chunk(self.chunks, dim = self.dim)\n        return torch.cat([self.fn(c, **kwargs) for c in chunks], dim = self.dim)\n\n# LSH attention as described in https://openreview.net/pdf?id=rkgNKkHtvB\n# adapted from trax, stripped to what paper said needed to work\n# namely that buckets need to be at least 64 with 8 rounds of hashing\n# https://github.com/google/trax/blob/master/trax/layers/research/efficient_attention.py#L442\n\nclass LSHAttention(nn.Module):\n    def __init__( self,\n                  dropout = 0.,\n                  bucket_size = 64,\n                  n_hashes = 8,\n                  causal = False,\n                  allow_duplicate_attention = True,\n                  attend_across_buckets = True,\n                  rehash_each_round = True,\n                  drop_for_hash_rate = 0.0,\n                  random_rotations_per_head = False,\n                  return_attn = False):\n        super().__init__()\n        if dropout >= 1.0:\n            raise ValueError(\'Dropout rates must be lower than 1.\')\n\n        self.dropout = nn.Dropout(dropout)\n        self.dropout_for_hash = nn.Dropout(drop_for_hash_rate)\n\n        assert rehash_each_round or allow_duplicate_attention, (\n            \'The setting {allow_duplicate_attention=False, rehash_each_round=False}\'\n            \' is not implemented.\')\n\n        self.causal = causal\n        self.bucket_size = bucket_size\n\n        self.n_hashes = n_hashes\n\n        self._allow_duplicate_attention = allow_duplicate_attention\n        self._attend_across_buckets = attend_across_buckets\n        self._rehash_each_round = rehash_each_round\n        self._random_rotations_per_head = random_rotations_per_head\n\n        # will expend extra computation to return attention matrix\n        self._return_attn = return_attn\n\n        # cache buckets for reversible network, reported by authors to make Reformer work at depth\n        self._cache = {}\n\n    @cache_method_decorator(\'_cache\', \'buckets\', reexecute=True)\n    def hash_vectors(self, n_buckets, vecs):\n        batch_size = vecs.shape[0]\n        device = vecs.device\n\n        # See https://arxiv.org/pdf/1509.02897.pdf\n        # We sample a different random rotation for each round of hashing to\n        # decrease the probability of hash misses.\n        assert n_buckets % 2 == 0\n\n        rot_size = n_buckets\n\n        rotations_shape = (\n            batch_size if self._random_rotations_per_head else 1,\n            vecs.shape[-1],\n            self.n_hashes if self._rehash_each_round else 1,\n            rot_size // 2)\n\n        random_rotations = torch.randn(rotations_shape, dtype=vecs.dtype, device=device).expand(batch_size, -1, -1, -1)\n\n        dropped_vecs = self.dropout_for_hash(vecs)\n        rotated_vecs = torch.einsum(\'btf,bfhi->bhti\', dropped_vecs, random_rotations)\n\n        if self._rehash_each_round:\n            rotated_vecs = torch.cat([rotated_vecs, -rotated_vecs], dim=-1)\n            buckets = torch.argmax(rotated_vecs, dim=-1)\n            # buckets is now (self.n_hashes, seqlen). Next we add offsets so that\n            # bucket numbers from different hashing rounds don\'t overlap.\n            offsets = torch.arange(self.n_hashes, device=device)\n            offsets = torch.reshape(offsets * n_buckets, (1, -1, 1))\n            buckets = torch.reshape(buckets + offsets, (batch_size, -1,))\n        else:\n            rotated_vecs = torch.cat([rotated_vecs, -rotated_vecs], dim=-1)\n            # In this configuration, we map each item to the top self.n_hashes buckets\n            rotated_vecs = torch.squeeze(rotated_vecs, 0)\n            bucket_range = torch.arange(rotated_vecs.shape[-1], device=device)\n            bucket_range = torch.reshape(bucket_range, (1, -1))\n            bucket_range = bucket_range.expand_as(rotated_vecs.shape)\n\n            _, buckets = sort_key_val(rotated_vecs, bucket_range, dim=-1)\n            buckets = buckets[:, -self.n_hashes:]\n\n            h, *_ = buckets.shape \n            buckets = torch.reshape(buckets.permute((*_, h)), (-1,))\n\n        return buckets\n\n    def forward(self, qk, v, query_len = None, input_mask = None, input_attn_mask = None, **kwargs):\n        batch_size, seqlen, dim, device = *qk.shape, qk.device\n\n        query_len = default(query_len, seqlen)\n        is_reverse = kwargs.pop(\'_reverse\', False)\n        depth = kwargs.pop(\'_depth\', None)\n\n        assert seqlen % (self.bucket_size * 2) == 0, f\'Sequence length ({seqlen}) needs to be divisible by target bucket size  x 2 - {self.bucket_size * 2}\'\n\n        n_buckets = seqlen // self.bucket_size\n        buckets = self.hash_vectors(n_buckets, qk, key_namespace=depth, fetch=is_reverse, set_cache=self.training)\n\n        # We use the same vector as both a query and a key.\n        assert int(buckets.shape[1]) == self.n_hashes * seqlen\n\n        total_hashes = self.n_hashes\n\n        ticker = torch.arange(total_hashes * seqlen, device=device).unsqueeze(0).expand_as(buckets)\n        buckets_and_t = seqlen * buckets + (ticker % seqlen)\n        buckets_and_t = buckets_and_t.detach()\n\n        # Hash-based sort (""s"" at the start of variable names means ""sorted"")\n        sbuckets_and_t, sticker = sort_key_val(buckets_and_t, ticker, dim=-1)\n        _, undo_sort = sort_key_val(sticker, ticker, dim=-1)\n        del ticker\n\n        sbuckets_and_t = sbuckets_and_t.detach()\n        sticker = sticker.detach()\n        undo_sort = undo_sort.detach()\n\n        st = (sticker % seqlen)\n        sqk = batched_index_select(qk, st)\n        sv = batched_index_select(v, st)\n\n        # Split off a ""bin"" axis so that attention only occurs within chunks.\n        chunk_size = total_hashes * n_buckets\n        bq_t = bkv_t = torch.reshape(st, (batch_size, chunk_size, -1))\n        bqk = torch.reshape(sqk, (batch_size, chunk_size, -1, dim))\n        bv = torch.reshape(sv, (batch_size, chunk_size, -1, dim))\n\n        # Hashing operates on unit-length vectors. Unnormalized query vectors are\n        # fine because they effectively provide a learnable temperature for the\n        # attention softmax, but normalizing keys is needed so that similarity for\n        # the purposes of attention correctly corresponds to hash locality.\n        bq = bqk\n        bk = F.normalize(bqk, p=2, dim=-1).type(bq.type())\n\n        # Allow each chunk to attend within itself, and also one chunk back. Chunk\n        # boundaries might occur in the middle of a sequence of items from the\n        # same bucket, so this increases the chances of attending to relevant items.\n        def look_one_back(x):\n            x_extra = torch.cat([x[:, -1:, ...], x[:, :-1, ...]], dim=1)\n            return torch.cat([x, x_extra], dim=2)\n\n        bk = look_one_back(bk)\n        bv = look_one_back(bv)\n        bkv_t = look_one_back(bkv_t)\n\n        # Dot-product attention.\n        dots = torch.einsum(\'bhie,bhje->bhij\', bq, bk) * (dim ** -0.5)\n        masked_value = max_neg_value(dots)\n\n        # Mask for post qk attention logits of the input sequence\n        if input_attn_mask is not None:\n            input_attn_mask = F.pad(input_attn_mask, (0, seqlen - input_attn_mask.shape[-1], 0, seqlen - input_attn_mask.shape[-2]), value=True)\n            dot_attn_indices = ((bq_t * seqlen)[:, :, :, None] + bkv_t[:, :, None, :])\n            input_attn_mask = input_attn_mask.reshape(batch_size, -1)\n            dot_attn_indices = dot_attn_indices.reshape(batch_size, -1)\n            mask = input_attn_mask.gather(1, dot_attn_indices).reshape_as(dots)\n            dots.masked_fill_(~mask, masked_value)\n            del mask\n\n        # Input mask for padding in variable lengthed sequences\n        if input_mask is not None:\n            input_mask = F.pad(input_mask, (0, seqlen - input_mask.shape[1]), value=True)\n            mq = input_mask.gather(1, st).reshape((batch_size, chunk_size, -1))\n            mkv = look_one_back(mq)\n            mask = mq[:, :, :, None] * mkv[:, :, None, :]\n            dots.masked_fill_(~mask, masked_value)\n            del mask\n\n        # Causal masking\n        if self.causal:\n            mask = bq_t[:, :, :, None] < bkv_t[:, :, None, :]\n            if seqlen > query_len:\n                mask = mask & (bkv_t[:, :, None, :] < query_len)\n            dots.masked_fill_(mask, masked_value)\n            del mask\n\n        # Mask out attention to self except when no other targets are available.\n        self_mask = bq_t[:, :, :, None] == bkv_t[:, :, None, :]\n        dots.masked_fill_(self_mask, TOKEN_SELF_ATTN_VALUE)\n        del self_mask\n\n        # Mask out attention to other hash buckets.\n        if not self._attend_across_buckets:\n            bq_buckets = bkv_buckets = torch.reshape(sbuckets_and_t // seqlen, (batch_size, chunk_size, -1))\n            bkv_buckets = look_one_back(bkv_buckets)\n            bucket_mask = bq_buckets[:, :, :, None] != bkv_buckets[:, :, None, :]\n            dots.masked_fill_(bucket_mask, masked_value)\n            del bucket_mask\n\n        # Don\'t double-count query-key pairs across multiple rounds of hashing.\n        # There are two possible strategies here. (1) The default is to count how\n        # many times a query-key pair is repeated, and to lower its log-prob\n        # correspondingly at each repetition. (2) When hard_k is set, the code\n        # instead masks all but the first occurence of each query-key pair.\n        if not self._allow_duplicate_attention:\n            locs1 = undo_sort // bq_t.shape[-1]\n            locs2 = (locs1 + 1) % chunk_size\n            if not self._attend_across_buckets:\n                locs1 = buckets * chunk_size + locs1\n                locs2 = buckets * chunk_size + locs2\n            locs = torch.cat([\n                torch.reshape(locs1, (batch_size, total_hashes, seqlen)),\n                torch.reshape(locs2, (batch_size, total_hashes, seqlen)),\n            ], 1).permute((0, 2, 1))\n\n            slocs = batched_index_select(locs, st)\n            b_locs = torch.reshape(slocs, (batch_size, chunk_size, -1, 2 * total_hashes))\n\n            b_locs1 = b_locs[:, :, :, None, :total_hashes]\n\n            bq_locs = b_locs1.expand(b_locs.shape[:3] + (2, total_hashes))\n            bq_locs = torch.reshape(bq_locs, b_locs.shape)\n            bkv_locs = look_one_back(b_locs)\n\n            dup_counts = (bq_locs[:, :, :, None, :] == bkv_locs[:, :, None, :, :])\n            # for memory considerations, chunk summation of last dimension for counting duplicates\n            dup_counts = chunked_sum(dup_counts, chunks=(total_hashes * batch_size))\n            dup_counts = dup_counts.detach()\n            assert dup_counts.shape == dots.shape\n            dots = dots - torch.log(dup_counts + 1e-9)\n            del dup_counts\n\n        # Softmax.\n        dots_logsumexp = torch.logsumexp(dots, dim=-1, keepdim=True)\n        dots = torch.exp(dots - dots_logsumexp).type(dots.type())\n        dropped_dots = self.dropout(dots)\n\n        bo = torch.einsum(\'buij,buje->buie\', dropped_dots, bv)\n        so = torch.reshape(bo, (batch_size, -1, dim))\n        slogits = torch.reshape(dots_logsumexp, (batch_size, -1,))\n\n        class UnsortLogits(Function):\n            @staticmethod\n            def forward(ctx, so, slogits):\n                so = so.detach()\n                slogits = slogits.detach()\n                o = batched_index_select(so, undo_sort)\n                _, logits = sort_key_val(sticker, slogits, dim=-1)\n                return o, logits\n\n            @staticmethod\n            def backward(ctx, grad_x, grad_y):\n                so_grad = batched_index_select(grad_x, sticker)\n                _, slogits_grad = sort_key_val(buckets_and_t, grad_y, dim=-1)\n                return so_grad, slogits_grad\n\n        o, logits = UnsortLogits.apply(so, slogits)\n        o = torch.reshape(o, (batch_size, total_hashes, seqlen, dim))\n        logits = torch.reshape(logits, (batch_size, total_hashes, seqlen, 1))\n\n        if query_len != seqlen:\n            query_slice = (slice(None), slice(None), slice(0, query_len))\n            o, logits = o[query_slice], logits[query_slice]\n\n        probs = torch.exp(logits - torch.logsumexp(logits, dim=1, keepdim=True))\n        out = torch.sum(o * probs, dim=1)\n\n        attn = torch.empty(0, device=device)\n\n        # return unsorted attention weights\n        if self._return_attn:\n            attn_unsort = ((bq_t * seqlen)[:, :, :, None] + bkv_t[:, :, None, :])\n            attn_unsort = attn_unsort.view(batch_size * total_hashes, -1).long()\n            unsorted_dots = torch.zeros(batch_size * total_hashes, seqlen * seqlen, device=device)\n            unsorted_dots.scatter_add_(1, attn_unsort, dots.view_as(attn_unsort))\n            del attn_unsort\n            unsorted_dots = unsorted_dots.reshape(batch_size, total_hashes, seqlen, seqlen)\n            attn = torch.sum(unsorted_dots[:, :, 0:query_len, :] * probs, dim=1)\n\n        # return output, attention matrix, and bucket distribution\n        return out, attn, buckets\n\n# local attention\n\nclass LocalAttention(nn.Module):\n    def __init__(self, bucket_size, causal = False, look_backward = 1, look_forward = 0, dropout = 0., shared_qk = False):\n        super().__init__()\n        assert not (causal and look_forward > 0), \'you cannot look forward if causal\'\n        self.bucket_size = bucket_size\n        self.causal = causal\n        self.look_backward = look_backward\n        self.look_forward = look_forward\n        self.shared_qk = shared_qk\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, q, k, v, input_mask = None):\n        b, t, e, device, dtype = *q.shape, q.device, q.dtype\n        bucket_size, causal, look_backward, look_forward, shared_qk = self.bucket_size, self.causal, self.look_backward, self.look_forward, self.shared_qk\n\n        buckets = t // bucket_size\n\n        if shared_qk:\n            k = F.normalize(k, 2, dim=-1).type(q.type())\n\n        ticker = torch.arange(t, device=device, dtype=dtype)[None, :]\n        b_t = ticker.reshape(1, buckets, bucket_size)\n\n        bucket_fn = lambda t: t.reshape(b, buckets, bucket_size, -1)\n        bq, bk, bv = map(bucket_fn, (q, k, v))\n\n        look_around_kwargs = {\'backward\': look_backward, \'forward\': look_forward}\n        bk = look_around(bk, **look_around_kwargs)\n        bv = look_around(bv, **look_around_kwargs)\n\n        bq_t = b_t\n        bq_k = look_around(b_t, **look_around_kwargs)\n\n        dots = torch.einsum(\'bhie,bhje->bhij\', bq, bk) * (e ** -0.5)\n        mask_value = max_neg_value(dots)\n\n        if shared_qk:\n            mask = bq_t[:, :, :, None] == bq_k[:, :, None, :]\n            dots.masked_fill_(mask, TOKEN_SELF_ATTN_VALUE)\n            del mask\n\n        if causal:\n            mask = bq_t[:, :, :, None] < bq_k[:, :, None, :]\n            dots.masked_fill_(mask, mask_value)\n            del mask\n\n        mask = bq_k[:, :, None, :] == -1\n        dots.masked_fill_(mask, mask_value)\n        del mask\n\n        if input_mask is not None:\n            h = b // input_mask.shape[0]\n            input_mask = input_mask.reshape(-1, buckets, bucket_size)\n            mq = mk = input_mask\n            mk = look_around(mk, pad_value=False, **look_around_kwargs)\n            mask = (mq[:, None, :, :, None] * mk[:, None, :, None, :])\n            mask = merge_dims(0, 1, mask.expand(-1, h, -1, -1, -1))\n            dots.masked_fill_(~mask, mask_value)\n            del mask\n\n        attn = dots.softmax(dim=-1)\n        attn = self.dropout(attn)\n\n        out = torch.einsum(\'bhij,bhje->bhie\', attn, bv)\n        out = out.reshape(b, t, e)\n        return out\n\n# simple full attention\n\nclass FullQKAttention(nn.Module):\n    def __init__(self, causal = False, dropout = 0.):\n        super().__init__()\n        self.causal = causal\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, qk, v, query_len = None, input_mask = None, input_attn_mask = None, **kwargs):\n        b, seq_len, dim = qk.shape\n        query_len = default(query_len, seq_len)\n        t = query_len\n\n        q = qk[:, 0:query_len]\n        qk = F.normalize(qk, 2, dim=-1).type(q.type())\n\n        dot = torch.einsum(\'bie,bje->bij\', q, qk) * (dim ** -0.5)\n\n        # qk attention requires tokens not attend to self\n        i = torch.arange(t)\n        dot[:, i, i] = TOKEN_SELF_ATTN_VALUE\n        masked_value = max_neg_value(dot)\n\n        # Input mask for padding in variable lengthed sequences\n        if input_mask is not None:\n            mask = input_mask[:, 0:query_len, None] * input_mask[:, None, :]\n            mask = F.pad(mask, (0, seq_len - mask.shape[-1]), value=True)\n            dot.masked_fill_(~mask, masked_value)\n\n        # Mask for post qk attention logits of the input sequence\n        if input_attn_mask is not None:\n            input_attn_mask = F.pad(input_attn_mask, (0, seq_len - input_attn_mask.shape[-1]), value=True)\n            dot.masked_fill_(~input_attn_mask, masked_value)\n\n        if self.causal:\n            i, j = torch.triu_indices(t, t, 1)\n            dot[:, i, j] = masked_value\n\n        dot = dot.softmax(dim=-1)\n        dot = self.dropout(dot)\n\n        out = torch.einsum(\'bij,bje->bie\', dot, v)\n\n        return out, dot, torch.empty(0)\n\n# Shared qk attention, using either full or LSH attention\n\nclass LSHSelfAttention(nn.Module):\n    def __init__(self, dim, heads = 8, bucket_size = 64, n_hashes = 8, causal = False, attn_chunks = 1, random_rotations_per_head = False, attend_across_buckets = True, allow_duplicate_attention = True, num_mem_kv = 0, one_value_head = False, use_full_attn = False, full_attn_thres = None, return_attn = False, post_attn_dropout = 0., dropout = 0., n_local_attn_heads = 0, **kwargs):\n        super().__init__()\n        assert dim % heads == 0, \'dimensions must be divisible by number of heads\'\n        assert n_local_attn_heads < heads, \'local attention heads must be less than number of heads\'\n\n        self.dim = dim\n        self.heads = heads\n        self.attn_chunks = default(attn_chunks, 1)\n\n        self.v_head_repeats = (heads if one_value_head else 1)\n        v_dim = dim // self.v_head_repeats\n\n        self.toqk = nn.Linear(dim, dim, bias = False)\n        self.tov = nn.Linear(dim, v_dim, bias = False)\n        self.to_out = nn.Linear(dim, dim)\n\n        self.bucket_size = bucket_size\n        self.lsh_attn = LSHAttention(bucket_size=bucket_size, n_hashes=n_hashes, causal=causal, random_rotations_per_head=random_rotations_per_head, attend_across_buckets = attend_across_buckets,  allow_duplicate_attention = allow_duplicate_attention, return_attn = return_attn, dropout = dropout, **kwargs)\n        self.full_attn = FullQKAttention(causal=causal, dropout=dropout)\n        self.post_attn_dropout = nn.Dropout(post_attn_dropout)\n\n        self.use_full_attn = use_full_attn\n        self.full_attn_thres = default(full_attn_thres, bucket_size)\n\n        self.num_mem_kv = num_mem_kv\n        self.mem_kv = nn.Parameter(torch.randn(1, num_mem_kv, dim, requires_grad=True)) if num_mem_kv > 0 else None\n\n        self.n_local_attn_heads = n_local_attn_heads\n        self.local_attn = LocalAttention(bucket_size=bucket_size * 2, causal=causal, dropout=dropout, shared_qk=True, look_forward=(1 if not causal else 0))\n\n        self.callback = None\n\n    def forward(self, x, keys = None, input_mask = None, input_attn_mask = None, context_mask = None, **kwargs):\n        device, dtype = x.device, x.dtype\n        b, t, e, h, m, l_h = *x.shape, self.heads, self.num_mem_kv, self.n_local_attn_heads\n\n        mem_kv = default(self.mem_kv, torch.empty(b, 0, e, dtype=dtype, device=device))\n        mem = mem_kv.expand(b, m, e)\n\n        keys = default(keys, torch.empty(b, 0, e, dtype=dtype, device=device))\n        c = keys.shape[1]\n\n        kv_len = t + m + c\n        use_full_attn = self.use_full_attn or kv_len <= self.full_attn_thres\n\n        x = torch.cat((x, mem, keys), dim=1)\n        qk = self.toqk(x)\n        v = self.tov(x)\n        v = v.repeat(1, 1, self.v_head_repeats)\n\n        def merge_heads(v):\n            return v.view(b, kv_len, h, -1).transpose(1, 2)\n\n        def split_heads(v):\n            return v.view(b, h, t, -1).transpose(1, 2).contiguous()\n\n        merge_batch_and_heads = partial(merge_dims, 0, 1)\n\n        qk, v = map(merge_heads, (qk, v))\n\n        has_local = l_h > 0\n        lsh_h = h - l_h\n\n        split_index_fn = partial(split_at_index, 1, l_h)\n        (lqk, qk), (lv, v) = map(split_index_fn, (qk, v))\n        lqk, qk, lv, v = map(merge_batch_and_heads, (lqk, qk, lv, v))\n\n        masks = {}\n        if input_mask is not None or context_mask is not None:\n            default_mask = torch.tensor([True], device=device)\n            i_mask = default(input_mask, default_mask.expand(b, t))\n            m_mask = default_mask.expand(b, m)\n            c_mask = default(context_mask, default_mask.expand(b, c))\n            mask = torch.cat((i_mask, m_mask, c_mask), dim=1)\n            mask = merge_batch_and_heads(expand_dim(1, lsh_h, mask))\n            masks[\'input_mask\'] = mask\n\n        if input_attn_mask is not None:\n            input_attn_mask = merge_batch_and_heads(expand_dim(1, lsh_h, input_attn_mask))\n            masks[\'input_attn_mask\'] = input_attn_mask\n\n        attn_fn = self.lsh_attn if not use_full_attn else self.full_attn\n        partial_attn_fn = partial(attn_fn, query_len = t, **kwargs)\n        attn_fn_in_chunks = process_inputs_chunk(partial_attn_fn, chunks = self.attn_chunks)\n\n        out, attn, buckets = attn_fn_in_chunks(qk, v, **masks)\n\n        if self.callback is not None:\n            self.callback(attn.reshape(b, h, t, -1), buckets.reshape(b, h, -1))\n\n        if has_local:\n            lqk, lv = lqk[:, :t], lv[:, :t]\n            local_out = self.local_attn(lqk, lqk, lv, input_mask=input_mask)\n            local_out = local_out.reshape(b, l_h, t, -1)\n            out = out.reshape(b, lsh_h, t, -1)\n            out = torch.cat((local_out, out), dim=1)\n\n        out = split_heads(out).view(b, t, e)\n        out = self.to_out(out)\n        return self.post_attn_dropout(out)\n\n# feed forward\n\nclass GELU_(nn.Module):\n    def forward(self, x):\n        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n\nGELU = nn.GELU if hasattr(nn, \'GELU\') else GELU_\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, mult = 4, dropout = 0., activation = None, glu = False):\n        super().__init__()\n        activation = default(activation, GELU)\n\n        self.glu = glu\n        self.w1 = nn.Linear(dim, dim * mult * (2 if glu else 1))\n        self.act = activation()\n        self.dropout = nn.Dropout(dropout)\n        self.w2 = nn.Linear(dim * mult, dim)\n\n    def forward(self, x, **kwargs):\n        if not self.glu:\n            x = self.w1(x)\n            x = self.act(x)\n        else:\n            x, v = self.w1(x).chunk(2, dim=-1)\n            x = self.act(x) * v\n\n        x = self.dropout(x)\n        x = self.w2(x)\n        return x\n\n# positional embeddings\n\nclass AbsolutePositionalEmbedding(nn.Module):\n    def __init__(self, dim, max_seq_len):\n        super().__init__()\n        self.emb = nn.Embedding(max_seq_len, dim)\n\n    def forward(self, x):\n        t = torch.arange(x.shape[1], device=x.device)\n        return self.emb(t)\n\nclass FixedPositionalEmbedding(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        inv_freq = 1. / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer(\'inv_freq\', inv_freq)\n\n    def forward(self, x):\n        t = torch.arange(x.shape[1], device=x.device).type(self.inv_freq.type())\n        sinusoid_inp = torch.einsum(""i,j->ij"", t, self.inv_freq)\n        emb = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=-1)\n        return emb[None, :, :]\n\n# reformer lm\n\nclass Reformer(nn.Module):\n    def __init__(self, dim, depth, max_seq_len, heads = 8, bucket_size = 64, n_hashes = 8, ff_chunks = 100, attn_chunks = None, causal = False, weight_tie = False, lsh_dropout = 0., ff_dropout = 0., ff_activation = None, ff_mult = 4, ff_glu = False, post_attn_dropout = 0., layer_dropout = 0., lsh_attend_across_buckets = True, lsh_allow_duplicate_attention = True, random_rotations_per_head = False, twin_attention = False, use_scale_norm = False, use_rezero = False, use_full_attn = False, full_attn_thres = 0, reverse_thres = 0, num_mem_kv = 0, one_value_head = False, n_local_attn_heads = 0, pkm_layers = tuple(), pkm_num_keys = 128):\n        super().__init__()\n        self.dim = dim\n        self.depth = depth\n\n        self.bucket_size = bucket_size\n        self.num_mem_kv = num_mem_kv\n\n        self.twin_attention = twin_attention\n        self.full_attn_thres = full_attn_thres\n\n        get_attn = lambda: LSHSelfAttention(dim, heads, bucket_size, n_hashes, causal = causal, dropout = lsh_dropout, post_attn_dropout = post_attn_dropout, attn_chunks = attn_chunks, allow_duplicate_attention = lsh_allow_duplicate_attention, attend_across_buckets = lsh_attend_across_buckets, random_rotations_per_head = random_rotations_per_head, num_mem_kv = num_mem_kv, use_full_attn = use_full_attn, full_attn_thres = full_attn_thres, one_value_head = one_value_head, n_local_attn_heads = n_local_attn_heads)\n        get_ff = lambda: Chunk(ff_chunks, FeedForward(dim, dropout = ff_dropout, activation = ff_activation, mult = ff_mult, glu = ff_glu), along_dim = -2)\n        get_pkm = lambda: PKM(dim, num_keys = pkm_num_keys)\n\n        if weight_tie:\n            get_attn, get_ff, get_pkm = map(cache_fn, (get_attn, get_ff, get_pkm))\n\n        blocks = []\n\n        norm_type = ScaleNorm if use_scale_norm else nn.LayerNorm\n\n        residual_fn_wrapper = ReZero if use_rezero else partial(PreNorm, norm_type, dim)\n\n        for ind in range(depth):\n            layer_num = ind + 1\n            use_pkm = layer_num in cast_tuple(pkm_layers)\n            parallel_net = None\n\n            attn = get_attn()\n\n            if use_pkm:\n                parallel_net = get_pkm()\n            elif twin_attention:\n                parallel_net = get_attn()\n            else:\n                parallel_net = get_ff()\n\n            f = residual_fn_wrapper(attn)\n            g = residual_fn_wrapper(parallel_net)\n\n            blocks.append(nn.ModuleList([f, g]))\n\n        self.layers = ReversibleSequence(nn.ModuleList(blocks), layer_dropout = layer_dropout, reverse_thres = reverse_thres, send_signal = True)\n\n    def forward(self, x, **kwargs):\n        x = torch.cat([x, x], dim = -1)\n        arg_route = (True, self.twin_attention)\n        x = self.layers(x, arg_route = arg_route, **kwargs)\n        return torch.stack(x.chunk(2, dim=-1)).mean(dim=0)\n\nclass ReformerLM(nn.Module):\n    def __init__(self, num_tokens, dim, depth, max_seq_len, heads = 8, bucket_size = 64, n_hashes = 4, ff_chunks = 100, attn_chunks = 1, causal = False, weight_tie = False, lsh_dropout = 0., ff_dropout = 0., ff_mult = 4, ff_activation = None, ff_glu = False, post_attn_dropout = 0., layer_dropout = 0., random_rotations_per_head = False, twin_attention = False, use_scale_norm = False, use_rezero = False, use_full_attn = False, full_attn_thres = 0, reverse_thres = 0, num_mem_kv = 0, one_value_head = False, emb_dim = None, return_embeddings = False, weight_tie_embedding = False, fixed_position_emb = False, absolute_position_emb = False, axial_position_shape = None, n_local_attn_heads = 0, pkm_layers = tuple(), pkm_num_keys = 128):\n        super().__init__()\n        emb_dim = default(emb_dim, dim)\n        self.max_seq_len = max_seq_len\n\n        self.token_emb = nn.Embedding(num_tokens, emb_dim)\n\n        self.to_model_dim = Identity() if emb_dim == dim else nn.Linear(emb_dim, dim)\n\n        if absolute_position_emb:\n            self.pos_emb = AbsolutePositionalEmbedding(emb_dim, max_seq_len)\n        elif fixed_position_emb:\n            self.pos_emb = FixedPositionalEmbedding(emb_dim)\n        else:\n            axial_position_shape = default(axial_position_shape, (max_seq_len // bucket_size, bucket_size))\n            self.pos_emb = AxialPositionalEmbedding(emb_dim, axial_position_shape)\n\n        self.reformer = Reformer(dim, depth, max_seq_len, heads = heads, bucket_size = bucket_size, n_hashes = n_hashes, ff_chunks = ff_chunks, attn_chunks = attn_chunks, causal = causal, weight_tie = weight_tie, lsh_dropout = lsh_dropout, ff_mult = ff_mult, ff_activation = ff_activation, ff_glu = ff_glu, ff_dropout = ff_dropout, post_attn_dropout = 0., layer_dropout = layer_dropout, random_rotations_per_head = random_rotations_per_head, twin_attention = twin_attention, use_scale_norm = use_scale_norm, use_rezero = use_rezero, use_full_attn = use_full_attn, full_attn_thres = full_attn_thres, reverse_thres = reverse_thres, num_mem_kv = num_mem_kv, one_value_head = one_value_head, n_local_attn_heads = n_local_attn_heads, pkm_layers = pkm_layers, pkm_num_keys = pkm_num_keys)\n\n        if return_embeddings:\n            self.out = Identity()\n            return\n\n        self.out = nn.Sequential(\n            nn.Linear(dim, emb_dim) if emb_dim != dim else Identity(),\n            nn.Linear(emb_dim, num_tokens) if not weight_tie_embedding else MatrixMultiply(self.token_emb.weight, transpose=True, normalize=True)\n        )\n\n    def forward(self, x, **kwargs):\n        x = self.token_emb(x)\n        x = x + self.pos_emb(x).type(x.type())\n\n        x = self.to_model_dim(x)\n        x = self.reformer(x, **kwargs)\n        return self.out(x)\n'"
reformer_pytorch/reversible.py,25,"b""import torch\nimport torch.nn as nn\nfrom torch.autograd.function import Function\nfrom torch.utils.checkpoint import get_device_states, set_device_states\n\n# following example for saving and setting rng here https://pytorch.org/docs/stable/_modules/torch/utils/checkpoint.html\nclass Deterministic(nn.Module):\n    def __init__(self, net):\n        super().__init__()\n        self.net = net\n        self.cpu_state = None\n        self.cuda_in_fwd = None\n        self.gpu_devices = None\n        self.gpu_states = None\n\n    def record_rng(self, *args):\n        self.cpu_state = torch.get_rng_state()\n        if torch.cuda._initialized:\n            self.cuda_in_fwd = True\n            self.gpu_devices, self.gpu_states = get_device_states(*args)\n\n    def forward(self, *args, record_rng = False, set_rng = False, **kwargs):\n        if record_rng:\n            self.record_rng(*args)\n\n        if not set_rng:\n            return self.net(*args, **kwargs)\n\n        rng_devices = []\n        if self.cuda_in_fwd:\n            rng_devices = self.gpu_devices\n\n        with torch.random.fork_rng(devices=rng_devices, enabled=True):\n            torch.set_rng_state(self.cpu_state)\n            if self.cuda_in_fwd:\n                set_device_states(self.gpu_devices, self.gpu_states)\n            return self.net(*args, **kwargs)\n\n# heavily inspired by https://github.com/RobinBruegger/RevTorch/blob/master/revtorch/revtorch.py\n# once multi-GPU is confirmed working, refactor and send PR back to source\nclass ReversibleBlock(nn.Module):\n    def __init__(self, f, g, depth=None, send_signal = False):\n        super().__init__()\n        self.f = Deterministic(f)\n        self.g = Deterministic(g)\n\n        self.depth = depth\n        self.send_signal = send_signal\n\n    def forward(self, x, f_args = {}, g_args = {}):\n        x1, x2 = torch.chunk(x, 2, dim=2)\n        y1, y2 = None, None\n\n        if self.send_signal:\n            f_args['_reverse'] = g_args['_reverse'] = False\n            f_args['_depth'] = g_args['_depth'] = self.depth\n\n        with torch.no_grad():\n            y1 = x1 + self.f(x2, record_rng=self.training, **f_args)\n            y2 = x2 + self.g(y1, record_rng=self.training, **g_args)\n\n        return torch.cat([y1, y2], dim=2)\n\n    def backward_pass(self, y, dy, f_args = {}, g_args = {}):\n        y1, y2 = torch.chunk(y, 2, dim=2)\n        del y\n\n        dy1, dy2 = torch.chunk(dy, 2, dim=2)\n        del dy\n\n        if self.send_signal:\n            f_args['_reverse'] = g_args['_reverse'] = True\n            f_args['_depth'] = g_args['_depth'] = self.depth\n\n        with torch.enable_grad():\n            y1.requires_grad = True\n            gy1 = self.g(y1, set_rng=True, **g_args)\n            torch.autograd.backward(gy1, dy2)\n\n        with torch.no_grad():\n            x2 = y2 - gy1\n            del y2, gy1\n\n            dx1 = dy1 + y1.grad\n            del dy1\n            y1.grad = None\n\n        with torch.enable_grad():\n            x2.requires_grad = True\n            fx2 = self.f(x2, set_rng=True, **f_args)\n            torch.autograd.backward(fx2, dx1, retain_graph=True)\n\n        with torch.no_grad():\n            x1 = y1 - fx2\n            del y1, fx2\n\n            dx2 = dy2 + x2.grad\n            del dy2\n            x2.grad = None\n\n            x = torch.cat([x1, x2.detach()], dim=2)\n            dx = torch.cat([dx1, dx2], dim=2)\n\n        return x, dx\n\nclass IrreversibleBlock(nn.Module):\n    def __init__(self, f, g):\n        super().__init__()\n        self.f = f\n        self.g = g\n\n    def forward(self, x, f_args, g_args):\n        x1, x2 = torch.chunk(x, 2, dim=2)\n        y1 = x1 + self.f(x2, **f_args)\n        y2 = x2 + self.g(y1, **g_args)\n        return torch.cat([y1, y2], dim=2)\n\nclass _ReversibleFunction(Function):\n    @staticmethod\n    def forward(ctx, x, blocks, kwargs):\n        ctx.kwargs = kwargs\n        for block in blocks:\n            x = block(x, **kwargs)\n        ctx.y = x.detach()\n        ctx.blocks = blocks\n        return x\n\n    @staticmethod\n    def backward(ctx, dy):\n        y = ctx.y\n        kwargs = ctx.kwargs\n        for block in ctx.blocks[::-1]:\n            y, dy = block.backward_pass(y, dy, **kwargs)\n        return dy, None, None\n\nclass ReversibleSequence(nn.Module):\n    def __init__(self, blocks, layer_dropout = 0., reverse_thres = 0, send_signal = False):\n        super().__init__()\n        self.layer_dropout = layer_dropout\n        self.reverse_thres = reverse_thres\n\n        self.blocks = nn.ModuleList([ReversibleBlock(f, g, depth, send_signal) for depth, (f, g) in enumerate(blocks)])\n        self.irrev_blocks = nn.ModuleList([IrreversibleBlock(f=f, g=g) for f, g in blocks])\n\n    def forward(self, x, arg_route = (True, True), **kwargs):\n        reverse = x.shape[1] > self.reverse_thres\n        blocks = self.blocks if reverse else self.irrev_blocks\n\n        if self.training and self.layer_dropout > 0:\n            to_drop = torch.empty(len(self.blocks)).uniform_(0, 1) < self.layer_dropout\n            blocks = [block for block, drop in zip(self.blocks, to_drop) if not drop]\n            blocks = self.blocks[:1] if len(blocks) == 0 else blocks\n\n        f_args, g_args = map(lambda route: kwargs if route else {}, arg_route)\n        block_kwargs = {'f_args': f_args, 'g_args': g_args}\n\n        if not reverse:\n            for block in blocks:\n                x = block(x, **block_kwargs)\n            return x\n\n        return _ReversibleFunction.apply(x, blocks, block_kwargs)\n"""
examples/enwik8_deepspeed/train.py,7,"b""import deepspeed\n\nfrom reformer_pytorch import ReformerLM\nfrom reformer_pytorch.generative_tools import TrainingWrapper\n\nimport argparse\nimport random\nimport tqdm\nimport gzip\nimport numpy as np\nimport torch\nimport torch.optim as optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, Dataset\n\ndef add_argument():\n    parser=argparse.ArgumentParser(description='enwik8')\n\n    parser.add_argument('--with_cuda', default=False, action='store_true',\n                        help='use CPU in case there\\'s no GPU support')\n    parser.add_argument('--use_ema', default=False, action='store_true',\n                        help='whether use exponential moving average')\n    parser.add_argument('-b', '--batch_size', default=32, type=int,\n                        help='mini-batch size (default: 32)')\n    parser.add_argument('-e', '--epochs', default=30, type=int,\n                        help='number of total epochs (default: 30)')\n    parser.add_argument('--local_rank', type=int, default=-1,\n                       help='local rank passed from distributed launcher')\n\n    parser = deepspeed.add_config_arguments(parser)\n    args=parser.parse_args()\n    return args\n\n# constants\n\nEPOCHS = 20\nGRADIENT_ACCUMULATE_EVERY = 4\nVALIDATE_EVERY  = 100\nGENERATE_EVERY  = 500\nGENERATE_LENGTH = 1024\nSEQ_LEN = 4096\n\n# helpers\n\ndef decode_token(token):\n    return str(chr(max(32, token)))\n\ndef decode_tokens(tokens):\n    return ''.join(list(map(decode_token, tokens)))\n\n# instantiate model\n\nmodel = ReformerLM(\n    dim = 512,\n    depth = 6,\n    max_seq_len = SEQ_LEN,\n    num_tokens = 256,\n    heads = 8,\n    bucket_size = 64,\n    n_hashes = 4,\n    ff_chunks = 10,\n    lsh_dropout = 0.1,\n    weight_tie = True,\n    causal = True,\n    n_local_attn_heads = 4,\n    use_full_attn = False # set this to true for comparison with full attention\n)\n\nmodel = TrainingWrapper(model)\nmodel.cuda()\n\n# prepare enwik8 data\n\nwith gzip.open('./data/enwik8.gz') as file:\n    X = np.fromstring(file.read(int(95e6)), dtype=np.uint8)\n    trX, vaX = np.split(X, [int(90e6)])\n    data_train, data_val = torch.from_numpy(trX), torch.from_numpy(vaX)\n\nclass TextSamplerDataset(Dataset):\n    def __init__(self, data, seq_len):\n        super().__init__()\n        self.data = data\n        self.seq_len = seq_len\n\n    def __getitem__(self, index):\n        rand_start = torch.randint(0, self.data.size(0) - self.seq_len - 1, (1,))\n        full_seq = self.data[rand_start: rand_start + self.seq_len + 1].long()\n        return full_seq\n\n    def __len__(self):\n        return self.data.size(0) // self.seq_len\n\ntrain_dataset = TextSamplerDataset(data_train, SEQ_LEN)\nval_dataset   = TextSamplerDataset(data_val, SEQ_LEN)\n\n# setup deepspeed\n\ncmd_args = add_argument()\nmodel_engine, optimizer, trainloader, _ = deepspeed.initialize(args=cmd_args, model=model, model_parameters=model.parameters(),  training_data=train_dataset)\n\n# training\n\nfor _ in range(EPOCHS):\n    for i, data in enumerate(trainloader):\n        model_engine.train()\n        data = data.to(model_engine.local_rank)\n        loss = model_engine(data, return_loss = True)\n        model_engine.backward(loss)\n        model_engine.step()\n        print(loss.item() * GRADIENT_ACCUMULATE_EVERY)\n\n        if i % VALIDATE_EVERY == 0:\n            model.eval()\n            with torch.no_grad():\n                inp = random.choice(val_dataset)[:-1]\n                loss = model(inp[None, :].cuda(), return_loss = True)\n                print(f'validation loss: {loss.item()}')\n\n        if i % GENERATE_EVERY == 0:\n            model.eval()\n            inp = random.choice(val_dataset)[:-1]\n            prime = decode_tokens(inp)\n            print(f'%s \\n\\n %s', (prime, '*' * 100))\n\n            sample = model.generate(inp.cuda(), GENERATE_LENGTH)\n            output_str = decode_tokens(sample)\n            print(output_str)\n"""
examples/enwik8_simple/train.py,9,"b""from reformer_pytorch import ReformerLM\nfrom reformer_pytorch.generative_tools import TrainingWrapper\n\nimport random\nimport tqdm\nimport gzip\nimport numpy as np\nimport torch\nimport torch.optim as optim\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader, Dataset\n\n# constants\n\nNUM_BATCHES = int(1e5)\nBATCH_SIZE = 4\nGRADIENT_ACCUMULATE_EVERY = 4\nLEARNING_RATE = 1e-4\nVALIDATE_EVERY  = 100\nGENERATE_EVERY  = 500\nGENERATE_LENGTH = 512\nSEQ_LEN = 4096\n\n# helpers\n\ndef cycle(loader):\n    while True:\n        for data in loader:\n            yield data\n\ndef decode_token(token):\n    return str(chr(max(32, token)))\n\ndef decode_tokens(tokens):\n    return ''.join(list(map(decode_token, tokens)))\n\n# instantiate model\n\nmodel = ReformerLM(\n    dim = 512,\n    depth = 6,\n    max_seq_len = SEQ_LEN,\n    num_tokens = 256,\n    heads = 8,\n    bucket_size = 64,\n    n_hashes = 4,\n    ff_chunks = 10,\n    lsh_dropout = 0.1,\n    weight_tie = True,\n    causal = True,\n    n_local_attn_heads = 4,\n    use_full_attn = False # set this to true for comparison with full attention\n)\n\nmodel = TrainingWrapper(model)\nmodel.cuda()\n\n# prepare enwik8 data\n\nwith gzip.open('./data/enwik8.gz') as file:\n    X = np.fromstring(file.read(int(95e6)), dtype=np.uint8)\n    trX, vaX = np.split(X, [int(90e6)])\n    data_train, data_val = torch.from_numpy(trX), torch.from_numpy(vaX)\n\nclass TextSamplerDataset(Dataset):\n    def __init__(self, data, seq_len):\n        super().__init__()\n        self.data = data\n        self.seq_len = seq_len\n\n    def __getitem__(self, index):\n        rand_start = torch.randint(0, self.data.size(0) - self.seq_len - 1, (1,))\n        full_seq = self.data[rand_start: rand_start + self.seq_len + 1].long()\n        return full_seq.cuda()\n\n    def __len__(self):\n        return self.data.size(0) // self.seq_len\n\ntrain_dataset = TextSamplerDataset(data_train, SEQ_LEN)\nval_dataset   = TextSamplerDataset(data_val, SEQ_LEN)\ntrain_loader  = cycle(DataLoader(train_dataset, batch_size = BATCH_SIZE))\nval_loader    = cycle(DataLoader(val_dataset, batch_size = BATCH_SIZE))\n\n# optimizer\n\noptim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n# training\n\nfor i in tqdm.tqdm(range(NUM_BATCHES), mininterval=10., desc='training'):\n    model.train()\n\n    for __ in range(GRADIENT_ACCUMULATE_EVERY):\n        loss = model(next(train_loader), return_loss = True)\n        loss.backward()\n\n    print(f'training loss: {loss.item()}')\n    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n    optim.step()\n    optim.zero_grad()\n\n    if i % VALIDATE_EVERY == 0:\n        model.eval()\n        with torch.no_grad():\n            loss = model(next(val_loader), return_loss = True)\n            print(f'validation loss: {loss.item()}')\n\n    if i % GENERATE_EVERY == 0:\n        model.eval()\n        inp = random.choice(val_dataset)[:-1]\n        prime = decode_tokens(inp)\n        print(f'%s \\n\\n %s', (prime, '*' * 100))\n\n        sample = model.generate(inp, GENERATE_LENGTH)\n        output_str = decode_tokens(sample)\n        print(output_str)\n"""
