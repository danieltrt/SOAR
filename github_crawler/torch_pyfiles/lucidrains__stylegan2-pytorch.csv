file_path,api_count,code
setup.py,0,"b""from setuptools import setup, find_packages\n\nsetup(\n  name = 'stylegan2_pytorch',\n  packages = find_packages(),\n  scripts=['bin/stylegan2_pytorch'],\n  version = '0.10.3',\n  license='GPLv3+',\n  description = 'StyleGan2 in Pytorch',\n  author = 'Phil Wang',\n  author_email = 'lucidrains@gmail.com',\n  url = 'https://github.com/lucidrains/stylegan2-pytorch',\n  download_url = 'https://github.com/lucidrains/stylegan2-pytorch/archive/v_036.tar.gz',\n  keywords = ['generative adversarial networks', 'artificial intelligence'],\n  install_requires=[\n      'fire',\n      'numpy',\n      'retry',\n      'tqdm',\n      'torch',\n      'torchvision',\n      'pillow',\n      'torch_optimizer',\n      'contrastive_learner>=0.1.0',\n      'linear_attention_transformer',\n      'vector-quantize-pytorch'\n  ],\n  classifiers=[\n      'Development Status :: 4 - Beta',\n      'Intended Audience :: Developers',\n      'Topic :: Scientific/Engineering :: Artificial Intelligence',\n      'License :: OSI Approved :: MIT License',\n      'Programming Language :: Python :: 3.6',\n  ],\n)"""
stylegan2_pytorch/__init__.py,1,"b'from stylegan2_pytorch.stylegan2_pytorch import Trainer, StyleGAN2, NanException'"
stylegan2_pytorch/stylegan2_pytorch.py,29,"b""import os\nimport sys\nimport math\nimport fire\nimport json\nfrom math import floor, log2\nfrom random import random\nfrom shutil import rmtree\nfrom functools import partial\nimport multiprocessing\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils import data\nimport torch.nn.functional as F\n\nfrom torch_optimizer import DiffGrad\nfrom torch.autograd import grad as torch_grad\n\nimport torchvision\nfrom torchvision import transforms\n\nfrom vector_quantize_pytorch import VectorQuantize\nfrom linear_attention_transformer import ImageLinearAttention\nfrom contrastive_learner import ContrastiveLearner\n\nfrom PIL import Image\nfrom pathlib import Path\n\ntry:\n    from apex import amp\n    APEX_AVAILABLE = True\nexcept:\n    APEX_AVAILABLE = False\n\nassert torch.cuda.is_available(), 'You need to have an Nvidia GPU with CUDA installed.'\n\nnum_cores = multiprocessing.cpu_count()\n\n# constants\n\nEXTS = ['jpg', 'png']\nEPS = 1e-8\n\n# helper classes\n\nclass NanException(Exception):\n    pass\n\nclass EMA():\n    def __init__(self, beta):\n        super().__init__()\n        self.beta = beta\n    def update_average(self, old, new):\n        if old is None:\n            return new\n        return old * self.beta + (1 - self.beta) * new\n\nclass Flatten(nn.Module):\n    def forward(self, x):\n        return x.reshape(x.shape[0], -1)\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n    def forward(self, x):\n        return self.fn(x) + x\n\nclass Rezero(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n        self.g = nn.Parameter(torch.zeros(1))\n    def forward(self, x):\n        return self.fn(x) * self.g\n\nclass PermuteToFrom(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n    def forward(self, x):\n        x = x.permute(0, 2, 3, 1)\n        out, loss = self.fn(x)\n        out = out.permute(0, 3, 1, 2)\n        return out, loss\n\n# helpers\n\ndef default(value, d):\n    return d if value is None else value\n\ndef cycle(iterable):\n    while True:\n        for i in iterable:\n            yield i\n\ndef cast_list(el):\n    return el if isinstance(el, list) else [el]\n\ndef is_empty(t):\n    if isinstance(t, torch.Tensor):\n        return t.nelement() == 0\n    return t is None\n\ndef raise_if_nan(t):\n    if torch.isnan(t):\n        raise NanException\n\ndef loss_backwards(fp16, loss, optimizer, **kwargs):\n    if fp16:\n        with amp.scale_loss(loss, optimizer) as scaled_loss:\n            scaled_loss.backward(**kwargs)\n    else:\n        loss.backward(**kwargs)\n\ndef gradient_penalty(images, output, weight = 10):\n    batch_size = images.shape[0]\n    gradients = torch_grad(outputs=output, inputs=images,\n                           grad_outputs=torch.ones(output.size()).cuda(),\n                           create_graph=True, retain_graph=True, only_inputs=True)[0]\n\n    gradients = gradients.view(batch_size, -1)\n    return weight * ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n\ndef noise(n, latent_dim):\n    return torch.randn(n, latent_dim).cuda()\n\ndef noise_list(n, layers, latent_dim):\n    return [(noise(n, latent_dim), layers)]\n\ndef mixed_list(n, layers, latent_dim):\n    tt = int(torch.rand(()).numpy() * layers)\n    return noise_list(n, tt, latent_dim) + noise_list(n, layers - tt, latent_dim)\n\ndef latent_to_w(style_vectorizer, latent_descr):\n    return [(style_vectorizer(z), num_layers) for z, num_layers in latent_descr]\n\ndef image_noise(n, im_size):\n    return torch.FloatTensor(n, im_size, im_size, 1).uniform_(0., 1.).cuda()\n\ndef leaky_relu(p=0.2):\n    return nn.LeakyReLU(p, inplace=True)\n\ndef evaluate_in_chunks(max_batch_size, model, *args):\n    split_args = list(zip(*list(map(lambda x: x.split(max_batch_size, dim=0), args))))\n    chunked_outputs = [model(*i) for i in split_args]\n    if len(chunked_outputs) == 1:\n        return chunked_outputs[0]\n    return torch.cat(chunked_outputs, dim=0)\n\ndef styles_def_to_tensor(styles_def):\n    return torch.cat([t[:, None, :].expand(-1, n, -1) for t, n in styles_def], dim=1)\n\ndef set_requires_grad(model, bool):\n    for p in model.parameters():\n        p.requires_grad = bool\n\n# dataset\n\ndef convert_rgb_to_transparent(image):\n    if image.mode == 'RGB':\n        return image.convert('RGBA')\n    return image\n\ndef convert_transparent_to_rgb(image):\n    if image.mode == 'RGBA':\n        return image.convert('RGB')\n    return image\n\nclass expand_greyscale(object):\n    def __init__(self, num_channels):\n        self.num_channels = num_channels\n    def __call__(self, tensor):\n        return tensor.expand(self.num_channels, -1, -1)\n\ndef resize_to_minimum_size(min_size, image):\n    if max(*image.size) < min_size:\n        return torchvision.transforms.functional.resize(image, min_size)\n    return image\n\nclass Dataset(data.Dataset):\n    def __init__(self, folder, image_size, transparent = False):\n        super().__init__()\n        self.folder = folder\n        self.image_size = image_size\n        self.paths = [p for ext in EXTS for p in Path(f'{folder}').glob(f'**/*.{ext}')]\n\n        convert_image_fn = convert_transparent_to_rgb if not transparent else convert_rgb_to_transparent\n        num_channels = 3 if not transparent else 4\n\n        self.transform = transforms.Compose([\n            transforms.Lambda(convert_image_fn),\n            transforms.Lambda(partial(resize_to_minimum_size, image_size)),\n            transforms.RandomHorizontalFlip(),\n            transforms.Resize(image_size),\n            transforms.CenterCrop(image_size),\n            transforms.ToTensor(),\n            transforms.Lambda(expand_greyscale(num_channels))\n        ])\n\n    def __len__(self):\n        return len(self.paths)\n\n    def __getitem__(self, index):\n        path = self.paths[index]\n        img = Image.open(path)\n        return self.transform(img)\n\n# stylegan2 classes\n\nclass StyleVectorizer(nn.Module):\n    def __init__(self, emb, depth):\n        super().__init__()\n\n        layers = []\n        for i in range(depth):\n            layers.extend([nn.Linear(emb, emb), leaky_relu()])\n\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.net(x)\n\nclass RGBBlock(nn.Module):\n    def __init__(self, latent_dim, input_channel, upsample, rgba = False):\n        super().__init__()\n        self.input_channel = input_channel\n        self.to_style = nn.Linear(latent_dim, input_channel)\n\n        out_filters = 3 if not rgba else 4\n        self.conv = Conv2DMod(input_channel, out_filters, 1, demod=False)\n\n        self.upsample = nn.Upsample(scale_factor = 2, mode='bilinear', align_corners=False) if upsample else None\n\n    def forward(self, x, prev_rgb, istyle):\n        b, c, h, w = x.shape\n        style = self.to_style(istyle)\n        x = self.conv(x, style)\n\n        if prev_rgb is not None:\n            x = x + prev_rgb\n\n        if self.upsample is not None:\n            x = self.upsample(x)\n\n        return x\n\nclass Conv2DMod(nn.Module):\n    def __init__(self, in_chan, out_chan, kernel, demod=True, stride=1, dilation=1, **kwargs):\n        super().__init__()\n        self.filters = out_chan\n        self.demod = demod\n        self.kernel = kernel\n        self.stride = stride\n        self.dilation = dilation\n        self.weight = nn.Parameter(torch.randn((out_chan, in_chan, kernel, kernel)))\n        nn.init.kaiming_normal_(self.weight, a=0, mode='fan_in', nonlinearity='leaky_relu')\n\n    def _get_same_padding(self, size, kernel, dilation, stride):\n        return ((size - 1) * (stride - 1) + dilation * (kernel - 1)) // 2\n\n    def forward(self, x, y):\n        b, c, h, w = x.shape\n\n        w1 = y[:, None, :, None, None]\n        w2 = self.weight[None, :, :, :, :]\n        weights = w2 * (w1 + 1)\n\n        if self.demod:\n            d = torch.rsqrt((weights ** 2).sum(dim=(2, 3, 4), keepdim=True) + EPS)\n            weights = weights * d\n\n        x = x.reshape(1, -1, h, w)\n\n        _, _, *ws = weights.shape\n        weights = weights.reshape(b * self.filters, *ws)\n\n        padding = self._get_same_padding(h, self.kernel, self.dilation, self.stride)\n        x = F.conv2d(x, weights, padding=padding, groups=b)\n\n        x = x.reshape(-1, self.filters, h, w)\n        return x\n\nclass GeneratorBlock(nn.Module):\n    def __init__(self, latent_dim, input_channels, filters, upsample = True, upsample_rgb = True, rgba = False):\n        super().__init__()\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False) if upsample else None\n\n        self.to_style1 = nn.Linear(latent_dim, input_channels)\n        self.to_noise1 = nn.Linear(1, filters)\n        self.conv1 = Conv2DMod(input_channels, filters, 3)\n        \n        self.to_style2 = nn.Linear(latent_dim, filters)\n        self.to_noise2 = nn.Linear(1, filters)\n        self.conv2 = Conv2DMod(filters, filters, 3)\n\n        self.activation = leaky_relu()\n        self.to_rgb = RGBBlock(latent_dim, filters, upsample_rgb, rgba)\n\n    def forward(self, x, prev_rgb, istyle, inoise):\n        if self.upsample is not None:\n            x = self.upsample(x)\n\n        inoise = inoise[:, :x.shape[2], :x.shape[3], :]\n        noise1 = self.to_noise1(inoise).permute((0, 3, 2, 1))\n        noise2 = self.to_noise2(inoise).permute((0, 3, 2, 1))\n\n        style1 = self.to_style1(istyle)\n        x = self.conv1(x, style1)\n        x = self.activation(x + noise1)\n\n        style2 = self.to_style2(istyle)\n        x = self.conv2(x, style2)\n        x = self.activation(x + noise2)\n\n        rgb = self.to_rgb(x, prev_rgb, istyle)\n        return x, rgb\n\nclass DiscriminatorBlock(nn.Module):\n    def __init__(self, input_channels, filters, downsample=True):\n        super().__init__()\n        self.conv_res = nn.Conv2d(input_channels, filters, 1)\n\n        self.net = nn.Sequential(\n            nn.Conv2d(input_channels, filters, 3, padding=1),\n            leaky_relu(),\n            nn.Conv2d(filters, filters, 3, padding=1),\n            leaky_relu()\n        )\n\n        self.downsample = nn.Conv2d(filters, filters, 3, padding = 1, stride = 2) if downsample else None\n\n    def forward(self, x):\n        res = self.conv_res(x)\n        x = self.net(x)\n        x = x + res\n        if self.downsample is not None:\n            x = self.downsample(x)\n        return x\n\nclass Generator(nn.Module):\n    def __init__(self, image_size, latent_dim, network_capacity = 16, transparent = False):\n        super().__init__()\n        self.image_size = image_size\n        self.latent_dim = latent_dim\n        self.num_layers = int(log2(image_size) - 1)\n\n        init_channels = 4 * network_capacity\n        self.initial_block = nn.Parameter(torch.randn((init_channels, 4, 4)))\n        filters = [init_channels] + [network_capacity * (2 ** (i + 1)) for i in range(self.num_layers)][::-1]\n        in_out_pairs = zip(filters[0:-1], filters[1:])\n\n        self.blocks = nn.ModuleList([])\n        for ind, (in_chan, out_chan) in enumerate(in_out_pairs):\n            not_first = ind != 0\n            not_last = ind != (self.num_layers - 1)\n\n            block = GeneratorBlock(\n                latent_dim,\n                in_chan,\n                out_chan,\n                upsample = not_first,\n                upsample_rgb = not_last,\n                rgba = transparent\n            )\n            self.blocks.append(block)\n\n    def forward(self, styles, input_noise):\n        batch_size = styles.shape[0]\n        image_size = self.image_size\n        x = self.initial_block.expand(batch_size, -1, -1, -1)\n        styles = styles.transpose(0, 1)\n\n        rgb = None\n        for style, block in zip(styles, self.blocks):\n            x, rgb = block(x, rgb, style, input_noise)\n\n        return rgb\n\nclass Discriminator(nn.Module):\n    def __init__(self, image_size, network_capacity = 16, fq_layers = [], fq_dict_size = 256, attn_layers = [], transparent = False):\n        super().__init__()\n        num_layers = int(log2(image_size) - 1)\n        num_init_filters = 3 if not transparent else 4\n\n        blocks = []\n        filters = [num_init_filters] + [(network_capacity) * (2 ** i) for i in range(num_layers + 1)]\n        chan_in_out = list(zip(filters[0:-1], filters[1:]))\n\n        blocks = []\n        quantize_blocks = []\n        attn_blocks = []\n\n        for ind, (in_chan, out_chan) in enumerate(chan_in_out):\n            num_layer = ind + 1\n            is_not_last = ind != (len(chan_in_out) - 1)\n\n            block = DiscriminatorBlock(in_chan, out_chan, downsample = is_not_last)\n            blocks.append(block)\n\n            attn_fn = nn.Sequential(*[\n                Residual(Rezero(ImageLinearAttention(out_chan))) for _ in range(2)\n            ]) if num_layer in attn_layers else None\n\n            attn_blocks.append(attn_fn)\n\n            quantize_fn = PermuteToFrom(VectorQuantize(out_chan, fq_dict_size)) if num_layer in fq_layers else None\n            quantize_blocks.append(quantize_fn)\n\n        self.blocks = nn.ModuleList(blocks)\n        self.attn_blocks = nn.ModuleList(attn_blocks)\n        self.quantize_blocks = nn.ModuleList(quantize_blocks)\n\n        latent_dim = 2 * 2 * filters[-1]\n\n        self.flatten = Flatten()\n        self.to_logit = nn.Linear(latent_dim, 1)\n\n    def forward(self, x):\n        b, *_ = x.shape\n\n        quantize_loss = torch.zeros(1).to(x)\n\n        for (block, attn_block, q_block) in zip(self.blocks, self.attn_blocks, self.quantize_blocks):\n            x = block(x)\n\n            if attn_block is not None:\n                x = attn_block(x)\n\n            if q_block is not None:\n                x, loss = q_block(x)\n                quantize_loss += loss\n\n        x = self.flatten(x)\n        x = self.to_logit(x)\n        return x.squeeze(), quantize_loss\n\nclass StyleGAN2(nn.Module):\n    def __init__(self, image_size, latent_dim = 512, style_depth = 8, network_capacity = 16, transparent = False, fp16 = False, cl_reg = False, steps = 1, lr = 1e-4, fq_layers = [], fq_dict_size = 256, attn_layers = []):\n        super().__init__()\n        self.lr = lr\n        self.steps = steps\n        self.ema_updater = EMA(0.995)\n\n        self.S = StyleVectorizer(latent_dim, style_depth)\n        self.G = Generator(image_size, latent_dim, network_capacity, transparent = transparent)\n        self.D = Discriminator(image_size, network_capacity, fq_layers = fq_layers, fq_dict_size = fq_dict_size, attn_layers = attn_layers, transparent = transparent)\n\n        self.SE = StyleVectorizer(latent_dim, style_depth)\n        self.GE = Generator(image_size, latent_dim, network_capacity, transparent = transparent)\n\n        # experimental contrastive loss discriminator regularization\n        self.D_cl = ContrastiveLearner(self.D, image_size, hidden_layer='flatten') if cl_reg else None\n\n        set_requires_grad(self.SE, False)\n        set_requires_grad(self.GE, False)\n\n        generator_params = list(self.G.parameters()) + list(self.S.parameters())\n        self.G_opt = DiffGrad(generator_params, lr = self.lr, betas=(0.5, 0.9))\n        self.D_opt = DiffGrad(self.D.parameters(), lr = self.lr, betas=(0.5, 0.9))\n\n        self._init_weights()\n        self.reset_parameter_averaging()\n\n        self.cuda()\n        \n        if fp16:\n            (self.S, self.G, self.D, self.SE, self.GE), (self.G_opt, self.D_opt) = amp.initialize([self.S, self.G, self.D, self.SE, self.GE], [self.G_opt, self.D_opt], opt_level='O2')\n\n    def _init_weights(self):\n        for m in self.modules():\n            if type(m) in {nn.Conv2d, nn.Linear}:\n                nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in', nonlinearity='leaky_relu')\n\n        for block in self.G.blocks:\n            nn.init.zeros_(block.to_noise1.weight)\n            nn.init.zeros_(block.to_noise2.weight)\n            nn.init.zeros_(block.to_noise1.bias)\n            nn.init.zeros_(block.to_noise2.bias)\n\n    def EMA(self):\n        def update_moving_average(ma_model, current_model):\n            for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n                old_weight, up_weight = ma_params.data, current_params.data\n                ma_params.data = self.ema_updater.update_average(old_weight, up_weight)\n\n        update_moving_average(self.SE, self.S)\n        update_moving_average(self.GE, self.G)\n\n    def reset_parameter_averaging(self):\n        self.SE.load_state_dict(self.S.state_dict())\n        self.GE.load_state_dict(self.G.state_dict())\n\n    def forward(self, x):\n        return x\n\nclass Trainer():\n    def __init__(self, name, results_dir, models_dir, image_size, network_capacity, transparent = False, batch_size = 4, mixed_prob = 0.9, gradient_accumulate_every=1, lr = 2e-4, num_workers = None, save_every = 1000, trunc_psi = 0.6, fp16 = False, cl_reg = False, fq_layers = [], fq_dict_size = 256, attn_layers = [], *args, **kwargs):\n        self.GAN_params = [args, kwargs]\n        self.GAN = None\n\n        self.name = name\n        self.results_dir = Path(results_dir)\n        self.models_dir = Path(models_dir)\n        self.config_path = self.models_dir / name / '.config.json'\n\n        assert log2(image_size).is_integer(), 'image size must be a power of 2 (64, 128, 256, 512, 1024)'\n        self.image_size = image_size\n        self.network_capacity = network_capacity\n        self.transparent = transparent\n        self.fq_layers = cast_list(fq_layers)\n        self.fq_dict_size = fq_dict_size\n\n        self.attn_layers = cast_list(attn_layers)\n\n        self.lr = lr\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.mixed_prob = mixed_prob\n\n        self.save_every = save_every\n        self.steps = 0\n\n        self.av = None\n        self.trunc_psi = trunc_psi\n\n        self.pl_mean = 0\n\n        self.gradient_accumulate_every = gradient_accumulate_every\n\n        assert not fp16 or fp16 and APEX_AVAILABLE, 'Apex is not available for you to use mixed precision training'\n        self.fp16 = fp16\n\n        self.cl_reg = cl_reg\n\n        self.d_loss = 0\n        self.g_loss = 0\n        self.last_gp_loss = 0\n        self.last_cr_loss = 0\n        self.q_loss = 0\n\n        self.pl_length_ma = EMA(0.99)\n        self.init_folders()\n\n        self.loader = None\n\n    def init_GAN(self):\n        args, kwargs = self.GAN_params\n        self.GAN = StyleGAN2(lr=self.lr, image_size = self.image_size, network_capacity = self.network_capacity, transparent = self.transparent, fq_layers = self.fq_layers, fq_dict_size = self.fq_dict_size, attn_layers = self.attn_layers, fp16 = self.fp16, cl_reg = self.cl_reg, *args, **kwargs)\n\n    def write_config(self):\n        self.config_path.write_text(json.dumps(self.config()))\n\n    def load_config(self):\n        config = self.config() if not self.config_path.exists() else json.loads(self.config_path.read_text())\n        self.image_size = config['image_size']\n        self.network_capacity = config['network_capacity']\n        self.transparent = config['transparent']\n        self.fq_layers = config['fq_layers']\n        self.fq_dict_size = config['fq_dict_size']\n        self.attn_layers = config.pop('attn_layers', [])\n        del self.GAN\n        self.init_GAN()\n\n    def config(self):\n        return {'image_size': self.image_size, 'network_capacity': self.network_capacity, 'transparent': self.transparent, 'fq_layers': self.fq_layers, 'fq_dict_size': self.fq_dict_size, 'attn_layers': self.attn_layers}\n\n    def set_data_src(self, folder):\n        self.dataset = Dataset(folder, self.image_size, transparent = self.transparent)\n        self.loader = cycle(data.DataLoader(self.dataset, num_workers = default(self.num_workers, num_cores), batch_size = self.batch_size, drop_last = True, shuffle=True, pin_memory=True))\n\n    def train(self):\n        assert self.loader is not None, 'You must first initialize the data source with `.set_data_src(<folder of images>)`'\n\n        if self.GAN is None:\n            self.init_GAN()\n\n        self.GAN.train()\n        total_disc_loss = torch.tensor(0.).cuda()\n        total_gen_loss = torch.tensor(0.).cuda()\n\n        batch_size = self.batch_size\n\n        image_size = self.GAN.G.image_size\n        latent_dim = self.GAN.G.latent_dim\n        num_layers = self.GAN.G.num_layers\n\n        apply_gradient_penalty = self.steps % 4 == 0\n        apply_path_penalty = self.steps % 32 == 0\n        apply_cl_reg_to_generated = self.steps > 20000\n\n        backwards = partial(loss_backwards, self.fp16)\n\n        if self.GAN.D_cl is not None:\n            self.GAN.D_opt.zero_grad()\n\n            if apply_cl_reg_to_generated:\n                for i in range(self.gradient_accumulate_every):\n                    get_latents_fn = mixed_list if random() < self.mixed_prob else noise_list\n                    style = get_latents_fn(batch_size, num_layers, latent_dim)\n                    noise = image_noise(batch_size, image_size)\n\n                    w_space = latent_to_w(self.GAN.S, style)\n                    w_styles = styles_def_to_tensor(w_space)\n\n                    generated_images = self.GAN.G(w_styles, noise)\n                    self.GAN.D_cl(generated_images.clone().detach(), accumulate=True)\n\n            for i in range(self.gradient_accumulate_every):\n                image_batch = next(self.loader).cuda()\n                self.GAN.D_cl(image_batch, accumulate=True)\n\n            loss = self.GAN.D_cl.calculate_loss()\n            self.last_cr_loss = loss.clone().detach().item()\n            backwards(loss, self.GAN.D_opt)\n\n            self.GAN.D_opt.step()\n\n        # train discriminator\n\n        avg_pl_length = self.pl_mean\n        self.GAN.D_opt.zero_grad()\n\n        for i in range(self.gradient_accumulate_every):\n            get_latents_fn = mixed_list if random() < self.mixed_prob else noise_list\n            style = get_latents_fn(batch_size, num_layers, latent_dim)\n            noise = image_noise(batch_size, image_size)\n\n            w_space = latent_to_w(self.GAN.S, style)\n            w_styles = styles_def_to_tensor(w_space)\n\n            generated_images = self.GAN.G(w_styles, noise)\n            fake_output, fake_q_loss = self.GAN.D(generated_images.clone().detach())\n\n            image_batch = next(self.loader).cuda()\n            image_batch.requires_grad_()\n            real_output, real_q_loss = self.GAN.D(image_batch)\n\n            divergence = (F.relu(1 + real_output) + F.relu(1 - fake_output)).mean()\n            disc_loss = divergence\n\n            quantize_loss = (fake_q_loss + real_q_loss).mean()\n            self.q_loss = float(quantize_loss.detach().item())\n\n            disc_loss = disc_loss + quantize_loss\n\n            if apply_gradient_penalty:\n                gp = gradient_penalty(image_batch, real_output)\n                self.last_gp_loss = gp.clone().detach().item()\n                disc_loss = disc_loss + gp\n\n            disc_loss = disc_loss / self.gradient_accumulate_every\n            disc_loss.register_hook(raise_if_nan)\n            backwards(disc_loss, self.GAN.D_opt)\n\n            total_disc_loss += divergence.detach().item() / self.gradient_accumulate_every\n\n        self.d_loss = float(total_disc_loss)\n        self.GAN.D_opt.step()\n\n        # train generator\n\n        self.GAN.G_opt.zero_grad()\n        for i in range(self.gradient_accumulate_every):\n            style = get_latents_fn(batch_size, num_layers, latent_dim)\n            noise = image_noise(batch_size, image_size)\n\n            w_space = latent_to_w(self.GAN.S, style)\n            w_styles = styles_def_to_tensor(w_space)\n\n            generated_images = self.GAN.G(w_styles, noise)\n            fake_output, _ = self.GAN.D(generated_images)\n            loss = fake_output.mean()\n            gen_loss = loss\n\n            if apply_path_penalty:\n                std = 0.1 / (w_styles.std(dim = 0, keepdim = True) + EPS)\n                w_styles_2 = w_styles + torch.randn(w_styles.shape).cuda() / (std + EPS)\n                pl_images = self.GAN.G(w_styles_2, noise)\n                pl_lengths = ((pl_images - generated_images) ** 2).mean(dim = (1, 2, 3))\n                avg_pl_length = np.mean(pl_lengths.detach().cpu().numpy())\n\n                if not is_empty(self.pl_mean):\n                    pl_loss = ((pl_lengths - self.pl_mean) ** 2).mean()\n                    if not torch.isnan(pl_loss):\n                        gen_loss = gen_loss + pl_loss\n\n            gen_loss = gen_loss / self.gradient_accumulate_every\n            gen_loss.register_hook(raise_if_nan)\n            backwards(gen_loss, self.GAN.G_opt)\n\n            total_gen_loss += loss.detach().item() / self.gradient_accumulate_every\n\n        self.g_loss = float(total_gen_loss)\n        self.GAN.G_opt.step()\n\n        # calculate moving averages\n\n        if apply_path_penalty and not np.isnan(avg_pl_length):\n            self.pl_mean = self.pl_length_ma.update_average(self.pl_mean, avg_pl_length)\n\n        if self.steps % 10 == 0 and self.steps > 20000:\n            self.GAN.EMA()\n\n        if self.steps <= 25000 and self.steps % 1000 == 2:\n            self.GAN.reset_parameter_averaging()\n\n        # save from NaN errors\n\n        checkpoint_num = floor(self.steps / self.save_every)\n\n        if any(torch.isnan(l) for l in (total_gen_loss, total_disc_loss)):\n            print(f'NaN detected for generator or discriminator. Loading from checkpoint #{checkpoint_num}')\n            self.load(checkpoint_num)\n            raise NanException\n\n        # periodically save results\n\n        if self.steps % self.save_every == 0:\n            self.save(checkpoint_num)\n\n        if self.steps % 1000 == 0 or (self.steps % 100 == 0 and self.steps < 2500):\n            self.evaluate(floor(self.steps / 1000))\n\n        self.steps += 1\n        self.av = None\n\n    @torch.no_grad()\n    def evaluate(self, num = 0, num_image_tiles = 8, trunc = 1.0):\n        self.GAN.eval()\n        ext = 'jpg' if not self.transparent else 'png'\n        num_rows = num_image_tiles\n\n        def generate_images(stylizer, generator, latents, noise):\n            w = latent_to_w(stylizer, latents)\n            w_styles = styles_def_to_tensor(w)\n            generated_images = evaluate_in_chunks(self.batch_size, generator, w_styles, noise)\n            generated_images.clamp_(0., 1.)\n            return generated_images\n    \n        latent_dim = self.GAN.G.latent_dim\n        image_size = self.GAN.G.image_size\n        num_layers = self.GAN.G.num_layers\n\n        # latents and noise\n\n        latents = noise_list(num_rows ** 2, num_layers, latent_dim)\n        n = image_noise(num_rows ** 2, image_size)\n\n        # regular\n\n        generated_images = self.generate_truncated(self.GAN.S, self.GAN.G, latents, n, trunc_psi = self.trunc_psi)\n        torchvision.utils.save_image(generated_images, str(self.results_dir / self.name / f'{str(num)}.{ext}'), nrow=num_rows)\n        \n        # moving averages\n\n        generated_images = self.generate_truncated(self.GAN.SE, self.GAN.GE, latents, n, trunc_psi = self.trunc_psi)\n        torchvision.utils.save_image(generated_images, str(self.results_dir / self.name / f'{str(num)}-ema.{ext}'), nrow=num_rows)\n\n        # mixing regularities\n\n        def tile(a, dim, n_tile):\n            init_dim = a.size(dim)\n            repeat_idx = [1] * a.dim()\n            repeat_idx[dim] = n_tile\n            a = a.repeat(*(repeat_idx))\n            order_index = torch.LongTensor(np.concatenate([init_dim * np.arange(n_tile) + i for i in range(init_dim)])).cuda()\n            return torch.index_select(a, dim, order_index)\n\n        nn = noise(num_rows, latent_dim)\n        tmp1 = tile(nn, 0, num_rows)\n        tmp2 = nn.repeat(num_rows, 1)\n\n        tt = int(num_layers / 2)\n        mixed_latents = [(tmp1, tt), (tmp2, num_layers - tt)]\n\n        generated_images = self.generate_truncated(self.GAN.SE, self.GAN.GE, mixed_latents, n, trunc_psi = self.trunc_psi)\n        torchvision.utils.save_image(generated_images, str(self.results_dir / self.name / f'{str(num)}-mr.{ext}'), nrow=num_rows)\n\n    @torch.no_grad()\n    def generate_truncated(self, S, G, style, noi, trunc_psi = 0.75, num_image_tiles = 8):\n        latent_dim = G.latent_dim\n\n        if self.av is None:\n            z = noise(2000, latent_dim)\n            samples = evaluate_in_chunks(self.batch_size, S, z).cpu().numpy()\n            self.av = np.mean(samples, axis = 0)\n            self.av = np.expand_dims(self.av, axis = 0)\n            \n        w_space = []\n        for tensor, num_layers in style:\n            tmp = S(tensor)\n            av_torch = torch.from_numpy(self.av).cuda()\n            tmp = trunc_psi * (tmp - av_torch) + av_torch\n            w_space.append((tmp, num_layers))\n\n        w_styles = styles_def_to_tensor(w_space)\n        generated_images = evaluate_in_chunks(self.batch_size, G, w_styles, noi)\n        return generated_images.clamp_(0., 1.)\n\n    def print_log(self):\n        print(f'G: {self.g_loss:.2f} | D: {self.d_loss:.2f} | GP: {self.last_gp_loss:.2f} | PL: {self.pl_mean:.2f} | CR: {self.last_cr_loss:.2f} | Q: {self.q_loss:.2f}')\n\n    def model_name(self, num):\n        return str(self.models_dir / self.name / f'model_{num}.pt')\n\n    def init_folders(self):\n        (self.results_dir / self.name).mkdir(parents=True, exist_ok=True)\n        (self.models_dir / self.name).mkdir(parents=True, exist_ok=True)\n\n    def clear(self):\n        rmtree(f'./models/{self.name}', True)\n        rmtree(f'./results/{self.name}', True)\n        rmtree(str(self.config_path), True)\n        self.init_folders()\n\n    def save(self, num):\n        torch.save(self.GAN.state_dict(), self.model_name(num))\n        self.write_config()\n\n    def load(self, num = -1):\n        self.load_config()\n\n        name = num\n        if num == -1:\n            file_paths = [p for p in Path(self.models_dir / self.name).glob('model_*.pt')]\n            saved_nums = sorted(map(lambda x: int(x.stem.split('_')[1]), file_paths))\n            if len(saved_nums) == 0:\n                return\n            name = saved_nums[-1]\n            print(f'continuing from previous epoch - {name}')\n        self.steps = name * self.save_every\n        self.GAN.load_state_dict(torch.load(self.model_name(name)))\n"""
