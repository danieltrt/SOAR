file_path,api_count,code
demo.py,3,"b'import glob\nimport os\nimport time\n\nimport torch\nfrom PIL import Image\nfrom vizer.draw import draw_boxes\n\nfrom ssd.config import cfg\nfrom ssd.data.datasets import COCODataset, VOCDataset\nimport argparse\nimport numpy as np\n\nfrom ssd.data.transforms import build_transforms\nfrom ssd.modeling.detector import build_detection_model\nfrom ssd.utils import mkdir\nfrom ssd.utils.checkpoint import CheckPointer\n\n\n@torch.no_grad()\ndef run_demo(cfg, ckpt, score_threshold, images_dir, output_dir, dataset_type):\n    if dataset_type == ""voc"":\n        class_names = VOCDataset.class_names\n    elif dataset_type == \'coco\':\n        class_names = COCODataset.class_names\n    else:\n        raise NotImplementedError(\'Not implemented now.\')\n    device = torch.device(cfg.MODEL.DEVICE)\n\n    model = build_detection_model(cfg)\n    model = model.to(device)\n    checkpointer = CheckPointer(model, save_dir=cfg.OUTPUT_DIR)\n    checkpointer.load(ckpt, use_latest=ckpt is None)\n    weight_file = ckpt if ckpt else checkpointer.get_checkpoint_file()\n    print(\'Loaded weights from {}\'.format(weight_file))\n\n    image_paths = glob.glob(os.path.join(images_dir, \'*.jpg\'))\n    mkdir(output_dir)\n\n    cpu_device = torch.device(""cpu"")\n    transforms = build_transforms(cfg, is_train=False)\n    model.eval()\n    for i, image_path in enumerate(image_paths):\n        start = time.time()\n        image_name = os.path.basename(image_path)\n\n        image = np.array(Image.open(image_path).convert(""RGB""))\n        height, width = image.shape[:2]\n        images = transforms(image)[0].unsqueeze(0)\n        load_time = time.time() - start\n\n        start = time.time()\n        result = model(images.to(device))[0]\n        inference_time = time.time() - start\n\n        result = result.resize((width, height)).to(cpu_device).numpy()\n        boxes, labels, scores = result[\'boxes\'], result[\'labels\'], result[\'scores\']\n\n        indices = scores > score_threshold\n        boxes = boxes[indices]\n        labels = labels[indices]\n        scores = scores[indices]\n        meters = \' | \'.join(\n            [\n                \'objects {:02d}\'.format(len(boxes)),\n                \'load {:03d}ms\'.format(round(load_time * 1000)),\n                \'inference {:03d}ms\'.format(round(inference_time * 1000)),\n                \'FPS {}\'.format(round(1.0 / inference_time))\n            ]\n        )\n        print(\'({:04d}/{:04d}) {}: {}\'.format(i + 1, len(image_paths), image_name, meters))\n\n        drawn_image = draw_boxes(image, boxes, labels, scores, class_names).astype(np.uint8)\n        Image.fromarray(drawn_image).save(os.path.join(output_dir, image_name))\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=""SSD Demo."")\n    parser.add_argument(\n        ""--config-file"",\n        default="""",\n        metavar=""FILE"",\n        help=""path to config file"",\n        type=str,\n    )\n    parser.add_argument(""--ckpt"", type=str, default=None, help=""Trained weights."")\n    parser.add_argument(""--score_threshold"", type=float, default=0.7)\n    parser.add_argument(""--images_dir"", default=\'demo\', type=str, help=\'Specify a image dir to do prediction.\')\n    parser.add_argument(""--output_dir"", default=\'demo/result\', type=str, help=\'Specify a image dir to save predicted images.\')\n    parser.add_argument(""--dataset_type"", default=""voc"", type=str, help=\'Specify dataset type. Currently support voc and coco.\')\n\n    parser.add_argument(\n        ""opts"",\n        help=""Modify config options using the command-line"",\n        default=None,\n        nargs=argparse.REMAINDER,\n    )\n    args = parser.parse_args()\n    print(args)\n\n    cfg.merge_from_file(args.config_file)\n    cfg.merge_from_list(args.opts)\n    cfg.freeze()\n\n    print(""Loaded configuration file {}"".format(args.config_file))\n    with open(args.config_file, ""r"") as cf:\n        config_str = ""\\n"" + cf.read()\n        print(config_str)\n    print(""Running with config:\\n{}"".format(cfg))\n\n    run_demo(cfg=cfg,\n             ckpt=args.ckpt,\n             score_threshold=args.score_threshold,\n             images_dir=args.images_dir,\n             output_dir=args.output_dir,\n             dataset_type=args.dataset_type)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
setup.py,0,"b'from setuptools import setup, find_packages\n\nwith open(""README.md"", ""r"") as fh:\n    long_description = fh.read()\n\nsetup(\n    name=""torch-ssd"",\n    version=""1.2.0"",\n    packages=find_packages(exclude=[\'ext\']),\n    install_requires=[\n        ""torch>=1.3"",\n        ""torchvision>=0.3"",\n        ""opencv-python~=4.0"",\n        ""yacs==0.1.6"",\n        ""Vizer~=0.1.4"",\n    ],\n    author=""Congcong Li"",\n    author_email=""luffy.lcc@gmail.com"",\n    description=""High quality, fast, modular reference implementation of SSD in PyTorch"",\n    long_description=long_description,\n    long_description_content_type=""text/markdown"",\n    url=""https://github.com/lufficc/SSD"",\n    classifiers=[\n        ""Programming Language :: Python :: 3"",\n        ""License :: OSI Approved :: MIT License"",\n        ""Operating System :: OS Independent"",\n        ""Topic :: Scientific/Engineering :: Artificial Intelligence"",\n    ],\n    license=""MIT"",\n    python_requires="">=3.6"",\n    include_package_data=True,\n)\n'"
test.py,6,"b'import argparse\nimport logging\nimport os\n\nimport torch\nimport torch.utils.data\n\nfrom ssd.config import cfg\nfrom ssd.engine.inference import do_evaluation\nfrom ssd.modeling.detector import build_detection_model\nfrom ssd.utils import dist_util\nfrom ssd.utils.checkpoint import CheckPointer\nfrom ssd.utils.dist_util import synchronize\nfrom ssd.utils.logger import setup_logger\n\n\ndef evaluation(cfg, ckpt, distributed):\n    logger = logging.getLogger(""SSD.inference"")\n\n    model = build_detection_model(cfg)\n    checkpointer = CheckPointer(model, save_dir=cfg.OUTPUT_DIR, logger=logger)\n    device = torch.device(cfg.MODEL.DEVICE)\n    model.to(device)\n    checkpointer.load(ckpt, use_latest=ckpt is None)\n    do_evaluation(cfg, model, distributed)\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'SSD Evaluation on VOC and COCO dataset.\')\n    parser.add_argument(\n        ""--config-file"",\n        default="""",\n        metavar=""FILE"",\n        help=""path to config file"",\n        type=str,\n    )\n    parser.add_argument(""--local_rank"", type=int, default=0)\n    parser.add_argument(\n        ""--ckpt"",\n        help=""The path to the checkpoint for test, default is the latest checkpoint."",\n        default=None,\n        type=str,\n    )\n\n    parser.add_argument(""--output_dir"", default=""eval_results"", type=str, help=""The directory to store evaluation results."")\n\n    parser.add_argument(\n        ""opts"",\n        help=""Modify config options using the command-line"",\n        default=None,\n        nargs=argparse.REMAINDER,\n    )\n    args = parser.parse_args()\n\n    num_gpus = int(os.environ[""WORLD_SIZE""]) if ""WORLD_SIZE"" in os.environ else 1\n    distributed = num_gpus > 1\n\n    if torch.cuda.is_available():\n        # This flag allows you to enable the inbuilt cudnn auto-tuner to\n        # find the best algorithm to use for your hardware.\n        torch.backends.cudnn.benchmark = True\n    if distributed:\n        torch.cuda.set_device(args.local_rank)\n        torch.distributed.init_process_group(backend=""nccl"", init_method=""env://"")\n        synchronize()\n\n    cfg.merge_from_file(args.config_file)\n    cfg.merge_from_list(args.opts)\n    cfg.freeze()\n\n    logger = setup_logger(""SSD"", dist_util.get_rank(), cfg.OUTPUT_DIR)\n    logger.info(""Using {} GPUs"".format(num_gpus))\n    logger.info(args)\n\n    logger.info(""Loaded configuration file {}"".format(args.config_file))\n    with open(args.config_file, ""r"") as cf:\n        config_str = ""\\n"" + cf.read()\n        logger.info(config_str)\n    logger.info(""Running with config:\\n{}"".format(cfg))\n    evaluation(cfg, ckpt=args.ckpt, distributed=distributed)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
train.py,8,"b'import argparse\nimport logging\nimport os\n\nimport torch\nimport torch.distributed as dist\n\nfrom ssd.engine.inference import do_evaluation\nfrom ssd.config import cfg\nfrom ssd.data.build import make_data_loader\nfrom ssd.engine.trainer import do_train\nfrom ssd.modeling.detector import build_detection_model\nfrom ssd.solver.build import make_optimizer, make_lr_scheduler\nfrom ssd.utils import dist_util, mkdir\nfrom ssd.utils.checkpoint import CheckPointer\nfrom ssd.utils.dist_util import synchronize\nfrom ssd.utils.logger import setup_logger\nfrom ssd.utils.misc import str2bool\n\n\ndef train(cfg, args):\n    logger = logging.getLogger(\'SSD.trainer\')\n    model = build_detection_model(cfg)\n    device = torch.device(cfg.MODEL.DEVICE)\n    model.to(device)\n    if args.distributed:\n        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank], output_device=args.local_rank)\n\n    lr = cfg.SOLVER.LR * args.num_gpus  # scale by num gpus\n    optimizer = make_optimizer(cfg, model, lr)\n\n    milestones = [step // args.num_gpus for step in cfg.SOLVER.LR_STEPS]\n    scheduler = make_lr_scheduler(cfg, optimizer, milestones)\n\n    arguments = {""iteration"": 0}\n    save_to_disk = dist_util.get_rank() == 0\n    checkpointer = CheckPointer(model, optimizer, scheduler, cfg.OUTPUT_DIR, save_to_disk, logger)\n    extra_checkpoint_data = checkpointer.load()\n    arguments.update(extra_checkpoint_data)\n\n    max_iter = cfg.SOLVER.MAX_ITER // args.num_gpus\n    train_loader = make_data_loader(cfg, is_train=True, distributed=args.distributed, max_iter=max_iter, start_iter=arguments[\'iteration\'])\n\n    model = do_train(cfg, model, train_loader, optimizer, scheduler, checkpointer, device, arguments, args)\n    return model\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Single Shot MultiBox Detector Training With PyTorch\')\n    parser.add_argument(\n        ""--config-file"",\n        default="""",\n        metavar=""FILE"",\n        help=""path to config file"",\n        type=str,\n    )\n    parser.add_argument(""--local_rank"", type=int, default=0)\n    parser.add_argument(\'--log_step\', default=10, type=int, help=\'Print logs every log_step\')\n    parser.add_argument(\'--save_step\', default=2500, type=int, help=\'Save checkpoint every save_step\')\n    parser.add_argument(\'--eval_step\', default=2500, type=int, help=\'Evaluate dataset every eval_step, disabled when eval_step < 0\')\n    parser.add_argument(\'--use_tensorboard\', default=True, type=str2bool)\n    parser.add_argument(\n        ""--skip-test"",\n        dest=""skip_test"",\n        help=""Do not test the final model"",\n        action=""store_true"",\n    )\n    parser.add_argument(\n        ""opts"",\n        help=""Modify config options using the command-line"",\n        default=None,\n        nargs=argparse.REMAINDER,\n    )\n    args = parser.parse_args()\n    num_gpus = int(os.environ[""WORLD_SIZE""]) if ""WORLD_SIZE"" in os.environ else 1\n    args.distributed = num_gpus > 1\n    args.num_gpus = num_gpus\n\n    if torch.cuda.is_available():\n        # This flag allows you to enable the inbuilt cudnn auto-tuner to\n        # find the best algorithm to use for your hardware.\n        torch.backends.cudnn.benchmark = True\n    if args.distributed:\n        torch.cuda.set_device(args.local_rank)\n        torch.distributed.init_process_group(backend=""nccl"", init_method=""env://"")\n        synchronize()\n\n    cfg.merge_from_file(args.config_file)\n    cfg.merge_from_list(args.opts)\n    cfg.freeze()\n\n    if cfg.OUTPUT_DIR:\n        mkdir(cfg.OUTPUT_DIR)\n\n    logger = setup_logger(""SSD"", dist_util.get_rank(), cfg.OUTPUT_DIR)\n    logger.info(""Using {} GPUs"".format(num_gpus))\n    logger.info(args)\n\n    logger.info(""Loaded configuration file {}"".format(args.config_file))\n    with open(args.config_file, ""r"") as cf:\n        config_str = ""\\n"" + cf.read()\n        logger.info(config_str)\n    logger.info(""Running with config:\\n{}"".format(cfg))\n\n    model = train(cfg, args)\n\n    if not args.skip_test:\n        logger.info(\'Start evaluating...\')\n        torch.cuda.empty_cache()  # speed up evaluating after training finished\n        do_evaluation(cfg, model, distributed=args.distributed)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
ssd/__init__.py,0,b''
ssd/config/__init__.py,0,b'from .defaults import _C as cfg\n'
ssd/config/defaults.py,0,"b'from yacs.config import CfgNode as CN\n\n_C = CN()\n\n_C.MODEL = CN()\n_C.MODEL.META_ARCHITECTURE = \'SSDDetector\'\n_C.MODEL.DEVICE = ""cuda""\n# match default boxes to any ground truth with jaccard overlap higher than a threshold (0.5)\n_C.MODEL.THRESHOLD = 0.5\n_C.MODEL.NUM_CLASSES = 21\n# Hard negative mining\n_C.MODEL.NEG_POS_RATIO = 3\n_C.MODEL.CENTER_VARIANCE = 0.1\n_C.MODEL.SIZE_VARIANCE = 0.2\n\n# ---------------------------------------------------------------------------- #\n# Backbone\n# ---------------------------------------------------------------------------- #\n_C.MODEL.BACKBONE = CN()\n_C.MODEL.BACKBONE.NAME = \'vgg\'\n_C.MODEL.BACKBONE.OUT_CHANNELS = (512, 1024, 512, 256, 256, 256)\n_C.MODEL.BACKBONE.PRETRAINED = True\n\n# -----------------------------------------------------------------------------\n# PRIORS\n# -----------------------------------------------------------------------------\n_C.MODEL.PRIORS = CN()\n_C.MODEL.PRIORS.FEATURE_MAPS = [38, 19, 10, 5, 3, 1]\n_C.MODEL.PRIORS.STRIDES = [8, 16, 32, 64, 100, 300]\n_C.MODEL.PRIORS.MIN_SIZES = [30, 60, 111, 162, 213, 264]\n_C.MODEL.PRIORS.MAX_SIZES = [60, 111, 162, 213, 264, 315]\n_C.MODEL.PRIORS.ASPECT_RATIOS = [[2], [2, 3], [2, 3], [2, 3], [2], [2]]\n# When has 1 aspect ratio, every location has 4 boxes, 2 ratio 6 boxes.\n# #boxes = 2 + #ratio * 2\n_C.MODEL.PRIORS.BOXES_PER_LOCATION = [4, 6, 6, 6, 4, 4]  # number of boxes per feature map location\n_C.MODEL.PRIORS.CLIP = True\n\n# -----------------------------------------------------------------------------\n# Box Head\n# -----------------------------------------------------------------------------\n_C.MODEL.BOX_HEAD = CN()\n_C.MODEL.BOX_HEAD.NAME = \'SSDBoxHead\'\n_C.MODEL.BOX_HEAD.PREDICTOR = \'SSDBoxPredictor\'\n\n# -----------------------------------------------------------------------------\n# INPUT\n# -----------------------------------------------------------------------------\n_C.INPUT = CN()\n# Image size\n_C.INPUT.IMAGE_SIZE = 300\n# Values to be used for image normalization, RGB layout\n_C.INPUT.PIXEL_MEAN = [123, 117, 104]\n\n# -----------------------------------------------------------------------------\n# Dataset\n# -----------------------------------------------------------------------------\n_C.DATASETS = CN()\n# List of the dataset names for training, as present in paths_catalog.py\n_C.DATASETS.TRAIN = ()\n# List of the dataset names for testing, as present in paths_catalog.py\n_C.DATASETS.TEST = ()\n\n# -----------------------------------------------------------------------------\n# DataLoader\n# -----------------------------------------------------------------------------\n_C.DATA_LOADER = CN()\n# Number of data loading threads\n_C.DATA_LOADER.NUM_WORKERS = 8\n_C.DATA_LOADER.PIN_MEMORY = True\n\n# ---------------------------------------------------------------------------- #\n# Solver\n# ---------------------------------------------------------------------------- #\n_C.SOLVER = CN()\n# train configs\n_C.SOLVER.MAX_ITER = 120000\n_C.SOLVER.LR_STEPS = [80000, 100000]\n_C.SOLVER.GAMMA = 0.1\n_C.SOLVER.BATCH_SIZE = 32\n_C.SOLVER.LR = 1e-3\n_C.SOLVER.MOMENTUM = 0.9\n_C.SOLVER.WEIGHT_DECAY = 5e-4\n_C.SOLVER.WARMUP_FACTOR = 1.0 / 3\n_C.SOLVER.WARMUP_ITERS = 500\n\n# ---------------------------------------------------------------------------- #\n# Specific test options\n# ---------------------------------------------------------------------------- #\n_C.TEST = CN()\n_C.TEST.NMS_THRESHOLD = 0.45\n_C.TEST.CONFIDENCE_THRESHOLD = 0.01\n_C.TEST.MAX_PER_CLASS = -1\n_C.TEST.MAX_PER_IMAGE = 100\n_C.TEST.BATCH_SIZE = 10\n\n_C.OUTPUT_DIR = \'outputs\'\n'"
ssd/config/path_catlog.py,0,"b'import os\n\n\nclass DatasetCatalog:\n    DATA_DIR = \'datasets\'\n    DATASETS = {\n        \'voc_2007_train\': {\n            ""data_dir"": ""VOC2007"",\n            ""split"": ""train""\n        },\n        \'voc_2007_val\': {\n            ""data_dir"": ""VOC2007"",\n            ""split"": ""val""\n        },\n        \'voc_2007_trainval\': {\n            ""data_dir"": ""VOC2007"",\n            ""split"": ""trainval""\n        },\n        \'voc_2007_test\': {\n            ""data_dir"": ""VOC2007"",\n            ""split"": ""test""\n        },\n        \'voc_2012_train\': {\n            ""data_dir"": ""VOC2012"",\n            ""split"": ""train""\n        },\n        \'voc_2012_val\': {\n            ""data_dir"": ""VOC2012"",\n            ""split"": ""val""\n        },\n        \'voc_2012_trainval\': {\n            ""data_dir"": ""VOC2012"",\n            ""split"": ""trainval""\n        },\n        \'voc_2012_test\': {\n            ""data_dir"": ""VOC2012"",\n            ""split"": ""test""\n        },\n        \'coco_2014_valminusminival\': {\n            ""data_dir"": ""val2014"",\n            ""ann_file"": ""annotations/instances_valminusminival2014.json""\n        },\n        \'coco_2014_minival\': {\n            ""data_dir"": ""val2014"",\n            ""ann_file"": ""annotations/instances_minival2014.json""\n        },\n        \'coco_2014_train\': {\n            ""data_dir"": ""train2014"",\n            ""ann_file"": ""annotations/instances_train2014.json""\n        },\n        \'coco_2014_val\': {\n            ""data_dir"": ""val2014"",\n            ""ann_file"": ""annotations/instances_val2014.json""\n        },\n    }\n\n    @staticmethod\n    def get(name):\n        if ""voc"" in name:\n            voc_root = DatasetCatalog.DATA_DIR\n            if \'VOC_ROOT\' in os.environ:\n                voc_root = os.environ[\'VOC_ROOT\']\n\n            attrs = DatasetCatalog.DATASETS[name]\n            args = dict(\n                data_dir=os.path.join(voc_root, attrs[""data_dir""]),\n                split=attrs[""split""],\n            )\n            return dict(factory=""VOCDataset"", args=args)\n        elif ""coco"" in name:\n            coco_root = DatasetCatalog.DATA_DIR\n            if \'COCO_ROOT\' in os.environ:\n                coco_root = os.environ[\'COCO_ROOT\']\n\n            attrs = DatasetCatalog.DATASETS[name]\n            args = dict(\n                data_dir=os.path.join(coco_root, attrs[""data_dir""]),\n                ann_file=os.path.join(coco_root, attrs[""ann_file""]),\n            )\n            return dict(factory=""COCODataset"", args=args)\n\n        raise RuntimeError(""Dataset not available: {}"".format(name))\n'"
ssd/data/__init__.py,0,b''
ssd/data/build.py,5,"b'import torch\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataloader import default_collate\n\nfrom ssd.data import samplers\nfrom ssd.data.datasets import build_dataset\nfrom ssd.data.transforms import build_transforms, build_target_transform\nfrom ssd.structures.container import Container\n\n\nclass BatchCollator:\n    def __init__(self, is_train=True):\n        self.is_train = is_train\n\n    def __call__(self, batch):\n        transposed_batch = list(zip(*batch))\n        images = default_collate(transposed_batch[0])\n        img_ids = default_collate(transposed_batch[2])\n\n        if self.is_train:\n            list_targets = transposed_batch[1]\n            targets = Container(\n                {key: default_collate([d[key] for d in list_targets]) for key in list_targets[0]}\n            )\n        else:\n            targets = None\n        return images, targets, img_ids\n\n\ndef make_data_loader(cfg, is_train=True, distributed=False, max_iter=None, start_iter=0):\n    train_transform = build_transforms(cfg, is_train=is_train)\n    target_transform = build_target_transform(cfg) if is_train else None\n    dataset_list = cfg.DATASETS.TRAIN if is_train else cfg.DATASETS.TEST\n    datasets = build_dataset(dataset_list, transform=train_transform, target_transform=target_transform, is_train=is_train)\n\n    shuffle = is_train or distributed\n\n    data_loaders = []\n\n    for dataset in datasets:\n        if distributed:\n            sampler = samplers.DistributedSampler(dataset, shuffle=shuffle)\n        elif shuffle:\n            sampler = torch.utils.data.RandomSampler(dataset)\n        else:\n            sampler = torch.utils.data.sampler.SequentialSampler(dataset)\n\n        batch_size = cfg.SOLVER.BATCH_SIZE if is_train else cfg.TEST.BATCH_SIZE\n        batch_sampler = torch.utils.data.sampler.BatchSampler(sampler=sampler, batch_size=batch_size, drop_last=False)\n        if max_iter is not None:\n            batch_sampler = samplers.IterationBasedBatchSampler(batch_sampler, num_iterations=max_iter, start_iter=start_iter)\n\n        data_loader = DataLoader(dataset, num_workers=cfg.DATA_LOADER.NUM_WORKERS, batch_sampler=batch_sampler,\n                                 pin_memory=cfg.DATA_LOADER.PIN_MEMORY, collate_fn=BatchCollator(is_train))\n        data_loaders.append(data_loader)\n\n    if is_train:\n        # during training, a single (possibly concatenated) data_loader is returned\n        assert len(data_loaders) == 1\n        return data_loaders[0]\n    return data_loaders\n'"
ssd/engine/__init__.py,0,b''
ssd/engine/inference.py,8,"b'import logging\nimport os\n\nimport torch\nimport torch.utils.data\nfrom tqdm import tqdm\n\nfrom ssd.data.build import make_data_loader\nfrom ssd.data.datasets.evaluation import evaluate\n\nfrom ssd.utils import dist_util, mkdir\nfrom ssd.utils.dist_util import synchronize, is_main_process\n\n\ndef _accumulate_predictions_from_multiple_gpus(predictions_per_gpu):\n    all_predictions = dist_util.all_gather(predictions_per_gpu)\n    if not dist_util.is_main_process():\n        return\n    # merge the list of dicts\n    predictions = {}\n    for p in all_predictions:\n        predictions.update(p)\n    # convert a dict where the key is the index in a list\n    image_ids = list(sorted(predictions.keys()))\n    if len(image_ids) != image_ids[-1] + 1:\n        logger = logging.getLogger(""SSD.inference"")\n        logger.warning(\n            ""Number of images that were gathered from multiple processes is not ""\n            ""a contiguous set. Some images might be missing from the evaluation""\n        )\n\n    # convert to a list\n    predictions = [predictions[i] for i in image_ids]\n    return predictions\n\n\ndef compute_on_dataset(model, data_loader, device):\n    results_dict = {}\n    for batch in tqdm(data_loader):\n        images, targets, image_ids = batch\n        cpu_device = torch.device(""cpu"")\n        with torch.no_grad():\n            outputs = model(images.to(device))\n\n            outputs = [o.to(cpu_device) for o in outputs]\n        results_dict.update(\n            {img_id: result for img_id, result in zip(image_ids, outputs)}\n        )\n    return results_dict\n\n\ndef inference(model, data_loader, dataset_name, device, output_folder=None, use_cached=False, **kwargs):\n    dataset = data_loader.dataset\n    logger = logging.getLogger(""SSD.inference"")\n    logger.info(""Evaluating {} dataset({} images):"".format(dataset_name, len(dataset)))\n    predictions_path = os.path.join(output_folder, \'predictions.pth\')\n    if use_cached and os.path.exists(predictions_path):\n        predictions = torch.load(predictions_path, map_location=\'cpu\')\n    else:\n        predictions = compute_on_dataset(model, data_loader, device)\n        synchronize()\n        predictions = _accumulate_predictions_from_multiple_gpus(predictions)\n    if not is_main_process():\n        return\n    if output_folder:\n        torch.save(predictions, predictions_path)\n    return evaluate(dataset=dataset, predictions=predictions, output_dir=output_folder, **kwargs)\n\n\n@torch.no_grad()\ndef do_evaluation(cfg, model, distributed, **kwargs):\n    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n        model = model.module\n    model.eval()\n    device = torch.device(cfg.MODEL.DEVICE)\n    data_loaders_val = make_data_loader(cfg, is_train=False, distributed=distributed)\n    eval_results = []\n    for dataset_name, data_loader in zip(cfg.DATASETS.TEST, data_loaders_val):\n        output_folder = os.path.join(cfg.OUTPUT_DIR, ""inference"", dataset_name)\n        if not os.path.exists(output_folder):\n            mkdir(output_folder)\n        eval_result = inference(model, data_loader, dataset_name, device, output_folder, **kwargs)\n        eval_results.append(eval_result)\n    return eval_results\n'"
ssd/engine/trainer.py,5,"b'import collections\nimport datetime\nimport logging\nimport os\nimport time\nimport torch\nimport torch.distributed as dist\n\nfrom ssd.engine.inference import do_evaluation\nfrom ssd.utils import dist_util\nfrom ssd.utils.metric_logger import MetricLogger\n\n\ndef write_metric(eval_result, prefix, summary_writer, global_step):\n    for key in eval_result:\n        value = eval_result[key]\n        tag = \'{}/{}\'.format(prefix, key)\n        if isinstance(value, collections.Mapping):\n            write_metric(value, tag, summary_writer, global_step)\n        else:\n            summary_writer.add_scalar(tag, value, global_step=global_step)\n\n\ndef reduce_loss_dict(loss_dict):\n    """"""\n    Reduce the loss dictionary from all processes so that process with rank\n    0 has the averaged results. Returns a dict with the same fields as\n    loss_dict, after reduction.\n    """"""\n    world_size = dist_util.get_world_size()\n    if world_size < 2:\n        return loss_dict\n    with torch.no_grad():\n        loss_names = []\n        all_losses = []\n        for k in sorted(loss_dict.keys()):\n            loss_names.append(k)\n            all_losses.append(loss_dict[k])\n        all_losses = torch.stack(all_losses, dim=0)\n        dist.reduce(all_losses, dst=0)\n        if dist.get_rank() == 0:\n            # only main process gets accumulated, so only divide by\n            # world_size in this case\n            all_losses /= world_size\n        reduced_losses = {k: v for k, v in zip(loss_names, all_losses)}\n    return reduced_losses\n\n\ndef do_train(cfg, model,\n             data_loader,\n             optimizer,\n             scheduler,\n             checkpointer,\n             device,\n             arguments,\n             args):\n    logger = logging.getLogger(""SSD.trainer"")\n    logger.info(""Start training ..."")\n    meters = MetricLogger()\n\n    model.train()\n    save_to_disk = dist_util.get_rank() == 0\n    if args.use_tensorboard and save_to_disk:\n        try:\n            from torch.utils.tensorboard import SummaryWriter\n        except ImportError:\n            from tensorboardX import SummaryWriter\n        summary_writer = SummaryWriter(log_dir=os.path.join(cfg.OUTPUT_DIR, \'tf_logs\'))\n    else:\n        summary_writer = None\n\n    max_iter = len(data_loader)\n    start_iter = arguments[""iteration""]\n    start_training_time = time.time()\n    end = time.time()\n    for iteration, (images, targets, _) in enumerate(data_loader, start_iter):\n        iteration = iteration + 1\n        arguments[""iteration""] = iteration\n\n        images = images.to(device)\n        targets = targets.to(device)\n        loss_dict = model(images, targets=targets)\n        loss = sum(loss for loss in loss_dict.values())\n\n        # reduce losses over all GPUs for logging purposes\n        loss_dict_reduced = reduce_loss_dict(loss_dict)\n        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n        meters.update(total_loss=losses_reduced, **loss_dict_reduced)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        batch_time = time.time() - end\n        end = time.time()\n        meters.update(time=batch_time)\n        if iteration % args.log_step == 0:\n            eta_seconds = meters.time.global_avg * (max_iter - iteration)\n            eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n            logger.info(\n                meters.delimiter.join([\n                    ""iter: {iter:06d}"",\n                    ""lr: {lr:.5f}"",\n                    \'{meters}\',\n                    ""eta: {eta}"",\n                    \'mem: {mem}M\',\n                ]).format(\n                    iter=iteration,\n                    lr=optimizer.param_groups[0][\'lr\'],\n                    meters=str(meters),\n                    eta=eta_string,\n                    mem=round(torch.cuda.max_memory_allocated() / 1024.0 / 1024.0),\n                )\n            )\n            if summary_writer:\n                global_step = iteration\n                summary_writer.add_scalar(\'losses/total_loss\', losses_reduced, global_step=global_step)\n                for loss_name, loss_item in loss_dict_reduced.items():\n                    summary_writer.add_scalar(\'losses/{}\'.format(loss_name), loss_item, global_step=global_step)\n                summary_writer.add_scalar(\'lr\', optimizer.param_groups[0][\'lr\'], global_step=global_step)\n\n        if iteration % args.save_step == 0:\n            checkpointer.save(""model_{:06d}"".format(iteration), **arguments)\n\n        if args.eval_step > 0 and iteration % args.eval_step == 0 and not iteration == max_iter:\n            eval_results = do_evaluation(cfg, model, distributed=args.distributed, iteration=iteration)\n            if dist_util.get_rank() == 0 and summary_writer:\n                for eval_result, dataset in zip(eval_results, cfg.DATASETS.TEST):\n                    write_metric(eval_result[\'metrics\'], \'metrics/\' + dataset, summary_writer, iteration)\n            model.train()  # *IMPORTANT*: change to train mode after eval.\n\n    checkpointer.save(""model_final"", **arguments)\n    # compute training time\n    total_training_time = int(time.time() - start_training_time)\n    total_time_str = str(datetime.timedelta(seconds=total_training_time))\n    logger.info(""Total training time: {} ({:.4f} s / it)"".format(total_time_str, total_training_time / max_iter))\n    return model\n'"
ssd/layers/__init__.py,4,"b""import torch\nimport torch.nn as nn\nimport torch.nn.init as init\nfrom .separable_conv import SeparableConv2d\n\n__all__ = ['L2Norm', 'SeparableConv2d']\n\n\nclass L2Norm(nn.Module):\n    def __init__(self, n_channels, scale):\n        super(L2Norm, self).__init__()\n        self.n_channels = n_channels\n        self.gamma = scale or None\n        self.eps = 1e-10\n        self.weight = nn.Parameter(torch.Tensor(self.n_channels))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        init.constant_(self.weight, self.gamma)\n\n    def forward(self, x):\n        norm = x.pow(2).sum(dim=1, keepdim=True).sqrt() + self.eps\n        x = torch.div(x, norm)\n        out = self.weight.unsqueeze(0).unsqueeze(2).unsqueeze(3).expand_as(x) * x\n        return out\n"""
ssd/layers/separable_conv.py,0,"b'from torch import nn\n\n\nclass SeparableConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0, onnx_compatible=False):\n        super().__init__()\n        ReLU = nn.ReLU if onnx_compatible else nn.ReLU6\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=kernel_size,\n                      groups=in_channels, stride=stride, padding=padding),\n            nn.BatchNorm2d(in_channels),\n            ReLU(),\n            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1),\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n'"
ssd/modeling/__init__.py,0,b''
ssd/modeling/registry.py,0,b'from ssd.utils.registry import Registry\n\nBACKBONES = Registry()\nBOX_HEADS = Registry()\nBOX_PREDICTORS = Registry()\n'
ssd/solver/__init__.py,0,b''
ssd/solver/build.py,1,"b'import torch\n\nfrom .lr_scheduler import WarmupMultiStepLR\n\n\ndef make_optimizer(cfg, model, lr=None):\n    lr = cfg.SOLVER.BASE_LR if lr is None else lr\n    return torch.optim.SGD(model.parameters(), lr=lr, momentum=cfg.SOLVER.MOMENTUM, weight_decay=cfg.SOLVER.WEIGHT_DECAY)\n\n\ndef make_lr_scheduler(cfg, optimizer, milestones=None):\n    return WarmupMultiStepLR(optimizer=optimizer,\n                             milestones=cfg.SOLVER.LR_STEPS if milestones is None else milestones,\n                             gamma=cfg.SOLVER.GAMMA,\n                             warmup_factor=cfg.SOLVER.WARMUP_FACTOR,\n                             warmup_iters=cfg.SOLVER.WARMUP_ITERS)\n'"
ssd/solver/lr_scheduler.py,1,"b'from bisect import bisect_right\n\nfrom torch.optim.lr_scheduler import _LRScheduler\n\n\nclass WarmupMultiStepLR(_LRScheduler):\n    def __init__(self, optimizer, milestones, gamma=0.1, warmup_factor=1.0 / 3,\n                 warmup_iters=500, last_epoch=-1):\n        if not list(milestones) == sorted(milestones):\n            raise ValueError(\n                ""Milestones should be a list of"" "" increasing integers. Got {}"",\n                milestones,\n            )\n\n        self.milestones = milestones\n        self.gamma = gamma\n        self.warmup_factor = warmup_factor\n        self.warmup_iters = warmup_iters\n        super().__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        warmup_factor = 1\n        if self.last_epoch < self.warmup_iters:\n            alpha = float(self.last_epoch) / self.warmup_iters\n            warmup_factor = self.warmup_factor * (1 - alpha) + alpha\n        return [\n            base_lr\n            * warmup_factor\n            * self.gamma ** bisect_right(self.milestones, self.last_epoch)\n            for base_lr in self.base_lrs\n        ]\n'"
ssd/structures/__init__.py,0,b''
ssd/structures/container.py,0,"b'class Container:\n    """"""\n    Help class for manage boxes, labels, etc...\n    Not inherit dict due to `default_collate` will change dict\'s subclass to dict.\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        self._data_dict = dict(*args, **kwargs)\n\n    def __setattr__(self, key, value):\n        object.__setattr__(self, key, value)\n\n    def __getitem__(self, key):\n        return self._data_dict[key]\n\n    def __iter__(self):\n        return self._data_dict.__iter__()\n\n    def __setitem__(self, key, value):\n        self._data_dict[key] = value\n\n    def _call(self, name, *args, **kwargs):\n        keys = list(self._data_dict.keys())\n        for key in keys:\n            value = self._data_dict[key]\n            if hasattr(value, name):\n                self._data_dict[key] = getattr(value, name)(*args, **kwargs)\n        return self\n\n    def to(self, *args, **kwargs):\n        return self._call(\'to\', *args, **kwargs)\n\n    def numpy(self):\n        return self._call(\'numpy\')\n\n    def resize(self, size):\n        """"""resize boxes\n        Args:\n            size: (width, height)\n        Returns:\n            self\n        """"""\n        img_width = getattr(self, \'img_width\', -1)\n        img_height = getattr(self, \'img_height\', -1)\n        assert img_width > 0 and img_height > 0\n        assert \'boxes\' in self._data_dict\n        boxes = self._data_dict[\'boxes\']\n        new_width, new_height = size\n        boxes[:, 0::2] *= (new_width / img_width)\n        boxes[:, 1::2] *= (new_height / img_height)\n        return self\n\n    def __repr__(self):\n        return self._data_dict.__repr__()\n'"
ssd/utils/__init__.py,0,b'from .misc import *\n'
ssd/utils/box_utils.py,10,"b'import torch\nimport math\n\n\ndef convert_locations_to_boxes(locations, priors, center_variance,\n                               size_variance):\n    """"""Convert regressional location results of SSD into boxes in the form of (center_x, center_y, h, w).\n\n    The conversion:\n        $$predicted\\_center * center_variance = \\frac {real\\_center - prior\\_center} {prior\\_hw}$$\n        $$exp(predicted\\_hw * size_variance) = \\frac {real\\_hw} {prior\\_hw}$$\n    We do it in the inverse direction here.\n    Args:\n        locations (batch_size, num_priors, 4): the regression output of SSD. It will contain the outputs as well.\n        priors (num_priors, 4) or (batch_size/1, num_priors, 4): prior boxes.\n        center_variance: a float used to change the scale of center.\n        size_variance: a float used to change of scale of size.\n    Returns:\n        boxes:  priors: [[center_x, center_y, w, h]]. All the values\n            are relative to the image size.\n    """"""\n    # priors can have one dimension less.\n    if priors.dim() + 1 == locations.dim():\n        priors = priors.unsqueeze(0)\n    return torch.cat([\n        locations[..., :2] * center_variance * priors[..., 2:] + priors[..., :2],\n        torch.exp(locations[..., 2:] * size_variance) * priors[..., 2:]\n    ], dim=locations.dim() - 1)\n\n\ndef convert_boxes_to_locations(center_form_boxes, center_form_priors, center_variance, size_variance):\n    # priors can have one dimension less\n    if center_form_priors.dim() + 1 == center_form_boxes.dim():\n        center_form_priors = center_form_priors.unsqueeze(0)\n    return torch.cat([\n        (center_form_boxes[..., :2] - center_form_priors[..., :2]) / center_form_priors[..., 2:] / center_variance,\n        torch.log(center_form_boxes[..., 2:] / center_form_priors[..., 2:]) / size_variance\n    ], dim=center_form_boxes.dim() - 1)\n\n\ndef area_of(left_top, right_bottom) -> torch.Tensor:\n    """"""Compute the areas of rectangles given two corners.\n\n    Args:\n        left_top (N, 2): left top corner.\n        right_bottom (N, 2): right bottom corner.\n\n    Returns:\n        area (N): return the area.\n    """"""\n    hw = torch.clamp(right_bottom - left_top, min=0.0)\n    return hw[..., 0] * hw[..., 1]\n\n\ndef iou_of(boxes0, boxes1, eps=1e-5):\n    """"""Return intersection-over-union (Jaccard index) of boxes.\n\n    Args:\n        boxes0 (N, 4): ground truth boxes.\n        boxes1 (N or 1, 4): predicted boxes.\n        eps: a small number to avoid 0 as denominator.\n    Returns:\n        iou (N): IoU values.\n    """"""\n    overlap_left_top = torch.max(boxes0[..., :2], boxes1[..., :2])\n    overlap_right_bottom = torch.min(boxes0[..., 2:], boxes1[..., 2:])\n\n    overlap_area = area_of(overlap_left_top, overlap_right_bottom)\n    area0 = area_of(boxes0[..., :2], boxes0[..., 2:])\n    area1 = area_of(boxes1[..., :2], boxes1[..., 2:])\n    return overlap_area / (area0 + area1 - overlap_area + eps)\n\n\ndef assign_priors(gt_boxes, gt_labels, corner_form_priors,\n                  iou_threshold):\n    """"""Assign ground truth boxes and targets to priors.\n\n    Args:\n        gt_boxes (num_targets, 4): ground truth boxes.\n        gt_labels (num_targets): labels of targets.\n        priors (num_priors, 4): corner form priors\n    Returns:\n        boxes (num_priors, 4): real values for priors.\n        labels (num_priros): labels for priors.\n    """"""\n    # size: num_priors x num_targets\n    ious = iou_of(gt_boxes.unsqueeze(0), corner_form_priors.unsqueeze(1))\n    # size: num_priors\n    best_target_per_prior, best_target_per_prior_index = ious.max(1)\n    # size: num_targets\n    best_prior_per_target, best_prior_per_target_index = ious.max(0)\n\n    for target_index, prior_index in enumerate(best_prior_per_target_index):\n        best_target_per_prior_index[prior_index] = target_index\n    # 2.0 is used to make sure every target has a prior assigned\n    best_target_per_prior.index_fill_(0, best_prior_per_target_index, 2)\n    # size: num_priors\n    labels = gt_labels[best_target_per_prior_index]\n    labels[best_target_per_prior < iou_threshold] = 0  # the backgournd id\n    boxes = gt_boxes[best_target_per_prior_index]\n    return boxes, labels\n\n\ndef hard_negative_mining(loss, labels, neg_pos_ratio):\n    """"""\n    It used to suppress the presence of a large number of negative prediction.\n    It works on image level not batch level.\n    For any example/image, it keeps all the positive predictions and\n     cut the number of negative predictions to make sure the ratio\n     between the negative examples and positive examples is no more\n     the given ratio for an image.\n\n    Args:\n        loss (N, num_priors): the loss for each example.\n        labels (N, num_priors): the labels.\n        neg_pos_ratio:  the ratio between the negative examples and positive examples.\n    """"""\n    pos_mask = labels > 0\n    num_pos = pos_mask.long().sum(dim=1, keepdim=True)\n    num_neg = num_pos * neg_pos_ratio\n\n    loss[pos_mask] = -math.inf\n    _, indexes = loss.sort(dim=1, descending=True)\n    _, orders = indexes.sort(dim=1)\n    neg_mask = orders < num_neg\n    return pos_mask | neg_mask\n\n\ndef center_form_to_corner_form(locations):\n    return torch.cat([locations[..., :2] - locations[..., 2:] / 2,\n                      locations[..., :2] + locations[..., 2:] / 2], locations.dim() - 1)\n\n\ndef corner_form_to_center_form(boxes):\n    return torch.cat([\n        (boxes[..., :2] + boxes[..., 2:]) / 2,\n        boxes[..., 2:] - boxes[..., :2]\n    ], boxes.dim() - 1)\n'"
ssd/utils/checkpoint.py,3,"b'import logging\nimport os\n\nimport torch\nfrom torch.nn.parallel import DistributedDataParallel\n\nfrom ssd.utils.model_zoo import cache_url\n\n\nclass CheckPointer:\n    _last_checkpoint_name = \'last_checkpoint.txt\'\n\n    def __init__(self,\n                 model,\n                 optimizer=None,\n                 scheduler=None,\n                 save_dir="""",\n                 save_to_disk=None,\n                 logger=None):\n        self.model = model\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.save_dir = save_dir\n        self.save_to_disk = save_to_disk\n        if logger is None:\n            logger = logging.getLogger(__name__)\n        self.logger = logger\n\n    def save(self, name, **kwargs):\n        if not self.save_dir:\n            return\n\n        if not self.save_to_disk:\n            return\n\n        data = {}\n        if isinstance(self.model, DistributedDataParallel):\n            data[\'model\'] = self.model.module.state_dict()\n        else:\n            data[\'model\'] = self.model.state_dict()\n        if self.optimizer is not None:\n            data[""optimizer""] = self.optimizer.state_dict()\n        if self.scheduler is not None:\n            data[""scheduler""] = self.scheduler.state_dict()\n        data.update(kwargs)\n\n        save_file = os.path.join(self.save_dir, ""{}.pth"".format(name))\n        self.logger.info(""Saving checkpoint to {}"".format(save_file))\n        torch.save(data, save_file)\n\n        self.tag_last_checkpoint(save_file)\n\n    def load(self, f=None, use_latest=True):\n        if self.has_checkpoint() and use_latest:\n            # override argument with existing checkpoint\n            f = self.get_checkpoint_file()\n        if not f:\n            # no checkpoint could be found\n            self.logger.info(""No checkpoint found."")\n            return {}\n\n        self.logger.info(""Loading checkpoint from {}"".format(f))\n        checkpoint = self._load_file(f)\n        model = self.model\n        if isinstance(model, DistributedDataParallel):\n            model = self.model.module\n\n        model.load_state_dict(checkpoint.pop(""model""))\n        if ""optimizer"" in checkpoint and self.optimizer:\n            self.logger.info(""Loading optimizer from {}"".format(f))\n            self.optimizer.load_state_dict(checkpoint.pop(""optimizer""))\n        if ""scheduler"" in checkpoint and self.scheduler:\n            self.logger.info(""Loading scheduler from {}"".format(f))\n            self.scheduler.load_state_dict(checkpoint.pop(""scheduler""))\n\n        # return any further checkpoint data\n        return checkpoint\n\n    def get_checkpoint_file(self):\n        save_file = os.path.join(self.save_dir, self._last_checkpoint_name)\n        try:\n            with open(save_file, ""r"") as f:\n                last_saved = f.read()\n                last_saved = last_saved.strip()\n        except IOError:\n            # if file doesn\'t exist, maybe because it has just been\n            # deleted by a separate process\n            last_saved = """"\n        return last_saved\n\n    def has_checkpoint(self):\n        save_file = os.path.join(self.save_dir, self._last_checkpoint_name)\n        return os.path.exists(save_file)\n\n    def tag_last_checkpoint(self, last_filename):\n        save_file = os.path.join(self.save_dir, self._last_checkpoint_name)\n        with open(save_file, ""w"") as f:\n            f.write(last_filename)\n\n    def _load_file(self, f):\n        # download url files\n        if f.startswith(""http""):\n            # if the file is a url path, download it and cache it\n            cached_f = cache_url(f)\n            self.logger.info(""url {} cached in {}"".format(f, cached_f))\n            f = cached_f\n        return torch.load(f, map_location=torch.device(""cpu""))\n'"
ssd/utils/dist_util.py,10,"b'import pickle\n\nimport torch\nimport torch.distributed as dist\n\n\ndef get_world_size():\n    if not dist.is_available():\n        return 1\n    if not dist.is_initialized():\n        return 1\n    return dist.get_world_size()\n\n\ndef get_rank():\n    if not dist.is_available():\n        return 0\n    if not dist.is_initialized():\n        return 0\n    return dist.get_rank()\n\n\ndef is_main_process():\n    return get_rank() == 0\n\n\ndef synchronize():\n    """"""\n       Helper function to synchronize (barrier) among all processes when\n       using distributed training\n    """"""\n    if not dist.is_available():\n        return\n    if not dist.is_initialized():\n        return\n    world_size = dist.get_world_size()\n    if world_size == 1:\n        return\n    dist.barrier()\n\n\ndef _encode(encoded_data, data):\n    # gets a byte representation for the data\n    encoded_bytes = pickle.dumps(data)\n    # convert this byte string into a byte tensor\n    storage = torch.ByteStorage.from_buffer(encoded_bytes)\n    tensor = torch.ByteTensor(storage).to(""cuda"")\n    # encoding: first byte is the size and then rest is the data\n    s = tensor.numel()\n    assert s <= 255, ""Can\'t encode data greater than 255 bytes""\n    # put the encoded data in encoded_data\n    encoded_data[0] = s\n    encoded_data[1: (s + 1)] = tensor\n\n\ndef all_gather(data):\n    """"""\n    Run all_gather on arbitrary picklable data (not necessarily tensors)\n    Args:\n        data: any picklable object\n    Returns:\n        list[data]: list of data gathered from each rank\n    """"""\n    world_size = get_world_size()\n    if world_size == 1:\n        return [data]\n\n    # serialized to a Tensor\n    buffer = pickle.dumps(data)\n    storage = torch.ByteStorage.from_buffer(buffer)\n    tensor = torch.ByteTensor(storage).to(""cuda"")\n\n    # obtain Tensor size of each rank\n    local_size = torch.LongTensor([tensor.numel()]).to(""cuda"")\n    size_list = [torch.LongTensor([0]).to(""cuda"") for _ in range(world_size)]\n    dist.all_gather(size_list, local_size)\n    size_list = [int(size.item()) for size in size_list]\n    max_size = max(size_list)\n\n    # receiving Tensor from all ranks\n    # we pad the tensor because torch all_gather does not support\n    # gathering tensors of different shapes\n    tensor_list = []\n    for _ in size_list:\n        tensor_list.append(torch.ByteTensor(size=(max_size,)).to(""cuda""))\n    if local_size != max_size:\n        padding = torch.ByteTensor(size=(max_size - local_size,)).to(""cuda"")\n        tensor = torch.cat((tensor, padding), dim=0)\n    dist.all_gather(tensor_list, tensor)\n\n    data_list = []\n    for size, tensor in zip(size_list, tensor_list):\n        buffer = tensor.cpu().numpy().tobytes()[:size]\n        data_list.append(pickle.loads(buffer))\n\n    return data_list\n'"
ssd/utils/logger.py,0,"b'import logging\nimport os\nimport sys\n\n\ndef setup_logger(name, distributed_rank, save_dir=None):\n    logger = logging.getLogger(name)\n    logger.setLevel(logging.DEBUG)\n    # don\'t log results for the non-master process\n    if distributed_rank > 0:\n        return logger\n    stream_handler = logging.StreamHandler(stream=sys.stdout)\n    stream_handler.setLevel(logging.DEBUG)\n    formatter = logging.Formatter(""%(asctime)s %(name)s %(levelname)s: %(message)s"")\n    stream_handler.setFormatter(formatter)\n    logger.addHandler(stream_handler)\n    if save_dir:\n        fh = logging.FileHandler(os.path.join(save_dir, \'log.txt\'))\n        fh.setLevel(logging.DEBUG)\n        fh.setFormatter(formatter)\n        logger.addHandler(fh)\n    return logger\n'"
ssd/utils/metric_logger.py,1,"b'from collections import deque, defaultdict\nimport numpy as np\nimport torch\n\n\nclass SmoothedValue:\n    """"""Track a series of values and provide access to smoothed values over a\n    window or the global series average.\n    """"""\n\n    def __init__(self, window_size=10):\n        self.deque = deque(maxlen=window_size)\n        self.value = np.nan\n        self.series = []\n        self.total = 0.0\n        self.count = 0\n\n    def update(self, value):\n        self.deque.append(value)\n        self.series.append(value)\n        self.count += 1\n        self.total += value\n        self.value = value\n\n    @property\n    def median(self):\n        values = np.array(self.deque)\n        return np.median(values)\n\n    @property\n    def avg(self):\n        values = np.array(self.deque)\n        return np.mean(values)\n\n    @property\n    def global_avg(self):\n        return self.total / self.count\n\n\nclass MetricLogger:\n    def __init__(self, delimiter="", ""):\n        self.meters = defaultdict(SmoothedValue)\n        self.delimiter = delimiter\n\n    def update(self, **kwargs):\n        for k, v in kwargs.items():\n            if isinstance(v, torch.Tensor):\n                v = v.item()\n            assert isinstance(v, (float, int))\n            self.meters[k].update(v)\n\n    def __getattr__(self, attr):\n        if attr in self.meters:\n            return self.meters[attr]\n        if attr in self.__dict__:\n            return self.__dict__[attr]\n        raise AttributeError(""\'{}\' object has no attribute \'{}\'"".format(\n            type(self).__name__, attr))\n\n    def __str__(self):\n        loss_str = []\n        for name, meter in self.meters.items():\n            loss_str.append(\n                ""{}: {:.3f} ({:.3f})"".format(name, meter.avg, meter.global_avg)\n            )\n        return self.delimiter.join(loss_str)\n'"
ssd/utils/misc.py,0,"b""import errno\nimport os\n\n\ndef str2bool(s):\n    return s.lower() in ('true', '1')\n\n\ndef mkdir(path):\n    try:\n        os.makedirs(path)\n    except OSError as e:\n        if e.errno != errno.EEXIST:\n            raise\n"""
ssd/utils/model_zoo.py,4,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport os\nimport sys\n\nimport torch\n\nfrom ssd.utils.dist_util import is_main_process, synchronize\n\nfrom torch.hub import download_url_to_file\nfrom torch.hub import urlparse\nfrom torch.hub import HASH_REGEX\n\n\n# very similar to https://github.com/pytorch/pytorch/blob/master/torch/utils/model_zoo.py\n# but with a few improvements and modifications\ndef cache_url(url, model_dir=None, progress=True):\n    r""""""Loads the Torch serialized object at the given URL.\n    If the object is already present in `model_dir`, it\'s deserialized and\n    returned. The filename part of the URL should follow the naming convention\n    ``filename-<sha256>.ext`` where ``<sha256>`` is the first eight or more\n    digits of the SHA256 hash of the contents of the file. The hash is used to\n    ensure unique names and to verify the contents of the file.\n    The default value of `model_dir` is ``$TORCH_HOME/models`` where\n    ``$TORCH_HOME`` defaults to ``~/.torch``. The default directory can be\n    overridden with the ``$TORCH_MODEL_ZOO`` environment variable.\n    Args:\n        url (string): URL of the object to download\n        model_dir (string, optional): directory in which to save the object\n        progress (bool, optional): whether or not to display a progress bar to stderr\n    Example:\n        >>> cached_file = maskrcnn_benchmark.utils.model_zoo.cache_url(\'https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth\')\n    """"""\n    if model_dir is None:\n        torch_home = os.path.expanduser(os.getenv(""TORCH_HOME"", ""~/.torch""))\n        model_dir = os.getenv(""TORCH_MODEL_ZOO"", os.path.join(torch_home, ""models""))\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n    parts = urlparse(url)\n    filename = os.path.basename(parts.path)\n    if filename == ""model_final.pkl"":\n        # workaround as pre-trained Caffe2 models from Detectron have all the same filename\n        # so make the full path the filename by replacing / with _\n        filename = parts.path.replace(""/"", ""_"")\n    cached_file = os.path.join(model_dir, filename)\n    if not os.path.exists(cached_file) and is_main_process():\n        sys.stderr.write(\'Downloading: ""{}"" to {}\\n\'.format(url, cached_file))\n        hash_prefix = HASH_REGEX.search(filename)\n        if hash_prefix is not None:\n            hash_prefix = hash_prefix.group(1)\n            # workaround: Caffe2 models don\'t have a hash, but follow the R-50 convention,\n            # which matches the hash PyTorch uses. So we skip the hash matching\n            # if the hash_prefix is less than 6 characters\n            if len(hash_prefix) < 6:\n                hash_prefix = None\n        download_url_to_file(url, cached_file, hash_prefix, progress=progress)\n    synchronize()\n    return cached_file\n\n\ndef load_state_dict_from_url(url, map_location=\'cpu\'):\n    cached_file = cache_url(url)\n    return torch.load(cached_file, map_location=map_location)\n'"
ssd/utils/nms.py,1,"b'import sys\nimport warnings\n\nimport torch\nimport torchvision\n\nif torchvision.__version__ >= \'0.3.0\':\n    _nms = torchvision.ops.nms\nelse:\n    warnings.warn(\'No NMS is available. Please upgrade torchvision to 0.3.0+\')\n    sys.exit(-1)\n\n\ndef nms(boxes, scores, nms_thresh):\n    """""" Performs non-maximum suppression, run on GPU or CPU according to\n    boxes\'s device.\n    Args:\n        boxes(Tensor[N, 4]): boxes in (x1, y1, x2, y2) format, use absolute coordinates(or relative coordinates)\n        scores(Tensor[N]): scores\n        nms_thresh(float): thresh\n    Returns:\n        indices kept.\n    """"""\n    keep = _nms(boxes, scores, nms_thresh)\n    return keep\n\n\ndef batched_nms(boxes, scores, idxs, iou_threshold):\n    """"""\n    Performs non-maximum suppression in a batched fashion.\n\n    Each index value correspond to a category, and NMS\n    will not be applied between elements of different categories.\n\n    Parameters\n    ----------\n    boxes : Tensor[N, 4]\n        boxes where NMS will be performed. They\n        are expected to be in (x1, y1, x2, y2) format\n    scores : Tensor[N]\n        scores for each one of the boxes\n    idxs : Tensor[N]\n        indices of the categories for each one of the boxes.\n    iou_threshold : float\n        discards all overlapping boxes\n        with IoU < iou_threshold\n\n    Returns\n    -------\n    keep : Tensor\n        int64 tensor with the indices of\n        the elements that have been kept by NMS, sorted\n        in decreasing order of scores\n    """"""\n    if boxes.numel() == 0:\n        return torch.empty((0,), dtype=torch.int64, device=boxes.device)\n    # strategy: in order to perform NMS independently per class.\n    # we add an offset to all the boxes. The offset is dependent\n    # only on the class idx, and is large enough so that boxes\n    # from different classes do not overlap\n    max_coordinate = boxes.max()\n    offsets = idxs.to(boxes) * (max_coordinate + 1)\n    boxes_for_nms = boxes + offsets[:, None]\n    keep = nms(boxes_for_nms, scores, iou_threshold)\n    return keep\n'"
ssd/utils/registry.py,0,"b'def _register_generic(module_dict, module_name, module):\n    assert module_name not in module_dict\n    module_dict[module_name] = module\n\n\nclass Registry(dict):\n    """"""\n    A helper class for managing registering modules, it extends a dictionary\n    and provides a register functions.\n    Eg. creating a registry:\n        some_registry = Registry({""default"": default_module})\n    There\'re two ways of registering new modules:\n    1): normal way is just calling register function:\n        def foo():\n            ...\n        some_registry.register(""foo_module"", foo)\n    2): used as decorator when declaring the module:\n        @some_registry.register(""foo_module"")\n        @some_registry.register(""foo_module_nickname"")\n        def foo():\n            ...\n    Access of module is just like using a dictionary, eg:\n        f = some_registry[""foo_module""]\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super(Registry, self).__init__(*args, **kwargs)\n\n    def register(self, module_name, module=None):\n        # used as function call\n        if module is not None:\n            _register_generic(self, module_name, module)\n            return\n\n        # used as decorator\n        def register_fn(fn):\n            _register_generic(self, module_name, fn)\n            return fn\n\n        return register_fn\n'"
ssd/data/datasets/__init__.py,1,"b""from torch.utils.data import ConcatDataset\n\nfrom ssd.config.path_catlog import DatasetCatalog\nfrom .voc import VOCDataset\nfrom .coco import COCODataset\n\n_DATASETS = {\n    'VOCDataset': VOCDataset,\n    'COCODataset': COCODataset,\n}\n\n\ndef build_dataset(dataset_list, transform=None, target_transform=None, is_train=True):\n    assert len(dataset_list) > 0\n    datasets = []\n    for dataset_name in dataset_list:\n        data = DatasetCatalog.get(dataset_name)\n        args = data['args']\n        factory = _DATASETS[data['factory']]\n        args['transform'] = transform\n        args['target_transform'] = target_transform\n        if factory == VOCDataset:\n            args['keep_difficult'] = not is_train\n        elif factory == COCODataset:\n            args['remove_empty'] = is_train\n        dataset = factory(**args)\n        datasets.append(dataset)\n    # for testing, return a list of datasets\n    if not is_train:\n        return datasets\n    dataset = datasets[0]\n    if len(datasets) > 1:\n        dataset = ConcatDataset(datasets)\n\n    return [dataset]\n"""
ssd/data/datasets/coco.py,2,"b'import os\nimport torch.utils.data\nimport numpy as np\nfrom PIL import Image\n\nfrom ssd.structures.container import Container\n\n\nclass COCODataset(torch.utils.data.Dataset):\n    class_names = (\'__background__\',\n                   \'person\', \'bicycle\', \'car\', \'motorcycle\', \'airplane\', \'bus\',\n                   \'train\', \'truck\', \'boat\', \'traffic light\', \'fire hydrant\',\n                   \'stop sign\', \'parking meter\', \'bench\', \'bird\', \'cat\', \'dog\',\n                   \'horse\', \'sheep\', \'cow\', \'elephant\', \'bear\', \'zebra\',\n                   \'giraffe\', \'backpack\', \'umbrella\', \'handbag\', \'tie\',\n                   \'suitcase\', \'frisbee\', \'skis\', \'snowboard\', \'sports ball\',\n                   \'kite\', \'baseball bat\', \'baseball glove\', \'skateboard\',\n                   \'surfboard\', \'tennis racket\', \'bottle\', \'wine glass\', \'cup\',\n                   \'fork\', \'knife\', \'spoon\', \'bowl\', \'banana\', \'apple\',\n                   \'sandwich\', \'orange\', \'broccoli\', \'carrot\', \'hot dog\', \'pizza\',\n                   \'donut\', \'cake\', \'chair\', \'couch\', \'potted plant\', \'bed\',\n                   \'dining table\', \'toilet\', \'tv\', \'laptop\', \'mouse\', \'remote\',\n                   \'keyboard\', \'cell phone\', \'microwave\', \'oven\', \'toaster\', \'sink\',\n                   \'refrigerator\', \'book\', \'clock\', \'vase\', \'scissors\',\n                   \'teddy bear\', \'hair drier\', \'toothbrush\')\n\n    def __init__(self, data_dir, ann_file, transform=None, target_transform=None, remove_empty=False):\n        from pycocotools.coco import COCO\n        self.coco = COCO(ann_file)\n        self.data_dir = data_dir\n        self.transform = transform\n        self.target_transform = target_transform\n        self.remove_empty = remove_empty\n        if self.remove_empty:\n            # when training, images without annotations are removed.\n            self.ids = list(self.coco.imgToAnns.keys())\n        else:\n            # when testing, all images used.\n            self.ids = list(self.coco.imgs.keys())\n        coco_categories = sorted(self.coco.getCatIds())\n        self.coco_id_to_contiguous_id = {coco_id: i + 1 for i, coco_id in enumerate(coco_categories)}\n        self.contiguous_id_to_coco_id = {v: k for k, v in self.coco_id_to_contiguous_id.items()}\n\n    def __getitem__(self, index):\n        image_id = self.ids[index]\n        boxes, labels = self._get_annotation(image_id)\n        image = self._read_image(image_id)\n        if self.transform:\n            image, boxes, labels = self.transform(image, boxes, labels)\n        if self.target_transform:\n            boxes, labels = self.target_transform(boxes, labels)\n        targets = Container(\n            boxes=boxes,\n            labels=labels,\n        )\n        return image, targets, index\n\n    def get_annotation(self, index):\n        image_id = self.ids[index]\n        return image_id, self._get_annotation(image_id)\n\n    def __len__(self):\n        return len(self.ids)\n\n    def _get_annotation(self, image_id):\n        ann_ids = self.coco.getAnnIds(imgIds=image_id)\n        ann = self.coco.loadAnns(ann_ids)\n        # filter crowd annotations\n        ann = [obj for obj in ann if obj[""iscrowd""] == 0]\n        boxes = np.array([self._xywh2xyxy(obj[""bbox""]) for obj in ann], np.float32).reshape((-1, 4))\n        labels = np.array([self.coco_id_to_contiguous_id[obj[""category_id""]] for obj in ann], np.int64).reshape((-1,))\n        # remove invalid boxes\n        keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n        boxes = boxes[keep]\n        labels = labels[keep]\n        return boxes, labels\n\n    def _xywh2xyxy(self, box):\n        x1, y1, w, h = box\n        return [x1, y1, x1 + w, y1 + h]\n\n    def get_img_info(self, index):\n        image_id = self.ids[index]\n        img_data = self.coco.imgs[image_id]\n        return img_data\n\n    def _read_image(self, image_id):\n        file_name = self.coco.loadImgs(image_id)[0][\'file_name\']\n        image_file = os.path.join(self.data_dir, file_name)\n        image = Image.open(image_file).convert(""RGB"")\n        image = np.array(image)\n        return image\n'"
ssd/data/datasets/voc.py,2,"b'import os\nimport torch.utils.data\nimport numpy as np\nimport xml.etree.ElementTree as ET\nfrom PIL import Image\n\nfrom ssd.structures.container import Container\n\n\nclass VOCDataset(torch.utils.data.Dataset):\n    class_names = (\'__background__\',\n                   \'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n                   \'bottle\', \'bus\', \'car\', \'cat\', \'chair\',\n                   \'cow\', \'diningtable\', \'dog\', \'horse\',\n                   \'motorbike\', \'person\', \'pottedplant\',\n                   \'sheep\', \'sofa\', \'train\', \'tvmonitor\')\n\n    def __init__(self, data_dir, split, transform=None, target_transform=None, keep_difficult=False):\n        """"""Dataset for VOC data.\n        Args:\n            data_dir: the root of the VOC2007 or VOC2012 dataset, the directory contains the following sub-directories:\n                Annotations, ImageSets, JPEGImages, SegmentationClass, SegmentationObject.\n        """"""\n        self.data_dir = data_dir\n        self.split = split\n        self.transform = transform\n        self.target_transform = target_transform\n        image_sets_file = os.path.join(self.data_dir, ""ImageSets"", ""Main"", ""%s.txt"" % self.split)\n        self.ids = VOCDataset._read_image_ids(image_sets_file)\n        self.keep_difficult = keep_difficult\n\n        self.class_dict = {class_name: i for i, class_name in enumerate(self.class_names)}\n\n    def __getitem__(self, index):\n        image_id = self.ids[index]\n        boxes, labels, is_difficult = self._get_annotation(image_id)\n        if not self.keep_difficult:\n            boxes = boxes[is_difficult == 0]\n            labels = labels[is_difficult == 0]\n        image = self._read_image(image_id)\n        if self.transform:\n            image, boxes, labels = self.transform(image, boxes, labels)\n        if self.target_transform:\n            boxes, labels = self.target_transform(boxes, labels)\n        targets = Container(\n            boxes=boxes,\n            labels=labels,\n        )\n        return image, targets, index\n\n    def get_annotation(self, index):\n        image_id = self.ids[index]\n        return image_id, self._get_annotation(image_id)\n\n    def __len__(self):\n        return len(self.ids)\n\n    @staticmethod\n    def _read_image_ids(image_sets_file):\n        ids = []\n        with open(image_sets_file) as f:\n            for line in f:\n                ids.append(line.rstrip())\n        return ids\n\n    def _get_annotation(self, image_id):\n        annotation_file = os.path.join(self.data_dir, ""Annotations"", ""%s.xml"" % image_id)\n        objects = ET.parse(annotation_file).findall(""object"")\n        boxes = []\n        labels = []\n        is_difficult = []\n        for obj in objects:\n            class_name = obj.find(\'name\').text.lower().strip()\n            bbox = obj.find(\'bndbox\')\n            # VOC dataset format follows Matlab, in which indexes start from 0\n            x1 = float(bbox.find(\'xmin\').text) - 1\n            y1 = float(bbox.find(\'ymin\').text) - 1\n            x2 = float(bbox.find(\'xmax\').text) - 1\n            y2 = float(bbox.find(\'ymax\').text) - 1\n            boxes.append([x1, y1, x2, y2])\n            labels.append(self.class_dict[class_name])\n            is_difficult_str = obj.find(\'difficult\').text\n            is_difficult.append(int(is_difficult_str) if is_difficult_str else 0)\n\n        return (np.array(boxes, dtype=np.float32),\n                np.array(labels, dtype=np.int64),\n                np.array(is_difficult, dtype=np.uint8))\n\n    def get_img_info(self, index):\n        img_id = self.ids[index]\n        annotation_file = os.path.join(self.data_dir, ""Annotations"", ""%s.xml"" % img_id)\n        anno = ET.parse(annotation_file).getroot()\n        size = anno.find(""size"")\n        im_info = tuple(map(int, (size.find(""height"").text, size.find(""width"").text)))\n        return {""height"": im_info[0], ""width"": im_info[1]}\n\n    def _read_image(self, image_id):\n        image_file = os.path.join(self.data_dir, ""JPEGImages"", ""%s.jpg"" % image_id)\n        image = Image.open(image_file).convert(""RGB"")\n        image = np.array(image)\n        return image\n'"
ssd/data/samplers/__init__.py,0,"b""from .iteration_based_batch_sampler import IterationBasedBatchSampler\nfrom .distributed import DistributedSampler\n\n__all__ = ['IterationBasedBatchSampler', 'DistributedSampler']\n"""
ssd/data/samplers/distributed.py,7,"b'# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n# Code is copy-pasted exactly as in torch.utils.data.distributed.\n# FIXME remove this once c10d fixes the bug it has\nimport math\nimport torch\nimport torch.distributed as dist\nfrom torch.utils.data.sampler import Sampler\n\n\nclass DistributedSampler(Sampler):\n    """"""Sampler that restricts data loading to a subset of the dataset.\n    It is especially useful in conjunction with\n    :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each\n    process can pass a DistributedSampler instance as a DataLoader sampler,\n    and load a subset of the original dataset that is exclusive to it.\n    .. note::\n        Dataset is assumed to be of constant size.\n    Arguments:\n        dataset: Dataset used for sampling.\n        num_replicas (optional): Number of processes participating in\n            distributed training.\n        rank (optional): Rank of the current process within num_replicas.\n    """"""\n\n    def __init__(self, dataset, num_replicas=None, rank=None, shuffle=True):\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(""Requires distributed package to be available"")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(""Requires distributed package to be available"")\n            rank = dist.get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n        self.num_samples = int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))\n        self.total_size = self.num_samples * self.num_replicas\n        self.shuffle = shuffle\n\n    def __iter__(self):\n        if self.shuffle:\n            # deterministically shuffle based on epoch\n            g = torch.Generator()\n            g.manual_seed(self.epoch)\n            indices = torch.randperm(len(self.dataset), generator=g).tolist()\n        else:\n            indices = torch.arange(len(self.dataset)).tolist()\n\n        # add extra samples to make it evenly divisible\n        indices += indices[: (self.total_size - len(indices))]\n        assert len(indices) == self.total_size\n\n        # subsample\n        offset = self.num_samples * self.rank\n        indices = indices[offset: offset + self.num_samples]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch\n'"
ssd/data/samplers/iteration_based_batch_sampler.py,1,"b'from torch.utils.data.sampler import BatchSampler\n\n\nclass IterationBasedBatchSampler(BatchSampler):\n    """"""\n    Wraps a BatchSampler, re-sampling from it until\n    a specified number of iterations have been sampled\n    """"""\n\n    def __init__(self, batch_sampler, num_iterations, start_iter=0):\n        self.batch_sampler = batch_sampler\n        self.num_iterations = num_iterations\n        self.start_iter = start_iter\n\n    def __iter__(self):\n        iteration = self.start_iter\n        while iteration <= self.num_iterations:\n            # if the underlying sampler has a set_epoch method, like\n            # DistributedSampler, used for making each process see\n            # a different split of the dataset, then set it\n            if hasattr(self.batch_sampler.sampler, ""set_epoch""):\n                self.batch_sampler.sampler.set_epoch(iteration)\n            for batch in self.batch_sampler:\n                iteration += 1\n                if iteration > self.num_iterations:\n                    break\n                yield batch\n\n    def __len__(self):\n        return self.num_iterations\n'"
ssd/data/transforms/__init__.py,0,"b'from ssd.modeling.anchors.prior_box import PriorBox\nfrom .target_transform import SSDTargetTransform\nfrom .transforms import *\n\n\ndef build_transforms(cfg, is_train=True):\n    if is_train:\n        transform = [\n            ConvertFromInts(),\n            PhotometricDistort(),\n            Expand(cfg.INPUT.PIXEL_MEAN),\n            RandomSampleCrop(),\n            RandomMirror(),\n            ToPercentCoords(),\n            Resize(cfg.INPUT.IMAGE_SIZE),\n            SubtractMeans(cfg.INPUT.PIXEL_MEAN),\n            ToTensor(),\n        ]\n    else:\n        transform = [\n            Resize(cfg.INPUT.IMAGE_SIZE),\n            SubtractMeans(cfg.INPUT.PIXEL_MEAN),\n            ToTensor()\n        ]\n    transform = Compose(transform)\n    return transform\n\n\ndef build_target_transform(cfg):\n    transform = SSDTargetTransform(PriorBox(cfg)(),\n                                   cfg.MODEL.CENTER_VARIANCE,\n                                   cfg.MODEL.SIZE_VARIANCE,\n                                   cfg.MODEL.THRESHOLD)\n    return transform\n'"
ssd/data/transforms/target_transform.py,2,"b'import numpy as np\nimport torch\n\nfrom ssd.utils import box_utils\n\n\nclass SSDTargetTransform:\n    def __init__(self, center_form_priors, center_variance, size_variance, iou_threshold):\n        self.center_form_priors = center_form_priors\n        self.corner_form_priors = box_utils.center_form_to_corner_form(center_form_priors)\n        self.center_variance = center_variance\n        self.size_variance = size_variance\n        self.iou_threshold = iou_threshold\n\n    def __call__(self, gt_boxes, gt_labels):\n        if type(gt_boxes) is np.ndarray:\n            gt_boxes = torch.from_numpy(gt_boxes)\n        if type(gt_labels) is np.ndarray:\n            gt_labels = torch.from_numpy(gt_labels)\n        boxes, labels = box_utils.assign_priors(gt_boxes, gt_labels,\n                                                self.corner_form_priors, self.iou_threshold)\n        boxes = box_utils.corner_form_to_center_form(boxes)\n        locations = box_utils.convert_boxes_to_locations(boxes, self.center_form_priors, self.center_variance, self.size_variance)\n       \n        return locations, labels\n'"
ssd/data/transforms/transforms.py,2,"b'# from https://github.com/amdegroot/ssd.pytorch\n\n\nimport torch\nfrom torchvision import transforms\nimport cv2\nimport numpy as np\nimport types\nfrom numpy import random\n\n\ndef intersect(box_a, box_b):\n    max_xy = np.minimum(box_a[:, 2:], box_b[2:])\n    min_xy = np.maximum(box_a[:, :2], box_b[:2])\n    inter = np.clip((max_xy - min_xy), a_min=0, a_max=np.inf)\n    return inter[:, 0] * inter[:, 1]\n\n\ndef jaccard_numpy(box_a, box_b):\n    """"""Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n    is simply the intersection over union of two boxes.\n    E.g.:\n        A \xe2\x88\xa9 B / A \xe2\x88\xaa B = A \xe2\x88\xa9 B / (area(A) + area(B) - A \xe2\x88\xa9 B)\n    Args:\n        box_a: Multiple bounding boxes, Shape: [num_boxes,4]\n        box_b: Single bounding box, Shape: [4]\n    Return:\n        jaccard overlap: Shape: [box_a.shape[0], box_a.shape[1]]\n    """"""\n    inter = intersect(box_a, box_b)\n    area_a = ((box_a[:, 2] - box_a[:, 0]) *\n              (box_a[:, 3] - box_a[:, 1]))  # [A,B]\n    area_b = ((box_b[2] - box_b[0]) *\n              (box_b[3] - box_b[1]))  # [A,B]\n    union = area_a + area_b - inter\n    return inter / union  # [A,B]\n\n\ndef remove_empty_boxes(boxes, labels):\n    """"""Removes bounding boxes of W or H equal to 0 and its labels\n\n    Args:\n        boxes   (ndarray): NP Array with bounding boxes as lines\n                           * BBOX[x1, y1, x2, y2]\n        labels  (labels): Corresponding labels with boxes\n\n    Returns:\n        ndarray: Valid bounding boxes\n        ndarray: Corresponding labels\n    """"""\n    del_boxes = []\n    for idx, box in enumerate(boxes):\n        if box[0] == box[2] or box[1] == box[3]:\n            del_boxes.append(idx)\n\n    return np.delete(boxes, del_boxes, 0), np.delete(labels, del_boxes)\n\n\nclass Compose(object):\n    """"""Composes several augmentations together.\n    Args:\n        transforms (List[Transform]): list of transforms to compose.\n    Example:\n        >>> augmentations.Compose([\n        >>>     transforms.CenterCrop(10),\n        >>>     transforms.ToTensor(),\n        >>> ])\n    """"""\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img, boxes=None, labels=None):\n        for t in self.transforms:\n            img, boxes, labels = t(img, boxes, labels)\n            if boxes is not None:\n                boxes, labels = remove_empty_boxes(boxes, labels)\n        return img, boxes, labels\n\n\nclass Lambda(object):\n    """"""Applies a lambda as a transform.""""""\n\n    def __init__(self, lambd):\n        assert isinstance(lambd, types.LambdaType)\n        self.lambd = lambd\n\n    def __call__(self, img, boxes=None, labels=None):\n        return self.lambd(img, boxes, labels)\n\n\nclass ConvertFromInts(object):\n    def __call__(self, image, boxes=None, labels=None):\n        return image.astype(np.float32), boxes, labels\n\n\nclass SubtractMeans(object):\n    def __init__(self, mean):\n        self.mean = np.array(mean, dtype=np.float32)\n\n    def __call__(self, image, boxes=None, labels=None):\n        image = image.astype(np.float32)\n        image -= self.mean\n        return image.astype(np.float32), boxes, labels\n\n\nclass ToAbsoluteCoords(object):\n    def __call__(self, image, boxes=None, labels=None):\n        height, width, channels = image.shape\n        boxes[:, 0] *= width\n        boxes[:, 2] *= width\n        boxes[:, 1] *= height\n        boxes[:, 3] *= height\n\n        return image, boxes, labels\n\n\nclass ToPercentCoords(object):\n    def __call__(self, image, boxes=None, labels=None):\n        height, width, channels = image.shape\n        boxes[:, 0] /= width\n        boxes[:, 2] /= width\n        boxes[:, 1] /= height\n        boxes[:, 3] /= height\n\n        return image, boxes, labels\n\n\nclass Resize(object):\n    def __init__(self, size=300):\n        self.size = size\n\n    def __call__(self, image, boxes=None, labels=None):\n        image = cv2.resize(image, (self.size,\n                                   self.size))\n        return image, boxes, labels\n\n\nclass RandomSaturation(object):\n    def __init__(self, lower=0.5, upper=1.5):\n        self.lower = lower\n        self.upper = upper\n        assert self.upper >= self.lower, ""contrast upper must be >= lower.""\n        assert self.lower >= 0, ""contrast lower must be non-negative.""\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            image[:, :, 1] *= random.uniform(self.lower, self.upper)\n\n        return image, boxes, labels\n\n\nclass RandomHue(object):\n    def __init__(self, delta=18.0):\n        assert delta >= 0.0 and delta <= 360.0\n        self.delta = delta\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            image[:, :, 0] += random.uniform(-self.delta, self.delta)\n            image[:, :, 0][image[:, :, 0] > 360.0] -= 360.0\n            image[:, :, 0][image[:, :, 0] < 0.0] += 360.0\n        return image, boxes, labels\n\n\nclass RandomLightingNoise(object):\n    def __init__(self):\n        self.perms = ((0, 1, 2), (0, 2, 1),\n                      (1, 0, 2), (1, 2, 0),\n                      (2, 0, 1), (2, 1, 0))\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            swap = self.perms[random.randint(len(self.perms))]\n            shuffle = SwapChannels(swap)  # shuffle channels\n            image = shuffle(image)\n        return image, boxes, labels\n\n\nclass ConvertColor(object):\n    def __init__(self, current, transform):\n        self.transform = transform\n        self.current = current\n\n    def __call__(self, image, boxes=None, labels=None):\n        if self.current == \'BGR\' and self.transform == \'HSV\':\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n        elif self.current == \'RGB\' and self.transform == \'HSV\':\n            image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n        elif self.current == \'BGR\' and self.transform == \'RGB\':\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        elif self.current == \'HSV\' and self.transform == \'BGR\':\n            image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n        elif self.current == \'HSV\' and self.transform == ""RGB"":\n            image = cv2.cvtColor(image, cv2.COLOR_HSV2RGB)\n        else:\n            raise NotImplementedError\n        return image, boxes, labels\n\n\nclass RandomContrast(object):\n    def __init__(self, lower=0.5, upper=1.5):\n        self.lower = lower\n        self.upper = upper\n        assert self.upper >= self.lower, ""contrast upper must be >= lower.""\n        assert self.lower >= 0, ""contrast lower must be non-negative.""\n\n    # expects float image\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            alpha = random.uniform(self.lower, self.upper)\n            image *= alpha\n        return image, boxes, labels\n\n\nclass RandomBrightness(object):\n    def __init__(self, delta=32):\n        assert delta >= 0.0\n        assert delta <= 255.0\n        self.delta = delta\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            delta = random.uniform(-self.delta, self.delta)\n            image += delta\n        return image, boxes, labels\n\n\nclass ToCV2Image(object):\n    def __call__(self, tensor, boxes=None, labels=None):\n        return tensor.cpu().numpy().astype(np.float32).transpose((1, 2, 0)), boxes, labels\n\n\nclass ToTensor(object):\n    def __call__(self, cvimage, boxes=None, labels=None):\n        return torch.from_numpy(cvimage.astype(np.float32)).permute(2, 0, 1), boxes, labels\n\n\nclass RandomSampleCrop(object):\n    """"""Crop\n    Arguments:\n        img (Image): the image being input during training\n        boxes (Tensor): the original bounding boxes in pt form\n        labels (Tensor): the class labels for each bbox\n        mode (float tuple): the min and max jaccard overlaps\n    Return:\n        (img, boxes, classes)\n            img (Image): the cropped image\n            boxes (Tensor): the adjusted bounding boxes in pt form\n            labels (Tensor): the class labels for each bbox\n    """"""\n\n    def __init__(self):\n        self.sample_options = (\n            # using entire original input image\n            None,\n            # sample a patch s.t. MIN jaccard w/ obj in .1,.3,.4,.7,.9\n            (0.1, None),\n            (0.3, None),\n            (0.7, None),\n            (0.9, None),\n            # randomly sample a patch\n            (None, None),\n        )\n\n    def __call__(self, image, boxes=None, labels=None):\n        # guard against no boxes\n        if boxes is not None and boxes.shape[0] == 0:\n            return image, boxes, labels\n        height, width, _ = image.shape\n        while True:\n            # randomly choose a mode\n            mode = random.choice(self.sample_options)\n            if mode is None:\n                return image, boxes, labels\n\n            min_iou, max_iou = mode\n            if min_iou is None:\n                min_iou = float(\'-inf\')\n            if max_iou is None:\n                max_iou = float(\'inf\')\n\n            # max trails (50)\n            for _ in range(50):\n                current_image = image\n\n                w = random.uniform(0.3 * width, width)\n                h = random.uniform(0.3 * height, height)\n\n                # aspect ratio constraint b/t .5 & 2\n                if h / w < 0.5 or h / w > 2:\n                    continue\n\n                left = random.uniform(width - w)\n                top = random.uniform(height - h)\n\n                # convert to integer rect x1,y1,x2,y2\n                rect = np.array([int(left), int(top), int(left + w), int(top + h)])\n\n                # calculate IoU (jaccard overlap) b/t the cropped and gt boxes\n                overlap = jaccard_numpy(boxes, rect)\n\n                # is min and max overlap constraint satisfied? if not try again\n                if overlap.max() < min_iou or overlap.min() > max_iou:\n                    continue\n\n                # cut the crop from the image\n                current_image = current_image[rect[1]:rect[3], rect[0]:rect[2],\n                                :]\n\n                # keep overlap with gt box IF center in sampled patch\n                centers = (boxes[:, :2] + boxes[:, 2:]) / 2.0\n\n                # mask in all gt boxes that above and to the left of centers\n                m1 = (rect[0] < centers[:, 0]) * (rect[1] < centers[:, 1])\n\n                # mask in all gt boxes that under and to the right of centers\n                m2 = (rect[2] > centers[:, 0]) * (rect[3] > centers[:, 1])\n\n                # mask in that both m1 and m2 are true\n                mask = m1 * m2\n\n                # have any valid boxes? try again if not\n                if not mask.any():\n                    continue\n\n                # take only matching gt boxes\n                current_boxes = boxes[mask, :].copy()\n\n                # take only matching gt labels\n                current_labels = labels[mask]\n\n                # should we use the box left and top corner or the crop\'s\n                current_boxes[:, :2] = np.maximum(current_boxes[:, :2],\n                                                  rect[:2])\n                # adjust to crop (by substracting crop\'s left,top)\n                current_boxes[:, :2] -= rect[:2]\n\n                current_boxes[:, 2:] = np.minimum(current_boxes[:, 2:],\n                                                  rect[2:])\n                # adjust to crop (by substracting crop\'s left,top)\n                current_boxes[:, 2:] -= rect[:2]\n\n                return current_image, current_boxes, current_labels\n\n\nclass Expand(object):\n    def __init__(self, mean):\n        self.mean = mean\n\n    def __call__(self, image, boxes, labels):\n        if random.randint(2):\n            return image, boxes, labels\n\n        height, width, depth = image.shape\n        ratio = random.uniform(1, 4)\n        left = random.uniform(0, width * ratio - width)\n        top = random.uniform(0, height * ratio - height)\n\n        expand_image = np.zeros(\n            (int(height * ratio), int(width * ratio), depth),\n            dtype=image.dtype)\n        expand_image[:, :, :] = self.mean\n        expand_image[int(top):int(top + height),\n        int(left):int(left + width)] = image\n        image = expand_image\n\n        boxes = boxes.copy()\n        boxes[:, :2] += (int(left), int(top))\n        boxes[:, 2:] += (int(left), int(top))\n\n        return image, boxes, labels\n\n\nclass RandomMirror(object):\n    def __call__(self, image, boxes, classes):\n        _, width, _ = image.shape\n        if random.randint(2):\n            image = image[:, ::-1]\n            boxes = boxes.copy()\n            boxes[:, 0::2] = width - boxes[:, 2::-2]\n        return image, boxes, classes\n\n\nclass SwapChannels(object):\n    """"""Transforms a tensorized image by swapping the channels in the order\n     specified in the swap tuple.\n    Args:\n        swaps (int triple): final order of channels\n            eg: (2, 1, 0)\n    """"""\n\n    def __init__(self, swaps):\n        self.swaps = swaps\n\n    def __call__(self, image):\n        """"""\n        Args:\n            image (Tensor): image tensor to be transformed\n        Return:\n            a tensor with channels swapped according to swap\n        """"""\n        # if torch.is_tensor(image):\n        #     image = image.data.cpu().numpy()\n        # else:\n        #     image = np.array(image)\n        image = image[:, :, self.swaps]\n        return image\n\n\nclass PhotometricDistort(object):\n    def __init__(self):\n        self.pd = [\n            RandomContrast(),  # RGB\n            ConvertColor(current=""RGB"", transform=\'HSV\'),  # HSV\n            RandomSaturation(),  # HSV\n            RandomHue(),  # HSV\n            ConvertColor(current=\'HSV\', transform=\'RGB\'),  # RGB\n            RandomContrast()  # RGB\n        ]\n        self.rand_brightness = RandomBrightness()\n        self.rand_light_noise = RandomLightingNoise()\n\n    def __call__(self, image, boxes, labels):\n        im = image.copy()\n        im, boxes, labels = self.rand_brightness(im, boxes, labels)\n        if random.randint(2):\n            distort = Compose(self.pd[:-1])\n        else:\n            distort = Compose(self.pd[1:])\n        im, boxes, labels = distort(im, boxes, labels)\n        return self.rand_light_noise(im, boxes, labels)\n'"
ssd/modeling/anchors/__init__.py,0,b''
ssd/modeling/anchors/prior_box.py,1,"b'from itertools import product\n\nimport torch\nfrom math import sqrt\n\n\nclass PriorBox:\n    def __init__(self, cfg):\n        self.image_size = cfg.INPUT.IMAGE_SIZE\n        prior_config = cfg.MODEL.PRIORS\n        self.feature_maps = prior_config.FEATURE_MAPS\n        self.min_sizes = prior_config.MIN_SIZES\n        self.max_sizes = prior_config.MAX_SIZES\n        self.strides = prior_config.STRIDES\n        self.aspect_ratios = prior_config.ASPECT_RATIOS\n        self.clip = prior_config.CLIP\n\n    def __call__(self):\n        """"""Generate SSD Prior Boxes.\n            It returns the center, height and width of the priors. The values are relative to the image size\n            Returns:\n                priors (num_priors, 4): The prior boxes represented as [[center_x, center_y, w, h]]. All the values\n                    are relative to the image size.\n        """"""\n        priors = []\n        for k, f in enumerate(self.feature_maps):\n            scale = self.image_size / self.strides[k]\n            for i, j in product(range(f), repeat=2):\n                # unit center x,y\n                cx = (j + 0.5) / scale\n                cy = (i + 0.5) / scale\n\n                # small sized square box\n                size = self.min_sizes[k]\n                h = w = size / self.image_size\n                priors.append([cx, cy, w, h])\n\n                # big sized square box\n                size = sqrt(self.min_sizes[k] * self.max_sizes[k])\n                h = w = size / self.image_size\n                priors.append([cx, cy, w, h])\n\n                # change h/w ratio of the small sized box\n                size = self.min_sizes[k]\n                h = w = size / self.image_size\n                for ratio in self.aspect_ratios[k]:\n                    ratio = sqrt(ratio)\n                    priors.append([cx, cy, w * ratio, h / ratio])\n                    priors.append([cx, cy, w / ratio, h * ratio])\n\n        priors = torch.tensor(priors)\n        if self.clip:\n            priors.clamp_(max=1, min=0)\n        return priors\n'"
ssd/modeling/backbone/__init__.py,0,"b""from ssd.modeling import registry\nfrom .vgg import VGG\nfrom .mobilenet import MobileNetV2\nfrom .efficient_net import EfficientNet\n\n__all__ = ['build_backbone', 'VGG', 'MobileNetV2', 'EfficientNet']\n\n\ndef build_backbone(cfg):\n    return registry.BACKBONES[cfg.MODEL.BACKBONE.NAME](cfg, cfg.MODEL.BACKBONE.PRETRAINED)\n"""
ssd/modeling/backbone/mobilenet.py,1,"b'from torch import nn\n\nfrom ssd.modeling import registry\nfrom ssd.utils.model_zoo import load_state_dict_from_url\n\nmodel_urls = {\n    \'mobilenet_v2\': \'https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\',\n}\n\n\nclass ConvBNReLU(nn.Sequential):\n    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):\n        padding = (kernel_size - 1) // 2\n        super(ConvBNReLU, self).__init__(\n            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n            nn.BatchNorm2d(out_planes),\n            nn.ReLU6(inplace=True)\n        )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        hidden_dim = int(round(inp * expand_ratio))\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        layers = []\n        if expand_ratio != 1:\n            # pw\n            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1))\n        layers.extend([\n            # dw\n            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim),\n            # pw-linear\n            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(oup),\n        ])\n        self.conv = nn.Sequential(*layers)\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass MobileNetV2(nn.Module):\n    def __init__(self, width_mult=1.0, inverted_residual_setting=None):\n        super(MobileNetV2, self).__init__()\n        block = InvertedResidual\n        input_channel = 32\n        last_channel = 1280\n\n        if inverted_residual_setting is None:\n            inverted_residual_setting = [\n                # t, c, n, s\n                [1, 16, 1, 1],\n                [6, 24, 2, 2],\n                [6, 32, 3, 2],\n                [6, 64, 4, 2],\n                [6, 96, 3, 1],\n                [6, 160, 3, 2],\n                [6, 320, 1, 1],\n            ]\n\n        # only check the first element, assuming user knows t,c,n,s are required\n        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n            raise ValueError(""inverted_residual_setting should be non-empty ""\n                             ""or a 4-element list, got {}"".format(inverted_residual_setting))\n\n        # building first layer\n        input_channel = int(input_channel * width_mult)\n        self.last_channel = int(last_channel * max(1.0, width_mult))\n        features = [ConvBNReLU(3, input_channel, stride=2)]\n        # building inverted residual blocks\n        for t, c, n, s in inverted_residual_setting:\n            output_channel = int(c * width_mult)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                features.append(block(input_channel, output_channel, stride, expand_ratio=t))\n                input_channel = output_channel\n        # building last several layers\n        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))\n        # make it nn.Sequential\n        self.features = nn.Sequential(*features)\n        self.extras = nn.ModuleList([\n            InvertedResidual(1280, 512, 2, 0.2),\n            InvertedResidual(512, 256, 2, 0.25),\n            InvertedResidual(256, 256, 2, 0.5),\n            InvertedResidual(256, 64, 2, 0.25)\n        ])\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        # weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        features = []\n        for i in range(14):\n            x = self.features[i](x)\n        features.append(x)\n\n        for i in range(14, len(self.features)):\n            x = self.features[i](x)\n        features.append(x)\n\n        for i in range(len(self.extras)):\n            x = self.extras[i](x)\n            features.append(x)\n\n        return tuple(features)\n\n\n@registry.BACKBONES.register(\'mobilenet_v2\')\ndef mobilenet_v2(cfg, pretrained=True):\n    model = MobileNetV2()\n    if pretrained:\n        model.load_state_dict(load_state_dict_from_url(model_urls[\'mobilenet_v2\']), strict=False)\n    return model\n'"
ssd/modeling/backbone/vgg.py,2,"b""import torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ssd.layers import L2Norm\nfrom ssd.modeling import registry\nfrom ssd.utils.model_zoo import load_state_dict_from_url\n\nmodel_urls = {\n    'vgg': 'https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth',\n}\n\n\n# borrowed from https://github.com/amdegroot/ssd.pytorch/blob/master/ssd.py\ndef add_vgg(cfg, batch_norm=False):\n    layers = []\n    in_channels = 3\n    for v in cfg:\n        if v == 'M':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        elif v == 'C':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n        else:\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n            else:\n                layers += [conv2d, nn.ReLU(inplace=True)]\n            in_channels = v\n    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n    conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n    conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n    layers += [pool5, conv6,\n               nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)]\n    return layers\n\n\ndef add_extras(cfg, i, size=300):\n    # Extra layers added to VGG for feature scaling\n    layers = []\n    in_channels = i\n    flag = False\n    for k, v in enumerate(cfg):\n        if in_channels != 'S':\n            if v == 'S':\n                layers += [nn.Conv2d(in_channels, cfg[k + 1], kernel_size=(1, 3)[flag], stride=2, padding=1)]\n            else:\n                layers += [nn.Conv2d(in_channels, v, kernel_size=(1, 3)[flag])]\n            flag = not flag\n        in_channels = v\n    if size == 512:\n        layers.append(nn.Conv2d(in_channels, 128, kernel_size=1, stride=1))\n        layers.append(nn.Conv2d(128, 256, kernel_size=4, stride=1, padding=1))\n    return layers\n\n\nvgg_base = {\n    '300': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M',\n            512, 512, 512],\n    '512': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M',\n            512, 512, 512],\n}\nextras_base = {\n    '300': [256, 'S', 512, 128, 'S', 256, 128, 256, 128, 256],\n    '512': [256, 'S', 512, 128, 'S', 256, 128, 'S', 256, 128, 'S', 256],\n}\n\n\nclass VGG(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        size = cfg.INPUT.IMAGE_SIZE\n        vgg_config = vgg_base[str(size)]\n        extras_config = extras_base[str(size)]\n\n        self.vgg = nn.ModuleList(add_vgg(vgg_config))\n        self.extras = nn.ModuleList(add_extras(extras_config, i=1024, size=size))\n        self.l2_norm = L2Norm(512, scale=20)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        for m in self.extras.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_uniform_(m.weight)\n                nn.init.zeros_(m.bias)\n\n    def init_from_pretrain(self, state_dict):\n        self.vgg.load_state_dict(state_dict)\n\n    def forward(self, x):\n        features = []\n        for i in range(23):\n            x = self.vgg[i](x)\n        s = self.l2_norm(x)  # Conv4_3 L2 normalization\n        features.append(s)\n\n        # apply vgg up to fc7\n        for i in range(23, len(self.vgg)):\n            x = self.vgg[i](x)\n        features.append(x)\n\n        for k, v in enumerate(self.extras):\n            x = F.relu(v(x), inplace=True)\n            if k % 2 == 1:\n                features.append(x)\n\n        return tuple(features)\n\n\n@registry.BACKBONES.register('vgg')\ndef vgg(cfg, pretrained=True):\n    model = VGG(cfg)\n    if pretrained:\n        model.init_from_pretrain(load_state_dict_from_url(model_urls['vgg']))\n    return model\n"""
ssd/modeling/box_head/__init__.py,0,"b""from ssd.modeling import registry\nfrom .box_head import SSDBoxHead\n\n__all__ = ['build_box_head', 'SSDBoxHead']\n\n\ndef build_box_head(cfg):\n    return registry.BOX_HEADS[cfg.MODEL.BOX_HEAD.NAME](cfg)\n"""
ssd/modeling/box_head/box_head.py,1,"b""from torch import nn\nimport torch.nn.functional as F\n\nfrom ssd.modeling import registry\nfrom ssd.modeling.anchors.prior_box import PriorBox\nfrom ssd.modeling.box_head.box_predictor import make_box_predictor\nfrom ssd.utils import box_utils\nfrom .inference import PostProcessor\nfrom .loss import MultiBoxLoss\n\n\n@registry.BOX_HEADS.register('SSDBoxHead')\nclass SSDBoxHead(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.predictor = make_box_predictor(cfg)\n        self.loss_evaluator = MultiBoxLoss(neg_pos_ratio=cfg.MODEL.NEG_POS_RATIO)\n        self.post_processor = PostProcessor(cfg)\n        self.priors = None\n\n    def forward(self, features, targets=None):\n        cls_logits, bbox_pred = self.predictor(features)\n        if self.training:\n            return self._forward_train(cls_logits, bbox_pred, targets)\n        else:\n            return self._forward_test(cls_logits, bbox_pred)\n\n    def _forward_train(self, cls_logits, bbox_pred, targets):\n        gt_boxes, gt_labels = targets['boxes'], targets['labels']\n        reg_loss, cls_loss = self.loss_evaluator(cls_logits, bbox_pred, gt_labels, gt_boxes)\n        loss_dict = dict(\n            reg_loss=reg_loss,\n            cls_loss=cls_loss,\n        )\n        detections = (cls_logits, bbox_pred)\n        return detections, loss_dict\n\n    def _forward_test(self, cls_logits, bbox_pred):\n        if self.priors is None:\n            self.priors = PriorBox(self.cfg)().to(bbox_pred.device)\n        scores = F.softmax(cls_logits, dim=2)\n        boxes = box_utils.convert_locations_to_boxes(\n            bbox_pred, self.priors, self.cfg.MODEL.CENTER_VARIANCE, self.cfg.MODEL.SIZE_VARIANCE\n        )\n        boxes = box_utils.center_form_to_corner_form(boxes)\n        detections = (scores, boxes)\n        detections = self.post_processor(detections)\n        return detections, {}\n"""
ssd/modeling/box_head/box_predictor.py,2,"b""import torch\nfrom torch import nn\n\nfrom ssd.layers import SeparableConv2d\nfrom ssd.modeling import registry\n\n\nclass BoxPredictor(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.cls_headers = nn.ModuleList()\n        self.reg_headers = nn.ModuleList()\n        for level, (boxes_per_location, out_channels) in enumerate(zip(cfg.MODEL.PRIORS.BOXES_PER_LOCATION, cfg.MODEL.BACKBONE.OUT_CHANNELS)):\n            self.cls_headers.append(self.cls_block(level, out_channels, boxes_per_location))\n            self.reg_headers.append(self.reg_block(level, out_channels, boxes_per_location))\n        self.reset_parameters()\n\n    def cls_block(self, level, out_channels, boxes_per_location):\n        raise NotImplementedError\n\n    def reg_block(self, level, out_channels, boxes_per_location):\n        raise NotImplementedError\n\n    def reset_parameters(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_uniform_(m.weight)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, features):\n        cls_logits = []\n        bbox_pred = []\n        for feature, cls_header, reg_header in zip(features, self.cls_headers, self.reg_headers):\n            cls_logits.append(cls_header(feature).permute(0, 2, 3, 1).contiguous())\n            bbox_pred.append(reg_header(feature).permute(0, 2, 3, 1).contiguous())\n\n        batch_size = features[0].shape[0]\n        cls_logits = torch.cat([c.view(c.shape[0], -1) for c in cls_logits], dim=1).view(batch_size, -1, self.cfg.MODEL.NUM_CLASSES)\n        bbox_pred = torch.cat([l.view(l.shape[0], -1) for l in bbox_pred], dim=1).view(batch_size, -1, 4)\n\n        return cls_logits, bbox_pred\n\n\n@registry.BOX_PREDICTORS.register('SSDBoxPredictor')\nclass SSDBoxPredictor(BoxPredictor):\n    def cls_block(self, level, out_channels, boxes_per_location):\n        return nn.Conv2d(out_channels, boxes_per_location * self.cfg.MODEL.NUM_CLASSES, kernel_size=3, stride=1, padding=1)\n\n    def reg_block(self, level, out_channels, boxes_per_location):\n        return nn.Conv2d(out_channels, boxes_per_location * 4, kernel_size=3, stride=1, padding=1)\n\n\n@registry.BOX_PREDICTORS.register('SSDLiteBoxPredictor')\nclass SSDLiteBoxPredictor(BoxPredictor):\n    def cls_block(self, level, out_channels, boxes_per_location):\n        num_levels = len(self.cfg.MODEL.BACKBONE.OUT_CHANNELS)\n        if level == num_levels - 1:\n            return nn.Conv2d(out_channels, boxes_per_location * self.cfg.MODEL.NUM_CLASSES, kernel_size=1)\n        return SeparableConv2d(out_channels, boxes_per_location * self.cfg.MODEL.NUM_CLASSES, kernel_size=3, stride=1, padding=1)\n\n    def reg_block(self, level, out_channels, boxes_per_location):\n        num_levels = len(self.cfg.MODEL.BACKBONE.OUT_CHANNELS)\n        if level == num_levels - 1:\n            return nn.Conv2d(out_channels, boxes_per_location * 4, kernel_size=1)\n        return SeparableConv2d(out_channels, boxes_per_location * 4, kernel_size=3, stride=1, padding=1)\n\n\ndef make_box_predictor(cfg):\n    return registry.BOX_PREDICTORS[cfg.MODEL.BOX_HEAD.PREDICTOR](cfg)\n"""
ssd/modeling/box_head/inference.py,2,"b'import torch\n\nfrom ssd.structures.container import Container\nfrom ssd.utils.nms import batched_nms\n\n\nclass PostProcessor:\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.width = cfg.INPUT.IMAGE_SIZE\n        self.height = cfg.INPUT.IMAGE_SIZE\n\n    def __call__(self, detections):\n        batches_scores, batches_boxes = detections\n        device = batches_scores.device\n        batch_size = batches_scores.size(0)\n        results = []\n        for batch_id in range(batch_size):\n            scores, boxes = batches_scores[batch_id], batches_boxes[batch_id]  # (N, #CLS) (N, 4)\n            num_boxes = scores.shape[0]\n            num_classes = scores.shape[1]\n\n            boxes = boxes.view(num_boxes, 1, 4).expand(num_boxes, num_classes, 4)\n            labels = torch.arange(num_classes, device=device)\n            labels = labels.view(1, num_classes).expand_as(scores)\n\n            # remove predictions with the background label\n            boxes = boxes[:, 1:]\n            scores = scores[:, 1:]\n            labels = labels[:, 1:]\n\n            # batch everything, by making every class prediction be a separate instance\n            boxes = boxes.reshape(-1, 4)\n            scores = scores.reshape(-1)\n            labels = labels.reshape(-1)\n\n            # remove low scoring boxes\n            indices = torch.nonzero(scores > self.cfg.TEST.CONFIDENCE_THRESHOLD).squeeze(1)\n            boxes, scores, labels = boxes[indices], scores[indices], labels[indices]\n\n            boxes[:, 0::2] *= self.width\n            boxes[:, 1::2] *= self.height\n\n            keep = batched_nms(boxes, scores, labels, self.cfg.TEST.NMS_THRESHOLD)\n            # keep only topk scoring predictions\n            keep = keep[:self.cfg.TEST.MAX_PER_IMAGE]\n            boxes, scores, labels = boxes[keep], scores[keep], labels[keep]\n\n            container = Container(boxes=boxes, labels=labels, scores=scores)\n            container.img_width = self.width\n            container.img_height = self.height\n            results.append(container)\n        return results\n'"
ssd/modeling/box_head/loss.py,3,"b'import torch.nn as nn\nimport torch.nn.functional as F\nimport torch\n\nfrom ssd.utils import box_utils\n\n\nclass MultiBoxLoss(nn.Module):\n    def __init__(self, neg_pos_ratio):\n        """"""Implement SSD MultiBox Loss.\n\n        Basically, MultiBox loss combines classification loss\n         and Smooth L1 regression loss.\n        """"""\n        super(MultiBoxLoss, self).__init__()\n        self.neg_pos_ratio = neg_pos_ratio\n\n    def forward(self, confidence, predicted_locations, labels, gt_locations):\n        """"""Compute classification loss and smooth l1 loss.\n\n        Args:\n            confidence (batch_size, num_priors, num_classes): class predictions.\n            predicted_locations (batch_size, num_priors, 4): predicted locations.\n            labels (batch_size, num_priors): real labels of all the priors.\n            gt_locations (batch_size, num_priors, 4): real boxes corresponding all the priors.\n        """"""\n        num_classes = confidence.size(2)\n        with torch.no_grad():\n            # derived from cross_entropy=sum(log(p))\n            loss = -F.log_softmax(confidence, dim=2)[:, :, 0]\n            mask = box_utils.hard_negative_mining(loss, labels, self.neg_pos_ratio)\n\n        confidence = confidence[mask, :]\n        classification_loss = F.cross_entropy(confidence.view(-1, num_classes), labels[mask], reduction=\'sum\')\n\n        pos_mask = labels > 0\n        predicted_locations = predicted_locations[pos_mask, :].view(-1, 4)\n        gt_locations = gt_locations[pos_mask, :].view(-1, 4)\n        smooth_l1_loss = F.smooth_l1_loss(predicted_locations, gt_locations, reduction=\'sum\')\n        num_pos = gt_locations.size(0)\n        return smooth_l1_loss / num_pos, classification_loss / num_pos\n'"
ssd/modeling/detector/__init__.py,0,"b'from .ssd_detector import SSDDetector\n\n_DETECTION_META_ARCHITECTURES = {\n    ""SSDDetector"": SSDDetector\n}\n\n\ndef build_detection_model(cfg):\n    meta_arch = _DETECTION_META_ARCHITECTURES[cfg.MODEL.META_ARCHITECTURE]\n    return meta_arch(cfg)\n'"
ssd/modeling/detector/ssd_detector.py,0,"b'from torch import nn\n\nfrom ssd.modeling.backbone import build_backbone\nfrom ssd.modeling.box_head import build_box_head\n\n\nclass SSDDetector(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        self.backbone = build_backbone(cfg)\n        self.box_head = build_box_head(cfg)\n\n    def forward(self, images, targets=None):\n        features = self.backbone(images)\n        detections, detector_losses = self.box_head(features, targets)\n        if self.training:\n            return detector_losses\n        return detections\n'"
ssd/data/datasets/evaluation/__init__.py,0,"b'from ssd.data.datasets import VOCDataset, COCODataset\nfrom .coco import coco_evaluation\nfrom .voc import voc_evaluation\n\n\ndef evaluate(dataset, predictions, output_dir, **kwargs):\n    """"""evaluate dataset using different methods based on dataset type.\n    Args:\n        dataset: Dataset object\n        predictions(list[(boxes, labels, scores)]): Each item in the list represents the\n            prediction results for one image. And the index should match the dataset index.\n        output_dir: output folder, to save evaluation files or results.\n    Returns:\n        evaluation result\n    """"""\n    args = dict(\n        dataset=dataset, predictions=predictions, output_dir=output_dir, **kwargs,\n    )\n    if isinstance(dataset, VOCDataset):\n        return voc_evaluation(**args)\n    elif isinstance(dataset, COCODataset):\n        return coco_evaluation(**args)\n    else:\n        raise NotImplementedError\n'"
ssd/modeling/backbone/efficient_net/__init__.py,0,"b""from ssd.modeling import registry\nfrom .efficient_net import EfficientNet\n\n__all__ = ['efficient_net_b3', 'EfficientNet']\n\n\n@registry.BACKBONES.register('efficient_net-b3')\ndef efficient_net_b3(cfg, pretrained=True):\n    if pretrained:\n        model = EfficientNet.from_pretrained('efficientnet-b3')\n    else:\n        model = EfficientNet.from_name('efficientnet-b3')\n\n    return model\n"""
ssd/modeling/backbone/efficient_net/efficient_net.py,2,"b'import torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom .utils import (\n    relu_fn,\n    round_filters,\n    round_repeats,\n    drop_connect,\n    Conv2dSamePadding,\n    get_model_params,\n    efficientnet_params,\n    load_pretrained_weights,\n)\n\nINDICES = {\n    \'efficientnet-b3\': [7, 17, 25]\n}\n\nEXTRAS = {\n    \'efficientnet-b3\': [\n        # in,  out, k, s, p\n        [(384, 128, 1, 1, 0), (128, 256, 3, 2, 1)],  # 5 x 5\n        [(256, 128, 1, 1, 0), (128, 256, 3, 1, 0)],  # 3 x 3\n        [(256, 128, 1, 1, 0), (128, 256, 3, 1, 0)],  # 1 x 1\n\n    ]\n}\n\n\ndef add_extras(cfgs):\n    extras = nn.ModuleList()\n    for cfg in cfgs:\n        extra = []\n        for params in cfg:\n            in_channels, out_channels, kernel_size, stride, padding = params\n            extra.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding))\n            extra.append(nn.ReLU())\n        extras.append(nn.Sequential(*extra))\n    return extras\n\n\nclass MBConvBlock(nn.Module):\n    """"""\n    Mobile Inverted Residual Bottleneck Block\n\n    Args:\n        block_args (namedtuple): BlockArgs, see above\n        global_params (namedtuple): GlobalParam, see above\n\n    Attributes:\n        has_se (bool): Whether the block contains a Squeeze and Excitation layer.\n    """"""\n\n    def __init__(self, block_args, global_params):\n        super().__init__()\n        self._block_args = block_args\n        self._bn_mom = 1 - global_params.batch_norm_momentum\n        self._bn_eps = global_params.batch_norm_epsilon\n        self.has_se = (self._block_args.se_ratio is not None) and (0 < self._block_args.se_ratio <= 1)\n        self.id_skip = block_args.id_skip  # skip connection and drop connect\n\n        # Expansion phase\n        inp = self._block_args.input_filters  # number of input channels\n        oup = self._block_args.input_filters * self._block_args.expand_ratio  # number of output channels\n        if self._block_args.expand_ratio != 1:\n            self._expand_conv = Conv2dSamePadding(in_channels=inp, out_channels=oup, kernel_size=1, bias=False)\n            self._bn0 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n\n        # Depthwise convolution phase\n        k = self._block_args.kernel_size\n        s = self._block_args.stride\n        self._depthwise_conv = Conv2dSamePadding(\n            in_channels=oup, out_channels=oup, groups=oup,  # groups makes it depthwise\n            kernel_size=k, stride=s, bias=False)\n        self._bn1 = nn.BatchNorm2d(num_features=oup, momentum=self._bn_mom, eps=self._bn_eps)\n\n        # Squeeze and Excitation layer, if desired\n        if self.has_se:\n            num_squeezed_channels = max(1, int(self._block_args.input_filters * self._block_args.se_ratio))\n            self._se_reduce = Conv2dSamePadding(in_channels=oup, out_channels=num_squeezed_channels, kernel_size=1)\n            self._se_expand = Conv2dSamePadding(in_channels=num_squeezed_channels, out_channels=oup, kernel_size=1)\n\n        # Output phase\n        final_oup = self._block_args.output_filters\n        self._project_conv = Conv2dSamePadding(in_channels=oup, out_channels=final_oup, kernel_size=1, bias=False)\n        self._bn2 = nn.BatchNorm2d(num_features=final_oup, momentum=self._bn_mom, eps=self._bn_eps)\n\n    def forward(self, inputs, drop_connect_rate=None):\n        """"""\n        :param inputs: input tensor\n        :param drop_connect_rate: drop connect rate (float, between 0 and 1)\n        :return: output of block\n        """"""\n\n        # Expansion and Depthwise Convolution\n        x = inputs\n        if self._block_args.expand_ratio != 1:\n            x = relu_fn(self._bn0(self._expand_conv(inputs)))\n        x = relu_fn(self._bn1(self._depthwise_conv(x)))\n\n        # Squeeze and Excitation\n        if self.has_se:\n            x_squeezed = F.adaptive_avg_pool2d(x, 1)\n            x_squeezed = self._se_expand(relu_fn(self._se_reduce(x_squeezed)))\n            x = torch.sigmoid(x_squeezed) * x\n\n        x = self._bn2(self._project_conv(x))\n\n        # Skip connection and drop connect\n        input_filters, output_filters = self._block_args.input_filters, self._block_args.output_filters\n        if self.id_skip and self._block_args.stride == 1 and input_filters == output_filters:\n            if drop_connect_rate:\n                x = drop_connect(x, p=drop_connect_rate, training=self.training)\n            x = x + inputs  # skip connection\n        return x\n\n\nclass EfficientNet(nn.Module):\n    """"""\n    An EfficientNet model. Most easily loaded with the .from_name or .from_pretrained methods\n\n    Args:\n        blocks_args (list): A list of BlockArgs to construct blocks\n        global_params (namedtuple): A set of GlobalParams shared between blocks\n\n    Example:\n        model = EfficientNet.from_pretrained(\'efficientnet-b0\')\n\n    """"""\n\n    def __init__(self, model_name, blocks_args=None, global_params=None):\n        super().__init__()\n        self.indices = INDICES[model_name]\n        self.extras = add_extras(EXTRAS[model_name])\n        assert isinstance(blocks_args, list), \'blocks_args should be a list\'\n        assert len(blocks_args) > 0, \'block args must be greater than 0\'\n        self._global_params = global_params\n        self._blocks_args = blocks_args\n\n        # Batch norm parameters\n        bn_mom = 1 - self._global_params.batch_norm_momentum\n        bn_eps = self._global_params.batch_norm_epsilon\n\n        # Stem\n        in_channels = 3  # rgb\n        out_channels = round_filters(32, self._global_params)  # number of output channels\n        self._conv_stem = Conv2dSamePadding(in_channels, out_channels, kernel_size=3, stride=2, bias=False)\n        self._bn0 = nn.BatchNorm2d(num_features=out_channels, momentum=bn_mom, eps=bn_eps)\n\n        # Build blocks\n        self._blocks = nn.ModuleList([])\n        for block_args in self._blocks_args:\n\n            # Update block input and output filters based on depth multiplier.\n            block_args = block_args._replace(\n                input_filters=round_filters(block_args.input_filters, self._global_params),\n                output_filters=round_filters(block_args.output_filters, self._global_params),\n                num_repeat=round_repeats(block_args.num_repeat, self._global_params)\n            )\n\n            # The first block needs to take care of stride and filter size increase.\n            self._blocks.append(MBConvBlock(block_args, self._global_params))\n            if block_args.num_repeat > 1:\n                block_args = block_args._replace(input_filters=block_args.output_filters, stride=1)\n            for _ in range(block_args.num_repeat - 1):\n                self._blocks.append(MBConvBlock(block_args, self._global_params))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        for m in self.extras.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_uniform_(m.weight)\n                nn.init.zeros_(m.bias)\n\n    def extract_features(self, inputs):\n        """""" Returns output of the final convolution layer """"""\n\n        # Stem\n        x = relu_fn(self._bn0(self._conv_stem(inputs)))\n\n        features = []\n\n        # Blocks\n        for idx, block in enumerate(self._blocks):\n            drop_connect_rate = self._global_params.drop_connect_rate\n            if drop_connect_rate:\n                drop_connect_rate *= float(idx) / len(self._blocks)\n            x = block(x, drop_connect_rate)\n            if idx in self.indices:\n                features.append(x)\n\n        return x, features\n\n    def forward(self, inputs):\n        """""" Calls extract_features to extract features, applies final linear layer, and returns logits. """"""\n\n        # Convolution layers\n        x, features = self.extract_features(inputs)\n\n        for layer in self.extras:\n            x = layer(x)\n            features.append(x)\n\n        return tuple(features)\n\n    @classmethod\n    def from_name(cls, model_name, override_params=None):\n        cls._check_model_name_is_valid(model_name)\n        blocks_args, global_params = get_model_params(model_name, override_params)\n        return EfficientNet(model_name, blocks_args, global_params)\n\n    @classmethod\n    def from_pretrained(cls, model_name):\n        model = EfficientNet.from_name(model_name)\n        load_pretrained_weights(model, model_name)\n        return model\n\n    @classmethod\n    def get_image_size(cls, model_name):\n        cls._check_model_name_is_valid(model_name)\n        _, _, res, _ = efficientnet_params(model_name)\n        return res\n\n    @classmethod\n    def _check_model_name_is_valid(cls, model_name, also_need_pretrained_weights=False):\n        """""" Validates model name. None that pretrained weights are only available for\n        the first four models (efficientnet-b{i} for i in 0,1,2,3) at the moment. """"""\n        num_models = 4 if also_need_pretrained_weights else 8\n        valid_models = [\'efficientnet_b\' + str(i) for i in range(num_models)]\n        if model_name.replace(\'-\', \'_\') not in valid_models:\n            raise ValueError(\'model_name should be one of: \' + \', \'.join(valid_models))\n'"
ssd/modeling/backbone/efficient_net/utils.py,4,"b'""""""\nThis file contains helper functions for building the model and for loading model parameters.\nThese helper functions are built to mirror those in the official TensorFlow implementation.\n""""""\n\nimport re\nimport math\nimport collections\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom ssd.utils.model_zoo import load_state_dict_from_url\n\n########################################################################\n############### HELPERS FUNCTIONS FOR MODEL ARCHITECTURE ###############\n########################################################################\n\n\n# Parameters for the entire model (stem, all blocks, and head)\n\nGlobalParams = collections.namedtuple(\'GlobalParams\', [\n    \'batch_norm_momentum\', \'batch_norm_epsilon\', \'dropout_rate\',\n    \'num_classes\', \'width_coefficient\', \'depth_coefficient\',\n    \'depth_divisor\', \'min_depth\', \'drop_connect_rate\', ])\n\n# Parameters for an individual model block\nBlockArgs = collections.namedtuple(\'BlockArgs\', [\n    \'kernel_size\', \'num_repeat\', \'input_filters\', \'output_filters\',\n    \'expand_ratio\', \'id_skip\', \'stride\', \'se_ratio\'])\n\n# Change namedtuple defaults\nGlobalParams.__new__.__defaults__ = (None,) * len(GlobalParams._fields)\nBlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)\n\n\ndef relu_fn(x):\n    """""" Swish activation function """"""\n    return x * torch.sigmoid(x)\n\n\ndef round_filters(filters, global_params):\n    """""" Calculate and round number of filters based on depth multiplier. """"""\n    multiplier = global_params.width_coefficient\n    if not multiplier:\n        return filters\n    divisor = global_params.depth_divisor\n    min_depth = global_params.min_depth\n    filters *= multiplier\n    min_depth = min_depth or divisor\n    new_filters = max(min_depth, int(filters + divisor / 2) // divisor * divisor)\n    if new_filters < 0.9 * filters:  # prevent rounding by more than 10%\n        new_filters += divisor\n    return int(new_filters)\n\n\ndef round_repeats(repeats, global_params):\n    """""" Round number of filters based on depth multiplier. """"""\n    multiplier = global_params.depth_coefficient\n    if not multiplier:\n        return repeats\n    return int(math.ceil(multiplier * repeats))\n\n\ndef drop_connect(inputs, p, training):\n    """""" Drop connect. """"""\n    if not training: return inputs\n    batch_size = inputs.shape[0]\n    keep_prob = 1 - p\n    random_tensor = keep_prob\n    random_tensor += torch.rand([batch_size, 1, 1, 1], dtype=inputs.dtype, device=inputs.device)\n    binary_tensor = torch.floor(random_tensor)\n    output = inputs / keep_prob * binary_tensor\n    return output\n\n\nclass Conv2dSamePadding(nn.Conv2d):\n    """""" 2D Convolutions like TensorFlow """"""\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True):\n        super().__init__(in_channels, out_channels, kernel_size, stride, 0, dilation, groups, bias)\n        self.stride = self.stride if len(self.stride) == 2 else [self.stride[0]] * 2\n\n    def forward(self, x):\n        ih, iw = x.size()[-2:]\n        kh, kw = self.weight.size()[-2:]\n        sh, sw = self.stride\n        oh, ow = math.ceil(ih / sh), math.ceil(iw / sw)\n        pad_h = max((oh - 1) * self.stride[0] + (kh - 1) * self.dilation[0] + 1 - ih, 0)\n        pad_w = max((ow - 1) * self.stride[1] + (kw - 1) * self.dilation[1] + 1 - iw, 0)\n        if pad_h > 0 or pad_w > 0:\n            x = F.pad(x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2])\n        return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n\n\n########################################################################\n############## HELPERS FUNCTIONS FOR LOADING MODEL PARAMS ##############\n########################################################################\n\n\ndef efficientnet_params(model_name):\n    """""" Map EfficientNet model name to parameter coefficients. """"""\n    params_dict = {\n        # Coefficients:   width,depth,res,dropout\n        \'efficientnet-b0\': (1.0, 1.0, 224, 0.2),\n        \'efficientnet-b1\': (1.0, 1.1, 240, 0.2),\n        \'efficientnet-b2\': (1.1, 1.2, 260, 0.3),\n        \'efficientnet-b3\': (1.2, 1.4, 300, 0.3),\n        \'efficientnet-b4\': (1.4, 1.8, 380, 0.4),\n        \'efficientnet-b5\': (1.6, 2.2, 456, 0.4),\n        \'efficientnet-b6\': (1.8, 2.6, 528, 0.5),\n        \'efficientnet-b7\': (2.0, 3.1, 600, 0.5),\n    }\n    return params_dict[model_name]\n\n\nclass BlockDecoder(object):\n    """""" Block Decoder for readability, straight from the official TensorFlow repository """"""\n\n    @staticmethod\n    def _decode_block_string(block_string):\n        """""" Gets a block through a string notation of arguments. """"""\n        assert isinstance(block_string, str)\n\n        ops = block_string.split(\'_\')\n        options = {}\n        for op in ops:\n            splits = re.split(r\'(\\d.*)\', op)\n            if len(splits) >= 2:\n                key, value = splits[:2]\n                options[key] = value\n\n        # Check stride\n        assert ((\'s\' in options and len(options[\'s\']) == 1) or\n                (len(options[\'s\']) == 2 and options[\'s\'][0] == options[\'s\'][1]))\n\n        return BlockArgs(\n            kernel_size=int(options[\'k\']),\n            num_repeat=int(options[\'r\']),\n            input_filters=int(options[\'i\']),\n            output_filters=int(options[\'o\']),\n            expand_ratio=int(options[\'e\']),\n            id_skip=(\'noskip\' not in block_string),\n            se_ratio=float(options[\'se\']) if \'se\' in options else None,\n            stride=[int(options[\'s\'][0])])\n\n    @staticmethod\n    def _encode_block_string(block):\n        """"""Encodes a block to a string.""""""\n        args = [\n            \'r%d\' % block.num_repeat,\n            \'k%d\' % block.kernel_size,\n            \'s%d%d\' % (block.strides[0], block.strides[1]),\n            \'e%s\' % block.expand_ratio,\n            \'i%d\' % block.input_filters,\n            \'o%d\' % block.output_filters\n        ]\n        if 0 < block.se_ratio <= 1:\n            args.append(\'se%s\' % block.se_ratio)\n        if block.id_skip is False:\n            args.append(\'noskip\')\n        return \'_\'.join(args)\n\n    @staticmethod\n    def decode(string_list):\n        """"""\n        Decodes a list of string notations to specify blocks inside the network.\n\n        :param string_list: a list of strings, each string is a notation of block\n        :return: a list of BlockArgs namedtuples of block args\n        """"""\n        assert isinstance(string_list, list)\n        blocks_args = []\n        for block_string in string_list:\n            blocks_args.append(BlockDecoder._decode_block_string(block_string))\n        return blocks_args\n\n    @staticmethod\n    def encode(blocks_args):\n        """"""\n        Encodes a list of BlockArgs to a list of strings.\n\n        :param blocks_args: a list of BlockArgs namedtuples of block args\n        :return: a list of strings, each string is a notation of block\n        """"""\n        block_strings = []\n        for block in blocks_args:\n            block_strings.append(BlockDecoder._encode_block_string(block))\n        return block_strings\n\n\ndef efficientnet(width_coefficient=None, depth_coefficient=None,\n                 dropout_rate=0.2, drop_connect_rate=0.2):\n    """""" Creates a efficientnet model. """"""\n\n    blocks_args = [\n        \'r1_k3_s11_e1_i32_o16_se0.25\', \'r2_k3_s22_e6_i16_o24_se0.25\',\n        \'r2_k5_s22_e6_i24_o40_se0.25\', \'r3_k3_s22_e6_i40_o80_se0.25\',\n        \'r3_k5_s11_e6_i80_o112_se0.25\', \'r4_k5_s22_e6_i112_o192_se0.25\',\n        \'r1_k3_s11_e6_i192_o320_se0.25\',\n    ]\n    blocks_args = BlockDecoder.decode(blocks_args)\n\n    global_params = GlobalParams(\n        batch_norm_momentum=0.99,\n        batch_norm_epsilon=1e-3,\n        dropout_rate=dropout_rate,\n        drop_connect_rate=drop_connect_rate,\n        # data_format=\'channels_last\',  # removed, this is always true in PyTorch\n        num_classes=1000,\n        width_coefficient=width_coefficient,\n        depth_coefficient=depth_coefficient,\n        depth_divisor=8,\n        min_depth=None\n    )\n\n    return blocks_args, global_params\n\n\ndef get_model_params(model_name, override_params):\n    """""" Get the block args and global params for a given model """"""\n    if model_name.startswith(\'efficientnet\'):\n        w, d, _, p = efficientnet_params(model_name)\n        # note: all models have drop connect rate = 0.2\n        blocks_args, global_params = efficientnet(width_coefficient=w, depth_coefficient=d, dropout_rate=p)\n    else:\n        raise NotImplementedError(\'model name is not pre-defined: %s\' % model_name)\n    if override_params:\n        # ValueError will be raised here if override_params has fields not included in global_params.\n        global_params = global_params._replace(**override_params)\n    return blocks_args, global_params\n\n\nurl_map = {\n    \'efficientnet-b0\': \'http://storage.googleapis.com/public-models/efficientnet-b0-08094119.pth\',\n    \'efficientnet-b1\': \'http://storage.googleapis.com/public-models/efficientnet-b1-dbc7070a.pth\',\n    \'efficientnet-b2\': \'http://storage.googleapis.com/public-models/efficientnet-b2-27687264.pth\',\n    \'efficientnet-b3\': \'http://storage.googleapis.com/public-models/efficientnet-b3-c8376fa2.pth\',\n    \'efficientnet-b4\': \'http://storage.googleapis.com/public-models/efficientnet-b4-e116e8b3.pth\',\n    \'efficientnet-b5\': \'http://storage.googleapis.com/public-models/efficientnet-b5-586e6cc6.pth\',\n}\n\n\ndef load_pretrained_weights(model, model_name):\n    """""" Loads pretrained weights, and downloads if loading for the first time. """"""\n    state_dict = load_state_dict_from_url(url_map[model_name])\n    model.load_state_dict(state_dict, strict=False)\n    print(\'Loaded pretrained weights for {}\'.format(model_name))\n'"
ssd/data/datasets/evaluation/coco/__init__.py,0,"b'import json\nimport logging\nimport os\nfrom datetime import datetime\n\n\ndef coco_evaluation(dataset, predictions, output_dir, iteration=None):\n    coco_results = []\n    for i, prediction in enumerate(predictions):\n        img_info = dataset.get_img_info(i)\n        prediction = prediction.resize((img_info[\'width\'], img_info[\'height\'])).numpy()\n        boxes, labels, scores = prediction[\'boxes\'], prediction[\'labels\'], prediction[\'scores\']\n\n        image_id, annotation = dataset.get_annotation(i)\n        class_mapper = dataset.contiguous_id_to_coco_id\n        if labels.shape[0] == 0:\n            continue\n\n        boxes = boxes.tolist()\n        labels = labels.tolist()\n        scores = scores.tolist()\n        coco_results.extend(\n            [\n                {\n                    ""image_id"": image_id,\n                    ""category_id"": class_mapper[labels[k]],\n                    ""bbox"": [box[0], box[1], box[2] - box[0], box[3] - box[1]],  # to xywh format\n                    ""score"": scores[k],\n                }\n                for k, box in enumerate(boxes)\n            ]\n        )\n    iou_type = \'bbox\'\n    json_result_file = os.path.join(output_dir, iou_type + "".json"")\n    logger = logging.getLogger(""SSD.inference"")\n    logger.info(\'Writing results to {}...\'.format(json_result_file))\n    with open(json_result_file, ""w"") as f:\n        json.dump(coco_results, f)\n    from pycocotools.cocoeval import COCOeval\n    coco_gt = dataset.coco\n    coco_dt = coco_gt.loadRes(json_result_file)\n    coco_eval = COCOeval(coco_gt, coco_dt, iou_type)\n    coco_eval.evaluate()\n    coco_eval.accumulate()\n    coco_eval.summarize()\n\n    result_strings = []\n    keys = [""AP"", ""AP50"", ""AP75"", ""APs"", ""APm"", ""APl""]\n    metrics = {}\n    for i, key in enumerate(keys):\n        metrics[key] = coco_eval.stats[i]\n        logger.info(\'{:<10}: {}\'.format(key, round(coco_eval.stats[i], 3)))\n        result_strings.append(\'{:<10}: {}\'.format(key, round(coco_eval.stats[i], 3)))\n\n    if iteration is not None:\n        result_path = os.path.join(output_dir, \'result_{:07d}.txt\'.format(iteration))\n    else:\n        result_path = os.path.join(output_dir, \'result_{}.txt\'.format(datetime.now().strftime(\'%Y-%m-%d_%H-%M-%S\')))\n    with open(result_path, ""w"") as f:\n        f.write(\'\\n\'.join(result_strings))\n\n    return dict(metrics=metrics)\n'"
ssd/data/datasets/evaluation/voc/__init__.py,0,"b'import logging\nimport os\nfrom datetime import datetime\n\nimport numpy as np\n\nfrom .eval_detection_voc import eval_detection_voc\n\n\ndef voc_evaluation(dataset, predictions, output_dir, iteration=None):\n    class_names = dataset.class_names\n\n    pred_boxes_list = []\n    pred_labels_list = []\n    pred_scores_list = []\n    gt_boxes_list = []\n    gt_labels_list = []\n    gt_difficults = []\n\n    for i in range(len(dataset)):\n        image_id, annotation = dataset.get_annotation(i)\n        gt_boxes, gt_labels, is_difficult = annotation\n        gt_boxes_list.append(gt_boxes)\n        gt_labels_list.append(gt_labels)\n        gt_difficults.append(is_difficult.astype(np.bool))\n\n        img_info = dataset.get_img_info(i)\n        prediction = predictions[i]\n        prediction = prediction.resize((img_info[\'width\'], img_info[\'height\'])).numpy()\n        boxes, labels, scores = prediction[\'boxes\'], prediction[\'labels\'], prediction[\'scores\']\n\n        pred_boxes_list.append(boxes)\n        pred_labels_list.append(labels)\n        pred_scores_list.append(scores)\n    result = eval_detection_voc(pred_bboxes=pred_boxes_list,\n                                pred_labels=pred_labels_list,\n                                pred_scores=pred_scores_list,\n                                gt_bboxes=gt_boxes_list,\n                                gt_labels=gt_labels_list,\n                                gt_difficults=gt_difficults,\n                                iou_thresh=0.5,\n                                use_07_metric=True)\n    logger = logging.getLogger(""SSD.inference"")\n    result_str = ""mAP: {:.4f}\\n"".format(result[""map""])\n    metrics = {\'mAP\': result[""map""]}\n    for i, ap in enumerate(result[""ap""]):\n        if i == 0:  # skip background\n            continue\n        metrics[class_names[i]] = ap\n        result_str += ""{:<16}: {:.4f}\\n"".format(class_names[i], ap)\n    logger.info(result_str)\n\n    if iteration is not None:\n        result_path = os.path.join(output_dir, \'result_{:07d}.txt\'.format(iteration))\n    else:\n        result_path = os.path.join(output_dir, \'result_{}.txt\'.format(datetime.now().strftime(\'%Y-%m-%d_%H-%M-%S\')))\n    with open(result_path, ""w"") as f:\n        f.write(result_str)\n\n    return dict(metrics=metrics)\n'"
ssd/data/datasets/evaluation/voc/eval_detection_voc.py,0,"b'from __future__ import division\n\nfrom collections import defaultdict\nimport itertools\nimport numpy as np\nimport six\n\n\ndef bbox_iou(bbox_a, bbox_b):\n    """"""Calculate the Intersection of Unions (IoUs) between bounding boxes.\n    IoU is calculated as a ratio of area of the intersection\n    and area of the union.\n    This function accepts both :obj:`numpy.ndarray` and :obj:`cupy.ndarray` as\n    inputs. Please note that both :obj:`bbox_a` and :obj:`bbox_b` need to be\n    same type.\n    The output is same type as the type of the inputs.\n    Args:\n        bbox_a (array): An array whose shape is :math:`(N, 4)`.\n            :math:`N` is the number of bounding boxes.\n            The dtype should be :obj:`numpy.float32`.\n        bbox_b (array): An array similar to :obj:`bbox_a`,\n            whose shape is :math:`(K, 4)`.\n            The dtype should be :obj:`numpy.float32`.\n    Returns:\n        array:\n        An array whose shape is :math:`(N, K)`. \\\n        An element at index :math:`(n, k)` contains IoUs between \\\n        :math:`n` th bounding box in :obj:`bbox_a` and :math:`k` th bounding \\\n        box in :obj:`bbox_b`.\n    """"""\n    if bbox_a.shape[1] != 4 or bbox_b.shape[1] != 4:\n        raise IndexError\n\n    # top left\n    tl = np.maximum(bbox_a[:, None, :2], bbox_b[:, :2])\n    # bottom right\n    br = np.minimum(bbox_a[:, None, 2:], bbox_b[:, 2:])\n\n    area_i = np.prod(br - tl, axis=2) * (tl < br).all(axis=2)\n    area_a = np.prod(bbox_a[:, 2:] - bbox_a[:, :2], axis=1)\n    area_b = np.prod(bbox_b[:, 2:] - bbox_b[:, :2], axis=1)\n    return area_i / (area_a[:, None] + area_b - area_i)\n\n\ndef eval_detection_voc(\n        pred_bboxes,\n        pred_labels,\n        pred_scores,\n        gt_bboxes,\n        gt_labels,\n        gt_difficults=None,\n        iou_thresh=0.5,\n        use_07_metric=False):\n    """"""Calculate average precisions based on evaluation code of PASCAL VOC.\n\n    This function evaluates predicted bounding boxes obtained from a dataset\n    which has :math:`N` images by using average precision for each class.\n    The code is based on the evaluation code used in PASCAL VOC Challenge.\n\n    Args:\n        pred_bboxes (iterable of numpy.ndarray): An iterable of :math:`N`\n            sets of bounding boxes.\n            Its index corresponds to an index for the base dataset.\n            Each element of :obj:`pred_bboxes` is a set of coordinates\n            of bounding boxes. This is an array whose shape is :math:`(R, 4)`,\n            where :math:`R` corresponds\n            to the number of bounding boxes, which may vary among boxes.\n            The second axis corresponds to\n            :math:`y_{min}, x_{min}, y_{max}, x_{max}` of a bounding box.\n        pred_labels (iterable of numpy.ndarray): An iterable of labels.\n            Similar to :obj:`pred_bboxes`, its index corresponds to an\n            index for the base dataset. Its length is :math:`N`.\n        pred_scores (iterable of numpy.ndarray): An iterable of confidence\n            scores for predicted bounding boxes. Similar to :obj:`pred_bboxes`,\n            its index corresponds to an index for the base dataset.\n            Its length is :math:`N`.\n        gt_bboxes (iterable of numpy.ndarray): An iterable of ground truth\n            bounding boxes\n            whose length is :math:`N`. An element of :obj:`gt_bboxes` is a\n            bounding box whose shape is :math:`(R, 4)`. Note that the number of\n            bounding boxes in each image does not need to be same as the number\n            of corresponding predicted boxes.\n        gt_labels (iterable of numpy.ndarray): An iterable of ground truth\n            labels which are organized similarly to :obj:`gt_bboxes`.\n        gt_difficults (iterable of numpy.ndarray): An iterable of boolean\n            arrays which is organized similarly to :obj:`gt_bboxes`.\n            This tells whether the\n            corresponding ground truth bounding box is difficult or not.\n            By default, this is :obj:`None`. In that case, this function\n            considers all bounding boxes to be not difficult.\n        iou_thresh (float): A prediction is correct if its Intersection over\n            Union with the ground truth is above this value.\n        use_07_metric (bool): Whether to use PASCAL VOC 2007 evaluation metric\n            for calculating average precision. The default value is\n            :obj:`False`.\n\n    Returns:\n        dict:\n\n        The keys, value-types and the description of the values are listed\n        below.\n\n        * **ap** (*numpy.ndarray*): An array of average precisions. \\\n            The :math:`l`-th value corresponds to the average precision \\\n            for class :math:`l`. If class :math:`l` does not exist in \\\n            either :obj:`pred_labels` or :obj:`gt_labels`, the corresponding \\\n            value is set to :obj:`numpy.nan`.\n        * **map** (*float*): The average of Average Precisions over classes.\n\n    """"""\n\n    prec, rec = calc_detection_voc_prec_rec(pred_bboxes,\n                                            pred_labels,\n                                            pred_scores,\n                                            gt_bboxes,\n                                            gt_labels,\n                                            gt_difficults,\n                                            iou_thresh=iou_thresh)\n\n    ap = calc_detection_voc_ap(prec, rec, use_07_metric=use_07_metric)\n\n    return {\'ap\': ap, \'map\': np.nanmean(ap)}\n\n\ndef calc_detection_voc_prec_rec(\n        pred_bboxes, pred_labels, pred_scores, gt_bboxes, gt_labels,\n        gt_difficults=None,\n        iou_thresh=0.5):\n    """"""Calculate precision and recall based on evaluation code of PASCAL VOC.\n\n    This function calculates precision and recall of\n    predicted bounding boxes obtained from a dataset which has :math:`N`\n    images.\n    The code is based on the evaluation code used in PASCAL VOC Challenge.\n\n    Args:\n        pred_bboxes (iterable of numpy.ndarray): An iterable of :math:`N`\n            sets of bounding boxes.\n            Its index corresponds to an index for the base dataset.\n            Each element of :obj:`pred_bboxes` is a set of coordinates\n            of bounding boxes. This is an array whose shape is :math:`(R, 4)`,\n            where :math:`R` corresponds\n            to the number of bounding boxes, which may vary among boxes.\n            The second axis corresponds to\n            :math:`y_{min}, x_{min}, y_{max}, x_{max}` of a bounding box.\n        pred_labels (iterable of numpy.ndarray): An iterable of labels.\n            Similar to :obj:`pred_bboxes`, its index corresponds to an\n            index for the base dataset. Its length is :math:`N`.\n        pred_scores (iterable of numpy.ndarray): An iterable of confidence\n            scores for predicted bounding boxes. Similar to :obj:`pred_bboxes`,\n            its index corresponds to an index for the base dataset.\n            Its length is :math:`N`.\n        gt_bboxes (iterable of numpy.ndarray): An iterable of ground truth\n            bounding boxes\n            whose length is :math:`N`. An element of :obj:`gt_bboxes` is a\n            bounding box whose shape is :math:`(R, 4)`. Note that the number of\n            bounding boxes in each image does not need to be same as the number\n            of corresponding predicted boxes.\n        gt_labels (iterable of numpy.ndarray): An iterable of ground truth\n            labels which are organized similarly to :obj:`gt_bboxes`.\n        gt_difficults (iterable of numpy.ndarray): An iterable of boolean\n            arrays which is organized similarly to :obj:`gt_bboxes`.\n            This tells whether the\n            corresponding ground truth bounding box is difficult or not.\n            By default, this is :obj:`None`. In that case, this function\n            considers all bounding boxes to be not difficult.\n        iou_thresh (float): A prediction is correct if its Intersection over\n            Union with the ground truth is above this value..\n\n    Returns:\n        tuple of two lists:\n        This function returns two lists: :obj:`prec` and :obj:`rec`.\n\n        * :obj:`prec`: A list of arrays. :obj:`prec[l]` is precision \\\n            for class :math:`l`. If class :math:`l` does not exist in \\\n            either :obj:`pred_labels` or :obj:`gt_labels`, :obj:`prec[l]` is \\\n            set to :obj:`None`.\n        * :obj:`rec`: A list of arrays. :obj:`rec[l]` is recall \\\n            for class :math:`l`. If class :math:`l` that is not marked as \\\n            difficult does not exist in \\\n            :obj:`gt_labels`, :obj:`rec[l]` is \\\n            set to :obj:`None`.\n\n    """"""\n\n    pred_bboxes = iter(pred_bboxes)\n    pred_labels = iter(pred_labels)\n    pred_scores = iter(pred_scores)\n    gt_bboxes = iter(gt_bboxes)\n    gt_labels = iter(gt_labels)\n    if gt_difficults is None:\n        gt_difficults = itertools.repeat(None)\n    else:\n        gt_difficults = iter(gt_difficults)\n\n    n_pos = defaultdict(int)\n    score = defaultdict(list)\n    match = defaultdict(list)\n\n    for pred_bbox, pred_label, pred_score, gt_bbox, gt_label, gt_difficult in \\\n            six.moves.zip(\n                pred_bboxes, pred_labels, pred_scores,\n                gt_bboxes, gt_labels, gt_difficults):\n\n        if gt_difficult is None:\n            gt_difficult = np.zeros(gt_bbox.shape[0], dtype=bool)\n\n        for l in np.unique(np.concatenate((pred_label, gt_label)).astype(int)):\n            pred_mask_l = pred_label == l\n            pred_bbox_l = pred_bbox[pred_mask_l]\n            pred_score_l = pred_score[pred_mask_l]\n            # sort by score\n            order = pred_score_l.argsort()[::-1]\n            pred_bbox_l = pred_bbox_l[order]\n            pred_score_l = pred_score_l[order]\n\n            gt_mask_l = gt_label == l\n            gt_bbox_l = gt_bbox[gt_mask_l]\n            gt_difficult_l = gt_difficult[gt_mask_l]\n\n            n_pos[l] += np.logical_not(gt_difficult_l).sum()\n            score[l].extend(pred_score_l)\n\n            if len(pred_bbox_l) == 0:\n                continue\n            if len(gt_bbox_l) == 0:\n                match[l].extend((0,) * pred_bbox_l.shape[0])\n                continue\n\n            # VOC evaluation follows integer typed bounding boxes.\n            pred_bbox_l = pred_bbox_l.copy()\n            pred_bbox_l[:, 2:] += 1\n            gt_bbox_l = gt_bbox_l.copy()\n            gt_bbox_l[:, 2:] += 1\n\n            iou = bbox_iou(pred_bbox_l, gt_bbox_l)\n            gt_index = iou.argmax(axis=1)\n            # set -1 if there is no matching ground truth\n            gt_index[iou.max(axis=1) < iou_thresh] = -1\n            del iou\n\n            selec = np.zeros(gt_bbox_l.shape[0], dtype=bool)\n            for gt_idx in gt_index:\n                if gt_idx >= 0:\n                    if gt_difficult_l[gt_idx]:\n                        match[l].append(-1)\n                    else:\n                        if not selec[gt_idx]:\n                            match[l].append(1)\n                        else:\n                            match[l].append(0)\n                    selec[gt_idx] = True\n                else:\n                    match[l].append(0)\n\n    for iter_ in (\n            pred_bboxes, pred_labels, pred_scores,\n            gt_bboxes, gt_labels, gt_difficults):\n        if next(iter_, None) is not None:\n            raise ValueError(\'Length of input iterables need to be same.\')\n\n    n_fg_class = max(n_pos.keys()) + 1\n    prec = [None] * n_fg_class\n    rec = [None] * n_fg_class\n\n    for l in n_pos.keys():\n        score_l = np.array(score[l])\n        match_l = np.array(match[l], dtype=np.int8)\n\n        order = score_l.argsort()[::-1]\n        match_l = match_l[order]\n\n        tp = np.cumsum(match_l == 1)\n        fp = np.cumsum(match_l == 0)\n\n        # If an element of fp + tp is 0,\n        # the corresponding element of prec[l] is nan.\n        prec[l] = tp / (fp + tp)\n        # If n_pos[l] is 0, rec[l] is None.\n        if n_pos[l] > 0:\n            rec[l] = tp / n_pos[l]\n\n    return prec, rec\n\n\ndef calc_detection_voc_ap(prec, rec, use_07_metric=False):\n    """"""Calculate average precisions based on evaluation code of PASCAL VOC.\n\n    This function calculates average precisions\n    from given precisions and recalls.\n    The code is based on the evaluation code used in PASCAL VOC Challenge.\n\n    Args:\n        prec (list of numpy.array): A list of arrays.\n            :obj:`prec[l]` indicates precision for class :math:`l`.\n            If :obj:`prec[l]` is :obj:`None`, this function returns\n            :obj:`numpy.nan` for class :math:`l`.\n        rec (list of numpy.array): A list of arrays.\n            :obj:`rec[l]` indicates recall for class :math:`l`.\n            If :obj:`rec[l]` is :obj:`None`, this function returns\n            :obj:`numpy.nan` for class :math:`l`.\n        use_07_metric (bool): Whether to use PASCAL VOC 2007 evaluation metric\n            for calculating average precision. The default value is\n            :obj:`False`.\n\n    Returns:\n        ~numpy.ndarray:\n        This function returns an array of average precisions.\n        The :math:`l`-th value corresponds to the average precision\n        for class :math:`l`. If :obj:`prec[l]` or :obj:`rec[l]` is\n        :obj:`None`, the corresponding value is set to :obj:`numpy.nan`.\n\n    """"""\n\n    n_fg_class = len(prec)\n    ap = np.empty(n_fg_class)\n    for l in six.moves.range(n_fg_class):\n        if prec[l] is None or rec[l] is None:\n            ap[l] = np.nan\n            continue\n\n        if use_07_metric:\n            # 11 point metric\n            ap[l] = 0\n            for t in np.arange(0., 1.1, 0.1):\n                if np.sum(rec[l] >= t) == 0:\n                    p = 0\n                else:\n                    p = np.max(np.nan_to_num(prec[l])[rec[l] >= t])\n                ap[l] += p / 11\n        else:\n            # correct AP calculation\n            # first append sentinel values at the end\n            mpre = np.concatenate(([0], np.nan_to_num(prec[l]), [0]))\n            mrec = np.concatenate(([0], rec[l], [1]))\n\n            mpre = np.maximum.accumulate(mpre[::-1])[::-1]\n\n            # to calculate area under PR curve, look for points\n            # where X axis (recall) changes value\n            i = np.where(mrec[1:] != mrec[:-1])[0]\n\n            # and sum (\\Delta recall) * prec\n            ap[l] = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n\n    return ap\n'"
