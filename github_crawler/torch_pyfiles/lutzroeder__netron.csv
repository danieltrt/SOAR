file_path,api_count,code
setup.py,2,"b'#!/usr/bin/env python\n\nimport distutils\nimport io\nimport json\nimport os\nimport re\nimport setuptools\nimport setuptools.command.build_py\nimport distutils.command.build\n\nnode_dependencies = [ \n    ( \'netron\', [\n        \'node_modules/d3/dist/d3.min.js\',\n        \'node_modules/dagre/dist/dagre.min.js\',\n        \'node_modules/marked/marked.min.js\',\n        \'node_modules/pako/dist/pako.min.js\',\n        \'node_modules/long/dist/long.js\',\n        \'node_modules/protobufjs/dist/protobuf.min.js\',\n        \'node_modules/protobufjs/ext/prototxt/prototxt.js\',\n        \'node_modules/flatbuffers/js/flatbuffers.js\' ] )\n]\n\nclass build(distutils.command.build.build):\n    user_options = distutils.command.build.build.user_options + [ (\'version\', None, \'version\' ) ]\n    def initialize_options(self):\n        distutils.command.build.build.initialize_options(self)\n        self.version = None\n    def finalize_options(self):\n        distutils.command.build.build.finalize_options(self)\n    def run(self):\n        build_py.version = bool(self.version)\n        return distutils.command.build.build.run(self)\n\nclass build_py(setuptools.command.build_py.build_py):\n    user_options = setuptools.command.build_py.build_py.user_options + [ (\'version\', None, \'version\' ) ]\n    def initialize_options(self):\n        setuptools.command.build_py.build_py.initialize_options(self)\n        self.version = None\n    def finalize_options(self):\n        setuptools.command.build_py.build_py.finalize_options(self)\n    def run(self):\n        setuptools.command.build_py.build_py.run(self)\n        for target, files in node_dependencies:\n            target = os.path.join(self.build_lib, target)\n            if not os.path.exists(target):\n                os.makedirs(target)\n            for file in files:\n                self.copy_file(file, target)\n        if build_py.version:\n            for package, src_dir, build_dir, filenames in self.data_files:\n                for filename in filenames:\n                    if filename == \'index.html\':\n                        filepath = os.path.join(build_dir, filename)\n                        with open(filepath, \'r\') as file :\n                            content = file.read()\n                        content = re.sub(r\'(<meta name=""version"" content="")\\d+.\\d+.\\d+("">)\', r\'\\g<1>\' + package_version() + r\'\\g<2>\', content)\n                        with open(filepath, \'w\') as file:\n                            file.write(content)\n    def build_module(self, module, module_file, package):\n        setuptools.command.build_py.build_py.build_module(self, module, module_file, package)\n        if build_py.version and module == \'__version__\':\n            outfile = self.get_module_outfile(self.build_lib, package.split(\'.\'), module)\n            with open(outfile, \'w+\') as file:\n                file.write(""__version__ = \'"" + package_version() + ""\'\\n"")\n\ndef package_version():\n    folder = os.path.realpath(os.path.dirname(__file__))\n    with open(os.path.join(folder, \'package.json\')) as package_file:\n        package_manifest = json.load(package_file)\n        return package_manifest[\'version\']\n\nsetuptools.setup(\n    name=""netron"",\n    version=package_version(),\n    description=""Viewer for neural network, deep learning and machine learning models"",\n    long_description=\'Netron is a viewer for neural network, deep learning and machine learning models.\\n\\n\' +\n                     \'Netron supports **ONNX** (`.onnx`, `.pb`), **Keras** (`.h5`, `.keras`), **Core ML** (`.mlmodel`), **Caffe** (`.caffemodel`, `.prototxt`), **Caffe2** (`predict_net.pb`), **Darknet** (`.cfg`), **MXNet** (`.model`, `-symbol.json`), ncnn (`.param`) and **TensorFlow Lite** (`.tflite`). Netron has experimental support for **TorchScript** (`.pt`, `.pth`), **PyTorch** (`.pt`, `.pth`), **Torch** (`.t7`), **ArmNN** (`.armnn`), **Barracuda** (`.nn`), **BigDL** (`.bigdl`, `.model`), **Chainer** (`.npz`, `.h5`), **CNTK** (`.model`, `.cntk`), **Deeplearning4j** (`.zip`), **PaddlePaddle** (`__model__`), **MediaPipe** (`.pbtxt`), **ML.NET** (`.zip`), MNN (`.mnn`), **OpenVINO** (`.xml`), **scikit-learn** (`.pkl`), **Tengine** (`.tmfile`), **TensorFlow.js** (`model.json`, `.pb`) and **TensorFlow** (`.pb`, `.meta`, `.pbtxt`, `.ckpt`, `.index`).\',\n    keywords=[\n        \'onnx\', \'keras\', \'tensorflow\', \'tflite\', \'coreml\', \'mxnet\', \'caffe\', \'caffe2\', \'torchscript\', \'pytorch\', \'ncnn\', \'mnn\', \'openvino\', \'darknet\', \'paddlepaddle\', \'chainer\',\n        \'artificial intelligence\', \'machine learning\', \'deep learning\', \'neural network\',\n        \'visualizer\', \'viewer\'\n    ],\n    license=""MIT"",\n    cmdclass={\n        \'build\': build,\n        \'build_py\': build_py\n    },\n    package_dir={\n        \'netron\': \'src\'\n    },\n    packages=[\n        \'netron\'\n    ],\n    package_data={\n        \'netron\': [ \n            \'favicon.ico\', \'icon.png\',\n            \'base.js\', \n            \'numpy.js\', \'pickle.js\', \'hdf5.js\', \'bson.js\',\n            \'zip.js\', \'tar.js\', \'gzip.js\',\n            \'armnn.js\', \'armnn-metadata.json\', \'armnn-schema.js\',\n            \'bigdl.js\', \'bigdl-metadata.json\', \'bigdl-proto.js\',\n            \'barracuda.js\',\n            \'caffe.js\', \'caffe-metadata.json\', \'caffe-proto.js\',\n            \'caffe2.js\', \'caffe2-metadata.json\', \'caffe2-proto.js\',\n            \'chainer.js\',\n            \'cntk.js\', \'cntk-metadata.json\', \'cntk-proto.js\',\n            \'coreml.js\', \'coreml-metadata.json\', \'coreml-proto.js\',\n            \'darknet.js\', \'darknet-metadata.json\',\n            \'dl4j.js\', \'dl4j-metadata.json\',\n            \'flux.js\', \'flux-metadata.json\',\n            \'keras.js\', \'keras-metadata.json\',\n            \'mediapipe.js\',\n            \'mlnet.js\', \'mlnet-metadata.json\',\n            \'mnn.js\', \'mnn-metadata.json\', \'mnn-schema.js\',\n            \'mxnet.js\', \'mxnet-metadata.json\',\n            \'ncnn.js\', \'ncnn-metadata.json\',\n            \'onnx.js\', \'onnx-metadata.json\', \'onnx-proto.js\',\n            \'openvino.js\', \'openvino-metadata.json\', \'openvino-parser.js\',\n            \'paddle.js\', \'paddle-metadata.json\', \'paddle-proto.js\',\n            \'pytorch.js\', \'pytorch-metadata.json\', \'python.js\',\n            \'sklearn.js\', \'sklearn-metadata.json\',\n            \'tengine.js\', \'tengine-metadata.json\', \n            \'tensorrt.js\', \n            \'tf.js\', \'tf-metadata.json\', \'tf-proto.js\', \n            \'tflite.js\', \'tflite-metadata.json\', \'tflite-schema.js\',\n            \'torch.js\', \'torch-metadata.json\',\n            \'index.html\', \'index.js\',\n            \'view-grapher.css\', \'view-grapher.js\',\n            \'view-sidebar.css\', \'view-sidebar.js\',\n            \'view.js\',\n            \'server.py\'\n        ]\n    },\n    install_requires=[],\n    author=\'Lutz Roeder\',\n    author_email=\'lutzroeder@users.noreply.github.com\',\n    url=\'https://github.com/lutzroeder/netron\',\n    entry_points={\n        \'console_scripts\': [ \'netron = netron:main\' ]\n    },\n    classifiers=[\n        \'Intended Audience :: Developers\',\n        \'Intended Audience :: Education\',\n        \'Intended Audience :: Science/Research\',\n        \'Programming Language :: Python :: 2\',\n        \'Programming Language :: Python :: 2.7\',\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Topic :: Software Development\',\n        \'Topic :: Software Development :: Libraries\',\n        \'Topic :: Software Development :: Libraries :: Python Modules\',\n        \'Topic :: Scientific/Engineering\',\n        \'Topic :: Scientific/Engineering :: Mathematics\',\n        \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n        \'Topic :: Scientific/Engineering :: Visualization\'\n    ]\n)'"
src/__init__.py,0,"b'\nfrom .server import start\nfrom .server import stop\nfrom .server import wait\nfrom .server import serve\nfrom .__version__ import __version__\n\nimport argparse\nimport sys\nimport os\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Viewer for neural network, deep learning and machine learning models.\')\n    parser.add_argument(\'file\', metavar=\'MODEL_FILE\', help=\'model file to serve\', nargs=\'?\', default=None)\n    parser.add_argument(\'-v\', \'--version\', help=""print version"", action=\'store_true\')\n    parser.add_argument(\'-b\', \'--browse\', help=\'launch web browser\', action=\'store_true\')\n    parser.add_argument(\'-p\', \'--port\', help=\'port to serve (default: 8080)\', type=int, default=8080)\n    parser.add_argument(\'--host\', help=""host to serve (default: \'localhost\')"", default=\'localhost\')\n    parser.add_argument(\'--log\', help=\'log details to console\', action=\'store_true\')\n    args = parser.parse_args()\n    if args.file and not os.path.exists(args.file):\n        print(""Model file \'"" + args.file + ""\' does not exist."")\n        sys.exit(2)\n    if args.version:\n        print(__version__)\n        sys.exit(0)\n    serve(args.file, None, log=args.log, browse=args.browse, port=args.port, host=args.host)\n    wait()\n    sys.exit(0)\n\nif __name__ == \'__main__\':\n    main()\n'"
src/__version__.py,0,"b""__version__ = '0.0.0'"""
src/server.py,0,"b'\nimport codecs\nimport errno\nimport os\nimport platform\nimport re\nimport sys\nimport threading\nimport webbrowser\nimport time\nimport urllib.parse\n\nfrom .__version__ import __version__\n\nif sys.version_info[0] > 2:\n    from urllib.parse import urlparse\n    from http.server import HTTPServer\n    from http.server import BaseHTTPRequestHandler\n    from socketserver import ThreadingMixIn\nelse:\n    from urlparse import urlparse\n    from BaseHTTPServer import HTTPServer\n    from BaseHTTPServer import BaseHTTPRequestHandler\n    from SocketServer import ThreadingMixIn\n\nclass HTTPRequestHandler(BaseHTTPRequestHandler):\n    def handler(self):\n        if not hasattr(self, \'mime_types_map\'):\n            self.mime_types_map = {\n                \'.html\': \'text/html\',\n                \'.js\':   \'text/javascript\',\n                \'.css\':  \'text/css\',\n                \'.png\':  \'image/png\',\n                \'.gif\':  \'image/gif\',\n                \'.jpg\':  \'image/jpeg\',\n                \'.ico\':  \'image/x-icon\',\n                \'.json\': \'application/json\',\n                \'.pb\': \'application/octet-stream\',\n                \'.ttf\': \'font/truetype\',\n                \'.otf\': \'font/opentype\',\n                \'.eot\': \'application/vnd.ms-fontobject\',\n                \'.woff\': \'font/woff\',\n                \'.woff2\': \'application/font-woff2\',\n                \'.svg\': \'image/svg+xml\'\n            }\n        pathname = urlparse(self.path).path\n        folder = os.path.dirname(os.path.realpath(__file__))\n        location = folder + pathname\n        status_code = 0\n        headers = {}\n        buffer = None\n        data = \'/data/\'\n        if status_code == 0:\n            if pathname == \'/\':\n                meta = []\n                meta.append(\'<meta name=""type"" content=""Python"">\')\n                meta.append(\'<meta name=""version"" content=""\' + __version__ + \'"">\')\n                if self.file:\n                    meta.append(\'<meta name=""file"" content=""/data/\' + self.file + \'"">\')\n                with codecs.open(location + \'index.html\', mode=""r"", encoding=""utf-8"") as open_file:\n                    buffer = open_file.read()\n                buffer = re.sub(r\'<meta name=""version"" content=""\\d+.\\d+.\\d+"">\', \'\\n\'.join(meta), buffer)\n                buffer = buffer.encode(\'utf-8\')\n                headers[\'Content-Type\'] = \'text/html\'\n                headers[\'Content-Length\'] = len(buffer)\n                status_code = 200\n            elif pathname.startswith(data):\n                file = pathname[len(data):]\n                if file == self.file and self.data:\n                    buffer = self.data\n                else:\n                    file = self.folder + \'/\' + urllib.parse.unquote(file)\n                    status_code = 404\n                    if os.path.exists(file):\n                        with open(file, \'rb\') as binary:\n                            buffer = binary.read()\n                if buffer:\n                    headers[\'Content-Type\'] = \'application/octet-stream\'\n                    headers[\'Content-Length\'] = len(buffer)\n                    status_code = 200\n            else:\n                if os.path.exists(location) and not os.path.isdir(location):\n                    extension = os.path.splitext(location)[1]\n                    content_type = self.mime_types_map[extension]\n                    if content_type:\n                        with open(location, \'rb\') as binary:\n                            buffer = binary.read()\n                        headers[\'Content-Type\'] = content_type\n                        headers[\'Content-Length\'] = len(buffer)\n                        status_code = 200\n                else:\n                    status_code = 404\n        if self.log:\n            sys.stdout.write(str(status_code) + \' \' + self.command + \' \' + self.path + \'\\n\')\n        sys.stdout.flush()\n        self.send_response(status_code)\n        for key in headers:\n            self.send_header(key, headers[key])\n        self.end_headers()\n        if self.command != \'HEAD\':\n            if status_code == 404 and buffer is None:\n                self.wfile.write(bytes(status_code))\n            elif (status_code == 200 or status_code == 404) and buffer != None:\n                self.wfile.write(buffer)\n        return\n    def do_GET(self):\n        self.handler()\n    def do_HEAD(self):\n        self.handler()\n    def log_message(self, format, *args):\n        return\n\nclass ThreadedHTTPServer(ThreadingMixIn, HTTPServer): pass\n\nclass HTTPServerThread(threading.Thread):\n    def __init__(self, data, file, log, port, host, url):\n        threading.Thread.__init__(self)\n        self.port = port\n        self.host = host\n        self.file = file\n        self.url = url\n        self.server = ThreadedHTTPServer((host, port), HTTPRequestHandler)\n        self.server.timeout = 0.25\n        if file:\n            self.server.RequestHandlerClass.folder = os.path.dirname(file) if os.path.dirname(file) else \'.\'\n            self.server.RequestHandlerClass.file = os.path.basename(file)\n        else:\n            self.server.RequestHandlerClass.folder = \'\'\n            self.server.RequestHandlerClass.file = \'\'\n        self.server.RequestHandlerClass.data = data\n        self.server.RequestHandlerClass.log = log\n        self.terminate_event = threading.Event()\n        self.terminate_event.set()\n        self.stop_event = threading.Event()\n\n    def run(self):\n        self.stop_event.clear()\n        self.terminate_event.clear()\n        try:\n            while not self.stop_event.is_set():\n                self.server.handle_request()\n        except Exception:\n            pass\n        self.terminate_event.set()\n        self.stop_event.clear()\n\n    def stop(self):\n        if self.alive():\n            sys.stdout.write(""\\nStopping "" + self.url + ""\\n"")\n            self.stop_event.set()\n            self.server.server_close()\n            self.terminate_event.wait(1000)\n\n    def alive(self):\n        return not self.terminate_event.is_set()\n\nthread_list = []\n\ndef stop(port=8080, host=\'localhost\'):\n    \'\'\'Stop serving model at host:port.\n\n    Args:\n        port (int, optional): port to stop. Default: 8080\n        host (string, optional): host to stop. Default: \'\'\n    \'\'\'\n    global thread_list\n    for thread in thread_list:\n        if port == thread.port and host == thread.host:\n            thread.stop()\n    thread_list = [ thread for thread in thread_list if thread.alive() ]\n\ndef wait():\n    \'\'\'Wait for console exit and stop all model servers.\'\'\'\n    global thread_list\n    try:\n        while len(thread_list) > 0:\n            thread_list = [ thread for thread in thread_list if thread.alive() ]\n            time.sleep(1000)\n    except (KeyboardInterrupt, SystemExit):\n        for thread in thread_list:\n            thread.stop()\n        thread_list = [ thread for thread in thread_list if thread.alive() ]\n\ndef serve(file, data, log=False, browse=False, port=8080, host=\'localhost\'):\n    \'\'\'Start serving model from file or data buffer at host:port and open in web browser.\n    \n    Args:\n        file (string): Model file to serve. Required to detect format.\n        data (bytes): Model data to serve. None will load data from file.\n        log (bool, optional): Log details to console. Default: False\n        browse (bool, optional): Launch web browser, Default: True\n        port (int, optional): Port to serve. Default: 8080\n        host (string, optional): Host to serve. Default: \'\'\n    \'\'\'\n    global thread_list\n\n    if not data and file and not os.path.exists(file):\n        raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), file)\n\n    stop(port, host)\n\n    url = \'http://\' + host + \':\' + str(port)\n\n    thread_list = [ thread for thread in thread_list if thread.alive() ]\n    thread = HTTPServerThread(data, file, log, port, host, url)\n    thread.start()\n    while not thread.alive():\n        time.sleep(10)\n    thread_list.append(thread)\n\n    if file:\n        sys.stdout.write(""Serving \'"" + file + ""\' at "" + url + ""\\n"")\n    else:\n        sys.stdout.write(""Serving at "" + url + ""\\n"")\n    sys.stdout.flush()\n    if browse:\n        webbrowser.open(url)\n\ndef start(file=None, log=False, browse=True, port=8080, host=\'localhost\'):\n    \'\'\'Start serving model file at host:port and open in web browser\n    \n    Args:\n        file (string): Model file to serve.\n        log (bool, optional): Log details to console. Default: False\n        browse (bool, optional): Launch web browser, Default: True\n        port (int, optional): Port to serve. Default: 8080\n        host (string, optional): Host to serve. Default: \'\'\n    \'\'\'\n    serve(file, None, log=log, browse=browse, port=port, host=host)\n'"
tools/caffe2-script.py,0,"b'\nfrom __future__ import unicode_literals\nfrom __future__ import print_function\n\nimport io\nimport json\nimport logging\nimport pydoc\nimport os\nimport re\nimport sys\n\ndef get_support_level(dir):\n    dir = dir.replace(\'\\\\\', \'/\')\n    if \'caffe2/caffe2/operators\' in dir:\n        return \'core\'\n    if \'contrib\' in dir.split(\'/\'):\n        return \'contribution\'\n    if \'experiments\' in dir.split(\'/\'):\n        return \'experimental\'\n    return \'default\'\n\ndef update_argument_type(type):\n    if type == \'int\' or type == \'int64_t\':\n        return \'int64\'\n    if type == \'int32_t\':\n        return \'int32\'\n    elif type == \'[int]\' or type == \'int[]\':\n        return \'int64[]\'\n    elif type == \'float\':\n        return \'float32\'\n    elif type == \'string\':\n        return \'string\'\n    elif type == \'List(string)\':\n        return \'string[]\'\n    elif type == \'bool\':\n        return \'boolean\'\n    raise Exception(\'Unknown argument type \' + str(type))\n\ndef update_argument_default(value, type):\n    if type == \'int64\':\n        return int(value)\n    elif type == \'float32\':\n        return float(value.rstrip(\'~\'))\n    elif type == \'boolean\':\n        if value == \'True\':\n            return True\n        if value == \'False\':\n            return False\n    elif type == \'string\':\n        return value.strip(\'\\""\')\n    raise Exception(\'Unknown argument type \' + str(type))\n\ndef update_argument(schema, arg):\n    if not \'attributes\' in schema:\n        schema[\'attributes\'] = []\n    attribute = None\n    for current_attribute in schema[\'attributes\']:\n        if \'name\' in current_attribute and current_attribute[\'name\'] == arg.name:\n            attribute = current_attribute\n            break\n    if not attribute:\n        attribute = {}\n        attribute[\'name\'] = arg.name\n        schema[\'attributes\'].append(attribute)\n    description = arg.description.strip()\n    if description.startswith(\'*(\'):\n        index = description.find(\')*\')\n        properties = []\n        if index != -1:\n            properties = description[2:index].split(\';\')\n            description = description[index+2:].lstrip()\n        else:\n            index = description.index(\')\')\n            properties = description[2:index].split(\';\')\n            description = description[index+1:].lstrip()\n        if len(properties) == 1 and properties[0].find(\',\') != -1:\n            properties = properties[0].split(\',\')\n        for property in properties:\n            parts = property.split(\':\')\n            name = parts[0].strip()\n            if name == \'type\':\n                type = parts[1].strip()\n                if type == \'primitive\' or type == \'int | Tuple(int)\' or type == \'[]\' or type == \'TensorProto_DataType\' or type == \'Tuple(int)\':\n                    continue\n                attribute[\'type\'] = update_argument_type(type)\n            elif name == \'default\':\n                if \'type\' in attribute:\n                    type = attribute[\'type\']\n                    default = parts[1].strip()\n                    if default == \'2, possible values\':\n                        default = \'2\'\n                    if type == \'float32\' and default == \'\\\'NCHW\\\'\':\n                        continue\n                    if type == \'int64[]\':\n                        continue\n                    attribute[\'default\'] = update_argument_default(default, type)\n            elif name == \'optional\':\n                attribute[\'option\'] = \'optional\'\n            elif name == \'must be > 1.0\' or name == \'default=\\\'NCHW\\\'\' or name == \'type depends on dtype\' or name == \'Required=True\':\n                continue\n            elif name == \'List(string)\':\n                attribute[\'type\'] = \'string[]\'\n            else:\n                raise Exception(\'Unknown property \' + str(parts[0].strip()))\n    attribute[\'description\'] = description\n    if not arg.required:\n        attribute[\'option\'] = \'optional\'\n    return\n\ndef update_input(schema, input_desc):\n    input_name = input_desc[0]\n    description = input_desc[1]\n    if not \'inputs\' in schema:\n        schema[\'inputs\'] = []\n    input_arg = None\n    for current_input in schema[\'inputs\']:\n        if \'name\' in current_input and current_input[\'name\'] == input_name:\n            input_arg = current_input\n            break\n    if not input_arg:\n        input_arg = {}\n        input_arg[\'name\'] = input_name\n        schema[\'inputs\'].append(input_arg)\n    input_arg[\'description\'] = description\n    if len(input_desc) > 2:\n        return\n\ndef update_output(operator_name, schema, output_desc):\n    output_name = output_desc[0]\n    description = output_desc[1]\n    if not \'outputs\' in schema:\n        schema[\'outputs\'] = []\n    output_arg = None\n    for current_output in schema[\'outputs\']:\n        if \'name\' in current_output and current_output[\'name\'] == output_name:\n            output_arg = current_output\n            break\n    if not output_arg:\n        output_arg = {}\n        output_arg[\'name\'] = output_name\n        schema[\'outputs\'].append(output_arg)\n    output_arg[\'description\'] = description\n    if len(output_desc) > 2:\n        return\n\nclass Caffe2Filter(logging.Filter):\n    def filter(self, record):\n        return record.getMessage().startswith(\'WARNING:root:This caffe2 python run does not have GPU support.\')\n\ndef metadata():\n\n    logging.getLogger(\'\').addFilter(Caffe2Filter())\n\n    import caffe2.python.core\n\n    json_file = os.path.join(os.path.dirname(__file__), \'../src/caffe2-metadata.json\')\n    json_data = open(json_file).read()\n    json_root = json.loads(json_data)\n\n    schema_map = {}\n\n    for entry in json_root:\n        operator_name = entry[\'name\']\n        schema = entry[\'schema\']\n        schema_map[operator_name] = schema\n\n    for operator_name in caffe2.python.core._GetRegisteredOperators():\n        op_schema = caffe2.python.workspace.C.OpSchema.get(operator_name)\n        if op_schema:\n            if operator_name == \'Crash\':\n                continue\n            if operator_name in schema_map:\n                schema = schema_map[operator_name]\n            else:\n                schema = {}\n                entry = { \'name\': operator_name, \'schema\': schema }\n                schema_map[operator_name] = entry\n                json_root.append(entry)\n            schema[\'description\'] = op_schema.doc\n            for arg in op_schema.args:\n                update_argument(schema, arg)\n            for input_desc in op_schema.input_desc:\n                update_input(schema, input_desc)\n            for output_desc in op_schema.output_desc:\n                update_output(operator_name, schema, output_desc)\n            schema[\'support_level\'] = get_support_level(os.path.dirname(op_schema.file))\n\n    with io.open(json_file, \'w\', newline=\'\') as fout:\n        json_data = json.dumps(json_root, sort_keys=True, indent=2)\n        for line in json_data.splitlines():\n            line = line.rstrip()\n            if sys.version_info[0] < 3:\n                line = unicode(line)\n            fout.write(line)\n            fout.write(\'\\n\')\n\nif __name__ == \'__main__\':\n    command_table = { \'metadata\': metadata }\n    command = sys.argv[1];\n    command_table[command]()\n'"
tools/coreml-script.py,0,"b""#!/usr/bin/env python\n\nimport os\nimport sys\n\ndef convert():\n    file = sys.argv[2];\n    base, extension = os.path.splitext(file)\n    if extension == '.h5':\n        import coremltools\n        coreml_model = coremltools.converters.keras.convert(file)\n        coreml_model.save(base + '.mlmodel')\n    elif extension == '.pkl':\n        import coremltools\n        import sklearn\n        sklearn_model = sklearn.externals.joblib.load(file)\n        coreml_model = coremltools.converters.sklearn.convert(sklearn_model)\n        coreml_model.save(base + '.mlmodel')\n\nif __name__ == '__main__':\n    command_table = { 'convert': convert }\n    command = sys.argv[1];\n    command_table[command]()\n"""
tools/keras-script.py,0,"b""\nfrom __future__ import unicode_literals\nfrom __future__ import print_function\n\nimport io\nimport json\nimport os\nimport pydoc\nimport re\nimport sys\n\nstderr = sys.stderr\nsys.stderr = open(os.devnull, 'w')\nimport keras\nsys.stderr = stderr\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\ndef count_leading_spaces(s):\n    ws = re.search(r'\\S', s)\n    if ws:\n        return ws.start()\n    else:\n        return 0\n\ndef process_list_block(docstring, starting_point, leading_spaces, marker):\n    ending_point = docstring.find('\\n\\n', starting_point)\n    block = docstring[starting_point:(None if ending_point == -1 else\n                                      ending_point - 1)]\n    # Place marker for later reinjection.\n    docstring = docstring.replace(block, marker)\n    lines = block.split('\\n')\n    # Remove the computed number of leading white spaces from each line.\n    lines = [re.sub('^' + ' ' * leading_spaces, '', line) for line in lines]\n    # Usually lines have at least 4 additional leading spaces.\n    # These have to be removed, but first the list roots have to be detected.\n    top_level_regex = r'^    ([^\\s\\\\\\(]+):(.*)'\n    top_level_replacement = r'- __\\1__:\\2'\n    lines = [re.sub(top_level_regex, top_level_replacement, line) for line in lines]\n    # All the other lines get simply the 4 leading space (if present) removed\n    lines = [re.sub(r'^    ', '', line) for line in lines]\n    # Fix text lines after lists\n    indent = 0\n    text_block = False\n    for i in range(len(lines)):\n        line = lines[i]\n        spaces = re.search(r'\\S', line)\n        if spaces:\n            # If it is a list element\n            if line[spaces.start()] == '-':\n                indent = spaces.start() + 1\n                if text_block:\n                    text_block = False\n                    lines[i] = '\\n' + line\n            elif spaces.start() < indent:\n                text_block = True\n                indent = spaces.start()\n                lines[i] = '\\n' + line\n        else:\n            text_block = False\n            indent = 0\n    block = '\\n'.join(lines)\n    return docstring, block\n\ndef process_docstring(docstring):\n    # First, extract code blocks and process them.\n    code_blocks = []\n    if '```' in docstring:\n        tmp = docstring[:]\n        while '```' in tmp:\n            tmp = tmp[tmp.find('```'):]\n            index = tmp[3:].find('```') + 6\n            snippet = tmp[:index]\n            # Place marker in docstring for later reinjection.\n            docstring = docstring.replace(\n                snippet, '$CODE_BLOCK_%d' % len(code_blocks))\n            snippet_lines = snippet.split('\\n')\n            # Remove leading spaces.\n            num_leading_spaces = snippet_lines[-1].find('`')\n            snippet_lines = ([snippet_lines[0]] +\n                             [line[num_leading_spaces:]\n                             for line in snippet_lines[1:]])\n            # Most code snippets have 3 or 4 more leading spaces\n            # on inner lines, but not all. Remove them.\n            inner_lines = snippet_lines[1:-1]\n            leading_spaces = None\n            for line in inner_lines:\n                if not line or line[0] == '\\n':\n                    continue\n                spaces = count_leading_spaces(line)\n                if leading_spaces is None:\n                    leading_spaces = spaces\n                if spaces < leading_spaces:\n                    leading_spaces = spaces\n            if leading_spaces:\n                snippet_lines = ([snippet_lines[0]] +\n                                 [line[leading_spaces:]\n                                  for line in snippet_lines[1:-1]] +\n                                 [snippet_lines[-1]])\n            snippet = '\\n'.join(snippet_lines)\n            code_blocks.append(snippet)\n            tmp = tmp[index:]\n\n    # Format docstring lists.\n    section_regex = r'\\n( +)# (.*)\\n'\n    section_idx = re.search(section_regex, docstring)\n    shift = 0\n    sections = {}\n    while section_idx and section_idx.group(2):\n        anchor = section_idx.group(2)\n        leading_spaces = len(section_idx.group(1))\n        shift += section_idx.end()\n        marker = '$' + anchor.replace(' ', '_') + '$'\n        docstring, content = process_list_block(docstring,\n                                                shift,\n                                                leading_spaces,\n                                                marker)\n        sections[marker] = content\n        section_idx = re.search(section_regex, docstring[shift:])\n\n    # Format docstring section titles.\n    docstring = re.sub(r'\\n(\\s+)# (.*)\\n',\n                       r'\\n\\1__\\2__\\n\\n',\n                       docstring)\n\n    # Strip all remaining leading spaces.\n    lines = docstring.split('\\n')\n    docstring = '\\n'.join([line.lstrip(' ') for line in lines])\n\n    # Reinject list blocks.\n    for marker, content in sections.items():\n        docstring = docstring.replace(marker, content)\n\n    # Reinject code blocks.\n    for i, code_block in enumerate(code_blocks):\n        docstring = docstring.replace(\n            '$CODE_BLOCK_%d' % i, code_block)\n    return docstring\n\ndef split_docstring(docstring):\n    headers = {}\n    current_header = ''\n    current_lines = []\n    lines = docstring.split('\\n')\n    for line in lines:\n        if line.startswith('__') and line.endswith('__'):\n            headers[current_header] = current_lines\n            current_lines = []\n            current_header = line[2:-2]\n            if current_header == 'Masking' or current_header.startswith('Note '):\n                headline = '**' + current_header + '**'\n                current_lines = headers['']\n                current_header = ''\n                current_lines.append(headline)\n        else:\n            current_lines.append(line)\n    if len(current_lines) > 0:\n        headers[current_header] = current_lines\n    return headers\n\ndef update_hyperlink(description):\n    def replace_hyperlink(match):\n        name = match.group(1)\n        link = match.group(2)\n        if link.endswith('.md'):\n            if link.startswith('../'):\n                link = link.replace('../', 'https://keras.io/').rstrip('.md')\n            else:\n                link = 'https://keras.io/layers/' + link.rstrip('.md')\n            return '[' + name + '](' + link + ')'\n        return match.group(0)\n    return re.sub(r'\\[(.*?)\\]\\((.*?)\\)', replace_hyperlink, description)\n\ndef update_argument(schema, name, lines):\n    attribute = None\n    if not 'attributes' in schema:\n        schema['attributes'] = []\n    for current_attribute in schema['attributes']:\n        if 'name' in current_attribute and current_attribute['name'] == name:\n            attribute = current_attribute\n            break\n    if not attribute:\n        attribute = {}\n        attribute['name'] = name\n        schema['attributes'].append(attribute)\n    description = '\\n'.join(lines)\n    description = update_hyperlink(description)\n    attribute['description'] = description\n\ndef update_arguments(schema, lines):\n    argument_name = None\n    argument_lines = []\n    for line in lines:\n        if line.startswith('- __'):\n            line = line.lstrip('- ')\n            colon = line.index(':')\n            if colon > 0:\n                name = line[0:colon]\n                line = line[colon+1:].lstrip(' ')\n                if name.startswith('__') and name.endswith('__'):\n                    if argument_name:\n                        update_argument(schema, argument_name, argument_lines)\n                    argument_name = name[2:-2]\n                    argument_lines = []\n        if argument_name:\n            argument_lines.append(line)\n    if argument_name:\n        update_argument(schema, argument_name, argument_lines)\n    return\n\ndef update_examples(schema, lines):\n    if 'examples' in schema:\n        del schema['examples']\n    summary_lines = []\n    code_lines = None\n    for line in lines:\n        if line.startswith('```'):\n            if code_lines != None:\n                example = {}\n                example['code'] = '\\n'.join(code_lines)\n                if len(summary_lines) > 0:\n                    example['summary'] = '\\n'.join(summary_lines)\n                if not 'examples' in schema:\n                    schema['examples'] = []\n                schema['examples'].append(example)\n                summary_lines = []\n                code_lines = None\n            else:\n                code_lines = [ ]\n        else:\n            if code_lines != None:\n                code_lines.append(line)\n            elif line != '':\n                summary_lines.append(line)\n\ndef update_references(schema, lines):\n    if 'references' in schema:\n        del schema['references']\n    references = []\n    reference = ''\n    for line in lines:\n        if line.startswith('- '):\n            if len(reference) > 0:\n                references.append(reference)\n            reference = line.lstrip('- ')\n        else:\n            if line.startswith('  '):\n                line = line[2:]\n            reference = reference + line\n    if len(reference) > 0:\n        references.append(reference)\n    for reference in references:\n        if not 'references' in schema:\n            schema['references'] = []\n        schema['references'].append({ 'description': reference })\n\ndef update_input(schema, description):\n    entry = None\n    if 'inputs' in schema:\n        for current_input in schema['inputs']:\n            if current_input['name'] == 'input':\n                entry = current_input\n                break\n    else:\n        entry = {}\n        entry['name'] = 'input'\n        schema['inputs'] = []\n        schema['inputs'].append(entry)\n    if entry:\n        entry['description'] = description\n\ndef update_output(schema, description):\n    entry = None\n    if 'outputs' in schema:\n        for current_output in schema['outputs']:\n            if current_output['name'] == 'output':\n                entry = current_output\n                break\n    else:\n        entry = {}\n        entry['name'] = 'output'\n        schema['outputs'] = []\n        schema['outputs'].append(entry)\n    if entry:\n        entry['description'] = description\n\ndef metadata():\n    json_file = os.path.join(os.path.dirname(__file__), '../src/keras-metadata.json')\n    json_data = open(json_file).read()\n    json_root = json.loads(json_data)\n\n    for entry in json_root:\n        name = entry['name']\n        schema = entry['schema']\n        if 'package' in schema:\n            class_name = schema['package'] + '.' + name\n            class_definition = pydoc.locate(class_name)\n            if not class_definition:\n                raise Exception('\\'' + class_name + '\\' not found.')\n            docstring = class_definition.__doc__\n            if not docstring:\n                raise Exception('\\'' + class_name + '\\' missing __doc__.')\n            docstring = process_docstring(docstring)\n            headers = split_docstring(docstring)\n            if '' in headers:\n                schema['description'] = '\\n'.join(headers[''])\n                del headers['']\n            if 'Arguments' in headers:\n                update_arguments(schema, headers['Arguments'])\n                del headers['Arguments']\n            if 'Input shape' in headers:\n                update_input(schema, '\\n'.join(headers['Input shape']))\n                del headers['Input shape']\n            if 'Output shape' in headers:\n                update_output(schema, '\\n'.join(headers['Output shape']))\n                del headers['Output shape']\n            if 'Examples' in headers:\n                update_examples(schema, headers['Examples'])\n                del headers['Examples']\n            if 'Example' in headers:\n                update_examples(schema, headers['Example'])\n                del headers['Example']\n            if 'References' in headers:\n                update_references(schema, headers['References'])\n                del headers['References']\n            if 'Raises' in headers:\n                del headers['Raises']\n            if len(headers) > 0:\n                raise Exception('\\'' + class_name + '.__doc__\\' contains unprocessed headers.')\n\n    with io.open(json_file, 'w', newline='') as fout:\n        json_data = json.dumps(json_root, sort_keys=True, indent=2)\n        for line in json_data.splitlines():\n            line = line.rstrip()\n            if sys.version_info[0] < 3:\n                line = unicode(line)\n            fout.write(line)\n            fout.write('\\n')\n\ndef download_model(type, file):\n    file = os.path.expandvars(file)\n    if not os.path.exists(file):\n        folder = os.path.dirname(file);\n        if not os.path.exists(folder):\n            os.makedirs(folder)\n        model = pydoc.locate(type)()\n        model.save(file);\n\ndef zoo():\n    if not os.environ.get('test'):\n        os.environ['test'] = os.path.normpath(os.path.join(os.path.dirname(os.path.abspath(__file__)), '../test'))\n    download_model('keras.applications.densenet.DenseNet121', '${test}/data/keras/DenseNet121.h5')\n    download_model('keras.applications.inception_resnet_v2.InceptionResNetV2', '${test}/data/keras/InceptionResNetV2.h5')\n    download_model('keras.applications.inception_v3.InceptionV3', '${test}/data/keras/InceptionV3.h5')\n    download_model('keras.applications.mobilenet_v2.MobileNetV2', '${test}/data/keras/MobileNetV2.h5')\n    download_model('keras.applications.nasnet.NASNetMobile', '${test}/data/keras/NASNetMobile.h5')\n    download_model('keras.applications.resnet50.ResNet50', '${test}/data/keras/ResNet50.h5')\n    download_model('keras.applications.vgg19.VGG19', '${test}/data/keras/VGG19.h5')\n    download_model('keras.applications.xception.Xception', '${test}/data/keras/Xception.h5')\n\nif __name__ == '__main__':\n    command_table = { 'metadata': metadata, 'zoo': zoo }\n    command = sys.argv[1];\n    command_table[command]()\n"""
tools/mlnet-script.py,0,"b""\nfrom __future__ import unicode_literals\nfrom __future__ import print_function\n\nimport io\nimport json\nimport os\nimport re\nimport sys\n\ndef metadata():\n    json_file = os.path.join(os.path.dirname(__file__), '../src/mlnet-metadata.json')\n    json_data = open(json_file).read()\n    json_root = json.loads(json_data)\n    manifest_file = os.path.join(os.path.dirname(__file__), '../third_party/src/mlnet/test/BaselineOutput/Common/EntryPoints/core_manifest.json')\n    manifest_data = open(manifest_file).read()\n    manifest_root = json.loads(manifest_data)\n    schema_map = {}\n    # for manifest in manifest_root['EntryPoints']:\n    #     print(manifest['Name'])\n\nif __name__ == '__main__':\n    command_table = { 'metadata': metadata }\n    command = sys.argv[1];\n    command_table[command]()\n"""
tools/mxnet-script.py,0,"b""\nfrom __future__ import unicode_literals\nfrom __future__ import print_function\n\nimport io\nimport json\nimport pydoc\nimport re\nimport sys\n\njson_file = os.path.join(os.path.dirname(__file__), '../src/mxnet-metadata.json')\njson_data = open(json_file).read()\njson_root = json.loads(json_data)\n\nfor entry in json_root:\n    name = entry['name']\n    schema = entry['schema']\n    class_name = 'mxnet.symbol.' + name\n    class_definition = pydoc.locate(class_name)\n    if not class_definition:\n        print('NOT FOUND: ' + class_name)\n        # raise Exception('\\'' + class_name + '\\' not found.')\n    else:\n        docstring = class_definition.__doc__\n        if docstring:\n            schema['description'] = docstring\n    # if not docstring:\n        # print('NO DOCSTRING: ' + class_name)\n        # raise Exception('\\'' + class_name + '\\' missing __doc__.')\n    # print(docstring)\n \nwith io.open(json_file, 'w', newline='') as fout:\n    json_data = json.dumps(json_root, sort_keys=True, indent=2)\n    for line in json_data.splitlines():\n        line = line.rstrip()\n        if sys.version_info[0] < 3:\n            line = unicode(line)\n        fout.write(line)\n        fout.write('\\n')\n\n"""
tools/onnx-script.py,0,"b'\nfrom __future__ import unicode_literals\n\nimport onnx\n\nimport json\nimport io\nimport os\nimport re\nimport sys\n\nfrom onnx import defs\nfrom onnx.defs import OpSchema\nfrom onnx.backend.test.case import collect_snippets\n\nsnippets = collect_snippets()\n\ncategories = {\n    \'Constant\': \'Constant\',\n\n    \'Conv\': \'Layer\',\n    \'ConvTranspose\': \'Layer\',\n    \'FC\': \'Layer\',\n    \'RNN\': \'Layer\',\n    \'LSTM\': \'Layer\',\n    \'GRU\': \'Layer\',\n    \'Gemm\': \'Layer\',\n\n    \'Dropout\': \'Dropout\',\n\n    \'Elu\': \'Activation\',\n    \'HardSigmoid\': \'Activation\',\n    \'LeakyRelu\': \'Activation\',\n    \'PRelu\': \'Activation\',\n    \'ThresholdedRelu\': \'Activation\',\n    \'Relu\': \'Activation\',\n    \'Selu\': \'Activation\',\n    \'Sigmoid\': \'Activation\',\n    \'Tanh\': \'Activation\',\n    \'LogSoftmax\': \'Activation\',\n    \'Softmax\': \'Activation\',\n    \'Softplus\': \'Activation\',\n    \'Softsign\': \'Activation\',\n\n    \'BatchNormalization\': \'Normalization\',\n    \'InstanceNormalization\': \'Normalization\',\n    \'LpNormalization\': \'Normalization\',\n    \'LRN\': \'Normalization\',\n\n    \'Flatten\': \'Shape\',\n    \'Reshape\': \'Shape\',\n    \'Tile\': \'Shape\',\n\n    \'Xor\': \'Logic\',\n    \'Not\': \'Logic\',\n    \'Or\': \'Logic\',\n    \'Less\': \'Logic\',\n    \'And\': \'Logic\',\n    \'Greater\': \'Logic\',\n    \'Equal\': \'Logic\',\n\n    \'AveragePool\': \'Pool\',\n    \'GlobalAveragePool\': \'Pool\',\n    \'GlobalLpPool\': \'Pool\',\n    \'GlobalMaxPool\': \'Pool\',\n    \'LpPool\': \'Pool\',\n    \'MaxPool\': \'Pool\',\n    \'MaxRoiPool\': \'Pool\',\n\n    \'Concat\': \'Tensor\',\n    \'Slice\': \'Tensor\',\n    \'Split\': \'Tensor\',\n    \'Pad\': \'Tensor\',\n\n    \'ImageScaler\': \'Data\',\n    \'Crop\': \'Data\',\n    \'Upsample\': \'Data\',\n\n    \'Transpose\': \'Transform\',\n    \'Gather\': \'Transform\',\n    \'Unsqueeze\': \'Transform\',\n    \'Squeeze\': \'Transform\',\n}\n\nattribute_type_table = {\n    \'undefined\': None,\n    \'float\': \'float32\', \'int\': \'int64\', \'string\': \'string\', \'tensor\': \'tensor\', \'graph\': \'graph\',\n    \'floats\': \'float32[]\', \'ints\': \'int64[]\', \'strings\': \'string[]\', \'tensors\': \'tensor[]\', \'graphs\': \'graph[]\',\n}\n\ndef generate_json_attr_type(type):\n    assert isinstance(type, OpSchema.AttrType)\n    s = str(type)\n    s = s[s.rfind(\'.\')+1:].lower()\n    if s in attribute_type_table:\n        return attribute_type_table[s]\n    return None\n\ndef generate_json_attr_default_value(attr_value):\n    if not str(attr_value):\n        return None\n    if attr_value.HasField(\'i\'):\n        return attr_value.i\n    if attr_value.HasField(\'s\'):\n        return attr_value.s.decode(\'utf8\')\n    if attr_value.HasField(\'f\'):\n        return attr_value.f\n    return None\n\ndef generate_json_support_level_name(support_level):\n    assert isinstance(support_level, OpSchema.SupportType)\n    s = str(support_level)\n    return s[s.rfind(\'.\')+1:].lower()\n\ndef generate_json_types(types):\n    r = []\n    for type in types:\n        r.append(type)\n    r = sorted(r)\n    return r\n\ndef format_range(value):\n    if value == 2147483647:\n        return \'&#8734;\'\n    return str(value)\n\ndef format_description(description):\n    def replace_line(match):\n        link = match.group(1)\n        url = match.group(2)\n        if not url.startswith(""http://"") and not url.startswith(""https://""):\n            url = ""https://github.com/onnx/onnx/blob/master/docs/"" + url\n        return ""["" + link + ""]("" + url + "")"";\n    description = re.sub(""\\\\[(.+)\\\\]\\\\(([^ ]+?)( \\""(.+)\\"")?\\\\)"", replace_line, description)\n    return description\n\ndef generate_json(schemas, json_file):\n    json_root = []\n    for schema in schemas:\n        json_schema = {}\n        if schema.domain:\n            json_schema[\'domain\'] = schema.domain\n        else:\n            json_schema[\'domain\'] = \'ai.onnx\'\n        json_schema[\'since_version\'] = schema.since_version\n        json_schema[\'support_level\'] = generate_json_support_level_name(schema.support_level)\n        if schema.doc:\n            json_schema[\'description\'] = format_description(schema.doc.lstrip())\n        if schema.inputs:\n            json_schema[\'inputs\'] = []\n            for input in schema.inputs:\n                json_input = {}\n                json_input[\'name\'] = input.name\n                json_input[\'description\'] = format_description(input.description)\n                json_input[\'type\'] = input.typeStr\n                if input.option == OpSchema.FormalParameterOption.Optional:\n                    json_input[\'option\'] = \'optional\'\n                elif input.option == OpSchema.FormalParameterOption.Variadic:\n                    json_input[\'option\'] = \'variadic\'\n                json_schema[\'inputs\'].append(json_input)\n        json_schema[\'min_input\'] = schema.min_input\n        json_schema[\'max_input\'] = schema.max_input\n        if schema.outputs:\n            json_schema[\'outputs\'] = []\n            for output in schema.outputs:\n                json_output = {}\n                json_output[\'name\'] = output.name\n                json_output[\'description\'] = format_description(output.description)\n                json_output[\'type\'] = output.typeStr\n                if output.option == OpSchema.FormalParameterOption.Optional:\n                    json_output[\'option\'] = \'optional\'\n                elif output.option == OpSchema.FormalParameterOption.Variadic:\n                    json_output[\'option\'] = \'variadic\'\n                json_schema[\'outputs\'].append(json_output)\n        json_schema[\'min_output\'] = schema.min_output\n        json_schema[\'max_output\'] = schema.max_output\n        if schema.min_input != schema.max_input:\n            json_schema[\'inputs_range\'] = format_range(schema.min_input) + \' - \' + format_range(schema.max_input);\n        if schema.min_output != schema.max_output:\n            json_schema[\'outputs_range\'] = format_range(schema.min_output) + \' - \' + format_range(schema.max_output);\n        if schema.attributes:\n            json_schema[\'attributes\'] = []\n            for _, attribute in sorted(schema.attributes.items()):\n                json_attribute = {}\n                json_attribute[\'name\'] = attribute.name\n                json_attribute[\'description\'] = format_description(attribute.description)\n                attribute_type = generate_json_attr_type(attribute.type)\n                if attribute_type:\n                    json_attribute[\'type\'] = attribute_type\n                elif \'type\' in json_attribute:\n                    del json_attribute[\'type\']\n                json_attribute[\'required\'] = attribute.required\n                default_value = generate_json_attr_default_value(attribute.default_value)\n                if default_value:\n                    json_attribute[\'default\'] = default_value\n                json_schema[\'attributes\'].append(json_attribute)\n        if schema.type_constraints:\n            json_schema[\'type_constraints\'] = []\n            for type_constraint in schema.type_constraints:\n                json_schema[\'type_constraints\'].append({\n                    \'description\': type_constraint.description,\n                    \'type_param_str\': type_constraint.type_param_str,\n                    \'allowed_type_strs\': type_constraint.allowed_type_strs\n                })\n        if schema.name in snippets:\n            json_schema[\'examples\'] = []\n            for summary, code in sorted(snippets[schema.name]):\n                json_schema[\'examples\'].append({\n                    \'summary\': summary,\n                    \'code\': code\n                })\n        if schema.name in categories:\n            json_schema[\'category\'] = categories[schema.name]\n        json_root.append({\n            \'name\': schema.name,\n            \'schema\': json_schema \n        })\n    with io.open(json_file, \'w\', newline=\'\') as fout:\n        json_root = json.dumps(json_root, sort_keys=True, indent=2)\n        for line in json_root.splitlines():\n            line = line.rstrip()\n            if sys.version_info[0] < 3:\n                line = unicode(line)\n            fout.write(line)\n            fout.write(\'\\n\')\n\ndef metadata():\n    schemas = defs.get_all_schemas_with_history()\n    schemas = sorted(schemas, key=lambda schema: schema.name)\n    json_file = os.path.join(os.path.dirname(__file__), \'../src/onnx-metadata.json\')\n    generate_json(schemas, json_file)\n\ndef convert():\n    def pip_import(package):\n        import importlib\n        try:\n            importlib.import_module(package)\n        except:\n            import subprocess\n            subprocess.call([ \'pip\', \'install\', \'--quiet\', package ])\n        finally:\n            globals()[package] = importlib.import_module(package)\n    file = sys.argv[2]\n    base, extension = os.path.splitext(file)\n    if extension == \'.mlmodel\':\n        pip_import(\'coremltools\')\n        import onnxmltools\n        coreml_model = coremltools.utils.load_spec(file)\n        onnx_model = onnxmltools.convert.convert_coreml(coreml_model)\n        onnxmltools.utils.save_model(onnx_model, base + \'.onnx\')\n    elif extension == \'.h5\':\n        pip_import(\'tensorflow\')\n        pip_import(\'keras\')\n        import onnxmltools\n        keras_model = keras.models.load_model(file)\n        onnx_model = onnxmltools.convert.convert_keras(keras_model)\n        onnxmltools.utils.save_model(onnx_model, base + \'.onnx\')\n    elif extension == \'.pkl\':\n        pip_import(\'sklearn\')\n        import onnxmltools\n        sklearn_model = sklearn.externals.joblib.load(file)\n        onnx_model = onnxmltools.convert.convert_sklearn(sklearn_model)\n        onnxmltools.utils.save_model(onnx_model, base + \'.onnx\')\n    base, extension = os.path.splitext(file)\n    if extension == \'.onnx\':\n        import onnx\n        from google.protobuf import text_format\n        onnx_model = onnx.load(file)\n        text = text_format.MessageToString(onnx_model)\n        with open(base + \'.pbtxt\', \'w\') as text_file:\n            text_file.write(text)\n\ndef optimize():\n    import onnx\n    from onnx import optimizer\n    file = sys.argv[2]\n    base = os.path.splitext(file)\n    onnx_model = onnx.load(file)\n    passes = optimizer.get_available_passes()\n    optimized_model = optimizer.optimize(onnx_model, passes)\n    onnx.save(optimized_model, base + \'.optimized.onnx\')\n\ndef infer():\n    import onnx\n    import onnx.shape_inference\n    from onnx import shape_inference\n    file = sys.argv[2]\n    base = os.path.splitext(file)[0]\n    onnx_model = onnx.load(base + \'.onnx\')\n    onnx_model = onnx.shape_inference.infer_shapes(onnx_model)\n    onnx.save(onnx_model, base + \'.shape.onnx\')\n\nif __name__ == \'__main__\':\n    command_table = { \'metadata\': metadata, \'convert\': convert, \'optimize\': optimize, \'infer\': infer }\n    command = sys.argv[1]\n    command_table[command]()\n'"
tools/pytorch-script.py,5,"b""\nfrom __future__ import unicode_literals\nfrom __future__ import print_function\n\nimport io\nimport json\nimport pydoc\nimport os\nimport re\nimport sys\n\ndef metadata():\n    json_file = os.path.join(os.path.dirname(__file__), '../src/pytorch-metadata.json')\n    json_data = open(json_file).read()\n    json_root = json.loads(json_data)\n\n    schema_map = {}\n\n    for entry in json_root:\n        name = entry['name']\n        schema = entry['schema']\n        schema_map[name] = schema\n\n    for entry in json_root:\n        name = entry['name']\n        schema = entry['schema']\n        if 'package' in schema:\n            class_name = schema['package'] + '.' + name\n            # print(class_name)\n            class_definition = pydoc.locate(class_name)\n            if not class_definition:\n                raise Exception('\\'' + class_name + '\\' not found.')\n            docstring = class_definition.__doc__\n            if not docstring:\n                raise Exception('\\'' + class_name + '\\' missing __doc__.')\n            # print(docstring)\n\n    with io.open(json_file, 'w', newline='') as fout:\n        json_data = json.dumps(json_root, sort_keys=True, indent=2)\n        for line in json_data.splitlines():\n            line = line.rstrip()\n            if sys.version_info[0] < 3:\n                line = unicode(line)\n            fout.write(line)\n            fout.write('\\n')\n\ndef download_torchvision_model(name, input):\n    folder = os.path.expandvars('${test}/data/pytorch')\n    if not os.path.exists(folder):\n        os.makedirs(folder)\n    base = folder + '/' + name.split('.')[-1]\n    model = pydoc.locate(name)(pretrained=True)\n    import torch\n    torch.save(model, base + '.pkl.pth', _use_new_zipfile_serialization=False);\n    torch.save(model, base + '.zip.pth', _use_new_zipfile_serialization=True);\n    model.eval()\n    torch.jit.script(model).save(base + '.pt')\n    traced_model = torch.jit.trace(model, torch.rand(input))\n    torch.jit.save(traced_model, base + '_traced.pt')\n\ndef zoo():\n    if not os.environ.get('test'):\n        os.environ['test'] = os.path.normpath(os.path.join(os.path.dirname(os.path.abspath(__file__)), '../test'))\n    download_torchvision_model('torchvision.models.alexnet', [ 1, 3, 299, 299 ])\n    download_torchvision_model('torchvision.models.densenet161', [ 1, 3, 224, 224 ])\n    download_torchvision_model('torchvision.models.inception_v3', [ 1, 3, 299, 299 ])\n    download_torchvision_model('torchvision.models.mobilenet_v2', [ 1, 3, 224, 224 ])\n    # download_torchvision_model('torchvision.models.resnet18', [ 1, 3, 224, 224 ])\n    download_torchvision_model('torchvision.models.resnet101', [ 1, 3, 224, 224 ])\n    download_torchvision_model('torchvision.models.shufflenet_v2_x1_0', [ 1, 3, 224, 224 ])\n    download_torchvision_model('torchvision.models.squeezenet1_1', [ 1, 3, 224, 224 ])\n    download_torchvision_model('torchvision.models.video.r3d_18', [ 1, 3, 4, 112, 112 ])\n    # download_torchvision_model('torchvision.models.vgg11_bn', [ 1, 3, 224, 224 ])\n    # download_torchvision_model('torchvision.models.vgg16', [ 1, 3, 224, 224 ])\n\nif __name__ == '__main__':\n    command_table = { 'metadata': metadata, 'zoo': zoo }\n    command = sys.argv[1];\n    command_table[command]()\n"""
tools/sklearn-script.py,0,"b'\nfrom __future__ import unicode_literals\nfrom __future__ import print_function\n\nimport io\nimport json\nimport os\nimport pydoc\nimport re\nimport sys\n\njson_file = os.path.join(os.path.dirname(__file__), \'../src/sklearn-metadata.json\')\njson_data = open(json_file).read()\njson_root = json.loads(json_data)\n\ndef split_docstring(docstring):\n    headers = {}\n    current_header = \'\'\n    current_lines = []\n    lines = docstring.split(\'\\n\')\n    index = 0\n    while index < len(lines):\n        if index + 1 < len(lines) and len(lines[index + 1].strip(\' \')) > 0 and len(lines[index + 1].strip(\' \').strip(\'-\')) == 0:\n            headers[current_header] = current_lines\n            current_header = lines[index].strip(\' \')\n            current_lines = []\n            index = index + 1\n        else:\n            current_lines.append(lines[index])\n        index = index + 1\n    headers[current_header] = current_lines\n    return headers\n\ndef update_description(schema, lines):\n    if len(\'\'.join(lines).strip(\' \')) > 0:\n        for i in range(0, len(lines)):\n            lines[i] = lines[i].lstrip(\' \')\n        schema[\'description\'] = \'\\n\'.join(lines)\n\ndef update_attribute(schema, name, description, attribute_type, option, default_value):\n    attribute = None\n    if not \'attributes\' in schema:\n        schema[\'attributes\'] = []\n    for current_attribute in schema[\'attributes\']:\n        if \'name\' in current_attribute and current_attribute[\'name\'] == name:\n            attribute = current_attribute\n            break\n    if not attribute:\n        attribute = {}\n        attribute[\'name\'] = name\n        schema[\'attributes\'].append(attribute)\n    attribute[\'description\'] = description\n    if attribute_type:\n        attribute[\'type\'] = attribute_type\n    if option:\n        attribute[\'option\'] = option\n    if default_value:\n        if attribute_type == \'float32\':\n            if default_value == \'None\':\n                attribute[\'default\'] = None\n            elif default_value != ""\'auto\'"":\n                attribute[\'default\'] = float(default_value)\n            else:\n                attribute[\'default\'] = default_value.strip(""\'"").strip(\'""\')\n        elif attribute_type == \'int32\':\n            if default_value == \'None\':\n                attribute[\'default\'] = None\n            elif default_value == ""\'auto\'"" or default_value == \'""auto""\':\n                attribute[\'default\'] = default_value.strip(""\'"").strip(\'""\')\n            else:\n                attribute[\'default\'] = int(default_value)\n        elif attribute_type == \'string\':\n            attribute[\'default\'] = default_value.strip(""\'"").strip(\'""\')\n        elif attribute_type == \'boolean\':\n            if default_value == \'True\':\n                attribute[\'default\'] = True\n            elif default_value == \'False\':\n                attribute[\'default\'] = False\n            elif default_value == ""\'auto\'"":\n                attribute[\'default\'] = default_value.strip(""\'"").strip(\'""\')\n            else:\n                raise Exception(""Unknown boolean default value \'"" + str(default_value) + ""\'."")\n        else:\n            if attribute_type:\n                raise Exception(""Unknown default type \'"" + attribute_type + ""\'."")\n            else:\n                if default_value == \'None\':\n                    attribute[\'default\'] = None\n                else:\n                    attribute[\'default\'] = default_value.strip(""\'"")\n\ndef update_attributes(schema, lines):\n    index = 0\n    while index < len(lines):\n        line = lines[index]\n        if line.endswith(\'.\'):\n            line = line[0:-1]\n        colon = line.find(\':\')\n        if colon == -1:\n            raise Exception(""Expected \':\' in parameter."")\n        name = line[0:colon].strip(\' \')\n        line = line[colon + 1:].strip(\' \')\n        attribute_type = None\n        type_map = { \'float\': \'float32\', \'boolean\': \'boolean\', \'bool\': \'boolean\', \'string\': \'string\', \'int\': \'int32\', \'integer\': \'int32\' }\n        skip_map = {\n            ""\'sigmoid\' or \'isotonic\'"",\n            \'instance BaseEstimator\',\n            \'callable or None (default)\',\n            \'str or callable\',\n            ""string {\'english\'}, list, or None (default)"",\n            \'tuple (min_n, max_n)\',\n            ""string, {\'word\', \'char\', \'char_wb\'} or callable"",\n            ""{\'word\', \'char\'} or callable"",\n            ""string, {\'word\', \'char\'} or callable"",\n            \'int, float, None or string\',\n            ""int, float, None or str"",\n            ""int or None, optional (default=None)"",\n            ""\'l1\', \'l2\' or None, optional"",\n            ""{\'strict\', \'ignore\', \'replace\'} (default=\'strict\')"",\n            ""{\'ascii\', \'unicode\', None} (default=None)"",\n            ""string {\'english\'}, list, or None (default=None)"",\n            ""tuple (min_n, max_n) (default=(1, 1))"",\n            ""float in range [0.0, 1.0] or int (default=1.0)"",\n            ""float in range [0.0, 1.0] or int (default=1)"",\n            ""\'l1\', \'l2\' or None, optional (default=\'l2\')"",\n            ""{\'scale\', \'auto\'} or float, optional (default=\'scale\')"",\n            ""str {\'auto\', \'full\', \'arpack\', \'randomized\'}"",\n            ""str {\'filename\', \'file\', \'content\'}"",\n            ""str, {\'word\', \'char\', \'char_wb\'} or callable"",\n            ""str {\'english\'}, list, or None (default=None)"",\n            ""{\'scale\', \'auto\'} or float, optional (default=\'scale\')"",\n            ""{\'word\', \'char\', \'char_wb\'} or callable, default=\'word\'"",\n            ""{\'scale\', \'auto\'} or float, default=\'scale\'"",\n            ""{\'uniform\', \'distance\'} or callable, default=\'uniform\'"",\n            ""int, RandomState instance or None (default)"",\n            ""list of (string, transformer) tuples"",\n            ""list of tuples"",\n            ""{\'drop\', \'passthrough\'} or estimator, default=\'drop\'"",\n            ""\'auto\' or a list of array-like, default=\'auto\'"",\n            ""{\'first\', \'if_binary\'} or a array-like of shape (n_features,),             default=None"",\n            ""callable"",\n            ""int or \\""all\\"", optional, default=10"",\n            ""number, string, np.nan (default) or None"",\n            ""estimator object"",\n            ""dict or list of dictionaries"",\n            ""int, or str, default=n_jobs"",\n            ""\'raise\' or numeric, default=np.nan"",\n            ""\'auto\' or float, default=None""\n        }\n        if line == \'str\':\n            line = \'string\'\n        if line in skip_map:\n            line = \'\'\n        elif line.startswith(\'{\'):\n            if line.endswith(\'}\'):\n                line = \'\'\n            else:\n                end = line.find(\'},\')\n                if end == -1:\n                    raise Exception(""Expected \'}\' in parameter."")\n                # attribute_type = line[0:end + 1]\n                line = line[end + 2:].strip(\' \')\n        elif line.startswith(""\'""):\n            while line.startswith(""\'""):\n                end = line.find(""\',"")\n                if end == -1:\n                    raise Exception(""Expected \\\' in parameter."")\n                line = line[end + 2:].strip(\' \')\n        elif line in type_map:\n            attribute_type = line\n            line = \'\'\n        elif line.startswith(\'int, RandomState instance or None,\'):\n            line = line[len(\'int, RandomState instance or None,\'):]\n        elif line.find(\'|\') != -1:\n            line = \'\'\n        else:\n            space = line.find(\' {\')\n            if space != -1 and line[0:space] in type_map and line[space:].find(\'}\') != -1:\n                attribute_type = line[0:space]\n                end = line[space:].find(\'}\')\n                line = line[space+end+1:]\n            else:\n                comma = line.find(\',\')\n                if comma == -1:\n                    comma = line.find(\' (\')\n                    if comma == -1:\n                        raise Exception(""Expected \',\' in parameter."")\n                attribute_type = line[0:comma]\n                line = line[comma + 1:].strip(\' \')\n        if attribute_type in type_map:\n            attribute_type = type_map[attribute_type]\n        else:\n            attribute_type = None\n        # elif type == ""{dict, \'balanced\'}"":\n        #    v = \'map\'\n        # else:\n        #    raise Exception(""Unknown attribute type \'"" + attribute_type + ""\'."")\n        option = None\n        default = None\n        while len(line.strip(\' \')) > 0:\n            line = line.strip(\' \')\n            if line.startswith(\'optional \') or line.startswith(\'optional,\'):\n                option = \'optional\'\n                line = line[9:]\n            elif line.startswith(\'optional\'):\n                option = \'optional\'\n                line = \'\'\n            elif line.startswith(\'(\'):\n                close = line.index(\')\')\n                if (close == -1):\n                    raise Exception(""Expected \')\' in parameter."")\n                line = line[1:close]\n            elif line.endswith(\' by default\'):\n                default = line[0:-11]\n                line = \'\'\n            elif line.startswith(\'default =\') or line.startswith(\'default :\'):\n                default = line[9:].strip(\' \')\n                line = \'\'\n            elif line.startswith(\'default \') or line.startswith(\'default=\') or line.startswith(\'default:\'):\n                default = line[8:].strip(\' \')\n                line = \'\'\n            else:\n                comma = line.index(\',\')\n                if comma == -1:\n                    raise Exception(""Expected \',\' in parameter."")\n                line = line[comma+1:]\n        index = index + 1\n        attribute_lines = []\n        while index < len(lines) and (len(lines[index].strip(\' \')) == 0 or lines[index].startswith(\'        \')):\n            attribute_lines.append(lines[index].lstrip(\' \'))\n            index = index + 1\n        description = \'\\n\'.join(attribute_lines)\n        update_attribute(schema, name, description, attribute_type, option, default)\n\nfor entry in json_root:\n    name = entry[\'name\']\n    entry[\'schema\'] = entry[\'schema\'] if \'schema\' in entry else {};\n    schema = entry[\'schema\']\n    skip_modules = [\n        \'lightgbm.\',\n        \'sklearn.svm.classes\',\n        \'sklearn.ensemble.forest.\',\n        \'sklearn.ensemble.weight_boosting.\',\n        \'sklearn.neural_network.multilayer_perceptron.\',\n        \'sklearn.tree.tree.\'\n    ]\n    if not any(name.startswith(module) for module in skip_modules):\n        class_definition = pydoc.locate(name)\n        if not class_definition:\n            raise Exception(\'\\\'\' + name + \'\\\' not found.\')\n        docstring = class_definition.__doc__\n        if not docstring:\n            raise Exception(\'\\\'\' + name + \'\\\' missing __doc__.\')\n        headers = split_docstring(docstring)\n        if \'\' in headers:\n            update_description(schema, headers[\'\'])\n        if \'Parameters\' in headers:\n            update_attributes(schema, headers[\'Parameters\'])\n\nwith io.open(json_file, \'w\', newline=\'\') as fout:\n    json_data = json.dumps(json_root, sort_keys=True, indent=2)\n    for line in json_data.splitlines():\n        fout.write(line.rstrip())\n        fout.write(\'\\n\')\n'"
tools/tf-script.py,0,"b'\nfrom __future__ import unicode_literals\n\nimport json\nimport io\nimport sys\nimport os\n\nfrom tensorflow.core.framework import api_def_pb2\nfrom tensorflow.core.framework import op_def_pb2\nfrom tensorflow.core.framework import types_pb2\nfrom google.protobuf import text_format\n\ndef metadata():\n    categories = {\n        \'Assign\': \'Control\',\n        \'AvgPool\': \'Pool\',\n        \'BatchNormWithGlobalNormalization\': \'Normalization\',\n        \'BiasAdd\': \'Layer\',\n        \'ConcatV2\': \'Tensor\',\n        \'Const\': \'Constant\',\n        \'Conv2D\': \'Layer\',\n        \'DepthwiseConv2dNative\': \'Layer\',\n        \'Dequantize\': \'Tensor\',\n        \'Elu\': \'Activation\',\n        \'FusedBatchNorm\': \'Normalization\',\n        \'FusedBatchNormV2\': \'Normalization\',\n        \'FusedBatchNormV3\': \'Normalization\',\n        \'Identity\': \'Control\',\n        \'LeakyRelu\': \'Activation\',\n        \'LRN\': \'Normalization\',\n        \'MaxPool\': \'Pool\',\n        \'MaxPoolV2\': \'Pool\',\n        \'Pad\': \'Tensor\',\n        \'Relu\': \'Activation\',\n        \'Relu6\': \'Activation\',\n        \'Reshape\': \'Shape\',\n        \'Sigmoid\': \'Activation\',\n        \'Slice\': \'Tensor\',\n        \'Softmax\': \'Activation\',\n        \'Split\': \'Tensor\',\n        \'Squeeze\': \'Shape\',\n        \'StridedSlice\': \'Tensor\',\n        \'swish_f32\': \'Activation\',\n        \'Variable\': \'Control\',\n        \'VariableV2\': \'Control\',\n    };\n\n    def find_multiline(line, colon):\n        if colon == -1:\n            return None\n        line = line[colon+1:]\n        while line.startswith(\' \'):\n            line = line[1:]\n        if line.startswith(\'<<\'):\n            line = line[2:]\n            return line\n        return None\n\n    def str_escape(text):\n        result = \'\'\n        for c in text:\n            if (c == \'\\n\'):\n                result += \'\\\\n\'\n            elif (c == \'\\r\'):\n                result += ""\\\\r""\n            elif (c == \'\\t\'):\n                result += ""\\\\t""\n            elif (c == \'\\""\'):\n                result += ""\\\\\\""""\n            elif (c == \'\\\'\'):\n                result += ""\\\\\'""\n            elif (c == \'\\\\\'):\n                result += ""\\\\\\\\""\n            else:\n                result += c\n        return result\n\n    def pbtxt_from_multiline(multiline_pbtxt):\n        pbtxt = \'\'\n        while len(multiline_pbtxt) > 0:\n            index = multiline_pbtxt.find(\'\\n\')\n            if index == -1:\n                pbtxt = pbtxt + multiline_pbtxt\n                multiline_pbtxt = \'\'\n                break\n            line = multiline_pbtxt[0:index]\n            multiline_pbtxt = multiline_pbtxt[index+1:]\n            colon = line.find(\':\')\n            end = find_multiline(line, colon)\n            if end == None:\n                pbtxt = pbtxt + line + \'\\n\'\n                continue\n            pbtxt = pbtxt + line[0:colon+1]\n            unescaped = \'\'\n            newline = False\n            line = \'\'\n            while len(multiline_pbtxt) > 0:\n                index = multiline_pbtxt.find(\'\\n\')\n                line = multiline_pbtxt[0:index]\n                multiline_pbtxt = multiline_pbtxt[index+1:]\n                if line.startswith(end):\n                    line = line[len(end):]\n                    break\n                if newline:\n                    unescaped = unescaped + \'\\n\'\n                newline = True\n                unescaped = unescaped + line\n                line = \'\'\n            pbtxt = pbtxt + \'\\""\' + str_escape(unescaped) + \'\\""\' + line + \'\\n\'\n        return pbtxt\n\n    def read_api_def_map(folder):\n        api_def_map = {}\n        file_list = os.listdir(folder)\n        file_list = sorted(file_list)\n        for filename in file_list:\n            api_defs = api_def_pb2.ApiDefs()\n            filename = folder + \'/\' + filename\n            with open(filename) as handle:\n                multiline_pbtxt = handle.read()\n                pbtxt = pbtxt_from_multiline(multiline_pbtxt)\n                text_format.Merge(pbtxt, api_defs)\n            for api_def in api_defs.op:\n                api_def_map[api_def.graph_op_name] = api_def\n        return api_def_map\n\n    def convert_type(type):\n        return { \'type\': \'type\', \'value\': type }\n\n    def convert_tensor(tensor):\n        return { \'type\': \'tensor\', \'value\': \'?\' }\n\n    def convert_shape(shape):\n        return { \'type\': \'shape\', \'value\': \'?\' }\n\n    def convert_number(number):\n        if number == float(\'inf\'):\n            return \'NaN\'\n        if number == float(\'-inf\'):\n            return \'-NaN\'\n        return number\n\n    attr_type_table = {\n        \'type\': \'type\', \'list(type)\': \'type[]\',\n        \'bool\': \'boolean\',\n        \'int\': \'int64\', \'list(int)\': \'int64[]\',\n        \'float\': \'float32\', \'list(float)\': \'float32[]\',\n        \'string\': \'string\', \'list(string)\': \'string[]\',\n        \'shape\': \'shape\', \'list(shape)\': \'shape[]\',\n        \'tensor\': \'tensor\',\n        \'func\': \'function\', \'list(func)\': \'function[]\'\n    }\n\n    def convert_attr_type(type):\n        if type in attr_type_table:\n            return attr_type_table[type]\n        print(type)\n        return type\n\n    def convert_attr_value(attr_value):\n        if attr_value.HasField(\'list\'):\n            list = []\n            attr_value_list = attr_value.list\n            if len(attr_value_list.s) > 0:\n                for s in attr_value_list.s:\n                    list.append(s.decode(\'utf8\'))\n            if len(attr_value_list.i) > 0:\n                for i in attr_value_list.i:\n                    list.append(i)\n            if len(attr_value_list.f) > 0:\n                for f in attr_value_list.f:\n                    list.append(convert_number(f))\n            if len(attr_value_list.type) > 0:\n                for type in attr_value_list.type:\n                    list.append(convert_type(type))\n            if len(list) == 0:\n                for _, value in attr_value_list.ListFields():\n                    if len(value) > 0:\n                        raise Exception()\n            return list\n        if attr_value.HasField(\'s\'):\n            return attr_value.s.decode(\'utf8\')\n        if attr_value.HasField(\'i\'):\n            return attr_value.i\n        if attr_value.HasField(\'f\'):\n            return convert_number(attr_value.f)\n        if attr_value.HasField(\'b\'):\n            return attr_value.b\n        if attr_value.HasField(\'type\'):\n            return convert_type(attr_value.type)\n        if attr_value.HasField(\'tensor\'):\n            return convert_tensor(attr_value.tensor)\n        if attr_value.HasField(\'shape\'):\n            return convert_shape(attr_value.shape)\n        raise Exception()\n\n    _TYPE_TO_STRING = {\n        types_pb2.DataType.DT_HALF: ""float16"",\n        types_pb2.DataType.DT_FLOAT: ""float32"",\n        types_pb2.DataType.DT_DOUBLE: ""float64"",\n        types_pb2.DataType.DT_INT32: ""int32"",\n        types_pb2.DataType.DT_UINT8: ""uint8"",\n        types_pb2.DataType.DT_UINT16: ""uint16"",\n        types_pb2.DataType.DT_UINT32: ""uint32"",\n        types_pb2.DataType.DT_UINT64: ""uint64"",\n        types_pb2.DataType.DT_INT16: ""int16"",\n        types_pb2.DataType.DT_INT8: ""int8"",\n        types_pb2.DataType.DT_STRING: ""string"",\n        types_pb2.DataType.DT_COMPLEX64: ""complex64"",\n        types_pb2.DataType.DT_COMPLEX128: ""complex128"",\n        types_pb2.DataType.DT_INT64: ""int64"",\n        types_pb2.DataType.DT_BOOL: ""bool"",\n        types_pb2.DataType.DT_QINT8: ""qint8"",\n        types_pb2.DataType.DT_QUINT8: ""quint8"",\n        types_pb2.DataType.DT_QINT16: ""qint16"",\n        types_pb2.DataType.DT_QUINT16: ""quint16"",\n        types_pb2.DataType.DT_QINT32: ""qint32"",\n        types_pb2.DataType.DT_BFLOAT16: ""bfloat16"",\n        types_pb2.DataType.DT_RESOURCE: ""resource"",\n        types_pb2.DataType.DT_VARIANT: ""variant"",\n        types_pb2.DataType.DT_HALF_REF: ""float16_ref"",\n        types_pb2.DataType.DT_FLOAT_REF: ""float32_ref"",\n        types_pb2.DataType.DT_DOUBLE_REF: ""float64_ref"",\n        types_pb2.DataType.DT_INT32_REF: ""int32_ref"",\n        types_pb2.DataType.DT_UINT32_REF: ""uint32_ref"",\n        types_pb2.DataType.DT_UINT8_REF: ""uint8_ref"",\n        types_pb2.DataType.DT_UINT16_REF: ""uint16_ref"",\n        types_pb2.DataType.DT_INT16_REF: ""int16_ref"",\n        types_pb2.DataType.DT_INT8_REF: ""int8_ref"",\n        types_pb2.DataType.DT_STRING_REF: ""string_ref"",\n        types_pb2.DataType.DT_COMPLEX64_REF: ""complex64_ref"",\n        types_pb2.DataType.DT_COMPLEX128_REF: ""complex128_ref"",\n        types_pb2.DataType.DT_INT64_REF: ""int64_ref"",\n        types_pb2.DataType.DT_UINT64_REF: ""uint64_ref"",\n        types_pb2.DataType.DT_BOOL_REF: ""bool_ref"",\n        types_pb2.DataType.DT_QINT8_REF: ""qint8_ref"",\n        types_pb2.DataType.DT_QUINT8_REF: ""quint8_ref"",\n        types_pb2.DataType.DT_QINT16_REF: ""qint16_ref"",\n        types_pb2.DataType.DT_QUINT16_REF: ""quint16_ref"",\n        types_pb2.DataType.DT_QINT32_REF: ""qint32_ref"",\n        types_pb2.DataType.DT_BFLOAT16_REF: ""bfloat16_ref"",\n        types_pb2.DataType.DT_RESOURCE_REF: ""resource_ref"",\n        types_pb2.DataType.DT_VARIANT_REF: ""variant_ref"",\n    }\n\n    def format_data_type(data_type):\n        if data_type in _TYPE_TO_STRING:\n            return _TYPE_TO_STRING[data_type]\n        raise Exception()\n\n    def format_attribute_value(value):\n        if type(value) is dict and \'type\' in value and \'value\' in value and value[\'type\'] == \'type\':\n            return format_data_type(value[\'value\'])\n        if type(value) is str:\n            return value\n        if value == True:\n            return \'true\'\n        if value == False:\n            return \'false\'\n        raise Exception()\n\n    tensorflow_repo_dir = os.path.join(os.path.dirname(__file__), \'../third_party/src/tensorflow\')\n    api_def_map = read_api_def_map(os.path.join(tensorflow_repo_dir, \'tensorflow/core/api_def/base_api\'))\n    input_file = os.path.join(tensorflow_repo_dir, \'tensorflow/core/ops/ops.pbtxt\')\n    ops_list = op_def_pb2.OpList()\n    with open(input_file) as input_handle:\n        text_format.Merge(input_handle.read(), ops_list)\n\n    json_root = []\n\n    for op in ops_list.op:\n        # print(op.name)\n        json_schema = {}\n        if op.name in categories:\n            json_schema[\'category\'] = categories[op.name]\n        api_def = api_def_pb2.ApiDef()\n        if op.name in api_def_map:\n            api_def = api_def_map[op.name]\n        # if op.deprecation.version != 0:\n        #    print(\'[\' + op.name + \']\')\n        #    print(op.deprecation.version)\n        #    print(op.deprecation.explanation)\n        api_def_attr_map = {}\n        for attr in api_def.attr:\n            api_def_attr_map[attr.name] = attr\n        api_def_in_arg_map = {}\n        for in_arg in api_def.in_arg:\n            api_def_in_arg_map[in_arg.name] = in_arg\n        api_def_out_arg_map = {}\n        for out_arg in api_def.out_arg:\n            api_def_out_arg_map[out_arg.name] = out_arg\n        if api_def.summary:\n            json_schema[\'summary\'] = api_def.summary\n        if api_def.description:\n            json_schema[\'description\'] = api_def.description\n        for attr in op.attr:\n            if not \'attributes\' in json_schema:\n                json_schema[\'attributes\'] = []\n            json_attribute = {}\n            json_attribute[\'name\'] = attr.name\n            attr_type = convert_attr_type(attr.type)\n            if attr_type:\n                json_attribute[\'type\'] = attr_type\n            else:\n                del json_attribute[\'type\']\n            if attr.name in api_def_attr_map:\n                api_def_attr = api_def_attr_map[attr.name]\n                if api_def_attr.description:\n                    json_attribute[\'description\'] = api_def_attr.description\n            if attr.has_minimum:\n                json_attribute[\'minimum\'] = attr.minimum\n            if attr.HasField(\'allowed_values\'):\n                allowed_values = convert_attr_value(attr.allowed_values)\n                description = json_attribute[\'description\'] + \' \' if \'description\' in json_attribute else \'\'\n                description = description + \'Must be one of the following: \' + \', \'.join(list(map(lambda x: ""`"" + format_attribute_value(x) + ""`"", allowed_values))) + \'.\'\n                json_attribute[\'description\'] = description\n            if attr.HasField(\'default_value\'):\n                default_value = convert_attr_value(attr.default_value)\n                json_attribute[\'default\'] = default_value\n            json_schema[\'attributes\'].append(json_attribute)\n        for input_arg in op.input_arg:\n            if not \'inputs\' in json_schema:\n                json_schema[\'inputs\'] = []\n            json_input = {}\n            json_input[\'name\'] = input_arg.name\n            if input_arg.name in api_def_in_arg_map:\n                api_def_in_arg = api_def_in_arg_map[input_arg.name]\n                if api_def_in_arg.description:\n                    json_input[\'description\'] = api_def_in_arg.description\n            if input_arg.number_attr:\n                json_input[\'numberAttr\'] = input_arg.number_attr\n            if input_arg.type:\n                json_input[\'type\'] = input_arg.type\n            if input_arg.type_attr:\n                json_input[\'typeAttr\'] = input_arg.type_attr\n            if input_arg.type_list_attr:\n                json_input[\'typeListAttr\'] = input_arg.type_list_attr\n            if input_arg.is_ref:\n                json_input[\'isRef\'] = True\n            json_schema[\'inputs\'].append(json_input)\n        for output_arg in op.output_arg:\n            if not \'outputs\' in json_schema:\n                json_schema[\'outputs\'] = []\n            json_output = {}\n            json_output[\'name\'] = output_arg.name\n            if output_arg.name in api_def_out_arg_map:\n                api_def_out_arg = api_def_out_arg_map[output_arg.name]\n                if api_def_out_arg.description:\n                    json_output[\'description\'] = api_def_out_arg.description\n            if output_arg.number_attr:\n                json_output[\'numberAttr\'] = output_arg.number_attr\n            if output_arg.type:\n                json_output[\'type\'] = output_arg.type\n            elif output_arg.type_attr:\n                json_output[\'typeAttr\'] = output_arg.type_attr\n            elif output_arg.type_list_attr:\n                json_output[\'typeListAttr\'] = output_arg.type_list_attr\n            if output_arg.is_ref:\n                json_output[\'isRef\'] = True\n            json_schema[\'outputs\'].append(json_output)\n        json_root.append({\n            \'name\': op.name,\n            \'schema\': json_schema \n        })\n\n    json_file = os.path.join(os.path.dirname(__file__), \'../src/tf-metadata.json\')\n    with io.open(json_file, \'w\', newline=\'\') as fout:\n        json_data = json.dumps(json_root, sort_keys=True, indent=2)\n        for line in json_data.splitlines():\n            line = line.rstrip()\n            if sys.version_info[0] < 3:\n                line = unicode(line)\n            fout.write(line)\n            fout.write(\'\\n\')\n\nif __name__ == \'__main__\':\n    command_table = { \'metadata\': metadata }\n    command = sys.argv[1];\n    command_table[command]()'"
