file_path,api_count,code
build_readme.py,0,"b""def _read_readme():\n\n    with open('docs/readme.rst', 'r') as readme:\n        return readme.read()\n\n\ndef _substitute(readme):\n\n    readme = (readme.replace('_static', 'docs/_static')\n              .replace('.. testcode::', '.. code-block:: python')\n              .replace('.. testoutput::\\n   :hide:', ''))\n\n    return readme\n\n\ndef _write(readme):\n\n    with open('readme.rst', 'w') as out:\n        out.write(readme)\n\n\nif __name__ == '__main__':\n\n    readme = _read_readme()\n    readme = _substitute(readme)\n    _write(readme)\n"""
setup.py,0,"b""from setuptools import find_packages, setup\n\n\n# Import version\n__builtins__.__SPOTLIGHT_SETUP__ = True\nfrom spotlight import __version__ as version  # NOQA\n\n\nsetup(\n    name='spotlight',\n    version=version,\n    packages=find_packages(),\n    install_requires=['torch>=0.4.0'],\n    license='MIT',\n    classifiers=['Development Status :: 3 - Alpha',\n                 'License :: OSI Approved :: MIT License',\n                 'Topic :: Scientific/Engineering :: Artificial Intelligence'],\n)\n"""
docs/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Spotlight documentation build configuration file, created by\n# sphinx-quickstart on Thu Apr 21 12:26:52 2016.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport sys\nimport os\n\nimport spotlight\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\nsys.path.insert(0, os.path.abspath(\'..\'))\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.githubpages\',\n    \'sphinx.ext.napoleon\',\n    \'sphinx.ext.viewcode\',\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The encoding of source files.\n# source_encoding = \'utf-8-sig\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = u\'Spotlight\'\ncopyright = u\'2017, Maciej Kula\'\nauthor = u\'Maciej Kula\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\nversion = spotlight.__version__\n# The full version, including alpha/beta/rc tags.\nrelease = spotlight.__version__\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# There are two options for replacing |today|: either, you set today to some\n# non-false value, then it is used:\n#today = \'\'\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = \'%B %d, %Y\'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n#default_role = None\n\n# If true, \'()\' will be appended to :func: etc. cross-reference text.\n#add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# A list of ignored prefixes for module index sorting.\n#modindex_common_prefix = []\n\n# If true, keep warnings as ""system message"" paragraphs in the built documents.\n#keep_warnings = False\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n#html_theme_path = []\n\n# The name for this set of Sphinx documents.\n# ""<project> v<release> documentation"" by default.\n\n# A shorter title for the navigation bar.  Default is the same as html_title.\n#html_short_title = None\n\nhtml_theme_options = {\n    \'collapse_navigation\': False,\n    \'display_version\': True,\n    \'logo_only\': True,\n}\n\nhtml_context = {\n    \'css_files\': [\n        \'_static/css/spotlight_theme.css\'\n    ],\n}\n\n# The name of an image file (relative to this directory) to place at the top\n# of the sidebar.\nhtml_logo = \'_static/img/spotlight.svg\'\n\n# The name of an image file (relative to this directory) to use as a favicon of\n# the docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32\n# pixels large.\n#html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Add any extra paths that contain custom files (such as robots.txt or\n# .htaccess) here, relative to this directory. These files are copied\n# directly to the root of the documentation.\n#html_extra_path = []\n\n# If not None, a \'Last updated on:\' timestamp is inserted at every page\n# bottom, using the given strftime format.\n# The empty string is equivalent to \'%b %d, %Y\'.\n#html_last_updated_fmt = None\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names to\n# template names.\n#html_additional_pages = {}\n\n# If false, no module index is generated.\n#html_domain_indices = True\n\n# If false, no index is generated.\n#html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#html_show_sourcelink = True\n\n# If true, ""Created using Sphinx"" is shown in the HTML footer. Default is True.\n#html_show_sphinx = True\n\n# If true, ""(C) Copyright ..."" is shown in the HTML footer. Default is True.\n#html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages will\n# contain a <link> tag referring to it.  The value of this option must be the\n# base URL from which the finished HTML is served.\n#html_use_opensearch = \'\'\n\n# This is the file name suffix for HTML files (e.g. "".xhtml"").\n#html_file_suffix = None\n\n# Language to be used for generating the HTML full-text search index.\n# Sphinx supports the following languages:\n#   \'da\', \'de\', \'en\', \'es\', \'fi\', \'fr\', \'hu\', \'it\', \'ja\'\n#   \'nl\', \'no\', \'pt\', \'ro\', \'ru\', \'sv\', \'tr\', \'zh\'\n#html_search_language = \'en\'\n\n# A dictionary with options for the search language support, empty by default.\n# \'ja\' uses this config value.\n# \'zh\' user can custom change `jieba` dictionary path.\n#html_search_options = {\'type\': \'default\'}\n\n# The name of a javascript file (relative to the configuration directory) that\n# implements a search results scorer. If empty, the default will be used.\n#html_search_scorer = \'scorer.js\'\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'spotlightdoc\'\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n# The paper size (\'letterpaper\' or \'a4paper\').\n#\'papersize\': \'letterpaper\',\n\n# The font size (\'10pt\', \'11pt\' or \'12pt\').\n#\'pointsize\': \'10pt\',\n\n# Additional stuff for the LaTeX preamble.\n#\'preamble\': \'\',\n\n# Latex figure (float) alignment\n#\'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'spotlight.tex\', u\'Spotlight Documentation\',\n     u\'Maciej Kula\', \'manual\'),\n]\n\n# The name of an image file (relative to this directory) to place at the top of\n# the title page.\n#latex_logo = None\n\n# For ""manual"" documents, if this is true, then toplevel headings are parts,\n# not chapters.\n#latex_use_parts = False\n\n# If true, show page references after internal links.\n#latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#latex_appendices = []\n\n# If false, no module index is generated.\n#latex_domain_indices = True\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'spotlight\', u\'Spotlight Documentation\',\n     [author], 1)\n]\n\n# If true, show URL addresses after external links.\n#man_show_urls = False\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'Spotlight\', u\'Spotlight Documentation\',\n     author, \'Spotlight\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n# Documents to append as an appendix to all manuals.\n#texinfo_appendices = []\n\n# If false, no module index is generated.\n#texinfo_domain_indices = True\n\n# How to display URL addresses: \'footnote\', \'no\', or \'inline\'.\n#texinfo_show_urls = \'footnote\'\n\n# If true, do not generate a @detailmenu in the ""Top"" node\'s menu.\n#texinfo_no_detailmenu = False\n\n# Compact attribute lists\nnapoleon_use_ivar = True\n'"
spotlight/__init__.py,0,"b""__version__ = 'v0.1.6'\n"""
spotlight/cross_validation.py,0,"b'""""""\nModule with functionality for splitting and shuffling datasets.\n""""""\n\nimport numpy as np\n\nfrom sklearn.utils import murmurhash3_32\n\nfrom spotlight.interactions import Interactions\n\n\ndef _index_or_none(array, shuffle_index):\n\n    if array is None:\n        return None\n    else:\n        return array[shuffle_index]\n\n\ndef shuffle_interactions(interactions,\n                         random_state=None):\n    """"""\n    Shuffle interactions.\n\n    Parameters\n    ----------\n\n    interactions: :class:`spotlight.interactions.Interactions`\n        The interactions to shuffle.\n    random_state: np.random.RandomState, optional\n        The random state used for the shuffle.\n\n    Returns\n    -------\n\n    interactions: :class:`spotlight.interactions.Interactions`\n        The shuffled interactions.\n    """"""\n\n    if random_state is None:\n        random_state = np.random.RandomState()\n\n    shuffle_indices = np.arange(len(interactions.user_ids))\n    random_state.shuffle(shuffle_indices)\n\n    return Interactions(interactions.user_ids[shuffle_indices],\n                        interactions.item_ids[shuffle_indices],\n                        ratings=_index_or_none(interactions.ratings,\n                                               shuffle_indices),\n                        timestamps=_index_or_none(interactions.timestamps,\n                                                  shuffle_indices),\n                        weights=_index_or_none(interactions.weights,\n                                               shuffle_indices),\n                        num_users=interactions.num_users,\n                        num_items=interactions.num_items)\n\n\ndef random_train_test_split(interactions,\n                            test_percentage=0.2,\n                            random_state=None):\n    """"""\n    Randomly split interactions between training and testing.\n\n    Parameters\n    ----------\n\n    interactions: :class:`spotlight.interactions.Interactions`\n        The interactions to shuffle.\n    test_percentage: float, optional\n        The fraction of interactions to place in the test set.\n    random_state: np.random.RandomState, optional\n        The random state used for the shuffle.\n\n    Returns\n    -------\n\n    (train, test): (:class:`spotlight.interactions.Interactions`,\n                    :class:`spotlight.interactions.Interactions`)\n         A tuple of (train data, test data)\n    """"""\n\n    interactions = shuffle_interactions(interactions,\n                                        random_state=random_state)\n\n    cutoff = int((1.0 - test_percentage) * len(interactions))\n\n    train_idx = slice(None, cutoff)\n    test_idx = slice(cutoff, None)\n\n    train = Interactions(interactions.user_ids[train_idx],\n                         interactions.item_ids[train_idx],\n                         ratings=_index_or_none(interactions.ratings,\n                                                train_idx),\n                         timestamps=_index_or_none(interactions.timestamps,\n                                                   train_idx),\n                         weights=_index_or_none(interactions.weights,\n                                                train_idx),\n                         num_users=interactions.num_users,\n                         num_items=interactions.num_items)\n    test = Interactions(interactions.user_ids[test_idx],\n                        interactions.item_ids[test_idx],\n                        ratings=_index_or_none(interactions.ratings,\n                                               test_idx),\n                        timestamps=_index_or_none(interactions.timestamps,\n                                                  test_idx),\n                        weights=_index_or_none(interactions.weights,\n                                               test_idx),\n                        num_users=interactions.num_users,\n                        num_items=interactions.num_items)\n\n    return train, test\n\n\ndef user_based_train_test_split(interactions,\n                                test_percentage=0.2,\n                                random_state=None):\n    """"""\n    Split interactions between a train and a test set based on\n    user ids, so that a given user\'s entire interaction history\n    is either in the train, or the test set.\n\n    Parameters\n    ----------\n\n    interactions: :class:`spotlight.interactions.Interactions`\n        The interactions to shuffle.\n    test_percentage: float, optional\n        The fraction of users to place in the test set.\n    random_state: np.random.RandomState, optional\n        The random state used for the shuffle.\n\n    Returns\n    -------\n\n    (train, test): (:class:`spotlight.interactions.Interactions`,\n                    :class:`spotlight.interactions.Interactions`)\n         A tuple of (train data, test data)\n    """"""\n\n    if random_state is None:\n        random_state = np.random.RandomState()\n\n    minint = np.iinfo(np.uint32).min\n    maxint = np.iinfo(np.uint32).max\n\n    seed = random_state.randint(minint, maxint, dtype=np.int64)\n\n    in_test = ((murmurhash3_32(interactions.user_ids,\n                               seed=seed,\n                               positive=True) % 100 /\n                100.0) <\n               test_percentage)\n    in_train = np.logical_not(in_test)\n\n    train = Interactions(interactions.user_ids[in_train],\n                         interactions.item_ids[in_train],\n                         ratings=_index_or_none(interactions.ratings,\n                                                in_train),\n                         timestamps=_index_or_none(interactions.timestamps,\n                                                   in_train),\n                         weights=_index_or_none(interactions.weights,\n                                                in_train),\n                         num_users=interactions.num_users,\n                         num_items=interactions.num_items)\n    test = Interactions(interactions.user_ids[in_test],\n                        interactions.item_ids[in_test],\n                        ratings=_index_or_none(interactions.ratings,\n                                               in_test),\n                        timestamps=_index_or_none(interactions.timestamps,\n                                                  in_test),\n                        weights=_index_or_none(interactions.weights,\n                                               in_test),\n                        num_users=interactions.num_users,\n                        num_items=interactions.num_items)\n\n    return train, test\n'"
spotlight/evaluation.py,0,"b'import numpy as np\n\nimport scipy.stats as st\n\n\nFLOAT_MAX = np.finfo(np.float32).max\n\n\ndef mrr_score(model, test, train=None):\n    """"""\n    Compute mean reciprocal rank (MRR) scores. One score\n    is given for every user with interactions in the test\n    set, representing the mean reciprocal rank of all their\n    test items.\n\n    Parameters\n    ----------\n\n    model: fitted instance of a recommender model\n        The model to evaluate.\n    test: :class:`spotlight.interactions.Interactions`\n        Test interactions.\n    train: :class:`spotlight.interactions.Interactions`, optional\n        Train interactions. If supplied, scores of known\n        interactions will be set to very low values and so not\n        affect the MRR.\n\n    Returns\n    -------\n\n    mrr scores: numpy array of shape (num_users,)\n        Array of MRR scores for each user in test.\n    """"""\n\n    test = test.tocsr()\n\n    if train is not None:\n        train = train.tocsr()\n\n    mrrs = []\n\n    for user_id, row in enumerate(test):\n\n        if not len(row.indices):\n            continue\n\n        predictions = -model.predict(user_id)\n\n        if train is not None:\n            predictions[train[user_id].indices] = FLOAT_MAX\n\n        mrr = (1.0 / st.rankdata(predictions)[row.indices]).mean()\n\n        mrrs.append(mrr)\n\n    return np.array(mrrs)\n\n\ndef sequence_mrr_score(model, test, exclude_preceding=False):\n    """"""\n    Compute mean reciprocal rank (MRR) scores. Each sequence\n    in test is split into two parts: the first part, containing\n    all but the last elements, is used to predict the last element.\n\n    The reciprocal rank of the last element is returned for each\n    sequence.\n\n    Parameters\n    ----------\n\n    model: fitted instance of a recommender model\n        The model to evaluate.\n    test: :class:`spotlight.interactions.SequenceInteractions`\n        Test interactions.\n    exclude_preceding: boolean, optional\n        When true, items already present in the sequence will\n        be excluded from evaluation.\n\n    Returns\n    -------\n\n    mrr scores: numpy array of shape (num_users,)\n        Array of MRR scores for each sequence in test.\n    """"""\n\n    sequences = test.sequences[:, :-1]\n    targets = test.sequences[:, -1:]\n\n    mrrs = []\n\n    for i in range(len(sequences)):\n\n        predictions = -model.predict(sequences[i])\n\n        if exclude_preceding:\n            predictions[sequences[i]] = FLOAT_MAX\n\n        mrr = (1.0 / st.rankdata(predictions)[targets[i]]).mean()\n\n        mrrs.append(mrr)\n\n    return np.array(mrrs)\n\n\ndef sequence_precision_recall_score(model, test, k=10, exclude_preceding=False):\n    """"""\n    Compute sequence precision and recall scores. Each sequence\n    in test is split into two parts: the first part, containing\n    all but the last k elements, is used to predict the last k\n    elements.\n\n    Parameters\n    ----------\n\n    model: fitted instance of a recommender model\n        The model to evaluate.\n    test: :class:`spotlight.interactions.SequenceInteractions`\n        Test interactions.\n    exclude_preceding: boolean, optional\n        When true, items already present in the sequence will\n        be excluded from evaluation.\n\n    Returns\n    -------\n\n    mrr scores: numpy array of shape (num_users,)\n        Array of MRR scores for each sequence in test.\n    """"""\n    sequences = test.sequences[:, :-k]\n    targets = test.sequences[:, -k:]\n    precision_recalls = []\n    for i in range(len(sequences)):\n        predictions = -model.predict(sequences[i])\n        if exclude_preceding:\n            predictions[sequences[i]] = FLOAT_MAX\n\n        predictions = predictions.argsort()[:k]\n        precision_recall = _get_precision_recall(predictions, targets[i], k)\n        precision_recalls.append(precision_recall)\n\n    precision = np.array(precision_recalls)[:, 0]\n    recall = np.array(precision_recalls)[:, 1]\n    return precision, recall\n\n\ndef _get_precision_recall(predictions, targets, k):\n\n    predictions = predictions[:k]\n    num_hit = len(set(predictions).intersection(set(targets)))\n\n    return float(num_hit) / len(predictions), float(num_hit) / len(targets)\n\n\ndef precision_recall_score(model, test, train=None, k=10):\n    """"""\n    Compute Precision@k and Recall@k scores. One score\n    is given for every user with interactions in the test\n    set, representing the Precision@k and Recall@k of all their\n    test items.\n\n    Parameters\n    ----------\n\n    model: fitted instance of a recommender model\n        The model to evaluate.\n    test: :class:`spotlight.interactions.Interactions`\n        Test interactions.\n    train: :class:`spotlight.interactions.Interactions`, optional\n        Train interactions. If supplied, scores of known\n        interactions will not affect the computed metrics.\n    k: int or array of int,\n        The maximum number of predicted items\n    Returns\n    -------\n\n    (Precision@k, Recall@k): numpy array of shape (num_users, len(k))\n        A tuple of Precisions@k and Recalls@k for each user in test.\n        If k is a scalar, will return a tuple of vectors. If k is an\n        array, will return a tuple of arrays, where each row corresponds\n        to a user and each column corresponds to a value of k.\n    """"""\n\n    test = test.tocsr()\n\n    if train is not None:\n        train = train.tocsr()\n\n    if np.isscalar(k):\n        k = np.array([k])\n\n    precision = []\n    recall = []\n\n    for user_id, row in enumerate(test):\n\n        if not len(row.indices):\n            continue\n\n        predictions = -model.predict(user_id)\n\n        if train is not None:\n            rated = train[user_id].indices\n            predictions[rated] = FLOAT_MAX\n\n        predictions = predictions.argsort()\n\n        targets = row.indices\n\n        user_precision, user_recall = zip(*[\n            _get_precision_recall(predictions, targets, x)\n            for x in k\n        ])\n\n        precision.append(user_precision)\n        recall.append(user_recall)\n\n    precision = np.array(precision).squeeze()\n    recall = np.array(recall).squeeze()\n\n    return precision, recall\n\n\ndef rmse_score(model, test):\n    """"""\n    Compute RMSE score for test interactions.\n\n    Parameters\n    ----------\n\n    model: fitted instance of a recommender model\n        The model to evaluate.\n    test: :class:`spotlight.interactions.Interactions`\n        Test interactions.\n\n    Returns\n    -------\n\n    rmse_score: float\n        The RMSE score.\n    """"""\n\n    predictions = model.predict(test.user_ids, test.item_ids)\n\n    return np.sqrt(((test.ratings - predictions) ** 2).mean())\n'"
spotlight/helpers.py,0,"b""def _repr_model(model):\n\n    if model._net is None:\n        net_representation = '[uninitialised]'\n    else:\n        net_representation = repr(model._net)\n\n    return ('<{}: {}>'\n            .format(\n                model.__class__.__name__,\n                net_representation,\n            ))\n"""
spotlight/interactions.py,0,"b'""""""\nClasses describing datasets of user-item interactions. Instances of these\nare returned by dataset-fetching and dataset-processing functions.\n""""""\n\nimport numpy as np\n\nimport scipy.sparse as sp\n\n\ndef _sliding_window(tensor, window_size, step_size=1):\n\n    for i in range(len(tensor), 0, -step_size):\n        yield tensor[max(i - window_size, 0):i]\n\n\ndef _generate_sequences(user_ids, item_ids,\n                        indices,\n                        max_sequence_length,\n                        step_size):\n\n    for i in range(len(indices)):\n\n        start_idx = indices[i]\n\n        if i >= len(indices) - 1:\n            stop_idx = None\n        else:\n            stop_idx = indices[i + 1]\n\n        for seq in _sliding_window(item_ids[start_idx:stop_idx],\n                                   max_sequence_length,\n                                   step_size):\n\n            yield (user_ids[i], seq)\n\n\nclass Interactions(object):\n    """"""\n    Interactions object. Contains (at a minimum) pair of user-item\n    interactions, but can also be enriched with ratings, timestamps,\n    and interaction weights.\n\n    For *implicit feedback* scenarios, user ids and item ids should\n    only be provided for user-item pairs where an interaction was\n    observed. All pairs that are not provided are treated as missing\n    observations, and often interpreted as (implicit) negative\n    signals.\n\n    For *explicit feedback* scenarios, user ids, item ids, and\n    ratings should be provided for all user-item-rating triplets\n    that were observed in the dataset.\n\n    Parameters\n    ----------\n\n    user_ids: array of np.int32\n        array of user ids of the user-item pairs\n    item_ids: array of np.int32\n        array of item ids of the user-item pairs\n    ratings: array of np.float32, optional\n        array of ratings\n    timestamps: array of np.int32, optional\n        array of timestamps\n    weights: array of np.float32, optional\n        array of weights\n    num_users: int, optional\n        Number of distinct users in the dataset.\n        Must be larger than the maximum user id\n        in user_ids.\n    num_items: int, optional\n        Number of distinct items in the dataset.\n        Must be larger than the maximum item id\n        in item_ids.\n\n    Attributes\n    ----------\n\n    user_ids: array of np.int32\n        array of user ids of the user-item pairs\n    item_ids: array of np.int32\n        array of item ids of the user-item pairs\n    ratings: array of np.float32, optional\n        array of ratings\n    timestamps: array of np.int32, optional\n        array of timestamps\n    weights: array of np.float32, optional\n        array of weights\n    num_users: int, optional\n        Number of distinct users in the dataset.\n    num_items: int, optional\n        Number of distinct items in the dataset.\n    """"""\n\n    def __init__(self, user_ids, item_ids,\n                 ratings=None,\n                 timestamps=None,\n                 weights=None,\n                 num_users=None,\n                 num_items=None):\n\n        self.num_users = num_users or int(user_ids.max() + 1)\n        self.num_items = num_items or int(item_ids.max() + 1)\n\n        self.user_ids = user_ids\n        self.item_ids = item_ids\n        self.ratings = ratings\n        self.timestamps = timestamps\n        self.weights = weights\n\n        self._check()\n\n    def __repr__(self):\n\n        return (\'<Interactions dataset ({num_users} users x {num_items} items \'\n                \'x {num_interactions} interactions)>\'\n                .format(\n                    num_users=self.num_users,\n                    num_items=self.num_items,\n                    num_interactions=len(self)\n                ))\n\n    def __len__(self):\n\n        return len(self.user_ids)\n\n    def _check(self):\n\n        if self.user_ids.max() >= self.num_users:\n            raise ValueError(\'Maximum user id greater \'\n                             \'than declared number of users.\')\n        if self.item_ids.max() >= self.num_items:\n            raise ValueError(\'Maximum item id greater \'\n                             \'than declared number of items.\')\n\n        num_interactions = len(self.user_ids)\n\n        for name, value in ((\'item IDs\', self.item_ids),\n                            (\'ratings\', self.ratings),\n                            (\'timestamps\', self.timestamps),\n                            (\'weights\', self.weights)):\n\n            if value is None:\n                continue\n\n            if len(value) != num_interactions:\n                raise ValueError(\'Invalid {} dimensions: length \'\n                                 \'must be equal to number of interactions\'\n                                 .format(name))\n\n    def tocoo(self):\n        """"""\n        Transform to a scipy.sparse COO matrix.\n        """"""\n\n        row = self.user_ids\n        col = self.item_ids\n        data = self.ratings if self.ratings is not None else np.ones(len(self))\n\n        return sp.coo_matrix((data, (row, col)),\n                             shape=(self.num_users, self.num_items))\n\n    def tocsr(self):\n        """"""\n        Transform to a scipy.sparse CSR matrix.\n        """"""\n\n        return self.tocoo().tocsr()\n\n    def to_sequence(self, max_sequence_length=10, min_sequence_length=None, step_size=None):\n        """"""\n        Transform to sequence form.\n\n        User-item interaction pairs are sorted by their timestamps,\n        and sequences of up to max_sequence_length events are arranged\n        into a (zero-padded from the left) matrix with dimensions\n        (num_sequences x max_sequence_length).\n\n        Valid subsequences of users\' interactions are returned. For\n        example, if a user interacted with items [1, 2, 3, 4, 5], the\n        returned interactions matrix at sequence length 5 and step size\n        1 will be be given by:\n\n        .. code-block:: python\n\n           [[1, 2, 3, 4, 5],\n            [0, 1, 2, 3, 4],\n            [0, 0, 1, 2, 3],\n            [0, 0, 0, 1, 2],\n            [0, 0, 0, 0, 1]]\n\n        At step size 2:\n\n        .. code-block:: python\n\n           [[1, 2, 3, 4, 5],\n            [0, 0, 1, 2, 3],\n            [0, 0, 0, 0, 1]]\n\n        Parameters\n        ----------\n\n        max_sequence_length: int, optional\n            Maximum sequence length. Subsequences shorter than this\n            will be left-padded with zeros.\n        min_sequence_length: int, optional\n            If set, only sequences with at least min_sequence_length\n            non-padding elements will be returned.\n        step-size: int, optional\n            The returned subsequences are the effect of moving a\n            a sliding window over the input. This parameter\n            governs the stride of that window. Increasing it will\n            result in fewer subsequences being returned.\n\n        Returns\n        -------\n\n        sequence interactions: :class:`~SequenceInteractions`\n            The resulting sequence interactions.\n        """"""\n\n        if self.timestamps is None:\n            raise ValueError(\'Cannot convert to sequences, \'\n                             \'timestamps not available.\')\n\n        if 0 in self.item_ids:\n            raise ValueError(\'0 is used as an item id, conflicting \'\n                             \'with the sequence padding value.\')\n\n        if step_size is None:\n            step_size = max_sequence_length\n\n        # Sort first by user id, then by timestamp\n        sort_indices = np.lexsort((self.timestamps,\n                                   self.user_ids))\n\n        user_ids = self.user_ids[sort_indices]\n        item_ids = self.item_ids[sort_indices]\n\n        user_ids, indices, counts = np.unique(user_ids,\n                                              return_index=True,\n                                              return_counts=True)\n\n        num_subsequences = int(np.ceil(counts / float(step_size)).sum())\n\n        sequences = np.zeros((num_subsequences, max_sequence_length),\n                             dtype=np.int32)\n        sequence_users = np.empty(num_subsequences,\n                                  dtype=np.int32)\n        for i, (uid,\n                seq) in enumerate(_generate_sequences(user_ids,\n                                                      item_ids,\n                                                      indices,\n                                                      max_sequence_length,\n                                                      step_size)):\n            sequences[i][-len(seq):] = seq\n            sequence_users[i] = uid\n\n        if min_sequence_length is not None:\n            long_enough = sequences[:, -min_sequence_length] != 0\n            sequences = sequences[long_enough]\n            sequence_users = sequence_users[long_enough]\n\n        return (SequenceInteractions(sequences,\n                                     user_ids=sequence_users,\n                                     num_items=self.num_items))\n\n\nclass SequenceInteractions(object):\n    """"""\n    Interactions encoded as a sequence matrix.\n\n    Parameters\n    ----------\n\n    sequences: array of np.int32 of shape (num_sequences x max_sequence_length)\n        The interactions sequence matrix, as produced by\n        :func:`~Interactions.to_sequence`\n    num_items: int, optional\n        The number of distinct items in the data\n\n    Attributes\n    ----------\n\n    sequences: array of np.int32 of shape (num_sequences x max_sequence_length)\n        The interactions sequence matrix, as produced by\n        :func:`~Interactions.to_sequence`\n    """"""\n\n    def __init__(self,\n                 sequences,\n                 user_ids=None, num_items=None):\n\n        self.sequences = sequences\n        self.user_ids = user_ids\n        self.max_sequence_length = sequences.shape[1]\n\n        if num_items is None:\n            self.num_items = sequences.max() + 1\n        else:\n            self.num_items = num_items\n\n    def __repr__(self):\n\n        num_sequences, sequence_length = self.sequences.shape\n\n        return (\'<Sequence interactions dataset ({num_sequences} \'\n                \'sequences x {sequence_length} sequence length)>\'\n                .format(\n                    num_sequences=num_sequences,\n                    sequence_length=sequence_length,\n                ))\n'"
spotlight/layers.py,4,"b'""""""\nEmbedding layers useful for recommender models.\n""""""\n\nimport numpy as np\n\nfrom sklearn.utils import murmurhash3_32\n\nimport torch\nimport torch.nn as nn\n\n\nSEEDS = [\n    179424941, 179425457, 179425907, 179426369,\n    179424977, 179425517, 179425943, 179426407,\n    179424989, 179425529, 179425993, 179426447,\n    179425003, 179425537, 179426003, 179426453,\n    179425019, 179425559, 179426029, 179426491,\n    179425027, 179425579, 179426081, 179426549\n]\n\n\nclass ScaledEmbedding(nn.Embedding):\n    """"""\n    Embedding layer that initialises its values\n    to using a normal variable scaled by the inverse\n    of the embedding dimension.\n    """"""\n\n    def reset_parameters(self):\n        """"""\n        Initialize parameters.\n        """"""\n\n        self.weight.data.normal_(0, 1.0 / self.embedding_dim)\n        if self.padding_idx is not None:\n            self.weight.data[self.padding_idx].fill_(0)\n\n\nclass ZeroEmbedding(nn.Embedding):\n    """"""\n    Embedding layer that initialises its values\n    to using a normal variable scaled by the inverse\n    of the embedding dimension.\n\n    Used for biases.\n    """"""\n\n    def reset_parameters(self):\n        """"""\n        Initialize parameters.\n        """"""\n\n        self.weight.data.zero_()\n        if self.padding_idx is not None:\n            self.weight.data[self.padding_idx].fill_(0)\n\n\nclass ScaledEmbeddingBag(nn.EmbeddingBag):\n    """"""\n    EmbeddingBag layer that initialises its values\n    to using a normal variable scaled by the inverse\n    of the embedding dimension.\n    """"""\n\n    def reset_parameters(self):\n        """"""\n        Initialize parameters.\n        """"""\n\n        self.weight.data.normal_(0, 1.0 / self.embedding_dim)\n\n\nclass BloomEmbedding(nn.Module):\n    """"""\n    An embedding layer that compresses the number of embedding\n    parameters required by using bloom filter-like hashing.\n\n    Parameters\n    ----------\n\n    num_embeddings: int\n        Number of entities to be represented.\n    embedding_dim: int\n        Latent dimension of the embedding.\n    compression_ratio: float, optional\n        The underlying number of rows in the embedding layer\n        after compression. Numbers below 1.0 will use more\n        and more compression, reducing the number of parameters\n        in the layer.\n    num_hash_functions: int, optional\n        Number of hash functions used to compute the bloom filter indices.\n    bag: bool, optional\n        Whether to use the ``EmbeddingBag`` layer for the underlying embedding.\n        This should be faster in principle, but currently seems to perform\n        very poorly.\n\n    Notes\n    -----\n\n    Large embedding layers are a performance problem for fitting models:\n    even though the gradients are sparse (only a handful of user and item\n    vectors need parameter updates in every minibatch), PyTorch updates\n    the entire embedding layer at every backward pass. Computation time\n    is then wasted on applying zero gradient steps to whole embedding matrix.\n\n    To alleviate this problem, we can use a smaller underlying embedding layer,\n    and probabilistically hash users and items into that smaller space. With\n    good hash functions, collisions should be rare, and we should observe\n    fitting speedups without a decrease in accuracy.\n\n    The idea follows the RecSys 2017 ""Getting recommenders fit""[1]_\n    paper. The authors use a bloom-filter-like approach to hashing. Their approach\n    uses one-hot encoded inputs followed by fully connected layers as\n    well as softmax layers for the output, and their hashing reduces the\n    size of the fully connected layers rather than embedding layers as\n    implemented here; mathematically, however, the two formulations are\n    identical.\n\n    The hash function used is murmurhash3, hashing the indices with a different\n    seed for every hash function, modulo the size of the compressed embedding layer.\n    The hash mapping is computed once at the start of training, and indexed\n    into for every minibatch.\n\n    References\n    ----------\n\n    .. [1] Serra, Joan, and Alexandros Karatzoglou.\n       ""Getting deep recommenders fit: Bloom embeddings\n       for sparse binary input/output networks.""\n       arXiv preprint arXiv:1706.03993 (2017).\n    """"""\n\n    def __init__(self, num_embeddings, embedding_dim,\n                 compression_ratio=0.2,\n                 num_hash_functions=4,\n                 bag=False,\n                 padding_idx=0):\n\n        super(BloomEmbedding, self).__init__()\n\n        self.num_embeddings = num_embeddings\n        self.embedding_dim = embedding_dim\n        self.compression_ratio = compression_ratio\n        self.compressed_num_embeddings = int(compression_ratio *\n                                             num_embeddings)\n        self.num_hash_functions = num_hash_functions\n        self.padding_idx = padding_idx\n        self._bag = bag\n\n        if num_hash_functions > len(SEEDS):\n            raise ValueError(\'Can use at most {} hash functions ({} requested)\'\n                             .format(len(SEEDS), num_hash_functions))\n\n        self._masks = SEEDS[:self.num_hash_functions]\n\n        if self._bag:\n            self.embeddings = ScaledEmbeddingBag(self.compressed_num_embeddings,\n                                                 self.embedding_dim,\n                                                 mode=\'sum\')\n        else:\n            self.embeddings = ScaledEmbedding(self.compressed_num_embeddings,\n                                              self.embedding_dim,\n                                              padding_idx=self.padding_idx)\n\n        # Hash cache. We pre-hash all the indices, and then just\n        # map the indices to their pre-hashed values as we go\n        # through the minibatches.\n        self._hashes = None\n        self._offsets = None\n\n    def __repr__(self):\n\n        return (\'<BloomEmbedding (compression_ratio: {}): {}>\'\n                .format(self.compression_ratio,\n                        repr(self.embeddings)))\n\n    def _get_hashed_indices(self, original_indices):\n\n        def _hash(x, seed):\n\n            # TODO: integrate with padding index\n            result = murmurhash3_32(x, seed=seed)\n            result[self.padding_idx] = 0\n\n            return result % self.compressed_num_embeddings\n\n        if self._hashes is None:\n            indices = np.arange(self.num_embeddings, dtype=np.int32)\n            hashes = np.stack([_hash(indices, seed)\n                               for seed in self._masks],\n                              axis=1).astype(np.int64)\n            assert hashes[self.padding_idx].sum() == 0\n\n            self._hashes = torch.from_numpy(hashes)\n\n            if original_indices.is_cuda:\n                self._hashes = self._hashes.cuda()\n\n        hashed_indices = torch.index_select(self._hashes,\n                                            0,\n                                            original_indices.squeeze())\n\n        return hashed_indices\n\n    def forward(self, indices):\n        """"""\n        Retrieve embeddings corresponding to indices.\n\n        See documentation on PyTorch ``nn.Embedding`` for details.\n        """"""\n\n        if indices.dim() == 2:\n            batch_size, seq_size = indices.size()\n        else:\n            batch_size, seq_size = indices.size(0), 1\n\n        if not indices.is_contiguous():\n            indices = indices.contiguous()\n\n        indices = indices.data.view(batch_size * seq_size, 1)\n\n        if self._bag:\n            if (self._offsets is None or\n                    self._offsets.size(0) != (batch_size * seq_size)):\n\n                self._offsets = torch.arange(0,\n                                             indices.numel(),\n                                             indices.size(1)).long()\n\n                if indices.is_cuda:\n                    self._offsets = self._offsets.cuda()\n\n            hashed_indices = self._get_hashed_indices(indices)\n            embedding = self.embeddings(hashed_indices.view(-1), self._offsets)\n            embedding = embedding.view(batch_size, seq_size, -1)\n        else:\n            hashed_indices = self._get_hashed_indices(indices)\n\n            embedding = self.embeddings(hashed_indices)\n            embedding = embedding.sum(1)\n            embedding = embedding.view(batch_size, seq_size, -1)\n\n        return embedding\n'"
spotlight/losses.py,8,"b'""""""\nLoss functions for recommender models.\n\nThe pointwise, BPR, and hinge losses are a good fit for\nimplicit feedback models trained through negative sampling.\n\nThe regression and Poisson losses are used for explicit feedback\nmodels.\n""""""\n\nimport torch\n\nimport torch.nn.functional as F\n\nfrom spotlight.torch_utils import assert_no_grad\n\n\ndef pointwise_loss(positive_predictions, negative_predictions, mask=None):\n    """"""\n    Logistic loss function.\n\n    Parameters\n    ----------\n\n    positive_predictions: tensor\n        Tensor containing predictions for known positive items.\n    negative_predictions: tensor\n        Tensor containing predictions for sampled negative items.\n    mask: tensor, optional\n        A binary tensor used to zero the loss from some entries\n        of the loss tensor.\n\n    Returns\n    -------\n\n    loss, float\n        The mean value of the loss function.\n    """"""\n\n    positives_loss = (1.0 - torch.sigmoid(positive_predictions))\n    negatives_loss = torch.sigmoid(negative_predictions)\n\n    loss = (positives_loss + negatives_loss)\n\n    if mask is not None:\n        mask = mask.float()\n        loss = loss * mask\n        return loss.sum() / mask.sum()\n\n    return loss.mean()\n\n\ndef bpr_loss(positive_predictions, negative_predictions, mask=None):\n    """"""\n    Bayesian Personalised Ranking [1]_ pairwise loss function.\n\n    Parameters\n    ----------\n\n    positive_predictions: tensor\n        Tensor containing predictions for known positive items.\n    negative_predictions: tensor\n        Tensor containing predictions for sampled negative items.\n    mask: tensor, optional\n        A binary tensor used to zero the loss from some entries\n        of the loss tensor.\n\n    Returns\n    -------\n\n    loss, float\n        The mean value of the loss function.\n\n    References\n    ----------\n\n    .. [1] Rendle, Steffen, et al. ""BPR: Bayesian personalized ranking from\n       implicit feedback."" Proceedings of the twenty-fifth conference on\n       uncertainty in artificial intelligence. AUAI Press, 2009.\n    """"""\n\n    loss = (1.0 - torch.sigmoid(positive_predictions -\n                                negative_predictions))\n\n    if mask is not None:\n        mask = mask.float()\n        loss = loss * mask\n        return loss.sum() / mask.sum()\n\n    return loss.mean()\n\n\ndef hinge_loss(positive_predictions, negative_predictions, mask=None):\n    """"""\n    Hinge pairwise loss function.\n\n    Parameters\n    ----------\n\n    positive_predictions: tensor\n        Tensor containing predictions for known positive items.\n    negative_predictions: tensor\n        Tensor containing predictions for sampled negative items.\n    mask: tensor, optional\n        A binary tensor used to zero the loss from some entries\n        of the loss tensor.\n\n    Returns\n    -------\n\n    loss, float\n        The mean value of the loss function.\n    """"""\n\n    loss = torch.clamp(negative_predictions -\n                       positive_predictions +\n                       1.0, 0.0)\n\n    if mask is not None:\n        mask = mask.float()\n        loss = loss * mask\n        return loss.sum() / mask.sum()\n\n    return loss.mean()\n\n\ndef adaptive_hinge_loss(positive_predictions, negative_predictions, mask=None):\n    """"""\n    Adaptive hinge pairwise loss function. Takes a set of predictions\n    for implicitly negative items, and selects those that are highest,\n    thus sampling those negatives that are closes to violating the\n    ranking implicit in the pattern of user interactions.\n\n    Approximates the idea of weighted approximate-rank pairwise loss\n    introduced in [2]_\n\n    Parameters\n    ----------\n\n    positive_predictions: tensor\n        Tensor containing predictions for known positive items.\n    negative_predictions: tensor\n        Iterable of tensors containing predictions for sampled negative items.\n        More tensors increase the likelihood of finding ranking-violating\n        pairs, but risk overfitting.\n    mask: tensor, optional\n        A binary tensor used to zero the loss from some entries\n        of the loss tensor.\n\n    Returns\n    -------\n\n    loss, float\n        The mean value of the loss function.\n\n    References\n    ----------\n\n    .. [2] Weston, Jason, Samy Bengio, and Nicolas Usunier. ""Wsabie:\n       Scaling up to large vocabulary image annotation."" IJCAI.\n       Vol. 11. 2011.\n    """"""\n\n    highest_negative_predictions, _ = torch.max(negative_predictions, 0)\n\n    return hinge_loss(positive_predictions, highest_negative_predictions.squeeze(), mask=mask)\n\n\ndef regression_loss(observed_ratings, predicted_ratings):\n    """"""\n    Regression loss.\n\n    Parameters\n    ----------\n\n    observed_ratings: tensor\n        Tensor containing observed ratings.\n    predicted_ratings: tensor\n        Tensor containing rating predictions.\n\n    Returns\n    -------\n\n    loss, float\n        The mean value of the loss function.\n    """"""\n\n    assert_no_grad(observed_ratings)\n\n    return ((observed_ratings - predicted_ratings) ** 2).mean()\n\n\ndef poisson_loss(observed_ratings, predicted_ratings):\n    """"""\n    Poisson loss.\n\n    Parameters\n    ----------\n\n    observed_ratings: tensor\n        Tensor containing observed ratings.\n    predicted_ratings: tensor\n        Tensor containing rating predictions.\n\n    Returns\n    -------\n\n    loss, float\n        The mean value of the loss function.\n    """"""\n\n    assert_no_grad(observed_ratings)\n\n    return (predicted_ratings - observed_ratings * torch.log(predicted_ratings)).mean()\n\n\ndef logistic_loss(observed_ratings, predicted_ratings):\n    """"""\n    Logistic loss for explicit data.\n\n    Parameters\n    ----------\n\n    observed_ratings: tensor\n        Tensor containing observed ratings which\n        should be +1 or -1 for this loss function.\n    predicted_ratings: tensor\n        Tensor containing rating predictions.\n\n    Returns\n    -------\n\n    loss, float\n        The mean value of the loss function.\n    """"""\n\n    assert_no_grad(observed_ratings)\n\n    # Convert target classes from (-1, 1) to (0, 1)\n    observed_ratings = torch.clamp(observed_ratings, 0, 1)\n\n    return F.binary_cross_entropy_with_logits(predicted_ratings,\n                                              observed_ratings,\n                                              size_average=True)\n'"
spotlight/sampling.py,0,"b'""""""\nModule containing functions for negative item sampling.\n""""""\n\nimport numpy as np\n\n\ndef sample_items(num_items, shape, random_state=None):\n    """"""\n    Randomly sample a number of items.\n\n    Parameters\n    ----------\n\n    num_items: int\n        Total number of items from which we should sample:\n        the maximum value of a sampled item id will be smaller\n        than this.\n    shape: int or tuple of ints\n        Shape of the sampled array.\n    random_state: np.random.RandomState instance, optional\n        Random state to use for sampling.\n\n    Returns\n    -------\n\n    items: np.array of shape [shape]\n        Sampled item ids.\n    """"""\n\n    if random_state is None:\n        random_state = np.random.RandomState()\n\n    items = random_state.randint(0, num_items, shape, dtype=np.int64)\n\n    return items\n'"
spotlight/torch_utils.py,2,"b'import numpy as np\n\nimport torch\n\n\ndef gpu(tensor, gpu=False):\n\n    if gpu:\n        return tensor.cuda()\n    else:\n        return tensor\n\n\ndef cpu(tensor):\n\n    if tensor.is_cuda:\n        return tensor.cpu()\n    else:\n        return tensor\n\n\ndef minibatch(*tensors, **kwargs):\n\n    batch_size = kwargs.get(\'batch_size\', 128)\n\n    if len(tensors) == 1:\n        tensor = tensors[0]\n        for i in range(0, len(tensor), batch_size):\n            yield tensor[i:i + batch_size]\n    else:\n        for i in range(0, len(tensors[0]), batch_size):\n            yield tuple(x[i:i + batch_size] for x in tensors)\n\n\ndef shuffle(*arrays, **kwargs):\n\n    random_state = kwargs.get(\'random_state\')\n\n    if len(set(len(x) for x in arrays)) != 1:\n        raise ValueError(\'All inputs to shuffle must have \'\n                         \'the same length.\')\n\n    if random_state is None:\n        random_state = np.random.RandomState()\n\n    shuffle_indices = np.arange(len(arrays[0]))\n    random_state.shuffle(shuffle_indices)\n\n    if len(arrays) == 1:\n        return arrays[0][shuffle_indices]\n    else:\n        return tuple(x[shuffle_indices] for x in arrays)\n\n\ndef assert_no_grad(variable):\n\n    if variable.requires_grad:\n        raise ValueError(\n            ""nn criterions don\'t compute the gradient w.r.t. targets - please ""\n            ""mark these variables as volatile or not requiring gradients""\n        )\n\n\ndef set_seed(seed, cuda=False):\n\n    torch.manual_seed(seed)\n\n    if cuda:\n        torch.cuda.manual_seed(seed)\n'"
tests/test_cross_validation.py,0,"b""import numpy as np\n\nfrom spotlight import cross_validation\nfrom spotlight.datasets import movielens\n\n\nRANDOM_STATE = np.random.RandomState(42)\n\n\ndef test_user_based_split():\n\n    interactions = movielens.get_movielens_dataset('100K')\n\n    train, test = (cross_validation\n                   .user_based_train_test_split(interactions,\n                                                test_percentage=0.2,\n                                                random_state=RANDOM_STATE))\n\n    assert len(train) + len(test) == len(interactions)\n\n    users_in_test = len(np.unique(test.user_ids))\n    assert np.allclose(float(users_in_test) / interactions.num_users,\n                       0.2, atol=0.001)\n"""
tests/test_datasets.py,0,b'from spotlight.datasets import movielens\n\n\ndef test_dataset_downloading():\n\n    for variant in movielens.VARIANTS[:2]:\n        movielens.get_movielens_dataset(variant)\n'
tests/test_evaluation_metrics.py,0,"b""import os\n\nimport numpy as np\n\nimport pytest\n\nfrom spotlight.evaluation import precision_recall_score, sequence_precision_recall_score\nfrom spotlight.cross_validation import random_train_test_split, user_based_train_test_split\nfrom spotlight.datasets import movielens\nfrom spotlight.factorization.implicit import ImplicitFactorizationModel\nfrom spotlight.sequence.implicit import ImplicitSequenceModel\n\nRANDOM_STATE = np.random.RandomState(42)\nCUDA = bool(os.environ.get('SPOTLIGHT_CUDA', False))\n# Acceptable variation in specific test runs\nEPSILON = .005\n\n\n@pytest.fixture(scope='module')\ndef data_implicit_factorization():\n\n    interactions = movielens.get_movielens_dataset('100K')\n\n    train, test = random_train_test_split(interactions,\n                                          random_state=RANDOM_STATE)\n\n    model = ImplicitFactorizationModel(loss='bpr',\n                                       n_iter=1,\n                                       batch_size=1024,\n                                       learning_rate=1e-2,\n                                       l2=1e-6,\n                                       random_state=RANDOM_STATE,\n                                       use_cuda=CUDA)\n    model.fit(train)\n\n    return train, test, model\n\n\n@pytest.fixture(scope='module')\ndef data_implicit_sequence():\n\n    max_sequence_length = 200\n    min_sequence_length = 20\n    step_size = 200\n\n    interactions = movielens.get_movielens_dataset('100K')\n\n    train, test = user_based_train_test_split(interactions,\n                                              random_state=RANDOM_STATE)\n\n    train = train.to_sequence(max_sequence_length=max_sequence_length,\n                              min_sequence_length=min_sequence_length,\n                              step_size=step_size)\n\n    test = test.to_sequence(max_sequence_length=max_sequence_length,\n                            min_sequence_length=min_sequence_length,\n                            step_size=step_size)\n\n    model = ImplicitSequenceModel(loss='adaptive_hinge',\n                                  representation='lstm',\n                                  batch_size=8,\n                                  learning_rate=1e-2,\n                                  l2=1e-3,\n                                  n_iter=2,\n                                  use_cuda=CUDA,\n                                  random_state=RANDOM_STATE)\n\n    model.fit(train, verbose=True)\n\n    return train, test, model\n\n\n@pytest.mark.parametrize('k', [10])\ndef test_sequence_precision_recall(data_implicit_sequence, k):\n\n    (train, test, model) = data_implicit_sequence\n\n    precision, recall = sequence_precision_recall_score(model, test, k)\n    precision = precision.mean()\n    recall = recall.mean()\n\n    # with respect to the hyper-parameters specified in data_implicit_sequence\n    expected_precision = 0.064\n    expected_recall = 0.064\n\n    # true_pos/(true_pos + false_pos) == true_pos/(true_pos + false_neg)\n    # because num_predictions is set equal to num_targets in sequence_precision_recall_score\n    assert precision == recall\n    assert expected_precision - EPSILON < precision and precision < expected_precision + EPSILON\n    assert expected_recall - EPSILON < recall and recall < expected_recall + EPSILON\n\n\n@pytest.mark.parametrize('k', [\n    1,\n    [1, 1],\n    [1, 1, 1]\n])\ndef test_precision_recall(data_implicit_factorization, k):\n\n    (train, test, model) = data_implicit_factorization\n\n    interactions = movielens.get_movielens_dataset('100K')\n    train, test = random_train_test_split(interactions,\n                                          random_state=RANDOM_STATE)\n\n    precision, recall = precision_recall_score(model, test, train, k=k)\n\n    assert precision.shape == recall.shape\n\n    if not isinstance(k, list):\n        assert len(precision.shape) == 1\n    else:\n        assert precision.shape[1] == len(k)\n"""
tests/test_interactions.py,0,"b'import numpy as np\n\nimport pytest\n\nfrom spotlight.cross_validation import random_train_test_split\nfrom spotlight.datasets import movielens\nfrom spotlight.interactions import Interactions\n\n\ndef _test_just_padding(sequences):\n    """"""\n    There should be no rows with only padding in them.\n    """"""\n\n    row_sum = sequences.sum(axis=1)\n\n    assert len(row_sum) == sequences.shape[0]\n    assert np.all(row_sum > 0)\n\n\ndef _test_final_column_no_padding(sequences):\n    """"""\n    The final column should always have an interaction.\n    """"""\n\n    assert np.all(sequences[:, -1] > 0)\n\n\ndef _test_shifted(sequence_users, sequences, step_size):\n    """"""\n    Unless there was a change of user, row i + 1\'s interactions\n    should contain row i\'s interactions shifted to the right by\n    step size.\n    """"""\n\n    for i in range(1, len(sequences)):\n\n        if sequence_users[i] != sequence_users[i - 1]:\n            # Change of user\n            continue\n\n        assert np.all(sequences[i][step_size:] == sequences[i - 1][:-step_size])\n\n\ndef _test_temporal_order(sequence_users, sequences, interactions):\n\n    interaction_matrix = interactions.tocoo()\n    interaction_matrix.data = interactions.timestamps\n    interaction_matrix = interaction_matrix.tocsr().todense()\n\n    for i, sequence in enumerate(sequences):\n\n        user_id = sequence_users[i]\n        nonpadded_sequence = sequence[sequence != 0]\n\n        for j in range(0, len(nonpadded_sequence) - 1):\n            item_id = nonpadded_sequence[j]\n\n            next_item_id = nonpadded_sequence[j + 1]\n\n            item_timestamp = interaction_matrix[user_id, item_id]\n            next_item_timestamp = interaction_matrix[user_id, next_item_id]\n\n            assert item_timestamp <= next_item_timestamp\n\n\ndef test_known_output_step_1():\n\n    interactions = Interactions(np.zeros(5),\n                                np.arange(5) + 1,\n                                timestamps=np.arange(5))\n    sequences = interactions.to_sequence(max_sequence_length=5,\n                                         step_size=1).sequences\n\n    expected = np.array([\n        [1, 2, 3, 4, 5],\n        [0, 1, 2, 3, 4],\n        [0, 0, 1, 2, 3],\n        [0, 0, 0, 1, 2],\n        [0, 0, 0, 0, 1]\n    ])\n\n    assert np.all(sequences == expected)\n\n\ndef test_known_output_step_2():\n\n    interactions = Interactions(np.zeros(5),\n                                np.arange(5) + 1,\n                                timestamps=np.arange(5))\n    sequences = interactions.to_sequence(max_sequence_length=5,\n                                         step_size=2).sequences\n\n    expected = np.array([\n        [1, 2, 3, 4, 5],\n        [0, 0, 1, 2, 3],\n        [0, 0, 0, 0, 1],\n    ])\n\n    assert np.all(sequences == expected)\n\n\n@pytest.mark.parametrize(\'max_sequence_length, step_size\', [\n    (5, 1),\n    (5, 3),\n    (20, 1),\n    (20, 4),\n    (1024, 1024),\n    (1024, 5)\n])\ndef test_to_sequence(max_sequence_length, step_size):\n\n    interactions = movielens.get_movielens_dataset(\'100K\')\n    _, interactions = random_train_test_split(interactions)\n\n    sequences = interactions.to_sequence(\n        max_sequence_length=max_sequence_length,\n        step_size=step_size)\n\n    if step_size == 1:\n        assert sequences.sequences.shape == (len(interactions),\n                                             max_sequence_length)\n    else:\n        assert sequences.sequences.shape[1] == max_sequence_length\n\n    _test_just_padding(sequences.sequences)\n    _test_final_column_no_padding(sequences.sequences)\n    _test_shifted(sequences.user_ids,\n                  sequences.sequences,\n                  step_size)\n    _test_temporal_order(sequences.user_ids,\n                         sequences.sequences,\n                         interactions)\n\n\ndef test_to_sequence_min_length():\n\n    min_sequence_length = 10\n    interactions = movielens.get_movielens_dataset(\'100K\')\n\n    # Check that with default arguments there are sequences\n    # that are shorter than we want\n    sequences = interactions.to_sequence(max_sequence_length=20)\n    assert np.any((sequences.sequences != 0).sum(axis=1) < min_sequence_length)\n\n    # But no such sequences after we specify min length.\n    sequences = interactions.to_sequence(max_sequence_length=20,\n                                         min_sequence_length=min_sequence_length)\n    assert not np.any((sequences.sequences != 0).sum(axis=1) < min_sequence_length)\n'"
tests/test_layers.py,3,"b""import numpy as np\nimport pytest\nimport torch\n\nimport torch.nn as nn\n\nfrom spotlight.layers import BloomEmbedding, ScaledEmbedding\n\n\n@pytest.mark.parametrize('embedding_class', [\n    nn.Embedding,\n    ScaledEmbedding,\n    BloomEmbedding\n])\ndef test_embeddings(embedding_class):\n\n    num_embeddings = 1000\n    embedding_dim = 16\n\n    batch_size = 32\n    sequence_length = 8\n\n    layer = embedding_class(num_embeddings,\n                            embedding_dim)\n\n    # Test 1-d inputs (minibatch)\n    indices = torch.from_numpy(\n        np.random.randint(0, num_embeddings, size=batch_size, dtype=np.int64))\n    representation = layer(indices)\n    assert representation.size()[0] == batch_size\n    assert representation.size()[-1] == embedding_dim\n\n    # Test 2-d inputs (minibatch x sequence_length)\n    indices = torch.from_numpy(\n        np.random.randint(0, num_embeddings,\n                          size=(batch_size, sequence_length), dtype=np.int64))\n    representation = layer(indices)\n    assert representation.size() == (batch_size, sequence_length, embedding_dim)\n"""
tests/test_serialization.py,2,"b'import os\nimport shutil\nimport tempfile\n\nimport numpy as np\nimport pytest\nimport torch\n\nfrom spotlight.cross_validation import random_train_test_split\nfrom spotlight.datasets import movielens\nfrom spotlight.evaluation import mrr_score, sequence_mrr_score\nfrom spotlight.evaluation import rmse_score\nfrom spotlight.factorization.explicit import ExplicitFactorizationModel\nfrom spotlight.factorization.implicit import ImplicitFactorizationModel\nfrom spotlight.sequence.implicit import ImplicitSequenceModel\nfrom spotlight.sequence.representations import CNNNet\n\n\nRANDOM_STATE = np.random.RandomState(42)\nCUDA = bool(os.environ.get(\'SPOTLIGHT_CUDA\', False))\n\n\ndef _reload(model):\n    dirname = tempfile.mkdtemp()\n\n    try:\n        fname = os.path.join(dirname, ""model.pkl"")\n\n        torch.save(model, fname)\n        model = torch.load(fname)\n\n    finally:\n        shutil.rmtree(dirname)\n\n    return model\n\n\n@pytest.fixture(scope=""module"")\ndef data():\n\n    interactions = movielens.get_movielens_dataset(\'100K\')\n\n    train, test = random_train_test_split(interactions,\n                                          random_state=RANDOM_STATE)\n\n    return train, test\n\n\ndef test_explicit_serialization(data):\n\n    train, test = data\n\n    model = ExplicitFactorizationModel(loss=\'regression\',\n                                       n_iter=3,\n                                       batch_size=1024,\n                                       learning_rate=1e-3,\n                                       l2=1e-5,\n                                       use_cuda=CUDA)\n    model.fit(train)\n\n    rmse_original = rmse_score(model, test)\n    rmse_recovered = rmse_score(_reload(model), test)\n\n    assert rmse_original == rmse_recovered\n\n\ndef test_implicit_serialization(data):\n\n    train, test = data\n\n    model = ImplicitFactorizationModel(loss=\'bpr\',\n                                       n_iter=3,\n                                       batch_size=1024,\n                                       learning_rate=1e-2,\n                                       l2=1e-6,\n                                       use_cuda=CUDA)\n    model.fit(train)\n\n    mrr_original = mrr_score(model, test, train=train).mean()\n    mrr_recovered = mrr_score(_reload(model), test, train=train).mean()\n\n    assert mrr_original == mrr_recovered\n\n\ndef test_implicit_sequence_serialization(data):\n\n    train, test = data\n    train = train.to_sequence(max_sequence_length=128)\n    test = test.to_sequence(max_sequence_length=128)\n\n    model = ImplicitSequenceModel(loss=\'bpr\',\n                                  representation=CNNNet(train.num_items,\n                                                        embedding_dim=32,\n                                                        kernel_width=3,\n                                                        dilation=(1, ),\n                                                        num_layers=1),\n                                  batch_size=128,\n                                  learning_rate=1e-1,\n                                  l2=0.0,\n                                  n_iter=5,\n                                  random_state=RANDOM_STATE,\n                                  use_cuda=CUDA)\n    model.fit(train)\n\n    mrr_original = sequence_mrr_score(model, test).mean()\n    mrr_recovered = sequence_mrr_score(_reload(model), test).mean()\n\n    assert mrr_original == mrr_recovered\n'"
examples/bloom_embeddings/example.py,0,"b""import argparse\nimport hashlib\nimport json\nimport os\nimport shutil\nimport time\n\nimport numpy as np\n\nfrom sklearn.model_selection import ParameterSampler\n\nfrom spotlight.datasets.movielens import get_movielens_dataset\nfrom spotlight.datasets.amazon import get_amazon_dataset\nfrom spotlight.cross_validation import (random_train_test_split,\n                                        user_based_train_test_split)\nfrom spotlight.sequence.implicit import ImplicitSequenceModel\nfrom spotlight.factorization.implicit import ImplicitFactorizationModel\nfrom spotlight.sequence.representations import LSTMNet\nfrom spotlight.factorization.representations import BilinearNet\nfrom spotlight.layers import BloomEmbedding, ScaledEmbedding\nfrom spotlight.evaluation import mrr_score, sequence_mrr_score\nfrom spotlight.torch_utils import set_seed\n\n\nCUDA = (os.environ.get('CUDA') is not None or\n        shutil.which('nvidia-smi') is not None)\n\nNUM_SAMPLES = 50\n\nLEARNING_RATES = [1e-4, 5 * 1e-4, 1e-3, 1e-2, 5 * 1e-2, 1e-1]\nLOSSES = ['bpr', 'adaptive_hinge']\nBATCH_SIZE = [16, 32, 64, 128, 256, 512]\nEMBEDDING_DIM = [32, 64, 128, 256]\nN_ITER = list(range(1, 20))\nL2 = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.0]\n\n\nclass Results:\n\n    def __init__(self, filename):\n\n        self._filename = filename\n\n        open(self._filename, 'a+')\n\n    def _hash(self, x):\n\n        return hashlib.md5(json.dumps(x, sort_keys=True).encode('utf-8')).hexdigest()\n\n    def save(self, hyperparams, test_mrr, validation_mrr, elapsed):\n\n        result = hyperparams.copy()\n\n        result.update({'test_mrr': test_mrr,\n                       'validation_mrr': validation_mrr,\n                       'elapsed': elapsed,\n                       'hash': self._hash(hyperparams)})\n\n        with open(self._filename, 'a+') as out:\n            out.write(json.dumps(result) + '\\n')\n\n    def best_baseline(self):\n\n        results = sorted([x for x in self\n                          if x['compression_ratio'] == 1.0 and\n                          x['embedding_dim'] >= 32],\n                         key=lambda x: -x['test_mrr'])\n\n        if results:\n            return results[0]\n        else:\n            return None\n\n    def best(self):\n\n        results = sorted([x for x in self],\n                         key=lambda x: -x['test_mrr'])\n\n        if results:\n            return results[0]\n        else:\n            return None\n\n    def __getitem__(self, hyperparams):\n\n        params_hash = self._hash(hyperparams)\n\n        with open(self._filename, 'r+') as fle:\n            for line in fle:\n                datum = json.loads(line)\n\n                if datum['hash'] == params_hash:\n                    del datum['hash']\n                    return datum\n\n        raise KeyError\n\n    def __contains__(self, x):\n\n        try:\n            self[x]\n            return True\n        except KeyError:\n            return False\n\n    def __iter__(self):\n\n        with open(self._filename, 'r+') as fle:\n            for line in fle:\n                datum = json.loads(line)\n\n                del datum['hash']\n\n                yield datum\n\n\ndef sample_hyperparameters(random_state, num):\n\n    space = {\n        'n_iter': N_ITER,\n        'batch_size': BATCH_SIZE,\n        'l2': L2,\n        'learning_rate': LEARNING_RATES,\n        'loss': LOSSES,\n        'embedding_dim': EMBEDDING_DIM,\n    }\n\n    sampler = ParameterSampler(space,\n                               n_iter=num,\n                               random_state=random_state)\n\n    for params in sampler:\n        yield params\n\n\ndef build_factorization_model(hyperparameters, train, random_state):\n    h = hyperparameters\n\n    set_seed(42, CUDA)\n\n    if h['compression_ratio'] < 1.0:\n        item_embeddings = BloomEmbedding(train.num_items, h['embedding_dim'],\n                                         compression_ratio=h['compression_ratio'],\n                                         num_hash_functions=4,\n                                         padding_idx=0)\n        user_embeddings = BloomEmbedding(train.num_users, h['embedding_dim'],\n                                         compression_ratio=h['compression_ratio'],\n                                         num_hash_functions=4,\n                                         padding_idx=0)\n    else:\n        item_embeddings = ScaledEmbedding(train.num_items, h['embedding_dim'],\n                                          padding_idx=0)\n        user_embeddings = ScaledEmbedding(train.num_users, h['embedding_dim'],\n                                          padding_idx=0)\n\n    network = BilinearNet(train.num_users,\n                          train.num_items,\n                          user_embedding_layer=user_embeddings,\n                          item_embedding_layer=item_embeddings)\n\n    model = ImplicitFactorizationModel(loss=h['loss'],\n                                       n_iter=h['n_iter'],\n                                       batch_size=h['batch_size'],\n                                       learning_rate=h['learning_rate'],\n                                       embedding_dim=h['embedding_dim'],\n                                       l2=h['l2'],\n                                       representation=network,\n                                       use_cuda=CUDA,\n                                       random_state=np.random.RandomState(42))\n\n    return model\n\n\ndef build_sequence_model(hyperparameters, train, random_state):\n\n    h = hyperparameters\n\n    set_seed(42, CUDA)\n\n    if h['compression_ratio'] < 1.0:\n        item_embeddings = BloomEmbedding(train.num_items, h['embedding_dim'],\n                                         compression_ratio=h['compression_ratio'],\n                                         num_hash_functions=4,\n                                         padding_idx=0)\n    else:\n        item_embeddings = ScaledEmbedding(train.num_items, h['embedding_dim'],\n                                          padding_idx=0)\n\n    network = LSTMNet(train.num_items, h['embedding_dim'],\n                      item_embedding_layer=item_embeddings)\n\n    model = ImplicitSequenceModel(loss=h['loss'],\n                                  n_iter=h['n_iter'],\n                                  batch_size=h['batch_size'],\n                                  learning_rate=h['learning_rate'],\n                                  embedding_dim=h['embedding_dim'],\n                                  l2=h['l2'],\n                                  representation=network,\n                                  use_cuda=CUDA,\n                                  random_state=np.random.RandomState(42))\n\n    return model\n\n\ndef evaluate_model(model, train, test, validation):\n\n    start_time = time.time()\n    model.fit(train, verbose=True)\n    elapsed = time.time() - start_time\n\n    print('Elapsed {}'.format(elapsed))\n    print(model)\n\n    if hasattr(test, 'sequences'):\n        test_mrr = sequence_mrr_score(model, test)\n        val_mrr = sequence_mrr_score(model, validation)\n    else:\n        test_mrr = mrr_score(model, test)\n        val_mrr = mrr_score(model, test.tocsr() + validation.tocsr())\n\n    return test_mrr, val_mrr, elapsed\n\n\ndef run(experiment_name, train, test, validation, random_state):\n\n    results = Results('{}_results.txt'.format(experiment_name))\n    compression_ratios = (np.arange(1, 10) / 10).tolist()\n\n    best_result = results.best()\n\n    if best_result is not None:\n        print('Best result: {}'.format(results.best()))\n\n    # Find a good baseline\n    for hyperparameters in sample_hyperparameters(random_state, NUM_SAMPLES):\n\n        if 'factorization' in experiment_name:\n            # We want bigger batches for factorization models\n            hyperparameters['batch_size'] = hyperparameters['batch_size'] * 4\n\n        hyperparameters['compression_ratio'] = 1.0\n\n        if hyperparameters in results:\n            print('Done, skipping...')\n            continue\n\n        if 'factorization' in experiment_name:\n            model = build_factorization_model(hyperparameters,\n                                              train,\n                                              random_state)\n        else:\n            model = build_sequence_model(hyperparameters,\n                                         train,\n                                         random_state)\n\n        print('Fitting {}'.format(hyperparameters))\n        (test_mrr, val_mrr, elapsed) = evaluate_model(model,\n                                                      train,\n                                                      test,\n                                                      validation)\n\n        print('Test MRR {} val MRR {} elapsed {}'.format(\n            test_mrr.mean(), val_mrr.mean(), elapsed\n        ))\n\n        results.save(hyperparameters, test_mrr.mean(), val_mrr.mean(), elapsed)\n\n    best_baseline = results.best_baseline()\n    print('Best baseline: {}'.format(best_baseline))\n\n    # Compute compression results\n    for compression_ratio in compression_ratios:\n\n        hyperparameters = best_baseline\n        hyperparameters['compression_ratio'] = compression_ratio\n\n        if hyperparameters in results:\n            print('Compression computed')\n            continue\n\n        if 'factorization' in experiment_name:\n            model = build_factorization_model(hyperparameters,\n                                              train,\n                                              random_state)\n        else:\n            model = build_sequence_model(hyperparameters,\n                                         train,\n                                         random_state)\n\n        print('Evaluating {}'.format(hyperparameters))\n\n        (test_mrr, val_mrr, elapsed) = evaluate_model(model,\n                                                      train,\n                                                      test,\n                                                      validation)\n        print('Test MRR {} val MRR {} elapsed {}'.format(\n            test_mrr.mean(), val_mrr.mean(), elapsed\n        ))\n\n        results.save(hyperparameters, test_mrr.mean(), val_mrr.mean(), elapsed)\n\n    return results\n\n\nif __name__ == '__main__':\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('dataset', type=str)\n    parser.add_argument('model', type=str)\n\n    args = parser.parse_args()\n\n    random_state = np.random.RandomState(100)\n\n    if args.dataset == 'movielens':\n        dataset = get_movielens_dataset('1M')\n        test_percentage = 0.2\n    else:\n        test_percentage = 0.01\n        dataset = get_amazon_dataset(min_user_interactions=20,\n                                     min_item_interactions=5)\n\n    print(dataset)\n\n    if args.model == 'sequence':\n        max_sequence_length = int(np.percentile(dataset.tocsr()\n                                                .getnnz(axis=1),\n                                                95))\n        min_sequence_length = 20\n        step_size = max_sequence_length\n\n        train, rest = user_based_train_test_split(dataset,\n                                                  test_percentage=0.05,\n                                                  random_state=random_state)\n        test, validation = user_based_train_test_split(rest,\n                                                       test_percentage=0.5,\n                                                       random_state=random_state)\n        train = train.to_sequence(max_sequence_length=max_sequence_length,\n                                  min_sequence_length=min_sequence_length,\n                                  step_size=step_size)\n        test = test.to_sequence(max_sequence_length=max_sequence_length,\n                                min_sequence_length=min_sequence_length,\n                                step_size=step_size)\n        validation = validation.to_sequence(max_sequence_length=max_sequence_length,\n                                            min_sequence_length=min_sequence_length,\n                                            step_size=step_size)\n        print('In test {}, in validation {}'.format(\n            len(test.sequences),\n            len(validation.sequences))\n        )\n    elif args.model == 'factorization':\n        train, rest = random_train_test_split(dataset,\n                                              test_percentage=test_percentage,\n                                              random_state=random_state)\n        test, validation = random_train_test_split(rest,\n                                                   test_percentage=0.5,\n                                                   random_state=random_state)\n\n    experiment_name = '{}_{}'.format(args.dataset, args.model)\n\n    run(experiment_name, train, test, validation, random_state)\n"""
examples/bloom_embeddings/helpers.py,0,"b""import pandas as pd\n\nfrom tabulate import tabulate\n\n\ndef _load_data(filename, columns=None):\n\n    data = pd.read_json(filename, lines=True)\n    data = data.sort_values('validation_mrr', ascending=False)\n\n    mrr_cols = ['validation_mrr', 'test_mrr']\n\n    if columns is None:\n        columns = [x for x in data.columns if\n                   (x not in mrr_cols and x != 'hash')]\n\n    cols = data.columns\n    cols = mrr_cols + columns\n\n    return data[cols]\n\n\ndef _print_df(df):\n\n    print(tabulate(df, headers=df.columns,\n                   showindex=False,\n                   tablefmt='pipe'))\n\n\ndef print_data():\n\n    cnn_data = _load_data('results/cnn_results.txt',\n                          ['residual',\n                           'nonlinearity',\n                           'loss',\n                           'num_layers',\n                           'kernel_width',\n                           'dilation',\n                           'embedding_dim'])\n    _print_df(cnn_data[:5])\n\n    lstm_data = _load_data('results/lstm_results.txt')\n\n    _print_df(lstm_data[:5])\n\n    pooling_data = _load_data('results/pooling_results.txt')\n\n    _print_df(pooling_data[:5])\n\n\nif __name__ == '__main__':\n    print_data()\n"""
examples/bloom_embeddings/performance.py,1,"b'import os\nimport pickle\nimport time\n\nimport numpy as np\n\nimport torch\n\nfrom spotlight.layers import BloomEmbedding, ScaledEmbedding\nfrom spotlight.factorization.implicit import ImplicitFactorizationModel\nfrom spotlight.factorization.representations import BilinearNet\nfrom spotlight.sequence.implicit import ImplicitSequenceModel\nfrom spotlight.sequence.representations import LSTMNet\n\nfrom spotlight.datasets.movielens import get_movielens_dataset\n\n\nCUDA = torch.cuda.is_available()\nEMBEDDING_DIM = 64\nN_ITER = 2\nNUM_HASH_FUNCTIONS = 4\n\n\ndef time_fitting(model, data, repetitions=2):\n\n    timings = []\n\n    # Warm-up epoch\n    model.fit(data)\n\n    for _ in range(repetitions):\n        start_time = time.time()\n        model.fit(data)\n        timings.append(time.time() - start_time)\n\n    print(min(timings))\n\n    return min(timings)\n\n\ndef factorization_model(num_embeddings, bloom):\n\n    if bloom:\n        user_embeddings = BloomEmbedding(num_embeddings, EMBEDDING_DIM,\n                                         num_hash_functions=NUM_HASH_FUNCTIONS)\n        item_embeddings = BloomEmbedding(num_embeddings, EMBEDDING_DIM,\n                                         num_hash_functions=NUM_HASH_FUNCTIONS)\n    else:\n        user_embeddings = ScaledEmbedding(num_embeddings, EMBEDDING_DIM)\n        item_embeddings = ScaledEmbedding(num_embeddings, EMBEDDING_DIM)\n\n    network = BilinearNet(num_embeddings,\n                          num_embeddings,\n                          user_embedding_layer=user_embeddings,\n                          item_embedding_layer=item_embeddings)\n\n    model = ImplicitFactorizationModel(loss=\'adaptive_hinge\',\n                                       n_iter=N_ITER,\n                                       embedding_dim=EMBEDDING_DIM,\n                                       batch_size=2048,\n                                       learning_rate=1e-2,\n                                       l2=1e-6,\n                                       representation=network,\n                                       use_cuda=CUDA)\n\n    return model\n\n\ndef sequence_model(num_embeddings, bloom):\n\n    if bloom:\n        item_embeddings = BloomEmbedding(num_embeddings, EMBEDDING_DIM,\n                                         num_hash_functions=NUM_HASH_FUNCTIONS)\n    else:\n        item_embeddings = ScaledEmbedding(num_embeddings, EMBEDDING_DIM)\n\n    network = LSTMNet(num_embeddings, EMBEDDING_DIM,\n                      item_embedding_layer=item_embeddings)\n\n    model = ImplicitSequenceModel(loss=\'adaptive_hinge\',\n                                  n_iter=N_ITER,\n                                  batch_size=512,\n                                  learning_rate=1e-3,\n                                  l2=1e-2,\n                                  representation=network,\n                                  use_cuda=CUDA)\n\n    return model\n\n\ndef get_sequence_data():\n\n    dataset = get_movielens_dataset(\'1M\')\n    max_sequence_length = 200\n    min_sequence_length = 20\n    data = dataset.to_sequence(max_sequence_length=max_sequence_length,\n                               min_sequence_length=min_sequence_length,\n                               step_size=max_sequence_length)\n    print(data.sequences.shape)\n\n    return data\n\n\ndef get_factorization_data():\n\n    dataset = get_movielens_dataset(\'1M\')\n\n    return dataset\n\n\ndef embedding_size_scalability():\n\n    sequence_data = get_sequence_data()\n    factorization_data = get_factorization_data()\n\n    embedding_dims = (1e4,\n                      1e4 * 5,\n                      1e5,\n                      1e5 * 5,\n                      1e6,\n                      1e6 * 5)\n\n    bloom_sequence = np.array([time_fitting(sequence_model(int(dim), True),\n                                            sequence_data)\n                               for dim in embedding_dims])\n    baseline_sequence = np.array([time_fitting(sequence_model(int(dim), False),\n                                               sequence_data)\n                                  for dim in embedding_dims])\n    sequence_ratio = bloom_sequence / baseline_sequence\n\n    print(\'Sequence ratio {}\'.format(sequence_ratio))\n\n    bloom_factorization = np.array([time_fitting(factorization_model(int(dim), True),\n                                                 factorization_data)\n                                    for dim in embedding_dims])\n    baseline_factorization = np.array([time_fitting(factorization_model(int(dim), False),\n                                                    factorization_data)\n                                       for dim in embedding_dims])\n    factorization_ratio = bloom_factorization / baseline_factorization\n\n    print(\'Factorization ratio {}\'.format(factorization_ratio))\n\n    return np.array(embedding_dims), sequence_ratio, factorization_ratio\n\n\ndef plot(dims, sequence, factorization):\n\n    import matplotlib\n    matplotlib.use(\'Agg\')  # NOQA\n    import matplotlib.pyplot as plt\n    import seaborn as sns\n\n    sns.set_style(""darkgrid"")\n\n    plt.ylabel(""Speed improvement"")\n    plt.xlabel(""Size of embedding layers"")\n    plt.title(""Fitting speed (1.0 = no change)"")\n    plt.xscale(\'log\')\n\n    plt.plot(dims,\n             1.0 / sequence,\n             label=\'Sequence model\')\n    plt.plot(dims,\n             1.0 / factorization,\n             label=\'Factorization model\')\n    plt.legend(loc=\'lower right\')\n    plt.savefig(\'speed.png\')\n    plt.close()\n\n\nif __name__ == \'__main__\':\n\n    fname = \'performance.pickle\'\n\n    if not os.path.exists(fname):\n        dims, sequence, factorization = embedding_size_scalability()\n        with open(fname, \'wb\') as fle:\n            pickle.dump((dims, sequence, factorization), fle)\n\n    with open(fname, \'rb\') as fle:\n        (dims, sequence, factorization) = pickle.load(fle)\n\n    plot(dims, sequence, factorization)\n'"
examples/bloom_embeddings/plot.py,0,"b'import argparse\n\nimport pandas as pd\n\nimport matplotlib\nmatplotlib.use(\'Agg\')  # NOQA\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nfrom example import Results\n\n\ndef process_results(results, verbose=False):\n\n    baseline = results.best_baseline()\n\n    def like_baseline(x):\n        for key in (\'n_iter\',\n                    \'batch_size\',\n                    \'l2\',\n                    \'learning_rate\',\n                    \'loss\',\n                    \'embedding_dim\'):\n            if x[key] != baseline[key]:\n                return False\n\n        return True\n\n    data = pd.DataFrame([x for x in results\n                         if like_baseline(x)])\n\n    best = (data.sort_values(\'test_mrr\', ascending=False)\n            .groupby(\'compression_ratio\', as_index=False).first())\n\n    # Normalize per iteration\n    best[\'elapsed\'] = best[\'elapsed\'] / best[\'n_iter\']\n\n    if verbose:\n        print(best)\n\n    baseline_mrr = (best[best[\'compression_ratio\'] == 1.0]\n                    [\'validation_mrr\'].values[0])\n    baseline_time = (best[best[\'compression_ratio\'] == 1.0]\n                     [\'elapsed\'].values[0])\n\n    compression_ratio = best[\'compression_ratio\'].values\n    mrr = best[\'validation_mrr\'].values / baseline_mrr\n    elapsed = best[\'elapsed\'].values / baseline_time\n\n    return compression_ratio[:-1], mrr[:-1], elapsed[:-1]\n\n\ndef plot_results(model, movielens, amazon):\n\n    sns.set_style(""darkgrid"")\n\n    for name, result in ((\'Movielens\',\n                          movielens), (\'Amazon\', amazon)):\n\n        print(\'Dataset: {}\'.format(name))\n\n        (compression_ratio,\n         mrr,\n         elapsed) = process_results(result, verbose=True)\n\n        plt.plot(compression_ratio, mrr,\n                 label=name)\n\n    plt.ylabel(""MRR ratio to baseline"")\n    plt.xlabel(""Compression ratio"")\n    plt.title(""Compression ratio vs MRR ratio"")\n\n    plt.legend(loc=\'lower right\')\n    plt.savefig(\'{}_plot.png\'.format(model))\n    plt.close()\n\n    for name, result in ((\'Movielens\',\n                          movielens), (\'Amazon\', amazon)):\n\n        (compression_ratio,\n         mrr,\n         elapsed) = process_results(result)\n\n        plt.plot(compression_ratio, elapsed,\n                 label=name)\n\n    plt.ylabel(""Time ratio to baseline"")\n    plt.xlabel(""Compression ratio"")\n    plt.title(""Compression ratio vs time ratio"")\n    plt.legend(loc=\'lower right\')\n\n    plt.savefig(\'{}_time.png\'.format(model))\n    plt.close()\n\n\nif __name__ == \'__main__\':\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'model\', type=str)\n\n    args = parser.parse_args()\n\n    plot_results(args.model,\n                 Results(\'movielens_{}_results.txt\'.format(args.model)),\n                 Results(\'amazon_{}_results.txt\'.format(args.model)))\n'"
examples/movielens_sequence/helpers.py,0,"b""import pandas as pd\n\nfrom tabulate import tabulate\n\n\ndef _load_data(filename, columns=None):\n\n    data = pd.read_json(filename, lines=True)\n    data = data.sort_values('validation_mrr', ascending=False)\n\n    mrr_cols = ['validation_mrr', 'test_mrr']\n\n    if columns is None:\n        columns = [x for x in data.columns if\n                   (x not in mrr_cols and x != 'hash')]\n\n    cols = data.columns\n    cols = mrr_cols + columns\n\n    return data[cols]\n\n\ndef _print_df(df):\n\n    print(tabulate(df, headers=df.columns,\n                   showindex=False,\n                   tablefmt='pipe'))\n\n\ndef print_data():\n\n    cnn_data = _load_data('results/cnn_results.txt',\n                          ['residual',\n                           'nonlinearity',\n                           'loss',\n                           'num_layers',\n                           'kernel_width',\n                           'dilation',\n                           'embedding_dim'])\n    _print_df(cnn_data[:5])\n\n    lstm_data = _load_data('results/lstm_results.txt')\n\n    _print_df(lstm_data[:5])\n\n    pooling_data = _load_data('results/pooling_results.txt')\n\n    _print_df(pooling_data[:5])\n\n\nif __name__ == '__main__':\n    print_data()\n"""
examples/movielens_sequence/movielens_sequence.py,0,"b""import hashlib\nimport json\nimport os\nimport shutil\nimport sys\n\nimport numpy as np\n\nfrom sklearn.model_selection import ParameterSampler\n\nfrom spotlight.datasets.movielens import get_movielens_dataset\nfrom spotlight.cross_validation import user_based_train_test_split\nfrom spotlight.sequence.implicit import ImplicitSequenceModel\nfrom spotlight.sequence.representations import CNNNet\nfrom spotlight.evaluation import sequence_mrr_score\n\n\nCUDA = (os.environ.get('CUDA') is not None or\n        shutil.which('nvidia-smi') is not None)\n\nNUM_SAMPLES = 100\n\nLEARNING_RATES = [1e-3, 1e-2, 5 * 1e-2, 1e-1]\nLOSSES = ['bpr', 'hinge', 'adaptive_hinge', 'pointwise']\nBATCH_SIZE = [8, 16, 32, 256]\nEMBEDDING_DIM = [8, 16, 32, 64, 128, 256]\nN_ITER = list(range(5, 20))\nL2 = [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 0.0]\n\n\nclass Results:\n\n    def __init__(self, filename):\n\n        self._filename = filename\n\n        open(self._filename, 'a+')\n\n    def _hash(self, x):\n\n        return hashlib.md5(json.dumps(x, sort_keys=True).encode('utf-8')).hexdigest()\n\n    def save(self, hyperparams, test_mrr, validation_mrr):\n\n        result = {'test_mrr': test_mrr,\n                  'validation_mrr': validation_mrr,\n                  'hash': self._hash(hyperparams)}\n        result.update(hyperparams)\n\n        with open(self._filename, 'a+') as out:\n            out.write(json.dumps(result) + '\\n')\n\n    def best(self):\n\n        results = sorted([x for x in self],\n                         key=lambda x: -x['test_mrr'])\n\n        if results:\n            return results[0]\n        else:\n            return None\n\n    def __getitem__(self, hyperparams):\n\n        params_hash = self._hash(hyperparams)\n\n        with open(self._filename, 'r+') as fle:\n            for line in fle:\n                datum = json.loads(line)\n\n                if datum['hash'] == params_hash:\n                    del datum['hash']\n                    return datum\n\n        raise KeyError\n\n    def __contains__(self, x):\n\n        try:\n            self[x]\n            return True\n        except KeyError:\n            return False\n\n    def __iter__(self):\n\n        with open(self._filename, 'r+') as fle:\n            for line in fle:\n                datum = json.loads(line)\n\n                del datum['hash']\n\n                yield datum\n\n\ndef sample_cnn_hyperparameters(random_state, num):\n\n    space = {\n        'n_iter': N_ITER,\n        'batch_size': BATCH_SIZE,\n        'l2': L2,\n        'learning_rate': LEARNING_RATES,\n        'loss': LOSSES,\n        'embedding_dim': EMBEDDING_DIM,\n        'kernel_width': [3, 5, 7],\n        'num_layers': list(range(1, 10)),\n        'dilation_multiplier': [1, 2],\n        'nonlinearity': ['tanh', 'relu'],\n        'residual': [True, False]\n    }\n\n    sampler = ParameterSampler(space,\n                               n_iter=num,\n                               random_state=random_state)\n\n    for params in sampler:\n        params['dilation'] = list(params['dilation_multiplier'] ** (i % 8)\n                                  for i in range(params['num_layers']))\n\n        yield params\n\n\ndef sample_lstm_hyperparameters(random_state, num):\n\n    space = {\n        'n_iter': N_ITER,\n        'batch_size': BATCH_SIZE,\n        'l2': L2,\n        'learning_rate': LEARNING_RATES,\n        'loss': LOSSES,\n        'embedding_dim': EMBEDDING_DIM,\n    }\n\n    sampler = ParameterSampler(space,\n                               n_iter=num,\n                               random_state=random_state)\n\n    for params in sampler:\n\n        yield params\n\n\ndef sample_pooling_hyperparameters(random_state, num):\n\n    space = {\n        'n_iter': N_ITER,\n        'batch_size': BATCH_SIZE,\n        'l2': L2,\n        'learning_rate': LEARNING_RATES,\n        'loss': LOSSES,\n        'embedding_dim': EMBEDDING_DIM,\n    }\n\n    sampler = ParameterSampler(space,\n                               n_iter=num,\n                               random_state=random_state)\n\n    for params in sampler:\n\n        yield params\n\n\ndef evaluate_cnn_model(hyperparameters, train, test, validation, random_state):\n\n    h = hyperparameters\n\n    net = CNNNet(train.num_items,\n                 embedding_dim=h['embedding_dim'],\n                 kernel_width=h['kernel_width'],\n                 dilation=h['dilation'],\n                 num_layers=h['num_layers'],\n                 nonlinearity=h['nonlinearity'],\n                 residual_connections=h['residual'])\n\n    model = ImplicitSequenceModel(loss=h['loss'],\n                                  representation=net,\n                                  batch_size=h['batch_size'],\n                                  learning_rate=h['learning_rate'],\n                                  l2=h['l2'],\n                                  n_iter=h['n_iter'],\n                                  use_cuda=CUDA,\n                                  random_state=random_state)\n\n    model.fit(train, verbose=True)\n\n    test_mrr = sequence_mrr_score(model, test)\n    val_mrr = sequence_mrr_score(model, validation)\n\n    return test_mrr, val_mrr\n\n\ndef evaluate_lstm_model(hyperparameters, train, test, validation, random_state):\n\n    h = hyperparameters\n\n    model = ImplicitSequenceModel(loss=h['loss'],\n                                  representation='lstm',\n                                  batch_size=h['batch_size'],\n                                  learning_rate=h['learning_rate'],\n                                  l2=h['l2'],\n                                  n_iter=h['n_iter'],\n                                  use_cuda=CUDA,\n                                  random_state=random_state)\n\n    model.fit(train, verbose=True)\n\n    test_mrr = sequence_mrr_score(model, test)\n    val_mrr = sequence_mrr_score(model, validation)\n\n    return test_mrr, val_mrr\n\n\ndef evaluate_pooling_model(hyperparameters, train, test, validation, random_state):\n\n    h = hyperparameters\n\n    model = ImplicitSequenceModel(loss=h['loss'],\n                                  representation='pooling',\n                                  batch_size=h['batch_size'],\n                                  learning_rate=h['learning_rate'],\n                                  l2=h['l2'],\n                                  n_iter=h['n_iter'],\n                                  use_cuda=CUDA,\n                                  random_state=random_state)\n\n    model.fit(train, verbose=True)\n\n    test_mrr = sequence_mrr_score(model, test)\n    val_mrr = sequence_mrr_score(model, validation)\n\n    return test_mrr, val_mrr\n\n\ndef run(train, test, validation, ranomd_state, model_type):\n\n    results = Results('{}_results.txt'.format(model_type))\n\n    best_result = results.best()\n\n    if model_type == 'pooling':\n        eval_fnc, sample_fnc = (evaluate_pooling_model,\n                                sample_pooling_hyperparameters)\n    elif model_type == 'cnn':\n        eval_fnc, sample_fnc = (evaluate_cnn_model,\n                                sample_cnn_hyperparameters)\n    elif model_type == 'lstm':\n        eval_fnc, sample_fnc = (evaluate_lstm_model,\n                                sample_lstm_hyperparameters)\n    else:\n        raise ValueError('Unknown model type')\n\n    if best_result is not None:\n        print('Best {} result: {}'.format(model_type, results.best()))\n\n    for hyperparameters in sample_fnc(random_state, NUM_SAMPLES):\n\n        if hyperparameters in results:\n            continue\n\n        print('Evaluating {}'.format(hyperparameters))\n\n        (test_mrr, val_mrr) = eval_fnc(hyperparameters,\n                                       train,\n                                       test,\n                                       validation,\n                                       random_state)\n\n        print('Test MRR {} val MRR {}'.format(\n            test_mrr.mean(), val_mrr.mean()\n        ))\n\n        results.save(hyperparameters, test_mrr.mean(), val_mrr.mean())\n\n    return results\n\n\nif __name__ == '__main__':\n\n    max_sequence_length = 200\n    min_sequence_length = 20\n    step_size = 200\n    random_state = np.random.RandomState(100)\n\n    dataset = get_movielens_dataset('1M')\n\n    train, rest = user_based_train_test_split(dataset,\n                                              random_state=random_state)\n    test, validation = user_based_train_test_split(rest,\n                                                   test_percentage=0.5,\n                                                   random_state=random_state)\n    train = train.to_sequence(max_sequence_length=max_sequence_length,\n                              min_sequence_length=min_sequence_length,\n                              step_size=step_size)\n    test = test.to_sequence(max_sequence_length=max_sequence_length,\n                            min_sequence_length=min_sequence_length,\n                            step_size=step_size)\n    validation = validation.to_sequence(max_sequence_length=max_sequence_length,\n                                        min_sequence_length=min_sequence_length,\n                                        step_size=step_size)\n\n    mode = sys.argv[1]\n\n    run(train, test, validation, random_state, mode)\n"""
spotlight/datasets/__init__.py,0,"b'""""""\nDatasets.\n""""""\n'"
spotlight/datasets/_transport.py,0,"b""import os\n\nimport requests\n\n\nDATA_DIR = os.path.join(os.path.expanduser('~'),\n                        'spotlight_data')\n\n\ndef create_data_dir(path):\n\n    if not os.path.isdir(path):\n        os.makedirs(path)\n\n\ndef download(url, dest_path, data_dir=DATA_DIR):\n\n    req = requests.get(url, stream=True)\n    req.raise_for_status()\n\n    with open(dest_path, 'wb') as fd:\n        for chunk in req.iter_content(chunk_size=2**20):\n            fd.write(chunk)\n\n\ndef get_data(url, dest_subdir, dest_filename, download_if_missing=True):\n\n    data_dir = os.path.join(os.path.abspath(DATA_DIR), dest_subdir)\n\n    create_data_dir(data_dir)\n\n    dest_path = os.path.join(data_dir, dest_filename)\n\n    if not os.path.isfile(dest_path):\n        if download_if_missing:\n            download(url, dest_path)\n        else:\n            raise IOError('Dataset missing.')\n\n    return dest_path\n"""
spotlight/datasets/amazon.py,0,"b'""""""\nUtilities for fetching Amazon datasets\n""""""\n\nimport h5py\n\nimport numpy as np\n\nfrom spotlight.datasets import _transport\nfrom spotlight.interactions import Interactions\n\n\ndef _download_amazon():\n\n    extension = \'.hdf5\'\n    url = (\'https://github.com/maciejkula/recommender_datasets/\'\n           \'releases/download\')\n    version = \'0.1.0\'\n\n    path = _transport.get_data(\'/\'.join((url,\n                                         version,\n                                         \'amazon_co_purchasing\' + extension)),\n                               \'amazon\',\n                               \'amazon_co_purchasing{}\'.format(extension))\n\n    with h5py.File(path, \'r\') as data:\n        return (data[\'/user_id\'][:],\n                data[\'/item_id\'][:],\n                data[\'/rating\'][:],\n                data[\'/timestamp\'][:],\n                data[\'/features_item_id\'][:],\n                data[\'/features_feature_id\'][:])\n\n\ndef _filter_by_count(elements, min_count):\n\n    unique_elements, element_counts = np.unique(elements,\n                                                return_counts=True)\n\n    return unique_elements[element_counts >= min_count]\n\n\ndef _build_contiguous_map(elements):\n\n    return dict(zip(elements, np.arange(len(elements)) + 1))\n\n\ndef _map(elements, mapping):\n\n    for idx, elem in enumerate(elements):\n        elements[idx] = mapping[elem]\n\n    return elements\n\n\ndef get_amazon_dataset(min_user_interactions=10, min_item_interactions=10):\n    """"""\n    Data on Amazon products from the SNAP `archive\n    <https://snap.stanford.edu/data/amazon-meta.html>`_[1]_.\n\n    The dataset contains almost 8 million ratings given to 550,000 Amazon products:\n    interactions represent ratings given to users to products they have reviewed.\n\n    Compared to the Movielens dataset, the Amazon dataset is relatively sparse,\n    and the number of products represented is much higher. It may therefore be\n    more useful for prototyping models for sparse and high-dimensional settings.\n\n    Parameters\n    ----------\n\n    min_user_interactions: int, optional\n        Exclude observations from users that have given fewer ratings.\n    min_item_interactions: int, optional\n        Exclude observations from items that have given fewer ratings.\n\n    Notes\n    -----\n\n    You may want to reduce the dimensionality of the dataset by excluding users\n    and items with particularly few interactions. Note that the exclusions are\n    applied independently, so it is possible for users and items in the remaining\n    set to have fewer interactions than specified via the parameters.\n\n    References\n    ----------\n\n    .. [1] J. Leskovec, L. Adamic and B. Adamic.\n       The Dynamics of Viral Marketing.\n       ACM Transactions on the Web (ACM TWEB), 1(1), 2007.\n    """"""\n\n    (user_ids, item_ids, ratings,\n     timestamps, feature_item_ids,\n     feature_ids) = _download_amazon()\n\n    retain_user_ids = _filter_by_count(user_ids, min_user_interactions)\n    retain_item_ids = _filter_by_count(item_ids, min_item_interactions)\n\n    retain = np.logical_and(np.in1d(user_ids, retain_user_ids),\n                            np.in1d(item_ids, retain_item_ids))\n\n    user_ids = user_ids[retain]\n    item_ids = item_ids[retain]\n    ratings = ratings[retain]\n    timestamps = timestamps[retain]\n\n    retain_user_map = _build_contiguous_map(retain_user_ids)\n    retain_item_map = _build_contiguous_map(retain_item_ids)\n\n    user_ids = _map(user_ids, retain_user_map)\n    item_ids = _map(item_ids, retain_item_map)\n\n    return Interactions(user_ids,\n                        item_ids,\n                        ratings=ratings,\n                        timestamps=timestamps,\n                        num_users=len(retain_user_map) + 1,\n                        num_items=len(retain_item_map) + 1)\n'"
spotlight/datasets/goodbooks.py,0,"b'""""""\nUtilities for fetching the Goodbooks-10K dataset [1]_.\n\nReferences\n----------\n\n.. [1] https://github.com/zygmuntz/goodbooks-10k\n""""""\n\nimport h5py\n\nimport numpy as np\n\nfrom spotlight.datasets import _transport\nfrom spotlight.interactions import Interactions\n\n\ndef _get_dataset():\n\n    path = _transport.get_data(\'https://github.com/zygmuntz/goodbooks-10k/\'\n                               \'releases/download/v1.0/goodbooks-10k.hdf5\',\n                               \'goodbooks\',\n                               \'goodbooks.hdf5\')\n\n    with h5py.File(path, \'r\') as data:\n        return (data[\'ratings\'][:, 0],\n                data[\'ratings\'][:, 1],\n                data[\'ratings\'][:, 2].astype(np.float32),\n                np.arange(len(data[\'ratings\']), dtype=np.int32))\n\n\ndef get_goodbooks_dataset():\n    """"""\n    Download and return the goodbooks-10K dataset [2]_.\n\n    Returns\n    -------\n\n    Interactions: :class:`spotlight.interactions.Interactions`\n        instance of the interactions class\n\n    References\n    ----------\n\n    .. [2] https://github.com/zygmuntz/goodbooks-10k\n    """"""\n\n    return Interactions(*_get_dataset())\n'"
spotlight/datasets/movielens.py,0,"b'""""""\nUtilities for fetching the Movielens datasets [1]_.\n\nReferences\n----------\n\n.. [1] https://grouplens.org/datasets/movielens/\n""""""\n\nimport os\n\nimport h5py\n\nfrom spotlight.datasets import _transport\nfrom spotlight.interactions import Interactions\n\nVARIANTS = (\'100K\',\n            \'1M\',\n            \'10M\',\n            \'20M\')\n\n\nURL_PREFIX = (\'https://github.com/maciejkula/recommender_datasets/\'\n              \'releases/download\')\nVERSION = \'v0.2.0\'\n\n\ndef _get_movielens(dataset):\n\n    extension = \'.hdf5\'\n\n    path = _transport.get_data(\'/\'.join((URL_PREFIX,\n                                         VERSION,\n                                         dataset + extension)),\n                               os.path.join(\'movielens\', VERSION),\n                               \'movielens_{}{}\'.format(dataset,\n                                                       extension))\n\n    with h5py.File(path, \'r\') as data:\n        return (data[\'/user_id\'][:],\n                data[\'/item_id\'][:],\n                data[\'/rating\'][:],\n                data[\'/timestamp\'][:])\n\n\ndef get_movielens_dataset(variant=\'100K\'):\n    """"""\n    Download and return one of the Movielens datasets.\n\n    Parameters\n    ----------\n\n    variant: string, optional\n         String specifying which of the Movielens datasets\n         to download. One of (\'100K\', \'1M\', \'10M\', \'20M\').\n\n    Returns\n    -------\n\n    Interactions: :class:`spotlight.interactions.Interactions`\n        instance of the interactions class\n    """"""\n\n    if variant not in VARIANTS:\n        raise ValueError(\'Variant must be one of {}, \'\n                         \'got {}.\'.format(VARIANTS, variant))\n\n    url = \'movielens_{}\'.format(variant)\n\n    return Interactions(*_get_movielens(url))\n'"
spotlight/datasets/synthetic.py,0,"b'""""""\nModule containing functions for generating synthetic\ndatasets with known properties, for model testing and\nexperimentation.\n""""""\n\nimport numpy as np\n\nfrom spotlight.interactions import Interactions\n\n\ndef _build_transition_matrix(num_items,\n                             concentration_parameter,\n                             random_state,\n                             atol=0.001):\n\n    def _is_doubly_stochastic(matrix, atol):\n\n        return (np.all(np.abs(1.0 - matrix.sum(axis=0)) < atol) and\n                np.all(np.abs(1.0 - matrix.sum(axis=1)) < atol))\n\n    transition_matrix = random_state.dirichlet(\n        np.repeat(concentration_parameter, num_items),\n        num_items)\n\n    for _ in range(100):\n\n        if _is_doubly_stochastic(transition_matrix, atol):\n            break\n\n        transition_matrix /= transition_matrix.sum(axis=0)\n        transition_matrix /= transition_matrix.sum(1)[:, np.newaxis]\n\n    return transition_matrix\n\n\ndef _generate_sequences(num_steps,\n                        transition_matrix,\n                        order,\n                        random_state):\n\n    elements = []\n\n    num_states = transition_matrix.shape[0]\n\n    transition_matrix = np.cumsum(transition_matrix,\n                                  axis=1)\n\n    rvs = random_state.rand(num_steps)\n    state = random_state.randint(transition_matrix.shape[0], size=order,\n                                 dtype=np.int64)\n\n    for rv in rvs:\n\n        row = transition_matrix[state].mean(axis=0)\n        new_state = min(num_states - 1,\n                        np.searchsorted(row, rv))\n\n        state[:-1] = state[1:]\n        state[-1] = new_state\n\n        elements.append(new_state)\n\n    return np.array(elements, dtype=np.int32)\n\n\ndef generate_sequential(num_users=100,\n                        num_items=1000,\n                        num_interactions=10000,\n                        concentration_parameter=0.1,\n                        order=3,\n                        random_state=None):\n    """"""\n    Generate a dataset of user-item interactions where sequential\n    information matters.\n\n    The interactions are generated by a n-th order Markov chain with\n    a uniform stationary distribution, where transition probabilities\n    are given by doubly-stochastic transition matrix. For n-th order chains,\n    transition probabilities are a convex combination of the transition\n    probabilities of the last n states in the chain.\n\n    The transition matrix is sampled from a Dirichlet distribution described\n    by a constant concentration parameter. Concentration parameters closer\n    to zero generate more predictable sequences.\n\n    Parameters\n    ----------\n\n    num_users: int, optional\n        number of users in the dataset\n    num_items: int, optional\n        number of items (Markov states) in the dataset\n    num_interactions: int, optional\n        number of interactions to generate\n    concentration_parameter: float, optional\n        Controls how predictable the sequence is. Values\n        closer to zero give more predictable sequences.\n    order: int, optional\n        order of the Markov chain\n    random_state: numpy.random.RandomState, optional\n        random state used to generate the data\n\n    Returns\n    -------\n\n    Interactions: :class:`spotlight.interactions.Interactions`\n        instance of the interactions class\n    """"""\n\n    if random_state is None:\n        random_state = np.random.RandomState()\n\n    transition_matrix = _build_transition_matrix(\n        num_items - 1,\n        concentration_parameter,\n        random_state)\n\n    user_ids = np.sort(random_state.randint(0,\n                                            num_users,\n                                            num_interactions,\n                                            dtype=np.int32))\n    item_ids = _generate_sequences(num_interactions,\n                                   transition_matrix,\n                                   order,\n                                   random_state) + 1\n    timestamps = np.arange(len(user_ids), dtype=np.int32)\n    ratings = np.ones(len(user_ids), dtype=np.float32)\n\n    return Interactions(user_ids,\n                        item_ids,\n                        ratings=ratings,\n                        timestamps=timestamps,\n                        num_users=num_users,\n                        num_items=num_items)\n'"
spotlight/factorization/__init__.py,0,b''
spotlight/factorization/_components.py,2,"b'import numpy as np\n\nimport torch\n\nfrom spotlight.torch_utils import gpu\n\n\ndef _predict_process_ids(user_ids, item_ids, num_items, use_cuda):\n\n    if item_ids is None:\n        item_ids = np.arange(num_items, dtype=np.int64)\n\n    if np.isscalar(user_ids):\n        user_ids = np.array(user_ids, dtype=np.int64)\n\n    user_ids = torch.from_numpy(user_ids.reshape(-1, 1).astype(np.int64))\n    item_ids = torch.from_numpy(item_ids.reshape(-1, 1).astype(np.int64))\n\n    if item_ids.size()[0] != user_ids.size(0):\n        user_ids = user_ids.expand(item_ids.size())\n\n    user_var = gpu(user_ids, use_cuda)\n    item_var = gpu(item_ids, use_cuda)\n\n    return user_var.squeeze(), item_var.squeeze()\n'"
spotlight/factorization/explicit.py,7,"b'""""""\nFactorization models for explicit feedback problems.\n""""""\n\nimport numpy as np\n\nimport torch\n\nimport torch.optim as optim\n\nfrom spotlight.helpers import _repr_model\nfrom spotlight.factorization._components import _predict_process_ids\nfrom spotlight.factorization.representations import BilinearNet\nfrom spotlight.losses import (poisson_loss,\n                              regression_loss,\n                              logistic_loss)\n\nfrom spotlight.torch_utils import cpu, gpu, minibatch, set_seed, shuffle\n\n\nclass ExplicitFactorizationModel(object):\n    """"""\n    An explicit feedback matrix factorization model. Uses a classic\n    matrix factorization [1]_ approach, with latent vectors used\n    to represent both users and items. Their dot product gives the\n    predicted score for a user-item pair.\n\n    The latent representation is given by\n    :class:`spotlight.factorization.representations.BilinearNet`.\n\n    .. [1] Koren, Yehuda, Robert Bell, and Chris Volinsky.\n       ""Matrix factorization techniques for recommender systems.""\n       Computer 42.8 (2009).\n\n    Parameters\n    ----------\n\n    loss: string, optional\n        One of \'regression\', \'poisson\', \'logistic\'\n        corresponding to losses from :class:`spotlight.losses`.\n    embedding_dim: int, optional\n        Number of embedding dimensions to use for users and items.\n    n_iter: int, optional\n        Number of iterations to run.\n    batch_size: int, optional\n        Minibatch size.\n    l2: float, optional\n        L2 loss penalty.\n    learning_rate: float, optional\n        Initial learning rate.\n    optimizer_func: function, optional\n        Function that takes in module parameters as the first argument and\n        returns an instance of a PyTorch optimizer. Overrides l2 and learning\n        rate if supplied. If no optimizer supplied, then use ADAM by default.\n    use_cuda: boolean, optional\n        Run the model on a GPU.\n    representation: a representation module, optional\n        If supplied, will override default settings and be used as the\n        main network module in the model. Intended to be used as an escape\n        hatch when you want to reuse the model\'s training functions but\n        want full freedom to specify your network topology.\n    sparse: boolean, optional\n        Use sparse gradients for embedding layers.\n    random_state: instance of numpy.random.RandomState, optional\n        Random state to use when fitting.\n    """"""\n\n    def __init__(self,\n                 loss=\'regression\',\n                 embedding_dim=32,\n                 n_iter=10,\n                 batch_size=256,\n                 l2=0.0,\n                 learning_rate=1e-2,\n                 optimizer_func=None,\n                 use_cuda=False,\n                 representation=None,\n                 sparse=False,\n                 random_state=None):\n\n        assert loss in (\'regression\',\n                        \'poisson\',\n                        \'logistic\')\n\n        self._loss = loss\n        self._embedding_dim = embedding_dim\n        self._n_iter = n_iter\n        self._learning_rate = learning_rate\n        self._batch_size = batch_size\n        self._l2 = l2\n        self._use_cuda = use_cuda\n        self._representation = representation\n        self._sparse = sparse\n        self._optimizer_func = optimizer_func\n        self._random_state = random_state or np.random.RandomState()\n\n        self._num_users = None\n        self._num_items = None\n        self._net = None\n        self._optimizer = None\n        self._loss_func = None\n\n        set_seed(self._random_state.randint(-10**8, 10**8),\n                 cuda=self._use_cuda)\n\n    def __repr__(self):\n\n        return _repr_model(self)\n\n    @property\n    def _initialized(self):\n        return self._net is not None\n\n    def _initialize(self, interactions):\n\n        (self._num_users,\n         self._num_items) = (interactions.num_users,\n                             interactions.num_items)\n\n        if self._representation is not None:\n            self._net = gpu(self._representation,\n                            self._use_cuda)\n        else:\n            self._net = gpu(\n                BilinearNet(self._num_users,\n                            self._num_items,\n                            self._embedding_dim,\n                            sparse=self._sparse),\n                self._use_cuda\n            )\n\n        if self._optimizer_func is None:\n            self._optimizer = optim.Adam(\n                self._net.parameters(),\n                weight_decay=self._l2,\n                lr=self._learning_rate\n            )\n        else:\n            self._optimizer = self._optimizer_func(self._net.parameters())\n\n        if self._loss == \'regression\':\n            self._loss_func = regression_loss\n        elif self._loss == \'poisson\':\n            self._loss_func = poisson_loss\n        elif self._loss == \'logistic\':\n            self._loss_func = logistic_loss\n        else:\n            raise ValueError(\'Unknown loss: {}\'.format(self._loss))\n\n    def _check_input(self, user_ids, item_ids, allow_items_none=False):\n\n        if isinstance(user_ids, int):\n            user_id_max = user_ids\n        else:\n            user_id_max = user_ids.max()\n\n        if user_id_max >= self._num_users:\n            raise ValueError(\'Maximum user id greater \'\n                             \'than number of users in model.\')\n\n        if allow_items_none and item_ids is None:\n            return\n\n        if isinstance(item_ids, int):\n            item_id_max = item_ids\n        else:\n            item_id_max = item_ids.max()\n\n        if item_id_max >= self._num_items:\n            raise ValueError(\'Maximum item id greater \'\n                             \'than number of items in model.\')\n\n    def fit(self, interactions, verbose=False):\n        """"""\n        Fit the model.\n\n        When called repeatedly, model fitting will resume from\n        the point at which training stopped in the previous fit\n        call.\n\n        Parameters\n        ----------\n\n        interactions: :class:`spotlight.interactions.Interactions`\n            The input dataset. Must have ratings.\n\n        verbose: bool\n            Output additional information about current epoch and loss.\n        """"""\n\n        user_ids = interactions.user_ids.astype(np.int64)\n        item_ids = interactions.item_ids.astype(np.int64)\n\n        if not self._initialized:\n            self._initialize(interactions)\n\n        self._check_input(user_ids, item_ids)\n\n        for epoch_num in range(self._n_iter):\n\n            users, items, ratings = shuffle(user_ids,\n                                            item_ids,\n                                            interactions.ratings,\n                                            random_state=self._random_state)\n\n            user_ids_tensor = gpu(torch.from_numpy(users),\n                                  self._use_cuda)\n            item_ids_tensor = gpu(torch.from_numpy(items),\n                                  self._use_cuda)\n            ratings_tensor = gpu(torch.from_numpy(ratings),\n                                 self._use_cuda)\n\n            epoch_loss = 0.0\n\n            for (minibatch_num,\n                 (batch_user,\n                  batch_item,\n                  batch_ratings)) in enumerate(minibatch(user_ids_tensor,\n                                                         item_ids_tensor,\n                                                         ratings_tensor,\n                                                         batch_size=self._batch_size)):\n\n                predictions = self._net(batch_user, batch_item)\n\n                if self._loss == \'poisson\':\n                    predictions = torch.exp(predictions)\n\n                self._optimizer.zero_grad()\n\n                loss = self._loss_func(batch_ratings, predictions)\n                epoch_loss += loss.item()\n\n                loss.backward()\n                self._optimizer.step()\n\n            epoch_loss /= minibatch_num + 1\n\n            if verbose:\n                print(\'Epoch {}: loss {}\'.format(epoch_num, epoch_loss))\n\n            if np.isnan(epoch_loss) or epoch_loss == 0.0:\n                raise ValueError(\'Degenerate epoch loss: {}\'\n                                 .format(epoch_loss))\n\n    def predict(self, user_ids, item_ids=None):\n        """"""\n        Make predictions: given a user id, compute the recommendation\n        scores for items.\n\n        Parameters\n        ----------\n\n        user_ids: int or array\n           If int, will predict the recommendation scores for this\n           user for all items in item_ids. If an array, will predict\n           scores for all (user, item) pairs defined by user_ids and\n           item_ids.\n        item_ids: array, optional\n            Array containing the item ids for which prediction scores\n            are desired. If not supplied, predictions for all items\n            will be computed.\n\n        Returns\n        -------\n\n        predictions: np.array\n            Predicted scores for all items in item_ids.\n        """"""\n\n        self._check_input(user_ids, item_ids, allow_items_none=True)\n        self._net.train(False)\n\n        user_ids, item_ids = _predict_process_ids(user_ids, item_ids,\n                                                  self._num_items,\n                                                  self._use_cuda)\n\n        out = self._net(user_ids, item_ids)\n\n        if self._loss == \'poisson\':\n            out = torch.exp(out)\n        elif self._loss == \'logistic\':\n            out = torch.sigmoid(out)\n\n        return cpu(out).detach().numpy().flatten()\n'"
spotlight/factorization/implicit.py,4,"b'""""""\nFactorization models for implicit feedback problems.\n""""""\n\nimport numpy as np\n\nimport torch\n\nimport torch.optim as optim\n\nfrom spotlight.helpers import _repr_model\nfrom spotlight.factorization._components import _predict_process_ids\nfrom spotlight.losses import (adaptive_hinge_loss,\n                              bpr_loss,\n                              hinge_loss,\n                              pointwise_loss)\nfrom spotlight.factorization.representations import BilinearNet\nfrom spotlight.sampling import sample_items\nfrom spotlight.torch_utils import cpu, gpu, minibatch, set_seed, shuffle\n\n\nclass ImplicitFactorizationModel(object):\n    """"""\n    An implicit feedback matrix factorization model. Uses a classic\n    matrix factorization [1]_ approach, with latent vectors used\n    to represent both users and items. Their dot product gives the\n    predicted score for a user-item pair.\n\n    The latent representation is given by\n    :class:`spotlight.factorization.representations.BilinearNet`.\n\n    The model is trained through negative sampling: for any known\n    user-item pair, one or more items are randomly sampled to act\n    as negatives (expressing a lack of preference by the user for\n    the sampled item).\n\n    .. [1] Koren, Yehuda, Robert Bell, and Chris Volinsky.\n       ""Matrix factorization techniques for recommender systems.""\n       Computer 42.8 (2009).\n\n    Parameters\n    ----------\n\n    loss: string, optional\n        One of \'pointwise\', \'bpr\', \'hinge\', or \'adaptive hinge\',\n        corresponding to losses from :class:`spotlight.losses`.\n    embedding_dim: int, optional\n        Number of embedding dimensions to use for users and items.\n    n_iter: int, optional\n        Number of iterations to run.\n    batch_size: int, optional\n        Minibatch size.\n    l2: float, optional\n        L2 loss penalty.\n    learning_rate: float, optional\n        Initial learning rate.\n    optimizer_func: function, optional\n        Function that takes in module parameters as the first argument and\n        returns an instance of a PyTorch optimizer. Overrides l2 and learning\n        rate if supplied. If no optimizer supplied, then use ADAM by default.\n    use_cuda: boolean, optional\n        Run the model on a GPU.\n    representation: a representation module, optional\n        If supplied, will override default settings and be used as the\n        main network module in the model. Intended to be used as an escape\n        hatch when you want to reuse the model\'s training functions but\n        want full freedom to specify your network topology.\n    sparse: boolean, optional\n        Use sparse gradients for embedding layers.\n    random_state: instance of numpy.random.RandomState, optional\n        Random state to use when fitting.\n    num_negative_samples: int, optional\n        Number of negative samples to generate for adaptive hinge loss.\n    """"""\n\n    def __init__(self,\n                 loss=\'pointwise\',\n                 embedding_dim=32,\n                 n_iter=10,\n                 batch_size=256,\n                 l2=0.0,\n                 learning_rate=1e-2,\n                 optimizer_func=None,\n                 use_cuda=False,\n                 representation=None,\n                 sparse=False,\n                 random_state=None,\n                 num_negative_samples=5):\n\n        assert loss in (\'pointwise\',\n                        \'bpr\',\n                        \'hinge\',\n                        \'adaptive_hinge\')\n\n        self._loss = loss\n        self._embedding_dim = embedding_dim\n        self._n_iter = n_iter\n        self._learning_rate = learning_rate\n        self._batch_size = batch_size\n        self._l2 = l2\n        self._use_cuda = use_cuda\n        self._representation = representation\n        self._sparse = sparse\n        self._optimizer_func = optimizer_func\n        self._random_state = random_state or np.random.RandomState()\n        self._num_negative_samples = num_negative_samples\n\n        self._num_users = None\n        self._num_items = None\n        self._net = None\n        self._optimizer = None\n        self._loss_func = None\n\n        set_seed(self._random_state.randint(-10**8, 10**8),\n                 cuda=self._use_cuda)\n\n    def __repr__(self):\n\n        return _repr_model(self)\n\n    @property\n    def _initialized(self):\n        return self._net is not None\n\n    def _initialize(self, interactions):\n\n        (self._num_users,\n         self._num_items) = (interactions.num_users,\n                             interactions.num_items)\n\n        if self._representation is not None:\n            self._net = gpu(self._representation,\n                            self._use_cuda)\n        else:\n            self._net = gpu(\n                BilinearNet(self._num_users,\n                            self._num_items,\n                            self._embedding_dim,\n                            sparse=self._sparse),\n                self._use_cuda\n            )\n\n        if self._optimizer_func is None:\n            self._optimizer = optim.Adam(\n                self._net.parameters(),\n                weight_decay=self._l2,\n                lr=self._learning_rate\n            )\n        else:\n            self._optimizer = self._optimizer_func(self._net.parameters())\n\n        if self._loss == \'pointwise\':\n            self._loss_func = pointwise_loss\n        elif self._loss == \'bpr\':\n            self._loss_func = bpr_loss\n        elif self._loss == \'hinge\':\n            self._loss_func = hinge_loss\n        else:\n            self._loss_func = adaptive_hinge_loss\n\n    def _check_input(self, user_ids, item_ids, allow_items_none=False):\n\n        if isinstance(user_ids, int):\n            user_id_max = user_ids\n        else:\n            user_id_max = user_ids.max()\n\n        if user_id_max >= self._num_users:\n            raise ValueError(\'Maximum user id greater \'\n                             \'than number of users in model.\')\n\n        if allow_items_none and item_ids is None:\n            return\n\n        if isinstance(item_ids, int):\n            item_id_max = item_ids\n        else:\n            item_id_max = item_ids.max()\n\n        if item_id_max >= self._num_items:\n            raise ValueError(\'Maximum item id greater \'\n                             \'than number of items in model.\')\n\n    def fit(self, interactions, verbose=False):\n        """"""\n        Fit the model.\n\n        When called repeatedly, model fitting will resume from\n        the point at which training stopped in the previous fit\n        call.\n\n        Parameters\n        ----------\n\n        interactions: :class:`spotlight.interactions.Interactions`\n            The input dataset.\n\n        verbose: bool\n            Output additional information about current epoch and loss.\n        """"""\n\n        user_ids = interactions.user_ids.astype(np.int64)\n        item_ids = interactions.item_ids.astype(np.int64)\n\n        if not self._initialized:\n            self._initialize(interactions)\n\n        self._check_input(user_ids, item_ids)\n\n        for epoch_num in range(self._n_iter):\n\n            users, items = shuffle(user_ids,\n                                   item_ids,\n                                   random_state=self._random_state)\n\n            user_ids_tensor = gpu(torch.from_numpy(users),\n                                  self._use_cuda)\n            item_ids_tensor = gpu(torch.from_numpy(items),\n                                  self._use_cuda)\n\n            epoch_loss = 0.0\n\n            for (minibatch_num,\n                 (batch_user,\n                  batch_item)) in enumerate(minibatch(user_ids_tensor,\n                                                      item_ids_tensor,\n                                                      batch_size=self._batch_size)):\n\n                positive_prediction = self._net(batch_user, batch_item)\n\n                if self._loss == \'adaptive_hinge\':\n                    negative_prediction = self._get_multiple_negative_predictions(\n                        batch_user, n=self._num_negative_samples)\n                else:\n                    negative_prediction = self._get_negative_prediction(batch_user)\n\n                self._optimizer.zero_grad()\n\n                loss = self._loss_func(positive_prediction, negative_prediction)\n                epoch_loss += loss.item()\n\n                loss.backward()\n                self._optimizer.step()\n\n            epoch_loss /= minibatch_num + 1\n\n            if verbose:\n                print(\'Epoch {}: loss {}\'.format(epoch_num, epoch_loss))\n\n            if np.isnan(epoch_loss) or epoch_loss == 0.0:\n                raise ValueError(\'Degenerate epoch loss: {}\'\n                                 .format(epoch_loss))\n\n    def _get_negative_prediction(self, user_ids):\n\n        negative_items = sample_items(\n            self._num_items,\n            len(user_ids),\n            random_state=self._random_state)\n        negative_var = gpu(torch.from_numpy(negative_items), self._use_cuda)\n\n        negative_prediction = self._net(user_ids, negative_var)\n\n        return negative_prediction\n\n    def _get_multiple_negative_predictions(self, user_ids, n=5):\n\n        batch_size = user_ids.size(0)\n\n        negative_prediction = self._get_negative_prediction(user_ids\n                                                            .view(batch_size, 1)\n                                                            .expand(batch_size, n)\n                                                            .reshape(batch_size * n))\n\n        return negative_prediction.view(n, len(user_ids))\n\n    def predict(self, user_ids, item_ids=None):\n        """"""\n        Make predictions: given a user id, compute the recommendation\n        scores for items.\n\n        Parameters\n        ----------\n\n        user_ids: int or array\n           If int, will predict the recommendation scores for this\n           user for all items in item_ids. If an array, will predict\n           scores for all (user, item) pairs defined by user_ids and\n           item_ids.\n        item_ids: array, optional\n            Array containing the item ids for which prediction scores\n            are desired. If not supplied, predictions for all items\n            will be computed.\n\n        Returns\n        -------\n\n        predictions: np.array\n            Predicted scores for all items in item_ids.\n        """"""\n\n        self._check_input(user_ids, item_ids, allow_items_none=True)\n        self._net.train(False)\n\n        user_ids, item_ids = _predict_process_ids(user_ids, item_ids,\n                                                  self._num_items,\n                                                  self._use_cuda)\n\n        out = self._net(user_ids, item_ids)\n\n        return cpu(out).detach().numpy().flatten()\n'"
spotlight/factorization/representations.py,1,"b'""""""\nClasses defining user and item latent representations in\nfactorization models.\n""""""\n\nimport torch.nn as nn\n\nfrom spotlight.layers import ScaledEmbedding, ZeroEmbedding\n\n\nclass BilinearNet(nn.Module):\n    """"""\n    Bilinear factorization representation.\n\n    Encodes both users and items as an embedding layer; the score\n    for a user-item pair is given by the dot product of the item\n    and user latent vectors.\n\n    Parameters\n    ----------\n\n    num_users: int\n        Number of users in the model.\n    num_items: int\n        Number of items in the model.\n    embedding_dim: int, optional\n        Dimensionality of the latent representations.\n    user_embedding_layer: an embedding layer, optional\n        If supplied, will be used as the user embedding layer\n        of the network.\n    item_embedding_layer: an embedding layer, optional\n        If supplied, will be used as the item embedding layer\n        of the network.\n    sparse: boolean, optional\n        Use sparse gradients.\n\n    """"""\n\n    def __init__(self, num_users, num_items, embedding_dim=32,\n                 user_embedding_layer=None, item_embedding_layer=None, sparse=False):\n\n        super(BilinearNet, self).__init__()\n\n        self.embedding_dim = embedding_dim\n\n        if user_embedding_layer is not None:\n            self.user_embeddings = user_embedding_layer\n        else:\n            self.user_embeddings = ScaledEmbedding(num_users, embedding_dim,\n                                                   sparse=sparse)\n\n        if item_embedding_layer is not None:\n            self.item_embeddings = item_embedding_layer\n        else:\n            self.item_embeddings = ScaledEmbedding(num_items, embedding_dim,\n                                                   sparse=sparse)\n\n        self.user_biases = ZeroEmbedding(num_users, 1, sparse=sparse)\n        self.item_biases = ZeroEmbedding(num_items, 1, sparse=sparse)\n\n    def forward(self, user_ids, item_ids):\n        """"""\n        Compute the forward pass of the representation.\n\n        Parameters\n        ----------\n\n        user_ids: tensor\n            Tensor of user indices.\n        item_ids: tensor\n            Tensor of item indices.\n\n        Returns\n        -------\n\n        predictions: tensor\n            Tensor of predictions.\n        """"""\n\n        user_embedding = self.user_embeddings(user_ids)\n        item_embedding = self.item_embeddings(item_ids)\n\n        user_embedding = user_embedding.squeeze()\n        item_embedding = item_embedding.squeeze()\n\n        user_bias = self.user_biases(user_ids).squeeze()\n        item_bias = self.item_biases(item_ids).squeeze()\n\n        dot = (user_embedding * item_embedding).sum(1)\n\n        return dot + user_bias + item_bias\n'"
spotlight/sequence/__init__.py,0,"b'""""""\nModule for building recommender models that use sequences of\nusers\' interactions to represent them.\n""""""\n'"
spotlight/sequence/implicit.py,5,"b'""""""\nModels for recommending items given a sequence of previous items\na user has interacted with.\n""""""\n\nimport numpy as np\n\nimport torch\n\nimport torch.optim as optim\n\nfrom spotlight.helpers import _repr_model\nfrom spotlight.losses import (adaptive_hinge_loss,\n                              bpr_loss,\n                              hinge_loss,\n                              pointwise_loss)\nfrom spotlight.sequence.representations import (PADDING_IDX, CNNNet,\n                                                LSTMNet,\n                                                MixtureLSTMNet,\n                                                PoolNet)\nfrom spotlight.sampling import sample_items\nfrom spotlight.torch_utils import cpu, gpu, minibatch, set_seed, shuffle\n\n\nclass ImplicitSequenceModel(object):\n    """"""\n    Model for sequential recommendations using implicit feedback.\n\n    Parameters\n    ----------\n\n    loss: string, optional\n        The loss function for approximating a softmax with negative sampling.\n        One of \'pointwise\', \'bpr\', \'hinge\', \'adaptive_hinge\', corresponding\n        to losses from :class:`spotlight.losses`.\n    representation: string or instance of :class:`spotlight.sequence.representations`, optional\n        Sequence representation to use. If string, it must be one\n        of \'pooling\', \'cnn\', \'lstm\', \'mixture\'; otherwise must be one of the\n        representations from :class:`spotlight.sequence.representations`\n    embedding_dim: int, optional\n        Number of embedding dimensions to use for representing items.\n        Overridden if representation is an instance of a representation class.\n    n_iter: int, optional\n        Number of iterations to run.\n    batch_size: int, optional\n        Minibatch size.\n    l2: float, optional\n        L2 loss penalty.\n    learning_rate: float, optional\n        Initial learning rate.\n    optimizer_func: function, optional\n        Function that takes in module parameters as the first argument and\n        returns an instance of a PyTorch optimizer. Overrides l2 and learning\n        rate if supplied. If no optimizer supplied, then use ADAM by default.\n    use_cuda: boolean, optional\n        Run the model on a GPU.\n    sparse: boolean, optional\n        Use sparse gradients for embedding layers.\n    random_state: instance of numpy.random.RandomState, optional\n        Random state to use when fitting.\n    num_negative_samples: int, optional\n        Number of negative samples to generate for adaptive hinge loss.\n\n    Notes\n    -----\n\n    During fitting, the model computes the loss for each timestep of the\n    supplied sequence. For example, suppose the following sequences are\n    passed to the ``fit`` function:\n\n    .. code-block:: python\n\n       [[1, 2, 3, 4, 5],\n        [0, 0, 7, 1, 4]]\n\n\n    In this case, the loss for the first example will be the mean loss\n    of trying to predict ``2`` from ``[1]``, ``3`` from ``[1, 2]``,\n    ``4`` from ``[1, 2, 3]`` and so on. This means that explicit padding\n    of all subsequences is not necessary (although it is possible by using\n    the ``step_size`` parameter of\n    :func:`spotlight.interactions.Interactions.to_sequence`.\n    """"""\n\n    def __init__(self,\n                 loss=\'pointwise\',\n                 representation=\'pooling\',\n                 embedding_dim=32,\n                 n_iter=10,\n                 batch_size=256,\n                 l2=0.0,\n                 learning_rate=1e-2,\n                 optimizer_func=None,\n                 use_cuda=False,\n                 sparse=False,\n                 random_state=None,\n                 num_negative_samples=5):\n\n        assert loss in (\'pointwise\',\n                        \'bpr\',\n                        \'hinge\',\n                        \'adaptive_hinge\')\n\n        if isinstance(representation, str):\n            assert representation in (\'pooling\',\n                                      \'cnn\',\n                                      \'lstm\',\n                                      \'mixture\')\n\n        self._loss = loss\n        self._representation = representation\n        self._embedding_dim = embedding_dim\n        self._n_iter = n_iter\n        self._learning_rate = learning_rate\n        self._batch_size = batch_size\n        self._l2 = l2\n        self._use_cuda = use_cuda\n        self._sparse = sparse\n        self._optimizer_func = optimizer_func\n        self._random_state = random_state or np.random.RandomState()\n        self._num_negative_samples = num_negative_samples\n\n        self._num_items = None\n        self._net = None\n        self._optimizer = None\n        self._loss_func = None\n\n        set_seed(self._random_state.randint(-10**8, 10**8),\n                 cuda=self._use_cuda)\n\n    def __repr__(self):\n\n        return _repr_model(self)\n\n    @property\n    def _initialized(self):\n        return self._net is not None\n\n    def _initialize(self, interactions):\n\n        self._num_items = interactions.num_items\n\n        if self._representation == \'pooling\':\n            self._net = PoolNet(self._num_items,\n                                self._embedding_dim,\n                                sparse=self._sparse)\n        elif self._representation == \'cnn\':\n            self._net = CNNNet(self._num_items,\n                               self._embedding_dim,\n                               sparse=self._sparse)\n        elif self._representation == \'lstm\':\n            self._net = LSTMNet(self._num_items,\n                                self._embedding_dim,\n                                sparse=self._sparse)\n        elif self._representation == \'mixture\':\n            self._net = MixtureLSTMNet(self._num_items,\n                                       self._embedding_dim,\n                                       sparse=self._sparse)\n        else:\n            self._net = self._representation\n\n        self._net = gpu(self._net, self._use_cuda)\n\n        if self._optimizer_func is None:\n            self._optimizer = optim.Adam(\n                self._net.parameters(),\n                weight_decay=self._l2,\n                lr=self._learning_rate\n            )\n        else:\n            self._optimizer = self._optimizer_func(self._net.parameters())\n\n        if self._loss == \'pointwise\':\n            self._loss_func = pointwise_loss\n        elif self._loss == \'bpr\':\n            self._loss_func = bpr_loss\n        elif self._loss == \'hinge\':\n            self._loss_func = hinge_loss\n        else:\n            self._loss_func = adaptive_hinge_loss\n\n    def _check_input(self, item_ids):\n\n        if isinstance(item_ids, int):\n            item_id_max = item_ids\n        else:\n            item_id_max = item_ids.max()\n\n        if item_id_max >= self._num_items:\n            raise ValueError(\'Maximum item id greater \'\n                             \'than number of items in model.\')\n\n    def fit(self, interactions, verbose=False):\n        """"""\n        Fit the model.\n\n        When called repeatedly, model fitting will resume from\n        the point at which training stopped in the previous fit\n        call.\n\n        Parameters\n        ----------\n\n        interactions: :class:`spotlight.interactions.SequenceInteractions`\n            The input sequence dataset.\n        """"""\n\n        sequences = interactions.sequences.astype(np.int64)\n\n        if not self._initialized:\n            self._initialize(interactions)\n\n        self._check_input(sequences)\n\n        for epoch_num in range(self._n_iter):\n\n            sequences = shuffle(sequences,\n                                random_state=self._random_state)\n\n            sequences_tensor = gpu(torch.from_numpy(sequences),\n                                   self._use_cuda)\n\n            epoch_loss = 0.0\n\n            for minibatch_num, batch_sequence in enumerate(minibatch(sequences_tensor,\n                                                                     batch_size=self._batch_size)):\n\n                sequence_var = batch_sequence\n\n                user_representation, _ = self._net.user_representation(\n                    sequence_var\n                )\n\n                positive_prediction = self._net(user_representation,\n                                                sequence_var)\n\n                if self._loss == \'adaptive_hinge\':\n                    negative_prediction = self._get_multiple_negative_predictions(\n                        sequence_var.size(),\n                        user_representation,\n                        n=self._num_negative_samples)\n                else:\n                    negative_prediction = self._get_negative_prediction(sequence_var.size(),\n                                                                        user_representation)\n\n                self._optimizer.zero_grad()\n\n                loss = self._loss_func(positive_prediction,\n                                       negative_prediction,\n                                       mask=(sequence_var != PADDING_IDX))\n                epoch_loss += loss.item()\n\n                loss.backward()\n\n                self._optimizer.step()\n\n            epoch_loss /= minibatch_num + 1\n\n            if verbose:\n                print(\'Epoch {}: loss {}\'.format(epoch_num, epoch_loss))\n\n            if np.isnan(epoch_loss) or epoch_loss == 0.0:\n                raise ValueError(\'Degenerate epoch loss: {}\'\n                                 .format(epoch_loss))\n\n    def _get_negative_prediction(self, shape, user_representation):\n\n        negative_items = sample_items(\n            self._num_items,\n            shape,\n            random_state=self._random_state)\n        negative_var = gpu(torch.from_numpy(negative_items), self._use_cuda)\n\n        negative_prediction = self._net(user_representation, negative_var)\n\n        return negative_prediction\n\n    def _get_multiple_negative_predictions(self, shape, user_representation,\n                                           n=5):\n        batch_size, sliding_window = shape\n        size = (n,) + (1,) * (user_representation.dim() - 1)\n        negative_prediction = self._get_negative_prediction(\n            (n * batch_size, sliding_window),\n            user_representation.repeat(*size))\n\n        return negative_prediction.view(n, batch_size, sliding_window)\n\n    def predict(self, sequences, item_ids=None):\n        """"""\n        Make predictions: given a sequence of interactions, predict\n        the next item in the sequence.\n\n        Parameters\n        ----------\n\n        sequences: array, (1 x max_sequence_length)\n            Array containing the indices of the items in the sequence.\n        item_ids: array (num_items x 1), optional\n            Array containing the item ids for which prediction scores\n            are desired. If not supplied, predictions for all items\n            will be computed.\n\n        Returns\n        -------\n\n        predictions: array\n            Predicted scores for all items in item_ids.\n        """"""\n\n        self._net.train(False)\n\n        sequences = np.atleast_2d(sequences)\n\n        if item_ids is None:\n            item_ids = np.arange(self._num_items).reshape(-1, 1)\n\n        self._check_input(item_ids)\n        self._check_input(sequences)\n\n        sequences = torch.from_numpy(sequences.astype(np.int64).reshape(1, -1))\n        item_ids = torch.from_numpy(item_ids.astype(np.int64))\n\n        sequence_var = gpu(sequences, self._use_cuda)\n        item_var = gpu(item_ids, self._use_cuda)\n\n        _, sequence_representations = self._net.user_representation(sequence_var)\n        size = (len(item_var),) + sequence_representations.size()[1:]\n        out = self._net(sequence_representations.expand(*size),\n                        item_var)\n\n        return cpu(out).detach().numpy().flatten()\n'"
spotlight/sequence/representations.py,5,"b'""""""\nThis module contains prototypes of various ways of representing users\nas functions of the items they have interacted with in the past.\n""""""\n\nimport torch\n\nfrom torch.backends import cudnn\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom spotlight.layers import ScaledEmbedding, ZeroEmbedding\n\n\nPADDING_IDX = 0\n\n\ndef _to_iterable(val, num):\n\n    try:\n        iter(val)\n        return val\n    except TypeError:\n        return (val,) * num\n\n\nclass PoolNet(nn.Module):\n    """"""\n    Module representing users through averaging the representations of items\n    they have interacted with, a\'la [1]_.\n\n    To represent a sequence, it simply averages the representations of all\n    the items that occur in the sequence up to that point.\n\n    During training, representations for all timesteps of the sequence are\n    computed in one go. Loss functions using the outputs will therefore\n    be aggregating both across the minibatch and across time in the sequence.\n\n    Parameters\n    ----------\n\n    num_items: int\n        Number of items to be represented.\n    embedding_dim: int, optional\n        Embedding dimension of the embedding layer.\n    item_embedding_layer: an embedding layer, optional\n        If supplied, will be used as the item embedding layer\n        of the network.\n\n    References\n    ----------\n\n    .. [1] Covington, Paul, Jay Adams, and Emre Sargin. ""Deep neural networks for\n       youtube recommendations."" Proceedings of the 10th ACM Conference\n       on Recommender Systems. ACM, 2016.\n\n    """"""\n\n    def __init__(self, num_items, embedding_dim=32,\n                 item_embedding_layer=None, sparse=False):\n\n        super(PoolNet, self).__init__()\n\n        self.embedding_dim = embedding_dim\n\n        if item_embedding_layer is not None:\n            self.item_embeddings = item_embedding_layer\n        else:\n            self.item_embeddings = ScaledEmbedding(num_items, embedding_dim,\n                                                   padding_idx=PADDING_IDX,\n                                                   sparse=sparse)\n\n        self.item_biases = ZeroEmbedding(num_items, 1, sparse=sparse,\n                                         padding_idx=PADDING_IDX)\n\n    def user_representation(self, item_sequences):\n        """"""\n        Compute user representation from a given sequence.\n\n        Returns\n        -------\n\n        tuple (all_representations, final_representation)\n            The first element contains all representations from step\n            -1 (no items seen) to t - 1 (all but the last items seen).\n            The second element contains the final representation\n            at step t (all items seen). This final state can be used\n            for prediction or evaluation.\n        """"""\n\n        # Make the embedding dimension the channel dimension\n        sequence_embeddings = (self.item_embeddings(item_sequences)\n                               .permute(0, 2, 1))\n\n        # Add a trailing dimension of 1\n        sequence_embeddings = (sequence_embeddings\n                               .unsqueeze(3))\n\n        # Pad it with zeros from left\n        sequence_embeddings = F.pad(sequence_embeddings,\n                                    (0, 0, 1, 0))\n\n        # Average representations, ignoring padding.\n        sequence_embedding_sum = torch.cumsum(sequence_embeddings, 2)\n        non_padding_entries = (\n            torch.cumsum((sequence_embeddings != 0.0).float(), 2)\n            .expand_as(sequence_embedding_sum)\n        )\n\n        user_representations = (\n            sequence_embedding_sum / (non_padding_entries + 1)\n        ).squeeze(3)\n\n        return user_representations[:, :, :-1], user_representations[:, :, -1]\n\n    def forward(self, user_representations, targets):\n        """"""\n        Compute predictions for target items given user representations.\n\n        Parameters\n        ----------\n\n        user_representations: tensor\n            Result of the user_representation_method.\n        targets: tensor\n            Minibatch of item sequences of shape\n            (minibatch_size, sequence_length).\n\n        Returns\n        -------\n\n        predictions: tensor\n            of shape (minibatch_size, sequence_length)\n        """"""\n\n        target_embedding = (self.item_embeddings(targets)\n                            .permute(0, 2, 1)\n                            .squeeze())\n        target_bias = self.item_biases(targets).squeeze()\n\n        dot = ((user_representations * target_embedding)\n               .sum(1))\n\n        return target_bias + dot\n\n\nclass LSTMNet(nn.Module):\n    """"""\n    Module representing users through running a recurrent neural network\n    over the sequence, using the hidden state at each timestep as the\n    sequence representation, a\'la [2]_\n\n    During training, representations for all timesteps of the sequence are\n    computed in one go. Loss functions using the outputs will therefore\n    be aggregating both across the minibatch and across time in the sequence.\n\n    Parameters\n    ----------\n\n    num_items: int\n        Number of items to be represented.\n    embedding_dim: int, optional\n        Embedding dimension of the embedding layer, and the number of hidden\n        units in the LSTM layer.\n    item_embedding_layer: an embedding layer, optional\n        If supplied, will be used as the item embedding layer\n        of the network.\n\n    References\n    ----------\n\n    .. [2] Hidasi, Balazs, et al. ""Session-based recommendations with\n       recurrent neural networks."" arXiv preprint arXiv:1511.06939 (2015).\n    """"""\n\n    def __init__(self, num_items, embedding_dim=32,\n                 item_embedding_layer=None, sparse=False):\n\n        super(LSTMNet, self).__init__()\n\n        self.embedding_dim = embedding_dim\n\n        if item_embedding_layer is not None:\n            self.item_embeddings = item_embedding_layer\n        else:\n            self.item_embeddings = ScaledEmbedding(num_items, embedding_dim,\n                                                   padding_idx=PADDING_IDX,\n                                                   sparse=sparse)\n\n        self.item_biases = ZeroEmbedding(num_items, 1, sparse=sparse,\n                                         padding_idx=PADDING_IDX)\n\n        self.lstm = nn.LSTM(batch_first=True,\n                            input_size=embedding_dim,\n                            hidden_size=embedding_dim)\n\n    def user_representation(self, item_sequences):\n        """"""\n        Compute user representation from a given sequence.\n\n        Returns\n        -------\n\n        tuple (all_representations, final_representation)\n            The first element contains all representations from step\n            -1 (no items seen) to t - 1 (all but the last items seen).\n            The second element contains the final representation\n            at step t (all items seen). This final state can be used\n            for prediction or evaluation.\n        """"""\n\n        # Make the embedding dimension the channel dimension\n        sequence_embeddings = (self.item_embeddings(item_sequences)\n                               .permute(0, 2, 1))\n        # Add a trailing dimension of 1\n        sequence_embeddings = (sequence_embeddings\n                               .unsqueeze(3))\n        # Pad it with zeros from left\n        sequence_embeddings = (F.pad(sequence_embeddings,\n                                     (0, 0, 1, 0))\n                               .squeeze(3))\n        sequence_embeddings = sequence_embeddings.permute(0, 2, 1)\n\n        user_representations, _ = self.lstm(sequence_embeddings)\n        user_representations = user_representations.permute(0, 2, 1)\n\n        return user_representations[:, :, :-1], user_representations[:, :, -1]\n\n    def forward(self, user_representations, targets):\n        """"""\n        Compute predictions for target items given user representations.\n\n        Parameters\n        ----------\n\n        user_representations: tensor\n            Result of the user_representation_method.\n        targets: tensor\n            A minibatch of item sequences of shape\n            (minibatch_size, sequence_length).\n\n        Returns\n        -------\n\n        predictions: tensor\n            of shape (minibatch_size, sequence_length)\n        """"""\n\n        target_embedding = (self.item_embeddings(targets)\n                            .permute(0, 2, 1)\n                            .squeeze())\n        target_bias = self.item_biases(targets).squeeze()\n\n        dot = ((user_representations * target_embedding)\n               .sum(1)\n               .squeeze())\n\n        return target_bias + dot\n\n\nclass CNNNet(nn.Module):\n    """"""\n    Module representing users through stacked causal atrous convolutions ([3]_, [4]_).\n\n    To represent a sequence, it runs a 1D convolution over the input sequence,\n    from left to right. At each timestep, the output of the convolution is\n    the representation of the sequence up to that point. The convolution is causal\n    because future states are never part of the convolution\'s receptive field;\n    this is achieved by left-padding the sequence.\n\n    In order to increase the receptive field (and the capacity to encode states\n    further back in the sequence), one can increase the kernel width, stack\n    more layers, or increase the dilation factor.\n    Input dimensionality is preserved from layer to layer.\n\n    Residual connections can be added between all layers.\n\n    During training, representations for all timesteps of the sequence are\n    computed in one go. Loss functions using the outputs will therefore\n    be aggregating both across the minibatch and across time in the sequence.\n\n    Parameters\n    ----------\n\n    num_items: int\n        Number of items to be represented.\n    embedding_dim: int, optional\n        Embedding dimension of the embedding layer, and the number of filters\n        in each convolutional layer.\n    kernel_width: tuple or int, optional\n        The kernel width of the convolutional layers. If tuple, should contain\n        the kernel widths for all convolutional layers. If int, it will be\n        expanded into a tuple to match the number of layers.\n    dilation: tuple or int, optional\n        The dilation factor for atrous convolutions. Setting this to a number\n        greater than 1 inserts gaps into the convolutional layers, increasing\n        their receptive field without increasing the number of parameters.\n        If tuple, should contain the dilation factors for all convolutional\n        layers. If int, it will be expanded into a tuple to match the number\n        of layers.\n    num_layers: int, optional\n        Number of stacked convolutional layers.\n    nonlinearity: string, optional\n        One of (\'tanh\', \'relu\'). Denotes the type of non-linearity to apply\n        after each convolutional layer.\n    residual_connections: boolean, optional\n        Whether to use residual connections between convolutional layers.\n    item_embedding_layer: an embedding layer, optional\n        If supplied, will be used as the item embedding layer\n        of the network.\n\n    References\n    ----------\n\n    .. [3] Oord, Aaron van den, et al. ""Wavenet: A generative model for raw audio.""\n       arXiv preprint arXiv:1609.03499 (2016).\n    .. [4] Kalchbrenner, Nal, et al. ""Neural machine translation in linear time.""\n       arXiv preprint arXiv:1610.10099 (2016).\n    """"""\n\n    def __init__(self, num_items,\n                 embedding_dim=32,\n                 kernel_width=3,\n                 dilation=1,\n                 num_layers=1,\n                 nonlinearity=\'tanh\',\n                 residual_connections=True,\n                 sparse=False,\n                 benchmark=True,\n                 item_embedding_layer=None):\n\n        super(CNNNet, self).__init__()\n\n        cudnn.benchmark = benchmark\n\n        self.embedding_dim = embedding_dim\n        self.kernel_width = _to_iterable(kernel_width, num_layers)\n        self.dilation = _to_iterable(dilation, num_layers)\n        if nonlinearity == \'tanh\':\n            self.nonlinearity = F.tanh\n        elif nonlinearity == \'relu\':\n            self.nonlinearity = F.relu\n        else:\n            raise ValueError(\'Nonlinearity must be one of (tanh, relu)\')\n        self.residual_connections = residual_connections\n\n        if item_embedding_layer is not None:\n            self.item_embeddings = item_embedding_layer\n        else:\n            self.item_embeddings = ScaledEmbedding(num_items, embedding_dim,\n                                                   padding_idx=PADDING_IDX,\n                                                   sparse=sparse)\n\n        self.item_biases = ZeroEmbedding(num_items, 1, sparse=sparse,\n                                         padding_idx=PADDING_IDX)\n\n        self.cnn_layers = [\n            nn.Conv2d(embedding_dim,\n                      embedding_dim,\n                      (_kernel_width, 1),\n                      dilation=(_dilation, 1)) for\n            (_kernel_width, _dilation) in zip(self.kernel_width,\n                                              self.dilation)\n        ]\n\n        for i, layer in enumerate(self.cnn_layers):\n            self.add_module(\'cnn_{}\'.format(i),\n                            layer)\n\n    def user_representation(self, item_sequences):\n        """"""\n        Compute user representation from a given sequence.\n\n        Returns\n        -------\n\n        tuple (all_representations, final_representation)\n            The first element contains all representations from step\n            -1 (no items seen) to t - 1 (all but the last items seen).\n            The second element contains the final representation\n            at step t (all items seen). This final state can be used\n            for prediction or evaluation.\n        """"""\n\n        # Make the embedding dimension the channel dimension\n        sequence_embeddings = (self.item_embeddings(item_sequences)\n                               .permute(0, 2, 1))\n        # Add a trailing dimension of 1\n        sequence_embeddings = (sequence_embeddings\n                               .unsqueeze(3))\n\n        # Pad so that the CNN doesn\'t have the future\n        # of the sequence in its receptive field.\n        receptive_field_width = (self.kernel_width[0] +\n                                 (self.kernel_width[0] - 1) *\n                                 (self.dilation[0] - 1))\n\n        x = F.pad(sequence_embeddings,\n                  (0, 0, receptive_field_width, 0))\n        x = self.nonlinearity(self.cnn_layers[0](x))\n\n        if self.residual_connections:\n            residual = F.pad(sequence_embeddings,\n                             (0, 0, 1, 0))\n            x = x + residual\n\n        for (cnn_layer, kernel_width, dilation) in zip(self.cnn_layers[1:],\n                                                       self.kernel_width[1:],\n                                                       self.dilation[1:]):\n            receptive_field_width = (kernel_width +\n                                     (kernel_width - 1) *\n                                     (dilation - 1))\n            residual = x\n            x = F.pad(x, (0, 0, receptive_field_width - 1, 0))\n            x = self.nonlinearity(cnn_layer(x))\n\n            if self.residual_connections:\n                x = x + residual\n\n        x = x.squeeze(3)\n\n        return x[:, :, :-1], x[:, :, -1]\n\n    def forward(self, user_representations, targets):\n        """"""\n        Compute predictions for target items given user representations.\n\n        Parameters\n        ----------\n\n        user_representations: tensor\n            Result of the user_representation_method.\n        targets: tensor\n            Minibatch of item sequences of shape\n            (minibatch_size, sequence_length).\n\n        Returns\n        -------\n\n        predictions: tensor\n            Of shape (minibatch_size, sequence_length).\n        """"""\n\n        target_embedding = (self.item_embeddings(targets)\n                            .permute(0, 2, 1)\n                            .squeeze())\n        target_bias = self.item_biases(targets).squeeze()\n\n        dot = ((user_representations * target_embedding)\n               .sum(1)\n               .squeeze())\n\n        return target_bias + dot\n\n\nclass MixtureLSTMNet(nn.Module):\n    """"""\n    A representation that models users as mixtures-of-tastes.\n\n    This is accomplished via an LSTM with a layer on top that\n    projects the last hidden state taste vectors and\n    taste attention vectors that match items with the taste\n    vectors that are best for evaluating them.\n\n    For a full description of the model, see [5]_.\n\n    Parameters\n    ----------\n\n    num_items: int\n        Number of items to be represented.\n    embedding_dim: int, optional\n        Embedding dimension of the embedding layer, and the number of hidden\n        units in the LSTM layer.\n    num_mixtures: int, optional\n        Number of mixture components (distinct user tastes) that\n        the network should model.\n    item_embedding_layer: an embedding layer, optional\n        If supplied, will be used as the item embedding layer\n        of the network.\n\n    References\n    ----------\n\n    .. [5] Kula, Maciej. ""Mixture-of-tastes Models for Representing\n       Users with Diverse Interests"" https://github.com/maciejkula/mixture (2017)\n    """"""\n\n    def __init__(self, num_items,\n                 embedding_dim=32,\n                 num_mixtures=4,\n                 item_embedding_layer=None,\n                 sparse=False):\n\n        super(MixtureLSTMNet, self).__init__()\n\n        self.embedding_dim = embedding_dim\n        self.num_mixtures = num_mixtures\n\n        if item_embedding_layer is not None:\n            self.item_embeddings = item_embedding_layer\n        else:\n            self.item_embeddings = ScaledEmbedding(num_items, embedding_dim,\n                                                   padding_idx=PADDING_IDX,\n                                                   sparse=sparse)\n\n        self.item_biases = ZeroEmbedding(num_items, 1, sparse=sparse,\n                                         padding_idx=PADDING_IDX)\n\n        self.lstm = nn.LSTM(batch_first=True,\n                            input_size=embedding_dim,\n                            hidden_size=embedding_dim)\n        self.projection = nn.Conv1d(embedding_dim,\n                                    embedding_dim * self.num_mixtures * 2,\n                                    kernel_size=1)\n\n    def user_representation(self, item_sequences):\n        """"""\n        Compute user representation from a given sequence.\n\n        Returns\n        -------\n\n        tuple (all_representations, final_representation)\n            The first element contains all representations from step\n            -1 (no items seen) to t - 1 (all but the last items seen).\n            The second element contains the final representation\n            at step t (all items seen). This final state can be used\n            for prediction or evaluation.\n        """"""\n\n        batch_size, sequence_length = item_sequences.size()\n\n        # Make the embedding dimension the channel dimension\n        sequence_embeddings = (self.item_embeddings(item_sequences)\n                               .permute(0, 2, 1))\n        # Add a trailing dimension of 1\n        sequence_embeddings = (sequence_embeddings\n                               .unsqueeze(3))\n        # Pad it with zeros from left\n        sequence_embeddings = (F.pad(sequence_embeddings,\n                                     (0, 0, 1, 0))\n                               .squeeze(3))\n        sequence_embeddings = sequence_embeddings\n        sequence_embeddings = sequence_embeddings.permute(0, 2, 1)\n\n        user_representations, _ = self.lstm(sequence_embeddings)\n        user_representations = user_representations.permute(0, 2, 1)\n        user_representations = self.projection(user_representations)\n        user_representations = user_representations.view(batch_size,\n                                                         self.num_mixtures * 2,\n                                                         self.embedding_dim,\n                                                         sequence_length + 1)\n\n        return user_representations[:, :, :, :-1], user_representations[:, :, :, -1:]\n\n    def forward(self, user_representations, targets):\n        """"""\n        Compute predictions for target items given user representations.\n\n        Parameters\n        ----------\n\n        user_representations: tensor\n            Result of the user_representation_method.\n        targets: tensor\n            A minibatch of item sequences of shape\n            (minibatch_size, sequence_length).\n\n        Returns\n        -------\n\n        predictions: tensor\n            of shape (minibatch_size, sequence_length)\n        """"""\n\n        user_components = user_representations[:, :self.num_mixtures, :, :]\n        mixture_vectors = user_representations[:, self.num_mixtures:, :, :]\n\n        target_embedding = (self.item_embeddings(targets)\n                            .permute(0, 2, 1))\n        target_bias = self.item_biases(targets).squeeze()\n\n        mixture_weights = (mixture_vectors * target_embedding\n                           .unsqueeze(1)\n                           .expand_as(user_components))\n        mixture_weights = (F.softmax(mixture_weights.sum(2), 1)\n                           .unsqueeze(2)\n                           .expand_as(user_components))\n        weighted_user_representations = (mixture_weights * user_components).sum(1)\n\n        dot = ((weighted_user_representations * target_embedding)\n               .sum(1)\n               .squeeze())\n\n        return target_bias + dot\n'"
tests/factorization/test_api.py,0,"b""import os\n\nimport numpy as np\n\nimport pytest\n\nfrom spotlight.datasets import movielens\nfrom spotlight.factorization.explicit import ExplicitFactorizationModel\nfrom spotlight.factorization.implicit import ImplicitFactorizationModel\n\n\nCUDA = bool(os.environ.get('SPOTLIGHT_CUDA', False))\n\n\n@pytest.mark.parametrize('model_class', [\n    ImplicitFactorizationModel,\n    ExplicitFactorizationModel\n])\ndef test_predict_movielens(model_class):\n\n    interactions = movielens.get_movielens_dataset('100K')\n\n    model = model_class(n_iter=1,\n                        use_cuda=CUDA)\n    model.fit(interactions)\n\n    for user_id in np.random.randint(0, interactions.num_users, size=10):\n        user_ids = np.repeat(user_id, interactions.num_items)\n        item_ids = np.arange(interactions.num_items)\n\n        uid_predictions = model.predict(user_id)\n        iid_predictions = model.predict(user_id, item_ids)\n        pair_predictions = model.predict(user_ids, item_ids)\n\n        assert (uid_predictions == iid_predictions).all()\n        assert (uid_predictions == pair_predictions).all()\n"""
tests/factorization/test_explicit.py,0,"b""import os\n\nimport numpy as np\nimport pytest\n\nfrom spotlight.cross_validation import random_train_test_split\nfrom spotlight.datasets import movielens\nfrom spotlight.evaluation import rmse_score\nfrom spotlight.factorization.explicit import ExplicitFactorizationModel\nfrom spotlight.factorization.representations import BilinearNet\nfrom spotlight.layers import BloomEmbedding\n\n\nRANDOM_STATE = np.random.RandomState(42)\nCUDA = bool(os.environ.get('SPOTLIGHT_CUDA', False))\n# Acceptable variation in specific test runs\nEPSILON = .005\n\n\ndef test_regression():\n\n    interactions = movielens.get_movielens_dataset('100K')\n\n    train, test = random_train_test_split(interactions,\n                                          random_state=RANDOM_STATE)\n\n    model = ExplicitFactorizationModel(loss='regression',\n                                       n_iter=10,\n                                       batch_size=1024,\n                                       learning_rate=1e-3,\n                                       l2=1e-5,\n                                       use_cuda=CUDA)\n    model.fit(train)\n\n    rmse = rmse_score(model, test)\n\n    assert rmse - EPSILON < 1.0\n\n\ndef test_poisson():\n\n    interactions = movielens.get_movielens_dataset('100K')\n\n    train, test = random_train_test_split(interactions,\n                                          random_state=RANDOM_STATE)\n\n    model = ExplicitFactorizationModel(loss='poisson',\n                                       n_iter=10,\n                                       batch_size=1024,\n                                       learning_rate=1e-3,\n                                       l2=1e-6,\n                                       use_cuda=CUDA)\n    model.fit(train)\n\n    rmse = rmse_score(model, test)\n\n    assert rmse - EPSILON < 1.0\n\n\ndef test_logistic():\n\n    interactions = movielens.get_movielens_dataset('100K')\n\n    # Convert to binary\n    interactions.ratings = (interactions.ratings > 3).astype(np.float32)\n    # Convert from (0, 1) to (-1, 1)\n    interactions.ratings = interactions.ratings * 2 - 1\n\n    train, test = random_train_test_split(interactions,\n                                          random_state=RANDOM_STATE)\n\n    model = ExplicitFactorizationModel(loss='logistic',\n                                       n_iter=10,\n                                       batch_size=1024,\n                                       learning_rate=1e-3,\n                                       l2=1e-6,\n                                       use_cuda=CUDA)\n    model.fit(train)\n\n    rmse = rmse_score(model, test)\n\n    assert rmse - EPSILON < 1.05\n\n\ndef test_check_input():\n    # Train for single iter.\n    interactions = movielens.get_movielens_dataset('100K')\n\n    train, test = random_train_test_split(interactions,\n                                          random_state=RANDOM_STATE)\n\n    model = ExplicitFactorizationModel(loss='regression',\n                                       n_iter=1,\n                                       batch_size=1024,\n                                       learning_rate=1e-3,\n                                       l2=1e-6,\n                                       use_cuda=CUDA)\n    model.fit(train)\n\n    # Modify data to make imcompatible with original model.\n    train.user_ids[0] = train.user_ids.max() + 1\n    with pytest.raises(ValueError):\n        model.fit(train)\n\n\n@pytest.mark.parametrize('compression_ratio, expected_rmse', [\n    (0.2, 1.5),\n    (0.5, 1.5),\n    (1.0, 1.5),\n    (1.5, 1.5),\n    (2.0, 1.5),\n])\ndef test_bloom(compression_ratio, expected_rmse):\n\n    interactions = movielens.get_movielens_dataset('100K')\n\n    train, test = random_train_test_split(interactions,\n                                          random_state=RANDOM_STATE)\n\n    user_embeddings = BloomEmbedding(interactions.num_users, 32,\n                                     compression_ratio=compression_ratio,\n                                     num_hash_functions=2)\n    item_embeddings = BloomEmbedding(interactions.num_items, 32,\n                                     compression_ratio=compression_ratio,\n                                     num_hash_functions=2)\n    network = BilinearNet(interactions.num_users,\n                          interactions.num_items,\n                          user_embedding_layer=user_embeddings,\n                          item_embedding_layer=item_embeddings)\n\n    model = ExplicitFactorizationModel(loss='regression',\n                                       n_iter=10,\n                                       batch_size=1024,\n                                       learning_rate=1e-2,\n                                       l2=1e-5,\n                                       representation=network,\n                                       use_cuda=CUDA)\n\n    model.fit(train)\n    print(model)\n\n    rmse = rmse_score(model, test)\n    print(rmse)\n\n    assert rmse - EPSILON < expected_rmse\n"""
tests/factorization/test_implicit.py,1,"b""import os\n\nimport numpy as np\nimport pytest\nimport torch\n\nfrom spotlight.cross_validation import random_train_test_split\nfrom spotlight.datasets import movielens\nfrom spotlight.evaluation import mrr_score\nfrom spotlight.factorization.implicit import ImplicitFactorizationModel\nfrom spotlight.factorization.representations import BilinearNet\nfrom spotlight.layers import BloomEmbedding\n\nRANDOM_STATE = np.random.RandomState(42)\nCUDA = bool(os.environ.get('SPOTLIGHT_CUDA', False))\n# Acceptable variation in specific test runs\nEPSILON = .005\n\n\ndef test_pointwise():\n\n    interactions = movielens.get_movielens_dataset('100K')\n\n    train, test = random_train_test_split(interactions,\n                                          random_state=RANDOM_STATE)\n\n    model = ImplicitFactorizationModel(loss='pointwise',\n                                       n_iter=10,\n                                       batch_size=1024,\n                                       learning_rate=1e-2,\n                                       l2=1e-6,\n                                       use_cuda=CUDA)\n    model.fit(train)\n\n    mrr = mrr_score(model, test, train=train).mean()\n\n    assert mrr + EPSILON > 0.05\n\n\ndef test_bpr():\n\n    interactions = movielens.get_movielens_dataset('100K')\n\n    train, test = random_train_test_split(interactions,\n                                          random_state=RANDOM_STATE)\n\n    model = ImplicitFactorizationModel(loss='bpr',\n                                       n_iter=10,\n                                       batch_size=1024,\n                                       learning_rate=1e-2,\n                                       l2=1e-6,\n                                       use_cuda=CUDA)\n    model.fit(train)\n\n    mrr = mrr_score(model, test, train=train).mean()\n\n    assert mrr + EPSILON > 0.07\n\n\ndef test_bpr_custom_optimizer():\n\n    interactions = movielens.get_movielens_dataset('100K')\n\n    train, test = random_train_test_split(interactions,\n                                          random_state=RANDOM_STATE)\n\n    def adagrad_optimizer(model_params,\n                          lr=1e-2,\n                          weight_decay=1e-6):\n\n        return torch.optim.Adagrad(model_params,\n                                   lr=lr,\n                                   weight_decay=weight_decay)\n\n    model = ImplicitFactorizationModel(loss='bpr',\n                                       n_iter=10,\n                                       batch_size=1024,\n                                       optimizer_func=adagrad_optimizer,\n                                       use_cuda=CUDA)\n    model.fit(train)\n\n    mrr = mrr_score(model, test, train=train).mean()\n\n    assert mrr + EPSILON > 0.05\n\n\ndef test_hinge():\n\n    interactions = movielens.get_movielens_dataset('100K')\n\n    train, test = random_train_test_split(interactions,\n                                          random_state=RANDOM_STATE)\n\n    model = ImplicitFactorizationModel(loss='hinge',\n                                       n_iter=10,\n                                       batch_size=1024,\n                                       learning_rate=1e-2,\n                                       l2=1e-6,\n                                       use_cuda=CUDA)\n    model.fit(train)\n\n    mrr = mrr_score(model, test, train=train).mean()\n\n    assert mrr + EPSILON > 0.07\n\n\ndef test_adaptive_hinge():\n\n    interactions = movielens.get_movielens_dataset('100K')\n\n    train, test = random_train_test_split(interactions,\n                                          random_state=RANDOM_STATE)\n\n    model = ImplicitFactorizationModel(loss='adaptive_hinge',\n                                       n_iter=10,\n                                       batch_size=1024,\n                                       learning_rate=1e-2,\n                                       l2=1e-6,\n                                       use_cuda=CUDA)\n    model.fit(train)\n\n    mrr = mrr_score(model, test, train=train).mean()\n\n    assert mrr + EPSILON > 0.07\n\n\n@pytest.mark.parametrize('compression_ratio, expected_mrr', [\n    (0.5, 0.03),\n    (1.0, 0.04),\n    (1.5, 0.045),\n    (2.0, 0.045),\n])\ndef test_bpr_bloom(compression_ratio, expected_mrr):\n\n    interactions = movielens.get_movielens_dataset('100K')\n\n    train, test = random_train_test_split(interactions,\n                                          random_state=RANDOM_STATE)\n\n    user_embeddings = BloomEmbedding(interactions.num_users, 32,\n                                     compression_ratio=compression_ratio,\n                                     num_hash_functions=2)\n    item_embeddings = BloomEmbedding(interactions.num_items, 32,\n                                     compression_ratio=compression_ratio,\n                                     num_hash_functions=2)\n    network = BilinearNet(interactions.num_users,\n                          interactions.num_items,\n                          user_embedding_layer=user_embeddings,\n                          item_embedding_layer=item_embeddings)\n\n    model = ImplicitFactorizationModel(loss='bpr',\n                                       n_iter=10,\n                                       batch_size=1024,\n                                       learning_rate=1e-2,\n                                       l2=1e-6,\n                                       representation=network,\n                                       use_cuda=CUDA)\n\n    model.fit(train)\n    print(model)\n\n    mrr = mrr_score(model, test, train=train).mean()\n\n    assert mrr + EPSILON > expected_mrr\n"""
tests/sequence/test_sequence_implicit.py,0,"b""import os\n\nimport numpy as np\n\nimport pytest\n\nfrom spotlight.cross_validation import user_based_train_test_split\nfrom spotlight.datasets import synthetic\nfrom spotlight.evaluation import sequence_mrr_score\nfrom spotlight.layers import BloomEmbedding\nfrom spotlight.sequence.implicit import ImplicitSequenceModel\nfrom spotlight.sequence.representations import CNNNet, LSTMNet, PoolNet\n\n\nRANDOM_SEED = 42\nNUM_EPOCHS = 5\nEMBEDDING_DIM = 32\nBATCH_SIZE = 128\nLOSS = 'bpr'\nVERBOSE = True\nCUDA = bool(os.environ.get('SPOTLIGHT_CUDA', False))\n\n\ndef _get_synthetic_data(num_users=100,\n                        num_items=100,\n                        num_interactions=10000,\n                        randomness=0.01,\n                        order=2,\n                        max_sequence_length=10,\n                        random_state=None):\n\n    interactions = synthetic.generate_sequential(num_users=num_users,\n                                                 num_items=num_items,\n                                                 num_interactions=num_interactions,\n                                                 concentration_parameter=randomness,\n                                                 order=order,\n                                                 random_state=random_state)\n\n    print('Max prob {}'.format((np.unique(interactions.item_ids,\n                                          return_counts=True)[1] /\n                                num_interactions).max()))\n\n    train, test = user_based_train_test_split(interactions,\n                                              random_state=random_state)\n\n    train = train.to_sequence(max_sequence_length=max_sequence_length,\n                              step_size=None)\n    test = test.to_sequence(max_sequence_length=max_sequence_length,\n                            step_size=None)\n\n    return train, test\n\n\ndef _evaluate(model, test):\n\n    test_mrr = sequence_mrr_score(model, test)\n\n    print('Test MRR {}'.format(\n        test_mrr.mean()\n    ))\n\n    return test_mrr\n\n\n@pytest.mark.parametrize('randomness, expected_mrr', [\n    (1e-3, 0.18),\n    (1e2, 0.03),\n])\ndef test_implicit_pooling_synthetic(randomness, expected_mrr):\n\n    random_state = np.random.RandomState(RANDOM_SEED)\n    train, test = _get_synthetic_data(randomness=randomness,\n                                      random_state=random_state)\n\n    model = ImplicitSequenceModel(loss=LOSS,\n                                  batch_size=BATCH_SIZE,\n                                  embedding_dim=EMBEDDING_DIM,\n                                  learning_rate=1e-1,\n                                  l2=1e-9,\n                                  n_iter=NUM_EPOCHS,\n                                  random_state=random_state,\n                                  use_cuda=CUDA)\n    model.fit(train, verbose=VERBOSE)\n\n    mrr = _evaluate(model, test)\n\n    assert mrr.mean() > expected_mrr\n\n\n@pytest.mark.parametrize('randomness, expected_mrr', [\n    (1e-3, 0.61),\n    (1e2, 0.03),\n])\ndef test_implicit_lstm_synthetic(randomness, expected_mrr):\n\n    random_state = np.random.RandomState(RANDOM_SEED)\n    train, test = _get_synthetic_data(randomness=randomness,\n                                      random_state=random_state)\n\n    model = ImplicitSequenceModel(loss=LOSS,\n                                  representation='lstm',\n                                  batch_size=BATCH_SIZE,\n                                  embedding_dim=EMBEDDING_DIM,\n                                  learning_rate=1e-2,\n                                  l2=1e-7,\n                                  n_iter=NUM_EPOCHS * 5,\n                                  random_state=random_state,\n                                  use_cuda=CUDA)\n\n    model.fit(train, verbose=VERBOSE)\n\n    mrr = _evaluate(model, test)\n\n    assert mrr.mean() > expected_mrr\n\n\n@pytest.mark.parametrize('randomness, expected_mrr', [\n    (1e-3, 0.65),\n    (1e2, 0.03),\n])\ndef test_implicit_cnn_synthetic(randomness, expected_mrr):\n\n    random_state = np.random.RandomState(RANDOM_SEED)\n    train, test = _get_synthetic_data(randomness=randomness,\n                                      random_state=random_state)\n\n    model = ImplicitSequenceModel(loss=LOSS,\n                                  representation=CNNNet(train.num_items,\n                                                        embedding_dim=EMBEDDING_DIM,\n                                                        kernel_width=5,\n                                                        num_layers=1),\n                                  batch_size=BATCH_SIZE,\n                                  learning_rate=1e-2,\n                                  l2=0.0,\n                                  n_iter=NUM_EPOCHS * 5,\n                                  random_state=random_state,\n                                  use_cuda=CUDA)\n\n    model.fit(train, verbose=VERBOSE)\n\n    mrr = _evaluate(model, test)\n\n    assert mrr.mean() > expected_mrr\n\n\n@pytest.mark.parametrize('num_layers, dilation, expected_mrr', [\n    (1, (1,), 0.65),\n    (2, (1, 2), 0.65),\n])\ndef test_implicit_cnn_dilation_synthetic(num_layers, dilation, expected_mrr):\n\n    random_state = np.random.RandomState(RANDOM_SEED)\n    train, test = _get_synthetic_data(randomness=1e-03,\n                                      num_interactions=20000,\n                                      random_state=random_state)\n\n    model = ImplicitSequenceModel(loss=LOSS,\n                                  representation=CNNNet(train.num_items,\n                                                        embedding_dim=EMBEDDING_DIM,\n                                                        kernel_width=3,\n                                                        dilation=dilation,\n                                                        num_layers=num_layers),\n                                  batch_size=BATCH_SIZE,\n                                  learning_rate=1e-2,\n                                  l2=0.0,\n                                  n_iter=NUM_EPOCHS * 5 * num_layers,\n                                  random_state=random_state,\n                                  use_cuda=CUDA)\n\n    model.fit(train, verbose=VERBOSE)\n\n    mrr = _evaluate(model, test)\n\n    assert mrr.mean() > expected_mrr\n\n\n@pytest.mark.parametrize('randomness, expected_mrr', [\n    (1e-3, 0.3),\n    (1e2, 0.03),\n])\ndef test_implicit_lstm_mixture_synthetic(randomness, expected_mrr):\n\n    random_state = np.random.RandomState(RANDOM_SEED)\n    train, test = _get_synthetic_data(randomness=randomness,\n                                      random_state=random_state)\n\n    model = ImplicitSequenceModel(loss=LOSS,\n                                  representation='mixture',\n                                  batch_size=BATCH_SIZE,\n                                  embedding_dim=EMBEDDING_DIM,\n                                  learning_rate=1e-2,\n                                  l2=1e-7,\n                                  n_iter=NUM_EPOCHS * 10,\n                                  random_state=random_state,\n                                  use_cuda=CUDA)\n\n    model.fit(train, verbose=VERBOSE)\n\n    mrr = _evaluate(model, test)\n\n    assert mrr.mean() > expected_mrr\n\n\n@pytest.mark.parametrize('loss, expected_mrr', [\n    ('pointwise', 0.15),\n    ('hinge', 0.16),\n    ('bpr', 0.18),\n    ('adaptive_hinge', 0.16),\n])\ndef test_implicit_pooling_losses(loss, expected_mrr):\n\n    random_state = np.random.RandomState(RANDOM_SEED)\n    train, test = _get_synthetic_data(randomness=1e-3,\n                                      random_state=random_state)\n\n    model = ImplicitSequenceModel(loss=loss,\n                                  batch_size=BATCH_SIZE,\n                                  embedding_dim=EMBEDDING_DIM,\n                                  learning_rate=1e-1,\n                                  l2=1e-9,\n                                  n_iter=NUM_EPOCHS,\n                                  random_state=random_state,\n                                  use_cuda=CUDA)\n    model.fit(train, verbose=VERBOSE)\n\n    mrr = _evaluate(model, test)\n\n    assert mrr.mean() > expected_mrr\n\n\n@pytest.mark.parametrize('compression_ratio, expected_mrr', [\n    (0.2, 0.14),\n    (0.5, 0.30),\n    (1.0, 0.5),\n])\ndef test_bloom_cnn(compression_ratio, expected_mrr):\n\n    random_state = np.random.RandomState(RANDOM_SEED)\n    train, test = _get_synthetic_data(randomness=1e-03,\n                                      num_interactions=20000,\n                                      random_state=random_state)\n\n    embedding = BloomEmbedding(train.num_items,\n                               32,\n                               compression_ratio=compression_ratio,\n                               num_hash_functions=2)\n\n    representation = CNNNet(train.num_items,\n                            embedding_dim=EMBEDDING_DIM,\n                            kernel_width=3,\n                            item_embedding_layer=embedding)\n\n    model = ImplicitSequenceModel(loss=LOSS,\n                                  representation=representation,\n                                  batch_size=BATCH_SIZE,\n                                  learning_rate=1e-2,\n                                  l2=0.0,\n                                  n_iter=NUM_EPOCHS,\n                                  random_state=random_state,\n                                  use_cuda=CUDA)\n\n    model.fit(train, verbose=VERBOSE)\n\n    mrr = _evaluate(model, test)\n\n    assert mrr.mean() > expected_mrr\n\n\n@pytest.mark.parametrize('compression_ratio, expected_mrr', [\n    (0.2, 0.18),\n    (0.5, 0.40),\n    (1.0, 0.60),\n])\ndef test_bloom_lstm(compression_ratio, expected_mrr):\n\n    random_state = np.random.RandomState(RANDOM_SEED)\n    train, test = _get_synthetic_data(randomness=1e-03,\n                                      num_interactions=20000,\n                                      random_state=random_state)\n\n    embedding = BloomEmbedding(train.num_items,\n                               32,\n                               compression_ratio=compression_ratio,\n                               num_hash_functions=4)\n\n    representation = LSTMNet(train.num_items,\n                             embedding_dim=EMBEDDING_DIM,\n                             item_embedding_layer=embedding)\n\n    model = ImplicitSequenceModel(loss=LOSS,\n                                  representation=representation,\n                                  batch_size=BATCH_SIZE,\n                                  learning_rate=1e-2,\n                                  l2=1e-7,\n                                  n_iter=NUM_EPOCHS * 5,\n                                  random_state=random_state,\n                                  use_cuda=CUDA)\n\n    model.fit(train, verbose=VERBOSE)\n\n    mrr = _evaluate(model, test)\n\n    assert mrr.mean() > expected_mrr\n\n\n@pytest.mark.parametrize('compression_ratio, expected_mrr', [\n    (0.2, 0.06),\n    (0.5, 0.07),\n    (1.0, 0.13),\n])\ndef test_bloom_pooling(compression_ratio, expected_mrr):\n\n    random_state = np.random.RandomState(RANDOM_SEED)\n    train, test = _get_synthetic_data(randomness=1e-03,\n                                      num_interactions=20000,\n                                      random_state=random_state)\n\n    embedding = BloomEmbedding(train.num_items,\n                               32,\n                               compression_ratio=compression_ratio,\n                               num_hash_functions=2)\n\n    representation = PoolNet(train.num_items,\n                             embedding_dim=EMBEDDING_DIM,\n                             item_embedding_layer=embedding)\n\n    model = ImplicitSequenceModel(loss=LOSS,\n                                  representation=representation,\n                                  batch_size=BATCH_SIZE,\n                                  learning_rate=1e-2,\n                                  l2=1e-7,\n                                  n_iter=NUM_EPOCHS * 5,\n                                  random_state=random_state,\n                                  use_cuda=CUDA)\n\n    model.fit(train, verbose=VERBOSE)\n\n    mrr = _evaluate(model, test)\n\n    assert mrr.mean() > expected_mrr\n"""
