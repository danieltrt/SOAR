file_path,api_count,code
main.py,0,"b'import argparse\nimport datetime\nfrom pathlib import Path\n\nfrom allennlp.commands import main, Subcommand\nfrom allennlp.commands.train import train_model\nfrom allennlp.common import Params\nfrom allennlp.common.util import import_submodules\nfrom allennlp.models import Model\n\n\nclass MyTrain(Subcommand):\n    def add_subparser(self, name: str, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:\n        # pylint: disable=protected-access\n        description = \'\'\'Train the specified model on the specified dataset.\'\'\'\n        subparser = parser.add_parser(name, description=description, help=\'Train a model\')\n\n        subparser.add_argument(\'param_path\',\n                               type=str,\n                               help=\'path to parameter file describing the model to be trained\')\n\n        subparser.add_argument(\'-s\', \'--serialization-dir\',\n                               required=False,\n                               default="""",\n                               type=str,\n                               help=\'directory in which to save the model and its logs\')\n\n        subparser.add_argument(\'-r\', \'--recover\',\n                               action=\'store_true\',\n                               default=False,\n                               help=\'recover training from the state in serialization_dir\')\n\n        subparser.add_argument(\'-f\', \'--force\',\n                               action=\'store_true\',\n                               required=False,\n                               help=\'overwrite the output directory if it exists\')\n\n        subparser.add_argument(\'-o\', \'--overrides\',\n                               type=str,\n                               default="""",\n                               help=\'a JSON structure used to override the experiment configuration\')\n\n\n        subparser.add_argument(\'-e\', \'--ext-vars\',\n                               type=str,\n                               default=None,\n                               help=\'Used to provide ext variable to jsonnet\')\n\n        subparser.add_argument(\'--fp16\',\n                               action=\'store_true\',\n                               required=False,\n                               help=\'use fp 16 training\')\n\n        subparser.add_argument(\'--file-friendly-logging\',\n                               action=\'store_true\',\n                               default=False,\n                               help=\'outputs tqdm status on separate lines and slows tqdm refresh rate\')\n\n        subparser.set_defaults(func=train_model_from_args)\n\n        return subparser\n\n\ndef train_model_from_args(args: argparse.Namespace):\n    """"""\n    Just converts from an ``argparse.Namespace`` object to string paths.\n    """"""\n\n    start_time = datetime.datetime.now().strftime(\'%b-%d_%H-%M\')\n\n    if args.serialization_dir:\n        serialization_dir = args.serialization_dir\n    else:\n        path = Path(args.param_path.replace(""configs/"", ""results/"")).resolve()\n        serialization_dir = path.with_name(path.stem) / start_time\n\n\n    train_model_from_file(args.param_path,\n                          serialization_dir,\n                          args.overrides,\n                          args.file_friendly_logging,\n                          args.recover,\n                          args.force,\n                          args.ext_vars)\n\ndef train_model_from_file(parameter_filename: str,\n                          serialization_dir: str,\n                          overrides: str = """",\n                          file_friendly_logging: bool = False,\n                          recover: bool = False,\n                          force: bool = False,\n                          ext_vars=None) -> Model:\n    """"""\n    A wrapper around :func:`train_model` which loads the params from a file.\n\n    Parameters\n    ----------\n    param_path : ``str``\n        A json parameter file specifying an AllenNLP experiment.\n    serialization_dir : ``str``\n        The directory in which to save results and logs. We just pass this along to\n        :func:`train_model`.\n    overrides : ``str``\n        A JSON string that we will use to override values in the input parameter file.\n    file_friendly_logging : ``bool``, optional (default=False)\n        If ``True``, we make our output more friendly to saved model files.  We just pass this\n        along to :func:`train_model`.\n    recover : ``bool`, optional (default=False)\n        If ``True``, we will try to recover a training run from an existing serialization\n        directory.  This is only intended for use when something actually crashed during the middle\n        of a run.  For continuing training a model on new data, see the ``fine-tune`` command.\n    """"""\n    # Load the experiment config from a file and pass it to ``train_model``.\n    params = Params.from_file(parameter_filename, overrides, ext_vars=ext_vars)\n    return train_model(params, serialization_dir, file_friendly_logging, recover, force)\n\n\nif __name__ == ""__main__"":\n    import_submodules(""qa"")\n    import_submodules(""modules"")\n    main(prog=""ReadingZoo"",subcommand_overrides={""train"": MyTrain()})'"
modules/__init__.py,0,b''
modules/dropout.py,0,"b'from torch import nn\n\n\nclass RNNDropout(nn.Module):\n    def __init__(self, p, batch_first=False):\n        super().__init__()\n        self.dropout = nn.Dropout(p)\n        self.batch_first = batch_first\n\n    def forward(self, inputs):\n\n        if not self.training:\n            return inputs\n        if self.batch_first:\n            mask = inputs.new_ones(inputs.size(0), 1, inputs.size(2), requires_grad=False)\n        else:\n            mask = inputs.new_ones(1, inputs.size(1), inputs.size(2), requires_grad=False)\n        return self.dropout(mask) * inputs\n'"
modules/gate.py,0,"b'from torch import nn\n\nfrom modules.dropout import RNNDropout\n\n\nclass Gate(nn.Module):\n    def __init__(self, input_size, dropout=0.3):\n        super().__init__()\n        self.gate = nn.Sequential(\n            RNNDropout(dropout),\n            nn.Linear(input_size, input_size, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, inputs):\n        return inputs * self.gate(inputs)\n'"
modules/utils.py,3,"b'import torch\n\n\ndef reverse_padded_sequence_fast(inputs, lengths, batch_first=False):\n    """"""Reverses sequences according to their lengths.\n    Inputs should have size ``T x B x *`` if ``batch_first`` is False, or\n    ``B x T x *`` if True. T is the length of the longest sequence (or larger),\n    B is the batch size, and * is any number of dimensions (including 0).\n    Arguments:\n        inputs (Variable): padded batch of variable length sequences.\n        lengths (list[int]): list of sequence lengths\n        batch_first (bool, optional): if True, inputs should be B x T x *.\n    Returns:\n        A Variable with the same size as inputs, but with each sequence\n        reversed according to its length.\n    """"""\n    if not batch_first:\n        inputs = inputs.transpose(0, 1)\n    if inputs.size(0) != len(lengths):\n        raise ValueError(\'inputs incompatible with lengths.\')\n    reversed_indices = [list(range(inputs.size(1))) for _ in range(inputs.size(0))]\n    for i, length in enumerate(lengths):\n        if length > 0:\n            reversed_indices[i][:length] = reversed_indices[i][length-1::-1]\n    reversed_indices = torch.LongTensor(reversed_indices).unsqueeze(2).expand_as(inputs)\n    if inputs.is_cuda:\n        reversed_indices = reversed_indices.cuda()\n    reversed_inputs = torch.gather(inputs, 1, reversed_indices)\n    if not batch_first:\n        reversed_inputs = reversed_inputs.transpose(0, 1)\n    return reversed_inputs\n\n\ndef get_rnn(rnn_type):\n    return getattr(torch.nn, rnn_type)'"
qa/__init__.py,0,b''
tests/__init__.py,0,b''
modules/pair_encoder/__init__.py,0,b''
modules/pair_encoder/attentions.py,8,"b'import torch\nfrom torch import nn\n\nfrom modules.dropout import RNNDropout\nfrom allennlp.nn.util import masked_softmax\nfrom torch import Tensor\n\ndef unroll_attention_cell(cell, inputs, memory, memory_mask, batch_first=False, initial_state=None, backward=False):\n    if batch_first:\n        inputs = inputs.transpose(0, 1)\n    output = []\n    state = initial_state\n    steps = range(inputs.size(0))\n    if backward:\n        steps = range(inputs.size(0)-1, -1, -1)\n    for t in steps:\n        state = cell(inputs[t], memory=memory, memory_mask=memory_mask, state=state)\n        output.append(state)\n    if backward:\n        output = output[::-1]\n    output = torch.stack(output, dim=1 if batch_first else 0)\n    return output, state\n\n\ndef bidirectional_unroll_attention_cell(cell_fw, cell_bw, inputs, memory, memory_mask, batch_first=False,\n                                        initial_state=None):\n    if initial_state is None:\n        initial_state = [None, None]\n\n    output_fw, state_fw = unroll_attention_cell(\n        cell_fw, inputs, memory, memory_mask,\n        batch_first=batch_first,\n        initial_state=initial_state[0], backward=False)\n\n    output_bw, state_bw = unroll_attention_cell(\n        cell_bw, inputs, memory, memory_mask,\n        batch_first=batch_first,\n        initial_state=initial_state[1], backward=True)\n\n    return torch.cat([output_fw, output_bw], dim=-1), (state_fw, state_bw)\n\n\n# class StaticAddAttention(nn.Module):\n#     def __init__(self, memory_size, input_size, attention_size, dropout=0.2, batch_first=False):\n#         super().__init__()\n#         self.batch_first = batch_first\n\n#         self.attention_w = nn.Sequential(\n#             nn.Linear(input_size + memory_size, attention_size, bias=False),\n#             nn.Dropout(dropout),\n#             nn.Tanh(),\n#             nn.Linear(attention_size, 1, bias=False),\n#             nn.Dropout(dropout),\n#         )\n\n#     def forward(self, inputs: Tensor, memory: Tensor, memory_mask: Tensor):\n#         if not self.batch_first:\n#             raise NotImplementedError\n\n#         T = inputs.size(0)\n#         memory_mask = memory_mask.unsqueeze(0)\n#         memory_key = memory.unsqueeze(0).expand(T, -1, -1, -1)\n#         input_key = inputs.unsqueeze(1).expand(-1, T, -1, -1)\n#         attention_logits = self.attention_w(torch.cat([input_key, memory_key], -1)).squeeze(-1)\n#         logits, score = softmax_mask(attention_logits, memory_mask, dim=1)\n#         context = torch.sum(score.unsqueeze(-1) * inputs.unsqueeze(0), dim=1)\n#         new_input = torch.cat([context, inputs], dim=-1)\n#         return new_input\n\n\nclass StaticDotAttention(nn.Module):\n    def __init__(self, memory_size, input_size, attention_size, batch_first=False, dropout=0.2):\n\n        super().__init__()\n\n        self.input_linear = nn.Sequential(\n            RNNDropout(dropout, batch_first=True),\n            nn.Linear(input_size, attention_size, bias=False),\n            nn.ReLU()\n        )\n\n        self.memory_linear = nn.Sequential(\n            RNNDropout(dropout, batch_first=True),\n            nn.Linear(memory_size, attention_size, bias=False),\n            nn.ReLU()\n        )\n        self.attention_size = attention_size\n        self.batch_first = batch_first\n\n    def forward(self, inputs: Tensor, memory: Tensor, memory_mask: Tensor):\n        if not self.batch_first:\n            inputs = inputs.transpose(0, 1)\n            memory = memory.transpose(0, 1)\n            memory_mask = memory_mask.transpose(0, 1)\n\n        input_ = self.input_linear(inputs)\n        memory_ = self.memory_linear(memory)\n\n        logits = torch.bmm(input_, memory_.transpose(2, 1)) / (self.attention_size ** 0.5)\n\n        memory_mask = memory_mask.unsqueeze(1).expand(-1, inputs.size(1), -1)\n        score = masked_softmax(logits, memory_mask, dim=-1)\n\n        context = torch.bmm(score, memory)\n        new_input = torch.cat([context, inputs], dim=-1)\n\n        if not self.batch_first:\n            return new_input.transpose(0, 1)\n        return new_input\n\n'"
modules/pair_encoder/cells.py,4,"b'import torch\nfrom torch import nn\nfrom torch import Tensor\nfrom allennlp.nn.util import masked_softmax\n\nclass _PairEncodeCell(nn.Module):\n    def __init__(self, input_size, cell, attention_size, memory_size=None, use_state_in_attention=True, batch_first=False):\n        super().__init__()\n        if memory_size is None:\n            memory_size = input_size\n\n        self.cell = cell\n        self.use_state = use_state_in_attention\n\n        attention_input_size = input_size + memory_size\n        if use_state_in_attention:\n            attention_input_size += cell.hidden_size\n\n        self.attention_w = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(attention_input_size, attention_size, bias=False),\n            nn.Tanh(),\n            nn.Linear(attention_size, 1, bias=False))\n\n        self.batch_first = batch_first\n\n    def forward(self, inputs: Tensor, memory: Tensor = None, memory_mask: Tensor = None, state: Tensor = None):\n        """"""\n        :param inputs:  B x H\n        :param memory: T x B x H if not batch_first\n        :param memory_mask: T x B if not batch_first\n        :param state: B x H\n        :return:\n        """"""\n        if self.batch_first:\n            memory = memory.transpose(0, 1)\n            memory_mask = memory_mask.transpose(0, 1)\n\n        assert inputs.size(0) == memory.size(1) == memory_mask.size(\n            1), ""inputs batch size does not match memory batch size""\n\n        memory_time_length = memory.size(0)\n\n        if state is None:\n            state = inputs.new_zeros(inputs.size(0), self.cell.hidden_size, requires_grad=False)\n\n        if self.use_state:\n            hx = state\n            if isinstance(state, tuple):\n                hx = state[0]\n            attention_input = torch.cat([inputs, hx], dim=-1)\n            attention_input = attention_input.unsqueeze(0).expand(memory_time_length, -1, -1)  # T B H\n        else:\n            attention_input = inputs.unsqueeze(0).expand(memory_time_length, -1, -1)\n\n        attention_logits = self.attention_w(torch.cat([attention_input, memory], dim=-1)).squeeze(-1)\n\n        attention_scores = masked_softmax(attention_logits, memory_mask, dim=0)\n\n        attention_vector = torch.sum(attention_scores.unsqueeze(-1) * memory, dim=0)\n\n        new_input = torch.cat([inputs, attention_vector], dim=-1)\n\n        return self.cell(new_input, state)\n\n\nclass PairEncodeCell(_PairEncodeCell):\n    def __init__(self, input_size, cell, attention_size,\n                 memory_size=None, batch_first=False):\n        super().__init__(\n            input_size, cell, attention_size,\n            memory_size=memory_size, use_state_in_attention=True, batch_first=batch_first)\n\n\nclass SelfMatchCell(_PairEncodeCell):\n    def __init__(self, input_size, cell, attention_size,\n                 memory_size=None, batch_first=False):\n        super().__init__(\n            input_size, cell, attention_size,\n            memory_size=memory_size, use_state_in_attention=False, batch_first=batch_first)\n'"
modules/pair_encoder/pair_encoder.py,0,"b'from overrides import overrides\nfrom torch import nn\n\nfrom modules.dropout import RNNDropout\nfrom modules.gate import Gate\nfrom modules.pair_encoder.attentions import bidirectional_unroll_attention_cell, unroll_attention_cell, \\\n    StaticDotAttention\nfrom modules.pair_encoder.cells import PairEncodeCell, SelfMatchCell\nfrom allennlp.common import Registrable\n\nfrom modules.utils import get_rnn\nfrom allennlp.modules import Seq2SeqEncoder\n\n\nclass AttentionEncoder(nn.Module, Registrable):\n    def forward(self, inputs, inputs_mask, memory, memory_mask):\n        raise NotImplementedError\n\n\nclass DynamicAttentionEncoder(AttentionEncoder):\n    def __init__(self, memory_size, input_size, hidden_size, attention_size,\n                 bidirectional, dropout, cell_factory=PairEncodeCell, batch_first=False):\n        super().__init__()\n        self.batch_first = batch_first\n        cell_fn = lambda: cell_factory(input_size, cell=nn.GRUCell(input_size + memory_size, hidden_size),\n                                       attention_size=attention_size, memory_size=memory_size,\n                                       batch_first=batch_first)\n\n        num_directions = 2 if bidirectional else 1\n        self.bidirectional = bidirectional\n\n        self.cells = nn.ModuleList([cell_fn() for _ in range(num_directions)])\n        self.dropout = RNNDropout(dropout)\n        self.gate = Gate(input_size=hidden_size*2 if bidirectional else hidden_size, dropout=dropout)\n\n    @overrides\n    def forward(self, inputs, inputs_mask, memory, memory_mask):\n        if self.bidirectional:\n            cell_fw, cell_bw = self.cells\n            output, _ = bidirectional_unroll_attention_cell(\n                cell_fw, cell_bw, inputs, memory, memory_mask,\n                batch_first=self.batch_first)\n        else:\n            cell, = self.cells\n            output, _ = unroll_attention_cell(\n                cell, inputs, memory, memory_mask, batch_first=self.batch_first)\n\n        return self.gate(output)\n\n\n@AttentionEncoder.register(""dynamic_pair_encoder"")\nclass DynamicPairEncoder(DynamicAttentionEncoder):\n    def __init__(self, memory_size, input_size, hidden_size, attention_size,\n                 bidirectional, dropout, batch_first=False):\n        super().__init__(memory_size, input_size, hidden_size, attention_size,\n                         bidirectional, dropout, PairEncodeCell, batch_first=batch_first)\n\n\n@AttentionEncoder.register(""dynamic_self_encoder"")\nclass DynamicSelfEncoder(DynamicAttentionEncoder):\n    def __init__(self, memory_size, input_size, hidden_size, attention_size,\n                 bidirectional, dropout, batch_first=False):\n        super().__init__(memory_size, input_size, hidden_size, attention_size,\n                         bidirectional, dropout, SelfMatchCell, batch_first=batch_first)\n\n\n\n\n@AttentionEncoder.register(""pass_through"")\nclass PassThrough(AttentionEncoder):\n    def __init__(self, memory_size, input_size, hidden_size):\n        super().__init__()\n\n\n    @overrides\n    def forward(self, inputs, inputs_mask, memory, memory_mask):\n        return inputs\n\n\n\n@AttentionEncoder.register(""static_pair_encoder"")\nclass StaticPairEncoder(AttentionEncoder):\n    def __init__(self, memory_size, input_size, hidden_size, attention_size, bidirectional, dropout,\n                 attention_factory=StaticDotAttention, rnn_type=""GRU"", batch_first=True):\n        super().__init__()\n        rnn_fn = Seq2SeqEncoder.by_name(rnn_type.lower())\n\n        self.attention = attention_factory(memory_size, input_size, attention_size,\n                                           dropout=dropout, batch_first=batch_first)\n\n        self.gate = nn.Sequential(\n            Gate(input_size + memory_size, dropout=dropout),\n            RNNDropout(dropout, batch_first=batch_first)\n        )\n\n        self.encoder = rnn_fn(input_size=memory_size + input_size, hidden_size=hidden_size,\n                              bidirectional=bidirectional, batch_first=batch_first)\n\n    @overrides\n    def forward(self, inputs, inputs_mask, memory, memory_mask):\n        """"""\n        Memory: T B H\n        input: T B H\n        """"""\n        new_inputs = self.gate(self.attention(inputs, memory, memory_mask))\n        outputs = self.encoder(new_inputs, inputs_mask)\n        return outputs\n\n\n@AttentionEncoder.register(""static_self_encoder"")\nclass StaticSelfMatchEncoder(StaticPairEncoder):\n    pass\n'"
modules/pointer_network/__init__.py,0,b'from .pointer_network import PointerNetwork'
modules/pointer_network/pointer_network.py,5,"b'from allennlp.common import Registrable\nfrom torch import nn\nimport torch\nfrom allennlp.nn.util import masked_softmax\nfrom modules.dropout import RNNDropout\nfrom overrides import overrides\n\nclass QAOutputLayer(nn.Module, Registrable):\n    pass\n\n@QAOutputLayer.register(""pointer_network"")\nclass PointerNetwork(QAOutputLayer):\n    def __init__(self, question_size, passage_size, attention_size=75,\n                 cell_type=nn.GRUCell, dropout=0, batch_first=False):\n        super().__init__()\n\n        self.batch_first = batch_first\n\n        # TODO: what is V_q? (section 3.4)\n        v_q_size = question_size\n\n        self.cell = cell_type(passage_size, question_size)\n        self.dropout = dropout\n\n        self.passage_linear = nn.Sequential(\n            RNNDropout(dropout),\n            nn.Linear(question_size + passage_size, attention_size, bias=False),\n            nn.Tanh(),\n            nn.Linear(attention_size, 1, bias=False),\n        )\n\n        self.question_linear = nn.Sequential(\n            RNNDropout(dropout),\n            nn.Linear(question_size + v_q_size, attention_size, bias=False),\n            nn.Tanh(),\n            nn.Linear(attention_size, 1, bias=False),\n        )\n\n        self.V_q = nn.Parameter(torch.randn(1, 1, v_q_size), requires_grad=True)\n\n    @overrides\n    def forward(self, question, question_mask, passage, passage_mask):\n        """"""\n        :param question: T B H\n        :param question_mask: T B\n        :param passage:\n        :param passage_mask:\n        :return: B x 2\n        """"""\n\n        if self.batch_first:\n            question = question.transpose(0, 1)\n            question_mask = question_mask.transpose(0, 1)\n            passage = passage.transpose(0, 1)\n            passage_mask = passage_mask.transpose(0, 1)\n\n        state = self._question_pooling(question, question_mask)\n        cell_input, ans_start_logits = self._passage_attention(passage, passage_mask, state)\n        state = self.cell(cell_input, hx=state)\n        _, ans_end_logits = self._passage_attention(passage, passage_mask, state)\n\n\n        return ans_start_logits.transpose(0, 1), ans_end_logits.transpose(0, 1)\n\n    def _question_pooling(self, question, question_mask):\n        V_q = self.V_q.expand(question.size(0), question.size(1), -1)\n        logits = self.question_linear(torch.cat([question, V_q], dim=-1)).squeeze(-1)\n        score = masked_softmax(logits, question_mask, dim=0)\n        state = torch.sum(score.unsqueeze(-1) * question, dim=0)\n        return state\n\n    def _passage_attention(self, passage, passage_mask, state):\n        state_expand = state.unsqueeze(0).expand(passage.size(0), -1, -1)\n        logits = self.passage_linear(torch.cat([passage, state_expand], dim=-1)).squeeze(-1)\n        score = masked_softmax(logits, passage_mask, dim=0)\n        cell_input = torch.sum(score.unsqueeze(-1) * passage, dim=0)\n        return cell_input, logits\n'"
modules/rnn/__init__.py,0,b''
modules/rnn/stacked_rnn.py,2,"b'import torch\nfrom allennlp.modules import Seq2SeqEncoder\n\nfrom modules.dropout import RNNDropout\n\n\n@Seq2SeqEncoder.register(""concat_rnn"")\nclass ConcatRNN(Seq2SeqEncoder):\n    def __init__(self, input_size, hidden_size, num_layers, bidirectional, rnn_type=""GRU"", dropout=0., stateful=False, batch_first=True):\n        super().__init__(stateful=stateful)\n        rnn_cls = Seq2SeqEncoder.by_name(rnn_type.lower())\n        self.rnn_list = torch.nn.ModuleList(\n            [rnn_cls(input_size=input_size, hidden_size=hidden_size, bidirectional=bidirectional, dropout=dropout)])\n\n        for _ in range(num_layers - 1):\n            self.rnn_list.append(\n                rnn_cls(input_size=hidden_size * 2, hidden_size=hidden_size, bidirectional=bidirectional,\n                        dropout=dropout))\n\n        self.dropout = RNNDropout(dropout, batch_first=batch_first)\n\n    def forward(self, inputs, mask, hidden=None):\n        outputs_list = []\n\n        for layer in self.rnn_list:\n            outputs = layer(self.dropout(inputs), mask, hidden)\n            outputs_list.append(outputs)\n            inputs = outputs\n\n        return torch.cat(outputs_list, -1)'"
qa/squad/__init__.py,0,b''
qa/squad/dataset.py,0,"b'import logging\nfrom typing import Dict, Tuple, Dict, List\n\nfrom allennlp.common.file_utils import cached_path\nfrom allennlp.data import DatasetReader, Tokenizer, TokenIndexer, Token, Instance\nfrom allennlp.data.dataset_readers.reading_comprehension.util import make_reading_comprehension_instance, \\\n    char_span_to_token_span\nfrom allennlp.data.token_indexers import SingleIdTokenIndexer\nfrom allennlp.data.tokenizers import WordTokenizer\nfrom overrides import overrides\nimport json\n\nlogger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n\n\n\n@DatasetReader.register(""squad_truncated"")\nclass SquadReader(DatasetReader):\n    """"""\n    Modified from allennlp.data.dataset_readers.reading_comprehension.squad\n\n    Reads a JSON-formatted SQuAD file and returns a ``Dataset`` where the ``Instances`` have four\n    fields: ``question``, a ``TextField``, ``passage``, another ``TextField``, and ``span_start``\n    and ``span_end``, both ``IndexFields`` into the ``passage`` ``TextField``.  We also add a\n    ``MetadataField`` that stores the instance\'s ID, the original passage text, gold answer strings,\n    and token offsets into the original passage, accessible as ``metadata[\'id\']``,\n    ``metadata[\'original_passage\']``, ``metadata[\'answer_texts\']`` and\n    ``metadata[\'token_offsets\']``.  This is so that we can more easily use the official SQuAD\n    evaluation script to get metrics.\n\n    Parameters\n    ----------\n    tokenizer : ``Tokenizer``, optional (default=``WordTokenizer()``)\n        We use this ``Tokenizer`` for both the question and the passage.  See :class:`Tokenizer`.\n        Default is ```WordTokenizer()``.\n    token_indexers : ``Dict[str, TokenIndexer]``, optional\n        We similarly use this for both the question and the passage.  See :class:`TokenIndexer`.\n        Default is ``{""tokens"": SingleIdTokenIndexer()}``.\n    """"""\n    def __init__(self,\n                 tokenizer: Tokenizer = None,\n                 token_indexers: Dict[str, TokenIndexer] = None,\n                 lazy: bool = False,\n                 max_passage_len=400,\n                 truncate_train_only=True) -> None:\n        super().__init__(lazy)\n        self._tokenizer = tokenizer or WordTokenizer()\n        self._token_indexers = token_indexers or {\'tokens\': SingleIdTokenIndexer()}\n        self.max_passage_len = max_passage_len\n        self.truncate_train_only = truncate_train_only\n\n    @overrides\n    def _read(self, file_path: str):\n        if ""train"" in file_path:\n            is_train = True\n        else:\n            is_train = False\n\n        # if `file_path` is a URL, redirect to the cache\n        file_path = cached_path(file_path)\n\n        logger.info(""Reading file at %s"", file_path)\n        with open(file_path) as dataset_file:\n            dataset_json = json.load(dataset_file)\n            dataset = dataset_json[\'data\']\n        logger.info(""Reading the dataset"")\n        for article in dataset:\n            for paragraph_json in article[\'paragraphs\']:\n                paragraph = paragraph_json[""context""]\n                tokenized_paragraph = self._tokenizer.tokenize(paragraph)\n\n                if len(tokenized_paragraph) > self.max_passage_len:\n                    if is_train or not self.truncate_train_only:\n                        continue\n\n                for question_answer in paragraph_json[\'qas\']:\n                    question_text = question_answer[""question""].strip().replace(""\\n"", """")\n                    answer_texts = [answer[\'text\'] for answer in question_answer[\'answers\']]\n                    span_starts = [answer[\'answer_start\'] for answer in question_answer[\'answers\']]\n                    span_ends = [start + len(answer) for start, answer in zip(span_starts, answer_texts)]\n                    instance = self.text_to_instance(question_text,\n                                                     paragraph,\n                                                     zip(span_starts, span_ends),\n                                                     answer_texts,\n                                                     tokenized_paragraph)\n                    yield instance\n\n    @overrides\n    def text_to_instance(self,  # type: ignore\n                         question_text: str,\n                         passage_text: str,\n                         char_spans: List[Tuple[int, int]] = None,\n                         answer_texts: List[str] = None,\n                         passage_tokens: List[Token] = None) -> Instance:\n        # pylint: disable=arguments-differ\n        if not passage_tokens:\n            passage_tokens = self._tokenizer.tokenize(passage_text)\n        char_spans = char_spans or []\n\n        # We need to convert character indices in `passage_text` to token indices in\n        # `passage_tokens`, as the latter is what we\'ll actually use for supervision.\n        token_spans: List[Tuple[int, int]] = []\n        passage_offsets = [(token.idx, token.idx + len(token.text)) for token in passage_tokens]\n        for char_span_start, char_span_end in char_spans:\n            (span_start, span_end), error = char_span_to_token_span(passage_offsets,\n                                                                         (char_span_start, char_span_end))\n            if error:\n                logger.debug(""Passage: %s"", passage_text)\n                logger.debug(""Passage tokens: %s"", passage_tokens)\n                logger.debug(""Question text: %s"", question_text)\n                logger.debug(""Answer span: (%d, %d)"", char_span_start, char_span_end)\n                logger.debug(""Token span: (%d, %d)"", span_start, span_end)\n                logger.debug(""Tokens in answer: %s"", passage_tokens[span_start:span_end + 1])\n                logger.debug(""Answer: %s"", passage_text[char_span_start:char_span_end])\n            token_spans.append((span_start, span_end))\n\n        return make_reading_comprehension_instance(self._tokenizer.tokenize(question_text),\n                                                        passage_tokens,\n                                                        self._token_indexers,\n                                                        passage_text,\n                                                        token_spans,\n                                                        answer_texts)\n'"
qa/squad/rnet.py,10,"b'from typing import Dict, Optional, List, Any\n\nimport torch\nfrom allennlp.data import Vocabulary\nfrom allennlp.models import BidirectionalAttentionFlow\nfrom allennlp.models.model import Model\nfrom allennlp.modules import TextFieldEmbedder, Seq2SeqEncoder\nfrom allennlp.nn import InitializerApplicator, RegularizerApplicator\nfrom allennlp.nn import util\nfrom allennlp.nn.util import get_text_field_mask\nfrom allennlp.training.metrics import CategoricalAccuracy, BooleanAccuracy, SquadEmAndF1\nfrom torch.nn.functional import nll_loss\n\nfrom modules.pair_encoder.pair_encoder import AttentionEncoder\nfrom modules.pointer_network.pointer_network import QAOutputLayer\n\n@Model.register(""r_net"")\nclass RNet(Model):\n    def __init__(self,\n                 vocab: Vocabulary,\n                 text_field_embedder: TextFieldEmbedder,\n                 question_encoder: Seq2SeqEncoder,\n                 passage_encoder: Seq2SeqEncoder,\n                 pair_encoder: AttentionEncoder,\n                 self_encoder: AttentionEncoder,\n                 output_layer: QAOutputLayer,\n                 initializer: InitializerApplicator = InitializerApplicator(),\n                 regularizer: Optional[RegularizerApplicator] = None,\n                 share_encoder: bool = False):\n\n        super().__init__(vocab, regularizer)\n        self.text_field_embedder = text_field_embedder\n        self.question_encoder = question_encoder\n        self.passage_encoder = passage_encoder\n        self.pair_encoder = pair_encoder\n        self.self_encoder = self_encoder\n        self.output_layer = output_layer\n\n        self._span_start_accuracy = CategoricalAccuracy()\n        self._span_end_accuracy = CategoricalAccuracy()\n        self._span_accuracy = BooleanAccuracy()\n        self._squad_metrics = SquadEmAndF1()\n        self.share_encoder = share_encoder\n        self.loss = torch.nn.CrossEntropyLoss()\n        initializer(self)\n\n    def forward(self,\n                question: Dict[str, torch.LongTensor],\n                passage: Dict[str, torch.LongTensor],\n                span_start: torch.IntTensor = None,\n                span_end: torch.IntTensor = None,\n                metadata: List[Dict[str, Any]] = None) -> Dict[str, torch.Tensor]:\n\n        question_embeded = self.text_field_embedder(question)\n        passage_embeded = self.text_field_embedder(passage)\n\n        question_mask = get_text_field_mask(question).byte()\n        passage_mask = get_text_field_mask(passage).byte()\n\n        quetion_encoded = self.question_encoder(\n            question_embeded, question_mask)\n        \n        if self.share_encoder:\n            passage_encoded = self.question_encoder(passage_embeded, passage_mask)\n        else:\n            passage_encoded = self.passage_encoder(passage_embeded, passage_mask)\n\n        passage_encoded = self.pair_encoder(\n            passage_encoded, passage_mask, quetion_encoded, question_mask)\n        passage_encoded = self.self_encoder(\n            passage_encoded, passage_mask, passage_encoded, passage_mask)\n\n        span_start_logits, span_end_logits = self.output_layer(\n            quetion_encoded, question_mask, passage_encoded, passage_mask)\n\n        # Calculating loss and making prediction\n        # Following code is copied from allennlp.models.BidirectionalAttentionFlow\n        span_start_probs = util.masked_softmax(span_start_logits, passage_mask)\n        span_end_probs = util.masked_softmax(span_end_logits, passage_mask)\n\n        span_start_logits = util.replace_masked_values(\n            span_start_logits, passage_mask, -1e7)\n        span_end_logits = util.replace_masked_values(\n            span_end_logits, passage_mask, -1e7)\n\n        best_span = self.get_best_span(span_start_logits, span_end_logits)\n\n        output_dict = {\n            ""span_start_logits"": span_start_logits,\n            ""span_start_probs"": span_start_probs,\n            ""span_end_logits"": span_end_logits,\n            ""span_end_probs"": span_end_probs,\n            ""best_span"": best_span,\n        }\n\n\n        if span_start is not None:\n            loss = nll_loss(util.masked_log_softmax(\n                span_start_logits, passage_mask), span_start.squeeze(-1))\n            self._span_start_accuracy(\n                span_start_logits, span_start.squeeze(-1))\n            loss += nll_loss(util.masked_log_softmax(span_end_logits,\n                                                     passage_mask), span_end.squeeze(-1))\n            self._span_end_accuracy(span_end_logits, span_end.squeeze(-1))\n            self._span_accuracy(best_span, torch.stack(\n                [span_start, span_end], -1))\n            output_dict[""loss""] = loss\n\n        # Compute the EM and F1 on SQuAD and add the tokenized input to the output.\n        if metadata is not None:\n            output_dict[\'best_span_str\'] = []\n            question_tokens = []\n            passage_tokens = []\n            batch_size = question_embeded.size(0)\n            for i in range(batch_size):\n                question_tokens.append(metadata[i][\'question_tokens\'])\n                passage_tokens.append(metadata[i][\'passage_tokens\'])\n                passage_str = metadata[i][\'original_passage\']\n                offsets = metadata[i][\'token_offsets\']\n                predicted_span = tuple(best_span[i].detach().cpu().numpy())\n                start_offset = offsets[predicted_span[0]][0]\n                end_offset = offsets[predicted_span[1]][1]\n                best_span_string = passage_str[start_offset:end_offset]\n                output_dict[\'best_span_str\'].append(best_span_string)\n                answer_texts = metadata[i].get(\'answer_texts\', [])\n                if answer_texts:\n                    self._squad_metrics(best_span_string, answer_texts)\n            output_dict[\'question_tokens\'] = question_tokens\n            output_dict[\'passage_tokens\'] = passage_tokens\n        return output_dict\n\n    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n        exact_match, f1_score = self._squad_metrics.get_metric(reset)\n        return {\'start_acc\': self._span_start_accuracy.get_metric(reset),\n                \'end_acc\': self._span_end_accuracy.get_metric(reset),\n                \'span_acc\': self._span_accuracy.get_metric(reset),\n                \'em\': exact_match,\n                \'f1\': f1_score,\n                }\n\n    @staticmethod\n    def get_best_span(span_start_logits: torch.Tensor, span_end_logits: torch.Tensor) -> torch.Tensor:\n        if span_start_logits.dim() != 2 or span_end_logits.dim() != 2:\n            raise ValueError(""Input shapes must be (batch_size, passage_length)"")\n        batch_size, passage_length = span_start_logits.size()\n        max_span_log_prob = [-1e20] * batch_size\n        span_start_argmax = [0] * batch_size\n        best_word_span = span_start_logits.new_zeros((batch_size, 2), dtype=torch.long)\n\n        span_start_logits = span_start_logits.detach().cpu().numpy()\n        span_end_logits = span_end_logits.detach().cpu().numpy()\n\n        for b in range(batch_size):  # pylint: disable=invalid-name\n            for j in range(passage_length):\n                val1 = span_start_logits[b, span_start_argmax[b]]\n                if val1 < span_start_logits[b, j]:\n                    span_start_argmax[b] = j\n                    val1 = span_start_logits[b, j]\n\n                val2 = span_end_logits[b, j]\n\n                if val1 + val2 > max_span_log_prob[b]:\n                    best_word_span[b, 0] = span_start_argmax[b]\n                    best_word_span[b, 1] = j\n                    max_span_log_prob[b] = val1 + val2\n        return best_word_span\n'"
tests/models/__init__.py,0,b''
tests/models/r-net_dynamic_test.py,0,"b""from allennlp.common.testing import ModelTestCase\nfrom qa.squad.rnet import RNet\n\nclass RNetDynamicTest(ModelTestCase):\n    def setUp(self):\n        super().setUp()\n        self.set_up_model('tests/fixtures/rnet/experiment_dynamic.jsonnet',\n                          'tests/fixtures/data/squad.json')\n\n    def test_model_can_train_save_and_load(self):\n        self.ensure_model_can_train_save_and_load(self.param_file)\n"""
tests/models/r-net_test.py,0,"b""from allennlp.common.testing import ModelTestCase\nfrom qa.squad.rnet import RNet\nfrom modules.rnn.stacked_rnn import ConcatRNN\nclass RNetTest(ModelTestCase):\n    def setUp(self):\n        super().setUp()\n        self.set_up_model('tests/fixtures/rnet/experiment.jsonnet',\n                          'tests/fixtures/data/squad.json')\n \n    def test_model_can_train_save_and_load(self):\n        self.ensure_model_can_train_save_and_load(self.param_file)\n\n"""
