file_path,api_count,code
local.py,0,"b'import sys\nsys.path.append(""/usr/local/lib/python3.5/dist-packages"")\n'"
make_graph.py,1,"b""# From https://gist.github.com/apaszke/01aae7a0494c55af6242f06fad1f8b70\nfrom graphviz import Digraph\nfrom torch.autograd import Variable\n\ndef save(fname, creator):\n    dot = Digraph(comment='LRP',\n                node_attr={'style': 'filled', 'shape': 'box'})\n    #, 'fillcolor': 'lightblue'})\n\n    seen = set()\n\n    def add_nodes(var):\n        if var not in seen:\n            if isinstance(var, Variable):\n                dot.node(str(id(var)), str(var.size()), fillcolor='lightblue')\n            else:\n                dot.node(str(id(var)), type(var).__name__)\n            seen.add(var)\n            if hasattr(var, 'previous_functions'):\n                for u in var.previous_functions:\n                    dot.edge(str(id(u[0])), str(id(var)))\n                    add_nodes(u[0])\n\n    add_nodes(creator)\n    dot.save(fname)\n"""
plot.py,0,"b""#!/usr/bin/env python3\n\nimport argparse\nimport os\nimport numpy as np\n\nimport matplotlib as mpl\nmpl.use('Agg')\nimport matplotlib.pyplot as plt\nplt.style.use('bmh')\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('nBatches', type=int)\n    parser.add_argument('expDir', type=str)\n    args = parser.parse_args()\n\n    trainP = os.path.join(args.expDir, 'train.csv')\n    trainData = np.loadtxt(trainP, delimiter=',').reshape(-1, 3)\n    testP = os.path.join(args.expDir, 'test.csv')\n    testData = np.loadtxt(testP, delimiter=',').reshape(-1, 3)\n\n    N = args.nBatches\n    trainI, trainLoss, trainErr = np.split(trainData, [1,2], axis=1)\n    trainI, trainLoss, trainErr = [x.ravel() for x in\n                                   (trainI, trainLoss, trainErr)]\n    trainI_, trainLoss_, trainErr_ = rolling(N, trainI, trainLoss, trainErr)\n\n    testI, testLoss, testErr = np.split(testData, [1,2], axis=1)\n\n    fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n    # plt.plot(trainI, trainLoss, label='Train')\n    plt.plot(trainI_, trainLoss_, label='Train')\n    plt.plot(testI, testLoss, label='Test')\n    plt.xlabel('Epoch')\n    plt.ylabel('Cross-Entropy Loss')\n    plt.legend()\n    ax.set_yscale('log')\n    loss_fname = os.path.join(args.expDir, 'loss.png')\n    plt.savefig(loss_fname)\n    print('Created {}'.format(loss_fname))\n\n    fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n    # plt.plot(trainI, trainErr, label='Train')\n    plt.plot(trainI_, trainErr_, label='Train')\n    plt.plot(testI, testErr, label='Test')\n    plt.xlabel('Epoch')\n    plt.ylabel('Error')\n    ax.set_yscale('log')\n    plt.legend()\n    err_fname = os.path.join(args.expDir, 'error.png')\n    plt.savefig(err_fname)\n    print('Created {}'.format(err_fname))\n\n    loss_err_fname = os.path.join(args.expDir, 'loss-error.png')\n    os.system('convert +append {} {} {}'.format(loss_fname, err_fname, loss_err_fname))\n    print('Created {}'.format(loss_err_fname))\n\ndef rolling(N, i, loss, err):\n    i_ = i[N-1:]\n    K = np.full(N, 1./N)\n    loss_ = np.convolve(loss, K, 'valid')\n    err_ = np.convolve(err, K, 'valid')\n    return i_, loss_, err_\n\nif __name__ == '__main__':\n    main()\n"""
train.py,13,"b'#!/usr/bin/env python3\n\nfrom local import *\nimport time\nimport argparse\nimport torch\n\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.optim as optim\n\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport torchvision.transforms as transforms\n\nfrom torch.utils.data import DataLoader\n\nimport torchbiomed.datasets as dset\nimport torchbiomed.transforms as biotransforms\nimport torchbiomed.loss as bioloss\nimport torchbiomed.utils as utils\n\nimport os\nimport sys\nimport math\n\nimport shutil\n\nimport setproctitle\n\nimport vnet\nimport make_graph\nfrom functools import reduce\nimport operator\n\n\n#nodule_masks = ""normalized_mask_5_0""\n#lung_masks = ""normalized_seg_lungs_5_0""\n#ct_images = ""normalized_CT_5_0""\n#target_split = [1, 1, 1]\n#ct_targets = nodule_masks\n\n\nnodule_masks = ""normalized_brightened_CT_2_5""\nlung_masks = ""inferred_seg_lungs_2_5""\nct_images = ""luna16_ct_normalized""\nct_targets = nodule_masks\ntarget_split = [2, 2, 2]\n\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find(\'Conv3d\') != -1:\n        nn.init.kaiming_normal(m.weight)\n        m.bias.data.zero_()\n\ndef datestr():\n    now = time.gmtime()\n    return \'{}{:02}{:02}_{:02}{:02}\'.format(now.tm_year, now.tm_mon, now.tm_mday, now.tm_hour, now.tm_min)\n\n\ndef save_checkpoint(state, is_best, path, prefix, filename=\'checkpoint.pth.tar\'):\n    prefix_save = os.path.join(path, prefix)\n    name = prefix_save + \'_\' + filename\n    torch.save(state, name)\n    if is_best:\n        shutil.copyfile(name, prefix_save + \'_model_best.pth.tar\')\n\n\ndef inference(args, loader, model, transforms):\n    src = args.inference\n    dst = args.save\n\n    model.eval()\n    nvols = reduce(operator.mul, target_split, 1)\n    # assume single GPU / batch size 1\n    for data in loader:\n        data, series, origin, spacing = data[0]\n        shape = data.size()\n        # convert names to batch tensor\n        if args.cuda:\n            data.pin_memory()\n            data = data.cuda()\n        data = Variable(data, volatile=True)\n        output = model(data)\n        _, output = output.max(1)\n        output = output.view(shape)\n        output = output.cpu()\n        # merge subvolumes and save\n        results = output.chunk(nvols)\n        results = map(lambda var : torch.squeeze(var.data).numpy().astype(np.int16), results)\n        volume = utils.merge_image([*results], target_split)\n        print(""save {}"".format(series))\n        utils.save_updated_image(volume, os.path.join(dst, series + "".mhd""), origin, spacing)\n\n# performing post-train inference:\n# train.py --resume <model checkpoint> --i <input directory (*.mhd)> --save <output directory>\n\ndef noop(x):\n    return x\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--batchSz\', type=int, default=10)\n    parser.add_argument(\'--dice\', action=\'store_true\')\n    parser.add_argument(\'--ngpu\', type=int, default=1)\n    parser.add_argument(\'--nEpochs\', type=int, default=300)\n    parser.add_argument(\'--start-epoch\', default=0, type=int, metavar=\'N\',\n                        help=\'manual epoch number (useful on restarts)\')\n    parser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                        help=\'path to latest checkpoint (default: none)\')\n    parser.add_argument(\'-e\', \'--evaluate\', dest=\'evaluate\', action=\'store_true\',\n                        help=\'evaluate model on validation set\')\n    parser.add_argument(\'-i\', \'--inference\', default=\'\', type=str, metavar=\'PATH\',\n                        help=\'run inference on data set and save results\')\n\n    # 1e-8 works well for lung masks but seems to prevent\n    # rapid learning for nodule masks\n    parser.add_argument(\'--weight-decay\', \'--wd\', default=1e-8, type=float,\n                        metavar=\'W\', help=\'weight decay (default: 1e-8)\')\n    parser.add_argument(\'--no-cuda\', action=\'store_true\')\n    parser.add_argument(\'--save\')\n    parser.add_argument(\'--seed\', type=int, default=1)\n    parser.add_argument(\'--opt\', type=str, default=\'adam\',\n                        choices=(\'sgd\', \'adam\', \'rmsprop\'))\n    args = parser.parse_args()\n    best_prec1 = 100.\n    args.cuda = not args.no_cuda and torch.cuda.is_available()\n    args.save = args.save or \'work/vnet.base.{}\'.format(datestr())\n    nll = True\n    if args.dice:\n        nll = False\n    weight_decay = args.weight_decay\n    setproctitle.setproctitle(args.save)\n\n    torch.manual_seed(args.seed)\n    if args.cuda:\n        torch.cuda.manual_seed(args.seed)\n\n    print(""build vnet"")\n    model = vnet.VNet(elu=False, nll=nll)\n    batch_size = args.ngpu*args.batchSz\n    gpu_ids = range(args.ngpu)\n    model = nn.parallel.DataParallel(model, device_ids=gpu_ids)\n\n    if args.resume:\n        if os.path.isfile(args.resume):\n            print(""=> loading checkpoint \'{}\'"".format(args.resume))\n            checkpoint = torch.load(args.resume)\n            args.start_epoch = checkpoint[\'epoch\']\n            best_prec1 = checkpoint[\'best_prec1\']\n            model.load_state_dict(checkpoint[\'state_dict\'])\n            print(""=> loaded checkpoint \'{}\' (epoch {})""\n                  .format(args.evaluate, checkpoint[\'epoch\']))\n        else:\n            print(""=> no checkpoint found at \'{}\'"".format(args.resume))\n    else:\n        model.apply(weights_init)\n\n    if nll:\n        train = train_nll\n        test = test_nll\n        class_balance = True\n    else:\n        train = train_dice\n        test = test_dice\n        class_balance = False\n\n    print(\'  + Number of params: {}\'.format(\n        sum([p.data.nelement() for p in model.parameters()])))\n    if args.cuda:\n        model = model.cuda()\n\n    if os.path.exists(args.save):\n        shutil.rmtree(args.save)\n    os.makedirs(args.save, exist_ok=True)\n\n    # LUNA16 dataset isotropically scaled to 2.5mm^3\n    # and then truncated or zero-padded to 160x128x160\n    normMu = [-642.794]\n    normSigma = [459.512]\n    normTransform = transforms.Normalize(normMu, normSigma)\n\n    trainTransform = transforms.Compose([\n        transforms.ToTensor(),\n        normTransform\n    ])\n    testTransform = transforms.Compose([\n        transforms.ToTensor(),\n        normTransform\n    ])\n    if ct_targets == nodule_masks:\n        masks = lung_masks\n    else:\n        masks = None\n\n    if args.inference != \'\':\n        if not args.resume:\n            print(""args.resume must be set to do inference"")\n            exit(1)\n        kwargs = {\'num_workers\': 1} if args.cuda else {}\n        src = args.inference\n        dst = args.save\n        inference_batch_size = args.ngpu\n        root = os.path.dirname(src)\n        images = os.path.basename(src)\n        dataset = dset.LUNA16(root=root, images=images, transform=testTransform, split=target_split, mode=""infer"")\n        loader = DataLoader(dataset, batch_size=inference_batch_size, shuffle=False, collate_fn=noop, **kwargs)\n        inference(args, loader, model, trainTransform)\n        return\n\n    kwargs = {\'num_workers\': 1, \'pin_memory\': True} if args.cuda else {}\n    print(""loading training set"")\n    trainSet = dset.LUNA16(root=\'luna16\', images=ct_images, targets=ct_targets,\n                           mode=""train"", transform=trainTransform, \n                           class_balance=class_balance, split=target_split, seed=args.seed, masks=masks)\n    trainLoader = DataLoader(trainSet, batch_size=batch_size, shuffle=True, **kwargs)\n    print(""loading test set"")\n    testLoader = DataLoader(\n        dset.LUNA16(root=\'luna16\', images=ct_images, targets=ct_targets,\n                    mode=""test"", transform=testTransform, seed=args.seed, masks=masks, split=target_split),\n        batch_size=batch_size, shuffle=False, **kwargs)\n\n    target_mean = trainSet.target_mean()\n    bg_weight = target_mean / (1. + target_mean)\n    fg_weight = 1. - bg_weight\n    print(bg_weight)\n    class_weights = torch.FloatTensor([bg_weight, fg_weight])\n    if args.cuda:\n        class_weights = class_weights.cuda()\n\n    if args.opt == \'sgd\':\n        optimizer = optim.SGD(model.parameters(), lr=1e-1,\n                              momentum=0.99, weight_decay=weight_decay)\n    elif args.opt == \'adam\':\n        optimizer = optim.Adam(model.parameters(), weight_decay=weight_decay)\n    elif args.opt == \'rmsprop\':\n        optimizer = optim.RMSprop(model.parameters(), weight_decay=weight_decay)\n\n    trainF = open(os.path.join(args.save, \'train.csv\'), \'w\')\n    testF = open(os.path.join(args.save, \'test.csv\'), \'w\')\n    err_best = 100.\n    for epoch in range(1, args.nEpochs + 1):\n        adjust_opt(args.opt, optimizer, epoch)\n        train(args, epoch, model, trainLoader, optimizer, trainF, class_weights)\n        err = test(args, epoch, model, testLoader, optimizer, testF, class_weights)\n        is_best = False\n        if err < best_prec1:\n            is_best = True\n            best_prec1 = err\n        save_checkpoint({\'epoch\': epoch,\n                         \'state_dict\': model.state_dict(),\n                         \'best_prec1\': best_prec1},\n                        is_best, args.save, ""vnet"")\n        os.system(\'./plot.py {} {} &\'.format(len(trainLoader), args.save))\n\n    trainF.close()\n    testF.close()\n\n\ndef train_nll(args, epoch, model, trainLoader, optimizer, trainF, weights):\n    model.train()\n    nProcessed = 0\n    nTrain = len(trainLoader.dataset)\n    for batch_idx, (data, target) in enumerate(trainLoader):\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data), Variable(target)\n        optimizer.zero_grad()\n        output = model(data)\n        target = target.view(target.numel())\n        loss = F.nll_loss(output, target, weight=weights)\n        dice_loss = bioloss.dice_error(output, target)\n        # make_graph.save(\'/tmp/t.dot\', loss.creator); assert(False)\n        loss.backward()\n        optimizer.step()\n        nProcessed += len(data)\n        pred = output.data.max(1)[1]  # get the index of the max log-probability\n        incorrect = pred.ne(target.data).cpu().sum()\n        err = 100.*incorrect/target.numel()\n        partialEpoch = epoch + batch_idx / len(trainLoader) - 1\n        print(\'Train Epoch: {:.2f} [{}/{} ({:.0f}%)]\\tLoss: {:.4f}\\tError: {:.3f}\\t Dice: {:.6f}\'.format(\n            partialEpoch, nProcessed, nTrain, 100. * batch_idx / len(trainLoader),\n            loss.data[0], err, dice_loss))\n\n        trainF.write(\'{},{},{}\\n\'.format(partialEpoch, loss.data[0], err))\n        trainF.flush()\n\n\ndef test_nll(args, epoch, model, testLoader, optimizer, testF, weights):\n    model.eval()\n    test_loss = 0\n    dice_loss = 0\n    incorrect = 0\n    numel = 0\n    for data, target in testLoader:\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data, volatile=True), Variable(target)\n        target = target.view(target.numel())\n        numel += target.numel()\n        output = model(data)\n        test_loss += F.nll_loss(output, target, weight=weights).data[0]\n        dice_loss += bioloss.dice_error(output, target)\n        pred = output.data.max(1)[1]  # get the index of the max log-probability\n        incorrect += pred.ne(target.data).cpu().sum()\n\n    test_loss /= len(testLoader)  # loss function already averages over batch size\n    dice_loss /= len(testLoader)\n    err = 100.*incorrect/numel\n    print(\'\\nTest set: Average loss: {:.4f}, Error: {}/{} ({:.3f}%) Dice: {:.6f}\\n\'.format(\n        test_loss, incorrect, numel, err, dice_loss))\n\n    testF.write(\'{},{},{}\\n\'.format(epoch, test_loss, err))\n    testF.flush()\n    return err\n\n\ndef train_dice(args, epoch, model, trainLoader, optimizer, trainF, weights):\n    model.train()\n    nProcessed = 0\n    nTrain = len(trainLoader.dataset)\n    for batch_idx, (data, target) in enumerate(trainLoader):\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data), Variable(target)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = bioloss.dice_loss(output, target)\n        # make_graph.save(\'/tmp/t.dot\', loss.creator); assert(False)\n        loss.backward()\n        optimizer.step()\n        nProcessed += len(data)\n        err = 100.*(1. - loss.data[0])\n        partialEpoch = epoch + batch_idx / len(trainLoader) - 1\n        print(\'Train Epoch: {:.2f} [{}/{} ({:.0f}%)]\\tLoss: {:.8f}\\tError: {:.8f}\'.format(\n            partialEpoch, nProcessed, nTrain, 100. * batch_idx / len(trainLoader),\n            loss.data[0], err))\n\n        trainF.write(\'{},{},{}\\n\'.format(partialEpoch, loss.data[0], err))\n        trainF.flush()\n\n\ndef test_dice(args, epoch, model, testLoader, optimizer, testF, weights):\n    model.eval()\n    test_loss = 0\n    incorrect = 0\n    for data, target in testLoader:\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = Variable(data, volatile=True), Variable(target)\n        output = model(data)\n        loss = bioloss.dice_loss(output, target).data[0]\n        test_loss += loss\n        incorrect += (1. - loss)\n\n    test_loss /= len(testLoader)  # loss function already averages over batch size\n    nTotal = len(testLoader)\n    err = 100.*incorrect/nTotal\n    print(\'\\nTest set: Average Dice Coeff: {:.4f}, Error: {}/{} ({:.0f}%)\\n\'.format(\n        test_loss, incorrect, nTotal, err))\n\n    testF.write(\'{},{},{}\\n\'.format(epoch, test_loss, err))\n    testF.flush()\n    return err\n\n\ndef adjust_opt(optAlg, optimizer, epoch):\n    if optAlg == \'sgd\':\n        if epoch < 150:\n            lr = 1e-1\n        elif epoch == 150:\n            lr = 1e-2\n        elif epoch == 225:\n            lr = 1e-3\n        else:\n            return\n\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = lr\n\nif __name__ == \'__main__\':\n    main()\n'"
vnet.py,7,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef passthrough(x, **kwargs):\n    return x\n\ndef ELUCons(elu, nchan):\n    if elu:\n        return nn.ELU(inplace=True)\n    else:\n        return nn.PReLU(nchan)\n\n# normalization between sub-volumes is necessary\n# for good performance\nclass ContBatchNorm3d(nn.modules.batchnorm._BatchNorm):\n    def _check_input_dim(self, input):\n        if input.dim() != 5:\n            raise ValueError('expected 5D input (got {}D input)'\n                             .format(input.dim()))\n        super(ContBatchNorm3d, self)._check_input_dim(input)\n\n    def forward(self, input):\n        self._check_input_dim(input)\n        return F.batch_norm(\n            input, self.running_mean, self.running_var, self.weight, self.bias,\n            True, self.momentum, self.eps)\n\n\nclass LUConv(nn.Module):\n    def __init__(self, nchan, elu):\n        super(LUConv, self).__init__()\n        self.relu1 = ELUCons(elu, nchan)\n        self.conv1 = nn.Conv3d(nchan, nchan, kernel_size=5, padding=2)\n        self.bn1 = ContBatchNorm3d(nchan)\n\n    def forward(self, x):\n        out = self.relu1(self.bn1(self.conv1(x)))\n        return out\n\n\ndef _make_nConv(nchan, depth, elu):\n    layers = []\n    for _ in range(depth):\n        layers.append(LUConv(nchan, elu))\n    return nn.Sequential(*layers)\n\n\nclass InputTransition(nn.Module):\n    def __init__(self, outChans, elu):\n        super(InputTransition, self).__init__()\n        self.conv1 = nn.Conv3d(1, 16, kernel_size=5, padding=2)\n        self.bn1 = ContBatchNorm3d(16)\n        self.relu1 = ELUCons(elu, 16)\n\n    def forward(self, x):\n        # do we want a PRELU here as well?\n        out = self.bn1(self.conv1(x))\n        # split input in to 16 channels\n        x16 = torch.cat((x, x, x, x, x, x, x, x,\n                         x, x, x, x, x, x, x, x), 0)\n        out = self.relu1(torch.add(out, x16))\n        return out\n\n\nclass DownTransition(nn.Module):\n    def __init__(self, inChans, nConvs, elu, dropout=False):\n        super(DownTransition, self).__init__()\n        outChans = 2*inChans\n        self.down_conv = nn.Conv3d(inChans, outChans, kernel_size=2, stride=2)\n        self.bn1 = ContBatchNorm3d(outChans)\n        self.do1 = passthrough\n        self.relu1 = ELUCons(elu, outChans)\n        self.relu2 = ELUCons(elu, outChans)\n        if dropout:\n            self.do1 = nn.Dropout3d()\n        self.ops = _make_nConv(outChans, nConvs, elu)\n\n    def forward(self, x):\n        down = self.relu1(self.bn1(self.down_conv(x)))\n        out = self.do1(down)\n        out = self.ops(out)\n        out = self.relu2(torch.add(out, down))\n        return out\n\n\nclass UpTransition(nn.Module):\n    def __init__(self, inChans, outChans, nConvs, elu, dropout=False):\n        super(UpTransition, self).__init__()\n        self.up_conv = nn.ConvTranspose3d(inChans, outChans // 2, kernel_size=2, stride=2)\n        self.bn1 = ContBatchNorm3d(outChans // 2)\n        self.do1 = passthrough\n        self.do2 = nn.Dropout3d()\n        self.relu1 = ELUCons(elu, outChans // 2)\n        self.relu2 = ELUCons(elu, outChans)\n        if dropout:\n            self.do1 = nn.Dropout3d()\n        self.ops = _make_nConv(outChans, nConvs, elu)\n\n    def forward(self, x, skipx):\n        out = self.do1(x)\n        skipxdo = self.do2(skipx)\n        out = self.relu1(self.bn1(self.up_conv(out)))\n        xcat = torch.cat((out, skipxdo), 1)\n        out = self.ops(xcat)\n        out = self.relu2(torch.add(out, xcat))\n        return out\n\n\nclass OutputTransition(nn.Module):\n    def __init__(self, inChans, elu, nll):\n        super(OutputTransition, self).__init__()\n        self.conv1 = nn.Conv3d(inChans, 2, kernel_size=5, padding=2)\n        self.bn1 = ContBatchNorm3d(2)\n        self.conv2 = nn.Conv3d(2, 2, kernel_size=1)\n        self.relu1 = ELUCons(elu, 2)\n        if nll:\n            self.softmax = F.log_softmax\n        else:\n            self.softmax = F.softmax\n\n    def forward(self, x):\n        # convolve 32 down to 2 channels\n        out = self.relu1(self.bn1(self.conv1(x)))\n        out = self.conv2(out)\n\n        # make channels the last axis\n        out = out.permute(0, 2, 3, 4, 1).contiguous()\n        # flatten\n        out = out.view(out.numel() // 2, 2)\n        out = self.softmax(out)\n        # treat channel 0 as the predicted output\n        return out\n\n\nclass VNet(nn.Module):\n    # the number of convolutions in each layer corresponds\n    # to what is in the actual prototxt, not the intent\n    def __init__(self, elu=True, nll=False):\n        super(VNet, self).__init__()\n        self.in_tr = InputTransition(16, elu)\n        self.down_tr32 = DownTransition(16, 1, elu)\n        self.down_tr64 = DownTransition(32, 2, elu)\n        self.down_tr128 = DownTransition(64, 3, elu, dropout=True)\n        self.down_tr256 = DownTransition(128, 2, elu, dropout=True)\n        self.up_tr256 = UpTransition(256, 256, 2, elu, dropout=True)\n        self.up_tr128 = UpTransition(256, 128, 2, elu, dropout=True)\n        self.up_tr64 = UpTransition(128, 64, 1, elu)\n        self.up_tr32 = UpTransition(64, 32, 1, elu)\n        self.out_tr = OutputTransition(32, elu, nll)\n\n    # The network topology as described in the diagram\n    # in the VNet paper\n    # def __init__(self):\n    #     super(VNet, self).__init__()\n    #     self.in_tr =  InputTransition(16)\n    #     # the number of convolutions in each layer corresponds\n    #     # to what is in the actual prototxt, not the intent\n    #     self.down_tr32 = DownTransition(16, 2)\n    #     self.down_tr64 = DownTransition(32, 3)\n    #     self.down_tr128 = DownTransition(64, 3)\n    #     self.down_tr256 = DownTransition(128, 3)\n    #     self.up_tr256 = UpTransition(256, 3)\n    #     self.up_tr128 = UpTransition(128, 3)\n    #     self.up_tr64 = UpTransition(64, 2)\n    #     self.up_tr32 = UpTransition(32, 1)\n    #     self.out_tr = OutputTransition(16)\n    def forward(self, x):\n        out16 = self.in_tr(x)\n        out32 = self.down_tr32(out16)\n        out64 = self.down_tr64(out32)\n        out128 = self.down_tr128(out64)\n        out256 = self.down_tr256(out128)\n        out = self.up_tr256(out256, out128)\n        out = self.up_tr128(out, out64)\n        out = self.up_tr64(out, out32)\n        out = self.up_tr32(out, out16)\n        out = self.out_tr(out)\n        return out\n"""
