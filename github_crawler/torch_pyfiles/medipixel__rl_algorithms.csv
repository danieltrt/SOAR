file_path,api_count,code
run_lunarlander_continuous_v2.py,0,"b'# -*- coding: utf-8 -*-\n""""""Train or test algorithms on LunarLanderContinuous-v2.\n\n- Author: Curt Park\n- Contact: curt.park@medipixel.io\n""""""\n\nimport argparse\nimport datetime\n\nimport gym\n\nfrom rl_algorithms import build_agent\nimport rl_algorithms.common.env.utils as env_utils\nimport rl_algorithms.common.helper_functions as common_utils\nfrom rl_algorithms.utils import Config\n\n\ndef parse_args() -> argparse.Namespace:\n    # configurations\n    parser = argparse.ArgumentParser(description=""Pytorch RL algorithms"")\n    parser.add_argument(\n        ""--seed"", type=int, default=777, help=""random seed for reproducibility""\n    )\n    parser.add_argument(\n        ""--cfg-path"",\n        type=str,\n        default=""./configs/lunarlander_continuous_v2/ddpg.py"",\n        help=""config path"",\n    )\n    parser.add_argument(\n        ""--test"", dest=""test"", action=""store_true"", help=""test mode (no training)""\n    )\n    parser.add_argument(\n        ""--load-from"",\n        type=str,\n        default=None,\n        help=""load the saved model and optimizer at the beginning"",\n    )\n    parser.add_argument(\n        ""--off-render"", dest=""render"", action=""store_false"", help=""turn off rendering""\n    )\n    parser.add_argument(\n        ""--render-after"",\n        type=int,\n        default=0,\n        help=""start rendering after the input number of episode"",\n    )\n    parser.add_argument(\n        ""--log"", dest=""log"", action=""store_true"", help=""turn on logging""\n    )\n    parser.add_argument(\n        ""--save-period"", type=int, default=100, help=""save model period""\n    )\n    parser.add_argument(\n        ""--episode-num"", type=int, default=1500, help=""total episode num""\n    )\n    parser.add_argument(\n        ""--max-episode-steps"", type=int, default=300, help=""max episode step""\n    )\n    parser.add_argument(\n        ""--interim-test-num"",\n        type=int,\n        default=10,\n        help=""number of test during training"",\n    )\n    parser.add_argument(\n        ""--demo-path"",\n        type=str,\n        default=""data/lunarlander_continuous_demo.pkl"",\n        help=""demonstration path for learning from demo"",\n    )\n\n    return parser.parse_args()\n\n\ndef main():\n    """"""Main.""""""\n    args = parse_args()\n\n    # env initialization\n    env = gym.make(""LunarLanderContinuous-v2"")\n    env_utils.set_env(env, args)\n\n    # set a random seed\n    common_utils.set_random_seed(args.seed, env)\n\n    # run\n    NOWTIMES = datetime.datetime.now()\n    curr_time = NOWTIMES.strftime(""%y%m%d_%H%M%S"")\n\n    cfg = Config.fromfile(args.cfg_path)\n    cfg.agent[""log_cfg""] = dict(agent=cfg.agent.type, curr_time=curr_time)\n    build_args = dict(args=args, env=env)\n    agent = build_agent(cfg.agent, build_args)\n\n    if not args.test:\n        agent.train()\n    else:\n        agent.test()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
run_lunarlander_v2.py,0,"b'# -*- coding: utf-8 -*-\n""""""Train or test algorithms on LunarLander-v2.\n\n- Author: Curt Park\n- Contact: curt.park@medipixel.io\n""""""\n\nimport argparse\nimport datetime\n\nimport gym\n\nfrom rl_algorithms import build_agent\nimport rl_algorithms.common.env.utils as env_utils\nimport rl_algorithms.common.helper_functions as common_utils\nfrom rl_algorithms.utils import Config\n\n\ndef parse_args() -> argparse.Namespace:\n    # configurations\n    parser = argparse.ArgumentParser(description=""Pytorch RL algorithms"")\n    parser.add_argument(\n        ""--seed"", type=int, default=777, help=""random seed for reproducibility""\n    )\n    parser.add_argument(\n        ""--cfg-path"",\n        type=str,\n        default=""./configs/lunarlander_v2/dqn.py"",\n        help=""config path"",\n    )\n    parser.add_argument(\n        ""--test"", dest=""test"", action=""store_true"", help=""test mode (no training)""\n    )\n    parser.add_argument(\n        ""--load-from"",\n        type=str,\n        default=None,\n        help=""load the saved model and optimizer at the beginning"",\n    )\n    parser.add_argument(\n        ""--off-render"", dest=""render"", action=""store_false"", help=""turn off rendering""\n    )\n    parser.add_argument(\n        ""--render-after"",\n        type=int,\n        default=0,\n        help=""start rendering after the input number of episode"",\n    )\n    parser.add_argument(\n        ""--log"", dest=""log"", action=""store_true"", help=""turn on logging""\n    )\n    parser.add_argument(\n        ""--save-period"", type=int, default=100, help=""save model period""\n    )\n    parser.add_argument(\n        ""--episode-num"", type=int, default=1500, help=""total episode num""\n    )\n    parser.add_argument(\n        ""--max-episode-steps"", type=int, default=300, help=""max episode step""\n    )\n    parser.add_argument(\n        ""--interim-test-num"",\n        type=int,\n        default=10,\n        help=""number of test during training"",\n    )\n    parser.add_argument(\n        ""--demo-path"",\n        type=str,\n        default=""data/lunarlander_discrete_demo.pkl"",\n        help=""demonstration path for learning from demo"",\n    )\n\n    return parser.parse_args()\n\n\ndef main():\n    """"""Main.""""""\n    args = parse_args()\n\n    # env initialization\n    env = gym.make(""LunarLander-v2"")\n    env_utils.set_env(env, args)\n\n    # set a random seed\n    common_utils.set_random_seed(args.seed, env)\n\n    # run\n    NOWTIMES = datetime.datetime.now()\n    curr_time = NOWTIMES.strftime(""%y%m%d_%H%M%S"")\n\n    cfg = Config.fromfile(args.cfg_path)\n    cfg.agent[""log_cfg""] = dict(agent=cfg.agent.type, curr_time=curr_time)\n    build_args = dict(args=args, env=env)\n    agent = build_agent(cfg.agent, build_args)\n\n    if not args.test:\n        agent.train()\n    else:\n        agent.test()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
run_pong_no_frameskip_v4.py,0,"b'# -*- coding: utf-8 -*-\n""""""Train or test algorithms on PongNoFrameskip-v4.\n\n- Author: Curt Park\n- Contact: curt.park@medipixel.io\n""""""\n\nimport argparse\nimport datetime\n\nfrom rl_algorithms import build_agent\nfrom rl_algorithms.common.env.atari_wrappers import atari_env_generator\nimport rl_algorithms.common.helper_functions as common_utils\nfrom rl_algorithms.utils import Config\n\n\ndef parse_args() -> argparse.Namespace:\n    # configurations\n    parser = argparse.ArgumentParser(description=""Pytorch RL algorithms"")\n    parser.add_argument(\n        ""--seed"", type=int, default=777, help=""random seed for reproducibility""\n    )\n    parser.add_argument(\n        ""--cfg-path"",\n        type=str,\n        default=""./configs/pong_no_frameskip_v4/dqn.py"",\n        help=""config path"",\n    )\n    parser.add_argument(\n        ""--test"", dest=""test"", action=""store_true"", help=""test mode (no training)""\n    )\n    parser.add_argument(\n        ""--grad-cam"",\n        dest=""grad_cam"",\n        action=""store_true"",\n        help=""test mode with viewing Grad-CAM"",\n    )\n    parser.add_argument(\n        ""--load-from"",\n        type=str,\n        default=None,\n        help=""load the saved model and optimizer at the beginning"",\n    )\n    parser.add_argument(\n        ""--off-render"", dest=""render"", action=""store_false"", help=""turn off rendering""\n    )\n    parser.add_argument(\n        ""--render-after"",\n        type=int,\n        default=0,\n        help=""start rendering after the input number of episode"",\n    )\n    parser.add_argument(\n        ""--log"", dest=""log"", action=""store_true"", help=""turn on logging""\n    )\n    parser.add_argument(""--save-period"", type=int, default=20, help=""save model period"")\n    parser.add_argument(\n        ""--episode-num"", type=int, default=1500, help=""total episode num""\n    )\n    parser.add_argument(\n        ""--max-episode-steps"", type=int, default=None, help=""max episode step""\n    )\n    parser.add_argument(\n        ""--interim-test-num"", type=int, default=10, help=""interim test number""\n    )\n\n    return parser.parse_args()\n\n\ndef main():\n    """"""Main.""""""\n    args = parse_args()\n\n    # env initialization\n    env_name = ""PongNoFrameskip-v4""\n    env = atari_env_generator(env_name, args.max_episode_steps)\n\n    # set a random seed\n    common_utils.set_random_seed(args.seed, env)\n\n    # run\n    NOWTIMES = datetime.datetime.now()\n    curr_time = NOWTIMES.strftime(""%y%m%d_%H%M%S"")\n\n    cfg = Config.fromfile(args.cfg_path)\n    cfg.agent.log_cfg = dict(agent=cfg.agent.type, curr_time=curr_time)\n    build_args = dict(args=args, env=env)\n    agent = build_agent(cfg.agent, build_args)\n\n    if not args.test:\n        agent.train()\n    elif args.test and args.grad_cam:\n        agent.test_with_gradcam()\n    else:\n        agent.test()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
run_reacher_v2.py,0,"b'# -*- coding: utf-8 -*-\n""""""Train or test algorithms on Reacher-v2 of Mujoco.\n\n- Author: Kyunghwan Kim\n- Contact: kh.kim@medipixel.io\n""""""\n\nimport argparse\nimport datetime\n\nimport gym\n\nfrom rl_algorithms import build_agent\nimport rl_algorithms.common.env.utils as env_utils\nimport rl_algorithms.common.helper_functions as common_utils\nfrom rl_algorithms.utils import Config\n\n\ndef parse_args() -> argparse.Namespace:\n    # configurations\n    parser = argparse.ArgumentParser(description=""Pytorch RL rl_algorithms"")\n    parser.add_argument(\n        ""--seed"", type=int, default=777, help=""random seed for reproducibility""\n    )\n    parser.add_argument(""--algo"", type=str, default=""ddpg"", help=""choose an algorithm"")\n    parser.add_argument(\n        ""--cfg-path"",\n        type=str,\n        default=""./configs/reacher_v2/ddpg.py"",\n        help=""config path"",\n    )\n    parser.add_argument(\n        ""--test"", dest=""test"", action=""store_true"", help=""test mode (no training)""\n    )\n    parser.add_argument(\n        ""--load-from"",\n        type=str,\n        default=None,\n        help=""load the saved model and optimizer at the beginning"",\n    )\n    parser.add_argument(\n        ""--off-render"", dest=""render"", action=""store_false"", help=""turn off rendering""\n    )\n    parser.add_argument(\n        ""--render-after"",\n        type=int,\n        default=0,\n        help=""start rendering after the input number of episode"",\n    )\n    parser.add_argument(\n        ""--log"", dest=""log"", action=""store_true"", help=""turn on logging""\n    )\n    parser.add_argument(\n        ""--save-period"", type=int, default=200, help=""save model period""\n    )\n    parser.add_argument(\n        ""--episode-num"", type=int, default=20000, help=""total episode num""\n    )\n    parser.add_argument(\n        ""--max-episode-steps"", type=int, default=-1, help=""max episode step""\n    )\n    parser.add_argument(\n        ""--interim-test-num"",\n        type=int,\n        default=10,\n        help=""number of test during training"",\n    )\n    parser.add_argument(\n        ""--demo-path"",\n        type=str,\n        default=""data/reacher_demo.pkl"",\n        help=""demonstration path for learning from demo"",\n    )\n\n    return parser.parse_args()\n\n\ndef main():\n    """"""Main.""""""\n    args = parse_args()\n\n    # env initialization\n    env = gym.make(""Reacher-v2"")\n    env_utils.set_env(env, args)\n\n    # set a random seed\n    common_utils.set_random_seed(args.seed, env)\n\n    # run\n    NOWTIMES = datetime.datetime.now()\n    curr_time = NOWTIMES.strftime(""%y%m%d_%H%M%S"")\n\n    cfg = Config.fromfile(args.cfg_path)\n    cfg.agent[""log_cfg""] = dict(agent=cfg.agent.type, curr_time=curr_time)\n    build_args = dict(args=args, env=env)\n    agent = build_agent(cfg.agent, build_args)\n\n    if not args.test:\n        agent.train()\n    else:\n        agent.test()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
setup.py,0,"b'import setuptools\n\nwith open(""README.md"", ""r"", encoding=""utf-8"") as fh:\n    long_description = fh.read()\n\n# pylint: disable=line-too-long\nsetuptools.setup(\n    name=""rl_algorithms"",\n    version=""0.0.1"",\n    author=""medipixel"",\n    author_email=""kh.kim@medipixel.io"",\n    description=""Reinforcement Learning algorithms which are being used for research activities at Medipixel."",  # noqa\n    long_description=long_description,\n    long_description_content_type=""text/markdown"",\n    url=""https://github.com/medipixel/rl_algorithms.git"",\n    packages=setuptools.find_packages(),\n    classifiers=[\n        ""Programming Language :: Python :: 3"",\n        ""License :: OSI Approved :: MIT License"",\n        ""Operating System :: OS Independent"",\n    ],\n    python_requires="">=3.6"",\n    zip_safe=False,\n)\n'"
rl_algorithms/__init__.py,0,"b'from .a2c.agent import A2CAgent\nfrom .bc.ddpg_agent import BCDDPGAgent\nfrom .bc.her import LunarLanderContinuousHER, ReacherHER\nfrom .bc.sac_agent import BCSACAgent\nfrom .common.networks.backbones import CNN, ResNet\nfrom .ddpg.agent import DDPGAgent\nfrom .dqn.agent import DQNAgent\nfrom .dqn.losses import C51Loss, DQNLoss, IQNLoss\nfrom .fd.ddpg_agent import DDPGfDAgent\nfrom .fd.dqn_agent import DQfDAgent\nfrom .fd.sac_agent import SACfDAgent\nfrom .per.ddpg_agent import PERDDPGAgent\nfrom .ppo.agent import PPOAgent\nfrom .registry import build_agent, build_her\nfrom .sac.agent import SACAgent\nfrom .td3.agent import TD3Agent\n\n__all__ = [\n    ""A2CAgent"",\n    ""BCDDPGAgent"",\n    ""BCSACAgent"",\n    ""DDPGAgent"",\n    ""DQNAgent"",\n    ""DDPGfDAgent"",\n    ""DQfDAgent"",\n    ""SACfDAgent"",\n    ""PERDDPGAgent"",\n    ""PPOAgent"",\n    ""SACAgent"",\n    ""TD3Agent"",\n    ""LunarLanderContinuousHER"",\n    ""ReacherHER"",\n    ""build_agent"",\n    ""build_her"",\n    ""CNN"",\n    ""ResNet"",\n    ""IQNLoss"",\n    ""C51Loss"",\n    ""DQNLoss"",\n]\n'"
rl_algorithms/registry.py,0,"b'from rl_algorithms.utils import Registry, build_from_cfg\nfrom rl_algorithms.utils.config import ConfigDict\n\nAGENTS = Registry(""agents"")\nBACKBONES = Registry(""backbones"")\nHEADS = Registry(""heads"")\nLOSSES = Registry(""losses"")\nHERS = Registry(""hers"")\n\n\ndef build_agent(cfg: ConfigDict, build_args: dict = None):\n    """"""Build agent using config and additional arguments.""""""\n    return build_from_cfg(cfg, AGENTS, build_args)\n\n\ndef build_backbone(cfg: ConfigDict, build_args: dict = None):\n    """"""Build backbone using config and additional arguments.""""""\n    return build_from_cfg(cfg, BACKBONES, build_args)\n\n\ndef build_head(cfg: ConfigDict, build_args: dict = None):\n    """"""Build head using config and additional arguments.""""""\n    return build_from_cfg(cfg, HEADS, build_args)\n\n\ndef build_loss(cfg: ConfigDict, build_args: dict = None):\n    """"""Build loss using config and additional arguments.""""""\n    return build_from_cfg(cfg, LOSSES, build_args)\n\n\ndef build_her(cfg: ConfigDict, build_args: dict = None):\n    """"""Build her using config and additional arguments.""""""\n    return build_from_cfg(cfg, HERS, build_args)\n'"
tests/test_cnn_cfg.py,3,"b'import torch\nimport torch.nn as nn\n\nfrom rl_algorithms.common.helper_functions import identity\nfrom rl_algorithms.common.networks.backbones import CNN, ResNet\nfrom rl_algorithms.common.networks.brain import Brain\nfrom rl_algorithms.utils.config import ConfigDict\n\ncnn_cfg = ConfigDict(\n    type=""CNN"",\n    configs=dict(\n        input_sizes=[3, 32, 32],\n        output_sizes=[32, 32, 64],\n        kernel_sizes=[5, 3, 3],\n        strides=[4, 3, 2],\n        paddings=[2, 0, 1],\n    ),\n)\n\nresnet_cfg = ConfigDict(\n    type=""ResNet"",\n    configs=dict(\n        use_bottleneck=False,\n        num_blocks=[1, 1, 1, 1],\n        block_output_sizes=[32, 32, 64, 64],\n        block_strides=[1, 2, 2, 2],\n        first_input_size=3,\n        first_output_size=32,\n        expansion=4,\n        channel_compression=4,\n    ),\n)\n\nhead_cfg = ConfigDict(\n    type=""IQNMLP"",\n    configs=dict(\n        hidden_sizes=[512],\n        n_tau_samples=64,\n        n_tau_prime_samples=64,\n        n_quantile_samples=32,\n        quantile_embedding_dim=64,\n        kappa=1.0,\n        output_activation=identity,\n        # NoisyNet\n        use_noisy_net=True,\n        std_init=0.5,\n    ),\n)\n\ntest_state_dim = (3, 256, 256)\n\n\ndef test_brain():\n    """"""Test wheter brain make fc layer based on backbone\'s output size.""""""\n\n    head_cfg.configs.state_size = test_state_dim\n    head_cfg.configs.output_size = 8\n\n    model = Brain(resnet_cfg, head_cfg)\n    assert model.head.input_size == 16384\n\n\ndef test_cnn_with_config():\n    """"""Test whether CNN module can make proper model according to the configs given.""""""\n    conv_layer_size = [[1, 32, 64, 64], [1, 32, 21, 21], [1, 64, 11, 11]]\n    test_cnn_model = CNN(configs=cnn_cfg.configs)\n    conv_layers = [\n        module for module in test_cnn_model.modules() if isinstance(module, nn.Conv2d)\n    ]\n    x = torch.zeros(test_state_dim).unsqueeze(0)\n    for i, layer in enumerate(conv_layers):\n        layer_output = layer(x)\n        x = layer_output\n        assert list(x.shape) == conv_layer_size[i]\n\n\ndef test_resnet_with_config():\n    """"""Test whether ResNet module can make proper model according to the configs given.""""""\n    conv_layer_size = [\n        [1, 32, 256, 256],\n        [1, 32, 256, 256],\n        [1, 128, 256, 256],\n        [1, 128, 256, 256],\n        [1, 32, 128, 128],\n        [1, 128, 128, 128],\n        [1, 128, 128, 128],\n        [1, 64, 64, 64],\n        [1, 256, 64, 64],\n        [1, 256, 64, 64],\n        [1, 64, 32, 32],\n        [1, 256, 32, 32],\n        [1, 256, 32, 32],\n        [1, 16, 32, 32],\n    ]\n    test_resnet_model = ResNet(configs=resnet_cfg.configs)\n    conv_layers = [\n        module\n        for module in test_resnet_model.modules()\n        if isinstance(module, nn.Conv2d)\n    ]\n    x = torch.zeros(test_state_dim).unsqueeze(0)\n    skip_x = x\n    for i, layer in enumerate(conv_layers):\n        if i % 3 == 0:\n            layer_output = layer(skip_x)\n            skip_x = layer_output\n            x = layer_output\n        else:\n            layer_output = layer(x)\n            x = layer_output\n        assert list(x.shape) == conv_layer_size[i]\n\n\nif __name__ == ""__main__"":\n    test_brain()\n    test_cnn_with_config()\n'"
tests/test_config_registry.py,0,"b'import argparse\nimport datetime\n\nimport gym\n\nfrom rl_algorithms import build_agent\nfrom rl_algorithms.common.abstract.agent import Agent\nfrom rl_algorithms.utils import Config\n\n\ndef parse_args(args: list):\n    parser = argparse.ArgumentParser(description=""Pytorch RL rl_algorithms"")\n    parser.add_argument(\n        ""--load-from"",\n        default=None,\n        type=str,\n        help=""load the saved model and optimizer at the beginning"",\n    )\n    parser.add_argument(\n        ""--test"", dest=""test"", action=""store_true"", help=""test mode (no training)""\n    )\n    parser.add_argument(\n        ""--cfg-path"",\n        type=str,\n        default=""./configs/lunarlander_continuous_v2/ddpg.py"",\n        help=""config path"",\n    )\n    return parser.parse_args(args)\n\n\ndef test_config_registry():\n    # configurations\n    args = parse_args([""--test""])\n\n    # set env\n    env = gym.make(""LunarLanderContinuous-v2"")\n\n    # check start time\n    NOWTIMES = datetime.datetime.now()\n    curr_time = NOWTIMES.strftime(""%y%m%d_%H%M%S"")\n\n    cfg = Config.fromfile(args.cfg_path)\n    cfg.agent[""log_cfg""] = dict(agent=cfg.agent.type, curr_time=curr_time)\n    default_args = dict(args=args, env=env)\n    agent = build_agent(cfg.agent, default_args)\n    assert isinstance(agent, Agent)\n\n\nif __name__ == ""__main__"":\n    test_config_registry()\n'"
configs/lunarlander_continuous_v2/__init__.py,0,"b'""""""Empty.""""""\n'"
configs/lunarlander_continuous_v2/a2c.py,0,"b'""""""Config for A2C on LunarLanderContinuous-v2.\n\n- Author: Kyunghwan Kim\n- Contact: kh.kim@medipixel.io\n""""""\nfrom rl_algorithms.common.helper_functions import identity\n\nagent = dict(\n    type=""A2CAgent"",\n    hyper_params=dict(\n        gamma=0.99,\n        w_entropy=1e-3,  # multiple learning updates\n        gradient_clip_ac=0.1,\n        gradient_clip_cr=0.5,\n    ),\n    backbone=dict(actor=dict(), critic=dict(),),\n    head=dict(\n        actor=dict(\n            type=""GaussianDist"",\n            configs=dict(hidden_sizes=[256, 256], output_activation=identity,),\n        ),\n        critic=dict(\n            type=""MLP"",\n            configs=dict(\n                hidden_sizes=[256, 256], output_activation=identity, output_size=1,\n            ),\n        ),\n    ),\n    optim_cfg=dict(lr_actor=4e-5, lr_critic=3e-4, weight_decay=0.0),\n)\n'"
configs/lunarlander_continuous_v2/bc_ddpg.py,1,"b'""""""Config for DDPG with Behavior Cloning on LunarLanderContinuous-v2.\n\n- Author: Kyunghwan Kim\n- Contact: kh.kim@medipixel.io\n""""""\nimport torch.nn.functional as F\n\nfrom rl_algorithms.common.helper_functions import identity\n\nagent = dict(\n    type=""BCDDPGAgent"",\n    hyper_params=dict(\n        gamma=0.99,\n        tau=1e-3,\n        buffer_size=int(1e5),\n        batch_size=512,\n        initial_random_action=int(1e4),\n        multiple_update=1,  # multiple learning updates\n        gradient_clip_ac=0.5,\n        gradient_clip_cr=0.5,\n        # BC\n        demo_batch_size=64,\n        lambda1=1e-3,\n        # HER\n        use_her=False,\n        her=dict(type=""LunarLanderContinuousHER"",),\n        success_score=250.0,\n        desired_states_from_demo=True,\n    ),\n    backbone=dict(actor=dict(), critic=dict(),),\n    head=dict(\n        actor=dict(\n            type=""MLP"",\n            configs=dict(hidden_sizes=[256, 256], output_activation=F.tanh,),\n        ),\n        critic=dict(\n            type=""MLP"",\n            configs=dict(\n                hidden_sizes=[256, 256], output_size=1, output_activation=identity,\n            ),\n        ),\n    ),\n    optim_cfg=dict(lr_actor=1e-4, lr_critic=1e-3, weight_decay=1e-4),\n    noise_cfg=dict(ou_noise_theta=0.0, ou_noise_sigma=0.0),\n)\n'"
configs/lunarlander_continuous_v2/bc_sac.py,0,"b'""""""Config for SAC with Behavior Cloning on LunarLanderContinuous-v2.\n\n- Author: Kyunghwan Kim\n- Contact: kh.kim@medipixel.io\n""""""\nfrom rl_algorithms.common.helper_functions import identity\n\nagent = dict(\n    type=""BCSACAgent"",\n    hyper_params=dict(\n        gamma=0.99,\n        tau=5e-3,\n        buffer_size=int(1e6),\n        batch_size=512,\n        initial_random_action=int(1e4),\n        multiple_update=1,  # multiple learning updates\n        policy_update_freq=2,\n        w_entropy=1e-3,\n        w_mean_reg=1e-3,\n        w_std_reg=1e-3,\n        w_pre_activation_reg=0.0,\n        auto_entropy_tuning=True,\n        # BC\n        demo_batch_size=64,\n        lambda1=1e-3,\n        # HER\n        use_her=False,\n        her=dict(type=""LunarLanderContinuousHER"",),\n        success_score=250.0,\n        desired_states_from_demo=True,\n    ),\n    backbone=dict(actor=dict(), critic_vf=dict(), critic_qf=dict()),\n    head=dict(\n        actor=dict(\n            type=""TanhGaussianDistParams"",\n            configs=dict(hidden_sizes=[256, 256], output_activation=identity,),\n        ),\n        critic_vf=dict(\n            type=""MLP"",\n            configs=dict(\n                hidden_sizes=[256, 256], output_activation=identity, output_size=1,\n            ),\n        ),\n        critic_qf=dict(\n            type=""MLP"",\n            configs=dict(\n                hidden_sizes=[256, 256], output_activation=identity, output_size=1,\n            ),\n        ),\n    ),\n    optim_cfg=dict(\n        lr_actor=3e-4,\n        lr_vf=3e-4,\n        lr_qf1=3e-4,\n        lr_qf2=3e-4,\n        lr_entropy=3e-4,\n        weight_decay=0.0,\n    ),\n)\n'"
configs/lunarlander_continuous_v2/ddpg.py,1,"b'""""""Config for DDPG on LunarLanderContinuous-v2.\n\n- Author: Kyunghwan Kim\n- Contact: kh.kim@medipixel.io\n""""""\nimport torch.nn.functional as F\n\nfrom rl_algorithms.common.helper_functions import identity\n\nagent = dict(\n    type=""DDPGAgent"",\n    hyper_params=dict(\n        gamma=0.99,\n        tau=5e-3,\n        buffer_size=int(1e4),\n        batch_size=64,\n        initial_random_action=int(1e4),\n        multiple_update=1,  # multiple learning updates\n        gradient_clip_ac=0.5,\n        gradient_clip_cr=1.0,\n    ),\n    backbone=dict(actor=dict(), critic=dict(),),\n    head=dict(\n        actor=dict(\n            type=""MLP"",\n            configs=dict(hidden_sizes=[256, 256], output_activation=F.tanh,),\n        ),\n        critic=dict(\n            type=""MLP"",\n            configs=dict(\n                hidden_sizes=[256, 256], output_size=1, output_activation=identity,\n            ),\n        ),\n    ),\n    optim_cfg=dict(lr_actor=3e-4, lr_critic=3e-4, weight_decay=1e-6),\n    noise_cfg=dict(ou_noise_theta=0.0, ou_noise_sigma=0.0),\n)\n'"
configs/lunarlander_continuous_v2/ddpgfd.py,1,"b'""""""Config for DDPGfD on LunarLanderContinuous-v2.\n\n- Author: Kyunghwan Kim\n- Contact: kh.kim@medipixel.io\n""""""\nimport torch.nn.functional as F\n\nfrom rl_algorithms.common.helper_functions import identity\n\nagent = dict(\n    type=""DDPGfDAgent"",\n    hyper_params=dict(\n        gamma=0.99,\n        tau=5e-3,\n        buffer_size=int(1e5),\n        batch_size=128,\n        initial_random_action=int(1e4),\n        multiple_update=1,  # multiple learning updates\n        gradient_clip_ac=0.5,\n        gradient_clip_cr=1.0,\n        # fD\n        n_step=1,\n        pretrain_step=int(5e3),\n        lambda1=1.0,  # N-step return weight\n        # lambda2 = weight_decay\n        lambda3=1.0,  # actor loss contribution of prior weight\n        per_alpha=0.3,\n        per_beta=1.0,\n        per_eps=1e-6,\n        per_eps_demo=1.0,\n    ),\n    backbone=dict(actor=dict(), critic=dict(),),\n    head=dict(\n        actor=dict(\n            type=""MLP"",\n            configs=dict(hidden_sizes=[256, 256], output_activation=F.tanh,),\n        ),\n        critic=dict(\n            type=""MLP"",\n            configs=dict(\n                hidden_sizes=[256, 256], output_size=1, output_activation=identity,\n            ),\n        ),\n    ),\n    optim_cfg=dict(lr_actor=3e-4, lr_critic=3e-4, weight_decay=1e-4),\n    noise_cfg=dict(ou_noise_theta=0.0, ou_noise_sigma=0.0),\n)\n'"
configs/lunarlander_continuous_v2/per_ddpg.py,1,"b'""""""Config for DDPG with PER on LunarLanderContinuous-v2.\n\n- Author: Kyunghwan Kim\n- Contact: kh.kim@medipixel.io\n""""""\nimport torch.nn.functional as F\n\nfrom rl_algorithms.common.helper_functions import identity\n\nagent = dict(\n    type=""PERDDPGAgent"",\n    hyper_params=dict(\n        gamma=0.99,\n        tau=5e-3,\n        buffer_size=int(1e6),\n        batch_size=128,\n        initial_random_action=int(1e4),\n        multiple_update=1,  # multiple learning updates\n        gradient_clip_ac=0.5,\n        gradient_clip_cr=1.0,\n        # PER\n        per_alpha=0.6,\n        per_beta=0.4,\n        per_eps=1e-6,\n    ),\n    backbone=dict(actor=dict(), critic=dict(),),\n    head=dict(\n        actor=dict(\n            type=""MLP"",\n            configs=dict(hidden_sizes=[256, 256], output_activation=F.tanh,),\n        ),\n        critic=dict(\n            type=""MLP"",\n            configs=dict(\n                hidden_sizes=[256, 256], output_size=1, output_activation=identity,\n            ),\n        ),\n    ),\n    optim_cfg=dict(lr_actor=3e-4, lr_critic=3e-4, weight_decay=5e-6),\n    noise_cfg=dict(ou_noise_theta=0.0, ou_noise_sigma=0.0),\n)\n'"
configs/lunarlander_continuous_v2/ppo.py,1,"b'""""""Config for PPO on LunarLanderContinuous-v2.\n\n- Author: Kyunghwan Kim\n- Contact: kh.kim@medipixel.io\n""""""\nimport torch.nn.functional as F\n\nagent = dict(\n    type=""PPOAgent"",\n    hyper_params=dict(\n        gamma=0.99,\n        tau=0.95,\n        batch_size=32,\n        max_epsilon=0.2,\n        min_epsilon=0.2,\n        epsilon_decay_period=1500,\n        w_value=1.0,\n        w_entropy=1e-3,\n        gradient_clip_ac=0.1,\n        gradient_clip_cr=0.5,\n        epoch=16,\n        rollout_len=256,\n        n_workers=12,\n        use_clipped_value_loss=True,\n        standardize_advantage=True,\n    ),\n    backbone=dict(actor=dict(), critic=dict(),),\n    head=dict(\n        actor=dict(\n            type=""GaussianDist"",\n            configs=dict(hidden_sizes=[256, 256], output_activation=F.tanh,),\n        ),\n        critic=dict(\n            type=""MLP"",\n            configs=dict(\n                hidden_sizes=[256, 256], output_size=1, output_activation=F.tanh,\n            ),\n        ),\n    ),\n    optim_cfg=dict(lr_actor=3e-4, lr_critic=1e-3, weight_decay=0.0),\n)\n'"
configs/lunarlander_continuous_v2/sac.py,0,"b'""""""Config for SAC on LunarLanderContinuous-v2.\n\n- Author: Kyunghwan Kim\n- Contact: kh.kim@medipixel.io\n""""""\nfrom rl_algorithms.common.helper_functions import identity\n\nagent = dict(\n    type=""SACAgent"",\n    hyper_params=dict(\n        gamma=0.99,\n        tau=5e-3,\n        buffer_size=int(1e6),\n        batch_size=128,\n        initial_random_action=int(1e4),\n        multiple_update=1,  # multiple learning updates\n        policy_update_freq=2,\n        w_entropy=1e-3,\n        w_mean_reg=0.0,\n        w_std_reg=0.0,\n        w_pre_activation_reg=0.0,\n        auto_entropy_tuning=True,\n    ),\n    backbone=dict(actor=dict(), critic_vf=dict(), critic_qf=dict()),\n    head=dict(\n        actor=dict(\n            type=""TanhGaussianDistParams"",\n            configs=dict(hidden_sizes=[256, 256], output_activation=identity,),\n        ),\n        critic_vf=dict(\n            type=""MLP"",\n            configs=dict(\n                hidden_sizes=[256, 256], output_activation=identity, output_size=1,\n            ),\n        ),\n        critic_qf=dict(\n            type=""MLP"",\n            configs=dict(\n                hidden_sizes=[256, 256], output_activation=identity, output_size=1,\n            ),\n        ),\n    ),\n    optim_cfg=dict(\n        lr_actor=3e-4,\n        lr_vf=3e-4,\n        lr_qf1=3e-4,\n        lr_qf2=3e-4,\n        lr_entropy=3e-4,\n        weight_decay=0.0,\n    ),\n)\n'"
configs/lunarlander_continuous_v2/sacfd.py,0,"b'""""""Config for SACfD on LunarLanderContinuous-v2.\n\n- Author: Kyunghwan Kim\n- Contact: kh.kim@medipixel.io\n""""""\nfrom rl_algorithms.common.helper_functions import identity\n\nagent = dict(\n    type=""SACfDAgent"",\n    hyper_params=dict(\n        gamma=0.99,\n        tau=1e-3,\n        buffer_size=int(1e5),\n        batch_size=64,\n        initial_random_action=int(5e3),\n        multiple_update=2,  # multiple learning updates\n        policy_update_freq=2,\n        w_entropy=1e-3,\n        w_mean_reg=1e-3,\n        w_std_reg=1e-3,\n        w_pre_activation_reg=0.0,\n        auto_entropy_tuning=True,\n        # fD\n        n_step=3,\n        pretrain_step=100,\n        lambda1=1.0,  # N-step return weight\n        # lambda2 = weight_decay\n        lambda3=1.0,  # actor loss contribution of prior weight\n        per_alpha=0.6,\n        per_beta=0.4,\n        per_eps=1e-6,\n        per_eps_demo=1.0,\n    ),\n    backbone=dict(actor=dict(), critic_vf=dict(), critic_qf=dict()),\n    head=dict(\n        actor=dict(\n            type=""TanhGaussianDistParams"",\n            configs=dict(hidden_sizes=[256, 256], output_activation=identity,),\n        ),\n        critic_vf=dict(\n            type=""MLP"",\n            configs=dict(\n                hidden_sizes=[256, 256], output_activation=identity, output_size=1,\n            ),\n        ),\n        critic_qf=dict(\n            type=""MLP"",\n            configs=dict(\n                hidden_sizes=[256, 256], output_activation=identity, output_size=1,\n            ),\n        ),\n    ),\n    optim_cfg=dict(\n        lr_actor=3e-4,\n        lr_vf=3e-4,\n        lr_qf1=3e-4,\n        lr_qf2=3e-4,\n        lr_entropy=3e-4,\n        weight_decay=1e-5,\n    ),\n)\n'"
configs/lunarlander_continuous_v2/td3.py,1,"b'""""""Config for TD3 on LunarLanderContinuous-v2.\n\n- Author: Kyunghwan Kim\n- Contact: kh.kim@medipixel.io\n""""""\nimport torch.nn.functional as F\n\nfrom rl_algorithms.common.helper_functions import identity\n\nagent = dict(\n    type=""TD3Agent"",\n    hyper_params=dict(\n        gamma=0.99,\n        tau=5e-3,\n        buffer_size=int(1e6),\n        batch_size=100,\n        initial_random_action=int(1e4),\n        policy_update_freq=2,\n    ),\n    backbone=dict(actor=dict(), critic=dict()),\n    head=dict(\n        actor=dict(\n            type=""MLP"",\n            configs=dict(hidden_sizes=[400, 300], output_activation=F.tanh,),\n        ),\n        critic=dict(\n            type=""MLP"",\n            configs=dict(\n                hidden_sizes=[400, 300], output_size=1, output_activation=identity,\n            ),\n        ),\n    ),\n    optim_cfg=dict(lr_actor=1e-3, lr_critic=1e-3, weight_decay=0.0),\n    noise_cfg=dict(\n        exploration_noise=0.1, target_policy_noise=0.2, target_policy_noise_clip=0.5\n    ),\n)\n'"
configs/lunarlander_v2/__init__.py,0,"b'""""""Empty.""""""\n'"
configs/lunarlander_v2/dqfd.py,0,"b'""""""Config for DQfD on LunarLander-v2.\n\n- Author: Kyunghwan Kim\n- Contact: kh.kim@medipixel.io\n""""""\nfrom rl_algorithms.common.helper_functions import identity\n\nagent = dict(\n    type=""DQfDAgent"",\n    hyper_params=dict(\n        gamma=0.99,\n        tau=5e-3,\n        buffer_size=int(1e5),  # openai baselines: int(1e4)\n        batch_size=64,  # openai baselines: 32\n        update_starts_from=int(1e3),  # openai baselines: int(1e4)\n        multiple_update=1,  # multiple learning updates\n        train_freq=8,  # in openai baselines, train_freq = 4\n        gradient_clip=0.5,  # dueling: 10.0\n        n_step=3,\n        w_n_step=1.0,\n        w_q_reg=1e-7,\n        per_alpha=0.6,  # openai baselines: 0.6\n        per_beta=0.4,\n        per_eps=1e-3,\n        # fD\n        per_eps_demo=1.0,\n        lambda1=1.0,  # N-step return weight\n        lambda2=1.0,  # Supervised loss weight\n        # lambda3 = weight_decay (l2 regularization weight)\n        margin=0.8,\n        pretrain_step=int(1e2),\n        loss_type=dict(type=""C51Loss""),\n        # Epsilon Greedy\n        max_epsilon=1.0,\n        min_epsilon=0.01,  # openai baselines: 0.01\n        epsilon_decay=2e-5,  # openai baselines: 1e-7 / 1e-1\n    ),\n    backbone=dict(),\n    head=dict(\n        type=""C51DuelingMLP"",\n        configs=dict(\n            hidden_sizes=[128, 64],\n            use_noisy_net=False,\n            v_min=-300,\n            v_max=300,\n            atom_size=1530,\n            output_activation=identity,\n        ),\n    ),\n    optim_cfg=dict(lr_dqn=1e-4, weight_decay=1e-5, adam_eps=1e-8),\n)\n'"
configs/lunarlander_v2/dqn.py,0,"b'""""""Config for DQN on LunarLander-v2.\n\n- Author: Kyunghwan Kim\n- Contact: kh.kim@medipixel.io\n""""""\nfrom rl_algorithms.common.helper_functions import identity\n\nagent = dict(\n    type=""DQNAgent"",\n    hyper_params=dict(\n        gamma=0.99,\n        tau=5e-3,\n        buffer_size=int(1e5),  # openai baselines: int(1e4)\n        batch_size=64,  # openai baselines: 32\n        update_starts_from=int(1e4),  # openai baselines: int(1e4)\n        multiple_update=1,  # multiple learning updates\n        train_freq=1,  # in openai baselines, train_freq = 4\n        gradient_clip=10.0,  # dueling: 10.0\n        n_step=3,\n        w_n_step=1.0,\n        w_q_reg=1e-7,\n        per_alpha=0.6,  # openai baselines: 0.6\n        per_beta=0.4,\n        per_eps=1e-6,\n        loss_type=dict(type=""C51Loss""),\n        # Epsilon Greedy\n        max_epsilon=1.0,\n        min_epsilon=0.01,  # openai baselines: 0.01\n        epsilon_decay=1e-5,  # openai baselines: 1e-7 / 1e-1\n    ),\n    backbone=dict(),\n    head=dict(\n        type=""C51DuelingMLP"",\n        configs=dict(\n            hidden_sizes=[128, 64],\n            use_noisy_net=False,\n            v_min=-300,\n            v_max=300,\n            atom_size=1530,\n            output_activation=identity,\n        ),\n    ),\n    optim_cfg=dict(lr_dqn=1e-4, weight_decay=1e-7, adam_eps=1e-8),\n)\n'"
configs/pong_no_frameskip_v4/__init__.py,0,"b'""""""Empty.""""""\n'"
configs/pong_no_frameskip_v4/c51.py,0,"b'""""""Config for DQN on Pong-No_FrameSkip-v4.\n\n- Author: Kyunghwan Kim\n- Contact: kh.kim@medipixel.io\n""""""\nfrom rl_algorithms.common.helper_functions import identity\n\nagent = dict(\n    type=""DQNAgent"",\n    hyper_params=dict(\n        gamma=0.99,\n        tau=5e-3,\n        buffer_size=int(1e4),  # openai baselines: int(1e4)\n        batch_size=32,  # openai baselines: 32\n        update_starts_from=int(1e4),  # openai baselines: int(1e4)\n        multiple_update=1,  # multiple learning updates\n        train_freq=4,  # in openai baselines, train_freq = 4\n        gradient_clip=10.0,  # dueling: 10.0\n        n_step=3,\n        w_n_step=1.0,\n        w_q_reg=0.0,\n        per_alpha=0.6,  # openai baselines: 0.6\n        per_beta=0.4,\n        per_eps=1e-6,\n        loss_type=dict(type=""C51Loss""),\n        # Epsilon Greedy\n        max_epsilon=0.0,\n        min_epsilon=0.0,  # openai baselines: 0.01\n        epsilon_decay=1e-6,  # openai baselines: 1e-7 / 1e-1\n        # grad_cam\n        grad_cam_layer_list=[\n            ""backbone.cnn.cnn_0.cnn"",\n            ""backbone.cnn.cnn_1.cnn"",\n            ""backbone.cnn.cnn_2.cnn"",\n        ],\n    ),\n    backbone=dict(\n        type=""CNN"",\n        configs=dict(\n            input_sizes=[4, 32, 64],\n            output_sizes=[32, 64, 64],\n            kernel_sizes=[8, 4, 3],\n            strides=[4, 2, 1],\n            paddings=[1, 0, 0],\n        ),\n    ),\n    head=dict(\n        type=""C51DuelingMLP"",\n        configs=dict(\n            hidden_sizes=[512],\n            v_min=-10,\n            v_max=10,\n            atom_size=51,\n            output_activation=identity,\n            # NoisyNet\n            use_noisy_net=True,\n            std_init=0.5,\n        ),\n    ),\n    optim_cfg=dict(\n        lr_dqn=1e-4,  # dueling: 6.25e-5, openai baselines: 1e-4\n        weight_decay=0.0,  # this makes saturation in cnn weights\n        adam_eps=1e-8,  # rainbow: 1.5e-4, openai baselines: 1e-8\n    ),\n)\n'"
configs/pong_no_frameskip_v4/dqn.py,0,"b'""""""Config for DQN on Pong-No_FrameSkip-v4.\n\n- Author: Kyunghwan Kim\n- Contact: kh.kim@medipixel.io\n""""""\nfrom rl_algorithms.common.helper_functions import identity\n\nagent = dict(\n    type=""DQNAgent"",\n    hyper_params=dict(\n        gamma=0.99,\n        tau=5e-3,\n        buffer_size=int(1e4),  # openai baselines: int(1e4)\n        batch_size=32,  # openai baselines: 32\n        update_starts_from=int(1e4),  # openai baselines: int(1e4)\n        multiple_update=1,  # multiple learning updates\n        train_freq=4,  # in openai baselines, train_freq = 4\n        gradient_clip=10.0,  # dueling: 10.0\n        n_step=3,\n        w_n_step=1.0,\n        w_q_reg=0.0,\n        per_alpha=0.6,  # openai baselines: 0.6\n        per_beta=0.4,\n        per_eps=1e-6,\n        loss_type=dict(type=""DQNLoss""),\n        # Epsilon Greedy\n        max_epsilon=1.0,\n        min_epsilon=0.01,  # openai baselines: 0.01\n        epsilon_decay=1e-6,  # openai baselines: 1e-7 / 1e-1\n        # grad_cam\n        grad_cam_layer_list=[\n            ""backbone.cnn.cnn_0.cnn"",\n            ""backbone.cnn.cnn_1.cnn"",\n            ""backbone.cnn.cnn_2.cnn"",\n        ],\n    ),\n    backbone=dict(\n        type=""CNN"",\n        configs=dict(\n            input_sizes=[4, 32, 64],\n            output_sizes=[32, 64, 64],\n            kernel_sizes=[8, 4, 3],\n            strides=[4, 2, 1],\n            paddings=[1, 0, 0],\n        ),\n    ),\n    head=dict(\n        type=""DuelingMLP"",\n        configs=dict(\n            use_noisy_net=False, hidden_sizes=[512], output_activation=identity\n        ),\n    ),\n    optim_cfg=dict(\n        lr_dqn=1e-4,  # dueling: 6.25e-5, openai baselines: 1e-4\n        weight_decay=0.0,  # this makes saturation in cnn weights\n        adam_eps=1e-8,  # rainbow: 1.5e-4, openai baselines: 1e-8\n    ),\n)\n'"
configs/pong_no_frameskip_v4/iqn.py,0,"b'""""""Config for DQN on Pong-No_FrameSkip-v4.\n\n- Author: Kyunghwan Kim\n- Contact: kh.kim@medipixel.io\n""""""\nfrom rl_algorithms.common.helper_functions import identity\n\nagent = dict(\n    type=""DQNAgent"",\n    hyper_params=dict(\n        gamma=0.99,\n        tau=5e-3,\n        buffer_size=int(1e4),  # openai baselines: int(1e4)\n        batch_size=32,  # openai baselines: 32\n        update_starts_from=int(1e4),  # openai baselines: int(1e4)\n        multiple_update=1,  # multiple learning updates\n        train_freq=4,  # in openai baselines, train_freq = 4\n        gradient_clip=10.0,  # dueling: 10.0\n        n_step=3,\n        w_n_step=1.0,\n        w_q_reg=0.0,\n        per_alpha=0.6,  # openai baselines: 0.6\n        per_beta=0.4,\n        per_eps=1e-6,\n        loss_type=dict(type=""IQNLoss""),\n        # Epsilon Greedy\n        max_epsilon=0.0,\n        min_epsilon=0.0,  # openai baselines: 0.01\n        epsilon_decay=1e-6,  # openai baselines: 1e-7 / 1e-1\n        # grad_cam\n        grad_cam_layer_list=[\n            ""backbone.cnn.cnn_0.cnn"",\n            ""backbone.cnn.cnn_1.cnn"",\n            ""backbone.cnn.cnn_2.cnn"",\n        ],\n    ),\n    backbone=dict(\n        type=""CNN"",\n        configs=dict(\n            input_sizes=[4, 32, 64],\n            output_sizes=[32, 64, 64],\n            kernel_sizes=[8, 4, 3],\n            strides=[4, 2, 1],\n            paddings=[1, 0, 0],\n        ),\n    ),\n    head=dict(\n        type=""IQNMLP"",\n        configs=dict(\n            hidden_sizes=[512],\n            n_tau_samples=64,\n            n_tau_prime_samples=64,\n            n_quantile_samples=32,\n            quantile_embedding_dim=64,\n            kappa=1.0,\n            output_activation=identity,\n            # NoisyNet\n            use_noisy_net=True,\n            std_init=0.5,\n        ),\n    ),\n    optim_cfg=dict(\n        lr_dqn=1e-4,  # dueling: 6.25e-5, openai baselines: 1e-4\n        weight_decay=0.0,  # this makes saturation in cnn weights\n        adam_eps=1e-8,  # rainbow: 1.5e-4, openai baselines: 1e-8\n    ),\n)\n'"
configs/pong_no_frameskip_v4/iqn_resnet.py,0,"b'""""""Config for DQN on Pong-No_FrameSkip-v4.\n\n- Author: Kyunghwan Kim\n- Contact: kh.kim@medipixel.io\n""""""\nfrom rl_algorithms.common.helper_functions import identity\n\nagent = dict(\n    type=""DQNAgent"",\n    hyper_params=dict(\n        gamma=0.99,\n        tau=5e-3,\n        buffer_size=int(1e4),  # openai baselines: int(1e4)\n        batch_size=16,  # openai baselines: 32\n        update_starts_from=int(1e4),  # openai baselines: int(1e4)\n        multiple_update=1,  # multiple learning updates\n        train_freq=4,  # in openai baselines, train_freq = 4\n        gradient_clip=10.0,  # dueling: 10.0\n        n_step=3,\n        w_n_step=1.0,\n        w_q_reg=0.0,\n        per_alpha=0.6,  # openai baselines: 0.6\n        per_beta=0.4,\n        per_eps=1e-6,\n        loss_type=dict(type=""IQNLoss""),\n        # Epsilon Greedy\n        max_epsilon=0.0,\n        min_epsilon=0.0,  # openai baselines: 0.01\n        epsilon_decay=1e-6,  # openai baselines: 1e-7 / 1e-1\n        # grad_cam\n        grad_cam_layer_list=[\n            ""backbone.layer1.0.conv2"",\n            ""backbone.layer2.0.shortcut.0"",\n            ""backbone.layer3.0.shortcut.0"",\n            ""backbone.layer4.0.shortcut.0"",\n            ""backbone.conv_out"",\n        ],\n    ),\n    backbone=dict(\n        type=""ResNet"",\n        configs=dict(\n            use_bottleneck=False,\n            num_blocks=[1, 1, 1, 1],\n            block_output_sizes=[32, 32, 64, 64],\n            block_strides=[1, 2, 2, 2],\n            first_input_size=4,\n            first_output_size=32,\n            expansion=1,\n            channel_compression=4,  # output channel // channel_compression in last conv layer\n        ),\n    ),\n    head=dict(\n        type=""IQNMLP"",\n        configs=dict(\n            hidden_sizes=[512],\n            n_tau_samples=64,\n            n_tau_prime_samples=64,\n            n_quantile_samples=32,\n            quantile_embedding_dim=64,\n            kappa=1.0,\n            output_activation=identity,\n            # NoisyNet\n            use_noisy_net=True,\n            std_init=0.5,\n        ),\n    ),\n    optim_cfg=dict(\n        lr_dqn=1e-4,  # dueling: 6.25e-5, openai baselines: 1e-4\n        weight_decay=0.0,  # this makes saturation in cnn weights\n        adam_eps=1e-8,  # rainbow: 1.5e-4, openai baselines: 1e-8\n    ),\n)\n'"
configs/reacher_v2/__init__.py,0,"b'""""""Empty.""""""\n'"
configs/reacher_v2/bc_ddpg.py,1,"b'""""""Config for DDPG with Behavior Cloning on Reacher-v2.\n\n- Author: Kyunghwan Kim\n- Contact: kh.kim@medipixel.io\n""""""\nimport torch.nn.functional as F\n\nfrom rl_algorithms.common.helper_functions import identity\n\nagent = dict(\n    type=""BCDDPGAgent"",\n    hyper_params=dict(\n        gamma=0.99,\n        tau=1e-3,\n        buffer_size=int(1e5),\n        batch_size=512,\n        initial_random_action=int(1e4),\n        multiple_update=1,  # multiple learning updates\n        gradient_clip_ac=0.5,\n        gradient_clip_cr=1.0,\n        # BC\n        demo_batch_size=64,\n        lambda1=1e-3,\n        lambda2=1.0,\n        # HER\n        use_her=False,\n        her=dict(type=""ReacherHER"",),\n        success_score=-5.0,\n        desired_states_from_demo=False,\n    ),\n    backbone=dict(actor=dict(), critic=dict(),),\n    head=dict(\n        actor=dict(\n            type=""MLP"",\n            configs=dict(hidden_sizes=[256, 256], output_activation=F.tanh,),\n        ),\n        critic=dict(\n            type=""MLP"",\n            configs=dict(\n                hidden_sizes=[256, 256], output_size=1, output_activation=identity,\n            ),\n        ),\n    ),\n    optim_cfg=dict(lr_actor=1e-4, lr_critic=1e-3, weight_decay=1e-6),\n    noise_cfg=dict(ou_noise_theta=0.0, ou_noise_sigma=0.0),\n)\n'"
configs/reacher_v2/bc_sac.py,0,"b'""""""Config for SAC with Behavior Cloning on Reacher-v2.\n\n- Author: Kyunghwan Kim\n- Contact: kh.kim@medipixel.io\n""""""\nfrom rl_algorithms.common.helper_functions import identity\n\nagent = dict(\n    type=""BCSACAgent"",\n    hyper_params=dict(\n        gamma=0.99,\n        tau=5e-3,\n        buffer_size=int(1e6),\n        batch_size=512,\n        initial_random_action=int(1e4),\n        multiple_update=1,  # multiple learning updates\n        policy_update_freq=2,\n        w_entropy=1e-3,\n        w_mean_reg=1e-3,\n        w_std_reg=1e-3,\n        w_pre_activation_reg=0.0,\n        auto_entropy_tuning=True,\n        # BC\n        demo_batch_size=64,\n        lambda1=1e-3,\n        lambda2=1.0,\n        # HER\n        use_her=True,\n        her=dict(type=""ReacherHER"",),\n        success_score=-5.0,\n        desired_states_from_demo=False,\n    ),\n    backbone=dict(actor=dict(), critic_vf=dict(), critic_qf=dict()),\n    head=dict(\n        actor=dict(\n            type=""TanhGaussianDistParams"",\n            configs=dict(hidden_sizes=[256, 256], output_activation=identity,),\n        ),\n        critic_vf=dict(\n            type=""MLP"",\n            configs=dict(\n                hidden_sizes=[256, 256], output_activation=identity, output_size=1,\n            ),\n        ),\n        critic_qf=dict(\n            type=""MLP"",\n            configs=dict(\n                hidden_sizes=[256, 256], output_activation=identity, output_size=1,\n            ),\n        ),\n    ),\n    optim_cfg=dict(\n        lr_actor=3e-4,\n        lr_vf=3e-4,\n        lr_qf1=3e-4,\n        lr_qf2=3e-4,\n        lr_entropy=3e-4,\n        weight_decay=0.0,\n    ),\n)\n'"
configs/reacher_v2/ddpg.py,1,"b'""""""Config for DDPG on Reacher-v2.\n\n- Author: Kyunghwan Kim\n- Contact: kh.kim@medipixel.io\n""""""\nimport torch.nn.functional as F\n\nfrom rl_algorithms.common.helper_functions import identity\n\nagent = dict(\n    type=""DDPGAgent"",\n    hyper_params=dict(\n        gamma=0.99,\n        tau=1e-3,\n        buffer_size=int(1e5),\n        batch_size=128,\n        initial_random_action=int(1e4),\n        multiple_update=1,  # multiple learning updates\n        gradient_clip_ac=0.5,\n        gradient_clip_cr=1.0,\n    ),\n    backbone=dict(actor=dict(), critic=dict(),),\n    head=dict(\n        actor=dict(\n            type=""MLP"",\n            configs=dict(hidden_sizes=[256, 256], output_activation=F.tanh,),\n        ),\n        critic=dict(\n            type=""MLP"",\n            configs=dict(\n                hidden_sizes=[256, 256], output_size=1, output_activation=identity,\n            ),\n        ),\n    ),\n    optim_cfg=dict(lr_actor=1e-3, lr_critic=1e-3, weight_decay=1e-6),\n    noise_cfg=dict(ou_noise_theta=0.0, ou_noise_sigma=0.0),\n)\n'"
configs/reacher_v2/sac.py,0,"b'""""""Config for SAC on Reacher-v2.\n\n- Author: Kyunghwan Kim\n- Contact: kh.kim@medipixel.io\n""""""\nfrom rl_algorithms.common.helper_functions import identity\n\nagent = dict(\n    type=""SACAgent"",\n    hyper_params=dict(\n        gamma=0.99,\n        tau=5e-3,\n        buffer_size=int(1e6),\n        batch_size=512,\n        initial_random_action=int(2e4),\n        multiple_update=1,  # multiple learning updates\n        policy_update_freq=2,\n        w_entropy=1e-3,\n        w_mean_reg=1e-3,\n        w_std_reg=1e-3,\n        w_pre_activation_reg=0.0,\n        auto_entropy_tuning=True,\n    ),\n    backbone=dict(actor=dict(), critic_vf=dict(), critic_qf=dict()),\n    head=dict(\n        actor=dict(\n            type=""TanhGaussianDistParams"",\n            configs=dict(hidden_sizes=[256, 256], output_activation=identity,),\n        ),\n        critic_vf=dict(\n            type=""MLP"",\n            configs=dict(\n                hidden_sizes=[256, 256], output_activation=identity, output_size=1,\n            ),\n        ),\n        critic_qf=dict(\n            type=""MLP"",\n            configs=dict(\n                hidden_sizes=[256, 256], output_activation=identity, output_size=1,\n            ),\n        ),\n    ),\n    optim_cfg=dict(\n        lr_actor=3e-4,\n        lr_vf=3e-4,\n        lr_qf1=3e-4,\n        lr_qf2=3e-4,\n        lr_entropy=3e-4,\n        weight_decay=0.0,\n    ),\n)\n'"
configs/reacher_v2/td3.py,1,"b'""""""Config for TD3 on Reacher-v2.\n\n- Author: Kyunghwan Kim\n- Contact: kh.kim@medipixel.io\n""""""\nimport torch.nn.functional as F\n\nfrom rl_algorithms.common.helper_functions import identity\n\nagent = dict(\n    type=""TD3Agent"",\n    hyper_params=dict(\n        gamma=0.95,\n        tau=5e-3,\n        buffer_size=int(1e6),\n        batch_size=100,\n        initial_random_action=int(1e4),\n        policy_update_freq=2,\n    ),\n    backbone=dict(actor=dict(), critic=dict()),\n    head=dict(\n        actor=dict(\n            type=""MLP"",\n            configs=dict(hidden_sizes=[400, 300], output_activation=F.tanh,),\n        ),\n        critic=dict(\n            type=""MLP"",\n            configs=dict(\n                hidden_sizes=[400, 300], output_size=1, output_activation=identity,\n            ),\n        ),\n    ),\n    optim_cfg=dict(lr_actor=1e-3, lr_critic=1e-3, weight_decay=0.0),\n    noise_cfg=dict(\n        exploration_noise=0.1, target_policy_noise=0.2, target_policy_noise_clip=0.5\n    ),\n)\n'"
rl_algorithms/a2c/__init__.py,0,"b'""""""Empty.""""""\n'"
rl_algorithms/a2c/agent.py,4,"b'# -*- coding: utf-8 -*-\n""""""1-Step Advantage Actor-Critic agent for episodic tasks in OpenAI Gym.\n\n- Author: Curt Park\n- Contact: curt.park@medipixel.io\n""""""\n\nimport argparse\nfrom typing import Tuple\n\nimport gym\nimport numpy as np\nimport torch\nimport wandb\n\nfrom rl_algorithms.a2c.learner import A2CLearner\nfrom rl_algorithms.common.abstract.agent import Agent\nfrom rl_algorithms.registry import AGENTS\nfrom rl_algorithms.utils.config import ConfigDict\n\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n\n@AGENTS.register_module\nclass A2CAgent(Agent):\n    """"""1-Step Advantage Actor-Critic interacting with environment.\n\n    Attributes:\n        env (gym.Env): openAI Gym environment\n        args (argparse.Namespace): arguments including hyperparameters and training settings\n        hyper_params (ConfigDict): hyper-parameters\n        network_cfg (ConfigDict): config of network for training agent\n        optim_cfg (ConfigDict): config of optimizer\n        state_dim (int): state size of env\n        action_dim (int): action size of env\n        actor (nn.Module): policy model to select actions\n        critic (nn.Module): critic model to evaluate states\n        actor_optim (Optimizer): optimizer for actor\n        critic_optim (Optimizer): optimizer for critic\n        episode_step (int): step number of the current episode\n        i_episode (int): current episode number\n        transition (list): recent transition information\n\n    """"""\n\n    def __init__(\n        self,\n        env: gym.Env,\n        args: argparse.Namespace,\n        log_cfg: ConfigDict,\n        hyper_params: ConfigDict,\n        backbone: ConfigDict,\n        head: ConfigDict,\n        optim_cfg: ConfigDict,\n    ):\n        """"""Initialize.""""""\n        Agent.__init__(self, env, args, log_cfg)\n\n        self.transition: list = list()\n        self.episode_step = 0\n        self.i_episode = 0\n\n        self.hyper_params = hyper_params\n        self.backbone_cfg = backbone\n        self.head_cfg = head\n        self.optim_cfg = optim_cfg\n\n        self.head_cfg.actor.configs.state_size = (\n            self.head_cfg.critic.configs.state_size\n        ) = self.env.observation_space.shape\n        self.head_cfg.actor.configs.output_size = self.env.action_space.shape[0]\n\n        self.learner = A2CLearner(\n            self.args,\n            self.hyper_params,\n            self.log_cfg,\n            self.head_cfg,\n            self.backbone_cfg,\n            self.optim_cfg,\n            device,\n        )\n\n    def select_action(self, state: np.ndarray) -> torch.Tensor:\n        """"""Select an action from the input space.""""""\n        state = torch.FloatTensor(state).to(device)\n\n        selected_action, dist = self.learner.actor(state)\n\n        if self.args.test:\n            selected_action = dist.mean\n        else:\n            predicted_value = self.learner.critic(state)\n            log_prob = dist.log_prob(selected_action).sum(dim=-1)\n            self.transition = []\n            self.transition.extend([log_prob, predicted_value])\n\n        return selected_action\n\n    def step(self, action: torch.Tensor) -> Tuple[np.ndarray, np.float64, bool, dict]:\n        """"""Take an action and return the response of the env.""""""\n\n        action = action.detach().cpu().numpy()\n        next_state, reward, done, info = self.env.step(action)\n\n        if not self.args.test:\n            done_bool = done\n            if self.episode_step == self.args.max_episode_steps:\n                done_bool = False\n            self.transition.extend([next_state, reward, done_bool])\n\n        return next_state, reward, done, info\n\n    def write_log(self, log_value: tuple):\n        i, score, policy_loss, value_loss = log_value\n        total_loss = policy_loss + value_loss\n\n        print(\n            ""[INFO] episode %d\\tepisode step: %d\\ttotal score: %d\\n""\n            ""total loss: %.4f\\tpolicy loss: %.4f\\tvalue loss: %.4f\\n""\n            % (i, self.episode_step, score, total_loss, policy_loss, value_loss)\n        )\n\n        if self.args.log:\n            wandb.log(\n                {\n                    ""total loss"": total_loss,\n                    ""policy loss"": policy_loss,\n                    ""value loss"": value_loss,\n                    ""score"": score,\n                }\n            )\n\n    def train(self):\n        """"""Train the agent.""""""\n        # logger\n        if self.args.log:\n            self.set_wandb()\n            # wandb.watch([self.actor, self.critic], log=""parameters"")\n\n        for self.i_episode in range(1, self.args.episode_num + 1):\n            state = self.env.reset()\n            done = False\n            score = 0\n            policy_loss_episode = list()\n            value_loss_episode = list()\n            self.episode_step = 0\n\n            while not done:\n                if self.args.render and self.i_episode >= self.args.render_after:\n                    self.env.render()\n\n                action = self.select_action(state)\n                next_state, reward, done, _ = self.step(action)\n                self.episode_step += 1\n\n                policy_loss, value_loss = self.learner.update_model(self.transition)\n\n                policy_loss_episode.append(policy_loss)\n                value_loss_episode.append(value_loss)\n\n                state = next_state\n                score += reward\n\n            # logging\n            policy_loss = np.array(policy_loss_episode).mean()\n            value_loss = np.array(value_loss_episode).mean()\n            log_value = (self.i_episode, score, policy_loss, value_loss)\n            self.write_log(log_value)\n\n            if self.i_episode % self.args.save_period == 0:\n                self.learner.save_params(self.i_episode)\n                self.interim_test()\n\n        # termination\n        self.env.close()\n        self.learner.save_params(self.i_episode)\n        self.interim_test()\n'"
rl_algorithms/a2c/learner.py,6,"b'import argparse\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.utils import clip_grad_norm_\nimport torch.optim as optim\n\nfrom rl_algorithms.common.abstract.learner import Learner, TensorTuple\nfrom rl_algorithms.common.networks.brain import Brain\nfrom rl_algorithms.utils.config import ConfigDict\n\n\nclass A2CLearner(Learner):\n    """"""Learner for A2C Agent\n\n    Attributes:\n        args (argparse.Namespace): arguments including hyperparameters and training settings\n        hyper_params (ConfigDict): hyper-parameters\n        log_cfg (ConfigDict): configuration for saving log and checkpoint\n        actor (nn.Module): actor model to select actions\n        critic (nn.Module): critic model to predict state values\n        actor_optim (Optimizer): optimizer for training actor\n        critic_optim (Optimizer): optimizer for training critic\n\n    """"""\n\n    def __init__(\n        self,\n        args: argparse.Namespace,\n        hyper_params: ConfigDict,\n        log_cfg: ConfigDict,\n        head_cfg: ConfigDict,\n        backbone_cfg: ConfigDict,\n        optim_cfg: ConfigDict,\n        device: torch.device,\n    ):\n        Learner.__init__(self, args, hyper_params, log_cfg, device)\n\n        self.head_cfg = head_cfg\n        self.backbone_cfg = backbone_cfg\n        self.optim_cfg = optim_cfg\n\n        self._init_network()\n\n    def _init_network(self):\n        """"""Initialize networks and optimizers.""""""\n        self.actor = Brain(self.backbone_cfg.actor, self.head_cfg.actor).to(self.device)\n        self.critic = Brain(self.backbone_cfg.critic, self.head_cfg.critic).to(\n            self.device\n        )\n\n        # create optimizer\n        self.actor_optim = optim.Adam(\n            self.actor.parameters(),\n            lr=self.optim_cfg.lr_actor,\n            weight_decay=self.optim_cfg.weight_decay,\n        )\n\n        self.critic_optim = optim.Adam(\n            self.critic.parameters(),\n            lr=self.optim_cfg.lr_critic,\n            weight_decay=self.optim_cfg.weight_decay,\n        )\n\n        if self.args.load_from is not None:\n            self.load_params(self.args.load_from)\n\n    def update_model(self, experience: TensorTuple) -> TensorTuple:\n        """"""Update A2C actor and critic networks""""""\n\n        log_prob, pred_value, next_state, reward, done = experience\n        next_state = torch.FloatTensor(next_state).to(self.device)\n\n        # Q_t   = r + gamma * V(s_{t+1})  if state != Terminal\n        #       = r                       otherwise\n        mask = 1 - done\n        next_value = self.critic(next_state).detach()\n        q_value = reward + self.hyper_params.gamma * next_value * mask\n        q_value = q_value.to(self.device)\n\n        # advantage = Q_t - V(s_t)\n        advantage = q_value - pred_value\n\n        # calculate loss at the current step\n        policy_loss = -advantage.detach() * log_prob  # adv. is not backpropagated\n        policy_loss += self.hyper_params.w_entropy * -log_prob  # entropy\n        value_loss = F.smooth_l1_loss(pred_value, q_value.detach())\n\n        # train\n        gradient_clip_ac = self.hyper_params.gradient_clip_ac\n        gradient_clip_cr = self.hyper_params.gradient_clip_cr\n\n        self.actor_optim.zero_grad()\n        policy_loss.backward()\n        clip_grad_norm_(self.actor.parameters(), gradient_clip_ac)\n        self.actor_optim.step()\n\n        self.critic_optim.zero_grad()\n        value_loss.backward()\n        clip_grad_norm_(self.critic.parameters(), gradient_clip_cr)\n        self.critic_optim.step()\n\n        return policy_loss.item(), value_loss.item()\n\n    def save_params(self, n_episode: int):\n        """"""Save model and optimizer parameters.""""""\n        params = {\n            ""actor_state_dict"": self.actor.state_dict(),\n            ""critic_state_dict"": self.critic.state_dict(),\n            ""actor_optim_state_dict"": self.actor_optim.state_dict(),\n            ""critic_optim_state_dict"": self.critic_optim.state_dict(),\n        }\n\n        Learner._save_params(self, params, n_episode)\n\n    def load_params(self, path: str):\n        """"""Load model and optimizer parameters.""""""\n        Learner.load_params(self, path)\n\n        params = torch.load(path)\n        self.actor.load_state_dict(params[""actor_state_dict""])\n        self.critic.load_state_dict(params[""critic_state_dict""])\n        self.actor_optim.load_state_dict(params[""actor_optim_state_dict""])\n        self.critic_optim.load_state_dict(params[""critic_optim_state_dict""])\n        print(""[INFO] Loaded the model and optimizer from"", path)\n'"
rl_algorithms/bc/__init__.py,0,"b'""""""Empty.""""""\n'"
rl_algorithms/bc/ddpg_agent.py,3,"b'# -*- coding: utf-8 -*-\n""""""Behaviour Cloning with DDPG agent for episodic tasks in OpenAI Gym.\n\n- Author: Kyunghwan Kim\n- Contact: kh.kim@medipixel.io\n- Paper: https://arxiv.org/pdf/1709.10089.pdf\n""""""\n\nimport pickle\nimport time\nfrom typing import Tuple\n\nimport numpy as np\nimport torch\nimport wandb\n\nfrom rl_algorithms.bc.ddpg_learner import BCDDPGLearner\nfrom rl_algorithms.common.buffer.replay_buffer import ReplayBuffer\nfrom rl_algorithms.common.helper_functions import numpy2floattensor\nfrom rl_algorithms.ddpg.agent import DDPGAgent\nfrom rl_algorithms.registry import AGENTS, build_her\n\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n\n@AGENTS.register_module\nclass BCDDPGAgent(DDPGAgent):\n    """"""BC with DDPG agent interacting with environment.\n\n    Attributes:\n        her (HER): hinsight experience replay\n        transitions_epi (list): transitions per episode (for HER)\n        desired_state (np.ndarray): desired state of current episode\n        memory (ReplayBuffer): replay memory\n        demo_memory (ReplayBuffer): replay memory for demo\n        lambda2 (float): proportion of BC loss\n\n    """"""\n\n    # pylint: disable=attribute-defined-outside-init\n    def _initialize(self):\n        """"""Initialize non-common things.""""""\n        # load demo replay memory\n        with open(self.args.demo_path, ""rb"") as f:\n            demo = list(pickle.load(f))\n\n        # HER\n        if self.hyper_params.use_her:\n            self.her = build_her(self.hyper_params.her)\n            print(f""[INFO] Build {str(self.her)}."")\n\n            if self.hyper_params.desired_states_from_demo:\n                self.her.fetch_desired_states_from_demo(demo)\n\n            self.transitions_epi: list = list()\n            self.desired_state = np.zeros((1,))\n            demo = self.her.generate_demo_transitions(demo)\n\n            if not self.her.is_goal_in_state:\n                self.state_dim = (self.state_dim[0] * 2,)\n        else:\n            self.her = None\n\n        if not self.args.test:\n            # Replay buffers\n            demo_batch_size = self.hyper_params.demo_batch_size\n            self.demo_memory = ReplayBuffer(len(demo), demo_batch_size)\n            self.demo_memory.extend(demo)\n\n            self.memory = ReplayBuffer(\n                self.hyper_params.buffer_size, self.hyper_params.batch_size\n            )\n\n            # set hyper parameters\n            self.hyper_params[""lambda2""] = 1.0 / demo_batch_size\n\n        self.learner = BCDDPGLearner(\n            self.args,\n            self.hyper_params,\n            self.log_cfg,\n            self.head_cfg,\n            self.backbone_cfg,\n            self.optim_cfg,\n            device,\n        )\n\n    def _preprocess_state(self, state: np.ndarray) -> torch.Tensor:\n        """"""Preprocess state so that actor selects an action.""""""\n        if self.hyper_params.use_her:\n            self.desired_state = self.her.get_desired_state()\n            state = np.concatenate((state, self.desired_state), axis=-1)\n        state = torch.FloatTensor(state).to(device)\n        return state\n\n    def _add_transition_to_memory(self, transition: Tuple[np.ndarray, ...]):\n        """"""Add 1 step and n step transitions to memory.""""""\n        if self.hyper_params.use_her:\n            self.transitions_epi.append(transition)\n            done = transition[-1] or self.episode_step == self.args.max_episode_steps\n            if done:\n                # insert generated transitions if the episode is done\n                transitions = self.her.generate_transitions(\n                    self.transitions_epi,\n                    self.desired_state,\n                    self.hyper_params.success_score,\n                )\n                self.memory.extend(transitions)\n                self.transitions_epi.clear()\n        else:\n            self.memory.add(transition)\n\n    def write_log(self, log_value: tuple):\n        """"""Write log about loss and score""""""\n        i, loss, score, avg_time_cost = log_value\n        total_loss = loss.sum()\n\n        print(\n            ""[INFO] episode %d, episode step: %d, total step: %d, total score: %d\\n""\n            ""total loss: %f actor_loss: %.3f critic_loss: %.3f, n_qf_mask: %d ""\n            ""(spent %.6f sec/step)\\n""\n            % (\n                i,\n                self.episode_step,\n                self.total_step,\n                score,\n                total_loss,\n                loss[0],\n                loss[1],\n                loss[2],\n                avg_time_cost,\n            )  # actor loss  # critic loss\n        )\n\n        if self.args.log:\n            wandb.log(\n                {\n                    ""score"": score,\n                    ""total loss"": total_loss,\n                    ""actor loss"": loss[0],\n                    ""critic loss"": loss[1],\n                    ""time per each step"": avg_time_cost,\n                }\n            )\n\n    def train(self):\n        """"""Train the agent.""""""\n        # logger\n        if self.args.log:\n            self.set_wandb()\n            # wandb.watch([self.actor, self.critic], log=""parameters"")\n\n        # pre-training if needed\n        self.pretrain()\n\n        for self.i_episode in range(1, self.args.episode_num + 1):\n            state = self.env.reset()\n            done = False\n            score = 0\n            self.episode_step = 0\n            losses = list()\n\n            t_begin = time.time()\n\n            while not done:\n                if self.args.render and self.i_episode >= self.args.render_after:\n                    self.env.render()\n\n                action = self.select_action(state)\n                next_state, reward, done, _ = self.step(action)\n                self.total_step += 1\n                self.episode_step += 1\n\n                if len(self.memory) >= self.hyper_params.batch_size:\n                    for _ in range(self.hyper_params.multiple_update):\n                        experience = self.memory.sample()\n                        demos = self.demo_memory.sample()\n                        experience, demos = (\n                            numpy2floattensor(experience),\n                            numpy2floattensor(demos),\n                        )\n                        loss = self.learner.update_model(experience, demos)\n                        losses.append(loss)  # for logging\n\n                state = next_state\n                score += reward\n\n            t_end = time.time()\n            avg_time_cost = (t_end - t_begin) / self.episode_step\n\n            # logging\n            if losses:\n                avg_loss = np.vstack(losses).mean(axis=0)\n                log_value = (self.i_episode, avg_loss, score, avg_time_cost)\n                self.write_log(log_value)\n                losses.clear()\n\n            if self.i_episode % self.args.save_period == 0:\n                self.learner.save_params(self.i_episode)\n                self.interim_test()\n\n        # termination\n        self.env.close()\n        self.learner.save_params(self.i_episode)\n        self.interim_test()\n'"
rl_algorithms/bc/ddpg_learner.py,16,"b'import argparse\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.utils import clip_grad_norm_\n\nfrom rl_algorithms.common.abstract.learner import TensorTuple\nimport rl_algorithms.common.helper_functions as common_utils\nfrom rl_algorithms.ddpg.learner import DDPGLearner\nfrom rl_algorithms.utils.config import ConfigDict\n\n\nclass BCDDPGLearner(DDPGLearner):\n    """"""Learner for BCDDPG Agent\n\n    Attributes:\n        args (argparse.Namespace): arguments including hyperparameters and training settings\n        hyper_params (ConfigDict): hyper-parameters\n        optim_cfg (ConfigDict): config of optimizer\n        log_cfg (ConfigDict): configuration for saving log and checkpoint\n        actor (nn.Module): actor model to select actions\n        actor_target (nn.Module): target actor model to select actions\n        critic (nn.Module): critic model to predict state values\n        critic_target (nn.Module): target critic model to predict state values\n        actor_optim (Optimizer): optimizer for training actor\n        critic_optim (Optimizer): optimizer for training critic\n\n    """"""\n\n    def __init__(\n        self,\n        args: argparse.Namespace,\n        hyper_params: ConfigDict,\n        log_cfg: ConfigDict,\n        head_cfg: ConfigDict,\n        backbone_cfg: ConfigDict,\n        optim_cfg: ConfigDict,\n        device: torch.device,\n    ):\n        DDPGLearner.__init__(\n            self, args, hyper_params, log_cfg, head_cfg, backbone_cfg, optim_cfg, device\n        )\n\n    def update_model(\n        self, experience: TensorTuple, demos: TensorTuple\n    ) -> TensorTuple:  # type: ignore\n        """"""Update ddpg actor and critic networks""""""\n        exp_states, exp_actions, exp_rewards, exp_next_states, exp_dones = experience\n        demo_states, demo_actions, demo_rewards, demo_next_states, demo_dones = demos\n\n        states = torch.cat((exp_states, demo_states), dim=0)\n        actions = torch.cat((exp_actions, demo_actions), dim=0)\n        rewards = torch.cat((exp_rewards, demo_rewards), dim=0)\n        next_states = torch.cat((exp_next_states, demo_next_states), dim=0)\n        dones = torch.cat((exp_dones, demo_dones), dim=0)\n\n        # G_t   = r + gamma * v(s_{t+1})  if state != Terminal\n        #       = r                       otherwise\n        masks = 1 - dones\n        next_actions = self.actor_target(next_states)\n        next_values = self.critic_target(torch.cat((next_states, next_actions), dim=-1))\n        curr_returns = rewards + (self.hyper_params.gamma * next_values * masks)\n        curr_returns = curr_returns.to(self.device)\n\n        # critic loss\n        gradient_clip_ac = self.hyper_params.gradient_clip_ac\n        gradient_clip_cr = self.hyper_params.gradient_clip_cr\n\n        values = self.critic(torch.cat((states, actions), dim=-1))\n        critic_loss = F.mse_loss(values, curr_returns)\n\n        # train critic\n        self.critic_optim.zero_grad()\n        critic_loss.backward()\n        clip_grad_norm_(self.critic.parameters(), gradient_clip_cr)\n        self.critic_optim.step()\n\n        # policy loss\n        actions = self.actor(states)\n        policy_loss = -self.critic(torch.cat((states, actions), dim=-1)).mean()\n\n        # bc loss\n        pred_actions = self.actor(demo_states)\n        qf_mask = torch.gt(\n            self.critic(torch.cat((demo_states, demo_actions), dim=-1)),\n            self.critic(torch.cat((demo_states, pred_actions), dim=-1)),\n        ).to(self.device)\n        qf_mask = qf_mask.float()\n        n_qf_mask = int(qf_mask.sum().item())\n\n        if n_qf_mask == 0:\n            bc_loss = torch.zeros(1, device=self.device)\n        else:\n            bc_loss = (\n                torch.mul(pred_actions, qf_mask) - torch.mul(demo_actions, qf_mask)\n            ).pow(2).sum() / n_qf_mask\n\n        # train actor: pg loss + BC loss\n        actor_loss = (\n            self.hyper_params.lambda1 * policy_loss\n            + self.hyper_params.lambda2 * bc_loss\n        )\n        self.actor_optim.zero_grad()\n        actor_loss.backward()\n        clip_grad_norm_(self.actor.parameters(), gradient_clip_ac)\n        self.actor_optim.step()\n\n        # update target networks\n        common_utils.soft_update(self.actor, self.actor_target, self.hyper_params.tau)\n        common_utils.soft_update(self.critic, self.critic_target, self.hyper_params.tau)\n\n        return actor_loss.item(), critic_loss.item(), n_qf_mask\n'"
rl_algorithms/bc/her.py,0,"b'# -*- coding: utf-8 -*-\n""""""HER class and reward function for Behavior Cloning.\n\n- Author: Kyunghwan Kim\n- Contact: kh.kim@medipixel.io\n""""""\nfrom typing import Callable, Tuple\n\nimport numpy as np\n\nfrom rl_algorithms.common.abstract.her import HER\nfrom rl_algorithms.common.abstract.reward_fn import RewardFn\nfrom rl_algorithms.registry import HERS\n\n\nclass L1DistanceRewardFn(RewardFn):\n    def __call__(self, transition: tuple, goal_state: np.ndarray) -> np.float64:\n        """"""L1 Distance reward function.""""""\n        next_state = transition[3]\n        eps = 1e-6\n        if np.abs(next_state - goal_state).sum() < eps:\n            return np.float64(0.0)\n        else:\n            return np.float64(-1.0)\n\n\nl1_distance_reward_fn = L1DistanceRewardFn()\n\n\n@HERS.register_module\nclass LunarLanderContinuousHER(HER):\n    """"""HER for LunarLanderContinuous-v2 environment.\n\n    Attributes:\n        demo_goal_indices (np.ndarray): indices about goal of demo list\n        desired_states (np.ndarray): desired states from demonstration\n\n    """"""\n\n    def __init__(\n        self,\n        reward_fn: Callable[[tuple, np.ndarray], np.float64] = l1_distance_reward_fn,\n    ):\n        """"""Initialize.""""""\n        HER.__init__(self, reward_fn=reward_fn)\n        self.is_goal_in_state = False\n\n    # pylint: disable=attribute-defined-outside-init\n    def fetch_desired_states_from_demo(self, demo: list):\n        """"""Return desired goal states from demonstration data.""""""\n        np_demo: np.ndarray = np.array(demo)\n        self.demo_goal_indices: np.ndarray = np.where(np_demo[:, 4])[0]\n        self.desired_states: np.ndarray = np_demo[self.demo_goal_indices][:, 0]\n\n    def get_desired_state(self, *args) -> np.ndarray:\n        """"""Sample one of the desired states.""""""\n        return np.random.choice(self.desired_states, 1).item()\n\n    def _get_final_state(self, transition: tuple) -> np.ndarray:\n        """"""Get final state from transitions for making HER transitions.""""""\n        return transition[0]\n\n    def generate_demo_transitions(self, demo: list) -> list:\n        """"""Return generated demo transitions for HER.""""""\n        new_demo: list = list()\n\n        # generate demo transitions\n        prev_idx = 0\n        for idx in self.demo_goal_indices:\n            demo_final_state = self._get_final_state(demo[idx])\n            transitions = [demo[i] for i in range(prev_idx, idx + 1)]\n            prev_idx = idx + 1\n\n            transitions = self.generate_transitions(\n                transitions, demo_final_state, 0, is_demo=True\n            )\n\n            new_demo.extend(transitions)\n\n        return new_demo\n\n\nclass ReacherRewardFn(RewardFn):\n    def __call__(self, transition: tuple, _) -> np.float64:\n        """"""Reward function for Reacher-v2 environment.""""""\n        state, action = transition[0:2]\n        diff_vec = state[-3:]\n        reward_dist = -1 * np.linalg.norm(diff_vec)\n        reward_ctrl = -np.square(action).sum()\n\n        return reward_dist + reward_ctrl\n\n\nreacher_reward_fn = ReacherRewardFn()\n\n\n@HERS.register_module\nclass ReacherHER(HER):\n    """"""HER for Reacher-v2 environment.""""""\n\n    def __init__(\n        self, reward_fn: Callable[[tuple, np.ndarray], np.float64] = reacher_reward_fn\n    ):\n        """"""Initialize.""""""\n        HER.__init__(self, reward_fn=reward_fn)\n        self.is_goal_in_state = True\n\n    def fetch_desired_states_from_demo(self, _: list):\n        """"""Return desired goal states from demonstration data.\n\n        DO NOT use this method because demo states have a goal position.\n        """"""\n        raise Exception(""Do not use this method."")\n\n    def get_desired_state(self, *args) -> np.ndarray:\n        """"""Sample one of the desired states.\n\n        Returns an empty array since demo states have a goal position.\n        """"""\n        return np.array([])\n\n    def _get_final_state(self, transition_final: tuple) -> np.ndarray:\n        """"""Get a finger-tip position from the final transition.""""""\n        return transition_final[0][8:10] + transition_final[0][2:4]\n\n    def generate_demo_transitions(self, demo: list) -> list:\n        """"""Return generated demo transitions for HER.\n\n        Works as an identity function in this class.\n        """"""\n        return demo\n\n    def _append_origin_transitions(\n        self, origin_transitions: list, transition: tuple, _: np.ndarray\n    ):\n        """"""Append original transitions for training.""""""\n        origin_transitions.append(transition)\n\n    def _get_transition(\n        self, transition: tuple, goal_state: np.ndarray\n    ) -> Tuple[np.ndarray, np.ndarray, np.float64, np.ndarray, bool]:\n        """"""Get a single transition concatenated with a goal state.""""""\n        state, action, _, next_state, done = transition\n\n        reward = self.reward_fn(transition, goal_state)\n        state_ = state\n        state_[4:6] = goal_state\n        next_state_ = next_state\n        next_state_[4:6] = goal_state\n\n        return state_, action, reward, next_state_, done\n'"
rl_algorithms/bc/sac_agent.py,3,"b'# -*- coding: utf-8 -*-\n""""""BC with SAC agent for episodic tasks in OpenAI Gym.\n\n- Author: Curt Park\n- Contact: curt.park@medipixel.io\n- Paper: https://arxiv.org/pdf/1801.01290.pdf\n         https://arxiv.org/pdf/1812.05905.pdf\n         https://arxiv.org/pdf/1709.10089.pdf\n""""""\n\nimport pickle\nimport time\nfrom typing import Tuple\n\nimport numpy as np\nimport torch\nimport wandb\n\nfrom rl_algorithms.bc.sac_learner import BCSACLearner\nfrom rl_algorithms.common.buffer.replay_buffer import ReplayBuffer\nfrom rl_algorithms.common.helper_functions import numpy2floattensor\nfrom rl_algorithms.registry import AGENTS, build_her\nfrom rl_algorithms.sac.agent import SACAgent\n\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n\n@AGENTS.register_module\nclass BCSACAgent(SACAgent):\n    """"""BC with SAC agent interacting with environment.\n\n    Attrtibutes:\n        her (HER): hinsight experience replay\n        transitions_epi (list): transitions per episode (for HER)\n        desired_state (np.ndarray): desired state of current episode\n        memory (ReplayBuffer): replay memory\n        demo_memory (ReplayBuffer): replay memory for demo\n        lambda2 (float): proportion of BC loss\n\n    """"""\n\n    # pylint: disable=attribute-defined-outside-init\n    def _initialize(self):\n        """"""Initialize non-common things.""""""\n        # load demo replay memory\n        with open(self.args.demo_path, ""rb"") as f:\n            demo = list(pickle.load(f))\n\n        # HER\n        if self.hyper_params.use_her:\n            self.her = build_her(self.hyper_params.her)\n            print(f""[INFO] Build {str(self.her)}."")\n\n            if self.hyper_params.desired_states_from_demo:\n                self.her.fetch_desired_states_from_demo(demo)\n\n            self.transitions_epi: list = list()\n            self.desired_state = np.zeros((1,))\n            demo = self.her.generate_demo_transitions(demo)\n\n            if not self.her.is_goal_in_state:\n                self.state_dim = (self.state_dim[0] * 2,)\n        else:\n            self.her = None\n\n        if not self.args.test:\n            # Replay buffers\n            demo_batch_size = self.hyper_params.demo_batch_size\n            self.demo_memory = ReplayBuffer(len(demo), demo_batch_size)\n            self.demo_memory.extend(demo)\n\n            self.memory = ReplayBuffer(self.hyper_params.buffer_size, demo_batch_size)\n\n            # set hyper parameters\n            self.hyper_params[""lambda2""] = 1.0 / demo_batch_size\n\n        self.learner = BCSACLearner(\n            self.args,\n            self.hyper_params,\n            self.log_cfg,\n            self.head_cfg,\n            self.backbone_cfg,\n            self.optim_cfg,\n            device,\n        )\n\n    def _preprocess_state(self, state: np.ndarray) -> torch.Tensor:\n        """"""Preprocess state so that actor selects an action.""""""\n        if self.hyper_params.use_her:\n            self.desired_state = self.her.get_desired_state()\n            state = np.concatenate((state, self.desired_state), axis=-1)\n        state = torch.FloatTensor(state).to(device)\n        return state\n\n    def _add_transition_to_memory(self, transition: Tuple[np.ndarray, ...]):\n        """"""Add 1 step and n step transitions to memory.""""""\n        if self.hyper_params.use_her:\n            self.transitions_epi.append(transition)\n            done = transition[-1] or self.episode_step == self.args.max_episode_steps\n            if done:\n                # insert generated transitions if the episode is done\n                transitions = self.her.generate_transitions(\n                    self.transitions_epi,\n                    self.desired_state,\n                    self.hyper_params.success_score,\n                )\n                self.memory.extend(transitions)\n                self.transitions_epi.clear()\n        else:\n            self.memory.add(transition)\n\n    def write_log(self, log_value: tuple):\n        """"""Write log about loss and score""""""\n        i, loss, score, policy_update_freq, avg_time_cost = log_value\n        total_loss = loss.sum()\n\n        print(\n            ""[INFO] episode %d, episode_step %d, total step %d, total score: %d\\n""\n            ""total loss: %.3f actor_loss: %.3f qf_1_loss: %.3f qf_2_loss: %.3f ""\n            ""vf_loss: %.3f alpha_loss: %.3f n_qf_mask: %d (spent %.6f sec/step)\\n""\n            % (\n                i,\n                self.episode_step,\n                self.total_step,\n                score,\n                total_loss,\n                loss[0] * policy_update_freq,  # actor loss\n                loss[1],  # qf_1 loss\n                loss[2],  # qf_2 loss\n                loss[3],  # vf loss\n                loss[4],  # alpha loss\n                loss[5],  # n_qf_mask\n                avg_time_cost,\n            )\n        )\n\n        if self.args.log:\n            wandb.log(\n                {\n                    ""score"": score,\n                    ""total loss"": total_loss,\n                    ""actor loss"": loss[0] * policy_update_freq,\n                    ""qf_1 loss"": loss[1],\n                    ""qf_2 loss"": loss[2],\n                    ""vf loss"": loss[3],\n                    ""alpha loss"": loss[4],\n                    ""time per each step"": avg_time_cost,\n                }\n            )\n\n    def train(self):\n        """"""Train the agent.""""""\n        # logger\n        if self.args.log:\n            self.set_wandb()\n            # wandb.watch([self.actor, self.vf, self.qf_1, self.qf_2], log=""parameters"")\n\n        # pre-training if needed\n        self.pretrain()\n\n        for self.i_episode in range(1, self.args.episode_num + 1):\n            state = self.env.reset()\n            done = False\n            score = 0\n            self.episode_step = 0\n            loss_episode = list()\n\n            t_begin = time.time()\n\n            while not done:\n                if self.args.render and self.i_episode >= self.args.render_after:\n                    self.env.render()\n\n                action = self.select_action(state)\n                next_state, reward, done, _ = self.step(action)\n                self.total_step += 1\n                self.episode_step += 1\n\n                state = next_state\n                score += reward\n\n                # training\n                if len(self.memory) >= self.hyper_params.batch_size:\n                    for _ in range(self.hyper_params.multiple_update):\n                        experience = self.memory.sample()\n                        demos = self.demo_memory.sample()\n                        experience, demo = (\n                            numpy2floattensor(experience),\n                            numpy2floattensor(demos),\n                        )\n                        loss = self.learner.update_model(experience, demo)\n                        loss_episode.append(loss)  # for logging\n\n            t_end = time.time()\n            avg_time_cost = (t_end - t_begin) / self.episode_step\n\n            # logging\n            if loss_episode:\n                avg_loss = np.vstack(loss_episode).mean(axis=0)\n                log_value = (\n                    self.i_episode,\n                    avg_loss,\n                    score,\n                    self.hyper_params.policy_update_freq,\n                    avg_time_cost,\n                )\n                self.write_log(log_value)\n\n            if self.i_episode % self.args.save_period == 0:\n                self.learner.save_params(self.i_episode)\n                self.interim_test()\n\n        # termination\n        self.env.close()\n        self.learner.save_params(self.i_episode)\n        self.interim_test()\n'"
rl_algorithms/bc/sac_learner.py,12,"b'import argparse\n\nimport torch\nimport torch.nn.functional as F\n\nfrom rl_algorithms.common.abstract.learner import TensorTuple\nimport rl_algorithms.common.helper_functions as common_utils\nfrom rl_algorithms.sac.learner import SACLearner\nfrom rl_algorithms.utils.config import ConfigDict\n\n\nclass BCSACLearner(SACLearner):\n    """"""Learner for BCSAC Agent\n\n    Attributes:\n        args (argparse.Namespace): arguments including hyperparameters and training settings\n        hyper_params (ConfigDict): hyper-parameters\n        log_cfg (ConfigDict): configuration for saving log and checkpoint\n    """"""\n\n    def __init__(\n        self,\n        args: argparse.Namespace,\n        hyper_params: ConfigDict,\n        log_cfg: ConfigDict,\n        head_cfg: ConfigDict,\n        backbone_cfg: ConfigDict,\n        optim_cfg: ConfigDict,\n        device: torch.device,\n    ):\n        SACLearner.__init__(\n            self, args, hyper_params, log_cfg, head_cfg, backbone_cfg, optim_cfg, device\n        )\n\n    def update_model(\n        self, experience: TensorTuple, demos: TensorTuple\n    ) -> TensorTuple:  # type: ignore\n        """"""Train the model after each episode.""""""\n        states, actions, rewards, next_states, dones = experience\n        demo_states, demo_actions, _, _, _ = demos\n        new_actions, log_prob, pre_tanh_value, mu, std = self.actor(states)\n        pred_actions, _, _, _, _ = self.actor(demo_states)\n\n        # train alpha\n        if self.hyper_params.auto_entropy_tuning:\n            alpha_loss = (\n                -self.log_alpha * (log_prob + self.target_entropy).detach()\n            ).mean()\n\n            self.alpha_optim.zero_grad()\n            alpha_loss.backward()\n            self.alpha_optim.step()\n\n            alpha = self.log_alpha.exp()\n        else:\n            alpha_loss = torch.zeros(1)\n            alpha = self.hyper_params.w_entropy\n\n        # Q function loss\n        masks = 1 - dones\n        states_actions = torch.cat((states, actions), dim=-1)\n        q_1_pred = self.qf_1(states_actions)\n        q_2_pred = self.qf_2(states_actions)\n        v_target = self.vf_target(next_states)\n        q_target = rewards + self.hyper_params.gamma * v_target * masks\n        qf_1_loss = F.mse_loss(q_1_pred, q_target.detach())\n        qf_2_loss = F.mse_loss(q_2_pred, q_target.detach())\n\n        # V function loss\n        states_actions = torch.cat((states, new_actions), dim=-1)\n        v_pred = self.vf(states)\n        q_pred = torch.min(self.qf_1(states_actions), self.qf_2(states_actions))\n        v_target = q_pred - alpha * log_prob\n        vf_loss = F.mse_loss(v_pred, v_target.detach())\n\n        # train Q functions\n        self.qf_1_optim.zero_grad()\n        qf_1_loss.backward()\n        self.qf_1_optim.step()\n\n        self.qf_2_optim.zero_grad()\n        qf_2_loss.backward()\n        self.qf_2_optim.step()\n\n        # train V function\n        self.vf_optim.zero_grad()\n        vf_loss.backward()\n        self.vf_optim.step()\n\n        # update actor\n        actor_loss = torch.zeros(1)\n        n_qf_mask = 0\n        if self.update_step % self.hyper_params.policy_update_freq == 0:\n            # bc loss\n            qf_mask = torch.gt(\n                self.qf_1(torch.cat((demo_states, demo_actions), dim=-1)),\n                self.qf_1(torch.cat((demo_states, pred_actions), dim=-1)),\n            ).to(self.device)\n            qf_mask = qf_mask.float()\n            n_qf_mask = int(qf_mask.sum().item())\n\n            if n_qf_mask == 0:\n                bc_loss = torch.zeros(1, device=self.device)\n            else:\n                bc_loss = (\n                    torch.mul(pred_actions, qf_mask) - torch.mul(demo_actions, qf_mask)\n                ).pow(2).sum() / n_qf_mask\n\n            # actor loss\n            advantage = q_pred - v_pred.detach()\n            actor_loss = (alpha * log_prob - advantage).mean()\n            actor_loss = (\n                self.hyper_params.lambda1 * actor_loss\n                + self.hyper_params.lambda2 * bc_loss\n            )\n\n            # regularization\n            mean_reg, std_reg = (\n                self.hyper_params.w_mean_reg * mu.pow(2).mean(),\n                self.hyper_params.w_std_reg * std.pow(2).mean(),\n            )\n            pre_activation_reg = self.hyper_params.w_pre_activation_reg * (\n                pre_tanh_value.pow(2).sum(dim=-1).mean()\n            )\n            actor_reg = mean_reg + std_reg + pre_activation_reg\n\n            # actor loss + regularization\n            actor_loss += actor_reg\n\n            # train actor\n            self.actor_optim.zero_grad()\n            actor_loss.backward()\n            self.actor_optim.step()\n\n            # update target networks\n            common_utils.soft_update(self.vf, self.vf_target, self.hyper_params.tau)\n\n        return (\n            actor_loss.item(),\n            qf_1_loss.item(),\n            qf_2_loss.item(),\n            vf_loss.item(),\n            alpha_loss.item(),\n            n_qf_mask,\n        )\n'"
rl_algorithms/common/__init__.py,0,"b'""""""Empty.""""""\n'"
rl_algorithms/common/grad_cam.py,12,"b'""""""Grad-CAM class for analyzing CNN network.\n\n- Author: Kyunghwan Kim\n- Contact: kh.kim@medipixel.io\n- Paper: https://arxiv.org/pdf/1610.02391v1.pdf\n- Reference: https://github.com/RRoundTable/XAI\n""""""\n\nfrom collections import OrderedDict\nfrom typing import Callable\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n\n# pylint: disable=attribute-defined-outside-init\nclass CAMBaseWrapper:\n    """"""Base Wrapping module for CAM.""""""\n\n    def __init__(self, model: nn.Module):\n        """"""Initialize.""""""\n        super(CAMBaseWrapper, self).__init__()\n        self.device = next(model.parameters()).device\n        self.model = model\n        self.handlers = []  # a set of hook function handlers\n\n    def _encode_one_hot(self, ids: torch.Tensor) -> torch.Tensor:\n        """"""Convert input to one-hot.""""""\n        one_hot = torch.zeros_like(self.logits).to(self.device)\n        one_hot[0][ids] = 1\n        return one_hot\n\n    def forward(self, image: torch.Tensor) -> torch.Tensor:\n        """"""\n        Simple classification\n        """"""\n        self.model.zero_grad()\n        self.logits = self.model(image)\n        return self.logits\n\n    def backward(self, ids: torch.Tensor) -> torch.Tensor:\n        """"""\n        Class-specific backpropagation.\n        Either way works:\n        1. self.logits.backward(gradient=one_hot, retain_graph=True)\n        2. (self.logits * one_hot).sum().backward(retain_graph=True)\n        """"""\n\n        one_hot = self._encode_one_hot(ids)\n        self.logits.backward(gradient=one_hot, retain_graph=True)\n\n    def generate(self, target_layer: str):\n        raise NotImplementedError\n\n    def remove_hook(self):\n        """"""\n        Remove all the forward/backward hook functions\n        """"""\n        for handle in self.handlers:\n            handle.remove()\n\n\n# pylint: disable=attribute-defined-outside-init\nclass GradCAM(CAMBaseWrapper):\n    """"""\n    ""Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization""\n    https://arxiv.org/pdf/1610.02391.pdf\n    Look at Figure 2 on page 4\n    """"""\n\n    def __init__(self, model: nn.Module, candidate_layers: list = None):\n        """"""Initialize.""""""\n        super(GradCAM, self).__init__(model)\n        self.fmap_pool = OrderedDict()\n        self.grad_pool = OrderedDict()\n        self.candidate_layers = candidate_layers  # list\n\n        def forward_hook(key: str) -> Callable:\n            def forward_hook_(_, __, output: torch.Tensor):\n                # Save featuremaps\n                self.fmap_pool[key] = output.detach()\n\n            return forward_hook_\n\n        def backward_hook(key: str) -> Callable:\n            def backward_hook_(_, __, grad_out: tuple):\n                # Save the gradients correspond to the featuremaps\n                self.grad_pool[key] = grad_out[0].detach()\n\n            return backward_hook_\n\n        # If any candidates are not specified, the hook is registered to all the layers.\n        for name, module in self.model.named_modules():\n            print(name, module)\n            if self.candidate_layers is None or name in self.candidate_layers:\n                self.handlers.append(module.register_forward_hook(forward_hook(name)))\n                self.handlers.append(module.register_backward_hook(backward_hook(name)))\n\n    @staticmethod\n    def _find(pool: OrderedDict, target_layer: str) -> torch.Tensor:\n        """"""Get designated layer from model.""""""\n        if target_layer in pool.keys():\n            return pool[target_layer]\n        else:\n            raise ValueError(""Invalid layer name: {}"".format(target_layer))\n\n    @staticmethod\n    def _compute_grad_weights(grads: torch.Tensor) -> torch.Tensor:\n        """"""Compute gradient weight with average pooling.""""""\n        return F.adaptive_avg_pool2d(grads, 1)\n\n    def forward(self, image: np.ndarray) -> torch.Tensor:\n        """"""Forward method implementation.""""""\n        self.image_shape = image.shape[1:]\n        return super(GradCAM, self).forward(image)\n\n    def generate(self, target_layer: str) -> torch.Tensor:\n        """"""Generate feature map of target layer with Grad-CAM.""""""\n        fmaps = self._find(self.fmap_pool, target_layer)\n        grads = self._find(self.grad_pool, target_layer)\n        weights = self._compute_grad_weights(grads)\n\n        gcam = torch.mul(fmaps, weights).sum(dim=1, keepdim=True)\n        gcam = F.relu(gcam)\n\n        gcam = F.interpolate(\n            gcam, self.image_shape, mode=""bilinear"", align_corners=False\n        )\n\n        B, C, H, W = gcam.shape\n        gcam = gcam.view(B, -1)\n        gcam -= gcam.min(dim=1, keepdim=True)[0]\n        gcam /= gcam.max(dim=1, keepdim=True)[0]\n        gcam = gcam.view(B, C, H, W)\n\n        return gcam\n'"
rl_algorithms/common/helper_functions.py,9,"b'# -*- coding: utf-8 -*-\n""""""Common util functions for all algorithms.\n\n- Author: Curt Park\n- Contact: curt.park@medipixel.io\n""""""\n\nfrom collections import deque\nimport random\nfrom typing import Deque, List, Tuple\n\nimport gym\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n\ndef identity(x: torch.Tensor) -> torch.Tensor:\n    """"""Return input without any change.""""""\n    return x\n\n\ndef soft_update(local: nn.Module, target: nn.Module, tau: float):\n    """"""Soft-update: target = tau*local + (1-tau)*target.""""""\n    for t_param, l_param in zip(target.parameters(), local.parameters()):\n        t_param.data.copy_(tau * l_param.data + (1.0 - tau) * t_param.data)\n\n\ndef hard_update(local: nn.Module, target: nn.Module):\n    """"""Hard update: target <- local.""""""\n    target.load_state_dict(local.state_dict())\n\n\ndef set_random_seed(seed: int, env: gym.Env):\n    """"""Set random seed""""""\n    env.seed(seed)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\n\ndef make_one_hot(labels: torch.Tensor, c: int):\n    """"""Converts an integer label to a one-hot Variable.""""""\n    y = torch.eye(c).to(device)\n    labels = labels.type(torch.LongTensor)\n    return y[labels]\n\n\ndef get_n_step_info_from_demo(\n    demo: List, n_step: int, gamma: float\n) -> Tuple[List, List]:\n    """"""Return 1 step and n step demos.""""""\n    assert n_step > 1\n\n    demos_1_step = list()\n    demos_n_step = list()\n    n_step_buffer: Deque = deque(maxlen=n_step)\n\n    for transition in demo:\n        n_step_buffer.append(transition)\n\n        if len(n_step_buffer) == n_step:\n            # add a single step transition\n            demos_1_step.append(n_step_buffer[0])\n\n            # add a multi step transition\n            curr_state, action = n_step_buffer[0][:2]\n            reward, next_state, done = get_n_step_info(n_step_buffer, gamma)\n            transition = (curr_state, action, reward, next_state, done)\n            demos_n_step.append(transition)\n\n    return demos_1_step, demos_n_step\n\n\ndef get_n_step_info(\n    n_step_buffer: Deque, gamma: float\n) -> Tuple[np.int64, np.ndarray, bool]:\n    """"""Return n step reward, next state, and done.""""""\n    # info of the last transition\n    reward, next_state, done = n_step_buffer[-1][-3:]\n\n    for transition in reversed(list(n_step_buffer)[:-1]):\n        r, n_s, d = transition[-3:]\n\n        reward = r + gamma * reward * (1 - d)\n        next_state, done = (n_s, d) if d else (next_state, done)\n\n    return reward, next_state, done\n\n\ndef numpy2floattensor(arrays: Tuple[np.ndarray]) -> Tuple[np.ndarray]:\n    """"""Convert numpy arrays to torch float tensor.""""""\n    tensors = []\n    for array in arrays:\n        tensor = torch.FloatTensor(array).to(device)\n        if torch.cuda.is_available():\n            tensor = tensor.cuda(non_blocking=True)\n        tensors.append(tensor)\n\n    return tuple(tensors)\n'"
rl_algorithms/common/noise.py,0,"b'# -*- coding: utf-8 -*-\n""""""Noise classes for algorithms.""""""\n\nimport copy\nimport random\n\nimport numpy as np\n\n\nclass GaussianNoise:\n    """"""Gaussian Noise.\n\n    Taken from https://github.com/vitchyr/rlkit\n    """"""\n\n    def __init__(\n        self,\n        action_dim: int,\n        min_sigma: float = 1.0,\n        max_sigma: float = 1.0,\n        decay_period: int = 1000000,\n    ):\n        """"""Initialize.""""""\n        self.action_dim = action_dim\n        self.max_sigma = max_sigma\n        self.min_sigma = min_sigma\n        self.decay_period = decay_period\n\n    def sample(self, t: int = 0) -> float:\n        """"""Get an action with gaussian noise.""""""\n        sigma = self.max_sigma - (self.max_sigma - self.min_sigma) * min(\n            1.0, t / self.decay_period\n        )\n        return np.random.normal(0, sigma, size=self.action_dim)\n\n\nclass OUNoise:\n    """"""Ornstein-Uhlenbeck process.\n\n    Taken from Udacity deep-reinforcement-learning github repository:\n    https://github.com/udacity/deep-reinforcement-learning/blob/master/\n    ddpg-pendulum/ddpg_agent.py\n    """"""\n\n    def __init__(\n        self, size: int, mu: float = 0.0, theta: float = 0.15, sigma: float = 0.2\n    ):\n        """"""Initialize parameters and noise process.""""""\n        self.state = np.float64(0.0)\n        self.mu = mu * np.ones(size)\n        self.theta = theta\n        self.sigma = sigma\n        self.reset()\n\n    def reset(self):\n        """"""Reset the internal state (= noise) to mean (mu).""""""\n        self.state = copy.copy(self.mu)\n\n    def sample(self) -> float:\n        """"""Update internal state and return it as a noise sample.""""""\n        x = self.state\n        dx = self.theta * (self.mu - x) + self.sigma * np.array(\n            [random.random() for _ in range(len(x))]\n        )\n        self.state = x + dx\n        return self.state\n'"
rl_algorithms/ddpg/__init__.py,0,"b'""""""Empty.""""""\n'"
rl_algorithms/ddpg/agent.py,3,"b'# -*- coding: utf-8 -*-\n""""""DDPG agent for episodic tasks in OpenAI Gym.\n\n- Author: Curt Park\n- Contact: curt.park@medipixel.io\n- Paper: https://arxiv.org/pdf/1509.02971.pdf\n""""""\n\nimport argparse\nimport time\nfrom typing import Tuple\n\nimport gym\nimport numpy as np\nimport torch\nimport wandb\n\nfrom rl_algorithms.common.abstract.agent import Agent\nfrom rl_algorithms.common.buffer.replay_buffer import ReplayBuffer\nfrom rl_algorithms.common.helper_functions import numpy2floattensor\nfrom rl_algorithms.common.noise import OUNoise\nfrom rl_algorithms.ddpg.learner import DDPGLearner\nfrom rl_algorithms.registry import AGENTS\nfrom rl_algorithms.utils.config import ConfigDict\n\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n\n@AGENTS.register_module\nclass DDPGAgent(Agent):\n    """"""ActorCritic interacting with environment.\n\n    Attributes:\n        env (gym.Env): openAI Gym environment\n        args (argparse.Namespace): arguments including hyperparameters and training settings\n        hyper_params (ConfigDict): hyper-parameters\n        log_cfg (ConfigDict): configuration for saving log and checkpoint\n        network_cfg (ConfigDict): config of network for training agent\n        optim_cfg (ConfigDict): config of optimizer\n        state_dim (int): state size of env\n        action_dim (int): action size of env\n        memory (ReplayBuffer): replay memory\n        noise (OUNoise): random noise for exploration\n        curr_state (np.ndarray): temporary storage of the current state\n        total_step (int): total step numbers\n        episode_step (int): step number of the current episode\n        i_episode (int): current episode number\n\n    """"""\n\n    def __init__(\n        self,\n        env: gym.Env,\n        args: argparse.Namespace,\n        log_cfg: ConfigDict,\n        hyper_params: ConfigDict,\n        backbone: ConfigDict,\n        head: ConfigDict,\n        optim_cfg: ConfigDict,\n        noise_cfg: ConfigDict,\n    ):\n        """"""Initialize.""""""\n        Agent.__init__(self, env, args, log_cfg)\n\n        self.curr_state = np.zeros((1,))\n        self.total_step = 0\n        self.episode_step = 0\n        self.i_episode = 0\n\n        self.hyper_params = hyper_params\n        self.backbone_cfg = backbone\n        self.head_cfg = head\n        self.optim_cfg = optim_cfg\n\n        self.state_dim = self.env.observation_space.shape\n        self.action_dim = self.env.action_space.shape[0]\n\n        self.head_cfg.actor.configs.state_size = self.state_dim\n\n        # ddpg critic gets state & action as input,\n        # and make the type to tuple to conform the gym action_space type.\n        self.head_cfg.critic.configs.state_size = (self.state_dim[0] + self.action_dim,)\n        self.head_cfg.actor.configs.output_size = self.action_dim\n\n        # set noise\n        self.noise = OUNoise(\n            self.action_dim,\n            theta=noise_cfg.ou_noise_theta,\n            sigma=noise_cfg.ou_noise_sigma,\n        )\n\n        self._initialize()\n\n    # pylint: disable=attribute-defined-outside-init\n    def _initialize(self):\n        """"""Initialize non-common things.""""""\n        if not self.args.test:\n            # replay memory\n            self.memory = ReplayBuffer(\n                self.hyper_params.buffer_size, self.hyper_params.batch_size\n            )\n\n        self.learner = DDPGLearner(\n            self.args,\n            self.hyper_params,\n            self.log_cfg,\n            self.head_cfg,\n            self.backbone_cfg,\n            self.optim_cfg,\n            device,\n        )\n\n    def select_action(self, state: np.ndarray) -> np.ndarray:\n        """"""Select an action from the input space.""""""\n        self.curr_state = state\n        state = self._preprocess_state(state)\n\n        # if initial random action should be conducted\n        if (\n            self.total_step < self.hyper_params.initial_random_action\n            and not self.args.test\n        ):\n            return np.array(self.env.action_space.sample())\n\n        selected_action = self.learner.actor(state).detach().cpu().numpy()\n\n        if not self.args.test:\n            noise = self.noise.sample()\n            selected_action = np.clip(selected_action + noise, -1.0, 1.0)\n\n        return selected_action\n\n    # pylint: disable=no-self-use\n    def _preprocess_state(self, state: np.ndarray) -> torch.Tensor:\n        """"""Preprocess state so that actor selects an action.""""""\n        state = torch.FloatTensor(state).to(device)\n        return state\n\n    def step(self, action: np.ndarray) -> Tuple[np.ndarray, np.float64, bool, dict]:\n        """"""Take an action and return the response of the env.""""""\n        next_state, reward, done, info = self.env.step(action)\n\n        if not self.args.test:\n            # if the last state is not a terminal state, store done as false\n            done_bool = (\n                False if self.episode_step == self.args.max_episode_steps else done\n            )\n            transition = (self.curr_state, action, reward, next_state, done_bool)\n            self._add_transition_to_memory(transition)\n\n        return next_state, reward, done, info\n\n    def _add_transition_to_memory(self, transition: Tuple[np.ndarray, ...]):\n        """"""Add 1 step and n step transitions to memory.""""""\n        self.memory.add(transition)\n\n    def write_log(self, log_value: tuple):\n        """"""Write log about loss and score""""""\n        i, loss, score, avg_time_cost = log_value\n        total_loss = loss.sum()\n\n        print(\n            ""[INFO] episode %d, episode step: %d, total step: %d, total score: %d\\n""\n            ""total loss: %f actor_loss: %.3f critic_loss: %.3f (spent %.6f sec/step)\\n""\n            % (\n                i,\n                self.episode_step,\n                self.total_step,\n                score,\n                total_loss,\n                loss[0],\n                loss[1],\n                avg_time_cost,\n            )  # actor loss  # critic loss\n        )\n\n        if self.args.log:\n            wandb.log(\n                {\n                    ""score"": score,\n                    ""total loss"": total_loss,\n                    ""actor loss"": loss[0],\n                    ""critic loss"": loss[1],\n                    ""time per each step"": avg_time_cost,\n                }\n            )\n\n    # pylint: disable=no-self-use, unnecessary-pass\n    def pretrain(self):\n        """"""Pretraining steps.""""""\n        pass\n\n    def train(self):\n        """"""Train the agent.""""""\n        # logger\n        if self.args.log:\n            self.set_wandb()\n            # wandb.watch([self.actor, self.critic], log=""parameters"")\n\n        # pre-training if needed\n        self.pretrain()\n\n        for self.i_episode in range(1, self.args.episode_num + 1):\n            state = self.env.reset()\n            done = False\n            score = 0\n            self.episode_step = 0\n            losses = list()\n\n            t_begin = time.time()\n\n            while not done:\n                if self.args.render and self.i_episode >= self.args.render_after:\n                    self.env.render()\n\n                action = self.select_action(state)\n                next_state, reward, done, _ = self.step(action)\n                self.total_step += 1\n                self.episode_step += 1\n\n                if len(self.memory) >= self.hyper_params.batch_size:\n                    for _ in range(self.hyper_params.multiple_update):\n                        experience = self.memory.sample()\n                        experience = numpy2floattensor(experience)\n                        loss = self.learner.update_model(experience)\n                        losses.append(loss)  # for logging\n\n                state = next_state\n                score += reward\n\n            t_end = time.time()\n            avg_time_cost = (t_end - t_begin) / self.episode_step\n\n            # logging\n            if losses:\n                avg_loss = np.vstack(losses).mean(axis=0)\n                log_value = (self.i_episode, avg_loss, score, avg_time_cost)\n                self.write_log(log_value)\n                losses.clear()\n\n            if self.i_episode % self.args.save_period == 0:\n                self.learner.save_params(self.i_episode)\n                self.interim_test()\n\n        # termination\n        self.env.close()\n        self.learner.save_params(self.i_episode)\n        self.interim_test()\n'"
rl_algorithms/ddpg/learner.py,10,"b'import argparse\nfrom typing import Tuple\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.utils import clip_grad_norm_\nimport torch.optim as optim\n\nfrom rl_algorithms.common.abstract.learner import Learner\nimport rl_algorithms.common.helper_functions as common_utils\nfrom rl_algorithms.common.networks.brain import Brain\nfrom rl_algorithms.utils.config import ConfigDict\n\n\nclass DDPGLearner(Learner):\n    """"""Learner for DDPG Agent\n\n    Attributes:\n        args (argparse.Namespace): arguments including hyperparameters and training settings\n        hyper_params (ConfigDict): hyper-parameters\n        optim_cfg (ConfigDict): config of optimizer\n        log_cfg (ConfigDict): configuration for saving log and checkpoint\n        actor (nn.Module): actor model to select actions\n        actor_target (nn.Module): target actor model to select actions\n        critic (nn.Module): critic model to predict state values\n        critic_target (nn.Module): target critic model to predict state values\n        actor_optim (Optimizer): optimizer for training actor\n        critic_optim (Optimizer): optimizer for training critic\n\n    """"""\n\n    def __init__(\n        self,\n        args: argparse.Namespace,\n        hyper_params: ConfigDict,\n        log_cfg: ConfigDict,\n        head_cfg: ConfigDict,\n        backbone_cfg: ConfigDict,\n        optim_cfg: ConfigDict,\n        device: torch.device,\n    ):\n        Learner.__init__(self, args, hyper_params, log_cfg, device)\n\n        self.head_cfg = head_cfg\n        self.backbone_cfg = backbone_cfg\n        self.optim_cfg = optim_cfg\n\n        self._init_network()\n\n    def _init_network(self):\n        """"""Initialize networks and optimizers.""""""\n        # create actor\n        self.actor = Brain(self.backbone_cfg.actor, self.head_cfg.actor).to(self.device)\n        self.actor_target = Brain(self.backbone_cfg.actor, self.head_cfg.actor).to(\n            self.device\n        )\n        self.actor_target.load_state_dict(self.actor.state_dict())\n\n        # create critic\n        self.critic = Brain(self.backbone_cfg.critic, self.head_cfg.critic).to(\n            self.device\n        )\n        self.critic_target = Brain(self.backbone_cfg.critic, self.head_cfg.critic).to(\n            self.device\n        )\n        self.critic_target.load_state_dict(self.critic.state_dict())\n\n        # create optimizer\n        self.actor_optim = optim.Adam(\n            self.actor.parameters(),\n            lr=self.optim_cfg.lr_actor,\n            weight_decay=self.optim_cfg.weight_decay,\n        )\n\n        self.critic_optim = optim.Adam(\n            self.critic.parameters(),\n            lr=self.optim_cfg.lr_critic,\n            weight_decay=self.optim_cfg.weight_decay,\n        )\n\n        # load the optimizer and model parameters\n        if self.args.load_from is not None:\n            self.load_params(self.args.load_from)\n\n    def update_model(\n        self, experience: Tuple[torch.Tensor, ...]\n    ) -> Tuple[torch.Tensor, ...]:\n        """"""Update ddpg actor and critic networks""""""\n        states, actions, rewards, next_states, dones = experience\n\n        # G_t   = r + gamma * v(s_{t+1})  if state != Terminal\n        #       = r                       otherwise\n        masks = 1 - dones\n        next_actions = self.actor_target(next_states)\n        next_values = self.critic_target(torch.cat((next_states, next_actions), dim=-1))\n        curr_returns = rewards + self.hyper_params.gamma * next_values * masks\n        curr_returns = curr_returns.to(self.device)\n\n        # train critic\n        gradient_clip_ac = self.hyper_params.gradient_clip_ac\n        gradient_clip_cr = self.hyper_params.gradient_clip_cr\n\n        values = self.critic(torch.cat((states, actions), dim=-1))\n        critic_loss = F.mse_loss(values, curr_returns)\n        self.critic_optim.zero_grad()\n        critic_loss.backward()\n        clip_grad_norm_(self.critic.parameters(), gradient_clip_cr)\n        self.critic_optim.step()\n\n        # train actor\n        actions = self.actor(states)\n        actor_loss = -self.critic(torch.cat((states, actions), dim=-1)).mean()\n        self.actor_optim.zero_grad()\n        actor_loss.backward()\n        clip_grad_norm_(self.actor.parameters(), gradient_clip_ac)\n        self.actor_optim.step()\n\n        # update target networks\n        common_utils.soft_update(self.actor, self.actor_target, self.hyper_params.tau)\n        common_utils.soft_update(self.critic, self.critic_target, self.hyper_params.tau)\n\n        return actor_loss.item(), critic_loss.item()\n\n    def save_params(self, n_episode: int):\n        """"""Save model and optimizer parameters.""""""\n        params = {\n            ""actor_state_dict"": self.actor.state_dict(),\n            ""actor_target_state_dict"": self.actor_target.state_dict(),\n            ""critic_state_dict"": self.critic.state_dict(),\n            ""critic_target_state_dict"": self.critic_target.state_dict(),\n            ""actor_optim_state_dict"": self.actor_optim.state_dict(),\n            ""critic_optim_state_dict"": self.critic_optim.state_dict(),\n        }\n        Learner._save_params(self, params, n_episode)\n\n    def load_params(self, path: str):\n        """"""Load model and optimizer parameters.""""""\n        Learner.load_params(self, path)\n\n        params = torch.load(path)\n        self.actor.load_state_dict(params[""actor_state_dict""])\n        self.actor_target.load_state_dict(params[""actor_target_state_dict""])\n        self.critic.load_state_dict(params[""critic_state_dict""])\n        self.critic_target.load_state_dict(params[""critic_target_state_dict""])\n        self.actor_optim.load_state_dict(params[""actor_optim_state_dict""])\n        self.critic_optim.load_state_dict(params[""critic_optim_state_dict""])\n        print(""[INFO] loaded the model and optimizer from"", path)\n'"
rl_algorithms/dqn/__init__.py,0,"b'""""""Empty.""""""\n'"
rl_algorithms/dqn/agent.py,4,"b'# -*- coding: utf-8 -*-\n""""""DQN agent for episodic tasks in OpenAI Gym.\n\n- Author: Kyunghwan Kim\n- Contact: kh.kim@medipixel.io\n- Paper: https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf (DQN)\n         https://arxiv.org/pdf/1509.06461.pdf (Double DQN)\n         https://arxiv.org/pdf/1511.05952.pdf (PER)\n         https://arxiv.org/pdf/1511.06581.pdf (Dueling)\n         https://arxiv.org/pdf/1706.10295.pdf (NoisyNet)\n         https://arxiv.org/pdf/1707.06887.pdf (C51)\n         https://arxiv.org/pdf/1710.02298.pdf (Rainbow)\n         https://arxiv.org/pdf/1806.06923.pdf (IQN)\n""""""\n\nimport argparse\nimport time\nfrom typing import Tuple\n\nimport gym\nimport numpy as np\nimport torch\nimport wandb\n\nfrom rl_algorithms.common.abstract.agent import Agent\nfrom rl_algorithms.common.buffer.priortized_replay_buffer import PrioritizedReplayBuffer\nfrom rl_algorithms.common.buffer.replay_buffer import ReplayBuffer\nfrom rl_algorithms.common.helper_functions import numpy2floattensor\nfrom rl_algorithms.dqn.learner import DQNLearner\nfrom rl_algorithms.registry import AGENTS\nfrom rl_algorithms.utils.config import ConfigDict\n\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n\n@AGENTS.register_module\nclass DQNAgent(Agent):\n    """"""DQN interacting with environment.\n\n    Attribute:\n        env (gym.Env): openAI Gym environment\n        args (argparse.Namespace): arguments including hyperparameters and training settings\n        hyper_params (ConfigDict): hyper-parameters\n        log_cfg (ConfigDict): configuration for saving log and checkpoint\n        network_cfg (ConfigDict): config of network for training agent\n        optim_cfg (ConfigDict): config of optimizer\n        state_dim (int): state size of env\n        action_dim (int): action size of env\n        memory (PrioritizedReplayBuffer): replay memory\n        curr_state (np.ndarray): temporary storage of the current state\n        total_step (int): total step number\n        episode_step (int): step number of the current episode\n        i_episode (int): current episode number\n        epsilon (float): parameter for epsilon greedy policy\n        n_step_buffer (deque): n-size buffer to calculate n-step returns\n        per_beta (float): beta parameter for prioritized replay buffer\n        use_conv (bool): whether or not to use convolution layer\n        use_n_step (bool): whether or not to use n-step returns\n\n    """"""\n\n    def __init__(\n        self,\n        env: gym.Env,\n        args: argparse.Namespace,\n        log_cfg: ConfigDict,\n        hyper_params: ConfigDict,\n        backbone: ConfigDict,\n        head: ConfigDict,\n        optim_cfg: ConfigDict,\n    ):\n        """"""Initialize.""""""\n        Agent.__init__(self, env, args, log_cfg)\n\n        self.curr_state = np.zeros(1)\n        self.episode_step = 0\n        self.total_step = 0\n        self.i_episode = 0\n\n        self.hyper_params = hyper_params\n        self.optim_cfg = optim_cfg\n        self.backbone_cfg = backbone\n        self.head_cfg = head\n\n        self.head_cfg.configs.state_size = self.env.observation_space.shape\n        self.head_cfg.configs.output_size = self.env.action_space.n\n\n        self.per_beta = hyper_params.per_beta\n        self.use_conv = len(self.head_cfg.configs.state_size) > 1\n        self.use_n_step = hyper_params.n_step > 1\n\n        if head.configs.use_noisy_net:\n            self.max_epsilon = 0.0\n            self.min_epsilon = 0.0\n            self.epsilon = 0.0\n        else:\n            self.max_epsilon = hyper_params.max_epsilon\n            self.min_epsilon = hyper_params.min_epsilon\n            self.epsilon = hyper_params.max_epsilon\n\n        self._initialize()\n\n    # pylint: disable=attribute-defined-outside-init\n    def _initialize(self):\n        """"""Initialize non-common things.""""""\n        if not self.args.test:\n            # replay memory for a single step\n            self.memory = PrioritizedReplayBuffer(\n                self.hyper_params.buffer_size,\n                self.hyper_params.batch_size,\n                alpha=self.hyper_params.per_alpha,\n            )\n\n            # replay memory for multi-steps\n            if self.use_n_step:\n                self.memory_n = ReplayBuffer(\n                    self.hyper_params.buffer_size,\n                    self.hyper_params.batch_size,\n                    n_step=self.hyper_params.n_step,\n                    gamma=self.hyper_params.gamma,\n                )\n\n        self.learner = DQNLearner(\n            self.args,\n            self.hyper_params,\n            self.log_cfg,\n            self.head_cfg,\n            self.backbone_cfg,\n            self.optim_cfg,\n            device,\n        )\n\n    def select_action(self, state: np.ndarray) -> np.ndarray:\n        """"""Select an action from the input space.""""""\n        self.curr_state = state\n\n        # epsilon greedy policy\n        # pylint: disable=comparison-with-callable\n        if not self.args.test and self.epsilon > np.random.random():\n            selected_action = np.array(self.env.action_space.sample())\n        else:\n            state = self._preprocess_state(state)\n            selected_action = self.learner.dqn(state).argmax()\n            selected_action = selected_action.detach().cpu().numpy()\n        return selected_action\n\n    # pylint: disable=no-self-use\n    def _preprocess_state(self, state: np.ndarray) -> torch.Tensor:\n        """"""Preprocess state so that actor selects an action.""""""\n        state = torch.FloatTensor(state).to(device)\n        return state\n\n    def step(self, action: np.ndarray) -> Tuple[np.ndarray, np.float64, bool, dict]:\n        """"""Take an action and return the response of the env.""""""\n        next_state, reward, done, info = self.env.step(action)\n\n        if not self.args.test:\n            # if the last state is not a terminal state, store done as false\n            done_bool = (\n                False if self.episode_step == self.args.max_episode_steps else done\n            )\n\n            transition = (self.curr_state, action, reward, next_state, done_bool)\n            self._add_transition_to_memory(transition)\n\n        return next_state, reward, done, info\n\n    def _add_transition_to_memory(self, transition: Tuple[np.ndarray, ...]):\n        """"""Add 1 step and n step transitions to memory.""""""\n        # add n-step transition\n        if self.use_n_step:\n            transition = self.memory_n.add(transition)\n\n        # add a single step transition\n        # if transition is not an empty tuple\n        if transition:\n            self.memory.add(transition)\n\n    def write_log(self, log_value: tuple):\n        """"""Write log about loss and score""""""\n        i, loss, score, avg_time_cost = log_value\n        print(\n            ""[INFO] episode %d, episode step: %d, total step: %d, total score: %f\\n""\n            ""epsilon: %f, loss: %f, avg q-value: %f (spent %.6f sec/step)\\n""\n            % (\n                i,\n                self.episode_step,\n                self.total_step,\n                score,\n                self.epsilon,\n                loss[0],\n                loss[1],\n                avg_time_cost,\n            )\n        )\n\n        if self.args.log:\n            wandb.log(\n                {\n                    ""score"": score,\n                    ""epsilon"": self.epsilon,\n                    ""dqn loss"": loss[0],\n                    ""avg q values"": loss[1],\n                    ""time per each step"": avg_time_cost,\n                }\n            )\n\n    # pylint: disable=no-self-use, unnecessary-pass\n    def pretrain(self):\n        """"""Pretraining steps.""""""\n        pass\n\n    def sample_experience(self) -> Tuple[torch.Tensor, ...]:\n        experience_1 = self.memory.sample(self.per_beta)\n        if self.use_n_step:\n            indices = experience_1[-2]\n            experience_n = self.memory_n.sample(indices)\n            return numpy2floattensor(experience_1), numpy2floattensor(experience_n)\n\n        return numpy2floattensor(experience_1)\n\n    def train(self):\n        """"""Train the agent.""""""\n        # logger\n        if self.args.log:\n            self.set_wandb()\n            # wandb.watch([self.dqn], log=""parameters"")\n\n        # pre-training if needed\n        self.pretrain()\n\n        for self.i_episode in range(1, self.args.episode_num + 1):\n            state = self.env.reset()\n            self.episode_step = 0\n            losses = list()\n            done = False\n            score = 0\n\n            t_begin = time.time()\n\n            while not done:\n                if self.args.render and self.i_episode >= self.args.render_after:\n                    self.env.render()\n\n                action = self.select_action(state)\n                next_state, reward, done, _ = self.step(action)\n                self.total_step += 1\n                self.episode_step += 1\n\n                if len(self.memory) >= self.hyper_params.update_starts_from:\n                    if self.total_step % self.hyper_params.train_freq == 0:\n                        for _ in range(self.hyper_params.multiple_update):\n                            experience = self.sample_experience()\n                            info = self.learner.update_model(experience)\n                            loss = info[0:2]\n                            indices, new_priorities = info[2:4]\n                            losses.append(loss)  # for logging\n                            self.memory.update_priorities(indices, new_priorities)\n\n                    # decrease epsilon\n                    self.epsilon = max(\n                        self.epsilon\n                        - (self.max_epsilon - self.min_epsilon)\n                        * self.hyper_params.epsilon_decay,\n                        self.min_epsilon,\n                    )\n\n                    # increase priority beta\n                    fraction = min(float(self.i_episode) / self.args.episode_num, 1.0)\n                    self.per_beta = self.per_beta + fraction * (1.0 - self.per_beta)\n\n                state = next_state\n                score += reward\n\n            t_end = time.time()\n            avg_time_cost = (t_end - t_begin) / self.episode_step\n\n            if losses:\n                avg_loss = np.vstack(losses).mean(axis=0)\n                log_value = (self.i_episode, avg_loss, score, avg_time_cost)\n                self.write_log(log_value)\n\n            if self.i_episode % self.args.save_period == 0:\n                self.learner.save_params(self.i_episode)\n                self.interim_test()\n\n        # termination\n        self.env.close()\n        self.learner.save_params(self.i_episode)\n        self.interim_test()\n'"
rl_algorithms/dqn/learner.py,8,"b'import argparse\nfrom typing import Tuple, Union\n\nimport numpy as np\nimport torch\nfrom torch.nn.utils import clip_grad_norm_\nimport torch.optim as optim\n\nfrom rl_algorithms.common.abstract.learner import Learner, TensorTuple\nimport rl_algorithms.common.helper_functions as common_utils\nfrom rl_algorithms.common.networks.brain import Brain\nfrom rl_algorithms.registry import build_loss\nfrom rl_algorithms.utils.config import ConfigDict\n\n\nclass DQNLearner(Learner):\n    """"""Learner for DQN Agent\n\n    Attributes:\n        args (argparse.Namespace): arguments including hyperparameters and training settings\n        hyper_params (ConfigDict): hyper-parameters\n        log_cfg (ConfigDict): configuration for saving log and checkpoint\n        dqn (nn.Module): dqn model to predict state Q values\n        dqn_target (nn.Module): target dqn model to predict state Q values\n        dqn_optim (Optimizer): optimizer for training dqn\n\n    """"""\n\n    def __init__(\n        self,\n        args: argparse.Namespace,\n        hyper_params: ConfigDict,\n        log_cfg: ConfigDict,\n        head_cfg: ConfigDict,\n        backbone_cfg: ConfigDict,\n        optim_cfg: ConfigDict,\n        device: torch.device,\n    ):\n        Learner.__init__(self, args, hyper_params, log_cfg, device)\n\n        self.head_cfg = head_cfg\n        self.backbone_cfg = backbone_cfg\n        self.optim_cfg = optim_cfg\n        self.use_n_step = hyper_params.n_step > 1\n\n        self._init_network()\n\n    # pylint: disable=attribute-defined-outside-init\n    def _init_network(self):\n        """"""Initialize networks and optimizers.""""""\n        self.dqn = Brain(self.backbone_cfg, self.head_cfg).to(self.device)\n        self.dqn_target = Brain(self.backbone_cfg, self.head_cfg).to(self.device)\n        self.loss_fn = build_loss(self.hyper_params.loss_type)\n\n        self.dqn_target.load_state_dict(self.dqn.state_dict())\n\n        # create optimizer\n        self.dqn_optim = optim.Adam(\n            self.dqn.parameters(),\n            lr=self.optim_cfg.lr_dqn,\n            weight_decay=self.optim_cfg.weight_decay,\n            eps=self.optim_cfg.adam_eps,\n        )\n\n        # load the optimizer and model parameters\n        if self.args.load_from is not None:\n            self.load_params(self.args.load_from)\n\n    def update_model(\n        self, experience: Union[TensorTuple, Tuple[TensorTuple]]\n    ) -> Tuple[torch.Tensor, torch.Tensor, list, np.ndarray]:  # type: ignore\n        """"""Update dqn and dqn target""""""\n\n        if self.use_n_step:\n            experience_1, experience_n = experience\n        else:\n            experience_1 = experience\n\n        weights, indices = experience_1[-3:-1]\n\n        gamma = self.hyper_params.gamma\n\n        dq_loss_element_wise, q_values = self.loss_fn(\n            self.dqn, self.dqn_target, experience_1, gamma, self.head_cfg\n        )\n        dq_loss = torch.mean(dq_loss_element_wise * weights)\n\n        # n step loss\n        if self.use_n_step:\n            gamma = self.hyper_params.gamma ** self.hyper_params.n_step\n\n            dq_loss_n_element_wise, q_values_n = self.loss_fn(\n                self.dqn, self.dqn_target, experience_n, gamma, self.head_cfg\n            )\n\n            # to update loss and priorities\n            q_values = 0.5 * (q_values + q_values_n)\n            dq_loss_element_wise += dq_loss_n_element_wise * self.hyper_params.w_n_step\n            dq_loss = torch.mean(dq_loss_element_wise * weights)\n\n        # q_value regularization\n        q_regular = torch.norm(q_values, 2).mean() * self.hyper_params.w_q_reg\n\n        # total loss\n        loss = dq_loss + q_regular\n\n        self.dqn_optim.zero_grad()\n        loss.backward()\n        clip_grad_norm_(self.dqn.parameters(), self.hyper_params.gradient_clip)\n        self.dqn_optim.step()\n\n        # update target networks\n        common_utils.soft_update(self.dqn, self.dqn_target, self.hyper_params.tau)\n\n        # update priorities in PER\n        loss_for_prior = dq_loss_element_wise.detach().cpu().numpy()\n        new_priorities = loss_for_prior + self.hyper_params.per_eps\n\n        if self.head_cfg.configs.use_noisy_net:\n            self.dqn.head.reset_noise()\n            self.dqn_target.head.reset_noise()\n\n        return (\n            loss.item(),\n            q_values.mean().item(),\n            indices.long().cpu().numpy(),\n            new_priorities,\n        )\n\n    def save_params(self, n_episode: int):\n        """"""Save model and optimizer parameters.""""""\n        params = {\n            ""dqn_state_dict"": self.dqn.state_dict(),\n            ""dqn_target_state_dict"": self.dqn_target.state_dict(),\n            ""dqn_optim_state_dict"": self.dqn_optim.state_dict(),\n        }\n\n        Learner._save_params(self, params, n_episode)\n\n    # pylint: disable=attribute-defined-outside-init\n    def load_params(self, path: str):\n        """"""Load model and optimizer parameters.""""""\n        Learner.load_params(self, path)\n\n        params = torch.load(path)\n        self.dqn.load_state_dict(params[""dqn_state_dict""])\n        self.dqn_target.load_state_dict(params[""dqn_target_state_dict""])\n        self.dqn_optim.load_state_dict(params[""dqn_optim_state_dict""])\n        print(""[INFO] loaded the model and optimizer from"", path)\n'"
rl_algorithms/dqn/linear.py,12,"b'# -*- coding: utf-8 -*-\n""""""Linear module for dqn algorithms\n\n- Author: Kyunghwan Kim\n- Contact: kh.kim@medipixel.io\n""""""\n\nimport math\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n\nclass NoisyLinear(nn.Module):\n    """"""Noisy linear module for NoisyNet.\n\n    References:\n        https://github.com/higgsfield/RL-Adventure/blob/master/5.noisy%20dqn.ipynb\n        https://github.com/Kaixhin/Rainbow/blob/master/model.py\n\n    Attributes:\n        in_features (int): input size of linear module\n        out_features (int): output size of linear module\n        std_init (float): initial std value\n        weight_mu (nn.Parameter): mean value weight parameter\n        weight_sigma (nn.Parameter): std value weight parameter\n        bias_mu (nn.Parameter): mean value bias parameter\n        bias_sigma (nn.Parameter): std value bias parameter\n\n    """"""\n\n    def __init__(self, in_features: int, out_features: int, std_init: float = 0.5):\n        """"""Initialize.""""""\n        super(NoisyLinear, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.std_init = std_init\n\n        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.weight_sigma = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.register_buffer(""weight_epsilon"", torch.Tensor(out_features, in_features))\n\n        self.bias_mu = nn.Parameter(torch.Tensor(out_features))\n        self.bias_sigma = nn.Parameter(torch.Tensor(out_features))\n        self.register_buffer(""bias_epsilon"", torch.Tensor(out_features))\n\n        self.reset_parameters()\n        self.reset_noise()\n\n    def reset_parameters(self):\n        """"""Reset trainable network parameters (factorized gaussian noise).""""""\n        mu_range = 1 / math.sqrt(self.in_features)\n        self.weight_mu.data.uniform_(-mu_range, mu_range)\n        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.in_features))\n        self.bias_mu.data.uniform_(-mu_range, mu_range)\n        self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.out_features))\n\n    @staticmethod\n    def scale_noise(size: int) -> torch.Tensor:\n        """"""Set scale to make noise (factorized gaussian noise).""""""\n        x = torch.FloatTensor(np.random.normal(loc=0.0, scale=1.0, size=size))\n\n        return x.sign().mul(x.abs().sqrt())\n\n    def reset_noise(self):\n        """"""Make new noise.""""""\n        epsilon_in = self.scale_noise(self.in_features)\n        epsilon_out = self.scale_noise(self.out_features)\n\n        # outer product\n        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n        self.bias_epsilon.copy_(epsilon_out)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """"""Forward method implementation.\n\n        We don\'t use separate statements on train / eval mode.\n        It doesn\'t show remarkable difference of performance.\n        """"""\n        return F.linear(\n            x,\n            self.weight_mu + self.weight_sigma * self.weight_epsilon,\n            self.bias_mu + self.bias_sigma * self.bias_epsilon,\n        )\n\n\nclass NoisyLinearConstructor:\n    """"""Constructor class for changing hyper parameters of NoisyLinear.\n\n    Attributes:\n        std_init (float): initial std value\n\n    """"""\n\n    def __init__(self, std_init: float = 0.5):\n        """"""Initialize.""""""\n        self.std_init = std_init\n\n    def __call__(self, in_features: int, out_features: int) -> NoisyLinear:\n        """"""Return NoisyLinear instance set hyper parameters""""""\n        return NoisyLinear(in_features, out_features, self.std_init)\n\n\nclass NoisyMLPHandler:\n    """"""Includes methods to handle noisy linear.""""""\n\n    def reset_noise(self):\n        """"""Re-sample noise""""""\n        for _, module in self.named_children():\n            module.reset_noise()\n'"
rl_algorithms/dqn/losses.py,22,"b'# -*- coding: utf-8 -*-\n""""""Loss functions for DQN.\n\nThis module has DQN loss functions.\n\n- Author: Curt Park\n- Contact: curt.park@medipixel.io\n""""""\n\nfrom typing import Tuple\n\nimport torch\nimport torch.nn.functional as F\n\nfrom rl_algorithms.common.networks.brain import Brain\nfrom rl_algorithms.registry import LOSSES\nfrom rl_algorithms.utils.config import ConfigDict\n\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n\n@LOSSES.register_module\nclass IQNLoss:\n    def __call__(\n        self,\n        model: Brain,\n        target_model: Brain,\n        experiences: Tuple[torch.Tensor, ...],\n        gamma: float,\n        head_cfg: ConfigDict,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        """"""Return element-wise IQN loss and Q-values.\n\n        Reference: https://github.com/google/dopamine\n        """"""\n        states, actions, rewards, next_states, dones = experiences[:5]\n        batch_size = states.shape[0]\n\n        # size of rewards: (n_tau_prime_samples x batch_size) x 1.\n        rewards = rewards.repeat(head_cfg.configs.n_tau_prime_samples, 1)\n\n        # size of gamma_with_terminal: (n_tau_prime_samples x batch_size) x 1.\n        masks = 1 - dones\n        gamma_with_terminal = masks * gamma\n        gamma_with_terminal = gamma_with_terminal.repeat(\n            head_cfg.configs.n_tau_prime_samples, 1\n        )\n\n        # Get the indices of the maximium Q-value across the action dimension.\n        # Shape of replay_next_qt_argmax: (n_tau_prime_samples x batch_size) x 1.\n        next_actions = model(next_states).argmax(dim=1)  # double Q\n        next_actions = next_actions[:, None]\n        next_actions = next_actions.repeat(head_cfg.configs.n_tau_prime_samples, 1)\n\n        # Shape of next_target_values: (n_tau_prime_samples x batch_size) x 1.\n        target_quantile_values, _ = target_model.forward_(\n            next_states, head_cfg.configs.n_tau_prime_samples\n        )\n        target_quantile_values = target_quantile_values.gather(1, next_actions)\n        target_quantile_values = rewards + gamma_with_terminal * target_quantile_values\n        target_quantile_values = target_quantile_values.detach()\n\n        # Reshape to n_tau_prime_samples x batch_size x 1 since this is\n        # the manner in which the target_quantile_values are tiled.\n        target_quantile_values = target_quantile_values.view(\n            head_cfg.configs.n_tau_prime_samples, batch_size, 1\n        )\n\n        # Transpose dimensions so that the dimensionality is batch_size x\n        # n_tau_prime_samples x 1 to prepare for computation of Bellman errors.\n        target_quantile_values = torch.transpose(target_quantile_values, 0, 1)\n\n        # Get quantile values: (n_tau_samples x batch_size) x action_dim.\n        quantile_values, quantiles = model.forward_(\n            states, head_cfg.configs.n_tau_samples\n        )\n        reshaped_actions = actions[:, None].repeat(head_cfg.configs.n_tau_samples, 1)\n        chosen_action_quantile_values = quantile_values.gather(\n            1, reshaped_actions.long()\n        )\n        chosen_action_quantile_values = chosen_action_quantile_values.view(\n            head_cfg.configs.n_tau_samples, batch_size, 1\n        )\n\n        # Transpose dimensions so that the dimensionality is batch_size x\n        # n_tau_prime_samples x 1 to prepare for computation of Bellman errors.\n        chosen_action_quantile_values = torch.transpose(\n            chosen_action_quantile_values, 0, 1\n        )\n\n        # Shape of bellman_erors and huber_loss:\n        # batch_size x num_tau_prime_samples x num_tau_samples x 1.\n        bellman_errors = (\n            target_quantile_values[:, :, None, :]\n            - chosen_action_quantile_values[:, None, :, :]\n        )\n\n        # The huber loss (introduced in QR-DQN) is defined via two cases:\n        # case_one: |bellman_errors| <= kappa\n        # case_two: |bellman_errors| > kappa\n        huber_loss_case_one = (\n            (torch.abs(bellman_errors) <= head_cfg.configs.kappa).float()\n            * 0.5\n            * bellman_errors ** 2\n        )\n        huber_loss_case_two = (\n            (torch.abs(bellman_errors) > head_cfg.configs.kappa).float()\n            * head_cfg.configs.kappa\n            * (torch.abs(bellman_errors) - 0.5 * head_cfg.configs.kappa)\n        )\n        huber_loss = huber_loss_case_one + huber_loss_case_two\n\n        # Reshape quantiles to batch_size x num_tau_samples x 1\n        quantiles = quantiles.view(head_cfg.configs.n_tau_samples, batch_size, 1)\n        quantiles = torch.transpose(quantiles, 0, 1)\n\n        # Tile by num_tau_prime_samples along a new dimension. Shape is now\n        # batch_size x num_tau_prime_samples x num_tau_samples x 1.\n        # These quantiles will be used for computation of the quantile huber loss\n        # below (see section 2.3 of the paper).\n        quantiles = quantiles[:, None, :, :].repeat(\n            1, head_cfg.configs.n_tau_prime_samples, 1, 1\n        )\n        quantiles = quantiles.to(device)\n\n        # Shape: batch_size x n_tau_prime_samples x n_tau_samples x 1.\n        quantile_huber_loss = (\n            torch.abs(quantiles - (bellman_errors < 0).float().detach())\n            * huber_loss\n            / head_cfg.configs.kappa\n        )\n\n        # Sum over current quantile value (n_tau_samples) dimension,\n        # average over target quantile value (n_tau_prime_samples) dimension.\n        # Shape: batch_size x n_tau_prime_samples x 1.\n        loss = torch.sum(quantile_huber_loss, dim=2)\n\n        # Shape: batch_size x 1.\n        iqn_loss_element_wise = torch.mean(loss, dim=1)\n\n        # q values for regularization.\n        q_values = model(states)\n\n        return iqn_loss_element_wise, q_values\n\n\n@LOSSES.register_module\nclass C51Loss:\n    def __call__(\n        self,\n        model: Brain,\n        target_model: Brain,\n        experiences: Tuple[torch.Tensor, ...],\n        gamma: float,\n        head_cfg: ConfigDict,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        """"""Return element-wise C51 loss and Q-values.""""""\n        states, actions, rewards, next_states, dones = experiences[:5]\n        batch_size = states.shape[0]\n\n        support = torch.linspace(\n            head_cfg.configs.v_min, head_cfg.configs.v_max, head_cfg.configs.atom_size\n        ).to(device)\n        delta_z = float(head_cfg.configs.v_max - head_cfg.configs.v_min) / (\n            head_cfg.configs.atom_size - 1\n        )\n\n        with torch.no_grad():\n            # According to noisynet paper,\n            # it resamples noisynet parameters on online network when using double q\n            # but we don\'t because there is no remarkable difference in performance.\n            next_actions = model.forward_(next_states)[1].argmax(1)\n\n            next_dist = target_model.forward_(next_states)[0]\n            next_dist = next_dist[range(batch_size), next_actions]\n\n            t_z = rewards + (1 - dones) * gamma * support\n            t_z = t_z.clamp(min=head_cfg.configs.v_min, max=head_cfg.configs.v_max)\n            b = (t_z - head_cfg.configs.v_min) / delta_z\n            l = b.floor().long()  # noqa: E741\n            u = b.ceil().long()\n\n            offset = (\n                torch.linspace(\n                    0, (batch_size - 1) * head_cfg.configs.atom_size, batch_size\n                )\n                .long()\n                .unsqueeze(1)\n                .expand(batch_size, head_cfg.configs.atom_size)\n                .to(device)\n            )\n\n            proj_dist = torch.zeros(next_dist.size(), device=device)\n            proj_dist.view(-1).index_add_(\n                0, (l + offset).view(-1), (next_dist * (u.float() - b)).view(-1)\n            )\n            proj_dist.view(-1).index_add_(\n                0, (u + offset).view(-1), (next_dist * (b - l.float())).view(-1)\n            )\n\n        dist, q_values = model.forward_(states)\n        log_p = torch.log(dist[range(batch_size), actions.long()])\n\n        dq_loss_element_wise = -(proj_dist * log_p).sum(1)\n\n        return dq_loss_element_wise, q_values\n\n\n@LOSSES.register_module\nclass DQNLoss:\n    def __call__(\n        self,\n        model: Brain,\n        target_model: Brain,\n        experiences: Tuple[torch.Tensor, ...],\n        gamma: float,\n        head_cfg: ConfigDict = None,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        """"""Return element-wise dqn loss and Q-values.""""""\n        states, actions, rewards, next_states, dones = experiences[:5]\n\n        q_values = model(states)\n        # According to noisynet paper,\n        # it resamples noisynet parameters on online network when using double q\n        # but we don\'t because there is no remarkable difference in performance.\n        next_q_values = model(next_states)\n\n        next_target_q_values = target_model(next_states)\n\n        curr_q_value = q_values.gather(1, actions.long().unsqueeze(1))\n        next_q_value = next_target_q_values.gather(  # Double DQN\n            1, next_q_values.argmax(1).unsqueeze(1)\n        )\n\n        # G_t   = r + gamma * v(s_{t+1})  if state != Terminal\n        #       = r                       otherwise\n        masks = 1 - dones\n        target = rewards + gamma * next_q_value * masks\n        target = target.to(device)\n\n        # calculate dq loss\n        dq_loss_element_wise = F.smooth_l1_loss(\n            curr_q_value, target.detach(), reduction=""none""\n        )\n\n        return dq_loss_element_wise, q_values\n'"
rl_algorithms/dqn/networks.py,17,"b'# -*- coding: utf-8 -*-\n""""""MLP module for dqn algorithms\n\n- Author: Kyunghwan Kim\n- Contact: kh.kim@medipixel.io\n""""""\n\nimport math\nfrom typing import Callable, Tuple\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom rl_algorithms.common.helper_functions import identity\nfrom rl_algorithms.common.networks.heads import MLP, init_layer_uniform\nfrom rl_algorithms.dqn.linear import NoisyLinearConstructor, NoisyMLPHandler\nfrom rl_algorithms.registry import HEADS\nfrom rl_algorithms.utils.config import ConfigDict\n\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n\n@HEADS.register_module\nclass DuelingMLP(MLP, NoisyMLPHandler):\n    """"""Multilayer perceptron with dueling construction.""""""\n\n    def __init__(\n        self, configs: ConfigDict, hidden_activation: Callable = F.relu,\n    ):\n        """"""Initialize.""""""\n        if configs.use_noisy_net:\n            linear_layer = NoisyLinearConstructor(configs.std_init)\n            init_fn: Callable = identity\n        else:\n            linear_layer = nn.Linear\n            init_fn = init_layer_uniform\n        super(DuelingMLP, self).__init__(\n            configs=configs,\n            hidden_activation=hidden_activation,\n            linear_layer=linear_layer,\n            use_output_layer=False,\n        )\n        in_size = configs.hidden_sizes[-1]\n\n        # set advantage layer\n        self.advantage_hidden_layer = self.linear_layer(in_size, in_size)\n        self.advantage_layer = self.linear_layer(in_size, configs.output_size)\n        self.advantage_layer = init_fn(self.advantage_layer)\n\n        # set value layer\n        self.value_hidden_layer = self.linear_layer(in_size, in_size)\n        self.value_layer = self.linear_layer(in_size, 1)\n        self.value_layer = init_fn(self.value_layer)\n\n    def forward_(self, x: torch.Tensor) -> torch.Tensor:\n        adv_x = self.hidden_activation(self.advantage_hidden_layer(x))\n        val_x = self.hidden_activation(self.value_hidden_layer(x))\n\n        advantage = self.advantage_layer(adv_x)\n        value = self.value_layer(val_x)\n        advantage_mean = advantage.mean(dim=-1, keepdim=True)\n\n        q = value + advantage - advantage_mean\n\n        return q\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """"""Forward method implementation.""""""\n        x = super(DuelingMLP, self).forward(x)\n        x = self.forward_(x)\n\n        return x\n\n\n@HEADS.register_module\nclass C51DuelingMLP(MLP, NoisyMLPHandler):\n    """"""Multilayered perceptron for C51 with dueling construction.""""""\n\n    def __init__(\n        self, configs: ConfigDict, hidden_activation: Callable = F.relu,\n    ):\n        """"""Initialize.""""""\n        if configs.use_noisy_net:\n            linear_layer = NoisyLinearConstructor(configs.std_init)\n            init_fn: Callable = identity\n        else:\n            linear_layer = nn.Linear\n            init_fn = init_layer_uniform\n        super(C51DuelingMLP, self).__init__(\n            configs=configs,\n            hidden_activation=hidden_activation,\n            linear_layer=linear_layer,\n            use_output_layer=False,\n        )\n        in_size = configs.hidden_sizes[-1]\n        self.action_size = configs.output_size\n        self.atom_size = configs.atom_size\n        self.output_size = configs.output_size * configs.atom_size\n        self.v_min, self.v_max = configs.v_min, configs.v_max\n\n        # set advantage layer\n        self.advantage_hidden_layer = self.linear_layer(in_size, in_size)\n        self.advantage_layer = self.linear_layer(in_size, self.output_size)\n        self.advantage_layer = init_fn(self.advantage_layer)\n\n        # set value layer\n        self.value_hidden_layer = self.linear_layer(in_size, in_size)\n        self.value_layer = self.linear_layer(in_size, self.atom_size)\n        self.value_layer = init_fn(self.value_layer)\n\n    def forward_(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        """"""Get distribution for atoms.""""""\n        action_size, atom_size = self.action_size, self.atom_size\n\n        x = super(C51DuelingMLP, self).forward(x)\n        adv_x = self.hidden_activation(self.advantage_hidden_layer(x))\n        val_x = self.hidden_activation(self.value_hidden_layer(x))\n\n        advantage = self.advantage_layer(adv_x).view(-1, action_size, atom_size)\n        value = self.value_layer(val_x).view(-1, 1, atom_size)\n        advantage_mean = advantage.mean(dim=1, keepdim=True)\n\n        q_atoms = value + advantage - advantage_mean\n        dist = F.softmax(q_atoms, dim=2)\n\n        support = torch.linspace(self.v_min, self.v_max, self.atom_size).to(device)\n        q = torch.sum(dist * support, dim=2)\n\n        return dist, q\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """"""Forward method implementation.""""""\n        _, q = self.forward_(x)\n\n        return q\n\n\n@HEADS.register_module\nclass IQNMLP(MLP, NoisyMLPHandler):\n    """"""Multilayered perceptron for IQN with dueling construction.\n\n    Reference: https://github.com/google/dopamine\n    """"""\n\n    def __init__(\n        self, configs: ConfigDict, hidden_activation: Callable = F.relu,\n    ):\n        """"""Initialize.""""""\n        if configs.use_noisy_net:\n            linear_layer = NoisyLinearConstructor(configs.std_init)\n            init_fn: Callable = identity\n        else:\n            linear_layer = nn.Linear\n            init_fn = init_layer_uniform\n        super(IQNMLP, self).__init__(\n            configs=configs,\n            hidden_activation=hidden_activation,\n            linear_layer=linear_layer,\n            init_fn=init_fn,\n        )\n\n        IQNMLP.n_quantiles = configs.n_quantile_samples\n        self.quantile_embedding_dim = configs.quantile_embedding_dim\n        self.input_size = configs.input_size\n        self.output_size = configs.output_size\n\n        # set quantile_net layer\n        self.quantile_fc_layer = self.linear_layer(\n            self.quantile_embedding_dim, self.input_size\n        )\n        self.quantile_fc_layer = init_fn(self.quantile_fc_layer)\n\n    def forward_(\n        self, state: torch.Tensor, n_tau_samples: int\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        """"""Get quantile values and quantiles.""""""\n        batch_size = np.prod(state.size()) // self.input_size\n\n        state_tiled = state.repeat(n_tau_samples, 1)\n\n        # torch.rand (CPU) may make a segmentation fault due to its non-thread safety.\n        # on v0.4.1\n        # check: https://bit.ly/2TXlNbq\n        quantiles = np.random.rand(n_tau_samples * batch_size, 1)\n        quantiles = torch.FloatTensor(quantiles)\n        quantile_net = quantiles.repeat(1, self.quantile_embedding_dim)\n        quantile_net = (\n            torch.arange(1, self.quantile_embedding_dim + 1, dtype=torch.float)\n            * math.pi\n            * quantile_net\n        )\n        quantile_net = torch.cos(quantile_net).to(device)\n        quantile_net = F.relu(self.quantile_fc_layer(quantile_net))\n\n        # Hadamard product\n        quantile_net = state_tiled * quantile_net\n\n        quantile_values = super(IQNMLP, self).forward(quantile_net)\n\n        return quantile_values, quantiles\n\n    def forward(self, state: torch.Tensor) -> torch.Tensor:\n        """"""Forward method implementation.""""""\n\n        quantile_values, _ = self.forward_(state, IQNMLP.n_quantiles)\n        quantile_values = quantile_values.view(IQNMLP.n_quantiles, -1, self.output_size)\n        q = torch.mean(quantile_values, dim=0)\n\n        return q\n'"
rl_algorithms/fd/__init__.py,0,"b'""""""Empty.""""""\n'"
rl_algorithms/fd/ddpg_agent.py,2,"b'# -*- coding: utf-8 -*-\n""""""DDPGfD agent using demo agent for episodic tasks in OpenAI Gym.\n\n- Author: Kyunghwan Kim\n- Contact: kh.kim@medipixel.io\n- Paper: https://arxiv.org/pdf/1509.02971.pdf\n         https://arxiv.org/pdf/1511.05952.pdf\n         https://arxiv.org/pdf/1707.08817.pdf\n""""""\n\nimport pickle\nimport time\nfrom typing import Tuple\n\nimport numpy as np\nimport torch\n\nfrom rl_algorithms.common.buffer.priortized_replay_buffer import PrioritizedReplayBuffer\nfrom rl_algorithms.common.buffer.replay_buffer import ReplayBuffer\nimport rl_algorithms.common.helper_functions as common_utils\nfrom rl_algorithms.common.helper_functions import numpy2floattensor\nfrom rl_algorithms.ddpg.agent import DDPGAgent\nfrom rl_algorithms.fd.ddpg_learner import DDPGfDLearner\nfrom rl_algorithms.registry import AGENTS\n\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n\n@AGENTS.register_module\nclass DDPGfDAgent(DDPGAgent):\n    """"""ActorCritic interacting with environment.\n\n    Attributes:\n        memory (PrioritizedReplayBuffer): replay memory\n        per_beta (float): beta parameter for prioritized replay buffer\n        use_n_step (bool): whether or not to use n-step returns\n\n    """"""\n\n    # pylint: disable=attribute-defined-outside-init\n    def _initialize(self):\n        """"""Initialize non-common things.""""""\n        self.per_beta = self.hyper_params.per_beta\n\n        self.use_n_step = self.hyper_params.n_step > 1\n\n        if not self.args.test:\n            # load demo replay memory\n            with open(self.args.demo_path, ""rb"") as f:\n                demos = pickle.load(f)\n\n            if self.use_n_step:\n                demos, demos_n_step = common_utils.get_n_step_info_from_demo(\n                    demos, self.hyper_params.n_step, self.hyper_params.gamma\n                )\n\n                # replay memory for multi-steps\n                self.memory_n = ReplayBuffer(\n                    buffer_size=self.hyper_params.buffer_size,\n                    batch_size=self.hyper_params.batch_size,\n                    n_step=self.hyper_params.n_step,\n                    gamma=self.hyper_params.gamma,\n                    demo=demos_n_step,\n                )\n\n            # replay memory for a single step\n            self.memory = PrioritizedReplayBuffer(\n                self.hyper_params.buffer_size,\n                self.hyper_params.batch_size,\n                demo=demos,\n                alpha=self.hyper_params.per_alpha,\n                epsilon_d=self.hyper_params.per_eps_demo,\n            )\n\n        self.learner = DDPGfDLearner(\n            self.args,\n            self.hyper_params,\n            self.log_cfg,\n            self.head_cfg,\n            self.backbone_cfg,\n            self.optim_cfg,\n            device,\n        )\n\n    def _add_transition_to_memory(self, transition: Tuple[np.ndarray, ...]):\n        """"""Add 1 step and n step transitions to memory.""""""\n        # add n-step transition\n        if self.use_n_step:\n            transition = self.memory_n.add(transition)\n\n        # add a single step transition\n        # if transition is not an empty tuple\n        if transition:\n            self.memory.add(transition)\n\n    def sample_experience(self) -> Tuple[torch.Tensor, ...]:\n        experience_1 = self.memory.sample(self.per_beta)\n        if self.use_n_step:\n            indices = experience_1[-2]\n            experience_n = self.memory_n.sample(indices)\n            return numpy2floattensor(experience_1), numpy2floattensor(experience_n)\n\n        return numpy2floattensor(experience_1)\n\n    def pretrain(self):\n        """"""Pretraining steps.""""""\n        pretrain_loss = list()\n        pretrain_step = self.hyper_params.pretrain_step\n        print(""[INFO] Pre-Train %d step."" % pretrain_step)\n        for i_step in range(1, pretrain_step + 1):\n            t_begin = time.time()\n            experience = self.sample_experience()\n            info = self.learner.update_model(experience)\n            loss = info[0:2]\n            t_end = time.time()\n            pretrain_loss.append(loss)  # for logging\n\n            # logging\n            if i_step == 1 or i_step % 100 == 0:\n                avg_loss = np.vstack(pretrain_loss).mean(axis=0)\n                pretrain_loss.clear()\n                log_value = (0, avg_loss, 0, t_end - t_begin)\n                self.write_log(log_value)\n        print(""[INFO] Pre-Train Complete!\\n"")\n\n    def train(self):\n        """"""Train the agent.""""""\n        # logger\n        if self.args.log:\n            self.set_wandb()\n            # wandb.watch([self.actor, self.critic], log=""parameters"")\n\n        # pre-training if needed\n        self.pretrain()\n\n        for self.i_episode in range(1, self.args.episode_num + 1):\n            state = self.env.reset()\n            done = False\n            score = 0\n            self.episode_step = 0\n            losses = list()\n\n            t_begin = time.time()\n\n            while not done:\n                if self.args.render and self.i_episode >= self.args.render_after:\n                    self.env.render()\n\n                action = self.select_action(state)\n                next_state, reward, done, _ = self.step(action)\n                self.total_step += 1\n                self.episode_step += 1\n\n                if len(self.memory) >= self.hyper_params.batch_size:\n                    for _ in range(self.hyper_params.multiple_update):\n                        experience = self.sample_experience()\n                        info = self.learner.update_model(experience)\n                        loss = info[0:2]\n                        indices, new_priorities = info[2:4]\n                        losses.append(loss)  # for logging\n                        self.memory.update_priorities(indices, new_priorities)\n\n                # increase priority beta\n                fraction = min(float(self.i_episode) / self.args.episode_num, 1.0)\n                self.per_beta = self.per_beta + fraction * (1.0 - self.per_beta)\n\n                state = next_state\n                score += reward\n\n            t_end = time.time()\n            avg_time_cost = (t_end - t_begin) / self.episode_step\n\n            # logging\n            if losses:\n                avg_loss = np.vstack(losses).mean(axis=0)\n                log_value = (self.i_episode, avg_loss, score, avg_time_cost)\n                self.write_log(log_value)\n                losses.clear()\n\n            if self.i_episode % self.args.save_period == 0:\n                self.learner.save_params(self.i_episode)\n                self.interim_test()\n\n        # termination\n        self.env.close()\n        self.learner.save_params(self.i_episode)\n        self.interim_test()\n'"
rl_algorithms/fd/ddpg_learner.py,9,"b'import argparse\nfrom typing import Tuple, Union\n\nimport torch\nfrom torch.nn.utils import clip_grad_norm_\n\nfrom rl_algorithms.common.abstract.learner import TensorTuple\nimport rl_algorithms.common.helper_functions as common_utils\nfrom rl_algorithms.ddpg.learner import DDPGLearner\nfrom rl_algorithms.utils.config import ConfigDict\n\n\nclass DDPGfDLearner(DDPGLearner):\n    """"""Learner for DDPGfD Agent\n\n    Attributes:\n        args (argparse.Namespace): arguments including hyperparameters and training settings\n        hyper_params (ConfigDict): hyper-parameters\n        optim_cfg (ConfigDict): config of optimizer\n        log_cfg (ConfigDict): configuration for saving log and checkpoint\n        actor (nn.Module): actor model to select actions\n        actor_target (nn.Module): target actor model to select actions\n        critic (nn.Module): critic model to predict state values\n        critic_target (nn.Module): target critic model to predict state values\n        actor_optim (Optimizer): optimizer for training actor\n        critic_optim (Optimizer): optimizer for training critic\n\n    """"""\n\n    def __init__(\n        self,\n        args: argparse.Namespace,\n        hyper_params: ConfigDict,\n        log_cfg: ConfigDict,\n        head_cfg: ConfigDict,\n        backbone_cfg: ConfigDict,\n        optim_cfg: ConfigDict,\n        device: torch.device,\n    ):\n        DDPGLearner.__init__(\n            self, args, hyper_params, log_cfg, head_cfg, backbone_cfg, optim_cfg, device\n        )\n\n        self.use_n_step = self.hyper_params.n_step > 1\n\n    def _get_critic_loss(\n        self, experiences: Tuple[TensorTuple, ...], gamma: float\n    ) -> torch.Tensor:\n        """"""Return element-wise critic loss.""""""\n        states, actions, rewards, next_states, dones = experiences[:5]\n\n        # G_t   = r + gamma * v(s_{t+1})  if state != Terminal\n        #       = r                       otherwise\n        masks = 1 - dones\n        next_actions = self.actor_target(next_states)\n        next_states_actions = torch.cat((next_states, next_actions), dim=-1)\n        next_values = self.critic_target(next_states_actions)\n        curr_returns = rewards + gamma * next_values * masks\n        curr_returns = curr_returns.to(self.device).detach()\n\n        # train critic\n        values = self.critic(torch.cat((states, actions), dim=-1))\n        critic_loss_element_wise = (values - curr_returns).pow(2)\n\n        return critic_loss_element_wise\n\n    def update_model(\n        self, experience: Union[TensorTuple, Tuple[TensorTuple]]\n    ) -> TensorTuple:  # type: ignore\n        """"""Train the model after each episode.""""""\n\n        if self.use_n_step:\n            experience_1, experience_n = experience\n        else:\n            experience_1 = experience\n\n        states, actions = experience_1[:2]\n        weights, indices, eps_d = experience_1[-3:]\n        gamma = self.hyper_params.gamma\n\n        # train critic\n        gradient_clip_ac = self.hyper_params.gradient_clip_ac\n        gradient_clip_cr = self.hyper_params.gradient_clip_cr\n\n        critic_loss_element_wise = self._get_critic_loss(experience_1, gamma)\n        critic_loss = torch.mean(critic_loss_element_wise * weights)\n\n        if self.use_n_step:\n            gamma = gamma ** self.hyper_params.n_step\n\n            critic_loss_n_element_wise = self._get_critic_loss(experience_n, gamma)\n            # to update loss and priorities\n            critic_loss_element_wise += (\n                critic_loss_n_element_wise * self.hyper_params.lambda1\n            )\n            critic_loss = torch.mean(critic_loss_element_wise * weights)\n\n        self.critic_optim.zero_grad()\n        critic_loss.backward()\n        clip_grad_norm_(self.critic.parameters(), gradient_clip_cr)\n        self.critic_optim.step()\n\n        # train actor\n        actions = self.actor(states)\n        actor_loss_element_wise = -self.critic(torch.cat((states, actions), dim=-1))\n        actor_loss = torch.mean(actor_loss_element_wise * weights)\n        self.actor_optim.zero_grad()\n        actor_loss.backward()\n        clip_grad_norm_(self.actor.parameters(), gradient_clip_ac)\n        self.actor_optim.step()\n\n        # update target networks\n        common_utils.soft_update(self.actor, self.actor_target, self.hyper_params.tau)\n        common_utils.soft_update(self.critic, self.critic_target, self.hyper_params.tau)\n\n        # update priorities\n        new_priorities = critic_loss_element_wise\n        new_priorities += self.hyper_params.lambda3 * actor_loss_element_wise.pow(2)\n        new_priorities += self.hyper_params.per_eps\n        new_priorities = new_priorities.data.cpu().numpy().squeeze()\n        new_priorities += eps_d.cpu().numpy()\n\n        return (\n            actor_loss.item(),\n            critic_loss.item(),\n            indices.long().cpu().numpy(),\n            new_priorities,\n        )\n'"
rl_algorithms/fd/dqn_agent.py,1,"b'# -*- coding: utf-8 -*-\n""""""DQfD agent using demo agent for episodic tasks in OpenAI Gym.\n\n- Author: Kyunghwan Kim, Curt Park\n- Contact: kh.kim@medipixel.io, curt.park@medipixel.io\n- Paper: https://arxiv.org/pdf/1704.03732.pdf (DQfD)\n""""""\n\nimport pickle\nimport time\n\nimport numpy as np\nimport torch\nimport wandb\n\nfrom rl_algorithms.common.buffer.priortized_replay_buffer import PrioritizedReplayBuffer\nfrom rl_algorithms.common.buffer.replay_buffer import ReplayBuffer\nimport rl_algorithms.common.helper_functions as common_utils\nfrom rl_algorithms.dqn.agent import DQNAgent\nfrom rl_algorithms.fd.dqn_learner import DQfDLearner\nfrom rl_algorithms.registry import AGENTS\n\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n\n@AGENTS.register_module\nclass DQfDAgent(DQNAgent):\n    """"""DQN interacting with environment.\n\n    Attribute:\n        memory (PrioritizedReplayBuffer): replay memory\n\n    """"""\n\n    # pylint: disable=attribute-defined-outside-init\n    def _initialize(self):\n        """"""Initialize non-common things.""""""\n        if not self.args.test:\n            # load demo replay memory\n            demos = self._load_demos()\n\n            if self.use_n_step:\n                demos, demos_n_step = common_utils.get_n_step_info_from_demo(\n                    demos, self.hyper_params.n_step, self.hyper_params.gamma\n                )\n\n                self.memory_n = ReplayBuffer(\n                    buffer_size=self.hyper_params.buffer_size,\n                    batch_size=self.hyper_params.batch_size,\n                    n_step=self.hyper_params.n_step,\n                    gamma=self.hyper_params.gamma,\n                    demo=demos_n_step,\n                )\n\n            # replay memory\n            self.memory = PrioritizedReplayBuffer(\n                self.hyper_params.buffer_size,\n                self.hyper_params.batch_size,\n                demo=demos,\n                alpha=self.hyper_params.per_alpha,\n                epsilon_d=self.hyper_params.per_eps_demo,\n            )\n\n        self.learner = DQfDLearner(\n            self.args,\n            self.hyper_params,\n            self.log_cfg,\n            self.head_cfg,\n            self.backbone_cfg,\n            self.optim_cfg,\n            device,\n        )\n\n    def _load_demos(self) -> list:\n        """"""Load expert\'s demonstrations.""""""\n        # load demo replay memory\n        with open(self.args.demo_path, ""rb"") as f:\n            demos = pickle.load(f)\n\n        return demos\n\n    def write_log(self, log_value: tuple):\n        """"""Write log about loss and score""""""\n        i, avg_loss, score, avg_time_cost = log_value\n        print(\n            ""[INFO] episode %d, episode step: %d, total step: %d, total score: %f\\n""\n            ""epsilon: %f, total loss: %f, dq loss: %f, supervised loss: %f\\n""\n            ""avg q values: %f, demo num in minibatch: %d (spent %.6f sec/step)\\n""\n            % (\n                i,\n                self.episode_step,\n                self.total_step,\n                score,\n                self.epsilon,\n                avg_loss[0],\n                avg_loss[1],\n                avg_loss[2],\n                avg_loss[3],\n                avg_loss[4],\n                avg_time_cost,\n            )\n        )\n\n        if self.args.log:\n            wandb.log(\n                {\n                    ""score"": score,\n                    ""epsilon"": self.epsilon,\n                    ""total loss"": avg_loss[0],\n                    ""dq loss"": avg_loss[1],\n                    ""supervised loss"": avg_loss[2],\n                    ""avg q values"": avg_loss[3],\n                    ""demo num in minibatch"": avg_loss[4],\n                    ""time per each step"": avg_time_cost,\n                }\n            )\n\n    def pretrain(self):\n        """"""Pretraining steps.""""""\n        pretrain_loss = list()\n        pretrain_step = self.hyper_params.pretrain_step\n        print(""[INFO] Pre-Train %d step."" % pretrain_step)\n        for i_step in range(1, pretrain_step + 1):\n            t_begin = time.time()\n            experience = self.sample_experience()\n            info = self.learner.update_model(experience)\n            loss = info[0:5]\n            t_end = time.time()\n            pretrain_loss.append(loss)  # for logging\n\n            # logging\n            if i_step == 1 or i_step % 100 == 0:\n                avg_loss = np.vstack(pretrain_loss).mean(axis=0)\n                pretrain_loss.clear()\n                log_value = (0, avg_loss, 0.0, t_end - t_begin)\n                self.write_log(log_value)\n        print(""[INFO] Pre-Train Complete!\\n"")\n\n    def train(self):\n        """"""Train the agent.""""""\n        # logger\n        if self.args.log:\n            self.set_wandb()\n            # wandb.watch([self.dqn], log=""parameters"")\n\n        # pre-training if needed\n        self.pretrain()\n\n        for self.i_episode in range(1, self.args.episode_num + 1):\n            state = self.env.reset()\n            self.episode_step = 0\n            losses = list()\n            done = False\n            score = 0\n\n            t_begin = time.time()\n\n            while not done:\n                if self.args.render and self.i_episode >= self.args.render_after:\n                    self.env.render()\n\n                action = self.select_action(state)\n                next_state, reward, done, _ = self.step(action)\n                self.total_step += 1\n                self.episode_step += 1\n\n                if len(self.memory) >= self.hyper_params.update_starts_from:\n                    if self.total_step % self.hyper_params.train_freq == 0:\n                        for _ in range(self.hyper_params.multiple_update):\n                            experience = self.sample_experience()\n                            info = self.learner.update_model(experience)\n                            loss = info[0:5]\n                            indices, new_priorities = info[5:7]\n                            losses.append(loss)  # for logging\n                            self.memory.update_priorities(indices, new_priorities)\n\n                    # decrease epsilon\n                    self.epsilon = max(\n                        self.epsilon\n                        - (self.max_epsilon - self.min_epsilon)\n                        * self.hyper_params.epsilon_decay,\n                        self.min_epsilon,\n                    )\n\n                    # increase priority beta\n                    fraction = min(float(self.i_episode) / self.args.episode_num, 1.0)\n                    self.per_beta = self.per_beta + fraction * (1.0 - self.per_beta)\n\n                state = next_state\n                score += reward\n\n            t_end = time.time()\n            avg_time_cost = (t_end - t_begin) / self.episode_step\n\n            if losses:\n                avg_loss = np.vstack(losses).mean(axis=0)\n                log_value = (self.i_episode, avg_loss, score, avg_time_cost)\n                self.write_log(log_value)\n\n            if self.i_episode % self.args.save_period == 0:\n                self.learner.save_params(self.i_episode)\n                self.interim_test()\n\n        # termination\n        self.env.close()\n        self.learner.save_params(self.i_episode)\n        self.interim_test()\n'"
rl_algorithms/fd/dqn_learner.py,9,"b'import argparse\nfrom typing import Tuple, Union\n\nimport numpy as np\nimport torch\nfrom torch.nn.utils import clip_grad_norm_\n\nfrom rl_algorithms.common.abstract.learner import TensorTuple\nimport rl_algorithms.common.helper_functions as common_utils\nfrom rl_algorithms.dqn.learner import DQNLearner\nfrom rl_algorithms.utils.config import ConfigDict\n\n\nclass DQfDLearner(DQNLearner):\n    """"""Learner for DDPGfD Agent\n\n    Attributes:\n        args (argparse.Namespace): arguments including hyperparameters and training settings\n        hyper_params (ConfigDict): hyper-parameters\n        optim_cfg (ConfigDict): config of optimizer\n        log_cfg (ConfigDict): configuration for saving log and checkpoint\n        actor (nn.Module): actor model to select actions\n        actor_target (nn.Module): target actor model to select actions\n        critic (nn.Module): critic model to predict state values\n        critic_target (nn.Module): target critic model to predict state values\n        actor_optim (Optimizer): optimizer for training actor\n        critic_optim (Optimizer): optimizer for training critic\n\n    """"""\n\n    def __init__(\n        self,\n        args: argparse.Namespace,\n        hyper_params: ConfigDict,\n        log_cfg: ConfigDict,\n        head_cfg: ConfigDict,\n        backbone_cfg: ConfigDict,\n        optim_cfg: ConfigDict,\n        device: torch.device,\n    ):\n        DQNLearner.__init__(\n            self, args, hyper_params, log_cfg, head_cfg, backbone_cfg, optim_cfg, device\n        )\n\n    def update_model(\n        self, experience: Union[TensorTuple, Tuple[TensorTuple]]\n    ) -> TensorTuple:  # type: ignore\n        """"""Train the model after each episode.""""""\n\n        if self.use_n_step:\n            experience_1, experience_n = experience\n        else:\n            experience_1 = experience\n\n        weights, indices, eps_d = experience_1[-3:]\n        eps_d = eps_d.cpu().numpy()\n        actions = experience_1[1]\n\n        # 1 step loss\n        gamma = self.hyper_params.gamma\n        dq_loss_element_wise, q_values = self.loss_fn(\n            self.dqn, self.dqn_target, experience_1, gamma, self.head_cfg\n        )\n        dq_loss = torch.mean(dq_loss_element_wise * weights)\n\n        # n step loss\n        if self.use_n_step:\n            gamma = self.hyper_params.gamma ** self.hyper_params.n_step\n            dq_loss_n_element_wise, q_values_n = self.loss_fn(\n                self.dqn, self.dqn_target, experience_n, gamma, self.head_cfg\n            )\n\n            # to update loss and priorities\n            q_values = 0.5 * (q_values + q_values_n)\n            dq_loss_element_wise += dq_loss_n_element_wise * self.hyper_params.lambda1\n            dq_loss = torch.mean(dq_loss_element_wise * weights)\n\n        # supervised loss using demo for only demo transitions\n        demo_idxs = np.where(eps_d != 0.0)\n        n_demo = demo_idxs[0].size\n        if n_demo != 0:  # if 1 or more demos are sampled\n            # get margin for each demo transition\n            action_idxs = actions[demo_idxs].long()\n            margin = torch.ones(q_values.size()) * self.hyper_params.margin\n            margin[demo_idxs, action_idxs] = 0.0  # demo actions have 0 margins\n            margin = margin.to(self.device)\n\n            # calculate supervised loss\n            demo_q_values = q_values[demo_idxs, action_idxs].squeeze()\n            supervised_loss = torch.max(q_values + margin, dim=-1)[0]\n            supervised_loss = supervised_loss[demo_idxs] - demo_q_values\n            supervised_loss = torch.mean(supervised_loss) * self.hyper_params.lambda2\n        else:  # no demo sampled\n            supervised_loss = torch.zeros(1, device=self.device)\n\n        # q_value regularization\n        q_regular = torch.norm(q_values, 2).mean() * self.hyper_params.w_q_reg\n\n        # total loss\n        loss = dq_loss + supervised_loss + q_regular\n\n        # train dqn\n        self.dqn_optim.zero_grad()\n        loss.backward()\n        clip_grad_norm_(self.dqn.parameters(), self.hyper_params.gradient_clip)\n        self.dqn_optim.step()\n\n        # update target networks\n        common_utils.soft_update(self.dqn, self.dqn_target, self.hyper_params.tau)\n\n        # update priorities in PER\n        loss_for_prior = dq_loss_element_wise.detach().cpu().numpy().squeeze()\n        new_priorities = loss_for_prior + self.hyper_params.per_eps\n        new_priorities += eps_d\n\n        if self.head_cfg.configs.use_noisy_net:\n            self.dqn.head.reset_noise()\n            self.dqn_target.head.reset_noise()\n\n        return (\n            loss.item(),\n            dq_loss.item(),\n            supervised_loss.item(),\n            q_values.mean().item(),\n            n_demo,\n            indices.long().cpu().numpy(),\n            new_priorities,\n        )\n'"
rl_algorithms/fd/sac_agent.py,2,"b'# -*- coding: utf-8 -*-\n""""""SAC agent from demonstration for episodic tasks in OpenAI Gym.\n\n- Author: Curt Park\n- Contact: curt.park@medipixel.io\n- Paper: https://arxiv.org/pdf/1801.01290.pdf\n         https://arxiv.org/pdf/1812.05905.pdf\n         https://arxiv.org/pdf/1511.05952.pdf\n         https://arxiv.org/pdf/1707.08817.pdf\n""""""\n\nimport pickle\nimport time\nfrom typing import Tuple\n\nimport numpy as np\nimport torch\n\nfrom rl_algorithms.common.buffer.priortized_replay_buffer import PrioritizedReplayBuffer\nfrom rl_algorithms.common.buffer.replay_buffer import ReplayBuffer\nimport rl_algorithms.common.helper_functions as common_utils\nfrom rl_algorithms.common.helper_functions import numpy2floattensor\nfrom rl_algorithms.fd.sac_learner import SACfDLearner\nfrom rl_algorithms.registry import AGENTS\nfrom rl_algorithms.sac.agent import SACAgent\n\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n\n@AGENTS.register_module\nclass SACfDAgent(SACAgent):\n    """"""SAC agent interacting with environment.\n\n    Attrtibutes:\n        memory (PrioritizedReplayBuffer): replay memory\n        beta (float): beta parameter for prioritized replay buffer\n        use_n_step (bool): whether or not to use n-step returns\n\n    """"""\n\n    # pylint: disable=attribute-defined-outside-init\n    def _initialize(self):\n        """"""Initialize non-common things.""""""\n        self.per_beta = self.hyper_params.per_beta\n        self.use_n_step = self.hyper_params.n_step > 1\n\n        if not self.args.test:\n            # load demo replay memory\n            with open(self.args.demo_path, ""rb"") as f:\n                demos = pickle.load(f)\n\n            if self.use_n_step:\n                demos, demos_n_step = common_utils.get_n_step_info_from_demo(\n                    demos, self.hyper_params.n_step, self.hyper_params.gamma\n                )\n\n                # replay memory for multi-steps\n                self.memory_n = ReplayBuffer(\n                    buffer_size=self.hyper_params.buffer_size,\n                    batch_size=self.hyper_params.batch_size,\n                    n_step=self.hyper_params.n_step,\n                    gamma=self.hyper_params.gamma,\n                    demo=demos_n_step,\n                )\n\n            # replay memory\n            self.memory = PrioritizedReplayBuffer(\n                self.hyper_params.buffer_size,\n                self.hyper_params.batch_size,\n                demo=demos,\n                alpha=self.hyper_params.per_alpha,\n                epsilon_d=self.hyper_params.per_eps_demo,\n            )\n\n        self.learner = SACfDLearner(\n            self.args,\n            self.hyper_params,\n            self.log_cfg,\n            self.head_cfg,\n            self.backbone_cfg,\n            self.optim_cfg,\n            device,\n        )\n\n    def _add_transition_to_memory(self, transition: Tuple[np.ndarray, ...]):\n        """"""Add 1 step and n step transitions to memory.""""""\n        # add n-step transition\n        if self.use_n_step:\n            transition = self.memory_n.add(transition)\n\n        # add a single step transition\n        # if transition is not an empty tuple\n        if transition:\n            self.memory.add(transition)\n\n    def sample_experience(self) -> Tuple[torch.Tensor, ...]:\n        experience_1 = self.memory.sample(self.per_beta)\n        if self.use_n_step:\n            indices = experience_1[-2]\n            experience_n = self.memory_n.sample(indices)\n            return numpy2floattensor(experience_1), numpy2floattensor(experience_n)\n\n        return numpy2floattensor(experience_1)\n\n    def pretrain(self):\n        """"""Pretraining steps.""""""\n        pretrain_loss = list()\n        pretrain_step = self.hyper_params.pretrain_step\n        print(""[INFO] Pre-Train %d steps."" % pretrain_step)\n        for i_step in range(1, pretrain_step + 1):\n            t_begin = time.time()\n            experience = self.sample_experience()\n            info = self.learner.update_model(experience)\n            loss = info[0:5]\n            t_end = time.time()\n            pretrain_loss.append(loss)  # for logging\n\n            # logging\n            if i_step == 1 or i_step % 100 == 0:\n                avg_loss = np.vstack(pretrain_loss).mean(axis=0)\n                pretrain_loss.clear()\n                log_value = (\n                    0,\n                    avg_loss,\n                    0,\n                    self.hyper_params.policy_update_freq,\n                    t_end - t_begin,\n                )\n                self.write_log(log_value)\n        print(""[INFO] Pre-Train Complete!\\n"")\n\n    def train(self):\n        """"""Train the agent.""""""\n        # logger\n        if self.args.log:\n            self.set_wandb()\n            # wandb.watch([self.actor, self.vf, self.qf_1, self.qf_2], log=""parameters"")\n\n        # pre-training if needed\n        self.pretrain()\n\n        for self.i_episode in range(1, self.args.episode_num + 1):\n            state = self.env.reset()\n            done = False\n            score = 0\n            self.episode_step = 0\n            loss_episode = list()\n\n            t_begin = time.time()\n\n            while not done:\n                if self.args.render and self.i_episode >= self.args.render_after:\n                    self.env.render()\n\n                action = self.select_action(state)\n                next_state, reward, done, _ = self.step(action)\n                self.total_step += 1\n                self.episode_step += 1\n\n                state = next_state\n                score += reward\n\n                # training\n                if len(self.memory) >= self.hyper_params.batch_size:\n                    for _ in range(self.hyper_params.multiple_update):\n                        experience = self.sample_experience()\n                        info = self.learner.update_model(experience)\n                        loss = info[0:5]\n                        indices, new_priorities = info[5:7]\n                        loss_episode.append(loss)  # for logging\n                        self.memory.update_priorities(indices, new_priorities)\n\n                # increase priority beta\n                fraction = min(float(self.i_episode) / self.args.episode_num, 1.0)\n                self.per_beta = self.per_beta + fraction * (1.0 - self.per_beta)\n\n            t_end = time.time()\n            avg_time_cost = (t_end - t_begin) / self.episode_step\n\n            # logging\n            if loss_episode:\n                avg_loss = np.vstack(loss_episode).mean(axis=0)\n                log_value = (\n                    self.i_episode,\n                    avg_loss,\n                    score,\n                    self.hyper_params.policy_update_freq,\n                    avg_time_cost,\n                )\n                self.write_log(log_value)\n\n            if self.i_episode % self.args.save_period == 0:\n                self.learner.save_params(self.i_episode)\n                self.interim_test()\n\n        # termination\n        self.env.close()\n        self.learner.save_params(self.i_episode)\n        self.interim_test()\n'"
rl_algorithms/fd/sac_learner.py,12,"b'import argparse\nfrom typing import Tuple\n\nimport torch\n\nfrom rl_algorithms.common.abstract.learner import TensorTuple\nimport rl_algorithms.common.helper_functions as common_utils\nfrom rl_algorithms.sac.learner import SACLearner\nfrom rl_algorithms.utils.config import ConfigDict\n\n\nclass SACfDLearner(SACLearner):\n    """"""Learner for BCSAC Agent\n\n    Attributes:\n        args (argparse.Namespace): arguments including hyperparameters and training settings\n        hyper_params (ConfigDict): hyper-parameters\n        log_cfg (ConfigDict): configuration for saving log and checkpoint\n    """"""\n\n    def __init__(\n        self,\n        args: argparse.Namespace,\n        hyper_params: ConfigDict,\n        log_cfg: ConfigDict,\n        head_cfg: ConfigDict,\n        backbone_cfg: ConfigDict,\n        optim_cfg: ConfigDict,\n        device: torch.device,\n    ):\n        SACLearner.__init__(\n            self, args, hyper_params, log_cfg, head_cfg, backbone_cfg, optim_cfg, device\n        )\n\n        self.use_n_step = self.hyper_params.n_step > 1\n\n    # pylint: disable=too-many-statements\n    def update_model(\n        self, experience: Tuple[TensorTuple, ...]\n    ) -> TensorTuple:  # type: ignore\n        self.update_step += 1\n\n        if self.use_n_step:\n            experience_1, experience_n = experience\n        else:\n            experience_1 = experience\n\n        states, actions, rewards, next_states, dones = experience_1[:-3]\n        weights, indices, eps_d = experience_1[-3:]\n        new_actions, log_prob, pre_tanh_value, mu, std = self.actor(states)\n\n        # train alpha\n        if self.hyper_params.auto_entropy_tuning:\n            alpha_loss = torch.mean(\n                (-self.log_alpha * (log_prob + self.target_entropy).detach()) * weights\n            )\n\n            self.alpha_optim.zero_grad()\n            alpha_loss.backward()\n            self.alpha_optim.step()\n\n            alpha = self.log_alpha.exp()\n        else:\n            alpha_loss = torch.zeros(1)\n            alpha = self.hyper_params.w_entropy\n\n        # Q function loss\n        masks = 1 - dones\n        gamma = self.hyper_params.gamma\n        states_actions = torch.cat((states, actions), dim=-1)\n        q_1_pred = self.qf_1(states_actions)\n        q_2_pred = self.qf_2(states_actions)\n        v_target = self.vf_target(next_states)\n        q_target = rewards + self.hyper_params.gamma * v_target * masks\n        qf_1_loss = torch.mean((q_1_pred - q_target.detach()).pow(2) * weights)\n        qf_2_loss = torch.mean((q_2_pred - q_target.detach()).pow(2) * weights)\n\n        if self.use_n_step:\n            _, _, rewards, next_states, dones = experience_n\n\n            gamma = gamma ** self.hyper_params.n_step\n            masks = 1 - dones\n\n            v_target = self.vf_target(next_states)\n            q_target = rewards + gamma * v_target * masks\n            qf_1_loss_n = torch.mean((q_1_pred - q_target.detach()).pow(2) * weights)\n            qf_2_loss_n = torch.mean((q_2_pred - q_target.detach()).pow(2) * weights)\n\n            # to update loss and priorities\n            qf_1_loss = qf_1_loss + qf_1_loss_n * self.hyper_params.lambda1\n            qf_2_loss = qf_2_loss + qf_2_loss_n * self.hyper_params.lambda1\n\n        # V function loss\n        states_actions = torch.cat((states, new_actions), dim=-1)\n        v_pred = self.vf(states)\n        q_pred = torch.min(self.qf_1(states_actions), self.qf_2(states_actions))\n        v_target = (q_pred - alpha * log_prob).detach()\n        vf_loss_element_wise = (v_pred - v_target).pow(2)\n        vf_loss = torch.mean(vf_loss_element_wise * weights)\n\n        # train Q functions\n        self.qf_1_optim.zero_grad()\n        qf_1_loss.backward()\n        self.qf_1_optim.step()\n\n        self.qf_2_optim.zero_grad()\n        qf_2_loss.backward()\n        self.qf_2_optim.step()\n\n        # train V function\n        self.vf_optim.zero_grad()\n        vf_loss.backward()\n        self.vf_optim.step()\n\n        # actor loss\n        advantage = q_pred - v_pred.detach()\n        actor_loss_element_wise = alpha * log_prob - advantage\n        actor_loss = torch.mean(actor_loss_element_wise * weights)\n\n        # regularization\n        mean_reg = self.hyper_params.w_mean_reg * mu.pow(2).mean()\n        std_reg = self.hyper_params.w_std_reg * std.pow(2).mean()\n        pre_activation_reg = self.hyper_params.w_pre_activation_reg * (\n            pre_tanh_value.pow(2).sum(dim=-1).mean()\n        )\n        actor_reg = mean_reg + std_reg + pre_activation_reg\n\n        # actor loss + regularization\n        actor_loss += actor_reg\n\n        # train actor\n        self.actor_optim.zero_grad()\n        actor_loss.backward()\n        self.actor_optim.step()\n\n        # update target networks\n        common_utils.soft_update(self.vf, self.vf_target, self.hyper_params.tau)\n\n        # update priorities\n        new_priorities = vf_loss_element_wise\n        new_priorities += self.hyper_params.lambda3 * actor_loss_element_wise.pow(2)\n        new_priorities += self.hyper_params.per_eps\n        new_priorities = new_priorities.data.cpu().numpy().squeeze()\n        new_priorities += eps_d.cpu().numpy()\n\n        return (\n            actor_loss.item(),\n            qf_1_loss.item(),\n            qf_2_loss.item(),\n            vf_loss.item(),\n            alpha_loss.item(),\n            indices.long().cpu().numpy(),\n            new_priorities,\n        )\n'"
rl_algorithms/per/__init__.py,0,"b'""""""Empty.""""""\n'"
rl_algorithms/per/ddpg_agent.py,8,"b'# -*- coding: utf-8 -*-\n""""""DDPG agent with PER for episodic tasks in OpenAI Gym.\n\n- Author: Kyunghwan Kim\n- Contact: kh.kim@medipixel.io\n- Paper: https://arxiv.org/pdf/1509.02971.pdf\n         https://arxiv.org/pdf/1511.05952.pdf\n""""""\n\nfrom typing import Tuple\n\nimport torch\nimport torch.nn as nn\n\nfrom rl_algorithms.common.buffer.priortized_replay_buffer import PrioritizedReplayBuffer\nimport rl_algorithms.common.helper_functions as common_utils\nfrom rl_algorithms.ddpg.agent import DDPGAgent\nfrom rl_algorithms.registry import AGENTS\n\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n\n@AGENTS.register_module\nclass PERDDPGAgent(DDPGAgent):\n    """"""ActorCritic interacting with environment.\n\n    Attributes:\n        memory (PrioritizedReplayBuffer): replay memory\n        per_beta (float): beta parameter for prioritized replay buffer\n\n    """"""\n\n    # pylint: disable=attribute-defined-outside-init\n    def _initialize(self):\n        """"""Initialize non-common things.""""""\n        self.per_beta = self.hyper_params.per_beta\n\n        if not self.args.test:\n            # replay memory\n            self.memory = PrioritizedReplayBuffer(\n                self.hyper_params.buffer_size,\n                self.hyper_params.batch_size,\n                alpha=self.hyper_params.per_alpha,\n            )\n\n    def update_model(self) -> Tuple[torch.Tensor, ...]:\n        """"""Train the model after each episode.""""""\n        experiences = self.memory.sample(self.per_beta)\n        experiences = self.numpy2floattensor(experiences[:6]) + experiences[6:]\n        states, actions, rewards, next_states, dones, weights, indices, _ = experiences\n\n        # G_t   = r + gamma * v(s_{t+1})  if state != Terminal\n        #       = r                       otherwise\n        masks = 1 - dones\n        next_actions = self.actor_target(next_states)\n        next_values = self.critic_target(torch.cat((next_states, next_actions), dim=-1))\n        curr_returns = rewards + self.hyper_params.gamma * next_values * masks\n        curr_returns = curr_returns.to(device).detach()\n\n        # train critic\n        gradient_clip_ac = self.hyper_params.gradient_clip_ac\n        gradient_clip_cr = self.hyper_params.gradient_clip_cr\n\n        values = self.critic(torch.cat((states, actions), dim=-1))\n        critic_loss_element_wise = (values - curr_returns).pow(2)\n        critic_loss = torch.mean(critic_loss_element_wise * weights)\n        self.critic_optim.zero_grad()\n        critic_loss.backward()\n        nn.utils.clip_grad_norm_(self.critic.parameters(), gradient_clip_cr)\n        self.critic_optim.step()\n\n        # train actor\n        actions = self.actor(states)\n        actor_loss_element_wise = -self.critic(torch.cat((states, actions), dim=-1))\n        actor_loss = torch.mean(actor_loss_element_wise * weights)\n        self.actor_optim.zero_grad()\n        actor_loss.backward()\n        nn.utils.clip_grad_norm_(self.actor.parameters(), gradient_clip_ac)\n        self.actor_optim.step()\n\n        # update target networks\n        common_utils.soft_update(self.actor, self.actor_target, self.hyper_params.tau)\n        common_utils.soft_update(self.critic, self.critic_target, self.hyper_params.tau)\n\n        # update priorities in PER\n        new_priorities = critic_loss_element_wise\n        new_priorities = new_priorities.data.cpu().numpy() + self.hyper_params.per_eps\n        self.memory.update_priorities(indices, new_priorities)\n\n        # increase beta\n        fraction = min(float(self.i_episode) / self.args.episode_num, 1.0)\n        self.per_beta = self.per_beta + fraction * (1.0 - self.per_beta)\n\n        return actor_loss.item(), critic_loss.item()\n'"
rl_algorithms/ppo/__init__.py,0,"b'""""""Empty.""""""\n'"
rl_algorithms/ppo/agent.py,6,"b'# -*- coding: utf-8 -*-\n""""""PPO agent for episodic tasks in OpenAI Gym.\n\n- Author: Curt Park\n- Contact: curt.park@medipixel.io\n- Paper: https://arxiv.org/abs/1707.06347\n""""""\n\nimport argparse\nfrom typing import Tuple\n\nimport gym\nimport numpy as np\nimport torch\nimport wandb\n\nfrom rl_algorithms.common.abstract.agent import Agent\nfrom rl_algorithms.common.env.utils import env_generator, make_envs\nfrom rl_algorithms.ppo.learner import PPOLearner\nfrom rl_algorithms.registry import AGENTS\nfrom rl_algorithms.utils.config import ConfigDict\n\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n\n@AGENTS.register_module\nclass PPOAgent(Agent):\n    """"""PPO Agent.\n\n    Attributes:\n        env (gym.Env): openAI Gym environment\n        args (argparse.Namespace): arguments including hyperparameters and training settings\n        hyper_params (ConfigDict): hyper-parameters\n        network_cfg (ConfigDict): config of network for training agent\n        optim_cfg (ConfigDict): config of optimizer\n        state_dim (int): state size of env\n        action_dim (int): action size of env\n        actor (nn.Module): policy gradient model to select actions\n        critic (nn.Module): policy gradient model to predict values\n        actor_optim (Optimizer): optimizer for training actor\n        critic_optim (Optimizer): optimizer for training critic\n        episode_steps (np.ndarray): step numbers of the current episode\n        states (list): memory for experienced states\n        actions (list): memory for experienced actions\n        rewards (list): memory for experienced rewards\n        values (list): memory for experienced values\n        masks (list): memory for masks\n        log_probs (list): memory for log_probs\n        i_episode (int): current episode number\n        epsilon (float): value for clipping loss\n\n    """"""\n\n    def __init__(\n        self,\n        env: gym.Env,  # for testing\n        args: argparse.Namespace,\n        log_cfg: ConfigDict,\n        hyper_params: ConfigDict,\n        backbone: ConfigDict,\n        head: ConfigDict,\n        optim_cfg: ConfigDict,\n    ):\n        """"""Initialize.\n\n        Args:\n            env (gym.Env): openAI Gym environment\n            args (argparse.Namespace): arguments including hyperparameters and training settings\n\n        """"""\n        env_gen = env_generator(env.spec.id, args)\n        env_multi = make_envs(env_gen, n_envs=hyper_params.n_workers)\n\n        Agent.__init__(self, env, args, log_cfg)\n\n        self.episode_steps = np.zeros(hyper_params.n_workers, dtype=np.int)\n        self.states: list = []\n        self.actions: list = []\n        self.rewards: list = []\n        self.values: list = []\n        self.masks: list = []\n        self.log_probs: list = []\n        self.i_episode = 0\n        self.next_state = np.zeros((1,))\n\n        self.hyper_params = hyper_params\n        self.backbone_cfg = backbone\n        self.head_cfg = head\n        self.optim_cfg = optim_cfg\n\n        if not self.args.test:\n            self.env = env_multi\n\n        self.head_cfg.actor.configs.state_size = (\n            self.head_cfg.critic.configs.state_size\n        ) = self.env.observation_space.shape\n        self.head_cfg.actor.configs.output_size = self.env.action_space.shape[0]\n\n        self.epsilon = hyper_params.max_epsilon\n\n        self.learner = PPOLearner(\n            self.args,\n            self.hyper_params,\n            self.log_cfg,\n            self.head_cfg,\n            self.backbone_cfg,\n            self.optim_cfg,\n            device,\n            self.is_discrete,\n        )\n\n    def select_action(self, state: np.ndarray) -> torch.Tensor:\n        """"""Select an action from the input space.""""""\n        state = torch.FloatTensor(state).to(device)\n        selected_action, dist = self.learner.actor(state)\n\n        if self.args.test and not self.is_discrete:\n            selected_action = dist.mean\n\n        if not self.args.test:\n            value = self.learner.critic(state)\n            self.states.append(state)\n            self.actions.append(selected_action)\n            self.values.append(value)\n            self.log_probs.append(dist.log_prob(selected_action))\n\n        return selected_action\n\n    def step(self, action: torch.Tensor) -> Tuple[np.ndarray, np.float64, bool, dict]:\n        next_state, reward, done, info = self.env.step(action.detach().cpu().numpy())\n\n        if not self.args.test:\n            # if the last state is not a terminal state, store done as false\n            done_bool = done.copy()\n            done_bool[\n                np.where(self.episode_steps == self.args.max_episode_steps)\n            ] = False\n\n            self.rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n            self.masks.append(torch.FloatTensor(1 - done_bool).unsqueeze(1).to(device))\n\n        return next_state, reward, done, info\n\n    def decay_epsilon(self, t: int = 0):\n        """"""Decay epsilon until reaching the minimum value.""""""\n        max_epsilon = self.hyper_params.max_epsilon\n        min_epsilon = self.hyper_params.min_epsilon\n        epsilon_decay_period = self.hyper_params.epsilon_decay_period\n\n        self.epsilon = self.epsilon - (max_epsilon - min_epsilon) * min(\n            1.0, t / (epsilon_decay_period + 1e-7)\n        )\n\n    def write_log(\n        self, log_value: tuple,\n    ):\n        i_episode, n_step, score, actor_loss, critic_loss, total_loss = log_value\n        print(\n            ""[INFO] episode %d\\tepisode steps: %d\\ttotal score: %d\\n""\n            ""total loss: %f\\tActor loss: %f\\tCritic loss: %f\\n""\n            % (i_episode, n_step, score, total_loss, actor_loss, critic_loss)\n        )\n\n        if self.args.log:\n            wandb.log(\n                {\n                    ""total loss"": total_loss,\n                    ""actor loss"": actor_loss,\n                    ""critic loss"": critic_loss,\n                    ""score"": score,\n                }\n            )\n\n    def train(self):\n        """"""Train the agent.""""""\n        # logger\n        if self.args.log:\n            self.set_wandb()\n            # wandb.watch([self.actor, self.critic], log=""parameters"")\n\n        score = 0\n        i_episode_prev = 0\n        loss = [0.0, 0.0, 0.0]\n        state = self.env.reset()\n\n        while self.i_episode <= self.args.episode_num:\n            for _ in range(self.hyper_params.rollout_len):\n                if self.args.render and self.i_episode >= self.args.render_after:\n                    self.env.render()\n\n                action = self.select_action(state)\n                next_state, reward, done, _ = self.step(action)\n                self.episode_steps += 1\n\n                state = next_state\n                score += reward[0]\n                i_episode_prev = self.i_episode\n                self.i_episode += done.sum()\n\n                if (self.i_episode // self.args.save_period) != (\n                    i_episode_prev // self.args.save_period\n                ):\n                    self.learner.save_params(self.i_episode)\n\n                if done[0]:\n                    n_step = self.episode_steps[0]\n                    log_value = (\n                        self.i_episode,\n                        n_step,\n                        score,\n                        loss[0],\n                        loss[1],\n                        loss[2],\n                    )\n                    self.write_log(log_value)\n                    score = 0\n\n                self.episode_steps[np.where(done)] = 0\n            self.next_state = next_state\n            loss = self.learner.update_model(\n                (\n                    self.states,\n                    self.actions,\n                    self.rewards,\n                    self.values,\n                    self.log_probs,\n                    self.next_state,\n                    self.masks,\n                ),\n                self.epsilon,\n            )\n            self.states, self.actions, self.rewards = [], [], []\n            self.values, self.masks, self.log_probs = [], [], []\n            self.decay_epsilon(self.i_episode)\n\n        # termination\n        self.env.close()\n        self.learner.save_params(self.i_episode)\n'"
rl_algorithms/ppo/learner.py,14,"b'import argparse\n\nimport torch\nfrom torch.nn.utils import clip_grad_norm_\nimport torch.optim as optim\n\nfrom rl_algorithms.common.abstract.learner import Learner, TensorTuple\nfrom rl_algorithms.common.networks.brain import Brain\nimport rl_algorithms.ppo.utils as ppo_utils\nfrom rl_algorithms.utils.config import ConfigDict\n\n\nclass PPOLearner(Learner):\n    """"""Learner for PPO Agent\n\n    Attributes:\n        args (argparse.Namespace): arguments including hyperparameters and training settings\n        hyper_params (ConfigDict): hyper-parameters\n        log_cfg (ConfigDict): configuration for saving log and checkpoint\n        actor (nn.Module): actor model to select actions\n        critic (nn.Module): critic model to predict state values\n        actor_optim (Optimizer): optimizer for training actor\n        critic_optim (Optimizer): optimizer for training critic\n\n    """"""\n\n    def __init__(\n        self,\n        args: argparse.Namespace,\n        hyper_params: ConfigDict,\n        log_cfg: ConfigDict,\n        head_cfg: ConfigDict,\n        backbone_cfg: ConfigDict,\n        optim_cfg: ConfigDict,\n        device: torch.device,\n        is_discrete: bool,\n    ):\n        Learner.__init__(self, args, hyper_params, log_cfg, device)\n\n        self.head_cfg = head_cfg\n        self.backbone_cfg = backbone_cfg\n        self.optim_cfg = optim_cfg\n        self.is_discrete = is_discrete\n\n        self._init_network()\n\n    def _init_network(self):\n        """"""Initialize networks and optimizers.""""""\n        # create actor\n        self.actor = Brain(self.backbone_cfg.actor, self.head_cfg.actor).to(self.device)\n        self.critic = Brain(self.backbone_cfg.critic, self.head_cfg.critic).to(\n            self.device\n        )\n\n        # create optimizer\n        self.actor_optim = optim.Adam(\n            self.actor.parameters(),\n            lr=self.optim_cfg.lr_actor,\n            weight_decay=self.optim_cfg.weight_decay,\n        )\n\n        self.critic_optim = optim.Adam(\n            self.critic.parameters(),\n            lr=self.optim_cfg.lr_critic,\n            weight_decay=self.optim_cfg.weight_decay,\n        )\n\n        # load model parameters\n        if self.args.load_from is not None:\n            self.load_params(self.args.load_from)\n\n    def update_model(self, experience: TensorTuple, epsilon: float) -> TensorTuple:\n        """"""Update PPO actor and critic networks""""""\n        states, actions, rewards, values, log_probs, next_state, masks = experience\n        next_state = torch.FloatTensor(next_state).to(self.device)\n        next_value = self.critic(next_state)\n\n        returns = ppo_utils.compute_gae(\n            next_value,\n            rewards,\n            masks,\n            values,\n            self.hyper_params.gamma,\n            self.hyper_params.tau,\n        )\n\n        states = torch.cat(states)\n        actions = torch.cat(actions)\n        returns = torch.cat(returns).detach()\n        values = torch.cat(values).detach()\n        log_probs = torch.cat(log_probs).detach()\n        advantages = returns - values\n\n        if self.is_discrete:\n            actions = actions.unsqueeze(1)\n            log_probs = log_probs.unsqueeze(1)\n\n        if self.hyper_params.standardize_advantage:\n            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-7)\n\n        actor_losses, critic_losses, total_losses = [], [], []\n\n        for state, action, old_value, old_log_prob, return_, adv in ppo_utils.ppo_iter(\n            self.hyper_params.epoch,\n            self.hyper_params.batch_size,\n            states,\n            actions,\n            values,\n            log_probs,\n            returns,\n            advantages,\n        ):\n            # calculate ratios\n            _, dist = self.actor(state)\n            log_prob = dist.log_prob(action)\n            ratio = (log_prob - old_log_prob).exp()\n\n            # actor_loss\n            surr_loss = ratio * adv\n            clipped_surr_loss = torch.clamp(ratio, 1.0 - epsilon, 1.0 + epsilon) * adv\n            actor_loss = -torch.min(surr_loss, clipped_surr_loss).mean()\n\n            # critic_loss\n            value = self.critic(state)\n            if self.hyper_params.use_clipped_value_loss:\n                value_pred_clipped = old_value + torch.clamp(\n                    (value - old_value), -epsilon, epsilon\n                )\n                value_loss_clipped = (return_ - value_pred_clipped).pow(2)\n                value_loss = (return_ - value).pow(2)\n                critic_loss = 0.5 * torch.max(value_loss, value_loss_clipped).mean()\n            else:\n                critic_loss = 0.5 * (return_ - value).pow(2).mean()\n\n            # entropy\n            entropy = dist.entropy().mean()\n\n            # total_loss\n            w_value = self.hyper_params.w_value\n            w_entropy = self.hyper_params.w_entropy\n\n            total_loss = actor_loss + w_value * critic_loss - w_entropy * entropy\n\n            # train critic\n            gradient_clip_ac = self.hyper_params.gradient_clip_ac\n            gradient_clip_cr = self.hyper_params.gradient_clip_cr\n\n            self.critic_optim.zero_grad()\n            total_loss.backward(retain_graph=True)\n            clip_grad_norm_(self.critic.parameters(), gradient_clip_ac)\n            self.critic_optim.step()\n\n            # train actor\n            self.actor_optim.zero_grad()\n            total_loss.backward()\n            clip_grad_norm_(self.actor.parameters(), gradient_clip_cr)\n            self.actor_optim.step()\n\n            actor_losses.append(actor_loss.item())\n            critic_losses.append(critic_loss.item())\n            total_losses.append(total_loss.item())\n\n        actor_loss = sum(actor_losses) / len(actor_losses)\n        critic_loss = sum(critic_losses) / len(critic_losses)\n        total_loss = sum(total_losses) / len(total_losses)\n\n        return actor_loss, critic_loss, total_loss\n\n    def save_params(self, n_episode: int):\n        """"""Save model and optimizer parameters.""""""\n        params = {\n            ""actor_state_dict"": self.actor.state_dict(),\n            ""critic_state_dict"": self.critic.state_dict(),\n            ""actor_optim_state_dict"": self.actor_optim.state_dict(),\n            ""critic_optim_state_dict"": self.critic_optim.state_dict(),\n        }\n        Learner._save_params(self, params, n_episode)\n\n    def load_params(self, path: str):\n        """"""Load model and optimizer parameters.""""""\n        Learner.load_params(self, path)\n\n        params = torch.load(path)\n        self.actor.load_state_dict(params[""actor_state_dict""])\n        self.critic.load_state_dict(params[""critic_state_dict""])\n        self.actor_optim.load_state_dict(params[""actor_optim_state_dict""])\n        self.critic_optim.load_state_dict(params[""critic_optim_state_dict""])\n        print(""[INFO] loaded the model and optimizer from"", path)\n'"
rl_algorithms/ppo/utils.py,6,"b'# -*- coding: utf-8 -*-\n""""""Utility functions for PPO.\n\nThis module has PPO util functions.\n\n- Author: Curt Park\n- Contact: curt.park@medipixel.io\n- Paper: https://arxiv.org/abs/1707.06347\n""""""\n\nfrom collections import deque\nfrom typing import List\n\nimport numpy as np\nimport torch\n\n\ndef compute_gae(\n    next_value: list,\n    rewards: list,\n    masks: list,\n    values: list,\n    gamma: float = 0.99,\n    tau: float = 0.95,\n) -> List:\n    """"""Compute gae.""""""\n    values = values + [next_value]\n    gae = 0\n    returns: deque = deque()\n\n    for step in reversed(range(len(rewards))):\n        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n        gae = delta + gamma * tau * masks[step] * gae\n        returns.appendleft(gae + values[step])\n\n    return list(returns)\n\n\ndef ppo_iter(\n    epoch: int,\n    mini_batch_size: int,\n    states: torch.Tensor,\n    actions: torch.Tensor,\n    values: torch.Tensor,\n    log_probs: torch.Tensor,\n    returns: torch.Tensor,\n    advantages: torch.Tensor,\n):\n    """"""Yield mini-batches.""""""\n    batch_size = states.size(0)\n    for _ in range(epoch):\n        for _ in range(batch_size // mini_batch_size):\n            rand_ids = np.random.choice(batch_size, mini_batch_size)\n            yield states[rand_ids, :], actions[rand_ids, :], values[\n                rand_ids, :\n            ], log_probs[rand_ids, :], returns[rand_ids, :], advantages[rand_ids, :]\n'"
rl_algorithms/sac/__init__.py,0,"b'""""""Empty.""""""\n'"
rl_algorithms/sac/agent.py,3,"b'# -*- coding: utf-8 -*-\n""""""SAC agent for episodic tasks in OpenAI Gym.\n\n- Author: Curt Park\n- Contact: curt.park@medipixel.io\n- Paper: https://arxiv.org/pdf/1801.01290.pdf\n         https://arxiv.org/pdf/1812.05905.pdf\n""""""\n\nimport argparse\nimport time\nfrom typing import Tuple\n\nimport gym\nimport numpy as np\nimport torch\nimport wandb\n\nfrom rl_algorithms.common.abstract.agent import Agent\nfrom rl_algorithms.common.buffer.replay_buffer import ReplayBuffer\nfrom rl_algorithms.common.helper_functions import numpy2floattensor\nfrom rl_algorithms.registry import AGENTS\nfrom rl_algorithms.sac.learner import SACLearner\nfrom rl_algorithms.utils.config import ConfigDict\n\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n\n@AGENTS.register_module\nclass SACAgent(Agent):\n    """"""SAC agent interacting with environment.\n\n    Attrtibutes:\n        env (gym.Env): openAI Gym environment\n        args (argparse.Namespace): arguments including hyperparameters and training settings\n        hyper_params (ConfigDict): hyper-parameters\n        network_cfg (ConfigDict): config of network for training agent\n        optim_cfg (ConfigDict): config of optimizer\n        state_dim (int): state size of env\n        action_dim (int): action size of env\n        memory (ReplayBuffer): replay memory\n        curr_state (np.ndarray): temporary storage of the current state\n        total_step (int): total step numbers\n        episode_step (int): step number of the current episode\n        update_step (int): step number of updates\n        i_episode (int): current episode number\n\n    """"""\n\n    def __init__(\n        self,\n        env: gym.Env,\n        args: argparse.Namespace,\n        log_cfg: ConfigDict,\n        hyper_params: ConfigDict,\n        backbone: ConfigDict,\n        head: ConfigDict,\n        optim_cfg: ConfigDict,\n    ):\n        """"""Initialize.\n\n        Args:\n            env (gym.Env): openAI Gym environment\n            args (argparse.Namespace): arguments including hyperparameters and training settings\n\n        """"""\n        Agent.__init__(self, env, args, log_cfg)\n\n        self.curr_state = np.zeros((1,))\n        self.total_step = 0\n        self.episode_step = 0\n        self.update_step = 0\n        self.i_episode = 0\n\n        self.hyper_params = hyper_params\n        self.backbone_cfg = backbone\n        self.head_cfg = head\n        self.optim_cfg = optim_cfg\n\n        self.state_dim = self.env.observation_space.shape\n        self.action_dim = self.env.action_space.shape[0]\n        self.head_cfg[""action_dim""] = self.action_dim\n        self.head_cfg.actor.configs.state_size = (\n            self.head_cfg.critic_vf.configs.state_size\n        ) = self.state_dim\n        self.head_cfg.critic_qf.configs.state_size = (\n            self.state_dim[0] + self.action_dim,\n        )\n        self.head_cfg.actor.configs.output_size = self.action_dim\n\n        self._initialize()\n\n    # pylint: disable=attribute-defined-outside-init\n    def _initialize(self):\n        """"""Initialize non-common things.""""""\n        if not self.args.test:\n            # replay memory\n            self.memory = ReplayBuffer(\n                self.hyper_params.buffer_size, self.hyper_params.batch_size\n            )\n\n        self.learner = SACLearner(\n            self.args,\n            self.hyper_params,\n            self.log_cfg,\n            self.head_cfg,\n            self.backbone_cfg,\n            self.optim_cfg,\n            device,\n        )\n\n    def select_action(self, state: np.ndarray) -> np.ndarray:\n        """"""Select an action from the input space.""""""\n        self.curr_state = state\n        state = self._preprocess_state(state)\n\n        # if initial random action should be conducted\n        if (\n            self.total_step < self.hyper_params.initial_random_action\n            and not self.args.test\n        ):\n            return np.array(self.env.action_space.sample())\n\n        if self.args.test:\n            _, _, _, selected_action, _ = self.learner.actor(state)\n        else:\n            selected_action, _, _, _, _ = self.learner.actor(state)\n\n        return selected_action.detach().cpu().numpy()\n\n    # pylint: disable=no-self-use\n    def _preprocess_state(self, state: np.ndarray) -> torch.Tensor:\n        """"""Preprocess state so that actor selects an action.""""""\n        state = torch.FloatTensor(state).to(device)\n        return state\n\n    def step(self, action: np.ndarray) -> Tuple[np.ndarray, np.float64, bool, dict]:\n        """"""Take an action and return the response of the env.""""""\n        next_state, reward, done, info = self.env.step(action)\n\n        if not self.args.test:\n            # if the last state is not a terminal state, store done as false\n            done_bool = (\n                False if self.episode_step == self.args.max_episode_steps else done\n            )\n            transition = (self.curr_state, action, reward, next_state, done_bool)\n            self._add_transition_to_memory(transition)\n\n        return next_state, reward, done, info\n\n    def _add_transition_to_memory(self, transition: Tuple[np.ndarray, ...]):\n        """"""Add 1 step and n step transitions to memory.""""""\n        self.memory.add(transition)\n\n    def write_log(self, log_value: tuple):\n        """"""Write log about loss and score""""""\n        i, loss, score, policy_update_freq, avg_time_cost = log_value\n        total_loss = loss.sum()\n\n        print(\n            ""[INFO] episode %d, episode_step %d, total step %d, total score: %d\\n""\n            ""total loss: %.3f actor_loss: %.3f qf_1_loss: %.3f qf_2_loss: %.3f ""\n            ""vf_loss: %.3f alpha_loss: %.3f (spent %.6f sec/step)\\n""\n            % (\n                i,\n                self.episode_step,\n                self.total_step,\n                score,\n                total_loss,\n                loss[0] * policy_update_freq,  # actor loss\n                loss[1],  # qf_1 loss\n                loss[2],  # qf_2 loss\n                loss[3],  # vf loss\n                loss[4],  # alpha loss\n                avg_time_cost,\n            )\n        )\n\n        if self.args.log:\n            wandb.log(\n                {\n                    ""score"": score,\n                    ""total loss"": total_loss,\n                    ""actor loss"": loss[0] * policy_update_freq,\n                    ""qf_1 loss"": loss[1],\n                    ""qf_2 loss"": loss[2],\n                    ""vf loss"": loss[3],\n                    ""alpha loss"": loss[4],\n                    ""time per each step"": avg_time_cost,\n                }\n            )\n\n    # pylint: disable=no-self-use, unnecessary-pass\n    def pretrain(self):\n        """"""Pretraining steps.""""""\n        pass\n\n    def train(self):\n        """"""Train the agent.""""""\n        # logger\n        if self.args.log:\n            self.set_wandb()\n            # wandb.watch([self.actor, self.vf, self.qf_1, self.qf_2], log=""parameters"")\n\n        # pre-training if needed\n        self.pretrain()\n\n        for self.i_episode in range(1, self.args.episode_num + 1):\n            state = self.env.reset()\n            done = False\n            score = 0\n            self.episode_step = 0\n            loss_episode = list()\n\n            t_begin = time.time()\n\n            while not done:\n                if self.args.render and self.i_episode >= self.args.render_after:\n                    self.env.render()\n\n                action = self.select_action(state)\n                next_state, reward, done, _ = self.step(action)\n                self.total_step += 1\n                self.episode_step += 1\n\n                state = next_state\n                score += reward\n\n                # training\n                if len(self.memory) >= self.hyper_params.batch_size:\n                    for _ in range(self.hyper_params.multiple_update):\n                        experience = self.memory.sample()\n                        experience = numpy2floattensor(experience)\n                        loss = self.learner.update_model(experience)\n                        loss_episode.append(loss)  # for logging\n\n            t_end = time.time()\n            avg_time_cost = (t_end - t_begin) / self.episode_step\n\n            # logging\n            if loss_episode:\n                avg_loss = np.vstack(loss_episode).mean(axis=0)\n                log_value = (\n                    self.i_episode,\n                    avg_loss,\n                    score,\n                    self.hyper_params.policy_update_freq,\n                    avg_time_cost,\n                )\n                self.write_log(log_value)\n\n            if self.i_episode % self.args.save_period == 0:\n                self.learner.save_params(self.i_episode)\n                self.interim_test()\n\n        # termination\n        self.env.close()\n        self.learner.save_params(self.i_episode)\n        self.interim_test()\n'"
rl_algorithms/sac/learner.py,12,"b'import argparse\nfrom typing import Tuple, Union\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom rl_algorithms.common.abstract.learner import Learner, TensorTuple\nimport rl_algorithms.common.helper_functions as common_utils\nfrom rl_algorithms.common.networks.brain import Brain\nfrom rl_algorithms.utils.config import ConfigDict\n\n\nclass SACLearner(Learner):\n    """"""Learner for SAC Agent\n\n    Attributes:\n        args (argparse.Namespace): arguments including hyperparameters and training settings\n        hyper_params (ConfigDict): hyper-parameters\n        log_cfg (ConfigDict): configuration for saving log and checkpoint\n        update_step (int): step number of updates\n        target_entropy (int): desired entropy used for the inequality constraint\n        log_alpha (torch.Tensor): weight for entropy\n        alpha_optim (Optimizer): optimizer for alpha\n        actor (nn.Module): actor model to select actions\n        actor_optim (Optimizer): optimizer for training actor\n        critic_1 (nn.Module): critic model to predict state values\n        critic_2 (nn.Module): critic model to predict state values\n        critic_target1 (nn.Module): target critic model to predict state values\n        critic_target2 (nn.Module): target critic model to predict state values\n        critic_optim1 (Optimizer): optimizer for training critic_1\n        critic_optim2 (Optimizer): optimizer for training critic_2\n\n    """"""\n\n    def __init__(\n        self,\n        args: argparse.Namespace,\n        hyper_params: ConfigDict,\n        log_cfg: ConfigDict,\n        head_cfg: ConfigDict,\n        backbone_cfg: ConfigDict,\n        optim_cfg: ConfigDict,\n        device: torch.device,\n    ):\n        Learner.__init__(self, args, hyper_params, log_cfg, device)\n\n        self.head_cfg = head_cfg\n        self.backbone_cfg = backbone_cfg\n        self.optim_cfg = optim_cfg\n\n        self.update_step = 0\n        if self.hyper_params.auto_entropy_tuning:\n            self.target_entropy = -np.prod((self.head_cfg[""action_dim""],)).item()\n            self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n            self.alpha_optim = optim.Adam([self.log_alpha], lr=optim_cfg.lr_entropy)\n\n        self._init_network()\n\n    # pylint: disable=attribute-defined-outside-init\n    def _init_network(self):\n        """"""Initialize networks and optimizers.""""""\n        # create actor\n        self.actor = Brain(self.backbone_cfg.actor, self.head_cfg.actor).to(self.device)\n\n        # create v_critic\n        self.vf = Brain(self.backbone_cfg.critic_vf, self.head_cfg.critic_vf).to(\n            self.device\n        )\n        self.vf_target = Brain(self.backbone_cfg.critic_vf, self.head_cfg.critic_vf).to(\n            self.device\n        )\n        self.vf_target.load_state_dict(self.vf.state_dict())\n\n        # create q_critic\n        self.qf_1 = Brain(self.backbone_cfg.critic_qf, self.head_cfg.critic_qf).to(\n            self.device\n        )\n        self.qf_2 = Brain(self.backbone_cfg.critic_qf, self.head_cfg.critic_qf).to(\n            self.device\n        )\n\n        # create optimizers\n        self.actor_optim = optim.Adam(\n            self.actor.parameters(),\n            lr=self.optim_cfg.lr_actor,\n            weight_decay=self.optim_cfg.weight_decay,\n        )\n        self.vf_optim = optim.Adam(\n            self.vf.parameters(),\n            lr=self.optim_cfg.lr_vf,\n            weight_decay=self.optim_cfg.weight_decay,\n        )\n        self.qf_1_optim = optim.Adam(\n            self.qf_1.parameters(),\n            lr=self.optim_cfg.lr_qf1,\n            weight_decay=self.optim_cfg.weight_decay,\n        )\n        self.qf_2_optim = optim.Adam(\n            self.qf_2.parameters(),\n            lr=self.optim_cfg.lr_qf2,\n            weight_decay=self.optim_cfg.weight_decay,\n        )\n\n        # load the optimizer and model parameters\n        if self.args.load_from is not None:\n            self.load_params(self.args.load_from)\n\n    def update_model(\n        self, experience: Union[TensorTuple, Tuple[TensorTuple]]\n    ) -> Tuple[torch.Tensor, torch.Tensor, list, np.ndarray]:  # type: ignore\n        """"""Update ddpg actor and critic networks""""""\n        states, actions, rewards, next_states, dones = experience\n        new_actions, log_prob, pre_tanh_value, mu, std = self.actor(states)\n\n        # train alpha\n        if self.hyper_params.auto_entropy_tuning:\n            alpha_loss = (\n                -self.log_alpha * (log_prob + self.target_entropy).detach()\n            ).mean()\n\n            self.alpha_optim.zero_grad()\n            alpha_loss.backward()\n            self.alpha_optim.step()\n\n            alpha = self.log_alpha.exp()\n        else:\n            alpha_loss = torch.zeros(1)\n            alpha = self.hyper_params.w_entropy\n\n        # Q function loss\n        masks = 1 - dones\n        states_actions = torch.cat((states, actions), dim=-1)\n        q_1_pred = self.qf_1(states_actions)\n        q_2_pred = self.qf_2(states_actions)\n        v_target = self.vf_target(next_states)\n        q_target = rewards + self.hyper_params.gamma * v_target * masks\n        qf_1_loss = F.mse_loss(q_1_pred, q_target.detach())\n        qf_2_loss = F.mse_loss(q_2_pred, q_target.detach())\n\n        # V function loss\n        states_actions = torch.cat((states, new_actions), dim=-1)\n        v_pred = self.vf(states)\n        q_pred = torch.min(self.qf_1(states_actions), self.qf_2(states_actions))\n        v_target = q_pred - alpha * log_prob\n        vf_loss = F.mse_loss(v_pred, v_target.detach())\n\n        # train Q functions\n        self.qf_1_optim.zero_grad()\n        qf_1_loss.backward()\n        self.qf_1_optim.step()\n\n        self.qf_2_optim.zero_grad()\n        qf_2_loss.backward()\n        self.qf_2_optim.step()\n\n        # train V function\n        self.vf_optim.zero_grad()\n        vf_loss.backward()\n        self.vf_optim.step()\n\n        if self.update_step % self.hyper_params.policy_update_freq == 0:\n            # actor loss\n            advantage = q_pred - v_pred.detach()\n            actor_loss = (alpha * log_prob - advantage).mean()\n\n            # regularization\n            mean_reg = self.hyper_params.w_mean_reg * mu.pow(2).mean()\n            std_reg = self.hyper_params.w_std_reg * std.pow(2).mean()\n            pre_activation_reg = self.hyper_params.w_pre_activation_reg * (\n                pre_tanh_value.pow(2).sum(dim=-1).mean()\n            )\n            actor_reg = mean_reg + std_reg + pre_activation_reg\n\n            # actor loss + regularization\n            actor_loss += actor_reg\n\n            # train actor\n            self.actor_optim.zero_grad()\n            actor_loss.backward()\n            self.actor_optim.step()\n\n            # update target networks\n            common_utils.soft_update(self.vf, self.vf_target, self.hyper_params.tau)\n        else:\n            actor_loss = torch.zeros(1)\n\n        return (\n            actor_loss.item(),\n            qf_1_loss.item(),\n            qf_2_loss.item(),\n            vf_loss.item(),\n            alpha_loss.item(),\n        )\n\n    def save_params(self, n_episode: int):\n        """"""Save model and optimizer parameters.""""""\n        params = {\n            ""actor"": self.actor.state_dict(),\n            ""qf_1"": self.qf_1.state_dict(),\n            ""qf_2"": self.qf_2.state_dict(),\n            ""vf"": self.vf.state_dict(),\n            ""vf_target"": self.vf_target.state_dict(),\n            ""actor_optim"": self.actor_optim.state_dict(),\n            ""qf_1_optim"": self.qf_1_optim.state_dict(),\n            ""qf_2_optim"": self.qf_2_optim.state_dict(),\n            ""vf_optim"": self.vf_optim.state_dict(),\n        }\n\n        if self.hyper_params.auto_entropy_tuning:\n            params[""alpha_optim""] = self.alpha_optim.state_dict()\n\n        Learner._save_params(self, params, n_episode)\n\n    def load_params(self, path: str):\n        """"""Load model and optimizer parameters.""""""\n        Learner.load_params(self, path)\n\n        params = torch.load(path)\n        self.actor.load_state_dict(params[""actor""])\n        self.qf_1.load_state_dict(params[""qf_1""])\n        self.qf_2.load_state_dict(params[""qf_2""])\n        self.vf.load_state_dict(params[""vf""])\n        self.vf_target.load_state_dict(params[""vf_target""])\n        self.actor_optim.load_state_dict(params[""actor_optim""])\n        self.qf_1_optim.load_state_dict(params[""qf_1_optim""])\n        self.qf_2_optim.load_state_dict(params[""qf_2_optim""])\n        self.vf_optim.load_state_dict(params[""vf_optim""])\n\n        if self.hyper_params.auto_entropy_tuning:\n            self.alpha_optim.load_state_dict(params[""alpha_optim""])\n\n        print(""[INFO] loaded the model and optimizer from"", path)\n'"
rl_algorithms/td3/__init__.py,0,"b'""""""Empty.""""""\n'"
rl_algorithms/td3/agent.py,2,"b'# -*- coding: utf-8 -*-\n""""""TD3 agent for episodic tasks in OpenAI Gym.\n\n- Author: Curt Park\n- Contact: curt.park@medipixel.io\n- Paper: https://arxiv.org/pdf/1802.09477.pdf\n""""""\n\nimport argparse\nimport time\nfrom typing import Tuple\n\nimport gym\nimport numpy as np\nimport torch\nimport wandb\n\nfrom rl_algorithms.common.abstract.agent import Agent\nfrom rl_algorithms.common.buffer.replay_buffer import ReplayBuffer\nfrom rl_algorithms.common.helper_functions import numpy2floattensor\nfrom rl_algorithms.common.noise import GaussianNoise\nfrom rl_algorithms.registry import AGENTS\nfrom rl_algorithms.td3.learner import TD3Learner\nfrom rl_algorithms.utils.config import ConfigDict\n\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n\n@AGENTS.register_module\nclass TD3Agent(Agent):\n    """"""ActorCritic interacting with environment.\n\n    Attributes:\n        env (gym.Env): openAI Gym environment\n        args (argparse.Namespace): arguments including hyperparameters and training settings\n        hyper_params (ConfigDict): hyper-parameters\n        network_cfg (ConfigDict): config of network for training agent\n        optim_cfg (ConfigDict): config of optimizer\n        state_dim (int): state size of env\n        action_dim (int): action size of env\n        memory (ReplayBuffer): replay memory\n        exploration_noise (GaussianNoise): random noise for exploration\n        target_policy_noise (GaussianNoise): random noise for target values\n        curr_state (np.ndarray): temporary storage of the current state\n        total_steps (int): total step numbers\n        episode_steps (int): step number of the current episode\n        i_episode (int): current episode number\n        noise_cfg (ConfigDict): config of noise\n\n    """"""\n\n    def __init__(\n        self,\n        env: gym.Env,\n        args: argparse.Namespace,\n        log_cfg: ConfigDict,\n        hyper_params: ConfigDict,\n        backbone: ConfigDict,\n        head: ConfigDict,\n        optim_cfg: ConfigDict,\n        noise_cfg: ConfigDict,\n    ):\n        """"""Initialize.\n\n        Args:\n            env (gym.Env): openAI Gym environment\n            args (argparse.Namespace): arguments including hyperparameters and training settings\n\n        """"""\n        Agent.__init__(self, env, args, log_cfg)\n\n        self.curr_state = np.zeros((1,))\n        self.total_step = 0\n        self.episode_step = 0\n        self.update_step = 0\n        self.i_episode = 0\n\n        self.hyper_params = hyper_params\n        self.noise_cfg = noise_cfg\n        self.backbone_cfg = backbone\n        self.head_cfg = head\n        self.optim_cfg = optim_cfg\n\n        self.state_dim = self.env.observation_space.shape\n        self.action_dim = self.env.action_space.shape[0]\n        self.head_cfg.actor.configs.state_size = self.state_dim\n        self.head_cfg.critic.configs.state_size = (self.state_dim[0] + self.action_dim,)\n        self.head_cfg.actor.configs.output_size = self.action_dim\n\n        # noise instance to make randomness of action\n        self.exploration_noise = GaussianNoise(\n            self.action_dim, noise_cfg.exploration_noise, noise_cfg.exploration_noise\n        )\n\n        self.target_policy_noise = GaussianNoise(\n            self.action_dim,\n            noise_cfg.target_policy_noise,\n            noise_cfg.target_policy_noise,\n        )\n\n        if not self.args.test:\n            # replay memory\n            self.memory = ReplayBuffer(\n                self.hyper_params.buffer_size, self.hyper_params.batch_size\n            )\n\n        self.learner = TD3Learner(\n            self.args,\n            self.hyper_params,\n            self.log_cfg,\n            self.head_cfg,\n            self.backbone_cfg,\n            self.optim_cfg,\n            device,\n            self.noise_cfg,\n            self.target_policy_noise,\n        )\n\n    def select_action(self, state: np.ndarray) -> np.ndarray:\n        """"""Select an action from the input space.""""""\n        # initial training step, try random action for exploration\n        self.curr_state = state\n\n        if (\n            self.total_step < self.hyper_params.initial_random_action\n            and not self.args.test\n        ):\n            return np.array(self.env.action_space.sample())\n\n        state = torch.FloatTensor(state).to(device)\n        selected_action = self.learner.actor(state).detach().cpu().numpy()\n\n        if not self.args.test:\n            noise = self.exploration_noise.sample()\n            selected_action = np.clip(selected_action + noise, -1.0, 1.0)\n\n        return selected_action\n\n    def step(self, action: np.ndarray) -> Tuple[np.ndarray, np.float64, bool, dict]:\n        """"""Take an action and return the response of the env.""""""\n        next_state, reward, done, info = self.env.step(action)\n\n        if not self.args.test:\n            # if last state is not terminal state in episode, done is false\n            done_bool = (\n                False if self.episode_step == self.args.max_episode_steps else done\n            )\n            self.memory.add((self.curr_state, action, reward, next_state, done_bool))\n\n        return next_state, reward, done, info\n\n    def write_log(self, log_value: tuple):\n        """"""Write log about loss and score""""""\n        i, loss, score, policy_update_freq, avg_time_cost = log_value\n        total_loss = loss.sum()\n        print(\n            ""[INFO] episode %d, episode_step: %d, total_step: %d, total score: %d\\n""\n            ""total loss: %f actor_loss: %.3f critic1_loss: %.3f critic2_loss: %.3f ""\n            ""(spent %.6f sec/step)\\n""\n            % (\n                i,\n                self.episode_step,\n                self.total_step,\n                score,\n                total_loss,\n                loss[0] * policy_update_freq,  # actor loss\n                loss[1],  # critic1 loss\n                loss[2],  # critic2 loss\n                avg_time_cost,\n            )\n        )\n\n        if self.args.log:\n            wandb.log(\n                {\n                    ""score"": score,\n                    ""total loss"": total_loss,\n                    ""actor loss"": loss[0] * policy_update_freq,\n                    ""critic1 loss"": loss[1],\n                    ""critic2 loss"": loss[2],\n                    ""time per each step"": avg_time_cost,\n                }\n            )\n\n    def train(self):\n        """"""Train the agent.""""""\n        # logger\n        if self.args.log:\n            self.set_wandb()\n            # wandb.watch([self.actor, self.critic1, self.critic2], log=""parameters"")\n\n        for self.i_episode in range(1, self.args.episode_num + 1):\n            state = self.env.reset()\n            done = False\n            score = 0\n            loss_episode = list()\n            self.episode_step = 0\n\n            t_begin = time.time()\n\n            while not done:\n                if self.args.render and self.i_episode >= self.args.render_after:\n                    self.env.render()\n\n                action = self.select_action(state)\n                next_state, reward, done, _ = self.step(action)\n                self.total_step += 1\n                self.episode_step += 1\n\n                state = next_state\n                score += reward\n\n                if len(self.memory) >= self.hyper_params.batch_size:\n                    experience = self.memory.sample()\n                    experience = numpy2floattensor(experience)\n                    loss = self.learner.update_model(experience)\n                    loss_episode.append(loss)  # for logging\n\n            t_end = time.time()\n            avg_time_cost = (t_end - t_begin) / self.episode_step\n\n            # logging\n            if loss_episode:\n                avg_loss = np.vstack(loss_episode).mean(axis=0)\n                log_value = (\n                    self.i_episode,\n                    avg_loss,\n                    score,\n                    self.hyper_params.policy_update_freq,\n                    avg_time_cost,\n                )\n                self.write_log(log_value)\n            if self.i_episode % self.args.save_period == 0:\n                self.learner.save_params(self.i_episode)\n                self.interim_test()\n\n        # termination\n        self.env.close()\n        self.learner.save_params(self.i_episode)\n        self.interim_test()\n'"
rl_algorithms/td3/learner.py,13,"b'import argparse\nfrom typing import Tuple\n\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom rl_algorithms.common.abstract.learner import Learner\nimport rl_algorithms.common.helper_functions as common_utils\nfrom rl_algorithms.common.networks.brain import Brain\nfrom rl_algorithms.common.noise import GaussianNoise\nfrom rl_algorithms.utils.config import ConfigDict\n\n\nclass TD3Learner(Learner):\n    """"""Learner for DDPG Agent\n\n    Attributes:\n        args (argparse.Namespace): arguments including hyperparameters and training settings\n        hyper_params (ConfigDict): hyper-parameters\n        network_cfg (ConfigDict): config of network for training agent\n        optim_cfg (ConfigDict): config of optimizer\n        noise_cfg (ConfigDict): config of noise\n        target_policy_noise (GaussianNoise): random noise for target values\n        actor (nn.Module): actor model to select actions\n        critic1 (nn.Module): critic model to predict state values\n        critic2 (nn.Module): critic model to predict state values\n        critic_target1 (nn.Module): target critic model to predict state values\n        critic_target2 (nn.Module): target critic model to predict state values\n        actor_target (nn.Module): target actor model to select actions\n        critic_optim (Optimizer): optimizer for training critic\n        actor_optim (Optimizer): optimizer for training actor\n\n    """"""\n\n    def __init__(\n        self,\n        args: argparse.Namespace,\n        hyper_params: ConfigDict,\n        log_cfg: ConfigDict,\n        head_cfg: ConfigDict,\n        backbone_cfg: ConfigDict,\n        optim_cfg: ConfigDict,\n        device: torch.device,\n        noise_cfg: ConfigDict,\n        target_policy_noise: GaussianNoise,\n    ):\n        Learner.__init__(self, args, hyper_params, log_cfg, device)\n\n        self.head_cfg = head_cfg\n        self.backbone_cfg = backbone_cfg\n        self.optim_cfg = optim_cfg\n        self.update_step = 0\n        self.noise_cfg = noise_cfg\n        self.target_policy_noise = target_policy_noise\n\n        self._init_network()\n\n    def _init_network(self):\n        """"""Initialize networks and optimizers.""""""\n        # create actor\n        self.actor = Brain(self.backbone_cfg.actor, self.head_cfg.actor).to(self.device)\n        self.actor_target = Brain(self.backbone_cfg.actor, self.head_cfg.actor).to(\n            self.device\n        )\n        self.actor_target.load_state_dict(self.actor.state_dict())\n\n        # create q_critic\n        self.critic1 = Brain(self.backbone_cfg.critic, self.head_cfg.critic).to(\n            self.device\n        )\n        self.critic2 = Brain(self.backbone_cfg.critic, self.head_cfg.critic).to(\n            self.device\n        )\n\n        self.critic_target1 = Brain(self.backbone_cfg.critic, self.head_cfg.critic).to(\n            self.device\n        )\n        self.critic_target2 = Brain(self.backbone_cfg.critic, self.head_cfg.critic).to(\n            self.device\n        )\n\n        self.critic_target1.load_state_dict(self.critic1.state_dict())\n        self.critic_target2.load_state_dict(self.critic2.state_dict())\n\n        # concat critic parameters to use one optim\n        critic_parameters = list(self.critic1.parameters()) + list(\n            self.critic2.parameters()\n        )\n\n        # create optimizers\n        self.actor_optim = optim.Adam(\n            self.actor.parameters(),\n            lr=self.optim_cfg.lr_actor,\n            weight_decay=self.optim_cfg.weight_decay,\n        )\n\n        self.critic_optim = optim.Adam(\n            critic_parameters,\n            lr=self.optim_cfg.lr_critic,\n            weight_decay=self.optim_cfg.weight_decay,\n        )\n\n        # load the optimizer and model parameters\n        if self.args.load_from is not None:\n            self.load_params(self.args.load_from)\n\n    def update_model(\n        self, experience: Tuple[torch.Tensor, ...]\n    ) -> Tuple[torch.Tensor, ...]:\n        """"""Update TD3 actor and critic networks""""""\n        states, actions, rewards, next_states, dones = experience\n        masks = 1 - dones\n\n        # get actions with noise\n        noise = torch.FloatTensor(self.target_policy_noise.sample()).to(self.device)\n        clipped_noise = torch.clamp(\n            noise,\n            -self.noise_cfg.target_policy_noise_clip,\n            self.noise_cfg.target_policy_noise_clip,\n        )\n        next_actions = (self.actor_target(next_states) + clipped_noise).clamp(-1.0, 1.0)\n\n        # min (Q_1\', Q_2\')\n        next_states_actions = torch.cat((next_states, next_actions), dim=-1)\n        next_values1 = self.critic_target1(next_states_actions)\n        next_values2 = self.critic_target2(next_states_actions)\n        next_values = torch.min(next_values1, next_values2)\n\n        # G_t   = r + gamma * v(s_{t+1})  if state != Terminal\n        #       = r                       otherwise\n        curr_returns = rewards + self.hyper_params.gamma * next_values * masks\n        curr_returns = curr_returns.detach()\n\n        # critic loss\n        state_actions = torch.cat((states, actions), dim=-1)\n        values1 = self.critic1(state_actions)\n        values2 = self.critic2(state_actions)\n        critic1_loss = F.mse_loss(values1, curr_returns)\n        critic2_loss = F.mse_loss(values2, curr_returns)\n\n        # train critic\n        critic_loss = critic1_loss + critic2_loss\n        self.critic_optim.zero_grad()\n        critic_loss.backward()\n        self.critic_optim.step()\n\n        if self.update_step % self.hyper_params.policy_update_freq == 0:\n            # policy loss\n            actions = self.actor(states)\n            state_actions = torch.cat((states, actions), dim=-1)\n            actor_loss = -self.critic1(state_actions).mean()\n\n            # train actor\n            self.actor_optim.zero_grad()\n            actor_loss.backward()\n            self.actor_optim.step()\n\n            # update target networks\n            tau = self.hyper_params.tau\n            common_utils.soft_update(self.critic1, self.critic_target1, tau)\n            common_utils.soft_update(self.critic2, self.critic_target2, tau)\n            common_utils.soft_update(self.actor, self.actor_target, tau)\n        else:\n            actor_loss = torch.zeros(1)\n\n        return actor_loss.item(), critic1_loss.item(), critic2_loss.item()\n\n    def save_params(self, n_episode: int):\n        """"""Save model and optimizer parameters.""""""\n        params = {\n            ""actor"": self.actor.state_dict(),\n            ""actor_target"": self.actor_target.state_dict(),\n            ""actor_optim"": self.actor_optim.state_dict(),\n            ""critic1"": self.critic1.state_dict(),\n            ""critic2"": self.critic2.state_dict(),\n            ""critic_target1"": self.critic_target1.state_dict(),\n            ""critic_target2"": self.critic_target2.state_dict(),\n            ""critic_optim"": self.critic_optim.state_dict(),\n        }\n\n        Learner._save_params(self, params, n_episode)\n\n    def load_params(self, path: str):\n        """"""Load model and optimizer parameters.""""""\n        Learner.load_params(self, path)\n\n        params = torch.load(path)\n        self.critic1.load_state_dict(params[""critic1""])\n        self.critic2.load_state_dict(params[""critic2""])\n        self.critic_target1.load_state_dict(params[""critic_target1""])\n        self.critic_target2.load_state_dict(params[""critic_target2""])\n        self.critic_optim.load_state_dict(params[""critic_optim""])\n        self.actor.load_state_dict(params[""actor""])\n        self.actor_target.load_state_dict(params[""actor_target""])\n        self.actor_optim.load_state_dict(params[""actor_optim""])\n        print(""[INFO] loaded the model and optimizer from"", path)\n'"
rl_algorithms/utils/__init__.py,0,"b'from .config import Config\nfrom .registry import Registry, build_from_cfg\n\n__all__ = [""Registry"", ""build_from_cfg"", ""Config""]\n'"
rl_algorithms/utils/config.py,0,"b'from argparse import ArgumentParser\nimport collections.abc as collections_abc\nfrom importlib import import_module\nimport os.path as osp\nimport sys\n\nfrom addict import Dict\n\n\nclass ConfigDict(Dict):\n    def __missing__(self, name):\n        raise KeyError(name)\n\n    def __getattr__(self, name):\n        try:\n            value = super(ConfigDict, self).__getattr__(name)\n        except KeyError:\n            ex = AttributeError(\n                ""\'{}\' object has no attribute \'{}\'"".format(\n                    self.__class__.__name__, name\n                )\n            )\n        except Exception as e:\n            ex = e\n        else:\n            return value\n        raise ex\n\n    def __setitem__(self, name, value):\n        if isinstance(value, dict):\n            value = ConfigDict(value)\n\n        super(ConfigDict, self).__setitem__(name, value)\n\n\ndef add_args(parser, cfg, prefix=""""):\n    for k, v in cfg.items():\n        if isinstance(v, str):\n            parser.add_argument(""--"" + prefix + k)\n        elif isinstance(v, int):\n            parser.add_argument(""--"" + prefix + k, type=int)\n        elif isinstance(v, float):\n            parser.add_argument(""--"" + prefix + k, type=float)\n        elif isinstance(v, bool):\n            parser.add_argument(""--"" + prefix + k, action=""store_true"")\n        elif isinstance(v, dict):\n            add_args(parser, v, k + ""."")\n        elif isinstance(v, collections_abc.Iterable):\n            parser.add_argument(""--"" + prefix + k, type=type(v[0]), nargs=""+"")\n        else:\n            print(""connot parse key {} of type {}"".format(prefix + k, type(v)))\n    return parser\n\n\nclass Config:\n    """"""A facility for config and config files.\n\n    It supports common file formats as configs: python/json/yaml. The interface\n    is the same as a dict object and also allows access config values as\n    attributes.\n\n    Example:\n        >>> cfg = Config(dict(a=1, b=dict(b1=[0, 1])))\n        >>> cfg.a\n        1\n        >>> cfg.b\n        {\'b1\': [0, 1]}\n        >>> cfg.b.b1\n        [0, 1]\n        >>> cfg = Config.fromfile(\'tests/data/config/a.py\')\n        >>> cfg.filename\n        ""/home/kchen/projects/mmcv/tests/data/config/a.py""\n        >>> cfg.item4\n        \'test\'\n        >>> cfg\n        ""Config [path: /home/kchen/projects/mmcv/tests/data/config/a.py]: ""\n        ""{\'item1\': [1, 2], \'item2\': {\'a\': 0}, \'item3\': True, \'item4\': \'test\'}""\n\n    """"""\n\n    def __init__(self, cfg_dict=None, filename=None):\n        if cfg_dict is None:\n            cfg_dict = dict()\n        elif not isinstance(cfg_dict, dict):\n            raise TypeError(\n                ""cfg_dict must be a dict, but got {}"".format(type(cfg_dict))\n            )\n\n        super(Config, self).__setattr__(""_cfg_dict"", ConfigDict(cfg_dict))\n        super(Config, self).__setattr__(""_filename"", filename)\n        if filename:\n            with open(filename, ""r"", encoding=""utf-8"") as f:\n                super(Config, self).__setattr__(""_text"", f.read())\n        else:\n            super(Config, self).__setattr__(""_text"", """")\n\n    @staticmethod\n    def fromfile(filename):\n        filename = osp.abspath(osp.expanduser(filename))\n        if not osp.isfile(filename):\n            raise FileNotFoundError(f""file {filename} does not exist"")\n        if filename.endswith("".py""):\n            module_name = osp.basename(filename)[:-3]\n            if ""."" in module_name:\n                raise ValueError(""Dots are not allowed in config file path."")\n            config_dir = osp.dirname(filename)\n            sys.path.insert(0, config_dir)\n            mod = import_module(module_name)\n            sys.path.pop(0)\n            cfg_dict = {\n                name: value\n                for name, value in mod.__dict__.items()\n                if not name.startswith(""__"")\n            }\n        else:\n            raise IOError(""Only py/yml/yaml/json type are supported now!"")\n        return Config(cfg_dict, filename=filename)\n\n    @staticmethod\n    def auto_argparser(description=None):\n        """"""Generate argparser from config file automatically (experimental)\n        """"""\n        partial_parser = ArgumentParser(description=description)\n        partial_parser.add_argument(""config"", help=""config file path"")\n        cfg_file = partial_parser.parse_known_args()[0].config\n        cfg = Config.fromfile(cfg_file)\n        parser = ArgumentParser(description=description)\n        parser.add_argument(""config"", help=""config file path"")\n        add_args(parser, cfg)\n        return parser, cfg\n\n    @property\n    def filename(self):\n        return self._filename\n\n    @property\n    def text(self):\n        return self._text\n\n    def __repr__(self):\n        return ""Config (path: {}): {}"".format(self.filename, self._cfg_dict.__repr__())\n\n    def __len__(self):\n        return len(self._cfg_dict)\n\n    def __getattr__(self, name):\n        return getattr(self._cfg_dict, name)\n\n    def __getitem__(self, name):\n        return self._cfg_dict.__getitem__(name)\n\n    def __setattr__(self, name, value):\n        if isinstance(value, dict):\n            value = ConfigDict(value)\n        self._cfg_dict.__setattr__(name, value)\n\n    def __setitem__(self, name, value):\n        if isinstance(value, dict):\n            value = ConfigDict(value)\n        self._cfg_dict.__setitem__(name, value)\n\n    def __iter__(self):\n        return iter(self._cfg_dict)\n'"
rl_algorithms/utils/registry.py,0,"b'import inspect\n\nfrom rl_algorithms.utils.config import ConfigDict\n\n\nclass Registry:\n    def __init__(self, name):\n        self._name = name\n        self._module_dict = dict()\n\n    def __repr__(self):\n        format_str = self.__class__.__name__ + ""(name={}, items={})"".format(\n            self._name, list(self._module_dict.keys())\n        )\n        return format_str\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def module_dict(self):\n        return self._module_dict\n\n    def get(self, key):\n        return self._module_dict.get(key, None)\n\n    def _register_module(self, module_class):\n        """"""Register a module.\n        Args:\n            module (:obj:`nn.Module`): Module to be registered.\n        """"""\n        if not inspect.isclass(module_class):\n            raise TypeError(\n                ""module must be a class, but got {}"".format(type(module_class))\n            )\n        module_name = module_class.__name__\n        if module_name in self._module_dict:\n            raise KeyError(\n                ""{} is already registered in {}"".format(module_name, self.name)\n            )\n        self._module_dict[module_name] = module_class\n\n    def register_module(self, cls):\n        self._register_module(cls)\n        return cls\n\n\ndef build_from_cfg(cfg: ConfigDict, registry: Registry, default_args: dict = None):\n    """"""Build a module from config dict.\n    Args:\n        cfg (:obj: `ConfigDict`): Config dict. It should at least contain the key ""type"".\n        registry (:obj:`Registry`): The registry to search the type from.\n        default_args (dict, optional): Default initialization arguments.\n    Returns:\n        obj: The constructed object.\n    """"""\n    assert isinstance(cfg, dict) and ""type"" in cfg\n    assert isinstance(default_args, dict) or default_args is None\n    args = cfg.copy()\n    obj_type = args.pop(""type"")\n    if isinstance(obj_type, str):\n        obj_cls = registry.get(obj_type)\n        if obj_cls is None:\n            raise KeyError(\n                ""{} is not in the {} registry"".format(obj_type, registry.name)\n            )\n    elif inspect.isclass(obj_type):\n        obj_cls = obj_type\n    else:\n        raise TypeError(\n            ""type must be a str or valid type, but got {}"".format(type(obj_type))\n        )\n\n    if default_args is not None:\n        for name, value in default_args.items():\n            args.setdefault(name, value)\n    return obj_cls(**args)\n'"
rl_algorithms/common/abstract/__init__.py,0,b''
rl_algorithms/common/abstract/agent.py,4,"b'# -*- coding: utf-8 -*-\n""""""Abstract Agent used for all agents.\n\n- Author: Curt Park\n- Contact: curt.park@medipixel.io\n""""""\n\nfrom abc import ABC, abstractmethod\nimport argparse\nimport os\nimport shutil\nfrom typing import Tuple, Union\n\nimport cv2\nimport gym\nfrom gym.spaces import Discrete\nimport numpy as np\nimport torch\nimport wandb\n\nfrom rl_algorithms.common.grad_cam import GradCAM\nfrom rl_algorithms.utils.config import ConfigDict\n\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n\nclass Agent(ABC):\n    """"""Abstract Agent used for all agents.\n\n    Attributes:\n        env (gym.Env): openAI Gym environment\n        args (argparse.Namespace): arguments including hyperparameters and training settings\n        log_cfg (ConfigDict): configuration for saving log\n        state_dim (int): dimension of states\n        action_dim (int): dimension of actions\n        is_discrete (bool): shows whether the action is discrete\n\n    """"""\n\n    def __init__(self, env: gym.Env, args: argparse.Namespace, log_cfg: ConfigDict):\n        """"""Initialize.""""""\n        self.args = args\n        self.env = env\n        self.log_cfg = log_cfg\n        self.log_cfg.env_name = env.spec.id if env.spec is not None else env.name\n\n        if isinstance(env.action_space, Discrete):\n            self.is_discrete = True\n        else:\n            self.is_discrete = False\n\n    @abstractmethod\n    def select_action(self, state: np.ndarray) -> Union[torch.Tensor, np.ndarray]:\n        pass\n\n    @abstractmethod\n    def step(\n        self, action: Union[torch.Tensor, np.ndarray]\n    ) -> Tuple[np.ndarray, np.float64, bool, dict]:\n        pass\n\n    @abstractmethod\n    def write_log(self, log_value: tuple):  # type: ignore\n        pass\n\n    @abstractmethod\n    def train(self):\n        pass\n\n    def set_wandb(self):\n        """"""Set configuration for wandb logging.""""""\n        wandb.init(\n            project=self.log_cfg.env_name,\n            name=f""{self.log_cfg.agent}/{self.log_cfg.curr_time}"",\n        )\n        wandb.config.update(vars(self.args))\n        shutil.copy(self.args.cfg_path, os.path.join(wandb.run.dir, ""config.py""))\n\n    def interim_test(self):\n        """"""Test in the middle of training.""""""\n        self.args.test = True\n\n        print()\n        print(""==========="")\n        print(""Start Test!"")\n        print(""==========="")\n\n        self._test(interim_test=True)\n\n        print(""==========="")\n        print(""Test done!"")\n        print(""==========="")\n        print()\n\n        self.args.test = False\n\n    def test(self):\n        """"""Test the agent.""""""\n        # logger\n        if self.args.log:\n            self.set_wandb()\n\n        self._test()\n\n        # termination\n        self.env.close()\n\n    def _test(self, interim_test: bool = False):\n        """"""Common test routine.""""""\n\n        if interim_test:\n            test_num = self.args.interim_test_num\n        else:\n            test_num = self.args.episode_num\n\n        for i_episode in range(test_num):\n            state = self.env.reset()\n            done = False\n            score = 0\n            step = 0\n\n            while not done:\n                if self.args.render:\n                    self.env.render()\n\n                action = self.select_action(state)\n                next_state, reward, done, _ = self.step(action)\n\n                state = next_state\n                score += reward\n                step += 1\n\n            print(\n                ""[INFO] test %d\\tstep: %d\\ttotal score: %d"" % (i_episode, step, score)\n            )\n\n            if self.args.log:\n                wandb.log({""test score"": score})\n\n    def test_with_gradcam(self):\n        """"""Test agent with Grad-CAM.""""""\n        gcam = GradCAM(model=self.dqn.eval())\n\n        for i_episode in range(self.args.episode_num):\n            state = self.env.reset()\n            done = False\n            score = 0\n            step = 0\n\n            key = 0\n            print(""\\nPress Any Key to move to next step... (quit: ESC key)"")\n            while not done:\n                state = self._preprocess_state(state)\n                action = self.dqn(state).argmax()\n                action = action.detach().cpu().numpy()\n                next_state, reward, done, _ = self.step(action)\n\n                _ = gcam.forward(state)\n                ids = torch.LongTensor([[int(action)]]).cuda()\n                gcam.backward(ids=ids)\n\n                state = state[-1].detach().cpu().numpy().astype(np.uint8)\n                state = np.transpose(state)\n                state = cv2.cvtColor(state, cv2.COLOR_GRAY2BGR)\n                state = cv2.resize(state, (150, 150), interpolation=cv2.INTER_LINEAR)\n\n                # Get Grad-CAM image\n                result_images = None\n                for target_layer in self.hyper_params.grad_cam_layer_list:\n                    regions = gcam.generate(target_layer)\n                    regions = regions.detach().cpu().numpy()\n                    regions = np.squeeze(regions) * 255\n                    regions = np.transpose(regions)\n                    regions = cv2.applyColorMap(\n                        regions.astype(np.uint8), cv2.COLORMAP_JET\n                    )\n                    regions = cv2.resize(\n                        regions, (150, 150), interpolation=cv2.INTER_LINEAR\n                    )\n                    overlay = cv2.addWeighted(state, 1.0, regions, 0.5, 0)\n                    result = np.hstack([state, regions, overlay])\n                    result_images = (\n                        result\n                        if result_images is None\n                        else np.vstack([result_images, result])\n                    )\n                # Show action on result image\n                cv2.putText(\n                    img=result_images,\n                    text=f""action: {action}"",\n                    org=(50, 50),\n                    fontFace=cv2.FONT_HERSHEY_PLAIN,\n                    fontScale=1,\n                    color=(0, 0, 255),\n                    thickness=2,\n                )\n\n                cv2.imshow(""result"", result_images)\n                key = cv2.waitKey(0)\n                if key == 27 & 0xFF:  # ESC key\n                    cv2.destroyAllWindows()\n                    break\n\n                state = next_state\n                score += reward\n                step += 1\n\n            print(\n                ""[INFO] test %d\\tstep: %d\\ttotal score: %d"" % (i_episode, step, score)\n            )\n            if key == 27 & 0xFF:  # ESC key\n                break\n'"
rl_algorithms/common/abstract/her.py,0,"b'# -*- coding: utf-8 -*-\n""""""Abstract class used for Hindsight Experience Replay.\n\n- Author: Kyunghwan Kim\n- Contact: kh.kim@medipixel.io\n- Paper: https://arxiv.org/pdf/1707.01495.pdf\n""""""\n\nfrom abc import ABC, abstractmethod\nfrom typing import Callable, Tuple\n\nimport numpy as np\n\n\nclass HER(ABC):\n    """"""Abstract class for HER (final strategy).\n\n    Attributes:\n        reward_fn (Callable): returns reward from state, action, next_state\n\n    """"""\n\n    def __init__(self, reward_fn: Callable[[tuple, np.ndarray], np.float64]):\n        """"""Initialize.\n\n        Args:\n            reward_fn (Callable): returns reward from state, action, next_state\n\n        """"""\n        self.reward_fn = reward_fn\n\n    @abstractmethod\n    def fetch_desired_states_from_demo(self, demo: list):\n        pass\n\n    @abstractmethod\n    def get_desired_state(self, *args) -> np.ndarray:\n        pass\n\n    @abstractmethod\n    def generate_demo_transitions(self, demo: list) -> list:\n        pass\n\n    @abstractmethod\n    def _get_final_state(self, transition: tuple) -> np.ndarray:\n        pass\n\n    def _append_origin_transitions(\n        self, origin_transitions: list, transition: tuple, desired_state: np.ndarray\n    ):\n        """"""Append original transitions adding goal state for training.""""""\n        origin_transitions.append(self._get_transition(transition, desired_state))\n\n    def _append_new_transitions(\n        self, new_transitions: list, transition: tuple, final_state: np.ndarray\n    ):\n        """"""Append new transitions made by HER strategy (final) for training.""""""\n        new_transitions.append(self._get_transition(transition, final_state))\n\n    def _get_transition(\n        self, transition: tuple, goal_state: np.ndarray\n    ) -> Tuple[np.ndarray, np.ndarray, np.float64, np.ndarray, bool]:\n        """"""Get a single transition concatenated with a goal state.""""""\n        state, action, _, next_state, done = transition\n\n        done = np.array_equal(next_state, goal_state)\n        reward = self.reward_fn(transition, goal_state)\n        state = np.concatenate((state, goal_state), axis=-1)\n        next_state = np.concatenate((next_state, goal_state), axis=-1)\n\n        return state, action, reward, next_state, done\n\n    def generate_transitions(\n        self,\n        transitions: list,\n        desired_state: np.ndarray,\n        success_score: float,\n        is_demo: bool = False,\n    ) -> list:\n        """"""Generate new transitions concatenated with desired states.""""""\n        origin_transitions: list = list()\n        new_transitions: list = list()\n        final_state = self._get_final_state(transitions[-1])\n        score = np.sum(np.array(transitions), axis=0)[2]\n\n        for transition in transitions:\n            # process transitions with the initial goal state\n            self._append_origin_transitions(\n                origin_transitions, transition, desired_state\n            )\n\n            # do not need to append new transitions if sum of reward is big enough\n            if not is_demo and score <= success_score:\n                self._append_new_transitions(new_transitions, transition, final_state)\n\n        return origin_transitions + new_transitions\n\n    def __str__(self):\n        return self.__class__.__name__\n'"
rl_algorithms/common/abstract/learner.py,3,"b'from abc import ABC, abstractmethod\nimport argparse\nimport os\nimport shutil\nimport subprocess\nfrom typing import Tuple, Union\n\nimport torch\n\nfrom rl_algorithms.utils.config import ConfigDict\n\nTensorTuple = Tuple[torch.Tensor, ...]\n\n\nclass Learner(ABC):\n    """"""Abstract learner for all learners\n\n    Attributes:\n        args (argparse.Namespace): arguments including hyperparameters and training settings\n        hyper_params (ConfigDict): hyper-parameters\n        log_cfg (ConfigDict): configuration for saving log\n        sha (str): sha code of current git commit\n\n    """"""\n\n    def __init__(\n        self,\n        args: argparse.Namespace,\n        hyper_params: ConfigDict,\n        log_cfg: ConfigDict,\n        device: torch.device,\n    ):\n        """"""Initialize.""""""\n        self.args = args\n        self.hyper_params = hyper_params\n        self.device = device\n\n        if not self.args.test:\n            self.ckpt_path = (\n                ""./checkpoint/""\n                f""{log_cfg.env_name}/{log_cfg.agent}/{log_cfg.curr_time}/""\n            )\n            os.makedirs(self.ckpt_path, exist_ok=True)\n\n            # save configuration\n            shutil.copy(self.args.cfg_path, os.path.join(self.ckpt_path, ""config.py""))\n\n        # for logging\n        self.sha = (\n            subprocess.check_output([""git"", ""rev-parse"", ""--short"", ""HEAD""])[:-1]\n            .decode(""ascii"")\n            .strip()\n        )\n\n    @abstractmethod\n    def _init_network(self):\n        pass\n\n    @abstractmethod\n    def update_model(self, experience: Union[TensorTuple, Tuple[TensorTuple]]) -> tuple:\n        pass\n\n    @abstractmethod\n    def save_params(self, n_episode: int):\n        pass\n\n    def _save_params(self, params: dict, n_episode: int):\n        """"""Save parameters of networks.""""""\n        os.makedirs(self.ckpt_path, exist_ok=True)\n\n        path = os.path.join(self.ckpt_path + self.sha + ""_ep_"" + str(n_episode) + "".pt"")\n        torch.save(params, path)\n\n        print(""[INFO] Saved the model and optimizer to"", path)\n\n    @abstractmethod\n    def load_params(self, path: str):\n        if not os.path.exists(path):\n            raise Exception(\n                f""[ERROR] the input path does not exist. Wrong path: {path}""\n            )\n'"
rl_algorithms/common/abstract/reward_fn.py,0,"b'# -*- coding: utf-8 -*-\n""""""Abstract class for computing reward.\n\n- Author: Kyunghwan Kim\n- Contact: kh.kim@medipixel.io\n""""""\nfrom abc import ABC, abstractmethod\n\nimport numpy as np\n\n\nclass RewardFn(ABC):\n    """"""Abstract class for computing reward.\n       New compute_reward class should redefine __call__()\n\n    """"""\n\n    @abstractmethod\n    def __call__(self, transition: tuple, goal_state: np.ndarray) -> np.float64:\n        pass\n'"
rl_algorithms/common/buffer/__init__.py,0,"b'""""""Empty.""""""\n'"
rl_algorithms/common/buffer/priortized_replay_buffer.py,2,"b'# -*- coding: utf-8 -*-\n""""""Prioritized Replay buffer for algorithms.\n\n- Author: Kyunghwan Kim\n- Contact: kh.kim@medipixel.io\n- Paper: https://arxiv.org/pdf/1511.05952.pdf\n         https://arxiv.org/pdf/1707.08817.pdf\n""""""\n\nimport random\nfrom typing import Any, List, Tuple\n\nimport numpy as np\nimport torch\n\nfrom rl_algorithms.common.buffer.replay_buffer import ReplayBuffer\nfrom rl_algorithms.common.buffer.segment_tree import MinSegmentTree, SumSegmentTree\n\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n\nclass PrioritizedReplayBuffer(ReplayBuffer):\n    """"""Create Prioritized Replay buffer.\n\n    Refer to OpenAI baselines github repository:\n    https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n\n    Attributes:\n        alpha (float): alpha parameter for prioritized replay buffer\n        epsilon_d (float): small positive constants to add to the priorities\n        tree_idx (int): next index of tree\n        sum_tree (SumSegmentTree): sum tree for prior\n        min_tree (MinSegmentTree): min tree for min prior to get max weight\n        _max_priority (float): max priority\n    """"""\n\n    def __init__(\n        self,\n        buffer_size: int,\n        batch_size: int = 32,\n        gamma: float = 0.99,\n        n_step: int = 1,\n        alpha: float = 0.6,\n        epsilon_d: float = 1.0,\n        demo: List[Tuple[np.ndarray, np.ndarray, float, np.ndarray, bool]] = None,\n    ):\n        """"""Initialize.\n\n        Args:\n            buffer_size (int): size of replay buffer for experience\n            batch_size (int): size of a batched sampled from replay buffer for training\n            alpha (float): alpha parameter for prioritized replay buffer\n\n        """"""\n        super(PrioritizedReplayBuffer, self).__init__(\n            buffer_size, batch_size, gamma, n_step, demo\n        )\n        assert alpha >= 0\n        self.alpha = alpha\n        self.epsilon_d = epsilon_d\n        self.tree_idx = 0\n\n        # capacity must be positive and a power of 2.\n        tree_capacity = 1\n        while tree_capacity < self.buffer_size:\n            tree_capacity *= 2\n\n        self.sum_tree = SumSegmentTree(tree_capacity)\n        self.min_tree = MinSegmentTree(tree_capacity)\n        self._max_priority = 1.0\n\n        # for init priority of demo\n        self.tree_idx = self.demo_size\n        for i in range(self.demo_size):\n            self.sum_tree[i] = self._max_priority ** self.alpha\n            self.min_tree[i] = self._max_priority ** self.alpha\n\n    def add(\n        self, transition: Tuple[np.ndarray, np.ndarray, float, np.ndarray, bool]\n    ) -> Tuple[Any, ...]:\n        """"""Add experience and priority.""""""\n        n_step_transition = super().add(transition)\n        if n_step_transition:\n            self.sum_tree[self.tree_idx] = self._max_priority ** self.alpha\n            self.min_tree[self.tree_idx] = self._max_priority ** self.alpha\n\n            self.tree_idx += 1\n            if self.tree_idx % self.buffer_size == 0:\n                self.tree_idx = self.demo_size\n\n        return n_step_transition\n\n    def _sample_proportional(self, batch_size: int) -> list:\n        """"""Sample indices based on proportional.""""""\n        indices = []\n        p_total = self.sum_tree.sum(0, len(self) - 1)\n        segment = p_total / batch_size\n\n        for i in range(batch_size):\n            a = segment * i\n            b = segment * (i + 1)\n            upperbound = random.uniform(a, b)\n            idx = self.sum_tree.retrieve(upperbound)\n            indices.append(idx)\n        return indices\n\n    def sample(self, beta: float = 0.4) -> Tuple[torch.Tensor, ...]:\n        """"""Sample a batch of experiences.""""""\n        assert len(self) >= self.batch_size\n        assert beta > 0\n\n        indices = self._sample_proportional(self.batch_size)\n\n        # get max weight\n        p_min = self.min_tree.min() / self.sum_tree.sum()\n        max_weight = (p_min * len(self)) ** (-beta)\n\n        # calculate weights\n        weights_, eps_d = [], []\n        for i in indices:\n            eps_d.append(self.epsilon_d if i < self.demo_size else 0.0)\n            p_sample = self.sum_tree[i] / self.sum_tree.sum()\n            weight = (p_sample * len(self)) ** (-beta)\n            weights_.append(weight / max_weight)\n\n        weights = np.array(weights_)\n        eps_d = np.array(eps_d)\n\n        weights = weights.reshape(-1, 1)\n\n        states, actions, rewards, next_states, dones = super().sample(indices)\n\n        return states, actions, rewards, next_states, dones, weights, indices, eps_d\n\n    def update_priorities(self, indices: list, priorities: np.ndarray):\n        """"""Update priorities of sampled transitions.""""""\n        assert len(indices) == len(priorities)\n\n        for idx, priority in zip(indices, priorities):\n            assert priority > 0\n            assert 0 <= idx < len(self)\n\n            self.sum_tree[idx] = priority ** self.alpha\n            self.min_tree[idx] = priority ** self.alpha\n\n            self._max_priority = max(self._max_priority, priority)\n'"
rl_algorithms/common/buffer/replay_buffer.py,0,"b'# -*- coding: utf-8 -*-\n""""""Replay buffer for baselines.""""""\n\nfrom collections import deque\nfrom typing import Any, Deque, List, Tuple\n\nimport numpy as np\n\nfrom rl_algorithms.common.helper_functions import get_n_step_info\n\n\nclass ReplayBuffer:\n    """"""Fixed-size buffer to store experience tuples.\n\n    Attributes:\n        obs_buf (np.ndarray): observations\n        acts_buf (np.ndarray): actions\n        rews_buf (np.ndarray): rewards\n        next_obs_buf (np.ndarray): next observations\n        done_buf (np.ndarray): dones\n        n_step_buffer (deque): recent n transitions\n        n_step (int): step size for n-step transition\n        gamma (float): discount factor\n        buffer_size (int): size of buffers\n        batch_size (int): batch size for training\n        demo_size (int): size of demo transitions\n        length (int): amount of memory filled\n        idx (int): memory index to add the next incoming transition\n    """"""\n\n    def __init__(\n        self,\n        buffer_size: int,\n        batch_size: int,\n        gamma: float = 0.99,\n        n_step: int = 1,\n        demo: List[Tuple[np.ndarray, np.ndarray, float, np.ndarray, bool]] = None,\n    ):\n        """"""Initialize a ReplayBuffer object.\n\n        Args:\n            buffer_size (int): size of replay buffer for experience\n            batch_size (int): size of a batched sampled from replay buffer for training\n            gamma (float): discount factor\n            n_step (int): step size for n-step transition\n            demo (list): transitions of human play\n        """"""\n        assert 0 < batch_size <= buffer_size\n        assert 0.0 <= gamma <= 1.0\n        assert 1 <= n_step <= buffer_size\n\n        self.obs_buf: np.ndarray = None\n        self.acts_buf: np.ndarray = None\n        self.rews_buf: np.ndarray = None\n        self.next_obs_buf: np.ndarray = None\n        self.done_buf: np.ndarray = None\n\n        self.n_step_buffer: Deque = deque(maxlen=n_step)\n        self.n_step = n_step\n        self.gamma = gamma\n\n        self.buffer_size = buffer_size\n        self.batch_size = batch_size\n        self.demo_size = len(demo) if demo else 0\n        self.demo = demo\n        self.length = 0\n        self.idx = self.demo_size\n\n        # demo may have empty tuple list [()]\n        if self.demo and self.demo[0]:\n            self.buffer_size += self.demo_size\n            self.length += self.demo_size\n            for idx, d in enumerate(self.demo):\n                state, action, reward, next_state, done = d\n                if idx == 0:\n                    action = (\n                        np.array(action).astype(np.int64)\n                        if isinstance(action, int)\n                        else action\n                    )\n                    self._initialize_buffers(state, action)\n                self.obs_buf[idx] = state\n                self.acts_buf[idx] = np.array(action)\n                self.rews_buf[idx] = reward\n                self.next_obs_buf[idx] = next_state\n                self.done_buf[idx] = done\n\n    def add(\n        self, transition: Tuple[np.ndarray, np.ndarray, float, np.ndarray, bool]\n    ) -> Tuple[Any, ...]:\n        """"""Add a new experience to memory.\n        If the buffer is empty, it is respectively initialized by size of arguments.\n        """"""\n        self.n_step_buffer.append(transition)\n\n        # single step transition is not ready\n        if len(self.n_step_buffer) < self.n_step:\n            return ()\n\n        if self.length == 0:\n            state, action = transition[:2]\n            self._initialize_buffers(state, action)\n\n        # add a multi step transition\n        reward, next_state, done = get_n_step_info(self.n_step_buffer, self.gamma)\n        curr_state, action = self.n_step_buffer[0][:2]\n\n        self.obs_buf[self.idx] = curr_state\n        self.acts_buf[self.idx] = action\n        self.rews_buf[self.idx] = reward\n        self.next_obs_buf[self.idx] = next_state\n        self.done_buf[self.idx] = done\n\n        self.idx += 1\n        self.idx = self.demo_size if self.idx % self.buffer_size == 0 else self.idx\n        self.length = min(self.length + 1, self.buffer_size)\n\n        # return a single step transition to insert to replay buffer\n        return self.n_step_buffer[0]\n\n    def extend(\n        self, transitions: List[Tuple[np.ndarray, np.ndarray, float, np.ndarray, bool]]\n    ):\n        """"""Add experiences to memory.""""""\n        for transition in transitions:\n            self.add(transition)\n\n    def sample(self, indices: List[int] = None) -> Tuple[np.ndarray, ...]:\n        """"""Randomly sample a batch of experiences from memory.""""""\n        assert len(self) >= self.batch_size\n\n        if indices is None:\n            indices = np.random.choice(len(self), size=self.batch_size, replace=False)\n\n        states = self.obs_buf[indices]\n        actions = self.acts_buf[indices]\n        rewards = self.rews_buf[indices].reshape(-1, 1)\n        next_states = self.next_obs_buf[indices]\n        dones = self.done_buf[indices].reshape(-1, 1)\n\n        return states, actions, rewards, next_states, dones\n\n    def _initialize_buffers(self, state: np.ndarray, action: np.ndarray) -> None:\n        """"""Initialze buffers for state, action, resward, next_state, done.""""""\n        # In case action of demo is not np.ndarray\n        self.obs_buf = np.zeros(\n            [self.buffer_size] + list(state.shape), dtype=state.dtype\n        )\n        self.acts_buf = np.zeros(\n            [self.buffer_size] + list(action.shape), dtype=action.dtype\n        )\n        self.rews_buf = np.zeros([self.buffer_size], dtype=float)\n        self.next_obs_buf = np.zeros(\n            [self.buffer_size] + list(state.shape), dtype=state.dtype\n        )\n        self.done_buf = np.zeros([self.buffer_size], dtype=float)\n\n    def __len__(self) -> int:\n        """"""Return the current size of internal memory.""""""\n        return self.length\n'"
rl_algorithms/common/buffer/segment_tree.py,0,"b'# -*- coding: utf-8 -*-\n""""""Segment tree for Proirtized Replay Buffer.""""""\n\nimport operator\nfrom typing import Callable\n\n\nclass SegmentTree:\n    """""" Create SegmentTree.\n\n    Taken from OpenAI baselines github repository:\n    https://github.com/openai/baselines/blob/master/baselines/common/segment_tree.py\n\n    Attributes:\n        capacity (int)\n        tree (list)\n        operation (function)\n\n    """"""\n\n    def __init__(self, capacity: int, operation: Callable, init_value: float):\n        """"""Initialize.\n\n        Args:\n            capacity (int)\n            operation (function)\n            init_value (float)\n\n        """"""\n        assert (\n            capacity > 0 and capacity & (capacity - 1) == 0\n        ), ""capacity must be positive and a power of 2.""\n        self.capacity = capacity\n        self.tree = [init_value for _ in range(2 * capacity)]\n        self.operation = operation\n\n    def _operate_helper(\n        self, start: int, end: int, node: int, node_start: int, node_end: int\n    ) -> float:\n        """"""Returns result of operation in segment.""""""\n        if start == node_start and end == node_end:\n            return self.tree[node]\n        mid = (node_start + node_end) // 2\n        if end <= mid:\n            return self._operate_helper(start, end, 2 * node, node_start, mid)\n        else:\n            if mid + 1 <= start:\n                return self._operate_helper(start, end, 2 * node + 1, mid + 1, node_end)\n            else:\n                return self.operation(\n                    self._operate_helper(start, mid, 2 * node, node_start, mid),\n                    self._operate_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end),\n                )\n\n    def operate(self, start: int = 0, end: int = 0) -> float:\n        """"""Returns result of applying `self.operation`.""""""\n        if end <= 0:\n            end += self.capacity\n        end -= 1\n\n        return self._operate_helper(start, end, 1, 0, self.capacity - 1)\n\n    def __setitem__(self, idx: int, val: float):\n        """"""Set value in tree.""""""\n        idx += self.capacity\n        self.tree[idx] = val\n\n        idx //= 2\n        while idx >= 1:\n            self.tree[idx] = self.operation(self.tree[2 * idx], self.tree[2 * idx + 1])\n            idx //= 2\n\n    def __getitem__(self, idx: int) -> float:\n        """"""Get real value in leaf node of tree.""""""\n        assert 0 <= idx < self.capacity\n\n        return self.tree[self.capacity + idx]\n\n\nclass SumSegmentTree(SegmentTree):\n    """""" Create SumSegmentTree.\n\n    Taken from OpenAI baselines github repository:\n    https://github.com/openai/baselines/blob/master/baselines/common/segment_tree.py\n\n    """"""\n\n    def __init__(self, capacity: int):\n        """"""Initialize.\n\n        Args:\n            capacity (int)\n\n        """"""\n        super(SumSegmentTree, self).__init__(\n            capacity=capacity, operation=operator.add, init_value=0.0\n        )\n\n    def sum(self, start: int = 0, end: int = 0) -> float:\n        """"""Returns arr[start] + ... + arr[end].""""""\n        return super(SumSegmentTree, self).operate(start, end)\n\n    def retrieve(self, upperbound: float) -> int:\n        """"""Find the highest index `i` about upper bound in the tree""""""\n        # TODO: Check assert case and fix bug\n        assert 0 <= upperbound <= self.sum() + 1e-5, ""upperbound: {}"".format(upperbound)\n\n        idx = 1\n\n        while idx < self.capacity:  # while non-leaf\n            left = 2 * idx\n            right = left + 1\n            if self.tree[left] > upperbound:\n                idx = 2 * idx\n            else:\n                upperbound -= self.tree[left]\n                idx = right\n        return idx - self.capacity\n\n\nclass MinSegmentTree(SegmentTree):\n    """""" Create SegmentTree.\n\n    Taken from OpenAI baselines github repository:\n    https://github.com/openai/baselines/blob/master/baselines/common/segment_tree.py\n\n    """"""\n\n    def __init__(self, capacity: int):\n        """"""Initialize.\n\n        Args:\n            capacity (int)\n\n        """"""\n        super(MinSegmentTree, self).__init__(\n            capacity=capacity, operation=min, init_value=float(""inf"")\n        )\n\n    def min(self, start: int = 0, end: int = 0) -> float:\n        """"""Returns min(arr[start], ...,  arr[end]).""""""\n        return super(MinSegmentTree, self).operate(start, end)\n'"
rl_algorithms/common/env/__init__.py,0,"b'""""""Empty.""""""\n'"
rl_algorithms/common/env/atari_wrappers.py,0,"b'""""""\nThis code taken from\nhttps://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py\n""""""\n\nfrom collections import deque\nimport os\n\nimport cv2\nimport gym\nimport gym.spaces as spaces\nimport numpy as np\n\nos.environ.setdefault(""PATH"", """")\n\ncv2.ocl.setUseOpenCL(False)\n\n\nclass TimeLimit(gym.Wrapper):\n    def __init__(self, env, max_episode_steps=None):\n        super(TimeLimit, self).__init__(env)\n        self._max_episode_steps = max_episode_steps\n        self._elapsed_steps = 0\n\n    # pylint: disable=method-hidden\n    def step(self, ac):\n        observation, reward, done, info = self.env.step(ac)\n        self._elapsed_steps += 1\n        if self._elapsed_steps >= self._max_episode_steps:\n            done = True\n            info[""TimeLimit.truncated""] = True\n        return observation, reward, done, info\n\n    # pylint: disable=method-hidden\n    def reset(self, **kwargs):\n        self._elapsed_steps = 0\n        return self.env.reset(**kwargs)\n\n\nclass NoopResetEnv(gym.Wrapper):\n    def __init__(self, env, noop_max=30):\n        """"""Sample initial states by taking random number of no-ops on reset.\n        No-op is assumed to be action 0.\n        """"""\n        gym.Wrapper.__init__(self, env)\n        self.noop_max = noop_max\n        self.override_num_noops = None\n        self.noop_action = 0\n        assert env.unwrapped.get_action_meanings()[0] == ""NOOP""\n\n    # pylint: disable=method-hidden\n    def reset(self, **kwargs):\n        """""" Do no-op action for a number of steps in [1, noop_max].""""""\n        self.env.reset(**kwargs)\n        if self.override_num_noops is not None:\n            noops = self.override_num_noops\n        else:\n            noops = self.unwrapped.np_random.randint(\n                1, self.noop_max + 1\n            )  # pylint: disable=E1101\n        assert noops > 0\n        obs = None\n        for _ in range(noops):\n            obs, _, done, _ = self.env.step(self.noop_action)\n            if done:\n                obs = self.env.reset(**kwargs)\n        return obs\n\n    # pylint: disable=method-hidden\n    def step(self, ac):\n        return self.env.step(ac)\n\n\nclass FireResetEnv(gym.Wrapper):\n    def __init__(self, env):\n        """"""Take action on reset for environments that are fixed until firing.""""""\n        gym.Wrapper.__init__(self, env)\n        assert env.unwrapped.get_action_meanings()[1] == ""FIRE""\n        assert len(env.unwrapped.get_action_meanings()) >= 3\n\n    # pylint: disable=method-hidden\n    def reset(self, **kwargs):\n        self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(1)\n        if done:\n            self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(2)\n        if done:\n            self.env.reset(**kwargs)\n        return obs\n\n    # pylint: disable=method-hidden\n    def step(self, ac):\n        return self.env.step(ac)\n\n\nclass EpisodicLifeEnv(gym.Wrapper):\n    def __init__(self, env):\n        """"""Make end-of-life == end-of-episode, but only reset on true game over.\n        Done by DeepMind for the DQN and co. since it helps value estimation.\n        """"""\n        gym.Wrapper.__init__(self, env)\n        self.lives = 0\n        self.was_real_done = True\n\n    # pylint: disable=method-hidden\n    def step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        self.was_real_done = done\n        # check current lives, make loss of life terminal,\n        # then update lives to handle bonus lives\n        lives = self.env.unwrapped.ale.lives()\n        if 0 < lives < self.lives:\n            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n            # so it\'s important to keep lives > 0, so that we only reset once\n            # the environment advertises done.\n            done = True\n        self.lives = lives\n        return obs, reward, done, info\n\n    # pylint: disable=method-hidden\n    def reset(self, **kwargs):\n        """"""Reset only when lives are exhausted.\n        This way all states are still reachable even though lives are episodic,\n        and the learner need not know about any of this behind-the-scenes.\n        """"""\n        if self.was_real_done:\n            obs = self.env.reset(**kwargs)\n        else:\n            # no-op step to advance from terminal/lost life state\n            obs, _, _, _ = self.env.step(0)\n        self.lives = self.env.unwrapped.ale.lives()\n        return obs\n\n\nclass MaxAndSkipEnv(gym.Wrapper):\n    def __init__(self, env, skip=4):\n        """"""Return only every `skip`-th frame""""""\n        gym.Wrapper.__init__(self, env)\n        # most recent raw observations (for max pooling across time steps)\n        self._obs_buffer = np.zeros((2,) + env.observation_space.shape, dtype=np.uint8)\n        self._skip = skip\n\n    # pylint: disable=method-hidden\n    def step(self, action):\n        """"""Repeat action, sum reward, and max over last observations.""""""\n        total_reward = 0.0\n        done = None\n        for i in range(self._skip):\n            obs, reward, done, info = self.env.step(action)\n            if i == self._skip - 2:\n                self._obs_buffer[0] = obs\n            if i == self._skip - 1:\n                self._obs_buffer[1] = obs\n            total_reward += reward\n            if done:\n                break\n        # Note that the observation on the done=True frame\n        # doesn\'t matter\n        max_frame = self._obs_buffer.max(axis=0)\n\n        return max_frame, total_reward, done, info\n\n    # pylint: disable=method-hidden\n    def reset(self, **kwargs):\n        return self.env.reset(**kwargs)\n\n\nclass ClipRewardEnv(gym.RewardWrapper):\n    def __init__(self, env):\n        gym.RewardWrapper.__init__(self, env)\n\n    def reward(self, reward):\n        """"""Bin reward to {+1, 0, -1} by its sign.""""""\n        return np.sign(reward)\n\n\nclass WarpFrame(gym.ObservationWrapper):\n    def __init__(self, env, width=84, height=84, grayscale=True):\n        """"""Warp frames to 84x84 as done in the Nature paper and later work.""""""\n        gym.ObservationWrapper.__init__(self, env)\n        self.width = width\n        self.height = height\n        self.grayscale = grayscale\n        if self.grayscale:\n            self.observation_space = spaces.Box(\n                low=0, high=255, shape=(self.height, self.width, 1), dtype=np.uint8\n            )\n        else:\n            self.observation_space = spaces.Box(\n                low=0, high=255, shape=(self.height, self.width, 3), dtype=np.uint8\n            )\n\n    def observation(self, frame):\n        if self.grayscale:\n            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n        frame = cv2.resize(\n            frame, (self.width, self.height), interpolation=cv2.INTER_AREA\n        )\n        if self.grayscale:\n            frame = np.expand_dims(frame, -1)\n        return frame\n\n\nclass FrameStack(gym.Wrapper):\n    def __init__(self, env, k):\n        """"""Stack k last frames.\n\n        Returns lazy array, which is much more memory efficient.\n\n        See Also\n        --------\n        baselines.common.atari_wrappers.LazyFrames\n        """"""\n        gym.Wrapper.__init__(self, env)\n        self.k = k\n        self.frames = deque([], maxlen=k)\n        shp = env.observation_space.shape\n        self.observation_space = spaces.Box(\n            low=0,\n            high=255,\n            shape=(shp[:-1] + (shp[-1] * k,)),\n            dtype=env.observation_space.dtype,\n        )\n\n    # pylint: disable=method-hidden\n    def reset(self):\n        ob = self.env.reset()\n        for _ in range(self.k):\n            self.frames.append(ob)\n        return self._get_ob()\n\n    # pylint: disable=method-hidden\n    def step(self, action):\n        ob, reward, done, info = self.env.step(action)\n        self.frames.append(ob)\n        return self._get_ob(), reward, done, info\n\n    def _get_ob(self):\n        assert len(self.frames) == self.k\n        return LazyFrames(list(self.frames))\n\n\nclass ScaledFloatFrame(gym.ObservationWrapper):\n    def __init__(self, env):\n        gym.ObservationWrapper.__init__(self, env)\n        self.observation_space = gym.spaces.Box(\n            low=0, high=1, shape=env.observation_space.shape, dtype=np.float32\n        )\n\n    def observation(self, observation):\n        # careful! This undoes the memory optimization, use\n        # with smaller replay buffers only.\n        return np.array(observation).astype(np.float32) / 255.0\n\n\nclass LazyFrames:\n    def __init__(self, frames):\n        """"""This object ensures that common frames between the observations are only stored once.\n        It exists purely to optimize memory usage which can be huge for DQN\'s 1M frames replay\n        buffers.\n\n        This object should only be converted to numpy array before being passed to the model.\n\n        You\'d not believe how complex the previous solution was.""""""\n        self._frames = frames\n        self._out = None\n\n    def _force(self):\n        if self._out is None:\n            self._out = np.concatenate(self._frames, axis=-1)\n            self._frames = None\n        return self._out\n\n    def __array__(self, dtype=None):\n        out = self._force()\n        if dtype is not None:\n            out = out.astype(dtype)\n        return out\n\n    def __len__(self):\n        return len(self._force())\n\n    def __getitem__(self, i):\n        return self._force()[i]\n\n\nclass ImageToPyTorch(gym.ObservationWrapper):\n    """"""\n    Image shape to num_channels x weight x height\n    """"""\n\n    def __init__(self, env):\n        super(ImageToPyTorch, self).__init__(env)\n        old_shape = self.observation_space.shape\n        self.observation_space = gym.spaces.Box(\n            low=0.0,\n            high=1.0,\n            shape=(old_shape[-1], old_shape[0], old_shape[1]),\n            dtype=np.uint8,\n        )\n\n    def observation(self, observation):\n        return np.swapaxes(observation, 2, 0)\n\n\ndef make_atari(env_id, max_episode_steps=None):\n    env = gym.make(env_id)\n    assert ""NoFrameskip"" in env.spec.id\n    env = NoopResetEnv(env, noop_max=30)\n    env = MaxAndSkipEnv(env, skip=4)\n    if max_episode_steps is not None:\n        env = TimeLimit(env, max_episode_steps=max_episode_steps)\n    return env\n\n\ndef wrap_deepmind(\n    env, episode_life=True, clip_rewards=True, frame_stack=False, scale=False\n):\n    """"""Configure environment for DeepMind-style Atari.\n    """"""\n    if episode_life:\n        env = EpisodicLifeEnv(env)\n    if ""FIRE"" in env.unwrapped.get_action_meanings():\n        env = FireResetEnv(env)\n    env = WarpFrame(env)\n    if scale:\n        env = ScaledFloatFrame(env)\n    if clip_rewards:\n        env = ClipRewardEnv(env)\n    if frame_stack:\n        env = FrameStack(env, 4)\n    return env\n\n\ndef wrap_pytorch(env):\n    return ImageToPyTorch(env)\n\n\ndef atari_env_generator(env_id, max_episode_steps=None, frame_stack=True, scale=False):\n    env = make_atari(env_id, max_episode_steps)\n    env = wrap_deepmind(env, frame_stack=frame_stack, scale=scale)\n    env = wrap_pytorch(env)\n    return env\n'"
rl_algorithms/common/env/multiprocessing_env.py,0,"b'""""""This code is taken from openai baseline\nhttps://github.com/openai/baselines/tree/master/baselines/common/vec_env\n""""""\n\nfrom abc import ABC, abstractmethod\nfrom multiprocessing import Pipe, Process\n\nimport numpy as np\n\n\ndef tile_images(img_nhwc):\n    """"""\n    Tile N images into one big PxQ image\n    (P,Q) are chosen to be as close as possible, and if N\n    is square, then P=Q.\n    input: img_nhwc, list or array of images, ndim=4 once turned into array\n        n = batch index, h = height, w = width, c = channel\n    returns:\n        bigim_HWc, ndarray with ndim=3\n    """"""\n    img_nhwc = np.asarray(img_nhwc)\n    N, h, w, c = img_nhwc.shape\n    H = int(np.ceil(np.sqrt(N)))\n    W = int(np.ceil(float(N) / H))\n    img_nhwc = np.array(list(img_nhwc) + [img_nhwc[0] * 0 for _ in range(N, H * W)])\n    img_HWhwc = img_nhwc.reshape((H, W, h, w, c))\n    img_HhWwc = img_HWhwc.transpose(0, 2, 1, 3, 4)\n    img_Hh_Ww_c = img_HhWwc.reshape((H * h, W * w, c))\n    return img_Hh_Ww_c\n\n\ndef worker(remote, parent_remote, env_fn_wrapper):\n    parent_remote.close()\n    env = env_fn_wrapper.x\n    try:\n        while True:\n            cmd, data = remote.recv()\n            if cmd == ""step"":\n                ob, reward, done, info = env.step(data)\n                if done:\n                    ob = env.reset()\n                remote.send((ob, reward, done, info))\n            elif cmd == ""reset"":\n                ob = env.reset()\n                remote.send(ob)\n            elif cmd == ""render"":\n                remote.send(env.render(mode=""rgb_array""))\n            elif cmd == ""sample"":\n                remote.send(env.action_space.sample())\n            elif cmd == ""close"":\n                remote.close()\n                break\n            elif cmd == ""get_spaces"":\n                remote.send((env.observation_space, env.action_space))\n            else:\n                raise NotImplementedError\n    except KeyboardInterrupt:\n        print(""SubprocVecEnv worker: got KeyboardInterrupt"")\n    finally:\n        env.close()\n\n\nclass VecEnv(ABC):\n    """"""An abstract asynchronous, vectorized environment.\n\n    Used to batch data from multiple copies of an environment, so that\n    each observation becomes an batch of observations, and expected action\n    is a batch of actions to be applied per-environment.\n    """"""\n\n    closed = False\n    viewer = None\n\n    metadata = {""render.modes"": [""human"", ""rgb_array""]}\n\n    def __init__(self, num_envs, observation_space, action_space):\n        self.num_envs = num_envs\n        self.observation_space = observation_space\n        self.action_space = action_space\n\n    @abstractmethod\n    def reset(self):\n        """"""\n        Reset all the environments and return an array of\n        observations, or a dict of observation arrays.\n        If step_async is still doing work, that work will\n        be cancelled and step_wait() should not be called\n        until step_async() is invoked again.\n        """"""\n        raise NotImplementedError\n\n    @abstractmethod\n    def step_async(self, actions):\n        """"""\n        Tell all the environments to start taking a step\n        with the given actions.\n        Call step_wait() to get the results of the step.\n        You should not call this if a step_async run is\n        already pending.\n        """"""\n        raise NotImplementedError\n\n    @abstractmethod\n    def step_wait(self):\n        """"""\n        Wait for the step taken with step_async().\n        Returns (obs, rews, dones, infos):\n         - obs: an array of observations, or a dict of\n                arrays of observations.\n         - rews: an array of rewards\n         - dones: an array of ""episode done"" booleans\n         - infos: a sequence of info objects\n        """"""\n        raise NotImplementedError\n\n    def close_extras(self):\n        """"""\n        Clean up the  extra resources, beyond what\'s in this base class.\n        Only runs when not self.closed.\n        """"""\n        raise NotImplementedError\n\n    def close(self):\n        if self.closed:\n            return\n        if self.viewer is not None:\n            self.viewer.close()\n        self.close_extras()\n        self.closed = True\n\n    def step(self, actions):\n        """"""\n        Step the environments synchronously.\n        This is available for backwards compatibility.\n        """"""\n        self.step_async(actions)\n        return self.step_wait()\n\n    def render(self, mode=""human""):\n        """"""Render.""""""\n        imgs = self.get_images()\n        bigimg = tile_images(imgs)\n        if mode == ""human"":\n            self.get_viewer().imshow(bigimg)\n            return self.get_viewer().isopen\n        elif mode == ""rgb_array"":\n            return bigimg\n        else:\n            raise NotImplementedError\n\n    def get_images(self):\n        """"""\n        Return RGB images from each environment\n        """"""\n        raise NotImplementedError\n\n    @property\n    def unwrapped(self):\n        if isinstance(self, VecEnvWrapper):\n            return self.venv.unwrapped\n        else:\n            return self\n\n    def get_viewer(self):\n        if self.viewer is None:\n            from gym.envs.classic_control import rendering\n\n            self.viewer = rendering.SimpleImageViewer()\n        return self.viewer\n\n\nclass VecEnvWrapper(VecEnv):\n    """"""\n    An environment wrapper that applies to an entire batch\n    of environments at once.\n    """"""\n\n    def __init__(self, venv, observation_space=None, action_space=None):\n        self.venv = venv\n        VecEnv.__init__(\n            self,\n            num_envs=venv.num_envs,\n            observation_space=observation_space or venv.observation_space,\n            action_space=action_space or venv.action_space,\n        )\n\n    def step_async(self, actions):\n        self.venv.step_async(actions)\n\n    @abstractmethod\n    def reset(self):\n        pass\n\n    @abstractmethod\n    def step_wait(self):\n        pass\n\n    def close(self):\n        return self.venv.close()\n\n    def render(self, mode=""human""):\n        return self.venv.render(mode=mode)\n\n    def get_images(self):\n        return self.venv.get_images()\n\n\nclass CloudpickleWrapper:\n    """"""Uses cloudpickle to serialize contents.\n\n    (otherwise multiprocessing tries to use pickle)\n    """"""\n\n    def __init__(self, x):\n        self.x = x\n\n    def __getstate__(self):\n        import cloudpickle\n\n        return cloudpickle.dumps(self.x)\n\n    def __setstate__(self, ob):\n        import pickle\n\n        self.x = pickle.loads(ob)\n\n\nclass SubprocVecEnv(VecEnv):\n    """"""VecEnv that runs multiple environments in parallel in subproceses\n    and communicates with them via pipes.\n    Recommended to use when num_envs > 1 and step() can be a bottleneck.\n    """"""\n\n    def __init__(self, env_fns):\n        """"""\n        Arguments:\n        env_fns: iterable of callables -  functions that create environments to run\n        in subprocesses. Need to be cloud-pickleable\n        """"""\n        self.waiting = False\n        self.closed = False\n        nenvs = len(env_fns)\n        self.remotes, self.work_remotes = zip(*[Pipe() for _ in range(nenvs)])\n        self.ps = [\n            Process(\n                target=worker, args=(work_remote, remote, CloudpickleWrapper(env_fn))\n            )\n            for (work_remote, remote, env_fn) in zip(\n                self.work_remotes, self.remotes, env_fns\n            )\n        ]\n        for p in self.ps:\n            p.daemon = (\n                True  # if the main process crashes, we should not cause things to hang\n            )\n            p.start()\n        for remote in self.work_remotes:\n            remote.close()\n\n        self.remotes[0].send((""get_spaces"", None))\n        observation_space, action_space = self.remotes[0].recv()\n        self.viewer = None\n        VecEnv.__init__(self, len(env_fns), observation_space, action_space)\n\n    def step_async(self, actions):\n        self._assert_not_closed()\n        for remote, action in zip(self.remotes, actions):\n            remote.send((""step"", action))\n        self.waiting = True\n\n    def step_wait(self):\n        self._assert_not_closed()\n        results = [remote.recv() for remote in self.remotes]\n        self.waiting = False\n        obs, rews, dones, infos = zip(*results)\n        return np.stack(obs), np.stack(rews), np.stack(dones), infos\n\n    def reset(self):\n        self._assert_not_closed()\n        for remote in self.remotes:\n            remote.send((""reset"", None))\n        return np.stack([remote.recv() for remote in self.remotes])\n\n    def sample(self):\n        self._assert_not_closed()\n        for remote in self.remotes:\n            remote.send((""sample"", None))\n        return np.stack([remote.recv() for remote in self.remotes])\n\n    def close_extras(self):\n        self.closed = True\n        if self.waiting:\n            for remote in self.remotes:\n                remote.recv()\n        for remote in self.remotes:\n            remote.send((""close"", None))\n        for p in self.ps:\n            p.join()\n\n    def get_images(self):\n        self._assert_not_closed()\n        for pipe in self.remotes:\n            pipe.send((""render"", None))\n        imgs = [pipe.recv() for pipe in self.remotes]\n        return imgs\n\n    def _assert_not_closed(self):\n        assert (\n            not self.closed\n        ), ""Trying to operate on a SubprocVecEnv after calling close()""\n'"
rl_algorithms/common/env/normalizers.py,0,"b'# -*- coding: utf-8 -*-\n""""""Collection of normalizers.\n\n- Author: Curt Park\n- Contact: curt.park@medipixel.io\n""""""\n\nimport gym\nimport numpy as np\n\n\nclass ActionNormalizer(gym.ActionWrapper):\n    """"""Rescale and relocate the actions.""""""\n\n    def action(self, action: np.ndarray) -> np.ndarray:\n        """"""Change the range (-1, 1) to (low, high).""""""\n        low = self.action_space.low\n        high = self.action_space.high\n\n        scale_factor = (high - low) / 2\n        reloc_factor = high - scale_factor\n\n        action = action * scale_factor + reloc_factor\n        action = np.clip(action, low, high)\n\n        return action\n\n    def reverse_action(self, action: np.ndarray) -> np.ndarray:\n        """"""Change the range (low, high) to (-1, 1).""""""\n        low = self.action_space.low\n        high = self.action_space.high\n\n        scale_factor = (high - low) / 2\n        reloc_factor = high - scale_factor\n\n        action = (action - reloc_factor) / scale_factor\n        action = np.clip(action, -1.0, 1.0)\n\n        return action\n'"
rl_algorithms/common/env/utils.py,0,"b'# -*- coding: utf-8 -*-\n""""""Util functions for env.\n\n- Author: Curt Park\n- Contact: curt.park@medipixel.io\n""""""\n\nimport argparse\nfrom typing import Callable, List\n\nimport gym\nfrom gym.spaces import Discrete\n\nfrom rl_algorithms.common.env.multiprocessing_env import SubprocVecEnv\nfrom rl_algorithms.common.env.normalizers import ActionNormalizer\n\n\ndef set_env(\n    env: gym.Env, args: argparse.Namespace, env_wrappers: List[gym.Wrapper] = None\n) -> gym.Env:\n    """"""Set environment according to user\'s config.""""""\n    if args.max_episode_steps > 0:\n        env._max_episode_steps = args.max_episode_steps\n    else:\n        args.max_episode_steps = env._max_episode_steps\n\n    if not isinstance(env.action_space, Discrete):\n        env = ActionNormalizer(env)\n\n    if env_wrappers:\n        for env_wrapper in env_wrappers:\n            env = env_wrapper(env)\n\n    return env\n\n\ndef env_generator(\n    env_name: str, args: argparse.Namespace, env_wrappers: List[gym.Wrapper] = None\n) -> Callable:\n    """"""Return env creating function (with normalizers).""""""\n\n    def _thunk(rank: int):\n        env = gym.make(env_name)\n        env.seed(args.seed + rank + 1)\n        env = set_env(env, args, env_wrappers)\n        return env\n\n    return _thunk\n\n\ndef make_envs(env_gen: Callable, n_envs: int = 8) -> SubprocVecEnv:\n    """"""Make multiple environments running on multiprocssors.""""""\n    envs = [env_gen(i) for i in range(n_envs)]\n    subproc_env = SubprocVecEnv(envs)\n    return subproc_env\n'"
rl_algorithms/common/networks/__init__.py,0,"b'""""""Empty.""""""\n'"
rl_algorithms/common/networks/brain.py,4,"b'# -*- coding: utf-8 -*-\n""""""Brain module for backbone & head holder.\n\n- Authors: Euijin Jeong & Kyunghwan Kim\n- Contacts: euijin.jeong@medipixel.io\n            kh.kim@medipixel.io\n""""""\n\nimport torch\nimport torch.nn as nn\n\nfrom rl_algorithms.common.helper_functions import identity\nfrom rl_algorithms.dqn.networks import IQNMLP\nfrom rl_algorithms.registry import build_backbone, build_head\nfrom rl_algorithms.utils.config import ConfigDict\n\n\nclass Brain(nn.Module):\n    """"""Class for holding backbone and head networks.""""""\n\n    def __init__(\n        self, backbone_cfg: ConfigDict, head_cfg: ConfigDict,\n    ):\n        """"""Initialize.""""""\n        super(Brain, self).__init__()\n        if not backbone_cfg:\n            self.backbone = identity\n            head_cfg.configs.input_size = head_cfg.configs.state_size[0]\n        else:\n            self.backbone = build_backbone(backbone_cfg)\n            head_cfg.configs.input_size = self.calculate_fc_input_size(\n                head_cfg.configs.state_size\n            )\n        self.head = build_head(head_cfg)\n\n    def forward(self, x: torch.Tensor):\n        """"""Forward method implementation. Use in get_action method in agent.""""""\n        x = self.backbone(x)\n        x = self.head(x)\n\n        return x\n\n    def forward_(self, x: torch.Tensor, n_tau_samples: int = None):\n        """"""Get output value for calculating loss.""""""\n        x = self.backbone(x)\n        if isinstance(self.head, IQNMLP):\n            x = self.head.forward_(x, n_tau_samples)\n        else:\n            x = self.head.forward_(x)\n        return x\n\n    def calculate_fc_input_size(self, state_dim: tuple):\n        """"""Calculate fc input size according to the shape of cnn.""""""\n        x = torch.zeros(state_dim).unsqueeze(0)\n        output = self.backbone(x).detach().view(-1)\n        return output.shape[0]\n'"
rl_algorithms/common/networks/heads.py,14,"b'# -*- coding: utf-8 -*-\n""""""MLP module for model of algorithms\n\n- Author: Kyunghwan Kim\n- Contact: kh.kim@medipixel.io\n""""""\n\nfrom typing import Callable, Tuple\n\nimport torch\nfrom torch.distributions import Normal\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom rl_algorithms.common.helper_functions import identity\nfrom rl_algorithms.registry import HEADS\nfrom rl_algorithms.utils.config import ConfigDict\n\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n\ndef init_layer_uniform(layer: nn.Linear, init_w: float = 3e-3) -> nn.Linear:\n    """"""Init uniform parameters on the single layer""""""\n    layer.weight.data.uniform_(-init_w, init_w)\n    layer.bias.data.uniform_(-init_w, init_w)\n\n    return layer\n\n\n@HEADS.register_module\nclass MLP(nn.Module):\n    """"""Baseline of Multilayer perceptron.\n\n    Attributes:\n        input_size (int): size of input\n        output_size (int): size of output layer\n        hidden_sizes (list): sizes of hidden layers\n        hidden_activation (function): activation function of hidden layers\n        output_activation (function): activation function of output layer\n        hidden_layers (list): list containing linear layers\n        use_output_layer (bool): whether or not to use the last layer\n        n_category (int): category number (-1 if the action is continuous)\n\n    """"""\n\n    def __init__(\n        self,\n        configs: ConfigDict,\n        hidden_activation: Callable = F.relu,\n        linear_layer: nn.Module = nn.Linear,\n        use_output_layer: bool = True,\n        n_category: int = -1,\n        init_fn: Callable = init_layer_uniform,\n    ):\n        """"""Initialize.""""""\n        super(MLP, self).__init__()\n\n        self.hidden_sizes = configs.hidden_sizes\n        self.input_size = configs.input_size\n        self.output_size = configs.output_size\n        self.hidden_activation = hidden_activation\n        self.output_activation = configs.output_activation\n        self.linear_layer = linear_layer\n        self.use_output_layer = use_output_layer\n        self.n_category = n_category\n\n        # set hidden layers\n        self.hidden_layers: list = []\n        in_size = self.input_size\n        for i, next_size in enumerate(configs.hidden_sizes):\n            fc = self.linear_layer(in_size, next_size)\n            in_size = next_size\n            self.__setattr__(""hidden_fc{}"".format(i), fc)\n            self.hidden_layers.append(fc)\n\n        # set output layers\n        if self.use_output_layer:\n            self.output_layer = self.linear_layer(in_size, configs.output_size)\n            self.output_layer = init_fn(self.output_layer)\n        else:\n            self.output_layer = identity\n            self.output_activation = identity\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """"""Forward method implementation.""""""\n        for hidden_layer in self.hidden_layers:\n            x = self.hidden_activation(hidden_layer(x))\n        x = self.output_activation(self.output_layer(x))\n\n        return x\n\n\n@HEADS.register_module\nclass GaussianDist(MLP):\n    """"""Multilayer perceptron with Gaussian distribution output.\n\n    Attributes:\n        mu_activation (function): bounding function for mean\n        log_std_min (float): lower bound of log std\n        log_std_max (float): upper bound of log std\n        mu_layer (nn.Linear): output layer for mean\n        log_std_layer (nn.Linear): output layer for log std\n    """"""\n\n    def __init__(\n        self,\n        configs: ConfigDict,\n        hidden_activation: Callable = F.relu,\n        mu_activation: Callable = torch.tanh,\n        log_std_min: float = -20,\n        log_std_max: float = 2,\n        init_fn: Callable = init_layer_uniform,\n    ):\n        """"""Initialize.""""""\n        super(GaussianDist, self).__init__(\n            configs=configs,\n            hidden_activation=hidden_activation,\n            use_output_layer=False,\n        )\n\n        self.mu_activation = mu_activation\n        self.log_std_min = log_std_min\n        self.log_std_max = log_std_max\n        in_size = configs.hidden_sizes[-1]\n\n        # set log_std layer\n        self.log_std_layer = nn.Linear(in_size, configs.output_size)\n        self.log_std_layer = init_fn(self.log_std_layer)\n\n        # set mean layer\n        self.mu_layer = nn.Linear(in_size, configs.output_size)\n        self.mu_layer = init_fn(self.mu_layer)\n\n    def get_dist_params(self, x: torch.Tensor) -> Tuple[torch.Tensor, ...]:\n        """"""Return gausian distribution parameters.""""""\n        hidden = super(GaussianDist, self).forward(x)\n\n        # get mean\n        mu = self.mu_activation(self.mu_layer(hidden))\n\n        # get std\n        log_std = torch.tanh(self.log_std_layer(hidden))\n        log_std = self.log_std_min + 0.5 * (self.log_std_max - self.log_std_min) * (\n            log_std + 1\n        )\n        std = torch.exp(log_std)\n\n        return mu, log_std, std\n\n    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, ...]:\n        """"""Forward method implementation.""""""\n        mu, _, std = self.get_dist_params(x)\n\n        # get normal distribution and action\n        dist = Normal(mu, std)\n        action = dist.sample()\n\n        return action, dist\n\n\n@HEADS.register_module\nclass TanhGaussianDistParams(GaussianDist):\n    """"""Multilayer perceptron with Gaussian distribution output.""""""\n\n    def __init__(self, **kwargs):\n        """"""Initialize.""""""\n        super(TanhGaussianDistParams, self).__init__(**kwargs, mu_activation=identity)\n\n    def forward(\n        self, x: torch.Tensor, epsilon: float = 1e-6\n    ) -> Tuple[torch.Tensor, ...]:\n        """"""Forward method implementation.""""""\n        mu, _, std = super(TanhGaussianDistParams, self).get_dist_params(x)\n\n        # sampling actions\n        dist = Normal(mu, std)\n        z = dist.rsample()\n\n        # normalize action and log_prob\n        # see appendix C of \'https://arxiv.org/pdf/1812.05905.pdf\'\n        action = torch.tanh(z)\n        log_prob = dist.log_prob(z) - torch.log(1 - action.pow(2) + epsilon)\n        log_prob = log_prob.sum(-1, keepdim=True)\n\n        return action, log_prob, z, mu, std\n'"
rl_algorithms/common/networks/backbones/__init__.py,0,"b'from rl_algorithms.common.networks.backbones.cnn import CNN\nfrom rl_algorithms.common.networks.backbones.resnet import ResNet\n\n__all__ = [\n    ""CNN"",\n    ""ResNet"",\n]\n'"
rl_algorithms/common/networks/backbones/cnn.py,4,"b'# -*- coding: utf-8 -*-\n""""""CNN modules for RL algorithms.\n\n- Authors: Kyunghwan Kim & Curt Park\n- Contacts: kh.kim@medipixel.io\n            curt.park@medipixel.io\n""""""\n\nfrom typing import Callable\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom rl_algorithms.common.helper_functions import identity\nfrom rl_algorithms.registry import BACKBONES\nfrom rl_algorithms.utils.config import ConfigDict\n\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n\nclass CNNLayer(nn.Module):\n    def __init__(\n        self,\n        input_size: int,\n        output_size: int,\n        kernel_size: int,\n        stride: int = 1,\n        padding: int = 0,\n        pre_activation_fn: Callable = identity,\n        activation_fn: Callable = F.relu,\n        post_activation_fn: Callable = identity,\n    ):\n        super(CNNLayer, self).__init__()\n\n        self.cnn = nn.Conv2d(\n            input_size,\n            output_size,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n        )\n\n        self.pre_activation_fn = pre_activation_fn\n        self.activation_fn = activation_fn\n        self.post_activation_fn = post_activation_fn\n\n    def forward(self, x):\n        x = self.cnn(x)\n        x = self.pre_activation_fn(x)\n        x = self.activation_fn(x)\n        x = self.post_activation_fn(x)\n\n        return x\n\n\n@BACKBONES.register_module\nclass CNN(nn.Module):\n    """"""Baseline of Convolution neural network.""""""\n\n    def __init__(self, configs: ConfigDict):\n        super(CNN, self).__init__()\n\n        cnn_layers = list(map(CNNLayer, *configs.values()))\n        self.cnn = nn.Sequential()\n        for i, cnn_layer in enumerate(cnn_layers):\n            self.cnn.add_module(""cnn_{}"".format(i), cnn_layer)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        """"""Forward method implementation.""""""\n        if len(x.size()) == 3:\n            x = x.unsqueeze(0)\n        x = self.cnn(x)\n        x = x.view(x.size(0), -1)\n        return x\n'"
rl_algorithms/common/networks/backbones/resnet.py,4,"b'# -*- coding: utf-8 -*-\n""""""ResNet modules for RL algorithms.\n\n- Authors: Minseop Kim & Kyunghwan Kim\n- Contacts: minseop.kim@medipixel.io\n            kh.kim@medipixel.io\n- Paper: https://arxiv.org/pdf/1512.03385.pdf\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom rl_algorithms.registry import BACKBONES\nfrom rl_algorithms.utils.config import ConfigDict\n\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\n\nclass BasicBlock(nn.Module):\n    """"""Basic building block for ResNet.""""""\n\n    def __init__(\n        self, in_planes: int, planes: int, stride: int = 1, expansion: int = 1\n    ):\n        super(BasicBlock, self).__init__()\n\n        self.expansion = expansion\n        self.conv1 = nn.Conv2d(\n            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False,\n        )\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(\n            planes,\n            self.expansion * planes,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            bias=False,\n        )\n        self.bn2 = nn.BatchNorm2d(self.expansion * planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion * planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(\n                    in_planes,\n                    self.expansion * planes,\n                    kernel_size=1,\n                    stride=stride,\n                    bias=False,\n                ),\n                nn.BatchNorm2d(self.expansion * planes),\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass Bottleneck(nn.Module):\n    """"""Bottleneck building block.""""""\n\n    def __init__(\n        self, in_planes: int, planes: int, stride: int = 1, expansion: int = 1\n    ):\n        super(Bottleneck, self).__init__()\n\n        self.expansion = expansion\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False,)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(\n            planes, planes, kernel_size=3, stride=stride, padding=1, bias=False,\n        )\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(\n            planes, self.expansion * planes, kernel_size=1, bias=False,\n        )\n        self.bn3 = nn.BatchNorm2d(self.expansion * planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion * planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(\n                    in_planes,\n                    self.expansion * planes,\n                    kernel_size=1,\n                    stride=stride,\n                    bias=False,\n                ),\n                nn.BatchNorm2d(self.expansion * planes),\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\n@BACKBONES.register_module\nclass ResNet(nn.Module):\n    """"""Baseline of ResNet(https://arxiv.org/pdf/1512.03385.pdf).""""""\n\n    def __init__(\n        self, configs: ConfigDict,\n    ):\n        super(ResNet, self).__init__()\n        block = Bottleneck if configs.use_bottleneck else BasicBlock\n        block_outputs = configs.block_output_sizes\n        block_strides = configs.block_strides\n        num_blocks = configs.num_blocks\n        self.expansion = configs.expansion\n        self.in_planes = configs.first_output_size\n        self.conv1 = nn.Conv2d(\n            configs.first_input_size,\n            self.in_planes,\n            kernel_size=3,\n            stride=1,\n            padding=1,\n            bias=False,\n        )\n        self.bn1 = nn.BatchNorm2d(self.in_planes)\n        self.layer1 = self._make_layer(\n            block, block_outputs[0], num_blocks[0], block_strides[0]\n        )\n        self.layer2 = self._make_layer(\n            block, block_outputs[1], num_blocks[1], block_strides[1]\n        )\n        self.layer3 = self._make_layer(\n            block, block_outputs[2], num_blocks[2], block_strides[2]\n        )\n        self.layer4 = self._make_layer(\n            block, block_outputs[3], num_blocks[3], block_strides[3]\n        )\n        self.conv_out = nn.Conv2d(\n            block_outputs[3] * self.expansion,\n            block_outputs[3] // configs.channel_compression,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=False,\n        )\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1] * (num_blocks - 1)\n        layers = []\n        for stride_ in strides:\n            layers.append(block(self.in_planes, planes, stride_, self.expansion))\n            self.in_planes = planes * self.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x: torch.Tensor):\n        if len(x.size()) == 3:\n            x = x.unsqueeze(0)\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.conv_out(x)\n        x = x.view(x.size(0), -1)\n        return x\n'"
