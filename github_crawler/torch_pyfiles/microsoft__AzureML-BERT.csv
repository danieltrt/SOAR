file_path,api_count,code
finetune/evaluate_squad.py,0,"b'"""""" Official evaluation script for v1.1 of the SQuAD dataset. """"""\nfrom __future__ import print_function\nfrom collections import Counter\nimport string\nimport re\nimport argparse\nimport json\nimport sys\n\n\ndef normalize_answer(s):\n    """"""Lower text and remove punctuation, articles and extra whitespace.""""""\n    def remove_articles(text):\n        return re.sub(r\'\\b(a|an|the)\\b\', \' \', text)\n\n    def white_space_fix(text):\n        return \' \'.join(text.split())\n\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return \'\'.join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\n\ndef f1_score(prediction, ground_truth):\n    prediction_tokens = normalize_answer(prediction).split()\n    ground_truth_tokens = normalize_answer(ground_truth).split()\n    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n    num_same = sum(common.values())\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(prediction_tokens)\n    recall = 1.0 * num_same / len(ground_truth_tokens)\n    f1 = (2 * precision * recall) / (precision + recall)\n    return f1\n\n\ndef exact_match_score(prediction, ground_truth):\n    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n\n\ndef metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n    scores_for_ground_truths = []\n    for ground_truth in ground_truths:\n        score = metric_fn(prediction, ground_truth)\n        scores_for_ground_truths.append(score)\n    return max(scores_for_ground_truths)\n\n\ndef evaluate(dataset, predictions):\n    f1 = exact_match = total = 0\n    for article in dataset:\n        for paragraph in article[\'paragraphs\']:\n            for qa in paragraph[\'qas\']:\n                total += 1\n                if qa[\'id\'] not in predictions:\n                    message = \'Unanswered question \' + qa[\'id\'] + \\\n                              \' will receive score 0.\'\n                    print(message, file=sys.stderr)\n                    continue\n                ground_truths = list(map(lambda x: x[\'text\'], qa[\'answers\']))\n                prediction = predictions[qa[\'id\']]\n                exact_match += metric_max_over_ground_truths(\n                    exact_match_score, prediction, ground_truths)\n                f1 += metric_max_over_ground_truths(\n                    f1_score, prediction, ground_truths)\n\n    exact_match = 100.0 * exact_match / total\n    f1 = 100.0 * f1 / total\n\n    return {\'exact_match\': exact_match, \'f1\': f1}\n\n\nif __name__ == \'__main__\':\n    expected_version = \'1.1\'\n    parser = argparse.ArgumentParser(\n        description=\'Evaluation for SQuAD \' + expected_version)\n    parser.add_argument(\'dataset_file\', help=\'Dataset file\')\n    parser.add_argument(\'prediction_file\', help=\'Prediction File\')\n    args = parser.parse_args()\n    with open(args.dataset_file) as dataset_file:\n        dataset_json = json.load(dataset_file)\n        if (dataset_json[\'version\'] != expected_version):\n            print(\'Evaluation expects v-\' + expected_version +\n                  \', but got dataset with v-\' + dataset_json[\'version\'],\n                  file=sys.stderr)\n        dataset = dataset_json[\'data\']\n    with open(args.prediction_file) as prediction_file:\n        predictions = json.load(prediction_file)\n    print(json.dumps(evaluate(dataset, predictions)))'"
finetune/run_classifier_azureml.py,43,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""BERT finetuning runner.""""""\n\nfrom __future__ import absolute_import, division, print_function\n\nimport argparse\nimport csv\nimport logging\nimport os\nimport random\nimport sys\n\nimport sys\nsys.path.append(""./pretrain/pytorch/"")\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n                              TensorDataset)\nfrom torch.utils.data.distributed import DistributedSampler\nfrom tqdm import tqdm, trange\nimport torch.distributed as dist\n\nfrom torch.nn import CrossEntropyLoss, MSELoss\nfrom scipy.stats import pearsonr, spearmanr\nfrom sklearn.metrics import matthews_corrcoef, f1_score\n\nfrom azureml_adapter import set_environment_variables_for_nccl_backend, get_local_rank, get_global_size, get_local_size\nfrom pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\nfrom pytorch_pretrained_bert.modeling import BertForSequenceClassification, BertConfig, WEIGHTS_NAME, CONFIG_NAME\nfrom pytorch_pretrained_bert.tokenization import BertTokenizer\nfrom pytorch_pretrained_bert.optimization import BertAdam\nfrom optimization import LinearWarmupExponentialSchedule, warmup_linear\n\n\nfrom azureml.core.run import Run\n\nlogging.basicConfig(format=\'%(asctime)s - %(levelname)s - %(name)s -   %(message)s\',\n                    datefmt=\'%m/%d/%Y %H:%M:%S\',\n                    level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\n\ndef run_evaluation(processor, output_mode, set_type):\n    examples = processor.get_dev_examples(args.data_dir) if set_type == ""dev"" else processor.get_test_examples(args.data_dir)\n    features = convert_examples_to_features(\n                    examples, processor.get_labels(), args.max_seq_length, tokenizer, output_mode)\n    logger.info(f"" Running Evaluation on {set_type}"")\n    logger.info(""  Num examples = %d"", len(examples))\n    logger.info(""  Batch size = %d"", args.eval_batch_size)\n    all_input_ids = torch.tensor(\n        [f.input_ids for f in features], dtype=torch.long)\n    all_input_mask = torch.tensor(\n        [f.input_mask for f in features], dtype=torch.long)\n    all_segment_ids = torch.tensor(\n        [f.segment_ids for f in features], dtype=torch.long)\n\n    all_label_ids = None\n    if output_mode == ""classification"":\n        all_label_ids = torch.tensor(\n            [f.label_id for f in features], dtype=torch.long)\n    elif output_mode == ""regression"":\n        all_label_ids = torch.tensor(\n            [f.label_id for f in features], dtype=torch.float)\n    \n    data = TensorDataset(\n        all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n    # Run prediction for full data\n    eval_sampler = SequentialSampler(data)\n    dataloader = DataLoader(\n        data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n    \n    model.eval()\n    eval_loss = 0\n    nb_eval_steps = 0\n    preds = []\n    \n    for input_ids, input_mask, segment_ids, label_ids in tqdm(dataloader, desc=""Evaluating""):\n        input_ids = input_ids.to(device)\n        input_mask = input_mask.to(device)\n        segment_ids = segment_ids.to(device)\n        label_ids = label_ids.to(device)\n\n        with torch.no_grad():\n            logits = model(input_ids, segment_ids,\n                        input_mask, labels=None)\n\n        # create eval loss and other metric required by the task\n        if set_type == ""dev"":\n            if output_mode == ""classification"":\n                if args.focal:\n                    continue\n                else:\n                    loss_fct = CrossEntropyLoss()\n                tmp_eval_loss = loss_fct(\n                    logits.view(-1, num_labels), label_ids.view(-1))\n            elif output_mode == ""regression"":\n                loss_fct = MSELoss()\n                if args.fp16:\n                    label_ids = label_ids.half()\n                tmp_eval_loss = loss_fct(\n                    logits.view(-1), label_ids.view(-1))\n\n            eval_loss += tmp_eval_loss.mean().item()\n            nb_eval_steps += 1\n\n        if len(preds) == 0:\n            preds.append(logits.detach().cpu().numpy())\n        else:\n            preds[0] = np.append(\n                preds[0], logits.detach().cpu().numpy(), axis=0)\n\n    preds = preds[0]\n    if output_mode == ""classification"":\n        preds = np.argmax(preds, axis=1)\n    elif output_mode == ""regression"":\n        preds = np.squeeze(preds)\n\n\n    if set_type == ""dev"":\n        eval_loss = eval_loss / nb_eval_steps\n        result = compute_metrics(task_name, preds, all_label_ids.numpy())\n        loss = tr_loss/nb_tr_steps if args.do_train else None\n\n        result[\'eval_loss\'] = eval_loss\n        result[\'global_step\'] = global_step\n        result[\'loss\'] = loss\n        logger.info(""***** Evaluation results *****"")\n        for key in sorted(result.keys()):\n            logger.info(""Epoch %s:  %s = %s"", epoch_num,\n                        key, str(result[key]))\n            if(epoch_num  ==2):\n                run.log(key, str(result[key]))\n    if set_type == ""test"":\n        output_eval_file = os.path.join(args.output_dir, f""{task_name.upper()}-{args.seed}-{args.learning_rate}-ep-{epoch_num}-tot-epochs-{args.num_train_epochs}.tsv"")\n        with open(output_eval_file, ""w"") as writer:\n            writer.write(""index\\tprediction\\n"")\n            for i, sample in enumerate(examples):\n                if output_mode == ""classification"":\n                    writer.write(f""{sample.guid}\\t{processor.get_labels()[preds[i].item()]}\\n"")\n                elif output_mode == ""regression"":\n                    writer.write(f""{sample.guid}\\t{preds[i].item()}\\n"")\n\n    return\nclass InputExample(object):\n    """"""A single training/test example for simple sequence classification.""""""\n\n    def __init__(self, guid, text_a, text_b=None, label=None):\n        """"""Constructs a InputExample.\n\n        Args:\n            guid: Unique id for the example.\n            text_a: string. The untokenized text of the first sequence. For single\n            sequence tasks, only this sequence must be specified.\n            text_b: (Optional) string. The untokenized text of the second sequence.\n            Only must be specified for sequence pair tasks.\n            label: (Optional) string. The label of the example. This should be\n            specified for train and dev examples, but not for test examples.\n        """"""\n        self.guid = guid\n        self.text_a = text_a\n        self.text_b = text_b\n        self.label = label\n\n\nclass InputFeatures(object):\n    """"""A single set of features of data.""""""\n\n    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.label_id = label_id\n\n\nclass DataProcessor(object):\n    """"""Base class for data converters for sequence classification data sets.""""""\n\n    def get_train_examples(self, data_dir):\n        """"""Gets a collection of `InputExample`s for the train set.""""""\n        raise NotImplementedError()\n\n    def get_dev_examples(self, data_dir):\n        """"""Gets a collection of `InputExample`s for the dev set.""""""\n        raise NotImplementedError()\n    \n    def get_test_examples(self, data_dir):\n        """"""Gets a collection of `InputExample`s for the test set.""""""\n        raise NotImplementedError()\n\n    def get_labels(self):\n        """"""Gets the list of labels for this data set.""""""\n        raise NotImplementedError()\n\n    @classmethod\n    def _read_tsv(cls, input_file, quotechar=None):\n        """"""Reads a tab separated value file.""""""\n        with open(input_file, ""r"", encoding=\'utf-8\') as f:\n            reader = csv.reader(f, delimiter=""\\t"", quotechar=quotechar)\n            lines = []\n            for line in reader:\n                if sys.version_info[0] == 2:\n                    line = list(unicode(cell, \'utf-8\') for cell in line)\n                lines.append(line)\n            return lines\n\n\nclass MrpcProcessor(DataProcessor):\n    """"""Processor for the MRPC data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        logger.info(""LOOKING AT {}"".format(\n            os.path.join(data_dir, ""train.tsv"")))\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n    \n    def get_test_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""test.tsv"")), ""test"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""0"", ""1""]\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, i) if set_type != ""test"" else line[0] \n            text_a = line[3]\n            text_b = line[4]\n            label = line[0] if set_type != ""test"" else ""0""\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n        return examples\n\n\nclass MnliProcessor(DataProcessor):\n    """"""Processor for the MultiNLI data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev_matched.tsv"")),\n            ""dev_matched"")\n    \n    def get_test_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""test_matched.tsv"")), ""test"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""contradiction"", ""entailment"", ""neutral""]\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, line[0]) if set_type != ""test"" else line[0]\n            text_a = line[8]\n            text_b = line[9]\n            label = line[-1] if set_type != ""test"" else ""entailment""\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n        return examples\n\n\nclass MnliMismatchedProcessor(MnliProcessor):\n    """"""Processor for the MultiNLI Mismatched data set (GLUE version).""""""\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev_mismatched.tsv"")),\n            ""dev_mismatched"")\n    \n    def get_test_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""test_mismatched.tsv"")),\n            ""test"")\n\n\nclass ColaProcessor(DataProcessor):\n    """"""Processor for the CoLA data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n    \n    def get_test_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""test.tsv"")), ""test"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""0"", ""1""]\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0 and set_type == ""test"":\n                continue\n            guid = ""%s-%s"" % (set_type, i) if set_type != ""test"" else line[0]\n            text_a = line[3] if set_type != ""test"" else line[1]\n            label = line[1] if set_type != ""test"" else ""0""\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n        return examples\n\n\nclass Sst2Processor(DataProcessor):\n    """"""Processor for the SST-2 data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n    \n    def get_test_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""test.tsv"")), ""test"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""0"", ""1""]\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, i) if set_type != ""test"" else line[0]\n            text_a = line[0] if set_type != ""test"" else line[1]\n            label = line[1] if set_type != ""test"" else ""0""\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n        return examples\n\n\nclass StsbProcessor(DataProcessor):\n    """"""Processor for the STS-B data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n    \n    def get_test_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""test.tsv"")), ""test"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [None]\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, line[0]) if set_type != ""test"" else line[0]\n            text_a = line[7]\n            text_b = line[8]\n            label = line[-1] if set_type != ""test"" else ""0.0""\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n        return examples\n\n\nclass QqpProcessor(DataProcessor):\n    """"""Processor for the STS-B data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n    \n    def get_test_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""test.tsv"")), ""test"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""0"", ""1""]\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, line[0]) if set_type != ""test"" else line[0]\n            try:\n                text_a = line[3] if set_type != ""test"" else line[1]\n                text_b = line[4] if set_type != ""test"" else line[2]\n                label = line[5] if set_type != ""test"" else ""0""\n            except IndexError:\n                continue\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n        return examples\n\n\nclass QnliProcessor(DataProcessor):\n    """"""Processor for the STS-B data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev.tsv"")),\n            ""dev_matched"")\n    \n    def get_test_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""test.tsv"")), ""test"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""entailment"", ""not_entailment""]\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, line[0]) if set_type != ""test"" else line[0]\n            text_a = line[1]\n            text_b = line[2]\n            label = line[-1] if set_type != ""test"" else ""entailment""\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n        return examples\n\n\nclass RteProcessor(DataProcessor):\n    """"""Processor for the RTE data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n        \n    def get_test_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""test.tsv"")), ""test"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""entailment"", ""not_entailment""]\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, line[0]) if set_type != ""test"" else line[0]\n            text_a = line[1]\n            text_b = line[2]\n            label = line[-1] if set_type != ""test"" else ""entailment""\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n        return examples\n\nclass WnliProcessor(DataProcessor):\n    """"""Processor for the WNLI data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n    \n    def get_test_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""test.tsv"")), ""test"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""0"", ""1""]\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, line[0]) if set_type != ""test"" else line[0]\n            text_a = line[1]\n            text_b = line[2]\n            label = line[-1] if set_type != ""test"" else ""0""\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n        return examples\n\n\ndef convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer, output_mode):\n    """"""Loads a data file into a list of `InputBatch`s.""""""\n\n    label_map = {label: i for i, label in enumerate(label_list)}\n\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        if ex_index % 10000 == 0:\n            logger.info(""Writing example %d of %d"" % (ex_index, len(examples)))\n\n        tokens_a = tokenizer.tokenize(example.text_a)\n\n        tokens_b = None\n        if example.text_b:\n            tokens_b = tokenizer.tokenize(example.text_b)\n            # Modifies `tokens_a` and `tokens_b` in place so that the total\n            # length is less than the specified length.\n            # Account for [CLS], [SEP], [SEP] with ""- 3""\n            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n        else:\n            # Account for [CLS] and [SEP] with ""- 2""\n            if len(tokens_a) > max_seq_length - 2:\n                tokens_a = tokens_a[:(max_seq_length - 2)]\n\n        # The convention in BERT is:\n        # (a) For sequence pairs:\n        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n        # (b) For single sequences:\n        #  tokens:   [CLS] the dog is hairy . [SEP]\n        #  type_ids: 0   0   0   0  0     0 0\n        #\n        # Where ""type_ids"" are used to indicate whether this is the first\n        # sequence or the second sequence. The embedding vectors for `type=0` and\n        # `type=1` were learned during pre-training and are added to the wordpiece\n        # embedding vector (and position vector). This is not *strictly* necessary\n        # since the [SEP] token unambiguously separates the sequences, but it makes\n        # it easier for the model to learn the concept of sequences.\n        #\n        # For classification tasks, the first vector (corresponding to [CLS]) is\n        # used as as the ""sentence vector"". Note that this only makes sense because\n        # the entire model is fine-tuned.\n        tokens = [""[CLS]""] + tokens_a + [""[SEP]""]\n        segment_ids = [0] * len(tokens)\n\n        if tokens_b:\n            tokens += tokens_b + [""[SEP]""]\n            segment_ids += [1] * (len(tokens_b) + 1)\n\n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n        # tokens are attended to.\n        input_mask = [1] * len(input_ids)\n\n        # Zero-pad up to the sequence length.\n        padding = [0] * (max_seq_length - len(input_ids))\n        input_ids += padding\n        input_mask += padding\n        segment_ids += padding\n\n        assert len(input_ids) == max_seq_length\n        assert len(input_mask) == max_seq_length\n        assert len(segment_ids) == max_seq_length\n\n        if output_mode == ""classification"":\n            label_id = label_map[example.label]\n        elif output_mode == ""regression"":\n            label_id = float(example.label)\n        else:\n            raise KeyError(output_mode)\n\n        # if ex_index < 5:\n        #     logger.info(""*** Example ***"")\n        #     logger.info(""guid: %s"" % (example.guid))\n        #     logger.info(""tokens: %s"" % "" "".join(\n        #         [str(x) for x in tokens]))\n        #     logger.info(""input_ids: %s"" %\n        #                 "" "".join([str(x) for x in input_ids]))\n        #     logger.info(""input_mask: %s"" %\n        #                 "" "".join([str(x) for x in input_mask]))\n        #     logger.info(\n        #         ""segment_ids: %s"" % "" "".join([str(x) for x in segment_ids]))\n        #     logger.info(""label: %s (id = %d)"" % (example.label, label_id))\n\n        features.append(\n            InputFeatures(input_ids=input_ids,\n                          input_mask=input_mask,\n                          segment_ids=segment_ids,\n                          label_id=label_id))\n    return features\n\n\ndef _truncate_seq_pair(tokens_a, tokens_b, max_length):\n    """"""Truncates a sequence pair in place to the maximum length.""""""\n\n    # This is a simple heuristic which will always truncate the longer sequence\n    # one token at a time. This makes more sense than truncating an equal percent\n    # of tokens from each, since if one sequence is very short then each token\n    # that\'s truncated likely contains more information than a longer sequence.\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_length:\n            break\n        if len(tokens_a) > len(tokens_b):\n            tokens_a.pop()\n        else:\n            tokens_b.pop()\n\n\ndef simple_accuracy(preds, labels):\n    return (preds == labels).mean()\n\n\ndef acc_and_f1(preds, labels):\n    acc = simple_accuracy(preds, labels)\n    f1 = f1_score(y_true=labels, y_pred=preds)\n    return {\n        ""acc"": acc,\n        ""f1"": f1,\n        ""acc_and_f1"": (acc + f1) / 2,\n    }\n\n\ndef pearson_and_spearman(preds, labels):\n    pearson_corr = pearsonr(preds, labels)[0]\n    spearman_corr = spearmanr(preds, labels)[0]\n    return {\n        ""pearson"": pearson_corr,\n        ""spearmanr"": spearman_corr,\n        ""corr"": (pearson_corr + spearman_corr) / 2,\n    }\n\n\ndef compute_metrics(task_name, preds, labels):\n    assert len(preds) == len(labels)\n    if task_name == ""cola"":\n        return {""mcc"": matthews_corrcoef(labels, preds)}\n    elif task_name == ""sst-2"":\n        return {""acc"": simple_accuracy(preds, labels)}\n    elif task_name == ""mrpc"":\n        return acc_and_f1(preds, labels)\n    elif task_name == ""sts-b"":\n        return pearson_and_spearman(preds, labels)\n    elif task_name == ""qqp"":\n        return acc_and_f1(preds, labels)\n    elif task_name == ""mnli"":\n        return {""acc"": simple_accuracy(preds, labels)}\n    elif task_name == ""mnli-mm"":\n        return {""acc"": simple_accuracy(preds, labels)}\n    elif task_name == ""qnli"":\n        return {""acc"": simple_accuracy(preds, labels)}\n    elif task_name == ""rte"":\n        return {""acc"": simple_accuracy(preds, labels)}\n    elif task_name == ""wnli"":\n        return {""acc"": simple_accuracy(preds, labels)}\n    else:\n        raise KeyError(task_name)\n\nprint(""The arguments are: "" + str(sys.argv))\nrun = Run.get_context()\n\nparser = argparse.ArgumentParser()\n\n# Required parameters\nparser.add_argument(""--data_dir"",\n                    default=None,\n                    type=str,\n                    required=True,\n                    help=""The input data dir. Should contain the .tsv files (or other data files) for the task."")\nparser.add_argument(""--bert_model"", default=None, type=str, required=True,\n                    help=""Bert pre-trained model selected in the list: bert-base-uncased, ""\n                    ""bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, ""\n                    ""bert-base-multilingual-cased, bert-base-chinese."")\nparser.add_argument(""--task_name"",\n                    default=None,\n                    type=str,\n                    required=True,\n                    help=""The name of the task to train."")\nparser.add_argument(""--output_dir"",\n                    default=None,\n                    type=str,\n                    required=True,\n                    help=""The output directory where the model predictions and checkpoints will be written."")\nparser.add_argument(""--model_file_location"", default=None, type=str, required=True,\n                    help=""The directory in the blob storage which contains data and config files."")\n# parser.add_argument(""--config_file_location"", default=None, type=str, required=True,\n#                     help=""The directory in the blob storage which contains data and config files."")\n# Other parameters\nparser.add_argument(""--cache_dir"",\n                    default="""",\n                    type=str,\n                    help=""Where do you want to store the pre-trained models downloaded from s3"")\nparser.add_argument(\'--use_pretrain\',\n                    type=str,\n                    default=\'False\',\n                    help=""Whether to use Bert Pretrain Weights or not"")\nparser.add_argument(""--files_location"", default=None, type=str, \n                    help=""The directory in the blob storage which contains data and config files."")\nparser.add_argument(""--max_seq_length"",\n                    default=128,\n                    type=int,\n                    help=""The maximum total input sequence length after WordPiece tokenization. \\n""\n                            ""Sequences longer than this will be truncated, and sequences shorter \\n""\n                            ""than this will be padded."")\nparser.add_argument(""--do_train"",\n                    action=\'store_true\',\n                    help=""Whether to run training."")\nparser.add_argument(""--do_test"",\n                    action=""store_true"",\n                    help=""Whether to run tesing."")\nparser.add_argument(""--do_eval"",\n                    action=\'store_true\',\n                    help=""Whether to run eval on the dev set."")\nparser.add_argument(""--do_lower_case"",\n                    action=\'store_true\',\n                    help=""Set this flag if you are using an uncased model."")\nparser.add_argument(""--train_batch_size"",\n                    default=32,\n                    type=int,\n                    help=""Total batch size for training."")\nparser.add_argument(""--eval_batch_size"",\n                    default=8,\n                    type=int,\n                    help=""Total batch size for eval."")\nparser.add_argument(""--learning_rate"",\n                    default=5e-5,\n                    type=float,\n                    help=""The initial learning rate for Adam."")\nparser.add_argument(""--final_learning_rate"",""--final_lr"",\n                    default=5e-6,\n                    type=float,\n                    help=""The final learning rate for Adam."")\nparser.add_argument(""--lr_decay_rate"",\n                    default=0.99,\n                    type=float,\n                    help=""The initial learning rate for Adam."")\nparser.add_argument(""--num_train_epochs"",\n                    default=3.0,\n                    type=float,\n                    help=""Total number of training epochs to perform."")\nparser.add_argument(""--warmup_proportion"",\n                    default=0.1,\n                    type=float,\n                    help=""Proportion of training to perform linear learning rate warmup for. ""\n                            ""E.g., 0.1 = 10%% of training."")\nparser.add_argument(""--no_cuda"",\n                    action=\'store_true\',\n                    help=""Whether not to use CUDA when available"")\nparser.add_argument(\'--seed\',\n                    type=int,\n                    default=42,\n                    help=""random seed for initialization"")\nparser.add_argument(\'--gradient_accumulation_steps\',\n                    type=int,\n                    default=1,\n                    help=""Number of updates steps to accumulate before performing a backward/update pass."")\nparser.add_argument(\'--fp16\',\n                    action=\'store_true\',\n                    help=""Whether to use 16-bit float precision instead of 32-bit"")\nparser.add_argument(\'--loss_scale\',\n                    type=float, default=0,\n                    help=""Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n""\n                            ""0 (default value): dynamic loss scaling.\\n""\n                            ""Positive power of 2: static loss scaling value.\\n"")\nparser.add_argument(\'--server_ip\', type=str, default=\'\',\n                    help=""Can be used for distant debugging."")\nparser.add_argument(\'--server_port\', type=str, default=\'\',\n                    help=""Can be used for distant debugging."")\nparser.add_argument(""--model_file"",\n                    type=str,\n                    default=""0"",\n                    help=""Path to the Pretrained BERT Encoder File."")\nparser.add_argument(\'--random\',\n                    default=False,\n                    action=\'store_true\',\n                    help=""Whether to fientune for random initialization"")\nparser.add_argument(\'--focal\',\n                    default=False,\n                    action=\'store_true\',\n                    help=""Whether to use Focal Loss for finetuning."")\nparser.add_argument(\'--gamma\', type=float, default=0.5,\n                    help=""Gamma parameter to be used in focal loss."")\nargs = parser.parse_args()\n\nif args.server_ip and args.server_port:\n    # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n    import ptvsd\n    print(""Waiting for debugger attach"")\n    ptvsd.enable_attach(\n        address=(args.server_ip, args.server_port), redirect_output=True)\n    ptvsd.wait_for_attach()\n\nprocessors = {\n    ""cola"": ColaProcessor,\n    ""mnli"": MnliProcessor,\n    ""mnli-mm"": MnliMismatchedProcessor,\n    ""mrpc"": MrpcProcessor,\n    ""sst-2"": Sst2Processor,\n    ""sts-b"": StsbProcessor,\n    ""qqp"": QqpProcessor,\n    ""qnli"": QnliProcessor,\n    ""rte"": RteProcessor,\n    ""wnli"": WnliProcessor,\n}\n\noutput_modes = {\n    ""cola"": ""classification"",\n    ""mnli"": ""classification"",\n    ""mrpc"": ""classification"",\n    ""sst-2"": ""classification"",\n    ""sts-b"": ""regression"",\n    ""qqp"": ""classification"",\n    ""qnli"": ""classification"",\n    ""rte"": ""classification"",\n    ""wnli"": ""classification"",\n}\n\nglobal_size = get_global_size()\nlocal_size = get_local_size()\t\n\nset_environment_variables_for_nccl_backend(local_size == global_size)\n\nlocal_rank = get_local_rank()\n\nis_distributed = bool(local_rank != -1)\n\n# hacky code -- fix\nargs.model_file = os.path.join(args.model_file_location, args.model_file)\n# output_dir = os.path.join(parent_dir, str(run.id))\n# os.makedirs(output_dir, exist_ok=True)\nargs.output_dir = os.path.join(args.output_dir, ""saved_models"", str(run.id))\n\n\n# config_path = os.path.join(args.config_file_location, \'bert-large.json\')\n\n# config = json.load(open(config_path, \'r\', encoding=\'utf-8\'))\n# # # Replace placeholder path prefix by path corresponding to ""ds.path(\'data/bert_data/\').as_mount()""\n# # config[\'data\'][\'datasets\'] = {key: value.replace(\'placeholder/\', args.files_location)\n# #                               for (key, value) in config[\'data\'][\'datasets\'].items()}\n# # config[\'validation\'][\'path\'] = config[\'validation\'][\'path\'].replace(\'placeholder/\', args.files_location)\n# args.config = config\n\n# print(""Running Config File: "", config[\'name\'])\n# args.logger = logger\n\n\nif not is_distributed or args.no_cuda:\n    device = torch.device(\n        ""cuda"" if torch.cuda.is_available() and not args.no_cuda else ""cpu"")\n    n_gpu = torch.cuda.device_count()\nelse:\n    torch.cuda.set_device(local_rank)\n    device = torch.device(""cuda"", local_rank)\n    n_gpu = 1\n    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n\n    torch.distributed.init_process_group(backend=\'nccl\')\nlogger.info(""device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}"".format(\n    device, n_gpu, bool(is_distributed), args.fp16))\n\nif args.gradient_accumulation_steps < 1:\n    raise ValueError(""Invalid gradient_accumulation_steps parameter: {}, should be >= 1"".format(\n        args.gradient_accumulation_steps))\n\nargs.train_batch_size = args.train_batch_size // args.gradient_accumulation_steps\nargs.seed = random.randint(1, 1000)\nrandom.seed(args.seed)\nnp.random.seed(args.seed)\ntorch.manual_seed(args.seed)\nlogger.info(""seed: {}"".format(args.seed))\nif n_gpu > 0:\n    torch.cuda.manual_seed_all(args.seed)\n\nif not args.do_train and not args.do_eval:\n    raise ValueError(\n        ""At least one of `do_train` or `do_eval` must be True."")\n\nif (torch.distributed.get_rank() == 0):\n     if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train:\n         raise ValueError(\n             ""Output directory ({}) already exists and is not empty."".format(args.output_dir))\n     if not os.path.exists(args.output_dir):\n        os.makedirs(args.output_dir)\n\ntask_name = args.task_name.lower()\n\nif task_name not in processors:\n    raise ValueError(""Task not found: %s"" % (task_name))\n\nprocessor = processors[task_name]()\noutput_mode = output_modes[task_name]\n\nlabel_list = processor.get_labels()\nnum_labels = len(label_list)\n\ntokenizer = BertTokenizer.from_pretrained(\n    args.bert_model, do_lower_case=args.do_lower_case)\n\ntrain_examples = None\nnum_train_optimization_steps = None\nif args.do_train:\n    train_examples = processor.get_train_examples(args.data_dir)\n    num_train_optimization_steps = int(\n        len(train_examples) / args.train_batch_size / args.gradient_accumulation_steps) * args.num_train_epochs\n    if is_distributed:\n        num_train_optimization_steps = num_train_optimization_steps // torch.distributed.get_world_size()\n\n# Prepare model\ncache_dir = args.cache_dir if args.cache_dir else os.path.join(\n    str(PYTORCH_PRETRAINED_BERT_CACHE), \'distributed_{}\'.format(local_rank))\nmodel = BertForSequenceClassification.from_pretrained(args.bert_model,\n                                                        cache_dir=cache_dir,\n                                                        num_labels=num_labels)\n\n#model = BertMultiTask(args)\n#logger.info(""Model of type: {}"".format(type(model)))\n\nif args.model_file is not ""0"":\n    logger.info(f""Loading Pretrained Bert Encoder from: {args.model_file}"")\n    bert_state_dict = torch.load(args.model_file, map_location=torch.device(""cpu""))\n    model.bert.load_state_dict(bert_state_dict)\n    #model.load(bert_state_dict)\n    logger.info(f""Pretrained Bert Encoder Loaded from: {args.model_file}"")\n\nif args.random:\n    logger.info(""USING RANDOM INITIALISATION FOR FINETUNING"")\n    model.apply(model.init_bert_weights)\n\nif args.fp16:\n    model.half()\nmodel.to(device)\nif is_distributed:\n    try:\n        from apex.parallel import DistributedDataParallel as DDP\n    except ImportError:\n        raise ImportError(\n            ""Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training."")\n\n    model = DDP(model)\nelif n_gpu > 1:\n    model = torch.nn.DataParallel(model)\n\n# Prepare optimizer\nparam_optimizer = list(model.named_parameters())\nno_decay = [\'bias\', \'LayerNorm.bias\', \'LayerNorm.weight\']\noptimizer_grouped_parameters = [\n    {\'params\': [p for n, p in param_optimizer if not any(\n        nd in n for nd in no_decay)], \'weight_decay\': 0.01},\n    {\'params\': [p for n, p in param_optimizer if any(\n        nd in n for nd in no_decay)], \'weight_decay\': 0.0}\n]\nif args.fp16:\n    try:\n        from apex.optimizers import FP16_Optimizer\n        from apex.optimizers import FusedAdam\n    except ImportError:\n        raise ImportError(\n            ""Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training."")\n\n    optimizer = FusedAdam(optimizer_grouped_parameters,\n                            lr=args.learning_rate,\n                            bias_correction=False,\n                            max_grad_norm=1.0)\n    if args.loss_scale == 0:\n        optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n    else:\n        optimizer = FP16_Optimizer(\n            optimizer, static_loss_scale=args.loss_scale)\n    warmup_schedule = LinearWarmupExponentialSchedule(warmup=args.warmup_proportion, t_total=num_train_optimization_steps, initial_lr=args.learning_rate, final_lr=args.final_learning_rate, decay_rate=args.lr_decay_rate)\n\nelse:\n    optimizer = BertAdam(optimizer_grouped_parameters,\n                            lr=args.learning_rate,\n                            warmup=args.warmup_proportion,\n                            t_total=num_train_optimization_steps)\n\nglobal_step = 0\nnb_tr_steps = 0\ntr_loss = 0\n\nif args.do_train:\n\n    if (not is_distributed or torch.distributed.get_rank() == 0):\n           run.log(\'lr\', np.float(args.learning_rate))\n\n    train_features = convert_examples_to_features(\n        train_examples, label_list, args.max_seq_length, tokenizer, output_mode)\n    logger.info(""***** Running training *****"")\n    logger.info(""  Num examples = %d"", len(train_examples))\n    logger.info(""  Batch size = %d"", args.train_batch_size)\n    logger.info(""  Num steps = %d"", num_train_optimization_steps)\n    all_input_ids = torch.tensor(\n        [f.input_ids for f in train_features], dtype=torch.long)\n    all_input_mask = torch.tensor(\n        [f.input_mask for f in train_features], dtype=torch.long)\n    all_segment_ids = torch.tensor(\n        [f.segment_ids for f in train_features], dtype=torch.long)\n\n    if output_mode == ""classification"":\n        all_label_ids = torch.tensor(\n            [f.label_id for f in train_features], dtype=torch.long)\n    elif output_mode == ""regression"":\n        all_label_ids = torch.tensor(\n            [f.label_id for f in train_features], dtype=torch.float)\n\n    train_data = TensorDataset(\n        all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n    if not is_distributed:\n        train_sampler = RandomSampler(train_data)\n    else:\n        train_sampler = DistributedSampler(train_data)\n\n    for epoch_num in trange(int(args.num_train_epochs), desc=""Epoch""):\n        model.train()\n        train_sampler.set_epoch(epoch_num)\n        train_dataloader = DataLoader(\n            train_data, sampler=train_sampler, batch_size=args.train_batch_size)\n        tr_loss = 0\n        nb_tr_examples, nb_tr_steps = 0, 0\n        for step, batch in enumerate(tqdm(train_dataloader, desc=""Iteration"")):\n            batch = tuple(t.to(device) for t in batch)\n            input_ids, input_mask, segment_ids, label_ids = batch\n\n            # define a new function to compute loss values for both output_modes\n            logits = model(input_ids, segment_ids, input_mask, labels=None)\n\n            if output_mode == ""classification"":\n                if args.focal:\n                    loss_fct = FocalLoss(\n                        class_num=num_labels, gamma=args.gamma, fp16=args.fp16)\n                else:\n                    loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, num_labels),\n                                label_ids.view(-1))\n            elif output_mode == ""regression"":\n                loss_fct = MSELoss()\n                if args.fp16:\n                    label_ids = label_ids.half()\n                loss = loss_fct(logits.view(-1), label_ids.view(-1))\n\n            if n_gpu > 1:\n                loss = loss.mean()  # mean() to average on multi-gpu.\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n\n            if args.fp16:\n                optimizer.backward(loss)\n            else:\n                loss.backward()\n\n            tr_loss += loss.item()\n            nb_tr_examples += input_ids.size(0)\n            nb_tr_steps += 1\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                if args.fp16:\n                    # modify learning rate with special warm up BERT uses\n                    # if args.fp16 is False, BertAdam is used that handles this automatically\n                    lr_this_step = args.learning_rate * \\\n                        warmup_linear(\n                            global_step/num_train_optimization_steps, args.warmup_proportion)\n                    # lr_this_step = args.learning_rate * warmup_schedule.get_lr(global_step)\n                    # logger.info(f""LR this step:{lr_this_step}"")\n                    for param_group in optimizer.param_groups:\n                        param_group[\'lr\'] = lr_this_step\n                optimizer.step()\n                optimizer.zero_grad()\n                global_step += 1\n\n        if(args.do_eval) and (not is_distributed or torch.distributed.get_rank() == 0):\n            run_evaluation(processor, output_mode, ""dev"")\n\n            # hack for MNLI-MM\n            if task_name == ""mnli"":\n                task_name = ""mnli-mm""\n                run_evaluation(processors[""mnli-mm""](), output_mode, ""dev"")\n                task_name = ""mnli""\n        if(args.do_test):\n           #and (not is_distributed or torch.distributed.get_rank() == 0):\n            run_evaluation(processor,output_mode,""test"")\n            #hack for MNLI-mm\n            if task_name == ""mnli"":\n                task_name = ""mnli-mm""\n                run_evaluation(processors[task_name](),output_mode,""test"")\n                task_name = ""mnli""\n        #dist.barrier()  # This is for synchronization between process while 1st GPU evlauates\n\n# if args.do_test and (not is_distributed or torch.distributed.get_rank() == 0):\n#     run_evaluation(processor,output_mode,""test"")\n#      #hack for MNLI-mm\n#     if task_name == ""mnli"":\n#         task_name = ""mnli-mm""\n#         run_evaluation(processors[task_name](),output_mode,""test"")\n\n\n\n\n        # # Save a trained model and the associated configuration\n        # model_to_save = model.module if hasattr(model, \'module\') else model  # Only save the model it-self\n        # output_model_file = os.path.join(args.output_dir, WEIGHTS_NAME)\n        # torch.save(model_to_save.state_dict(), output_model_file)\n        # output_config_file = os.path.join(args.output_dir, CONFIG_NAME)\n        # with open(output_config_file, \'w\') as f:\n        #     f.write(model_to_save.config.to_json_string())\n\n    #     # Load a trained model and config that you have fine-tuned\n    #     config = BertConfig(output_config_file)\n    #     model = BertForSequenceClassification(config, num_labels=num_labels)\n    #     model.load_state_dict(torch.load(output_model_file))\n    # else:\n    #     model = BertForSequenceClassification.from_pretrained(args.bert_model, num_labels=num_labels)\n    # model.to(device)'"
finetune/run_squad_azureml.py,28,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HugginFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""Run BERT on SQuAD.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport collections\nimport logging\nimport json\nimport math\nimport os\nimport random\nimport pickle\nfrom tqdm import tqdm, trange\n\nimport numpy as np\nimport torch\nimport sys\n\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom torch.utils.data.distributed import DistributedSampler\nsys.path.append(""./finetune/"")\nfrom evaluate_squad import evaluate\n\nfrom pytorch_pretrained_bert.tokenization import whitespace_tokenize, BasicTokenizer, BertTokenizer\nfrom pytorch_pretrained_bert.modeling import BertForQuestionAnswering\nfrom pytorch_pretrained_bert.optimization import BertAdam\nfrom pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n\n\nsys.path.append(""./pretrain/PyTorch/"")\nfrom azureml_adapter import set_environment_variables_for_nccl_backend, get_local_rank, get_global_size, get_local_size, get_world_size\nfrom azureml.core.run import Run\n\nrun = Run.get_context()\n\nlogging.basicConfig(format = \'%(asctime)s - %(levelname)s - %(name)s -   %(message)s\',\n                    datefmt = \'%m/%d/%Y %H:%M:%S\',\n                    level = logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass SquadExample(object):\n    """"""A single training/test example for the Squad dataset.""""""\n\n    def __init__(self,\n                 qas_id,\n                 question_text,\n                 doc_tokens,\n                 orig_answer_text=None,\n                 start_position=None,\n                 end_position=None):\n        self.qas_id = qas_id\n        self.question_text = question_text\n        self.doc_tokens = doc_tokens\n        self.orig_answer_text = orig_answer_text\n        self.start_position = start_position\n        self.end_position = end_position\n\n    def __str__(self):\n        return self.__repr__()\n\n    def __repr__(self):\n        s = """"\n        s += ""qas_id: %s"" % (self.qas_id)\n        s += "", question_text: %s"" % (\n            self.question_text)\n        s += "", doc_tokens: [%s]"" % ("" "".join(self.doc_tokens))\n        if self.start_position:\n            s += "", start_position: %d"" % (self.start_position)\n        if self.start_position:\n            s += "", end_position: %d"" % (self.end_position)\n        return s\n\n\nclass InputFeatures(object):\n    """"""A single set of features of data.""""""\n\n    def __init__(self,\n                 unique_id,\n                 example_index,\n                 doc_span_index,\n                 tokens,\n                 token_to_orig_map,\n                 token_is_max_context,\n                 input_ids,\n                 input_mask,\n                 segment_ids,\n                 start_position=None,\n                 end_position=None):\n        self.unique_id = unique_id\n        self.example_index = example_index\n        self.doc_span_index = doc_span_index\n        self.tokens = tokens\n        self.token_to_orig_map = token_to_orig_map\n        self.token_is_max_context = token_is_max_context\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.start_position = start_position\n        self.end_position = end_position\n\n\ndef read_squad_examples(input_file, is_training):\n    """"""Read a SQuAD json file into a list of SquadExample.""""""\n    with open(input_file, ""r"", encoding=\'utf-8\') as reader:\n        input_data = json.load(reader)[""data""]\n\n    def is_whitespace(c):\n        if c == "" "" or c == ""\\t"" or c == ""\\r"" or c == ""\\n"" or ord(c) == 0x202F:\n            return True\n        return False\n\n    examples = []\n    for entry in input_data:\n        for paragraph in entry[""paragraphs""]:\n            paragraph_text = paragraph[""context""]\n            doc_tokens = []\n            char_to_word_offset = []\n            prev_is_whitespace = True\n            for c in paragraph_text:\n                if is_whitespace(c):\n                    prev_is_whitespace = True\n                else:\n                    if prev_is_whitespace:\n                        doc_tokens.append(c)\n                    else:\n                        doc_tokens[-1] += c\n                    prev_is_whitespace = False\n                char_to_word_offset.append(len(doc_tokens) - 1)\n\n            for qa in paragraph[""qas""]:\n                qas_id = qa[""id""]\n                question_text = qa[""question""]\n                start_position = None\n                end_position = None\n                orig_answer_text = None\n                if is_training:\n                    if len(qa[""answers""]) != 1:\n                        raise ValueError(\n                            ""For training, each question should have exactly 1 answer."")\n                    answer = qa[""answers""][0]\n                    orig_answer_text = answer[""text""]\n                    answer_offset = answer[""answer_start""]\n                    answer_length = len(orig_answer_text)\n                    start_position = char_to_word_offset[answer_offset]\n                    end_position = char_to_word_offset[answer_offset + answer_length - 1]\n                    # Only add answers where the text can be exactly recovered from the\n                    # document. If this CAN\'T happen it\'s likely due to weird Unicode\n                    # stuff so we will just skip the example.\n                    #\n                    # Note that this means for training mode, every example is NOT\n                    # guaranteed to be preserved.\n                    actual_text = "" "".join(doc_tokens[start_position:(end_position + 1)])\n                    cleaned_answer_text = "" "".join(\n                        whitespace_tokenize(orig_answer_text))\n                    if actual_text.find(cleaned_answer_text) == -1:\n                        logger.warning(""Could not find answer: \'%s\' vs. \'%s\'"",\n                                           actual_text, cleaned_answer_text)\n                        continue\n\n                example = SquadExample(\n                    qas_id=qas_id,\n                    question_text=question_text,\n                    doc_tokens=doc_tokens,\n                    orig_answer_text=orig_answer_text,\n                    start_position=start_position,\n                    end_position=end_position)\n                examples.append(example)\n    return examples\n\n\ndef convert_examples_to_features(examples, tokenizer, max_seq_length,\n                                 doc_stride, max_query_length, is_training):\n    """"""Loads a data file into a list of `InputBatch`s.""""""\n\n    unique_id = 1000000000\n\n    features = []\n    for (example_index, example) in enumerate(examples):\n        query_tokens = tokenizer.tokenize(example.question_text)\n\n        if len(query_tokens) > max_query_length:\n            query_tokens = query_tokens[0:max_query_length]\n\n        tok_to_orig_index = []\n        orig_to_tok_index = []\n        all_doc_tokens = []\n        for (i, token) in enumerate(example.doc_tokens):\n            orig_to_tok_index.append(len(all_doc_tokens))\n            sub_tokens = tokenizer.tokenize(token)\n            for sub_token in sub_tokens:\n                tok_to_orig_index.append(i)\n                all_doc_tokens.append(sub_token)\n\n        tok_start_position = None\n        tok_end_position = None\n        if is_training:\n            tok_start_position = orig_to_tok_index[example.start_position]\n            if example.end_position < len(example.doc_tokens) - 1:\n                tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n            else:\n                tok_end_position = len(all_doc_tokens) - 1\n            (tok_start_position, tok_end_position) = _improve_answer_span(\n                all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n                example.orig_answer_text)\n\n        # The -3 accounts for [CLS], [SEP] and [SEP]\n        max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n\n        # We can have documents that are longer than the maximum sequence length.\n        # To deal with this we do a sliding window approach, where we take chunks\n        # of the up to our max length with a stride of `doc_stride`.\n        _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n            ""DocSpan"", [""start"", ""length""])\n        doc_spans = []\n        start_offset = 0\n        while start_offset < len(all_doc_tokens):\n            length = len(all_doc_tokens) - start_offset\n            if length > max_tokens_for_doc:\n                length = max_tokens_for_doc\n            doc_spans.append(_DocSpan(start=start_offset, length=length))\n            if start_offset + length == len(all_doc_tokens):\n                break\n            start_offset += min(length, doc_stride)\n\n        for (doc_span_index, doc_span) in enumerate(doc_spans):\n            tokens = []\n            token_to_orig_map = {}\n            token_is_max_context = {}\n            segment_ids = []\n            tokens.append(""[CLS]"")\n            segment_ids.append(0)\n            for token in query_tokens:\n                tokens.append(token)\n                segment_ids.append(0)\n            tokens.append(""[SEP]"")\n            segment_ids.append(0)\n\n            for i in range(doc_span.length):\n                split_token_index = doc_span.start + i\n                token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n\n                is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n                                                       split_token_index)\n                token_is_max_context[len(tokens)] = is_max_context\n                tokens.append(all_doc_tokens[split_token_index])\n                segment_ids.append(1)\n            tokens.append(""[SEP]"")\n            segment_ids.append(1)\n\n            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n            # The mask has 1 for real tokens and 0 for padding tokens. Only real\n            # tokens are attended to.\n            input_mask = [1] * len(input_ids)\n\n            # Zero-pad up to the sequence length.\n            while len(input_ids) < max_seq_length:\n                input_ids.append(0)\n                input_mask.append(0)\n                segment_ids.append(0)\n\n            assert len(input_ids) == max_seq_length\n            assert len(input_mask) == max_seq_length\n            assert len(segment_ids) == max_seq_length\n\n            start_position = None\n            end_position = None\n            if is_training:\n                # For training, if our document chunk does not contain an annotation\n                # we throw it out, since there is nothing to predict.\n                doc_start = doc_span.start\n                doc_end = doc_span.start + doc_span.length - 1\n                if (example.start_position < doc_start or\n                        example.end_position < doc_start or\n                        example.start_position > doc_end or example.end_position > doc_end):\n                    continue\n\n                doc_offset = len(query_tokens) + 2\n                start_position = tok_start_position - doc_start + doc_offset\n                end_position = tok_end_position - doc_start + doc_offset\n\n            if example_index < 20:\n                logger.info(""*** Example ***"")\n                logger.info(""unique_id: %s"" % (unique_id))\n                logger.info(""example_index: %s"" % (example_index))\n                logger.info(""doc_span_index: %s"" % (doc_span_index))\n                logger.info(""tokens: %s"" % "" "".join(tokens))\n                logger.info(""token_to_orig_map: %s"" % "" "".join([\n                    ""%d:%d"" % (x, y) for (x, y) in token_to_orig_map.items()]))\n                logger.info(""token_is_max_context: %s"" % "" "".join([\n                    ""%d:%s"" % (x, y) for (x, y) in token_is_max_context.items()\n                ]))\n                logger.info(""input_ids: %s"" % "" "".join([str(x) for x in input_ids]))\n                logger.info(\n                    ""input_mask: %s"" % "" "".join([str(x) for x in input_mask]))\n                logger.info(\n                    ""segment_ids: %s"" % "" "".join([str(x) for x in segment_ids]))\n                if is_training:\n                    answer_text = "" "".join(tokens[start_position:(end_position + 1)])\n                    logger.info(""start_position: %d"" % (start_position))\n                    logger.info(""end_position: %d"" % (end_position))\n                    logger.info(\n                        ""answer: %s"" % (answer_text))\n\n            features.append(\n                InputFeatures(\n                    unique_id=unique_id,\n                    example_index=example_index,\n                    doc_span_index=doc_span_index,\n                    tokens=tokens,\n                    token_to_orig_map=token_to_orig_map,\n                    token_is_max_context=token_is_max_context,\n                    input_ids=input_ids,\n                    input_mask=input_mask,\n                    segment_ids=segment_ids,\n                    start_position=start_position,\n                    end_position=end_position))\n            unique_id += 1\n\n    return features\n\n\ndef _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n                         orig_answer_text):\n    """"""Returns tokenized answer spans that better match the annotated answer.""""""\n\n    # The SQuAD annotations are character based. We first project them to\n    # whitespace-tokenized words. But then after WordPiece tokenization, we can\n    # often find a ""better match"". For example:\n    #\n    #   Question: What year was John Smith born?\n    #   Context: The leader was John Smith (1895-1943).\n    #   Answer: 1895\n    #\n    # The original whitespace-tokenized answer will be ""(1895-1943)."". However\n    # after tokenization, our tokens will be ""( 1895 - 1943 ) ."". So we can match\n    # the exact answer, 1895.\n    #\n    # However, this is not always possible. Consider the following:\n    #\n    #   Question: What country is the top exporter of electornics?\n    #   Context: The Japanese electronics industry is the lagest in the world.\n    #   Answer: Japan\n    #\n    # In this case, the annotator chose ""Japan"" as a character sub-span of\n    # the word ""Japanese"". Since our WordPiece tokenizer does not split\n    # ""Japanese"", we just use ""Japanese"" as the annotation. This is fairly rare\n    # in SQuAD, but does happen.\n    tok_answer_text = "" "".join(tokenizer.tokenize(orig_answer_text))\n\n    for new_start in range(input_start, input_end + 1):\n        for new_end in range(input_end, new_start - 1, -1):\n            text_span = "" "".join(doc_tokens[new_start:(new_end + 1)])\n            if text_span == tok_answer_text:\n                return (new_start, new_end)\n\n    return (input_start, input_end)\n\n\ndef _check_is_max_context(doc_spans, cur_span_index, position):\n    """"""Check if this is the \'max context\' doc span for the token.""""""\n\n    # Because of the sliding window approach taken to scoring documents, a single\n    # token can appear in multiple documents. E.g.\n    #  Doc: the man went to the store and bought a gallon of milk\n    #  Span A: the man went to the\n    #  Span B: to the store and bought\n    #  Span C: and bought a gallon of\n    #  ...\n    #\n    # Now the word \'bought\' will have two scores from spans B and C. We only\n    # want to consider the score with ""maximum context"", which we define as\n    # the *minimum* of its left and right context (the *sum* of left and\n    # right context will always be the same, of course).\n    #\n    # In the example the maximum context for \'bought\' would be span C since\n    # it has 1 left context and 3 right context, while span B has 4 left context\n    # and 0 right context.\n    best_score = None\n    best_span_index = None\n    for (span_index, doc_span) in enumerate(doc_spans):\n        end = doc_span.start + doc_span.length - 1\n        if position < doc_span.start:\n            continue\n        if position > end:\n            continue\n        num_left_context = position - doc_span.start\n        num_right_context = end - position\n        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n        if best_score is None or score > best_score:\n            best_score = score\n            best_span_index = span_index\n\n    return cur_span_index == best_span_index\n\n\n\nRawResult = collections.namedtuple(""RawResult"",\n                                   [""unique_id"", ""start_logits"", ""end_logits""])\n\n\ndef write_predictions(all_examples, all_features, all_results, n_best_size,\n                      max_answer_length, do_lower_case, output_prediction_file,\n                      output_nbest_file, verbose_logging):\n    """"""Write final predictions to the json file.""""""\n    logger.info(""Writing predictions to: %s"" % (output_prediction_file))\n    logger.info(""Writing nbest to: %s"" % (output_nbest_file))\n\n    example_index_to_features = collections.defaultdict(list)\n    for feature in all_features:\n        example_index_to_features[feature.example_index].append(feature)\n\n    unique_id_to_result = {}\n    for result in all_results:\n        unique_id_to_result[result.unique_id] = result\n\n    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n        ""PrelimPrediction"",\n        [""feature_index"", ""start_index"", ""end_index"", ""start_logit"", ""end_logit""])\n\n    all_predictions = collections.OrderedDict()\n    all_nbest_json = collections.OrderedDict()\n    for (example_index, example) in enumerate(all_examples):\n        features = example_index_to_features[example_index]\n\n        prelim_predictions = []\n        for (feature_index, feature) in enumerate(features):\n            result = unique_id_to_result[feature.unique_id]\n\n            start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n            end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # We could hypothetically create invalid predictions, e.g., predict\n                    # that the start of the span is in the question. We throw out all\n                    # invalid predictions.\n                    if start_index >= len(feature.tokens):\n                        continue\n                    if end_index >= len(feature.tokens):\n                        continue\n                    if start_index not in feature.token_to_orig_map:\n                        continue\n                    if end_index not in feature.token_to_orig_map:\n                        continue\n                    if not feature.token_is_max_context.get(start_index, False):\n                        continue\n                    if end_index < start_index:\n                        continue\n                    length = end_index - start_index + 1\n                    if length > max_answer_length:\n                        continue\n                    prelim_predictions.append(\n                        _PrelimPrediction(\n                            feature_index=feature_index,\n                            start_index=start_index,\n                            end_index=end_index,\n                            start_logit=result.start_logits[start_index],\n                            end_logit=result.end_logits[end_index]))\n\n        prelim_predictions = sorted(\n            prelim_predictions,\n            key=lambda x: (x.start_logit + x.end_logit),\n            reverse=True)\n\n        _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n            ""NbestPrediction"", [""text"", ""start_logit"", ""end_logit""])\n\n        seen_predictions = {}\n        nbest = []\n        for pred in prelim_predictions:\n            if len(nbest) >= n_best_size:\n                break\n            feature = features[pred.feature_index]\n\n            tok_tokens = feature.tokens[pred.start_index:(pred.end_index + 1)]\n            orig_doc_start = feature.token_to_orig_map[pred.start_index]\n            orig_doc_end = feature.token_to_orig_map[pred.end_index]\n            orig_tokens = example.doc_tokens[orig_doc_start:(orig_doc_end + 1)]\n            tok_text = "" "".join(tok_tokens)\n\n            # De-tokenize WordPieces that have been split off.\n            tok_text = tok_text.replace("" ##"", """")\n            tok_text = tok_text.replace(""##"", """")\n\n            # Clean whitespace\n            tok_text = tok_text.strip()\n            tok_text = "" "".join(tok_text.split())\n            orig_text = "" "".join(orig_tokens)\n\n            final_text = get_final_text(tok_text, orig_text, do_lower_case, verbose_logging)\n            if final_text in seen_predictions:\n                continue\n\n            seen_predictions[final_text] = True\n            nbest.append(\n                _NbestPrediction(\n                    text=final_text,\n                    start_logit=pred.start_logit,\n                    end_logit=pred.end_logit))\n\n        # In very rare edge cases we could have no valid predictions. So we\n        # just create a nonce prediction in this case to avoid failure.\n        if not nbest:\n            nbest.append(\n                _NbestPrediction(text=""empty"", start_logit=0.0, end_logit=0.0))\n\n        assert len(nbest) >= 1\n\n        total_scores = []\n        for entry in nbest:\n            total_scores.append(entry.start_logit + entry.end_logit)\n\n        probs = _compute_softmax(total_scores)\n\n        nbest_json = []\n        for (i, entry) in enumerate(nbest):\n            output = collections.OrderedDict()\n            output[""text""] = entry.text\n            output[""probability""] = probs[i]\n            output[""start_logit""] = entry.start_logit\n            output[""end_logit""] = entry.end_logit\n            nbest_json.append(output)\n\n        assert len(nbest_json) >= 1\n\n        all_predictions[example.qas_id] = nbest_json[0][""text""]\n        all_nbest_json[example.qas_id] = nbest_json\n\n    with open(output_prediction_file, ""w"") as writer:\n        writer.write(json.dumps(all_predictions, indent=4) + ""\\n"")\n\n    with open(output_nbest_file, ""w"") as writer:\n        writer.write(json.dumps(all_nbest_json, indent=4) + ""\\n"")\n\n\ndef get_final_text(pred_text, orig_text, do_lower_case, verbose_logging=False):\n    """"""Project the tokenized prediction back to the original text.""""""\n\n    # When we created the data, we kept track of the alignment between original\n    # (whitespace tokenized) tokens and our WordPiece tokenized tokens. So\n    # now `orig_text` contains the span of our original text corresponding to the\n    # span that we predicted.\n    #\n    # However, `orig_text` may contain extra characters that we don\'t want in\n    # our prediction.\n    #\n    # For example, let\'s say:\n    #   pred_text = steve smith\n    #   orig_text = Steve Smith\'s\n    #\n    # We don\'t want to return `orig_text` because it contains the extra ""\'s"".\n    #\n    # We don\'t want to return `pred_text` because it\'s already been normalized\n    # (the SQuAD eval script also does punctuation stripping/lower casing but\n    # our tokenizer does additional normalization like stripping accent\n    # characters).\n    #\n    # What we really want to return is ""Steve Smith"".\n    #\n    # Therefore, we have to apply a semi-complicated alignment heruistic between\n    # `pred_text` and `orig_text` to get a character-to-charcter alignment. This\n    # can fail in certain cases in which case we just return `orig_text`.\n\n    def _strip_spaces(text):\n        ns_chars = []\n        ns_to_s_map = collections.OrderedDict()\n        for (i, c) in enumerate(text):\n            if c == "" "":\n                continue\n            ns_to_s_map[len(ns_chars)] = i\n            ns_chars.append(c)\n        ns_text = """".join(ns_chars)\n        return (ns_text, ns_to_s_map)\n\n    # We first tokenize `orig_text`, strip whitespace from the result\n    # and `pred_text`, and check if they are the same length. If they are\n    # NOT the same length, the heuristic has failed. If they are the same\n    # length, we assume the characters are one-to-one aligned.\n    tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n\n    tok_text = "" "".join(tokenizer.tokenize(orig_text))\n\n    start_position = tok_text.find(pred_text)\n    if start_position == -1:\n        if verbose_logging:\n            logger.info(\n                ""Unable to find text: \'%s\' in \'%s\'"" % (pred_text, orig_text))\n        return orig_text\n    end_position = start_position + len(pred_text) - 1\n\n    (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n    (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n\n    if len(orig_ns_text) != len(tok_ns_text):\n        if verbose_logging:\n            logger.info(""Length not equal after stripping spaces: \'%s\' vs \'%s\'"",\n                            orig_ns_text, tok_ns_text)\n        return orig_text\n\n    # We then project the characters in `pred_text` back to `orig_text` using\n    # the character-to-character alignment.\n    tok_s_to_ns_map = {}\n    for (i, tok_index) in tok_ns_to_s_map.items():\n        tok_s_to_ns_map[tok_index] = i\n\n    orig_start_position = None\n    if start_position in tok_s_to_ns_map:\n        ns_start_position = tok_s_to_ns_map[start_position]\n        if ns_start_position in orig_ns_to_s_map:\n            orig_start_position = orig_ns_to_s_map[ns_start_position]\n\n    if orig_start_position is None:\n        if verbose_logging:\n            logger.info(""Couldn\'t map start position"")\n        return orig_text\n\n    orig_end_position = None\n    if end_position in tok_s_to_ns_map:\n        ns_end_position = tok_s_to_ns_map[end_position]\n        if ns_end_position in orig_ns_to_s_map:\n            orig_end_position = orig_ns_to_s_map[ns_end_position]\n\n    if orig_end_position is None:\n        if verbose_logging:\n            logger.info(""Couldn\'t map end position"")\n        return orig_text\n\n    output_text = orig_text[orig_start_position:(orig_end_position + 1)]\n    return output_text\n\n\ndef _get_best_indexes(logits, n_best_size):\n    """"""Get the n-best logits from a list.""""""\n    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n\n    best_indexes = []\n    for i in range(len(index_and_score)):\n        if i >= n_best_size:\n            break\n        best_indexes.append(index_and_score[i][0])\n    return best_indexes\n\n\ndef _compute_softmax(scores):\n    """"""Compute softmax probability over raw logits.""""""\n    if not scores:\n        return []\n\n    max_score = None\n    for score in scores:\n        if max_score is None or score > max_score:\n            max_score = score\n\n    exp_scores = []\n    total_sum = 0.0\n    for score in scores:\n        x = math.exp(score - max_score)\n        exp_scores.append(x)\n        total_sum += x\n\n    probs = []\n    for score in exp_scores:\n        probs.append(score / total_sum)\n    return probs\n\n\ndef warmup_linear(x, warmup=0.002):\n    if x < warmup:\n        return x/warmup\n    return 1.0 - x\n\ndef copy_optimizer_params_to_model(named_params_model, named_params_optimizer):\n    """""" Utility function for optimize_on_cpu and 16-bits training.\n        Copy the parameters optimized on CPU/RAM back to the model on GPU\n    """"""\n    for (name_opti, param_opti), (name_model, param_model) in zip(named_params_optimizer, named_params_model):\n        if name_opti != name_model:\n            logger.error(""name_opti != name_model: {} {}"".format(name_opti, name_model))\n            raise ValueError\n        param_model.data.copy_(param_opti.data)\n\ndef set_optimizer_params_grad(named_params_optimizer, named_params_model, test_nan=False):\n    """""" Utility function for optimize_on_cpu and 16-bits training.\n        Copy the gradient of the GPU parameters to the CPU/RAMM copy of the model\n    """"""\n    is_nan = False\n    for (name_opti, param_opti), (name_model, param_model) in zip(named_params_optimizer, named_params_model):\n        if name_opti != name_model:\n            logger.error(""name_opti != name_model: {} {}"".format(name_opti, name_model))\n            raise ValueError\n        if param_model.grad is not None:\n            max = param_model.grad.max\n            if test_nan and max != max:\n                is_nan = True\n            if param_opti.grad is None:\n                param_opti.grad = torch.nn.Parameter(param_opti.data.new().resize_(*param_opti.data.size()))\n            param_opti.grad.data.copy_(param_model.grad.data)\n        else:\n            param_opti.grad = None\n    return is_nan\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n\n    ## Required parameters\n    parser.add_argument(""--bert_model"", default=None, type=str, required=True,\n                        help=""Bert pre-trained model selected in the list: bert-base-uncased, ""\n                        ""bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, ""\n                        ""bert-base-multilingual-cased, bert-base-chinese."")\n    parser.add_argument(""--output_dir"", default=None, type=str, required=True,\n                        help=""The output directory where the model checkpoints and predictions will be written."")\n    parser.add_argument(""--model_file_location"", default=None, type=str, required=True,\n                    help=""The directory in the blob storage which contains data and config files."")\n    ## Other parameters\n    parser.add_argument(""--train_file"", default=None, type=str, help=""SQuAD json for training. E.g., train-v1.1.json"")\n    parser.add_argument(""--predict_file"", default=None, type=str,\n                        help=""SQuAD json for predictions. E.g., dev-v1.1.json or test-v1.1.json"")\n    parser.add_argument(""--max_seq_length"", default=384, type=int,\n                        help=""The maximum total input sequence length after WordPiece tokenization. Sequences ""\n                             ""longer than this will be truncated, and sequences shorter than this will be padded."")\n    parser.add_argument(""--doc_stride"", default=128, type=int,\n                        help=""When splitting up a long document into chunks, how much stride to take between chunks."")\n    parser.add_argument(""--max_query_length"", default=64, type=int,\n                        help=""The maximum number of tokens for the question. Questions longer than this will ""\n                             ""be truncated to this length."")\n    parser.add_argument(""--do_train"", action=\'store_true\', help=""Whether to run training."")\n    parser.add_argument(""--do_predict"", action=\'store_true\', help=""Whether to run eval on the dev set."")\n    parser.add_argument(""--train_batch_size"", default=32, type=int, help=""Total batch size for training."")\n    parser.add_argument(""--predict_batch_size"", default=8, type=int, help=""Total batch size for predictions."")\n    parser.add_argument(""--learning_rate"", default=5e-5, type=float, help=""The initial learning rate for Adam."")\n    parser.add_argument(""--num_train_epochs"", default=3.0, type=float,\n                        help=""Total number of training epochs to perform."")\n    parser.add_argument(""--warmup_proportion"", default=0.1, type=float,\n                        help=""Proportion of training to perform linear learning rate warmup for. E.g., 0.1 = 10% ""\n                             ""of training."")\n    parser.add_argument(""--n_best_size"", default=20, type=int,\n                        help=""The total number of n-best predictions to generate in the nbest_predictions.json ""\n                             ""output file."")\n    parser.add_argument(""--max_answer_length"", default=30, type=int,\n                        help=""The maximum length of an answer that can be generated. This is needed because the start ""\n                             ""and end predictions are not conditioned on one another."")\n    parser.add_argument(""--verbose_logging"", action=\'store_true\',\n                        help=""If true, all of the warnings related to data processing will be printed. ""\n                             ""A number of warnings are expected for a normal SQuAD evaluation."")\n    parser.add_argument(""--no_cuda"",\n                        action=\'store_true\',\n                        help=""Whether not to use CUDA when available"")\n    parser.add_argument(\'--seed\',\n                        type=int,\n                        default=42,\n                        help=""random seed for initialization"")\n    parser.add_argument(\'--gradient_accumulation_steps\',\n                        type=int,\n                        default=1,\n                        help=""Number of updates steps to accumulate before performing a backward/update pass."")\n    parser.add_argument(""--do_lower_case"",\n                        action=\'store_true\',\n                        help=""Whether to lower case the input text. True for uncased models, False for cased models."")\n    parser.add_argument(\'--fp16\',\n                        action=\'store_true\',\n                        help=""Whether to use 16-bit float precision instead of 32-bit"")\n    parser.add_argument(\'--loss_scale\',\n                        type=float, default=0,\n                        help=""Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n""\n                             ""0 (default value): dynamic loss scaling.\\n""\n                             ""Positive power of 2: static loss scaling value.\\n"")\n    parser.add_argument(""--model_file"",\n                        type=str,\n                        default=""0"",\n                        help=""Path to the Pretrained BERT Encoder File."")\n\n    args = parser.parse_args()\n    \n    global_size = get_global_size()\n    local_size = get_local_size()\t\n\n    set_environment_variables_for_nccl_backend(local_size == global_size)\n\n    local_rank = get_local_rank()\n\n    is_distributed = bool(local_rank != -1)\n\n    args.model_file = os.path.join(args.model_file_location, args.model_file)\n\n    if not is_distributed or args.no_cuda:\n        device = torch.device(\n            ""cuda"" if torch.cuda.is_available() and not args.no_cuda else ""cpu"")\n        n_gpu = torch.cuda.device_count()\n    else:\n        torch.cuda.set_device(local_rank)\n        device = torch.device(""cuda"", local_rank)\n        n_gpu = 1\n        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n\n        torch.distributed.init_process_group(backend=\'nccl\')\n    logger.info(""device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}"".format(\n        device, n_gpu, bool(is_distributed), args.fp16))\n\n    if args.gradient_accumulation_steps < 1:\n        raise ValueError(""Invalid gradient_accumulation_steps parameter: {}, should be >= 1"".format(\n                            args.gradient_accumulation_steps))\n\n    args.train_batch_size = int(args.train_batch_size / args.gradient_accumulation_steps)\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)\n\n    if not args.do_train and not args.do_predict:\n        raise ValueError(""At least one of `do_train` or `do_predict` must be True."")\n\n    if args.do_train:\n        if not args.train_file:\n            raise ValueError(\n                ""If `do_train` is True, then `train_file` must be specified."")\n    if args.do_predict:\n        if not args.predict_file:\n            raise ValueError(\n                ""If `do_predict` is True, then `predict_file` must be specified."")\n\n    if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train:\n        raise ValueError(""Output directory () already exists and is not empty."")\n    os.makedirs(args.output_dir, exist_ok=True)\n\n    tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n\n    train_examples = None\n    num_train_steps = None\n    if args.do_train:\n        train_examples = read_squad_examples(\n            input_file=args.train_file, is_training=True)\n        num_train_steps = int(\n            len(train_examples) / args.train_batch_size / args.gradient_accumulation_steps * args.num_train_epochs)\n\n    # Prepare model\n    model = BertForQuestionAnswering.from_pretrained(args.bert_model,\n                cache_dir=PYTORCH_PRETRAINED_BERT_CACHE / \'distributed_{}\'.format(local_rank))\n\n    if args.model_file is not ""0"":\n        logger.info(f""Loading Pretrained Bert Encoder from: {args.model_file}"")\n        bert_state_dict = torch.load(args.model_file, map_location=torch.device(""cpu""))\n        model.bert.load_state_dict(bert_state_dict)\n        logger.info(f""Pretrained Bert Encoder Loaded from: {args.model_file}"")\n    \n    if args.fp16:\n        model.half()\n    model.to(device)\n\n    if is_distributed:\n        try:\n            from apex.parallel import DistributedDataParallel as DDP\n        except ImportError:\n            raise ImportError(\n                ""Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training."")\n\n        model = DDP(model)\n    elif n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n\n    # Prepare optimizer\n    param_optimizer = list(model.named_parameters())\n\n    # hack to remove pooler, which is not used\n    # thus it produce None grad that break apex\n    param_optimizer = [n for n in param_optimizer if \'pooler\' not in n[0]]\n\n    no_decay = [\'bias\', \'LayerNorm.bias\', \'LayerNorm.weight\']\n    optimizer_grouped_parameters = [\n        {\'params\': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \'weight_decay\': 0.01},\n        {\'params\': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \'weight_decay\': 0.0}\n        ]\n\n    t_total = num_train_steps\n    if args.do_train and local_rank != -1:\n        t_total = t_total // get_world_size()\n    if args.fp16:\n        try:\n            from apex.optimizers import FP16_Optimizer\n            from apex.optimizers import FusedAdam\n        except ImportError:\n            raise ImportError(""Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training."")\n\n        optimizer = FusedAdam(optimizer_grouped_parameters,\n                              lr=args.learning_rate,\n                              bias_correction=False,\n                              max_grad_norm=1.0)\n        if args.loss_scale == 0:\n            optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n        else:\n            optimizer = FP16_Optimizer(optimizer, static_loss_scale=args.loss_scale)\n    else:\n        optimizer = BertAdam(optimizer_grouped_parameters,\n                             lr=args.learning_rate,\n                             warmup=args.warmup_proportion,\n                             t_total=t_total)\n\n    global_step = 0\n    if args.do_train:\n        if (not is_distributed or torch.distributed.get_rank() == 0):\n           run.log(\'lr\', np.float(args.learning_rate))\n\n        cached_train_features_file = args.train_file+\'_{0}_{1}_{2}_{3}\'.format(\n            list(filter(None, args.bert_model.split(\'/\'))).pop(), str(args.max_seq_length), str(args.doc_stride), str(args.max_query_length))\n        train_features = None\n        try:\n            with open(cached_train_features_file, ""rb"") as reader:\n                train_features = pickle.load(reader)\n        except:\n            train_features = convert_examples_to_features(\n                examples=train_examples,\n                tokenizer=tokenizer,\n                max_seq_length=args.max_seq_length,\n                doc_stride=args.doc_stride,\n                max_query_length=args.max_query_length,\n                is_training=True)\n            if local_rank == -1 or torch.distributed.get_rank() == 0:\n                logger.info(""  Saving train features into cached file %s"", cached_train_features_file)\n                with open(cached_train_features_file, ""wb"") as writer:\n                    pickle.dump(train_features, writer)\n        logger.info(""***** Running training *****"")\n        logger.info(""  Num orig examples = %d"", len(train_examples))\n        logger.info(""  Num split examples = %d"", len(train_features))\n        logger.info(""  Batch size = %d"", args.train_batch_size)\n        logger.info(""  Num steps = %d"", num_train_steps)\n        all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n        all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n        all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n        all_start_positions = torch.tensor([f.start_position for f in train_features], dtype=torch.long)\n        all_end_positions = torch.tensor([f.end_position for f in train_features], dtype=torch.long)\n        train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n                                   all_start_positions, all_end_positions)\n        if local_rank == -1:\n            train_sampler = RandomSampler(train_data)\n        else:\n            train_sampler = DistributedSampler(train_data)\n        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size)\n\n        model.train()\n        for _ in trange(int(args.num_train_epochs), desc=""Epoch""):\n            for step, batch in enumerate(tqdm(train_dataloader, desc=""Iteration"", smoothing=0)):\n                if n_gpu == 1:\n                    batch = tuple(t.to(device) for t in batch) # multi-gpu does scattering it-self\n                input_ids, input_mask, segment_ids, start_positions, end_positions = batch\n                loss = model(input_ids, segment_ids, input_mask, start_positions, end_positions)\n                if n_gpu > 1:\n                    loss = loss.mean() # mean() to average on multi-gpu.\n                if args.gradient_accumulation_steps > 1:\n                    loss = loss / args.gradient_accumulation_steps      \n                if args.fp16:\n                    optimizer.backward(loss)\n                else:\n                    loss.backward()\n                if (step + 1) % args.gradient_accumulation_steps == 0:\n                    # modify learning rate with special warm up BERT uses\n                    lr_this_step = args.learning_rate * warmup_linear(global_step/t_total, args.warmup_proportion)\n                    for param_group in optimizer.param_groups:\n                        param_group[\'lr\'] = lr_this_step\n                    optimizer.step()\n                    optimizer.zero_grad()\n                    global_step += 1\n\n    # Save a trained model\n    #model_to_save = model.module if hasattr(model, \'module\') else model  # Only save the model it-self\n    #output_model_file = os.path.join(args.output_dir, ""pytorch_model.bin"")\n    #if args.do_train:\n    #    torch.save(model_to_save.state_dict(), output_model_file)\n\n    # Load a trained model that you have fine-tuned\n    \n    #model_state_dict = torch.load(output_model_file)\n    #model = BertForQuestionAnswering.from_pretrained(args.bert_model, state_dict=model_state_dict)\n    #model.to(device)\n\n    if args.do_predict and (local_rank == -1 or torch.distributed.get_rank() == 0):\n        eval_examples = read_squad_examples(\n            input_file=args.predict_file, is_training=False)\n        eval_features = convert_examples_to_features(\n            examples=eval_examples,\n            tokenizer=tokenizer,\n            max_seq_length=args.max_seq_length,\n            doc_stride=args.doc_stride,\n            max_query_length=args.max_query_length,\n            is_training=False)\n\n        logger.info(""***** Running predictions *****"")\n        logger.info(""  Num orig examples = %d"", len(eval_examples))\n        logger.info(""  Num split examples = %d"", len(eval_features))\n        logger.info(""  Batch size = %d"", args.predict_batch_size)\n\n        all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n        all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n        all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n        all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n        eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_example_index)\n        # Run prediction for full data\n        eval_sampler = SequentialSampler(eval_data)\n        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.predict_batch_size)\n\n        model.eval()\n        all_results = []\n        logger.info(""Start evaluating"")\n        for input_ids, input_mask, segment_ids, example_indices in tqdm(eval_dataloader, desc=""Evaluating""):\n            if len(all_results) % 1000 == 0:\n                logger.info(""Processing example: %d"" % (len(all_results)))\n            input_ids = input_ids.to(device)\n            input_mask = input_mask.to(device)\n            segment_ids = segment_ids.to(device)\n            with torch.no_grad():\n                batch_start_logits, batch_end_logits = model(input_ids, segment_ids, input_mask)\n            for i, example_index in enumerate(example_indices):\n                start_logits = batch_start_logits[i].detach().cpu().tolist()\n                end_logits = batch_end_logits[i].detach().cpu().tolist()\n                eval_feature = eval_features[example_index.item()]\n                unique_id = int(eval_feature.unique_id)\n                all_results.append(RawResult(unique_id=unique_id,\n                                             start_logits=start_logits,\n                                             end_logits=end_logits))\n        output_prediction_file = os.path.join(args.output_dir, ""predictions.json"")\n        output_nbest_file = os.path.join(args.output_dir, ""nbest_predictions.json"")\n        write_predictions(eval_examples, eval_features, all_results,\n                          args.n_best_size, args.max_answer_length,\n                          args.do_lower_case, output_prediction_file,\n                          output_nbest_file, args.verbose_logging)\n        with open(args.predict_file) as predict_file:\n            dataset_json = json.load(predict_file)\n            dataset = dataset_json[\'data\']\n        with open(output_prediction_file) as prediction_file:\n            predictions = json.load(prediction_file)\n        \n        result = evaluate(dataset, predictions)\n        for key in result.keys():\n            logger.info(""  %s = %s"", key, str(result[key]))\n            run.log(key, result[key])\n\n\n\nif __name__ == ""__main__"":\n    main()'"
finetune/PyTorch/azureml_bert_util.py,8,"b'from horovod.torch.mpi_ops import allreduce, allreduce_async_, synchronize\nfrom horovod.torch.compression import Compression\nimport horovod.torch as hvd\nimport torch\nimport time\n\nfrom collections import OrderedDict\ntry: \n    from apex_C import flatten\n    from apex_C import unflatten\nexcept ImportError:\n    try:\n        _ = warned_flatten\n    except NameError:\n        print(""Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten."")\n        warned_flatten = True\n    from torch._utils import _flatten_dense_tensors as flatten\n    from torch._utils import _unflatten_dense_tensors as unflatten\n\n\ndef warmup_linear(x, warmup=0.002):\n    if x < warmup:\n        return x/warmup\n    return 1.0 - x\n\n\ndef adjust_gradient_accumulation_steps(x, initial_steps, target_steps, warmup):\n    return min(max(int(x/warmup*target_steps), initial_steps), target_steps)\n\n\nclass DistributedCommunicator:\n    def __init__(self, accumulation_step=1):\n        hvd.init()\n        self.local_rank = hvd.local_rank()\n        self.world_size = hvd.size()\n        self.rank = hvd.rank()\n        self.n_gpu = torch.cuda.device_count()\n        self.node_count = self.world_size // self.n_gpu\n        self.accumulation_step = accumulation_step\n        self.count_down = accumulation_step - 1\n        self._multi_node = self.node_count > 1 \n        if not self._multi_node:\n            # use PyTorch build-in NCCL backend for single node training\n            torch.distributed.init_process_group(backend=\'nccl\', init_method=\'tcp://127.0.0.1:6000\',\n                                world_size=self.n_gpu,  rank=self.local_rank)\n\n\n    def register_model(self, model, fp16):\n        #  broadcast model parameters\n        if self.node_count > 1:\n            hvd.broadcast_parameters(model.state_dict(), root_rank=0)\n        else:\n            for param in model.parameters():\n                torch.distributed.broadcast_multigpu([param], 0)\n\n        # register hook for reduce when backpropagate\n        self._parameter_names = {v: k for k, v in sorted(model.named_parameters())}\n        self._handles = {}\n        self._requires_update = set()\n        self._grad_accs = []\n        self._grad = []\n        self._compression = hvd.Compression.fp16 if fp16 else hvd.Compression.none\n        for p in model.parameters():\n            if p.requires_grad:\n                p.grad = p.data.new(p.size()).zero_()\n                self._requires_update.add(p)\n                p_tmp = p.expand_as(p)\n                grad_acc = p_tmp.grad_fn.next_functions[0][0]\n                grad_acc.register_hook(self._make_hook(p))\n                self._grad_accs.append(grad_acc)\n\n\n    def _allreduce_tensor(self, p):\n        assert p not in self._handles\n        assert not p.grad.requires_grad\n        tensor = p.grad\n        name = self._parameter_names.get(p)\n        if self._multi_node: \n            tensor_compressed, ctx = self._compression.compress(tensor)\n            handle = allreduce_async_(tensor_compressed, average=True, name=name)\n            self._handles[p] = (handle, ctx)\n        else:\n            self._handles[p] = tensor\n\n\n    def _make_hook(self, p):\n        def hook(*ignore):\n            if self.count_down == 0:\n                self._allreduce_tensor(p)\n        return hook\n\n\n    def synchronize(self):\n        synced = False\n        if self.count_down == 0:\n            missing_p = self._requires_update - set(self._handles.keys())\n            for p in missing_p:\n                self._allreduce_tensor(p)\n\n            if self._multi_node:\n                for p, value in self._handles.items():\n                    handle, ctx = value\n                    output = synchronize(handle)\n                    p.grad.set_(self._compression.decompress(output, ctx) / self.accumulation_step)\n            else:\n                buckets = OrderedDict()\n                for tensor in self._handles.values():\n                    tp = tensor.type()\n                    if tp not in buckets:\n                        buckets[tp] = []\n                    buckets[tp].append(tensor)\n                for tp in buckets:\n                    bucket = buckets[tp]\n                    coalesced = flatten(bucket) / self.world_size / self.accumulation_step\n                    torch.distributed.all_reduce_multigpu([coalesced])\n                    for buf, synced in zip(bucket, unflatten(coalesced, bucket)):\n                        buf.copy_(synced)\n            self._handles.clear()\n            synced = True\n            self.count_down = self.accumulation_step\n\n        self.count_down -= 1\n        return synced\n\n    def set_accumulation_step(self, accumulation_step):\n        self.accumulation_step = accumulation_step\n        self.count_down = self.accumulation_step - 1'"
finetune/PyTorch/run_classifier_azureml.py,18,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HugginFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""BERT finetuning runner.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport csv\nimport os\nimport logging\nimport argparse\nimport random\nfrom tqdm import tqdm, trange\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom torch.utils.data.distributed import DistributedSampler\nimport torch.multiprocessing as mp\n\nfrom pytorch_pretrained_bert.tokenization import BertTokenizer\nfrom pytorch_pretrained_bert.modeling import BertForSequenceClassification\nfrom pytorch_pretrained_bert.optimization import BertAdam\nfrom pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\nfrom azureml_bert_util import *\nfrom azureml.core.run import Run\n\nlogging.basicConfig(format = \'%(asctime)s - %(levelname)s - %(name)s -   %(message)s\', \n                    datefmt = \'%m/%d/%Y %H:%M:%S\',\n                    level = logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass InputExample(object):\n    """"""A single training/test example for simple sequence classification.""""""\n\n    def __init__(self, guid, text_a, text_b=None, label=None):\n        """"""Constructs a InputExample.\n\n        Args:\n            guid: Unique id for the example.\n            text_a: string. The untokenized text of the first sequence. For single\n            sequence tasks, only this sequence must be specified.\n            text_b: (Optional) string. The untokenized text of the second sequence.\n            Only must be specified for sequence pair tasks.\n            label: (Optional) string. The label of the example. This should be\n            specified for train and dev examples, but not for test examples.\n        """"""\n        self.guid = guid\n        self.text_a = text_a\n        self.text_b = text_b\n        self.label = label\n\n\nclass InputFeatures(object):\n    """"""A single set of features of data.""""""\n\n    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n        self.input_ids = input_ids\n        self.input_mask = input_mask\n        self.segment_ids = segment_ids\n        self.label_id = label_id\n\n\nclass DataProcessor(object):\n    """"""Base class for data converters for sequence classification data sets.""""""\n\n    def get_train_examples(self, data_dir):\n        """"""Gets a collection of `InputExample`s for the train set.""""""\n        raise NotImplementedError()\n\n    def get_dev_examples(self, data_dir):\n        """"""Gets a collection of `InputExample`s for the dev set.""""""\n        raise NotImplementedError()\n\n    def get_labels(self):\n        """"""Gets the list of labels for this data set.""""""\n        raise NotImplementedError()\n\n    @classmethod\n    def _read_tsv(cls, input_file, quotechar=None):\n        """"""Reads a tab separated value file.""""""\n        with open(input_file, ""r"") as f:\n            reader = csv.reader(f, delimiter=""\\t"", quotechar=quotechar)\n            lines = []\n            for line in reader:\n                lines.append(line)\n            return lines\n\n\nclass MrpcProcessor(DataProcessor):\n    """"""Processor for the MRPC data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        logger.info(""LOOKING AT {}"".format(os.path.join(data_dir, ""train.tsv"")))\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""0"", ""1""]\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, i)\n            text_a = line[3]\n            text_b = line[4]\n            label = line[0]\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n        return examples\n\n\nclass MnliProcessor(DataProcessor):\n    """"""Processor for the MultiNLI data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev_matched.tsv"")),\n            ""dev_matched"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""contradiction"", ""entailment"", ""neutral""]\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            if i == 0:\n                continue\n            guid = ""%s-%s"" % (set_type, line[0])\n            text_a = line[8]\n            text_b = line[9]\n            label = line[-1]\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n        return examples\n\n\nclass ColaProcessor(DataProcessor):\n    """"""Processor for the CoLA data set (GLUE version).""""""\n\n    def get_train_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n    def get_dev_examples(self, data_dir):\n        """"""See base class.""""""\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n\n    def get_labels(self):\n        """"""See base class.""""""\n        return [""0"", ""1""]\n\n    def _create_examples(self, lines, set_type):\n        """"""Creates examples for the training and dev sets.""""""\n        examples = []\n        for (i, line) in enumerate(lines):\n            guid = ""%s-%s"" % (set_type, i)\n            text_a = line[3]\n            label = line[1]\n            examples.append(\n                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n        return examples\n\n\ndef convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n    """"""Loads a data file into a list of `InputBatch`s.""""""\n\n    label_map = {}\n    for (i, label) in enumerate(label_list):\n        label_map[label] = i\n\n    features = []\n    for (ex_index, example) in enumerate(examples):\n        tokens_a = tokenizer.tokenize(example.text_a)\n\n        tokens_b = None\n        if example.text_b:\n            tokens_b = tokenizer.tokenize(example.text_b)\n\n        if tokens_b:\n            # Modifies `tokens_a` and `tokens_b` in place so that the total\n            # length is less than the specified length.\n            # Account for [CLS], [SEP], [SEP] with ""- 3""\n            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n        else:\n            # Account for [CLS] and [SEP] with ""- 2""\n            if len(tokens_a) > max_seq_length - 2:\n                tokens_a = tokens_a[0:(max_seq_length - 2)]\n\n        # The convention in BERT is:\n        # (a) For sequence pairs:\n        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n        # (b) For single sequences:\n        #  tokens:   [CLS] the dog is hairy . [SEP]\n        #  type_ids: 0   0   0   0  0     0 0\n        #\n        # Where ""type_ids"" are used to indicate whether this is the first\n        # sequence or the second sequence. The embedding vectors for `type=0` and\n        # `type=1` were learned during pre-training and are added to the wordpiece\n        # embedding vector (and position vector). This is not *strictly* necessary\n        # since the [SEP] token unambigiously separates the sequences, but it makes\n        # it easier for the model to learn the concept of sequences.\n        #\n        # For classification tasks, the first vector (corresponding to [CLS]) is\n        # used as as the ""sentence vector"". Note that this only makes sense because\n        # the entire model is fine-tuned.\n        tokens = []\n        segment_ids = []\n        tokens.append(""[CLS]"")\n        segment_ids.append(0)\n        for token in tokens_a:\n            tokens.append(token)\n            segment_ids.append(0)\n        tokens.append(""[SEP]"")\n        segment_ids.append(0)\n\n        if tokens_b:\n            for token in tokens_b:\n                tokens.append(token)\n                segment_ids.append(1)\n            tokens.append(""[SEP]"")\n            segment_ids.append(1)\n\n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n        # tokens are attended to.\n        input_mask = [1] * len(input_ids)\n\n        # Zero-pad up to the sequence length.\n        while len(input_ids) < max_seq_length:\n            input_ids.append(0)\n            input_mask.append(0)\n            segment_ids.append(0)\n\n        assert len(input_ids) == max_seq_length\n        assert len(input_mask) == max_seq_length\n        assert len(segment_ids) == max_seq_length\n\n        label_id = label_map[example.label]\n        if ex_index < 5:\n            logger.info(""*** Example ***"")\n            logger.info(""guid: %s"" % (example.guid))\n            logger.info(""tokens: %s"" % "" "".join(\n                    [str(x) for x in tokens]))\n            logger.info(""input_ids: %s"" % "" "".join([str(x) for x in input_ids]))\n            logger.info(""input_mask: %s"" % "" "".join([str(x) for x in input_mask]))\n            logger.info(\n                    ""segment_ids: %s"" % "" "".join([str(x) for x in segment_ids]))\n            logger.info(""label: %s (id = %d)"" % (example.label, label_id))\n\n        features.append(\n                InputFeatures(input_ids=input_ids,\n                              input_mask=input_mask,\n                              segment_ids=segment_ids,\n                              label_id=label_id))\n    return features\n\n\ndef _truncate_seq_pair(tokens_a, tokens_b, max_length):\n    """"""Truncates a sequence pair in place to the maximum length.""""""\n\n    # This is a simple heuristic which will always truncate the longer sequence\n    # one token at a time. This makes more sense than truncating an equal percent\n    # of tokens from each, since if one sequence is very short then each token\n    # that\'s truncated likely contains more information than a longer sequence.\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_length:\n            break\n        if len(tokens_a) > len(tokens_b):\n            tokens_a.pop()\n        else:\n            tokens_b.pop()\n\ndef accuracy(out, labels):\n    outputs = np.argmax(out, axis=1)\n    return np.sum(outputs == labels)\n\ndef copy_optimizer_params_to_model(named_params_model, named_params_optimizer):\n    """""" Utility function for optimize_on_cpu and 16-bits training.\n        Copy the parameters optimized on CPU/RAM back to the model on GPU\n    """"""\n    for (name_opti, param_opti), (name_model, param_model) in zip(named_params_optimizer, named_params_model):\n        if name_opti != name_model:\n            logger.error(""name_opti != name_model: {} {}"".format(name_opti, name_model))\n            raise ValueError\n        param_model.data.copy_(param_opti.data)\n\ndef set_optimizer_params_grad(named_params_optimizer, named_params_model, test_nan=False):\n    """""" Utility function for optimize_on_cpu and 16-bits training.\n        Copy the gradient of the GPU parameters to the CPU/RAMM copy of the model\n    """"""\n    is_nan = False\n    for (name_opti, param_opti), (name_model, param_model) in zip(named_params_optimizer, named_params_model):\n        if name_opti != name_model:\n            logger.error(""name_opti != name_model: {} {}"".format(name_opti, name_model))\n            raise ValueError\n        if param_model.grad is not None:\n            if test_nan and torch.isnan(param_model.grad).sum() > 0:\n                is_nan = True\n            if param_opti.grad is None:\n                param_opti.grad = torch.nn.Parameter(param_opti.data.new().resize_(*param_opti.data.size()))\n            param_opti.grad.data.copy_(param_model.grad.data)\n        else:\n            param_opti.grad = None\n    return is_nan\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n\n    ## Required parameters\n    parser.add_argument(""--data_dir"",\n                        default=None,\n                        type=str,\n                        required=True,\n                        help=""The input data dir. Should contain the .tsv files (or other data files) for the task."")\n    parser.add_argument(""--bert_model"", default=None, type=str, required=True,\n                        help=""Bert pre-trained model selected in the list: bert-base-uncased, ""\n                             ""bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese."")\n    parser.add_argument(""--task_name"",\n                        default=None,\n                        type=str,\n                        required=True,\n                        help=""The name of the task to train."")\n    parser.add_argument(""--output_dir"", default=None, type=str, required=True,\n                        help=""The output directory where the model checkpoints will be written."")\n                        \n    ## Other parameters\n    parser.add_argument(""--max_seq_length"",\n                        default=128,\n                        type=int,\n                        help=""The maximum total input sequence length after WordPiece tokenization. \\n""\n                             ""Sequences longer than this will be truncated, and sequences shorter \\n""\n                             ""than this will be padded."")\n    parser.add_argument(""--do_train"",\n                        default=False,\n                        action=\'store_true\',\n                        help=""Whether to run training."")\n    parser.add_argument(""--do_eval"",\n                        default=False,\n                        action=\'store_true\',\n                        help=""Whether to run eval on the dev set."")\n    parser.add_argument(""--do_lower_case"",\n                        default=False,\n                        action=\'store_true\',\n                        help=""Set this flag if you are using an uncased model."")\n    parser.add_argument(""--train_batch_size"",\n                        default=32,\n                        type=int,\n                        help=""Total batch size for training."")\n    parser.add_argument(""--eval_batch_size"",\n                        default=8,\n                        type=int,\n                        help=""Total batch size for eval."")\n    parser.add_argument(""--learning_rate"",\n                        default=5e-5,\n                        type=float,\n                        help=""The initial learning rate for Adam."")\n    parser.add_argument(""--num_train_epochs"",\n                        default=3.0,\n                        type=float,\n                        help=""Total number of training epochs to perform."")\n    parser.add_argument(""--warmup_proportion"",\n                        default=0.1,\n                        type=float,\n                        help=""Proportion of training to perform linear learning rate warmup for. ""\n                             ""E.g., 0.1 = 10%% of training."")\n    parser.add_argument(""--no_cuda"",\n                        default=False,\n                        action=\'store_true\',\n                        help=""Whether not to use CUDA when available"")\n    parser.add_argument(""--local_rank"",\n                        type=int,\n                        default=-1,\n                        help=""local_rank for distributed training on gpus"")\n    parser.add_argument(\'--seed\', \n                        type=int, \n                        default=42,\n                        help=""random seed for initialization"")\n    parser.add_argument(\'--gradient_accumulation_steps\',\n                        type=int,\n                        default=1,\n                        help=""Number of updates steps to accumulate before performing a backward/update pass."")                       \n    parser.add_argument(\'--optimize_on_cpu\',\n                        default=False,\n                        action=\'store_true\',\n                        help=""Whether to perform optimization and keep the optimizer averages on CPU"")\n    parser.add_argument(\'--fp16\',\n                        default=False,\n                        action=\'store_true\',\n                        help=""Whether to use 16-bit float precision instead of 32-bit"")\n    parser.add_argument(\'--loss_scale\',\n                        type=float, default=128,\n                        help=\'Loss scaling, positive power of 2 values can improve fp16 convergence.\')\n    parser.add_argument(\'--step_per_log\',\n                        type=int, default=5,\n                        help=\'Number of updates steps to log metrics.\')\n    parser.add_argument(""--process_count_per_node"", default=1, type=int,\n                        help=""Total number of process count to launch per node."")   \n\n    args = parser.parse_args()\n\n    run = Run.get_context()\n    \n    processors = {\n        ""cola"": ColaProcessor,\n        ""mnli"": MnliProcessor,\n        ""mrpc"": MrpcProcessor,\n    }\n\n    comm = DistributedCommunicator(accumulation_step=args.gradient_accumulation_steps)\n    rank = comm.rank\n    local_rank = comm.local_rank\n    world_size = comm.world_size\n    is_master = rank == 0\n    logger.info(""world size: {}, local rank: {}, global rank: {}, fp16: {}"".format(world_size, local_rank, rank, args.fp16))\n\n    torch.cuda.set_device(local_rank)\n    device = torch.device(""cuda"", local_rank)\n\n    if os.path.exists(args.output_dir) and os.listdir(args.output_dir):\n        raise ValueError(""Output directory () already exists and is not empty."")\n    os.makedirs(args.output_dir, exist_ok=True)\n    output_model_file = os.path.join(args.output_dir, ""pytorch_model.bin"")\n\n    if args.gradient_accumulation_steps < 1:\n        raise ValueError(""Invalid gradient_accumulation_steps parameter: {}, should be >= 1"".format(\n                            args.gradient_accumulation_steps))\n\n    if local_rank == -1:\n        args.train_batch_size = int(args.train_batch_size / args.gradient_accumulation_steps)\n\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n\n    if not args.do_train and not args.do_eval:\n        raise ValueError(""At least one of `do_train` or `do_eval` must be True."")\n\n    task_name = args.task_name.lower()\n\n    is_master = (local_rank == -1 or rank == 0)\n    if task_name not in processors:\n        raise ValueError(""Task not found: %s"" % (task_name))\n\n    processor = processors[task_name]()\n    label_list = processor.get_labels()\n\n    tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n\n    train_examples = None\n    num_train_steps = None\n    if args.do_train:\n        train_examples = processor.get_train_examples(args.data_dir)\n        num_train_steps = int(\n            len(train_examples) / args.train_batch_size / args.gradient_accumulation_steps * args.num_train_epochs)\n\n    # Prepare model\n    model = BertForSequenceClassification.from_pretrained(args.bert_model, \n                cache_dir=PYTORCH_PRETRAINED_BERT_CACHE / \'distributed_{}\'.format(local_rank))\n    if args.fp16:\n        model.half()\n    model.to(device)\n    comm.register_model(model, args.fp16)\n\n    if args.do_train:\n \n        param_optimizer = list(model.named_parameters())\n\n        # hack to remove pooler, which is not used\n        # thus it produce None grad that break apex\n        param_optimizer = [n for n in param_optimizer if \'pooler\' not in n[0]]\n\n        no_decay = [\'bias\', \'LayerNorm.bias\', \'LayerNorm.weight\']\n        optimizer_grouped_parameters = [\n            {\'params\': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \'weight_decay\': 0.01},\n            {\'params\': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \'weight_decay\': 0.0}\n            ]\n        t_total = num_train_steps // world_size\n\n        if args.fp16:\n            try:\n                from apex.optimizers import FP16_Optimizer\n                from apex.optimizers import FusedAdam\n            except ImportError:\n                raise ImportError(""Please install apex from https://www.github.com/nvidia/apex to run this."")\n\n            optimizer = FusedAdam(optimizer_grouped_parameters,\n                                lr=args.learning_rate,\n                                bias_correction=False,\n                                max_grad_norm=1.0)\n            if args.loss_scale == 0:\n                optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n            else:\n                optimizer = FP16_Optimizer(optimizer, static_loss_scale=args.loss_scale)\n        else:\n            optimizer = BertAdam(optimizer_grouped_parameters,\n                            lr=args.learning_rate,\n                            warmup=args.warmup_proportion,\n                            t_total=t_total)\n        if is_master:\n            run.log(\'lr\', np.float(args.learning_rate))\n \n        train_features = convert_examples_to_features(\n            train_examples, label_list, args.max_seq_length, tokenizer)\n        logger.info(""***** Running training *****"")\n        logger.info(""  Num examples = %d"", len(train_examples))\n        logger.info(""  Batch size = %d"", args.train_batch_size)\n        logger.info(""  Num steps = %d"", num_train_steps)\n        all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n        all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n        all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n        all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n        train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n        if local_rank != -1 and world_size > 1:\n            train_sampler = DistributedSampler(train_data)\n        else:\n            train_sampler = RandomSampler(train_data)\n        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size)\n\n        global_step, tr_loss = 0, 0\n        model.train()\n        for _ in trange(int(args.num_train_epochs), desc=""Epoch""):\n            for _, batch in enumerate(tqdm(train_dataloader, desc=""Iteration"")):\n                batch = tuple(t.to(device) for t in batch)\n                input_ids, input_mask, segment_ids, label_ids = batch\n                loss = model(input_ids, segment_ids, input_mask, label_ids)\n                loss = loss / args.gradient_accumulation_steps\n                loss.backward()\n                global_step += 1\n                tr_loss += loss.item()\n                if comm.synchronize():\n                    lr_this_step = args.learning_rate * warmup_linear(global_step/t_total, args.warmup_proportion)\n                    for param_group in optimizer.param_groups:\n                        param_group[\'lr\'] = lr_this_step\n                    optimizer.step()\n                    model.zero_grad()\n                if is_master and (global_step + 1) % args.step_per_log == 0:\n                    run.log(\'train_loss\', np.float(tr_loss / args.step_per_log))\n                    tr_loss = 0\n        if is_master:\n            # Save a trained model\n            torch.save(model.state_dict(), output_model_file)\n\n    if args.do_eval and is_master:\n        eval_examples = processor.get_dev_examples(args.data_dir)\n        eval_features = convert_examples_to_features(\n            eval_examples, label_list, args.max_seq_length, tokenizer)\n        logger.info(""***** Running evaluation *****"")\n        logger.info(""  Num examples = %d"", len(eval_examples))\n        logger.info(""  Batch size = %d"", args.eval_batch_size)\n        all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n        all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n        all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n        all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n        eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n        # Run prediction for full data\n        eval_sampler = SequentialSampler(eval_data)\n        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n        model.eval()\n        eval_loss, eval_accuracy = 0, 0\n        nb_eval_steps, nb_eval_examples = 0, 0\n        for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n            input_ids = input_ids.to(device)\n            input_mask = input_mask.to(device)\n            segment_ids = segment_ids.to(device)\n            label_ids = label_ids.to(device)\n            with torch.no_grad():\n                tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n                logits = model(input_ids, segment_ids, input_mask)\n            logits = logits.detach().cpu().numpy()\n            label_ids = label_ids.to(\'cpu\').numpy()\n            tmp_eval_accuracy = accuracy(logits, label_ids)\n            eval_loss += tmp_eval_loss.mean().item()\n            eval_accuracy += tmp_eval_accuracy\n            nb_eval_examples += input_ids.size(0)\n            nb_eval_steps += 1\n\n        eval_loss = eval_loss / nb_eval_steps\n        eval_accuracy = eval_accuracy / nb_eval_examples\n        result = {\'eval_loss\': eval_loss,\n                    \'eval_accuracy\': eval_accuracy}\n        logger.info(""***** Eval results *****"")\n        for key in sorted(result.keys()):\n            logger.info(""  %s = %s"", key, str(result[key]))\n            run.log(key, str(result[key]))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
finetune/TensorFlow/download_model_and_dataset.py,0,"b'from __future__ import print_function\nimport argparse\nimport sys\nimport os\nimport shutil\nimport zipfile\nimport urllib\n\nparser = argparse.ArgumentParser()\n\n## Required parameters\nparser.add_argument(""--bert_model_name"",\n                    default = None,\n                    type = str,\n                    required = True,\n                    help = ""Name of pretrained BERT model. Possible values: ""\n                           ""uncased_L-12_H-768_A-12,uncased_L-24_H-1024_A-16,cased_L-12_H-768_A-12,""\n                           ""multilingual_L-12_H-768_A-12,chinese_L-12_H-768_A-12"")\n\nparser.add_argument(""--model_dump_path"",\n                    default = None,\n                    type = str,\n                    required = True,\n                    help = ""Path to the output model."")\n\nparser.add_argument(""--glue_data_path"",\n                    default = None,\n                    type = str,\n                    required = True,\n                    help = ""Path to store downloaded GLUE dataset"")\n\nargs = parser.parse_args()\n\nbert_model_url_map = {\n    \'uncased_L-12_H-768_A-12\': \'https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\',\n    \'uncased_L-24_H-1024_A-16\': \'https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip\',\n    \'cased_L-12_H-768_A-12\': \'https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\',\n    \'multilingual_L-12_H-768_A-12\': \'https://storage.googleapis.com/bert_models/2018_11_03/multilingual_L-12_H-768_A-12.zip\',\n    \'chinese_L-12_H-768_A-12\': \'https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\'\n}\n\nif args.bert_model_name not in bert_model_url_map:\n    sys.stderr.write(\'Unknown BERT model name \' + args.bert_model_name)\n    sys.exit(1)\n\npretrained_model_url = bert_model_url_map.get(args.bert_model_name)\n\n# make local directory for pretrained tensorflow BERT model\ntensorflow_model_dir = \'./tensorflow_model\'\nif not os.path.exists(tensorflow_model_dir):\n    os.makedirs(tensorflow_model_dir)\n\n# download and extract pretrained tensorflow BERT model\ndownload_file_name = \'tensorflow_model.zip\'\nurllib.request.urlretrieve(pretrained_model_url, filename=download_file_name)\nprint(\'Extracting pretrained model...\')\nwith zipfile.ZipFile(download_file_name, \'r\') as z:\n    z.extractall(tensorflow_model_dir)\n\n# make destination path\nif not os.path.exists(args.model_dump_path):\n    os.makedirs(args.model_dump_path)\n\nfiles = [\'bert_model.ckpt.meta\', \'bert_model.ckpt.index\', \'bert_model.ckpt.data-00000-of-00001\', \'bert_config.json\', \'vocab.txt\']\nfor file in files:\n    shutil.copy(os.path.join(tensorflow_model_dir, args.bert_model_name, file), os.path.join(args.model_dump_path, file))\n\nprint(\'Start to download GLUE dataset...\\n\')\nurllib.request.urlretrieve(\n    \'https://gist.githubusercontent.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e/raw/17b8dd0d724281ed7c3b2aeeda662b92809aadd5/download_glue_data.py\',\n    filename=\'download_glue_data.py\')\nif os.system(\'python download_glue_data.py --data_dir {0} --tasks all\'.format(args.glue_data_path)) != 0: sys.exit(1)'"
finetune/TensorFlow/run_classifier.py,0,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""BERT finetuning runner.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport csv\nimport os\nimport modeling\nimport optimization\nimport tokenization\nimport tensorflow as tf\nimport numpy as np\nfrom azureml.core.run import Run\n# get the Azure ML run object\nrun = Run.get_context()\nflags = tf.flags\n\nFLAGS = flags.FLAGS\n\n## Required parameters\nflags.DEFINE_string(\n    ""data_dir"", None,\n    ""The input data dir. Should contain the .tsv files (or other data files) ""\n    ""for the task."")\n\nflags.DEFINE_string(\n    ""bert_config_file"", None,\n    ""The config json file corresponding to the pre-trained BERT model. ""\n    ""This specifies the model architecture."")\n\nflags.DEFINE_string(""task_name"", None, ""The name of the task to train."")\n\nflags.DEFINE_string(""vocab_file"", None,\n                    ""The vocabulary file that the BERT model was trained on."")\n\nflags.DEFINE_string(\n    ""output_dir"", None,\n    ""The output directory where the model checkpoints will be written."")\n\n## Other parameters\n\nflags.DEFINE_string(\n    ""init_checkpoint"", None,\n    ""Initial checkpoint (usually from a pre-trained BERT model)."")\n\nflags.DEFINE_bool(\n    ""do_lower_case"", True,\n    ""Whether to lower case the input text. Should be True for uncased ""\n    ""models and False for cased models."")\n\nflags.DEFINE_integer(\n    ""max_seq_length"", 128,\n    ""The maximum total input sequence length after WordPiece tokenization. ""\n    ""Sequences longer than this will be truncated, and sequences shorter ""\n    ""than this will be padded."")\n\nflags.DEFINE_bool(""do_train"", False, ""Whether to run training."")\n\nflags.DEFINE_bool(""do_eval"", False, ""Whether to run eval on the dev set."")\n\nflags.DEFINE_bool(\n    ""do_predict"", False,\n    ""Whether to run the model in inference mode on the test set."")\n\nflags.DEFINE_integer(""train_batch_size"", 32, ""Total batch size for training."")\n\nflags.DEFINE_integer(""eval_batch_size"", 8, ""Total batch size for eval."")\n\nflags.DEFINE_integer(""predict_batch_size"", 8, ""Total batch size for predict."")\n\nflags.DEFINE_float(""learning_rate"", 5e-5, ""The initial learning rate for Adam."")\n\nflags.DEFINE_float(""num_train_epochs"", 3.0,\n                   ""Total number of training epochs to perform."")\n\nflags.DEFINE_float(\n    ""warmup_proportion"", 0.1,\n    ""Proportion of training to perform linear learning rate warmup for. ""\n    ""E.g., 0.1 = 10% of training."")\n\nflags.DEFINE_integer(""save_checkpoints_steps"", 1000,\n                     ""How often to save the model checkpoint."")\n\nflags.DEFINE_integer(""iterations_per_loop"", 1000,\n                     ""How many steps to make in each estimator call."")\n\nflags.DEFINE_bool(""use_tpu"", False, ""Whether to use TPU or GPU/CPU."")\n\ntf.flags.DEFINE_string(\n    ""tpu_name"", None,\n    ""The Cloud TPU to use for training. This should be either the name ""\n    ""used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 ""\n    ""url."")\n\ntf.flags.DEFINE_string(\n    ""tpu_zone"", None,\n    ""[Optional] GCE zone where the Cloud TPU is located in. If not ""\n    ""specified, we will attempt to automatically detect the GCE project from ""\n    ""metadata."")\n\ntf.flags.DEFINE_string(\n    ""gcp_project"", None,\n    ""[Optional] Project name for the Cloud TPU-enabled project. If not ""\n    ""specified, we will attempt to automatically detect the GCE project from ""\n    ""metadata."")\n\ntf.flags.DEFINE_string(""master"", None, ""[Optional] TensorFlow master URL."")\n\nflags.DEFINE_integer(\n    ""num_tpu_cores"", 8,\n    ""Only used if `use_tpu` is True. Total number of TPU cores to use."")\n\n\nclass InputExample(object):\n  """"""A single training/test example for simple sequence classification.""""""\n\n  def __init__(self, guid, text_a, text_b=None, label=None):\n    """"""Constructs a InputExample.\n\n    Args:\n      guid: Unique id for the example.\n      text_a: string. The untokenized text of the first sequence. For single\n        sequence tasks, only this sequence must be specified.\n      text_b: (Optional) string. The untokenized text of the second sequence.\n        Only must be specified for sequence pair tasks.\n      label: (Optional) string. The label of the example. This should be\n        specified for train and dev examples, but not for test examples.\n    """"""\n    self.guid = guid\n    self.text_a = text_a\n    self.text_b = text_b\n    self.label = label\n\n\nclass InputFeatures(object):\n  """"""A single set of features of data.""""""\n\n  def __init__(self, input_ids, input_mask, segment_ids, label_id):\n    self.input_ids = input_ids\n    self.input_mask = input_mask\n    self.segment_ids = segment_ids\n    self.label_id = label_id\n\n\nclass DataProcessor(object):\n  """"""Base class for data converters for sequence classification data sets.""""""\n\n  def get_train_examples(self, data_dir):\n    """"""Gets a collection of `InputExample`s for the train set.""""""\n    raise NotImplementedError()\n\n  def get_dev_examples(self, data_dir):\n    """"""Gets a collection of `InputExample`s for the dev set.""""""\n    raise NotImplementedError()\n\n  def get_test_examples(self, data_dir):\n    """"""Gets a collection of `InputExample`s for prediction.""""""\n    raise NotImplementedError()\n\n  def get_labels(self):\n    """"""Gets the list of labels for this data set.""""""\n    raise NotImplementedError()\n\n  @classmethod\n  def _read_tsv(cls, input_file, quotechar=None):\n    """"""Reads a tab separated value file.""""""\n    with tf.gfile.Open(input_file, ""r"") as f:\n      reader = csv.reader(f, delimiter=""\\t"", quotechar=quotechar)\n      lines = []\n      for line in reader:\n        lines.append(line)\n      return lines\n\n\nclass XnliProcessor(DataProcessor):\n  """"""Processor for the XNLI data set.""""""\n\n  def __init__(self):\n    self.language = ""zh""\n\n  def get_train_examples(self, data_dir):\n    """"""See base class.""""""\n    lines = self._read_tsv(\n        os.path.join(data_dir, ""multinli"",\n                     ""multinli.train.%s.tsv"" % self.language))\n    examples = []\n    for (i, line) in enumerate(lines):\n      if i == 0:\n        continue\n      guid = ""train-%d"" % (i)\n      text_a = tokenization.convert_to_unicode(line[0])\n      text_b = tokenization.convert_to_unicode(line[1])\n      label = tokenization.convert_to_unicode(line[2])\n      if label == tokenization.convert_to_unicode(""contradictory""):\n        label = tokenization.convert_to_unicode(""contradiction"")\n      examples.append(\n          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n  def get_dev_examples(self, data_dir):\n    """"""See base class.""""""\n    lines = self._read_tsv(os.path.join(data_dir, ""xnli.dev.tsv""))\n    examples = []\n    for (i, line) in enumerate(lines):\n      if i == 0:\n        continue\n      guid = ""dev-%d"" % (i)\n      language = tokenization.convert_to_unicode(line[0])\n      if language != tokenization.convert_to_unicode(self.language):\n        continue\n      text_a = tokenization.convert_to_unicode(line[6])\n      text_b = tokenization.convert_to_unicode(line[7])\n      label = tokenization.convert_to_unicode(line[1])\n      examples.append(\n          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n  def get_labels(self):\n    """"""See base class.""""""\n    return [""contradiction"", ""entailment"", ""neutral""]\n\n\nclass MnliProcessor(DataProcessor):\n  """"""Processor for the MultiNLI data set (GLUE version).""""""\n\n  def get_train_examples(self, data_dir):\n    """"""See base class.""""""\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n  def get_dev_examples(self, data_dir):\n    """"""See base class.""""""\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, ""dev_matched.tsv"")),\n        ""dev_matched"")\n\n  def get_test_examples(self, data_dir):\n    """"""See base class.""""""\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, ""test_matched.tsv"")), ""test"")\n\n  def get_labels(self):\n    """"""See base class.""""""\n    return [""contradiction"", ""entailment"", ""neutral""]\n\n  def _create_examples(self, lines, set_type):\n    """"""Creates examples for the training and dev sets.""""""\n    examples = []\n    for (i, line) in enumerate(lines):\n      if i == 0:\n        continue\n      guid = ""%s-%s"" % (set_type, tokenization.convert_to_unicode(line[0]))\n      text_a = tokenization.convert_to_unicode(line[8])\n      text_b = tokenization.convert_to_unicode(line[9])\n      if set_type == ""test"":\n        label = ""contradiction""\n      else:\n        label = tokenization.convert_to_unicode(line[-1])\n      examples.append(\n          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n\nclass MrpcProcessor(DataProcessor):\n  """"""Processor for the MRPC data set (GLUE version).""""""\n\n  def get_train_examples(self, data_dir):\n    """"""See base class.""""""\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n  def get_dev_examples(self, data_dir):\n    """"""See base class.""""""\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n\n  def get_test_examples(self, data_dir):\n    """"""See base class.""""""\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, ""test.tsv"")), ""test"")\n\n  def get_labels(self):\n    """"""See base class.""""""\n    return [""0"", ""1""]\n\n  def _create_examples(self, lines, set_type):\n    """"""Creates examples for the training and dev sets.""""""\n    examples = []\n    for (i, line) in enumerate(lines):\n      if i == 0:\n        continue\n      guid = ""%s-%s"" % (set_type, i)\n      text_a = tokenization.convert_to_unicode(line[3])\n      text_b = tokenization.convert_to_unicode(line[4])\n      if set_type == ""test"":\n        label = ""0""\n      else:\n        label = tokenization.convert_to_unicode(line[0])\n      examples.append(\n          InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n    return examples\n\n\nclass ColaProcessor(DataProcessor):\n  """"""Processor for the CoLA data set (GLUE version).""""""\n\n  def get_train_examples(self, data_dir):\n    """"""See base class.""""""\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, ""train.tsv"")), ""train"")\n\n  def get_dev_examples(self, data_dir):\n    """"""See base class.""""""\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, ""dev.tsv"")), ""dev"")\n\n  def get_test_examples(self, data_dir):\n    """"""See base class.""""""\n    return self._create_examples(\n        self._read_tsv(os.path.join(data_dir, ""test.tsv"")), ""test"")\n\n  def get_labels(self):\n    """"""See base class.""""""\n    return [""0"", ""1""]\n\n  def _create_examples(self, lines, set_type):\n    """"""Creates examples for the training and dev sets.""""""\n    examples = []\n    for (i, line) in enumerate(lines):\n      # Only the test set has a header\n      if set_type == ""test"" and i == 0:\n        continue\n      guid = ""%s-%s"" % (set_type, i)\n      if set_type == ""test"":\n        text_a = tokenization.convert_to_unicode(line[1])\n        label = ""0""\n      else:\n        text_a = tokenization.convert_to_unicode(line[3])\n        label = tokenization.convert_to_unicode(line[1])\n      examples.append(\n          InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n    return examples\n\n\nclass _MetricLogger(tf.train.SessionRunHook):\n  def __init__(self, mode):\n    self._mode = mode\n    \n  def begin(self):\n    self._step = 0\n    self._loss_total = 0\n  \n  def before_run(self, run_context):\n    pass\n    self._step += 1\n    graph = run_context.session.graph\n    tensor_name = ""loss/Mean:0""\n    loss_tensor = graph.get_tensor_by_name(tensor_name)\n    return tf.train.SessionRunArgs(loss_tensor)\n    \n  def after_run(self, run_context, run_values):\n    loss_value = run_values.results\n    self._loss_total += loss_value\n    if self._step % 2 == 0:\n      mean_loss = self._loss_total / self._step\n      run.log(\'%s_mean_loss\' % self._mode, mean_loss)\n      run.log(\'%s_example_loss\' % self._mode, loss_value)\n\n    \ndef convert_single_example(ex_index, example, label_list, max_seq_length,\n                           tokenizer):\n  """"""Converts a single `InputExample` into a single `InputFeatures`.""""""\n  label_map = {}\n  for (i, label) in enumerate(label_list):\n    label_map[label] = i\n\n  tokens_a = tokenizer.tokenize(example.text_a)\n  tokens_b = None\n  if example.text_b:\n    tokens_b = tokenizer.tokenize(example.text_b)\n\n  if tokens_b:\n    # Modifies `tokens_a` and `tokens_b` in place so that the total\n    # length is less than the specified length.\n    # Account for [CLS], [SEP], [SEP] with ""- 3""\n    _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n  else:\n    # Account for [CLS] and [SEP] with ""- 2""\n    if len(tokens_a) > max_seq_length - 2:\n      tokens_a = tokens_a[0:(max_seq_length - 2)]\n\n  # The convention in BERT is:\n  # (a) For sequence pairs:\n  #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n  #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n  # (b) For single sequences:\n  #  tokens:   [CLS] the dog is hairy . [SEP]\n  #  type_ids: 0     0   0   0  0     0 0\n  #\n  # Where ""type_ids"" are used to indicate whether this is the first\n  # sequence or the second sequence. The embedding vectors for `type=0` and\n  # `type=1` were learned during pre-training and are added to the wordpiece\n  # embedding vector (and position vector). This is not *strictly* necessary\n  # since the [SEP] token unambiguously separates the sequences, but it makes\n  # it easier for the model to learn the concept of sequences.\n  #\n  # For classification tasks, the first vector (corresponding to [CLS]) is\n  # used as as the ""sentence vector"". Note that this only makes sense because\n  # the entire model is fine-tuned.\n  tokens = []\n  segment_ids = []\n  tokens.append(""[CLS]"")\n  segment_ids.append(0)\n  for token in tokens_a:\n    tokens.append(token)\n    segment_ids.append(0)\n  tokens.append(""[SEP]"")\n  segment_ids.append(0)\n\n  if tokens_b:\n    for token in tokens_b:\n      tokens.append(token)\n      segment_ids.append(1)\n    tokens.append(""[SEP]"")\n    segment_ids.append(1)\n\n  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n  # The mask has 1 for real tokens and 0 for padding tokens. Only real\n  # tokens are attended to.\n  input_mask = [1] * len(input_ids)\n\n  # Zero-pad up to the sequence length.\n  while len(input_ids) < max_seq_length:\n    input_ids.append(0)\n    input_mask.append(0)\n    segment_ids.append(0)\n\n  assert len(input_ids) == max_seq_length\n  assert len(input_mask) == max_seq_length\n  assert len(segment_ids) == max_seq_length\n\n  label_id = label_map[example.label]\n  if ex_index < 5:\n    tf.logging.info(""*** Example ***"")\n    tf.logging.info(""guid: %s"" % (example.guid))\n    tf.logging.info(""tokens: %s"" % "" "".join(\n        [tokenization.printable_text(x) for x in tokens]))\n    tf.logging.info(""input_ids: %s"" % "" "".join([str(x) for x in input_ids]))\n    tf.logging.info(""input_mask: %s"" % "" "".join([str(x) for x in input_mask]))\n    tf.logging.info(""segment_ids: %s"" % "" "".join([str(x) for x in segment_ids]))\n    tf.logging.info(""label: %s (id = %d)"" % (example.label, label_id))\n\n  feature = InputFeatures(\n      input_ids=input_ids,\n      input_mask=input_mask,\n      segment_ids=segment_ids,\n      label_id=label_id)\n  return feature\n\n\ndef file_based_convert_examples_to_features(\n    examples, label_list, max_seq_length, tokenizer, output_file):\n  """"""Convert a set of `InputExample`s to a TFRecord file.""""""\n\n  writer = tf.python_io.TFRecordWriter(output_file)\n\n  for (ex_index, example) in enumerate(examples):\n    if ex_index % 10000 == 0:\n      tf.logging.info(""Writing example %d of %d"" % (ex_index, len(examples)))\n\n    feature = convert_single_example(ex_index, example, label_list,\n                                     max_seq_length, tokenizer)\n\n    def create_int_feature(values):\n      f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n      return f\n\n    features = collections.OrderedDict()\n    features[""input_ids""] = create_int_feature(feature.input_ids)\n    features[""input_mask""] = create_int_feature(feature.input_mask)\n    features[""segment_ids""] = create_int_feature(feature.segment_ids)\n    features[""label_ids""] = create_int_feature([feature.label_id])\n\n    tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n    writer.write(tf_example.SerializeToString())\n\n\ndef file_based_input_fn_builder(input_file, seq_length, is_training,\n                                drop_remainder):\n  """"""Creates an `input_fn` closure to be passed to TPUEstimator.""""""\n\n  name_to_features = {\n      ""input_ids"": tf.FixedLenFeature([seq_length], tf.int64),\n      ""input_mask"": tf.FixedLenFeature([seq_length], tf.int64),\n      ""segment_ids"": tf.FixedLenFeature([seq_length], tf.int64),\n      ""label_ids"": tf.FixedLenFeature([], tf.int64),\n  }\n\n  def _decode_record(record, name_to_features):\n    """"""Decodes a record to a TensorFlow example.""""""\n    example = tf.parse_single_example(record, name_to_features)\n\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n      t = example[name]\n      if t.dtype == tf.int64:\n        t = tf.to_int32(t)\n      example[name] = t\n\n    return example\n\n  def input_fn(params):\n    """"""The actual input function.""""""\n    batch_size = params[""batch_size""]\n\n    # For training, we want a lot of parallel reading and shuffling.\n    # For eval, we want no shuffling and parallel reading doesn\'t matter.\n    d = tf.data.TFRecordDataset(input_file)\n    if is_training:\n      d = d.repeat()\n      d = d.shuffle(buffer_size=100)\n\n    d = d.apply(\n        tf.contrib.data.map_and_batch(\n            lambda record: _decode_record(record, name_to_features),\n            batch_size=batch_size,\n            drop_remainder=drop_remainder))\n\n    return d\n\n  return input_fn\n\n\ndef _truncate_seq_pair(tokens_a, tokens_b, max_length):\n  """"""Truncates a sequence pair in place to the maximum length.""""""\n\n  # This is a simple heuristic which will always truncate the longer sequence\n  # one token at a time. This makes more sense than truncating an equal percent\n  # of tokens from each, since if one sequence is very short then each token\n  # that\'s truncated likely contains more information than a longer sequence.\n  while True:\n    total_length = len(tokens_a) + len(tokens_b)\n    if total_length <= max_length:\n      break\n    if len(tokens_a) > len(tokens_b):\n      tokens_a.pop()\n    else:\n      tokens_b.pop()\n\n\ndef create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n                 labels, num_labels, use_one_hot_embeddings):\n  """"""Creates a classification model.""""""\n  model = modeling.BertModel(\n      config=bert_config,\n      is_training=is_training,\n      input_ids=input_ids,\n      input_mask=input_mask,\n      token_type_ids=segment_ids,\n      use_one_hot_embeddings=use_one_hot_embeddings)\n\n  # In the demo, we are doing a simple classification task on the entire\n  # segment.\n  #\n  # If you want to use the token-level output, use model.get_sequence_output()\n  # instead.\n  output_layer = model.get_pooled_output()\n\n  hidden_size = output_layer.shape[-1].value\n\n  output_weights = tf.get_variable(\n      ""output_weights"", [num_labels, hidden_size],\n      initializer=tf.truncated_normal_initializer(stddev=0.02))\n\n  output_bias = tf.get_variable(\n      ""output_bias"", [num_labels], initializer=tf.zeros_initializer())\n\n  with tf.variable_scope(""loss""):\n    if is_training:\n      # I.e., 0.1 dropout\n      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n\n    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    probabilities = tf.nn.softmax(logits, axis=-1)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n\n    return (loss, per_example_loss, logits, probabilities)\n\n\ndef model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\n                     num_train_steps, num_warmup_steps, use_tpu,\n                     use_one_hot_embeddings):\n  """"""Returns `model_fn` closure for TPUEstimator.""""""\n\n  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n    """"""The `model_fn` for TPUEstimator.""""""\n\n    tf.logging.info(""*** Features ***"")\n    for name in sorted(features.keys()):\n      tf.logging.info(""  name = %s, shape = %s"" % (name, features[name].shape))\n\n    input_ids = features[""input_ids""]\n    input_mask = features[""input_mask""]\n    segment_ids = features[""segment_ids""]\n    label_ids = features[""label_ids""]\n\n    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n\n    (total_loss, per_example_loss, logits, probabilities) = create_model(\n        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n        num_labels, use_one_hot_embeddings)\n    \n    tvars = tf.trainable_variables()\n    initialized_variable_names = {}\n    scaffold_fn = None\n    if init_checkpoint:\n      (assignment_map, initialized_variable_names\n      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n      if use_tpu:\n\n        def tpu_scaffold():\n          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n          return tf.train.Scaffold()\n\n        scaffold_fn = tpu_scaffold\n      else:\n        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n\n    tf.logging.info(""**** Trainable Variables ****"")\n    for var in tvars:\n      init_string = """"\n      if var.name in initialized_variable_names:\n        init_string = "", *INIT_FROM_CKPT*""\n      tf.logging.info(""  name = %s, shape = %s%s"", var.name, var.shape,\n                      init_string)\n\n    output_spec = None\n    if mode == tf.estimator.ModeKeys.TRAIN:\n\n      train_op = optimization.create_optimizer(\n          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          train_op=train_op,\n          scaffold_fn=scaffold_fn)\n    elif mode == tf.estimator.ModeKeys.EVAL:\n\n      def metric_fn(per_example_loss, label_ids, logits):\n        predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n        accuracy = tf.metrics.accuracy(label_ids, predictions)\n        loss = tf.metrics.mean(per_example_loss)\n        return {\n            ""eval_accuracy"": accuracy,\n            ""eval_loss"": loss,\n        }\n\n      eval_metrics = (metric_fn, [per_example_loss, label_ids, logits])\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          eval_metrics=eval_metrics,\n          scaffold_fn=scaffold_fn)\n    else:\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode, predictions=probabilities, scaffold_fn=scaffold_fn)\n    return output_spec\n\n  return model_fn\n\n\n# This function is not used by this file but is still used by the Colab and\n# people who depend on it.\ndef input_fn_builder(features, seq_length, is_training, drop_remainder):\n  """"""Creates an `input_fn` closure to be passed to TPUEstimator.""""""\n\n  all_input_ids = []\n  all_input_mask = []\n  all_segment_ids = []\n  all_label_ids = []\n\n  for feature in features:\n    all_input_ids.append(feature.input_ids)\n    all_input_mask.append(feature.input_mask)\n    all_segment_ids.append(feature.segment_ids)\n    all_label_ids.append(feature.label_id)\n\n  def input_fn(params):\n    """"""The actual input function.""""""\n    batch_size = params[""batch_size""]\n\n    num_examples = len(features)\n\n    # This is for demo purposes and does NOT scale to large data sets. We do\n    # not use Dataset.from_generator() because that uses tf.py_func which is\n    # not TPU compatible. The right way to load data is with TFRecordReader.\n    d = tf.data.Dataset.from_tensor_slices({\n        ""input_ids"":\n            tf.constant(\n                all_input_ids, shape=[num_examples, seq_length],\n                dtype=tf.int32),\n        ""input_mask"":\n            tf.constant(\n                all_input_mask,\n                shape=[num_examples, seq_length],\n                dtype=tf.int32),\n        ""segment_ids"":\n            tf.constant(\n                all_segment_ids,\n                shape=[num_examples, seq_length],\n                dtype=tf.int32),\n        ""label_ids"":\n            tf.constant(all_label_ids, shape=[num_examples], dtype=tf.int32),\n    })\n\n    if is_training:\n      d = d.repeat()\n      d = d.shuffle(buffer_size=100)\n\n    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n    return d\n\n  return input_fn\n\n\n# This function is not used by this file but is still used by the Colab and\n# people who depend on it.\ndef convert_examples_to_features(examples, label_list, max_seq_length,\n                                 tokenizer):\n  """"""Convert a set of `InputExample`s to a list of `InputFeatures`.""""""\n\n  features = []\n  for (ex_index, example) in enumerate(examples):\n    if ex_index % 10000 == 0:\n      tf.logging.info(""Writing example %d of %d"" % (ex_index, len(examples)))\n\n    feature = convert_single_example(ex_index, example, label_list,\n                                     max_seq_length, tokenizer)\n\n    features.append(feature)\n  return features\n\n\ndef main(_):\n  tf.logging.set_verbosity(tf.logging.INFO)\n\n  processors = {\n      ""cola"": ColaProcessor,\n      ""mnli"": MnliProcessor,\n      ""mrpc"": MrpcProcessor,\n      ""xnli"": XnliProcessor,\n  }\n\n  if not FLAGS.do_train and not FLAGS.do_eval and not FLAGS.do_predict:\n    raise ValueError(\n        ""At least one of `do_train`, `do_eval` or `do_predict\' must be True."")\n\n  bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\n\n  if FLAGS.max_seq_length > bert_config.max_position_embeddings:\n    raise ValueError(\n        ""Cannot use sequence length %d because the BERT model ""\n        ""was only trained up to sequence length %d"" %\n        (FLAGS.max_seq_length, bert_config.max_position_embeddings))\n\n  tf.gfile.MakeDirs(FLAGS.output_dir)\n\n  task_name = FLAGS.task_name.lower()\n\n  if task_name not in processors:\n    raise ValueError(""Task not found: %s"" % (task_name))\n\n  processor = processors[task_name]()\n\n  label_list = processor.get_labels()\n\n  tokenizer = tokenization.FullTokenizer(\n      vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n\n  tpu_cluster_resolver = None\n  if FLAGS.use_tpu and FLAGS.tpu_name:\n    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n        FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n\n  is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n  run_config = tf.contrib.tpu.RunConfig(\n      cluster=tpu_cluster_resolver,\n      master=FLAGS.master,\n      model_dir=FLAGS.output_dir,\n      save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n      tpu_config=tf.contrib.tpu.TPUConfig(\n          iterations_per_loop=FLAGS.iterations_per_loop,\n          num_shards=FLAGS.num_tpu_cores,\n          per_host_input_for_training=is_per_host))\n\n  train_examples = None\n  num_train_steps = None\n  num_warmup_steps = None\n  if FLAGS.do_train:\n    train_examples = processor.get_train_examples(FLAGS.data_dir)\n    num_train_steps = int(\n        len(train_examples) / FLAGS.train_batch_size * FLAGS.num_train_epochs)\n    num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)\n\n  model_fn = model_fn_builder(\n      bert_config=bert_config,\n      num_labels=len(label_list),\n      init_checkpoint=FLAGS.init_checkpoint,\n      learning_rate=FLAGS.learning_rate,\n      num_train_steps=num_train_steps,\n      num_warmup_steps=num_warmup_steps,\n      use_tpu=FLAGS.use_tpu,\n      use_one_hot_embeddings=FLAGS.use_tpu)\n\n  run.log(\'lr\', np.float(FLAGS.learning_rate))\n\n  # If TPU is not available, this will fall back to normal Estimator on CPU\n  # or GPU.\n  estimator = tf.contrib.tpu.TPUEstimator(\n      use_tpu=FLAGS.use_tpu,\n      model_fn=model_fn,\n      config=run_config,\n      train_batch_size=FLAGS.train_batch_size,\n      eval_batch_size=FLAGS.eval_batch_size,\n      predict_batch_size=FLAGS.predict_batch_size)\n  \n  if FLAGS.do_train:\n    train_file = os.path.join(FLAGS.output_dir, ""train.tf_record"")\n    file_based_convert_examples_to_features(\n        train_examples, label_list, FLAGS.max_seq_length, tokenizer, train_file)\n    tf.logging.info(""***** Running training *****"")\n    tf.logging.info(""  Num examples = %d"", len(train_examples))\n    tf.logging.info(""  Batch size = %d"", FLAGS.train_batch_size)\n    tf.logging.info(""  Num steps = %d"", num_train_steps)\n    train_input_fn = file_based_input_fn_builder(\n        input_file=train_file,\n        seq_length=FLAGS.max_seq_length,\n        is_training=True,\n        drop_remainder=True)\n    for n in tf.get_default_graph().as_graph_def().node:\n      tf.logging.info(""    Node Name = %s"", n.name)\n\n    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps, hooks=[_MetricLogger(""train"")])\n\n\n  if FLAGS.do_eval:\n    eval_examples = processor.get_dev_examples(FLAGS.data_dir)\n    eval_file = os.path.join(FLAGS.output_dir, ""eval.tf_record"")\n    file_based_convert_examples_to_features(\n        eval_examples, label_list, FLAGS.max_seq_length, tokenizer, eval_file)\n\n    tf.logging.info(""***** Running evaluation *****"")\n    tf.logging.info(""  Num examples = %d"", len(eval_examples))\n    tf.logging.info(""  Batch size = %d"", FLAGS.eval_batch_size)\n\n    # This tells the estimator to run through the entire set.\n    eval_steps = None\n    # However, if running eval on the TPU, you will need to specify the\n    # number of steps.\n    if FLAGS.use_tpu:\n      # Eval will be slightly WRONG on the TPU because it will truncate\n      # the last batch.\n      eval_steps = int(len(eval_examples) / FLAGS.eval_batch_size)\n\n    eval_drop_remainder = True if FLAGS.use_tpu else False\n    eval_input_fn = file_based_input_fn_builder(\n        input_file=eval_file,\n        seq_length=FLAGS.max_seq_length,\n        is_training=False,\n        drop_remainder=eval_drop_remainder)\n\n    result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps, hooks=[_MetricLogger(""eval"")])\n\n    output_eval_file = os.path.join(FLAGS.output_dir, ""eval_results.txt"")\n    with tf.gfile.GFile(output_eval_file, ""w"") as writer:\n      tf.logging.info(""***** Eval results *****"")\n      for key in sorted(result.keys()):\n        tf.logging.info(""  %s = %s"", key, str(result[key]))\n        writer.write(""%s = %s\\n"" % (key, str(result[key])))\n        run.log(key, result[key])\n\n  if FLAGS.do_predict:\n    predict_examples = processor.get_test_examples(FLAGS.data_dir)\n    predict_file = os.path.join(FLAGS.output_dir, ""predict.tf_record"")\n    file_based_convert_examples_to_features(predict_examples, label_list,\n                                            FLAGS.max_seq_length, tokenizer,\n                                            predict_file)\n\n    tf.logging.info(""***** Running prediction*****"")\n    tf.logging.info(""  Num examples = %d"", len(predict_examples))\n    tf.logging.info(""  Batch size = %d"", FLAGS.predict_batch_size)\n\n    if FLAGS.use_tpu:\n      # Warning: According to tpu_estimator.py Prediction on TPU is an\n      # experimental feature and hence not supported here\n      raise ValueError(""Prediction in TPU not supported"")\n\n    predict_drop_remainder = True if FLAGS.use_tpu else False\n    predict_input_fn = file_based_input_fn_builder(\n        input_file=predict_file,\n        seq_length=FLAGS.max_seq_length,\n        is_training=False,\n        drop_remainder=predict_drop_remainder)\n\n    result = estimator.predict(input_fn=predict_input_fn)\n\n    output_predict_file = os.path.join(FLAGS.output_dir, ""test_results.tsv"")\n    with tf.gfile.GFile(output_predict_file, ""w"") as writer:\n      tf.logging.info(""***** Predict results *****"")\n      for prediction in result:\n        output_line = ""\\t"".join(\n            str(class_probability) for class_probability in prediction) + ""\\n""\n        writer.write(output_line)\n\n\nif __name__ == ""__main__"":\n  flags.mark_flag_as_required(""data_dir"")\n  flags.mark_flag_as_required(""task_name"")\n  flags.mark_flag_as_required(""vocab_file"")\n  flags.mark_flag_as_required(""bert_config_file"")\n  flags.mark_flag_as_required(""output_dir"")\n  tf.app.run()\n'"
pretrain/PyTorch/azureml_adapter.py,0,"b""import os\n\n\ndef set_environment_variables_for_nccl_backend(single_node=False, master_port=6105):\n    os.environ['RANK'] = os.environ['OMPI_COMM_WORLD_RANK']\n    os.environ['WORLD_SIZE'] = os.environ['OMPI_COMM_WORLD_SIZE']\n\n    if not single_node: \n        master_node_params = os.environ['AZ_BATCH_MASTER_NODE'].split(':')\n        os.environ['MASTER_ADDR'] = master_node_params[0]\n\n        # Do not overwrite master port with that defined in AZ_BATCH_MASTER_NODE\n        if 'MASTER_PORT' not in os.environ:\n            os.environ['MASTER_PORT'] = str(master_port)\n    else:\n        os.environ['MASTER_ADDR'] = os.environ['AZ_BATCHAI_MPI_MASTER_NODE']\n        os.environ['MASTER_PORT'] = '54965'\n    print('NCCL_SOCKET_IFNAME original value = {}'.format(os.environ['NCCL_SOCKET_IFNAME']))\n    # TODO make this parameterizable\n    os.environ['NCCL_SOCKET_IFNAME'] = '^docker0,lo'\n\n    print('RANK = {}'.format(os.environ['RANK']))\n    print('WORLD_SIZE = {}'.format(os.environ['WORLD_SIZE']))\n    print('MASTER_ADDR = {}'.format(os.environ['MASTER_ADDR']))\n    print('MASTER_PORT = {}'.format(os.environ['MASTER_PORT']))\n    # print('MASTER_NODE = {}'.format(os.environ['MASTER_NODE']))\n    print('NCCL_SOCKET_IFNAME new value = {}'.format(os.environ['NCCL_SOCKET_IFNAME']))\n\ndef get_local_rank():\n    return int(os.environ['OMPI_COMM_WORLD_LOCAL_RANK'])\n\ndef get_global_size():\n    return int(os.environ['OMPI_COMM_WORLD_SIZE'])\n\ndef get_local_size():\n    return int(os.environ['OMPI_COMM_WORLD_LOCAL_SIZE'])\t\n\ndef get_world_size():\n    return int(os.environ['WORLD_SIZE'])\n\t \n"""
pretrain/PyTorch/benchmark.py,0,"b'import re\nfrom datetime import datetime\n\n\ndef get_timestamp(text):\n\tdatepattern = re.compile(""\\d{2}/\\d{2}/\\d{4} \\d{2}:\\d{2}:\\d{2}"")\n\tmatcher = datepattern.search(text)\n\treturn datetime.strptime(matcher.group(0), \'%m/%d/%Y %H:%M:%S\')\n\ndef get_perf_metrics(filename):\n\twith open(filename) as f:\n\t\tdatafile = f.readlines()\n\t\tthroughput = 0\n\t\tepoch = 1\n\t\ttime_diff=0\n\t\tnum_seq=0\n\t\tfor line in datafile:\n\t\t\tif \'Training epoch:\' in line:\n\t\t\t\tstart_time = get_timestamp(line)\n\n\t\t\t\tif epoch == 1:\n\t\t\t\t\ttraining_start_time = start_time\n\t\t\t\tepoch += 1\n\t\t\tif \'Completed processing\' in line:\n\t\t\t\tend_time = get_timestamp(line)\n\t\t\t\ttime_diff += int((end_time-start_time).total_seconds())\n\t\t\t\tnum_seq += [int(s) for s in line[int(line.find(\'Completed processing\')):].split() if s.isdigit()][0]\n\t\t\t\tthroughput = num_seq/time_diff\n\t\t\t\t#print(throughput)\n\t\t\tif \'Validation Loss\' in line:\n\t\t\t\tvalid_loss = float(line[int(line.find(\'is:\'))+3:])\n\t\tavg_throughput = (num_seq/time_diff)\n\t\ttotal_training_time = end_time-training_start_time\n\t\td = datetime(1,1,1) + total_training_time\n\n\t\tprint(\'Num epochs:\', epoch)\n\t\tprint(\'Total time to train:\', d.day-1,\'days,\', d.hour ,\'hours\')\n\t\tprint(\'Average throughput:\',avg_throughput, \'sequences/second\')\n\t\tprint(\'Final Validation Loss:\', valid_loss)\n'"
pretrain/PyTorch/checkpoint.py,3,"b'from logger import Logger\nimport torch\nimport os\nfrom operator import itemgetter\n\nfrom torch import __init__\n\ndef checkpoint_model(PATH, model, optimizer, epoch, last_global_step, **kwargs):\n    """"""Utility function for checkpointing model + optimizer dictionaries\n       The main purpose for this is to be able to resume training from that instant again\n    """"""\n    checkpoint_state_dict = {\'epoch\': epoch,\n                             \'last_global_step\': last_global_step,\n                             \'model_state_dict\': model.network.module.state_dict(),\n                             \'optimizer_state_dict\': optimizer.state_dict()}\n    # Add extra kwargs too\n    checkpoint_state_dict.update(kwargs)\n    torch.save(checkpoint_state_dict, PATH)\n    return\n\n\ndef load_checkpoint(model, optimizer, PATH):\n    """"""Utility function for checkpointing model + optimizer dictionaries\n       The main purpose for this is to be able to resume training from that instant again\n    """"""\n    checkpoint_state_dict = torch.load(PATH, map_location=torch.device(""cpu""))\n    #from train import model\n    model.network.module.load_state_dict(\n        checkpoint_state_dict[\'model_state_dict\'])\n    #from train import optimizer\n    optimizer.load_state_dict(checkpoint_state_dict[\'optimizer_state_dict\'])\n    epoch = checkpoint_state_dict[\'epoch\']\n    last_global_step = checkpoint_state_dict[\'last_global_step\']\n    del checkpoint_state_dict\n    return (epoch + 1, last_global_step)\n\n\ndef latest_checkpoint_file(reference_folder: str, no_cuda) -> str:\n    """"""Extracts the name of the last checkpoint file\n\n    :param reference_folder: (str) Path to the parent_folder\n    :return: (str) Path to the most recent checkpoint tar file\n    """"""\n\n    logger = Logger(cuda=torch.cuda.is_available() and not no_cuda)\n    \n    # For each folder inside the parent folder find all files\n    # ending with .tar and extreact the last checkpoint.\n    candidate_files = []\n    for dir_path, dir_names, filenames in os.walk(reference_folder):\n        logger.info(f""Searching for checkpoint in {reference_folder}"")\n        relevant_files = [f for f in filenames if f.endswith(\'.tar\')]\n        if relevant_files:\n            latest_file = max(relevant_files)  # assumes that checkpoint number is of format 000x\n            candidate_files.append((dir_path, latest_file))\n    \n    checkpoint_file = max(candidate_files, key=itemgetter(1))\n    checkpoint_path = os.path.join(checkpoint_file[0], checkpoint_file[1])\n\n    return checkpoint_path\n'"
pretrain/PyTorch/configuration.py,0,"b'import json\n\n\n# TODO better json handling\nclass BertJobConfiguration:\n    def __init__(self, config_file_path):\n        self.config = json.load(open(config_file_path, \'r\', encoding=\'utf-8\'))\n\n    # TODO improve this implementation\n    def replace_path_placeholders(self, files_location):\n        self.config[\'data\'][\'datasets\'] = {key: value.replace(\'placeholder/\', files_location)\n                                      for (key, value) in self.config[\'data\'][\'datasets\'].items()}\n        self.config[\'validation\'][\'path\'] = self.config[\'validation\'][\'path\'].replace(\'placeholder/\', files_location)\n\n    def get_name(self):\n        return self.config[\'name\']\n\n    def get_token_file_type(self):\n        return self.config[""bert_token_file""]\n\n    def get_model_file_type(self):\n        return self.config[""bert_model_file""]\n\n    def get_learning_rate(self):\n        return self.config[""training""][""learning_rate""]\n\n    def get_warmup_proportion(self):\n        return self.config[""training""][""warmup_proportion""]\n\n    def get_total_training_steps(self):\n        return self.config[""training""][""total_training_steps""]\n\n    def get_total_epoch_count(self):\n        return self.config[""training""][""num_epochs""]\n\n    def get_num_workers(self):\n        return self.config[\'training\'][\'num_workers\']\n\n    def get_validation_folder_path(self):\n        return self.config[\'validation\'][\'path\']\n\n    def get_wiki_pretrain_dataset_path(self):\n        return self.config[""data""][""datasets""][\'wiki_pretrain_dataset\']\n\n    def get_decay_rate(self):\n        return self.config[""training""][""decay_rate""]\n\n    def get_decay_step(self):\n        return self.config[""training""][""decay_step""]\n\n    def get_model_config(self):\n        return self.config[""bert_model_config""]\n'"
pretrain/PyTorch/dataset.py,3,"b'import torch\nimport os\nfrom torch.utils.data import DataLoader, Dataset\nfrom enum import IntEnum\nfrom random import choice\nimport random\nimport collections\n\nfrom text import mask, torch_long, PAD\nfrom sources import PretrainingDataCreator, TokenInstance, GenericPretrainingDataCreator\nfrom sources import WikiPretrainingDataCreator\nfrom pytorch_pretrained_bert.tokenization import BertTokenizer\n\n\nclass BatchType(IntEnum):\n    PRETRAIN_BATCH = 0\n\n\nclass PretrainDataType(IntEnum):\n    WIKIPEDIA = 1\n    VALIDATION = 2\n\nMaskedLMInstance = collections.namedtuple(\n    ""MaskedLMInstance"", [""index"", ""label""])\n\nPretrainBatch = collections.namedtuple(\n    \'PreTrainBatch\', [\'input_ids\', \'input_mask\', \'sequence_ids\',\n                      \'is_next_label\', \'masked_lm_output\']\n)\n\ndef get_random_partition(data_directory, index):\n    partitions = [os.path.join(data_directory, x)\n                  for x in os.listdir(data_directory)]\n    partitions = sorted(partitions)\n    i = index % len(partitions)\n    return partitions[i]\n\n\ndef map_to_torch(encoding):\n    encoding = torch_long(encoding)\n    encoding.requires_grad_(False)\n    return encoding\n\n\ndef map_to_torch_float(encoding):\n    encoding = torch.FloatTensor(encoding)\n    encoding.requires_grad_(False)\n    return encoding\n\n\ndef map_to_torch_half(encoding):\n    encoding = torch.HalfTensor(encoding)\n    encoding.requires_grad_(False)\n    return encoding\n\n\ndef encode_sequence(seqA, seqB, max_seq_len, tokenizer):\n    seqA = [""[CLS]""] + seqA + [""[SEP]""]\n    seqB = seqB + [""[SEP]""]\n\n    input_tokens = seqA + seqB\n    input_ids = tokenizer.convert_tokens_to_ids(input_tokens)\n    sequence_ids = [0]*len(seqA) + [1]*len(seqB)\n    input_mask = [1]*len(input_ids)\n\n    while len(input_ids) < max_seq_len:\n        input_ids.append(PAD)\n        sequence_ids.append(PAD)\n        input_mask.append(PAD)\n\n    return (map_to_torch(input_ids), map_to_torch(input_mask), map_to_torch(sequence_ids))\n\n\ndef truncate_input_sequence(tokens_a, tokens_b, max_num_tokens):\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if random.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\nclass PreTrainingDataset(Dataset):\n    def __init__(self, tokenizer: BertTokenizer, folder: str, logger, max_seq_length, index, data_type: PretrainDataType = PretrainDataType.WIKIPEDIA, max_predictions_per_seq=20, masked_lm_prob=0.15):\n        self.tokenizer = tokenizer\n        self.dir_path = folder\n        self.max_seq_length = max_seq_length\n        self.len = 0\n        self.masked_lm_prob = masked_lm_prob\n        self.max_predictions_per_seq = max_predictions_per_seq\n        self.vocab_words = list(tokenizer.vocab.keys())\n\n        path = get_random_partition(self.dir_path, index)\n\n        logger.info(f""Loading Pretraining Data from {path}"")\n        if data_type == PretrainDataType.WIKIPEDIA:\n            self.data = GenericPretrainingDataCreator.load(path)\n        elif data_type == PretrainDataType.VALIDATION:\n            self.data = WikiPretrainingDataCreator.load(path)\n        self.len = len(self.data)\n        logger.info(\n            f""Data Loading Completed for Pretraining Data from {path} with {self.len} samples."")\n\n    def __len__(self):\n        return self.len\n\n    def __getitem__(self, index):\n        i = index % self.len\n\n        instance: TokenInstance = self.data.instances[i]\n        return self.create_training_instance(instance)\n\n    def create_training_instance(self, instance: TokenInstance):\n        tokens_a, tokens_b, is_next = instance.get_values()\n        # print(f\'is_next label:{is_next}\')\n        # Create mapper\n        tokens = []\n        segment_ids = []\n        tokens.append(""[CLS]"")\n        segment_ids.append(0)\n        for token in tokens_a:\n            tokens.append(token)\n            segment_ids.append(0)\n\n        tokens.append(""[SEP]"")\n        segment_ids.append(0)\n\n        for token in tokens_b:\n            tokens.append(token)\n            segment_ids.append(1)\n\n        tokens.append(""[SEP]"")\n        segment_ids.append(1)\n\n        # Get Masked LM predictions\n        tokens, masked_lm_output = self.create_masked_lm_predictions(tokens)\n\n        # Convert to Ids\n        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n        input_mask = [1] * len(input_ids)\n\n        while len(input_ids) < self.max_seq_length:\n            input_ids.append(PAD)\n            segment_ids.append(PAD)\n            input_mask.append(PAD)\n            masked_lm_output.append(-1)\n        return([map_to_torch([BatchType.PRETRAIN_BATCH]), map_to_torch(input_ids), map_to_torch(input_mask), map_to_torch(segment_ids), map_to_torch([is_next]), map_to_torch(masked_lm_output)])\n\n    def create_masked_lm_predictions(self, tokens):\n        cand_indexes = []\n        for i, token in enumerate(tokens):\n            if token == ""[CLS]"" or token == ""[SEP]"":\n                continue\n            cand_indexes.append(i)\n\n        random.shuffle(cand_indexes)\n        output_tokens = list(tokens)\n\n        num_to_predict = min(self.max_predictions_per_seq, max(\n            1, int(round(len(tokens) * self.masked_lm_prob))))\n\n        masked_lms = []\n        covered_indexes = set()\n        for index in cand_indexes:\n            if len(masked_lms) >= num_to_predict:\n                break\n            if index in covered_indexes:\n                continue\n            covered_indexes.add(index)\n\n            masked_token = None\n            # 80% mask\n            if random.random() < 0.8:\n                masked_token = ""[MASK]""\n            else:\n                # 10% Keep Original\n                if random.random() < 0.5:\n                    masked_token = tokens[index]\n                # 10% replace w/ random word\n                else:\n                    masked_token = self.vocab_words[random.randint(\n                        0, len(self.vocab_words) - 1)]\n\n            output_tokens[index] = masked_token\n            masked_lms.append(MaskedLMInstance(\n                index=index, label=tokens[index]))\n\n        masked_lms = sorted(masked_lms, key=lambda x: x.index)\n        masked_lm_output = [-1] * len(output_tokens)\n        for p in masked_lms:\n            masked_lm_output[p.index] = self.tokenizer.vocab[p.label]\n\n        return (output_tokens, masked_lm_output)\n'"
pretrain/PyTorch/distributed_apex.py,26,"b'# TODO: This is a copy of apex code from NVIDIA/APEX. Details to be added on the updates made here.\n\nimport torch\n\ntry:\n    from apex_C import flatten\n    from apex_C import unflatten\nexcept ImportError:\n    try:\n        _ = warned_flatten\n    except NameError:\n        print(""Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten."")\n        warned_flatten = True\n    from torch._utils import _flatten_dense_tensors as flatten\n    from torch._utils import _unflatten_dense_tensors as unflatten\nimport torch.distributed as dist\nfrom torch.nn.modules import Module\nfrom torch.autograd import Variable\nfrom collections import OrderedDict\nfrom itertools import chain\nimport copy\n\n# apply_dist_call requires that tensors in \'bucket\' are all the same type.\n\n\ndef apply_flat_dist_call(bucket, call, extra_args=None):\n\n    coalesced = flatten(bucket)\n    #print(""Rank"", dist.get_rank(), ""Broadcasting "", coalesced.device, "" Size"", coalesced.size())\n    if extra_args is not None:\n        call(coalesced, *extra_args)\n    else:\n        call(coalesced)\n\n    if call is dist.all_reduce:\n        coalesced /= dist.get_world_size()\n\n    for buf, synced in zip(bucket, unflatten(coalesced, bucket)):\n        buf.copy_(synced)\n\n\ndef split_half_float_double(tensors):\n    dtypes = [""torch.cuda.HalfTensor"",\n              ""torch.cuda.FloatTensor"", ""torch.cuda.DoubleTensor""]\n    buckets = []\n    for i, dtype in enumerate(dtypes):\n        bucket = [t for t in tensors if t.type() == dtype]\n        if bucket:\n            buckets.append(bucket)\n    return buckets\n\n\ndef split_by_type(tensors):\n    buckets = OrderedDict()\n    for tensor in tensors:\n        tp = tensor.type()\n        if tp not in buckets:\n            buckets[tp] = []\n        buckets[tp].append(tensor)\n    return buckets\n\n# flat_dist_call organizes \'tensors\' by type.\n\n\ndef flat_dist_call(tensors, call, extra_args=None):\n    buckets = split_by_type(tensors)\n\n    for tp in buckets:\n        bucket = buckets[tp]\n        apply_flat_dist_call(bucket, call, extra_args)\n\n\ndef extract_tensors(maybe_tensor, tensor_list):\n    if torch.is_tensor(maybe_tensor):\n        tensor_list.append(maybe_tensor)\n    else:\n        try:\n            for item in maybe_tensor:\n                extract_tensors(item, tensor_list)\n        except TypeError:\n            return\n\n\nclass Reducer(object):\n    """"""\n    :class:`apex.parallel.Reducer` is a simple class that helps allreduce a module\'s parameters\n    across processes.  :class:`Reducer` is intended to give the user additional control:\n    Unlike :class:`DistributedDataParallel`, :class:`Reducer` will not automatically allreduce\n    parameters during ``backward()``.\n    Instead, :class:`Reducer` waits for the user to call `<reducer_instance>.reduce()` manually.\n    This enables, for example, delaying the allreduce to be carried out every \n    several iterations instead of every single iteration.\n\n    Like :class:`DistributedDataParallel`, :class:`Reducer` averages any tensors it allreduces \n    over the number of participating processes.\n\n    :class:`Reducer` is designed to work with the upstream launch utility script \n    ``torch.distributed.launch`` with ``--nproc_per_node <= number of gpus per node``.\n    When used with this launcher, :class:`Reducer` assumes 1:1 mapping of processes to GPUs.\n    It also assumes that your script calls ``torch.cuda.set_device(args.rank)`` before creating the model.\n\n    main_reducer.py in https://github.com/NVIDIA/apex/tree/master/examples/imagenet shows example usage.\n\n    Args:\n        module_or_grads_list: Either a network definition (module) being run in multi-gpu/distributed mode, or an iterable of gradients to be reduced.  If a module is passed in, the Reducer constructor will sync the parameters across processes (broadcasting from rank 0) to make sure they\'re all initialized with the same values.  If a list of gradients (that came from some module) is passed in, the user is responsible for manually syncing that module\'s parameters at the beginning of training.\n    """"""\n\n    def __init__(self, module_or_grads_list):\n        if isinstance(module_or_grads_list, Module):\n            self.module = module_or_grads_list\n            flat_dist_call(\n                [param.data for param in self.module.parameters()], dist.broadcast, (0,))\n\n        else:\n            self.module = None\n            self.grads = []\n            extract_tensors(module_or_grads_list, self.grads)\n\n    def reduce(self):\n        if self.module:\n            grads = [param.grad.data for param in self.module.parameters()\n                     if param.grad is not None]\n            flat_dist_call(grads, dist.all_reduce)\n        else:\n            flat_dist_call(self.grads, dist.all_reduce)\n\n\nclass DistributedDataParallel(Module):\n    """"""\n    :class:`apex.parallel.DistributedDataParallel` is a module wrapper that enables\n    easy multiprocess distributed data parallel training, similar to ``torch.nn.parallel.DistributedDataParallel``.  Parameters are broadcast across participating processes on initialization, and gradients are\n    allreduced and averaged over processes during ``backward()``.\n\n    :class:`DistributedDataParallel` is optimized for use with NCCL.  It achieves high performance by \n    overlapping communication with computation during ``backward()`` and bucketing smaller gradient\n    transfers to reduce the total number of transfers required.\n\n    :class:`DistributedDataParallel` is designed to work with the upstream launch utility script \n    ``torch.distributed.launch`` with ``--nproc_per_node <= number of gpus per node``.\n    When used with this launcher, :class:`DistributedDataParallel` assumes 1:1 mapping of processes to GPUs.\n    It also assumes that your script calls ``torch.cuda.set_device(args.rank)`` before creating the model.\n\n    https://github.com/NVIDIA/apex/tree/master/examples/distributed shows detailed usage.\n    https://github.com/NVIDIA/apex/tree/master/examples/imagenet shows another example\n    that combines :class:`DistributedDataParallel` with mixed precision training.\n\n    Args:\n        module: Network definition to be run in multi-gpu/distributed mode.\n        message_size (int, default=1e7): Minimum number of elements in a communication bucket.\n        delay_allreduce (bool, default=False):  Delay all communication to the end of the backward pass.  This disables overlapping communication with computation.\n        allreduce_trigger_params (list, optional, default=None):  If supplied, should contain a list of parameters drawn from the model.  Allreduces will be kicked off whenever one of these parameters receives its gradient (as opposed to when a bucket of size message_size is full).  At the end of backward(), a cleanup allreduce to catch any remaining gradients will also be performed automatically.  If allreduce_trigger_params is supplied, the message_size argument will be ignored.\n        allreduce_always_fp32 (bool, default=False):  Convert any FP16 gradients to FP32 before allreducing.  This can improve stability for widely scaled-out runs.\n        gradient_average (bool, default=True):  Option to toggle whether or not DDP averages the allreduced gradients over processes.  For proper scaling, the default value of True is recommended.\n        gradient_predivide_factor (float, default=1.0):  Allows perfoming the average of gradients over processes partially before and partially after the allreduce.  Before allreduce:  ``grads.mul_(1.0/gradient_predivide_factor)``.  After allreduce:  ``grads.mul_(gradient_predivide_factor/world size)``.  This can reduce the stress on the dynamic range of FP16 allreduces for widely scaled-out runs.\n\n    .. warning::\n        If ``gradient_average=False``, the pre-allreduce division (``grads.mul_(1.0/gradient_predivide_factor)``) will still be applied, but the post-allreduce gradient averaging (``grads.mul_(gradient_predivide_factor/world size)``) will be omitted.\n\n    """"""\n\n    def __init__(self,\n                 module,\n                 message_size=10000000,\n                 delay_allreduce=False,\n                 shared_param=None,\n                 allreduce_trigger_params=None,\n                 retain_allreduce_buffers=False,\n                 allreduce_always_fp32=False,\n                 gradient_average=True,\n                 gradient_predivide_factor=1.0,\n                 gradient_average_split_factor=None):\n        super(DistributedDataParallel, self).__init__()\n\n        # Backward/forward compatibility around\n        # https://github.com/pytorch/pytorch/commit/540ef9b1fc5506369a48491af8a285a686689b36 and\n        # https://github.com/pytorch/pytorch/commit/044d00516ccd6572c0d6ab6d54587155b02a3b86\n        if hasattr(dist, ""get_backend""):\n            self._backend = dist.get_backend()\n            if hasattr(dist, ""DistBackend""):\n                self.backend_enum_holder = dist.DistBackend\n            else:\n                self.backend_enum_holder = dist.Backend\n        else:\n            self._backend = dist._backend\n            self.backend_enum_holder = dist.dist_backend\n\n        self.warn_on_half = True if self._backend == self.backend_enum_holder.GLOO else False\n\n        if shared_param is not None:\n            raise ValueError(""shared_param is no longer supported as an option.  It was misleadingly named from the start.  It turns out overlapping communication with computation should work fine with shared parameters.  If you still wish to delay communication to the end of the backward pass, use delay_allreduce=True|False instead."")\n\n        if gradient_average_split_factor is not None:\n            print(""Warning:  gradient_average_split_factor has been renamed to gradient_predivide_factor.  For now, gradient_average_split_factor will also work, but please update to gradient_predivide_factor instead."")\n            self.gradient_predivide_factor = gradient_average_split_factor\n\n        self.world_size = float(dist.get_world_size())\n\n        self.retain_allreduce_buffers = retain_allreduce_buffers\n        self.allreduce_always_fp32 = allreduce_always_fp32\n        self.gradient_average = gradient_average\n        self.gradient_predivide_factor = gradient_predivide_factor\n\n        self.custom_allreduce_triggers = False\n        if allreduce_trigger_params is not None:\n            if delay_allreduce:\n                raise ValueError(\n                    ""Setting allreduce_trigger_params is only valid if delay_allreduce=False."")\n            self.custom_allreduce_triggers = True\n            self.allreduce_trigger_params = set(\n                [id(param) for param in allreduce_trigger_params])\n\n        self.delay_allreduce = delay_allreduce\n        self.message_size = message_size\n\n        self.reduction_stream = torch.cuda.Stream()\n        self.reduction_event = torch.cuda.Event(\n            enable_timing=False, blocking=False)\n\n        self.module = module\n\n        if self._backend == self.backend_enum_holder.NCCL:\n            for param in self.module.parameters():\n                assert param.is_cuda, ""NCCL backend only supports model parameters to be on GPU.""\n\n        self.active_params = []\n\n        self.param_type_to_tmp_i = {""torch.cuda.HalfTensor"": 0,\n                                    ""torch.cuda.FloatTensor"": 1,\n                                    ""torch.cuda.DoubleTensor"": 2}\n\n        # to make sure reduction only happens after gradient accumulation\n        self.need_reduction = False\n\n        self.create_hooks()\n\n        flat_dist_call(\n            [param.data for param in self.module.parameters()], dist.broadcast, (0,))\n\n    def enable_need_reduction(self):\n        self.need_reduction = True\n\n    def disable_need_reduction(self):\n        self.need_reduction = False\n\n    def __setstate__(self, state):\n        super(DistributedDataParallel, self).__setstate__(state)\n        self.reduction_stream = torch.cuda.Stream()\n        self.reduction_event = torch.cuda.Event(\n            enable_timing=False, blocking=False)\n\n    def __getstate__(self):\n        attrs = copy.copy(self.__dict__)\n        if self._backend != self.backend_enum_holder.NCCL:\n            del attrs[\'self.reduction_stream\']\n            del attrs[\'self.reduction_event\']\n            return attrs\n\n    # Broadcast rank 0\'s bucket structure across all processes, and have all processes\n    # regenerate their bucket structures to match.\n    def sync_bucket_structure(self):\n        # Append leftover buckets\n        for tmp_bucket in self.tmp_buckets:\n            if len(tmp_bucket) > 0:\n                self.active_i_buckets.append(tmp_bucket)\n\n        self.num_buckets = len(self.active_i_buckets)\n        self.bucket_sizes = [len(bucket) for bucket in self.active_i_buckets]\n\n        info_tensor = torch.cuda.IntTensor([self.num_buckets] +\n                                           self.bucket_sizes +\n                                           list(chain(*self.active_i_buckets)))\n        #print(""Sync Bucket Structure Broadcast. Rank"", dist.get_rank(), ""Tensor Size "", info_tensor.size(), ""Device "", info_tensor.device, ""Current Device "", torch.cuda.current_device())\n        dist.broadcast(info_tensor, 0)\n\n        info = [int(entry) for entry in info_tensor]\n\n        self.num_buckets = info[0]\n        self.bucket_sizes = info[1:self.num_buckets + 1]\n        self.buckets = [[None for _ in range(self.bucket_sizes[i])]\n                        for i in range(self.num_buckets)]\n        # Technically, active_i_buckets\' work is done.  But the information is still useful to\n        # keep around.  Therefore, refresh active_i_buckets based on rank 0 as well.\n        self.active_i_buckets = [[None for _ in range(self.bucket_sizes[i])]\n                                 for i in range(self.num_buckets)]\n\n        flattened_buckets = info[self.num_buckets + 1:]\n        flat_i = 0\n        for bucket_idx in range(self.num_buckets):\n            for bucket_loc in range(self.bucket_sizes[bucket_idx]):\n                param_i = flattened_buckets[flat_i]\n                self.active_i_buckets[bucket_idx][bucket_loc] = param_i\n                self.param_id_to_bucket[id(self.active_params[param_i])] = (\n                    bucket_idx, bucket_loc)\n                flat_i += 1\n\n    def create_hooks(self):\n        # Fallback hook that\'s only called at the end of backward.\n        # Used if you deliberately want to delay allreduces to the end, or to refresh the\n        # bucket structure that will be used to overlap communication with computation in later\n        # iterations.\n        def allreduce_params():\n            # Bucket record refresh\n            if not self.delay_allreduce:\n                if self.needs_refresh:\n                    self.sync_bucket_structure()\n\n                    self.needs_refresh = False\n\n            self.allreduce_fallback()\n\n        def overlapping_backward_epilogue():\n            self.reduction_stream.record_event(self.reduction_event)\n            torch.cuda.current_stream().wait_event(self.reduction_event)\n\n            # Sanity checks that all the buckets were kicked off\n            if self.next_bucket != self.num_buckets:\n                raise RuntimeError(""In epilogue, next_bucket ({}) != num_buckets ({}).  "".format(\n                                   self.next_bucket, self.num_buckets),\n                                   ""This probably indicates some buckets were not allreduced."")\n\n            for actual, expected in zip(self.buckets_ready_size, self.bucket_sizes):\n                if actual != expected:\n                    raise RuntimeError(\n                        ""Some param buckets were not allreduced."")\n\n        self.grad_accs = []\n        for param in self.module.parameters():\n            if param.requires_grad:\n                def wrapper(param):\n                    param_tmp = param.expand_as(param)\n                    grad_acc = param_tmp.grad_fn.next_functions[0][0]\n\n                    def allreduce_hook(*unused):\n                        # user must explicitly specify when to do all reduce\n                        if self.need_reduction == False:\n                            #print(""Does not need Reduction"")\n                            return\n                        #print(""Needs Reduction"")\n                        if self.delay_allreduce or self.needs_refresh:\n                            # TODO:  How do we want to handle multiple backward passes between\n                            # each forward, e.g., backward passes with retain_graph=True?\n                            # needs_refresh and callback_queued are both vulnerable states.\n                            if not self.delay_allreduce and self.needs_refresh:\n                                # Use the backward pass to build the bucket structure on the fly.\n                                active_i = self.param_id_to_active_i[id(param)]\n\n                                # Float, half, and double tensors are grouped into buckets separately.\n                                current_type = self.param_type_to_tmp_i[param.type(\n                                )]\n\n                                self.tmp_buckets[current_type].append(active_i)\n\n                                ship_tmp_bucket = False\n                                if self.custom_allreduce_triggers:\n                                    if id(param) in self.allreduce_trigger_params:\n                                        ship_tmp_bucket = True\n                                else:\n                                    self.tmp_numels[current_type] += param.numel()\n                                    if self.tmp_numels[current_type] >= self.message_size:\n                                        ship_tmp_bucket = True\n\n                                # To consider:  If custom_allreduce_triggers are in use, ship all\n                                # tmp_buckets, not just tmp_buckets[current_type].\n                                if ship_tmp_bucket:\n                                    self.active_i_buckets.append(\n                                        self.tmp_buckets[current_type])\n                                    self.tmp_buckets[current_type] = []\n                                    self.tmp_numels[current_type] = 0\n\n                            if not self.callback_queued:\n                                Variable._execution_engine.queue_callback(\n                                    allreduce_params)\n                                self.callback_queued = True\n                        else:\n                            if not self.callback_queued:\n                                Variable._execution_engine.queue_callback(\n                                    overlapping_backward_epilogue)\n                                self.callback_queued = True\n\n                            self.comm_ready_buckets(param)\n\n                    grad_acc.register_hook(allreduce_hook)\n                    self.grad_accs.append(grad_acc)\n\n                wrapper(param)\n\n    def allreduce_bucket(self, bucket):\n        tensor = flatten(bucket)\n\n        tensor_to_allreduce = tensor\n\n        if self.allreduce_always_fp32:\n            tensor_to_allreduce = tensor.float()\n\n        if self.gradient_predivide_factor != 1.0:\n            tensor_to_allreduce.mul_(1./self.gradient_predivide_factor)\n\n        dist.all_reduce(tensor_to_allreduce)\n\n        if self.gradient_average:\n            tensor_to_allreduce.mul_(\n                self.gradient_predivide_factor/self.world_size)\n\n        if self.allreduce_always_fp32 and tensor is not tensor_to_allreduce:\n            tensor.copy_(tensor_to_allreduce)\n\n        return tensor\n\n    def allreduce_maybe_retain(self, bucket, bucket_idx=-1):\n        allreduced = self.allreduce_bucket(bucket)\n        if self.retain_allreduce_buffers:\n            if self.allreduce_buffers[bucket_idx] is not None:\n                raise RuntimeError(""The backward pass is attempting to replace an already-filled ""\n                                   ""allreduce buffer.  This is almost certainly an error."")\n            self.allreduce_buffers[bucket_idx] = allreduced\n        else:\n            for buf, synced in zip(bucket, unflatten(allreduced, bucket)):\n                buf.copy_(synced)\n\n    def allreduce_fallback(self):\n        grads = [param.grad.data for param in self.module.parameters()\n                 if param.grad is not None]\n\n        split_buckets = split_half_float_double(grads)\n\n        # If retain_allreduce_buffers is True and delay_allreduce is False,\n        # this will only be done during the first backward pass, ignored by the\n        # training script, and overwritten in the next forward pass.  So it\'s harmless.\n        if self.retain_allreduce_buffers:\n            self.allreduce_buffers = [None for _ in range(len(split_buckets))]\n\n        for i, bucket in enumerate(split_buckets):\n            allreduced = self.allreduce_maybe_retain(bucket, i)\n\n    def comm_ready_buckets(self, param):\n        # Need to do this in every hook for compatibility with Ruberry\'s streaming backward PR.\n        # self.reduction_stream.wait_stream(torch.cuda.current_stream())\n        #if dist.get_rank() == 0:\n        #    print(""Parameter Name"", param.name)\n        bucket_idx, bucket_loc = self.param_id_to_bucket[id(param)]\n\n        if self.buckets[bucket_idx][bucket_loc] is not None:\n            raise RuntimeError(""The backward pass is attempting to replace an already-filled ""\n                               ""bucket slot.  This is almost certainly an error."")\n\n        self.buckets[bucket_idx][bucket_loc] = param.grad.data\n        self.buckets_ready_size[bucket_idx] += 1\n\n        if self.buckets_ready_size[bucket_idx] == self.bucket_sizes[bucket_idx]:\n            if bucket_idx == self.next_bucket:\n                torch.cuda.current_stream().record_event(self.reduction_event)\n                self.reduction_stream.wait_event(self.reduction_event)\n                with torch.cuda.stream(self.reduction_stream):\n                    self.allreduce_maybe_retain(\n                        self.buckets[bucket_idx], bucket_idx)\n\n                    self.next_bucket += 1\n\n                    # Reversing upstream\'s logic here, because we constructed our buckets based on\n                    # the order things were received during backward.\n                    if len(self.ready_buckets_not_reduced) > 0:\n                        sorted_todo = sorted(self.ready_buckets_not_reduced)\n                        for i in sorted_todo:\n                            # Nothing can be reduced now\n                            if i > self.next_bucket:\n                                break\n                            elif i == self.next_bucket:\n                                self.allreduce_maybe_retain(self.buckets[i], i)\n                                self.ready_buckets_not_reduced.remove(i)\n                                self.next_bucket += 1\n                            else:\n                                raise ValueError(\n                                    ""i should always be >= next_bucket"")\n            else:\n                self.ready_buckets_not_reduced.add(bucket_idx)\n\n\n    def needs_refresh(self):\n        self.needs_refresh = True\n\n    def forward(self, *inputs, **kwargs):\n        result = self.module(*inputs, **kwargs)\n\n        if not self.delay_allreduce:\n            param_list = [\n                param for param in self.module.parameters() if param.requires_grad]\n\n            # Conditions under which to refresh self.record\n            # Forward has the authority to set needs_refresh to True, but only allreduce_params\n            # in backward has the authority to set needs_refresh to False.\n            # Parentheses are not necessary for correct order of operations, but make the intent clearer.\n            if ((not self.active_params) or\n                (len(param_list) != len(self.active_params)) or\n                    any([param1 is not param2 for param1, param2 in zip(param_list, self.active_params)])):\n                self.needs_refresh = True\n            #self.needs_refresh = True\n            if self.needs_refresh:\n                self.active_i_buckets = []\n                self.buckets = []\n                # [running half, float, double buckets]\n                self.tmp_buckets = [[], [], []]\n                self.tmp_numels = [0, 0, 0]\n                self.bucket_sizes = []\n                self.param_id_to_active_i = {\n                    id(param): i for i, param in enumerate(param_list)}\n                self.param_id_to_bucket = {}\n            else:\n                self.buckets = [[None for _ in range(self.bucket_sizes[i])]\n                                for i in range(self.num_buckets)]\n                self.buckets_ready_size = [0 for i in range(self.num_buckets)]\n                if(self.retain_allreduce_buffers):\n                    self.allreduce_buffers = [\n                        None for _ in range(self.num_buckets)]\n                self.next_bucket = 0\n                self.ready_buckets_not_reduced = set()\n\n            self.active_params = param_list\n\n        self.callback_queued = False\n\n        return result\n'"
pretrain/PyTorch/logger.py,0,"b""import logging\nimport os\n\n\nlogging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n                    datefmt='%m/%d/%Y %H:%M:%S',\n                    level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass Logger():\n    def __init__(self, cuda=False):\n        self.logger = logging.getLogger(__name__)\n        self.cuda = cuda\n\n    def info(self, message, *args, **kwargs):\n        local_rank = int(os.environ['OMPI_COMM_WORLD_LOCAL_RANK'])\n        if (self.cuda and local_rank == 0) or not self.cuda:\n            self.logger.info(message, *args, **kwargs)\n\n    def error(self, message, *args, **kwargs):\n        self.logger.error(message, *args, **kwargs)\n"""
pretrain/PyTorch/models.py,9,"b'import argparse\nimport logging\nimport random\nimport numpy as np\nimport os\nimport torch\nimport json\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.distributed as dist\nfrom torch.nn import CrossEntropyLoss, MSELoss\nfrom logger import Logger\n\nfrom dataset import BatchType\nfrom pytorch_pretrained_bert.tokenization import BertTokenizer\nfrom pytorch_pretrained_bert.modeling import BertModel, BertConfig\nfrom pytorch_pretrained_bert.modeling import BertPreTrainingHeads, BertPreTrainedModel, BertPreTrainingHeads, BertLMPredictionHead\nfrom pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n\n\nclass BertPretrainingLoss(BertPreTrainedModel):\n    def __init__(self, bert_encoder, config):\n        super(BertPretrainingLoss, self).__init__(config)\n        self.bert = bert_encoder\n        self.cls = BertPreTrainingHeads(\n            config, self.bert.embeddings.word_embeddings.weight)\n        self.cls.apply(self.init_bert_weights)\n\n    def forward(self, input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None, next_sentence_label=None):\n        sequence_output, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n                                                   output_all_encoded_layers=False)\n        prediction_scores, seq_relationship_score = self.cls(\n            sequence_output, pooled_output)\n\n        if masked_lm_labels is not None and next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            next_sentence_loss = loss_fct(\n                seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n            masked_lm_loss = loss_fct(\n                prediction_scores.view(-1, self.config.vocab_size), masked_lm_labels.view(-1))\n            total_loss = masked_lm_loss + next_sentence_loss\n            return total_loss\n        else:\n            return prediction_scores, seq_relationship_score\n\n\nclass MTLRouting(nn.Module):\n    """"""This setup is to add MultiTask Training support in BERT Training. \n    """"""\n    def __init__(self, encoder: BertModel, write_log, summary_writer):\n        super(MTLRouting, self).__init__()\n        self.bert_encoder = encoder\n        self._batch_loss_calculation = nn.ModuleDict()\n        self._batch_counter = {}\n        self._batch_module_name = {}\n        self._batch_name = {}\n        self.write_log = write_log\n        self.logger = Logger(cuda=torch.cuda.is_available())\n        self.summary_writer = summary_writer\n\n    def register_batch(self, batch_type, module_name, loss_calculation: nn.Module):\n        assert isinstance(loss_calculation, nn.Module)\n        self._batch_loss_calculation[str(batch_type.value)] = loss_calculation\n        self._batch_counter[batch_type] = 0\n        self._batch_module_name[batch_type] = module_name\n\n    def log_summary_writer(self, batch_type, logs: dict, base=\'Train\'):\n        if self.write_log:\n            counter = self._batch_counter[batch_type]\n            module_name = self._batch_module_name.get(\n                batch_type, self._get_batch_type_error(batch_type))\n            for key, log in logs.items():\n                self.summary_writer.add_scalar(\n                    f\'{base}/{module_name}/{key}\', log, counter)\n            self._batch_counter[batch_type] = counter + 1\n\n    def _get_batch_type_error(self, batch_type):\n        def f(*args, **kwargs):\n            message = f\'Misunderstood batch type of {batch_type}\'\n            self.logger.error(message)\n            raise ValueError(message)\n        return f\n\n    def forward(self, batch, log=True):\n        batch_type = batch[0][0].item()\n\n        # Pretrain Batch\n        if batch_type == BatchType.PRETRAIN_BATCH:\n            loss_function = self._batch_loss_calculation[str(batch_type)]\n\n            loss = loss_function(input_ids=batch[1],\n                                 token_type_ids=batch[3],\n                                 attention_mask=batch[2],\n                                 masked_lm_labels=batch[5],\n                                 next_sentence_label=batch[4])\n            if log:\n                self.log_summary_writer(\n                    batch_type, logs={\'pretrain_loss\': loss.item()})\n            return loss\n\n\nclass BertMultiTask:\n    def __init__(self, job_config, use_pretrain, tokenizer, cache_dir, device, write_log, summary_writer):\n        self.job_config = job_config\n\n        if not use_pretrain:\n            model_config = self.job_config.get_model_config()\n            bert_config = BertConfig(**model_config)\n            bert_config.vocab_size = len(tokenizer.vocab)\n\n            self.bert_encoder = BertModel(bert_config)\n        # Use pretrained bert weights\n        else:\n            self.bert_encoder = BertModel.from_pretrained(self.job_config.get_model_file_type(), cache_dir=cache_dir)\n            bert_config = self.bert_encoder.config\n\n        self.network=MTLRouting(self.bert_encoder, write_log = write_log, summary_writer = summary_writer)\n\n        #config_data=self.config[\'data\']\n\n        # Pretrain Dataset\n        self.network.register_batch(BatchType.PRETRAIN_BATCH, ""pretrain_dataset"", loss_calculation=BertPretrainingLoss(self.bert_encoder, bert_config))\n\n        self.device=device\n        # self.network = self.network.float()\n        # print(f""Bert ID: {id(self.bert_encoder)}  from GPU: {dist.get_rank()}"")\n\n    def save(self, filename: str):\n        network=self.network.module\n        return torch.save(network.state_dict(), filename)\n\n    def load(self, model_state_dict: str):\n        return self.network.module.load_state_dict(torch.load(model_state_dict, map_location=lambda storage, loc: storage))\n\n    def move_batch(self, batch, non_blocking=False):\n        return batch.to(self.device, non_blocking)\n\n    def eval(self):\n        self.network.eval()\n\n    def train(self):\n        self.network.train()\n\n    def save_bert(self, filename: str):\n        return torch.save(self.bert_encoder.state_dict(), filename)\n\n    def to(self, device):\n        assert isinstance(device, torch.device)\n        self.network.to(device)\n\n    def half(self):\n        self.network.half()\n'"
pretrain/PyTorch/optimization.py,0,"b'import math\n\ndef warmup_linear(x, warmup=0.002):\n    if warmup == 0.0:\n        return 1.0\n    elif x < warmup:\n        return x/warmup\n    return 1.0 - x\n\n\ndef warmup_linear_decay_exp(global_step, decay_rate, decay_steps, total_steps, warmup=0.002):\n    x = global_step/total_steps\n    warmup_end = warmup * total_steps\n    if warmup == 0.0:\n        return 1.0\n    elif x < warmup:\n        return x/warmup\n    return decay_rate**((global_step-warmup_end)/decay_steps)\n\nclass LinearWarmupExponentialSchedule():\n    def __init__(self, warmup=0.002, t_total=-1, initial_lr = 2e-5, final_lr=5e-6, decay_rate=0.99):\n        self.warmup = warmup\n        self.total_steps = t_total\n        self.decay_rate = decay_rate\n        self.warmup_end = self.warmup * t_total\n\n        # Calculate the decay Steps\n        self.decay_steps = int(math.ceil((math.log(self.decay_rate)/ math.log(final_lr/initial_lr)) * (1.0 - warmup) * t_total))\n\n    def get_lr(self, global_step):\n        x = global_step/self.total_steps\n        if self.warmup == 0.0:\n            return 1.0\n        elif x < self.warmup:\n            return x/self.warmup\n        return self.decay_rate**((global_step-self.warmup_end)/self.decay_steps)\n'"
pretrain/PyTorch/sources.py,0,"b'from tqdm import tqdm\nfrom typing import Tuple\nfrom random import shuffle\nimport pickle\nimport random\n\nfrom pytorch_pretrained_bert.tokenization import BertTokenizer\n\n\ndef truncate_input_sequence(tokens_a, tokens_b, max_num_tokens):\n    while True:\n        total_length = len(tokens_a) + len(tokens_b)\n        if total_length <= max_num_tokens:\n            break\n\n        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n        assert len(trunc_tokens) >= 1\n\n        # We want to sometimes truncate from the front and sometimes from the\n        # back to add more randomness and avoid biases.\n        if random.random() < 0.5:\n            del trunc_tokens[0]\n        else:\n            trunc_tokens.pop()\n\n\nclass TokenInstance:\n    def __init__(self, tokens_a, tokens_b, is_next):\n        self.tokens_a = tokens_a\n        self.tokens_b = tokens_b\n        self.is_next = is_next  # 0 is if in continuation, 1 if is random\n\n    def get_values(self):\n        return (self.tokens_a, self.tokens_b, self.is_next)\n\n\nclass PretrainingDataCreator:\n    def __init__(self, path, tokenizer: BertTokenizer,  max_seq_length, readin: int = 2000000, dupe_factor: int = 5, small_seq_prob: float = 0.1):\n        self.dupe_factor = dupe_factor\n        self.max_seq_length = max_seq_length\n        self.small_seq_prob = small_seq_prob\n\n        documents = []\n        instances = []\n        with open(path, encoding=\'utf-8\') as fd:\n            for i, line in enumerate(tqdm(fd)):\n                line = line.replace(\'\\n\', \'\')\n                # Expected format (Q,T,U,S,D)\n                # query, title, url, snippet, document = line.split(\'\\t\')\n                # ! remove this following line later\n                document = line\n                if len(document.split(""<sep>"")) <= 3:\n                    continue\n                lines = document.split(""<sep>"")\n                document = []\n                for seq in lines:\n                    document.append(tokenizer.tokenize(seq))\n                # document = list(map(tokenizer.tokenize, lines))\n                documents.append(document)\n\n        documents = [x for x in documents if x]\n\n        self.documents = documents\n        for _ in range(self.dupe_factor):\n            for index in range(len(self.documents)):\n                instances.extend(self.create_training_instance(index))\n\n        shuffle(instances)\n        self.instances = instances\n        self.len = len(self.instances)\n        self.documents = None\n        documents = None\n\n    def __len__(self):\n        return self.len\n\n    def __getstate__(self):\n        state = self.__dict__.copy()\n        return state\n\n    def __setstate__(self, state):\n        self.__dict__.update(state)\n\n    def save(self, filename):\n        with open(filename, \'wb\') as outfile:\n            pickle.dump(self, outfile)\n\n    @staticmethod\n    def load(filename):\n        print(""Loading filename {}"".format(filename))\n        with open(filename, \'rb\') as f:\n            return pickle.load(f)\n\n    def create_training_instance(self, index):\n        document = self.documents[index]\n\n        # Need to add [CLS] + 2*[SEP] tokens\n        max_num_tokens = self.max_seq_length - 3\n\n        # We want to maximize the inp sequence but also want inputs similar\n        # to our generic task inputs which will be compartively smaller\n        # than the data on which we intend to pre-train.\n        target_seq_length = max_num_tokens\n        if random.random() < self.small_seq_prob:\n            target_seq_length = random.randint(5, max_num_tokens)\n\n        # Need to make the sequences split for NSP task for interesting\n        # rather than choosing some arbitrary point. If not the NSP\n        # task might become way too easy.\n        instances = []\n        current_chunk = []\n        current_length = 0\n        i = 0\n        while i < len(document):\n            segment = document[i]\n            current_chunk.append(segment)\n            current_length += len(segment)\n            if i == len(document)-1 or current_length >= target_seq_length:\n                if current_chunk:\n                    # `a_end` is how many segments from `current_chunk` go into the `A`\n                    # (first) sentence.\n                    a_end = 1\n                    if len(current_chunk) >= 2:\n                        a_end = random.randint(1, len(current_chunk) - 1)\n\n                    tokens_a = []\n                    for j in range(a_end):\n                        tokens_a.extend(current_chunk[j])\n\n                    tokens_b = []\n\n                    # Random Next\n                    is_random_next = False\n                    if len(current_chunk) == 1 or random.random() < 0.5:\n                        is_random_next = True\n                        target_b_length = target_seq_length - len(tokens_a)\n\n                        # Pick a random document\n                        for _ in range(10):\n                            random_doc_index = random.randint(\n                                0, len(self.documents) - 1)\n                            if random_doc_index != index:\n                                break\n\n                        random_doc = self.documents[random_doc_index]\n                        random_start = random.randint(0, len(random_doc)-1)\n                        for j in range(random_start, len(random_doc)):\n                            tokens_b.extend(random_doc[j])\n                            if len(tokens_b) >= target_b_length:\n                                break\n\n                        # We didn\'t actually use these segments so we ""put them back"" so\n                        # they don\'t go to waste.\n                        num_unused_segments = len(current_chunk) - a_end\n                        i -= num_unused_segments\n\n                    # Actual Next\n                    else:\n                        is_random_next = False\n                        for j in range(a_end, len(current_chunk)):\n                            tokens_b.extend(current_chunk[j])\n\n                    truncate_input_sequence(tokens_a, tokens_b, max_num_tokens)\n\n                    assert len(tokens_a) >= 1\n                    assert len(tokens_b) >= 1\n\n                    instances.append(TokenInstance(\n                        tokens_a, tokens_b, int(is_random_next)))\n\n                current_chunk = []\n                current_length = 0\n            i += 1\n\n        return instances\n\n\nclass GenericPretrainingDataCreator(PretrainingDataCreator):\n    def __init__(self, path, tokenizer: BertTokenizer,  max_seq_length: int = 512, readin: int = 2000000, dupe_factor: int = 6, small_seq_prob: float = 0.1):\n        self.dupe_factor = dupe_factor\n        self.max_seq_length = max_seq_length\n        self.small_seq_prob = small_seq_prob\n\n        documents = []\n        instances = []\n        with open(path, encoding=\'utf-8\') as fd:\n            document = []\n            for i, line in enumerate(tqdm(fd)):\n                line = line.replace(\'\\n\', \'\')\n                # document = line\n                # if len(document.split(""<sep>"")) <= 3:\n                #     continue\n                if len(line) == 0:  # This is end of document\n                    documents.append(document)\n                    document = []\n                if len(line.split(\' \')) > 2:\n                    document.append(tokenizer.tokenize(line))\n            if len(document) > 0:\n                documents.append(document)\n\n        documents = [x for x in documents if x]\n        print(documents[0])\n        print(len(documents))\n        self.documents = documents\n        for _ in range(self.dupe_factor):\n            for index in range(len(self.documents)):\n                instances.extend(self.create_training_instance(index))\n\n        shuffle(instances)\n        self.instances = instances\n        self.len = len(self.instances)\n        self.documents = None\n        documents = None\n\nclass WikiPretrainingDataCreator(PretrainingDataCreator):\n    def __init__(self, path, tokenizer: BertTokenizer,  max_seq_length: int = 512, readin: int = 2000000, dupe_factor: int = 6, small_seq_prob: float = 0.1):\n        self.dupe_factor = dupe_factor\n        self.max_seq_length = max_seq_length\n        self.small_seq_prob = small_seq_prob\n\n        documents = []\n        instances = []\n        with open(path, encoding=\'utf-8\') as fd:\n            document = []\n            for i, line in enumerate(tqdm(fd)):\n                line = line.replace(\'\\n\', \'\')\n                # document = line\n                # if len(document.split(""<sep>"")) <= 3:\n                #     continue\n                if len(line) > 0 and line[:2] ==  ""[["" : # This is end of document\n                    documents.append(document)\n                    document = []\n                if len(line.split(\' \')) > 2:\n                    document.append(tokenizer.tokenize(line))\n            if len(document) > 0:\n                documents.append(document)\n\n        documents = [x for x in documents if x]\n        self.documents = documents\n        for _ in range(self.dupe_factor):\n            for index in range(len(self.documents)):\n                instances.extend(self.create_training_instance(index))\n\n        shuffle(instances)\n        self.instances = instances\n        self.len = len(self.instances)\n        self.documents = None\n        documents = None'"
pretrain/PyTorch/text.py,1,b'import torch\n\nPAD = 0\n\ndef mask(x):\n    return x != PAD\n\ndef torch_long(x):\n    return torch.LongTensor(x)\n'
pretrain/PyTorch/train.py,13,"b'from datetime import datetime\n\nimport numpy as np\nimport random\nimport os\nimport sys\nimport json\nimport shutil\nimport torch\nimport torch.nn as nn\nimport torch.distributed as dist\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import RandomSampler\nfrom torch.utils.data.distributed import DistributedSampler\n\nimport argparse\nfrom tqdm import tqdm\nfrom checkpoint import checkpoint_model, load_checkpoint, latest_checkpoint_file\nfrom logger import Logger\nfrom utils import get_sample_writer\nfrom models import BertMultiTask\nfrom dataset import PreTrainingDataset\nfrom dataset import PretrainDataType\nfrom pytorch_pretrained_bert.tokenization import BertTokenizer\nfrom pytorch_pretrained_bert.optimization import BertAdam\nfrom optimization import warmup_linear_decay_exp\nfrom azureml_adapter import set_environment_variables_for_nccl_backend, get_local_rank, get_global_size, get_local_size\nfrom sources import PretrainingDataCreator, TokenInstance, GenericPretrainingDataCreator\nfrom sources import WikiPretrainingDataCreator\nfrom configuration import BertJobConfiguration\n\nfrom azureml.core.run import Run\n\n\ndef get_effective_batch(total):\n    if use_multigpu_with_single_device_per_process:\n        return total//dist.get_world_size()//train_batch_size//gradient_accumulation_steps\n    else:\n        return total//train_batch_size//gradient_accumulation_steps # Dividing with gradient_accumulation_steps since we multiplied it earlier\n\n\ndef get_dataloader(dataset: Dataset, eval_set=False):\n    if not use_multigpu_with_single_device_per_process:\n        train_sampler = RandomSampler(dataset)\n    else:\n        train_sampler = DistributedSampler(dataset)\n    return (x for x in DataLoader(dataset, batch_size=train_batch_size // 2 if eval_set else train_batch_size,\n                                  sampler=train_sampler, num_workers=job_config.get_num_workers()))\n\n\ndef pretrain_validation(index):\n    model.eval()\n    dataset = PreTrainingDataset(tokenizer=tokenizer,\n                                 folder=args.validation_path,\n                                 logger=logger, max_seq_length=max_seq_length,\n                                 index=index, data_type=PretrainDataType.VALIDATION,\n                                 max_predictions_per_seq=max_predictions_per_seq,\n                                 masked_lm_prob=masked_lm_prob)\n    data_batches = get_dataloader(dataset, eval_set=True)\n    eval_loss = 0\n    nb_eval_steps = 0\n\n    for batch in data_batches:\n        batch = tuple(t.to(device) for t in batch)\n        tmp_eval_loss = model.network(batch, log=False)\n        dist.reduce(tmp_eval_loss, 0)\n        # Reduce to get the loss from all the GPU\'s\n        tmp_eval_loss = tmp_eval_loss / dist.get_world_size()\n        eval_loss += tmp_eval_loss.mean().item()\n        nb_eval_steps += 1\n    eval_loss = eval_loss / nb_eval_steps\n    logger.info(f""Validation Loss for epoch {index + 1} is: {eval_loss}"")\n    if check_write_log():\n        summary_writer.add_scalar(f\'Validation/Loss\', eval_loss, index + 1)\n        run.log(""validation_loss"", np.float(eval_loss))\n        run.log_row(""validation_loss over epochs"", epoch = index, val_loss = np.float(eval_loss))\n    return eval_loss\n\n\ndef train(index):\n    model.train()\n    dataloaders = {}\n    i = 0\n    global global_step\n    datalengths = []\n    batchs_per_dataset = []\n\n    # Pretraining datasets\n    wiki_pretrain_dataset = PreTrainingDataset(tokenizer=tokenizer,\n                                               folder=args.train_path,\n                                               logger=logger, max_seq_length=max_seq_length,\n                                               index=index, data_type=PretrainDataType.WIKIPEDIA,\n                                               max_predictions_per_seq=max_predictions_per_seq,\n                                               masked_lm_prob=masked_lm_prob)\n\n    datalengths.append(len(wiki_pretrain_dataset))\n    dataloaders[i] = get_dataloader(wiki_pretrain_dataset)\n\n    num_batches_in_dataset = get_effective_batch(len(wiki_pretrain_dataset))\n    logger.info(\'Wikpedia data file: Number of samples {}, number of batches required to process these samples: {}\'.format(len(wiki_pretrain_dataset), num_batches_in_dataset))\n    \n    batchs_per_dataset.append(num_batches_in_dataset)\n    i += 1\n\n    logger.info(""Training on Wikipedia dataset"")\n\n    total_length = sum(datalengths)\n\n    dataset_batches = []\n    for i, batch_count in enumerate(batchs_per_dataset):\n        dataset_batches.extend([i] * batch_count)\n    logger.info(\'Number of batches to process *all* data samples in this epoch: {}\'.format(len(dataset_batches)))\n    # shuffle\n    random.shuffle(dataset_batches)\n\n    # We don\'t want the dataset to be n the form of alternate chunks if we have more than\n    # one dataset type, instead we want to organize them into contiguous chunks of each\n    # data type, hence the multiplication with grad_accumulation_steps with dataset_batch_type\n    dataset_picker = []\n    for dataset_batch_type in dataset_batches:\n        dataset_picker.extend([dataset_batch_type] * gradient_accumulation_steps )\n\n    logger.info(\'Number of steps to process all batches in this epoch: {}\'.format(len(dataset_picker)))\n    model.train()\n\n    # Counter of sequences in an ""epoch""\n    sequences_counter = 0\n    global_step_loss = 0\n\n    for step, dataset_type in enumerate(dataset_picker):\n        try:\n            batch = next(dataloaders[dataset_type])\n\n            sequences_counter += len(batch)\n\n            if n_gpu == 1:\n                batch = tuple(t.to(device) for t in batch)  # Move to GPU\n\n            if step > 1 and step % 1000 == 0:\n                logger.info(""{} Number of sequences processed so far: {} (cumulative in {} steps)"".format(datetime.utcnow(), sequences_counter, step))\n            # Calculate forward pass\n            loss = model.network(batch)\n\n            if n_gpu > 1:\n                # this is to average loss for multi-gpu. In DistributedDataParallel\n                # setting, we get tuple of losses form all proccesses\n                loss = loss.mean()\n\n            if gradient_accumulation_steps > 1:\n                loss = loss / gradient_accumulation_steps\n\n            # Enabling  optimized Reduction\n            # reduction only happens in backward if this method is called before\n            # when using the distributed module\n            if accumulate_gradients:\n                if use_multigpu_with_single_device_per_process and (step + 1) % gradient_accumulation_steps == 0:\n                    model.network.enable_need_reduction()\n                else:\n                    model.network.disable_need_reduction()\n            if fp16:\n                optimizer.backward(loss)\n            else:\n                loss.backward()\n\n            global_step_loss += loss\n            if (step + 1) % gradient_accumulation_steps == 0:\n                if fp16:\n                    # modify learning rate with special warm up BERT uses\n                    # if fp16 is False, BertAdam is used that handles this automatically\n                    lr_this_step = \\\n                        job_config.get_learning_rate() * warmup_linear_decay_exp(global_step,\n                                                                                 job_config.get_decay_rate(),\n                                                                                 job_config.get_decay_step(),\n                                                                                 job_config.get_total_training_steps(),\n                                                                                 job_config.get_warmup_proportion())\n                    for param_group in optimizer.param_groups:\n                        param_group[\'lr\'] = lr_this_step\n\n                    # Record the LR against global_step on tensorboard\n                    if check_write_log():\n                        summary_writer.add_scalar(f\'Train/lr\', lr_this_step, global_step)\n                    \n                optimizer.step()\n                optimizer.zero_grad()\n                global_step += 1\n                if check_write_log() and (global_step%args.log_steps == 0):\n                    run.log(""training_loss"", np.float(global_step_loss))\n                    run.log(""lr_this_step"", np.float(lr_this_step))\n                    run.log_row(""loss over steps"", global_step = global_step, loss =  np.float(global_step_loss))\n                    run.log_row(""lr over steps"", global_step = global_step, lr  = np.float(lr_this_step))\n                global_step_loss = 0\n        except StopIteration:\n            continue\n        \n    logger.info(""Completed {} steps"".format(step))\n    logger.info(""Completed processing {} sequences"".format(sequences_counter))\n\n    # Run Validation Loss\n    if max_seq_length == 512:\n        logger.info(f""TRAIN BATCH SIZE: {train_batch_size}"")\n        return pretrain_validation(index)\n    else:\n        return None\n\n\ndef str2bool(val):\n    return val.lower() == ""true"" or val.lower() == ""t"" or val.lower() == ""1""\n\ndef check_write_log():\n    return dist.get_rank() == 0 or not use_multigpu_with_single_device_per_process\n\nif __name__ == \'__main__\':\n    print(""The arguments are: "" + str(sys.argv))\n\n    parser = argparse.ArgumentParser()\n\n    # Required_parameters\n    parser.add_argument(""--config_file"", ""--cf"",\n                        help=""pointer to the configuration file of the experiment"", type=str, required=True)\n\n    parser.add_argument(""--config_file_path"", default=None, type=str, required=True,\n                        help=""The blob storage directory where config file is located."")\n\n    parser.add_argument(""--train_path"", default=None, type=str, required=True,\n                        help=""The blob storage directory for train data, cache and output."")\n\n    parser.add_argument(""--validation_path"", default=None, type=str, required=True,\n                        help=""The blob storage directory for validation data, cache and output."")\n\n    parser.add_argument(\'--tokenizer_path\', type=str, default=False,\n                    help=""Path to load the tokenizer from"")\n    parser.add_argument(""--output_dir"", default=None, type=str, required=True,\n                        help=""If given, model checkpoints will be saved to this directory."")\n    \n    # Optional Params\n    parser.add_argument(""--best_cp_dir"", default=None, type=str,\n                        help=""If given, model best checkpoint will be saved to this directory."")\n    parser.add_argument(""--latest_cp_dir"", default=None, type=str,\n                        help=""If given, model latest checkpoint will be saved to this directory."")\n    parser.add_argument(""--max_seq_length"", default=512, type=int,\n                        help=""The maximum total input sequence length after WordPiece tokenization. Sequences ""\n                             ""longer than this will be truncated, and sequences shorter than this will be padded."")\n    parser.add_argument(""--max_predictions_per_seq"", ""--max_pred"", default=80, type=int,\n                        help=""The maximum number of masked tokens in a sequence to be predicted."")\n    parser.add_argument(""--masked_lm_prob"", ""--mlm_prob"", default=0.15,\n                        type=float, help=""The masking probability for languge model."")\n    parser.add_argument(""--train_batch_size"", default=32,\n                        type=int, help=""Total batch size for training."")\n    parser.add_argument(""--no_cuda"",\n                        type=str,\n                        default=\'False\',\n                        help=""Whether not to use CUDA when available"")\n    parser.add_argument(\'--seed\',\n                        type=int,\n                        default=42,\n                        help=""random seed for initialization"")\n    parser.add_argument(\'--accumulate_gradients\',\n                        type=str,\n                        default=\'True\',\n                        help=""Enabling gradient accumulation optimization"")\n    parser.add_argument(\'--gradient_accumulation_steps\',\n                        type=int,\n                        default=1,\n                        help=""Number of updates steps to accumulate before performing a backward/update pass."")\n    parser.add_argument(\'--fp16\',\n                        type=str,\n                        default=\'False\',\n                        help=""Whether to use 16-bit float precision instead of 32-bit"")\n    parser.add_argument(\'--use_pretrain\',\n                        type=str,\n                        default=\'False\',\n                        help=""Whether to use Bert Pretrain Weights or not"")\n    parser.add_argument(\'--loss_scale\',\n                        type=float,\n                        default=0,\n                        help=\'Loss scaling, positive power of 2 values can improve fp16 convergence.\')\n    parser.add_argument(\'--load_training_checkpoint\', \'--load_cp\',\n                        type=str,\n                        default=\'False\',\n                        help=""This is the path to the TAR file which contains model+opt state_dict() checkpointed."")\n    parser.add_argument(\'--use_multigpu_with_single_device_per_process\',\n                        type=str,\n                        default=\'True\',\n                        help=""Whether only one device is managed per process"")\t    \n    parser.add_argument(\'--epochs\',\t\t\n                        type=int,\t\t\n                        default=250,\t\t\n                        help=""total number of epochs"")\n    parser.add_argument(\'--log_steps\',\t\t\n                        type=int,\t\t\n                        default=50,\t\t\n                        help=""logging intervals"")\n    parser.add_argument(\'--backend\',\t\t\n                        type=str,\t\t\n                        default=\'nccl\',\t\t\n                        help=""reduce backend to use"")\n\n    parser.add_argument(\'--master_port\',\t\t\n                        type=int,\t\t\n                        default=6105,\t\t\n                        help=""user specified master port for non-mpi job"")\n    \n    args = parser.parse_args()\n\n    if args.output_dir:\n        os.makedirs(args.output_dir, exist_ok=True)\n    if args.best_cp_dir:\n        os.makedirs(args.best_cp_dir, exist_ok=True)\n    if args.latest_cp_dir:\n        os.makedirs(args.latest_cp_dir, exist_ok=True)\n\n    no_cuda = str2bool(args.no_cuda)\n    fp16 = str2bool(args.fp16)\n    accumulate_gradients = str2bool(args.accumulate_gradients)\n    use_pretrain = str2bool(args.use_pretrain)\n    use_multigpu_with_single_device_per_process = str2bool(args.use_multigpu_with_single_device_per_process)\n\n    config_file = args.config_file\n    gradient_accumulation_steps = args.gradient_accumulation_steps\n    train_batch_size = args.train_batch_size\n    seed = args.seed\n    loss_scale = args.loss_scale\n    load_training_checkpoint = args.load_training_checkpoint\n    max_seq_length = args.max_seq_length\n    max_predictions_per_seq = args.max_predictions_per_seq\n    masked_lm_prob = args.masked_lm_prob\n    master_port = args.master_port\n\n    local_rank = -1\n\n    local_rank = get_local_rank()\n    global_size = get_global_size()\n    local_size = get_local_size()\t\n    # TODO use logger\t\n    print(\'local_rank = {}\'.format(local_rank))\n    print(\'global_size = {}\'.format(global_size))\n    print(\'local_size = {}\'.format(local_size))\n\n    set_environment_variables_for_nccl_backend(local_size == global_size, master_port)\n\n    # Prepare Logger\n    logger = Logger(cuda=torch.cuda.is_available())\n\n    # # Extact config file from blob storage\n    job_config = BertJobConfiguration(config_file_path=os.path.join(args.config_file_path, config_file))\n\n    job_name = job_config.get_name()\n    # Setting the distributed variables\n\n    run = Run.get_context()\n\n    if not use_multigpu_with_single_device_per_process:\n        device = torch.device(""cuda"")\n        n_gpu = torch.cuda.device_count()\n    else:\n        device = torch.device(""cuda"", local_rank)\n        n_gpu = 1\n        # Initializes the distributed backend which will take care of synchronizing nodes/GPUs\n        torch.distributed.init_process_group(backend=args.backend)\n        if fp16:\n            logger.info(""16-bits distributed training is not officially supported in the version of PyTorch currently used, but it works. Refer to https://github.com/pytorch/pytorch/pull/13496 for supported version."")\n            fp16 = True  #\n    logger.info(""device: {} n_gpu: {}, use_multigpu_with_single_device_per_process: {}, 16-bits training: {}"".format(\n        device, n_gpu, use_multigpu_with_single_device_per_process, fp16))\n\n    if gradient_accumulation_steps < 1:\n        raise ValueError(""Invalid gradient_accumulation_steps parameter: {}, should be >= 1"".format(\n            gradient_accumulation_steps))\n\n    train_batch_size = int(train_batch_size / gradient_accumulation_steps)\n\n    # Setting all the seeds so that the task is random but same accross processes\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    logger.info\n    if n_gpu > 0:\n        torch.cuda.manual_seed_all(seed)\n\n    # Create an outputs/ folder in the blob storage\n    if args.output_dir is None:\n        parent_dir = os.path.join(args.output_dir, \'outputs\', str(run.experiment.name))\n        output_dir = os.path.join(parent_dir, str(run.id))\n        os.makedirs(output_dir, exist_ok=True)\n        saved_model_path = os.path.join(output_dir, ""saved_models"", job_name)\n        os.makedirs(saved_model_path, exist_ok=True)\n    else:\n        saved_model_path = args.output_dir\n\n    summary_writer = None\n    # Prepare Summary Writer and saved_models path\n    if check_write_log():\n        #azureml.tensorboard only streams from /logs directory, therefore hardcoded\n        summary_writer = get_sample_writer(\n            name=job_name, base=\'./logs\')\n\n    # Loading Tokenizer (vocabulary from blob storage, if exists)\n    logger.info(""Extracting the vocabulary"")\n    if args.tokenizer_path:\n        logger.info(f\'Loading tokenizer from {args.tokenizer_path}\')\n        tokenizer = BertTokenizer.from_pretrained(\n            args.tokenizer_path, cache_dir=args.output_dir)\n    else:\n        tokenizer = BertTokenizer.from_pretrained(job_config.get_token_file_type(), cache_dir=args.output_dir)\n    logger.info(""Vocabulary contains {} tokens"".format(len(list(tokenizer.vocab.keys()))))\n\n\n    # Loading Model\n    logger.info(""Initializing BertMultiTask model"")\n    model = BertMultiTask(job_config = job_config, use_pretrain = use_pretrain, tokenizer = tokenizer, \n                          cache_dir = args.output_dir, device = device, write_log = check_write_log(), \n                          summary_writer = summary_writer)\n\n    logger.info(""Converting the input parameters"")\n    if fp16:\n        model.half()\n        \n    model.to(device)\n\n    if use_multigpu_with_single_device_per_process:\n        try:\n            if accumulate_gradients:\n                logger.info(""Enabling gradient accumulation by using a forked version of DistributedDataParallel implementation available in the branch bertonazureml/apex at https://www.github.com/microsoft/apex"")\n                from distributed_apex import DistributedDataParallel as DDP\n            else:\n                logger.info(""Using Default Apex DistributedDataParallel implementation"")\n                from apex.parallel import DistributedDataParallel as DDP\n        except ImportError:\n            raise ImportError(""To use distributed and fp16 training, please install apex from the branch bertonazureml/apex at https://www.github.com/microsoft/apex."")\n        torch.cuda.set_device(local_rank)\n        model.network = DDP(model.network, delay_allreduce=False)\n\n    elif n_gpu > 1:\n        model.network = nn.DataParallel(model.network)\n\n    # Prepare Optimizer\n    logger.info(""Preparing the optimizer"")\n    param_optimizer = list(model.network.named_parameters())\n    param_optimizer = [n for n in param_optimizer if \'pooler\' not in n[0]]\n    no_decay = [\'bias\', \'LayerNorm.bias\', \'LayerNorm.weight\']\n    optimizer_grouped_parameters = [\n        {\'params\': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \'weight_decay\': 0.01},\n        {\'params\': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \'weight_decay\': 0.0}\n    ]\n\n    logger.info(""Loading Apex and building the FusedAdam optimizer"")\n\n    if fp16:\n        try:\n            from apex.optimizers import FP16_Optimizer, FusedAdam\n        except:\n            raise ImportError(""To use distributed and fp16 training, please install apex from the branch bertonazureml/apex at https://www.github.com/microsoft/apex."")\n\n        optimizer = FusedAdam(optimizer_grouped_parameters,\n                              lr=job_config.get_learning_rate(),\n                              bias_correction=False,\n                              max_grad_norm=1.0)\n        if loss_scale == 0:\n            optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n        else:\n            optimizer = FP16_Optimizer(\n                optimizer, static_loss_scale=loss_scale)\n    else:\n        optimizer = BertAdam(optimizer_grouped_parameters,\n                             lr=job_config.get_learning_rate(),\n                             warmup=job_config.get_warmup_proportion(),\n                             t_total=job_config.get_total_training_steps())\n\n    global_step = 0\n    start_epoch = 0\n    \n    # if args.load_training_checkpoint is not None:\n    if load_training_checkpoint != \'False\':\n        logger.info(f""Looking for previous training checkpoint."")\n        latest_checkpoint_path = latest_checkpoint_file(args.load_training_checkpoint, no_cuda)\n\n        logger.info(f""Restoring previous training checkpoint from {latest_checkpoint_path}"")\n        start_epoch, global_step = load_checkpoint(model, optimizer, latest_checkpoint_path)\n        logger.info(f""The model is loaded from last checkpoint at epoch {start_epoch} when the global steps were at {global_step}"")\n\n\n    logger.info(""Training the model"")\n\n    best_loss = None\n    for index in range(start_epoch, args.epochs):\n        logger.info(f""Training epoch: {index + 1}"")\n        \n        eval_loss = train(index)\n\n        if check_write_log():\n            if best_loss is None or eval_loss is None or eval_loss < best_loss*0.99:\n                best_loss = eval_loss\n                epoch_ckp_path = os.path.join(saved_model_path, ""bert_encoder_epoch_{0:04d}.pt"".format(index + 1))\n                checkpoint_model(os.path.join(saved_model_path, ""training_state_checkpoint_{0:04d}.tar"".format(index + 1)), model, optimizer, index, global_step)\n                logger.info(f""Saving checkpoint of the model from epoch {index + 1} at {epoch_ckp_path}"")\n                model.save_bert(epoch_ckp_path)\n\n                #save best checkpoint in separate directory\n                if args.best_cp_dir:\n                    best_ckp_path = os.path.join(args.best_cp_dir, ""bert_encoder_epoch_{0:04d}.pt"".format(index + 1))\n                    shutil.rmtree(args.best_cp_dir)\n                    os.makedirs(args.best_cp_dir,exist_ok=True)\n                    model.save_bert(best_ckp_path)\n                \n            if args.latest_cp_dir:\n                shutil.rmtree(args.latest_cp_dir)\n                os.makedirs(args.latest_cp_dir,exist_ok=True)\n                checkpoint_model(os.path.join(args.latest_cp_dir, ""training_state_checkpoint_{0:04d}.tar"".format(index + 1)), model, optimizer, index, global_step)\n                latest_ckp_path = os.path.join(args.latest_cp_dir, ""bert_encoder_epoch_{0:04d}.pt"".format(index + 1))\n                model.save_bert(latest_ckp_path)\n'"
pretrain/PyTorch/utils.py,0,"b'import sys as _sys\n\nfrom typing import List\nfrom collections import _iskeyword  # type: ignore\nfrom tensorboardX import SummaryWriter\nimport os\n\nSUMMARY_WRITER_DIR_NAME = \'runs\'\n\n\ndef get_sample_writer(name, base=""..""):\n    """"""Returns a tensorboard summary writer\n    """"""\n    return SummaryWriter(log_dir=os.path.join(base, SUMMARY_WRITER_DIR_NAME, name))\n'"
pretrain/PyTorch/dataprep/create_pretraining.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport unicodedata\nimport os\nimport logging\nfrom multiprocessing import Pool\nimport multiprocessing\nimport os\nimport logging\nimport shutil\nimport tempfile\nimport argparse\nimport json\nfrom urllib.parse import urlparse\nfrom pathlib import Path\nfrom typing import Optional, Tuple, Union, IO, Callable, Set\nfrom hashlib import sha256\nfrom functools import wraps\nimport random\nfrom tqdm import tqdm\nfrom random import shuffle\nimport pickle\n\nimport sys\nsys.path.append("".."")\nfrom pytorch_pretrained_bert.tokenization import BertTokenizer\nfrom dataset import TokenInstance, PretrainingDataCreator, GenericPretrainingDataCreator\n\n\nlogging.basicConfig(format=\'%(asctime)s - %(levelname)s - %(name)s -   %(message)s\',\n                    datefmt=\'%m/%d/%Y %H:%M:%S\',\n                    level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\ndef parse_data(input_file, output_file):\n    if not os.path.exists(output_file):\n        print(input_file)\n        dataset = GenericPretrainingDataCreator(\n            input_file, tokenizer, dupe_factor=9, max_seq_length=512)\n        dataset.save(output_file)\n        print(f""Completed Pickling: {output_file}"")\n    else:\n        print(f\'Already parsed: {output_file}\')\n\n\nparser = argparse.ArgumentParser(\n    description=""Give initial arguments for parsing"")\n\nparser.add_argument(""--input_dir"", type=str, help=""This folder contains .txt files of Wikipedia Data. Each .txt file contains the text from the documents. \\\n                                              It makes an assumption that each line in the file represents a single line in the document too.\\\n                                              A blank line represents completion of a document."")\nparser.add_argument(""--output_dir"", type=str, help=""Path to Output Directory."")\nparser.add_argument(""--token_file"", default=""bert-large-uncased"", type=str)\nparser.add_argument(""--do_lower_case"", default=False, action=""store_true"",\n                    help=""This flag indicates the wheter the text should be lowercase or not"")\nparser.add_argument(""--processes"", ""-p"", default=0, type=int,\n                    help=""This is to do parallel processing of the txt files. It should be >=0. Default: 0 represents that it will use all the available cores in the CPU."")\n\nargs = parser.parse_args()\ntokenizer = BertTokenizer.from_pretrained(\n    args.token_file, do_lower_case=args.do_lower_case)\n\ninput_files = []\noutput_files = []\nnum_processes = 1\n\nif args.processes < 0 or args.processes > multiprocessing.cpu_count():\n    raise ValueError(\n        ""The value of --processes should be >=0 and less than the max cores in the CPU."")\nelif args.processes == 0:  # Use all cores\n    num_processes = multiprocessing.cpu_count()\nelse:\n    num_processes = args.processes\n\nfor filename in os.listdir(args.input_dir):\n    input_file = os.path.join(args.input_dir, filename)\n    outfilename = ""_"".join(filename.split(\'.\')[:-1]) + "".bin""\n    output_file = os.path.join(args.output_dir, outfilename)\n    input_files.append(input_file)\n    output_files.append(output_file)\n    parse_data(input_file, output_file)\n\nwith Pool(processes=num_processes) as pool:\n    pool.starmap(parse_data, zip(input_files, output_files))\n'"
pretrain/PyTorch/dataprep/sentence_segmentation.py,0,"b'import nltk\nimport os\nfrom tqdm import tqdm\nimport sys\n\nnltk.download(\'punkt\')\n\ninput_file = sys.argv[1]\noutput_file = sys.argv[2]\n\ndoc_seperator = ""\\n""\n\nwith open(input_file) as ifile:\n    with open(output_file, ""w"") as ofile:\n        for i, line in tqdm(enumerate(ifile)):\n            if line != ""\\n"":\n                sent_list = nltk.tokenize.sent_tokenize(line)\n                for sent in sent_list:\n                    ofile.write(sent + ""\\n"")\n                ofile.write(doc_seperator)\n'"
pretrain/PyTorch/dataprep/single_line_doc_file_creation.py,0,"b'import glob\nimport os\nfrom tqdm import tqdm\n\noutput_file = \'wikipedia.txt\'\n\nwith open(output_file, ""w"") as ofile:\n  for dirname in glob.glob(\'out2/*/\', recursive=False):\n    for filename in glob.glob(dirname + \'wiki_*\', recursive=True):\n      print(filename)\n      article_lines = []\n      article_open = False\n      \n      with open(filename, ""r"") as file:\n        for i, line in tqdm(enumerate(file)):\n          if ""<doc id="" in line:\n            article_open = True\n          elif ""</doc>"" in line:\n            article_open = False\n            for oline in article_lines[1:]:\n              if oline != ""\\n"":\n                ofile.write(oline.rstrip() + "" "")\n            ofile.write(""\\n\\n"")\n            article_lines = []\n          else:\n            if article_open:\n              article_lines.append(line)\n'"
pretrain/PyTorch/dataprep/split_data_into_files.py,0,"b'import os\nfrom tqdm import tqdm\n\ninput_file = \'wikipedia.segmented.nltk.txt\'\noutput_file = \'./data_shards/wikipedia.segmented.part.\'\n\ndoc_seperator = ""\\n""\n\nline_buffer = []\ntotal_partitions = 100  # Mostly will create 1 extra partition\n# shard_size = 396000 # Approximate, will split at next article break\nline_counter = 0\nshard_index = 0\n\nifile_lines = 0\nwith open(input_file) as ifile:\n    for i, line in tqdm(enumerate(ifile)):\n        ifile_lines += 1\n\nprint(""Input file contains"", ifile_lines, ""lines."")\n\nshard_size = ifile_lines // total_partitions\n\niline_counter = 1\nwith open(input_file) as ifile:\n    for i, line in tqdm(enumerate(ifile)):\n        if line_counter < shard_size and iline_counter < ifile_lines:\n            line_buffer.append(line)\n            line_counter += 1\n            iline_counter += 1\n        elif line_counter >= shard_size and line != ""\\n"" and iline_counter < ifile_lines:\n            line_buffer.append(line)\n            line_counter += 1\n            iline_counter += 1\n        else:\n            with open(output_file + str(shard_index) + "".txt"", ""w"") as ofile:\n                for oline in line_buffer:\n                    ofile.write(oline)\n                line_buffer = []\n                line_counter = 0\n                shard_index += 1\n'"
