file_path,api_count,code
basic_install_test.py,1,"b'import torch\nimport importlib\n\ntry:\n    import deepspeed as ds\n    print(""deepspeed successfully imported"")\nexcept ImportError as err:\n    raise err\n\nprint(f""torch version: {torch.__version__}"")\n\nprint(f""deepspeed info: {ds.__version__}, {ds.__git_hash__}, {ds.__git_branch__}"")\n\ntry:\n    apex_C = importlib.import_module(\'apex_C\')\n    print(""apex successfully installed"")\nexcept Exception as err:\n    raise err\n\ntry:\n    fused_lamb = importlib.import_module(\'deepspeed_lamb_cuda\')\n    print(\'deepspeed fused lamb kernels successfully installed\')\nexcept Exception as err:\n    raise err\n\ntry:\n    from apex.optimizers import FP16_Optimizer\n    print(""using old-style apex"")\nexcept ImportError:\n    print(""using new-style apex"")\n\ntry:\n    ds_transformer = importlib.import_module(\'deepspeed_transformer_cuda\')\n    print(\'deepspeed transformer kernels successfully installed\')\nexcept Exception as err:\n    raise err\n'"
setup.py,4,"b'""""""\nCopyright 2020 The Microsoft DeepSpeed Team\n\nDeepSpeed library\n\nCreate a new wheel via the following command: python setup.py bdist_wheel\n\nThe wheel will be located at: dist/*.whl\n""""""\n\nimport os\nimport torch\nfrom setuptools import setup, find_packages\nfrom torch.utils.cpp_extension import CUDAExtension, BuildExtension\n\ncmdclass = {}\ncmdclass[\'build_ext\'] = BuildExtension\n\nTORCH_MAJOR = int(torch.__version__.split(\'.\')[0])\nTORCH_MINOR = int(torch.__version__.split(\'.\')[1])\n\nif not torch.cuda.is_available():\n    # Fix to allow docker buils, similar to https://github.com/NVIDIA/apex/issues/486\n    print(\n        ""[WARNING] Torch did not find cuda available, if cross-compling or running with cpu only ""\n        ""you can ignore this message. Adding compute capability for Pascal, Volta, and Turing ""\n        ""(compute capabilities 6.0, 6.1, 6.2)"")\n    if os.environ.get(""TORCH_CUDA_ARCH_LIST"", None) is None:\n        os.environ[""TORCH_CUDA_ARCH_LIST""] = ""6.0;6.1;6.2;7.0;7.5""\n\n# Fix from apex that might be relevant for us as well, related to https://github.com/NVIDIA/apex/issues/456\nversion_ge_1_1 = []\nif (TORCH_MAJOR > 1) or (TORCH_MAJOR == 1 and TORCH_MINOR > 0):\n    version_ge_1_1 = [\'-DVERSION_GE_1_1\']\nversion_ge_1_3 = []\nif (TORCH_MAJOR > 1) or (TORCH_MAJOR == 1 and TORCH_MINOR > 2):\n    version_ge_1_3 = [\'-DVERSION_GE_1_3\']\nversion_ge_1_5 = []\nif (TORCH_MAJOR > 1) or (TORCH_MAJOR == 1 and TORCH_MINOR > 4):\n    version_ge_1_5 = [\'-DVERSION_GE_1_5\']\nversion_dependent_macros = version_ge_1_1 + version_ge_1_3 + version_ge_1_5\n\next_modules = [\n    CUDAExtension(\n        name=\'deepspeed_lamb_cuda\',\n        sources=[\'csrc/lamb/fused_lamb_cuda.cpp\',\n                 \'csrc/lamb/fused_lamb_cuda_kernel.cu\'],\n        include_dirs=[\'csrc/includes\'],\n        extra_compile_args={\n            \'cxx\': [\n                \'-O3\',\n            ] + version_dependent_macros,\n            \'nvcc\': [\'-O3\',\n                     \'--use_fast_math\'] + version_dependent_macros\n        }),\n    CUDAExtension(name=\'deepspeed_transformer_cuda\',\n                  sources=[\n                      \'csrc/transformer/ds_transformer_cuda.cpp\',\n                      \'csrc/transformer/cublas_wrappers.cu\',\n                      \'csrc/transformer/transform_kernels.cu\',\n                      \'csrc/transformer/gelu_kernels.cu\',\n                      \'csrc/transformer/dropout_kernels.cu\',\n                      \'csrc/transformer/normalize_kernels.cu\',\n                      \'csrc/transformer/softmax_kernels.cu\',\n                      \'csrc/transformer/general_kernels.cu\'\n                  ],\n                  include_dirs=[\'csrc/includes\'],\n                  extra_compile_args={\n                      \'cxx\': [\'-O3\',\n                              \'-std=c++14\',\n                              \'-g\',\n                              \'-Wno-reorder\'],\n                      \'nvcc\': [\n                          \'-O3\',\n                          \'--use_fast_math\',\n                          \'-gencode\',\n                          \'arch=compute_61,code=compute_61\',\n                          \'-gencode\',\n                          \'arch=compute_70,code=compute_70\',\n                          \'-std=c++14\',\n                          \'-U__CUDA_NO_HALF_OPERATORS__\',\n                          \'-U__CUDA_NO_HALF_CONVERSIONS__\',\n                          \'-U__CUDA_NO_HALF2_OPERATORS__\'\n                      ]\n                  }),\n    CUDAExtension(name=\'deepspeed_stochastic_transformer_cuda\',\n                  sources=[\n                      \'csrc/transformer/ds_transformer_cuda.cpp\',\n                      \'csrc/transformer/cublas_wrappers.cu\',\n                      \'csrc/transformer/transform_kernels.cu\',\n                      \'csrc/transformer/gelu_kernels.cu\',\n                      \'csrc/transformer/dropout_kernels.cu\',\n                      \'csrc/transformer/normalize_kernels.cu\',\n                      \'csrc/transformer/softmax_kernels.cu\',\n                      \'csrc/transformer/general_kernels.cu\'\n                  ],\n                  include_dirs=[\'csrc/includes\'],\n                  extra_compile_args={\n                      \'cxx\': [\'-O3\',\n                              \'-std=c++14\',\n                              \'-g\',\n                              \'-Wno-reorder\'],\n                      \'nvcc\': [\n                          \'-O3\',\n                          \'--use_fast_math\',\n                          \'-gencode\',\n                          \'arch=compute_61,code=compute_61\',\n                          \'-gencode\',\n                          \'arch=compute_70,code=compute_70\',\n                          \'-std=c++14\',\n                          \'-U__CUDA_NO_HALF_OPERATORS__\',\n                          \'-U__CUDA_NO_HALF_CONVERSIONS__\',\n                          \'-U__CUDA_NO_HALF2_OPERATORS__\',\n                          \'-D__STOCHASTIC_MODE__\'\n                      ]\n                  }),\n]\n\nsetup(name=\'deepspeed\',\n      version=\'0.2.0\',\n      description=\'DeepSpeed library\',\n      author=\'DeepSpeed Team\',\n      author_email=\'deepspeed@microsoft.com\',\n      url=\'http://aka.ms/deepspeed\',\n      packages=find_packages(exclude=[""docker"",\n                                      ""third_party"",\n                                      ""csrc""]),\n      scripts=[\'bin/deepspeed\',\n               \'bin/deepspeed.pt\',\n               \'bin/ds\',\n               \'bin/ds_ssh\'],\n      classifiers=[\'Programming Language :: Python :: 3.6\'],\n      ext_modules=ext_modules,\n      cmdclass=cmdclass)\n'"
deepspeed/__init__.py,3,"b'\'\'\'\nCopyright 2020 The Microsoft DeepSpeed Team\n\'\'\'\n\nfrom deepspeed.pt.deepspeed_light import DeepSpeedLight\nfrom deepspeed.pt.deepspeed_light import ADAM_OPTIMIZER, LAMB_OPTIMIZER\nfrom deepspeed.pt.deepspeed_lr_schedules import add_tuning_arguments\nfrom deepspeed.pt.log_utils import logger\nfrom deepspeed.pt.deepspeed_cuda import DeepSpeedTransformerLayer, DeepSpeedTransformerConfig\nfrom deepspeed.pt.deepspeed_config import DeepSpeedConfig\n\nimport deepspeed.pt.deepspeed_checkpointing as checkpointing\n\ntry:\n    from deepspeed.git_version_info import git_hash, git_branch\nexcept ImportError:\n    git_hash = None\n    git_branch = None\n\n# Export version information\n__version_major__ = 0\n__version_minor__ = 2\n__version_patch__ = 0\n__version__ = \'.\'.join(\n    map(str,\n        [__version_major__,\n         __version_minor__,\n         __version_patch__]))\n__git_hash__ = git_hash\n__git_branch__ = git_branch\n\n\ndef initialize(args,\n               model,\n               optimizer=None,\n               model_parameters=None,\n               training_data=None,\n               lr_scheduler=None,\n               mpu=None,\n               dist_init_required=None,\n               collate_fn=None,\n               config_params=None):\n    """"""Initialize the DeepSpeed Engine.\n\n    Arguments:\n        args: a dictionary containing local_rank and deepspeed_config\n            file location\n\n        model: Required: nn.module class before apply any wrappers\n\n        optimizer: Optional: a user defined optimizer, this is typically used instead of defining\n            an optimizer in the DeepSpeed json config.\n\n        model_parameters: Optional: An iterable of torch.Tensors or dicts.\n            Specifies what Tensors should be optimized.\n\n        training_data: Optional: Dataset of type torch.utils.data.Dataset\n\n        lr_scheduler: Optional: Learning Rate Scheduler Object. It should define a get_lr(),\n            step(), state_dict(), and load_state_dict() methods\n\n        mpu: Optional: A model parallelism unit object that implements\n            get_{model,data}_parallel_{rank,group,world_size}()\n\n        dist_init_required: Optional: None will auto-initialize torch.distributed if needed,\n            otherwise the user can force it to be initialized or not via boolean.\n\n        collate_fn: Optional: Merges a list of samples to form a\n            mini-batch of Tensor(s).  Used when using batched loading from a\n            map-style dataset.\n\n    Returns:\n        A tuple of ``engine``, ``optimizer``, ``training_dataloader``, ``lr_scheduler``\n\n        * ``engine``: DeepSpeed runtime engine which wraps the client model for distributed training.\n\n        * ``optimizer``: Wrapped optimizer if a user defined ``optimizer`` is supplied, or if\n          optimizer is specified in json config else ``None``.\n\n        * ``training_dataloader``: DeepSpeed dataloader if ``training_data`` was supplied,\n          otherwise ``None``.\n\n        * ``lr_scheduler``: Wrapped lr scheduler if user ``lr_scheduler`` is passed, or\n          if ``lr_scheduler`` specified in JSON configuration. Otherwise ``None``.\n    """"""\n    logger.info(\n        ""DeepSpeed info: version={}, git-hash={}, git-branch={}"".format(\n            __version__,\n            __git_hash__,\n            __git_branch__),\n    )\n\n    engine = DeepSpeedLight(args=args,\n                            model=model,\n                            optimizer=optimizer,\n                            model_parameters=model_parameters,\n                            training_data=training_data,\n                            lr_scheduler=lr_scheduler,\n                            mpu=mpu,\n                            dist_init_required=dist_init_required,\n                            collate_fn=collate_fn,\n                            config_params=config_params)\n\n    return_items = [\n        engine,\n        engine.optimizer,\n        engine.training_dataloader,\n        engine.lr_scheduler\n    ]\n    return tuple(return_items)\n\n\ndef _add_core_arguments(parser):\n    r""""""Helper (internal) function to update an argument parser with an argument group of the core DeepSpeed arguments.\n        The core set of DeepSpeed arguments include the following:\n        1) --deepspeed: boolean flag to enable DeepSpeed\n        2) --deepspeed_config <json file path>: path of a json configuration file to configure DeepSpeed runtime.\n\n        This is a helper function to the public add_config_arguments()\n\n    Arguments:\n        parser: argument parser\n    Return:\n        parser: Updated Parser\n    """"""\n    group = parser.add_argument_group(\'DeepSpeed\', \'DeepSpeed configurations\')\n\n    group.add_argument(\n        \'--deepspeed\',\n        default=False,\n        action=\'store_true\',\n        help=\n        \'Enable DeepSpeed (helper flag for user code, no impact on DeepSpeed backend)\')\n\n    group.add_argument(\'--deepspeed_config\',\n                       default=None,\n                       type=str,\n                       help=\'DeepSpeed json configuration file.\')\n\n    group.add_argument(\n        \'--deepscale\',\n        default=False,\n        action=\'store_true\',\n        help=\n        \'Deprecated enable DeepSpeed (helper flag for user code, no impact on DeepSpeed backend)\'\n    )\n\n    group.add_argument(\'--deepscale_config\',\n                       default=None,\n                       type=str,\n                       help=\'Deprecated DeepSpeed json configuration file.\')\n\n    group.add_argument(\n        \'--deepspeed_mpi\',\n        default=False,\n        action=\'store_true\',\n        help=\n        ""Run via MPI, this will attempt to discover the necessary variables to initialize torch ""\n        ""distributed from the MPI environment"")\n\n    return parser\n\n\ndef add_config_arguments(parser):\n    r""""""Update the argument parser to enabling parsing of DeepSpeed command line arguments.\n        The set of DeepSpeed arguments include the following:\n        1) --deepspeed: boolean flag to enable DeepSpeed\n        2) --deepspeed_config <json file path>: path of a json configuration file to configure DeepSpeed runtime.\n\n    Arguments:\n        parser: argument parser\n    Return:\n        parser: Updated Parser\n    """"""\n    parser = _add_core_arguments(parser)\n\n    return parser\n'"
deepspeed/pt/__init__.py,0,b''
deepspeed/pt/deepspeed_checkpointing.py,58,"b'\'\'\'\r\nCopyright (c) Microsoft Corporation\r\nLicensed under the MIT license.\r\n\r\nUse to partition the activations stored for backward propagation\r\nTherefore reduces the memory consumption\r\nAlso implements CPU checkpointing and contiguous memory checkpointing\r\nReduces memory consumption and memory fragmentation\r\n\r\nCode for rng checkpointing taken from NVIDIA Megatron-LM mpu/random.py\r\nb886b7bb972afe72bac0f5de4f42a4a7bae8ebef\r\n\'\'\'\r\n\r\n# Parts of the code here are adapted from PyTorch\r\n# repo: https://github.com/pytorch/pytorch\r\nimport contextlib\r\nimport copy\r\nimport torch.distributed as dist\r\nimport torch\r\nfrom torch import _C\r\nfrom torch.cuda import _lazy_call, device as device_ctx_manager\r\nfrom deepspeed.pt.deepspeed_timer import SynchronizedWallClockTimer as Timers\r\nimport torch.distributed as dist\r\nfrom deepspeed.pt.deepspeed_config import DeepSpeedConfig\r\nfrom deepspeed.pt.log_utils import logger\r\n\r\n#DeepSpeed Checkpointing Enabled or Disabled\r\ndeepspeed_checkpointing_enabled = False\r\n\r\n#MP parameters\r\nmpu = None\r\nmp_rank = None\r\nmp_size = None\r\nmp_group = None\r\n\r\n#Model Parameters\r\nnum_layers = None\r\n\r\n#Checkpointing buffers\r\ncontiguous_data_buffers = []\r\ndata_offsets = []\r\n\r\ncontiguous_size_buffers = []\r\nsize_offsets = []\r\n\r\ntimers = None\r\n\r\n#optimization flags\r\nPARTITION_ACTIVATIONS = False\r\nPA_TO_CPU = False\r\nCONTIGUOUS_CHECKPOINTING = False\r\nSYNCHRONIZE = False\r\nPROFILE_TIME = False\r\n\r\n\r\ndef see_memory_usage(message, force=False):\r\n    #return\r\n    if not force:\r\n        return\r\n    #dist.barrier()\r\n    if dist.get_rank() == 0:\r\n        logger.info(message)\r\n        logger.info(\r\n            ""Memory Allocated %s GigaBytes"",\r\n            torch.cuda.memory_allocated() / (1024 * 1024 * 1024),\r\n        )\r\n        logger.info(\r\n            ""Max Memory Allocated %s GigaBytes"",\r\n            torch.cuda.max_memory_allocated() / (1024 * 1024 * 1024),\r\n        )\r\n        logger.info(\r\n            ""Cache Allocated %s GigaBytes"",\r\n            torch.cuda.memory_cached() / (1024 * 1024 * 1024),\r\n        )\r\n        logger.info(\r\n            ""Max cache Allocated %s GigaBytes"",\r\n            torch.cuda.max_memory_cached() / (1024 * 1024 * 1024),\r\n        )\r\n        #input(""Press Any Key To Continue .."")\r\n\r\n\r\n# Default name for the model parallel rng tracker.\r\n_MODEL_PARALLEL_RNG_TRACKER_NAME = \'model-parallel-rng\'\r\ntransport_stream = None\r\ncuda_device = None\r\n\r\n\r\ndef detach_variable(inputs, device=None):\r\n    if isinstance(inputs, tuple):\r\n        out = []\r\n        for inp in inputs:\r\n            if not isinstance(inp, torch.Tensor):\r\n                out.append(inp)\r\n                continue\r\n\r\n            requires_grad = inp.requires_grad\r\n\r\n            if device is not None:\r\n                x = inp.to(device=device)\r\n            else:\r\n                x = inp\r\n\r\n            x = x.detach()\r\n            x.requires_grad = requires_grad\r\n            out.append(x)\r\n        return tuple(out)\r\n    else:\r\n        raise RuntimeError(\r\n            ""Only tuple of tensors is supported. Got Unsupported input type: "",\r\n            type(inputs).__name__)\r\n\r\n\r\ndef _set_cuda_rng_state(new_state, device=-1):\r\n    """"""Sets the random number generator state of the current GPU.\r\n\r\n    Arguments:\r\n        new_state (torch.ByteTensor): The desired state\r\n    This function is adapted from PyTorch repo (torch.cuda.set_rng_state)\r\n    with a single change: the input state is not cloned. Cloning caused\r\n    major performance issues for +4 GPU cases.\r\n    """"""\r\n    if hasattr(_C, \'_cuda_setRNGState\') and callable(_C._cuda_setRNGState):\r\n        # older PyTorch\r\n        def cb():\r\n            with device_ctx_manager(device):\r\n                _C._cuda_setRNGState(new_state)\r\n    else:\r\n        # newer PyTorch\r\n        if device == -1:\r\n            device = torch.device(\'cuda\')\r\n        elif isinstance(device, str):\r\n            device = torch.device(device)\r\n        elif isinstance(device, int):\r\n            device = torch.device(\'cuda\', device)\r\n\r\n        def cb():\r\n            idx = device.index\r\n            if idx is None:\r\n                idx = torch.cuda.current_device()\r\n            default_generator = torch.cuda.default_generators[idx]\r\n            default_generator.set_state(new_state)\r\n\r\n    _lazy_call(cb)\r\n\r\n\r\nclass CudaRNGStatesTracker:\r\n    """"""Tracker for the cuda RNG states.\r\n\r\n    Using the `add` method, a cuda rng state is initialized based on\r\n    the input `seed` and is assigned to `name`. Later, by forking the\r\n    rng state, we can perform operations and return to our starting\r\n    cuda state.\r\n    """"""\r\n    def __init__(self):\r\n        # Map from a string name to the cuda rng state.\r\n        self.states_ = {}\r\n        # Seeds are just for book keeping and ensure no seed is set twice.\r\n        self.seeds_ = set()\r\n\r\n    def reset(self):\r\n        """"""Set to the initial state (no tracker).""""""\r\n        self.states_ = {}\r\n        self.seeds_ = set()\r\n\r\n    def get_states(self):\r\n        """"""Get rng states. Copy the dictionary so we have direct\r\n        pointers to the states, not just a pointer to the dictionary.""""""\r\n        return copy.copy(self.states_)\r\n\r\n    def set_states(self, states):\r\n        """"""Set the rng states. For efficiency purposes, we do not check\r\n        the size of seed for compatibility.""""""\r\n        self.states_ = states\r\n\r\n    def add(self, name, seed):\r\n        """"""Track the rng state.""""""\r\n        # Check seed is not already used.\r\n        if seed in self.seeds_:\r\n            raise Exception(\'seed {} already exists\'.format(seed))\r\n        self.seeds_.add(seed)\r\n        # Check that state is not already defined.\r\n        if name in self.states_:\r\n            raise Exception(\'cuda rng state {} already exists\'.format(name))\r\n        # Get the current rng state.\r\n        orig_rng_state = torch.cuda.get_rng_state()\r\n        # Set the new state and store it.\r\n        torch.cuda.manual_seed(seed)\r\n        self.states_[name] = torch.cuda.get_rng_state()\r\n        # Reset rng state to what it was.\r\n        _set_cuda_rng_state(orig_rng_state)\r\n\r\n    @contextlib.contextmanager\r\n    def fork(self, name=_MODEL_PARALLEL_RNG_TRACKER_NAME):\r\n        """"""Fork the cuda rng state, perform operations, and exit with\r\n        the original state.""""""\r\n        # Check if we have added the state\r\n        if name not in self.states_:\r\n            raise Exception(\'cuda rng state {} is not added\'.format(name))\r\n        # Store current rng state.\r\n        orig_cuda_rng_state = torch.cuda.get_rng_state()\r\n        # Set rng state to the desired one\r\n        _set_cuda_rng_state(self.states_[name])\r\n        # Do the stuff we wanted to do.\r\n        try:\r\n            yield\r\n        finally:\r\n            # Update the current rng state for later use.\r\n            self.states_[name] = torch.cuda.get_rng_state()\r\n            # And set the state to the original state we started with.\r\n            _set_cuda_rng_state(orig_cuda_rng_state)\r\n\r\n\r\n# RNG tracker object.\r\n_CUDA_RNG_STATE_TRACKER = CudaRNGStatesTracker()\r\n\r\n\r\ndef get_cuda_rng_tracker():\r\n    """"""Get cuda rng tracker.""""""\r\n    return _CUDA_RNG_STATE_TRACKER\r\n\r\n\r\ndef model_parallel_cuda_manual_seed(seed):\r\n    """"""Initialize model parallel cuda seed.\r\n\r\n    This function should be called after the model parallel is\r\n    initialized. Also, no torch.cuda.manual_seed should be called\r\n    after this function. Basically, this is replacement for that\r\n    function.\r\n    Two set of RNG states are tracked:\r\n        default state: This is for data parallelism and is the same among a\r\n                       set of model parallel GPUs but different across\r\n                       different model paralle groups. This is used for\r\n                       example for dropout in the non-model-parallel regions.\r\n        model-parallel state: This state is different among a set of model\r\n                              parallel GPUs, but the same across data parallel\r\n                              groups. This is used for example for dropout in\r\n                              model parallel regions.\r\n    """"""\r\n    global mpu\r\n    # 2718 is just for fun and any POSITIVE value will work.\r\n    offset = seed + 2718\r\n    model_parallel_seed = offset + mpu.get_model_parallel_rank()\r\n    # Data parallel gets the original sedd.\r\n    data_parallel_seed = seed\r\n\r\n    if torch.distributed.get_rank() == 0:\r\n        logger.info(\r\n            \'> initializing model parallel cuda seeds on global rank {}, \'\r\n            \'model parallel rank {}, and data parallel rank {} with \'\r\n            \'model parallel seed: {} and data parallel seed: {}\'.format(\r\n                torch.distributed.get_rank(),\r\n                mpu.get_model_parallel_rank(),\r\n                mpu.get_data_parallel_rank(),\r\n                model_parallel_seed,\r\n                data_parallel_seed),\r\n        )\r\n    _CUDA_RNG_STATE_TRACKER.reset()\r\n    # Set the default state.\r\n    torch.cuda.manual_seed(data_parallel_seed)\r\n    # and model parallel state.\r\n    _CUDA_RNG_STATE_TRACKER.add(_MODEL_PARALLEL_RNG_TRACKER_NAME, model_parallel_seed)\r\n\r\n\r\ndef get_partition_start(item):\r\n    global mp_rank, mp_size, mp_group\r\n    size = item.numel()\r\n    partition_size = size / mp_size\r\n    start = partition_size * mp_rank\r\n    return int(start)\r\n\r\n\r\ndef get_partition_size(item):\r\n    global mp_rank, mp_size, mp_group\r\n    size = item.numel()\r\n    assert size % mp_size == 0, ""Doesn\'t handle if partition activation if item is not divisible by mp size""\r\n    partition_size = size / mp_size\r\n    return int(partition_size)\r\n\r\n\r\ndef get_full_inputs(tensors, device=None):\r\n    inputs = []\r\n    num_args = int(len(tensors) / 2)\r\n    for i in range(num_args - 1):\r\n\r\n        item = tensors[2 * i]\r\n        size = tensors[2 * i + 1]\r\n\r\n        partition_size = item.numel()\r\n        tensor_size = partition_size * mp_size\r\n        if device is not None:\r\n            flat_tensor = torch.zeros([tensor_size], dtype=item.dtype, device=device)\r\n        else:\r\n            flat_tensor = torch.zeros([tensor_size],\r\n                                      dtype=item.dtype,\r\n                                      device=item.device)\r\n        partitions = []\r\n        for i in range(mp_size):\r\n            part_i = flat_tensor.narrow(0, partition_size * i, partition_size)\r\n            if i == mp_rank:\r\n                part_i.copy_(item)\r\n            partitions.append(part_i)\r\n        if mp_group is not None:\r\n            dist.all_gather(partitions, partitions[mp_rank], group=mp_group)\r\n        input_tensor = flat_tensor.view(list(size.numpy()))\r\n        item.data = input_tensor.data\r\n\r\n        inputs.append(item)\r\n    inputs.append(tensors[-2])\r\n\r\n    return tuple(inputs)\r\n\r\n\r\nclass CheckpointFunction(torch.autograd.Function):\r\n    """"""This function is adapted from torch.utils.checkpoint with\r\n       two main changes:\r\n           1) torch.cuda.set_rng_state is replaced with `_set_cuda_rng_state`\r\n           2) the states in the model parallel tracker are also properly\r\n              tracked/set/reset.\r\n           3) Performance activation partitioning, contiguous memory optimization\r\n           4) CPU Checkpointing\r\n           5) Profile forward and backward functions\r\n    """"""\r\n    @staticmethod\r\n    def forward(ctx, run_function, *args):\r\n        global mpu, timers, SYNCHRONIZE, PROFILE_TIME\r\n\r\n        if SYNCHRONIZE:\r\n            torch.cuda.synchronize()\r\n\r\n        if timers is None and PROFILE_TIME:\r\n            timers = Timers()\r\n\r\n        if PROFILE_TIME:\r\n            timers(\'forward\').start()\r\n\r\n        ctx.run_function = run_function\r\n        global num_layers\r\n        global mp_rank, mp_size, mp_group\r\n        global contiguous_data_buffers, contiguous_size_buffers\r\n        global data_offsets, size_offsets\r\n        if mp_rank is None:\r\n            if mpu is not None:\r\n                mp_rank = mpu.get_model_parallel_rank()\r\n                mp_size = mpu.get_model_parallel_world_size()\r\n                mp_group = mpu.get_model_parallel_group()\r\n            else:\r\n                mp_rank = 0\r\n                mp_size = 1\r\n                mp_group = None\r\n\r\n        global cuda_device, transport_stream, PARTITION_ACTIVATIONS, buffer_0, buffer_1, buffer_0_offset, buffer_1_offset\r\n\r\n        if cuda_device is None:\r\n            see_memory_usage(""First Forward Begining"", force=True)\r\n            if dist.get_rank() == 0:\r\n                logger.info(f""Activation Checkpointing Information"")\r\n                logger.info(\r\n                    f""----Partition Activations {PARTITION_ACTIVATIONS}, CPU CHECKPOINTING {PA_TO_CPU}""\r\n                )\r\n                logger.info(\r\n                    f""----contiguous Memory Checkpointing {CONTIGUOUS_CHECKPOINTING} with {num_layers} total layers""\r\n                )\r\n                logger.info(f""----Synchronization {SYNCHRONIZE}"")\r\n                logger.info(f""----Profiling {PROFILE_TIME}"")\r\n\r\n            cuda_device = torch.cuda.current_device()\r\n            transport_stream = torch.cuda.Stream(device=cuda_device)\r\n\r\n        if PARTITION_ACTIVATIONS:\r\n            #inputs = [item.detach().contiguous().view(-1).narrow(0, get_partition_start(item), get_partition_size(item)).clone() for item in args[:-1]]\r\n            #inputs.append(args[-1])\r\n\r\n            inputs = []\r\n            for i, item in enumerate(args[:-1]):\r\n                partition_size = get_partition_size(item)\r\n                partition = item.detach().contiguous().view(-1).narrow(\r\n                    0,\r\n                    get_partition_start(item),\r\n                    partition_size).clone()\r\n\r\n                if CONTIGUOUS_CHECKPOINTING:\r\n                    buffer_device = torch.device(\r\n                        \'cpu\') if PA_TO_CPU else partition.device\r\n\r\n                    if i >= len(contiguous_data_buffers):\r\n                        tensor_list = [\r\n                            torch.tensor(()).new_empty([partition_size],\r\n                                                       dtype=partition.dtype,\r\n                                                       device=buffer_device)\r\n                            for i in range(num_layers)\r\n                        ]\r\n                        contiguous_data_buffers.append(tensor_list)\r\n                        data_offsets.append(0)\r\n                    elif contiguous_data_buffers[i] is None:\r\n                        tensor_list = [\r\n                            torch.tensor(()).new_empty([partition_size],\r\n                                                       dtype=partition.dtype,\r\n                                                       device=buffer_device)\r\n                            for i in range(num_layers)\r\n                        ]\r\n                        contiguous_data_buffers[i] = tensor_list\r\n                        data_offsets[i] = 0\r\n\r\n                    contiguous_partition = contiguous_data_buffers[i][\r\n                        data_offsets[i]].data.copy_(partition.data)\r\n                    data_offsets[i] = data_offsets[i] + 1\r\n                    inputs.append(contiguous_partition)\r\n                else:\r\n                    partition = partition.cpu() if PA_TO_CPU else partition\r\n                    inputs.append(partition)\r\n\r\n            inputs.append(args[-1])\r\n\r\n        #just in case something funky is happening such as reuse of inputs\r\n        inputs_cuda = [item.to(cuda_device) for item in args]\r\n\r\n        # Copy the rng states.\r\n        ctx.fwd_cpu_rng_state = torch.get_rng_state()\r\n        ctx.fwd_cuda_rng_state = torch.cuda.get_rng_state()\r\n        ctx.fwd_cuda_rng_state_tracker = get_cuda_rng_tracker().get_states()\r\n\r\n        #ctx.save_for_backward(*args)\r\n        with torch.no_grad():\r\n            outputs = run_function(*inputs_cuda)\r\n\r\n        del inputs_cuda\r\n\r\n        #with torch.cuda.stream(transport_stream):\r\n        #if PARTITION_ACTIVATIONS:\r\n        #    new_args = []\r\n        #    for arg, inp in zip(args,inputs):\r\n        #        size= torch.tensor(arg.size())\r\n        #        arg.data = inp.data\r\n        #        new_args.append(arg)\r\n        #        new_args.append(size)\r\n        #    ctx.save_for_backward(*new_args)\r\n\r\n        if PARTITION_ACTIVATIONS:\r\n            new_args = []\r\n            for i, (arg, inp) in enumerate(zip(args, inputs)):\r\n                size = torch.tensor(arg.size())\r\n\r\n                arg.data = inp.data\r\n                new_args.append(arg)\r\n\r\n                if CONTIGUOUS_CHECKPOINTING:\r\n                    numel = size.numel()\r\n                    if i >= len(contiguous_size_buffers):\r\n                        tmp = torch.tensor(())\r\n                        contiguous_size_buffers.append(\r\n                            tmp.new_empty([numel * num_layers],\r\n                                          dtype=size.dtype,\r\n                                          device=size.device))\r\n                        size_offsets.append(0)\r\n                    elif contiguous_size_buffers[i] is None:\r\n                        tmp = torch.tensor(())\r\n                        contiguous_size_buffers[i] = tmp.new_empty([numel * num_layers],\r\n                                                                   dtype=size.dtype,\r\n                                                                   device=size.device)\r\n                        size_offsets[i] = 0\r\n\r\n                    contiguous_size = contiguous_size_buffers[i].narrow(\r\n                        0,\r\n                        size_offsets[i],\r\n                        numel).data.copy_(size.data)\r\n                    contiguous_size = contiguous_size.view_as(size)\r\n                    size_offsets[i] = size_offsets[i] + numel\r\n                    new_args.append(contiguous_size)\r\n                else:\r\n                    new_args.append(size)\r\n                #if dist.get_rank() == 0:\r\n                #    logger.info(f""The stored tensor is {contiguous_size} and orginal one is {size} "")\r\n\r\n            ctx.save_for_backward(*new_args)\r\n        else:\r\n            ctx.save_for_backward(*args)\r\n        if PROFILE_TIME:\r\n            timers(\'forward\').stop()\r\n            timers.log([\'forward\'])\r\n        if SYNCHRONIZE:\r\n            torch.cuda.synchronize()\r\n        return outputs\r\n\r\n    @staticmethod\r\n    def backward(ctx, *args):\r\n        global timers\r\n        #see_memory_usage(""In backward"", force=True)\r\n        #removing pointers to the contiguous buffer memory\r\n        #so that they can be garbage collected once the checkpoints\r\n        #have been used\r\n        if SYNCHRONIZE:\r\n            torch.cuda.synchronize()\r\n        if PROFILE_TIME:\r\n            timers(\'backward\').start()\r\n\r\n        if CONTIGUOUS_CHECKPOINTING:\r\n            global data_offsets, size_offsets\r\n            global contiguous_data_buffers, contiguous_size_buffers\r\n\r\n            for buffers in contiguous_data_buffers:\r\n                buffers = []\r\n\r\n            #frees up all the pointers to the checkpoints except for the ones\r\n            #stored by save for backward\r\n            contiguous_data_buffers = []\r\n            contiguous_size_buffers = []\r\n            data_offsets = []\r\n            size_offsets = []\r\n\r\n        #see_memory_usage(""In backward checkpointing code"", force=True)\r\n        if not torch.autograd._is_checkpoint_valid():\r\n            raise RuntimeError(""Checkpointing is not compatible with .grad(), ""\r\n                               ""please use .backward() if possible"")\r\n\r\n        global cuda_device, transport_stream, PARTITION_ACTIVATIONS\r\n\r\n        if PARTITION_ACTIVATIONS:\r\n            #with torch.cuda.stream(transport_stream):\r\n            inputs = get_full_inputs(ctx.saved_tensors,\r\n                                     device=cuda_device if PA_TO_CPU else None)\r\n            detached_inputs = detach_variable(inputs)\r\n        else:\r\n            inputs = ctx.saved_tensors\r\n            detached_inputs = detach_variable(inputs)\r\n\r\n        # Store the current states.\r\n        bwd_cpu_rng_state = torch.get_rng_state()\r\n        bwd_cuda_rng_state = torch.cuda.get_rng_state()\r\n        bwd_cuda_rng_state_tracker = get_cuda_rng_tracker().get_states()\r\n\r\n        # Set the states to what it used to be before the forward pass.\r\n        torch.set_rng_state(ctx.fwd_cpu_rng_state)\r\n        _set_cuda_rng_state(ctx.fwd_cuda_rng_state)\r\n        get_cuda_rng_tracker().set_states(ctx.fwd_cuda_rng_state_tracker)\r\n\r\n        # if PARTITION_ACTIVATIONS:\r\n        #     current_stream=torch.cuda.current_stream()\r\n        #     current_stream.wait_stream(transport_stream)\r\n\r\n        with torch.enable_grad():\r\n            outputs = ctx.run_function(*detached_inputs)\r\n\r\n        # Set the states back to what it was at the start of this function.\r\n        torch.set_rng_state(bwd_cpu_rng_state)\r\n        _set_cuda_rng_state(bwd_cuda_rng_state)\r\n        get_cuda_rng_tracker().set_states(bwd_cuda_rng_state_tracker)\r\n\r\n        if isinstance(outputs, torch.Tensor):\r\n            outputs = (outputs, )\r\n        torch.autograd.backward(outputs, args)\r\n\r\n        if PROFILE_TIME:\r\n            timers(\'backward\').stop()\r\n            timers.log([\'backward\'])\r\n        if SYNCHRONIZE:\r\n            torch.cuda.synchronize()\r\n        return (None, ) + tuple(inp.grad for inp in detached_inputs)\r\n\r\n\r\ndef checkpoint(function, *args):\r\n    """"""Checkpoint a model or part of the model.\r\n    This has been directly copied from torch.utils.checkpoint. """"""\r\n    return CheckpointFunction.apply(function, *args)\r\n\r\n\r\ndef partition_activations_in_checkpoint(partition_activation):\r\n    global PARTITION_ACTIVATIONS\r\n    PARTITION_ACTIVATIONS = partition_activation\r\n    if dist.get_rank() == 0:\r\n        logger.info(\r\n            f""**************Partition Activations {PARTITION_ACTIVATIONS}************"")\r\n\r\n\r\ndef set_num_layers(nlayers):\r\n    global num_layers\r\n    num_layers = nlayers\r\n\r\n\r\ndef reset():\r\n    """"""Resets memory buffers related to contiguous memory optimizations.\r\n    Should be called during eval when multiple forward propagations are\r\n    computed without any backward propagation that usually clears these\r\n    buffers.\r\n    Arguments:\r\n        None\r\n\r\n    Return:\r\n        None\r\n    """"""\r\n    if CONTIGUOUS_CHECKPOINTING:\r\n        global data_offsets, size_offsets\r\n        global contiguous_data_buffers, contiguous_size_buffers\r\n\r\n        for buffers in contiguous_data_buffers:\r\n            buffers = []\r\n\r\n        #frees up all the pointers to the checkpoints except for the ones\r\n        #stored by save for backward\r\n        contiguous_data_buffers = []\r\n        contiguous_size_buffers = []\r\n        data_offsets = []\r\n        size_offsets = []\r\n\r\n\r\ndef _configure_using_config_file(deepspeed_config):\r\n    global num_layers, PARTITION_ACTIVATIONS, CONTIGUOUS_CHECKPOINTING, \\\r\n            PA_TO_CPU, SYNCHRONIZE, PROFILE_TIME\r\n\r\n    config = DeepSpeedConfig(deepspeed_config).activation_checkpointing_config\r\n    logger.info(config.repr())\r\n    PARTITION_ACTIVATIONS = config.partition_activations\r\n    CONTIGUOUS_CHECKPOINTING = config.contiguous_memory_optimization\r\n    num_layers = config.number_checkpoints\r\n    PA_TO_CPU = config.cpu_checkpointing\r\n    SYNCHRONIZE = config.synchronize_checkpoint_boundary\r\n    PROFILE_TIME = config.profile\r\n\r\n\r\ndef _configure_defaults():\r\n\r\n    global mpu, num_layers, deepspeed_checkpointing_enabled\r\n\r\n    global PARTITION_ACTIVATIONS, CONTIGUOUS_CHECKPOINTING, \\\r\n            PA_TO_CPU, SYNCHRONIZE, PROFILE_TIME\r\n\r\n    PARTITION_ACTIVATIONS = False\r\n    CONTIGUOUS_CHECKPOINTING = False\r\n    num_layers = False\r\n    PA_TO_CPU = False\r\n    SYNCHRONIZE = False\r\n    PROFILE_TIME = False\r\n    deepspeed_checkpointing_enabled = True\r\n\r\n\r\ndef configure(\r\n    mpu_,\r\n    deepspeed_config=None,\r\n    partition_activations=None,\r\n    contiguous_checkpointing=None,\r\n    num_checkpoints=None,\r\n    checkpoint_in_cpu=None,\r\n    synchronize=None,\r\n    profile=None,\r\n):\r\n    """"""Configure DeepSpeed Activation Checkpointing.\r\n\r\n    Arguments:\r\n        mpu_: Optional: An object that implements the following methods\r\n            get_model_parallel_rank/group/world_size, and get_data_parallel_rank/group/world_size\r\n\r\n        deepspeed_config: Optional: DeepSpeed Config json file when provided will be used to\r\n            configure DeepSpeed Activation Checkpointing\r\n\r\n        partition_activations: Optional: Partitions activation checkpoint across model parallel\r\n            GPUs when enabled. By default False. Will overwrite deepspeed_config if provided\r\n\r\n        contiguous_checkpointing: Optional: Copies activation checkpoints to a contiguous memory\r\n            buffer. Works only with homogeneous checkpoints when partition_activations is enabled.\r\n            Must provide num_checkpoints. By default False. Will overwrite deepspeed_config if\r\n            provided\r\n\r\n        num_checkpoints: Optional: Number of activation checkpoints stored during the forward\r\n            propagation of the model. Used to calculate the buffer size for contiguous_checkpointing\r\n            Will overwrite deepspeed_config if provided\r\n\r\n        checkpoint_in_cpu: Optional: Moves the activation checkpoint to CPU. Only works with\r\n            partition_activation. Default is false. Will overwrite deepspeed_config if provided\r\n\r\n        synchronize: Optional: Performs torch.cuda.synchronize() at the beginning and end of\r\n            each call to deepspeed.checkpointing.checkpoint for both forward and backward pass.\r\n            By default false. Will overwrite deepspeed_config if provided\r\n\r\n        profile: Optional: Logs the forward and backward time for each\r\n            deepspeed.checkpointing.checkpoint invocation. Will overwrite deepspeed_config\r\n            if provided\r\n\r\n    Returns:\r\n        None\r\n    """"""\r\n    global mpu, num_layers, deepspeed_checkpointing_enabled\r\n\r\n    global PARTITION_ACTIVATIONS, CONTIGUOUS_CHECKPOINTING, \\\r\n            PA_TO_CPU, SYNCHRONIZE, PROFILE_TIME\r\n\r\n    _configure_defaults()\r\n\r\n    if deepspeed_config is not None:\r\n        _configure_using_config_file(deepspeed_config)\r\n\r\n    if mpu_ is not None:\r\n        mpu = mpu_\r\n\r\n    if partition_activations is not None:\r\n        PARTITION_ACTIVATIONS = partition_activations\r\n\r\n    if contiguous_checkpointing is not None:\r\n        CONTIGUOUS_CHECKPOINTING = contiguous_checkpointing\r\n\r\n    if num_checkpoints is not None:\r\n        num_layers = num_checkpoints\r\n\r\n    if checkpoint_in_cpu is not None:\r\n        PA_TO_CPU = checkpoint_in_cpu\r\n\r\n    if synchronize is not None:\r\n        SYNCHRONIZE = synchronize\r\n\r\n    if profile is not None:\r\n        PROFILE_TIME = profile\r\n\r\n    if PA_TO_CPU or CONTIGUOUS_CHECKPOINTING:\r\n        assert PARTITION_ACTIVATIONS, ""CPU Checkpointing/Contiguous Checkpointing is only availble with partitioned activations. Set partitioned activations to true in deepspeed config""\r\n    if CONTIGUOUS_CHECKPOINTING:\r\n        assert num_layers is not None, ""Must specify the number of layers with contiguous memory checkpointing""\r\n\r\n\r\ndef is_configured():\r\n    """"""True if deepspeed activation checkpointing has been configured\r\n        by calling deepspeed.checkpointing.configure, else returns false\r\n\r\n    Arguments:\r\n        None\r\n\r\n    Return:\r\n        True of configured, else False\r\n    """"""\r\n    return deepspeed_checkpointing_enabled\r\n'"
deepspeed/pt/deepspeed_checkpointing_config.py,0,"b'""""""\r\nCopyright (c) Microsoft Corporation\r\nLicensed under the MIT license.\r\n""""""\r\n\r\nfrom deepspeed.pt.deepspeed_config_utils import get_scalar_param\r\n\r\n#########################################\r\n#  DeepSpeed Activation Checkpointing\r\n#########################################\r\n# Activation Checkpointing Allows to save memory by only keeping a select few\r\n#activations for the backpropagation.\r\nACTIVATION_CHKPT_FORMAT = \'\'\'\r\nActivation Checkpointing should be configured as:\r\n""session_params"": {\r\n  ""activation_checkpointing"": {\r\n    ""partitioned_activations"": [true|false],\r\n    ""number_checkpoints"": 100,\r\n    ""contiguous_memory_optimization"": [true|false],\r\n    ""cpu_checkpointing"": [true|false]\r\n    ""profile"": [true|false],\r\n    ""synchronize_checkpoint_boundary"": [true|false],\r\n    }\r\n}\r\n\'\'\'\r\n\r\nACT_CHKPT_PARTITION_ACTIVATIONS = \'partition_activations\'\r\nACT_CHKPT_PARTITION_ACTIVATIONS_DEFAULT = False\r\n\r\nACT_CHKPT_NUMBER_CHECKPOINTS = \'number_checkpoints\'\r\nACT_CHKPT_NUMBER_CHECKPOINTS_DEFAULT = None\r\n\r\nACT_CHKPT_CONTIGUOUS_MEMORY_OPTIMIZATION = \'contiguous_memory_optimization\'\r\nACT_CHKPT_CONTIGUOUS_MEMORY_OPTIMIZATION_DEFAULT = False\r\n\r\nACT_CHKPT_SYNCHRONIZE_CHECKPOINT_BOUNDARY = \'synchronize_checkpoint_boundary\'\r\nACT_CHKPT_SYNCHRONIZE_CHECKPOINT_BOUNDARY_DEFAULT = False\r\n\r\nACT_CHKPT_PROFILE = \'profile\'\r\nACT_CHKPT_PROFILE_DEFAULT = False\r\n\r\nACT_CHKPT_CPU_CHECKPOINTING = \'cpu_checkpointing\'\r\nACT_CHKPT_CPU_CHECKPOINTING_DEFAULT = False\r\n\r\nACT_CHKPT = \'activation_checkpointing\'\r\n\r\nACT_CHKPT_DEFAULT = {\r\n    ACT_CHKPT_PARTITION_ACTIVATIONS: ACT_CHKPT_PARTITION_ACTIVATIONS_DEFAULT,\r\n    ACT_CHKPT_NUMBER_CHECKPOINTS: ACT_CHKPT_NUMBER_CHECKPOINTS_DEFAULT,\r\n    ACT_CHKPT_CONTIGUOUS_MEMORY_OPTIMIZATION:\r\n    ACT_CHKPT_CONTIGUOUS_MEMORY_OPTIMIZATION_DEFAULT,\r\n    ACT_CHKPT_SYNCHRONIZE_CHECKPOINT_BOUNDARY:\r\n    ACT_CHKPT_SYNCHRONIZE_CHECKPOINT_BOUNDARY_DEFAULT,\r\n    ACT_CHKPT_PROFILE: ACT_CHKPT_PROFILE_DEFAULT,\r\n    ACT_CHKPT_CPU_CHECKPOINTING: ACT_CHKPT_CPU_CHECKPOINTING_DEFAULT\r\n}\r\n\r\n\r\nclass DeepSpeedActivationCheckpointingConfig(object):\r\n    def __init__(self, param_dict):\r\n        super(DeepSpeedActivationCheckpointingConfig, self).__init__()\r\n\r\n        self.partition_activations = None\r\n        self.contiguous_memory_optimization = None\r\n        self.cpu_checkpointing = None\r\n        self.number_checkpoints = None\r\n        self.synchronize_checkpoint_boundary = None\r\n        self.profile = None\r\n\r\n        if ACT_CHKPT in param_dict.keys():\r\n            act_chkpt_config_dict = param_dict[ACT_CHKPT]\r\n        else:\r\n            act_chkpt_config_dict = ACT_CHKPT_DEFAULT\r\n\r\n        self._initialize(act_chkpt_config_dict)\r\n\r\n    """"""\r\n    For json serialization\r\n    """"""\r\n\r\n    def repr(self):\r\n        return self.__dict__\r\n\r\n    def _initialize(self, act_chkpt_config_dict):\r\n        self.partition_activations = get_scalar_param(\r\n            act_chkpt_config_dict,\r\n            ACT_CHKPT_PARTITION_ACTIVATIONS,\r\n            ACT_CHKPT_PARTITION_ACTIVATIONS_DEFAULT)\r\n\r\n        self.contiguous_memory_optimization = get_scalar_param(\r\n            act_chkpt_config_dict,\r\n            ACT_CHKPT_CONTIGUOUS_MEMORY_OPTIMIZATION,\r\n            ACT_CHKPT_CONTIGUOUS_MEMORY_OPTIMIZATION_DEFAULT)\r\n\r\n        self.cpu_checkpointing = get_scalar_param(act_chkpt_config_dict,\r\n                                                  ACT_CHKPT_CPU_CHECKPOINTING,\r\n                                                  ACT_CHKPT_CPU_CHECKPOINTING_DEFAULT)\r\n\r\n        self.number_checkpoints = get_scalar_param(act_chkpt_config_dict,\r\n                                                   ACT_CHKPT_NUMBER_CHECKPOINTS,\r\n                                                   ACT_CHKPT_NUMBER_CHECKPOINTS_DEFAULT)\r\n\r\n        self.profile = get_scalar_param(act_chkpt_config_dict,\r\n                                        ACT_CHKPT_PROFILE,\r\n                                        ACT_CHKPT_PROFILE_DEFAULT)\r\n\r\n        self.synchronize_checkpoint_boundary = get_scalar_param(\r\n            act_chkpt_config_dict,\r\n            ACT_CHKPT_SYNCHRONIZE_CHECKPOINT_BOUNDARY,\r\n            ACT_CHKPT_SYNCHRONIZE_CHECKPOINT_BOUNDARY_DEFAULT)\r\n'"
deepspeed/pt/deepspeed_config.py,2,"b'""""""\r\nCopyright (c) Microsoft Corporation\r\nLicensed under the MIT license.\r\n""""""\r\n\r\nimport torch\r\nimport json\r\nfrom deepspeed.pt.deepspeed_constants import *\r\nfrom deepspeed.pt.loss_scaler import INITIAL_LOSS_SCALE, SCALE_WINDOW, DELAYED_SHIFT, MIN_LOSS_SCALE\r\nfrom deepspeed.pt.deepspeed_config_utils import get_scalar_param, dict_raise_error_on_duplicate_keys\r\nfrom deepspeed.pt.deepspeed_zero_config import DeepSpeedZeroConfig\r\nfrom deepspeed.pt.deepspeed_checkpointing_config import DeepSpeedActivationCheckpointingConfig\r\nfrom deepspeed.pt.log_utils import logger\r\n\r\nTENSOR_CORE_ALIGN_SIZE = 8\r\nADAM_OPTIMIZER = \'adam\'\r\nLAMB_OPTIMIZER = \'lamb\'\r\nDEEPSPEED_OPTIMIZERS = [ADAM_OPTIMIZER, LAMB_OPTIMIZER]\r\n\r\n\r\ndef get_fp16_enabled(param_dict):\r\n    if FP16 in param_dict.keys():\r\n        return get_scalar_param(param_dict[FP16], FP16_ENABLED, FP16_ENABLED_DEFAULT)\r\n    else:\r\n        return False\r\n\r\n\r\ndef get_loss_scale(param_dict):\r\n    if get_fp16_enabled(param_dict):\r\n        return get_scalar_param(param_dict[FP16],\r\n                                FP16_LOSS_SCALE,\r\n                                FP16_LOSS_SCALE_DEFAULT)\r\n    else:\r\n        return FP16_LOSS_SCALE_DEFAULT\r\n\r\n\r\ndef get_initial_dynamic_scale(param_dict):\r\n    if get_fp16_enabled(param_dict):\r\n        initial_scale_power = get_scalar_param(param_dict[FP16],\r\n                                               FP16_INITIAL_SCALE_POWER,\r\n                                               FP16_INITIAL_SCALE_POWER_DEFAULT)\r\n    else:\r\n        initial_scale_power = FP16_INITIAL_SCALE_POWER_DEFAULT\r\n\r\n    return 2**initial_scale_power\r\n\r\n\r\ndef get_dynamic_loss_scale_args(param_dict):\r\n    loss_scale_args = None\r\n    if get_fp16_enabled(param_dict):\r\n        fp16_dict = param_dict[FP16]\r\n        dynamic_loss_args = [\r\n            FP16_INITIAL_SCALE_POWER,\r\n            FP16_LOSS_SCALE_WINDOW,\r\n            FP16_MIN_LOSS_SCALE,\r\n            FP16_HYSTERESIS\r\n        ]\r\n        if any(arg in list(fp16_dict.keys()) for arg in dynamic_loss_args):\r\n            init_scale = get_scalar_param(fp16_dict,\r\n                                          FP16_INITIAL_SCALE_POWER,\r\n                                          FP16_INITIAL_SCALE_POWER_DEFAULT)\r\n            scale_window = get_scalar_param(fp16_dict,\r\n                                            FP16_LOSS_SCALE_WINDOW,\r\n                                            FP16_LOSS_SCALE_WINDOW_DEFAULT)\r\n            delayed_shift = get_scalar_param(fp16_dict,\r\n                                             FP16_HYSTERESIS,\r\n                                             FP16_HYSTERESIS_DEFAULT)\r\n            min_loss_scale = get_scalar_param(fp16_dict,\r\n                                              FP16_MIN_LOSS_SCALE,\r\n                                              FP16_MIN_LOSS_SCALE_DEFAULT)\r\n            loss_scale_args = {\r\n                INITIAL_LOSS_SCALE: 2**init_scale,\r\n                SCALE_WINDOW: scale_window,\r\n                DELAYED_SHIFT: delayed_shift,\r\n                MIN_LOSS_SCALE: min_loss_scale\r\n            }\r\n\r\n    return loss_scale_args\r\n\r\n\r\ndef get_gradient_accumulation_steps(param_dict):\r\n    return get_scalar_param(param_dict,\r\n                            GRADIENT_ACCUMULATION_STEPS,\r\n                            GRADIENT_ACCUMULATION_STEPS_DEFAULT)\r\n\r\n\r\ndef get_sparse_gradients_enabled(param_dict):\r\n    return get_scalar_param(param_dict, SPARSE_GRADIENTS, SPARSE_GRADIENTS_DEFAULT)\r\n\r\n\r\ndef get_zero_optimization(param_dict):\r\n    return get_scalar_param(param_dict, ZERO_OPTIMIZATION, ZERO_OPTIMIZATION_DEFAULT)\r\n\r\n\r\ndef get_zero_reduce_scatter(param_dict):\r\n    return get_scalar_param(param_dict, ZERO_REDUCE_SCATTER, ZERO_REDUCE_SCATTER_DEFAULT)\r\n\r\n\r\ndef get_zero_max_elements_per_comm(param_dict):\r\n    return get_scalar_param(param_dict,\r\n                            ZERO_MAX_ELEMENTS_PER_COMM,\r\n                            ZERO_MAX_ELEMENTS_PER_COMM_DEFAULT)\r\n\r\n\r\ndef get_allgather_size(param_dict):\r\n    return get_scalar_param(param_dict,\r\n                            ALLGATHER_SIZE,\r\n                            ALLGATHER_SIZE_DEFAULT) if get_scalar_param(\r\n                                param_dict,\r\n                                ALLGATHER_SIZE,\r\n                                ALLGATHER_SIZE_DEFAULT) > 0 else ALLGATHER_SIZE_DEFAULT\r\n\r\n\r\ndef get_allreduce_always_fp32(param_dict):\r\n    return get_scalar_param(param_dict, FP32_ALLREDUCE, FP32_ALLREDUCE_DEFAULT)\r\n\r\n\r\ndef get_prescale_gradients(param_dict):\r\n    return get_scalar_param(param_dict, PRESCALE_GRADIENTS, PRESCALE_GRADIENTS_DEFAULT)\r\n\r\n\r\ndef get_gradient_predivide_factor(param_dict):\r\n    return get_scalar_param(param_dict,\r\n                            GRADIENT_PREDIVIDE_FACTOR,\r\n                            GRADIENT_PREDIVIDE_FACTOR_DEFAULT)\r\n\r\n\r\ndef get_steps_per_print(param_dict):\r\n    return get_scalar_param(param_dict, STEPS_PER_PRINT, STEPS_PER_PRINT_DEFAULT)\r\n\r\n\r\ndef get_disable_allgather(param_dict):\r\n    return get_scalar_param(param_dict, DISABLE_ALLGATHER, DISABLE_ALLGATHER_DEFAULT)\r\n\r\n\r\ndef get_dump_state(param_dict):\r\n    return get_scalar_param(param_dict, DUMP_STATE, DUMP_STATE_DEFAULT)\r\n\r\n\r\ndef get_gradient_clipping(param_dict):\r\n    return get_scalar_param(param_dict, GRADIENT_CLIPPING, GRADIENT_CLIPPING_DEFAULT)\r\n\r\n\r\ndef get_optimizer_name(param_dict):\r\n    if OPTIMIZER in param_dict.keys() and \\\r\n            TYPE in param_dict[OPTIMIZER].keys():\r\n        return param_dict[OPTIMIZER][TYPE]\r\n    else:\r\n        return OPTIMIZER_TYPE_DEFAULT\r\n\r\n\r\ndef get_optimizer_params(param_dict):\r\n    if get_optimizer_name(param_dict) is not None and \\\r\n            OPTIMIZER_PARAMS in param_dict[OPTIMIZER].keys():\r\n        return param_dict[OPTIMIZER][OPTIMIZER_PARAMS]\r\n    else:\r\n        return None\r\n\r\n\r\ndef get_optimizer_gradient_clipping(param_dict):\r\n    optimizer_params = get_optimizer_params(param_dict)\r\n    if optimizer_params is not None and \\\r\n            MAX_GRAD_NORM in optimizer_params.keys():\r\n        return optimizer_params[MAX_GRAD_NORM]\r\n    else:\r\n        return None\r\n\r\n\r\ndef get_optimizer_legacy_fusion(param_dict):\r\n    if OPTIMIZER in param_dict.keys() and \\\r\n        LEGACY_FUSION in param_dict[OPTIMIZER].keys():\r\n        return param_dict[OPTIMIZER][LEGACY_FUSION]\r\n    else:\r\n        return LEGACY_FUSION_DEFAULT\r\n\r\n\r\ndef get_zero_allow_untested_optimizer(param_dict):\r\n    return get_scalar_param(param_dict,\r\n                            ZERO_ALLOW_UNTESTED_OPTIMIZER,\r\n                            ZERO_ALLOW_UNTESTED_OPTIMIZER_DEFAULT)\r\n\r\n\r\ndef get_scheduler_name(param_dict):\r\n    if SCHEDULER in param_dict.keys() and \\\r\n            TYPE in param_dict[SCHEDULER].keys():\r\n        return param_dict[SCHEDULER][TYPE]\r\n    else:\r\n        return SCHEDULER_TYPE_DEFAULT\r\n\r\n\r\ndef get_scheduler_params(param_dict):\r\n    if get_scheduler_name(param_dict) is not None and \\\r\n            SCHEDULER_PARAMS in param_dict[SCHEDULER].keys():\r\n        return param_dict[SCHEDULER][SCHEDULER_PARAMS]\r\n    else:\r\n        return None\r\n\r\n\r\ndef get_train_batch_size(param_dict):\r\n    return get_scalar_param(param_dict, TRAIN_BATCH_SIZE, TRAIN_BATCH_SIZE_DEFAULT)\r\n\r\n\r\ndef get_train_micro_batch_size_per_gpu(param_dict):\r\n    return get_scalar_param(param_dict,\r\n                            TRAIN_MICRO_BATCH_SIZE_PER_GPU,\r\n                            TRAIN_MICRO_BATCH_SIZE_PER_GPU_DEFAULT)\r\n\r\n\r\ndef get_wall_clock_breakdown(param_dict):\r\n    return get_scalar_param(param_dict,\r\n                            WALL_CLOCK_BREAKDOWN,\r\n                            WALL_CLOCK_BREAKDOWN_DEFAULT)\r\n\r\n\r\ndef get_memory_breakdown(param_dict):\r\n    return get_scalar_param(param_dict, MEMORY_BREAKDOWN, MEMORY_BREAKDOWN_DEFAULT)\r\n\r\n\r\ndef get_tensorboard_enabled(param_dict):\r\n    if TENSORBOARD in param_dict.keys():\r\n        return get_scalar_param(param_dict[TENSORBOARD],\r\n                                TENSORBOARD_ENABLED,\r\n                                TENSORBOARD_ENABLED_DEFAULT)\r\n    else:\r\n        return False\r\n\r\n\r\ndef get_tensorboard_output_path(param_dict):\r\n    if get_tensorboard_enabled(param_dict):\r\n        return get_scalar_param(param_dict[TENSORBOARD],\r\n                                TENSORBOARD_OUTPUT_PATH,\r\n                                TENSORBOARD_OUTPUT_PATH_DEFAULT)\r\n    else:\r\n        return TENSORBOARD_OUTPUT_PATH_DEFAULT\r\n\r\n\r\ndef get_tensorboard_job_name(param_dict):\r\n    if get_tensorboard_enabled(param_dict):\r\n        return get_scalar_param(param_dict[TENSORBOARD],\r\n                                TENSORBOARD_JOB_NAME,\r\n                                TENSORBOARD_JOB_NAME_DEFAULT)\r\n    else:\r\n        return TENSORBOARD_JOB_NAME_DEFAULT\r\n\r\n\r\n\'\'\'Write deepspeed config files by modifying basic templates.\r\nCan be used for quicly changing parameters via command line parameters.\'\'\'\r\n\r\n\r\nclass DeepSpeedConfigWriter:\r\n    def __init__(self, data=None):\r\n        self.data = data if data is not None else {}\r\n\r\n    def add_config(self, key, value):\r\n        self.data[key] = value\r\n\r\n    def load_config(self, filename):\r\n        self.data = json.load(open(filename,\r\n                                   \'r\'),\r\n                              object_pairs_hook=dict_raise_error_on_duplicate_keys)\r\n\r\n    def write_config(self, filename):\r\n        with open(filename, \'w\') as outfile:\r\n            json.dump(self.data, outfile)\r\n\r\n\r\nclass DeepSpeedConfig(object):\r\n    def __init__(self, json_file, mpu=None, param_dict=None):\r\n        super(DeepSpeedConfig, self).__init__()\r\n\r\n        if param_dict is None:\r\n            self._param_dict = json.load(\r\n                open(json_file,\r\n                     \'r\'),\r\n                object_pairs_hook=dict_raise_error_on_duplicate_keys)\r\n        else:\r\n            self._param_dict = param_dict\r\n\r\n        try:\r\n            self.global_rank = torch.distributed.get_rank()\r\n            if mpu is None:\r\n                self.world_size = torch.distributed.get_world_size()\r\n            else:\r\n                self.world_size = mpu.get_data_parallel_world_size()\r\n        except:\r\n            self.global_rank = 0\r\n            self.world_size = 1\r\n\r\n        self._initialize_params(self._param_dict)\r\n        self._configure_train_batch_size()\r\n        self._do_sanity_check()\r\n\r\n    def _initialize_params(self, param_dict):\r\n        self.train_batch_size = get_train_batch_size(param_dict)\r\n        self.train_micro_batch_size_per_gpu = get_train_micro_batch_size_per_gpu(\r\n            param_dict)\r\n        self.gradient_accumulation_steps = get_gradient_accumulation_steps(param_dict)\r\n        self.steps_per_print = get_steps_per_print(param_dict)\r\n        self.dump_state = get_dump_state(param_dict)\r\n\r\n        self.disable_allgather = get_disable_allgather(param_dict)\r\n        self.allreduce_always_fp32 = get_allreduce_always_fp32(param_dict)\r\n        self.prescale_gradients = get_prescale_gradients(param_dict)\r\n        self.gradient_predivide_factor = get_gradient_predivide_factor(param_dict)\r\n        self.sparse_gradients_enabled = get_sparse_gradients_enabled(param_dict)\r\n\r\n        self.allgather_size = get_allgather_size(param_dict)\r\n\r\n        self.zero_config = DeepSpeedZeroConfig(param_dict)\r\n        self.zero_optimization_stage = self.zero_config.stage\r\n        self.zero_enabled = self.zero_optimization_stage > 0\r\n\r\n        self.activation_checkpointing_config = DeepSpeedActivationCheckpointingConfig(\r\n            param_dict)\r\n\r\n        self.gradient_clipping = get_gradient_clipping(param_dict)\r\n        self.fp16_enabled = get_fp16_enabled(param_dict)\r\n        self.loss_scale = get_loss_scale(param_dict)\r\n        self.initial_dynamic_scale = get_initial_dynamic_scale(param_dict)\r\n        self.dynamic_loss_scale_args = get_dynamic_loss_scale_args(param_dict)\r\n\r\n        self.optimizer_name = get_optimizer_name(param_dict)\r\n        if self.optimizer_name is not None and \\\r\n            self.optimizer_name.lower() in DEEPSPEED_OPTIMIZERS:\r\n            self.optimizer_name = self.optimizer_name.lower()\r\n\r\n        self.optimizer_params = get_optimizer_params(param_dict)\r\n        self.optimizer_legacy_fusion = get_optimizer_legacy_fusion(param_dict)\r\n\r\n        self.zero_allow_untested_optimizer = get_zero_allow_untested_optimizer(\r\n            param_dict)\r\n\r\n        self.scheduler_name = get_scheduler_name(param_dict)\r\n        self.scheduler_params = get_scheduler_params(param_dict)\r\n\r\n        self.wall_clock_breakdown = get_wall_clock_breakdown(param_dict)\r\n        self.memory_breakdown = get_memory_breakdown(param_dict)\r\n        self.tensorboard_enabled = get_tensorboard_enabled(param_dict)\r\n        self.tensorboard_output_path = get_tensorboard_output_path(param_dict)\r\n        self.tensorboard_job_name = get_tensorboard_job_name(param_dict)\r\n\r\n    def _batch_assertion(self):\r\n\r\n        train_batch = self.train_batch_size\r\n        micro_batch = self.train_micro_batch_size_per_gpu\r\n        grad_acc = self.gradient_accumulation_steps\r\n\r\n        assert train_batch > 0, \\\r\n            f\'Train batch size: {train_batch} has to be greater than 0\'\r\n\r\n        assert micro_batch > 0, \\\r\n            f\'Micro batch size per gpu: {micro_batch} has to be greater than 0\'\r\n\r\n        assert grad_acc > 0, \\\r\n            f\'Gradient accumulation steps: {grad_acc} has to be greater than 0\'\r\n\r\n        assert train_batch == micro_batch * grad_acc * self.world_size, \\\r\n                (f\'Check batch related parameters. train_batch_size is not equal\'\r\n                \' to micro_batch_per_gpu * gradient_acc_step * world_size\'\r\n                f\'{train_batch} != {micro_batch} * {grad_acc} * {self.world_size}\')\r\n\r\n    def _set_batch_related_parameters(self):\r\n\r\n        train_batch = self.train_batch_size\r\n        micro_batch = self.train_micro_batch_size_per_gpu\r\n        grad_acc = self.gradient_accumulation_steps\r\n\r\n        #all values are provided nothing needs to be set\r\n        if train_batch is not None and \\\r\n            micro_batch is not None and \\\r\n            grad_acc is not None:\r\n            return\r\n\r\n        #global_accumulation_steps needs to be set\r\n        elif train_batch is not None and \\\r\n            micro_batch is not None:\r\n            grad_acc = train_batch // micro_batch\r\n            grad_acc //= self.world_size\r\n            self.gradient_accumulation_steps = grad_acc\r\n\r\n        #micro_batch_per_gpu needs to be set\r\n        elif train_batch is not None and \\\r\n            grad_acc is not None:\r\n            micro_batch = train_batch // self.world_size\r\n            micro_batch //= grad_acc\r\n            self.train_micro_batch_size_per_gpu = micro_batch\r\n\r\n        #train_batch_size needs to be set\r\n        elif micro_batch is not None and \\\r\n            grad_acc is not None:\r\n            train_batch_size = micro_batch * grad_acc\r\n            train_batch_size *= self.world_size\r\n            self.train_batch_size = train_batch_size\r\n\r\n        #gradient_accumulation_steps and micro_batch_per_gpus is set\r\n        elif train_batch is not None:\r\n            self.gradient_accumulation_steps = 1\r\n            self.train_micro_batch_size_per_gpu = train_batch // self.world_size\r\n\r\n        #train_batch_size and gradient_accumulation_step is set\r\n        elif micro_batch is not None:\r\n            self.train_batch_size = micro_batch * self.world_size\r\n            self.gradient_accumulation_steps = 1\r\n\r\n        #either none of the three parameters are provided or just gradient_accumulation_step is provided\r\n        else:\r\n            assert False, \\\r\n                \'Either train_batch_size or micro_batch_per_gpu needs to be provided\'\r\n\r\n        logger.info(\r\n            f\' After Train batch {self.train_batch_size} micro_batch {self.train_micro_batch_size_per_gpu} and grad_acc {self.gradient_accumulation_steps}\'\r\n        )\r\n\r\n    def _configure_train_batch_size(self):\r\n        self._set_batch_related_parameters()\r\n        self._batch_assertion()\r\n\r\n    def _do_sanity_check(self):\r\n        self._do_error_check()\r\n\r\n        self._do_warning_check()\r\n\r\n    def print(self, name):\r\n        logger.info(\'{}:\'.format(name))\r\n        for arg in sorted(vars(self)):\r\n            if arg != \'_param_dict\':\r\n                dots = \'.\' * (29 - len(arg))\r\n                logger.info(\'  {} {} {}\'.format(arg, dots, getattr(self, arg)))\r\n\r\n        logger.info(\'  json = {}\'.format(\r\n            json.dumps(self._param_dict,\r\n                       sort_keys=True,\r\n                       indent=4,\r\n                       separators=(\',\',\r\n                                   \':\'))))\r\n\r\n    def _do_error_check(self):\r\n        if self.zero_enabled:\r\n            assert self.fp16_enabled, ""DeepSpeedConfig: ZeRO is only supported if fp16 is enabled""\r\n            assert self.zero_optimization_stage <= MAX_STAGE_ZERO_OPTIMIZATION, ""DeepSpeedConfig: Maximum supported ZeRO stage is {}"".format(MAX_STAGE_ZERO_OPTIMIZATION)\r\n\r\n        assert self.train_micro_batch_size_per_gpu, ""DeepSpeedConfig: {} is not defined"".format(TRAIN_MICRO_BATCH_SIZE_PER_GPU)\r\n\r\n        assert self.gradient_accumulation_steps, \'DeepSpeedConfig: {} is not defined\'.format(\r\n            GRADIENT_ACCUMULATION_STEPS)\r\n\r\n    def _do_warning_check(self):\r\n        fp16_enabled = self.fp16_enabled or self.zero_enabled\r\n\r\n        vocabulary_size = self._param_dict.get(VOCABULARY_SIZE, VOCABULARY_SIZE_DEFAULT)\r\n        if vocabulary_size and vocabulary_size % TENSOR_CORE_ALIGN_SIZE != 0:\r\n            logger.warning(\r\n                ""DeepSpeedConfig: vocabulary size {} is not aligned to {}, may import tensor core utilization.""\r\n                .format(vocabulary_size,\r\n                        TENSOR_CORE_ALIGN_SIZE))\r\n\r\n        if self.optimizer_params is not None and \\\r\n            MAX_GRAD_NORM in self.optimizer_params.keys() and \\\r\n                self.optimizer_params[MAX_GRAD_NORM] > 0:\r\n            if fp16_enabled:\r\n                logger.warning(\r\n                    \'DeepSpeedConfig: In FP16 mode, DeepSpeed will pass {}:{} to FP16 wrapper\'\r\n                    .format(MAX_GRAD_NORM,\r\n                            self.optimizer_params[MAX_GRAD_NORM]))\r\n            else:\r\n                logger.warning(\r\n                    \'DeepSpeedConfig: In FP32 mode, DeepSpeed does not permit MAX_GRAD_NORM ({}) > 0, setting to zero\'\r\n                    .format(self.optimizer_params[MAX_GRAD_NORM]))\r\n                self.optimizer_params[MAX_GRAD_NORM] = 0.0\r\n'"
deepspeed/pt/deepspeed_config_utils.py,0,"b'""""""\r\nCopyright (c) Microsoft Corporation\r\nLicensed under the MIT license.\r\n""""""\r\n""""""\r\nCollection of DeepSpeed configuration utilities\r\n""""""\r\n\r\nfrom collections import Counter\r\n\r\n\r\ndef get_scalar_param(param_dict, param_name, param_default_value):\r\n    return param_dict.get(param_name, param_default_value)\r\n\r\n\r\ndef dict_raise_error_on_duplicate_keys(ordered_pairs):\r\n    """"""Reject duplicate keys.""""""\r\n    d = dict((k, v) for k, v in ordered_pairs)\r\n    if len(d) != len(ordered_pairs):\r\n        counter = Counter([pair[0] for pair in ordered_pairs])\r\n        keys = [key for key, value in counter.items() if value > 1]\r\n        raise ValueError(""Duplicate keys in DeepSpeed config: {}"".format(keys))\r\n    return d\r\n'"
deepspeed/pt/deepspeed_constants.py,0,"b'""""""\nCopyright (c) Microsoft Corporation\nLicensed under the MIT license.\n""""""\n\n#############################################\n# Routes\n#############################################\nROUTE_TRAIN = ""train""\nROUTE_EVAL = ""eval""\nROUTE_PREDICT = ""predict""\nROUTE_ENCODE = ""encode""\n\n#############################################\n# Batch size\n#############################################\nTRAIN_BATCH_SIZE = ""train_batch_size""\nTRAIN_BATCH_SIZE_DEFAULT = None\n\n#############################################\n# Optimizer and lr scheduler\n#############################################\nOPTIMIZER = ""optimizer""\nOPTIMIZER_TYPE_DEFAULT = None\nOPTIMIZER_PARAMS = ""params""\nTYPE = ""type""\nLEGACY_FUSION = ""legacy_fusion""\nLEGACY_FUSION_DEFAULT = False\nSCHEDULER = ""scheduler""\nSCHEDULER_TYPE_DEFAULT = None\nSCHEDULER_PARAMS = ""params""\nMAX_GRAD_NORM = \'max_grad_norm\'\n\n#############################################\n# Optimizer and lr scheduler\n#############################################\nZERO_ALLOW_UNTESTED_OPTIMIZER = ""zero_allow_untested_optimizer""\nZERO_ALLOW_UNTESTED_OPTIMIZER_DEFAULT = False\n\n#############################################\n# Torch distributed constants\n#############################################\nTORCH_DISTRIBUTED_DEFAULT_PORT = ""29500""\n\n# Steps\nSTEPS_PER_PRINT = ""steps_per_print""\nSTEPS_PER_PRINT_DEFAULT = 10\n\n#########################################\n# Training micro batch size per GPU\n#########################################\n# Batch size for one training step. This is used when the\n# TRAIN_BATCH_SIZE cannot fit in GPU memory to determine\n# the number of gradient accumulation steps. By default, this\n# is set to None. Users can configure in ds_config.json as below example:\nTRAIN_MICRO_BATCH_SIZE_PER_GPU = \'\'\'\nTRAIN_MICRO_BATCH_SIZE_PER_GPU is defined in this format:\n""train_micro_batch_size_per_gpu"": 1\n\'\'\'\nTRAIN_MICRO_BATCH_SIZE_PER_GPU = ""train_micro_batch_size_per_gpu""\nTRAIN_MICRO_BATCH_SIZE_PER_GPU_DEFAULT = None\n\n#########################################\n# Gradient Accumulation\n#########################################\n# Gradient accumulation feature. By default, this feature is not enabled.\n# Users can configure in ds_config.json as below example:\nGRADIENT_ACCUMULATION_FORMAT = \'\'\'\nGradient Accumulation should be of the format:\n""gradient_accumulation_steps"": 1\n\'\'\'\nGRADIENT_ACCUMULATION_STEPS = ""gradient_accumulation_steps""\nGRADIENT_ACCUMULATION_STEPS_DEFAULT = None\n\n# DeepSpeed CSR gradient sparsity\nSPARSE_GRADIENTS = ""sparse_gradients""\nSPARSE_GRADIENTS_DEFAULT = False\n\n#########################################\n# FP16 support\n#########################################\n# FP16 feature. By default, this feature is not enabled.\n# Users can configure in ds_config.json as below example:\nFP16_FORMAT = \'\'\'\nFP16 parameters should be of the format:\n""fp16"": {\n  ""enabled"": true,\n  ""loss_scale"": 0,\n  ""initial_scale_power"": 32,\n  ""loss_scale_window"": 1000,\n  ""hysteresis"": 2,\n  ""min_loss_scale"": 1\n}\n\'\'\'\nFP16 = ""fp16""\n\nFP16_ENABLED = ""enabled""\nFP16_ENABLED_DEFAULT = False\n\n# FP16 loss scale, zero means using dynamic scaling\nFP16_LOSS_SCALE = ""loss_scale""\nFP16_LOSS_SCALE_DEFAULT = 0\n\n# FP16 initial dynamic scale loss power\nFP16_INITIAL_SCALE_POWER = ""initial_scale_power""\nFP16_INITIAL_SCALE_POWER_DEFAULT = 32\n\n# FP16 loss scale window\nFP16_LOSS_SCALE_WINDOW = ""loss_scale_window""\nFP16_LOSS_SCALE_WINDOW_DEFAULT = 1000\n\n# FP16 hysteresis\nFP16_HYSTERESIS = ""hysteresis""\nFP16_HYSTERESIS_DEFAULT = 2\n\n# FP16 min loss scale\nFP16_MIN_LOSS_SCALE = ""min_loss_scale""\nFP16_MIN_LOSS_SCALE_DEFAULT = 1\n\n#########################################\n# Gradient clipping\n#########################################\n# Gradient clipping. By default, this feature is not enabled.\n# Users can configure in ds_config.json as below example:\nGRADIENT_CLIPPING_FORMAT = \'\'\'\nGradient clipping should be enabled as:\n""gradient_clipping"": 1.0\n\'\'\'\nGRADIENT_CLIPPING = \'gradient_clipping\'\nGRADIENT_CLIPPING_DEFAULT = 0.\n\n#########################################\n# ZeRO optimization\n#########################################\n# ZeRO optimization. By default, this optimization is not enabled.\n# Users have to configure the desired optimization (0 means disabled) in params.json as below example:\nZERO_FORMAT = \'\'\'\nZeRO optimization should be enabled as:\n""session_params"": {\n  ""zero_optimization"": [0|1|2],\n  ""zero_all_gather_size"": 200\n}\n\'\'\'\n\nZERO_OPTIMIZATION = \'zero_optimization\'\nZERO_OPTIMIZATION_DEFAULT = 0\nZERO_OPTIMIZATION_OPTIMIZER_STATES = 1\nZERO_OPTIMIZATION_GRADIENTS = 2\nZERO_OPTIMIZATION_WEIGHTS = 3\nMAX_STAGE_ZERO_OPTIMIZATION = ZERO_OPTIMIZATION_GRADIENTS\n\nZERO_REDUCE_SCATTER = ""zero_reduce_scatter""\nZERO_REDUCE_SCATTER_DEFAULT = True\n\nZERO_MAX_ELEMENTS_PER_COMM = ""zero_max_elements_per_comm""\nZERO_MAX_ELEMENTS_PER_COMM_DEFAULT = 5e8\n\nALLGATHER_SIZE = \'allgather_size\'\nALLGATHER_SIZE_DEFAULT = 500000000\n\n#########################################\n# FP32 AllReduce\n#########################################\n# FP32 All reduce. By default, this feature is not enabled.\n# Users can configure in ds_config.json as below example:\nFP32_ALLREDUCE_FORMAT = \'\'\'\nFP32 Allreduce should be enabled as:\n""fp32_allreduce"": true\n\'\'\'\nFP32_ALLREDUCE = ""fp32_allreduce""\nFP32_ALLREDUCE_DEFAULT = False\n\n#########################################\n# Scale/predivide gradients before allreduce\n#########################################\n# Prescale gradients. By default, this feature is not enabled.\n# Users can configure in ds_config.json as below example:\nPRESCALE_GRADIENTS_FORMAT = \'\'\'\nGradient prescaling should be enabled as:\n""prescale_gradients"": true\n\'\'\'\nPRESCALE_GRADIENTS = ""prescale_gradients""\nPRESCALE_GRADIENTS_DEFAULT = False\n\nGRADIENT_PREDIVIDE_FACTOR_FORMAT = \'\'\'\nGradient predivide factor should be enabled as:\n""gradient_predivide_factor"": 1.0\n\'\'\'\nGRADIENT_PREDIVIDE_FACTOR = ""gradient_predivide_factor""\nGRADIENT_PREDIVIDE_FACTOR_DEFAULT = 1.0\n\n#########################################\n# Disable AllGather\n#########################################\n# Disable AllGather. By default, this feature is not enabled.\n# Users can configure in ds_config.json as below example:\nDISABLE_ALLGATHER_FORMAT = \'\'\'\nDisable AllGather should be enabled as:\n""disable_allgather"": true\n\'\'\'\nDISABLE_ALLGATHER = ""disable_allgather""\nDISABLE_ALLGATHER_DEFAULT = False\n\n#########################################\n# Dump DeepSpeed state\n#########################################\n# Dump State. By default, this feature is not enabled.\n# Users can configure in ds_config.json as below example:\nDUMP_STATE_FORMAT = \'\'\'\nDump state should be enabled as:\n""dump_state"": true\n\'\'\'\nDUMP_STATE = \'dump_state\'\nDUMP_STATE_DEFAULT = False\n\n#########################################\n# Vocabulary size\n#########################################\n# Vocabulary size.\n# Users can configure in ds_config.json as below example:\nVOCABULARY_SIZE_FORMAT = \'\'\'\nVocabulary size can be specified as:\n""vocabulary_size"": 1024\n\'\'\'\nVOCABULARY_SIZE = \'vocabulary_size\'\nVOCABULARY_SIZE_DEFAULT = None\n\n#########################################\n# Wall block breakdown\n#########################################\n# Wall clock breakdown. By default, this feature is not enabled.\n# Users can configure in ds_config.json as below example:\nWALL_CLOCK_BREAKDOWN_FORMAT = \'\'\'\nWall block breakdown should be enabled as:\n""wall_clock_breakdown"": true\n\'\'\'\nWALL_CLOCK_BREAKDOWN = \'wall_clock_breakdown\'\nWALL_CLOCK_BREAKDOWN_DEFAULT = False\n\nMEMORY_BREAKDOWN = \'memory_breakdown\'\nMEMORY_BREAKDOWN_DEFAULT = False\n\n#########################################\n# Tensorboard\n#########################################\n# Tensorboard. By default, this feature is not enabled.\n# Users can configure in ds_config.json as below example:\nTENSORBOARD_FORMAT = \'\'\'\nTensorboard can be specified as:\n""tensorboard"": {\n  ""enabled"": true,\n  ""output_path"": ""/home/myname/foo"",\n  ""job_name"": ""model_lr2e-5_epoch3_seed2_seq64""\n}\n\'\'\'\nTENSORBOARD = ""tensorboard""\n\n# Tensorboard enable signal\nTENSORBOARD_ENABLED = ""enabled""\nTENSORBOARD_ENABLED_DEFAULT = False\n\n# Tensorboard output path\nTENSORBOARD_OUTPUT_PATH = ""output_path""\nTENSORBOARD_OUTPUT_PATH_DEFAULT = """"\n\n# Tensorboard job name\nTENSORBOARD_JOB_NAME = ""job_name""\nTENSORBOARD_JOB_NAME_DEFAULT = ""DeepSpeedJobName""\n'"
deepspeed/pt/deepspeed_csr_tensor.py,4,"b'""""""\nCopyright 2020 The Microsoft DeepSpeed Team\n\nImplementation of a compressed sparse row (CSR) tensor. Similar in\nfunctionality to TensorFlow\'s IndexedSlices implementation.\n""""""\n\nimport torch\n\n\nclass CSRTensor(object):\n    """""" Compressed Sparse Row (CSR) Tensor """"""\n    def __init__(self, dense_tensor=None):\n        self.orig_dense_tensor = dense_tensor\n        if dense_tensor is not None:\n            result = torch.sum(dense_tensor, dim=1)\n            self.indices = result.nonzero().flatten()\n            self.values = dense_tensor[self.indices]\n            self.dense_size = list(dense_tensor.size())\n        else:\n            self.indices = None\n            self.values = None\n            self.dense_size = None\n\n    @staticmethod\n    def type():\n        return ""deepspeed.CSRTensor""\n\n    def to_dense(self):\n        it = self.indices.unsqueeze(1)\n        full_indices = torch.cat([it for _ in range(self.dense_size[1])], dim=1)\n        return self.values.new_zeros(self.dense_size).scatter_add_(\n            0,\n            full_indices,\n            self.values)\n\n    def sparse_size(self):\n        index_size = list(self.indices.size())\n        index_size = index_size[0]\n        value_size = list(self.values.size())\n        value_size = value_size[0] * value_size[1]\n        dense_size = self.dense_size[0] * self.dense_size[1]\n        return index_size + value_size, dense_size\n\n    def add(self, b):\n        assert self.dense_size == b.dense_size\n        self.indices = torch.cat([self.indices, b.indices])\n        self.values = torch.cat([self.values, b.values])\n\n    def __str__(self):\n        sparse_size, dense_size = self.sparse_size()\n        return ""DeepSpeed.CSRTensor(indices_size={}, values_size={}, "" \\\n               ""dense_size={}, device={}, reduction_factor={})"".format(\n            self.indices.size(), self.values.size(), self.dense_size,\n            self.indices.get_device(), dense_size / sparse_size\n        )\n\n    def __repr__(self):\n        return self.__str__()\n'"
deepspeed/pt/deepspeed_cuda.py,18,"b'from torch import nn\nfrom torch.autograd import Function\nimport torch\nimport json\nimport math\nimport deepspeed_transformer_cuda as ds_transformer_cuda\nimport deepspeed_stochastic_transformer_cuda as ds_stochastic_transformer_cuda\n\n\nclass TransformerConfig():\n    def __init__(self,\n                 batch_size,\n                 max_seq_length,\n                 hidden_size,\n                 heads,\n                 attn_dropout_ratio,\n                 hidden_dropout_ratio,\n                 num_hidden_layers,\n                 initializer_range):\n        self.layer_id = -1\n        self.batch_size = batch_size\n        self.hidden_size = hidden_size\n        self.max_seq_length = max_seq_length\n        self.heads = heads\n        self.attn_dropout_ratio = attn_dropout_ratio\n        self.hidden_dropout_ratio = hidden_dropout_ratio\n        self.num_hidden_layers = num_hidden_layers\n        self.initializer_range = initializer_range\n\n\nclass DeepSpeedTransformerConfig(TransformerConfig):\n    """"""Initialize the DeepSpeed Transformer Config.\n\n        Arguments:\n            batch_size: The maximum batch size used for running the kernel on each GPU\n\n            max_seq_length: The sequence-length of the model being trained with DeepSpeed\n\n            hidden_size: The hidden size of the transformer layer\n\n            heads: The number of heads in the self-attention of the transformer layer\n\n            attn_dropout_ratio: The ratio of dropout for the attention\'s output\n\n            hidden_dropout_ratio: The ratio of dropout for the transformer\'s output\n\n            num_hidden_layers: The number of transformer layers\n\n            initializer_range: BERT model\'s initializer range for initializing parameter data\n\n            local_rank: Optional: The rank of GPU running the transformer kernel, it is not required\n                to use if the model already set the current device, otherwise need to set it\n                so that the transformer kernel can work on the right device\n\n            seed: The random seed for the dropout layers\n\n            fp16: Enable half-precision computation\n\n            pre_layer_norm: Select between Pre-LN or Post-LN transformer architecture\n\n            normalize_invertible: Optional: Enable invertible LayerNorm execution (dropping the input activation),\n                default is False\n\n            gelu_checkpoint: Optional: Enable checkpointing of Gelu activation output to save memory,\n                default is False\n\n            adjust_init_range: Optional: Set as True (default) if the model adjusts the weight initial values of\n                its self-attention output and layer output, False keeps the initializer_range no change.\n                See the adjustment below:\n                    output_std = self.config.initializer_range / math.sqrt(2.0 * num_layers)\n\n            attn_dropout_checkpoint: Optional: Enable checkpointing of attention dropout to save memory,\n                default is False\n\n            stochastic_mode:  Enable for high performance, please note that this flag has some level of\n                non-determinism and can produce different results on different runs.  However, we have seen\n                that by enabling it, the pretraining tasks such as BERT are not affected and can obtain\n                a high accuracy level. On the other hand, for the downstream tasks, such as fine-tuning, we recommend\n                to turn it off in order to be able to reproduce the same result through the regular kernel execution.\n    """"""\n    def __init__(self,\n                 batch_size=-1,\n                 max_seq_length=-1,\n                 hidden_size=-1,\n                 heads=-1,\n                 attn_dropout_ratio=-1,\n                 hidden_dropout_ratio=-1,\n                 num_hidden_layers=-1,\n                 initializer_range=-1,\n                 local_rank=-1,\n                 seed=-1,\n                 fp16=False,\n                 pre_layer_norm=True,\n                 normalize_invertible=False,\n                 gelu_checkpoint=False,\n                 adjust_init_range=True,\n                 attn_dropout_checkpoint=False,\n                 stochastic_mode=False):\n        super(DeepSpeedTransformerConfig,\n              self).__init__(batch_size,\n                             max_seq_length,\n                             hidden_size,\n                             heads,\n                             attn_dropout_ratio,\n                             hidden_dropout_ratio,\n                             num_hidden_layers,\n                             initializer_range)\n        self.fp16 = fp16\n        self.pre_layer_norm = pre_layer_norm\n        self.local_rank = local_rank\n        self.seed = seed\n        self.normalize_invertible = normalize_invertible\n        self.gelu_checkpoint = gelu_checkpoint  # True: if higher batch size is required\n        self.adjust_init_range = adjust_init_range\n        self.test_gemm = False\n        self.training = True\n        self.is_grad_enabled = True\n        self.attn_dropout_checkpoint = attn_dropout_checkpoint\n        self.stochastic_mode = stochastic_mode\n\n    @classmethod\n    def from_dict(cls, json_object):\n        config = DeepSpeedTransformerConfig()\n        for key, value in json_object.items():\n            config.__dict__[key] = value\n        return config\n\n    @classmethod\n    def from_json_file(cls, json_file):\n        with open(json_file, ""r"", encoding=\'utf-8\') as reader:\n            text = reader.read()\n        return cls.from_dict(json.loads(text))\n\n\nclass DeepSpeedTransformerFunction(Function):\n    @staticmethod\n    def forward(ctx,\n                input,\n                input_mask,\n                self,\n                grads,\n                layer_id,\n                attn_qkvw,\n                attn_qkvb,\n                attn_ow,\n                attn_ob,\n                attn_nw,\n                attn_nb,\n                inter_w,\n                inter_b,\n                output_w,\n                output_b,\n                norm_w,\n                norm_b,\n                config):\n\n        bsz = input.shape[0]\n\n        if bsz > config.batch_size:\n            raise ValueError(\'Input batch size exceeds the limit.\')\n\n        cuda_module = ds_stochastic_transformer_cuda if config.stochastic_mode else ds_transformer_cuda\n        forward_func = cuda_module.forward_fp16 if config.fp16 else cuda_module.forward_fp32\n\n        (output,\n         inp_norm,\n         qkv_tf,\n         soft_inp,\n         ctx_bufB,\n         attn_o_inp,\n         add_res,\n         ff1_inp,\n         gelu_inp,\n         ff2_inp,\n         attn_prob_dropout_mask,\n         attn_output_dropout_mask,\n         layer_output_dropout_mask) = forward_func(config.layer_id,\n                                                   input,\n                                                   input_mask,\n                                                   attn_qkvw,\n                                                   attn_qkvb,\n                                                   attn_ow,\n                                                   attn_ob,\n                                                   attn_nw,\n                                                   attn_nb,\n                                                   inter_w,\n                                                   inter_b,\n                                                   output_w,\n                                                   output_b,\n                                                   norm_w,\n                                                   norm_b,\n                                                   config.training,\n                                                   config.pre_layer_norm,\n                                                   config.attn_dropout_checkpoint,\n                                                   config.normalize_invertible,\n                                                   config.gelu_checkpoint)\n\n        # For testing only.\n        if grads is not None:\n            for i in [2]:\n                attn_qkvw.register_hook(\n                    lambda x,\n                    i=i,\n                    self=self: grads.append([\n                        x[i * attn_ow.size(0):(i + 1) * attn_ow.size(0)],\n                        (""Q_W"" if i == 0 else ""K_W"" if i == 1 else ""V_W"")\n                    ]))\n            for i in [2]:\n                attn_qkvb.register_hook(\n                    lambda x,\n                    i=i,\n                    self=self: grads.append([\n                        x[i * attn_ow.size(0):(i + 1) * attn_ow.size(0)],\n                        (""Q_B"" if i == 0 else ""K_B"" if i == 1 else ""V_B"")\n                    ]))\n\n            attn_ow.register_hook(lambda x, self=self: grads.append([x, ""O_W""]))\n            attn_ob.register_hook(lambda x, self=self: grads.append([x, ""O_B""]))\n            attn_nw.register_hook(lambda x, self=self: grads.append([x, ""N2_W""]))\n            attn_nb.register_hook(lambda x, self=self: grads.append([x, ""N2_B""]))\n            inter_w.register_hook(lambda x, self=self: grads.append([x, ""int_W""]))\n            inter_b.register_hook(lambda x, self=self: grads.append([x, ""int_B""]))\n            output_w.register_hook(lambda x, self=self: grads.append([x, ""out_W""]))\n            output_b.register_hook(lambda x, self=self: grads.append([x, ""out_B""]))\n            norm_w.register_hook(lambda x, self=self: grads.append([x, ""norm_W""]))\n            norm_b.register_hook(lambda x, self=self: grads.append([x, ""norm_B""]))\n\n        if config.is_grad_enabled:\n            if (config.pre_layer_norm and config.normalize_invertible):\n                ctx.save_for_backward(input_mask,\n                                      attn_qkvw,\n                                      attn_qkvb,\n                                      attn_ow,\n                                      attn_ob,\n                                      attn_nw,\n                                      attn_nb,\n                                      inter_w,\n                                      inter_b,\n                                      output_w,\n                                      output_b,\n                                      norm_w,\n                                      norm_b)\n            else:\n                ctx.save_for_backward(output,\n                                      input,\n                                      input_mask,\n                                      attn_qkvw,\n                                      attn_qkvb,\n                                      attn_ow,\n                                      attn_ob,\n                                      attn_nw,\n                                      attn_nb,\n                                      inter_w,\n                                      inter_b,\n                                      output_w,\n                                      output_b,\n                                      norm_w,\n                                      norm_b)\n\n            ctx.config = config\n            if (config.pre_layer_norm or not config.normalize_invertible):\n                ctx.inp_norm = inp_norm\n\n            ctx.qkv_tf = qkv_tf\n            ctx.soft_inp = soft_inp\n            if not config.attn_dropout_checkpoint:\n                ctx.ctx_bufB = ctx_bufB\n\n            ctx.attn_o_inp = attn_o_inp\n            if not config.normalize_invertible:\n                ctx.add_res = add_res\n\n            ctx.ff1_inp = ff1_inp\n            if not config.gelu_checkpoint:\n                ctx.gelu_inp = gelu_inp\n\n            ctx.ff2_inp = ff2_inp\n            ctx.attn_prob_dropout_mask = attn_prob_dropout_mask\n            ctx.attn_output_dropout_mask = attn_output_dropout_mask\n            ctx.layer_output_dropout_mask = layer_output_dropout_mask\n\n        return output\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        bsz = grad_output.shape[0]\n\n        if bsz > ctx.config.batch_size:\n            raise ValueError(\'grad_output batch size exceeds the limit.\')\n\n        assert ctx.config.training\n\n        if (ctx.config.pre_layer_norm and ctx.config.normalize_invertible):\n            (input_mask,\n             attn_qkvw,\n             attn_qkvb,\n             attn_ow,\n             attn_ob,\n             attn_nw,\n             attn_nb,\n             inter_w,\n             inter_b,\n             output_w,\n             output_b,\n             norm_w,\n             norm_b) = ctx.saved_tensors\n        else:\n            (output,\n             input,\n             input_mask,\n             attn_qkvw,\n             attn_qkvb,\n             attn_ow,\n             attn_ob,\n             attn_nw,\n             attn_nb,\n             inter_w,\n             inter_b,\n             output_w,\n             output_b,\n             norm_w,\n             norm_b) = ctx.saved_tensors\n\n        cuda_module = ds_stochastic_transformer_cuda if ctx.config.stochastic_mode else ds_transformer_cuda\n        backward_func = cuda_module.backward_fp16 if ctx.config.fp16 else cuda_module.backward_fp32\n\n        (grad_input,\n         grad_attn_qkvw,\n         grad_attn_qkvb,\n         grad_attn_ow,\n         grad_attn_ob,\n         grad_attn_nw,\n         grad_attn_nb,\n         grad_inter_w,\n         grad_inter_b,\n         grad_output_w,\n         grad_output_b,\n         grad_norm_w,\n         grad_norm_b) = backward_func(\n             ctx.config.layer_id,\n             grad_output,\n             (ctx.inp_norm if (ctx.config.pre_layer_norm\n                               and ctx.config.normalize_invertible) else output),\n             (ctx.inp_norm if (ctx.config.pre_layer_norm\n                               or not ctx.config.normalize_invertible) else input),\n             ctx.qkv_tf,\n             ctx.soft_inp,\n             (ctx.soft_inp if ctx.config.attn_dropout_checkpoint else ctx.ctx_bufB),\n             ctx.attn_o_inp,\n             (ctx.ff1_inp if ctx.config.normalize_invertible else ctx.add_res),\n             ctx.ff1_inp,\n             (ctx.ff2_inp if ctx.config.gelu_checkpoint else ctx.gelu_inp),\n             ctx.ff2_inp,\n             ctx.attn_prob_dropout_mask,\n             ctx.attn_output_dropout_mask,\n             ctx.layer_output_dropout_mask,\n             (ctx.inp_norm if (ctx.config.pre_layer_norm\n                               and ctx.config.normalize_invertible) else input),\n             input_mask,\n             attn_qkvw,\n             attn_qkvb,\n             attn_ow,\n             attn_ob,\n             attn_nw,\n             attn_nb,\n             inter_w,\n             inter_b,\n             output_w,\n             output_b,\n             norm_w,\n             norm_b)\n\n        return (grad_input,\n                None,\n                None,\n                None,\n                None,\n                grad_attn_qkvw,\n                grad_attn_qkvb,\n                grad_attn_ow,\n                grad_attn_ob,\n                grad_attn_nw,\n                grad_attn_nb,\n                grad_inter_w,\n                grad_inter_b,\n                grad_output_w,\n                grad_output_b,\n                grad_norm_w,\n                grad_norm_b,\n                None)\n\n\nclass DeepSpeedTransformerLayer(nn.Module):\n    """"""Initialize the DeepSpeed Transformer Layer.\n\n        Arguments:\n            layer_id: The layer index starting from 0, e.g. if model has 24 transformer layers,\n                layer_id will be 0,1,2...23 when each layer object is instantiated\n\n            config: An object of DeepSpeedTransformerConfig\n\n            initial_weights: Optional: Only used for unit test\n\n            initial_biases: Optional: Only used for unit test\n    """"""\n    def __init__(self, layer_id, config, initial_weights=None, initial_biases=None):\n        super(DeepSpeedTransformerLayer, self).__init__()\n\n        self.config = config\n        self.config.layer_id = layer_id\n\n        print(""DeepSpeed Transformer config is "", self.config.__dict__)\n\n        if self.config.local_rank >= 0:\n            torch.cuda.set_device(self.config.local_rank)\n\n        if initial_weights is None and initial_biases is None:\n            self.attn_qkvw = nn.Parameter(\n                torch.Tensor(self.config.hidden_size * 3,\n                             self.config.hidden_size))\n            self.attn_qkvb = nn.Parameter(torch.Tensor(self.config.hidden_size * 3))\n            self.attn_ow = nn.Parameter(\n                torch.Tensor(self.config.hidden_size,\n                             self.config.hidden_size))\n            self.attn_ob = nn.Parameter(torch.Tensor(self.config.hidden_size))\n            self.attn_nw = nn.Parameter(torch.Tensor(self.config.hidden_size))\n            self.attn_nb = nn.Parameter(torch.Tensor(self.config.hidden_size))\n            self.inter_w = nn.Parameter(\n                torch.Tensor(4 * self.config.hidden_size,\n                             self.config.hidden_size))\n            self.inter_b = nn.Parameter(torch.Tensor(4 * self.config.hidden_size))\n            self.output_w = nn.Parameter(\n                torch.Tensor(self.config.hidden_size,\n                             4 * self.config.hidden_size))\n            self.output_b = nn.Parameter(torch.Tensor(self.config.hidden_size))\n            self.norm_w = nn.Parameter(torch.Tensor(self.config.hidden_size))\n            self.norm_b = nn.Parameter(torch.Tensor(self.config.hidden_size))\n            self.init_transformer_weights(self.config.adjust_init_range)\n        else:\n            # For testing only.\n            self.attn_qkvw = nn.Parameter(\n                torch.Tensor(self.config.hidden_size * 3,\n                             self.config.hidden_size))\n            for i in range(3):\n                self.attn_qkvw[i * self.config.hidden_size:(i + 1) * self.config.hidden_size] = \\\n                    torch.empty_like(initial_weights[i]).copy_(initial_weights[i])\n            self.attn_qkvb = nn.Parameter(torch.Tensor(self.config.hidden_size * 3))\n            self.attn_qkvb.data.zero_()\n            self.attn_ow = initial_weights[3]\n            self.attn_ob = initial_biases[3]\n            self.attn_nw = initial_weights[4]\n            self.attn_nb = initial_biases[4]\n            self.inter_w = initial_weights[5]\n            self.inter_b = initial_biases[5]\n            self.output_w = initial_weights[6]\n            self.output_b = initial_biases[6]\n            self.norm_w = initial_weights[7]\n            self.norm_b = initial_biases[7]\n\n        # create the layer in cuda kernels.\n        cuda_module = ds_stochastic_transformer_cuda if self.config.stochastic_mode else ds_transformer_cuda\n        create_layer_func = cuda_module.create_transformer_layer_fp16 if self.config.fp16 else cuda_module.create_transformer_layer_fp32\n\n        create_layer_func(self.config.layer_id,\n                          self.config.batch_size,\n                          self.config.hidden_size,\n                          self.config.heads,\n                          4 * self.config.hidden_size,\n                          self.config.max_seq_length,\n                          self.config.attn_dropout_ratio,\n                          self.config.hidden_dropout_ratio,\n                          self.config.seed,\n                          self.config.pre_layer_norm,\n                          self.config.test_gemm,\n                          self.config.attn_dropout_checkpoint,\n                          self.config.normalize_invertible,\n                          self.config.gelu_checkpoint,\n                          self.config.stochastic_mode)\n\n    def init_transformer_weights(self, adjust_init_range=False):\n        num_layers = self.config.num_hidden_layers\n        output_std = self.config.initializer_range\n        if adjust_init_range and self.config.local_rank == 0:\n            print(""Accounting for accumulation on the residual path"")\n            output_std = self.config.initializer_range / math.sqrt(2.0 * num_layers)\n\n        self.attn_qkvw.data.normal_(mean=0.0, std=self.config.initializer_range)\n        self.attn_qkvb.data.zero_()\n        self.attn_ow.data.normal_(mean=0.0, std=output_std)\n        self.attn_ob.data.zero_()\n        self.attn_nw.data.fill_(1.0)\n        self.attn_nb.data.zero_()\n        self.inter_w.data.normal_(mean=0.0, std=self.config.initializer_range)\n        self.inter_b.data.zero_()\n        self.output_w.data.normal_(mean=0.0, std=output_std)\n        self.output_b.data.zero_()\n        self.norm_w.data.fill_(1.0)\n        self.norm_b.data.zero_()\n\n    def forward(self, input, input_mask, grads=None):\n        self.config.training = self.training\n        self.config.is_grad_enabled = torch.is_grad_enabled()\n        return DeepSpeedTransformerFunction.apply(input,\n                                                  input_mask,\n                                                  self,\n                                                  grads,\n                                                  self.config.layer_id,\n                                                  self.attn_qkvw,\n                                                  self.attn_qkvb,\n                                                  self.attn_ow,\n                                                  self.attn_ob,\n                                                  self.attn_nw,\n                                                  self.attn_nb,\n                                                  self.inter_w,\n                                                  self.inter_b,\n                                                  self.output_w,\n                                                  self.output_b,\n                                                  self.norm_w,\n                                                  self.norm_b,\n                                                  self.config)\n'"
deepspeed/pt/deepspeed_dataloader.py,3,"b""'''\nCopyright 2019 The Microsoft DeepSpeed Team\n'''\n\nimport torch\nfrom torch.utils.data import DataLoader, RandomSampler\nfrom torch.utils.data.distributed import DistributedSampler\n\n\nclass DeepSpeedDataLoader(object):\n    def __init__(self,\n                 dataset,\n                 batch_size,\n                 pin_memory,\n                 local_rank,\n                 tput_timer,\n                 collate_fn=None,\n                 num_local_io_workers=None,\n                 data_sampler=None,\n                 data_parallel_world_size=None,\n                 data_parallel_rank=None):\n        self.tput_timer = tput_timer\n        self.batch_size = batch_size\n\n        if local_rank >= 0:\n            if data_sampler is None:\n                data_sampler = DistributedSampler(dataset=dataset,\n                                                  num_replicas=data_parallel_world_size,\n                                                  rank=data_parallel_rank)\n            device_count = 1\n        else:\n            if data_sampler is None:\n                data_sampler = RandomSampler(dataset)\n            device_count = torch.cuda.device_count()\n            batch_size *= device_count\n\n        if num_local_io_workers is None:\n            num_local_io_workers = 2 * device_count\n\n        self.num_local_io_workers = num_local_io_workers\n        self.data_sampler = data_sampler\n        self.dataset = dataset\n        self.collate_fn = collate_fn\n        self.device_count = device_count\n        self.batch_size = batch_size\n        self.pin_memory = pin_memory\n        self.len = len(self.data_sampler)\n        self.data = None\n\n    def __iter__(self):\n        self._create_dataloader()\n        return self\n\n    def __len__(self):\n        return self.len\n\n    def __next__(self):\n        if self.tput_timer:\n            self.tput_timer.start()\n        return next(self.data)\n\n    def _create_dataloader(self):\n        if self.collate_fn is None:\n            self.dataloader = DataLoader(self.dataset,\n                                         batch_size=self.batch_size,\n                                         pin_memory=self.pin_memory,\n                                         sampler=self.data_sampler,\n                                         num_workers=self.num_local_io_workers)\n        else:\n            self.dataloader = DataLoader(self.dataset,\n                                         batch_size=self.batch_size,\n                                         pin_memory=self.pin_memory,\n                                         sampler=self.data_sampler,\n                                         collate_fn=self.collate_fn,\n                                         num_workers=self.num_local_io_workers)\n        self.data = (x for x in self.dataloader)\n\n        return self.dataloader\n"""
deepspeed/pt/deepspeed_fused_lamb.py,7,"b'\'\'\'\nCopyright 2019 The Microsoft DeepSpeed Team\n\nCopyright NVIDIA/apex\nThis file is adapted from NVIDIA/apex/optimizer/fused_adam and implements the LAMB optimizer\n\n\'\'\'\nimport types\nimport torch\nimport importlib\n\n\nclass FusedLamb(torch.optim.Optimizer):\n    """"""Implements LAMB algorithm. Currently GPU-only.  Requires DeepSpeed adapted Apex to be installed via\n    ``python setup.py install --cuda_ext --cpp_ext``.\n\n    For usage example please see, TODO DeepSpeed Tutorial\n\n    It has been proposed in `Large Batch Optimization for Deep Learning: Training BERT in 76 minutes.\n    https://arxiv.org/abs/1904.00962\n\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups.\n        lr (float, optional): learning rate. (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square. (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability. (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        max_coeff(float, optional): maximum value of the lamb coefficient (default: 10.0)\n        min_coeff(float, optional): minimum value of the lamb coefficient (default: 0.01)\n        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n            algorithm from the paper `On the Convergence of Adam and Beyond`_\n            (default: False) NOT SUPPORTED in FusedAdam!\n        eps_inside_sqrt (boolean, optional): in the \'update parameters\' step,\n            adds eps to the bias-corrected second moment estimate before\n            evaluating square root instead of adding it to the square root of\n            second moment estimate as in the original paper. (default: False)\n\n    .. _Adam\\: A Method for Stochastic Optimization:\n        https://arxiv.org/abs/1412.6980\n    .. _On the Convergence of Adam and Beyond:\n        https://openreview.net/forum?id=ryQu7f-RZ\n    """"""\n    def __init__(self,\n                 params,\n                 lr=1e-3,\n                 bias_correction=True,\n                 betas=(0.9,\n                        0.999),\n                 eps=1e-8,\n                 eps_inside_sqrt=False,\n                 weight_decay=0.,\n                 max_grad_norm=0.,\n                 max_coeff=10.0,\n                 min_coeff=0.01,\n                 amsgrad=False):\n        global fused_lamb_cuda\n        fused_lamb_cuda = importlib.import_module(""deepspeed_lamb_cuda"")\n\n        if amsgrad:\n            raise RuntimeError(\'FusedLamb does not support the AMSGrad variant.\')\n        defaults = dict(lr=lr,\n                        bias_correction=bias_correction,\n                        betas=betas,\n                        eps=eps,\n                        weight_decay=weight_decay,\n                        max_grad_norm=max_grad_norm,\n                        max_coeff=max_coeff,\n                        min_coeff=min_coeff)\n        super(FusedLamb, self).__init__(params, defaults)\n        self.eps_mode = 0 if eps_inside_sqrt else 1\n        self.lamb_coeffs = []\n\n    def step(self,\n             closure=None,\n             grads=None,\n             output_params=None,\n             scale=1.,\n             grad_norms=None):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n            grads (list of tensors, optional): weight gradient to use for the\n                optimizer update. If gradients have type torch.half, parameters\n                are expected to be in type torch.float. (default: None)\n            output params (list of tensors, optional): A reduced precision copy\n                of the updated weights written out in addition to the regular\n                updated weights. Have to be of same type as gradients. (default: None)\n            scale (float, optional): factor to divide gradient tensor values\n                by before applying to weights. (default: 1)\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        if grads is None:\n            grads_group = [None] * len(self.param_groups)\n        # backward compatibility\n        # assuming a list/generator of parameter means single group\n        elif isinstance(grads, types.GeneratorType):\n            grads_group = [grads]\n        elif type(grads[0]) != list:\n            grads_group = [grads]\n        else:\n            grads_group = grads\n\n        if output_params is None:\n            output_params_group = [None] * len(self.param_groups)\n        elif isinstance(output_params, types.GeneratorType):\n            output_params_group = [output_params]\n        elif type(output_params[0]) != list:\n            output_params_group = [output_params]\n        else:\n            output_params_group = output_params\n\n        if grad_norms is None:\n            grad_norms = [None] * len(self.param_groups)\n\n        #remove the previous coeffs\n        del self.lamb_coeffs[:]\n\n        for group, grads_this_group, output_params_this_group, grad_norm_group in zip(self.param_groups, grads_group, output_params_group, grad_norms):\n            if grads_this_group is None:\n                grads_this_group = [None] * len(group[\'params\'])\n            if output_params_this_group is None:\n                output_params_this_group = [None] * len(group[\'params\'])\n\n            if grad_norm_group is None:\n                grad_norm_group = [None] * len(group[\'params\'])\n            elif not isinstance(grad_norm_group, list):\n                grad_norm_group = [grad_norm_group]\n\n            bias_correction = 1 if group[\'bias_correction\'] else 0\n\n            for p, grad, output_param, grad_norm in zip(group[\'params\'], grads_this_group, output_params_this_group, grad_norm_group):\n\n                # compute combined scale factor for this group\n                combined_scale = scale\n                if group[\'max_grad_norm\'] > 0:\n                    # norm is in fact norm*scale\n                    clip = ((grad_norm / scale) + 1e-6) / group[\'max_grad_norm\']\n                    if clip > 1:\n                        combined_scale = clip * scale\n\n                #note: p.grad should not ever be set for correct operation of mixed precision optimizer that sometimes sends None gradients\n                if p.grad is None and grad is None:\n                    continue\n                if grad is None:\n                    grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        \'FusedAdam does not support sparse gradients, please consider SparseAdam instead\'\n                    )\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'exp_avg\'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n                max_coeff = group[\'max_coeff\']\n                min_coeff = group[\'min_coeff\']\n\n                state[\'step\'] += 1\n\n                out_p = torch.tensor(\n                    [],\n                    dtype=torch.float) if output_param is None else output_param\n                lamb_coeff = fused_lamb_cuda.lamb(p.data,\n                                                  out_p,\n                                                  exp_avg,\n                                                  exp_avg_sq,\n                                                  grad,\n                                                  group[\'lr\'],\n                                                  beta1,\n                                                  beta2,\n                                                  max_coeff,\n                                                  min_coeff,\n                                                  group[\'eps\'],\n                                                  combined_scale,\n                                                  state[\'step\'],\n                                                  self.eps_mode,\n                                                  bias_correction,\n                                                  group[\'weight_decay\'])\n                self.lamb_coeffs.append(lamb_coeff)\n        return loss\n\n    def get_lamb_coeffs(self):\n        lamb_coeffs = [lamb_coeff.item() for lamb_coeff in self.lamb_coeffs]\n        return lamb_coeffs\n'"
deepspeed/pt/deepspeed_launch.py,0,"b'""""""\nCopyright 2020 The Microsoft DeepSpeed Team: deepspeed@microsoft.com\n""""""\n\nimport sys\nimport subprocess\nimport os\nimport json\nimport base64\nfrom collections import defaultdict\nfrom argparse import ArgumentParser, REMAINDER\n\nfrom deepspeed.pt.log_utils import logger\n\n\ndef parse_args():\n    parser = ArgumentParser(description=""DeepSpeed distributed training launch""\n                            "" utility that creates multiple distributed""\n                            "" processes on a single node"")\n\n    # Optional arguments for the launch helper\n    parser.add_argument(""--node_rank"",\n                        type=int,\n                        default=0,\n                        help=""The rank of the node for multi-node distributed ""\n                        ""training"")\n    parser.add_argument(""--master_addr"",\n                        default=""127.0.0.1"",\n                        type=str,\n                        help=""Master node (rank 0)\'s address, should be either""\n                        "" the IP address or the hostname of node 0, for""\n                        "" single node multi-proc training, the""\n                        "" --master_addr can simply be 127.0.0.1"")\n    parser.add_argument(""--master_port"",\n                        default=29500,\n                        type=int,\n                        help=""Master node (rank 0)\'s free port that needs to ""\n                        ""be used for communication during distributed ""\n                        ""training"")\n    parser.add_argument(""--world_info"",\n                        default=""None"",\n                        type=str,\n                        help=""world info base64 encoded dictionary"")\n\n    # positional\n    parser.add_argument(""training_script"",\n                        type=str,\n                        help=""The full path to the single GPU training ""\n                        ""program/script to be launched in parallel, ""\n                        ""followed by all the arguments for the ""\n                        ""training script"")\n\n    # rest from the training program\n    parser.add_argument(\'training_script_args\', nargs=REMAINDER)\n    return parser.parse_args()\n\n\ndef main():\n    args = parse_args()\n    current_env = os.environ.copy()\n\n    for k in current_env.keys():\n        if ""NCCL"" in k:\n            logger.info(""%s %s %s"", args.node_rank, k, current_env[k])\n\n    world_info = None\n    assert args.world_info != ""None"", ""must provide world info dict""\n    world_info = base64.urlsafe_b64decode(args.world_info)\n    world_info = json.loads(world_info)\n\n    logger.info(""WORLD INFO DICT: {}"".format(world_info))\n    node_list = list(world_info.keys())\n    args.nnodes = len(node_list)\n    local_node = node_list[args.node_rank]\n    local_gpu_ids = world_info[local_node]\n    num_local_procs = len(local_gpu_ids)\n    logger.info(\n        ""nnodes={}, num_local_procs={}, node_rank={}"".format(args.nnodes,\n                                                             num_local_procs,\n                                                             args.node_rank),\n    )\n\n    global_rank_mapping = defaultdict(list)\n    curr_global_rank = 0\n    dist_world_size = 0\n    for node_id in node_list:\n        gids = world_info[node_id]\n        dist_world_size += len(gids)\n        for gid in gids:\n            global_rank_mapping[node_id].append(curr_global_rank)\n            curr_global_rank += 1\n    logger.info(""global_rank_mapping={}"".format(global_rank_mapping))\n    logger.info(""dist_world_size={}"".format(dist_world_size))\n    current_env[""CUDA_VISIBLE_DEVICES""] = "","".join(map(str, local_gpu_ids))\n    logger.info(""Setting CUDA_VISIBLE_DEVICES={}"".format(\n        current_env[""CUDA_VISIBLE_DEVICES""]))\n    exclusion_counts_per_node = None\n\n    # set PyTorch distributed related environmental variables\n    current_env[""MASTER_ADDR""] = args.master_addr\n    current_env[""MASTER_PORT""] = str(args.master_port)\n    current_env[""WORLD_SIZE""] = str(dist_world_size)\n\n    processes = []\n    for local_rank in range(0, num_local_procs):\n        # each process\'s rank\n        dist_rank = global_rank_mapping[local_node][local_rank]\n        current_env[""RANK""] = str(dist_rank)\n\n        # spawn the processes\n        cmd = [\n            sys.executable,\n            ""-u"",\n            args.training_script,\n            ""--local_rank={}"".format(local_rank)\n        ] + args.training_script_args\n        process = subprocess.Popen(cmd, env=current_env)\n        processes.append(process)\n\n    for process in processes:\n        process.wait()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
deepspeed/pt/deepspeed_light.py,36,"b'\'\'\'\nCopyright 2019 The Microsoft DeepSpeed Team\n\'\'\'\n\nimport torch\nimport os\nimport warnings\nimport torch.distributed as dist\nfrom torch.nn.modules import Module\nfrom torch.distributed.distributed_c10d import _get_global_rank\n\nfrom tensorboardX import SummaryWriter\n\nfrom deepspeed.pt.deepspeed_timer import ThroughputTimer, SynchronizedWallClockTimer\nfrom deepspeed.pt.deepspeed_zero_optimizer import FP16_DeepSpeedZeroOptimizer\nfrom deepspeed.pt.zero_optimizer_stage1 import FP16_DeepSpeedZeroOptimizer_Stage1\nfrom deepspeed.pt.log_utils import logger\nimport deepspeed.pt.deepspeed_checkpointing as deepspeed_activation_checkpointing\n\nfrom deepspeed.pt.fp16_optimizer import FP16_Optimizer\nfrom deepspeed.pt.fp16_unfused_optimizer import FP16_UnfusedOptimizer\nfrom deepspeed.pt.deepspeed_fused_lamb import FusedLamb\nfrom deepspeed.pt.deepspeed_config import DeepSpeedConfig, \\\n    ADAM_OPTIMIZER, LAMB_OPTIMIZER, DEEPSPEED_OPTIMIZERS\n\nfrom deepspeed.pt.deepspeed_dataloader import DeepSpeedDataLoader\nfrom deepspeed.pt.deepspeed_constants import \\\n    ROUTE_TRAIN, ROUTE_PREDICT, ROUTE_EVAL, \\\n    TORCH_DISTRIBUTED_DEFAULT_PORT, \\\n    ZERO_OPTIMIZATION_OPTIMIZER_STATES, ZERO_OPTIMIZATION_GRADIENTS\n\nimport deepspeed.pt.deepspeed_lr_schedules as lr_schedules\nfrom deepspeed.pt.deepspeed_csr_tensor import CSRTensor\n\nMEMORY_OPT_ALLREDUCE_SIZE = 500000000\nSUMMARY_WRITER_DIR_NAME = ""JobId""\n\ntry:\n    from apex_C import flatten\n    from apex_C import unflatten\nexcept ImportError:\n    try:\n        _ = warned_flatten\n    except NameError:\n        logger.warning(\n            ""Warning:  apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.""\n        )\n        warned_flatten = True\n    from torch._utils import _flatten_dense_tensors as flatten\n    from torch._utils import _unflatten_dense_tensors as unflatten\n\n\ndef split_half_float_double_csr(tensors):\n    dtypes = [\n        ""torch.cuda.HalfTensor"",\n        ""torch.cuda.FloatTensor"",\n        ""torch.cuda.DoubleTensor"",\n        CSRTensor.type()\n    ]\n    buckets = []\n    for i, dtype in enumerate(dtypes):\n        bucket = [t for t in tensors if t.type() == dtype]\n        if bucket:\n            buckets.append((dtype, bucket))\n    return buckets\n\n\ndef _initialize_parameter_parallel_groups(parameter_parallel_size=None):\n    data_parallel_size = int(dist.get_world_size())\n    if parameter_parallel_size is None:\n        parameter_parallel_size = int(data_parallel_size)\n    logger.info(""data_parallel_size: %s, parameter_parallel_size: %s"",\n                data_parallel_size,\n                parameter_parallel_size)\n    assert data_parallel_size % parameter_parallel_size == 0, \\\n        \'world size should be divisible by parameter parallel size\'\n    rank = dist.get_rank()\n    my_group = None\n    for i in range(dist.get_world_size() // parameter_parallel_size):\n        ranks = range(i * parameter_parallel_size, (i + 1) * parameter_parallel_size)\n        group = torch.distributed.new_group(ranks)\n        if rank in ranks:\n            my_group = group\n    return my_group\n\n\ndef print_configuration(args, name):\n    logger.info(\'{}:\'.format(name))\n    for arg in sorted(vars(args)):\n        dots = \'.\' * (29 - len(arg))\n        logger.info(\'  {} {} {}\'.format(arg, dots, getattr(args, arg)))\n\n\nclass DeepSpeedLight(Module):\n    r""""""DeepSpeed engine for training.\n    """"""\n    def __init__(self,\n                 args,\n                 model,\n                 optimizer=None,\n                 model_parameters=None,\n                 training_data=None,\n                 lr_scheduler=None,\n                 mpu=None,\n                 dist_init_required=None,\n                 collate_fn=None,\n                 config_params=None):\n        super(DeepSpeedLight, self).__init__()\n\n        self.client_optimizer = optimizer\n        self.client_model_parameters = model_parameters\n        self.client_lr_scheduler = lr_scheduler\n        self.training_data = training_data\n        self.collate_fn = collate_fn\n        self.mpu = mpu\n        self.data_parallel_group = None\n        self.global_steps = 0\n        self.micro_steps = 0\n        self.skipped_steps = 0\n        self.gradient_average = True\n        self.warn_unscaled_loss = True\n        self.config_params = config_params\n\n        if dist_init_required is None:\n            dist_init_required = not dist.is_initialized()\n\n        self._mpi_check(args, dist_init_required)\n\n        self.dist_backend = ""nccl""\n        if dist_init_required:\n            if not dist.is_initialized():\n                logger.info(""Initializing torch distributed with backend: {}"".format(\n                    self.dist_backend))\n                dist.init_process_group(backend=self.dist_backend)\n            else:\n                logger.warning(\n                    ""Was given dist_init_required=True but detected that torch""\n                    ""distributed was already initialized, cannot initialize twice."")\n\n        self._do_args_sanity_check(args)\n        self._configure_with_arguments(args, mpu)\n        self._do_sanity_check()\n\n        self.sample_count = 0\n        if self.tensorboard_enabled():\n            self.summary_writer = self.get_summary_writer()\n\n        self._init_distributed(dist_init_required)\n\n        # Configure distributed model\n        self._configure_distributed_model(model)\n\n        # Configure wall clock timer\n        self.timers = SynchronizedWallClockTimer()\n\n        # Throughput timer\n        self.tput_timer = ThroughputTimer(\n            batch_size=self.train_micro_batch_size_per_gpu(),\n            num_workers=self.dp_world_size,\n            monitor_memory=False)\n\n        self.training_dataloader = self.deepspeed_io(\n            training_data) if training_data else None\n\n        # Configure optimizer and scheduler\n        self.optimizer = None\n        self.lr_scheduler = None\n        if model_parameters or optimizer:\n            self._configure_optimizer(optimizer, model_parameters)\n            self._configure_lr_scheduler(lr_scheduler)\n            self._report_progress(0)\n\n        # Bookkeeping for csr support\n        self.csr_tensor_module_names = set()\n        if self.sparse_gradients_enabled():\n            for name, module in self.module.named_modules():\n                if isinstance(module, torch.nn.Embedding):\n                    self.csr_tensor_module_names.add(name + "".weight"")\n                    logger.info(""Will convert {} to sparse (csr) ""\n                                ""tensor during training"".format(name))\n\n        self.save_non_zero_checkpoint = False\n        self.save_zero_checkpoint = False\n        self._configure_checkpointing(dist_init_required)\n\n        if self.global_rank == 0:\n            self._config.print(\'DeepSpeedLight configuration\')\n            if self.dump_state():\n                print_configuration(self, \'DeepSpeedLight\')\n\n    def _mpi_check(self, args, dist_init_required):\n        if hasattr(args, \'deepspeed_mpi\') and args.deepspeed_mpi:\n            from mpi4py import MPI\n            import subprocess\n            comm = MPI.COMM_WORLD\n            rank = comm.Get_rank()\n            world_size = comm.Get_size()\n\n            master_addr = None\n            if rank == 0:\n                hostname_cmd = [""hostname -I""]\n                result = subprocess.check_output(hostname_cmd, shell=True)\n                master_addr = result.decode(\'utf-8\').split()[0]\n            master_addr = comm.bcast(master_addr, root=0)\n\n            # Determine local rank by assuming hostnames are unique\n            proc_name = MPI.Get_processor_name()\n            all_procs = comm.allgather(proc_name)\n            local_rank = sum([i == proc_name for i in all_procs[:rank]])\n\n            os.environ[\'RANK\'] = str(rank)\n            os.environ[\'WORLD_SIZE\'] = str(world_size)\n            args.local_rank = local_rank\n            os.environ[\'MASTER_ADDR\'] = master_addr\n            os.environ[\'MASTER_PORT\'] = TORCH_DISTRIBUTED_DEFAULT_PORT\n\n            logger.info(\n                ""Discovered MPI settings of world_rank={}, local_rank={}, world_size={}, master_addr={}, master_port={}""\n                .format(os.environ[\'RANK\'],\n                        args.local_rank,\n                        os.environ[\'WORLD_SIZE\'],\n                        os.environ[\'MASTER_ADDR\'],\n                        os.environ[\'MASTER_PORT\']))\n\n            if not dist_init_required and dist.is_initialized():\n                assert dist.get_rank() == rank, ""MPI rank {} does not match torch rank {}"".format(rank, dist.get_rank())\n                assert dist.get_world_size() == world_size, ""MPI world size {} does not match torch world size {}"".format(world_size, dist.get_world_size())\n\n    def tensorboard_enabled(self):\n        return self._config.tensorboard_enabled\n\n    def tensorboard_output_path(self):\n        return self._config.tensorboard_output_path\n\n    def tensorboard_job_name(self):\n        return self._config.tensorboard_job_name\n\n    def get_summary_writer(self,\n                           name=""DeepSpeedJobName"",\n                           base=os.environ[""HOME""] + ""/tensorboard""):\n        if self.tensorboard_job_name():\n            name = self.tensorboard_job_name()\n        if self.tensorboard_output_path():\n            return SummaryWriter(log_dir=self.tensorboard_output_path())\n        if \'DLWS_JOB_ID\' in os.environ:\n            SUMMARY_WRITER_DIR_NAME = os.environ[\'DLWS_JOB_ID\'] + ""/logs""\n        return SummaryWriter(log_dir=os.path.join(base, SUMMARY_WRITER_DIR_NAME, name))\n\n    def wall_clock_breakdown(self):\n        return self._config.wall_clock_breakdown\n\n    def memory_breakdown(self):\n        return self._config.memory_breakdown\n\n    def sparse_gradients_enabled(self):\n        return self._config.sparse_gradients_enabled\n\n    def train_batch_size(self):\n        return self._config.train_batch_size\n\n    def train_micro_batch_size_per_gpu(self):\n        return self._config.train_micro_batch_size_per_gpu\n\n    def optimizer_name(self):\n        return self._config.optimizer_name\n\n    def optimizer_params(self):\n        return self._config.optimizer_params\n\n    def optimizer_legacy_fusion(self):\n        return self._config.optimizer_legacy_fusion\n\n    def scheduler_name(self):\n        return self._config.scheduler_name\n\n    def scheduler_params(self):\n        return self._config.scheduler_params\n\n    def zero_optimization(self):\n        return self._config.zero_enabled\n\n    def zero_allow_untested_optimizer(self):\n        return self._config.zero_allow_untested_optimizer\n\n    def zero_reduce_scatter(self):\n        return self._config.zero_config.reduce_scatter\n\n    def zero_overlap_comm(self):\n        return self._config.zero_config.overlap_comm\n\n    def zero_max_elements_per_comm(self):\n        return self._config.zero_max_elements_per_comm\n\n    def zero_optimization_stage(self):\n        return self._config.zero_optimization_stage\n\n    def zero_reduce_bucket_size(self):\n        return self._config.zero_config.reduce_bucket_size\n\n    def zero_allgather_bucket_size(self):\n        return self._config.zero_config.allgather_bucket_size\n\n    def zero_optimization_partition_gradients(self):\n        return self.zero_optimization_stage() >= ZERO_OPTIMIZATION_GRADIENTS\n\n    def zero_contiguous_gradients(self):\n        return self._config.zero_config.contiguous_gradients\n\n    def allgather_size(self):\n        return self._config.allgather_size\n\n    def fp16_enabled(self):\n        return self._config.fp16_enabled\n\n    def loss_scale(self):\n        return self._config.loss_scale\n\n    def gradient_accumulation_steps(self):\n        return self._config.gradient_accumulation_steps\n\n    def allreduce_always_fp32(self):\n        return self._config.allreduce_always_fp32\n\n    def postscale_gradients(self):\n        return not self._config.prescale_gradients\n\n    def gradient_predivide_factor(self):\n        return self._config.gradient_predivide_factor\n\n    def steps_per_print(self):\n        return self._config.steps_per_print\n\n    def zero_allgather_partitions(self):\n        return self._config.zero_config.allgather_partitions\n\n    def dump_state(self):\n        return self._config.dump_state\n\n    def gradient_clipping(self):\n        return self._config.gradient_clipping\n\n    def dynamic_loss_scale(self):\n        return self._config.loss_scale == 0\n\n    def initial_dynamic_scale(self):\n        return self._config.initial_dynamic_scale\n\n    def dynamic_loss_scale_args(self):\n        return self._config.dynamic_loss_scale_args\n\n    def _configure_lr_scheduler(self, client_lr_scheduler):\n        # First check for scheduler in json configuration\n        lr_scheduler = self._scheduler_from_config(self.optimizer)\n        if lr_scheduler:\n            logger.info(\n                f\'DeepSpeed using configured LR scheduler = {self.scheduler_name()}\')\n            self.lr_scheduler = lr_scheduler\n        else:\n            logger.warning(\'DeepSpeed using client LR scheduler\')\n            self.lr_scheduler = client_lr_scheduler\n        logger.info(f\'DeepSpeed LR Scheduler = {self.lr_scheduler}\')\n\n    def _configure_checkpointing(self, dist_init_required):\n\n        dp_rank = self.global_rank\n        if self.mpu:\n            dp_rank = self.mpu.get_data_parallel_rank()\n\n        #only the first data parallel process needs to store the model checkpoint\n        self.save_non_zero_checkpoint = (dp_rank == 0)\n\n        if self.zero_optimization():\n            pp_rank = torch.distributed.get_rank(group=self.optimizer.dp_process_group)\n\n            # Only the first parameter parallel process needs to store the\n            # optimizer state checkpoints for zero\n            self.save_zero_checkpoint = (pp_rank == dp_rank)\n\n    def _scheduler_from_config(self, optimizer):\n        scheduler_name = self.scheduler_name()\n        if scheduler_name is not None:\n            if hasattr(lr_schedules, scheduler_name):\n                scheduler = getattr(lr_schedules, scheduler_name)\n            else:\n                assert hasattr(torch.optim.lr_scheduler, scheduler_name), \\\n                    f""DeepSpeed does not recognize LR scheduler {scheduler_name}""\n\n                scheduler = getattr(torch.optim.lr_scheduler, scheduler_name)\n\n            scheduler_params = self.scheduler_params()\n            instantiated_scheduler = scheduler(optimizer, **scheduler_params)\n            return instantiated_scheduler\n        else:\n            return None\n\n    def _init_distributed(self, dist_init_required):\n        if self.local_rank >= 0:\n            torch.cuda.set_device(self.local_rank)\n            self.device = torch.device(""cuda"", self.local_rank)\n            self.world_size = dist.get_world_size()\n            self.global_rank = dist.get_rank()\n            logger.info(""Set device to local rank {} within node."".format(\n                self.local_rank))\n        else:\n            self.world_size = 1\n            self.global_rank = 0\n            self.device = torch.device(""cuda"")\n\n    # Configure based on command line arguments\n    def _configure_with_arguments(self, args, mpu):\n        self.local_rank = args.local_rank if hasattr(args, \'local_rank\') else 0\n        self._config = DeepSpeedConfig(args.deepspeed_config,\n                                       mpu,\n                                       param_dict=self.config_params)\n\n    # Validate command line arguments\n    def _do_args_sanity_check(self, args):\n        if hasattr(args, \'deepscale_config\') and args.deepscale_config is not None:\n            logger.warning(\n                ""************ --deepscale_config is deprecated, please use --deepspeed_config ************""\n            )\n            if hasattr(args, \'deepspeed_config\'):\n                assert args.deepspeed_config is None, ""Not sure how to proceed, we were given both a deepscale_config and deepspeed_config""\n            args.deepspeed_config = args.deepscale_config\n\n        assert hasattr(args, \'local_rank\') and type(args.local_rank) == int, \\\n            \'DeepSpeed requires integer command line parameter --local_rank\'\n\n        if self.config_params is None:\n            assert hasattr(args, \'deepspeed_config\') and args.deepspeed_config is not None, \\\n                \'DeepSpeed requires --deepspeed_config to specify configuration file\'\n\n            assert os.path.isfile(args.deepspeed_config), \\\n                \'DeepSpeed configuration file: {} is not an existing file\'.format(args.deepspeed_config)\n\n    def _is_supported_optimizer(self, optimizer_name):\n        return optimizer_name in DEEPSPEED_OPTIMIZERS or \\\n            getattr(torch.optim, optimizer_name, None) is not None\n\n    # Validate configuration based on command line arguments\n    def _do_sanity_check(self):\n        if not self.client_optimizer:\n            assert self._is_supported_optimizer(self.optimizer_name()), \\\n                \'{} is not a supported DeepSpeed Optimizer\'.format(self.optimizer_name())\n            assert self.client_model_parameters, \\\n                \'DeepSpeed {} optimizer requires parameters in initialize() call\'.format(self.optimizer_name())\n\n        if self.optimizer_name() == LAMB_OPTIMIZER:\n            assert self.dynamic_loss_scale(), \\\n                \'DeepSpeed {} optimizer requires dynamic loss scaling\'.format(self.optimizer_name())\n\n    def _configure_distributed_model(self, model):\n        self.module = model\n        if self.fp16_enabled():\n            self.module.half()\n        self.module.to(self.device)\n        if self.mpu is None:\n            self.data_parallel_group = _initialize_parameter_parallel_groups()\n            self.dp_world_size = dist.get_world_size()\n            src_rank = 0\n        else:\n            self.data_parallel_group = self.mpu.get_data_parallel_group()\n            self.dp_world_size = self.mpu.get_data_parallel_world_size()\n            src_rank = _get_global_rank(self.mpu.get_data_parallel_group(), 0)\n            logger.info(f""global src_rank={src_rank}"")\n        for p in self.module.parameters():\n            if torch.is_tensor(p):\n                dist.broadcast(p, src_rank, group=self.data_parallel_group)\n\n        # TODO: support new AMP optimizer\n        # self.module.half()\n        # self.module.to(self.local_rank)\n        #self.module, self.optimizer = amp.initialize(self.module, self.optimizer, opt_level=""O2"")\n\n    # Configure optimizer\n    def _configure_optimizer(self, client_optimizer, model_parameters):\n        if client_optimizer is not None:\n            basic_optimizer = client_optimizer\n            logger.info(\'Using client Optimizer as basic optimizer\')\n        else:\n            basic_optimizer = self._configure_basic_optimizer(model_parameters)\n            logger.info(\n                \'Using DeepSpeed Optimizer param name {} as basic optimizer\'.format(\n                    self.optimizer_name()))\n\n        logger.info(\'DeepSpeed Basic Optimizer = {}\'.format(basic_optimizer))\n\n        if self.zero_optimization():\n            if self.optimizer_name() != ADAM_OPTIMIZER:\n                assert self.zero_allow_untested_optimizer(), \\\n                \'You are using an untested ZeRO Optimizer. Please add <""zero_allow_untested_optimizer"": true> in the configuration file to use it.\'\n\n                logger.warning(\n                    ""**** You are using ZeRO with an untested optimizer, proceed with caution *****""\n                )\n            self.optimizer = self._configure_zero_optimizer(basic_optimizer)\n        elif self.fp16_enabled():\n            self.optimizer = self._configure_fp16_optimizer(basic_optimizer)\n        else:\n            self.optimizer = basic_optimizer\n\n        # logger.info(\'DeepSpeed Final Optimizer = {}\'.format(self.optimizer.state_dict()))\n\n    def _configure_basic_optimizer(self, model_parameters):\n        optimizer_parameters = self.optimizer_params()\n        if \'max_grad_norm\' in optimizer_parameters.keys():\n            raise ValueError(\n                ""\'max_grad_norm\' is not supported as an optimizer parameter, please switch to using the deepspeed parameter \'gradient_clipping\' see: https://www.deepspeed.ai/docs/config-json/#gradient-clipping for more details""\n            )\n        if self.optimizer_name() == ADAM_OPTIMIZER:\n            from apex.optimizers.fused_adam import FusedAdam\n            optimizer = FusedAdam(model_parameters, **optimizer_parameters)\n        elif self.optimizer_name() == LAMB_OPTIMIZER:\n            optimizer = FusedLamb(model_parameters, **optimizer_parameters)\n        else:\n            torch_optimizer = getattr(torch.optim, self.optimizer_name())\n            optimizer = torch_optimizer(model_parameters, **optimizer_parameters)\n        return optimizer\n\n    def _configure_fp16_optimizer(self, optimizer):\n        initial_dynamic_scale = self.initial_dynamic_scale()\n        dynamic_loss_args = self.dynamic_loss_scale_args()\n        clip_grad = self.gradient_clipping()\n        if self.optimizer_name() == ADAM_OPTIMIZER:\n            if self.dynamic_loss_scale():\n                logger.info(\'Creating fp16 optimizer with dynamic loss scale\')\n                timers = self.timers if self.wall_clock_breakdown() else None\n                optimizer = FP16_Optimizer(\n                    optimizer,\n                    dynamic_loss_scale=True,\n                    initial_dynamic_scale=initial_dynamic_scale,\n                    dynamic_loss_args=dynamic_loss_args,\n                    mpu=self.mpu,\n                    clip_grad=clip_grad,\n                    fused_adam_legacy=self.optimizer_legacy_fusion(),\n                    timers=timers)\n            else:\n                logger.info(\'Creating fp16 optimizer with static loss scale: {}\'.format(\n                    self.loss_scale()))\n                optimizer = FP16_Optimizer(\n                    optimizer,\n                    static_loss_scale=self.loss_scale(),\n                    mpu=self.mpu,\n                    clip_grad=clip_grad,\n                    fused_adam_legacy=self.optimizer_legacy_fusion())\n        else:\n            logger.info(\'Creating fp16 unfused optimizer with dynamic loss scale\')\n            optimizer = FP16_UnfusedOptimizer(\n                optimizer,\n                dynamic_loss_scale=self.dynamic_loss_scale(),\n                dynamic_loss_args=dynamic_loss_args,\n                mpu=self.mpu,\n                clip_grad=clip_grad,\n                fused_lamb_legacy=self.optimizer_name() == LAMB_OPTIMIZER)\n\n        return optimizer\n\n    def _configure_zero_optimizer(self, optimizer):\n        zero_stage = self.zero_optimization_stage()\n        logger.info(\'Creating fp16 ZeRO stage {} optimizer\'.format(zero_stage))\n\n        if zero_stage == ZERO_OPTIMIZATION_OPTIMIZER_STATES:\n            assert self.zero_reduce_scatter(), \'Stage 1 only supports reduce scatter mode\'\n            logger.info(\'Creating fp16 ZeRO Optimizer Stage 1\')\n            optimizer = FP16_DeepSpeedZeroOptimizer_Stage1(\n                optimizer,\n                static_loss_scale=self.loss_scale(),\n                dynamic_loss_scale=self.dynamic_loss_scale(),\n                dynamic_loss_args=self.dynamic_loss_scale_args(),\n                clip_grad=self.gradient_clipping(),\n                all_gather_partitions=self.zero_allgather_partitions(),\n                allgather_size=self.zero_allgather_bucket_size(),\n                max_elements_per_comm=self.zero_reduce_bucket_size(),\n                dp_process_group=self.data_parallel_group,\n                mpu=self.mpu)\n        elif zero_stage == ZERO_OPTIMIZATION_GRADIENTS:\n            assert self.gradient_accumulation_steps() == 1, ""ZeRO stage 2 does not support gradient accumulation, if you need gradient accumulation please use stage 1""\n            optimizer = FP16_DeepSpeedZeroOptimizer(\n                optimizer,\n                timers=self.timers,\n                static_loss_scale=self.loss_scale(),\n                dynamic_loss_scale=self.dynamic_loss_scale(),\n                dynamic_loss_args=self.dynamic_loss_scale_args(),\n                clip_grad=self.gradient_clipping(),\n                contiguous_gradients=self.zero_contiguous_gradients(),\n                reduce_bucket_size=self.zero_reduce_bucket_size(),\n                allgather_bucket_size=self.zero_allgather_bucket_size(),\n                dp_process_group=self.data_parallel_group,\n                reduce_scatter=self.zero_reduce_scatter(),\n                overlap_comm=self.zero_overlap_comm(),\n                mpu=self.mpu,\n                postscale_gradients=self.postscale_gradients(),\n                gradient_predivide_factor=self.gradient_predivide_factor())\n        else:\n            raise NotImplementedError(""ZeRO stage {} not implemented"".format(zero_stage))\n        logger.info(\'Creating fp16 zero stage {} optimizer\'.format(zero_stage))\n\n        return optimizer\n\n    def deepspeed_io(self,\n                     dataset,\n                     batch_size=None,\n                     route=ROUTE_TRAIN,\n                     pin_memory=True,\n                     data_sampler=None,\n                     collate_fn=None,\n                     num_local_io_workers=None):\n        if not isinstance(dataset, torch.utils.data.Dataset):\n            raise ValueError(""Training data must be a torch Dataset"")\n\n        if data_sampler is None and (route == ROUTE_PREDICT or route == ROUTE_EVAL):\n            data_sampler = torch.utils.data.SequentialSampler(dataset)\n\n        if batch_size is None:\n            batch_size = self.train_micro_batch_size_per_gpu()\n\n        if collate_fn is None:\n            collate_fn = self.collate_fn\n\n        # Currently we only use timer in train route\n        deepspeed_io_timer = None\n        if route == ROUTE_TRAIN:\n            deepspeed_io_timer = self.tput_timer\n\n        # If mpu is provied, forward world size and parallel rank to sampler.\n        data_parallel_world_size = None\n        data_parallel_rank = None\n        if self.mpu is not None:\n            data_parallel_world_size = mpu.get_data_parallel_world_size()\n            data_parallel_rank = mpu.get_data_parallel_rank()\n\n        return DeepSpeedDataLoader(dataset=dataset,\n                                   batch_size=batch_size,\n                                   pin_memory=pin_memory,\n                                   collate_fn=collate_fn,\n                                   local_rank=self.local_rank,\n                                   tput_timer=deepspeed_io_timer,\n                                   num_local_io_workers=num_local_io_workers,\n                                   data_sampler=data_sampler,\n                                   data_parallel_world_size=data_parallel_world_size,\n                                   data_parallel_rank=data_parallel_rank)\n\n    def train(self):\n        r""""""\n        """"""\n\n        self.warn_unscaled_loss = True\n        self.module.train()\n\n    def eval(self):\n        r""""""\n        """"""\n\n        self.warn_unscaled_loss = True\n        self.module.train(False)\n\n    def _scale_loss(self, prescaled_loss):\n        if isinstance(prescaled_loss, torch.Tensor):\n            scaled_loss = prescaled_loss / self.gradient_accumulation_steps()\n        elif isinstance(prescaled_loss, tuple) or isinstance(prescaled_loss, list):\n            scaled_loss = []\n            for l in prescaled_loss:\n                if isinstance(l, torch.Tensor):\n                    scaled_loss.append(l / self.gradient_accumulation_steps())\n                else:\n                    scaled_loss.append(l)\n        else:\n            scaled_loss = prescaled_loss\n            if self.warn_unscaled_loss:\n                logger.warning(\n                    f\'DeepSpeed unable to scale loss because of type: {type(prescaled_loss)}\'\n                )\n                self.warn_unscaled_loss = False\n\n        return scaled_loss\n\n    def forward(self, *inputs, **kwargs):\n        r""""""Execute forward propagation\n\n        Arguments:\n            *inputs: Variable length input list\n            **kwargs: variable length keyword arguments\n        """"""\n\n        if self.wall_clock_breakdown():\n            self.timers(\'forward_microstep\').start()\n            self.timers(\'forward\').start()\n\n        if self.training_dataloader is None:\n            self.tput_timer.start()\n        loss = self.module(*inputs, **kwargs)\n\n        if self.wall_clock_breakdown():\n            self.timers(\'forward\').stop()\n            self.timers(\'forward_microstep\').stop()\n\n        return loss\n\n    def allreduce_gradients(self, bucket_size=MEMORY_OPT_ALLREDUCE_SIZE):\n        if self.is_gradient_accumulation_boundary():\n            if self.zero_optimization_stage() == ZERO_OPTIMIZATION_OPTIMIZER_STATES:\n                assert self.zero_reduce_scatter()\n                self.optimizer.reduce_scatter_gradients(\n                    postscale_gradients=self.postscale_gradients(),\n                    gradient_predivide_factor=self.gradient_predivide_factor(),\n                    gradient_average=self.gradient_average)\n            elif self.zero_optimization_partition_gradients():\n                self.optimizer.overlapping_partition_gradients_reduce_epilogue()\n            else:\n                self.buffered_allreduce_fallback(elements_per_buffer=bucket_size)\n\n    def backward(self, loss, allreduce_gradients=True):\n        r""""""Execute backward pass on the loss\n\n        Arguments:\n            loss: Torch tensor on which to execute backward propagation\n            allreduce_gradients: If this is False, then gradient averaging will be skipped. Default is True.\n        """"""\n\n        # scale loss w.r.t. gradient accumulation if needed\n        if self.gradient_accumulation_steps() > 1:\n            loss = self._scale_loss(loss.float())\n\n        # Log training Loss\n        if self.tensorboard_enabled():\n            if self.is_gradient_accumulation_boundary():\n                if self.global_rank == 0:\n                    self.sample_count += (self.train_micro_batch_size_per_gpu() *\n                                          self.dp_world_size *\n                                          self.gradient_accumulation_steps())\n                    self.summary_events = [\n                        (f\'Train/Samples/train_loss\',\n                         loss.mean().item() * self.gradient_accumulation_steps(),\n                         self.sample_count)\n                    ]\n                    for event in self.summary_events:  # write_summary_events\n                        self.summary_writer.add_scalar(event[0], event[1], event[2])\n                    self.summary_writer.flush()\n\n        if self.wall_clock_breakdown():\n            self.timers(\'backward_microstep\').start()\n            self.timers(\'backward\').start()\n\n        assert self.optimizer is not None, ""must provide optimizer during "" \\\n                                           ""init in order to use backward""\n\n        if self.wall_clock_breakdown():\n            self.timers(\'backward_inner_microstep\').start()\n            self.timers(\'backward_inner\').start()\n\n        if self.zero_optimization():\n            self.optimizer.backward(loss)\n        elif self.fp16_enabled():\n            self.optimizer.backward(loss)\n\n            # TODO: Use new AMP semantics as below\n            # with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n            #    scaled_loss.backward()\n        else:\n            loss.backward()\n\n        if self.wall_clock_breakdown():\n            self.timers(\'backward_inner\').stop()\n            self.timers(\'backward_inner_microstep\').stop()\n\n        if self.wall_clock_breakdown():\n            self.timers(\'backward_allreduce_microstep\').start()\n            self.timers(\'backward_allreduce\').start()\n\n        if allreduce_gradients:\n            self.allreduce_gradients()\n\n        if self.wall_clock_breakdown():\n            self.timers(\'backward_allreduce\').stop()\n            self.timers(\'backward_allreduce_microstep\').stop()\n            self.timers(\'backward\').stop()\n            self.timers(\'backward_microstep\').stop()\n\n        return loss\n\n    def is_gradient_accumulation_boundary(self):\n        return (self.micro_steps + 1) % \\\n            self.gradient_accumulation_steps() == 0\n\n    def zero_grad(self):\n        """"""\n        Zero parameter grads.\n        """"""\n        for param_name, param in self.module.named_parameters():\n            param.grad = None\n\n    def clip_fp32_gradients(self):\n        torch.nn.utils.clip_grad_norm_(parameters=self.module.parameters(),\n                                       max_norm=self.gradient_clipping())\n\n    def step(self):\n        r""""""Execute the weight update step after forward and backward propagation on effective_train_batch\n        """"""\n        if self.wall_clock_breakdown():\n            self.timers(\'step_microstep\').start()\n            self.timers(\'step\').start()\n\n        assert self.optimizer is not None, ""must provide optimizer during "" \\\n                                           ""init in order to use step""\n        report_progress = self.global_rank == 0 if self.global_rank else True\n\n        if self.is_gradient_accumulation_boundary():\n\n            if not self.fp16_enabled() and self.gradient_clipping() > 0.0:\n                self.clip_fp32_gradients()\n\n            self.optimizer.step()\n\n            #zero grad in basic optimizer could be unreliable and may not exhibit\n            #the behaviour that we want\n            if not self.zero_optimization() and not self.fp16_enabled():\n                self.zero_grad()\n            else:\n                self.optimizer.zero_grad()\n\n            # Check overlow here since in DS fp16 optimizer, the overflow is updated in above step() function.\n            overflow = False\n            if hasattr(self.optimizer, \'overflow\'):\n                overflow = self.optimizer.overflow\n\n            if overflow:\n                self.skipped_steps += 1\n            else:\n                if self.lr_scheduler is not None:\n                    self.lr_scheduler.step()\n                if report_progress and (self.global_steps +\n                                        1) % self.steps_per_print() == 0:\n                    self._report_progress(self.global_steps + 1)\n\n            self.global_steps += 1\n\n        self.tput_timer.stop(report_progress)\n\n        # Log learning rate\n        if self.tensorboard_enabled():\n            if self.is_gradient_accumulation_boundary():\n                if self.global_rank == 0:\n                    self.summary_events = [(f\'Train/Samples/lr\',\n                                            self.get_lr()[0],\n                                            self.sample_count)]\n                    for event in self.summary_events:  # write_summary_events\n                        self.summary_writer.add_scalar(event[0], event[1], event[2])\n                    self.summary_writer.flush()\n\n        if self.wall_clock_breakdown():\n            self.timers(\'step\').stop()\n            self.timers(\'step_microstep\').stop()\n            timer_names = [\n                \'forward_microstep\',\n                \'backward_microstep\',\n                \'backward_inner_microstep\',\n                \'backward_allreduce_microstep\',\n                \'step_microstep\'\n            ]\n            self.timers.log(names=timer_names, memory_breakdown=self.memory_breakdown())\n\n            # Log timing\n            if self.is_gradient_accumulation_boundary():\n                if self.tensorboard_enabled():\n                    if self.global_rank == 0:\n                        self.summary_events = [(f\'Train/Samples/elapsed_time_ms_forward\', self.timers(\'forward\').elapsed(reset=False) * 1000.0, self.sample_count), \\\n                                                (f\'Train/Samples/elapsed_time_ms_backward\', self.timers(\'backward\').elapsed(reset=False) * 1000.0, self.sample_count), \\\n                                                (f\'Train/Samples/elapsed_time_ms_backward_inner\', self.timers(\'backward_inner\').elapsed(reset=False) * 1000.0, self.sample_count), \\\n                                                (f\'Train/Samples/elapsed_time_ms_backward_allreduce\', self.timers(\'backward_allreduce\').elapsed(reset=False) * 1000.0, self.sample_count), \\\n                                                (f\'Train/Samples/elapsed_time_ms_step\', self.timers(\'step\').elapsed(reset=False) * 1000.0, self.sample_count)\n                                                ]\n                        for event in self.summary_events:  # write_summary_events\n                            self.summary_writer.add_scalar(event[0], event[1], event[2])\n                        self.summary_writer.flush()\n\n            if self.wall_clock_breakdown():\n                self.timers.log([\n                    \'forward\',\n                    \'backward\',\n                    \'backward_inner\',\n                    \'backward_allreduce\',\n                    \'step\'\n                ])\n\n        self.micro_steps += 1\n\n    def _get_optimizer_param(self, param_name):\n        result = []\n        if not self.optimizer:\n            return result\n        for group in self.optimizer.param_groups:\n            if param_name in group:\n                result.append(group[param_name])\n            else:\n                result.append(0.0)\n        return result\n\n    def get_lr(self):\n        return self._get_optimizer_param(\'lr\')\n\n    def get_mom(self):\n        return self._get_optimizer_param(\'betas\')\n\n    def _report_progress(self, step):\n        lr = self.get_lr()\n        mom = self.get_mom()\n        logger.info(\'rank:{} step={}, skipped={}, lr={}, mom={}\'.format(\n            self.global_rank,\n            step,\n            self.skipped_steps,\n            lr,\n            mom))\n\n    def allreduce_bucket(self, bucket):\n        tensor = flatten(bucket)\n\n        tensor_to_allreduce = tensor\n\n        if self.allreduce_always_fp32():\n            tensor_to_allreduce = tensor.float()\n\n        if self.postscale_gradients():\n            if self.gradient_predivide_factor() != 1.0:\n                tensor_to_allreduce.mul_(1. / self.gradient_predivide_factor())\n\n            dist.all_reduce(tensor_to_allreduce, group=self.data_parallel_group)\n\n            if self.gradient_average:\n                if self.gradient_predivide_factor() != self.dp_world_size:\n                    tensor_to_allreduce.mul_(self.gradient_predivide_factor() /\n                                             self.dp_world_size)\n        else:\n            tensor_to_allreduce.div_(self.dp_world_size)\n            dist.all_reduce(tensor_to_allreduce, group=self.data_parallel_group)\n\n        if self.allreduce_always_fp32() and tensor is not tensor_to_allreduce:\n            tensor.copy_(tensor_to_allreduce)\n\n        return tensor\n\n    def allreduce_and_copy(self, small_bucket):\n        allreduced = self.allreduce_bucket(small_bucket)\n        for buf, synced in zip(small_bucket, unflatten(allreduced, small_bucket)):\n            buf.copy_(synced)\n\n    def allreduce_no_retain(self, bucket, numel_per_bucket=500000000):\n        small_bucket = []\n        numel = 0\n        for tensor in bucket:\n            small_bucket.append(tensor)\n            numel = numel + tensor.numel()\n            if numel > numel_per_bucket:\n                self.allreduce_and_copy(small_bucket)\n                small_bucket = []\n                numel = 0\n        if len(small_bucket) > 0:\n            self.allreduce_and_copy(small_bucket)\n\n    def buffered_allreduce_fallback(self, grads=None, elements_per_buffer=500000000):\n        grads = []\n        for param_name, param in self.module.named_parameters():\n            if param.grad is not None:\n                grad_data = param.grad.data\n                if self.sparse_gradients_enabled(\n                ) and param_name in self.csr_tensor_module_names:\n                    grads.append(CSRTensor(grad_data))\n                else:\n                    grads.append(grad_data)\n\n        split_buckets = split_half_float_double_csr(grads)\n\n        for i, bucket_tuple in enumerate(split_buckets):\n            bucket_type, bucket = bucket_tuple\n            if bucket_type == CSRTensor.type():\n                self.csr_allreduce_no_retain(bucket)\n            else:\n                self.allreduce_no_retain(bucket, numel_per_bucket=elements_per_buffer)\n\n    def csr_allreduce_no_retain(self, bucket):\n        allreduced_csrs = self.csr_allreduce_bucket(bucket)\n        # Densify csr tensor and copy back to original location\n        for csr in allreduced_csrs:\n            dense_tensor = csr.to_dense()\n            csr.orig_dense_tensor.copy_(dense_tensor)\n\n    def csr_allreduce_bucket(self, bucket):\n        csr_list = []\n        for csr in bucket:\n            csr_list.append(self.csr_allreduce(csr))\n        return csr_list\n\n    def csr_allreduce(self, csr):\n        # Pre-divide for fp16 stability\n        csr.values.div_(self.dp_world_size)\n\n        indices_device_list = self.csr_all_gather(csr.indices)\n        values_device_list = self.csr_all_gather(csr.values)\n\n        csr.indices = torch.cat(indices_device_list)\n        csr.values = torch.cat(values_device_list)\n        return csr\n\n    def csr_all_gather(self, value):\n        my_size = torch.LongTensor([value.size()[0]]).to(self.device)\n        all_sizes = self.all_gather_scalar(my_size)\n        max_size = torch.cat(all_sizes).max()\n        fill_size = (max_size - my_size)\n\n        assert value.dim() in [1, 2]\n        if value.dim() == 1:\n            if fill_size > 0:\n                value = torch.cat([value, value.new_zeros(fill_size)])\n            tensor_list = [value.new_zeros(max_size) for _ in range(self.dp_world_size)]\n        else:\n            if fill_size > 0:\n                value = torch.cat([value, value.new_zeros(fill_size, value.size()[1])])\n            tensor_list = [\n                value.new_zeros(max_size,\n                                value.size()[1]) for _ in range(self.dp_world_size)\n            ]\n\n        dist.all_gather(tensor_list, value, group=self.data_parallel_group)\n        tensors = []\n        for dev_idx, t in enumerate(tensor_list):\n            size = all_sizes[dev_idx][0]\n            tensors.append(\n                t.index_select(0,\n                               torch.LongTensor(range(size)).to(self.device)))\n\n        return tensors\n\n    def all_gather_scalar(self, value):\n        tensor_list = [value.new_zeros(value.size()) for _ in range(self.dp_world_size)]\n        dist.all_gather(tensor_list, value, group=self.data_parallel_group)\n        return tensor_list\n\n    def module_state_dict(self, destination=None, prefix=\'\', keep_vars=False):\n        sd = self.module.state_dict(destination, prefix, keep_vars)\n        return sd\n\n    def load_module_state_dict(self, state_dict, strict=True):\n        self.module.load_state_dict(state_dict, strict=strict)\n\n    def _get_zero_ckpt_name(self, checkpoints_path, tag):\n\n        mp_rank = 0 if self.mpu is None else self.mpu.get_model_parallel_rank()\n        pp_rank = torch.distributed.get_rank(group=self.optimizer.dp_process_group)\n\n        filename = \'zero_pp_rank_{}\'.format(pp_rank)\n        zero_ckpt_name = os.path.join(\n            checkpoints_path,\n            str(tag),\n            filename + \'_mp_rank_{:02d}\'.format(mp_rank) + \'optim_states.pt\')\n        return zero_ckpt_name\n\n    def _get_ckpt_name(self, checkpoints_path, tag):\n\n        mp_rank = 0 if self.mpu is None else self.mpu.get_model_parallel_rank()\n        ckpt_name = os.path.join(checkpoints_path,\n                                 str(tag),\n                                 \'mp_rank_{:02d}\'.format(mp_rank) + \'_model_states.pt\')\n        return ckpt_name\n\n    def _ensure_directory_exists(self, filename):\n        dirname = os.path.dirname(filename)\n        if not os.path.exists(dirname):\n            os.makedirs(dirname)\n\n    def load_checkpoint(self,\n                        load_dir,\n                        tag,\n                        load_module_strict=True,\n                        load_optimizer_states=True,\n                        load_lr_scheduler_states=True):\n        r""""""Load training checkpoint\n\n        Arguments:\n            load_dir: Required. Directory to load the checkpoint from\n            tag: Required. Checkpoint tag used as a unique identifier for the checkpoint. Ex. Global Step.\n            load_module_strict: Optional. Boolean to strictly enforce that the keys in state_dict of module and checkpoint match.\n            load_optimizer_states: Optional. Boolean to load the training optimizer states from Checkpoint. Ex. ADAM\'s momentum and variance\n            load_lr_scheduler_states: Optional. Boolean to add the learning rate scheduler states from Checkpoint.\n        Return:\n            load_path: Path of the loaded checkpoint. None if loading the checkpoint failed\n            client_state: State dictionary used for loading required training states in the client code.\n        """"""\n\n        load_path, client_states = self._load_checkpoint(load_dir,\n                                                         tag,\n                                                         load_module_strict=load_module_strict,\n                                                         load_optimizer_states=load_optimizer_states,\n                                                         load_lr_scheduler_states=load_lr_scheduler_states)\n\n        if self.zero_optimization() and load_path is not None:\n            self._load_zero_checkpoint(load_dir,\n                                       tag,\n                                       load_optimizer_states=load_optimizer_states)\n\n        return load_path, client_states\n\n    def _load_checkpoint(self,\n                         load_dir,\n                         tag,\n                         load_module_strict=True,\n                         load_optimizer_states=True,\n                         load_lr_scheduler_states=True):\n\n        load_path = self._get_ckpt_name(load_dir, tag)\n\n        if not os.path.exists(load_path):\n            logger.warn(\n                \'Client provided checkpoint load path: {} does not exist ... skip checkpoint load\'\n                .format(load_path))\n            return None, None\n\n        logger.info(\'Loading checkpoint: {}\'.format(load_path))\n        checkpoint = torch.load(load_path, map_location=lambda storage, loc: storage)\n\n        self.load_module_state_dict(state_dict=checkpoint[\'module\'],\n                                    strict=load_module_strict)\n        if not self.zero_optimization():\n            self.optimizer.load_state_dict(checkpoint[\'optimizer\'],\n                                           load_optimizer_states=load_optimizer_states)\n\n        if load_lr_scheduler_states and self.lr_scheduler is not None:\n            self.lr_scheduler.load_state_dict(checkpoint[\'lr_scheduler\'])\n\n        self.csr_tensor_module_names = checkpoint[\'csr_tensor_module_names\']\n        self.global_steps = checkpoint[\'global_steps\']\n        self.skipped_steps = checkpoint[\'skipped_steps\']\n        deepspeed_states = [\n            \'module\',\n            \'optimizer\',\n            \'lr_scheduler\',\n            \'csr_tensor_module_names\',\n            \'skipped_steps\',\n            \'global_steps\'\n        ]\n        client_state = {\n            key: value\n            for key,\n            value in checkpoint.items() if not key in deepspeed_states\n        }\n\n        return load_path, client_state\n\n    def _load_zero_checkpoint(self, load_dir, tag, load_optimizer_states=True):\n        zero_checkpoint_name = self._get_zero_ckpt_name(load_dir, tag)\n\n        if not os.path.exists(zero_checkpoint_name):\n            logger.warn(\n                \'Client provided checkpoint load path: {} does not exist ... skip checkpoint load\'\n                .format(zero_checkpoint_name))\n            return None\n\n        zero_sd = torch.load(zero_checkpoint_name, map_location=\'cpu\')\n        self.optimizer.load_state_dict(zero_sd[\'optimizer_state_dict\'],\n                                       load_optimizer_states=load_optimizer_states)\n        logger.info(\'loading zero checkpoint {}\'.format(zero_checkpoint_name))\n\n    def save_checkpoint(self, save_dir, tag, client_state={}):\n        r""""""Save training checkpoint\n\n        Arguments:\n            save_dir: Required. Directory for saving the checkpoint\n            tag: Required. Checkpoint tag used as a unique identifier for the checkpoint. Ex. Global Step.\n            client_state: Optional. State dictionary used for saving required training states in the client code.\n        """"""\n\n        #This is to make sure the checkpoint names are created without collision\n        #There seems to be issue creating them in parallel\n        self._create_checkpoint_files(save_dir, tag)\n\n        if self.save_non_zero_checkpoint:\n            self._save_checkpoint(save_dir, tag, client_state=client_state)\n\n        if self.save_zero_checkpoint:\n            self._save_zero_checkpoint(save_dir, tag)\n\n        return True\n\n    def _create_checkpoint_files(self, save_dir, tag):\n        #checkpoint files are created sequentially\n        for rank in range(self.world_size):\n            if rank == self.global_rank:\n                try:\n                    if self.save_non_zero_checkpoint:\n                        checkpoint_name = self._get_ckpt_name(save_dir, tag)\n                        self._ensure_directory_exists(checkpoint_name)\n\n                    if self.save_zero_checkpoint:\n                        checkpoint_name = self._get_zero_ckpt_name(save_dir, tag)\n                        self._ensure_directory_exists(checkpoint_name)\n                except:\n                    logger.error(\n                        f\'Failed Saving model checkpoint to {save_dir} with tag {tag}\')\n                    return False\n            dist.barrier()\n\n    def _save_checkpoint(self, save_dir, tag, client_state={}):\n\n        save_path = self._get_ckpt_name(save_dir, tag)\n        #self._ensure_directory_exists(save_path)\n\n        state = {\n            \'module\':\n            self.module_state_dict(),\n            \'optimizer\':\n            self.optimizer.state_dict()\n            if self.optimizer and not self.zero_optimization() else None,\n            \'lr_scheduler\':\n            self.lr_scheduler.state_dict() if self.lr_scheduler is not None else None,\n            \'csr_tensor_module_names\':\n            self.csr_tensor_module_names,\n            \'skipped_steps\':\n            self.skipped_steps,\n            \'global_steps\':\n            self.global_steps,\n        }\n        state.update(client_state)\n\n        logger.info(\'Saving model checkpoint: {}\'.format(save_path))\n        torch.save(state, save_path)\n\n    def _save_zero_checkpoint(self, save_path, tag):\n        zero_checkpoint_name = self._get_zero_ckpt_name(save_path, tag)\n        #self._ensure_directory_exists(zero_checkpoint_name)\n        zero_sd = {\'optimizer_state_dict\': self.optimizer.state_dict()}\n        torch.save(zero_sd, zero_checkpoint_name)\n        logger.info(\'zero checkpoint saved {}\'.format(zero_checkpoint_name))\n'"
deepspeed/pt/deepspeed_lr_schedules.py,12,"b'""""""\nCopyright 2019 The Microsoft DeepSpeed Team\n\nImplementation of learning rate schedules.\n\nTaken and modified from PyTorch v1.0.1 source\nhttps://github.com/pytorch/pytorch/blob/v1.1.0/torch/optim/lr_scheduler.py\n\n""""""\n\nimport argparse\nfrom torch.optim import Optimizer\nfrom typing import Union, List\nimport math\nfrom deepspeed.pt.deepspeed_constants import *\nfrom deepspeed.pt.log_utils import logger\n\nLR_SCHEDULE = \'lr_schedule\'\nLR_RANGE_TEST = \'LRRangeTest\'\nONE_CYCLE = \'OneCycle\'\nWARMUP_LR = \'WarmupLR\'\nVALID_LR_SCHEDULES = [LR_RANGE_TEST, ONE_CYCLE, WARMUP_LR]\n\nLR_RANGE_TEST_MIN_LR = \'lr_range_test_min_lr\'\nLR_RANGE_TEST_STEP_RATE = \'lr_range_test_step_rate\'\nLR_RANGE_TEST_STEP_SIZE = \'lr_range_test_step_size\'\nLR_RANGE_TEST_STAIRCASE = \'lr_range_test_staircase\'\n\nEDGE_VALUE = \'edge_value\'\nMID_VALUE = \'mid_value\'\n\nCYCLE_FIRST_STEP_SIZE = \'cycle_first_step_size\'\nCYCLE_FIRST_STAIR_COUNT = \'cycle_first_stair_count\'\nCYCLE_SECOND_STEP_SIZE = \'cycle_second_step_size\'\nCYCLE_SECOND_STAIR_COUNT = \'cycle_second_stair_count\'\nDECAY_STEP_SIZE = \'decay_step_size\'\n\nCYCLE_MIN_LR = \'cycle_min_lr\'\nCYCLE_MAX_LR = \'cycle_max_lr\'\nDECAY_LR_RATE = \'decay_lr_rate\'\n\nCYCLE_MIN_MOM = \'cycle_min_mom\'\nCYCLE_MAX_MOM = \'cycle_max_mom\'\nDECAY_MOM_RATE = \'decay_mom_rate\'\n\nWARMUP_MIN_LR = \'warmup_min_lr\'\nWARMUP_MAX_LR = \'warmup_max_lr\'\nWARMUP_NUM_STEPS = \'warmup_num_steps\'\n\n\ndef add_tuning_arguments(parser):\n    group = parser.add_argument_group(\'Convergence Tuning\',\n                                      \'Convergence tuning configurations\')\n\n    # LR scheduler\n    group.add_argument(\'--lr_schedule\',\n                       type=str,\n                       default=None,\n                       help=\'LR schedule for training.\')\n\n    # Learning rate range test\n    group.add_argument(""--lr_range_test_min_lr"",\n                       type=float,\n                       default=0.001,\n                       help=\'Starting lr value.\')\n    group.add_argument(""--lr_range_test_step_rate"",\n                       type=float,\n                       default=1.0,\n                       help=\'scaling rate for LR range test.\')\n    group.add_argument(""--lr_range_test_step_size"",\n                       type=int,\n                       default=1000,\n                       help=\'training steps per LR change.\')\n    group.add_argument(""--lr_range_test_staircase"",\n                       type=bool,\n                       default=False,\n                       help=\'use staircase scaling for LR range test.\')\n\n    # OneCycle schedule\n    group.add_argument(""--cycle_first_step_size"",\n                       type=int,\n                       default=1000,\n                       help=\'size of first step of 1Cycle schedule (training steps).\')\n    group.add_argument(""--cycle_first_stair_count"",\n                       type=int,\n                       default=-1,\n                       help=\'first stair count for 1Cycle schedule.\')\n    group.add_argument(\n        ""--cycle_second_step_size"",\n        type=int,\n        default=-1,\n        help=\'size of second step of 1Cycle schedule (default first_step_size).\')\n    group.add_argument(""--cycle_second_stair_count"",\n                       type=int,\n                       default=-1,\n                       help=\'second stair count for 1Cycle schedule.\')\n    group.add_argument(\n        ""--decay_step_size"",\n        type=int,\n        default=1000,\n        help=\'size of intervals for applying post cycle decay (training steps).\')\n\n    # 1Cycle LR\n    group.add_argument(""--cycle_min_lr"",\n                       type=float,\n                       default=0.01,\n                       help=\'1Cycle LR lower bound.\')\n    group.add_argument(""--cycle_max_lr"",\n                       type=float,\n                       default=0.1,\n                       help=\'1Cycle LR upper bound.\')\n    group.add_argument(""--decay_lr_rate"",\n                       type=float,\n                       default=0.0,\n                       help=\'post cycle LR decay rate.\')\n\n    # 1Cycle Momentum\n    group.add_argument(\'--cycle_momentum\',\n                       default=False,\n                       action=\'store_true\',\n                       help=\'Enable 1Cycle momentum schedule.\')\n    group.add_argument(""--cycle_min_mom"",\n                       type=float,\n                       default=0.8,\n                       help=\'1Cycle momentum lower bound.\')\n    group.add_argument(""--cycle_max_mom"",\n                       type=float,\n                       default=0.9,\n                       help=\'1Cycle momentum upper bound.\')\n    group.add_argument(""--decay_mom_rate"",\n                       type=float,\n                       default=0.0,\n                       help=\'post cycle momentum decay rate.\')\n\n    # Warmup LR\n    group.add_argument(\'--warmup_min_lr\',\n                       type=float,\n                       default=0,\n                       help=\'WarmupLR minimum/initial LR value\')\n    group.add_argument(\'--warmup_max_lr\',\n                       type=float,\n                       default=0.001,\n                       help=\'WarmupLR maximum LR value.\')\n    group.add_argument(\'--warmup_num_steps\',\n                       type=int,\n                       default=1000,\n                       help=\'WarmupLR step count for LR warmup.\')\n\n    return parser\n\n\ndef parse_arguments():\n    parser = argparse.ArgumentParser()\n    parser = add_tuning_arguments(parser)\n\n    lr_sched_args, unknown_args = parser.parse_known_args()\n    return lr_sched_args, unknown_args\n\n\ndef override_lr_range_test_params(args, params):\n    if hasattr(args, LR_RANGE_TEST_MIN_LR) and args.lr_range_test_min_lr is not None:\n        params[LR_RANGE_TEST_MIN_LR] = args.lr_range_test_min_lr\n\n    if hasattr(args,\n               LR_RANGE_TEST_STEP_RATE) and args.lr_range_test_step_rate is not None:\n        params[LR_RANGE_TEST_STEP_RATE] = args.lr_range_test_step_rate\n\n    if hasattr(args,\n               LR_RANGE_TEST_STEP_SIZE) and args.lr_range_test_step_size is not None:\n        params[LR_RANGE_TEST_STEP_SIZE] = args.lr_range_test_step_size\n\n    if hasattr(args,\n               LR_RANGE_TEST_STAIRCASE) and args.lr_range_test_staircase is not None:\n        params[LR_RANGE_TEST_STAIRCASE] = args.lr_range_test_staircase\n\n\ndef override_1cycle_params(args, params):\n    if hasattr(args, CYCLE_FIRST_STEP_SIZE) and args.cycle_first_step_size is not None:\n        params[CYCLE_FIRST_STEP_SIZE] = args.cycle_first_step_size\n\n    if hasattr(args,\n               CYCLE_FIRST_STAIR_COUNT) and args.cycle_first_stair_count is not None:\n        params[CYCLE_FIRST_STAIR_COUNT] = args.cycle_first_stair_count\n\n    if hasattr(args, CYCLE_SECOND_STEP_SIZE) and args.cycle_second_step_size is not None:\n        params[CYCLE_SECOND_STEP_SIZE] = args.cycle_second_step_size\n\n    if hasattr(args,\n               CYCLE_SECOND_STAIR_COUNT) and args.cycle_second_stair_count is not None:\n        params[CYCLE_SECOND_STAIR_COUNT] = args.cycle_second_stair_count\n\n    if hasattr(args, DECAY_STEP_SIZE) and args.decay_step_size is not None:\n        params[DECAY_STEP_SIZE] = args.decay_step_size\n\n    # 1Cycle LR params\n    if hasattr(args, CYCLE_MIN_LR) and args.cycle_min_lr is not None:\n        params[CYCLE_MIN_LR] = args.cycle_min_lr\n\n    if hasattr(args, CYCLE_MAX_LR) and args.cycle_max_lr is not None:\n        params[CYCLE_MAX_LR] = args.cycle_max_lr\n\n    if hasattr(args, DECAY_LR_RATE) and args.decay_lr_rate is not None:\n        params[DECAY_LR_RATE] = args.decay_lr_rate\n\n    # 1Cycle MOM params\n    if hasattr(args, CYCLE_MIN_MOM) and args.cycle_min_mom is not None:\n        params[CYCLE_MIN_MOM] = args.cycle_min_mom\n\n    if hasattr(args, CYCLE_MAX_MOM) and args.cycle_max_mom is not None:\n        params[CYCLE_MAX_MOM] = args.cycle_max_mom\n\n    if hasattr(args, DECAY_MOM_RATE) and args.decay_mom_rate is not None:\n        params[DECAY_MOM_RATE] = args.decay_mom_rate\n\n\ndef override_warmupLR_params(args, params):\n    if hasattr(args, WARMUP_MIN_LR) and args.warmup_min_lr is not None:\n        params[WARMUP_MIN_LR] = args.warmup_min_lr\n\n    if hasattr(args, WARMUP_MAX_LR) and args.warmup_max_lr is not None:\n        params[WARMUP_MAX_LR] = args.warmup_max_lr\n\n    if hasattr(args, WARMUP_NUM_STEPS) and args.warmup_num_steps is not None:\n        params[WARMUP_NUM_STEPS] = args.warmup_num_steps\n\n\ndef override_params(args, params):\n    # LR range test params\n    override_lr_range_test_params(args, params)\n\n    # 1Cycle params\n    override_1cycle_params(args, params)\n\n    # WarmupLR params\n    override_warmupLR_params(args, params)\n\n\ndef get_config_from_args(args):\n    if not hasattr(args, LR_SCHEDULE) or args.lr_schedule is None:\n        return None, \'--{} not specified on command line\'.format(LR_SCHEDULE)\n\n    if not args.lr_schedule in VALID_LR_SCHEDULES:\n        return None, \'{} is not supported LR schedule\'.format(args.lr_schedule)\n\n    config = {}\n    config[\'type\'] = args.lr_schedule\n    config[\'params\'] = {}\n\n    if args.lr_schedule == LR_RANGE_TEST:\n        override_lr_range_test_params(args, config[\'params\'])\n    elif args.lr_schedule == ONE_CYCLE:\n        override_1cycle_params(args, config[\'params\'])\n    else:\n        override_warmupLR_params(args, config[\'params\'])\n\n    return config, None\n\n\ndef get_lr_from_config(config):\n    if not \'type\' in config:\n        return None, \'LR schedule type not defined in config\'\n\n    if not \'params\' in config:\n        return None, \'LR schedule params not defined in config\'\n\n    lr_schedule = config[\'type\']\n    lr_params = config[\'params\']\n\n    if not lr_schedule in VALID_LR_SCHEDULES:\n        return None, \'{} is not a valid LR schedule\'.format(lr_schedule)\n\n    if lr_schedule == LR_RANGE_TEST:\n        return lr_params[LR_RANGE_TEST_MIN_LR], \'\'\n    if lr_schedule == ONE_CYCLE:\n        return lr_params[CYCLE_MAX_LR], \'\'\n    # Warmup LR\n    return lr_params[WARMUP_MAX_LR], \'\'\n\n\n""""""\nOnly optimizers that are subclass of torch.optim.Optimizer are supported. So check the passed optimizer and wrapped\noptimizer to see if requirement is satisfied.\nTODO: Looking under the hood to examine the wrapped optimizer is a hack that requires a better long-term fix.\n""""""\n\n\ndef get_torch_optimizer(optimizer):\n    if isinstance(optimizer, Optimizer):\n        return optimizer\n\n    if hasattr(optimizer, \'optimizer\') and isinstance(optimizer.optimizer, Optimizer):\n        return optimizer.optimizer\n\n    raise TypeError(\'{} is not a subclass of torch.optim.Optimizer\'.format(\n        type(optimizer).__name__))\n\n\nclass LRRangeTest(object):\n    """"""Sets the learning rate of each parameter group according to\n    learning rate range test (LRRT) policy. The policy increases learning\n    rate starting from a base value with a constant frequency, as detailed in\n    the paper `A disciplined approach to neural network hyper-parameters: Part1`_.\n\n    LRRT policy is used for finding maximum LR that trains a model without divergence, and can be used to\n    configure the LR boundaries for Cylic LR schedules.\n\n    LRRT changes the learning rate after every batch.\n    `step` should be called after a batch has been used for training.\n\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        lr_range_test_min_lr (float or list): Initial learning rate which is the\n            lower boundary in the range test for each parameter group.\n        lr_range_test_step_size (int): Interval of training steps to increase learning rate. Default: 2000\n        lr_range_test_step_rate (float): Scaling rate for range test. Default: 1.0\n        lr_range_test_staircase (bool): Scale in staircase fashion, rather than continous. Default: False.\n        last_batch_iteration (int): The index of the last batch. This parameter is used when\n            resuming a training job. Since `step()` should be invoked after each\n            batch instead of after each epoch, this number represents the total\n            number of *batches* computed, not the total number of epochs computed.\n            When last_batch_iteration=-1, the schedule is started from the beginning.\n            Default: -1\n\n    Example:\n        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n        >>> scheduler = torch.optim.LRRangeTest(optimizer)\n        >>> data_loader = torch.utils.data.DataLoader(...)\n        >>> for epoch in range(10):\n        >>>     for batch in data_loader:\n        >>>         train_batch(...)\n        >>>         scheduler.step()\n\n        _A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay:\n        https://arxiv.org/abs/1803.09820\n""""""\n    def __init__(self,\n                 optimizer: Optimizer,\n                 lr_range_test_min_lr: float = 1e-3,\n                 lr_range_test_step_size: int = 2000,\n                 lr_range_test_step_rate: float = 1.0,\n                 lr_range_test_staircase: bool = False,\n                 last_batch_iteration: int = -1):\n\n        self.optimizer = get_torch_optimizer(optimizer)\n\n        if isinstance(lr_range_test_min_lr,\n                      list) or isinstance(lr_range_test_min_lr,\n                                          tuple):\n            if len(lr_range_test_min_lr) != len(self.optimizer.param_groups):\n                raise ValueError(""expected {} lr_range_test_min_lr, got {}"".format(\n                    len(self.optimizer.param_groups),\n                    len(lr_range_test_min_lr)))\n            self.min_lr = list(lr_range_test_min_lr)\n        else:\n            self.min_lr = [lr_range_test_min_lr] * len(self.optimizer.param_groups)\n\n        self.step_size = lr_range_test_step_size\n        self.step_rate = lr_range_test_step_rate\n        self.last_batch_iteration = last_batch_iteration\n        self.staircase = lr_range_test_staircase\n        self.interval_fn = self._staircase_interval if lr_range_test_staircase else self._continous_interval\n\n        if last_batch_iteration == -1:\n            self._update_optimizer(self.min_lr)\n\n    def _staircase_interval(self):\n        return math.floor(float(self.last_batch_iteration) / self.step_size)\n\n    def _continous_interval(self):\n        return float(self.last_batch_iteration) / self.step_size\n\n    def _get_increase(self):\n        return (1 + self.step_rate * self.interval_fn())\n\n    def get_lr(self):\n        lr_increase = self._get_increase()\n        return [\n            lr_range_test_min_lr * lr_increase for lr_range_test_min_lr in self.min_lr\n        ]\n\n    def _update_optimizer(self, group_lrs):\n        for param_group, lr in zip(self.optimizer.param_groups, group_lrs):\n            param_group[\'lr\'] = lr\n\n    def step(self, batch_iteration=None):\n        if batch_iteration is None:\n            batch_iteration = self.last_batch_iteration + 1\n        self.last_batch_iteration = batch_iteration\n        self._update_optimizer(self.get_lr())\n\n    def state_dict(self):\n        return {\'last_batch_iteration\': self.last_batch_iteration}\n\n    def load_state_dict(self, sd):\n        self.last_batch_iteration = sd[\'last_batch_iteration\']\n\n\nclass OneCycle(object):\n    """"""Sets the learning rate of each parameter group according to\n    1Cycle learning rate policy (1CLR). 1CLR is a variation of the\n    Cyclical Learning Rate (CLR) policy that involves one cycle followed by\n    decay. The policy simultaneously cycles the learning rate (and momentum)\n    between two boundaries with a constant frequency, as detailed in\n    the paper `A disciplined approach to neural network hyper-parameters`_.\n\n    1CLR policy changes the learning rate after every batch.\n    `step` should be called after a batch has been used for training.\n\n    This implementation was adapted from the github repo: `pytorch/pytorch`_\n\n    Args:\n        optimizer (Optimizer): Wrapped optimizer.\n        cycle_min_lr (float or list): Initial learning rate which is the\n            lower boundary in the cycle for each parameter group.\n        cycle_max_lr (float or list): Upper learning rate boundaries in the cycle\n            for each parameter group. Functionally,\n            it defines the cycle amplitude (cycle_max_lr - cycle_min_lr).\n            The lr at any cycle is the sum of cycle_min_lr\n            and some scaling of the amplitude; therefore\n            cycle_max_lr may not actually be reached depending on\n            scaling function.\n        decay_lr_rate(float): Decay rate for learning rate. Default: 0.\n        cycle_first_step_size (int): Number of training iterations in the\n            increasing half of a cycle. Default: 2000\n        cycle_second_step_size (int): Number of training iterations in the\n            decreasing half of a cycle. If cycle_second_step_size is None,\n            it is set to cycle_first_step_size. Default: None\n        cycle_first_stair_count(int): Number of stairs in first half of cycle phase. This means\n        lr/mom are changed in staircase fashion. Default 0, means staircase disabled.\n        cycle_second_stair_count(int): Number of stairs in second half of cycle phase. This means\n        lr/mom are changed in staircase fashion. Default 0, means staircase disabled.\n        decay_step_size (int): Intervals for applying decay in decay phase. Default: 0, means no decay.\n        cycle_momentum (bool): If ``True``, momentum is cycled inversely\n            to learning rate between \'cycle_min_mom\' and \'cycle_max_mom\'.\n            Default: True\n        cycle_min_mom (float or list): Initial momentum which is the\n            lower boundary in the cycle for each parameter group.\n            Default: 0.8\n        cycle_max_mom (float or list): Upper momentum boundaries in the cycle\n            for each parameter group. Functionally,\n            it defines the cycle amplitude (cycle_max_mom - cycle_min_mom).\n            The momentum at any cycle is the difference of cycle_max_mom\n            and some scaling of the amplitude; therefore\n            cycle_min_mom may not actually be reached depending on\n            scaling function. Default: 0.9\n        decay_mom_rate (float): Decay rate for momentum. Default: 0.\n        last_batch_iteration (int): The index of the last batch. This parameter is used when\n            resuming a training job. Since `step()` should be invoked after each\n            batch instead of after each epoch, this number represents the total\n            number of *batches* computed, not the total number of epochs computed.\n            When last_batch_iteration=-1, the schedule is started from the beginning.\n            Default: -1\n\n    Example:\n        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n        >>> scheduler = torch.optim.OneCycle(optimizer)\n        >>> data_loader = torch.utils.data.DataLoader(...)\n        >>> for epoch in range(10):\n        >>>     for batch in data_loader:\n        >>>         train_batch(...)\n        >>>         scheduler.step()\n\n\n    .. _A disciplined approach to neural network hyper-parameters: Part 1 -- learning rate, batch size, momentum, and weight decay: https://arxiv.org/abs/1803.09820\n    """"""\n    def __init__(self,\n                 optimizer,\n                 cycle_min_lr,\n                 cycle_max_lr,\n                 decay_lr_rate=0.,\n                 cycle_first_step_size=2000,\n                 cycle_second_step_size=None,\n                 cycle_first_stair_count=0,\n                 cycle_second_stair_count=None,\n                 decay_step_size=0,\n                 cycle_momentum=True,\n                 cycle_min_mom=0.8,\n                 cycle_max_mom=0.9,\n                 decay_mom_rate=0.,\n                 last_batch_iteration=-1):\n\n        self.optimizer = get_torch_optimizer(optimizer)\n\n        # Initialize cycle shape\n        self._initialize_cycle(cycle_first_step_size,\n                               cycle_second_step_size,\n                               cycle_first_stair_count,\n                               cycle_second_stair_count,\n                               decay_step_size)\n\n        # Initialize cycle lr\n        self._initialize_lr(self.optimizer,\n                            cycle_min_lr,\n                            cycle_max_lr,\n                            decay_lr_rate,\n                            last_batch_iteration)\n\n        # Initialize cyclic momentum\n        self.cycle_momentum = cycle_momentum\n        if cycle_momentum:\n            self._initialize_momentum(self.optimizer,\n                                      cycle_min_mom,\n                                      cycle_max_mom,\n                                      decay_mom_rate,\n                                      last_batch_iteration)\n\n        # Initalize batch iteration tracker\n        self.last_batch_iteration = last_batch_iteration\n\n    # Configure cycle shape\n\n    def _initialize_cycle(self,\n                          cycle_first_step_size,\n                          cycle_second_step_size,\n                          cycle_first_stair_count,\n                          cycle_second_stair_count,\n                          decay_step_size):\n        cycle_first_step_size = float(cycle_first_step_size)\n        cycle_second_step_size = float(\n            cycle_second_step_size\n        ) if cycle_second_step_size is not None else cycle_first_step_size\n\n        self.total_size = cycle_first_step_size + cycle_second_step_size\n        self.step_ratio = cycle_first_step_size / self.total_size\n        self.first_stair_count = cycle_first_stair_count\n        self.second_stair_count = cycle_first_stair_count if cycle_second_stair_count is None else cycle_second_stair_count\n        self.decay_step_size = decay_step_size\n\n    # Configure lr schedule\n    def _initialize_lr(self,\n                       optimizer,\n                       cycle_min_lr,\n                       cycle_max_lr,\n                       decay_lr_rate,\n                       last_batch_iteration):\n        self.min_lrs = [cycle_min_lr] * len(optimizer.param_groups)\n        if last_batch_iteration == -1:\n            for lr, group in zip(self.min_lrs, optimizer.param_groups):\n                group[\'lr\'] = lr\n\n        self.max_lrs = [cycle_max_lr] * len(optimizer.param_groups)\n        self.decay_lr_rate = decay_lr_rate\n\n    # Configure momentum schedule\n    def _initialize_momentum(self,\n                             optimizer,\n                             cycle_min_mom,\n                             cycle_max_mom,\n                             decay_mom_rate,\n                             last_batch_iteration):\n        if \'betas\' not in optimizer.defaults:\n            optimizer_name = type(optimizer).__name__\n            logger.warn(\n                f""cycle_momentum is disabled because optimizer {optimizer_name} does not support momentum, no betas attribute in defaults""\n            )\n            self.cycle_momentum = False\n            return\n\n        self.decay_mom_rate = decay_mom_rate\n        self.min_moms = [(cycle_min_mom, 0.99)] * len(optimizer.param_groups)\n        self.max_moms = [(cycle_max_mom, 0.99)] * len(optimizer.param_groups)\n\n        if last_batch_iteration == -1:\n            for momentum, group in zip(self.min_moms, optimizer.param_groups):\n                group[\'betas\'] = momentum\n\n    def _get_cycle_lr(self):\n        cycle = math.floor(1 + self.last_batch_iteration / self.total_size)\n        x = 1. + self.last_batch_iteration / self.total_size - cycle\n        if x <= self.step_ratio:\n            scale_factor = x / self.step_ratio\n        else:\n            scale_factor = (x - 1) / (self.step_ratio - 1)\n\n        lrs = []\n        for cycle_min_lr, cycle_max_lr in zip(self.min_lrs, self.max_lrs):\n            base_height = (cycle_max_lr - cycle_min_lr) * scale_factor\n            lr = cycle_min_lr + base_height\n            lrs.append(lr)\n\n        if self.cycle_momentum:\n            momentums = []\n            for base_betas, max_betas in zip(self.min_moms, self.max_moms):\n                cycle_min_mom = base_betas[0]\n                cycle_max_mom = max_betas[0]\n                base_height = (cycle_max_mom - cycle_min_mom) * scale_factor\n                momentum = cycle_max_mom - base_height\n                momentums.append((momentum, base_betas[1]))\n            for param_group, momentum in zip(self.optimizer.param_groups, momentums):\n                param_group[\'betas\'] = momentum\n\n        return lrs\n\n    def _get_decay_lr(self, decay_batch_iteration):\n        """"""Calculates the learning rate at batch index. This function is used\n        after the cycle completes and post cycle decaying of lr/mom is enabled.\n        This function treats `self.last_batch_iteration` as the last batch index.\n\n        If `self.cycle_momentum` is ``True``, this function has a side effect of\n        updating the optimizer\'s momentum.\n        """"""\n        decay_interval = decay_batch_iteration / self.decay_step_size\n\n        lr_decay_factor = (1 + self.decay_lr_rate * decay_interval)\n        lrs = [cycle_min_lr * lr_decay_factor for cycle_min_lr in self.min_lrs]\n\n        if self.cycle_momentum:\n            mom_decay_factor = (1 + self.decay_mom_rate * decay_interval)\n            momentums = [(beta0 * mom_decay_factor,\n                          beta1) for beta0,\n                         beta1 in self.max_moms]\n            for param_group, momentum in zip(self.optimizer.param_groups, momentums):\n                param_group[\'betas\'] = momentum\n\n        return lrs\n\n    def get_lr(self):\n        """"""Calculates the learning rate at batch index. This function treats\n        `self.last_batch_iteration` as the last batch index.\n\n        If `self.cycle_momentum` is ``True``, this function has a side effect of\n        updating the optimizer\'s momentum.\n        """"""\n        if self.last_batch_iteration <= self.total_size:\n            return self._get_cycle_lr()\n        return self._get_decay_lr(self.last_batch_iteration - self.total_size)\n\n    def step(self, batch_iteration=None):\n        if batch_iteration is None:\n            batch_iteration = self.last_batch_iteration + 1\n        self.last_batch_iteration = batch_iteration\n        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n            param_group[\'lr\'] = lr\n\n    def state_dict(self):\n        return {\'last_batch_iteration\': self.last_batch_iteration}\n\n    def load_state_dict(self, sd):\n        self.last_batch_iteration = sd[\'last_batch_iteration\']\n\n\nclass WarmupLR(object):\n    """"""Increase the learning rate of each parameter group from min lr to max lr\n        over warmup_num_steps steps, and then fix at max lr.\n\n        Args:\n            optimizer (Optimizer): Wrapped optimizer.\n            warmup_min_lr (float or list): minimum learning rate. Default: 0\n            warmup_max_lr (float or list): maximum learning rate. Default: 0.001\n            warmup_num_steps (int): number of steps to warm up from min_lr to max_lr. Default: 1000\n            last_batch_iteration (int): The index of the last batch. Default: -1.\n        Example:\n            >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n            >>> scheduler = torch.optim.WarmupLR(optimizer)\n            >>> data_loader = torch.utils.data.DataLoader(...)\n            >>> for epoch in range(10):\n            >>>     for batch in data_loader:\n            >>>         train_batch(...)\n            >>>         scheduler.step()\n\n    """"""\n    def __init__(self,\n                 optimizer: Optimizer,\n                 warmup_min_lr: float = 0.0,\n                 warmup_max_lr: float = 0.001,\n                 warmup_num_steps: int = 1000,\n                 last_batch_iteration: int = -1):\n\n        self.optimizer = get_torch_optimizer(optimizer)\n\n        self.min_lrs = self._format_param(self.optimizer, warmup_min_lr, ""min_lr"")\n        self.max_lrs = self._format_param(self.optimizer, warmup_max_lr, ""max_lr"")\n        self.delta_lrs = [big - small for big, small in zip(self.max_lrs, self.min_lrs)]\n        self.warmup_num_steps = warmup_num_steps\n        self.inverse_log_warm_up = 1.0 / math.log(warmup_num_steps)\n        self.last_batch_iteration = last_batch_iteration\n\n    def get_lr(self):\n        gamma = self._get_gamma()\n        return [\n            min_lr + (delta_lr * gamma) for min_lr,\n            delta_lr in zip(self.min_lrs,\n                            self.delta_lrs)\n        ]\n\n    def step(self, last_batch_iteration=None):\n        if last_batch_iteration is None:\n            last_batch_iteration = self.last_batch_iteration + 1\n        self.last_batch_iteration = last_batch_iteration\n        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n            param_group[\'lr\'] = lr\n\n    def state_dict(self):\n        return {\'last_batch_iteration\': self.last_batch_iteration}\n\n    def load_state_dict(self, sd):\n        self.last_batch_iteration = sd[\'last_batch_iteration\']\n\n    def _get_gamma(self):\n        if self.last_batch_iteration < self.warmup_num_steps:\n            return self.inverse_log_warm_up * math.log(self.last_batch_iteration + 1)\n        return 1.0\n\n    def _format_param(self, optimizer, param_value, param_name):\n        if isinstance(param_value, list) or isinstance(param_value, tuple):\n            if len(param_value) != len(optimizer.param_groups):\n                raise ValueError(""expected {} value for {}, got {}"".format(\n                    len(optimizer.param_groups),\n                    param_name,\n                    FileNotFoundError(param_value)))\n            return list(param_value)\n        return [param_value] * len(optimizer.param_groups)\n'"
deepspeed/pt/deepspeed_run.py,2,"b'""""""\nCopyright 2020 The Microsoft DeepSpeed Team\n""""""\n\nimport os\nimport sys\nimport json\nimport shutil\nimport base64\nimport argparse\nimport subprocess\nimport collections\nfrom copy import deepcopy\n\nimport torch.cuda\n\nfrom deepspeed.pt.deepspeed_constants import TORCH_DISTRIBUTED_DEFAULT_PORT\nfrom deepspeed.pt.log_utils import logger\n\nDLTS_HOSTFILE = ""/job/hostfile""\nEXPORT_ENVS = [""NCCL"", ""PYTHON""]\nDEEPSPEED_ENVIRONMENT_NAME = "".deepspeed_env""\nDEEPSPEED_ENVIRONMENT_PATHS = [os.path.expanduser(""~""), \'.\']\n\n\ndef parse_args(args=None):\n    parser = argparse.ArgumentParser(\n        description=""DeepSpeed runner to help launch distributed ""\n        ""multi-node/multi-gpu training jobs."")\n\n    parser.add_argument(""-H"",\n                        ""--hostfile"",\n                        type=str,\n                        default=DLTS_HOSTFILE,\n                        help=""Hostfile path (in MPI style) that defines the ""\n                        ""resource pool available to the job (e.g., ""\n                        ""worker-0 slots=4)"")\n\n    parser.add_argument(""-i"",\n                        ""--include"",\n                        type=str,\n                        default="""",\n                        help=\'\'\'Specify hardware resources to use during execution.\n                        String format is\n                                NODE_SPEC[@NODE_SPEC ...],\n                        where\n                                NODE_SPEC=NAME[:SLOT[,SLOT ...]].\n                        If :SLOT is omitted, include all slots on that host.\n                        Example: -i ""worker-0@worker-1:0,2"" will use all slots\n                        on worker-0 and slots [0, 2] on worker-1.\n                        \'\'\')\n\n    parser.add_argument(""-e"",\n                        ""--exclude"",\n                        type=str,\n                        default="""",\n                        help=\'\'\'Specify hardware resources to NOT use during execution.\n                        Mutually exclusive with --include. Resource formatting\n                        is the same as --include.\n                        Example: -e ""worker-1:0"" will use all available\n                        resources except slot 0 on worker-1.\n                        \'\'\')\n\n    parser.add_argument(""--num_nodes"", type=int, default=-1, help="""")\n\n    parser.add_argument(""--num_gpus"", type=int, default=-1, help="""")\n\n    parser.add_argument(""--master_port"",\n                        default=int(TORCH_DISTRIBUTED_DEFAULT_PORT),\n                        type=int,\n                        help=""(optional) Port used by PyTorch distributed for ""\n                        ""communication during training."")\n\n    parser.add_argument(""--master_addr"",\n                        default="""",\n                        type=str,\n                        help=""(optional) IP address of node 0, will be ""\n                        ""inferred via \'hostname -I\' if not specified."")\n\n    parser.add_argument(""user_script"",\n                        type=str,\n                        help=""User script to launch, followed by any required ""\n                        ""arguments."")\n    parser.add_argument(\'user_args\', nargs=argparse.REMAINDER)\n    return parser.parse_args(args=args)\n\n\ndef fetch_hostfile(hostfile_path):\n    if not os.path.isfile(hostfile_path):\n        logger.warning(""Unable to find hostfile, will proceed with training ""\n                       ""with local resources only."")\n        return None\n\n    # e.g., worker-0 slots=16\n    with open(hostfile_path, \'r\') as fd:\n\n        resource_pool = collections.OrderedDict()\n        for line in fd.readlines():\n            try:\n                hostname, slots = line.split()\n                _, slot_count = slots.split(""="")\n                slot_count = int(slot_count)\n            except ValueError as err:\n                logger.error(""Hostfile is not formatted correctly, unable to ""\n                             ""proceed with training."")\n                raise err\n            if hostname in resource_pool:\n                logger.error(""Hostfile contains duplicate hosts, unable to ""\n                             ""proceed with training."")\n                raise ValueError(""host {} is already defined"".format(hostname))\n            resource_pool[hostname] = slot_count\n\n    return resource_pool\n\n\ndef parse_resource_filter(host_info, include_str="""", exclude_str=""""):\n    \'\'\'Parse an inclusion or exclusion string and filter a hostfile dictionary.\n\n    String format is NODE_SPEC[@NODE_SPEC ...], where\n        NODE_SPEC = NAME[:SLOT[,SLOT ...]].\n    If :SLOT is omitted, include/exclude all slots on that host.\n\n    Examples:\n        include_str=""worker-0@worker-1:0,2"" will use all slots on worker-0 and\n          slots [0, 2] on worker-1.\n        exclude_str=""worker-1:0"" will use all available resources except\n          slot 0 on worker-1.\n    \'\'\'\n\n    # Constants that define our syntax\n    NODE_SEP = \'@\'\n    SLOT_LIST_START = \':\'\n    SLOT_SEP = \',\'\n\n    # Ensure include/exclude are mutually exclusive\n    if (include_str != """") and (exclude_str != """"):\n        raise ValueError(\'include_str and exclude_str are mutually exclusive.\')\n\n    # no-op\n    if (include_str == """") and (exclude_str == """"):\n        return host_info\n\n    # Either build from scratch or remove items\n    filtered_hosts = dict()\n    if include_str:\n        parse_str = include_str\n    if exclude_str != """":\n        filtered_hosts = deepcopy(host_info)\n        parse_str = exclude_str\n\n    # foreach node in the list\n    for node_config in parse_str.split(NODE_SEP):\n        # Node can either be alone or node:slot,slot,slot\n        if SLOT_LIST_START in node_config:\n            hostname, slots = node_config.split(SLOT_LIST_START)\n            slots = [int(x) for x in slots.split(SLOT_SEP)]\n\n            # sanity checks\n            if hostname not in host_info:\n                raise ValueError(""Hostname \'{}\' not found in hostfile"".format(hostname))\n            for s in slots:\n                if s not in host_info[hostname]:\n                    raise ValueError(""No slot \'{}\' specified on host \'{}\'"".format(\n                        s,\n                        hostname))\n\n            # If include string, build the list from here\n            if include_str:\n                filtered_hosts[hostname] = slots\n            elif exclude_str:\n                for s in slots:\n                    logger.info(\'removing {} from {}\'.format(s, hostname))\n                    filtered_hosts[hostname].remove(s)\n\n        # User just specified the whole node\n        else:\n            hostname = node_config\n            # sanity check hostname\n            if hostname not in host_info:\n                raise ValueError(""Hostname \'{}\' not found in hostfile"".format(hostname))\n\n            if include_str:\n                filtered_hosts[hostname] = host_info[hostname]\n            elif exclude_str:\n                filtered_hosts[hostname] = []\n\n    # Post-processing to remove duplicates and empty nodes\n    del_keys = []\n    for hostname in filtered_hosts:\n        # Remove duplicates\n        filtered_hosts[hostname] = list(set(filtered_hosts[hostname]))\n        # Remove empty hosts\n        if len(filtered_hosts[hostname]) == 0:\n            del_keys.append(hostname)\n    for name in del_keys:\n        del filtered_hosts[name]\n\n    # Lastly, go over filtered_hosts and convert to a OrderedDict() to ensure\n    # we map ranks to nodes correctly by maintaining host_info ordering.\n    ordered_hosts = collections.OrderedDict()\n    for host in host_info:\n        if host in filtered_hosts:\n            ordered_hosts[host] = filtered_hosts[host]\n\n    return ordered_hosts\n\n\ndef parse_inclusion_exclusion(resource_pool, inclusion, exclusion):\n    active_resources = collections.OrderedDict()\n    for hostname, slots in resource_pool.items():\n        active_resources[hostname] = list(range(slots))\n\n    return parse_resource_filter(active_resources,\n                                 include_str=inclusion,\n                                 exclude_str=exclusion)\n\n\ndef encode_world_info(world_info):\n    world_info_json = json.dumps(world_info).encode(\'utf-8\')\n    world_info_base64 = base64.urlsafe_b64encode(world_info_json).decode(\'utf-8\')\n    return world_info_base64\n\n\ndef main(args=None):\n    args = parse_args(args)\n\n    if args.num_nodes >= 0 or args.num_gpus >= 0:\n        if args.include != """" or args.exclude != """":\n            raise ValueError(""Cannot specify num_nodes/gpus with include/exclude"")\n\n    multi_node_exec = True\n    resource_pool = fetch_hostfile(args.hostfile)\n    if not resource_pool:\n        resource_pool = {}\n        device_count = torch.cuda.device_count()\n        if device_count == 0:\n            raise RuntimeError(""Unable to proceed, no GPU resources available"")\n        resource_pool[\'localhost\'] = device_count\n        args.master_addr = ""127.0.0.1""\n        multi_node_exec = False\n\n    if not multi_node_exec and args.num_nodes > 1:\n        raise ValueError(""Num nodes is >1 but no extra nodes available via hostfile"")\n\n    active_resources = parse_inclusion_exclusion(resource_pool,\n                                                 args.include,\n                                                 args.exclude)\n\n    if multi_node_exec and not shutil.which(\'pdsh\'):\n        raise RuntimeError(""pdsh is not installed, unable to proceed"")\n\n    env = os.environ.copy()\n\n    if not args.master_addr:\n        first_host = list(active_resources.keys())[0]\n        hostname_cmd = [""ssh {} hostname -I"".format(first_host)]\n        result = subprocess.check_output(hostname_cmd, shell=True)\n        args.master_addr = result.decode(\'utf-8\').split()[0]\n        logger.info(""Using IP address of {} for node {}"".format(\n            args.master_addr,\n            first_host))\n\n    if args.num_nodes > 0:\n        updated_active_resources = collections.OrderedDict()\n        for count, hostname in enumerate(active_resources.keys()):\n            if args.num_nodes == count:\n                break\n            updated_active_resources[hostname] = active_resources[hostname]\n        active_resources = updated_active_resources\n\n    if args.num_gpus > 0:\n        updated_active_resources = collections.OrderedDict()\n        for hostname in active_resources.keys():\n            updated_active_resources[hostname] = list(range(args.num_gpus))\n        active_resources = updated_active_resources\n\n    # encode world info as base64 to make it easier to pass via command line\n    world_info_base64 = encode_world_info(active_resources)\n\n    if not multi_node_exec:\n        deepspeed_launch = [\n            sys.executable,\n            ""-u"",\n            ""-m"",\n            ""deepspeed.pt.deepspeed_launch"",\n            ""--world_info={}"".format(world_info_base64),\n            ""--master_addr={}"".format(args.master_addr),\n            ""--master_port={}"".format(args.master_port)\n        ]\n        cmd = deepspeed_launch + [args.user_script] + args.user_args\n    else:\n        env[\'PDSH_RCMD_TYPE\'] = \'ssh\'\n\n        active_workers = "","".join(active_resources.keys())\n        logger.info(""Running on the following workers: %s"" % active_workers)\n\n        pdsh_cmd_args = [\'pdsh\', \'-w\', active_workers]\n\n        num_nodes = len(active_resources.keys())\n        num_gpus_per_node = None\n\n        curr_path = os.path.abspath(\'.\')\n        if \'PYTHONPATH\' in env:\n            env[\'PYTHONPATH\'] = curr_path + "":"" + env[\'PYTHONPATH\']\n        else:\n            env[\'PYTHONPATH\'] = curr_path\n\n        exports = """"\n        for var in env.keys():\n            if any(map(lambda name: var.startswith(name), EXPORT_ENVS)):\n                exports += ""export {}={}; "".format(var, env[var])\n\n        for environ_path in DEEPSPEED_ENVIRONMENT_PATHS:\n            environ_file = os.path.join(environ_path, DEEPSPEED_ENVIRONMENT_NAME)\n            if os.path.isfile(environ_file):\n                with open(environ_file, \'r\') as fd:\n                    for var in fd.readlines():\n                        exports += ""export {}; "".format(var.strip())\n\n        deepspeed_launch = [\n            exports,\n            ""cd {};"".format(curr_path),\n            sys.executable,\n            ""-u"",\n            ""-m"",\n            ""deepspeed.pt.deepspeed_launch"",\n            \'--world_info={}\'.format(world_info_base64),\n            ""--node_rank=%n"",\n            ""--master_addr={}"".format(args.master_addr),\n            ""--master_port={}"".format(args.master_port)\n        ]\n        cmd = pdsh_cmd_args + deepspeed_launch + [args.user_script] + args.user_args\n    logger.info(""cmd={}"".format(cmd))\n    result = subprocess.Popen(cmd, env=env)\n    result.wait()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
deepspeed/pt/deepspeed_timer.py,10,"b'\'\'\'\nCopyright 2019 The Microsoft DeepSpeed Team\n\'\'\'\n\nimport time\nimport psutil\nimport torch\n\nfrom deepspeed.pt.log_utils import logger\n\n\ndef print_rank_0(message):\n    if torch.distributed.is_initialized():\n        if torch.distributed.get_rank() == 0:\n            logger.info(message)\n    else:\n        logger.info(message)\n\n\nclass SynchronizedWallClockTimer:\n    """"""Group of timers. Borrowed from Nvidia Megatron code""""""\n    class Timer:\n        """"""Timer.""""""\n        def __init__(self, name):\n            self.name_ = name\n            self.elapsed_ = 0.0\n            self.started_ = False\n            self.start_time = time.time()\n\n        def start(self):\n            """"""Start the timer.""""""\n            assert not self.started_, \'timer has already been started\'\n            torch.cuda.synchronize()\n            self.start_time = time.time()\n            self.started_ = True\n\n        def stop(self):\n            """"""Stop the timer.""""""\n            assert self.started_, \'timer is not started\'\n            torch.cuda.synchronize()\n            self.elapsed_ += (time.time() - self.start_time)\n            self.started_ = False\n\n        def reset(self):\n            """"""Reset timer.""""""\n            self.elapsed_ = 0.0\n            self.started_ = False\n\n        def elapsed(self, reset=True):\n            """"""Calculate the elapsed time.""""""\n            started_ = self.started_\n            # If the timing in progress, end it first.\n            if self.started_:\n                self.stop()\n            # Get the elapsed time.\n            elapsed_ = self.elapsed_\n            # Reset the elapsed time\n            if reset:\n                self.reset()\n            # If timing was in progress, set it back.\n            if started_:\n                self.start()\n            return elapsed_\n\n    def __init__(self):\n        self.timers = {}\n\n    def __call__(self, name):\n        if name not in self.timers:\n            self.timers[name] = self.Timer(name)\n        return self.timers[name]\n\n    @staticmethod\n    def memory_usage():\n        alloc = ""mem_allocated: {:.4f} GB"".format(torch.cuda.memory_allocated() /\n                                                  (1024 * 1024 * 1024))\n        max_alloc = ""max_mem_allocated: {:.4f} GB"".format(\n            torch.cuda.max_memory_allocated() / (1024 * 1024 * 1024))\n        cache = ""cache_allocated: {:.4f} GB"".format(torch.cuda.memory_cached() /\n                                                    (1024 * 1024 * 1024))\n        max_cache = ""max_cache_allocated: {:.4f} GB"".format(\n            torch.cuda.max_memory_cached() / (1024 * 1024 * 1024))\n        return "" | {} | {} | {} | {}"".format(alloc, max_alloc, cache, max_cache)\n\n    def log(self, names, normalizer=1.0, reset=True, memory_breakdown=False):\n        """"""Log a group of timers.""""""\n        assert normalizer > 0.0\n        string = \'time (ms)\'\n        for name in names:\n            elapsed_time = self.timers[name].elapsed(reset=reset) * 1000.0 / normalizer\n            string += \' | {}: {:.2f}\'.format(name, elapsed_time)\n        if memory_breakdown:\n            string += self.memory_usage()\n        print_rank_0(string)\n\n\nclass ThroughputTimer():\n    def __init__(self,\n                 batch_size,\n                 num_workers,\n                 start_step=2,\n                 steps_per_output=50,\n                 monitor_memory=True,\n                 logging_fn=None):\n        self.start_time = 0\n        self.end_time = 0\n        self.started = False\n        self.batch_size = batch_size\n        if batch_size is None:\n            self.batch_size = 1\n        self.num_workers = num_workers\n        self.start_step = start_step\n        self.epoch_count = 0\n        self.local_step_count = 0\n        self.total_step_count = 0\n        self.total_elapsed_time = 0\n        self.steps_per_output = steps_per_output\n        self.monitor_memory = monitor_memory\n        self.logging = logging_fn\n        if self.logging is None:\n            self.logging = logger.info\n        self.initialized = False\n\n    def update_epoch_count(self):\n        self.epoch_count += 1\n        self.local_step_count = 0\n\n    def _init_timer(self):\n        self.initialized = True\n\n    def start(self):\n        self._init_timer()\n        self.started = True\n        if self.total_step_count >= self.start_step:\n            torch.cuda.synchronize()\n            self.start_time = time.time()\n\n    def stop(self, report_speed=True):\n        if not self.started:\n            return\n        self.started = False\n        self.total_step_count += 1\n        self.local_step_count += 1\n        if self.total_step_count > self.start_step:\n            torch.cuda.synchronize()\n            self.end_time = time.time()\n            duration = self.end_time - self.start_time\n            self.total_elapsed_time += duration\n            if self.local_step_count % self.steps_per_output == 0:\n                if report_speed:\n                    self.logging(""{}/{}, SamplesPerSec={}"".format(\n                        self.epoch_count,\n                        self.local_step_count,\n                        self.avg_samples_per_sec()))\n                if self.monitor_memory:\n                    virt_mem = psutil.virtual_memory()\n                    swap = psutil.swap_memory()\n                    self.logging(""{}/{}, vm percent: {}, swap percent: {}"".format(\n                        self.epoch_count,\n                        self.local_step_count,\n                        virt_mem.percent,\n                        swap.percent))\n\n    def avg_samples_per_sec(self):\n        if self.total_step_count > 0:\n            samples_per_step = self.batch_size * self.num_workers\n            total_step_offset = self.total_step_count - self.start_step\n            avg_time_per_step = self.total_elapsed_time / total_step_offset\n            # training samples per second\n            return samples_per_step / avg_time_per_step\n        return float(""-inf"")\n'"
deepspeed/pt/deepspeed_utils.py,40,"b'\'\'\'\nCopyright 2019 The Microsoft DeepSpeed Team\n\nCopyright NVIDIA/Megatron\n\nHelper functions and classes from multiple sources.\n\'\'\'\n\nimport torch\nfrom torch._six import inf\n\nfrom deepspeed.pt.log_utils import logger\n\n\nclass CheckOverflow(object):\n    \'\'\'Checks for overflow in gradient across parallel process\'\'\'\n    def __init__(self, param_groups=None, mpu=None, zero_reduce_scatter=False):\n        self.mpu = mpu\n        self.params = [] if param_groups else None\n        self.zero_reduce_scatter = zero_reduce_scatter\n        if param_groups:\n            for group in param_groups:\n                for param in group:\n                    self.params.append(param)\n\n    def check_using_norm(self, norm_group):\n        overflow = -1 in norm_group\n\n        if self.mpu is not None:\n            overflow_gpu = torch.cuda.ByteTensor([overflow])\n            torch.distributed.all_reduce(overflow_gpu,\n                                         op=torch.distributed.ReduceOp.MAX,\n                                         group=self.mpu.get_model_parallel_group())\n            overflow = overflow_gpu[0].item()\n\n        return bool(overflow)\n\n    def check(self, param_groups=None):\n\n        #TODO: what\'s the equivalent here? do we need this?\n        # for group in self.fp32_from_fp32_groups:\n        #     for param in group:\n        #         params.append(param)\n\n        params = []\n        if param_groups is None:\n            params = self.params\n        else:\n            assert param_groups is not None, \\\n                ""self.params and param_groups both cannot be none""\n\n            for group in param_groups:\n                for param in group:\n                    params.append(param)\n\n        return self.has_overflow(params)\n\n    # `params` is a list / generator of torch.Variable\n    def has_overflow_serial(self, params):\n        for i, p in enumerate(params):\n            if p.grad is not None and self._has_inf_or_nan(p.grad.data, i):\n                return True\n        return False\n\n    def has_overflow(self, params):\n        overflow = self.has_overflow_serial(params)\n        # Since each model parallel GPU carries only part of the model,\n        # make sure overflow flag is synced across all the model parallel GPUs\n        overflow_gpu = torch.cuda.ByteTensor([overflow])\n        #torch.distributed.all_reduce(overflow_gpu,\n        #                             op=torch.distributed.ReduceOp.MAX,\n        #                             group=mpu.get_model_parallel_group())\n        if self.zero_reduce_scatter:\n            torch.distributed.all_reduce(overflow_gpu,\n                                         op=torch.distributed.ReduceOp.MAX,\n                                         group=torch.distributed.group.WORLD)\n        elif self.mpu is not None:\n            torch.distributed.all_reduce(overflow_gpu,\n                                         op=torch.distributed.ReduceOp.MAX,\n                                         group=self.mpu.get_model_parallel_group())\n        overflow = overflow_gpu[0].item()\n        return bool(overflow)\n\n    # `x` is a torch.Tensor\n    @staticmethod\n    def _has_inf_or_nan(x, i):\n        try:\n            # if x is half, the .float() incurs an additional deep copy, but it\'s necessary if\n            # Pytorch\'s .sum() creates a one-element tensor of the same type as x\n            # (which is true for some recent version of pytorch).\n            cpu_sum = float(x.float().sum())\n            # More efficient version that can be used if .sum() returns a Python scalar\n            # cpu_sum = float(x.sum())\n        except RuntimeError as instance:\n            # We want to check if inst is actually an overflow exception.\n            # RuntimeError could come from a different error.\n            # If so, we still want the exception to propagate.\n            if ""value cannot be converted"" not in instance.args[0]:\n                raise\n            return True\n        else:\n            if cpu_sum == float(\'inf\') or cpu_sum == -float(\'inf\') or cpu_sum != cpu_sum:\n                _handle_overflow(cpu_sum, x, i)\n                return True\n            return False\n\n\ndef _handle_overflow(cpu_sum, x, i):\n    import math\n    rank = torch.distributed.get_rank()\n    if rank == 0:\n        t_i = -1\n        for v_i, v in enumerate(x.data.contiguous().view(-1)):\n            if not math.isfinite(float(v)):\n                t_i = v_i\n                break\n        logger.info(\n            f""rank {rank} detected overflow {cpu_sum} in tensor {i}:{t_i} shape {x.shape}""\n        )\n\n\ndef get_grad_norm(parameters, norm_type=2, mpu=None):\n    """"""Clips gradient norm of an iterable of parameters.\n\n    This is adapted from torch.nn.utils.clip_grad.clip_grad_norm_ and\n    added functionality to handle model parallel parameters. Note that\n    the gradients are modified in place. Taken from Nvidia Megatron.\n\n    Arguments:\n        parameters (Iterable[Tensor] or Tensor): an iterable of Tensors or a\n            single Tensor that will have gradients normalized\n        max_norm (float or int): max norm of the gradients\n        norm_type (float or int): type of the used p-norm. Can be ``\'inf\'`` for\n            infinity norm.\n\n    Returns:\n        Total norm of the parameters (viewed as a single vector).\n    """"""\n    if isinstance(parameters, torch.Tensor):\n        parameters = [parameters]\n    parameters = list(filter(lambda p: p.grad is not None, parameters))\n\n    norm_type = float(norm_type)\n    if norm_type == inf:\n        total_norm = max(p.grad.data.abs().max() for p in parameters)\n        total_norm_cuda = torch.cuda.FloatTensor([float(total_norm)])\n        # Take max across all GPUs.\n        if mpu is not None:\n            torch.distributed.all_reduce(total_norm_cuda,\n                                         op=torch.distributed.ReduceOp.MAX,\n                                         group=mpu.get_model_parallel_group())\n        total_norm = total_norm_cuda[0].item()\n    else:\n        total_norm = 0.\n        for p in parameters:\n            if mpu is not None:\n                if (mpu.get_model_parallel_rank() == 0) or (hasattr(p,\n                                                                    \'model_parallel\')\n                                                            and p.model_parallel):\n                    param_norm = p.grad.data.float().norm(norm_type)\n                    total_norm += param_norm.item()**norm_type\n            else:\n                param_norm = p.grad.data.float().norm(norm_type)\n                total_norm += param_norm.item()**norm_type\n\n        # Sum across all model parallel GPUs.\n        total_norm_cuda = torch.cuda.FloatTensor([float(total_norm)])\n        if mpu is not None:\n            torch.distributed.all_reduce(total_norm_cuda,\n                                         op=torch.distributed.ReduceOp.SUM,\n                                         group=mpu.get_model_parallel_group())\n        total_norm = total_norm_cuda[0].item()**(1. / norm_type)\n\n    if total_norm == float(\n            \'inf\') or total_norm == -float(\'inf\') or total_norm != total_norm:\n        total_norm = -1\n\n    return total_norm\n\n\ndef get_weight_norm(parameters, norm_type=2, mpu=None):\n    """"""Clips gradient norm of an iterable of parameters.\n\n    This is adapted from torch.nn.utils.clip_grad.clip_grad_norm_ and\n    added functionality to handle model parallel parameters. Note that\n    the gradients are modified in place. Taken from Nvidia Megatron.\n\n    Arguments:\n        parameters (Iterable[Tensor] or Tensor): an iterable of Tensors or a\n            single Tensor that will have gradients normalized\n        max_norm (float or int): max norm of the gradients\n        norm_type (float or int): type of the used p-norm. Can be ``\'inf\'`` for\n            infinity norm.\n\n    Returns:\n        Total norm of the parameters (viewed as a single vector).\n    """"""\n    if isinstance(parameters, torch.Tensor):\n        parameters = [parameters]\n\n    norm_type = float(norm_type)\n    if norm_type == inf:\n        total_norm = max(p.data.abs().max() for p in parameters)\n        total_norm_cuda = torch.cuda.FloatTensor([float(total_norm)])\n        # Take max across all GPUs.\n        if mpu is not None:\n            torch.distributed.all_reduce(total_norm_cuda,\n                                         op=torch.distributed.ReduceOp.MAX,\n                                         group=mpu.get_model_parallel_group())\n        total_norm = total_norm_cuda[0].item()\n    else:\n        total_norm = 0.\n        for p in parameters:\n            if mpu is not None:\n                if (mpu.get_model_parallel_rank() == 0) or (hasattr(p,\n                                                                    \'model_parallel\')\n                                                            and p.model_parallel):\n                    try:\n                        param_norm = float(torch.norm(p, norm_type, dtype=torch.float32))\n                    except TypeError as err:\n                        param_norm = float(torch.norm(p.float(), norm_type))\n\n                    #param_norm = p.data.float().norm(norm_type)\n                    total_norm += param_norm**norm_type\n            else:\n                try:\n                    param_norm = float(torch.norm(p, norm_type, dtype=torch.float32))\n                except TypeError as err:\n                    param_norm = float(torch.norm(p.float(), norm_type))\n                #param_norm = p.data.float().norm(norm_type)\n                total_norm += param_norm**norm_type\n\n        # Sum across all model parallel GPUs.\n        total_norm_cuda = torch.cuda.FloatTensor([float(total_norm)])\n        if mpu is not None:\n            torch.distributed.all_reduce(total_norm_cuda,\n                                         op=torch.distributed.ReduceOp.SUM,\n                                         group=mpu.get_model_parallel_group())\n        total_norm = total_norm_cuda[0].item()**(1. / norm_type)\n\n    if total_norm == float(\n            \'inf\') or total_norm == -float(\'inf\') or total_norm != total_norm:\n        total_norm = -1\n\n    return total_norm\n\n\ndef is_model_parallel_parameter(p):\n    return hasattr(p, \'model_parallel\') and p.model_parallel\n\n\ndef see_memory_usage(message):\n    return\n    if torch.distributed.is_initialized() and not torch.distributed.get_rank() == 0:\n        return\n\n    # Print message except when distributed but not rank 0\n    logger.info(message)\n    logger.info(\n        ""Memory Allocated %s GigaBytes "",\n        torch.cuda.memory_allocated() / (1024 * 1024 * 1024),\n    )\n    logger.info(\n        ""Max Memory Allocated %s GigaBytes"",\n        torch.cuda.max_memory_allocated() / (1024 * 1024 * 1024),\n    )\n    logger.info(\n        ""Cache Allocated %s GigaBytes"",\n        torch.cuda.memory_cached() / (1024 * 1024 * 1024),\n    )\n    logger.info(\n        ""Max cache Allocated %s GigaBytes"",\n        torch.cuda.max_memory_cached() / (1024 * 1024 * 1024),\n    )\n'"
deepspeed/pt/deepspeed_zero_config.py,0,"b'""""""\r\nCopyright (c) Microsoft Corporation\r\nLicensed under the MIT license.\r\n""""""\r\n\r\n#from deepspeed.pt.deepspeed_constants import *\r\nfrom deepspeed.pt.deepspeed_config_utils import get_scalar_param\r\nfrom deepspeed.pt.log_utils import logger\r\n\r\n#########################################\r\n# ZeRO optimization\r\n#########################################\r\n# ZeRO optimization. By default, this optimization is not enabled.\r\n# Users have to configure the desired optimization (0 means disabled) in params.json as below example:\r\nZERO_FORMAT = \'\'\'\r\nZeRO optimization should be enabled as:\r\n""session_params"": {\r\n  ""zero_optimization"": {\r\n    ""stage"": [0|1|2],\r\n    ""allgather_partitions"": [true|false],\r\n    ""allgather_bucket_size"": 500000000,\r\n    ""reduce_scatter"": [true|false],\r\n    ""contiguous_gradients"" : [true|false]\r\n    ""overlap_comm"": [true|false],\r\n    ""reduce_bucket_size"": 500000000\r\n    }\r\n}\r\n\'\'\'\r\n\r\nZERO_OPTIMIZATION = \'zero_optimization\'\r\nZERO_OPTIMIZATION_DISABLED = 0\r\nZERO_OPTIMIZATION_OPTIMIZER_STATES = 1\r\nZERO_OPTIMIZATION_GRADIENTS = 2\r\nZERO_OPTIMIZATION_WEIGHTS = 3\r\nMAX_STAGE_ZERO_OPTIMIZATION = ZERO_OPTIMIZATION_GRADIENTS\r\n\r\nZERO_OPTIMIZATION_STAGE = \'stage\'\r\nZERO_OPTIMIZATION_STAGE_1 = \'stage_1\'\r\nZERO_OPTIMIZATION_STAGE_2 = \'stage_2\'\r\nZERO_OPTIMIZATION_STAGE_3 = \'stage_3\'\r\n\r\nZERO_OPTIMIZATION_STAGE_DEFAULT = ZERO_OPTIMIZATION_DISABLED\r\n\r\nZERO_OPTIMIZATION_ALLGATHER_PARTITIONS = \'allgather_partitions\'\r\nZERO_OPTIMIZATION_ALLGATHER_PARTITIONS_DEFAULT = True\r\n\r\nZERO_OPTIMIZATION_REDUCE_SCATTER = \'reduce_scatter\'\r\nZERO_OPTIMIZATION_REDUCE_SCATTER_DEFAULT = True\r\n\r\nZERO_OPTIMIZATION_OVERLAP_COMM = \'overlap_comm\'\r\nZERO_OPTIMIZATION_OVERLAP_COMM_DEFAULT = False\r\n\r\nZERO_OPTIMIZATION_CONTIGUOUS_GRADIENTS = \'contiguous_gradients\'\r\nZERO_OPTIMIZATION_CONTIGUOUS_GRADIENTS_DEFAULT = False\r\n\r\nZERO_OPTIMIZATION_REDUCE_BUCKET_SIZE = \'reduce_bucket_size\'\r\nZERO_OPTIMIZATION_REDUCE_BUCKET_SIZE_DEFAULT = 500000000\r\n\r\nZERO_OPTIMIZATION_ALLGATHER_BUCKET_SIZE = \'allgather_bucket_size\'\r\nZERO_OPTIMIZATION_ALLGATHER_BUCKET_SIZE_DEFAULT = 500000000\r\nZERO_OPTIMIZATION_ALLGATHER_BUCKET_SIZE_DEPRECATED = \'allgather_size\'\r\n\r\nZERO_OPTIMIZATION_DEFAULT = {\r\n    ZERO_OPTIMIZATION_STAGE: ZERO_OPTIMIZATION_STAGE_DEFAULT,\r\n    ZERO_OPTIMIZATION_CONTIGUOUS_GRADIENTS:\r\n    ZERO_OPTIMIZATION_CONTIGUOUS_GRADIENTS_DEFAULT,\r\n    ZERO_OPTIMIZATION_REDUCE_SCATTER: ZERO_OPTIMIZATION_REDUCE_SCATTER_DEFAULT,\r\n    ZERO_OPTIMIZATION_REDUCE_BUCKET_SIZE: ZERO_OPTIMIZATION_REDUCE_BUCKET_SIZE_DEFAULT,\r\n    ZERO_OPTIMIZATION_ALLGATHER_PARTITIONS:\r\n    ZERO_OPTIMIZATION_ALLGATHER_PARTITIONS_DEFAULT,\r\n    ZERO_OPTIMIZATION_ALLGATHER_BUCKET_SIZE:\r\n    ZERO_OPTIMIZATION_ALLGATHER_BUCKET_SIZE_DEFAULT\r\n}\r\n\r\n\r\nclass DeepSpeedZeroConfig(object):\r\n    def __init__(self, param_dict):\r\n        super(DeepSpeedZeroConfig, self).__init__()\r\n\r\n        self.stage = None\r\n        self.contiguous_gradients = None\r\n        self.reduce_scatter = None\r\n        self.reduce_bucket_size = None\r\n        self.allgather_partitions = None\r\n        self.allgather_bucket_size = None\r\n        self.overlap_comm = None\r\n\r\n        if ZERO_OPTIMIZATION in param_dict.keys():\r\n            zero_config_dict = param_dict[ZERO_OPTIMIZATION]\r\n            if type(zero_config_dict) is bool:\r\n                zero_config_dict = self.read_zero_config_deprecated(param_dict)\r\n        else:\r\n            zero_config_dict = ZERO_OPTIMIZATION_DEFAULT\r\n\r\n        self._initialize(zero_config_dict)\r\n\r\n    def read_zero_config_deprecated(self, param_dict):\r\n        zero_config_dict = {}\r\n        zero_config_dict[\r\n            ZERO_OPTIMIZATION_STAGE] = 1 if param_dict[ZERO_OPTIMIZATION] else 0\r\n        if zero_config_dict[ZERO_OPTIMIZATION_STAGE] > 0:\r\n            zero_config_dict[ZERO_OPTIMIZATION_ALLGATHER_BUCKET_SIZE] = get_scalar_param(\r\n                param_dict,\r\n                ZERO_OPTIMIZATION_ALLGATHER_BUCKET_SIZE_DEPRECATED,\r\n                ZERO_OPTIMIZATION_ALLGATHER_BUCKET_SIZE_DEFAULT)\r\n\r\n        logger.warning(\r\n            \'DeepSpeedConfig: this format of ZeRO optimization setup is deprecated. Please use the following format: {}\'\r\n            .format(ZERO_FORMAT))\r\n        return zero_config_dict\r\n\r\n    """"""\r\n    For json serialization\r\n    """"""\r\n\r\n    def repr(self):\r\n        return self.__dict__\r\n\r\n    def _initialize(self, zero_config_dict):\r\n        self.stage = get_scalar_param(zero_config_dict,\r\n                                      ZERO_OPTIMIZATION_STAGE,\r\n                                      ZERO_OPTIMIZATION_STAGE_DEFAULT)\r\n\r\n        self.contiguous_gradients = get_scalar_param(\r\n            zero_config_dict,\r\n            ZERO_OPTIMIZATION_CONTIGUOUS_GRADIENTS,\r\n            ZERO_OPTIMIZATION_CONTIGUOUS_GRADIENTS_DEFAULT)\r\n\r\n        self.reduce_bucket_size = get_scalar_param(\r\n            zero_config_dict,\r\n            ZERO_OPTIMIZATION_REDUCE_BUCKET_SIZE,\r\n            ZERO_OPTIMIZATION_REDUCE_BUCKET_SIZE_DEFAULT)\r\n\r\n        self.reduce_scatter = get_scalar_param(zero_config_dict,\r\n                                               ZERO_OPTIMIZATION_REDUCE_SCATTER,\r\n                                               ZERO_OPTIMIZATION_REDUCE_SCATTER_DEFAULT)\r\n\r\n        self.overlap_comm = get_scalar_param(zero_config_dict,\r\n                                             ZERO_OPTIMIZATION_OVERLAP_COMM,\r\n                                             ZERO_OPTIMIZATION_OVERLAP_COMM_DEFAULT)\r\n\r\n        self.allgather_partitions = get_scalar_param(\r\n            zero_config_dict,\r\n            ZERO_OPTIMIZATION_ALLGATHER_PARTITIONS,\r\n            ZERO_OPTIMIZATION_ALLGATHER_PARTITIONS_DEFAULT)\r\n\r\n        self.allgather_bucket_size = get_scalar_param(\r\n            zero_config_dict,\r\n            ZERO_OPTIMIZATION_ALLGATHER_BUCKET_SIZE,\r\n            ZERO_OPTIMIZATION_ALLGATHER_BUCKET_SIZE_DEFAULT)\r\n'"
deepspeed/pt/deepspeed_zero_optimizer.py,60,"b'\'\'\'\r\nCopyright 2019 The Microsoft DeepSpeed Team\r\n\'\'\'\r\n\r\nimport torch\r\nfrom torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors\r\nfrom torch.distributed.distributed_c10d import _get_global_rank\r\nimport torch.distributed as dist\r\nimport math\r\nfrom torch._six import inf\r\nfrom torch.autograd import Variable\r\n\r\nfrom deepspeed.pt.loss_scaler import LossScaler, DynamicLossScaler\r\nfrom deepspeed.pt.deepspeed_utils import see_memory_usage, is_model_parallel_parameter\r\n\r\n#Toggle this to true to enable correctness test\r\n#with gradient partitioning and without\r\npg_correctness_test = False\r\n\r\nfrom deepspeed.pt.log_utils import logger\r\n\r\ntry:\r\n    from apex_C import flatten\r\n    from apex_C import unflatten\r\nexcept ImportError:\r\n    try:\r\n        _ = warned_flatten\r\n    except NameError:\r\n        logger.warning(\r\n            ""apex was installed without --cpp_ext.  Falling back to Python flatten and unflatten.""\r\n        )\r\n        warned_flatten = True\r\n    from torch._utils import _flatten_dense_tensors as flatten\r\n    from torch._utils import _unflatten_dense_tensors as unflatten\r\n\r\n\r\ndef input(msg):\r\n    return\r\n\r\n\r\ndef split_half_float_double(tensors):\r\n    dtypes = [\r\n        ""torch.cuda.HalfTensor"",\r\n        ""torch.cuda.FloatTensor"",\r\n        ""torch.cuda.DoubleTensor""\r\n    ]\r\n    buckets = []\r\n    for i, dtype in enumerate(dtypes):\r\n        bucket = [t for t in tensors if t.type() == dtype]\r\n        if bucket:\r\n            buckets.append(bucket)\r\n    return buckets\r\n\r\n\r\ndef isclose(a, b, rtol=1e-09, atol=0.0):\r\n    return abs(a - b) <= max(rtol * max(abs(a), abs(b)), atol)\r\n\r\n\r\ndef lcm(x, y):\r\n    from fractions import gcd  # or can import gcd from `math` in Python 3\r\n    return x * y // gcd(x, y)\r\n\r\n\r\n#create a flat tensor aligned at the alignment boundary\r\ndef flatten_dense_tensors_aligned(tensor_list, alignment, pg):\r\n    num_elements = 0\r\n    for tensor in tensor_list:\r\n        num_elements = num_elements + tensor.numel()\r\n\r\n    remaining = num_elements % alignment\r\n\r\n    if remaining:\r\n        elements_to_add = alignment - remaining\r\n        pad_tensor = torch.zeros(elements_to_add,\r\n                                 device=tensor_list[0].device,\r\n                                 dtype=tensor_list[0].dtype)\r\n        padded_tensor_list = tensor_list + [pad_tensor]\r\n\r\n        num_elements = num_elements + elements_to_add\r\n    else:\r\n        padded_tensor_list = tensor_list\r\n\r\n    return _flatten_dense_tensors(padded_tensor_list)\r\n\r\n\r\ndef move_to_cpu(tensor_list):\r\n    for tensor in tensor_list:\r\n        tensor.data = tensor.data.cpu()\r\n\r\n\r\nclass FP16_DeepSpeedZeroOptimizer(object):\r\n    """"""\r\n    DeepSpeedZeroOptimizer designed to reduce the memory footprint\r\n    required for training large deep learning models.\r\n\r\n    For more details please see ZeRO: Memory Optimization Towards Training A Trillion Parameter Models\r\n    https://arxiv.org/abs/1910.02054\r\n\r\n    For usage examples, refer to TODO: DeepSpeed Tutorial\r\n\r\n    """"""\r\n    def __init__(self,\r\n                 init_optimizer,\r\n                 timers,\r\n                 static_loss_scale=1.0,\r\n                 dynamic_loss_scale=False,\r\n                 dynamic_loss_args=None,\r\n                 verbose=True,\r\n                 contiguous_gradients=True,\r\n                 reduce_bucket_size=500000000,\r\n                 allgather_bucket_size=5000000000,\r\n                 dp_process_group=None,\r\n                 reduce_scatter=True,\r\n                 overlap_comm=False,\r\n                 mpu=None,\r\n                 clip_grad=0.0,\r\n                 allreduce_always_fp32=False,\r\n                 postscale_gradients=True,\r\n                 gradient_predivide_factor=1.0):\r\n\r\n        if dist.get_rank() == 0:\r\n            logger.info(f""Reduce bucket size {reduce_bucket_size}"")\r\n            logger.info(f""Allgather bucket size {allgather_bucket_size}"")\r\n        # The fused optimizer does all the work. We need this layer for two reason:\r\n        # 1. maintain same user API from apex.fp16_utils\r\n        # 2. keep common stuff here in case we need to add ne552w fused optimizer later\r\n\r\n        # differences from apex.fp16_utils:\r\n        # - assume all model params in fp16\r\n        # - assume all params requires grad\r\n        # - flat by groups, not keeping state. TODO: remove state explicitly?\r\n        # - master gard and unflat master weight never exist. TODO: a way to save out unflat master?\r\n        if not torch.cuda.is_available:\r\n            raise SystemError(""Cannot use fp16 without CUDA."")\r\n        self.optimizer = init_optimizer\r\n\r\n        self.timers = timers\r\n\r\n        self.reduce_scatter = reduce_scatter\r\n\r\n        self.overlap_comm = overlap_comm\r\n\r\n        self.dp_process_group = dp_process_group\r\n\r\n        self.partition_count = dist.get_world_size(group=self.dp_process_group)\r\n\r\n        if mpu is None:\r\n            self.model_parallel_group = None\r\n            self.model_parallel_rank = 0\r\n        else:\r\n            self.model_parallel_group = mpu.get_model_parallel_group()\r\n            self.model_parallel_rank = mpu.get_model_parallel_rank()\r\n\r\n        self.overflow = False\r\n        self.clip_grad = clip_grad\r\n        self.allreduce_always_fp32 = allreduce_always_fp32\r\n        self.gradient_predivide_factor = gradient_predivide_factor\r\n        self.postscale_gradients = postscale_gradients\r\n\r\n        if self.reduce_scatter:\r\n            assert not self.allreduce_always_fp32, ""allreduce_always_fp32 is not yet supported with ZeRO-2 with reduce scatter enabled""\r\n            assert self.gradient_predivide_factor == 1.0, ""gradient_predivide_factor != 1.0 is not yet supported with ZeRO-2 with reduce scatter enabled""\r\n            assert self.postscale_gradients, ""pre-scale gradients is not yet supported with ZeRO-2 with reduce scatter enabled""\r\n\r\n        # param flattened by groups\r\n        self.fp16_groups = []\r\n        self.fp16_groups_flat = []\r\n\r\n        #param partitioned by data parallel degree\r\n        #this will contain a list of equal sized tensors\r\n        #each of which will be updated by a different process\r\n        self.parallel_partitioned_fp16_groups = []\r\n\r\n        #a single 32-bit partition of the parallel partitioned parameters\r\n        #that this process will update\r\n        self.single_partition_of_fp32_groups = []\r\n\r\n        #param partition info\r\n\r\n        #These are the parameters in each group that will not be updated by this process directly\r\n        self.params_not_in_partition = []\r\n\r\n        #These are the parameters that will be updated by this process directly\r\n        self.params_in_partition = []\r\n\r\n        #Offset from the first paramter in the the self.params_in_partition\r\n        #the parameter boundaries may not align with partition boundaries\r\n        #so we need to keep track of the offset\r\n        self.first_offset = []\r\n\r\n        #number of elements per partition in each group\r\n        self.partition_size = []\r\n\r\n        partition_id = dist.get_rank(group=self.dp_process_group)\r\n\r\n        self.all_reduce_print = False\r\n\r\n        # loop to deal with groups\r\n        for i, param_group in enumerate(self.optimizer.param_groups):\r\n            # push this group to list before modify\r\n            self.fp16_groups.append(param_group[\'params\'])\r\n\r\n            #not sure why apex was cloning the weights before flattening\r\n            #removing cloning here\r\n\r\n            see_memory_usage(f""Before moving param group {i} to CPU"")\r\n            #move all the parameters to cpu to free up GPU space for creating flat buffer\r\n            move_to_cpu(self.fp16_groups[i])\r\n            see_memory_usage(f""After moving param group {i} to CPU"")\r\n\r\n            #create flat buffer in CPU and move to GPU\r\n            self.fp16_groups_flat.append(\r\n                flatten_dense_tensors_aligned(\r\n                    self.fp16_groups[i],\r\n                    dist.get_world_size(group=self.dp_process_group),\r\n                    self.dp_process_group).cuda(torch.cuda.current_device()))\r\n            see_memory_usage(f""After flattening and moving param group {i} to GPU"")\r\n\r\n            if dist.get_rank(group=self.dp_process_group) == 0:\r\n                see_memory_usage(\r\n                    f""After Flattening and after emptying param group {i} cache"")\r\n\r\n            # set model fp16 weight to slices of flattened buffer\r\n            updated_params = _unflatten_dense_tensors(self.fp16_groups_flat[i],\r\n                                                      self.fp16_groups[i])\r\n            for p, q in zip(self.fp16_groups[i], updated_params):\r\n                p.data = q.data\r\n\r\n            #divide the flat weights into near equal paritition equal to the data parallel degree\r\n            #each process will compute on a different part of the partition\r\n            data_parallel_partitions = self.get_data_parallel_partitions(\r\n                self.fp16_groups_flat[i])\r\n            self.parallel_partitioned_fp16_groups.append(data_parallel_partitions)\r\n\r\n            # a partition of the fp32 master weights that will be updated by this process\r\n            self.single_partition_of_fp32_groups.append(\r\n                self.parallel_partitioned_fp16_groups[i]\r\n                [partition_id].clone().float().detach())\r\n\r\n            # modify optimizer of have flat master weight\r\n            self.single_partition_of_fp32_groups[\r\n                i].requires_grad = True  # keep this in case internal optimizer uses it\r\n            param_group[\'params\'] = [self.single_partition_of_fp32_groups[i]]\r\n\r\n            partition_size = len(self.fp16_groups_flat[i]) / dist.get_world_size(\r\n                group=self.dp_process_group)\r\n            params_in_partition, params_not_in_partition, first_offset = self.get_partition_info(self.fp16_groups[i], partition_size, partition_id)\r\n\r\n            self.partition_size.append(partition_size)\r\n            self.params_in_partition.append(params_in_partition)\r\n            self.params_not_in_partition.append(params_not_in_partition)\r\n            self.first_offset.append(first_offset)\r\n\r\n        self.reduce_bucket_size = int(reduce_bucket_size)\r\n        self.allgather_bucket_size = int(allgather_bucket_size)\r\n\r\n        self.reduction_event = torch.cuda.Event(enable_timing=False, blocking=False)\r\n        self.reduction_stream = torch.cuda.Stream()\r\n        self.callback_queued = False\r\n\r\n        self.param_dict = {}\r\n\r\n        #map between param_id and bool to specify if a param is in this partition\r\n        self.is_param_in_current_partition = {}\r\n\r\n        self.contiguous_gradients = contiguous_gradients\r\n        self.grads_in_ipg_bucket = []\r\n        self.params_in_ipg_bucket = []\r\n        self.elements_in_ipg_bucket = 0\r\n        self.params_already_reduced = []\r\n        self._release_ipg_buffers()\r\n        self.previous_reduced_grads = None\r\n\r\n        #simplified param id\r\n        self.param_id = {}\r\n\r\n        count = 0\r\n        for i, params_group in enumerate(self.fp16_groups):\r\n            for param in params_group:\r\n                unique_id = id(param)\r\n                self.param_id[unique_id] = count\r\n                self.param_dict[count] = param\r\n                self.params_already_reduced.append(False)\r\n                count = count + 1\r\n\r\n        for param_group in self.params_in_partition:\r\n            for param in param_group:\r\n                self.is_param_in_current_partition[self.get_param_id(param)] = True\r\n\r\n        for param_group in self.params_not_in_partition:\r\n            for param in param_group:\r\n                self.is_param_in_current_partition[self.get_param_id(param)] = False\r\n\r\n        #mapping from parameter to partition that it belongs to\r\n        self.param_to_partition_ids = {}\r\n\r\n        #stores if a partition has been reduced in this step\r\n        self.is_partition_reduced = {}\r\n\r\n        #number of grads in partition that still need to be computed\r\n        self.remaining_grads_in_partition = {}\r\n\r\n        #total number of grads in partition\r\n        self.total_grads_in_partition = {}\r\n\r\n        #stores if a grad in a partition has been computed or not\r\n        self.is_grad_computed = {}\r\n\r\n        #stores the offset at which a parameter gradient needs to be inserted in a partition\r\n        self.grad_partition_insertion_offset = {}\r\n\r\n        #the offset in the gradient at which it must be inserted at the beginning of the paritition\r\n        self.grad_start_offset = {}\r\n\r\n        #will store the averaged gradients required by this parititon\r\n        self.averaged_gradients = {}\r\n\r\n        # store index of first parameter in each partition\r\n        self.first_param_index_in_partition = {}\r\n\r\n        #initializes all data structures for implementing gradient partitioning\r\n        self.initialize_gradient_partitioning_data_structures()\r\n\r\n        #resets the data structure value for the next backward propagation\r\n        self.reset_partition_gradient_structures()\r\n\r\n        #creates backward hooks for gradient partitioning\r\n        self.create_reduce_and_remove_grad_hooks()\r\n\r\n        # we may have a way of fusing dynamic scale. Do not support for now\r\n        if dynamic_loss_scale:\r\n            if dynamic_loss_args is None:\r\n                self.loss_scaler = DynamicLossScaler()\r\n            else:\r\n                self.loss_scaler = DynamicLossScaler(**dynamic_loss_args)\r\n\r\n            self.dynamic_loss_scale = True\r\n\r\n        else:\r\n            self.dynamic_loss_scale = False\r\n            self.loss_scaler = LossScaler(scale=static_loss_scale)\r\n            self.cur_iter = 0\r\n\r\n        see_memory_usage(""Before initializing optimizer states"")\r\n        self.initialize_optimizer_states()\r\n        see_memory_usage(""After initializing optimizer states"")\r\n\r\n        if dist.get_rank() == 0:\r\n            logger.info(f""optimizer state initialized"")\r\n\r\n        if dist.get_rank(group=self.dp_process_group) == 0:\r\n            see_memory_usage(f""After initializing ZeRO optimizer"")\r\n\r\n    def _release_ipg_buffers(self):\r\n        if self.contiguous_gradients:\r\n            self.ipg_buffer = None\r\n            self.grads_in_partition = None\r\n            self.grads_in_partition_offset = 0\r\n\r\n    def initialize_optimizer_states(self):\r\n\r\n        for i, group in enumerate(self.fp16_groups):\r\n            single_grad_partition = torch.zeros(\r\n                int(self.partition_size[i]),\r\n                dtype=self.single_partition_of_fp32_groups[i].dtype,\r\n                device=torch.cuda.current_device())\r\n            self.single_partition_of_fp32_groups[i].grad = single_grad_partition\r\n\r\n        self.optimizer.step()\r\n\r\n        for group in self.single_partition_of_fp32_groups:\r\n            group.grad = None\r\n\r\n        return\r\n\r\n    #########################################################################\r\n    #########################ZeRO Partition Gradients########################\r\n    #########################################################################\r\n\r\n    def get_first_param_index(self, group_id, param_group, partition_id):\r\n        for index, param in enumerate(param_group):\r\n            param_id = self.get_param_id(param)\r\n            if partition_id in self.param_to_partition_ids[group_id][param_id]:\r\n                return index\r\n        return None\r\n\r\n    def initialize_gradient_partitioning_data_structures(self):\r\n\r\n        total_partitions = dist.get_world_size(group=self.dp_process_group)\r\n\r\n        for i, param_group in enumerate(self.fp16_groups):\r\n\r\n            self.param_to_partition_ids[i] = {}\r\n            self.is_partition_reduced[i] = {}\r\n            self.total_grads_in_partition[i] = {}\r\n            self.remaining_grads_in_partition[i] = {}\r\n            self.is_grad_computed[i] = {}\r\n            self.grad_partition_insertion_offset[i] = {}\r\n            self.grad_start_offset[i] = {}\r\n            self.first_param_index_in_partition[i] = {}\r\n\r\n            for partition_id in range(total_partitions):\r\n                self.is_grad_computed[i][partition_id] = {}\r\n                self.grad_partition_insertion_offset[i][partition_id] = {}\r\n                self.grad_start_offset[i][partition_id] = {}\r\n                self.initialize_gradient_partition(i, param_group, partition_id)\r\n                self.is_partition_reduced[i][partition_id] = False\r\n                self.first_param_index_in_partition[i][\r\n                    partition_id] = self.get_first_param_index(\r\n                        i,\r\n                        param_group,\r\n                        partition_id)\r\n\r\n    def independent_gradient_partition_epilogue(self):\r\n        self.report_ipg_memory_usage(f""In ipg_epilogue before reduce_ipg_grads"", 0)\r\n        self.reduce_ipg_grads()\r\n        self.report_ipg_memory_usage(f""In ipg_epilogue after reduce_ipg_grads"", 0)\r\n\r\n        #if dist.get_rank() == 0:\r\n        #    logger.info(""Params already reduced %s"", self.params_already_reduced)\r\n        for i in range(len(self.params_already_reduced)):\r\n            self.params_already_reduced[i] = False\r\n\r\n        if self.overlap_comm:\r\n            torch.cuda.synchronize()\r\n\r\n        for i, _ in enumerate(self.fp16_groups):\r\n            self.averaged_gradients[i] = self.get_flat_partition(\r\n                self.params_in_partition[i],\r\n                self.first_offset[i],\r\n                self.partition_size[i],\r\n                return_tensor_list=True)\r\n\r\n        self._release_ipg_buffers()\r\n\r\n        see_memory_usage(f""End ipg_epilogue"")\r\n\r\n    # resets all partition to no reduced\r\n    # sets remianing grads to the total number of grads in each partition\r\n    # set is grad computed to false for all grads in partition\r\n    def reset_partition_gradient_structures(self):\r\n        total_partitions = dist.get_world_size(group=self.dp_process_group)\r\n        for i, _ in enumerate(self.fp16_groups):\r\n            for partition_id in range(total_partitions):\r\n                self.is_partition_reduced[i][partition_id] = False\r\n                self.remaining_grads_in_partition[i][\r\n                    partition_id] = self.total_grads_in_partition[i][partition_id]\r\n\r\n                for param_id in self.is_grad_computed[i][partition_id]:\r\n                    self.is_grad_computed[i][partition_id][param_id] = False\r\n\r\n    def initialize_gradient_partition(self, i, param_group, partition_id):\r\n        def set_key_value_list(dictionary, key, value):\r\n            if key in dictionary:\r\n                dictionary[key].append(value)\r\n            else:\r\n                dictionary[key] = [value]\r\n\r\n        def increment_value(dictionary, key):\r\n            if key in dictionary:\r\n                dictionary[key] += 1\r\n            else:\r\n                dictionary[key] = 1\r\n\r\n        partition_size = self.partition_size[i]\r\n\r\n        start_index = partition_size * partition_id\r\n        end_index = partition_size * (partition_id + 1)\r\n\r\n        current_index = 0\r\n        first_offset = 0\r\n\r\n        for param in param_group:\r\n\r\n            param_size = param.numel()\r\n            param_id = self.get_param_id(param)\r\n\r\n            if (current_index >= start_index and current_index < end_index):\r\n                set_key_value_list(self.param_to_partition_ids[i],\r\n                                   param_id,\r\n                                   partition_id)\r\n                increment_value(self.total_grads_in_partition[i], partition_id)\r\n\r\n                self.is_grad_computed[i][partition_id][param_id] = False\r\n\r\n                self.grad_partition_insertion_offset[i][partition_id][\r\n                    param_id] = current_index - start_index\r\n                self.grad_start_offset[i][partition_id][param_id] = 0\r\n\r\n            elif start_index > current_index and start_index < (current_index +\r\n                                                                param_size):\r\n                assert (first_offset==0), ""This can happen either zero or only once as this must be the first tensor in the partition""\r\n                first_offset = start_index - current_index\r\n\r\n                set_key_value_list(self.param_to_partition_ids[i],\r\n                                   param_id,\r\n                                   partition_id)\r\n                increment_value(self.total_grads_in_partition[i], partition_id)\r\n\r\n                self.is_grad_computed[i][partition_id][param_id] = False\r\n\r\n                self.grad_partition_insertion_offset[i][partition_id][param_id] = 0\r\n                self.grad_start_offset[i][partition_id][param_id] = first_offset\r\n\r\n            current_index = current_index + param_size\r\n\r\n    def overlapping_partition_gradients_reduce_epilogue(self):\r\n        self.independent_gradient_partition_epilogue()\r\n\r\n    def create_reduce_and_remove_grad_hooks(self):\r\n        self.grad_accs = []\r\n        for i, param_group in enumerate(self.fp16_groups):\r\n            for param in param_group:\r\n                if param.requires_grad:\r\n\r\n                    def wrapper(param, i):\r\n                        param_tmp = param.expand_as(param)\r\n                        grad_acc = param_tmp.grad_fn.next_functions[0][0]\r\n\r\n                        def reduce_partition_and_remove_grads(*notneeded):\r\n                            self.reduce_ready_partitions_and_remove_grads(param, i)\r\n\r\n                        grad_acc.register_hook(reduce_partition_and_remove_grads)\r\n                        self.grad_accs.append(grad_acc)\r\n\r\n                    wrapper(param, i)\r\n\r\n    def get_param_id(self, param):\r\n        unique_id = id(param)\r\n        return self.param_id[unique_id]\r\n\r\n    def report_ipg_memory_usage(self, tag, param_elems):\r\n        elem_count = self.elements_in_ipg_bucket + param_elems\r\n        percent_of_bucket_size = (100.0 * elem_count) // self.reduce_bucket_size\r\n        see_memory_usage(\r\n            f""{tag}: elems in_bucket {self.elements_in_ipg_bucket} param {param_elems} max_percent {percent_of_bucket_size}""\r\n        )\r\n\r\n    ###############Idependent Partition Gradient ########################\r\n    def reduce_independent_p_g_buckets_and_remove_grads(self, param, i):\r\n        if self.elements_in_ipg_bucket + param.numel() > self.reduce_bucket_size:\r\n            self.report_ipg_memory_usage(""In ipg_remove_grads before reduce_ipg_grads"",\r\n                                         param.numel())\r\n            self.reduce_ipg_grads()\r\n            if self.contiguous_gradients and self.overlap_comm:\r\n                # Swap ipg_index between 0 and 1\r\n                self.ipg_index = 1 - self.ipg_index\r\n            self.report_ipg_memory_usage(""In ipg_remove_grads after reduce_ipg_grads"",\r\n                                         param.numel())\r\n\r\n        param_id = self.get_param_id(param)\r\n\r\n        assert self.params_already_reduced[param_id] == False, \\\r\n            f""The parameter {param_id} has already been reduced. \\\r\n            Gradient computed twice for this partition. \\\r\n            Multiple gradient reduction is currently not supported""\r\n\r\n        #keeping the gradients contiguous to prevent memory fragmentation, and avoid flattening\r\n        if self.contiguous_gradients:\r\n            new_grad_tensor = self.ipg_buffer[self.ipg_index].narrow(\r\n                0,\r\n                self.elements_in_ipg_bucket,\r\n                param.numel())\r\n            new_grad_tensor.copy_(param.grad.view(-1))\r\n            param.grad.data = new_grad_tensor.data.view_as(param.grad)\r\n\r\n        self.elements_in_ipg_bucket += param.numel()\r\n        self.grads_in_ipg_bucket.append(param.grad)\r\n        self.params_in_ipg_bucket.append((i, param, param_id))\r\n\r\n        self.report_ipg_memory_usage(""End ipg_remove_grads"", 0)\r\n\r\n    def print_rank_0(self, message):\r\n        if dist.get_rank() == 0:\r\n            logger.info(message)\r\n\r\n    def gradient_reduction_w_predivide(self, tensor):\r\n        dp_world_size = dist.get_world_size(group=self.dp_process_group)\r\n\r\n        tensor_to_allreduce = tensor\r\n\r\n        if self.allreduce_always_fp32:\r\n            tensor_to_allreduce = tensor.float()\r\n\r\n        if self.postscale_gradients:\r\n            if self.gradient_predivide_factor != 1.0:\r\n                tensor_to_allreduce.mul_(1. / self.gradient_predivide_factor)\r\n\r\n            dist.all_reduce(tensor_to_allreduce, group=self.dp_process_group)\r\n\r\n            if self.gradient_predivide_factor() != dp_world_size:\r\n                tensor_to_allreduce.mul_(self.gradient_predivide_factor() /\r\n                                         dp_world_size)\r\n        else:\r\n            tensor_to_allreduce.div_(dp_world_size)\r\n            dist.all_reduce(tensor_to_allreduce, group=self.dp_process_group)\r\n\r\n        if self.allreduce_always_fp32 and tensor is not tensor_to_allreduce:\r\n            tensor.copy_(tensor_to_allreduce)\r\n\r\n        return tensor\r\n\r\n    def average_tensor(self, tensor):\r\n        if self.overlap_comm:\r\n            torch.cuda.synchronize()\r\n            stream = self.reduction_stream\r\n        else:\r\n            stream = torch.cuda.current_stream()\r\n\r\n        with torch.cuda.stream(stream):\r\n            if not self.reduce_scatter:\r\n                self.gradient_reduction_w_predivide(tensor)\r\n                return\r\n\r\n            # Accumulate destination ranks and bucket offsets for each gradient slice.\r\n            # Note: potential future optimization, record access pattern of parameters\r\n            # in backward pass and partition gradients w.r.t. access pattern so that our\r\n            # bucket is guaranteed to be contiguous w.r.t. ranks\r\n            rank_and_offsets = []\r\n            curr_size = 0\r\n            prev_id = -1\r\n            for i, param, param_id in self.params_in_ipg_bucket:\r\n                partition_ids = self.param_to_partition_ids[i][param_id]\r\n                partition_size = self.partition_size[i]\r\n                # Get all partition ids + their offsets\r\n                partition_ids_w_offsets = []\r\n                for partition_id in partition_ids:\r\n                    offset = self.grad_start_offset[i][partition_id][param_id]\r\n                    partition_ids_w_offsets.append((partition_id, offset))\r\n                partition_ids_w_offsets.sort(key=lambda t: t[1])\r\n\r\n                # Calculate rank and offsets for grad slices\r\n                for idx in range(len(partition_ids_w_offsets)):\r\n                    partition_id, offset = partition_ids_w_offsets[idx]\r\n\r\n                    # Calculate numel for grad slice depending on partition location\r\n                    if idx == len(partition_ids_w_offsets) - 1:\r\n                        # Last partition_id uses its own offset\r\n                        numel = param.numel() - offset\r\n                    else:\r\n                        # Set numel to next partition\'s offset\r\n                        numel = partition_ids_w_offsets[idx + 1][1] - offset\r\n\r\n                    # Merge bucket ranges if they belong to the same rank\r\n                    if partition_id == prev_id:\r\n                        prev_pid, prev_size, prev_numel = rank_and_offsets[-1]\r\n                        rank_and_offsets[-1] = (prev_pid, prev_size, prev_numel + numel)\r\n                    else:\r\n                        rank_and_offsets.append((partition_id, curr_size, numel))\r\n\r\n                    curr_size += numel\r\n                    prev_id = partition_id\r\n            tensor.div_(dist.get_world_size(group=self.dp_process_group))\r\n\r\n            async_handles = []\r\n            for dst, bucket_offset, numel in rank_and_offsets:\r\n                grad_slice = tensor.narrow(0, int(bucket_offset), int(numel))\r\n                dst_rank = _get_global_rank(self.dp_process_group, dst)\r\n                async_handle = dist.reduce(grad_slice,\r\n                                           dst=dst_rank,\r\n                                           group=self.dp_process_group,\r\n                                           async_op=True)\r\n                async_handles.append(async_handle)\r\n\r\n            for handle in async_handles:\r\n                handle.wait()\r\n\r\n    def copy_grads_in_partition(self, param):\r\n        if self.grads_in_partition is None:\r\n            self.grads_in_partition_offset = 0\r\n            total_size = 0\r\n            for group in self.params_in_partition:\r\n                for param_in_partition in group:\r\n                    total_size += param_in_partition.numel()\r\n\r\n            see_memory_usage(f""before copying {total_size} gradients into partition"")\r\n            self.grads_in_partition = torch.empty(int(total_size),\r\n                                                  dtype=torch.half,\r\n                                                  device=torch.cuda.current_device())\r\n            see_memory_usage(f""after copying {total_size} gradients into partition"")\r\n\r\n        #The allreduce buffer will be rewritted. Copy the gradients in partition to a new buffer\r\n        new_grad_tensor = self.grads_in_partition.narrow(0,\r\n                                                         self.grads_in_partition_offset,\r\n                                                         param.numel())\r\n        new_grad_tensor.copy_(param.grad.view(-1))\r\n        param.grad.data = new_grad_tensor.data.view_as(param.grad)\r\n        self.grads_in_partition_offset += param.numel()\r\n\r\n    def reduce_ipg_grads(self):\r\n        if self.overlap_comm:\r\n            stream = self.reduction_stream\r\n        else:\r\n            stream = torch.cuda.current_stream()\r\n\r\n        if self.contiguous_gradients:\r\n            self.average_tensor(self.ipg_buffer[self.ipg_index])\r\n        else:\r\n            self.buffered_reduce_fallback(\r\n                None,\r\n                self.grads_in_ipg_bucket,\r\n                elements_per_buffer=self.elements_in_ipg_bucket)\r\n\r\n        with torch.cuda.stream(stream):\r\n            for _, param, param_id in self.params_in_ipg_bucket:\r\n                self.params_already_reduced[param_id] = True\r\n\r\n                if not self.is_param_in_current_partition[param_id]:\r\n                    if self.overlap_comm and self.contiguous_gradients is False:\r\n                        # Clear the previous grads during the next reduction\r\n                        # to avoid clearing them before the reduction is complete.\r\n                        if self.previous_reduced_grads is None:\r\n                            self.previous_reduced_grads = []\r\n                        self.previous_reduced_grads.append(param)\r\n                    else:\r\n                        param.grad = None\r\n                elif self.contiguous_gradients:\r\n                    self.copy_grads_in_partition(param)\r\n\r\n        self.grads_in_ipg_bucket = []\r\n        self.params_in_ipg_bucket = []\r\n        self.elements_in_ipg_bucket = 0\r\n        #####################################################################\r\n\r\n    def reduce_ready_partitions_and_remove_grads(self, param, i):\r\n        self.reduce_independent_p_g_buckets_and_remove_grads(param, i)\r\n\r\n    def zero_reduced_gradients(self, partition_id, i):\r\n        def are_all_related_partitions_reduced(params_id):\r\n            for partition_id in self.param_to_partition_ids[i][params_id]:\r\n                if not self.is_partition_reduced[i][partition_id]:\r\n                    return False\r\n            return True\r\n\r\n        for params_id in self.is_grad_computed[i][partition_id]:\r\n            if are_all_related_partitions_reduced(params_id):\r\n                self.param_dict[params_id].grad = None\r\n\r\n    def flatten_and_print(self, message, tensors, start=0, n=5):\r\n        flatten_tensor = _flatten_dense_tensors(tensors)\r\n\r\n        def print_func():\r\n            logger.info(flatten_tensor.contiguous().view(-1).narrow(0, start, n))\r\n\r\n        self.sequential_execution(print_func, message)\r\n\r\n    def get_grads_to_reduce(self, i, partition_id):\r\n        def get_reducable_portion(key):\r\n            grad = self.param_dict[key].grad\r\n            total_elements = grad.numel()\r\n            start = self.grad_start_offset[i][partition_id][key]\r\n            num_elements = min(\r\n                total_elements - start,\r\n                self.partition_size[i] -\r\n                self.grad_partition_insertion_offset[i][partition_id][key])\r\n            if not pg_correctness_test:\r\n                if num_elements == total_elements:\r\n                    return grad\r\n                else:\r\n                    return grad.contiguous().view(-1).narrow(0,\r\n                                                             int(start),\r\n                                                             int(num_elements))\r\n            else:\r\n                if num_elements == total_elements:\r\n                    return grad.clone()\r\n                else:\r\n                    return grad.clone().contiguous().view(-1).narrow(\r\n                        0,\r\n                        int(start),\r\n                        int(num_elements))\r\n\r\n        grads_to_reduce = []\r\n        for key in self.is_grad_computed[i][partition_id]:\r\n            grad = get_reducable_portion(key)\r\n            grads_to_reduce.append(grad)\r\n        return grads_to_reduce\r\n\r\n    def sequential_execution(self, function, message, group=None):\r\n        if group is None:\r\n            group = self.dp_process_group\r\n        if dist.get_rank(group=group) == 0:\r\n            logger.info(message)\r\n        for id in range(dist.get_world_size(group=group)):\r\n            if id == dist.get_rank(group=group):\r\n                function()\r\n            dist.barrier(group=group)\r\n\r\n    def set_none_gradients_to_zero(self, i, partition_id):\r\n        for param_id in self.is_grad_computed[i][partition_id]:\r\n            param = self.param_dict[param_id]\r\n            if param.grad is None:\r\n                param.grad = torch.zero_like(param)\r\n\r\n    ######################Reduction Related Methods##############################\r\n\r\n    def allreduce_bucket(self, bucket, allreduce_always_fp32=False, rank=None, log=None):\r\n        rank = None\r\n        tensor = flatten(bucket)\r\n\r\n        tensor_to_allreduce = tensor\r\n\r\n        if pg_correctness_test:\r\n            allreduce_always_fp32 = True\r\n\r\n        if allreduce_always_fp32:\r\n            tensor_to_allreduce = tensor.float()\r\n\r\n        tensor_to_allreduce.div_(dist.get_world_size(group=self.dp_process_group))\r\n\r\n        if rank is None:\r\n            #    ""All Reducing""\r\n            dist.all_reduce(tensor_to_allreduce, group=self.dp_process_group)\r\n        else:\r\n            global_rank = _get_global_rank(self.dp_process_group, rank)\r\n            dist.reduce(tensor_to_allreduce, global_rank, group=self.dp_process_group)\r\n\r\n        if allreduce_always_fp32 and tensor is not tensor_to_allreduce:\r\n            if rank is None or rank == dist.get_rank(group=self.dp_process_group):\r\n                tensor.copy_(tensor_to_allreduce)\r\n\r\n        return tensor\r\n\r\n    #if rank is specified do a reduction instead of an allreduce\r\n    def allreduce_and_copy(self, small_bucket, rank=None, log=None):\r\n        if self.overlap_comm:\r\n            torch.cuda.synchronize()\r\n            if self.previous_reduced_grads is not None:\r\n                # previous_reduced_grads has the previous reduced grads,\r\n                # now it is safe to clear.\r\n                for param in self.previous_reduced_grads:\r\n                    param.grad = None\r\n                self.previous_reduced_grads = None\r\n            stream = self.reduction_stream\r\n        else:\r\n            stream = torch.cuda.current_stream()\r\n\r\n        with torch.cuda.stream(stream):\r\n            allreduced = self.allreduce_bucket(small_bucket, rank=rank, log=log)\r\n            if rank is None or rank == dist.get_rank(group=self.dp_process_group):\r\n                for buf, synced in zip(small_bucket, unflatten(allreduced, small_bucket)):\r\n                    buf.copy_(synced)\r\n\r\n    def allreduce_no_retain(self,\r\n                            bucket,\r\n                            numel_per_bucket=500000000,\r\n                            rank=None,\r\n                            log=None):\r\n        small_bucket = []\r\n        numel = 0\r\n        for tensor in bucket:\r\n            small_bucket.append(tensor)\r\n            numel = numel + tensor.numel()\r\n            if numel > numel_per_bucket:\r\n                self.allreduce_and_copy(small_bucket, rank=rank, log=None)\r\n                small_bucket = []\r\n        if len(small_bucket) > 0:\r\n            self.allreduce_and_copy(small_bucket, rank=rank, log=log)\r\n\r\n    #allows using reduction of gradients instead of using all_reduce\r\n    def buffered_reduce_fallback(self,\r\n                                 rank,\r\n                                 grads,\r\n                                 elements_per_buffer=500000000,\r\n                                 log=None):\r\n        split_buckets = split_half_float_double(grads)\r\n\r\n        for i, bucket in enumerate(split_buckets):\r\n            self.allreduce_no_retain(bucket,\r\n                                     numel_per_bucket=elements_per_buffer,\r\n                                     rank=rank,\r\n                                     log=log)\r\n\r\n    #############################################################################\r\n    #############################################################################\r\n    #############################################################################\r\n\r\n    #views the tensor as multiple partitions and returns\r\n    #those partitions\r\n    def get_data_parallel_partitions(self, tensor):\r\n        partitions = []\r\n\r\n        dp = dist.get_world_size(group=self.dp_process_group)\r\n        dp_id = dist.get_rank(group=self.dp_process_group)\r\n\r\n        total_num_elements = tensor.numel()\r\n\r\n        base_size = total_num_elements // dp\r\n        remaining = total_num_elements % dp\r\n\r\n        start = 0\r\n        for id in range(dp):\r\n            partition_size = base_size\r\n            if id < remaining:\r\n                partition_size = partition_size + 1\r\n            partitions.append(tensor.narrow(0, start, partition_size))\r\n            start = start + partition_size\r\n        return partitions\r\n\r\n    def get_partition_info(self, tensor_list, partition_size, partition_id):\r\n        params_in_partition = []\r\n        params_not_in_partition = []\r\n\r\n        start_index = partition_size * partition_id\r\n        end_index = partition_size * (partition_id + 1)\r\n\r\n        current_index = 0\r\n        first_offset = 0\r\n\r\n        for tensor in tensor_list:\r\n\r\n            tensor_size = tensor.numel()\r\n\r\n            if (current_index >= start_index and current_index < end_index):\r\n                params_in_partition.append(tensor)\r\n\r\n            elif start_index > current_index and start_index < (current_index +\r\n                                                                tensor_size):\r\n                params_in_partition.append(tensor)\r\n\r\n                assert (first_offset==0), ""This can happen either zero or only once as this must be the first tensor in the partition""\r\n                first_offset = start_index - current_index\r\n\r\n            else:\r\n                params_not_in_partition.append(tensor)\r\n\r\n            current_index = current_index + tensor_size\r\n\r\n        return params_in_partition, params_not_in_partition, first_offset\r\n\r\n    def zero_grad(self, set_grads_to_None=True):\r\n        """"""\r\n        Zero FP16 parameter grads.\r\n        """"""\r\n        # FP32 grad should never exist.\r\n        # For speed, set model fp16 grad to None by default\r\n        for group in self.fp16_groups:\r\n            for p in group:\r\n                if set_grads_to_None:\r\n                    p.grad = None\r\n                else:\r\n                    if p.grad is not None:\r\n                        p.grad.detach_()\r\n                        p.grad.zero_()\r\n\r\n    def _model_parallel_all_reduce(self, tensor, op):\r\n        """""" Perform all reduce within model parallel group, if any.\r\n        """"""\r\n        if self.model_parallel_group is None:\r\n            torch.distributed.all_reduce(tensor=tensor, op=op)\r\n        else:\r\n            torch.distributed.all_reduce(tensor=tensor,\r\n                                         op=op,\r\n                                         group=self.model_parallel_group)\r\n\r\n    def get_grad_norm_direct(self, gradients, params, norm_type=2):\r\n        """"""Clips gradient norm of an iterable of parameters.\r\n\r\n        This is adapted from torch.nn.utils.clip_grad.clip_grad_norm_ and\r\n        added functionality to handle model parallel parameters. Note that\r\n        the gradients are modified in place.\r\n\r\n        Arguments:\r\n            parameters (Iterable[Tensor] or Tensor): an iterable of Tensors or a\r\n                single Tensor that will have gradients normalized\r\n            max_norm (float or int): max norm of the gradients\r\n            norm_type (float or int): type of the used p-norm. Can be ``\'inf\'`` for\r\n                infinity norm.\r\n\r\n        Returns:\r\n            Total norm of the parameters (viewed as a single vector).\r\n        """"""\r\n        norm_type = float(norm_type)\r\n        if norm_type == inf:\r\n            total_norm = max(g.data.abs().max() for g in gradients)\r\n            total_norm_cuda = torch.cuda.FloatTensor([float(total_norm)])\r\n            torch.distributed.all_reduce(total_norm_cuda,\r\n                                         op=torch.distributed.ReduceOp.MAX,\r\n                                         group=self.dp_process_group)\r\n\r\n            # Take max across all GPUs.\r\n            self._model_parallel_all_reduce(tensor=total_norm_cuda,\r\n                                            op=torch.distributed.ReduceOp.MAX)\r\n            total_norm = total_norm_cuda[0].item()\r\n        else:\r\n            total_norm = 0.0\r\n            #if dist.get_rank() == 0:\r\n            #    logger.info(f""Total Norm begining {total_norm}"")\r\n            for g, p in zip(gradients, params):\r\n                if is_model_parallel_parameter(p) or (self.model_parallel_rank == 0):\r\n                    param_norm = g.data.double().norm(2)\r\n                    total_norm += param_norm.item()**2\r\n            # Sum across all model parallel GPUs.\r\n            total_norm_cuda = torch.cuda.FloatTensor([float(total_norm)])\r\n\r\n            torch.distributed.all_reduce(total_norm_cuda,\r\n                                         op=torch.distributed.ReduceOp.SUM,\r\n                                         group=self.dp_process_group)\r\n\r\n            self._model_parallel_all_reduce(tensor=total_norm_cuda,\r\n                                            op=torch.distributed.ReduceOp.SUM)\r\n\r\n            total_norm = total_norm_cuda[0].item()**(1. / norm_type)\r\n\r\n        if total_norm == float(\r\n                \'inf\') or total_norm == -float(\'inf\') or total_norm != total_norm:\r\n            total_norm = -1\r\n\r\n        return total_norm\r\n\r\n    #creates a flat fused tensor from the tensor list starting at the first_offset\r\n    #in the first tensor of the list. If there are not enough elements in the tensor\r\n    #list then the flat tensor will be padded with zeros\r\n    def get_flat_partition(self,\r\n                           tensor_list,\r\n                           first_offset,\r\n                           partition_size,\r\n                           return_tensor_list=False):\r\n        flat_tensor_list = []\r\n        current_size = 0\r\n        for i, tensor in enumerate(tensor_list):\r\n            if tensor.grad is None:\r\n                continue\r\n\r\n            tensor = tensor.grad\r\n            num_elements = tensor.numel()\r\n            tensor_offset = 0\r\n\r\n            #we need to offset to get to the right element\r\n            if i == 0 and first_offset > 0:\r\n                tensor_offset = first_offset\r\n                num_elements = num_elements - tensor_offset\r\n\r\n            #we dont need all elements of the tensor\r\n            if num_elements > (partition_size - current_size):\r\n                num_elements = partition_size - current_size\r\n\r\n            #we need a narrow view of the tensor based on the tensor offset and number of elements that\r\n            #we need from this tensor\r\n            if tensor_offset > 0 or num_elements < tensor.numel():\r\n                flat_tensor_list.append(tensor.contiguous().view(-1).narrow(\r\n                    0,\r\n                    int(tensor_offset),\r\n                    int(num_elements)))\r\n            else:\r\n                flat_tensor_list.append(tensor)\r\n\r\n            current_size = current_size + num_elements\r\n\r\n        #this means its the last partition and does not align with the dp boundary. We need to pad before flattening\r\n        if current_size < partition_size:\r\n            flat_tensor_list.append(\r\n                torch.zeros(int(partition_size - current_size),\r\n                            dtype=tensor_list[0].dtype,\r\n                            device=tensor_list[0].device))\r\n\r\n        if return_tensor_list:\r\n            return flat_tensor_list\r\n\r\n        return _flatten_dense_tensors(flat_tensor_list)\r\n\r\n    def free_grad_in_param_list(self, param_list):\r\n        for p in param_list:\r\n            p.grad = None\r\n\r\n    def step(self, closure=None):\r\n        """"""\r\n        Not supporting closure.\r\n        """"""\r\n        see_memory_usage(f""In step before checking overflow"")\r\n\r\n        # First compute norm for all group so we know if there is overflow\r\n        self.check_overflow()\r\n\r\n        timers = self.timers\r\n\r\n        prev_scale = self.loss_scale\r\n        self._update_scale(self.overflow)\r\n        if self.overflow:\r\n            see_memory_usage(\'After overflow before clearing gradients\')\r\n            self.zero_grad()\r\n            see_memory_usage(\'After overflow after clearing gradients\')\r\n\r\n            logger.info(\r\n                ""[deepscale] OVERFLOW! Rank {} Skipping step. Attempted loss scale: {}, ""\r\n                ""reducing to {}"".format(dist.get_rank(),\r\n                                        prev_scale,\r\n                                        self.loss_scale))\r\n            timers(\'optimizer_step\').start()\r\n            timers(\'optimizer_step\').stop()\r\n            timers(\'optimizer_allgather\').start()\r\n            timers(\'optimizer_allgather\').stop()\r\n            return\r\n\r\n        norm_groups = []\r\n        single_partition_grad_groups = []\r\n        skip = False\r\n        partition_id = dist.get_rank(group=self.dp_process_group)\r\n        for i, group in enumerate(self.fp16_groups):\r\n\r\n            norm_groups.append(\r\n                self.get_grad_norm_direct(self.averaged_gradients[i],\r\n                                          self.params_in_partition[i]))\r\n\r\n            #free gradients for all the prameters that are not updated by this process\r\n            self.free_grad_in_param_list(self.params_not_in_partition[i])\r\n\r\n            #create a flat gradients for parameters updated by this process\r\n            # If we are last partition, ensure we have same size grads and partition size, if not pad with zero tensors\r\n            if partition_id == dist.get_world_size(group=self.dp_process_group) - 1:\r\n                single_grad_partition = flatten_dense_tensors_aligned(\r\n                    self.averaged_gradients[i],\r\n                    int(self.partition_size[i]),\r\n                    self.dp_process_group).to(\r\n                        self.single_partition_of_fp32_groups[i].dtype)\r\n            else:\r\n                single_grad_partition = _flatten_dense_tensors(\r\n                    self.averaged_gradients[i]).to(\r\n                        self.single_partition_of_fp32_groups[i].dtype)\r\n            assert single_grad_partition.numel() == self.partition_size[i], \\\r\n                ""averaged gradients have different number of elements that partition size {} {} {} {}"".format(single_grad_partition.numel(), self.partition_size[i], i, partition_id)\r\n\r\n            self.single_partition_of_fp32_groups[i].grad = single_grad_partition\r\n            #release all the gradient since we have already created a necessary copy in dp_grad_partition\r\n            self.free_grad_in_param_list(self.params_in_partition[i])\r\n\r\n            self.averaged_gradients[i] = None\r\n\r\n            single_partition_grad_groups.append(single_grad_partition)\r\n\r\n        self.unscale_and_clip_grads(single_partition_grad_groups, norm_groups)\r\n\r\n        timers(\'optimizer_step\').start()\r\n        self.optimizer.step()\r\n        #get rid of the fp32 gradients. Not needed anymore\r\n        for group in self.single_partition_of_fp32_groups:\r\n            group.grad = None\r\n\r\n        for i in range(len(norm_groups)):\r\n            for fp16_partitions, fp32_partition in zip(self.parallel_partitioned_fp16_groups, self.single_partition_of_fp32_groups):\r\n                fp16_partitions[partition_id].data.copy_(fp32_partition.data)\r\n        timers(\'optimizer_step\').stop()\r\n\r\n        timers(\'optimizer_allgather\').start()\r\n        #gather the updated weights from everyone\r\n        for group_id, partitioned_params in enumerate(self.parallel_partitioned_fp16_groups):\r\n\r\n            #Sequential AllGather Best of both worlds\r\n            dp_world_size = dist.get_world_size(group=self.dp_process_group)\r\n            num_shards = max(\r\n                1,\r\n                partitioned_params[partition_id].numel() * dp_world_size //\r\n                self.allgather_bucket_size)\r\n\r\n            shard_size = partitioned_params[partition_id].numel() // num_shards\r\n            num_elements = shard_size\r\n\r\n            assert shard_size * num_shards <= partitioned_params[partition_id].numel()\r\n\r\n            for shard_id in range(num_shards):\r\n\r\n                if shard_id == (num_shards - 1):\r\n                    num_elements = partitioned_params[partition_id].numel(\r\n                    ) - shard_id * shard_size\r\n\r\n                shard_list = []\r\n                for dp_id in range(dp_world_size):\r\n                    curr_shard = partitioned_params[dp_id].narrow(\r\n                        0,\r\n                        shard_id * shard_size,\r\n                        num_elements).detach()\r\n                    shard_list.append(curr_shard)\r\n\r\n                dist.all_gather(shard_list,\r\n                                shard_list[partition_id],\r\n                                group=self.dp_process_group)\r\n        timers(\'optimizer_allgather\').stop()\r\n\r\n        # TODO: we probably don\'t need this? just to be safe\r\n        for i in range(len(norm_groups)):\r\n            updated_params = _unflatten_dense_tensors(self.fp16_groups_flat[i],\r\n                                                      self.fp16_groups[i])\r\n            for p, q in zip(self.fp16_groups[i], updated_params):\r\n                p.data = q.data\r\n\r\n        see_memory_usage(\'After zero_optimizer step\')\r\n        return\r\n\r\n    def unscale_and_clip_grads(self, grad_groups_flat, norm_groups):\r\n        total_norm = 0.0\r\n        for norm in norm_groups:\r\n            total_norm += norm**2.0\r\n        total_norm = math.sqrt(total_norm)\r\n\r\n        # compute combined scale factor for this group\r\n        combined_scale = self.loss_scale\r\n        if self.clip_grad > 0.:\r\n            # norm is in fact norm*scale\r\n            clip = ((total_norm / self.loss_scale) + 1e-6) / self.clip_grad\r\n            if clip > 1:\r\n                combined_scale = clip * self.loss_scale\r\n\r\n        for grad in grad_groups_flat:\r\n            if isinstance(grad, list):\r\n                sub_partitions = grad\r\n                for g in sub_partitions:\r\n                    g.data.mul_(1. / combined_scale)\r\n            else:\r\n                grad.data.mul_(1. / combined_scale)\r\n\r\n    def _check_overflow(self, partition_gradients=True):\r\n        self.overflow = self.has_overflow(partition_gradients)\r\n\r\n    # `params` is a list / generator of torch.Variable\r\n    def has_overflow_serial(self, params, is_grad_list=False):\r\n        for p in params:\r\n            if p.grad is not None and self._has_inf_or_nan(p.grad.data):\r\n                return True\r\n\r\n        return False\r\n\r\n    def has_overflow_partitioned_grads_serial(self):\r\n        for i in range(len(self.fp16_groups)):\r\n            for j, grad in enumerate(self.averaged_gradients[i]):\r\n                if grad is not None and self._has_inf_or_nan(grad.data, j):\r\n                    return True\r\n        return False\r\n\r\n    def has_overflow(self, partition_gradients=True):\r\n        if partition_gradients:\r\n            overflow = self.has_overflow_partitioned_grads_serial()\r\n            overflow_gpu = torch.cuda.ByteTensor([overflow])\r\n            torch.distributed.all_reduce(overflow_gpu,\r\n                                         op=torch.distributed.ReduceOp.MAX,\r\n                                         group=self.dp_process_group)\r\n\r\n        else:\r\n            params = []\r\n            for group in self.fp16_groups:\r\n                for param in group:\r\n                    params.append(param)\r\n\r\n            overflow = self.has_overflow_serial(params, is_grad_list=partition_gradients)\r\n            overflow_gpu = torch.cuda.ByteTensor([overflow])\r\n\r\n        # Since each model parallel GPU carries only part of the model,\r\n        # make sure overflow flag is synced across all the model parallel GPUs\r\n        self._model_parallel_all_reduce(tensor=overflow_gpu,\r\n                                        op=torch.distributed.ReduceOp.MAX)\r\n\r\n        overflow = overflow_gpu[0].item()\r\n        return bool(overflow)\r\n\r\n    # `x` is a torch.Tensor\r\n    @staticmethod\r\n    def _has_inf_or_nan(x, j=None):\r\n        try:\r\n            # if x is half, the .float() incurs an additional deep copy, but it\'s necessary if\r\n            # Pytorch\'s .sum() creates a one-element tensor of the same type as x\r\n            # (which is true for some recent version of pytorch).\r\n            cpu_sum = float(x.float().sum())\r\n            # More efficient version that can be used if .sum() returns a Python scalar\r\n            # cpu_sum = float(x.sum())\r\n        except RuntimeError as instance:\r\n            # We want to check if inst is actually an overflow exception.\r\n            # RuntimeError could come from a different error.\r\n            # If so, we still want the exception to propagate.\r\n            if ""value cannot be converted"" not in instance.args[0]:\r\n                raise\r\n            return True\r\n        else:\r\n            if cpu_sum == float(\'inf\') or cpu_sum == -float(\'inf\') or cpu_sum != cpu_sum:\r\n                if dist.get_rank() == 0 and j is not None:\r\n                    _handle_overflow(cpu_sum, x, j)\r\n                return True\r\n            return False\r\n\r\n    def backward(self, loss, retain_graph=False):\r\n        """"""\r\n        :attr:`backward` performs the following steps:\r\n\r\n        1. fp32_loss = loss.float()\r\n        2. scaled_loss = fp32_loss*loss_scale\r\n        3. scaled_loss.backward(), which accumulates scaled gradients into the ``.grad`` attributes of the model\'s fp16 leaves\r\n        """"""\r\n        if self.contiguous_gradients:\r\n            self.ipg_buffer = []\r\n            buf_0 = torch.empty(self.reduce_bucket_size,\r\n                                dtype=torch.half,\r\n                                device=torch.cuda.current_device())\r\n            self.ipg_buffer.append(buf_0)\r\n\r\n            # Use double buffers to avoid data access conflict when overlap_comm is enabled.\r\n            if self.overlap_comm:\r\n                buf_1 = torch.empty(self.reduce_bucket_size,\r\n                                    dtype=torch.half,\r\n                                    device=torch.cuda.current_device())\r\n                self.ipg_buffer.append(buf_1)\r\n            self.ipg_index = 0\r\n\r\n        self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)\r\n\r\n    def check_overflow(self, partition_gradients=True):\r\n        self._check_overflow(partition_gradients)\r\n\r\n    def _update_scale(self, has_overflow=False):\r\n        self.loss_scaler.update_scale(has_overflow)\r\n\r\n    # Promote state so it can be retrieved or set via ""fp16_optimizer_instance.state""\r\n    def _get_state(self):\r\n        return self.optimizer.state\r\n\r\n    def _set_state(self, value):\r\n        self.optimizer.state = value\r\n\r\n    state = property(_get_state, _set_state)\r\n\r\n    # Promote param_groups so it can be retrieved or set via ""fp16_optimizer_instance.param_groups""\r\n    # (for example, to adjust the learning rate)\r\n    def _get_param_groups(self):\r\n        return self.optimizer.param_groups\r\n\r\n    def _set_param_groups(self, value):\r\n        self.optimizer.param_groups = value\r\n\r\n    param_groups = property(_get_param_groups, _set_param_groups)\r\n\r\n    # Promote loss scale so it can be retrieved or set via ""fp16_optimizer_instance.loss_scale""\r\n    def _get_loss_scale(self):\r\n        return self.loss_scaler.loss_scale\r\n\r\n    def _set_loss_scale(self, value):\r\n        self.loss_scaler.cur_scale = value\r\n\r\n    loss_scale = property(_get_loss_scale, _set_loss_scale)\r\n    cur_scale = property(_get_loss_scale, _set_loss_scale)\r\n\r\n    def state_dict(self):\r\n        """"""\r\n        Returns a dict containing the current state of this :class:`FP16_Optimizer` instance.\r\n        This dict contains attributes of :class:`FP16_Optimizer`, as well as the state_dict\r\n        of the contained Pytorch optimizer.\r\n        Example::\r\n            checkpoint = {}\r\n            checkpoint[\'model\'] = model.state_dict()\r\n            checkpoint[\'optimizer\'] = optimizer.state_dict()\r\n            torch.save(checkpoint, ""saved.pth"")\r\n        """"""\r\n        state_dict = {}\r\n        state_dict[\'loss_scaler\'] = self.loss_scaler\r\n        state_dict[\'dynamic_loss_scale\'] = self.dynamic_loss_scale\r\n        state_dict[\'overflow\'] = self.overflow\r\n        state_dict[\'optimizer_state_dict\'] = self.optimizer.state_dict()\r\n        state_dict[\r\n            \'single_partition_of_fp32_groups\'] = self.single_partition_of_fp32_groups\r\n\r\n        state_dict[\'partition_count\'] = self.partition_count\r\n\r\n        return state_dict\r\n\r\n    def load_state_dict(self, state_dict, load_optimizer_states=True):\r\n        """"""\r\n        Loads a state_dict created by an earlier call to state_dict().\r\n        If ``fp16_optimizer_instance`` was constructed from some ``init_optimizer``,\r\n        whose parameters in turn came from ``model``, it is expected that the user\r\n        will call ``model.load_state_dict()`` before\r\n        ``fp16_optimizer_instance.load_state_dict()`` is called.\r\n        Example::\r\n            model = torch.nn.Linear(D_in, D_out).cuda().half()\r\n            optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\r\n            optimizer = FP16_Optimizer(optimizer, static_loss_scale = 128.0)\r\n            ...\r\n            checkpoint = torch.load(""saved.pth"")\r\n            model.load_state_dict(checkpoint[\'model\'])\r\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\r\n        """"""\r\n        # I think it should actually be ok to reload the optimizer before the model.\r\n        self.loss_scaler = state_dict[\'loss_scaler\']\r\n        self.dynamic_loss_scale = state_dict[\'dynamic_loss_scale\']\r\n        self.overflow = state_dict[\'overflow\']\r\n\r\n        if load_optimizer_states:\r\n            self.optimizer.load_state_dict(state_dict[\'optimizer_state_dict\'])\r\n\r\n        # At this point, the optimizer\'s references to the model\'s fp32 parameters are up to date.\r\n        # The optimizer\'s hyperparameters and internal buffers are also up to date.\r\n        # However, the fp32 master copies of the model\'s fp16 params stored by the optimizer are still\r\n        # out of date.  There are two options.\r\n        # 1:  Refresh the master params from the model\'s fp16 params.\r\n        # This requires less storage but incurs precision loss.\r\n        # 2:  Save and restore the fp32 master copies separately.\r\n        # We choose option 1 if changing DP degree and option 2 otherwise.\r\n        #\r\n        # Pytorch Optimizer.load_state_dict casts saved buffers (e.g. momentum) to the type and device\r\n        # of their associated parameters, because it\'s possible those buffers might not exist yet in\r\n        # the current optimizer instance.  In our case, as long as the current FP16_Optimizer has been\r\n        # constructed in the same way as the one whose state_dict we are loading, the same master params\r\n        # are guaranteed to exist, so we can just copy_() from the saved master params.\r\n\r\n        if \'partition_count\' in state_dict and state_dict[\r\n                \'partition_count\'] == self.partition_count:\r\n            # Use option 2\r\n            for current, saved in zip(self.single_partition_of_fp32_groups, state_dict[\'single_partition_of_fp32_groups\']):\r\n                current.data.copy_(saved.data)\r\n        else:\r\n            # Use option 1\r\n            partition_id = dist.get_rank(group=self.dp_process_group)\r\n            for fp16_partitions, fp32_partition in zip(self.parallel_partitioned_fp16_groups, self.single_partition_of_fp32_groups):\r\n                fp32_partition.data.copy_(fp16_partitions[partition_id].data)\r\n\r\n\r\ndef _handle_overflow(cpu_sum, x, i):\r\n    import math\r\n    rank = torch.distributed.get_rank()\r\n    if rank == 0:\r\n        t_i = -1\r\n        for v_i, v in enumerate(x.data.contiguous().view(-1)):\r\n            if not math.isfinite(float(v)):\r\n                t_i = v_i\r\n                break\r\n        logger.info(\r\n            f""rank {rank} detected overflow {cpu_sum} in tensor {i}:{t_i} shape {x.shape}""\r\n        )\r\n'"
deepspeed/pt/fp16_optimizer.py,12,"b'\'\'\'\nCopyright 2019 The Microsoft DeepSpeed Team\n\nCopyright NVIDIA/apex\nThis file is adapted from FP16_Optimizer in NVIDIA/apex\n\'\'\'\n\nimport torch\nimport math\nfrom torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors\n\nfrom deepspeed.pt.deepspeed_utils import get_grad_norm, CheckOverflow, get_weight_norm\nfrom deepspeed.pt.loss_scaler import INITIAL_LOSS_SCALE, SCALE_WINDOW, MIN_LOSS_SCALE\nfrom deepspeed.pt.log_utils import logger\n\n\nclass FP16_Optimizer(object):\n    """"""\n   FP16 Optimizer for training fp16 models. Handles loss scaling.\n\n   For usage example please see, TODO:  DeepSpeed V2 Tutorial\n    """"""\n    def __init__(self,\n                 init_optimizer,\n                 static_loss_scale=1.0,\n                 dynamic_loss_scale=False,\n                 initial_dynamic_scale=2**32,\n                 dynamic_loss_args=None,\n                 verbose=True,\n                 mpu=None,\n                 clip_grad=0.0,\n                 fused_adam_legacy=False,\n                 timers=None):\n\n        self.fused_adam_legacy = fused_adam_legacy\n        self.timers = timers\n\n        if not torch.cuda.is_available:\n            raise SystemError(""Cannot use fp16 without CUDA."")\n        self.optimizer = init_optimizer\n\n        # param flattened by groups\n        self.fp16_groups = []\n        self.fp16_groups_flat = []\n        self.fp32_groups_flat = []\n\n        # loop to deal with groups\n        for i, param_group in enumerate(self.optimizer.param_groups):\n            # push this group to list before modify\n            self.fp16_groups.append(param_group[\'params\'])\n            # init fp16 weight buffer, flattened\n            self.fp16_groups_flat.append(\n                _flatten_dense_tensors([p.clone().detach()\n                                        for p in self.fp16_groups[i]]))\n            # set model fp16 weight to slices of flattened buffer\n            updated_params = _unflatten_dense_tensors(self.fp16_groups_flat[i],\n                                                      self.fp16_groups[i])\n            for p, q in zip(self.fp16_groups[i], updated_params):\n                p.data = q.data\n            # init master weight, flattened\n            self.fp32_groups_flat.append(\n                self.fp16_groups_flat[i].clone().float().detach())\n            # modify optimizer of have flat master weight\n            self.fp32_groups_flat[\n                i].requires_grad = True  # keep this in case internal optimizer uses it\n            param_group[\'params\'] = [self.fp32_groups_flat[i]]\n\n        # we may have a way of fusing dynamic scale. Do not support for now\n        if dynamic_loss_scale:\n            self.dynamic_loss_scale = True\n            self.cur_iter = 0\n            self.last_overflow_iter = -1\n            self.scale_factor = 2\n\n            if dynamic_loss_args is None:\n                self.cur_scale = initial_dynamic_scale\n                self.scale_window = 1000\n                self.min_loss_scale = 1\n            else:\n                self.cur_scale = dynamic_loss_args[INITIAL_LOSS_SCALE]\n                self.scale_window = dynamic_loss_args[SCALE_WINDOW]\n                self.min_loss_scale = dynamic_loss_args[MIN_LOSS_SCALE]\n        else:\n            self.dynamic_loss_scale = False\n            self.cur_iter = 0\n            self.cur_scale = static_loss_scale\n        self.verbose = verbose\n\n        self.clip_grad = clip_grad\n        self.norm_type = 2\n\n        TORCH_MAJOR = int(torch.__version__.split(\'.\')[0])\n        TORCH_MINOR = int(torch.__version__.split(\'.\')[1])\n        if TORCH_MAJOR == 0 and TORCH_MINOR <= 4:\n            self.clip_grad_norm = torch.nn.utils.clip_grad_norm\n        else:\n            self.clip_grad_norm = torch.nn.utils.clip_grad_norm_\n\n        #model parallel object\n        self.mpu = None\n\n        self.overflow = False\n        self.overflow_checker = CheckOverflow(self.fp16_groups, mpu=self.mpu)\n\n    def zero_grad(self, set_grads_to_None=True):\n        """"""\n        Zero FP16 parameter grads.\n        """"""\n        # For speed, set model fp16 grad to None by default\n        for group in self.fp16_groups:\n            for p in group:\n                if set_grads_to_None:\n                    p.grad = None\n                else:\n                    if p.grad is not None:\n                        p.grad.detach_()\n                        p.grad.zero_()\n\n    def step_fused_adam(self, closure=None):\n        """"""\n        Not supporting closure.\n        """"""\n        # First compute norm for all group so we know if there is overflow\n        grads_groups_flat = []\n        norm_groups = []\n        for i, group in enumerate(self.fp16_groups):\n            grads_groups_flat.append(\n                _flatten_dense_tensors([\n                    torch.zeros(p.size(),\n                                dtype=p.dtype,\n                                device=p.device) if p.grad is None else p.grad\n                    for p in group\n                ]))\n            norm_groups.append(get_weight_norm(grads_groups_flat[i], mpu=self.mpu))\n\n        self.overflow = self.overflow_checker.check_using_norm(norm_groups)\n        prev_scale = self.cur_scale\n        self._update_scale(self.overflow)\n\n        if self.overflow:\n            if self.verbose:\n                logger.info(""[deepspeed] OVERFLOW! Skipping step. Attempted loss ""\n                            ""scale: {}, reducing to {}"".format(\n                                prev_scale,\n                                self.cur_scale))\n            return self.overflow\n        combined_scale = self.unscale_and_clip_grads(grads_groups_flat,\n                                                     norm_groups,\n                                                     apply_scale=False)\n        # norm is in fact norm*cur_scale\n        self.optimizer.step(grads=[[g] for g in grads_groups_flat],\n                            output_params=[[p] for p in self.fp16_groups_flat],\n                            scale=combined_scale,\n                            grad_norms=norm_groups)\n        # TODO: we probably don\'t need this? just to be safe\n        for i in range(len(norm_groups)):\n            updated_params = _unflatten_dense_tensors(self.fp16_groups_flat[i],\n                                                      self.fp16_groups[i])\n            for p, q in zip(self.fp16_groups[i], updated_params):\n                p.data = q.data\n        return self.overflow\n\n    def start_timers(self, name_list):\n        if self.timers is not None:\n            for name in name_list:\n                self.timers(name).start()\n\n    def stop_timers(self, name_list):\n        if self.timers is not None:\n            for name in name_list:\n                self.timers(name).stop()\n\n    def log_timers(self, name_list):\n        if self.timers is not None:\n            self.timers.log(name_list)\n\n    def step(self, closure=None):\n        """"""\n        Not supporting closure.\n        """"""\n\n        if self.fused_adam_legacy:\n            return self.step_fused_adam()\n\n        COMPUTE_NORM = ""compute_norm""\n        OVERFLOW_CHECK = \'overflow_check\'\n        OVERFLOW_TIMERS = [COMPUTE_NORM, OVERFLOW_CHECK]\n        UNSCALE_AND_CLIP = \'unscale_and_clip\'\n        BASIC_STEP = \'basic_step\'\n        UPDATE_FP16 = \'update_fp16\'\n        STEP_TIMERS = OVERFLOW_TIMERS + [UNSCALE_AND_CLIP, BASIC_STEP, UPDATE_FP16]\n\n        # First compute norm for all group so we know if there is overflow\n        grads_groups_flat = []\n\n        for i, group in enumerate(self.fp16_groups):\n            data_type = self.fp32_groups_flat[i].dtype\n\n            grads_groups_flat.append(\n                _flatten_dense_tensors([\n                    torch.zeros(p.size(),\n                                dtype=data_type,\n                                device=p.device)\n                    if p.grad is None else p.grad.to(data_type) for p in group\n                ]))\n\n            self.fp32_groups_flat[i].grad = grads_groups_flat[i]\n\n        self.start_timers([COMPUTE_NORM])\n        all_groups_norm = get_grad_norm(self.fp32_groups_flat, mpu=self.mpu)\n        self.stop_timers([COMPUTE_NORM])\n\n        self.start_timers([OVERFLOW_CHECK])\n        self.overflow = self.overflow_checker.check_using_norm([all_groups_norm])\n        self.stop_timers([OVERFLOW_CHECK])\n\n        prev_scale = self.cur_scale\n        self._update_scale(self.overflow)\n\n        if self.overflow:\n            if self.verbose:\n                print(""[deepspeed] OVERFLOW! Skipping step. Attempted loss ""\n                      ""scale: {}, reducing to {}"".format(prev_scale,\n                                                         self.cur_scale))\n            self.log_timers(OVERFLOW_TIMERS)\n            return self.overflow\n\n        self.start_timers([UNSCALE_AND_CLIP])\n        self.unscale_and_clip_grads(grads_groups_flat, [all_groups_norm])\n        self.stop_timers([UNSCALE_AND_CLIP])\n\n        self.start_timers([BASIC_STEP])\n        self.optimizer.step()\n        self.stop_timers([BASIC_STEP])\n\n        #get rid of the fp32 gradients. Not needed anymore\n        for group in self.fp32_groups_flat:\n            group.grad = None\n\n        self.start_timers([UPDATE_FP16])\n        for i in range(len(self.fp16_groups)):\n            updated_params = _unflatten_dense_tensors(self.fp32_groups_flat[i],\n                                                      self.fp16_groups[i])\n            for p, q in zip(self.fp16_groups[i], updated_params):\n                p.data.copy_(q.data)\n        self.stop_timers([UPDATE_FP16])\n\n        self.log_timers(STEP_TIMERS)\n\n        return self.overflow\n\n    def unscale_and_clip_grads(self, grad_groups_flat, norm_groups, apply_scale=True):\n        total_norm = 0.0\n        for norm in norm_groups:\n            total_norm += norm**2.0\n        total_norm = math.sqrt(total_norm)\n\n        # compute combined scale factor for this group\n        combined_scale = self.cur_scale\n        if self.clip_grad > 0.:\n            # norm is in fact norm*scale\n            clip = ((total_norm / self.cur_scale) + 1e-6) / self.clip_grad\n            if clip > 1:\n                combined_scale = clip * self.cur_scale\n\n        if apply_scale:\n            for grad in grad_groups_flat:\n                grad.data.mul_(1. / combined_scale)\n\n        return combined_scale\n\n    def backward(self, loss):\n        """"""\n        :attr:`backward` performs the following steps:\n\n        1. fp32_loss = loss.float()\n        2. scaled_loss = fp32_loss*loss_scale\n        3. scaled_loss.backward(), which accumulates scaled gradients into the ``.grad`` attributes of the model\'s fp16 leaves\n        """"""\n        scaled_loss = (loss.float()) * self.cur_scale\n        scaled_loss.backward()\n\n    def _update_scale(self, skip):\n        if self.dynamic_loss_scale:\n            prev_scale = self.cur_scale\n            if skip:\n                self.cur_scale = max(self.cur_scale / self.scale_factor,\n                                     self.min_loss_scale)\n                self.last_overflow_iter = self.cur_iter\n                if self.verbose:\n                    logger.info(f""\\nGrad overflow on iteration {self.cur_iter}"")\n                    logger.info(\n                        f""Reducing dynamic loss scale from {prev_scale} to {self.cur_scale}""\n                    )\n            else:\n                # Ensure self.scale_window updates since last overflow\n                stable_interval = (self.cur_iter - self.last_overflow_iter) - 1\n                if (stable_interval > 0) and (stable_interval % self.scale_window == 0):\n                    self.cur_scale *= self.scale_factor\n                    if self.verbose:\n                        logger.info(\n                            f""No Grad overflow for {self.scale_window} iterations"")\n                        logger.info(\n                            f""Increasing dynamic loss scale from {prev_scale} to {self.cur_scale}""\n                        )\n        else:\n            if skip:\n                logger.info(""Grad overflow on iteration: %s"", self.cur_iter)\n                logger.info(""Using static loss scale of: %s"", self.cur_scale)\n        self.cur_iter += 1\n        return\n\n    # Promote state so it can be retrieved or set via ""fp16_optimizer_instance.state""\n    def _get_state(self):\n        return self.optimizer.state\n\n    def _set_state(self, value):\n        self.optimizer.state = value\n\n    state = property(_get_state, _set_state)\n\n    # Promote param_groups so it can be retrieved or set via ""fp16_optimizer_instance.param_groups""\n    # (for example, to adjust the learning rate)\n    def _get_param_groups(self):\n        return self.optimizer.param_groups\n\n    def _set_param_groups(self, value):\n        self.optimizer.param_groups = value\n\n    param_groups = property(_get_param_groups, _set_param_groups)\n\n    def state_dict(self):\n        """"""\n        Returns a dict containing the current state of this :class:`FP16_Optimizer` instance.\n        This dict contains attributes of :class:`FP16_Optimizer`, as well as the state_dict\n        of the contained Pytorch optimizer.\n        Example::\n            checkpoint = {}\n            checkpoint[\'model\'] = model.state_dict()\n            checkpoint[\'optimizer\'] = optimizer.state_dict()\n            torch.save(checkpoint, ""saved.pth"")\n        """"""\n        state_dict = {}\n        state_dict[\'dynamic_loss_scale\'] = self.dynamic_loss_scale\n        state_dict[\'cur_scale\'] = self.cur_scale\n        state_dict[\'cur_iter\'] = self.cur_iter\n        if state_dict[\'dynamic_loss_scale\']:\n            state_dict[\'last_overflow_iter\'] = self.last_overflow_iter\n            state_dict[\'scale_factor\'] = self.scale_factor\n            state_dict[\'scale_window\'] = self.scale_window\n        state_dict[\'optimizer_state_dict\'] = self.optimizer.state_dict()\n        state_dict[\'fp32_groups_flat\'] = self.fp32_groups_flat\n        state_dict[\'clip_grad\'] = self.clip_grad\n        return state_dict\n\n    def refresh_fp32_params(self):\n        for current, saved in zip(self.fp32_groups_flat, self.fp16_groups_flat):\n            current.data.copy_(saved.data)\n\n    def load_state_dict(self, state_dict, load_optimizer_states=True):\n        """"""\n        Loads a state_dict created by an earlier call to state_dict().\n        If ``fp16_optimizer_instance`` was constructed from some ``init_optimizer``,\n        whose parameters in turn came from ``model``, it is expected that the user\n        will call ``model.load_state_dict()`` before\n        ``fp16_optimizer_instance.load_state_dict()`` is called.\n        Example::\n            model = torch.nn.Linear(D_in, D_out).cuda().half()\n            optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n            optimizer = FP16_Optimizer(optimizer, static_loss_scale = 128.0)\n            ...\n            checkpoint = torch.load(""saved.pth"")\n            model.load_state_dict(checkpoint[\'model\'])\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\n        """"""\n        # I think it should actually be ok to reload the optimizer before the model.\n        self.dynamic_loss_scale = state_dict[\'dynamic_loss_scale\']\n        self.cur_scale = state_dict[\'cur_scale\']\n        self.cur_iter = state_dict[\'cur_iter\']\n        if state_dict[\'dynamic_loss_scale\']:\n            self.last_overflow_iter = state_dict[\'last_overflow_iter\']\n            self.scale_factor = state_dict[\'scale_factor\']\n            self.scale_window = state_dict[\'scale_window\']\n        if load_optimizer_states:\n            self.optimizer.load_state_dict(state_dict[\'optimizer_state_dict\'])\n        self.clip_grad = state_dict[\'clip_grad\']\n        # At this point, the optimizer\'s references to the model\'s fp32 parameters are up to date.\n        # The optimizer\'s hyperparameters and internal buffers are also up to date.\n        # However, the fp32 master copies of the model\'s fp16 params stored by the optimizer are still\n        # out of date.  There are two options.\n        # 1:  Refresh the master params from the model\'s fp16 params.\n        # This requires less storage but incurs precision loss.\n        # 2:  Save and restore the fp32 master copies separately.\n        # We choose option 2.\n        #\n        # Pytorch Optimizer.load_state_dict casts saved buffers (e.g. momentum) to the type and device\n        # of their associated parameters, because it\'s possible those buffers might not exist yet in\n        # the current optimizer instance.  In our case, as long as the current FP16_Optimizer has been\n        # constructed in the same way as the one whose state_dict we are loading, the same master params\n        # are guaranteed to exist, so we can just copy_() from the saved master params.\n        for current, saved in zip(self.fp32_groups_flat, state_dict[\'fp32_groups_flat\']):\n            current.data.copy_(saved.data)\n\n    def __repr__(self):\n        return repr(self.optimizer)\n'"
deepspeed/pt/fp16_unfused_optimizer.py,13,"b'\'\'\'\nCopyright 2019 The Microsoft DeepSpeed Team\n\nCopyright NVIDIA/apex\nThis file is adapted from FP16_Optimizer in NVIDIA/apex\n\'\'\'\n\nimport torch\nfrom torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors\nimport math\n\nfrom deepspeed.pt.deepspeed_utils import get_grad_norm, CheckOverflow, get_weight_norm\nfrom deepspeed.pt.loss_scaler import INITIAL_LOSS_SCALE, SCALE_WINDOW, MIN_LOSS_SCALE\nfrom deepspeed.pt.log_utils import logger\n\n\nclass FP16_UnfusedOptimizer(object):\n    """"""\n    FP16 Optimizer without weight fusion to support LAMB optimizer\n\n    For usage example please see, TODO:  DeepSpeed V2 Tutorial\n    """"""\n    def __init__(self,\n                 init_optimizer,\n                 static_loss_scale=1.0,\n                 dynamic_loss_scale=False,\n                 dynamic_loss_args=None,\n                 verbose=True,\n                 mpu=None,\n                 clip_grad=0.0,\n                 fused_lamb_legacy=False):\n\n        self.fused_lamb_legacy = fused_lamb_legacy\n\n        if torch.distributed.get_rank() == 0:\n            logger.info(f\'Fused Lamb Legacy : {self.fused_lamb_legacy} \')\n\n        if not torch.cuda.is_available:\n            raise SystemError(""Cannot use fp16 without CUDA."")\n        self.optimizer = init_optimizer\n\n        # param groups\n        self.fp16_groups = []\n        self.fp32_groups = []\n\n        # loop to deal with groups\n        for i, param_group in enumerate(self.optimizer.param_groups):\n            #fp16 weights that represents the actual model weights\n            self.fp16_groups.append(param_group[\'params\'])\n\n            #creating a fp32 copy of the weights that will be updated first then\n            #copied to fp16 weights\n            fp32_group = [p.clone().float().detach() for p in param_group[\'params\']]\n\n            #incase the internal optimizer needs it\n            for p in fp32_group:\n                p.requires_grad = True\n\n            #setting the param groups in the optimizer to point to fp32\n            #note these are not the weights used by the model\n            #the model uses the fp16 version that we added to fp16_group\n            self.fp32_groups.append(fp32_group)\n            param_group[\'params\'] = self.fp32_groups[i]\n\n        # we may have a way of fusing dynamic scale. Do not support for now\n        if dynamic_loss_scale:\n            self.dynamic_loss_scale = True\n            self.cur_iter = 0\n            self.last_overflow_iter = -1\n            self.scale_factor = 2.0\n            if dynamic_loss_args is None:\n                self.cur_scale = 1.0 * 2**16\n                self.scale_window = 1000\n                self.min_loss_scale = 0.25\n            else:\n                self.cur_scale = dynamic_loss_args[INITIAL_LOSS_SCALE]\n                self.scale_window = dynamic_loss_args[SCALE_WINDOW]\n                self.min_loss_scale = dynamic_loss_args[MIN_LOSS_SCALE]\n        else:\n            self.dynamic_loss_scale = False\n            self.cur_iter = 0\n            self.cur_scale = static_loss_scale\n\n        self.verbose = verbose\n\n        self.clip_grad = clip_grad\n        self.norm_type = 2\n\n        TORCH_MAJOR = int(torch.__version__.split(\'.\')[0])\n        TORCH_MINOR = int(torch.__version__.split(\'.\')[1])\n        if TORCH_MAJOR == 0 and TORCH_MINOR <= 4:\n            self.clip_grad_norm = torch.nn.utils.clip_grad_norm\n        else:\n            self.clip_grad_norm = torch.nn.utils.clip_grad_norm_\n\n        self.mpu = None\n\n        self.overflow = False\n        self.overflow_checker = CheckOverflow(self.fp16_groups, mpu=self.mpu)\n\n    def zero_grad(self, set_grads_to_None=True):\n        """"""\n        Zero FP16 parameter grads.\n        """"""\n        # FP32 grad should never exist outside of the step function\n        # For speed, set model fp16 grad to None by default\n        for group in self.fp16_groups:\n            for p in group:\n                if set_grads_to_None:\n                    p.grad = None\n                else:\n                    if p.grad is not None:\n                        p.grad.detach_()\n                        p.grad.zero_()\n\n    def step_fused_lamb(self, closure=None):\n        """"""\n        Not supporting closure.\n        """"""\n        # First compute norm for all group so we know if there is overflow\n        grads_groups_flat = []\n        grads_groups = []\n        norm_groups = []\n        for i, group in enumerate(self.fp16_groups):\n            grads = [\n                torch.zeros(p.size(),\n                            dtype=p.dtype,\n                            device=p.device) if p.grad is None else p.grad for p in group\n            ]\n            grads_groups.append(grads)\n            grads_groups_flat.append(_flatten_dense_tensors(grads))\n            norm_groups.append(get_weight_norm(grads_groups_flat[i], mpu=self.mpu))\n\n        self.overflow = self.overflow_checker.check_using_norm(norm_groups)\n        prev_scale = self.cur_scale\n\n        self._update_scale(self.overflow)\n        if self.overflow:\n            if self.verbose:\n                logger.info(""[deepspeed] OVERFLOW! Skipping step. Attempted loss ""\n                            ""scale: {}, reducing to {}"".format(\n                                prev_scale,\n                                self.cur_scale))\n            return self.overflow\n\n        combined_scale = self.unscale_and_clip_grads(norm_groups, apply_scale=False)\n        self.optimizer.step(grads=grads_groups,\n                            output_params=self.fp16_groups,\n                            scale=combined_scale)\n\n        return self.overflow\n\n    def step(self, closure=None):\n        """"""\n        Not supporting closure.\n        """"""\n        if self.fused_lamb_legacy:\n            return self.step_fused_lamb()\n\n        self.overflow = self.overflow_checker.check()\n        prev_scale = self.cur_scale\n\n        self._update_scale(self.overflow)\n        if self.overflow:\n            if self.verbose:\n                logger.info(""[deepspeed] OVERFLOW! Skipping step. Attempted loss ""\n                            ""scale: {}, reducing to {}"".format(\n                                prev_scale,\n                                self.cur_scale))\n            return self.overflow\n\n        norm_groups = []\n        for i, group in enumerate(self.fp16_groups):\n            norm_groups.append(get_grad_norm(group, mpu=self.mpu))\n\n            # copying gradients to fp32 to work with fp32 parameters\n            for fp32_param, fp16_param in zip(self.fp32_groups[i], self.fp16_groups[i]):\n                if fp16_param.grad is None:\n                    fp32_param.grad = torch.zeros(fp16_param.size(),\n                                                  dtype=fp32_param.dtype,\n                                                  device=fp32_param.device)\n                else:\n                    fp32_param.grad = fp16_param.grad.to(fp32_param.dtype)\n\n        self.unscale_and_clip_grads(norm_groups)\n\n        self.optimizer.step()\n\n        for fp32_group, fp16_group in zip(self.fp32_groups, self.fp16_groups):\n            for fp32_param, fp16_param in zip(fp32_group, fp16_group):\n\n                #remove the fp32 grad\n                fp32_param.grad = None\n\n                #copy data from fp32 to fp16\n                fp16_param.data.copy_(fp32_param.data)\n\n        return self.overflow\n\n    def unscale_and_clip_grads(self, norm_groups, apply_scale=True):\n        total_norm = 0.0\n        for norm in norm_groups:\n            total_norm += norm**2.0\n        total_norm = math.sqrt(total_norm)\n\n        # compute combined scale factor for this group\n        combined_scale = self.cur_scale\n        if self.clip_grad > 0.:\n            # norm is in fact norm*scale\n            clip = ((total_norm / self.cur_scale) + 1e-6) / self.clip_grad\n            if clip > 1:\n                combined_scale = clip * self.cur_scale\n\n        if apply_scale:\n            for group in self.fp32_groups:\n                for param in group:\n                    if param.grad is not None:\n                        param.grad.data.mul_(1. / combined_scale)\n\n        return combined_scale\n\n    def backward(self, loss):\n        """"""\n        :attr:`backward` performs the following steps:\n\n        1. fp32_loss = loss.float()\n        2. scaled_loss = fp32_loss*loss_scale\n        3. scaled_loss.backward(), which accumulates scaled gradients into the ``.grad`` attributes of the model\'s fp16 leaves\n        """"""\n        scaled_loss = (loss.float()) * self.cur_scale\n        scaled_loss.backward()\n\n    def _update_scale(self, skip):\n        if self.dynamic_loss_scale:\n            prev_scale = self.cur_scale\n            if skip:\n                self.cur_scale = max(self.cur_scale / self.scale_factor,\n                                     self.min_loss_scale)\n                self.last_overflow_iter = self.cur_iter\n                if self.verbose:\n                    logger.info(""Grad overflow on iteration: %s"", self.cur_iter)\n                    logger.info(\n                        f""Reducing dynamic loss scale from {prev_scale} to {self.cur_scale}""\n                    )\n            else:\n                # Ensure self.scale_window updates since last overflow\n                stable_interval = (self.cur_iter - self.last_overflow_iter) - 1\n                if (stable_interval > 0) and (stable_interval % self.scale_window == 0):\n                    self.cur_scale *= self.scale_factor\n                    if self.verbose:\n                        logger.info(\n                            f""No Grad overflow for {self.scale_window} iterations"")\n                        logger.info(\n                            f""Increasing dynamic loss scale from {prev_scale} to {self.cur_scale}""\n                        )\n        else:\n            if skip:\n                logger.info(""Grad overflow on iteration %s"", self.cur_iter)\n                logger.info(""Using static loss scale of %s"", self.cur_scale)\n        self.cur_iter += 1\n        return\n\n    # Promote state so it can be retrieved or set via ""fp16_optimizer_instance.state""\n    def _get_state(self):\n        return self.optimizer.state\n\n    def _set_state(self, value):\n        self.optimizer.state = value\n\n    state = property(_get_state, _set_state)\n\n    # Promote param_groups so it can be retrieved or set via ""fp16_optimizer_instance.param_groups""\n    # (for example, to adjust the learning rate)\n    def _get_param_groups(self):\n        return self.optimizer.param_groups\n\n    def _set_param_groups(self, value):\n        self.optimizer.param_groups = value\n\n    param_groups = property(_get_param_groups, _set_param_groups)\n\n    def state_dict(self):\n        """"""\n        Returns a dict containing the current state of this :class:`FP16_Optimizer` instance.\n        This dict contains attributes of :class:`FP16_Optimizer`, as well as the state_dict\n        of the contained Pytorch optimizer.\n        Example::\n            checkpoint = {}\n            checkpoint[\'model\'] = model.state_dict()\n            checkpoint[\'optimizer\'] = optimizer.state_dict()\n            torch.save(checkpoint, ""saved.pth"")\n        """"""\n        state_dict = {}\n        state_dict[\'dynamic_loss_scale\'] = self.dynamic_loss_scale\n        state_dict[\'cur_scale\'] = self.cur_scale\n        state_dict[\'cur_iter\'] = self.cur_iter\n        if state_dict[\'dynamic_loss_scale\']:\n            state_dict[\'last_overflow_iter\'] = self.last_overflow_iter\n            state_dict[\'scale_factor\'] = self.scale_factor\n            state_dict[\'scale_window\'] = self.scale_window\n        state_dict[\'optimizer_state_dict\'] = self.optimizer.state_dict()\n        state_dict[\'fp32_groups\'] = self.fp32_groups\n        return state_dict\n\n    def load_state_dict(self, state_dict, load_optimizer_states=True):\n        """"""\n        Loads a state_dict created by an earlier call to state_dict().\n        If ``fp16_optimizer_instance`` was constructed from some ``init_optimizer``,\n        whose parameters in turn came from ``model``, it is expected that the user\n        will call ``model.load_state_dict()`` before\n        ``fp16_optimizer_instance.load_state_dict()`` is called.\n        Example::\n            model = torch.nn.Linear(D_in, D_out).cuda().half()\n            optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n            optimizer = FP16_Optimizer(optimizer, static_loss_scale = 128.0)\n            ...\n            checkpoint = torch.load(""saved.pth"")\n            model.load_state_dict(checkpoint[\'model\'])\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\n        """"""\n        # I think it should actually be ok to reload the optimizer before the model.\n        self.dynamic_loss_scale = state_dict[\'dynamic_loss_scale\']\n        self.cur_scale = state_dict[\'cur_scale\']\n        self.cur_iter = state_dict[\'cur_iter\']\n        if state_dict[\'dynamic_loss_scale\']:\n            self.last_overflow_iter = state_dict[\'last_overflow_iter\']\n            self.scale_factor = state_dict[\'scale_factor\']\n            self.scale_window = state_dict[\'scale_window\']\n\n        if load_optimizer_states:\n            self.optimizer.load_state_dict(state_dict[\'optimizer_state_dict\'])\n        # At this point, the optimizer\'s references to the model\'s fp32 parameters are up to date.\n        # The optimizer\'s hyperparameters and internal buffers are also up to date.\n        # However, the fp32 master copies of the model\'s fp16 params stored by the optimizer are still\n        # out of date.  There are two options.\n        # 1:  Refresh the master params from the model\'s fp16 params.\n        # This requires less storage but incurs precision loss.\n        # 2:  Save and restore the fp32 master copies separately.\n        # We choose option 2.\n        #\n        # Pytorch Optimizer.load_state_dict casts saved buffers (e.g. momentum) to the type and device\n        # of their associated parameters, because it\'s possible those buffers might not exist yet in\n        # the current optimizer instance.  In our case, as long as the current FP16_Optimizer has been\n        # constructed in the same way as the one whose state_dict we are loading, the same master params\n        # are guaranteed to exist, so we can just copy_() from the saved master params.\n        for current_group, saved_group in zip(self.fp32_groups, state_dict[\'fp32_groups\']):\n            for current, saved in zip(current_group, saved_group):\n                current.data.copy_(saved.data)\n\n    def __repr__(self):\n        return repr(self.optimizer)\n'"
deepspeed/pt/log_utils.py,1,"b'import logging\nimport sys\n\nimport torch.distributed as dist\n\n\nclass LoggerFactory:\n    @staticmethod\n    def create_logger(name=None, level=logging.INFO):\n        """"""create a logger\n\n        Args:\n            name (str): name of the logger\n            level: level of logger\n\n        Raises:\n            ValueError is name is None\n        """"""\n\n        if name is None:\n            raise ValueError(""name for logger cannot be None"")\n\n        formatter = logging.Formatter(\n            ""[%(asctime)s] [%(levelname)s] ""\n            ""[%(filename)s:%(lineno)d:%(funcName)s] %(message)s"")\n\n        logger_ = logging.getLogger(name)\n        logger_.setLevel(level)\n        logger_.propagate = False\n        ch = logging.StreamHandler(stream=sys.stdout)\n        ch.setLevel(level)\n        ch.setFormatter(formatter)\n        logger_.addHandler(ch)\n        return logger_\n\n\nlogger = LoggerFactory.create_logger(name=""DeepSpeed"", level=logging.INFO)\n\n\ndef log_dist(message, ranks=None, level=logging.INFO):\n    """"""Log message when one of following condition meets\n\n    + not dist.is_initialized()\n    + dist.get_rank() in ranks if ranks is not None or ranks = [-1]\n\n    Args:\n        message (str)\n        ranks (list)\n        level (int)\n\n    """"""\n    should_log = not dist.is_initialized()\n    ranks = ranks or []\n    my_rank = dist.get_rank() if dist.is_initialized() else -1\n    if ranks and not should_log:\n        should_log = ranks[0] == -1\n        should_log = should_log or (my_rank in set(ranks))\n    if should_log:\n        final_message = ""[Rank {}] {}"".format(my_rank, message)\n        logger.log(level, final_message)\n'"
deepspeed/pt/loss_scaler.py,10,"b'# Copyright 2019 The Microsoft DeepSpeed Team\n# Copyright (c) 2019, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#Taken and modified for DeepSpeed from:\n#    https://github.com/NVIDIA/Megatron-LM/blob/master/fp16/loss_scaler.py\n#Commit: 93ab4bea59dc5cbf97c079d313741866af4deac9\n\nimport torch\n\nINITIAL_LOSS_SCALE = \'init_scale\'\nSCALE_WINDOW = \'scale_window\'\nDELAYED_SHIFT = \'delayed_shift\'\nMIN_LOSS_SCALE = \'min_scale\'\n\n\n# item() is a recent addition, so this helps with backward compatibility.\ndef to_python_float(t):\n    if hasattr(t, \'item\'):\n        return t.item()\n    return t[0]\n\n\nclass LossScaler:\n    """"""\n    Class that manages a static loss scale.  This class is intended to interact with\n    :class:`FP16_Optimizer`, and should not be directly manipulated by the user.\n\n    Use of :class:`LossScaler` is enabled via the ``static_loss_scale`` argument to\n    :class:`FP16_Optimizer`\'s constructor.\n\n    Args:\n        scale (float, optional, default=1.0):  The loss scale.\n    """"""\n    def __init__(self, scale=1):\n        self.cur_scale = scale\n\n    # `params` is a list / generator of torch.Variable\n    def has_overflow(self, params):\n        return False\n\n    # `x` is a torch.Tensor\n    def _has_inf_or_nan(x):\n        return False\n\n    def update_scale(self, overflow):\n        pass\n\n    @property\n    def loss_scale(self):\n        return self.cur_scale\n\n    def scale_gradient(self, module, grad_in, grad_out):\n        return tuple(self.loss_scale * g for g in grad_in)\n\n    def backward(self, loss, retain_graph=False):\n        scaled_loss = loss * self.loss_scale\n        scaled_loss.backward(retain_graph=retain_graph)\n\n\nclass DynamicLossScaler:\n    """"""\n    Class that manages dynamic loss scaling.  It is recommended to use :class:`DynamicLossScaler`\n    indirectly, by supplying ``dynamic_loss_scale=True`` to the constructor of\n    :class:`FP16_Optimizer`.  However, it\'s important to understand how :class:`DynamicLossScaler`\n    operates, because the default options can be changed using the\n    the ``dynamic_loss_args`` argument to :class:`FP16_Optimizer`\'s constructor.\n\n    Loss scaling is designed to combat the problem of underflowing gradients encountered at long\n    times when training fp16 networks.  Dynamic loss scaling begins by attempting a very high loss\n    scale.  Ironically, this may result in OVERflowing gradients.  If overflowing gradients are\n    encountered, :class:`DynamicLossScaler` informs :class:`FP16_Optimizer` that an overflow has\n    occurred.\n    :class:`FP16_Optimizer` then skips the update step for this particular iteration/minibatch,\n    and :class:`DynamicLossScaler` adjusts the loss scale to a lower value.\n    If a certain number of iterations occur without overflowing gradients detected,\n    :class:`DynamicLossScaler` increases the loss scale once more.\n    In this way :class:`DynamicLossScaler` attempts to ""ride the edge"" of\n    always using the highest loss scale possible without incurring overflow.\n\n    Args:\n        init_scale (float, optional, default=2**32):  Initial loss scale attempted by :class:`DynamicLossScaler.`\n        scale_factor (float, optional, default=2.0):  Factor used when adjusting the loss scale. If an overflow is encountered, the loss scale is readjusted to loss scale/``scale_factor``.  If ``scale_window`` consecutive iterations take place without an overflow, the loss scale is readjusted to loss_scale*``scale_factor``.\n        scale_window (int, optional, default=1000):  Number of consecutive iterations without an overflow to wait before increasing the loss scale.\n    """"""\n    def __init__(self,\n                 init_scale=2**32,\n                 scale_factor=2.,\n                 scale_window=1000,\n                 min_scale=1,\n                 delayed_shift=1,\n                 consecutive_hysteresis=False):\n        self.cur_scale = init_scale\n        self.cur_iter = 0\n        self.last_overflow_iter = -1\n        self.scale_factor = scale_factor\n        self.scale_window = scale_window\n        self.min_scale = min_scale\n        self.delayed_shift = delayed_shift\n        self.cur_hysteresis = delayed_shift\n        self.consecutive_hysteresis = consecutive_hysteresis\n\n    # `params` is a list / generator of torch.Variable\n    def has_overflow_serial(self, params):\n        for p in params:\n            if p.grad is not None and DynamicLossScaler._has_inf_or_nan(p.grad.data):\n                return True\n\n        return False\n\n    # `x` is a torch.Tensor\n    def _has_inf_or_nan(x):\n        try:\n            # if x is half, the .float() incurs an additional deep copy, but it\'s necessary if\n            # Pytorch\'s .sum() creates a one-element tensor of the same type as x\n            # (which is true for some recent version of pytorch).\n            cpu_sum = float(x.float().sum())\n            # More efficient version that can be used if .sum() returns a Python scalar\n            # cpu_sum = float(x.sum())\n        except RuntimeError as instance:\n            # We want to check if inst is actually an overflow exception.\n            # RuntimeError could come from a different error.\n            # If so, we still want the exception to propagate.\n            if ""value cannot be converted"" not in instance.args[0]:\n                raise\n            return True\n        else:\n            if cpu_sum == float(\'inf\') or cpu_sum == -float(\'inf\') or cpu_sum != cpu_sum:\n                return True\n            return False\n\n    # `overflow` is boolean indicating whether the gradient overflowed\n    def update_scale(self, overflow):\n        if overflow:\n            # self.cur_scale /= self.scale_factor\n            if self.delayed_shift == 1 or self.cur_hysteresis == 1:\n                self.cur_scale = max(self.cur_scale / self.scale_factor, self.min_scale)\n            else:\n                self.cur_hysteresis -= 1\n            self.last_overflow_iter = self.cur_iter\n        else:\n            if self.consecutive_hysteresis:\n                self.cur_hysteresis = self.delayed_shift\n            if (self.cur_iter - self.last_overflow_iter) % self.scale_window == 0:\n                if not self.consecutive_hysteresis:\n                    self.cur_hysteresis = self.delayed_shift\n                self.cur_scale *= self.scale_factor\n        self.cur_iter += 1\n\n    @property\n    def loss_scale(self):\n        return self.cur_scale\n\n    def scale_gradient(self, module, grad_in, grad_out):\n        return tuple(self.loss_scale * g for g in grad_in)\n\n    def backward(self, loss, retain_graph=False):\n        scaled_loss = loss * self.loss_scale\n        scaled_loss.backward(retain_graph=retain_graph)\n\n\n##############################################################\n# Example usage below here -- assuming it\'s in a separate file\n##############################################################\n""""""\nTO-DO separate out into an example.\nif __name__ == ""__main__"":\n    import torch\n    from torch.autograd import Variable\n    from dynamic_loss_scaler import DynamicLossScaler\n\n    # N is batch size; D_in is input dimension;\n    # H is hidden dimension; D_out is output dimension.\n    N, D_in, H, D_out = 64, 1000, 100, 10\n\n    # Create random Tensors to hold inputs and outputs, and wrap them in Variables.\n    x = Variable(torch.randn(N, D_in), requires_grad=False)\n    y = Variable(torch.randn(N, D_out), requires_grad=False)\n\n    w1 = Variable(torch.randn(D_in, H), requires_grad=True)\n    w2 = Variable(torch.randn(H, D_out), requires_grad=True)\n    parameters = [w1, w2]\n\n    learning_rate = 1e-6\n    optimizer = torch.optim.SGD(parameters, lr=learning_rate)\n    loss_scaler = DynamicLossScaler()\n\n    for t in range(500):\n        y_pred = x.mm(w1).clamp(min=0).mm(w2)\n        loss = (y_pred - y).pow(2).sum() * loss_scaler.loss_scale\n        print(\'Iter {} loss scale: {}\'.format(t, loss_scaler.loss_scale))\n        print(\'Iter {} scaled loss: {}\'.format(t, loss.data[0]))\n        print(\'Iter {} unscaled loss: {}\'.format(t, loss.data[0] / loss_scaler.loss_scale))\n\n        # Run backprop\n        optimizer.zero_grad()\n        loss.backward()\n\n        # Check for overflow\n        has_overflow = DynamicLossScaler.has_overflow(parameters)\n\n        # If no overflow, unscale grad and update as usual\n        if not has_overflow:\n            for param in parameters:\n                param.grad.data.mul_(1. / loss_scaler.loss_scale)\n            optimizer.step()\n        # Otherwise, don\'t do anything -- ie, skip iteration\n        else:\n            print(\'OVERFLOW!\')\n\n        # Update loss scale for next iteration\n        loss_scaler.update_scale(has_overflow)\n\n""""""\n'"
deepspeed/pt/zero_optimizer_stage1.py,16,"b'import math\nimport torch\nimport torch.distributed as dist\nfrom torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors\nfrom collections import defaultdict\n\nfrom deepspeed.pt.zero_utils import _initialize_parameter_parallel_groups\nfrom deepspeed.pt.log_utils import log_dist, logger\nfrom deepspeed.pt.loss_scaler import LossScaler, DynamicLossScaler\nfrom deepspeed.pt.deepspeed_utils import get_grad_norm, CheckOverflow\n\n\ndef flatten_dense_tensors_sub_partition_aligned(tensor_list,\n                                                dp,\n                                                max_elements_per_comm,\n                                                pg):\n    num_elements = 0\n    for tensor in tensor_list:\n        num_elements = num_elements + tensor.numel()\n\n    log_dist(""Total number of elements in model: {}, max elements per com: {}"".format(\n        num_elements,\n        max_elements_per_comm),\n             ranks=[0])\n\n    max_elements_per_comm = min(max_elements_per_comm, num_elements)\n    sub_partition_size = int(max_elements_per_comm // dp)\n\n    alignment = sub_partition_size\n\n    # if alignment == 0:\n    #     # number of elements not divisible by dp, outside range and small model must pad with zeroes\n    #     pad_tensor = torch.zeros(max_elements_per_comm,\n    #                              device=tensor_list[0].device,\n    #                              dtype=tensor_list[0].dtype)\n    #     return _flatten_dense_tensors(pad_tensor)\n\n    remaining = int(num_elements % alignment)\n\n    # ensure we have equal sized sub-partitions\n    elements_to_add = 0\n    if remaining:\n        elements_to_add = alignment - remaining\n        # adding padded tensor later after we check comm alignment\n        log_dist(""adding pad tensor for alignment, {} + {}->{}"".format(\n            num_elements,\n            elements_to_add,\n            num_elements + elements_to_add),\n                 ranks=[0])\n        #num_elements = num_elements + elements_to_add\n    else:\n        padded_tensor_list = tensor_list\n\n    num_partitions = int((num_elements + elements_to_add) // sub_partition_size)\n    assert (num_elements + elements_to_add) % sub_partition_size == 0, ""num elements should be "" \\\n                                                                       ""aligned by sub partition "" \\\n                                                                       ""size""\n    num_comm_intervals = int(num_partitions // dp)\n    partition_remaining = int(num_partitions % dp)\n    log_dist(""num_comm_intervals={}, partition_remaining={}"".format(\n        num_comm_intervals,\n        partition_remaining),\n             ranks=[0])\n    if partition_remaining != 0:\n        log_dist(""adding pad tensor and/or extra sub partition"", ranks=[0])\n        # add pad tensor for alignment of comm interval, this overrules previous possibly sub-partition alignment\n        num_comm_intervals += 1\n        aligned_comm_elements = num_comm_intervals * sub_partition_size * dp\n        elements_to_add = aligned_comm_elements - num_elements\n\n        pad_tensor = torch.zeros(elements_to_add,\n                                 device=tensor_list[0].device,\n                                 dtype=tensor_list[0].dtype)\n        padded_tensor_list = tensor_list + [pad_tensor]\n        log_dist(""adding pad tensor and/or extra sub partition, {} + {}->{}"".format(\n            num_elements,\n            elements_to_add,\n            num_elements + elements_to_add),\n                 ranks=[0])\n        num_elements += elements_to_add\n    elif elements_to_add > 0:\n        # add pad tensor for just alignment of sub-partition\n        pad_tensor = torch.zeros(elements_to_add,\n                                 device=tensor_list[0].device,\n                                 dtype=tensor_list[0].dtype)\n        padded_tensor_list = tensor_list + [pad_tensor]\n        num_elements += elements_to_add\n\n    if pg is None or dist.get_rank(group=pg) == 0:\n        logger.info(""Number of Elements (w. padding) is %s"", num_elements)\n\n    padded_num_elems = 0\n    for p in padded_tensor_list:\n        padded_num_elems += p.numel()\n    assert num_elements == padded_num_elems, ""{} != {}, rank={}"".format(num_elements, padded_num_elems, dist.get_rank())\n\n    return _flatten_dense_tensors(padded_tensor_list)\n\n\ndef _single_range_check(current_index, start_index, end_index, tensor_size):\n    offset = 0\n    if (current_index >= start_index) and (current_index < end_index):\n        # Fully inside bounds\n        return True, offset\n    elif (start_index > current_index) and (start_index < (current_index + tensor_size)):\n        # Partially contained, compute offset\n        offset = start_index - current_index\n        return True, offset\n    else:\n        return False, offset\n\n\ndef _range_check(current_index, element_intervals, tensor_size):\n    results = []\n    for comm_idx, interval in enumerate(element_intervals):\n        start_index, end_index = interval\n        contained, offset = _single_range_check(current_index, start_index, end_index, tensor_size)\n        if contained:\n            results.append((contained, offset, comm_idx))\n    if len(results) == 0:\n        return [(False, 0, -1)]\n    return results\n\n\nclass FP16_DeepSpeedZeroOptimizer_Stage1(object):\n    """"""\n    FP16_DeepSpeedZeroOptimizer_Stage1 designed to reduce the memory footprint\n    required for training large deep learning models.\n\n    For more details please see ZeRO: Memory Optimization Towards Training A Trillion Parameter Models\n    https://arxiv.org/abs/1910.02054\n\n    This version aligns with stage-1 in the paper above.\n    """"""\n    def __init__(self,\n                 init_optimizer,\n                 static_loss_scale=1.0,\n                 dynamic_loss_scale=False,\n                 dynamic_loss_args=None,\n                 verbose=True,\n                 dp_process_group=None,\n                 partition_size=None,\n                 mpu=None,\n                 all_gather_partitions=True,\n                 allgather_size=500000000,\n                 clip_grad=0.0,\n                 max_elements_per_comm=5e8):\n\n        if dp_process_group is not None and partition_size is not None:\n            raise ValueError(""Cannot specify both dp_process_group ""\n                             ""and partition size"")\n\n        if dp_process_group is None:\n            dp_process_group = _initialize_parameter_parallel_groups(partition_size)\n\n        if not torch.cuda.is_available:\n            raise SystemError(""Cannot use fp16 without CUDA."")\n        self.optimizer = init_optimizer\n\n        self.verbose = verbose\n        self.dp_process_group = dp_process_group\n\n        # TODO: automatically turn off if #params > some_limit\n        self.all_gather_partitions = all_gather_partitions\n        self.allgather_size = allgather_size\n\n        self.max_elements_per_comm = max_elements_per_comm\n        logger.info(""max_elements_per_comm={}"".format(max_elements_per_comm))\n\n        # param flattened by groups\n        self.fp16_groups = []\n        self.fp16_groups_flat = []\n\n        # Setup bookkeeping data structures depending on partitioning type\n\n        # parallel_sub_partitioned_fp16_groups[group-idx] -> [comm-ids] -> [rank-ids]\n        self.parallel_sub_partitioned_fp16_groups = []\n        # same underlying data as above but viewed as: [groups] -> [rank-ids] -> [comm-ids]\n        self.parallel_comm_sub_partitioned_fp16_groups = []\n\n        # 32-bit sub-partitions of the parallel partitioned parameters\n        # that this process will update\n        self.local_sub_partitions_of_fp32_groups = []\n\n        # param partition info\n\n        # parameters in each group that will not be updated by this process directly\n        self.params_not_local = []\n\n        # parameters that will be updated by this process directly\n        self.params_in_rank_sub_partitions = []\n\n        # parameter offsets for parameters in sub-partitions. Parameter\n        # boundaries may not align with sub-partition boundaries\n        # so we need to keep track of the offsets\n        self.params_in_rank_sub_partitions_offsets = []\n\n        # number of elements per sub-partition in each group\n        self.sub_partition_sizes = []\n\n        # number of communication intervals for each group\n        self.num_comm_intervals_per_group = []\n\n        local_rank = dist.get_rank(group=self.dp_process_group)\n\n        # loop to deal with groups\n        for i, param_group in enumerate(self.optimizer.param_groups):\n            # push this group to list before modify\n            self.fp16_groups.append(param_group[\'params\'])\n\n            # flattens all tensors into single 1d tensor aligned with sub-partition size for later dividing\n            # RS: create aligned sub-partitions\n            self.fp16_groups_flat.append(\n                flatten_dense_tensors_sub_partition_aligned(\n                    tensor_list=self.fp16_groups[i],\n                    dp=dist.get_world_size(group=self.dp_process_group),\n                    max_elements_per_comm=self.max_elements_per_comm,\n                    pg=self.dp_process_group))\n\n            # TODO: I don\'t think this does anything?\n            # set model fp16 weight to slices of flattened buffer\n            updated_params = _unflatten_dense_tensors(self.fp16_groups_flat[i],\n                                                      self.fp16_groups[i])\n            for p, q in zip(self.fp16_groups[i], updated_params):\n                p.data = q.data\n\n            # divide the flat weights into near equal partition equal to the data parallel degree\n            # each process will compute on a different part of the partition\n            # RS: split into two layer list -> [comm-id] -> [sub-partitions per rank]\n            comm_partitions, dp_sub_partitions, element_intervals, sub_partition_size, num_comm_intervals = \\\n                self.get_data_parallel_sub_partitions(\n                    tensor=self.fp16_groups_flat[i],\n                    max_elements_per_comm=self.max_elements_per_comm,\n                    world_size=dist.get_world_size(\n                        group=self.dp_process_group),\n                    dp_process_group=self.dp_process_group\n                )\n            self.parallel_comm_sub_partitioned_fp16_groups.append(\n                comm_partitions)  # comm -> rank\n            self.parallel_sub_partitioned_fp16_groups.append(\n                dp_sub_partitions)  # rank -> comm\n            self.sub_partition_sizes.append(sub_partition_size)\n            self.num_comm_intervals_per_group.append(num_comm_intervals)\n            # data_parallel_partitions = self.get_data_parallel_partitions(self.fp16_groups_flat[i])\n            # self.parallel_partitioned_fp16_groups.append(data_parallel_partitions)\n\n            # a partition of the fp32 master weights that will be updated by this process\n            # RS: store/detach/cast our local sub-partitions\n            local_sub_partitions = []\n            for sub_partition in self.parallel_sub_partitioned_fp16_groups[i][\n                    local_rank]:\n                fp32_sub_partition = sub_partition.clone().float().detach()\n                fp32_sub_partition.requires_grad = True\n                local_sub_partitions.append(fp32_sub_partition)\n            self.local_sub_partitions_of_fp32_groups.append(local_sub_partitions)\n\n            # modify optimizer of have flat master weight\n            # self.single_partition_of_fp32_groups[i].requires_grad = True # keep this in case internal optimizer uses it\n            param_group[\'params\'] = self.local_sub_partitions_of_fp32_groups[i]\n\n            # RS: divide up the sub-partitions and keep track of offsets for each param\n            # partition_size = len(self.fp16_groups_flat[i]) / dist.get_world_size(group=self.dp_process_group)\n            params_in_rank_sub_partition, params_in_rank_sub_partitions_offsets, \\\n            params_not_local = self.get_all_sub_partition_info(\n                tensor_list=self.fp16_groups[i],\n                all_element_intervals=element_intervals,\n                local_rank=local_rank,\n                world_size=dist.get_world_size(group=self.dp_process_group)\n            )\n\n            self.params_in_rank_sub_partitions.append(params_in_rank_sub_partition)\n            self.params_not_local.append(params_not_local)\n            self.params_in_rank_sub_partitions_offsets.append(\n                params_in_rank_sub_partitions_offsets)\n\n        # we may have a way of fusing dynamic scale. Do not support for now\n        if dynamic_loss_scale:\n            if dynamic_loss_args is None:\n                self.loss_scaler = DynamicLossScaler()\n            else:\n                self.loss_scaler = DynamicLossScaler(**dynamic_loss_args)\n\n            self.dynamic_loss_scale = True\n\n        else:\n            self.dynamic_loss_scale = False\n            self.loss_scaler = LossScaler(scale=static_loss_scale)\n            self.cur_iter = 0\n\n        self.mpu = mpu\n        self.clip_grad = clip_grad\n\n        self.overflow = False\n        self.overflow_checker = CheckOverflow(self.fp16_groups,\n                                              mpu=self.mpu,\n                                              zero_reduce_scatter=True)\n\n    @staticmethod\n    def get_data_parallel_sub_partitions(tensor,\n                                         max_elements_per_comm,\n                                         world_size,\n                                         dp_process_group=None):\n        total_num_elements = tensor.numel()\n\n        # if total elements is less than our max, revert to splitting into dp partitions\n        max_elements_per_comm = min(total_num_elements, max_elements_per_comm)\n        sub_partition_size = int(max_elements_per_comm // world_size)\n\n        # Ensure partition alignment was done correctly\n        num_sub_partitions = int(total_num_elements // sub_partition_size)\n        assert total_num_elements % sub_partition_size == 0, ""{} % {} != 0"".format(total_num_elements, sub_partition_size)\n\n        # Ensure comm interval alignment was done correctly.\n        num_comm_intervals = int(num_sub_partitions // world_size)\n        assert num_sub_partitions % world_size == 0, ""{} % {} != 0"".format(num_sub_partitions, world_size)\n\n        if not dist.is_initialized() or dist.get_rank(group=dp_process_group) == 0:\n            logger.info(""**** partition info:"")\n            logger.info(""\\t total_num_elements=%s"", total_num_elements)\n            logger.info(""\\t world_size=%s"", world_size)\n            logger.info(""\\t max_elements_per_comm=%s"", max_elements_per_comm)\n            logger.info(""\\t sub_partition_size=%s"", sub_partition_size)\n            logger.info(""\\t num_sub_partitions=%s"", num_sub_partitions)\n            logger.info(""\\t num_comm_intervals=%s"", num_comm_intervals)\n            logger.info(""****"")\n\n        # [comm_id] -> [rank]\n        comm_partitions = []\n        for _ in range(num_comm_intervals):\n            comm_partitions.append([])\n\n        start = 0\n        comm_id = 0\n        element_intervals = defaultdict(\n            list)  # [rank] -> [(start,end), (start,end), ...]\n        for idx in range(num_sub_partitions):\n            rank_id = idx % world_size\n            sub_partition = tensor.narrow(0, start, sub_partition_size).detach()\n            element_intervals[rank_id].append((start, start + sub_partition_size))\n            comm_partitions[comm_id].append(sub_partition)\n            start = start + sub_partition_size\n            if rank_id == (world_size - 1):\n                comm_id += 1\n\n        # [rank] -> [comm_id]\n        sub_partitions = []\n        for _ in range(world_size):\n            sub_partitions.append([])\n        for comm_id, partitions in enumerate(comm_partitions):\n            for rank_id, partition in enumerate(partitions):\n                sub_partitions[rank_id].append(partition)\n\n        return comm_partitions, sub_partitions, element_intervals, sub_partition_size, num_comm_intervals\n\n    @staticmethod\n    def get_all_sub_partition_info(tensor_list,\n                                   all_element_intervals,\n                                   local_rank,\n                                   world_size):\n        params_not_local = []\n\n        # [rank] -> [comm-id] -> [param/offset]\n        params_in_rank_sub_partition = []\n        params_in_rank_sub_partitions_offsets = []\n\n        for rank in range(world_size):\n            params_in_local_sub_partition = []\n            local_sub_partition_offsets = []\n            comm_tensor_list = []\n            comm_offset_list = []\n            current_index = 0\n            prev_comm_idx = 0\n            for iii, tensor in enumerate(tensor_list):\n                tensor_size = tensor.numel()\n                #if local_rank == 0:\n                #    # logger.info(""rank={}, current_index={}, tensor_size={}, tensor-idx={}"".format(rank,\n                #        current_index, tensor_size, iii))\n                results_list = _range_check(current_index,\n                                            all_element_intervals[rank],\n                                            tensor_size)\n                for contained, offset, comm_idx in results_list:\n                    #if local_rank == 0:\n                    #    logger.info(""rank={}, contained={}, offset={}, comm_idx={}"".format(rank, contained,\n                    #        offset, comm_idx))\n                    if contained:\n                        if prev_comm_idx != comm_idx:\n                            params_in_local_sub_partition.append(comm_tensor_list)\n                            comm_tensor_list = []\n                            local_sub_partition_offsets.append(comm_offset_list)\n                            comm_offset_list = []\n                        comm_tensor_list.append(tensor)\n                        comm_offset_list.append(offset)\n                        prev_comm_idx = comm_idx\n                    elif rank == local_rank:\n                        params_not_local.append(tensor)\n\n                current_index = current_index + tensor_size\n\n            #assert len(comm_tensor_list) > 0\n            #assert len(comm_offset_list) > 0\n            params_in_local_sub_partition.append(comm_tensor_list)\n            local_sub_partition_offsets.append(comm_offset_list)\n\n            params_in_rank_sub_partition.append(params_in_local_sub_partition)\n            params_in_rank_sub_partitions_offsets.append(local_sub_partition_offsets)\n\n        return params_in_rank_sub_partition, params_in_rank_sub_partitions_offsets, params_not_local\n\n    @staticmethod\n    def get_flat_sub_partitions(comm_tensor_list,\n                                comm_param_offsets,\n                                sub_partition_size,\n                                dtype,\n                                num_comm_intervals=None,\n                                default_device=None,\n                                return_partition_params=False):\n        partition_params = []\n        final_param_offsets = []\n        flat_sub_partitions = []\n        for tensor_list, param_offsets in zip(comm_tensor_list, comm_param_offsets):\n            flat_tensor_list = []\n            current_size = 0\n            my_offsets = []\n            my_params = []\n\n            if dtype is None:\n                dtype = tensor_list[0].dtype\n\n            for i, tensor in enumerate(tensor_list):\n                if tensor.grad is None:\n                    tensor.grad = torch.zeros(tensor.size(),\n                                              dtype=tensor.dtype,\n                                              device=tensor.device)\n                param = tensor\n                tensor = tensor.grad\n                num_elements = tensor.numel()\n                tensor_offset = 0\n\n                #we need to offset to get to the right element\n                if i == 0 and param_offsets[i] > 0:\n                    tensor_offset = param_offsets[i]\n                    num_elements = num_elements - tensor_offset\n\n                # We don\'t need all elements of the tensor if this tensor is\n                # larger than we have space for in our curr sub-partition\n                if num_elements > (sub_partition_size - current_size):\n                    num_elements = sub_partition_size - current_size\n\n                #we need a narrow view of the tensor based on the tensor offset and number of elements that\n                #we need from this tensor\n                if tensor_offset > 0 or num_elements < tensor.numel():\n                    flat_tensor_list.append(tensor.contiguous().view(-1).narrow(\n                        0,\n                        int(tensor_offset),\n                        int(num_elements)).to(dtype))\n                else:\n                    flat_tensor_list.append(tensor.to(dtype))\n                my_params.append(param)\n\n                #remember offset into partition and #elems for this tensor\n                my_offsets.append((current_size, num_elements))\n\n                current_size = current_size + num_elements\n\n            #this means its the last partition and does not align with the dp boundary. We need to pad before flattening\n            if current_size < sub_partition_size:\n                my_offsets.append((None, None))\n                my_params.append(None)\n                if len(tensor_list) == 0:\n                    assert default_device != None\n                    flat_tensor_list.append(\n                        torch.zeros(int(sub_partition_size - current_size),\n                                    dtype=dtype,\n                                    device=default_device))\n                else:\n                    flat_tensor_list.append(\n                        torch.zeros(int(sub_partition_size - current_size),\n                                    dtype=dtype,\n                                    device=tensor_list[0].device))\n            partition_params.append(my_params)  #flat_tensor_list)\n            final_param_offsets.append(my_offsets)\n            assert len(flat_tensor_list) == len(my_offsets), ""{} {}"".format(len(flat_tensor_list), len(my_offsets))\n            flat_sub_partitions.append(_flatten_dense_tensors(flat_tensor_list))\n        if num_comm_intervals is not None and len(\n                flat_sub_partitions) < num_comm_intervals:\n            # logger.info(""padding w. sub partitions to ensure uniform communication"")\n            device = flat_sub_partitions[0].device\n            for _ in range(num_comm_intervals - len(flat_sub_partitions)):\n                flat_sub_partitions.append(\n                    torch.zeros(int(sub_partition_size),\n                                dtype=dtype,\n                                device=device))\n                partition_params.append([None])\n                final_param_offsets.append([(None, None)])\n\n        if return_partition_params:\n            assert len(flat_sub_partitions) == len(partition_params)\n            assert len(partition_params) == len(final_param_offsets), ""{} {}"".format(len(partition_params), len(final_param_offsets))\n            return flat_sub_partitions, partition_params, final_param_offsets\n        return flat_sub_partitions\n\n    def zero_grad(self, set_grads_to_None=True):\n        """"""\n        Zero FP16 parameter grads.\n        """"""\n        # FP32 grad should never exist.\n        # For speed, set model fp16 grad to None by default\n        for group in self.fp16_groups:\n            for p in group:\n                if set_grads_to_None:\n                    p.grad = None\n                else:\n                    if p.grad is not None:\n                        p.grad.detach_()\n                        p.grad.zero_()\n\n    def free_grad_in_param_list(self, param_list):\n        for p in param_list:\n            if isinstance(p, list):\n                for _p in p:\n                    _p.grad = None\n            else:\n                p.grad = None\n\n    def reduce_scatter_gradients(self,\n                                 postscale_gradients,\n                                 gradient_predivide_factor,\n                                 gradient_average):\n        world_size = dist.get_world_size(group=self.dp_process_group)\n        local_rank = dist.get_rank(group=self.dp_process_group)\n\n        for i, group in enumerate(self.fp16_groups):\n            partition_param_map = {}\n            param_partition_map = {}\n            my_params = set()\n\n            # [rank] -> [comm] -> partition\n            num_comm_intervals = self.num_comm_intervals_per_group[i]\n            all_sub_partitions = []\n            for rank in range(world_size):\n                # gsp is list of partitions indexed by comm_idx\n                #FIXME: currently hardcoding fp16, should infer dtype\n                grad_sub_partitions, partition_params, param_offsets = self.get_flat_sub_partitions(\n                    comm_tensor_list=self.params_in_rank_sub_partitions[i][rank],\n                    comm_param_offsets=self.params_in_rank_sub_partitions_offsets[i][rank],\n                    sub_partition_size=self.sub_partition_sizes[i],\n                    dtype=torch.half, #self.params_in_rank_sub_partitions[i][rank][0][0].dtype,\n                    num_comm_intervals=self.num_comm_intervals_per_group[i],\n                    default_device=\'cuda\', #self.params_in_rank_sub_partitions[i][rank][0][0].device,\n                    return_partition_params=True)\n                all_sub_partitions.append(grad_sub_partitions)\n\n                # create map from partition -> params in that partition\n                for comm_idx, part in enumerate(grad_sub_partitions):\n                    partition_param_map[part] = (partition_params[comm_idx],\n                                                 param_offsets[comm_idx])\n\n                for comm_idx, params in enumerate(partition_params):\n                    for pidx, p in enumerate(params):\n                        # store the parameters we care about locally\n                        if rank == local_rank:\n                            my_params.add(p)\n                        # map from param -> partitions\n                        if p in param_partition_map:\n                            param_partition_map[p].append(grad_sub_partitions[comm_idx])\n                        else:\n                            param_partition_map[p] = [grad_sub_partitions[comm_idx]]\n\n                assert len(grad_sub_partitions) == num_comm_intervals\n\n            if not postscale_gradients:\n                raise NotImplementedError(""pre-scale_gradients is not implemented"")\n\n            all_comm_partitions = []\n            for comm_idx in range(num_comm_intervals):\n                single_comm_all_partitions = []\n                for rank in range(world_size):\n                    single_comm_all_partitions.append(all_sub_partitions[rank][comm_idx])\n                dist.reduce_scatter(output=single_comm_all_partitions[local_rank],\n                                    input_list=single_comm_all_partitions,\n                                    group=self.dp_process_group)\n\n                if gradient_average:\n                    for partition in single_comm_all_partitions:\n                        partition.mul_(gradient_predivide_factor / world_size)\n\n                all_comm_partitions.append(single_comm_all_partitions)\n\n            for p in my_params:\n                partitions = param_partition_map[p]\n                parts = []\n                for part in partitions:\n                    params, offsets = partition_param_map[part]\n                    found = False\n                    for p_idx, _p in enumerate(params):\n                        if p.__hash__() == _p.__hash__():\n                            found = True\n                            if offsets[p_idx][0] is not None:\n                                my_part = part.narrow(0,\n                                                      offsets[p_idx][0],\n                                                      offsets[p_idx][1])\n                                parts.append(my_part)\n                    assert found\n                if p is not None:\n                    updated_grad = _unflatten_dense_tensors(torch.cat(parts), [p])\n                    p.grad.copy_(updated_grad[0])\n\n    def step(self, closure=None):\n        # First compute norm for all group so we know if there is overflow\n\n        self.overflow = self.overflow_checker.check()\n\n        prev_scale = self.loss_scale\n        self._update_scale(self.overflow)\n        if self.overflow:\n            self.zero_grad()\n            if self.verbose:\n                logger.info(""[deepspeed] OVERFLOW! Skipping step. Attempted loss ""\n                            ""scale: {}, reducing to {}"".format(\n                                prev_scale,\n                                self.loss_scale))\n            return self.overflow\n\n        norm_groups = []\n        local_sub_partitions_grad_groups = []\n\n        partition_id = dist.get_rank(group=self.dp_process_group)\n        for i, group in enumerate(self.fp16_groups):\n\n            #TODO RS: update get grad norm to support sub partitions\n            norm_groups.append(get_grad_norm(group, mpu=self.mpu))\n\n            #RS: update free grads w.r.t. sub partitions\n            #free gradients for all the parameters that are not updated by this process\n            self.free_grad_in_param_list(self.params_not_local[i])\n\n            #create flat gradients for parameters updated by this process\n            #tensor_list, first_offset, partition_size, dtype\n            #single_grad_partition = self.get_flat_partition(\n            #    tensor_list=self.params_in_partition[i],\n            #    first_offset=self.first_offset[i],\n            #    partition_size=self.partition_size[i],\n            #    dtype=self.single_partition_of_fp32_groups[i].dtype\n            #)\n\n            #TODO RS: can we safely use dtype of the first sub-partition? i think so\n            local_grad_sub_partitions = self.get_flat_sub_partitions(\n                comm_tensor_list=self.params_in_rank_sub_partitions[i][partition_id],\n                comm_param_offsets=self.params_in_rank_sub_partitions_offsets[i]\n                [partition_id],\n                sub_partition_size=self.sub_partition_sizes[i],\n                dtype=self.local_sub_partitions_of_fp32_groups[i][0].dtype,\n                num_comm_intervals=self.num_comm_intervals_per_group[i],\n                default_device=self.local_sub_partitions_of_fp32_groups[i][0].device)\n\n            #RS: update all our local params with sub-partition grads\n            #logger. info(""self.local_sub_partitions_of_fp32_groups[i]={}, local_grad_sub_partitions={}"".format(len(self.local_sub_partitions_of_fp32_groups[i]), len(local_grad_sub_partitions)))\n            for idx, sub_partition_param in enumerate(self.local_sub_partitions_of_fp32_groups[i]):\n                sub_partition_param.grad = local_grad_sub_partitions[idx]\n            #self.single_partition_of_fp32_groups[i].grad = single_grad_partition\n\n            #RS: update free grads for sub-partitions\n            #release all the gradient since we have already created a necessary copy in dp_grad_partition\n            self.free_grad_in_param_list(\n                self.params_in_rank_sub_partitions[i][partition_id])\n\n            local_sub_partitions_grad_groups.append(local_grad_sub_partitions)\n\n        #RS: update unscale/clip with sub partitions\n        self.unscale_and_clip_grads(local_sub_partitions_grad_groups, norm_groups)\n\n        self.optimizer.step()\n\n        #RS: clear our sub partition grads\n        #get rid of the fp32 gradients. Not needed anymore\n        for group in self.local_sub_partitions_of_fp32_groups:\n            for idx, sub_partition_param in enumerate(group):\n                sub_partition_param.grad = None\n            #group.grad = None\n\n        #NOTE RS: removed norm_groups outer loop from original code, i don\'t think it\'s needed\n        #RS: copy all sub-partition fp32 data to fp16 sub partitions\n        # copy fp32 param data to fp16 partitions w.r.t. our local rank\n        for fp16_all_sub_partitions, fp32_local_sub_partitions in zip(self.parallel_sub_partitioned_fp16_groups, self.local_sub_partitions_of_fp32_groups):\n            for local_sub_partition_param_fp16, local_sub_partition_param_fp32 in zip(fp16_all_sub_partitions[partition_id], fp32_local_sub_partitions):\n                local_sub_partition_param_fp16.data.copy_(\n                    local_sub_partition_param_fp32.data)\n\n        #RS: all_gather/broadcast sub-partitions in separate comm calls\n        #gather the updated weights from everyone\n        for fp16_all_sub_partitions in self.parallel_comm_sub_partitioned_fp16_groups:\n            for comm_id, sub_partitions in enumerate(fp16_all_sub_partitions):\n                dist.all_gather(sub_partitions,\n                                sub_partitions[partition_id],\n                                group=self.dp_process_group)\n\n        # TODO: we probably don\'t need this? just to be safe\n        for i in range(len(norm_groups)):\n            updated_params = _unflatten_dense_tensors(self.fp16_groups_flat[i],\n                                                      self.fp16_groups[i])\n            for p, q in zip(self.fp16_groups[i], updated_params):\n                p.data = q.data\n\n        return self.overflow\n\n    def unscale_and_clip_grads(self, grad_groups_flat, norm_groups):\n        total_norm = 0.0\n        for norm in norm_groups:\n            total_norm += norm**2.0\n        total_norm = math.sqrt(total_norm)\n\n        # compute combined scale factor for this group\n        combined_scale = self.loss_scale\n        if self.clip_grad > 0.:\n            # norm is in fact norm*scale\n            clip = ((total_norm / self.loss_scale) + 1e-6) / self.clip_grad\n            if clip > 1:\n                combined_scale = clip * self.loss_scale\n\n        for grad in grad_groups_flat:\n            if isinstance(grad, list):\n                sub_partitions = grad\n                for g in sub_partitions:\n                    g.data.mul_(1. / combined_scale)\n            else:\n                grad.data.mul_(1. / combined_scale)\n\n    def backward(self, loss, retain_graph=False):\n        self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)\n\n    def _update_scale(self, has_overflow=False):\n        self.loss_scaler.update_scale(has_overflow)\n\n    # Promote state so it can be retrieved or set via ""fp16_optimizer_instance.state""\n    def _get_state(self):\n        return self.optimizer.state\n\n    def _set_state(self, value):\n        self.optimizer.state = value\n\n    state = property(_get_state, _set_state)\n\n    # Promote param_groups so it can be retrieved or set via ""fp16_optimizer_instance.param_groups""\n    # (for example, to adjust the learning rate)\n    def _get_param_groups(self):\n        return self.optimizer.param_groups\n\n    def _set_param_groups(self, value):\n        self.optimizer.param_groups = value\n\n    param_groups = property(_get_param_groups, _set_param_groups)\n\n    # Promote loss scale so it can be retrieved or set via ""fp16_optimizer_instance.loss_scale""\n    def _get_loss_scale(self):\n        return self.loss_scaler.loss_scale\n\n    def _set_loss_scale(self, value):\n        self.loss_scaler.cur_scale = value\n\n    loss_scale = property(_get_loss_scale, _set_loss_scale)\n    cur_scale = property(_get_loss_scale, _set_loss_scale)\n\n    def state_dict(self):\n        """"""\n        Returns a dict containing the current state of this :class:`FP16_Optimizer` instance.\n        This dict contains attributes of :class:`FP16_Optimizer`, as well as the state_dict\n        of the contained Pytorch optimizer.\n        Example::\n            checkpoint = {}\n            checkpoint[\'model\'] = model.state_dict()\n            checkpoint[\'optimizer\'] = optimizer.state_dict()\n            torch.save(checkpoint, ""saved.pth"")\n        """"""\n        state_dict = {}\n        state_dict[\'loss_scaler\'] = self.loss_scaler\n        state_dict[\'dynamic_loss_scale\'] = self.dynamic_loss_scale\n        state_dict[\'overflow\'] = self.overflow\n        state_dict[\'optimizer_state_dict\'] = self.optimizer.state_dict()\n        state_dict[\n            \'local_sub_partitions_of_fp32_groups\'] = self.local_sub_partitions_of_fp32_groups\n        return state_dict\n\n    def load_state_dict(self, state_dict, load_optimizer_states=True):\n        """"""\n        Loads a state_dict created by an earlier call to state_dict().\n        If ``fp16_optimizer_instance`` was constructed from some ``init_optimizer``,\n        whose parameters in turn came from ``model``, it is expected that the user\n        will call ``model.load_state_dict()`` before\n        ``fp16_optimizer_instance.load_state_dict()`` is called.\n        Example::\n            model = torch.nn.Linear(D_in, D_out).cuda().half()\n            optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n            optimizer = FP16_Optimizer(optimizer, static_loss_scale = 128.0)\n            ...\n            checkpoint = torch.load(""saved.pth"")\n            model.load_state_dict(checkpoint[\'model\'])\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\n        """"""\n        # I think it should actually be ok to reload the optimizer before the model.\n        self.loss_scaler = state_dict[\'loss_scaler\']\n        self.dynamic_loss_scale = state_dict[\'dynamic_loss_scale\']\n        self.overflow = state_dict[\'overflow\']\n        if load_optimizer_states:\n            self.optimizer.load_state_dict(state_dict[\'optimizer_state_dict\'])\n\n        for curr_group, saved_group in zip(self.local_sub_partitions_of_fp32_groups, state_dict[\'local_sub_partitions_of_fp32_groups\']):\n            for curr_param, saved_param in zip(curr_group, saved_group):\n                curr_param.data.copy_(saved_param.data)\n'"
deepspeed/pt/zero_utils.py,2,"b'import torch\nimport torch.distributed as dist\n\nfrom deepspeed.pt.log_utils import logger\n\n\ndef _initialize_parameter_parallel_groups(parameter_parallel_size=None):\n    data_parallel_size = int(dist.get_world_size())\n    parameter_parallel_size = parameter_parallel_size or data_parallel_size\n    logger.info(""data_parallel_size: %s, parameter_parallel_size: %s"",\n                data_parallel_size,\n                parameter_parallel_size)\n    assert data_parallel_size % parameter_parallel_size == 0, \\\n        \'world size should be divisible by parameter parallel size\'\n    rank = dist.get_rank()\n    my_group = None\n    for i in range(data_parallel_size // parameter_parallel_size):\n        ranks = range(i * parameter_parallel_size, (i + 1) * parameter_parallel_size)\n        group = torch.distributed.new_group(ranks)\n        if rank in ranks:\n            my_group = group\n    return my_group\n'"
tests/model/run_sanity_check.py,0,"b'# coding=utf-8\n# Copyright (c) 2019, The Microsoft DeepSpeed Team. All rights reserved.\n#\n# Note: please copy webtext data to ""Megatron-LM"" folder, before running this script.\n\nimport sys\nimport unittest\n\nsys.path.append(\'../DeepSpeedExamples/Megatron_GPT2\')\nsys.path.append(\'../DeepSpeedExamples/BingBertSquad\')\n\nimport os\n\n# Import the test cases here.\nimport Megatron_GPT2\nimport BingBertSquad\n\n\ndef pytest_hack(runner_result):\n    \'\'\'This is an ugly hack to get the unittest suites to play nicely with\n    pytest. Otherwise failed tests are not reported by pytest for some reason.\n\n    Long-term, these model tests should be adapted to pytest.\n    \'\'\'\n    if not runner_result.wasSuccessful():\n        print(\'SUITE UNSUCCESSFUL:\', file=sys.stderr)\n        for fails in runner_result.failures:\n            print(fails, file=sys.stderr)\n        assert runner_result.wasSuccessful()  # fail the test\n\n\ndef test_megatron():\n    runner = unittest.TextTestRunner(failfast=True)\n    pytest_hack(runner.run(Megatron_GPT2.suite()))\n\n\ndef test_megatron_checkpoint():\n    runner = unittest.TextTestRunner(failfast=True)\n    pytest_hack(runner.run(Megatron_GPT2.checkpoint_suite()))\n\n\ndef test_squad():\n    runner = unittest.TextTestRunner(failfast=True)\n    pytest_hack(runner.run(BingBertSquad.suite()))\n'"
tests/small_model_debugging/test_model.py,14,"b'import os\nimport json\nimport argparse\nimport torch\nimport deepspeed\nfrom torch.utils.data.distributed import DistributedSampler\n\n\nclass SimpleModel(torch.nn.Module):\n    def __init__(self, hidden_dim, empty_grad=False):\n        super(SimpleModel, self).__init__()\n        self.linear = torch.nn.Linear(hidden_dim, hidden_dim)\n        if empty_grad:\n            self.layers2 = torch.nn.ModuleList([torch.nn.Linear(hidden_dim, hidden_dim)])\n        self.cross_entropy_loss = torch.nn.CrossEntropyLoss()\n\n    def forward(self, x, y):\n        hidden_dim = x\n        hidden_dim = self.linear(hidden_dim)\n        return self.cross_entropy_loss(hidden_dim, y)\n\n\ndef create_config_from_dict(tmpdir, config_dict):\n    config_path = os.path.join(tmpdir, \'temp_config.json\')\n    with open(config_path, \'w\') as fd:\n        json.dump(config_dict, fd)\n    return config_path\n\n\ndef get_data_loader(model, total_samples, hidden_dim, device):\n    batch_size = model.train_micro_batch_size_per_gpu()\n    train_data = torch.randn(total_samples, hidden_dim, device=device, dtype=torch.half)\n    train_label = torch.empty(total_samples,\n                              dtype=torch.long,\n                              device=device).random_(hidden_dim)\n    train_dataset = torch.utils.data.TensorDataset(train_data, train_label)\n    sampler = DistributedSampler(train_dataset)\n    train_loader = torch.utils.data.DataLoader(train_dataset,\n                                               batch_size=batch_size,\n                                               sampler=sampler)\n    return train_loader\n\n\ndef get_args(tmpdir, config_dict):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--local_rank"", type=int, default=0)\n    parser.add_argument(\'--zero\', type=int, default=0)\n    args = parser.parse_args()  #args=\'\'\n\n    config_dict[""zero_optimization""][""stage""] = args.zero\n    print(\'config_dict[""zero_optimization""]\', config_dict[""zero_optimization""])\n    config_path = create_config_from_dict(tmpdir, config_dict)\n\n    args.deepspeed_config = config_path\n    return args\n\n\ndef print0(msg):\n    if torch.distributed.get_rank() == 0:\n        print(msg, flush=True)\n\n\nrank = int(os.environ[\'RANK\'])\nprint(\'seed:\', 2222 + rank)\ntorch.random.manual_seed(2222 + rank)\n\nconfig_dict = {\n    ""train_batch_size"": 8,\n    ""steps_per_print"": 1,\n    ""optimizer"": {\n        ""type"": ""Adam"",\n        ""params"": {\n            ""lr"": 0.00015,\n        }\n    },\n    ""fp16"": {\n        ""enabled"": True,\n        ""initial_scale_power"": 15\n    },\n    ""zero_optimization"": {\n        ""stage"": 0,\n        ""reduce_bucket_size"": 20\n    }\n}\n#        ""initial_scale_power"": 15\nargs = get_args(\'/tmp/\', config_dict)\nhidden_dim = 4\n\nmodel = SimpleModel(hidden_dim, empty_grad=False)\n\nmodel, _, _,_ = deepspeed.initialize(args=args,\n                                     model=model,\n                                     model_parameters=model.parameters(),\n                                     dist_init_required=True)\n\n\ndef print_params(tag, model):\n    if torch.distributed.get_rank() == 0:\n        for n, p in model.named_parameters():\n            print0(""{} {}:{}"".format(tag, n, p))\n\n\ndata_loader = get_data_loader(model=model,\n                              total_samples=1000,\n                              hidden_dim=hidden_dim,\n                              device=model.device)\n#print_params(\'pre-train\', model)\nfor n, batch in enumerate(data_loader):\n    loss = model(batch[0], batch[1])\n    if torch.distributed.get_rank() == 0:\n        print(""LOSS:"", loss.item())\n    model.backward(loss)\n    model.step()\n    #print_params(\'step={}\'.format(n), model)\n    if n == 5: break\n'"
tests/unit/common.py,6,"b'import os\nimport time\n\nimport torch\nimport torch.distributed as dist\nfrom torch.multiprocessing import Process\n\nimport pytest\n\n# Worker timeout *after* the first worker has completed.\nDEEPSPEED_UNIT_WORKER_TIMEOUT = 120\n\n\ndef distributed_test(world_size=2, backend=\'nccl\'):\n    """"""A decorator for executing a function (e.g., a unit test) in a distributed manner.\n    This decorator manages the spawning and joining of processes, initialization of\n    torch.distributed, and catching of errors.\n\n    Usage example:\n        @distributed_test(worker_size=[2,3])\n        def my_test():\n            rank = dist.get_rank()\n            world_size = dist.get_world_size()\n            assert(rank < world_size)\n\n    Arguments:\n        world_size (int or list): number of ranks to spawn. Can be a list to spawn\n        multiple tests.\n    """"""\n    def dist_wrap(run_func):\n        """"""Second-level decorator for dist_test. This actually wraps the function. """"""\n        def dist_init(local_rank, num_procs, *func_args, **func_kwargs):\n            """"""Initialize torch.distributed and execute the user function. """"""\n            os.environ[\'MASTER_ADDR\'] = \'127.0.0.1\'\n            os.environ[\'MASTER_PORT\'] = \'29500\'\n            dist.init_process_group(backend=backend,\n                                    init_method=\'env://\',\n                                    rank=local_rank,\n                                    world_size=num_procs)\n\n            if torch.cuda.is_available():\n                torch.cuda.set_device(local_rank)\n\n            run_func(*func_args, **func_kwargs)\n\n        def dist_launcher(num_procs, *func_args, **func_kwargs):\n            """"""Launch processes and gracefully handle failures. """"""\n\n            # Spawn all workers on subprocesses.\n            processes = []\n            for local_rank in range(num_procs):\n                p = Process(target=dist_init,\n                            args=(local_rank,\n                                  num_procs,\n                                  *func_args),\n                            kwargs=func_kwargs)\n                p.start()\n                processes.append(p)\n\n            # Now loop and wait for a test to complete. The spin-wait here isn\'t a big\n            # deal because the number of processes will be O(#GPUs) << O(#CPUs).\n            any_done = False\n            while not any_done:\n                for p in processes:\n                    if not p.is_alive():\n                        any_done = True\n                        break\n\n            # Wait for all other processes to complete\n            for p in processes:\n                p.join(DEEPSPEED_UNIT_WORKER_TIMEOUT)\n\n            failed = [(rank, p) for rank, p in enumerate(processes) if p.exitcode != 0]\n            for rank, p in failed:\n                # If it still hasn\'t terminated, kill it because it hung.\n                if p.exitcode is None:\n                    p.terminate()\n                    pytest.fail(f\'Worker {rank} hung.\', pytrace=False)\n                if p.exitcode < 0:\n                    pytest.fail(f\'Worker {rank} killed by signal {-p.exitcode}\',\n                                pytrace=False)\n                if p.exitcode > 0:\n                    pytest.fail(f\'Worker {rank} exited with code {p.exitcode}\',\n                                pytrace=False)\n\n        def run_func_decorator(*func_args, **func_kwargs):\n            """"""Entry point for @distributed_test(). """"""\n\n            if isinstance(world_size, int):\n                dist_launcher(world_size, *func_args, **func_kwargs)\n            elif isinstance(world_size, list):\n                for procs in world_size:\n                    dist_launcher(procs, *func_args, **func_kwargs)\n                    time.sleep(0.5)\n            else:\n                raise TypeError(f\'world_size must be an integer or a list of integers.\')\n\n        return run_func_decorator\n\n    return dist_wrap\n'"
tests/unit/modeling.py,93,"b'# DeepSpeed note, code taken from commit 3d59216cec89a363649b4fe3d15295ba936ced0f\n# https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/BERT/modeling.py\n\n# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HugginFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""PyTorch BERT model.""""""\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport copy\nimport json\nimport logging\nimport math\nimport os\nimport shutil\nimport tarfile\nimport tempfile\nimport sys\nfrom io import open\n\nimport torch\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss\nfrom torch.utils import checkpoint\nimport torch.distributed as dist\n\nfrom torch.nn import Module\nfrom torch.nn.parameter import Parameter\nimport torch.nn.functional as F\nimport torch.nn.init as init\nimport time\n\n#from numba import cuda\n\n#from deepspeed_cuda import DeepSpeedSoftmaxConfig, DeepSpeedSoftmax\n\nlogger = logging.getLogger(__name__)\n\nPRETRAINED_MODEL_ARCHIVE_MAP = {\n    \'bert-base-uncased\':\n    ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz"",\n    \'bert-large-uncased\':\n    ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased.tar.gz"",\n    \'bert-base-cased\':\n    ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz"",\n    \'bert-large-cased\':\n    ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased.tar.gz"",\n    \'bert-base-multilingual-uncased\':\n    ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased.tar.gz"",\n    \'bert-base-multilingual-cased\':\n    ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased.tar.gz"",\n    \'bert-base-chinese\':\n    ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz"",\n}\nCONFIG_NAME = \'bert_config.json\'\nWEIGHTS_NAME = \'pytorch_model.bin\'\nTF_WEIGHTS_NAME = \'model.ckpt\'\n\n\ndef load_tf_weights_in_bert(model, tf_checkpoint_path):\n    """""" Load tf checkpoints in a pytorch model\n    """"""\n    try:\n        import re\n        import numpy as np\n        import tensorflow as tf\n    except ImportError:\n        print(\n            ""Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see ""\n            ""https://www.tensorflow.org/install/ for installation instructions."")\n        raise\n    tf_path = os.path.abspath(tf_checkpoint_path)\n    print(""Converting TensorFlow checkpoint from {}"".format(tf_path))\n    # Load weights from TF model\n    init_vars = tf.train.list_variables(tf_path)\n    names = []\n    arrays = []\n    for name, shape in init_vars:\n        print(""Loading TF weight {} with shape {}"".format(name, shape))\n        array = tf.train.load_variable(tf_path, name)\n        names.append(name)\n        arrays.append(array)\n\n    for name, array in zip(names, arrays):\n        name = name.split(\'/\')\n        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n        # which are not required for using pretrained model\n        if any(n in [""adam_v"", ""adam_m""] for n in name):\n            print(""Skipping {}"".format(""/"".join(name)))\n            continue\n        pointer = model\n        for m_name in name:\n            if re.fullmatch(r\'[A-Za-z]+_\\d+\', m_name):\n                l = re.split(r\'_(\\d+)\', m_name)\n            else:\n                l = [m_name]\n            if l[0] == \'kernel\' or l[0] == \'gamma\':\n                pointer = getattr(pointer, \'weight\')\n            elif l[0] == \'output_bias\' or l[0] == \'beta\':\n                pointer = getattr(pointer, \'bias\')\n            elif l[0] == \'output_weights\':\n                pointer = getattr(pointer, \'weight\')\n            else:\n                pointer = getattr(pointer, l[0])\n            if len(l) >= 2:\n                num = int(l[1])\n                pointer = pointer[num]\n        if m_name[-11:] == \'_embeddings\':\n            pointer = getattr(pointer, \'weight\')\n        elif m_name == \'kernel\':\n            array = np.transpose(array)\n        try:\n            assert pointer.shape == array.shape\n        except AssertionError as e:\n            e.args += (pointer.shape, array.shape)\n            raise\n        print(""Initialize PyTorch weight {}"".format(name))\n        pointer.data = torch.from_numpy(array)\n    return model\n\n\n@torch.jit.script\ndef f_gelu(x):\n    return x * 0.5 * (1.0 + torch.erf(x / 1.41421))\n\n\n@torch.jit.script\ndef bias_gelu(bias, y):\n    x = bias + y\n    return x * 0.5 * (1.0 + torch.erf(x / 1.41421))\n\n\n@torch.jit.script\ndef bias_tanh(bias, y):\n    x = bias + y\n    return torch.tanh(x)\n\n\ndef gelu(x):\n    """"""Implementation of the gelu activation function.\n        For information: OpenAI GPT\'s gelu is slightly different (and gives slightly different results):\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n        Also see https://arxiv.org/abs/1606.08415\n    """"""\n    return f_gelu(x)\n\n\ndef swish(x):\n    return x * torch.sigmoid(x)\n\n\nACT2FN = {""gelu"": gelu, ""relu"": torch.nn.functional.relu, ""swish"": swish}\n\n\nclass GPUTimer:\n    def __init__(self):\n        super().__init__()\n        self.start = cuda.event()\n        self.stop = cuda.event()\n\n    def record(self):\n        self.start.record()\n\n    def elapsed(self):\n        self.stop.record()\n        self.stop.synchronize()\n        return self.start.elapsed_time(self.stop) / 1000.0\n\n\nclass LinearActivation(Module):\n    r""""""Fused Linear and activation Module.\n    """"""\n    __constants__ = [\'bias\']\n\n    def __init__(self,\n                 in_features,\n                 out_features,\n                 weights,\n                 biases,\n                 act=\'gelu\',\n                 bias=True):\n        super(LinearActivation, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.fused_gelu = False\n        self.fused_tanh = False\n        if isinstance(act,\n                      str) or (sys.version_info[0] == 2 and isinstance(act,\n                                                                       unicode)):\n            if bias and act == \'gelu\':\n                self.fused_gelu = True\n            elif bias and act == \'tanh\':\n                self.fused_tanh = True\n            else:\n                self.act_fn = ACT2FN[act]\n        else:\n            self.act_fn = act\n        #self.weight = Parameter(torch.Tensor(out_features, in_features))\n        self.weight = weights[5]\n        self.bias = biases[5]\n        #if bias:\n        #    self.bias = Parameter(torch.Tensor(out_features))\n        #else:\n        #    self.register_parameter(\'bias\', None)\n        #self.reset_parameters()\n\n    def reset_parameters(self):\n        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in)\n            init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, input):\n        if self.fused_gelu:\n            #timing = []\n            #t1 = GPUTimer()\n            #t1.record()\n            y = F.linear(input, self.weight, None)\n            #timing.append(t1.elapsed())\n            #t1.record()\n            bg = bias_gelu(self.bias, y)\n            #timing.append(t1.elapsed())\n            return bg\n        elif self.fused_tanh:\n            return bias_tanh(self.bias, F.linear(input, self.weight, None))\n        else:\n            return self.act_fn(F.linear(input, self.weight, self.bias))\n\n    def extra_repr(self):\n        return \'in_features={}, out_features={}, bias={}\'.format(\n            self.in_features,\n            self.out_features,\n            self.bias is not None)\n\n\nclass BertConfig(object):\n    """"""Configuration class to store the configuration of a `BertModel`.\n    """"""\n    def __init__(self,\n                 vocab_size_or_config_json_file,\n                 hidden_size=768,\n                 num_hidden_layers=12,\n                 num_attention_heads=12,\n                 intermediate_size=3072,\n                 batch_size=8,\n                 hidden_act=""gelu"",\n                 hidden_dropout_prob=0.1,\n                 attention_probs_dropout_prob=0.1,\n                 max_position_embeddings=512,\n                 type_vocab_size=2,\n                 initializer_range=0.02,\n                 fp16=False):\n        """"""Constructs BertConfig.\n\n        Args:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n            hidden_size: Size of the encoder layers and the pooler layer.\n            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n            num_attention_heads: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            intermediate_size: The size of the ""intermediate"" (i.e., feed-forward)\n                layer in the Transformer encoder.\n            hidden_act: The non-linear activation function (function or string) in the\n                encoder and pooler. If string, ""gelu"", ""relu"" and ""swish"" are supported.\n            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            attention_probs_dropout_prob: The dropout ratio for the attention\n                probabilities.\n            max_position_embeddings: The maximum sequence length that this model might\n                ever be used with. Typically set this to something large just in case\n                (e.g., 512 or 1024 or 2048).\n            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n                `BertModel`.\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n        """"""\n        if isinstance(vocab_size_or_config_json_file,\n                      str) or (sys.version_info[0] == 2\n                               and isinstance(vocab_size_or_config_json_file,\n                                              unicode)):\n            with open(vocab_size_or_config_json_file, ""r"", encoding=\'utf-8\') as reader:\n                json_config = json.loads(reader.read())\n            for key, value in json_config.items():\n                self.__dict__[key] = value\n        elif isinstance(vocab_size_or_config_json_file, int):\n            self.vocab_size = vocab_size_or_config_json_file\n            self.hidden_size = hidden_size\n            self.num_hidden_layers = num_hidden_layers\n            self.num_attention_heads = num_attention_heads\n            self.batch_size = batch_size\n            self.hidden_act = hidden_act\n            self.intermediate_size = intermediate_size\n            self.hidden_dropout_prob = hidden_dropout_prob\n            self.attention_probs_dropout_prob = attention_probs_dropout_prob\n            self.max_position_embeddings = max_position_embeddings\n            self.type_vocab_size = type_vocab_size\n            self.initializer_range = initializer_range\n            self.fp16 = fp16\n        else:\n            raise ValueError(""First argument must be either a vocabulary size (int)""\n                             ""or the path to a pretrained model config file (str)"")\n\n    @classmethod\n    def from_dict(cls, json_object):\n        """"""Constructs a `BertConfig` from a Python dictionary of parameters.""""""\n        config = BertConfig(vocab_size_or_config_json_file=-1)\n        for key, value in json_object.items():\n            config.__dict__[key] = value\n        return config\n\n    @classmethod\n    def from_json_file(cls, json_file):\n        """"""Constructs a `BertConfig` from a json file of parameters.""""""\n        with open(json_file, ""r"", encoding=\'utf-8\') as reader:\n            text = reader.read()\n        return cls.from_dict(json.loads(text))\n\n    def __repr__(self):\n        return str(self.to_json_string())\n\n    def to_dict(self):\n        """"""Serializes this instance to a Python dictionary.""""""\n        output = copy.deepcopy(self.__dict__)\n        return output\n\n    def to_json_string(self):\n        """"""Serializes this instance to a JSON string.""""""\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\n\n\ntry:\n    import apex\n    #apex.amp.register_half_function(apex.normalization.fused_layer_norm, \'FusedLayerNorm\')\n    import apex.normalization\n    #apex.amp.register_float_function(apex.normalization.FusedLayerNorm, \'forward\')\n    BertLayerNorm = apex.normalization.FusedLayerNorm\nexcept ImportError:\n    print(\n        ""Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.""\n    )\n\n    class BertLayerNorm(nn.Module):\n        def __init__(self, hidden_size, eps=1e-12):\n            """"""Construct a layernorm module in the TF style (epsilon inside the square root).\n            """"""\n            super(BertLayerNorm, self).__init__()\n            self.weight = nn.Parameter(torch.ones(hidden_size))\n            self.bias = nn.Parameter(torch.zeros(hidden_size))\n            self.variance_epsilon = eps\n\n        def forward(self, x):\n            u = x.mean(-1, keepdim=True)\n            s = (x - u).pow(2).mean(-1, keepdim=True)\n            x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n            return self.weight * x + self.bias\n\n\nclass BertEmbeddings(nn.Module):\n    """"""Construct the embeddings from word, position and token_type embeddings.\n    """"""\n    def __init__(self, config):\n        super(BertEmbeddings, self).__init__()\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings,\n                                                config.hidden_size)\n        self.token_type_embeddings = nn.Embedding(config.type_vocab_size,\n                                                  config.hidden_size)\n\n        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n        # any TensorFlow checkpoint file\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, input_ids, token_type_ids=None):\n        seq_length = input_ids.size(1)\n        position_ids = torch.arange(seq_length,\n                                    dtype=torch.long,\n                                    device=input_ids.device)\n        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        words_embeddings = self.word_embeddings(input_ids)\n        position_embeddings = self.position_embeddings(position_ids)\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n\n        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n        embeddings = self.LayerNorm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings\n\n\nclass BertSelfAttention(nn.Module):\n    def __init__(self, i, config, weights, biases):\n        super(BertSelfAttention, self).__init__()\n        if config.hidden_size % config.num_attention_heads != 0:\n            raise ValueError(\n                ""The hidden size (%d) is not a multiple of the number of attention ""\n                ""heads (%d)"" % (config.hidden_size,\n                                config.num_attention_heads))\n        self.num_attention_heads = config.num_attention_heads\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\n\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n        self.query.weight = weights[0]\n        self.query.bias = biases[0]\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n        self.key.weight = weights[1]\n        self.key.bias = biases[1]\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n        self.value.weight = weights[2]\n        self.value.bias = biases[2]\n\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n        self.softmax = nn.Softmax(dim=-1)\n        #self.softmax_config = DeepSpeedSoftmaxConfig()\n        #self.softmax_config.batch_size = config.batch_size\n        #self.softmax_config.max_seq_length = config.max_position_embeddings\n        #self.softmax_config.hidden_size = config.hidden_size\n        #self.softmax_config.heads = config.num_attention_heads\n        #self.softmax_config.softmax_id = i\n        #self.softmax_config.fp16 = config.fp16\n        #self.softmax_config.prob_drop_out = 0.0\n        #self.softmax = DeepSpeedSoftmax(i, self.softmax_config)\n\n    def transpose_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads,\n                                       self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 1, 3)\n\n    def transpose_key_for_scores(self, x):\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads,\n                                       self.attention_head_size)\n        x = x.view(*new_x_shape)\n        return x.permute(0, 2, 3, 1)\n\n    def forward(self, hidden_states, attention_mask, grads=None):\n\n        mixed_query_layer = self.query(hidden_states)\n\n        mixed_key_layer = self.key(hidden_states)\n\n        mixed_value_layer = self.value(hidden_states)\n\n        query_layer = self.transpose_for_scores(mixed_query_layer)\n\n        key_layer = self.transpose_key_for_scores(mixed_key_layer)\n\n        value_layer = self.transpose_for_scores(mixed_value_layer)\n\n        attention_scores = torch.matmul(query_layer, key_layer)\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n        attention_scores = attention_scores + attention_mask\n        attention_probs = self.softmax(attention_scores)\n\n        # This is actually dropping out entire tokens to attend to, which might\n        # seem a bit unusual, but is taken from the original Transformer paper.\n        attention_probs = self.dropout(attention_probs)\n\n        context_layer = torch.matmul(attention_probs, value_layer)\n        context_layer1 = context_layer.permute(0, 2, 1, 3).contiguous()\n        new_context_layer_shape = context_layer1.size()[:-2] + (self.all_head_size, )\n        context_layer1 = context_layer1.view(*new_context_layer_shape)\n\n        return context_layer1\n\n\nclass BertSelfOutput(nn.Module):\n    def __init__(self, config, weights, biases):\n        super(BertSelfOutput, self).__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.dense.weight = weights[3]\n        self.dense.bias = biases[3]\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n    def get_w(self):\n        return self.dense.weight\n\n\nclass BertAttention(nn.Module):\n    def __init__(self, i, config, weights, biases):\n        super(BertAttention, self).__init__()\n        self.self = BertSelfAttention(i, config, weights, biases)\n        self.output = BertSelfOutput(config, weights, biases)\n\n    def forward(self, input_tensor, attention_mask):\n        self_output = self.self(input_tensor, attention_mask)\n        attention_output = self.output(self_output, input_tensor)\n        return attention_output\n\n    def get_w(self):\n        return self.output.get_w()\n\n\nclass BertIntermediate(nn.Module):\n    def __init__(self, config, weights, biases):\n        super(BertIntermediate, self).__init__()\n        self.dense_act = LinearActivation(config.hidden_size,\n                                          config.intermediate_size,\n                                          weights,\n                                          biases,\n                                          act=config.hidden_act)\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense_act(hidden_states)\n        return hidden_states\n\n\nclass BertOutput(nn.Module):\n    def __init__(self, config, weights, biases):\n        super(BertOutput, self).__init__()\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.dense.weight = weights[6]\n        self.dense.bias = biases[6]\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n\n    def forward(self, hidden_states, input_tensor):\n        hidden_states = self.dense(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n        return hidden_states\n\n\nclass BertLayer(nn.Module):\n    def __init__(self, i, config, weights, biases):\n        super(BertLayer, self).__init__()\n        self.attention = BertAttention(i, config, weights, biases)\n        self.intermediate = BertIntermediate(config, weights, biases)\n        self.output = BertOutput(config, weights, biases)\n        self.weight = weights\n        self.biases = biases\n\n    def forward(self, hidden_states, attention_mask, grads, collect_all_grads=False):\n        attention_output = self.attention(hidden_states, attention_mask)\n        intermediate_output = self.intermediate(attention_output)\n        layer_output = self.output(intermediate_output, attention_output)\n\n        if collect_all_grads:\n            # self.weight[0].register_hook(lambda x, self=self: grads.append([x,""Q_W""]))\n            # self.biases[0].register_hook(lambda x, self=self: grads.append([x,""Q_B""]))\n            # self.weight[1].register_hook(lambda x, self=self: grads.append([x,""K_W""]))\n            # self.biases[1].register_hook(lambda x, self=self: grads.append([x,""K_B""]))\n            self.weight[2].register_hook(lambda x, self=self: grads.append([x, ""V_W""]))\n            self.biases[2].register_hook(lambda x, self=self: grads.append([x, ""V_B""]))\n            self.weight[3].register_hook(lambda x, self=self: grads.append([x, ""O_W""]))\n            self.biases[3].register_hook(lambda x, self=self: grads.append([x, ""O_B""]))\n            self.attention.output.LayerNorm.weight.register_hook(\n                lambda x,\n                self=self: grads.append([x,\n                                         ""N2_W""]))\n            self.attention.output.LayerNorm.bias.register_hook(\n                lambda x,\n                self=self: grads.append([x,\n                                         ""N2_B""]))\n            self.weight[5].register_hook(lambda x, self=self: grads.append([x, ""int_W""]))\n            self.biases[5].register_hook(lambda x, self=self: grads.append([x, ""int_B""]))\n            self.weight[6].register_hook(lambda x, self=self: grads.append([x, ""out_W""]))\n            self.biases[6].register_hook(lambda x, self=self: grads.append([x, ""out_B""]))\n            self.output.LayerNorm.weight.register_hook(\n                lambda x,\n                self=self: grads.append([x,\n                                         ""norm_W""]))\n            self.output.LayerNorm.bias.register_hook(\n                lambda x,\n                self=self: grads.append([x,\n                                         ""norm_B""]))\n\n        return layer_output\n\n    def get_w(self):\n        return self.attention.get_w()\n\n\nclass BertEncoder(nn.Module):\n    def __init__(self, config, weights, biases):\n        super(BertEncoder, self).__init__()\n        #layer = BertLayer(config, weights, biases)\n        self.FinalLayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n\n        self.layer = nn.ModuleList([\n            copy.deepcopy(BertLayer(i,\n                                    config,\n                                    weights,\n                                    biases)) for i in range(config.num_hidden_layers)\n        ])\n        self.grads = []\n        self.graph = []\n\n    def get_grads(self):\n        return self.grads\n\n    # def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True):\n    #     all_encoder_layers = []\n    #     for layer_module in self.layer:\n    #         hidden_states = layer_module(hidden_states, attention_mask)\n    #         if output_all_encoded_layers:\n    #             all_encoder_layers.append(hidden_states)\n    #     if not output_all_encoded_layers:\n    #         all_encoder_layers.append(hidden_states)\n    #     return all_encoder_layers\n\n    def get_modules(self, big_node, input):\n        for mdl in big_node.named_children():\n            graph.append(mdl)\n            get_modules(self, mdl, input)\n\n    def forward(self,\n                hidden_states,\n                attention_mask,\n                output_all_encoded_layers=True,\n                checkpoint_activations=False):\n        all_encoder_layers = []\n\n        def custom(start, end):\n            def custom_forward(*inputs):\n                layers = self.layer[start:end]\n                x_ = inputs[0]\n                for layer in layers:\n                    x_ = layer(x_, inputs[1])\n                return x_\n\n            return custom_forward\n\n        if checkpoint_activations:\n            l = 0\n            num_layers = len(self.layer)\n            chunk_length = math.ceil(math.sqrt(num_layers))\n            while l < num_layers:\n                hidden_states = checkpoint.checkpoint(custom(l,\n                                                             l + chunk_length),\n                                                      hidden_states,\n                                                      attention_mask * 1)\n                l += chunk_length\n            # decoder layers\n        else:\n            for i, layer_module in enumerate(self.layer):\n                hidden_states = layer_module(hidden_states,\n                                             attention_mask,\n                                             self.grads,\n                                             collect_all_grads=True)\n                hidden_states.register_hook(\n                    lambda x,\n                    i=i,\n                    self=self: self.grads.append([x,\n                                                  ""hidden_state""]))\n                #print(""pytorch weight is: "", layer_module.get_w())\n\n                if output_all_encoded_layers:\n                    all_encoder_layers.append((hidden_states))\n\n        if not output_all_encoded_layers or checkpoint_activations:\n            all_encoder_layers.append((hidden_states))\n        return all_encoder_layers\n\n\n#class BertEncoder(nn.Module):\n#    def __init__(self, config):\n#        super(BertEncoder, self).__init__()\n#        layer = BertLayer(config)\n#        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)])\n#\n#    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True):\n#        all_encoder_layers = []\n#        for layer_module in self.layer:\n#            hidden_states = layer_module(hidden_states, attention_mask)\n#            if output_all_encoded_layers:\n#                all_encoder_layers.append(hidden_states)\n#        if not output_all_encoded_layers:\n#            all_encoder_layers.append(hidden_states)\n#        return all_encoder_layers\n\n\nclass BertPooler(nn.Module):\n    def __init__(self, config):\n        super(BertPooler, self).__init__()\n        self.dense_act = LinearActivation(config.hidden_size,\n                                          config.hidden_size,\n                                          act=""tanh"")\n\n    def forward(self, hidden_states):\n        # We ""pool"" the model by simply taking the hidden state corresponding\n        # to the first token.\n        first_token_tensor = hidden_states[:, 0]\n        pooled_output = self.dense_act(first_token_tensor)\n        return pooled_output\n\n\nclass BertPredictionHeadTransform(nn.Module):\n    def __init__(self, config):\n        super(BertPredictionHeadTransform, self).__init__()\n        self.dense_act = LinearActivation(config.hidden_size,\n                                          config.hidden_size,\n                                          act=config.hidden_act)\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n\n    def forward(self, hidden_states):\n        hidden_states = self.dense_act(hidden_states)\n        hidden_states = self.LayerNorm(hidden_states)\n        return hidden_states\n\n\nclass BertLMPredictionHead(nn.Module):\n    def __init__(self, config, bert_model_embedding_weights):\n        super(BertLMPredictionHead, self).__init__()\n        self.transform = BertPredictionHeadTransform(config)\n\n        # The output weights are the same as the input embeddings, but there is\n        # an output-only bias for each token.\n        self.decoder = nn.Linear(bert_model_embedding_weights.size(1),\n                                 bert_model_embedding_weights.size(0),\n                                 bias=False)\n        self.decoder.weight = bert_model_embedding_weights\n        self.bias = nn.Parameter(torch.zeros(bert_model_embedding_weights.size(0)))\n\n    def forward(self, hidden_states):\n        hidden_states = self.transform(hidden_states)\n        torch.cuda.nvtx.range_push(\n            ""decoder input.size() = {}, weight.size() = {}"".format(\n                hidden_states.size(),\n                self.decoder.weight.size()))\n        hidden_states = self.decoder(hidden_states) + self.bias\n        torch.cuda.nvtx.range_pop()\n        return hidden_states\n\n\nclass BertOnlyMLMHead(nn.Module):\n    def __init__(self, config, bert_model_embedding_weights):\n        super(BertOnlyMLMHead, self).__init__()\n        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\n\n    def forward(self, sequence_output):\n        prediction_scores = self.predictions(sequence_output)\n        return prediction_scores\n\n\nclass BertOnlyNSPHead(nn.Module):\n    def __init__(self, config):\n        super(BertOnlyNSPHead, self).__init__()\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n\n    def forward(self, pooled_output):\n        seq_relationship_score = self.seq_relationship(pooled_output)\n        return seq_relationship_score\n\n\nclass BertPreTrainingHeads(nn.Module):\n    def __init__(self, config, bert_model_embedding_weights):\n        super(BertPreTrainingHeads, self).__init__()\n        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n\n    def forward(self, sequence_output, pooled_output):\n        prediction_scores = self.predictions(sequence_output)\n        seq_relationship_score = self.seq_relationship(pooled_output)\n        return prediction_scores, seq_relationship_score\n\n\nclass BertPreTrainedModel(nn.Module):\n    """""" An abstract class to handle weights initialization and\n        a simple interface for dowloading and loading pretrained models.\n    """"""\n    def __init__(self, config, *inputs, **kwargs):\n        super(BertPreTrainedModel, self).__init__()\n        if not isinstance(config, BertConfig):\n            raise ValueError(\n                ""Parameter config in `{}(config)` should be an instance of class `BertConfig`. ""\n                ""To create a model from a Google pretrained model use ""\n                ""`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`"".format(\n                    self.__class__.__name__,\n                    self.__class__.__name__))\n        self.config = config\n\n    def init_bert_weights(self, module):\n        """""" Initialize the weights.\n        """"""\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            # Slightly different from the TF version which uses truncated_normal for initialization\n            # cf https://github.com/pytorch/pytorch/pull/5617\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n        elif isinstance(module, BertLayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        if isinstance(module, nn.Linear) and module.bias is not None:\n            module.bias.data.zero_()\n\n    @classmethod\n    def from_pretrained(cls,\n                        pretrained_model_name_or_path,\n                        state_dict=None,\n                        cache_dir=None,\n                        from_tf=False,\n                        *inputs,\n                        **kwargs):\n        """"""\n        Instantiate a BertPreTrainedModel from a pre-trained model file or a pytorch state dict.\n        Download and cache the pre-trained model file if needed.\n\n        Params:\n            pretrained_model_name_or_path: either:\n                - a str with the name of a pre-trained model to load selected in the list of:\n                    . `bert-base-uncased`\n                    . `bert-large-uncased`\n                    . `bert-base-cased`\n                    . `bert-large-cased`\n                    . `bert-base-multilingual-uncased`\n                    . `bert-base-multilingual-cased`\n                    . `bert-base-chinese`\n                - a path or url to a pretrained model archive containing:\n                    . `bert_config.json` a configuration file for the model\n                    . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\n                - a path or url to a pretrained model archive containing:\n                    . `bert_config.json` a configuration file for the model\n                    . `model.chkpt` a TensorFlow checkpoint\n            from_tf: should we load the weights from a locally saved TensorFlow checkpoint\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of Google pre-trained models\n            *inputs, **kwargs: additional input for the specific Bert class\n                (ex: num_labels for BertForSequenceClassification)\n        """"""\n        if pretrained_model_name_or_path in PRETRAINED_MODEL_ARCHIVE_MAP:\n            archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name_or_path]\n        else:\n            archive_file = pretrained_model_name_or_path\n        if resolved_archive_file == archive_file:\n            logger.info(""loading archive file {}"".format(archive_file))\n        else:\n            logger.info(""loading archive file {} from cache at {}"".format(\n                archive_file,\n                resolved_archive_file))\n        tempdir = None\n        if os.path.isdir(resolved_archive_file) or from_tf:\n            serialization_dir = resolved_archive_file\n        else:\n            # Extract archive to temp dir\n            tempdir = tempfile.mkdtemp()\n            logger.info(""extracting archive file {} to temp dir {}"".format(\n                resolved_archive_file,\n                tempdir))\n            with tarfile.open(resolved_archive_file, \'r:gz\') as archive:\n                archive.extractall(tempdir)\n            serialization_dir = tempdir\n        # Load config\n        config_file = os.path.join(serialization_dir, CONFIG_NAME)\n        config = BertConfig.from_json_file(config_file)\n        logger.info(""Model config {}"".format(config))\n        # Instantiate model.\n        model = cls(config, *inputs, **kwargs)\n        if state_dict is None and not from_tf:\n            weights_path = os.path.join(serialization_dir, WEIGHTS_NAME)\n            state_dict = torch.load(\n                weights_path,\n                map_location=\'cpu\' if not torch.cuda.is_available() else None)\n        if tempdir:\n            # Clean up temp dir\n            shutil.rmtree(tempdir)\n        if from_tf:\n            # Directly load from a TensorFlow checkpoint\n            weights_path = os.path.join(serialization_dir, TF_WEIGHTS_NAME)\n            return load_tf_weights_in_bert(model, weights_path)\n        # Load from a PyTorch state_dict\n        old_keys = []\n        new_keys = []\n        for key in state_dict.keys():\n            new_key = None\n            if \'gamma\' in key:\n                new_key = key.replace(\'gamma\', \'weight\')\n            if \'beta\' in key:\n                new_key = key.replace(\'beta\', \'bias\')\n            if new_key:\n                old_keys.append(key)\n                new_keys.append(new_key)\n        for old_key, new_key in zip(old_keys, new_keys):\n            state_dict[new_key] = state_dict.pop(old_key)\n\n        missing_keys = []\n        unexpected_keys = []\n        error_msgs = []\n        # copy state_dict so _load_from_state_dict can modify it\n        metadata = getattr(state_dict, \'_metadata\', None)\n        state_dict = state_dict.copy()\n        if metadata is not None:\n            state_dict._metadata = metadata\n\n        def load(module, prefix=\'\'):\n            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n            module._load_from_state_dict(state_dict,\n                                         prefix,\n                                         local_metadata,\n                                         True,\n                                         missing_keys,\n                                         unexpected_keys,\n                                         error_msgs)\n            for name, child in module._modules.items():\n                if child is not None:\n                    load(child, prefix + name + \'.\')\n\n        start_prefix = \'\'\n        if not hasattr(model,\n                       \'bert\') and any(s.startswith(\'bert.\') for s in state_dict.keys()):\n            start_prefix = \'bert.\'\n        load(model, prefix=start_prefix)\n        if len(missing_keys) > 0:\n            logger.info(""Weights of {} not initialized from pretrained model: {}"".format(\n                model.__class__.__name__,\n                missing_keys))\n        if len(unexpected_keys) > 0:\n            logger.info(""Weights from pretrained model not used in {}: {}"".format(\n                model.__class__.__name__,\n                unexpected_keys))\n        if len(error_msgs) > 0:\n            raise RuntimeError(\'Error(s) in loading state_dict for {}:\\n\\t{}\'.format(\n                model.__class__.__name__,\n                ""\\n\\t"".join(error_msgs)))\n        return model\n\n\nclass BertModel(BertPreTrainedModel):\n    """"""BERT model (""Bidirectional Embedding Representations from a Transformer"").\n\n    Params:\n        config: a BertConfig class instance with the configuration to build a new model\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n\n    Outputs: Tuple of (encoded_layers, pooled_output)\n        `encoded_layers`: controled by `output_all_encoded_layers` argument:\n            - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n                of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n                encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n            - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n                to the last attention block of shape [batch_size, sequence_length, hidden_size],\n        `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n            classifier pretrained on top of the hidden state associated to the first character of the\n            input (`CLS`) to train on the Next-Sentence task (see BERT\'s paper).\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = modeling.BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    model = modeling.BertModel(config=config)\n    all_encoder_layers, pooled_output = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config):\n        super(BertModel, self).__init__(config)\n        self.embeddings = BertEmbeddings(config)\n        self.encoder = BertEncoder(config)\n        self.pooler = BertPooler(config)\n        self.apply(self.init_bert_weights)\n\n    def forward(self,\n                input_ids,\n                token_type_ids=None,\n                attention_mask=None,\n                output_all_encoded_layers=True,\n                checkpoint_activations=False):\n        if attention_mask is None:\n            attention_mask = torch.ones_like(input_ids)\n        if token_type_ids is None:\n            token_type_ids = torch.zeros_like(input_ids)\n\n        # We create a 3D attention mask from a 2D tensor mask.\n        # Sizes are [batch_size, 1, 1, to_seq_length]\n        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n        # this attention mask is more simple than the triangular masking of causal attention\n        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n        # masked positions, this operation will create a tensor which is 0.0 for\n        # positions we want to attend and -10000.0 for masked positions.\n        # Since we are adding it to the raw scores before the softmax, this is\n        # effectively the same as removing these entirely.\n        extended_attention_mask = extended_attention_mask.to(dtype=next(\n            self.parameters()).dtype)  # fp16 compatibility\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n\n        embedding_output = self.embeddings(input_ids, token_type_ids)\n        encoded_layers = self.encoder(\n            embedding_output,\n            extended_attention_mask,\n            output_all_encoded_layers=output_all_encoded_layers,\n            checkpoint_activations=checkpoint_activations)\n        sequence_output = encoded_layers[-1]\n        pooled_output = self.pooler(sequence_output)\n        if not output_all_encoded_layers:\n            encoded_layers = encoded_layers[-1]\n        return encoded_layers, pooled_output\n\n\nclass BertForPreTraining(BertPreTrainedModel):\n    """"""BERT model with pre-training heads.\n    This module comprises the BERT model followed by the two pre-training heads:\n        - the masked language modeling head, and\n        - the next sentence classification head.\n\n    Params:\n        config: a BertConfig class instance with the configuration to build a new model.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `masked_lm_labels`: optional masked language modeling labels: torch.LongTensor of shape [batch_size, sequence_length]\n            with indices selected in [-1, 0, ..., vocab_size]. All labels set to -1 are ignored (masked), the loss\n            is only computed for the labels set in [0, ..., vocab_size]\n        `next_sentence_label`: optional next sentence classification loss: torch.LongTensor of shape [batch_size]\n            with indices selected in [0, 1].\n            0 => next sentence is the continuation, 1 => next sentence is a random sentence.\n\n    Outputs:\n        if `masked_lm_labels` and `next_sentence_label` are not `None`:\n            Outputs the total_loss which is the sum of the masked language modeling loss and the next\n            sentence classification loss.\n        if `masked_lm_labels` or `next_sentence_label` is `None`:\n            Outputs a tuple comprising\n            - the masked language modeling logits of shape [batch_size, sequence_length, vocab_size], and\n            - the next sentence classification logits of shape [batch_size, 2].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    model = BertForPreTraining(config)\n    masked_lm_logits_scores, seq_relationship_logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config, args):\n        super(BertForPreTraining, self).__init__(config)\n        self.summary_writer = None\n        if dist.get_rank() == 0:\n            self.summary_writer = args.summary_writer\n        self.samples_per_step = dist.get_world_size() * args.train_batch_size\n        self.sample_count = self.samples_per_step\n        self.bert = BertModel(config)\n        self.cls = BertPreTrainingHeads(config,\n                                        self.bert.embeddings.word_embeddings.weight)\n        self.apply(self.init_bert_weights)\n\n    def log_summary_writer(self, logs: dict, base=\'Train\'):\n        if dist.get_rank() == 0:\n            module_name = ""Samples""  #self._batch_module_name.get(batch_type, self._get_batch_type_error(batch_type))\n            for key, log in logs.items():\n                self.summary_writer.add_scalar(f\'{base}/{module_name}/{key}\',\n                                               log,\n                                               self.sample_count)\n            self.sample_count += self.samples_per_step\n\n    def forward(self, batch, log=True):\n        #input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None, next_sentence_label=None, checkpoint_activations=False):\n        input_ids = batch[1]\n        token_type_ids = batch[3]\n        attention_mask = batch[2]\n        masked_lm_labels = batch[5]\n        next_sentence_label = batch[4]\n        checkpoint_activations = False\n\n        sequence_output, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n                                                   output_all_encoded_layers=False, checkpoint_activations=checkpoint_activations)\n        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n\n        if masked_lm_labels is not None and next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            masked_lm_loss = loss_fct(prediction_scores.view(-1,\n                                                             self.config.vocab_size),\n                                      masked_lm_labels.view(-1))\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1,\n                                                                      2),\n                                          next_sentence_label.view(-1))\n            #print(""loss is {} {}"".format(masked_lm_loss, next_sentence_loss))\n            total_loss = masked_lm_loss + next_sentence_loss\n            #            if log:\n            #                self.log_summary_writer(logs={\'train_loss\': total_loss.item()})\n            return total_loss\n        else:\n            return prediction_scores, seq_relationship_score\n\n\nclass BertForMaskedLM(BertPreTrainedModel):\n    """"""BERT model with the masked language modeling head.\n    This module comprises the BERT model followed by the masked language modeling head.\n\n    Params:\n        config: a BertConfig class instance with the configuration to build a new model.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `masked_lm_labels`: masked language modeling labels: torch.LongTensor of shape [batch_size, sequence_length]\n            with indices selected in [-1, 0, ..., vocab_size]. All labels set to -1 are ignored (masked), the loss\n            is only computed for the labels set in [0, ..., vocab_size]\n\n    Outputs:\n        if `masked_lm_labels` is  not `None`:\n            Outputs the masked language modeling loss.\n        if `masked_lm_labels` is `None`:\n            Outputs the masked language modeling logits of shape [batch_size, sequence_length, vocab_size].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    model = BertForMaskedLM(config)\n    masked_lm_logits_scores = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config):\n        super(BertForMaskedLM, self).__init__(config)\n        self.bert = BertModel(config)\n        self.cls = BertOnlyMLMHead(config, self.bert.embeddings.word_embeddings.weight)\n        self.apply(self.init_bert_weights)\n\n    def forward(self,\n                input_ids,\n                token_type_ids=None,\n                attention_mask=None,\n                masked_lm_labels=None,\n                checkpoint_activations=False):\n        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask,\n                                       output_all_encoded_layers=False)\n        prediction_scores = self.cls(sequence_output)\n\n        if masked_lm_labels is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            masked_lm_loss = loss_fct(prediction_scores.view(-1,\n                                                             self.config.vocab_size),\n                                      masked_lm_labels.view(-1))\n            return masked_lm_loss\n        else:\n            return prediction_scores\n\n\nclass BertForNextSentencePrediction(BertPreTrainedModel):\n    """"""BERT model with next sentence prediction head.\n    This module comprises the BERT model followed by the next sentence classification head.\n\n    Params:\n        config: a BertConfig class instance with the configuration to build a new model.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `next_sentence_label`: next sentence classification loss: torch.LongTensor of shape [batch_size]\n            with indices selected in [0, 1].\n            0 => next sentence is the continuation, 1 => next sentence is a random sentence.\n\n    Outputs:\n        if `next_sentence_label` is not `None`:\n            Outputs the total_loss which is the sum of the masked language modeling loss and the next\n            sentence classification loss.\n        if `next_sentence_label` is `None`:\n            Outputs the next sentence classification logits of shape [batch_size, 2].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    model = BertForNextSentencePrediction(config)\n    seq_relationship_logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config):\n        super(BertForNextSentencePrediction, self).__init__(config)\n        self.bert = BertModel(config)\n        self.cls = BertOnlyNSPHead(config)\n        self.apply(self.init_bert_weights)\n\n    def forward(self,\n                input_ids,\n                token_type_ids=None,\n                attention_mask=None,\n                next_sentence_label=None,\n                checkpoint_activations=False):\n        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\n                                     output_all_encoded_layers=False)\n        seq_relationship_score = self.cls(pooled_output)\n\n        if next_sentence_label is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1,\n                                                                      2),\n                                          next_sentence_label.view(-1))\n            return next_sentence_loss\n        else:\n            return seq_relationship_score\n\n\nclass BertForSequenceClassification(BertPreTrainedModel):\n    """"""BERT model for classification.\n    This module is composed of the BERT model with a linear layer on top of\n    the pooled output.\n\n    Params:\n        `config`: a BertConfig class instance with the configuration to build a new model.\n        `num_labels`: the number of classes for the classifier. Default = 2.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n            with indices selected in [0, ..., num_labels].\n\n    Outputs:\n        if `labels` is not `None`:\n            Outputs the CrossEntropy classification loss of the output with the labels.\n        if `labels` is `None`:\n            Outputs the classification logits of shape [batch_size, num_labels].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    num_labels = 2\n\n    model = BertForSequenceClassification(config, num_labels)\n    logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config, num_labels):\n        super(BertForSequenceClassification, self).__init__(config)\n        self.num_labels = num_labels\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, num_labels)\n        self.apply(self.init_bert_weights)\n\n    def forward(self,\n                input_ids,\n                token_type_ids=None,\n                attention_mask=None,\n                labels=None,\n                checkpoint_activations=False):\n        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            return loss\n        else:\n            return logits\n\n\nclass BertForMultipleChoice(BertPreTrainedModel):\n    """"""BERT model for multiple choice tasks.\n    This module is composed of the BERT model with a linear layer on top of\n    the pooled output.\n\n    Params:\n        `config`: a BertConfig class instance with the configuration to build a new model.\n        `num_choices`: the number of classes for the classifier. Default = 2.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, num_choices, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, num_choices, sequence_length]\n            with the token types indices selected in [0, 1]. Type 0 corresponds to a `sentence A`\n            and type 1 corresponds to a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, num_choices, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n            with indices selected in [0, ..., num_choices].\n\n    Outputs:\n        if `labels` is not `None`:\n            Outputs the CrossEntropy classification loss of the output with the labels.\n        if `labels` is `None`:\n            Outputs the classification logits of shape [batch_size, num_labels].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[[31, 51, 99], [15, 5, 0]], [[12, 16, 42], [14, 28, 57]]])\n    input_mask = torch.LongTensor([[[1, 1, 1], [1, 1, 0]],[[1,1,0], [1, 0, 0]]])\n    token_type_ids = torch.LongTensor([[[0, 0, 1], [0, 1, 0]],[[0, 1, 1], [0, 0, 1]]])\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    num_choices = 2\n\n    model = BertForMultipleChoice(config, num_choices)\n    logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config, num_choices):\n        super(BertForMultipleChoice, self).__init__(config)\n        self.num_choices = num_choices\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, 1)\n        self.apply(self.init_bert_weights)\n\n    def forward(self,\n                input_ids,\n                token_type_ids=None,\n                attention_mask=None,\n                labels=None,\n                checkpoint_activations=False):\n        flat_input_ids = input_ids.view(-1, input_ids.size(-1))\n        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1))\n        _, pooled_output = self.bert(flat_input_ids, flat_token_type_ids, flat_attention_mask, output_all_encoded_layers=False)\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        reshaped_logits = logits.view(-1, self.num_choices)\n\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = loss_fct(reshaped_logits, labels)\n            return loss\n        else:\n            return reshaped_logits\n\n\nclass BertForTokenClassification(BertPreTrainedModel):\n    """"""BERT model for token-level classification.\n    This module is composed of the BERT model with a linear layer on top of\n    the full hidden state of the last layer.\n\n    Params:\n        `config`: a BertConfig class instance with the configuration to build a new model.\n        `num_labels`: the number of classes for the classifier. Default = 2.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size, sequence_length]\n            with indices selected in [0, ..., num_labels].\n\n    Outputs:\n        if `labels` is not `None`:\n            Outputs the CrossEntropy classification loss of the output with the labels.\n        if `labels` is `None`:\n            Outputs the classification logits of shape [batch_size, sequence_length, num_labels].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    num_labels = 2\n\n    model = BertForTokenClassification(config, num_labels)\n    logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config, num_labels):\n        super(BertForTokenClassification, self).__init__(config)\n        self.num_labels = num_labels\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, num_labels)\n        self.apply(self.init_bert_weights)\n\n    def forward(self,\n                input_ids,\n                token_type_ids=None,\n                attention_mask=None,\n                labels=None,\n                checkpoint_activations=False):\n        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n        sequence_output = self.dropout(sequence_output)\n        logits = self.classifier(sequence_output)\n\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            # Only keep active parts of the loss\n            if attention_mask is not None:\n                active_loss = attention_mask.view(-1) == 1\n                active_logits = logits.view(-1, self.num_labels)[active_loss]\n                active_labels = labels.view(-1)[active_loss]\n                loss = loss_fct(active_logits, active_labels)\n            else:\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            return loss\n        else:\n            return logits\n\n\nclass BertForQuestionAnswering(BertPreTrainedModel):\n    """"""BERT model for Question Answering (span extraction).\n    This module is composed of the BERT model with a linear layer on top of\n    the sequence output that computes start_logits and end_logits\n\n    Params:\n        `config`: a BertConfig class instance with the configuration to build a new model.\n\n    Inputs:\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n            a `sentence B` token (see BERT paper for more details).\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\n            a batch has varying length sentences.\n        `start_positions`: position of the first token for the labeled span: torch.LongTensor of shape [batch_size].\n            Positions are clamped to the length of the sequence and position outside of the sequence are not taken\n            into account for computing the loss.\n        `end_positions`: position of the last token for the labeled span: torch.LongTensor of shape [batch_size].\n            Positions are clamped to the length of the sequence and position outside of the sequence are not taken\n            into account for computing the loss.\n\n    Outputs:\n        if `start_positions` and `end_positions` are not `None`:\n            Outputs the total_loss which is the sum of the CrossEntropy loss for the start and end token positions.\n        if `start_positions` or `end_positions` is `None`:\n            Outputs a tuple of start_logits, end_logits which are the logits respectively for the start and end\n            position tokens of shape [batch_size, sequence_length].\n\n    Example usage:\n    ```python\n    # Already been converted into WordPiece token ids\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n\n    model = BertForQuestionAnswering(config)\n    start_logits, end_logits = model(input_ids, token_type_ids, input_mask)\n    ```\n    """"""\n    def __init__(self, config):\n        super(BertForQuestionAnswering, self).__init__(config)\n        self.bert = BertModel(config)\n        # TODO check with Google if it\'s normal there is no dropout on the token classifier of SQuAD in the TF version\n        # self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n        self.apply(self.init_bert_weights)\n\n    def forward(self,\n                input_ids,\n                token_type_ids=None,\n                attention_mask=None,\n                start_positions=None,\n                end_positions=None,\n                checkpoint_activations=False):\n        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n        logits = self.qa_outputs(sequence_output)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        if start_positions is not None and end_positions is not None:\n            # If we are on multi-GPU, split add a dimension\n            if len(start_positions.size()) > 1:\n                start_positions = start_positions.squeeze(-1)\n            if len(end_positions.size()) > 1:\n                end_positions = end_positions.squeeze(-1)\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n            ignored_index = start_logits.size(1)\n            start_positions.clamp_(0, ignored_index)\n            end_positions.clamp_(0, ignored_index)\n\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n            start_loss = loss_fct(start_logits, start_positions)\n            end_loss = loss_fct(end_logits, end_positions)\n            total_loss = (start_loss + end_loss) / 2\n            return total_loss\n        else:\n            return start_logits, end_logits\n'"
tests/unit/modelingpreln.py,93,"b'# DeepSpeed note, code taken from commit 3d59216cec89a363649b4fe3d15295ba936ced0f\r\n# https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/BERT/modeling.py\r\n\r\n# coding=utf-8\r\n# Copyright 2018 The Google AI Language Team Authors and The HugginFace Inc. team.\r\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\r\n#\r\n# Licensed under the Apache License, Version 2.0 (the ""License"");\r\n# you may not use this file except in compliance with the License.\r\n# You may obtain a copy of the License at\r\n#\r\n#     http://www.apache.org/licenses/LICENSE-2.0\r\n#\r\n# Unless required by applicable law or agreed to in writing, software\r\n# distributed under the License is distributed on an ""AS IS"" BASIS,\r\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n# See the License for the specific language governing permissions and\r\n# limitations under the License.\r\n""""""PyTorch BERT model.""""""\r\n\r\nfrom __future__ import absolute_import, division, print_function, unicode_literals\r\n\r\nimport copy\r\nimport json\r\nimport logging\r\nimport math\r\nimport os\r\nimport shutil\r\nimport tarfile\r\nimport tempfile\r\nimport sys\r\nfrom io import open\r\n\r\nimport torch\r\nfrom torch import nn\r\nfrom torch.nn import CrossEntropyLoss\r\nfrom torch.utils import checkpoint\r\nimport torch.distributed as dist\r\n\r\nfrom torch.nn import Module\r\nfrom torch.nn.parameter import Parameter\r\nimport torch.nn.functional as F\r\nimport torch.nn.init as init\r\nimport time\r\n\r\n#from numba import cuda\r\n\r\n#from deepspeed_cuda import DeepSpeedSoftmaxConfig, DeepSpeedSoftmax\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\nPRETRAINED_MODEL_ARCHIVE_MAP = {\r\n    \'bert-base-uncased\':\r\n    ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz"",\r\n    \'bert-large-uncased\':\r\n    ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased.tar.gz"",\r\n    \'bert-base-cased\':\r\n    ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz"",\r\n    \'bert-large-cased\':\r\n    ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased.tar.gz"",\r\n    \'bert-base-multilingual-uncased\':\r\n    ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased.tar.gz"",\r\n    \'bert-base-multilingual-cased\':\r\n    ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased.tar.gz"",\r\n    \'bert-base-chinese\':\r\n    ""https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz"",\r\n}\r\nCONFIG_NAME = \'bert_config.json\'\r\nWEIGHTS_NAME = \'pytorch_model.bin\'\r\nTF_WEIGHTS_NAME = \'model.ckpt\'\r\n\r\n\r\ndef load_tf_weights_in_bert(model, tf_checkpoint_path):\r\n    """""" Load tf checkpoints in a pytorch model\r\n    """"""\r\n    try:\r\n        import re\r\n        import numpy as np\r\n        import tensorflow as tf\r\n    except ImportError:\r\n        print(\r\n            ""Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see ""\r\n            ""https://www.tensorflow.org/install/ for installation instructions."")\r\n        raise\r\n    tf_path = os.path.abspath(tf_checkpoint_path)\r\n    print(""Converting TensorFlow checkpoint from {}"".format(tf_path))\r\n    # Load weights from TF model\r\n    init_vars = tf.train.list_variables(tf_path)\r\n    names = []\r\n    arrays = []\r\n    for name, shape in init_vars:\r\n        print(""Loading TF weight {} with shape {}"".format(name, shape))\r\n        array = tf.train.load_variable(tf_path, name)\r\n        names.append(name)\r\n        arrays.append(array)\r\n\r\n    for name, array in zip(names, arrays):\r\n        name = name.split(\'/\')\r\n        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\r\n        # which are not required for using pretrained model\r\n        if any(n in [""adam_v"", ""adam_m""] for n in name):\r\n            print(""Skipping {}"".format(""/"".join(name)))\r\n            continue\r\n        pointer = model\r\n        for m_name in name:\r\n            if re.fullmatch(r\'[A-Za-z]+_\\d+\', m_name):\r\n                l = re.split(r\'_(\\d+)\', m_name)\r\n            else:\r\n                l = [m_name]\r\n            if l[0] == \'kernel\' or l[0] == \'gamma\':\r\n                pointer = getattr(pointer, \'weight\')\r\n            elif l[0] == \'output_bias\' or l[0] == \'beta\':\r\n                pointer = getattr(pointer, \'bias\')\r\n            elif l[0] == \'output_weights\':\r\n                pointer = getattr(pointer, \'weight\')\r\n            else:\r\n                pointer = getattr(pointer, l[0])\r\n            if len(l) >= 2:\r\n                num = int(l[1])\r\n                pointer = pointer[num]\r\n        if m_name[-11:] == \'_embeddings\':\r\n            pointer = getattr(pointer, \'weight\')\r\n        elif m_name == \'kernel\':\r\n            array = np.transpose(array)\r\n        try:\r\n            assert pointer.shape == array.shape\r\n        except AssertionError as e:\r\n            e.args += (pointer.shape, array.shape)\r\n            raise\r\n        print(""Initialize PyTorch weight {}"".format(name))\r\n        pointer.data = torch.from_numpy(array)\r\n    return model\r\n\r\n\r\n@torch.jit.script\r\ndef f_gelu(x):\r\n    return x * 0.5 * (1.0 + torch.erf(x / 1.41421))\r\n\r\n\r\n@torch.jit.script\r\ndef bias_gelu(bias, y):\r\n    x = bias + y\r\n    return x * 0.5 * (1.0 + torch.erf(x / 1.41421))\r\n\r\n\r\n@torch.jit.script\r\ndef bias_tanh(bias, y):\r\n    x = bias + y\r\n    return torch.tanh(x)\r\n\r\n\r\ndef gelu(x):\r\n    """"""Implementation of the gelu activation function.\r\n        For information: OpenAI GPT\'s gelu is slightly different (and gives slightly different results):\r\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\r\n        Also see https://arxiv.org/abs/1606.08415\r\n    """"""\r\n    return f_gelu(x)\r\n\r\n\r\ndef swish(x):\r\n    return x * torch.sigmoid(x)\r\n\r\n\r\nACT2FN = {""gelu"": gelu, ""relu"": torch.nn.functional.relu, ""swish"": swish}\r\n\r\n\r\nclass GPUTimer:\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.start = cuda.event()\r\n        self.stop = cuda.event()\r\n\r\n    def record(self):\r\n        self.start.record()\r\n\r\n    def elapsed(self):\r\n        self.stop.record()\r\n        self.stop.synchronize()\r\n        return self.start.elapsed_time(self.stop) / 1000.0\r\n\r\n\r\nclass LinearActivation(Module):\r\n    r""""""Fused Linear and activation Module.\r\n    """"""\r\n    __constants__ = [\'bias\']\r\n\r\n    def __init__(self,\r\n                 in_features,\r\n                 out_features,\r\n                 weights,\r\n                 biases,\r\n                 act=\'gelu\',\r\n                 bias=True):\r\n        super(LinearActivation, self).__init__()\r\n        self.in_features = in_features\r\n        self.out_features = out_features\r\n        self.fused_gelu = False\r\n        self.fused_tanh = False\r\n        if isinstance(act,\r\n                      str) or (sys.version_info[0] == 2 and isinstance(act,\r\n                                                                       unicode)):\r\n            if bias and act == \'gelu\':\r\n                self.fused_gelu = True\r\n            elif bias and act == \'tanh\':\r\n                self.fused_tanh = True\r\n            else:\r\n                self.act_fn = ACT2FN[act]\r\n        else:\r\n            self.act_fn = act\r\n        #self.weight = Parameter(torch.Tensor(out_features, in_features))\r\n        self.weight = weights[5]\r\n        self.bias = biases[5]\r\n        #if bias:\r\n        #    self.bias = Parameter(torch.Tensor(out_features))\r\n        #else:\r\n        #    self.register_parameter(\'bias\', None)\r\n        #self.reset_parameters()\r\n\r\n    def reset_parameters(self):\r\n        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\r\n        if self.bias is not None:\r\n            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\r\n            bound = 1 / math.sqrt(fan_in)\r\n            init.uniform_(self.bias, -bound, bound)\r\n\r\n    def forward(self, input):\r\n        if self.fused_gelu:\r\n            #timing = []\r\n            #t1 = GPUTimer()\r\n            #t1.record()\r\n            y = F.linear(input, self.weight, None)\r\n            #timing.append(t1.elapsed())\r\n            #t1.record()\r\n            bg = bias_gelu(self.bias, y)\r\n            #timing.append(t1.elapsed())\r\n            return bg\r\n        elif self.fused_tanh:\r\n            return bias_tanh(self.bias, F.linear(input, self.weight, None))\r\n        else:\r\n            return self.act_fn(F.linear(input, self.weight, self.bias))\r\n\r\n    def extra_repr(self):\r\n        return \'in_features={}, out_features={}, bias={}\'.format(\r\n            self.in_features,\r\n            self.out_features,\r\n            self.bias is not None)\r\n\r\n\r\nclass BertConfig(object):\r\n    """"""Configuration class to store the configuration of a `BertModel`.\r\n    """"""\r\n    def __init__(self,\r\n                 vocab_size_or_config_json_file,\r\n                 hidden_size=768,\r\n                 num_hidden_layers=12,\r\n                 num_attention_heads=12,\r\n                 intermediate_size=3072,\r\n                 batch_size=8,\r\n                 hidden_act=""gelu"",\r\n                 hidden_dropout_prob=0.1,\r\n                 attention_probs_dropout_prob=0.1,\r\n                 max_position_embeddings=512,\r\n                 type_vocab_size=2,\r\n                 initializer_range=0.02,\r\n                 fp16=False):\r\n        """"""Constructs BertConfig.\r\n\r\n        Args:\r\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\r\n            hidden_size: Size of the encoder layers and the pooler layer.\r\n            num_hidden_layers: Number of hidden layers in the Transformer encoder.\r\n            num_attention_heads: Number of attention heads for each attention layer in\r\n                the Transformer encoder.\r\n            intermediate_size: The size of the ""intermediate"" (i.e., feed-forward)\r\n                layer in the Transformer encoder.\r\n            hidden_act: The non-linear activation function (function or string) in the\r\n                encoder and pooler. If string, ""gelu"", ""relu"" and ""swish"" are supported.\r\n            hidden_dropout_prob: The dropout probabilitiy for all fully connected\r\n                layers in the embeddings, encoder, and pooler.\r\n            attention_probs_dropout_prob: The dropout ratio for the attention\r\n                probabilities.\r\n            max_position_embeddings: The maximum sequence length that this model might\r\n                ever be used with. Typically set this to something large just in case\r\n                (e.g., 512 or 1024 or 2048).\r\n            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\r\n                `BertModel`.\r\n            initializer_range: The sttdev of the truncated_normal_initializer for\r\n                initializing all weight matrices.\r\n        """"""\r\n        if isinstance(vocab_size_or_config_json_file,\r\n                      str) or (sys.version_info[0] == 2\r\n                               and isinstance(vocab_size_or_config_json_file,\r\n                                              unicode)):\r\n            with open(vocab_size_or_config_json_file, ""r"", encoding=\'utf-8\') as reader:\r\n                json_config = json.loads(reader.read())\r\n            for key, value in json_config.items():\r\n                self.__dict__[key] = value\r\n        elif isinstance(vocab_size_or_config_json_file, int):\r\n            self.vocab_size = vocab_size_or_config_json_file\r\n            self.hidden_size = hidden_size\r\n            self.num_hidden_layers = num_hidden_layers\r\n            self.num_attention_heads = num_attention_heads\r\n            self.batch_size = batch_size\r\n            self.hidden_act = hidden_act\r\n            self.intermediate_size = intermediate_size\r\n            self.hidden_dropout_prob = hidden_dropout_prob\r\n            self.attention_probs_dropout_prob = attention_probs_dropout_prob\r\n            self.max_position_embeddings = max_position_embeddings\r\n            self.type_vocab_size = type_vocab_size\r\n            self.initializer_range = initializer_range\r\n            self.fp16 = fp16\r\n        else:\r\n            raise ValueError(""First argument must be either a vocabulary size (int)""\r\n                             ""or the path to a pretrained model config file (str)"")\r\n\r\n    @classmethod\r\n    def from_dict(cls, json_object):\r\n        """"""Constructs a `BertConfig` from a Python dictionary of parameters.""""""\r\n        config = BertConfig(vocab_size_or_config_json_file=-1)\r\n        for key, value in json_object.items():\r\n            config.__dict__[key] = value\r\n        return config\r\n\r\n    @classmethod\r\n    def from_json_file(cls, json_file):\r\n        """"""Constructs a `BertConfig` from a json file of parameters.""""""\r\n        with open(json_file, ""r"", encoding=\'utf-8\') as reader:\r\n            text = reader.read()\r\n        return cls.from_dict(json.loads(text))\r\n\r\n    def __repr__(self):\r\n        return str(self.to_json_string())\r\n\r\n    def to_dict(self):\r\n        """"""Serializes this instance to a Python dictionary.""""""\r\n        output = copy.deepcopy(self.__dict__)\r\n        return output\r\n\r\n    def to_json_string(self):\r\n        """"""Serializes this instance to a JSON string.""""""\r\n        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + ""\\n""\r\n\r\n\r\ntry:\r\n    import apex\r\n    #apex.amp.register_half_function(apex.normalization.fused_layer_norm, \'FusedLayerNorm\')\r\n    import apex.normalization\r\n    #apex.amp.register_float_function(apex.normalization.FusedLayerNorm, \'forward\')\r\n    BertLayerNorm = apex.normalization.FusedLayerNorm\r\nexcept ImportError:\r\n    print(\r\n        ""Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.""\r\n    )\r\n\r\n    class BertLayerNorm(nn.Module):\r\n        def __init__(self, hidden_size, eps=1e-12):\r\n            """"""Construct a layernorm module in the TF style (epsilon inside the square root).\r\n            """"""\r\n            super(BertLayerNorm, self).__init__()\r\n            self.weight = nn.Parameter(torch.ones(hidden_size))\r\n            self.bias = nn.Parameter(torch.zeros(hidden_size))\r\n            self.variance_epsilon = eps\r\n\r\n        def forward(self, x):\r\n            u = x.mean(-1, keepdim=True)\r\n            s = (x - u).pow(2).mean(-1, keepdim=True)\r\n            x = (x - u) / torch.sqrt(s + self.variance_epsilon)\r\n            return self.weight * x + self.bias\r\n\r\n\r\nclass BertEmbeddings(nn.Module):\r\n    """"""Construct the embeddings from word, position and token_type embeddings.\r\n    """"""\r\n    def __init__(self, config):\r\n        super(BertEmbeddings, self).__init__()\r\n        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\r\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings,\r\n                                                config.hidden_size)\r\n        self.token_type_embeddings = nn.Embedding(config.type_vocab_size,\r\n                                                  config.hidden_size)\r\n\r\n        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\r\n        # any TensorFlow checkpoint file\r\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\r\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\r\n\r\n    def forward(self, input_ids, token_type_ids=None):\r\n        seq_length = input_ids.size(1)\r\n        position_ids = torch.arange(seq_length,\r\n                                    dtype=torch.long,\r\n                                    device=input_ids.device)\r\n        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\r\n        if token_type_ids is None:\r\n            token_type_ids = torch.zeros_like(input_ids)\r\n\r\n        words_embeddings = self.word_embeddings(input_ids)\r\n        position_embeddings = self.position_embeddings(position_ids)\r\n        token_type_embeddings = self.token_type_embeddings(token_type_ids)\r\n\r\n        embeddings = words_embeddings + position_embeddings + token_type_embeddings\r\n        embeddings = self.LayerNorm(embeddings)\r\n        embeddings = self.dropout(embeddings)\r\n        return embeddings\r\n\r\n\r\nclass BertSelfAttention(nn.Module):\r\n    def __init__(self, i, config, weights, biases):\r\n        super(BertSelfAttention, self).__init__()\r\n        if config.hidden_size % config.num_attention_heads != 0:\r\n            raise ValueError(\r\n                ""The hidden size (%d) is not a multiple of the number of attention ""\r\n                ""heads (%d)"" % (config.hidden_size,\r\n                                config.num_attention_heads))\r\n        self.num_attention_heads = config.num_attention_heads\r\n        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\r\n        self.all_head_size = self.num_attention_heads * self.attention_head_size\r\n\r\n        self.query = nn.Linear(config.hidden_size, self.all_head_size)\r\n        self.query.weight = weights[0]\r\n        self.query.bias = biases[0]\r\n        self.key = nn.Linear(config.hidden_size, self.all_head_size)\r\n        self.key.weight = weights[1]\r\n        self.key.bias = biases[1]\r\n        self.value = nn.Linear(config.hidden_size, self.all_head_size)\r\n        self.value.weight = weights[2]\r\n        self.value.bias = biases[2]\r\n\r\n        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\r\n        self.softmax = nn.Softmax(dim=-1)\r\n        #self.softmax_config = DeepSpeedSoftmaxConfig()\r\n        #self.softmax_config.batch_size = config.batch_size\r\n        #self.softmax_config.max_seq_length = config.max_position_embeddings\r\n        #self.softmax_config.hidden_size = config.hidden_size\r\n        #self.softmax_config.heads = config.num_attention_heads\r\n        #self.softmax_config.softmax_id = i\r\n        #self.softmax_config.fp16 = config.fp16\r\n        #self.softmax_config.prob_drop_out = 0.0\r\n        #self.softmax = DeepSpeedSoftmax(i, self.softmax_config)\r\n\r\n    def transpose_for_scores(self, x):\r\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads,\r\n                                       self.attention_head_size)\r\n        x = x.view(*new_x_shape)\r\n        return x.permute(0, 2, 1, 3)\r\n\r\n    def transpose_key_for_scores(self, x):\r\n        new_x_shape = x.size()[:-1] + (self.num_attention_heads,\r\n                                       self.attention_head_size)\r\n        x = x.view(*new_x_shape)\r\n        return x.permute(0, 2, 3, 1)\r\n\r\n    def forward(self, hidden_states, attention_mask, grads=None):\r\n        #timing = []\r\n        #t1 = GPUTimer()\r\n        #t1.record()\r\n        mixed_query_layer = self.query(hidden_states)\r\n\r\n        #timing.append(t1.elapsed())\r\n        #print(""Query elapsed: %s"" % (time.clock() - start))\r\n        #t1.record()\r\n        mixed_key_layer = self.key(hidden_states)\r\n\r\n        #timing.append(t1.elapsed())\r\n        #print(""Key elapsed: %s"" % (time.clock() - start))\r\n        #t1.record()\r\n        mixed_value_layer = self.value(hidden_states)\r\n        #timing.append(t1.elapsed())\r\n        #print(""Value elapsed: %s"" % (time.clock() - start))\r\n\r\n        #t1.record()\r\n        query_layer = self.transpose_for_scores(mixed_query_layer)\r\n        # print(query_layer)\r\n        #timing.append(t1.elapsed())\r\n        #print(""Query-Transform elapsed: %s"" % (time.clock() - start))\r\n        #t1.record()\r\n        key_layer = self.transpose_key_for_scores(mixed_key_layer)\r\n        # print(key_layer)\r\n        #timing.append(t1.elapsed())\r\n        #print(""Key-Transform elapsed: %s"" % (time.clock() - start))\r\n        #t1.record()\r\n        value_layer = self.transpose_for_scores(mixed_value_layer)\r\n        #print(value_layer)\r\n        #timing.append(t1.elapsed())\r\n        #print(""Value-Transform elapsed: %s"" % (time.clock() - start))\r\n\r\n        # Take the dot product between ""query"" and ""key"" to get the raw attention scores.\r\n        #t1.record()\r\n        #print(query_layer.shape)\r\n        #print(key_layer.shape)\r\n        attention_scores = torch.matmul(query_layer, key_layer)\r\n        #print(attention_scores.shape)\r\n        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\r\n        #print(""Pytorch: "", attention_scores)\r\n        #timing.append(t1.elapsed())\r\n        #print(""Attention-Score elapsed: %s"" % (time.clock() - start))\r\n        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\r\n        #t1.record()\r\n\r\n        # context_layer = self.softmax(query_layer, key_layer, value_layer, attention_mask)\r\n        #print(""context shape is :"", context_layer.shape)\r\n        #print(""Cuda-ext:, "", attention_scores1)\r\n        # Normalize the attention scores to probabilities.\r\n        ####attention_probs = self.softmax(attention_scores)\r\n        #timing.append(t1.elapsed())\r\n        #print(""Softmax elapsed: %s"" % (time.clock() - start))\r\n        #t1 = GPUTimer()\r\n        #t1.record()\r\n        attention_scores = attention_scores + attention_mask\r\n        attention_probs = self.softmax(attention_scores)\r\n        #attention_scores = self.softmax(attention_scores, attention_mask)\r\n        #print(""Softmax elapse {0:8.2f} ms"", t1.elapsed() * 1000)\r\n\r\n        # This is actually dropping out entire tokens to attend to, which might\r\n        # seem a bit unusual, but is taken from the original Transformer paper.\r\n        attention_probs = self.dropout(attention_probs)\r\n\r\n        #t1.record()\r\n        context_layer = torch.matmul(attention_probs, value_layer)\r\n        #timing.append(t1.elapsed())\r\n        #print(""Context elapsed: %s"" % (time.clock() - start))\r\n        #t1.record()\r\n        #context_layer1 = context_layer.permute(\r\n        #                0, 1, 3, 2, 4).contiguous()\r\n        #if grads is not None:\r\n        # context_layer.register_hook(lambda x, self = self : grads.append([x, ""Context""]))\r\n        context_layer1 = context_layer.permute(0, 2, 1, 3).contiguous()\r\n        new_context_layer_shape = context_layer1.size()[:-2] + (self.all_head_size, )\r\n        context_layer1 = context_layer1.view(*new_context_layer_shape)\r\n        #timing.append(t1.elapsed())\r\n        #print(""Context-Transform elapsed: %s"" % (time.clock() - start))\r\n\r\n        if grads is not None:\r\n            query_layer.register_hook(lambda x, self=self: grads.append([x, ""Query""]))\r\n            key_layer.register_hook(lambda x, self=self: grads.append([x, ""Key""]))\r\n            value_layer.register_hook(lambda x, self=self: grads.append([x, ""Value""]))\r\n\r\n        return context_layer1\r\n\r\n\r\nclass BertSelfOutput(nn.Module):\r\n    def __init__(self, config, weights, biases):\r\n        super(BertSelfOutput, self).__init__()\r\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\r\n        self.dense.weight = weights[3]\r\n        self.dense.bias = biases[3]\r\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\r\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\r\n\r\n    def forward(self, hidden_states, input_tensor):\r\n        #timing = []\r\n        #t1 = GPUTimer()\r\n        #t1.record()\r\n        hidden_states = self.dense(hidden_states)\r\n        #timing.append(t1.elapsed())\r\n        #print(""Attention Output elapsed: %s"" % (time.clock() - start))\r\n        hidden_states = self.dropout(hidden_states)\r\n        #t1.record()\r\n        #hidden_states = self.LayerNorm(hidden_states + input_tensor)\r\n        #timing.append(t1.elapsed())\r\n        #print(""LayerNorm elapsed: %s"" % (time.clock() - start))\r\n        return hidden_states\r\n\r\n    def get_w(self):\r\n        return self.dense.weight\r\n\r\n\r\nclass BertAttention(nn.Module):\r\n    def __init__(self, i, config, weights, biases):\r\n        super(BertAttention, self).__init__()\r\n        self.self = BertSelfAttention(i, config, weights, biases)\r\n        self.output = BertSelfOutput(config, weights, biases)\r\n\r\n    def forward(self, input_tensor, attention_mask):\r\n        self_output = self.self(input_tensor, attention_mask)\r\n        attention_output = self.output(self_output, input_tensor)\r\n        return attention_output\r\n\r\n    def get_w(self):\r\n        return self.output.get_w()\r\n\r\n\r\nclass BertIntermediate(nn.Module):\r\n    def __init__(self, config, weights, biases):\r\n        super(BertIntermediate, self).__init__()\r\n        self.dense_act = LinearActivation(config.hidden_size,\r\n                                          config.intermediate_size,\r\n                                          weights,\r\n                                          biases,\r\n                                          act=config.hidden_act)\r\n\r\n    def forward(self, hidden_states):\r\n        hidden_states = self.dense_act(hidden_states)\r\n        return hidden_states\r\n\r\n\r\nclass BertOutput(nn.Module):\r\n    def __init__(self, config, weights, biases):\r\n        super(BertOutput, self).__init__()\r\n        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\r\n        self.dense.weight = weights[6]\r\n        self.dense.bias = biases[6]\r\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\r\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\r\n\r\n    def forward(self, hidden_states, input_tensor):\r\n        #timing = []\r\n        #t1 = GPUTimer()\r\n        #t1.record()\r\n        #print (hidden_states)\r\n        #print (self.dense.weight)\r\n        hidden_states = self.dense(hidden_states)\r\n        #timing.append(t1.elapsed())\r\n        #print(""FF2 elapsed: %s"" % (time.clock() - start))\r\n        hidden_states = self.dropout(hidden_states)\r\n        #t1.record()\r\n        #hidden_states = self.LayerNorm(hidden_states + input_tensor)\r\n        #timing.append(t1.elapsed())\r\n        #print(""LayerNorm elapsed: %s"" % (time.clock() - start))\r\n        return hidden_states\r\n\r\n\r\nclass BertLayer(nn.Module):\r\n    def __init__(self, i, config, weights, biases):\r\n        super(BertLayer, self).__init__()\r\n        self.attention = BertAttention(i, config, weights, biases)\r\n        self.PreAttentionLayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\r\n        self.PostAttentionLayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\r\n        self.intermediate = BertIntermediate(config, weights, biases)\r\n        self.output = BertOutput(config, weights, biases)\r\n        self.weight = weights\r\n        self.biases = biases\r\n\r\n    def forward(self, hidden_states, attention_mask, grads, collect_all_grads=False):\r\n        input_layer_norm = self.PreAttentionLayerNorm(hidden_states)\r\n        attention_output = self.attention(input_layer_norm, attention_mask)\r\n        #print (""hidden shape is :"", hidden_states.shape)\r\n        intermediate_input = hidden_states + attention_output\r\n\r\n        intermediate_layer_norm = self.PostAttentionLayerNorm(intermediate_input)\r\n        intermediate_output = self.intermediate(intermediate_layer_norm)\r\n        layer_output = self.output(intermediate_output, attention_output)\r\n\r\n        #attention_output = self.attention(hidden_states, attention_mask)\r\n        #intermediate_output = self.intermediate(attention_output)\r\n        #layer_output = self.output(intermediate_output, attention_output)\r\n\r\n        if collect_all_grads:\r\n            # self.weight[0].register_hook(lambda x, self=self: grads.append([x,""Q_W""]))\r\n            # self.biases[0].register_hook(lambda x, self=self: grads.append([x,""Q_B""]))\r\n            # self.weight[1].register_hook(lambda x, self=self: grads.append([x,""K_W""]))\r\n            # self.biases[1].register_hook(lambda x, self=self: grads.append([x,""K_B""]))\r\n            self.weight[2].register_hook(lambda x, self=self: grads.append([x, ""V_W""]))\r\n            self.biases[2].register_hook(lambda x, self=self: grads.append([x, ""V_B""]))\r\n            self.weight[3].register_hook(lambda x, self=self: grads.append([x, ""O_W""]))\r\n            self.biases[3].register_hook(lambda x, self=self: grads.append([x, ""O_B""]))\r\n            self.PostAttentionLayerNorm.weight.register_hook(\r\n                lambda x,\r\n                self=self: grads.append([x,\r\n                                         ""N2_W""]))\r\n            self.PostAttentionLayerNorm.bias.register_hook(\r\n                lambda x,\r\n                self=self: grads.append([x,\r\n                                         ""N2_B""]))\r\n            self.weight[5].register_hook(lambda x, self=self: grads.append([x, ""int_W""]))\r\n            self.biases[5].register_hook(lambda x, self=self: grads.append([x, ""int_B""]))\r\n            self.weight[6].register_hook(lambda x, self=self: grads.append([x, ""out_W""]))\r\n            self.biases[6].register_hook(lambda x, self=self: grads.append([x, ""out_B""]))\r\n            self.PreAttentionLayerNorm.weight.register_hook(\r\n                lambda x,\r\n                self=self: grads.append([x,\r\n                                         ""norm_W""]))\r\n            self.PreAttentionLayerNorm.bias.register_hook(\r\n                lambda x,\r\n                self=self: grads.append([x,\r\n                                         ""norm_B""]))\r\n\r\n        return layer_output + intermediate_input\r\n\r\n    def get_w(self):\r\n        return self.attention.get_w()\r\n\r\n\r\nclass BertEncoder(nn.Module):\r\n    def __init__(self, config, weights, biases):\r\n        super(BertEncoder, self).__init__()\r\n        #layer = BertLayer(config, weights, biases)\r\n        self.FinalLayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\r\n\r\n        self.layer = nn.ModuleList([\r\n            copy.deepcopy(BertLayer(i,\r\n                                    config,\r\n                                    weights,\r\n                                    biases)) for i in range(config.num_hidden_layers)\r\n        ])\r\n        self.grads = []\r\n        self.graph = []\r\n\r\n    def get_grads(self):\r\n        return self.grads\r\n\r\n    # def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True):\r\n    #     all_encoder_layers = []\r\n    #     for layer_module in self.layer:\r\n    #         hidden_states = layer_module(hidden_states, attention_mask)\r\n    #         if output_all_encoded_layers:\r\n    #             all_encoder_layers.append(hidden_states)\r\n    #     if not output_all_encoded_layers:\r\n    #         all_encoder_layers.append(hidden_states)\r\n    #     return all_encoder_layers\r\n\r\n    def get_modules(self, big_node, input):\r\n        for mdl in big_node.named_children():\r\n            graph.append(mdl)\r\n            get_modules(self, mdl, input)\r\n\r\n    def forward(self,\r\n                hidden_states,\r\n                attention_mask,\r\n                output_all_encoded_layers=True,\r\n                checkpoint_activations=False):\r\n        all_encoder_layers = []\r\n\r\n        def custom(start, end):\r\n            def custom_forward(*inputs):\r\n                layers = self.layer[start:end]\r\n                x_ = inputs[0]\r\n                for layer in layers:\r\n                    x_ = layer(x_, inputs[1])\r\n                return x_\r\n\r\n            return custom_forward\r\n\r\n        if checkpoint_activations:\r\n            l = 0\r\n            num_layers = len(self.layer)\r\n            chunk_length = math.ceil(math.sqrt(num_layers))\r\n            while l < num_layers:\r\n                hidden_states = checkpoint.checkpoint(custom(l,\r\n                                                             l + chunk_length),\r\n                                                      hidden_states,\r\n                                                      attention_mask * 1)\r\n                l += chunk_length\r\n            # decoder layers\r\n        else:\r\n            for i, layer_module in enumerate(self.layer):\r\n                hidden_states = layer_module(hidden_states,\r\n                                             attention_mask,\r\n                                             self.grads,\r\n                                             collect_all_grads=True)\r\n                hidden_states.register_hook(\r\n                    lambda x,\r\n                    i=i,\r\n                    self=self: self.grads.append([x,\r\n                                                  ""hidden_state""]))\r\n                #print(""pytorch weight is: "", layer_module.get_w())\r\n\r\n                if output_all_encoded_layers:\r\n                    all_encoder_layers.append((hidden_states))\r\n\r\n        if not output_all_encoded_layers or checkpoint_activations:\r\n            hidden_states = self.FinalLayerNorm(hidden_states)\r\n            all_encoder_layers.append((hidden_states))\r\n        return all_encoder_layers\r\n\r\n\r\n#class BertEncoder(nn.Module):\r\n#    def __init__(self, config):\r\n#        super(BertEncoder, self).__init__()\r\n#        layer = BertLayer(config)\r\n#        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)])\r\n#\r\n#    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True):\r\n#        all_encoder_layers = []\r\n#        for layer_module in self.layer:\r\n#            hidden_states = layer_module(hidden_states, attention_mask)\r\n#            if output_all_encoded_layers:\r\n#                all_encoder_layers.append(hidden_states)\r\n#        if not output_all_encoded_layers:\r\n#            all_encoder_layers.append(hidden_states)\r\n#        return all_encoder_layers\r\n\r\n\r\nclass BertPooler(nn.Module):\r\n    def __init__(self, config):\r\n        super(BertPooler, self).__init__()\r\n        self.dense_act = LinearActivation(config.hidden_size,\r\n                                          config.hidden_size,\r\n                                          act=""tanh"")\r\n\r\n    def forward(self, hidden_states):\r\n        # We ""pool"" the model by simply taking the hidden state corresponding\r\n        # to the first token.\r\n        first_token_tensor = hidden_states[:, 0]\r\n        pooled_output = self.dense_act(first_token_tensor)\r\n        return pooled_output\r\n\r\n\r\nclass BertPredictionHeadTransform(nn.Module):\r\n    def __init__(self, config):\r\n        super(BertPredictionHeadTransform, self).__init__()\r\n        self.dense_act = LinearActivation(config.hidden_size,\r\n                                          config.hidden_size,\r\n                                          act=config.hidden_act)\r\n        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\r\n\r\n    def forward(self, hidden_states):\r\n        hidden_states = self.dense_act(hidden_states)\r\n        hidden_states = self.LayerNorm(hidden_states)\r\n        return hidden_states\r\n\r\n\r\nclass BertLMPredictionHead(nn.Module):\r\n    def __init__(self, config, bert_model_embedding_weights):\r\n        super(BertLMPredictionHead, self).__init__()\r\n        self.transform = BertPredictionHeadTransform(config)\r\n\r\n        # The output weights are the same as the input embeddings, but there is\r\n        # an output-only bias for each token.\r\n        self.decoder = nn.Linear(bert_model_embedding_weights.size(1),\r\n                                 bert_model_embedding_weights.size(0),\r\n                                 bias=False)\r\n        self.decoder.weight = bert_model_embedding_weights\r\n        self.bias = nn.Parameter(torch.zeros(bert_model_embedding_weights.size(0)))\r\n\r\n    def forward(self, hidden_states):\r\n        hidden_states = self.transform(hidden_states)\r\n        torch.cuda.nvtx.range_push(\r\n            ""decoder input.size() = {}, weight.size() = {}"".format(\r\n                hidden_states.size(),\r\n                self.decoder.weight.size()))\r\n        hidden_states = self.decoder(hidden_states) + self.bias\r\n        torch.cuda.nvtx.range_pop()\r\n        return hidden_states\r\n\r\n\r\nclass BertOnlyMLMHead(nn.Module):\r\n    def __init__(self, config, bert_model_embedding_weights):\r\n        super(BertOnlyMLMHead, self).__init__()\r\n        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\r\n\r\n    def forward(self, sequence_output):\r\n        prediction_scores = self.predictions(sequence_output)\r\n        return prediction_scores\r\n\r\n\r\nclass BertOnlyNSPHead(nn.Module):\r\n    def __init__(self, config):\r\n        super(BertOnlyNSPHead, self).__init__()\r\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\r\n\r\n    def forward(self, pooled_output):\r\n        seq_relationship_score = self.seq_relationship(pooled_output)\r\n        return seq_relationship_score\r\n\r\n\r\nclass BertPreTrainingHeads(nn.Module):\r\n    def __init__(self, config, bert_model_embedding_weights):\r\n        super(BertPreTrainingHeads, self).__init__()\r\n        self.predictions = BertLMPredictionHead(config, bert_model_embedding_weights)\r\n        self.seq_relationship = nn.Linear(config.hidden_size, 2)\r\n\r\n    def forward(self, sequence_output, pooled_output):\r\n        prediction_scores = self.predictions(sequence_output)\r\n        seq_relationship_score = self.seq_relationship(pooled_output)\r\n        return prediction_scores, seq_relationship_score\r\n\r\n\r\nclass BertPreTrainedModel(nn.Module):\r\n    """""" An abstract class to handle weights initialization and\r\n        a simple interface for dowloading and loading pretrained models.\r\n    """"""\r\n    def __init__(self, config, *inputs, **kwargs):\r\n        super(BertPreTrainedModel, self).__init__()\r\n        if not isinstance(config, BertConfig):\r\n            raise ValueError(\r\n                ""Parameter config in `{}(config)` should be an instance of class `BertConfig`. ""\r\n                ""To create a model from a Google pretrained model use ""\r\n                ""`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`"".format(\r\n                    self.__class__.__name__,\r\n                    self.__class__.__name__))\r\n        self.config = config\r\n\r\n    def init_bert_weights(self, module):\r\n        """""" Initialize the weights.\r\n        """"""\r\n        if isinstance(module, (nn.Linear, nn.Embedding)):\r\n            # Slightly different from the TF version which uses truncated_normal for initialization\r\n            # cf https://github.com/pytorch/pytorch/pull/5617\r\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\r\n        elif isinstance(module, BertLayerNorm):\r\n            module.bias.data.zero_()\r\n            module.weight.data.fill_(1.0)\r\n        if isinstance(module, nn.Linear) and module.bias is not None:\r\n            module.bias.data.zero_()\r\n\r\n    @classmethod\r\n    def from_pretrained(cls,\r\n                        pretrained_model_name_or_path,\r\n                        state_dict=None,\r\n                        cache_dir=None,\r\n                        from_tf=False,\r\n                        *inputs,\r\n                        **kwargs):\r\n        """"""\r\n        Instantiate a BertPreTrainedModel from a pre-trained model file or a pytorch state dict.\r\n        Download and cache the pre-trained model file if needed.\r\n\r\n        Params:\r\n            pretrained_model_name_or_path: either:\r\n                - a str with the name of a pre-trained model to load selected in the list of:\r\n                    . `bert-base-uncased`\r\n                    . `bert-large-uncased`\r\n                    . `bert-base-cased`\r\n                    . `bert-large-cased`\r\n                    . `bert-base-multilingual-uncased`\r\n                    . `bert-base-multilingual-cased`\r\n                    . `bert-base-chinese`\r\n                - a path or url to a pretrained model archive containing:\r\n                    . `bert_config.json` a configuration file for the model\r\n                    . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\r\n                - a path or url to a pretrained model archive containing:\r\n                    . `bert_config.json` a configuration file for the model\r\n                    . `model.chkpt` a TensorFlow checkpoint\r\n            from_tf: should we load the weights from a locally saved TensorFlow checkpoint\r\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\r\n            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of Google pre-trained models\r\n            *inputs, **kwargs: additional input for the specific Bert class\r\n                (ex: num_labels for BertForSequenceClassification)\r\n        """"""\r\n        if pretrained_model_name_or_path in PRETRAINED_MODEL_ARCHIVE_MAP:\r\n            archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name_or_path]\r\n        else:\r\n            archive_file = pretrained_model_name_or_path\r\n        if resolved_archive_file == archive_file:\r\n            logger.info(""loading archive file {}"".format(archive_file))\r\n        else:\r\n            logger.info(""loading archive file {} from cache at {}"".format(\r\n                archive_file,\r\n                resolved_archive_file))\r\n        tempdir = None\r\n        if os.path.isdir(resolved_archive_file) or from_tf:\r\n            serialization_dir = resolved_archive_file\r\n        else:\r\n            # Extract archive to temp dir\r\n            tempdir = tempfile.mkdtemp()\r\n            logger.info(""extracting archive file {} to temp dir {}"".format(\r\n                resolved_archive_file,\r\n                tempdir))\r\n            with tarfile.open(resolved_archive_file, \'r:gz\') as archive:\r\n                archive.extractall(tempdir)\r\n            serialization_dir = tempdir\r\n        # Load config\r\n        config_file = os.path.join(serialization_dir, CONFIG_NAME)\r\n        config = BertConfig.from_json_file(config_file)\r\n        logger.info(""Model config {}"".format(config))\r\n        # Instantiate model.\r\n        model = cls(config, *inputs, **kwargs)\r\n        if state_dict is None and not from_tf:\r\n            weights_path = os.path.join(serialization_dir, WEIGHTS_NAME)\r\n            state_dict = torch.load(\r\n                weights_path,\r\n                map_location=\'cpu\' if not torch.cuda.is_available() else None)\r\n        if tempdir:\r\n            # Clean up temp dir\r\n            shutil.rmtree(tempdir)\r\n        if from_tf:\r\n            # Directly load from a TensorFlow checkpoint\r\n            weights_path = os.path.join(serialization_dir, TF_WEIGHTS_NAME)\r\n            return load_tf_weights_in_bert(model, weights_path)\r\n        # Load from a PyTorch state_dict\r\n        old_keys = []\r\n        new_keys = []\r\n        for key in state_dict.keys():\r\n            new_key = None\r\n            if \'gamma\' in key:\r\n                new_key = key.replace(\'gamma\', \'weight\')\r\n            if \'beta\' in key:\r\n                new_key = key.replace(\'beta\', \'bias\')\r\n            if new_key:\r\n                old_keys.append(key)\r\n                new_keys.append(new_key)\r\n        for old_key, new_key in zip(old_keys, new_keys):\r\n            state_dict[new_key] = state_dict.pop(old_key)\r\n\r\n        missing_keys = []\r\n        unexpected_keys = []\r\n        error_msgs = []\r\n        # copy state_dict so _load_from_state_dict can modify it\r\n        metadata = getattr(state_dict, \'_metadata\', None)\r\n        state_dict = state_dict.copy()\r\n        if metadata is not None:\r\n            state_dict._metadata = metadata\r\n\r\n        def load(module, prefix=\'\'):\r\n            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\r\n            module._load_from_state_dict(state_dict,\r\n                                         prefix,\r\n                                         local_metadata,\r\n                                         True,\r\n                                         missing_keys,\r\n                                         unexpected_keys,\r\n                                         error_msgs)\r\n            for name, child in module._modules.items():\r\n                if child is not None:\r\n                    load(child, prefix + name + \'.\')\r\n\r\n        start_prefix = \'\'\r\n        if not hasattr(model,\r\n                       \'bert\') and any(s.startswith(\'bert.\') for s in state_dict.keys()):\r\n            start_prefix = \'bert.\'\r\n        load(model, prefix=start_prefix)\r\n        if len(missing_keys) > 0:\r\n            logger.info(""Weights of {} not initialized from pretrained model: {}"".format(\r\n                model.__class__.__name__,\r\n                missing_keys))\r\n        if len(unexpected_keys) > 0:\r\n            logger.info(""Weights from pretrained model not used in {}: {}"".format(\r\n                model.__class__.__name__,\r\n                unexpected_keys))\r\n        if len(error_msgs) > 0:\r\n            raise RuntimeError(\'Error(s) in loading state_dict for {}:\\n\\t{}\'.format(\r\n                model.__class__.__name__,\r\n                ""\\n\\t"".join(error_msgs)))\r\n        return model\r\n\r\n\r\nclass BertModel(BertPreTrainedModel):\r\n    """"""BERT model (""Bidirectional Embedding Representations from a Transformer"").\r\n\r\n    Params:\r\n        config: a BertConfig class instance with the configuration to build a new model\r\n\r\n    Inputs:\r\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\r\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\r\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\r\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\r\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\r\n            a `sentence B` token (see BERT paper for more details).\r\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\r\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\r\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\r\n            a batch has varying length sentences.\r\n        `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\r\n\r\n    Outputs: Tuple of (encoded_layers, pooled_output)\r\n        `encoded_layers`: controled by `output_all_encoded_layers` argument:\r\n            - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\r\n                of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\r\n                encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\r\n            - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\r\n                to the last attention block of shape [batch_size, sequence_length, hidden_size],\r\n        `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\r\n            classifier pretrained on top of the hidden state associated to the first character of the\r\n            input (`CLS`) to train on the Next-Sentence task (see BERT\'s paper).\r\n\r\n    Example usage:\r\n    ```python\r\n    # Already been converted into WordPiece token ids\r\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\r\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\r\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\r\n\r\n    config = modeling.BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\r\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\r\n\r\n    model = modeling.BertModel(config=config)\r\n    all_encoder_layers, pooled_output = model(input_ids, token_type_ids, input_mask)\r\n    ```\r\n    """"""\r\n    def __init__(self, config):\r\n        super(BertModel, self).__init__(config)\r\n        self.embeddings = BertEmbeddings(config)\r\n        self.encoder = BertEncoder(config)\r\n        self.pooler = BertPooler(config)\r\n        self.apply(self.init_bert_weights)\r\n\r\n    def forward(self,\r\n                input_ids,\r\n                token_type_ids=None,\r\n                attention_mask=None,\r\n                output_all_encoded_layers=True,\r\n                checkpoint_activations=False):\r\n        if attention_mask is None:\r\n            attention_mask = torch.ones_like(input_ids)\r\n        if token_type_ids is None:\r\n            token_type_ids = torch.zeros_like(input_ids)\r\n\r\n        # We create a 3D attention mask from a 2D tensor mask.\r\n        # Sizes are [batch_size, 1, 1, to_seq_length]\r\n        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\r\n        # this attention mask is more simple than the triangular masking of causal attention\r\n        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\r\n        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\r\n\r\n        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\r\n        # masked positions, this operation will create a tensor which is 0.0 for\r\n        # positions we want to attend and -10000.0 for masked positions.\r\n        # Since we are adding it to the raw scores before the softmax, this is\r\n        # effectively the same as removing these entirely.\r\n        extended_attention_mask = extended_attention_mask.to(dtype=next(\r\n            self.parameters()).dtype)  # fp16 compatibility\r\n        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\r\n\r\n        embedding_output = self.embeddings(input_ids, token_type_ids)\r\n        encoded_layers = self.encoder(\r\n            embedding_output,\r\n            extended_attention_mask,\r\n            output_all_encoded_layers=output_all_encoded_layers,\r\n            checkpoint_activations=checkpoint_activations)\r\n        sequence_output = encoded_layers[-1]\r\n        pooled_output = self.pooler(sequence_output)\r\n        if not output_all_encoded_layers:\r\n            encoded_layers = encoded_layers[-1]\r\n        return encoded_layers, pooled_output\r\n\r\n\r\nclass BertForPreTraining(BertPreTrainedModel):\r\n    """"""BERT model with pre-training heads.\r\n    This module comprises the BERT model followed by the two pre-training heads:\r\n        - the masked language modeling head, and\r\n        - the next sentence classification head.\r\n\r\n    Params:\r\n        config: a BertConfig class instance with the configuration to build a new model.\r\n\r\n    Inputs:\r\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\r\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\r\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\r\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\r\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\r\n            a `sentence B` token (see BERT paper for more details).\r\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\r\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\r\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\r\n            a batch has varying length sentences.\r\n        `masked_lm_labels`: optional masked language modeling labels: torch.LongTensor of shape [batch_size, sequence_length]\r\n            with indices selected in [-1, 0, ..., vocab_size]. All labels set to -1 are ignored (masked), the loss\r\n            is only computed for the labels set in [0, ..., vocab_size]\r\n        `next_sentence_label`: optional next sentence classification loss: torch.LongTensor of shape [batch_size]\r\n            with indices selected in [0, 1].\r\n            0 => next sentence is the continuation, 1 => next sentence is a random sentence.\r\n\r\n    Outputs:\r\n        if `masked_lm_labels` and `next_sentence_label` are not `None`:\r\n            Outputs the total_loss which is the sum of the masked language modeling loss and the next\r\n            sentence classification loss.\r\n        if `masked_lm_labels` or `next_sentence_label` is `None`:\r\n            Outputs a tuple comprising\r\n            - the masked language modeling logits of shape [batch_size, sequence_length, vocab_size], and\r\n            - the next sentence classification logits of shape [batch_size, 2].\r\n\r\n    Example usage:\r\n    ```python\r\n    # Already been converted into WordPiece token ids\r\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\r\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\r\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\r\n\r\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\r\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\r\n\r\n    model = BertForPreTraining(config)\r\n    masked_lm_logits_scores, seq_relationship_logits = model(input_ids, token_type_ids, input_mask)\r\n    ```\r\n    """"""\r\n    def __init__(self, config, args):\r\n        super(BertForPreTraining, self).__init__(config)\r\n        self.summary_writer = None\r\n        if dist.get_rank() == 0:\r\n            self.summary_writer = args.summary_writer\r\n        self.samples_per_step = dist.get_world_size() * args.train_batch_size\r\n        self.sample_count = self.samples_per_step\r\n        self.bert = BertModel(config)\r\n        self.cls = BertPreTrainingHeads(config,\r\n                                        self.bert.embeddings.word_embeddings.weight)\r\n        self.apply(self.init_bert_weights)\r\n\r\n    def log_summary_writer(self, logs: dict, base=\'Train\'):\r\n        if dist.get_rank() == 0:\r\n            module_name = ""Samples""  #self._batch_module_name.get(batch_type, self._get_batch_type_error(batch_type))\r\n            for key, log in logs.items():\r\n                self.summary_writer.add_scalar(f\'{base}/{module_name}/{key}\',\r\n                                               log,\r\n                                               self.sample_count)\r\n            self.sample_count += self.samples_per_step\r\n\r\n    def forward(self, batch, log=True):\r\n        #input_ids, token_type_ids=None, attention_mask=None, masked_lm_labels=None, next_sentence_label=None, checkpoint_activations=False):\r\n        input_ids = batch[1]\r\n        token_type_ids = batch[3]\r\n        attention_mask = batch[2]\r\n        masked_lm_labels = batch[5]\r\n        next_sentence_label = batch[4]\r\n        checkpoint_activations = False\r\n\r\n        sequence_output, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\r\n                                                   output_all_encoded_layers=False, checkpoint_activations=checkpoint_activations)\r\n        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\r\n\r\n        if masked_lm_labels is not None and next_sentence_label is not None:\r\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\r\n            masked_lm_loss = loss_fct(prediction_scores.view(-1,\r\n                                                             self.config.vocab_size),\r\n                                      masked_lm_labels.view(-1))\r\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1,\r\n                                                                      2),\r\n                                          next_sentence_label.view(-1))\r\n            #print(""loss is {} {}"".format(masked_lm_loss, next_sentence_loss))\r\n            total_loss = masked_lm_loss + next_sentence_loss\r\n            #            if log:\r\n            #                self.log_summary_writer(logs={\'train_loss\': total_loss.item()})\r\n            return total_loss\r\n        else:\r\n            return prediction_scores, seq_relationship_score\r\n\r\n\r\nclass BertForMaskedLM(BertPreTrainedModel):\r\n    """"""BERT model with the masked language modeling head.\r\n    This module comprises the BERT model followed by the masked language modeling head.\r\n\r\n    Params:\r\n        config: a BertConfig class instance with the configuration to build a new model.\r\n\r\n    Inputs:\r\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\r\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\r\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\r\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\r\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\r\n            a `sentence B` token (see BERT paper for more details).\r\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\r\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\r\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\r\n            a batch has varying length sentences.\r\n        `masked_lm_labels`: masked language modeling labels: torch.LongTensor of shape [batch_size, sequence_length]\r\n            with indices selected in [-1, 0, ..., vocab_size]. All labels set to -1 are ignored (masked), the loss\r\n            is only computed for the labels set in [0, ..., vocab_size]\r\n\r\n    Outputs:\r\n        if `masked_lm_labels` is  not `None`:\r\n            Outputs the masked language modeling loss.\r\n        if `masked_lm_labels` is `None`:\r\n            Outputs the masked language modeling logits of shape [batch_size, sequence_length, vocab_size].\r\n\r\n    Example usage:\r\n    ```python\r\n    # Already been converted into WordPiece token ids\r\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\r\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\r\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\r\n\r\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\r\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\r\n\r\n    model = BertForMaskedLM(config)\r\n    masked_lm_logits_scores = model(input_ids, token_type_ids, input_mask)\r\n    ```\r\n    """"""\r\n    def __init__(self, config):\r\n        super(BertForMaskedLM, self).__init__(config)\r\n        self.bert = BertModel(config)\r\n        self.cls = BertOnlyMLMHead(config, self.bert.embeddings.word_embeddings.weight)\r\n        self.apply(self.init_bert_weights)\r\n\r\n    def forward(self,\r\n                input_ids,\r\n                token_type_ids=None,\r\n                attention_mask=None,\r\n                masked_lm_labels=None,\r\n                checkpoint_activations=False):\r\n        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask,\r\n                                       output_all_encoded_layers=False)\r\n        prediction_scores = self.cls(sequence_output)\r\n\r\n        if masked_lm_labels is not None:\r\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\r\n            masked_lm_loss = loss_fct(prediction_scores.view(-1,\r\n                                                             self.config.vocab_size),\r\n                                      masked_lm_labels.view(-1))\r\n            return masked_lm_loss\r\n        else:\r\n            return prediction_scores\r\n\r\n\r\nclass BertForNextSentencePrediction(BertPreTrainedModel):\r\n    """"""BERT model with next sentence prediction head.\r\n    This module comprises the BERT model followed by the next sentence classification head.\r\n\r\n    Params:\r\n        config: a BertConfig class instance with the configuration to build a new model.\r\n\r\n    Inputs:\r\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\r\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\r\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\r\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\r\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\r\n            a `sentence B` token (see BERT paper for more details).\r\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\r\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\r\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\r\n            a batch has varying length sentences.\r\n        `next_sentence_label`: next sentence classification loss: torch.LongTensor of shape [batch_size]\r\n            with indices selected in [0, 1].\r\n            0 => next sentence is the continuation, 1 => next sentence is a random sentence.\r\n\r\n    Outputs:\r\n        if `next_sentence_label` is not `None`:\r\n            Outputs the total_loss which is the sum of the masked language modeling loss and the next\r\n            sentence classification loss.\r\n        if `next_sentence_label` is `None`:\r\n            Outputs the next sentence classification logits of shape [batch_size, 2].\r\n\r\n    Example usage:\r\n    ```python\r\n    # Already been converted into WordPiece token ids\r\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\r\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\r\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\r\n\r\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\r\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\r\n\r\n    model = BertForNextSentencePrediction(config)\r\n    seq_relationship_logits = model(input_ids, token_type_ids, input_mask)\r\n    ```\r\n    """"""\r\n    def __init__(self, config):\r\n        super(BertForNextSentencePrediction, self).__init__(config)\r\n        self.bert = BertModel(config)\r\n        self.cls = BertOnlyNSPHead(config)\r\n        self.apply(self.init_bert_weights)\r\n\r\n    def forward(self,\r\n                input_ids,\r\n                token_type_ids=None,\r\n                attention_mask=None,\r\n                next_sentence_label=None,\r\n                checkpoint_activations=False):\r\n        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,\r\n                                     output_all_encoded_layers=False)\r\n        seq_relationship_score = self.cls(pooled_output)\r\n\r\n        if next_sentence_label is not None:\r\n            loss_fct = CrossEntropyLoss(ignore_index=-1)\r\n            next_sentence_loss = loss_fct(seq_relationship_score.view(-1,\r\n                                                                      2),\r\n                                          next_sentence_label.view(-1))\r\n            return next_sentence_loss\r\n        else:\r\n            return seq_relationship_score\r\n\r\n\r\nclass BertForSequenceClassification(BertPreTrainedModel):\r\n    """"""BERT model for classification.\r\n    This module is composed of the BERT model with a linear layer on top of\r\n    the pooled output.\r\n\r\n    Params:\r\n        `config`: a BertConfig class instance with the configuration to build a new model.\r\n        `num_labels`: the number of classes for the classifier. Default = 2.\r\n\r\n    Inputs:\r\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\r\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\r\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\r\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\r\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\r\n            a `sentence B` token (see BERT paper for more details).\r\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\r\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\r\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\r\n            a batch has varying length sentences.\r\n        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\r\n            with indices selected in [0, ..., num_labels].\r\n\r\n    Outputs:\r\n        if `labels` is not `None`:\r\n            Outputs the CrossEntropy classification loss of the output with the labels.\r\n        if `labels` is `None`:\r\n            Outputs the classification logits of shape [batch_size, num_labels].\r\n\r\n    Example usage:\r\n    ```python\r\n    # Already been converted into WordPiece token ids\r\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\r\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\r\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\r\n\r\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\r\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\r\n\r\n    num_labels = 2\r\n\r\n    model = BertForSequenceClassification(config, num_labels)\r\n    logits = model(input_ids, token_type_ids, input_mask)\r\n    ```\r\n    """"""\r\n    def __init__(self, config, num_labels):\r\n        super(BertForSequenceClassification, self).__init__(config)\r\n        self.num_labels = num_labels\r\n        self.bert = BertModel(config)\r\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\r\n        self.classifier = nn.Linear(config.hidden_size, num_labels)\r\n        self.apply(self.init_bert_weights)\r\n\r\n    def forward(self,\r\n                input_ids,\r\n                token_type_ids=None,\r\n                attention_mask=None,\r\n                labels=None,\r\n                checkpoint_activations=False):\r\n        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\r\n        pooled_output = self.dropout(pooled_output)\r\n        logits = self.classifier(pooled_output)\r\n\r\n        if labels is not None:\r\n            loss_fct = CrossEntropyLoss()\r\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\r\n            return loss\r\n        else:\r\n            return logits\r\n\r\n\r\nclass BertForMultipleChoice(BertPreTrainedModel):\r\n    """"""BERT model for multiple choice tasks.\r\n    This module is composed of the BERT model with a linear layer on top of\r\n    the pooled output.\r\n\r\n    Params:\r\n        `config`: a BertConfig class instance with the configuration to build a new model.\r\n        `num_choices`: the number of classes for the classifier. Default = 2.\r\n\r\n    Inputs:\r\n        `input_ids`: a torch.LongTensor of shape [batch_size, num_choices, sequence_length]\r\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\r\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\r\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, num_choices, sequence_length]\r\n            with the token types indices selected in [0, 1]. Type 0 corresponds to a `sentence A`\r\n            and type 1 corresponds to a `sentence B` token (see BERT paper for more details).\r\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, num_choices, sequence_length] with indices\r\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\r\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\r\n            a batch has varying length sentences.\r\n        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\r\n            with indices selected in [0, ..., num_choices].\r\n\r\n    Outputs:\r\n        if `labels` is not `None`:\r\n            Outputs the CrossEntropy classification loss of the output with the labels.\r\n        if `labels` is `None`:\r\n            Outputs the classification logits of shape [batch_size, num_labels].\r\n\r\n    Example usage:\r\n    ```python\r\n    # Already been converted into WordPiece token ids\r\n    input_ids = torch.LongTensor([[[31, 51, 99], [15, 5, 0]], [[12, 16, 42], [14, 28, 57]]])\r\n    input_mask = torch.LongTensor([[[1, 1, 1], [1, 1, 0]],[[1,1,0], [1, 0, 0]]])\r\n    token_type_ids = torch.LongTensor([[[0, 0, 1], [0, 1, 0]],[[0, 1, 1], [0, 0, 1]]])\r\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\r\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\r\n\r\n    num_choices = 2\r\n\r\n    model = BertForMultipleChoice(config, num_choices)\r\n    logits = model(input_ids, token_type_ids, input_mask)\r\n    ```\r\n    """"""\r\n    def __init__(self, config, num_choices):\r\n        super(BertForMultipleChoice, self).__init__(config)\r\n        self.num_choices = num_choices\r\n        self.bert = BertModel(config)\r\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\r\n        self.classifier = nn.Linear(config.hidden_size, 1)\r\n        self.apply(self.init_bert_weights)\r\n\r\n    def forward(self,\r\n                input_ids,\r\n                token_type_ids=None,\r\n                attention_mask=None,\r\n                labels=None,\r\n                checkpoint_activations=False):\r\n        flat_input_ids = input_ids.view(-1, input_ids.size(-1))\r\n        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\r\n        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1))\r\n        _, pooled_output = self.bert(flat_input_ids, flat_token_type_ids, flat_attention_mask, output_all_encoded_layers=False)\r\n        pooled_output = self.dropout(pooled_output)\r\n        logits = self.classifier(pooled_output)\r\n        reshaped_logits = logits.view(-1, self.num_choices)\r\n\r\n        if labels is not None:\r\n            loss_fct = CrossEntropyLoss()\r\n            loss = loss_fct(reshaped_logits, labels)\r\n            return loss\r\n        else:\r\n            return reshaped_logits\r\n\r\n\r\nclass BertForTokenClassification(BertPreTrainedModel):\r\n    """"""BERT model for token-level classification.\r\n    This module is composed of the BERT model with a linear layer on top of\r\n    the full hidden state of the last layer.\r\n\r\n    Params:\r\n        `config`: a BertConfig class instance with the configuration to build a new model.\r\n        `num_labels`: the number of classes for the classifier. Default = 2.\r\n\r\n    Inputs:\r\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\r\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\r\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\r\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\r\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\r\n            a `sentence B` token (see BERT paper for more details).\r\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\r\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\r\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\r\n            a batch has varying length sentences.\r\n        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size, sequence_length]\r\n            with indices selected in [0, ..., num_labels].\r\n\r\n    Outputs:\r\n        if `labels` is not `None`:\r\n            Outputs the CrossEntropy classification loss of the output with the labels.\r\n        if `labels` is `None`:\r\n            Outputs the classification logits of shape [batch_size, sequence_length, num_labels].\r\n\r\n    Example usage:\r\n    ```python\r\n    # Already been converted into WordPiece token ids\r\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\r\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\r\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\r\n\r\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\r\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\r\n\r\n    num_labels = 2\r\n\r\n    model = BertForTokenClassification(config, num_labels)\r\n    logits = model(input_ids, token_type_ids, input_mask)\r\n    ```\r\n    """"""\r\n    def __init__(self, config, num_labels):\r\n        super(BertForTokenClassification, self).__init__(config)\r\n        self.num_labels = num_labels\r\n        self.bert = BertModel(config)\r\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\r\n        self.classifier = nn.Linear(config.hidden_size, num_labels)\r\n        self.apply(self.init_bert_weights)\r\n\r\n    def forward(self,\r\n                input_ids,\r\n                token_type_ids=None,\r\n                attention_mask=None,\r\n                labels=None,\r\n                checkpoint_activations=False):\r\n        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\r\n        sequence_output = self.dropout(sequence_output)\r\n        logits = self.classifier(sequence_output)\r\n\r\n        if labels is not None:\r\n            loss_fct = CrossEntropyLoss()\r\n            # Only keep active parts of the loss\r\n            if attention_mask is not None:\r\n                active_loss = attention_mask.view(-1) == 1\r\n                active_logits = logits.view(-1, self.num_labels)[active_loss]\r\n                active_labels = labels.view(-1)[active_loss]\r\n                loss = loss_fct(active_logits, active_labels)\r\n            else:\r\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\r\n            return loss\r\n        else:\r\n            return logits\r\n\r\n\r\nclass BertForQuestionAnswering(BertPreTrainedModel):\r\n    """"""BERT model for Question Answering (span extraction).\r\n    This module is composed of the BERT model with a linear layer on top of\r\n    the sequence output that computes start_logits and end_logits\r\n\r\n    Params:\r\n        `config`: a BertConfig class instance with the configuration to build a new model.\r\n\r\n    Inputs:\r\n        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\r\n            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\r\n            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\r\n        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\r\n            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\r\n            a `sentence B` token (see BERT paper for more details).\r\n        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\r\n            selected in [0, 1]. It\'s a mask to be used if the input sequence length is smaller than the max\r\n            input sequence length in the current batch. It\'s the mask that we typically use for attention when\r\n            a batch has varying length sentences.\r\n        `start_positions`: position of the first token for the labeled span: torch.LongTensor of shape [batch_size].\r\n            Positions are clamped to the length of the sequence and position outside of the sequence are not taken\r\n            into account for computing the loss.\r\n        `end_positions`: position of the last token for the labeled span: torch.LongTensor of shape [batch_size].\r\n            Positions are clamped to the length of the sequence and position outside of the sequence are not taken\r\n            into account for computing the loss.\r\n\r\n    Outputs:\r\n        if `start_positions` and `end_positions` are not `None`:\r\n            Outputs the total_loss which is the sum of the CrossEntropy loss for the start and end token positions.\r\n        if `start_positions` or `end_positions` is `None`:\r\n            Outputs a tuple of start_logits, end_logits which are the logits respectively for the start and end\r\n            position tokens of shape [batch_size, sequence_length].\r\n\r\n    Example usage:\r\n    ```python\r\n    # Already been converted into WordPiece token ids\r\n    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\r\n    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\r\n    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\r\n\r\n    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\r\n        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\r\n\r\n    model = BertForQuestionAnswering(config)\r\n    start_logits, end_logits = model(input_ids, token_type_ids, input_mask)\r\n    ```\r\n    """"""\r\n    def __init__(self, config):\r\n        super(BertForQuestionAnswering, self).__init__(config)\r\n        self.bert = BertModel(config)\r\n        # TODO check with Google if it\'s normal there is no dropout on the token classifier of SQuAD in the TF version\r\n        # self.dropout = nn.Dropout(config.hidden_dropout_prob)\r\n        self.qa_outputs = nn.Linear(config.hidden_size, 2)\r\n        self.apply(self.init_bert_weights)\r\n\r\n    def forward(self,\r\n                input_ids,\r\n                token_type_ids=None,\r\n                attention_mask=None,\r\n                start_positions=None,\r\n                end_positions=None,\r\n                checkpoint_activations=False):\r\n        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\r\n        logits = self.qa_outputs(sequence_output)\r\n        start_logits, end_logits = logits.split(1, dim=-1)\r\n        start_logits = start_logits.squeeze(-1)\r\n        end_logits = end_logits.squeeze(-1)\r\n\r\n        if start_positions is not None and end_positions is not None:\r\n            # If we are on multi-GPU, split add a dimension\r\n            if len(start_positions.size()) > 1:\r\n                start_positions = start_positions.squeeze(-1)\r\n            if len(end_positions.size()) > 1:\r\n                end_positions = end_positions.squeeze(-1)\r\n            # sometimes the start/end positions are outside our model inputs, we ignore these terms\r\n            ignored_index = start_logits.size(1)\r\n            start_positions.clamp_(0, ignored_index)\r\n            end_positions.clamp_(0, ignored_index)\r\n\r\n            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\r\n            start_loss = loss_fct(start_logits, start_positions)\r\n            end_loss = loss_fct(end_logits, end_positions)\r\n            total_loss = (start_loss + end_loss) / 2\r\n            return total_loss\r\n        else:\r\n            return start_logits, end_logits\r\n'"
tests/unit/multi_output_model.py,9,"b'import os\nimport json\nimport argparse\nimport torch\n\n\nclass MultiOutputModel(torch.nn.Module):\n    def __init__(self, hidden_dim, weight_value):\n        super(MultiOutputModel, self).__init__()\n        self.linear = torch.nn.Linear(hidden_dim, hidden_dim, bias=False)\n        self.linear.weight.data.fill_(weight_value)\n        self.cross_entropy_loss = torch.nn.CrossEntropyLoss()\n\n    def forward(self, inputs, targets):\n        losses = []\n        for x, y in zip(inputs, targets):\n            hidden_dim = self.linear(x)\n            loss = self.cross_entropy_loss(hidden_dim, y)\n            losses.append(loss)\n        return tuple(losses)\n\n\ndef multi_output_dataloader(model, total_samples, hidden_dim, device, inputs, targets):\n    assert len(inputs) == len(targets)\n    batch_size = model.train_micro_batch_size_per_gpu()\n\n    train_data = [\n        torch.full(size=(total_samples,\n                         hidden_dim),\n                   fill_value=x,\n                   device=device,\n                   dtype=torch.half,\n                   requires_grad=True) for x in inputs\n    ]\n\n    train_label = [\n        torch.empty(total_samples,\n                    device=device,\n                    dtype=torch.long).fill_(y) for y in targets\n    ]\n\n    train_dataset = torch.utils.data.TensorDataset(*train_data, *train_label)\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n    return train_loader\n'"
tests/unit/simple_model.py,10,"b""import os\nimport json\nimport argparse\nimport torch\n\n\nclass SimpleModel(torch.nn.Module):\n    def __init__(self, hidden_dim, empty_grad=False):\n        super(SimpleModel, self).__init__()\n        self.linear = torch.nn.Linear(hidden_dim, hidden_dim)\n        if empty_grad:\n            self.layers2 = torch.nn.ModuleList([torch.nn.Linear(hidden_dim, hidden_dim)])\n        self.cross_entropy_loss = torch.nn.CrossEntropyLoss()\n\n    def forward(self, x, y):\n        hidden_dim = x\n        hidden_dim = self.linear(hidden_dim)\n        return self.cross_entropy_loss(hidden_dim, y)\n\n\nclass SimpleOptimizer(torch.optim.Optimizer):\n    def __init__(self, params, lr=0.11072018):\n        defaults = dict(lr=lr)\n        super(SimpleOptimizer, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(SimpleOptimizer, self).__setstate__(state)\n\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                d_p = p.grad.data\n                p.data.add_(-group['lr'], d_p)\n\n        return loss\n\n\ndef random_dataloader(model, total_samples, hidden_dim, device):\n    batch_size = model.train_micro_batch_size_per_gpu()\n    train_data = torch.randn(total_samples, hidden_dim, device=device, dtype=torch.half)\n    train_label = torch.empty(total_samples,\n                              dtype=torch.long,\n                              device=device).random_(hidden_dim)\n    train_dataset = torch.utils.data.TensorDataset(train_data, train_label)\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n    return train_loader\n\n\ndef create_config_from_dict(tmpdir, config_dict):\n    config_path = os.path.join(tmpdir, 'temp_config.json')\n    with open(config_path, 'w') as fd:\n        json.dump(config_dict, fd)\n    return config_path\n\n\ndef args_from_dict(tmpdir, config_dict):\n    config_path = create_config_from_dict(tmpdir, config_dict)\n    parser = argparse.ArgumentParser()\n    args = parser.parse_args(args='')\n    args.deepspeed = True\n    args.deepspeed_config = config_path\n    args.local_rank = 0\n    return args\n"""
tests/unit/test_checkpointing.py,7,"b'import torch\nimport deepspeed\nfrom deepspeed.pt.deepspeed_zero_optimizer import FP16_DeepSpeedZeroOptimizer\nfrom deepspeed.pt.zero_optimizer_stage1 import FP16_DeepSpeedZeroOptimizer_Stage1\n\nfrom deepspeed.pt.fp16_optimizer import FP16_Optimizer\nfrom deepspeed.pt.fp16_unfused_optimizer import FP16_UnfusedOptimizer\n\nimport argparse\nimport pytest\nimport json\nimport os\nimport numbers\nfrom common import distributed_test\nfrom simple_model import SimpleModel, random_dataloader, args_from_dict\n\n\ndef compare_deepspeed_states(saved_model, loaded_model):\n    # These are compared in more depth in other places\n    assert hasattr(loaded_model, \'module\')\n\n    assert saved_model.csr_tensor_module_names == loaded_model.csr_tensor_module_names\n    assert saved_model.skipped_steps == loaded_model.skipped_steps\n    assert saved_model.global_steps == loaded_model.global_steps\n\n\ndef compare_model_states(saved_model, loaded_model):\n    compare_deepspeed_states(saved_model, loaded_model)\n\n    for p0, p1 in zip(saved_model.module.parameters(), loaded_model.module.parameters()):\n        assert torch.allclose(p0,p1,atol=1e-07), f""FP16 model state {p0} is not equal to {p1}""\n\n    if isinstance(saved_model.optimizer, FP16_DeepSpeedZeroOptimizer):\n        for p0, p1 in zip(saved_model.optimizer.single_partition_of_fp32_groups, loaded_model.optimizer.single_partition_of_fp32_groups):\n            assert torch.allclose(p0,p1,atol=1e-07), f""Fp32 model states {p0} is not equal to {p1}""\n\n    elif isinstance(saved_model.optimizer, FP16_DeepSpeedZeroOptimizer_Stage1):\n        for partition0, partition1 in zip(saved_model.optimizer.local_sub_partitions_of_fp32_groups, loaded_model.optimizer.local_sub_partitions_of_fp32_groups):\n            for p0, p1 in zip(partition0, partition1):\n                assert torch.allclose(p0,p1,atol=1e-07), f""Fp32 model states {p0} is not equal to {p1}""\n\n    elif isinstance(saved_model.optimizer, FP16_Optimizer):\n        for p0, p1 in zip(saved_model.optimizer.fp32_groups_flat, loaded_model.optimizer.fp32_groups_flat):\n            assert torch.allclose(p0,p1,atol=1e-07), f""FP32 model states {p0} is not equal to {p1}""\n\n    elif isinstance(saved_model.optimizer, FP16_UnfusedOptimizer):\n        for params0, params1 in zip(saved_model.optimizer.fp32_groups, loaded_model.optimizer.fp32_groups):\n            for p0, p1 in zip(params0, params1):\n                assert torch.allclose(p0,p1,atol=1e-07), f""FP32 model states {p0} is not equal to {p1}""\n\n    else:\n        assert False, \'Unexpected Optimizer Type\'\n\n\ndef compare_optimizer_states(saved_model, loaded_model, hidden_dim):\n    for state0, state1 in zip(saved_model.optimizer.optimizer.state.values(),\n                              loaded_model.optimizer.optimizer.state.values()):\n        for s0, s1 in zip(state0.values(), state1.values()):\n            if isinstance(s0, torch.Tensor) and isinstance(s1, torch.Tensor):\n                assert torch.equal(s0, s1)\n            else:\n                assert s0 == s1\n\n\ndef compare_lr_scheduler_states(saved_model, loaded_model):\n    assert hasattr(saved_model, \'lr_scheduler\')\n    assert hasattr(loaded_model, \'lr_scheduler\')\n\n    saved_scheduler = saved_model.lr_scheduler\n    loaded_scheduler = loaded_model.lr_scheduler\n\n    assert hasattr(saved_scheduler, \'state_dict\')\n    assert hasattr(loaded_scheduler, \'state_dict\')\n\n    saved_sd = saved_scheduler.state_dict()\n    loaded_sd = loaded_scheduler.state_dict()\n\n    print(f""saved_sd = {saved_sd}"")\n    print(f""loaded_sd = {loaded_sd}"")\n\n    assert saved_sd.keys() == loaded_sd.keys()\n\n    for state0, state1 in zip(saved_sd.values(), loaded_sd.values()):\n        if isinstance(state0, numbers.Number) and isinstance(state1, numbers.Number):\n            assert state0 == state1\n\n\ndef checkpoint_correctness_verification(args,\n                                        model,\n                                        hidden_dim,\n                                        tmpdir,\n                                        load_optimizer_states=False,\n                                        load_lr_scheduler_states=False):\n\n    ds_model, _, _,_ = deepspeed.initialize(args=args,\n                                            model=model,\n                                            model_parameters=model.parameters())\n    data_loader = random_dataloader(model=ds_model,\n                                    total_samples=50,\n                                    hidden_dim=hidden_dim,\n                                    device=ds_model.device)\n    for n, batch in enumerate(data_loader):\n        loss = ds_model(batch[0], batch[1])\n        ds_model.backward(loss)\n        ds_model.step()\n\n    trained_model = ds_model\n\n    save_folder = os.path.join(tmpdir, \'saved_checkpoint\')\n    save_tag = \'1\'\n\n    trained_model.save_checkpoint(save_folder, save_tag)\n\n    loaded_model, _, _,_ = deepspeed.initialize(args=args,\n                                            model=model,\n                                            model_parameters=model.parameters())\n\n    loaded_model.load_checkpoint(save_folder,\n                                 save_tag,\n                                 load_optimizer_states=load_optimizer_states,\n                                 load_lr_scheduler_states=load_lr_scheduler_states)\n\n    compare_model_states(trained_model, loaded_model)\n\n    if load_optimizer_states:\n        compare_optimizer_states(trained_model, loaded_model, hidden_dim)\n\n    if load_lr_scheduler_states:\n        compare_lr_scheduler_states(trained_model, loaded_model)\n\n\ndef test_checkpoint_unfused_optimizer(tmpdir):\n    config_dict = {\n        ""train_batch_size"": 2,\n        ""steps_per_print"": 1,\n        ""optimizer"": {\n            ""type"": ""Lamb"",\n            ""params"": {\n                ""lr"": 0.00015\n            }\n        },\n        ""gradient_clipping"": 1.0,\n        ""fp16"": {\n            ""enabled"": True\n        },\n        ""scheduler"": {\n            ""type"": ""OneCycle"",\n            ""params"": {\n                ""cycle_first_step_size"": 1000,\n                ""cycle_first_stair_count"": 500,\n                ""cycle_second_step_size"": 1000,\n                ""cycle_second_stair_count"": 500,\n                ""decay_step_size"": 1000,\n                ""cycle_min_lr"": 0.0001,\n                ""cycle_max_lr"": 0.0010,\n                ""decay_lr_rate"": 0.001,\n                ""cycle_min_mom"": 0.85,\n                ""cycle_max_mom"": 0.99,\n                ""decay_mom_rate"": 0.0\n            }\n        }\n    }\n\n    args = args_from_dict(tmpdir, config_dict)\n    hidden_dim = 10\n\n    model = SimpleModel(hidden_dim, empty_grad=False)\n\n    @distributed_test(world_size=[2])\n    def _test_checkpoint_unfused_optimizer(args,\n                                           model,\n                                           hidden_dim,\n                                           load_optimizer_states):\n        checkpoint_correctness_verification(args,\n                                            model,\n                                            hidden_dim,\n                                            tmpdir,\n                                            load_optimizer_states=load_optimizer_states)\n\n    _test_checkpoint_unfused_optimizer(args=args,\n                                       model=model,\n                                       hidden_dim=hidden_dim,\n                                       load_optimizer_states=True)\n    _test_checkpoint_unfused_optimizer(args=args,\n                                       model=model,\n                                       hidden_dim=hidden_dim,\n                                       load_optimizer_states=False)\n\n\ndef test_checkpoint_fused_optimizer(tmpdir):\n    config_dict = {\n        ""train_batch_size"": 2,\n        ""steps_per_print"": 1,\n        ""optimizer"": {\n            ""type"": ""Adam"",\n            ""params"": {\n                ""lr"": 0.00015,\n                ""betas"": [0.8,\n                          0.999],\n                ""eps"": 1e-8,\n                ""weight_decay"": 3e-7\n            }\n        },\n        ""fp16"": {\n            ""enabled"": True\n        }\n    }\n\n    args = args_from_dict(tmpdir, config_dict)\n    hidden_dim = 10\n\n    model = SimpleModel(hidden_dim, empty_grad=False)\n\n    @distributed_test(world_size=[2])\n    def _test_checkpoint_fused_optimizer(args, model, hidden_dim, load_optimizer_states):\n        checkpoint_correctness_verification(args,\n                                            model,\n                                            hidden_dim,\n                                            tmpdir,\n                                            load_optimizer_states=load_optimizer_states)\n\n    _test_checkpoint_fused_optimizer(args=args,\n                                     model=model,\n                                     hidden_dim=hidden_dim,\n                                     load_optimizer_states=True)\n    _test_checkpoint_fused_optimizer(args=args,\n                                     model=model,\n                                     hidden_dim=hidden_dim,\n                                     load_optimizer_states=False)\n\n\n@pytest.mark.parametrize(""zero_stage"", [1, 2])\ndef test_checkpoint_zero_optimizer(tmpdir, zero_stage):\n    config_dict = {\n        ""train_batch_size"": 2,\n        ""steps_per_print"": 1,\n        ""optimizer"": {\n            ""type"": ""Adam"",\n            ""params"": {\n                ""lr"": 0.00015,\n                ""betas"": [0.8,\n                          0.999],\n                ""eps"": 1e-8,\n                ""weight_decay"": 3e-7\n            }\n        },\n        ""fp16"": {\n            ""enabled"": True\n        },\n        ""zero_optimization"": {\n            ""stage"": zero_stage\n        },\n    }\n    args = args_from_dict(tmpdir, config_dict)\n    hidden_dim = 10\n\n    model = SimpleModel(hidden_dim, empty_grad=False)\n\n    @distributed_test(world_size=[2])\n    def _test_checkpoint_zero_optimizer(args, model, hidden_dim, load_optimizer_states):\n        checkpoint_correctness_verification(args,\n                                            model,\n                                            hidden_dim,\n                                            tmpdir,\n                                            load_optimizer_states=load_optimizer_states)\n\n    _test_checkpoint_zero_optimizer(args=args,\n                                    model=model,\n                                    hidden_dim=hidden_dim,\n                                    load_optimizer_states=True)\n\n\n@pytest.mark.parametrize(""zero_stage"", [1, 2])\ndef test_checkpoint_zero_no_optimizer(tmpdir, zero_stage):\n    config_dict = {\n        ""train_batch_size"": 2,\n        ""steps_per_print"": 1,\n        ""optimizer"": {\n            ""type"": ""Adam"",\n            ""params"": {\n                ""lr"": 0.00015,\n                ""betas"": [0.8,\n                          0.999],\n                ""eps"": 1e-8,\n                ""weight_decay"": 3e-7\n            }\n        },\n        ""fp16"": {\n            ""enabled"": True\n        },\n        ""zero_optimization"": {\n            ""stage"": zero_stage\n        },\n    }\n    args = args_from_dict(tmpdir, config_dict)\n    hidden_dim = 10\n\n    model = SimpleModel(hidden_dim, empty_grad=False)\n\n    @distributed_test(world_size=[2])\n    def _test_checkpoint_zero_no_optimizer(args,\n                                           model,\n                                           hidden_dim,\n                                           load_optimizer_states):\n        checkpoint_correctness_verification(args,\n                                            model,\n                                            hidden_dim,\n                                            tmpdir,\n                                            load_optimizer_states=load_optimizer_states)\n\n    _test_checkpoint_zero_no_optimizer(args=args,\n                                       model=model,\n                                       hidden_dim=hidden_dim,\n                                       load_optimizer_states=False)\n\n\n@pytest.mark.parametrize(""zero_stage"", [0, 1, 2])\ndef test_checkpoint_lr_scheduler(tmpdir, zero_stage):\n    config_dict = {\n        ""train_batch_size"": 2,\n        ""steps_per_print"": 1,\n        ""optimizer"": {\n            ""type"": ""Adam"",\n            ""params"": {\n                ""lr"": 0.00015,\n                ""betas"": [0.8,\n                          0.999],\n                ""eps"": 1e-8,\n                ""weight_decay"": 3e-7\n            }\n        },\n        ""fp16"": {\n            ""enabled"": True\n        },\n        ""zero_optimization"": {\n            ""stage"": zero_stage\n        },\n        ""scheduler"": {\n            ""type"": ""WarmupLR"",\n            ""params"": {\n                ""warmup_min_lr"": 0,\n                ""warmup_max_lr"": 0.001,\n                ""warmup_num_steps"": 1000\n            }\n        }\n    }\n    args = args_from_dict(tmpdir, config_dict)\n    hidden_dim = 10\n\n    model = SimpleModel(hidden_dim, empty_grad=False)\n\n    @distributed_test(world_size=[2])\n    def _test_checkpoint_lr_scheduler(args,\n                                      model,\n                                      hidden_dim,\n                                      load_optimizer_states,\n                                      load_lr_scheduler_states):\n        checkpoint_correctness_verification(\n            args,\n            model,\n            hidden_dim,\n            tmpdir,\n            load_optimizer_states=load_optimizer_states,\n            load_lr_scheduler_states=load_lr_scheduler_states)\n\n    _test_checkpoint_lr_scheduler(args=args,\n                                  model=model,\n                                  hidden_dim=hidden_dim,\n                                  load_optimizer_states=False,\n                                  load_lr_scheduler_states=True)\n\n\n@pytest.mark.parametrize(""zero_stage"", [0, 1, 2])\ndef test_checkpoint_no_lr_scheduler(tmpdir, zero_stage):\n    config_dict = {\n        ""train_batch_size"": 2,\n        ""steps_per_print"": 1,\n        ""optimizer"": {\n            ""type"": ""Adam"",\n            ""params"": {\n                ""lr"": 1e-5\n            }\n        },\n        ""fp16"": {\n            ""enabled"": True\n        },\n        ""zero_optimization"": {\n            ""stage"": zero_stage\n        },\n        ""scheduler"": {\n            ""type"": ""WarmupLR"",\n            ""params"": {\n                ""warmup_min_lr"": 0,\n                ""warmup_max_lr"": 0.001,\n                ""warmup_num_steps"": 1000\n            }\n        }\n    }\n    args = args_from_dict(tmpdir, config_dict)\n    hidden_dim = 10\n\n    model = SimpleModel(hidden_dim, empty_grad=False)\n\n    @distributed_test(world_size=[2])\n    def _test_checkpoint_no_lr_scheduler(args,\n                                         model,\n                                         hidden_dim,\n                                         load_optimizer_states,\n                                         load_lr_scheduler_states):\n        checkpoint_correctness_verification(\n            args,\n            model,\n            hidden_dim,\n            tmpdir,\n            load_optimizer_states=load_optimizer_states,\n            load_lr_scheduler_states=load_lr_scheduler_states)\n\n    _test_checkpoint_no_lr_scheduler(args=args,\n                                     model=model,\n                                     hidden_dim=hidden_dim,\n                                     load_optimizer_states=False,\n                                     load_lr_scheduler_states=False)\n'"
tests/unit/test_config.py,2,"b'# A test on its own\nimport torch\nimport pytest\nimport json\nimport argparse\nfrom common import distributed_test\nfrom simple_model import SimpleModel, create_config_from_dict, random_dataloader\nimport torch.distributed as dist\n\n# A test on its own\nimport deepspeed\nfrom deepspeed.pt.deepspeed_config import DeepSpeedConfig\n\n\ndef test_cuda():\n    assert (torch.cuda.is_available())\n\n\ndef test_check_version():\n    assert hasattr(deepspeed, ""__git_hash__"")\n    assert hasattr(deepspeed, ""__git_branch__"")\n    assert hasattr(deepspeed, ""__version__"")\n    assert hasattr(deepspeed, ""__version_major__"")\n    assert hasattr(deepspeed, ""__version_minor__"")\n    assert hasattr(deepspeed, ""__version_patch__"")\n\n\ndef _run_batch_config(ds_config, train_batch=None, micro_batch=None, gas=None):\n    ds_config.train_batch_size = train_batch\n    ds_config.train_micro_batch_size_per_gpu = micro_batch\n    ds_config.gradient_accumulation_steps = gas\n    success = True\n    try:\n        ds_config._configure_train_batch_size()\n    except AssertionError:\n        success = False\n    return success\n\n\ndef _batch_assert(status, ds_config, batch, micro_batch, gas, success):\n\n    if not success:\n        assert not status\n        print(""Failed but All is well"")\n        return\n\n    assert ds_config.train_batch_size == batch\n    assert ds_config.train_micro_batch_size_per_gpu == micro_batch\n    assert ds_config.gradient_accumulation_steps == gas\n    print(""All is well"")\n\n\n#Tests different batch config provided in deepspeed json file\n@pytest.mark.parametrize(\'num_ranks,batch,micro_batch,gas,success\',\n                         [(2,32,16,1,True),\n                         (2,32,8,2,True),\n                         (2,33,17,2,False),\n                         (2,32,18,1,False)]) # yapf: disable\ndef test_batch_config(num_ranks, batch, micro_batch, gas, success):\n    @distributed_test(world_size=2)\n    def _test_batch_config(num_ranks, batch, micro_batch, gas, success):\n        assert dist.get_world_size() == num_ranks, \\\n        \'The test assumes a world size of f{num_ranks}\'\n\n        ds_batch_config = \'tests/unit/ds_batch_config.json\'\n        ds_config = DeepSpeedConfig(ds_batch_config)\n\n        #test cases when all parameters are provided\n        status = _run_batch_config(ds_config,\n                                   train_batch=batch,\n                                   micro_batch=micro_batch,\n                                   gas=gas)\n        _batch_assert(status, ds_config, batch, micro_batch, gas, success)\n\n        #test cases when two out of three parameters are provided\n        status = _run_batch_config(ds_config, train_batch=batch, micro_batch=micro_batch)\n        _batch_assert(status, ds_config, batch, micro_batch, gas, success)\n\n        if success:\n            #when gas is provided with one more parameter\n            status = _run_batch_config(ds_config, train_batch=batch, gas=gas)\n            _batch_assert(status, ds_config, batch, micro_batch, gas, success)\n\n            status = _run_batch_config(ds_config, micro_batch=micro_batch, gas=gas)\n            _batch_assert(status, ds_config, batch, micro_batch, gas, success)\n\n            #test the case when only micro_batch or train_batch is provided\n            if gas == 1:\n                status = _run_batch_config(ds_config, micro_batch=micro_batch)\n                _batch_assert(status, ds_config, batch, micro_batch, gas, success)\n\n                status = _run_batch_config(ds_config, train_batch=batch)\n                _batch_assert(status, ds_config, batch, micro_batch, gas, success)\n        else:\n            #when only gas is provided\n            status = _run_batch_config(ds_config, gas=gas)\n            _batch_assert(status, ds_config, batch, micro_batch, gas, success)\n\n            #when gas is provided with something else and gas does not divide batch\n            if gas != 1:\n                status = _run_batch_config(ds_config, train_batch=batch, gas=gas)\n                _batch_assert(status, ds_config, batch, micro_batch, gas, success)\n\n    """"""Run batch config test """"""\n    _test_batch_config(num_ranks, batch, micro_batch, gas, success)\n\n\ndef test_temp_config_json(tmpdir):\n    config_dict = {\n        ""train_batch_size"": 1,\n    }\n    config_path = create_config_from_dict(tmpdir, config_dict)\n    config_json = json.load(open(config_path, \'r\'))\n    assert \'train_batch_size\' in config_json\n\n\ndef test_deprecated_deepscale_config(tmpdir):\n    config_dict = {\n        ""train_batch_size"": 1,\n        ""optimizer"": {\n            ""type"": ""Adam"",\n            ""params"": {\n                ""lr"": 0.00015\n            }\n        },\n        ""fp16"": {\n            ""enabled"": True\n        }\n    }\n\n    config_path = create_config_from_dict(tmpdir, config_dict)\n    parser = argparse.ArgumentParser()\n    args = parser.parse_args(args=\'\')\n    args.deepscale_config = config_path\n    args.local_rank = 0\n\n    hidden_dim = 10\n\n    model = SimpleModel(hidden_dim)\n\n    @distributed_test(world_size=[1])\n    def _test_deprecated_deepscale_config(args, model, hidden_dim):\n        model, _, _,_ = deepspeed.initialize(args=args,\n                                             model=model,\n                                             model_parameters=model.parameters())\n        data_loader = random_dataloader(model=model,\n                                        total_samples=5,\n                                        hidden_dim=hidden_dim,\n                                        device=model.device)\n        for n, batch in enumerate(data_loader):\n            loss = model(batch[0], batch[1])\n            model.backward(loss)\n            model.step()\n\n    _test_deprecated_deepscale_config(args=args, model=model, hidden_dim=hidden_dim)\n\n\ndef test_dist_init_true(tmpdir):\n    config_dict = {\n        ""train_batch_size"": 1,\n        ""optimizer"": {\n            ""type"": ""Adam"",\n            ""params"": {\n                ""lr"": 0.00015\n            }\n        },\n        ""fp16"": {\n            ""enabled"": True\n        }\n    }\n\n    config_path = create_config_from_dict(tmpdir, config_dict)\n    parser = argparse.ArgumentParser()\n    args = parser.parse_args(args=\'\')\n    args.deepscale_config = config_path\n    args.local_rank = 0\n\n    hidden_dim = 10\n\n    model = SimpleModel(hidden_dim)\n\n    @distributed_test(world_size=[1])\n    def _test_dist_init_true(args, model, hidden_dim):\n        model, _, _,_ = deepspeed.initialize(args=args,\n                                             model=model,\n                                             model_parameters=model.parameters(),\n                                             dist_init_required=True)\n        data_loader = random_dataloader(model=model,\n                                        total_samples=5,\n                                        hidden_dim=hidden_dim,\n                                        device=model.device)\n        for n, batch in enumerate(data_loader):\n            loss = model(batch[0], batch[1])\n            model.backward(loss)\n            model.step()\n\n    _test_dist_init_true(args=args, model=model, hidden_dim=hidden_dim)\n'"
tests/unit/test_csr.py,12,"b'import torch\nimport random\nfrom deepspeed.pt.deepspeed_csr_tensor import CSRTensor\n\n\ndef test_csr_addition_self():\n    row_count = 10\n    random.seed(1234)\n\n    x = torch.ones(1, 5)\n    for i in range(row_count - 1):\n        if random.random() > 0.75:\n            x = torch.cat([x, torch.ones(1, 5)])\n        else:\n            x = torch.cat([x, torch.zeros(1, 5)])\n    dense_x = x.clone()\n    cx = CSRTensor(x)\n\n    assert torch.all(dense_x == cx.to_dense())\n\n    cx.add(cx)\n    assert torch.all(dense_x + dense_x == cx.to_dense())\n\n\ndef test_csr_addition_different():\n    row_count = 10\n    random.seed(1234)\n\n    x = torch.ones(1, 5)\n    for i in range(row_count - 1):\n        if random.random() > 0.75:\n            x = torch.cat([x, torch.ones(1, 5)])\n        else:\n            x = torch.cat([x, torch.zeros(1, 5)])\n    dense_x = x.clone()\n    cx = CSRTensor(x)\n\n    y = torch.ones(1, 5)\n    for i in range(row_count - 1):\n        if random.random() > 0.75:\n            y = torch.cat([y, torch.ones(1, 5)])\n        else:\n            y = torch.cat([y, torch.zeros(1, 5)])\n    dense_y = y.clone()\n    cy = CSRTensor(y)\n\n    dense_sum = dense_x + dense_y\n    cx.add(cy)\n\n    assert torch.all(dense_sum == cx.to_dense())\n'"
tests/unit/test_cuda_backward.py,20,"b'import argparse\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport pytest\nimport json\nimport random\nimport time\nimport copy\nfrom torch import nn\nfrom modelingpreln import BertEncoder as BertEncoderPreln\nfrom modeling import BertEncoder as BertEncoderPostln\nfrom modeling import BertConfig, BertLayerNorm\nfrom deepspeed import DeepSpeedTransformerLayer, DeepSpeedTransformerConfig\n\nimport sys\n\n\ndef check_equal(first, second, atol=1e-2, verbose=False):\n    diction_x = {}\n    diction_y = {}\n\n    for i, (x, y) in enumerate(zip(first, second)):\n        print(x[1], y[1])\n\n    for i, (x, y) in enumerate(zip(first, second)):\n        k = 0\n        while (diction_x.get((k, x[1])) is not None):\n            k = k + 1\n        diction_x[k, x[1]] = x[0]\n        k = 0\n        while (diction_y.get((k, y[1])) is not None):\n            k = k + 1\n        diction_y[k, y[1]] = y[0]\n    if verbose:\n        print()\n    for i, (x, y) in enumerate(zip(diction_x, diction_y)):\n        print(x, y)\n\n    for i, (x, y) in enumerate(zip(diction_x, diction_y)):\n        if (x[0] == 1): continue\n        print(""checking "", x[1], "":"")\n        y = diction_y[x[0], x[1]]\n        x = diction_x[x[0], x[1]]\n        x = x.cpu().detach().numpy()\n        y = y.cpu().detach().numpy()\n        print(x)\n        print(y)\n\n        avgx = np.sum(abs(x), dtype=float)\n        countx = x.shape[0]\n        for i in range(len(x.shape) - 1):\n            countx *= x.shape[i + 1]\n            avgx = np.sum(avgx)\n        tollerance = 1\n        if avgx != float(\'inf\') and avgx != -float(\'inf\'):\n            avgx = avgx / countx\n            tollerance = avgx * atol\n        print(""tollerance is "", tollerance)\n        if verbose:\n            print(""x = {}"".format(x.flatten()))\n            print(""y = {}"".format(y.flatten()))\n            print(\'-\' * 80)\n        np.testing.assert_allclose(x, y, err_msg=""Index: {}"".format(i), atol=tollerance)\n\n\ndef zero_grad(variables):\n    for variable in variables:\n        variable.grad.zero_()\n\n\ndevice = torch.device(""cuda"")\nkwargs_fp32 = {\'dtype\': torch.float, \'device\': device, \'requires_grad\': True}\nkwargs_fp16 = {\'dtype\': torch.half, \'device\': device, \'requires_grad\': True}\n\n\nclass DSEncoder(nn.Module):\n    def __init__(self, config, weights, biases):\n        super(DSEncoder, self).__init__()\n        self.FinalLayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n        self.layer = nn.ModuleList([\n            copy.deepcopy(DeepSpeedTransformerLayer(i,\n                                                    config,\n                                                    weights,\n                                                    biases))\n            for i in range(config.num_hidden_layers)\n        ])\n        self.grads = []\n        self.pre_or_post = config.pre_layer_norm\n\n    def forward(self,\n                hidden_states,\n                attention_mask,\n                output_all_encoded_layers=True,\n                checkpoint_activations=False):\n        all_encoder_layers = []\n\n        def custom(start, end):\n            def custom_forward(*inputs):\n                layers = self.layer[start:end]\n                x_ = inputs[0]\n                for layer in layers:\n                    x_ = layer(x_, inputs[1])\n                return x_\n\n            return custom_forward\n\n        if checkpoint_activations:\n            l = 0\n            num_layers = len(self.layer)\n            chunk_length = math.ceil(math.sqrt(num_layers))\n            while l < num_layers:\n                hidden_states = checkpoint.checkpoint(custom(l,\n                                                             l + chunk_length),\n                                                      hidden_states,\n                                                      attention_mask * 1)\n                l += chunk_length\n            # decoder layers\n        else:\n            for i, layer_module in enumerate(self.layer):\n                hidden_states = layer_module(hidden_states, attention_mask, self.grads)\n                hidden_states.register_hook(\n                    lambda x,\n                    self=self: self.grads.append([x,\n                                                  ""hidden_state""]))\n\n                if output_all_encoded_layers:\n                    all_encoder_layers.append(hidden_states)\n\n        if not output_all_encoded_layers or checkpoint_activations:\n            if (self.pre_or_post):\n                hidden_states = self.FinalLayerNorm(hidden_states)\n            all_encoder_layers.append(hidden_states)\n        return all_encoder_layers\n\n    def get_grads(self):\n        return self.grads\n\n\ndef create_models(ds_config):\n    bert_config = BertConfig(vocab_size_or_config_json_file=119547,\n                             hidden_size=ds_config.hidden_size,\n                             num_hidden_layers=ds_config.num_hidden_layers,\n                             num_attention_heads=ds_config.heads,\n                             intermediate_size=4 * ds_config.hidden_size,\n                             hidden_act=""gelu"",\n                             hidden_dropout_prob=ds_config.hidden_dropout_ratio,\n                             attention_probs_dropout_prob=ds_config.attn_dropout_ratio,\n                             max_position_embeddings=ds_config.max_seq_length,\n                             type_vocab_size=2,\n                             initializer_range=ds_config.initializer_range)\n\n    weights = []\n    biases = []\n\n    for i in range(4):\n        weights.append(\n            nn.Parameter(torch.Tensor(ds_config.hidden_size,\n                                      ds_config.hidden_size)))\n        weights[i].data.normal_(mean=0.0, std=ds_config.initializer_range)\n\n    weights.append(nn.Parameter(torch.Tensor(ds_config.hidden_size)))\n    weights[4].data.fill_(1.0)\n    weights.append(\n        nn.Parameter(torch.Tensor(4 * ds_config.hidden_size,\n                                  ds_config.hidden_size)))\n    weights[5].data.normal_(mean=0.0, std=ds_config.initializer_range)\n    weights.append(\n        nn.Parameter(torch.Tensor(ds_config.hidden_size,\n                                  4 * ds_config.hidden_size)))\n    weights[6].data.normal_(mean=0.0, std=ds_config.initializer_range)\n    weights.append(nn.Parameter(torch.Tensor(ds_config.hidden_size)))\n    weights[7].data.fill_(1.0)\n\n    biases.append(nn.Parameter(torch.Tensor(ds_config.hidden_size)))\n    biases[0].data.zero_()\n    for i in range(4):\n        biases.append(nn.Parameter(torch.Tensor(ds_config.hidden_size)))\n        biases[i + 1].data.zero_()\n    biases.append(nn.Parameter(torch.Tensor(4 * ds_config.hidden_size)))\n    biases[5].data.zero_()\n    biases.append(nn.Parameter(torch.Tensor(ds_config.hidden_size)))\n    biases[6].data.zero_()\n    biases.append(nn.Parameter(torch.Tensor(ds_config.hidden_size)))\n    biases[7].data.zero_()\n\n    if (ds_config.pre_layer_norm):\n        bert_encoder = BertEncoderPreln(bert_config, weights, biases)\n    else:\n        bert_encoder = BertEncoderPostln(bert_config, weights, biases)\n    ds_encoder = DSEncoder(ds_config, weights, biases)\n\n    if ds_config.fp16:\n        bert_encoder.half()\n        ds_encoder.half()\n\n    bert_encoder.cuda()\n    ds_encoder.cuda()\n\n    return bert_encoder, ds_encoder\n\n\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\n\ndef run_backward(ds_config, atol=1e-2, verbose=False):\n    set_seed(123)\n    bert_encoder, ds_encoder = create_models(ds_config)\n\n    # prepare test data\n    kwargs = kwargs_fp16 if ds_config.fp16 else kwargs_fp32\n    hidden_states = torch.randn(ds_config.batch_size,\n                                ds_config.max_seq_length,\n                                ds_config.hidden_size,\n                                **kwargs)\n    input_mask = torch.randn(ds_config.batch_size,\n                             1,\n                             1,\n                             ds_config.max_seq_length,\n                             **kwargs)\n    Y = torch.randn(ds_config.batch_size,\n                    ds_config.max_seq_length,\n                    ds_config.hidden_size,\n                    **kwargs)\n\n    # run baseline\n    base_results = bert_encoder(hidden_states,\n                                input_mask,\n                                output_all_encoded_layers=False,\n                                checkpoint_activations=False)\n\n    loss = (Y - base_results[0]).pow(2).sum()\n    loss.backward()\n    base_grads = bert_encoder.get_grads()\n\n    # run ds\n    ds_results = ds_encoder(hidden_states,\n                            input_mask,\n                            output_all_encoded_layers=False,\n                            checkpoint_activations=False)\n\n    loss = (Y - ds_results[0]).pow(2).sum()\n    loss.backward()\n    ds_grads = ds_encoder.get_grads()\n\n    # check grads\n    check_equal(base_grads, ds_grads, atol=atol, verbose=verbose)\n\n\n@pytest.mark.parametrize(\'batch_size, hidden_size, seq_len, heads, num_layers, is_preln, use_fp16, atol\',\n                         [\n                             (3,1024,128,16,24,True,False, 0.05),\n                             (3,1024,128,16,24,True,True, 0.05),\n                             (3,1024,128,16,24,False,False, 0.1),\n                             (3,1024,128,16,24,False,True, 0.2),\n                         ]) # yapf: disable\ndef test_backward(batch_size,\n                  hidden_size,\n                  seq_len,\n                  heads,\n                  num_layers,\n                  is_preln,\n                  use_fp16,\n                  atol):\n    # Only run fp16 test cases on devices with 7+ capability.\n    major, _ = torch.cuda.get_device_capability()\n    if major < 7 and (use_fp16 is True or is_preln is False):\n        return\n\n    ds_config = DeepSpeedTransformerConfig()\n    ds_config.layer_id = None\n    ds_config.batch_size = batch_size\n    ds_config.hidden_size = hidden_size\n    ds_config.max_seq_length = seq_len\n    ds_config.heads = heads\n    ds_config.attn_dropout_ratio = 0.0\n    ds_config.hidden_dropout_ratio = 0.0\n    ds_config.num_hidden_layers = num_layers\n    ds_config.pre_layer_norm = is_preln\n    ds_config.initializer_range = 0.02\n    ds_config.fp16 = use_fp16\n\n    run_backward(ds_config, atol=atol)\n\n\n#@pytest.mark.parametrize(\'batch_size, hidden_size, seq_len, heads, num_layers, is_preln, use_fp16, atol\',\n#                         [\n#                             (3,1024,128,16,24,True,False, 0.07),\n#                             (3,1024,128,16,24,True,True, 0.05),\n#                             (3,1024,128,16,24,False,False, 0.1),\n#                             (3,1024,128,16,24,False,True, 0.2),\n#                         ]) # yapf: disable\n#def test_backward_stochastic(batch_size,\n#                             hidden_size,\n#                             seq_len,\n#                             heads,\n#                             num_layers,\n#                             is_preln,\n#                             use_fp16,\n#                             atol):\n#    # Only run fp16 test cases on devices with 7+ capability.\n#    major, _ = torch.cuda.get_device_capability()\n#    if major < 7 and (use_fp16 is True or is_preln is False):\n#        return\n#\n#    ds_config = DeepSpeedTransformerConfig()\n#    ds_config.layer_id = None\n#    ds_config.batch_size = batch_size\n#    ds_config.hidden_size = hidden_size\n#    ds_config.max_seq_length = seq_len\n#    ds_config.heads = heads\n#    ds_config.attn_dropout_ratio = 0.0\n#    ds_config.hidden_dropout_ratio = 0.0\n#    ds_config.num_hidden_layers = num_layers\n#    ds_config.pre_layer_norm = is_preln\n#    ds_config.initializer_range = 0.02\n#    ds_config.fp16 = use_fp16\n#    ds_config.stochastic_mode = True\n#\n#    run_backward(ds_config, atol=atol)\n'"
tests/unit/test_cuda_forward.py,20,"b'import argparse\r\nimport numpy as np\r\nimport torch\r\nimport torch.nn.functional as F\r\nimport pytest\r\nimport json\r\nimport random\r\nimport time\r\nimport copy\r\nfrom torch import nn\r\nfrom modelingpreln import BertEncoder as BertEncoderPreln\r\nfrom modeling import BertEncoder as BertEncoderPostln\r\nfrom modeling import BertLayerNorm, BertConfig\r\nfrom deepspeed import DeepSpeedTransformerLayer, DeepSpeedTransformerConfig\r\n\r\nimport sys\r\n\r\n\r\ndef check_equal(first, second, atol=1e-2, verbose=False):\r\n    if verbose:\r\n        print()\r\n    for i, (x, y) in enumerate(zip(first, second)):\r\n        x = x[0].cpu().detach().numpy()\r\n        y = y[0].cpu().detach().numpy()\r\n        if verbose:\r\n            print(""x = {}"".format(x.flatten()))\r\n            print(""y = {}"".format(y.flatten()))\r\n            print(\'-\' * 80)\r\n        np.testing.assert_allclose(x, y, err_msg=""Index: {}"".format(i), atol=atol)\r\n\r\n\r\ndef zero_grad(variables):\r\n    for variable in variables:\r\n        variable.grad.zero_()\r\n\r\n\r\ndevice = torch.device(""cuda"")\r\nkwargs_fp32 = {\'dtype\': torch.float, \'device\': device, \'requires_grad\': True}\r\nkwargs_fp16 = {\'dtype\': torch.half, \'device\': device, \'requires_grad\': True}\r\n\r\n\r\nclass DSEncoder(nn.Module):\r\n    def __init__(self, config, weights, biases):\r\n        super(DSEncoder, self).__init__()\r\n        self.FinalLayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\r\n        self.layer = nn.ModuleList([\r\n            copy.deepcopy(DeepSpeedTransformerLayer(i,\r\n                                                    config,\r\n                                                    weights,\r\n                                                    biases))\r\n            for i in range(config.num_hidden_layers)\r\n        ])\r\n        self.grads = []\r\n        self.pre_or_post = config.pre_layer_norm\r\n\r\n    def forward(self,\r\n                hidden_states,\r\n                attention_mask,\r\n                output_all_encoded_layers=True,\r\n                checkpoint_activations=False):\r\n        all_encoder_layers = []\r\n\r\n        def custom(start, end):\r\n            def custom_forward(*inputs):\r\n                layers = self.layer[start:end]\r\n                x_ = inputs[0]\r\n                for layer in layers:\r\n                    x_ = layer(x_, inputs[1])\r\n                return x_\r\n\r\n            return custom_forward\r\n\r\n        if checkpoint_activations:\r\n            l = 0\r\n            num_layers = len(self.layer)\r\n            chunk_length = math.ceil(math.sqrt(num_layers))\r\n            while l < num_layers:\r\n                hidden_states = checkpoint.checkpoint(custom(l,\r\n                                                             l + chunk_length),\r\n                                                      hidden_states,\r\n                                                      attention_mask * 1)\r\n                l += chunk_length\r\n            # decoder layers\r\n        else:\r\n            for i, layer_module in enumerate(self.layer):\r\n                hidden_states = layer_module(hidden_states, attention_mask)\r\n                hidden_states.register_hook(\r\n                    lambda x,\r\n                    i=i,\r\n                    self=self: self.grads.append([x,\r\n                                                  ""hidden_state""]))\r\n\r\n                if output_all_encoded_layers:\r\n                    all_encoder_layers.append(hidden_states)\r\n\r\n        if not output_all_encoded_layers or checkpoint_activations:\r\n            if (self.pre_or_post):\r\n                hidden_states = self.FinalLayerNorm(hidden_states)\r\n            all_encoder_layers.append(hidden_states)\r\n        return all_encoder_layers\r\n\r\n    def get_grads(self):\r\n        return self.grads\r\n\r\n\r\ndef create_models(ds_config):\r\n    bert_config = BertConfig(vocab_size_or_config_json_file=119547,\r\n                             hidden_size=ds_config.hidden_size,\r\n                             num_hidden_layers=ds_config.num_hidden_layers,\r\n                             num_attention_heads=ds_config.heads,\r\n                             batch_size=ds_config.batch_size,\r\n                             intermediate_size=4 * ds_config.hidden_size,\r\n                             hidden_act=""gelu"",\r\n                             hidden_dropout_prob=ds_config.hidden_dropout_ratio,\r\n                             attention_probs_dropout_prob=ds_config.attn_dropout_ratio,\r\n                             max_position_embeddings=ds_config.max_seq_length,\r\n                             type_vocab_size=2,\r\n                             initializer_range=ds_config.initializer_range,\r\n                             fp16=ds_config.fp16)\r\n\r\n    weights = []\r\n    biases = []\r\n\r\n    for i in range(4):\r\n        weights.append(\r\n            nn.Parameter(torch.Tensor(ds_config.hidden_size,\r\n                                      ds_config.hidden_size)))\r\n        weights[i].data.normal_(mean=0.0, std=ds_config.initializer_range)\r\n\r\n    weights.append(nn.Parameter(torch.Tensor(ds_config.hidden_size)))\r\n    weights[4].data.fill_(1.0)\r\n    weights.append(\r\n        nn.Parameter(torch.Tensor(4 * ds_config.hidden_size,\r\n                                  ds_config.hidden_size)))\r\n    weights[5].data.normal_(mean=0.0, std=ds_config.initializer_range)\r\n    weights.append(\r\n        nn.Parameter(torch.Tensor(ds_config.hidden_size,\r\n                                  4 * ds_config.hidden_size)))\r\n    weights[6].data.normal_(mean=0.0, std=ds_config.initializer_range)\r\n    weights.append(nn.Parameter(torch.Tensor(ds_config.hidden_size)))\r\n    weights[7].data.fill_(1.0)\r\n\r\n    biases.append(nn.Parameter(torch.Tensor(ds_config.hidden_size)))\r\n    biases[0].data.zero_()\r\n    for i in range(4):\r\n        biases.append(nn.Parameter(torch.Tensor(ds_config.hidden_size)))\r\n        biases[i + 1].data.zero_()\r\n    biases.append(nn.Parameter(torch.Tensor(4 * ds_config.hidden_size)))\r\n    biases[5].data.zero_()\r\n    biases.append(nn.Parameter(torch.Tensor(ds_config.hidden_size)))\r\n    biases[6].data.zero_()\r\n    biases.append(nn.Parameter(torch.Tensor(ds_config.hidden_size)))\r\n    biases[7].data.zero_()\r\n\r\n    if (ds_config.pre_layer_norm):\r\n        bert_encoder = BertEncoderPreln(bert_config, weights, biases)\r\n    else:\r\n        bert_encoder = BertEncoderPostln(bert_config, weights, biases)\r\n    ds_encoder = DSEncoder(ds_config, weights, biases)\r\n\r\n    if ds_config.fp16:\r\n        bert_encoder.half()\r\n        ds_encoder.half()\r\n\r\n    bert_encoder.cuda()\r\n    ds_encoder.cuda()\r\n\r\n    return bert_encoder, ds_encoder\r\n\r\n\r\ndef set_seed(seed):\r\n    random.seed(seed)\r\n    np.random.seed(seed)\r\n    torch.manual_seed(seed)\r\n\r\n\r\ndef run_forward(ds_config, atol=1e-2, verbose=False, test_bsz=None):\r\n    set_seed(123)\r\n    bert_encoder, ds_encoder = create_models(ds_config)\r\n\r\n    bsz = ds_config.batch_size if test_bsz is None else test_bsz\r\n\r\n    # prepare test data\r\n    kwargs = kwargs_fp16 if ds_config.fp16 else kwargs_fp32\r\n    hidden_states = torch.randn(bsz,\r\n                                ds_config.max_seq_length,\r\n                                ds_config.hidden_size,\r\n                                **kwargs)\r\n    input_mask = torch.randn(bsz, 1, 1, ds_config.max_seq_length, **kwargs)\r\n\r\n    # run baseline\r\n    base_results = bert_encoder(hidden_states,\r\n                                input_mask,\r\n                                output_all_encoded_layers=False,\r\n                                checkpoint_activations=False)\r\n\r\n    # run ds\r\n    ds_results = ds_encoder(hidden_states,\r\n                            input_mask,\r\n                            output_all_encoded_layers=False,\r\n                            checkpoint_activations=False)\r\n\r\n    # check grads\r\n    check_equal(base_results, ds_results, atol=atol, verbose=verbose)\r\n\r\n\r\n# FP16 test cases can only run on the devices support FP16.\r\n@pytest.mark.parametrize(\'batch_size, hidden_size, seq_len, heads, num_layers, is_preln, use_fp16\',\r\n                         [\r\n                             (64,1024,128,16,3,True,False),\r\n                             (64,1024,128,16,3,True,True),\r\n                             (8,1024,384,16,3,True,False),\r\n                             (8,1024,384,16,3,True,True),\r\n                             (8,1024,512,16,3,True,False),\r\n                             (8,1024,512,16,3,True,True),\r\n                             (64,1024,128,16,3,False,False),\r\n                             (64,1024,128,16,3,False,True),\r\n                             (8,1024,384,16,3,False,False),\r\n                             (8,1024,384,16,3,False,True),\r\n                             (8,1024,512,16,3,False,False),\r\n                             (8,1024,512,16,3,False,True),\r\n                             (8,1536,128,24,3,False,False),\r\n                             (8,1536,128,24,3,False,True),\r\n                             (8,2048,128,32,3,False,False),\r\n                             (8,2048,128,32,3,False,True),\r\n                             (8,2560,128,40,3,False,False),\r\n                             (8,2560,128,40,3,False,True),\r\n                         ]) # yapf: disable\r\ndef test_forward(batch_size,\r\n                 hidden_size,\r\n                 seq_len,\r\n                 heads,\r\n                 num_layers,\r\n                 is_preln,\r\n                 use_fp16):\r\n    # Only run fp16 test cases on devices with 7+ capability.\r\n    major, _ = torch.cuda.get_device_capability()\r\n    if major < 7 and use_fp16 is True:\r\n        return\r\n\r\n    ds_config = DeepSpeedTransformerConfig()\r\n    ds_config.layer_id = None\r\n    ds_config.batch_size = batch_size\r\n    ds_config.hidden_size = hidden_size\r\n    ds_config.max_seq_length = seq_len\r\n    ds_config.heads = heads\r\n    ds_config.attn_dropout_ratio = 0.0\r\n    ds_config.hidden_dropout_ratio = 0.0\r\n    ds_config.num_hidden_layers = num_layers\r\n    ds_config.pre_layer_norm = is_preln\r\n    ds_config.initializer_range = 0.02\r\n    ds_config.fp16 = use_fp16\r\n\r\n    run_forward(ds_config, atol=2e-2)\r\n\r\n\r\n@pytest.mark.parametrize(\'batch_size, small_bsz, hidden_size, seq_len, heads, num_layers, is_preln, use_fp16\',\r\n                         [\r\n                             (8,3,1024,512,16,3,True,False),\r\n                             (8,7,1024,512,16,3,True,True),\r\n                             (8,3,1024,512,16,3,False,False),\r\n                             (8,7,1024,512,16,3,False,True),\r\n                         ]) # yapf: disable\r\ndef test_forward_with_small_bsz(batch_size,\r\n                                small_bsz,\r\n                                hidden_size,\r\n                                seq_len,\r\n                                heads,\r\n                                num_layers,\r\n                                is_preln,\r\n                                use_fp16):\r\n    # Only run fp16 test cases on devices with 7+ capability.\r\n    major, _ = torch.cuda.get_device_capability()\r\n    if major < 7 and use_fp16 is True:\r\n        return\r\n\r\n    ds_config = DeepSpeedTransformerConfig()\r\n    ds_config.layer_id = None\r\n    ds_config.batch_size = batch_size\r\n    ds_config.hidden_size = hidden_size\r\n    ds_config.max_seq_length = seq_len\r\n    ds_config.heads = heads\r\n    ds_config.attn_dropout_ratio = 0.0\r\n    ds_config.hidden_dropout_ratio = 0.0\r\n    ds_config.num_hidden_layers = num_layers\r\n    ds_config.pre_layer_norm = is_preln\r\n    ds_config.initializer_range = 0.02\r\n    ds_config.fp16 = use_fp16\r\n\r\n    run_forward(ds_config, atol=2e-2, test_bsz=small_bsz)\r\n\r\n@pytest.mark.parametrize(\'batch_size, hidden_size, seq_len, heads, num_layers, is_preln, use_fp16\',\r\n                         [\r\n                             (64,1024,128,16,3,True,False),\r\n                             (64,1024,128,16,3,True,True),\r\n                             (64,1024,128,16,3,False,False),\r\n                             (64,1024,128,16,3,False,True),\r\n                         ]) # yapf: disable\r\ndef test_forward_stochastic(batch_size,\r\n                            hidden_size,\r\n                            seq_len,\r\n                            heads,\r\n                            num_layers,\r\n                            is_preln,\r\n                            use_fp16):\r\n    # Only run fp16 test cases on devices with 7+ capability.\r\n    major, _ = torch.cuda.get_device_capability()\r\n    if major < 7 and use_fp16 is True:\r\n        return\r\n\r\n    ds_config = DeepSpeedTransformerConfig()\r\n    ds_config.layer_id = None\r\n    ds_config.batch_size = batch_size\r\n    ds_config.hidden_size = hidden_size\r\n    ds_config.max_seq_length = seq_len\r\n    ds_config.heads = heads\r\n    ds_config.attn_dropout_ratio = 0.0\r\n    ds_config.hidden_dropout_ratio = 0.0\r\n    ds_config.num_hidden_layers = num_layers\r\n    ds_config.pre_layer_norm = is_preln\r\n    ds_config.initializer_range = 0.02\r\n    ds_config.fp16 = use_fp16\r\n    ds_config.stochastic_mode = True\r\n\r\n    run_forward(ds_config, atol=7e-2)\r\n'"
tests/unit/test_dist.py,4,"b'import torch\nimport torch.distributed as dist\n\nfrom common import distributed_test\n\nimport pytest\n\n\n@distributed_test(world_size=3)\ndef test_init():\n    assert dist.is_initialized()\n    assert dist.get_world_size() == 3\n    assert dist.get_rank() < 3\n\n\n# Demonstration of pytest\'s paramaterization\n@pytest.mark.parametrize(\'number,color\', [(1138, \'purple\')])\ndef test_dist_args(number, color):\n    """"""Outer test function with inputs from pytest.mark.parametrize(). Uses a distributed\n    helper function.\n    """"""\n    @distributed_test(world_size=2)\n    def _test_dist_args_helper(x, color=\'red\'):\n        assert dist.get_world_size() == 2\n        assert x == 1138\n        assert color == \'purple\'\n\n    """"""Ensure that we can parse args to distributed_test decorated functions. """"""\n    _test_dist_args_helper(number, color=color)\n\n\n@distributed_test(world_size=[1, 2, 4])\ndef test_dist_allreduce():\n    x = torch.ones(1, 3).cuda() * (dist.get_rank() + 1)\n    sum_of_ranks = (dist.get_world_size() * (dist.get_world_size() + 1)) // 2\n    result = torch.ones(1, 3).cuda() * sum_of_ranks\n    dist.all_reduce(x)\n    assert torch.all(x == result)\n'"
tests/unit/test_ds_arguments.py,0,"b""import argparse\nimport pytest\nimport deepspeed\n\n\ndef basic_parser():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--num_epochs', type=int)\n    return parser\n\n\ndef test_no_ds_arguments_no_ds_parser():\n    parser = basic_parser()\n    args = parser.parse_args(['--num_epochs', '2'])\n    assert args\n\n    assert hasattr(args, 'num_epochs')\n    assert args.num_epochs == 2\n\n    assert not hasattr(args, 'deepspeed')\n    assert not hasattr(args, 'deepspeed_config')\n\n\ndef test_no_ds_arguments():\n    parser = basic_parser()\n    parser = deepspeed.add_config_arguments(parser)\n    args = parser.parse_args(['--num_epochs', '2'])\n    assert args\n\n    assert hasattr(args, 'num_epochs')\n    assert args.num_epochs == 2\n\n    assert hasattr(args, 'deepspeed')\n    assert args.deepspeed == False\n\n    assert hasattr(args, 'deepspeed_config')\n    assert args.deepspeed_config == None\n\n\ndef test_no_ds_enable_argument():\n    parser = basic_parser()\n    parser = deepspeed.add_config_arguments(parser)\n    args = parser.parse_args(['--num_epochs', '2', '--deepspeed_config', 'foo.json'])\n    assert args\n\n    assert hasattr(args, 'num_epochs')\n    assert args.num_epochs == 2\n\n    assert hasattr(args, 'deepspeed')\n    assert args.deepspeed == False\n\n    assert hasattr(args, 'deepspeed_config')\n    assert type(args.deepspeed_config) == str\n    assert args.deepspeed_config == 'foo.json'\n\n\ndef test_no_ds_config_argument():\n    parser = basic_parser()\n    parser = deepspeed.add_config_arguments(parser)\n    args = parser.parse_args(['--num_epochs', '2', '--deepspeed'])\n    assert args\n\n    assert hasattr(args, 'num_epochs')\n    assert args.num_epochs == 2\n\n    assert hasattr(args, 'deepspeed')\n    assert type(args.deepspeed) == bool\n    assert args.deepspeed == True\n\n    assert hasattr(args, 'deepspeed_config')\n    assert args.deepspeed_config == None\n\n\ndef test_no_ds_parser():\n    parser = basic_parser()\n    with pytest.raises(SystemExit):\n        args = parser.parse_args(['--num_epochs', '2', '--deepspeed'])\n\n\ndef test_core_deepscale_arguments():\n    parser = basic_parser()\n    parser = deepspeed.add_config_arguments(parser)\n    args = parser.parse_args(\n        ['--num_epochs',\n         '2',\n         '--deepspeed',\n         '--deepspeed_config',\n         'foo.json'])\n    assert args\n\n    assert hasattr(args, 'num_epochs')\n    assert args.num_epochs == 2\n\n    assert hasattr(args, 'deepspeed')\n    assert type(args.deepspeed) == bool\n    assert args.deepspeed == True\n\n    assert hasattr(args, 'deepspeed_config')\n    assert type(args.deepspeed_config) == str\n    assert args.deepspeed_config == 'foo.json'\n"""
tests/unit/test_ds_config.py,0,"b'import pytest\nimport os\nimport json\nfrom deepspeed.pt import deepspeed_config as ds_config\n\n\ndef test_only_required_fields(tmpdir):\n    \'\'\'Ensure that config containing only the required fields is accepted. \'\'\'\n    cfg_json = tmpdir.mkdir(\'ds_config_unit_test\').join(\'minimal.json\')\n\n    with open(cfg_json, \'w\') as f:\n        required_fields = {\'train_batch_size\': 64}\n        json.dump(required_fields, f)\n\n    run_cfg = ds_config.DeepSpeedConfig(cfg_json)\n    assert run_cfg is not None\n    assert run_cfg.train_batch_size == 64\n    assert run_cfg.train_micro_batch_size_per_gpu == 64\n    assert run_cfg.gradient_accumulation_steps == 1\n\n\ndef test_config_duplicate_key(tmpdir):\n    config_dict = \'\'\'\n    {\n        ""train_batch_size"": 24,\n        ""train_batch_size"": 24,\n    }\n    \'\'\'\n    config_path = os.path.join(tmpdir, \'temp_config.json\')\n\n    with open(config_path, \'w\') as jf:\n        jf.write(""%s"" % config_dict)\n\n    with pytest.raises(ValueError):\n        run_cfg = ds_config.DeepSpeedConfig(config_path)\n'"
tests/unit/test_dynamic_loss_scale.py,1,"b'import torch\nimport deepspeed\nimport argparse\nimport pytest\nimport json\nimport os\nimport numpy as np\nfrom common import distributed_test\nfrom simple_model import SimpleModel, args_from_dict\n\n\ndef run_model_step(model, gradient_list):\n    for value in gradient_list:\n        for p in model.parameters():\n            p.grad = torch.empty_like(p, dtype=p.dtype)\n            p.grad.fill_(value)\n        model.step()\n\n\ndef test_fused_no_overflow(tmpdir):\n    config_dict = {\n        ""train_batch_size"": 1,\n        ""steps_per_print"": 1,\n        ""optimizer"": {\n            ""type"": ""Adam"",\n            ""params"": {\n                ""lr"": 0.00015\n            }\n        },\n        ""fp16"": {\n            ""enabled"": True,\n            ""loss_scale"": 0,\n            ""initial_scale_power"": 8,\n            ""loss_scale_window"": 2\n        }\n    }\n    args = args_from_dict(tmpdir, config_dict)\n\n    @distributed_test(world_size=1)\n    def _test_fused_no_overflow(args):\n        hidden_dim = 1\n        model = SimpleModel(hidden_dim, empty_grad=True)\n        model, optim, _, _ = deepspeed.initialize(args=args,\n                                                  model=model,\n                                                  model_parameters=model.parameters())\n\n        expected_loss_scale = 2**8\n        expected_scale_window = 2\n        # Ensure the dynamic loss scaler is correctly configured.\n        assert optim.dynamic_loss_scale == True\n        assert optim.cur_scale == expected_loss_scale\n        assert optim.scale_window == expected_scale_window\n\n        for i, value in enumerate(np.random.uniform(-0.1, 0.1, 10)):\n            run_model_step(model, [value])\n            assert optim.cur_scale == expected_loss_scale\n            assert optim.cur_iter == (i + 1)\n            if optim.cur_iter % expected_scale_window == 0:\n                expected_loss_scale *= 2\n\n    _test_fused_no_overflow(args)\n\n\ndef test_fused_all_overflow(tmpdir):\n    config_dict = {\n        ""train_batch_size"": 1,\n        ""steps_per_print"": 1,\n        ""optimizer"": {\n            ""type"": ""Adam"",\n            ""params"": {\n                ""lr"": 0.00015\n            }\n        },\n        ""fp16"": {\n            ""enabled"": True,\n            ""loss_scale"": 0,\n            ""initial_scale_power"": 4,\n            ""loss_scale_window"": 2\n        }\n    }\n    args = args_from_dict(tmpdir, config_dict)\n\n    @distributed_test(world_size=1)\n    def _test_fused_all_overflow(args):\n        hidden_dim = 1\n        model = SimpleModel(hidden_dim, empty_grad=True)\n        model, optim, _, _ = deepspeed.initialize(args=args,\n                                                  model=model,\n                                                  model_parameters=model.parameters())\n\n        expected_loss_scale = 2**4\n        # Ensure the dynamic loss scaler is correctly configured.\n        assert optim.dynamic_loss_scale == True\n        assert optim.cur_scale == expected_loss_scale\n\n        overflow_gradients = [float(\'inf\'), float(\'-inf\')] + [float(\'nan\')] * 6\n        for i, value in enumerate(overflow_gradients):\n            run_model_step(model, [value])\n            expected_loss_scale = max(expected_loss_scale / 2, 1)\n            assert optim.cur_scale == expected_loss_scale\n            assert optim.cur_iter == (i + 1)\n\n    _test_fused_all_overflow(args)\n\n\ndef test_fused_some_overflow(tmpdir):\n    config_dict = {\n        ""train_batch_size"": 1,\n        ""steps_per_print"": 1,\n        ""optimizer"": {\n            ""type"": ""Adam"",\n            ""params"": {\n                ""lr"": 0.00015\n            }\n        },\n        ""fp16"": {\n            ""enabled"": True,\n            ""loss_scale"": 0,\n            ""initial_scale_power"": 8,\n            ""loss_scale_window"": 2\n        }\n    }\n    args = args_from_dict(tmpdir, config_dict)\n\n    @distributed_test(world_size=1)\n    def _test_fused_some_overflow(args):\n        hidden_dim = 1\n        model = SimpleModel(hidden_dim, empty_grad=True)\n        model, optim, _, _ = deepspeed.initialize(args=args,\n                                                  model=model,\n                                                  model_parameters=model.parameters())\n\n        expected_loss_scale = 2**8\n        expected_scale_window = 2\n        expected_iteration = 0\n        # Ensure the dynamic loss scaler is correctly configured.\n        assert optim.dynamic_loss_scale == True\n        assert optim.cur_scale == expected_loss_scale\n        assert optim.scale_window == expected_scale_window\n\n        # Run model with overflows to decrease scale\n        overflow_gradients = [float(\'inf\'), float(\'nan\')]\n        expected_iteration += len(overflow_gradients)\n        run_model_step(model, overflow_gradients)\n        expected_loss_scale /= (2**len(overflow_gradients))\n        assert optim.cur_scale == expected_loss_scale\n        assert optim.cur_iter == expected_iteration\n\n        # Run model scale_window + 1 times to increase scale once\n        normal_gradients = np.random.uniform(-0.1, 0.1, expected_scale_window + 1)\n        expected_iteration += len(normal_gradients)\n        run_model_step(model, normal_gradients)\n        expected_loss_scale *= 2\n        assert optim.cur_scale == expected_loss_scale\n        assert optim.cur_iter == expected_iteration\n\n        # Run model with overflows to decrease scale\n        overflow_gradients = [float(\'inf\')]\n        expected_iteration += len(overflow_gradients)\n        run_model_step(model, overflow_gradients)\n        expected_loss_scale /= (2**len(overflow_gradients))\n        assert optim.cur_scale == expected_loss_scale\n        assert optim.cur_iter == expected_iteration\n\n    _test_fused_some_overflow(args)\n\n\ndef test_unfused_no_overflow(tmpdir):\n    config_dict = {\n        ""train_batch_size"": 1,\n        ""steps_per_print"": 1,\n        ""optimizer"": {\n            ""type"": ""Lamb"",\n            ""params"": {\n                ""lr"": 0.00015\n            }\n        },\n        ""fp16"": {\n            ""enabled"": True,\n            ""loss_scale"": 0,\n            ""initial_scale_power"": 8,\n            ""loss_scale_window"": 2\n        }\n    }\n    args = args_from_dict(tmpdir, config_dict)\n\n    @distributed_test(world_size=1)\n    def _test_unfused_no_overflow(args):\n        hidden_dim = 1\n        model = SimpleModel(hidden_dim, empty_grad=True)\n        model, optim, _, _ = deepspeed.initialize(args=args,\n                                                  model=model,\n                                                  model_parameters=model.parameters())\n\n        expected_loss_scale = 2**8\n        expected_scale_window = 2\n        # Ensure the dynamic loss scaler is correctly configured.\n        assert optim.dynamic_loss_scale == True\n        assert optim.cur_scale == expected_loss_scale\n        assert optim.scale_window == expected_scale_window\n\n        for i, value in enumerate(np.random.uniform(-0.1, 0.1, 10)):\n            run_model_step(model, [value])\n            assert optim.cur_scale == expected_loss_scale\n            assert optim.cur_iter == (i + 1)\n            if optim.cur_iter % expected_scale_window == 0:\n                expected_loss_scale *= 2\n\n    _test_unfused_no_overflow(args)\n\n\ndef test_unfused_all_overflow(tmpdir):\n    config_dict = {\n        ""train_batch_size"": 1,\n        ""steps_per_print"": 1,\n        ""optimizer"": {\n            ""type"": ""Lamb"",\n            ""params"": {\n                ""lr"": 0.00015\n            }\n        },\n        ""fp16"": {\n            ""enabled"": True,\n            ""loss_scale"": 0,\n            ""initial_scale_power"": 4,\n            ""loss_scale_window"": 2,\n            ""min_loss_scale"": 0.25\n        }\n    }\n    args = args_from_dict(tmpdir, config_dict)\n\n    @distributed_test(world_size=1)\n    def _test_unfused_all_overflow(args):\n        hidden_dim = 1\n        model = SimpleModel(hidden_dim, empty_grad=True)\n        model, optim, _, _ = deepspeed.initialize(args=args,\n                                                  model=model,\n                                                  model_parameters=model.parameters())\n\n        expected_loss_scale = 2**4\n        expected_min_loss_scale = 0.25\n        # Ensure the dynamic loss scaler is correctly configured.\n        assert optim.dynamic_loss_scale == True\n        assert optim.cur_scale == expected_loss_scale\n        assert optim.min_loss_scale == expected_min_loss_scale\n\n        overflow_gradients = [float(\'inf\'), float(\'-inf\')] + [float(\'nan\')] * 6\n        for i, value in enumerate(overflow_gradients):\n            run_model_step(model, [value])\n            expected_loss_scale = max(expected_loss_scale / 2, expected_min_loss_scale)\n            assert optim.cur_scale == expected_loss_scale\n            assert optim.cur_iter == (i + 1)\n\n    _test_unfused_all_overflow(args)\n\n\ndef test_unfused_some_overflow(tmpdir):\n    config_dict = {\n        ""train_batch_size"": 1,\n        ""steps_per_print"": 1,\n        ""optimizer"": {\n            ""type"": ""Lamb"",\n            ""params"": {\n                ""lr"": 0.00015\n            }\n        },\n        ""fp16"": {\n            ""enabled"": True,\n            ""loss_scale"": 0,\n            ""initial_scale_power"": 8,\n            ""loss_scale_window"": 2\n        }\n    }\n    args = args_from_dict(tmpdir, config_dict)\n\n    @distributed_test(world_size=1)\n    def _test_unfused_some_overflow(args):\n        hidden_dim = 1\n        model = SimpleModel(hidden_dim, empty_grad=True)\n        model, optim, _, _ = deepspeed.initialize(args=args,\n                                                  model=model,\n                                                  model_parameters=model.parameters())\n\n        expected_loss_scale = 2**8\n        expected_scale_window = 2\n        expected_iteration = 0\n        # Ensure the dynamic loss scaler is correctly configured.\n        assert optim.dynamic_loss_scale == True\n        assert optim.cur_scale == expected_loss_scale\n        assert optim.scale_window == expected_scale_window\n\n        # Run model with overflows to decrease scale\n        overflow_gradients = [float(\'inf\'), float(\'nan\')]\n        expected_iteration += len(overflow_gradients)\n        run_model_step(model, overflow_gradients)\n        expected_loss_scale /= (2**len(overflow_gradients))\n        assert optim.cur_scale == expected_loss_scale\n        assert optim.cur_iter == expected_iteration\n\n        # Run model scale_window + 1 times to increase scale once\n        normal_gradients = np.random.uniform(-0.1, 0.1, expected_scale_window + 1)\n        expected_iteration += len(normal_gradients)\n        run_model_step(model, normal_gradients)\n        expected_loss_scale *= 2\n        assert optim.cur_scale == expected_loss_scale\n        assert optim.cur_iter == expected_iteration\n\n        # Run model with overflows to decrease scale\n        overflow_gradients = [float(\'inf\')]\n        expected_iteration += len(overflow_gradients)\n        run_model_step(model, overflow_gradients)\n        expected_loss_scale /= (2**len(overflow_gradients))\n        assert optim.cur_scale == expected_loss_scale\n        assert optim.cur_iter == expected_iteration\n\n    _test_unfused_some_overflow(args)\n'"
tests/unit/test_fp16.py,2,"b'import torch\nimport deepspeed\nimport argparse\nimport pytest\nimport json\nimport os\nfrom common import distributed_test\nfrom simple_model import SimpleModel, SimpleOptimizer, random_dataloader, args_from_dict\n\n\ndef test_lamb_fp32_grad_clip(tmpdir):\n    config_dict = {\n        ""train_batch_size"": 2,\n        ""steps_per_print"": 1,\n        ""optimizer"": {\n            ""type"": ""Lamb"",\n            ""params"": {\n                ""lr"": 0.00015\n            }\n        },\n        ""gradient_clipping"": 1.0\n    }\n    args = args_from_dict(tmpdir, config_dict)\n    hidden_dim = 10\n\n    model = SimpleModel(hidden_dim, empty_grad=False)\n\n    @distributed_test(world_size=[1, 2])\n    def _test_lamb_fp32_grad_clip(args, model, hidden_dim):\n        model, _, _,_ = deepspeed.initialize(args=args,\n                                             model=model,\n                                             model_parameters=model.parameters())\n        data_loader = random_dataloader(model=model,\n                                        total_samples=50,\n                                        hidden_dim=hidden_dim,\n                                        device=model.device)\n        for n, batch in enumerate(data_loader):\n            loss = model(batch[0].float(), batch[1])\n            model.backward(loss)\n            model.step()\n\n    _test_lamb_fp32_grad_clip(args=args, model=model, hidden_dim=hidden_dim)\n\n\ndef test_lamb_fp16_basic(tmpdir):\n    config_dict = {\n        ""train_batch_size"": 2,\n        ""steps_per_print"": 1,\n        ""optimizer"": {\n            ""type"": ""Lamb"",\n            ""params"": {\n                ""lr"": 0.00015\n            }\n        },\n        ""gradient_clipping"": 1.0,\n        ""fp16"": {\n            ""enabled"": True\n        }\n    }\n    args = args_from_dict(tmpdir, config_dict)\n    hidden_dim = 10\n\n    model = SimpleModel(hidden_dim, empty_grad=False)\n\n    @distributed_test(world_size=[1, 2])\n    def _test_lamb_fp16_basic(args, model, hidden_dim):\n        model, _, _,_ = deepspeed.initialize(args=args,\n                                             model=model,\n                                             model_parameters=model.parameters())\n        data_loader = random_dataloader(model=model,\n                                        total_samples=50,\n                                        hidden_dim=hidden_dim,\n                                        device=model.device)\n        for n, batch in enumerate(data_loader):\n            loss = model(batch[0], batch[1])\n            model.backward(loss)\n            model.step()\n\n    _test_lamb_fp16_basic(args=args, model=model, hidden_dim=hidden_dim)\n\n\ndef test_lamb_fp16_empty_grad(tmpdir):\n    config_dict = {\n        ""train_batch_size"": 1,\n        ""steps_per_print"": 1,\n        ""optimizer"": {\n            ""type"": ""Lamb"",\n            ""params"": {\n                ""lr"": 0.00015\n            }\n        },\n        ""gradient_clipping"": 1.0,\n        ""fp16"": {\n            ""enabled"": True\n        }\n    }\n    args = args_from_dict(tmpdir, config_dict)\n    hidden_dim = 10\n\n    model = SimpleModel(hidden_dim, empty_grad=True)\n\n    @distributed_test(world_size=[1])\n    def _test_lamb_fp16_empty_grad(args, model, hidden_dim):\n        model, _, _,_ = deepspeed.initialize(args=args,\n                                             model=model,\n                                             model_parameters=model.parameters())\n        data_loader = random_dataloader(model=model,\n                                        total_samples=50,\n                                        hidden_dim=hidden_dim,\n                                        device=model.device)\n        for n, batch in enumerate(data_loader):\n            loss = model(batch[0], batch[1])\n            model.backward(loss)\n            model.step()\n\n    _test_lamb_fp16_empty_grad(args=args, model=model, hidden_dim=hidden_dim)\n\n\ndef test_adamw_fp16_basic(tmpdir):\n    config_dict = {\n        ""train_batch_size"": 1,\n        ""steps_per_print"": 1,\n        ""fp16"": {\n            ""enabled"": True\n        }\n    }\n    args = args_from_dict(tmpdir, config_dict)\n    hidden_dim = 10\n\n    model = SimpleModel(hidden_dim, empty_grad=False)\n\n    @distributed_test(world_size=[1])\n    def _test_adamw_fp16_basic(args, model, hidden_dim):\n        optimizer = torch.optim.AdamW(params=model.parameters())\n        model, _, _,_ = deepspeed.initialize(args=args,\n                                             model=model,\n                                             optimizer=optimizer)\n        data_loader = random_dataloader(model=model,\n                                        total_samples=50,\n                                        hidden_dim=hidden_dim,\n                                        device=model.device)\n        for n, batch in enumerate(data_loader):\n            loss = model(batch[0], batch[1])\n            model.backward(loss)\n            model.step()\n\n    _test_adamw_fp16_basic(args=args, model=model, hidden_dim=hidden_dim)\n\n\ndef test_adamw_fp16_empty_grad(tmpdir):\n    config_dict = {\n        ""train_batch_size"": 1,\n        ""steps_per_print"": 1,\n        ""fp16"": {\n            ""enabled"": True\n        }\n    }\n    args = args_from_dict(tmpdir, config_dict)\n    hidden_dim = 10\n\n    model = SimpleModel(hidden_dim, empty_grad=True)\n\n    @distributed_test(world_size=[1])\n    def _test_adamw_fp16_empty_grad(args, model, hidden_dim):\n        optimizer = torch.optim.AdamW(params=model.parameters())\n        model, _, _,_ = deepspeed.initialize(args=args,\n                                             model=model,\n                                             optimizer=optimizer)\n        data_loader = random_dataloader(model=model,\n                                        total_samples=50,\n                                        hidden_dim=hidden_dim,\n                                        device=model.device)\n        for n, batch in enumerate(data_loader):\n            loss = model(batch[0], batch[1])\n            model.backward(loss)\n            model.step()\n\n    _test_adamw_fp16_empty_grad(args=args, model=model, hidden_dim=hidden_dim)\n\n\n@pytest.mark.parametrize(""zero_stage"", [0, 1, 2])\ndef test_adam_fp16_zero_onecycle_compatibility(tmpdir, zero_stage):\n    config_dict = {\n        ""train_batch_size"": 1,\n        ""steps_per_print"": 1,\n        ""optimizer"": {\n            ""type"": ""Adam"",\n            ""params"": {\n                ""lr"": 0.00015\n            }\n        },\n        ""scheduler"": {\n            ""type"": ""OneCycle"",\n            ""params"": {\n                ""cycle_first_step_size"": 16000,\n                ""cycle_first_stair_count"": 8000,\n                ""decay_step_size"": 16000,\n                ""cycle_min_lr"": 1e-06,\n                ""cycle_max_lr"": 3e-05,\n                ""decay_lr_rate"": 1e-07,\n                ""cycle_min_mom"": 0.85,\n                ""cycle_max_mom"": 0.99,\n                ""decay_mom_rate"": 0.0\n            }\n        },\n        ""fp16"": {\n            ""enabled"": True\n        },\n        ""zero_optimization"": {\n            ""stage"": zero_stage\n        }\n    }\n\n    args = args_from_dict(tmpdir, config_dict)\n    hidden_dim = 10\n\n    model = SimpleModel(hidden_dim, empty_grad=True)\n\n    @distributed_test(world_size=[1])\n    def _test_adam_fp16_zero_onecycle_compatibility(args, model, hidden_dim):\n        model, _, _,_ = deepspeed.initialize(args=args,\n                                             model=model,\n                                             model_parameters=model.parameters())\n        data_loader = random_dataloader(model=model,\n                                        total_samples=50,\n                                        hidden_dim=hidden_dim,\n                                        device=model.device)\n        for n, batch in enumerate(data_loader):\n            loss = model(batch[0], batch[1])\n            model.backward(loss)\n            model.step()\n\n    _test_adam_fp16_zero_onecycle_compatibility(args=args,\n                                                model=model,\n                                                hidden_dim=hidden_dim)\n\n\n@pytest.mark.parametrize(""zero_stage"", [1, 2])\ndef test_zero_static_scale(tmpdir, zero_stage):\n    config_dict = {\n        ""train_batch_size"": 4,\n        ""steps_per_print"": 1,\n        ""optimizer"": {\n            ""type"": ""Adam"",\n            ""params"": {\n                ""lr"": 0.00015\n            }\n        },\n        ""fp16"": {\n            ""enabled"": True,\n            ""loss_scale"": 138.\n        },\n        ""zero_optimization"": {\n            ""stage"": zero_stage\n        }\n    }\n    args = args_from_dict(tmpdir, config_dict)\n\n    @distributed_test(world_size=2)\n    def _test_zero_static_scale(args):\n        hidden_dim = 10\n        model = SimpleModel(hidden_dim, empty_grad=True)\n        model, optim, _,_ = deepspeed.initialize(args=args,\n                                            model=model,\n                                            model_parameters=model.parameters())\n\n        # Ensure the static scaler is configured.\n        assert optim.dynamic_loss_scale == False\n        assert optim.loss_scaler.loss_scale == 138.\n\n        # Now make sure things work..\n        data_loader = random_dataloader(model=model,\n                                        total_samples=10,\n                                        hidden_dim=hidden_dim,\n                                        device=model.device)\n        for n, batch in enumerate(data_loader):\n            loss = model(batch[0], batch[1])\n            model.backward(loss)\n            model.step()\n\n    _test_zero_static_scale(args)\n\n\ndef test_zero_static_scale_deprecated_format(tmpdir):\n    config_dict = {\n        ""train_batch_size"": 4,\n        ""steps_per_print"": 1,\n        ""optimizer"": {\n            ""type"": ""Adam"",\n            ""params"": {\n                ""lr"": 0.00015\n            }\n        },\n        ""fp16"": {\n            ""enabled"": True,\n            ""loss_scale"": 138.\n        },\n        ""zero_optimization"": True\n    }\n    args = args_from_dict(tmpdir, config_dict)\n\n    @distributed_test(world_size=2)\n    def _test_zero_static_scale(args):\n        hidden_dim = 10\n        model = SimpleModel(hidden_dim, empty_grad=True)\n        model, optim, _,_ = deepspeed.initialize(args=args,\n                                            model=model,\n                                            model_parameters=model.parameters())\n\n        # Ensure the static scaler is configured.\n        assert optim.dynamic_loss_scale == False\n        assert optim.loss_scaler.loss_scale == 138.\n\n        # Now make sure things work..\n        data_loader = random_dataloader(model=model,\n                                        total_samples=10,\n                                        hidden_dim=hidden_dim,\n                                        device=model.device)\n        for n, batch in enumerate(data_loader):\n            loss = model(batch[0], batch[1])\n            model.backward(loss)\n            model.step()\n\n    _test_zero_static_scale(args)\n\n\n@pytest.mark.parametrize(""zero_stage"", [1, 2])\ndef test_zero_allow_untested_optimizer(tmpdir, zero_stage):\n    config_dict = {\n        ""train_batch_size"": 4,\n        ""steps_per_print"": 1,\n        ""fp16"": {\n            ""enabled"": True,\n        },\n        ""zero_optimization"": {\n            ""stage"": zero_stage\n        },\n        ""zero_allow_untested_optimizer"": False\n    }\n    args = args_from_dict(tmpdir, config_dict)\n\n    @distributed_test(world_size=[1])\n    def _test_zero_allow_untested_optimizer(args):\n        hidden_dim = 10\n        model = SimpleModel(hidden_dim, empty_grad=True)\n        optimizer = SimpleOptimizer(model.parameters())\n        with pytest.raises(AssertionError):\n            model, optim, _,_ = deepspeed.initialize(args=args,\n                                                    model=model,\n                                                    optimizer=optimizer,\n                                                    model_parameters=model.parameters())\n\n    _test_zero_allow_untested_optimizer(args)\n\n\n# @pytest.mark.parametrize(""zero_stage"", [1])\n# def test_zero_empty_partition(tmpdir, zero_stage):\n#     config_dict = {\n#         ""train_batch_size"": 3,\n#         ""fp16"": {\n#             ""enabled"": True\n#         },\n#         ""optimizer"": {\n#             ""type"": ""Adam"",\n#             ""params"": {\n#                 ""lr"": 0.00015\n#             }\n#         },\n#         ""zero_optimization"": {\n#             ""stage"": zero_stage\n#         }\n#     }\n#     args = args_from_dict(tmpdir, config_dict)\n\n#     @distributed_test(world_size=[3])\n#     def _test_zero_empty_partition(args):\n#         hidden_dim = 1\n#         model = SimpleModel(hidden_dim)\n#         # Ensure model has 2 parameters, to cause empty partition with DP=3\n#         assert len(list(model.parameters())) == 2\n#         model, _, _, _ = deepspeed.initialize(args=args,\n#                                               model=model,\n#                                               model_parameters=model.parameters())\n#         model.step()\n\n#     _test_zero_empty_partition(args)\n'"
tests/unit/test_multi_output_model.py,6,"b'import torch\nimport deepspeed\nimport argparse\nimport pytest\nfrom pytest import approx\nimport json\nimport os\nfrom common import distributed_test\nfrom simple_model import args_from_dict\nfrom multi_output_model import MultiOutputModel, multi_output_dataloader\n\n\ndef create_config_dict(micro_batch_size, grad_accumulation_steps, world_size):\n    return {\n        ""train_micro_batch_size_per_gpu"": micro_batch_size,\n        ""gradient_accumulation_steps"": grad_accumulation_steps,\n        ""train_batch_size"": micro_batch_size * grad_accumulation_steps * world_size,\n        ""steps_per_print"": 1,\n        ""optimizer"": {\n            ""type"": ""Adam"",\n            ""params"": {\n                ""lr"": 0.00015\n            }\n        },\n        ""fp16"": {\n            ""enabled"": True\n        }\n    }\n\n\ndef test_two_output_model(tmpdir):\n    gradient_accumulation_steps = 2\n    micro_batch_size = 1\n    world_size = 1\n    config_dict = create_config_dict(micro_batch_size,\n                                     gradient_accumulation_steps,\n                                     world_size)\n\n    hidden_dim = 10\n    weight_value = 0.1\n    args = args_from_dict(tmpdir, config_dict)\n\n    model = MultiOutputModel(hidden_dim, weight_value)\n\n    @distributed_test(world_size=[1])\n    def _test_two_output_model(args, model, hidden_dim):\n        model, _, _, _ = deepspeed.initialize(args=args,\n                                              model=model,\n                                              model_parameters=model.parameters())\n        total_samples = 4\n        data_loader = multi_output_dataloader(model=model,\n                                              total_samples=total_samples,\n                                              hidden_dim=hidden_dim,\n                                              device=model.device,\n                                              inputs=[1.0,\n                                                      2.0],\n                                              targets=[1,\n                                                       2])\n        for n, batch in enumerate(data_loader):\n            assert len(batch) % 2 == 0, \\\n                 f""multi_output_dataloader failed to return even number of data samples (input+target)""\n\n            midpoint = len(batch) // 2\n            inputs, targets = batch[:midpoint], batch[midpoint:]\n            loss_tuple = model(inputs, targets)\n\n            expected_loss = torch.tensor(2.302734375,\n                                         dtype=torch.half,\n                                         device=model.device)\n            for loss in loss_tuple:\n                assert loss.shape == torch.Size([])\n                assert loss.item() == approx(expected_loss.item())\n\n            summed_loss = sum(loss_tuple)\n            scaled_loss = model.backward(summed_loss)\n            expected_scaled_loss = summed_loss.float() / gradient_accumulation_steps\n            assert scaled_loss.item() == approx(expected_scaled_loss.item())\n\n            model.step()\n\n    _test_two_output_model(args=args, model=model, hidden_dim=hidden_dim)\n\n\ndef test_three_output_model(tmpdir):\n    gradient_accumulation_steps = 3\n    micro_batch_size = 1\n    world_size = 1\n    config_dict = create_config_dict(micro_batch_size,\n                                     gradient_accumulation_steps,\n                                     world_size)\n\n    hidden_dim = 10\n    weight_value = 0.1\n    args = args_from_dict(tmpdir, config_dict)\n\n    model = MultiOutputModel(hidden_dim, weight_value)\n\n    @distributed_test(world_size=[1])\n    def _test_three_output_model(args, model, hidden_dim):\n        model, _, _, _ = deepspeed.initialize(args=args,\n                                              model=model,\n                                              model_parameters=model.parameters())\n\n        total_samples = gradient_accumulation_steps * micro_batch_size * 2\n        data_loader = multi_output_dataloader(model=model,\n                                              total_samples=total_samples,\n                                              hidden_dim=hidden_dim,\n                                              device=model.device,\n                                              inputs=[1.0,\n                                                      2.0,\n                                                      3.0],\n                                              targets=[1,\n                                                       2,\n                                                       3])\n        for n, batch in enumerate(data_loader):\n            assert len(batch) % 2 == 0, \\\n                 f""multi_output_dataloader failed to return even number of data samples (input+target)""\n\n            midpoint = len(batch) // 2\n            inputs, targets = batch[:midpoint], batch[midpoint:]\n            loss_tuple = model(inputs, targets)\n            assert len(loss_tuple) == 3\n\n            expected_loss = torch.tensor(2.302734375,\n                                         dtype=torch.half,\n                                         device=model.device)\n\n            for loss in loss_tuple:\n                assert loss.shape == torch.Size([])\n                assert loss.item() == approx(expected_loss.item())\n\n            summed_loss = sum(loss_tuple)\n            scaled_loss = model.backward(summed_loss)\n            expected_scaled_loss = summed_loss.float() / gradient_accumulation_steps\n            assert scaled_loss.item() == approx(expected_scaled_loss.item())\n\n            model.step()\n\n    _test_three_output_model(args=args, model=model, hidden_dim=hidden_dim)\n'"
tests/unit/test_run.py,0,"b'import pytest\n\nfrom deepspeed.pt import deepspeed_run as dsrun\n\n\ndef test_parser_mutual_exclusive():\n    \'\'\'Ensure dsrun.parse_resource_filter() raises a ValueError when include_str and\n    exclude_str are both provided.\n    \'\'\'\n    with pytest.raises(ValueError):\n        dsrun.parse_resource_filter({}, include_str=\'A\', exclude_str=\'B\')\n\n\ndef test_parser_local():\n    \'\'\' Test cases with only one node. \'\'\'\n    # First try no incude/exclude\n    hosts = {\'worker-0\': [0, 1, 2, 3]}\n    ret = dsrun.parse_resource_filter(hosts)\n    assert (ret == hosts)\n\n    # exclude slots\n    ret = dsrun.parse_resource_filter(hosts, exclude_str=\'worker-0:1\')\n    assert (ret == {\'worker-0\': [0, 2, 3]})\n\n    ret = dsrun.parse_resource_filter(hosts, exclude_str=\'worker-0:1,2\')\n    assert (ret == {\'worker-0\': [0, 3]})\n\n    # only use one slot\n    ret = dsrun.parse_resource_filter(hosts, include_str=\'worker-0:1\')\n    assert (ret == {\'worker-0\': [1]})\n\n    # including slots multiple times shouldn\'t break things\n    ret = dsrun.parse_resource_filter(hosts, include_str=\'worker-0:1,1\')\n    assert (ret == {\'worker-0\': [1]})\n    ret = dsrun.parse_resource_filter(hosts, include_str=\'worker-0:1@worker-0:0,1\')\n    assert (ret == {\'worker-0\': [0, 1]})\n\n    # including just \'worker-0\' without : should still use all GPUs\n    ret = dsrun.parse_resource_filter(hosts, include_str=\'worker-0\')\n    assert (ret == hosts)\n\n    # excluding just \'worker-0\' without : should eliminate everything\n    ret = dsrun.parse_resource_filter(hosts, exclude_str=\'worker-0\')\n    assert (ret == {})\n\n    # exclude all slots manually\n    ret = dsrun.parse_resource_filter(hosts, exclude_str=\'worker-0:0,1,2,3\')\n    assert (ret == {})\n\n\ndef test_parser_multinode():\n    # First try no incude/exclude\n    hosts = {\'worker-0\': [0, 1, 2, 3], \'worker-1\': [0, 1, 2, 3]}\n    ret = dsrun.parse_resource_filter(hosts)\n    assert (ret == hosts)\n\n    # include a node\n    ret = dsrun.parse_resource_filter(hosts, include_str=\'worker-1:0,3\')\n    assert (ret == {\'worker-1\': [0, 3]})\n\n    # exclude a node\n    ret = dsrun.parse_resource_filter(hosts, exclude_str=\'worker-1\')\n    assert (ret == {\'worker-0\': [0, 1, 2, 3]})\n\n    # exclude part of each node\n    ret = dsrun.parse_resource_filter(hosts, exclude_str=\'worker-0:0,1@worker-1:3\')\n    assert (ret == {\'worker-0\': [2, 3], \'worker-1\': [0, 1, 2]})\n\n\ndef test_parser_errors():\n    \'\'\'Ensure we catch errors. \'\'\'\n    hosts = {\'worker-0\': [0, 1, 2, 3], \'worker-1\': [0, 1, 2, 3]}\n\n    # host does not exist\n    with pytest.raises(ValueError):\n        dsrun.parse_resource_filter(hosts, include_str=\'jeff\')\n    with pytest.raises(ValueError):\n        dsrun.parse_resource_filter(hosts, exclude_str=\'jeff\')\n\n    # slot does not exist\n    with pytest.raises(ValueError):\n        dsrun.parse_resource_filter(hosts, include_str=\'worker-1:4\')\n    with pytest.raises(ValueError):\n        dsrun.parse_resource_filter(hosts, exclude_str=\'worker-1:4\')\n\n    # formatting\n    with pytest.raises(ValueError):\n        dsrun.parse_resource_filter(hosts, exclude_str=\'worker-1@worker-0:1@5\')\n\n\ndef test_num_plus_parser():\n    \'\'\' Ensure we catch errors relating to num_nodes/num_gpus + -i/-e being mutually exclusive\'\'\'\n\n    # inclusion\n    with pytest.raises(ValueError):\n        dsrun.main(args=""--num_nodes 1 -i localhost foo.py"".split())\n    with pytest.raises(ValueError):\n        dsrun.main(args=""--num_nodes 1 --num_gpus 1 -i localhost foo.py"".split())\n    with pytest.raises(ValueError):\n        dsrun.main(args=""--num_gpus 1 -i localhost foo.py"".split())\n\n    # exclusion\n    with pytest.raises(ValueError):\n        dsrun.main(args=""--num_nodes 1 -e localhost foo.py"".split())\n    with pytest.raises(ValueError):\n        dsrun.main(args=""--num_nodes 1 --num_gpus 1 -e localhost foo.py"".split())\n    with pytest.raises(ValueError):\n        dsrun.main(args=""--num_gpus 1 -e localhost foo.py"".split())\n'"
docs/code-docs/source/conf.py,0,"b'# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n\nimport os\nimport sys\n\n# -- Project information -----------------------------------------------------\n\nproject = \'DeepSpeed\'\ncopyright = \'2020, Microsoft\'\nauthor = \'Microsoft\'\n\n# The full version, including alpha/beta/rc tags\nrelease = \'0.1.0\'\n\nmaster_doc = \'index\'\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.napoleon\',\n    \'recommonmark\',\n    \'sphinx_rtd_theme\',\n]\n\npygments_style = \'sphinx\'\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = []\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# GitHub integration\nhtml_context = {\n    ""display_github"": True,\n    ""github_user"": ""microsoft"",\n    ""github_repo"": ""DeepSpeed"",\n    ""github_version"": ""master"",\n    ""conf_py_path"": ""/docs/code-docs/source/"",\n}\n\n# Mock imports so we don\'t have to install torch to build the docs.\nfrom unittest.mock import MagicMock\nsys.path.insert(0, os.path.abspath(\'../../../\'))\n\n# Prepend module names to class descriptions?\nadd_module_names = True\n\nautoclass_content = \'both\'\n\nautodoc_mock_imports = [\n    ""torch"",\n    ""apex"",\n    ""mpi4py"",\n    ""tensorboardX"",\n    ""deepspeed_transformer_cuda"",\n    ""deepspeed_stochastic_transformer_cuda"",\n]\n'"
tests/model/BingBertSquad/BingBertSquad_run_func_test.py,0,"b'# coding=utf-8\n# Copyright (c) 2019, The Microsoft DeepSpeed Team. All rights reserved.\n#\n# Note: please copy webtext data to ""Megatron-LM"" folder, before running this script.\n\nimport unittest\nimport subprocess\nimport os\nimport time\nimport re\nfrom .BingBertSquad_test_common import BaseTestCase\n\n\ndef grep_loss_from_file(file_name):\n    loss = 0.0\n\n    with open(file_name, \'r\') as f:\n        lines = f.readlines()\n        line_filter = ""bert_squad_progress: step=""\n        match_number = re.compile(\'loss=([-+]?[0-9]+\\.?[0-9]*(?:[Ee][-+]?[0-9]+)?)\')\n\n        for line in lines:\n            if line_filter in line:\n                loss = re.findall(match_number, line)\n                loss = float(loss[0])\n\n    if loss == 0.0:\n        print(""no loss found in file "", file_name)\n\n    return loss\n\n\nclass BingBertSquadFuncTestCase(BaseTestCase):\n    def __init__(self, methodName=""DeepSpeed function test on BingBertSquad model""):\n        super(BingBertSquadFuncTestCase, self).__init__(methodName)\n\n    def setUp(self):\n        self.save_dir = os.getcwd()\n        new_dir = os.path.dirname(__file__)\n        if new_dir:\n            os.chdir(new_dir)\n\n    def tearDown(self):\n        os.chdir(self.save_dir)\n\n    def test_gpu4_fp16(self):\n        test_config = {\n            ""gpus"": 4,\n            ""deepspeed"": False,\n            ""json"": ""deepspeed_bsz24_fp16_config.json"",\n            ""max_steps"": 8,\n            ""max_epoch_steps"": 4,\n            ""other_args"": ""--fp16 --print_steps 1""\n        }\n\n        succ = self.run_test(test_config, 0.01)\n        self.assertTrue(succ)\n\n    def test_gpu4_fp16_zero2(self):\n        test_config = {\n            ""gpus"": 4,\n            ""deepspeed"": False,\n            ""json"": ""deepspeed_bsz24_fp16_zero2_config.json"",\n            ""max_steps"": 8,\n            ""max_epoch_steps"": 4,\n            ""other_args"": ""--fp16 --print_steps 1""\n        }\n\n        succ = self.run_test(test_config, 0.01)\n        self.assertTrue(succ)\n\n    def test_gpu1_fp16(self):\n        test_config = {\n            ""gpus"": 1,\n            ""deepspeed"": False,\n            ""json"": ""deepspeed_bsz24_fp16_config.json"",\n            ""max_steps"": 8,\n            ""max_epoch_steps"": 4,\n            ""other_args"": ""--fp16 --print_steps 1""\n        }\n\n        succ = self.run_test(test_config, 0.01)\n        self.assertTrue(succ)\n\n    def test_gpu4_fp32(self):\n        test_config = {\n            ""gpus"": 4,\n            ""deepspeed"": False,\n            ""json"": ""deepspeed_bsz24_fp32_config.json"",\n            ""max_steps"": 8,\n            ""max_epoch_steps"": 4,\n            ""other_args"": ""--print_steps 1""\n        }\n\n        succ = self.run_test(test_config, 0.01)\n        self.assertTrue(succ)\n\n    def test_gpu1_fp32(self):\n        test_config = {\n            ""gpus"": 1,\n            ""deepspeed"": False,\n            ""json"": ""deepspeed_bsz24_fp32_config.json"",\n            ""max_steps"": 8,\n            ""max_epoch_steps"": 4,\n            ""other_args"": ""--print_steps 1""\n        }\n\n        succ = self.run_test(test_config, 0.01)\n        self.assertTrue(succ)\n\n    def run_test(self, test_config, r_tol):\n        print(""\\n"")\n        print(""{0}: starting......"".format(self.id()))\n\n        prefix = ""BingBertSquad_func""\n\n        test_config[\'other_args\'] += f"" --max_steps {test_config[\'max_steps\']}""\n        test_config[\n            \'other_args\'] += f"" --max_steps_per_epoch {test_config[\'max_epoch_steps\']}""\n\n        # baseline run...\n        test_config[""deepspeed""] = False\n        base_file = self.gen_output_name(test_config, prefix)\n\n        # skip baseline run if it exists.\n        if not self.has_loss_data(base_file):\n            print(""{0}: baseline run."".format(self.id()))\n            self.run_BingBertSquad_test(test_config, base_file)\n        else:\n            print(""{0}: baseline exists."".format(self.id()))\n\n        # DeepSpeed run...\n        test_config[""deepspeed""] = True\n        print(""{0}: DeepSpeed run."".format(self.id()))\n        test_file = self.gen_output_name(test_config, prefix)\n        self.run_BingBertSquad_test(test_config, test_file)\n\n        return self.check_parity(base_file, test_file, r_tol)\n\n    def has_loss_data(self, file_name):\n        has_loss = False\n        if os.path.exists(file_name):\n            loss = grep_loss_from_file(file_name)\n            if loss != 0.0:\n                has_loss = True\n\n        return has_loss\n\n    def check_parity(self, base_file, test_file, r_tol):\n        base_loss = grep_loss_from_file(base_file)\n        test_loss = grep_loss_from_file(test_file)\n\n        print(""baseline loss: {0}, test loss: {1}"".format(base_loss, test_loss))\n\n        if base_loss == 0.0 or test_loss == 0.0:\n            return False\n\n        if abs((base_loss - test_loss) / base_loss) > r_tol:\n            return False\n\n        return True\n\n\ndef suite():\n    suite = unittest.TestSuite()\n    suite.addTest(BingBertSquadFuncTestCase(\'test_gpu4_fp16\'))\n    suite.addTest(BingBertSquadFuncTestCase(\'test_gpu4_fp16_zero2\'))\n    suite.addTest(BingBertSquadFuncTestCase(\'test_gpu1_fp16\'))\n    suite.addTest(BingBertSquadFuncTestCase(\'test_gpu4_fp32\'))\n    suite.addTest(BingBertSquadFuncTestCase(\'test_gpu1_fp32\'))\n    return suite\n\n\nif __name__ == \'__main__\':\n    runner = unittest.TextTestRunner(failfast=True)\n    runner.run(suite())\n'"
tests/model/BingBertSquad/BingBertSquad_test_common.py,0,"b'# coding=utf-8\n# Copyright (c) 2019, The Microsoft DeepSpeed Team. All rights reserved.\n#\n\nimport unittest\nimport subprocess\nimport os\nimport time\nimport re\n\n\nclass BaseTestCase(unittest.TestCase):\n    def __init__(self, methodName=""DeepSpeed performance test""):\n        super(BaseTestCase, self).__init__(methodName)\n        self.test_dir = ""./test""\n        self.baseline_dir = ""./baseline""\n        self.timestr = time.strftime(""%Y%m%d-%H%M%S"")\n\n    def gen_output_name(self, test_config, prefix):\n        other_args = test_config[""other_args""] if ""other_args"" in test_config else """"\n        zero_args = ""_zero"" if ""zero"" in test_config and test_config[""zero""] else """"\n        other_args = other_args.strip(\' -\\\\\').replace("" "", """").replace(""\\"""", """")\n\n        if other_args:\n            other_args = ""_"" + other_args\n\n        if test_config[""deepspeed""]:\n            file_name = ""_gpu{0}_{1}_ds{2}-{3}.log"".format(test_config[""gpus""],\n                                                           other_args,\n                                                           zero_args,\n                                                           self.timestr)\n            save_dir = self.test_dir\n        else:\n            file_name = ""_gpu{0}_{1}.log"".format(test_config[""gpus""], other_args)\n            save_dir = self.baseline_dir\n\n        return os.path.join(save_dir, prefix + file_name)\n\n    def ensure_directory_exists(self, filename):\n        dirname = os.path.dirname(filename)\n        if not os.path.exists(dirname):\n            os.makedirs(dirname)\n\n    def clean_test_env(self):\n        cmd = ""dlts_ssh pkill -9 -f /usr/bin/python""\n        print(cmd)\n        subprocess.run(cmd, shell=True, check=False, executable=\'/bin/bash\')\n        time.sleep(20)\n\n    def run_BingBertSquad_test(self, test_config, output):\n        ds_flag = "" -d --deepspeed_config "" + test_config[""json""] if test_config[\n            ""deepspeed""] else "" ""\n        other_args = "" "" + test_config[\n            ""other_args""] if ""other_args"" in test_config else "" ""\n\n        cmd = ""./run_BingBertSquad_sanity.sh -e 1 -g {0} {1} {2}"".format(\n            test_config[""gpus""],\n            other_args,\n            ds_flag)\n\n        self.ensure_directory_exists(output)\n        with open(output, ""w"") as f:\n            print(cmd)\n            subprocess.run(cmd,\n                           shell=True,\n                           check=False,\n                           executable=\'/bin/bash\',\n                           stdout=f,\n                           stderr=f)\n'"
tests/model/BingBertSquad/__init__.py,0,"b'# coding=utf-8\n# Copyright (c) 2019, The Microsoft DeepSpeed Team. All rights reserved.\n\nfrom .BingBertSquad_run_func_test import BingBertSquadFuncTestCase\nfrom .BingBertSquad_run_func_test import suite\n'"
tests/model/BingBertSquad/test_e2e_squad.py,0,"b'import subprocess as sp\nimport datetime\nimport os\nfrom math import isclose\nimport sys\nimport pytest\nimport json\nimport argparse\n\nsys.path.append(""../../../DeepSpeedExamples/BingBertSquad"")\nimport evaluate as eval\n\nsquad_dir = ""/data/BingBertSquad""\nbase_dir = ""../../../DeepSpeedExamples/BingBertSquad""\n\nscript_file_name = ""run_squad_deepspeed.sh""\nmodel_file_name = ""training_state_checkpoint_162.tar""\neval_file_name = ""dev-v1.1.json""\npred_file_name = ""predictions.json""\n\nnum_gpus = ""4""\ntimeout_sec = 5 * 60 * 60  # 5 hours\n\neval_version = ""1.1""\n\n\ndef create_config_file(tmpdir, zeroenabled=False):\n    config_dict = {\n        ""train_batch_size"": 24,\n        ""train_micro_batch_size_per_gpu"": 6,\n        ""steps_per_print"": 10,\n        ""optimizer"": {\n            ""type"": ""Adam"",\n            ""params"": {\n                ""lr"": 3e-5,\n                ""weight_decay"": 0.0,\n                ""bias_correction"": False\n            }\n        },\n        ""gradient_clipping"": 1.0,\n        ""fp16"": {\n            ""enabled"": True\n        }\n    }\n    config_dict[""zero_optimization""] = zeroenabled\n\n    config_path = os.path.join(tmpdir, \'temp_config.json\')\n    with open(config_path, \'w\') as fd:\n        json.dump(config_dict, fd)\n    return config_path\n\n\ndef test_e2e_squad_deepspeed_base(tmpdir):\n    config_file = create_config_file(tmpdir)\n\n    # base run results => {""exact_match"": 83.9829706717124, ""f1"": 90.71138132004097}\n    expected_exact_match = 83.98\n    expected_f1 = 90.71\n\n    model_file = os.path.join(squad_dir, model_file_name)\n    eval_file = os.path.join(squad_dir, eval_file_name)\n\n    output_dir = os.path.join(tmpdir, ""output"")\n    pred_file = os.path.join(output_dir, pred_file_name)\n\n    proc = sp.Popen([\n        ""bash"",\n        script_file_name,\n        num_gpus,\n        model_file,\n        squad_dir,\n        output_dir,\n        config_file\n    ],\n                    cwd=base_dir)\n\n    try:\n        proc.communicate(timeout=timeout_sec)\n\n        if os.path.exists(pred_file):\n            eval_result = eval.evaluate(eval_version, eval_file, pred_file)\n\n            print(""evaluation result: "", json.dumps(eval_result))\n\n            assert isclose(eval_result[""exact_match""],\n                           expected_exact_match,\n                           abs_tol=1e-2)\n            assert isclose(eval_result[""f1""], expected_f1, abs_tol=1e-2)\n\n        else:\n            pytest.fail(""Error: Run Failed"")\n\n    except sp.TimeoutExpired:\n        proc.kill()\n        pytest.fail(""Error: Timeout"")\n    except sp.CalledProcessError:\n        pytest.fail(""Error: Run Failed"")\n\n\ndef test_e2e_squad_deepspeed_zero(tmpdir):\n    config_file = create_config_file(tmpdir, True)\n\n    # base run results => {""exact_match"": 84.1438032166509, ""f1"": 90.89776136505441}\n    expected_exact_match = 84.14\n    expected_f1 = 90.89\n\n    model_file = os.path.join(squad_dir, model_file_name)\n    eval_file = os.path.join(squad_dir, eval_file_name)\n\n    output_dir = os.path.join(tmpdir, ""output"")\n    pred_file = os.path.join(output_dir, pred_file_name)\n\n    proc = sp.Popen([\n        ""bash"",\n        script_file_name,\n        num_gpus,\n        model_file,\n        squad_dir,\n        output_dir,\n        config_file\n    ],\n                    cwd=base_dir)\n\n    try:\n        proc.communicate(timeout=timeout_sec)\n\n        if os.path.exists(pred_file):\n            eval_result = eval.evaluate(eval_version, eval_file, pred_file)\n\n            print(""evaluation result: "", json.dumps(eval_result))\n\n            assert isclose(eval_result[""exact_match""],\n                           expected_exact_match,\n                           abs_tol=1e-2)\n            assert isclose(eval_result[""f1""], expected_f1, abs_tol=1e-2)\n\n        else:\n            pytest.fail(""Error: Run Failed"")\n\n    except sp.TimeoutExpired:\n        proc.kill()\n        pytest.fail(""Error: Timeout"")\n    except sp.CalledProcessError:\n        pytest.fail(""Error: Run Failed"")\n'"
tests/model/Megatron_GPT2/__init__.py,0,"b'# coding=utf-8\n# Copyright (c) 2019, The Microsoft DeepSpeed Team. All rights reserved.\n#\n# Note: please copy webtext data to ""Megatron-LM"" folder, before running this script.\n\nfrom .run_func_test import GPT2FuncTestCase\nfrom .run_checkpoint_test import GPT2CheckpointTestCase, checkpoint_suite\nfrom .run_func_test import suite\n'"
tests/model/Megatron_GPT2/run_checkpoint_test.py,0,"b'# coding=utf-8\n# Copyright (c) 2019, The Microsoft DeepSpeed Team. All rights reserved.\n#\n# Note: please copy webtext data to ""Megatron-LM"" folder, before running this script.\n\nimport unittest\nimport subprocess\nimport os\nimport time\nimport re\nfrom .test_common import BaseTestCase\n\nLAYERS = 2\nHIDDEN_SIZE = 128\nATTN_HEADS = 8\n\n\ndef remove_file(test_id, filename):\n    cmd = f""if [ -f {filename} ] ; then rm -v {filename}; fi""\n    print(f""{test_id} cmd: {cmd}"")\n    subprocess.run(cmd, shell=True, check=False, executable=\'/bin/bash\')\n\n\ndef grep_loss_from_file(file_name):\n    loss = 0.0\n\n    with open(file_name, \'r\') as f:\n        lines = f.readlines()\n        line_filter = ""validation loss at the end of training for test data | LM loss:""\n        match_number = re.compile(\'LM loss: ([-+]?[0-9]+\\.?[0-9]*(?:[Ee][-+]?[0-9]+)?)\')\n\n        for line in lines:\n            if line_filter in line:\n                loss = re.findall(match_number, line)\n                loss = float(loss[0])\n\n    if loss == 0.0:\n        print(""no loss found in file "", file_name)\n\n    return loss\n\n\nclass GPT2CheckpointTestCase(BaseTestCase):\n    def __init__(self, methodName=""DeepSpeed function test on GPT2 model""):\n        super(GPT2CheckpointTestCase, self).__init__(methodName)\n\n    def setUp(self):\n        self.save_dir = os.getcwd()\n        new_dir = os.path.dirname(__file__)\n        if new_dir:\n            os.chdir(new_dir)\n\n    def tearDown(self):\n        os.chdir(self.save_dir)\n\n    def test_mp4_gpu16_node1_with_zero1(self):\n        test_config = {\n            ""mp"": 2,\n            ""gpus"": 4,\n            ""nodes"": 1,\n            ""bs"": 8,\n            ""steps"": 1100,\n            ""layers"": LAYERS,\n            ""hidden_size"": HIDDEN_SIZE,\n            ""seq_length"": 256,\n            ""heads"": ATTN_HEADS,\n            ""deepspeed"": True,\n            ""tag"": ""ds_zero1"",\n            ""zero"": True,\n            ""other_args"": """",\n            ""checkpoint_name"": ""ckpt_mp4_gpu16_w_zero1"",\n            ""checkpoint_interval"": 1000,\n            ""json"": ""ds_config_func_bs8_zero1.json"",\n        }\n        succ = self.run_test(test_config, 0.01)\n        self.assertTrue(succ)\n\n    def test_mp4_gpu16_node1_with_zero2(self):\n        test_config = {\n            ""mp"": 2,\n            ""gpus"": 4,\n            ""nodes"": 1,\n            ""bs"": 8,\n            ""steps"": 1100,\n            ""layers"": LAYERS,\n            ""hidden_size"": HIDDEN_SIZE,\n            ""seq_length"": 256,\n            ""heads"": ATTN_HEADS,\n            ""deepspeed"": True,\n            ""tag"": ""ds_zero2"",\n            ""zero"": True,\n            ""other_args"": """",\n            ""checkpoint_name"": ""ckpt_mp4_gpu16_w_zero2"",\n            ""checkpoint_interval"": 1000,\n            ""json"": ""ds_config_func_bs8_zero2.json"",\n        }\n        succ = self.run_test(test_config, 0.01)\n        self.assertTrue(succ)\n\n    def test_mp4_gpu16_node1_without_zero(self):\n        test_config = {\n            ""mp"": 2,\n            ""gpus"": 4,\n            ""nodes"": 1,\n            ""bs"": 8,\n            ""steps"": 1100,\n            ""layers"": LAYERS,\n            ""hidden_size"": HIDDEN_SIZE,\n            ""seq_length"": 256,\n            ""heads"": ATTN_HEADS,\n            ""deepspeed"": True,\n            ""zero"": False,\n            ""other_args"": """",\n            ""tag"": ""ds_without_zero"",\n            ""checkpoint_name"": ""ckpt_mp4_gpu16_wo_zero"",\n            ""checkpoint_interval"": 1000,\n            ""json"": ""ds_config_func_bs8_no_zero.json"",\n        }\n        succ = self.run_test(test_config, 0.01)\n        self.assertTrue(succ)\n\n    def gen_name(self, test_config, prefix):\n        save_dir = ""checkpoint_test_logs""\n        tag = test_config[""tag""]\n        file_name = f""_{tag}.log""\n        return os.path.join(save_dir, prefix + file_name)\n\n    def run_test(self, test_config, r_tol):\n        print(""\\n"")\n\n        print(""{0}: starting......"".format(self.id()))\n\n        # save to current directory.\n        checkpoint_folder = test_config[""checkpoint_name""]\n        checkpoint_interval = test_config[""checkpoint_interval""]\n        checkpoint_name = test_config[""checkpoint_name""]\n        #---------------remove old checkpoint---------------#\n        try:\n            cmd = f""rm -rf {checkpoint_name}""\n            print(f""{self.id()} cmd: {cmd}"")\n            subprocess.run(cmd, shell=True, check=False, executable=\'/bin/bash\')\n        except:\n            print(""No old checkpoint"")\n\n        #-----------------Saving Checkpoint-----------------#\n        #building checkpoint arguments\n        test_config[\n            ""other_args""] = f""\\""--save {checkpoint_folder} --save-interval {checkpoint_interval}\\""""\n\n        prefix = ""gpt2_saving_checkpoint""\n\n        # create checkpoint run...\n        base_file = self.gen_name(test_config, prefix)\n\n        # remove previous test log\n        try:\n            cmd = f""rm {base_file}""\n            subprocess.run(cmd, shell=True, check=False, executable=\'/bin/bash\')\n        except:\n            print(f""{self.id()} No old logs"")\n\n        print(""{0}: Run for saving checkpoint"".format(self.id()))\n        self.run_gpt2_test(test_config, base_file)\n\n        #-----------------Loading Checkpoint-----------------#\n\n        #building checkpoint arguments\n        test_config[""other_args""] = f""\\""--load {checkpoint_folder}\\""""\n\n        #set checkpoint load iteration\n        try:\n            cmd = f""echo {checkpoint_interval} > {checkpoint_name}/latest_checkpointed_iteration.txt""\n            print(f""{self.id()} running cmd: {cmd}"")\n            subprocess.run(cmd, shell=True, check=False, executable=\'/bin/bash\')\n        except:\n            print(f""{self.id()} Failed to update the checkpoint iteration file"")\n            return False\n\n        prefix = ""gpt2_loading_checkpoint""\n\n        print(""{0}: Second run loading checkpoint and continuing."".format(self.id()))\n        test_file = self.gen_name(test_config, prefix)\n\n        # remove previous test log\n        try:\n            cmd = f""rm {test_file}""\n            subprocess.run(cmd, shell=True, check=False, executable=\'/bin/bash\')\n        except:\n            print(f""{self.id()} no previous logs for"")\n        self.run_gpt2_test(test_config, test_file)\n        return self.check_parity(base_file, test_file, r_tol)\n\n    def has_loss_data(self, file_name):\n        has_loss = False\n        if os.path.exists(file_name):\n            loss = grep_loss_from_file(file_name)\n            if loss != 0.0:\n                has_loss = True\n\n        return has_loss\n\n    def check_parity(self, base_file, test_file, r_tol):\n        base_loss = grep_loss_from_file(base_file)\n        test_loss = grep_loss_from_file(test_file)\n\n        print(""baseline loss: {0}, test loss: {1}"".format(base_loss, test_loss))\n\n        if base_loss == 0.0 or test_loss == 0.0:\n            return False\n\n        if abs((base_loss - test_loss) / base_loss) > r_tol:\n            return False\n\n        return True\n\n\ndef checkpoint_suite():\n    suite = unittest.TestSuite()\n    suite.addTest(GPT2CheckpointTestCase(\'test_mp4_gpu16_node1_with_zero1\'))\n    suite.addTest(GPT2CheckpointTestCase(\'test_mp4_gpu16_node1_with_zero2\'))\n    suite.addTest(GPT2CheckpointTestCase(\'test_mp4_gpu16_node1_without_zero\'))\n\n    return suite\n\n\nif __name__ == \'__main__\':\n    runner = unittest.TextTestRunner(failfast=True)\n    runner.run(checkpoint_suite())\n'"
tests/model/Megatron_GPT2/run_func_test.py,0,"b'# coding=utf-8\n# Copyright (c) 2019, The Microsoft DeepSpeed Team. All rights reserved.\n#\n# Note: please copy webtext data to ""Megatron-LM"" folder, before running this script.\n\nimport unittest\nimport subprocess\nimport os\nimport time\nimport re\nfrom .test_common import BaseTestCase\n\nLAYERS = 2\nHIDDEN_SIZE = 128\nATTN_HEADS = 8\nSEQ_LEN = 64\n\n\ndef grep_loss_from_file(file_name):\n    loss = 0.0\n\n    with open(file_name, \'r\') as f:\n        lines = f.readlines()\n        line_filter = ""validation loss at the end of training for test data | LM loss:""\n        match_number = re.compile(\'LM loss: ([-+]?[0-9]+\\.?[0-9]*(?:[Ee][-+]?[0-9]+)?)\')\n\n        for line in lines:\n            if line_filter in line:\n                loss = re.findall(match_number, line)\n                loss = float(loss[0])\n\n    if loss == 0.0:\n        print(""no loss found in file "", file_name)\n\n    return loss\n\n\nclass GPT2FuncTestCase(BaseTestCase):\n    def __init__(self, methodName=""DeepSpeed function test on GPT2 model""):\n        super(GPT2FuncTestCase, self).__init__(methodName)\n\n    def setUp(self):\n        self.save_dir = os.getcwd()\n        new_dir = os.path.dirname(__file__)\n        if new_dir:\n            os.chdir(new_dir)\n\n    def tearDown(self):\n        os.chdir(self.save_dir)\n\n    def test_mp1_gpu1_node1_zero1(self):\n        test_config = {\n            ""mp"": 1,\n            ""gpus"": 1,\n            ""nodes"": 1,\n            ""bs"": 4,\n            ""steps"": 1000,\n            ""layers"": LAYERS,\n            ""hidden_size"": HIDDEN_SIZE,\n            ""seq_length"": SEQ_LEN,\n            ""heads"": ATTN_HEADS,\n            ""deepspeed"": False,\n            ""json"": ""ds_config_func_bs4_zero1.json"",\n        }\n\n        succ = self.run_test(test_config, 0.01)\n        self.assertTrue(succ)\n\n    def test_mp1_gpu2_node1_zero1(self):\n        test_config = {\n            ""mp"": 1,\n            ""gpus"": 2,\n            ""nodes"": 1,\n            ""bs"": 8,\n            ""steps"": 1000,\n            ""layers"": LAYERS,\n            ""hidden_size"": HIDDEN_SIZE,\n            ""seq_length"": SEQ_LEN,\n            ""heads"": ATTN_HEADS,\n            ""deepspeed"": False,\n            ""json"": ""ds_config_func_bs8_zero1.json"",\n        }\n\n        succ = self.run_test(test_config, 0.01)\n        self.assertTrue(succ)\n\n    def test_mp2_gpu4_node1_zero1(self):\n        test_config = {\n            ""mp"": 2,\n            ""gpus"": 4,\n            ""nodes"": 1,\n            ""bs"": 8,\n            ""steps"": 1000,\n            ""layers"": LAYERS,\n            ""hidden_size"": HIDDEN_SIZE,\n            ""seq_length"": SEQ_LEN,\n            ""heads"": ATTN_HEADS,\n            ""deepspeed"": False,\n            ""json"": ""ds_config_func_bs8_zero1.json"",\n        }\n\n        succ = self.run_test(test_config, 0.01)\n        self.assertTrue(succ)\n\n    def test_mp4_gpu4_node1_zero1(self):\n        test_config = {\n            ""mp"": 4,\n            ""gpus"": 4,\n            ""nodes"": 1,\n            ""bs"": 8,\n            ""steps"": 1000,\n            ""layers"": LAYERS,\n            ""hidden_size"": HIDDEN_SIZE,\n            ""seq_length"": SEQ_LEN,\n            ""heads"": ATTN_HEADS,\n            ""deepspeed"": False,\n            ""json"": ""ds_config_func_bs8_zero1.json"",\n        }\n\n        succ = self.run_test(test_config, 0.01)\n        self.assertTrue(succ)\n\n    def test_mp1_gpu1_node1_zero2(self):\n        test_config = {\n            ""mp"": 1,\n            ""gpus"": 1,\n            ""nodes"": 1,\n            ""bs"": 4,\n            ""steps"": 1000,\n            ""layers"": LAYERS,\n            ""hidden_size"": HIDDEN_SIZE,\n            ""seq_length"": SEQ_LEN,\n            ""heads"": ATTN_HEADS,\n            ""deepspeed"": False,\n            ""json"": ""ds_config_func_bs4_zero2.json"",\n        }\n\n        succ = self.run_test(test_config, 0.01)\n        self.assertTrue(succ)\n\n    def test_mp1_gpu2_node1_zero2(self):\n        test_config = {\n            ""mp"": 1,\n            ""gpus"": 2,\n            ""nodes"": 1,\n            ""bs"": 8,\n            ""steps"": 1000,\n            ""layers"": LAYERS,\n            ""hidden_size"": HIDDEN_SIZE,\n            ""seq_length"": SEQ_LEN,\n            ""heads"": ATTN_HEADS,\n            ""deepspeed"": False,\n            ""json"": ""ds_config_func_bs8_zero2.json"",\n        }\n\n        succ = self.run_test(test_config, 0.01)\n        self.assertTrue(succ)\n\n    def test_mp2_gpu4_node1_zero2(self):\n        test_config = {\n            ""mp"": 2,\n            ""gpus"": 4,\n            ""nodes"": 1,\n            ""bs"": 8,\n            ""steps"": 1000,\n            ""layers"": LAYERS,\n            ""hidden_size"": HIDDEN_SIZE,\n            ""seq_length"": SEQ_LEN,\n            ""heads"": ATTN_HEADS,\n            ""deepspeed"": False,\n            ""json"": ""ds_config_func_bs8_zero2.json"",\n        }\n\n        succ = self.run_test(test_config, 0.01)\n        self.assertTrue(succ)\n\n        succ = self.run_partition_activations_test(test_config, 0.01)\n        self.assertTrue(succ)\n\n    def test_mp4_gpu4_node1_zero2(self):\n        test_config = {\n            ""mp"": 4,\n            ""gpus"": 4,\n            ""nodes"": 1,\n            ""bs"": 8,\n            ""steps"": 1000,\n            ""layers"": LAYERS,\n            ""hidden_size"": HIDDEN_SIZE,\n            ""seq_length"": SEQ_LEN,\n            ""heads"": ATTN_HEADS,\n            ""deepspeed"": False,\n            ""json"": ""ds_config_func_bs8_zero2.json"",\n        }\n\n        succ = self.run_test(test_config, 0.01)\n        self.assertTrue(succ)\n\n        succ = self.run_partition_activations_test(test_config, 0.01)\n        self.assertTrue(succ)\n\n    def test_optimizer_scheduler(self):\n        test_config = {\n            ""mp"": 1,\n            ""gpus"": 1,\n            ""nodes"": 1,\n            ""bs"": 4,\n            ""steps"": 20,\n            ""layers"": LAYERS,\n            ""hidden_size"": HIDDEN_SIZE,\n            ""seq_length"": SEQ_LEN,\n            ""heads"": ATTN_HEADS,\n            ""deepspeed"": False,\n            ""json"": ""ds_config_func_scheduler.json"",\n        }\n\n        succ = self.run_test(test_config, 0.01)\n        # assure no crash.\n        self.assertTrue(True)\n\n    def run_partition_activations_test(self, test_config, r_tol):\n        print(""\\n"")\n        print(""{0}: starting......"".format(self.id()))\n\n        baseline_prefix = ""gpt2_func_""\n        prefix = ""gpt2_partition_activation_""\n\n        # baseline run...\n        test_config[""deepspeed""] = False\n        base_file = self.gen_output_name(test_config, baseline_prefix)\n\n        # skip baseline run if it exists.\n        if not self.has_loss_data(base_file):\n            print(""{0}: baseline run."".format(self.id()))\n            self.run_gpt2_test(test_config, base_file)\n        else:\n            print(""{0}: baseline exists."".format(self.id()))\n\n        # DeepSpeed run...\n        test_config[""deepspeed""] = True\n        test_config[""other_args""] = ""--deepspeed-activation-checkpointing""\n        print(""{0}: DeepSpeed run."".format(self.id()))\n        test_file = self.gen_output_name(test_config, prefix)\n        self.run_gpt2_test(test_config, test_file)\n\n        return self.check_parity(base_file, test_file, r_tol)\n\n    def run_test(self, test_config, r_tol):\n        print(""\\n"")\n        print(""{0}: starting......"".format(self.id()))\n\n        prefix = ""gpt2_func""\n\n        # baseline run...\n        test_config[""deepspeed""] = False\n        base_file = self.gen_output_name(test_config, prefix)\n\n        # skip baseline run if it exists.\n        if not self.has_loss_data(base_file):\n            print(""{0}: baseline run."".format(self.id()))\n            self.run_gpt2_test(test_config, base_file)\n        else:\n            print(""{0}: baseline exists."".format(self.id()))\n\n        # DeepSpeed run...\n        test_config[""deepspeed""] = True\n        print(""{0}: DeepSpeed run."".format(self.id()))\n        test_file = self.gen_output_name(test_config, prefix)\n        self.run_gpt2_test(test_config, test_file)\n\n        return self.check_parity(base_file, test_file, r_tol)\n\n    def has_loss_data(self, file_name):\n        has_loss = False\n        if os.path.exists(file_name):\n            loss = grep_loss_from_file(file_name)\n            if loss != 0.0:\n                has_loss = True\n\n        return has_loss\n\n    def check_parity(self, base_file, test_file, r_tol):\n        base_loss = grep_loss_from_file(base_file)\n        test_loss = grep_loss_from_file(test_file)\n\n        print(""baseline loss: {0}, test loss: {1}"".format(base_loss, test_loss))\n\n        if base_loss == 0.0 or test_loss == 0.0:\n            return False\n\n        if abs((base_loss - test_loss) / base_loss) > r_tol:\n            return False\n\n        return True\n\n\ndef suite():\n    suite = unittest.TestSuite()\n    suite.addTest(GPT2FuncTestCase(\'test_mp1_gpu1_node1_zero1\'))\n    suite.addTest(GPT2FuncTestCase(\'test_mp1_gpu2_node1_zero1\'))\n    suite.addTest(GPT2FuncTestCase(\'test_mp2_gpu4_node1_zero1\'))\n    suite.addTest(GPT2FuncTestCase(\'test_mp4_gpu4_node1_zero1\'))\n\n    suite.addTest(GPT2FuncTestCase(\'test_mp1_gpu1_node1_zero2\'))\n    suite.addTest(GPT2FuncTestCase(\'test_mp1_gpu2_node1_zero2\'))\n    suite.addTest(GPT2FuncTestCase(\'test_mp2_gpu4_node1_zero2\'))\n    suite.addTest(GPT2FuncTestCase(\'test_mp4_gpu4_node1_zero2\'))\n\n    suite.addTest(GPT2FuncTestCase(\'test_optimizer_scheduler\'))\n    return suite\n\n\nif __name__ == \'__main__\':\n    runner = unittest.TextTestRunner(failfast=True)\n    runner.run(suite())\n'"
tests/model/Megatron_GPT2/run_perf_baseline.py,0,"b'# coding=utf-8\n# Copyright (c) 2019, The Microsoft DeepSpeed Team. All rights reserved.\n#\n# Note: please copy webtext data to ""Megatron-LM"" folder, before running this script.\n\nimport unittest\nimport subprocess\nimport os\nimport time\nimport re\nfrom test_common import BaseTestCase\n\n\nclass GPT2PerfBaselineTestCase(BaseTestCase):\n    def __init__(self, methodName=""DeepSpeed performance test on GPT2 model""):\n        super(GPT2PerfBaselineTestCase, self).__init__(methodName)\n\n    def test_perf_1_5B(self):\n        test_config = {\n            ""mp"": 2,\n            ""gpus"": 16,\n            ""nodes"": 4,\n            ""bs"": 16,\n            ""steps"": 100,\n            ""layers"": 48,\n            ""hidden_size"": 1600,\n            ""seq_length"": 1024,\n            ""heads"": 16,\n            ""deepspeed"": False,\n        }\n\n        self.run_test(test_config)\n\n    def test_perf_4B(self):\n        test_config = {\n            ""mp"": 4,\n            ""gpus"": 16,\n            ""nodes"": 4,\n            ""bs"": 8,\n            ""steps"": 100,\n            ""layers"": 64,\n            ""hidden_size"": 2304,\n            ""seq_length"": 1024,\n            ""heads"": 16,\n            ""deepspeed"": False,\n        }\n\n        self.run_test(test_config)\n\n    def test_perf_8B(self):\n        test_config = {\n            ""mp"": 4,\n            ""gpus"": 16,\n            ""nodes"": 4,\n            ""bs"": 8,\n            ""steps"": 100,\n            ""layers"": 72,\n            ""hidden_size"": 3072,\n            ""seq_length"": 1024,\n            ""heads"": 24,\n            ""deepspeed"": False,\n        }\n\n        self.run_test(test_config)\n\n    def test_perf_20B(self):\n        test_config = {\n            ""mp"": 16,\n            ""gpus"": 16,\n            ""nodes"": 4,\n            ""bs"": 4,\n            ""steps"": 50,\n            ""layers"": 111,\n            ""hidden_size"": 3808,\n            ""seq_length"": 1024,\n            ""heads"": 32,\n            ""ckpt_num_layers"": 1,\n            ""deepspeed"": False,\n        }\n\n        self.run_test(test_config)\n\n    def run_test(self, test_config):\n        print(""\\n"")\n        print(""{0}: starting......"".format(self.id()))\n        prefix = ""gpt2_perf""\n\n        test_file = self.gen_output_name(test_config, prefix)\n        self.run_gpt2_test(test_config, test_file)\n        exec_time = self.grep_latency_from_file(test_file)\n\n        if exec_time == 0.0:\n            print(""{0}: no latency found in file {1}"".format(self.id(), test_file))\n        else:\n            print(""{0}: execution time per iteration is {1}ms."".format(\n                self.id(),\n                exec_time))\n\n    def grep_latency_from_file(self, file_name):\n        latency = 0.0\n        count = 0\n\n        with open(file_name, \'r\') as f:\n            lines = f.readlines()\n            line_filter = ""elapsed time per iteration""\n            match_number = re.compile(\n                \'elapsed time per iteration \\(ms\\): ([-+]?[0-9]+\\.?[0-9]*(?:[Ee][-+]?[0-9]+)?)\'\n            )\n\n            for line in lines:\n                if line_filter in line:\n                    ms_per_iter = re.findall(match_number, line)\n                    latency += float(ms_per_iter[0])\n                    count += 1\n\n        if count > 0:\n            latency /= count\n\n        return latency\n\n\ndef suite():\n    suite = unittest.TestSuite()\n    suite.addTest(GPT2PerfBaselineTestCase(\'test_perf_1_5B\'))\n    suite.addTest(GPT2PerfBaselineTestCase(\'test_perf_4B\'))\n    suite.addTest(GPT2PerfBaselineTestCase(\'test_perf_8B\'))\n    suite.addTest(GPT2PerfBaselineTestCase(\'test_perf_20B\'))\n    return suite\n\n\nif __name__ == \'__main__\':\n    runner = unittest.TextTestRunner(failfast=True)\n    runner.run(suite())\n'"
tests/model/Megatron_GPT2/run_perf_test.py,0,"b'# coding=utf-8\n# Copyright (c) 2019, The Microsoft DeepSpeed Team. All rights reserved.\n#\n# Note: please copy webtext data to ""Megatron-LM"" folder, before running this script.\n\nimport unittest\nimport subprocess\nimport os\nimport time\nimport re\nfrom test_common import BaseTestCase\n\n\nclass GPT2PerfTestCase(BaseTestCase):\n    def __init__(self, methodName=""DeepSpeed performance test on GPT2 model""):\n        super(GPT2PerfTestCase, self).__init__(methodName)\n\n    def test_perf_1_5B(self):\n        test_config = {\n            ""mp"": 1,\n            ""gpus"": 16,\n            ""nodes"": 4,\n            ""bs"": 32,\n            ""steps"": 100,\n            ""layers"": 48,\n            ""hidden_size"": 1600,\n            ""seq_length"": 1024,\n            ""heads"": 16,\n            ""deepspeed"": True,\n            ""json"": ""ds_config_perf_bs32.json"",\n        }\n\n        self.run_test(test_config)\n\n    def test_perf_4B(self):\n        test_config = {\n            ""mp"": 1,\n            ""gpus"": 16,\n            ""nodes"": 4,\n            ""bs"": 8,\n            ""steps"": 100,\n            ""layers"": 64,\n            ""hidden_size"": 2304,\n            ""seq_length"": 1024,\n            ""heads"": 16,\n            ""deepspeed"": True,\n            ""json"": ""ds_config_perf_bs8.json"",\n        }\n\n        self.run_test(test_config)\n\n    def test_perf_8B(self):\n        test_config = {\n            ""mp"": 2,\n            ""gpus"": 16,\n            ""nodes"": 4,\n            ""bs"": 16,\n            ""steps"": 100,\n            ""layers"": 72,\n            ""hidden_size"": 3072,\n            ""seq_length"": 1024,\n            ""heads"": 24,\n            ""deepspeed"": True,\n            ""json"": ""ds_config_perf_bs16.json"",\n        }\n\n        self.run_test(test_config)\n\n    def test_perf_20B(self):\n        test_config = {\n            ""mp"": 4,\n            ""gpus"": 16,\n            ""nodes"": 4,\n            ""bs"": 8,\n            ""steps"": 50,\n            ""layers"": 111,\n            ""hidden_size"": 3808,\n            ""seq_length"": 1024,\n            ""heads"": 32,\n            ""ckpt_num_layers"": 1,\n            ""deepspeed"": True,\n            ""json"": ""ds_config_perf_bs8.json"",\n        }\n\n        self.run_test(test_config)\n\n    def run_test(self, test_config):\n        print(""\\n"")\n        print(""{0}: starting......"".format(self.id()))\n        prefix = ""gpt2_perf""\n\n        test_file = self.gen_output_name(test_config, prefix)\n        self.run_gpt2_test(test_config, test_file)\n        exec_time = self.grep_latency_from_file(test_file)\n\n        if exec_time == 0.0:\n            print(""{0}: no latency found in file {1}"".format(self.id(), test_file))\n        else:\n            print(""{0}: execution time per iteration is {1}ms."".format(\n                self.id(),\n                exec_time))\n\n    def grep_latency_from_file(self, file_name):\n        latency = 0.0\n        count = 0\n\n        with open(file_name, \'r\') as f:\n            lines = f.readlines()\n            line_filter = ""elapsed time per iteration""\n            match_number = re.compile(\n                \'elapsed time per iteration \\(ms\\): ([-+]?[0-9]+\\.?[0-9]*(?:[Ee][-+]?[0-9]+)?)\'\n            )\n\n            for line in lines:\n                if line_filter in line:\n                    ms_per_iter = re.findall(match_number, line)\n                    latency += float(ms_per_iter[0])\n                    count += 1\n\n        if count > 0:\n            latency /= count\n\n        return latency\n\n\ndef suite():\n    suite = unittest.TestSuite()\n    suite.addTest(GPT2PerfTestCase(\'test_perf_1_5B\'))\n    suite.addTest(GPT2PerfTestCase(\'test_perf_4B\'))\n    suite.addTest(GPT2PerfTestCase(\'test_perf_8B\'))\n    suite.addTest(GPT2PerfTestCase(\'test_perf_20B\'))\n    return suite\n\n\nif __name__ == \'__main__\':\n    runner = unittest.TextTestRunner(failfast=True)\n    runner.run(suite())\n'"
tests/model/Megatron_GPT2/test_common.py,0,"b'# coding=utf-8\n# Copyright (c) 2019, The Microsoft DeepSpeed Team. All rights reserved.\n#\n\nimport unittest\nimport subprocess\nimport os\nimport time\nimport re\n\n\nclass BaseTestCase(unittest.TestCase):\n    def __init__(self, methodName=""DeepSpeed performance test""):\n        super(BaseTestCase, self).__init__(methodName)\n        self.test_dir = ""./test""\n        self.baseline_dir = ""./baseline""\n        self.timestr = time.strftime(""%Y%m%d-%H%M%S"")\n\n    def gen_output_name(self, test_config, prefix):\n        other_args = test_config[""other_args""] if ""other_args"" in test_config else """"\n        zero_args = ""_zero"" if ""zero"" in test_config and test_config[""zero""] else """"\n        other_args = other_args.strip(\' -\\\\\').replace("" "", """").replace(""\\"""", """")\n\n        if other_args:\n            other_args = ""_"" + other_args\n\n        if test_config[""deepspeed""]:\n            file_name = ""_mp{0}_gpu{1}_node{2}_bs{3}_step{4}_layer{5}_hidden{6}_seq{7}_head{8}{9}_ds{10}-{11}.log"".format(\n                test_config[""mp""],\n                test_config[""gpus""],\n                test_config[""nodes""],\n                test_config[""bs""],\n                test_config[""steps""],\n                test_config[""layers""],\n                test_config[""hidden_size""],\n                test_config[""seq_length""],\n                test_config[""heads""],\n                other_args,\n                zero_args,\n                self.timestr)\n            save_dir = self.test_dir\n        else:\n            file_name = ""_mp{0}_gpu{1}_node{2}_bs{3}_step{4}_layer{5}_hidden{6}_seq{7}_head{8}{9}.log"".format(\n                test_config[""mp""],\n                test_config[""gpus""],\n                test_config[""nodes""],\n                test_config[""bs""],\n                test_config[""steps""],\n                test_config[""layers""],\n                test_config[""hidden_size""],\n                test_config[""seq_length""],\n                test_config[""heads""],\n                other_args)\n            save_dir = self.baseline_dir\n\n        return os.path.join(save_dir, prefix + file_name)\n\n    def ensure_directory_exists(self, filename):\n        dirname = os.path.dirname(filename)\n        if not os.path.exists(dirname):\n            os.makedirs(dirname)\n\n    def clean_test_env(self):\n        cmd = ""dlts_ssh pkill -9 -f /usr/bin/python""\n        print(cmd)\n        subprocess.run(cmd, shell=True, check=False, executable=\'/bin/bash\')\n        time.sleep(20)\n\n    def run_gpt2_test(self, test_config, output):\n        ds_flag = ""-d "" + test_config[""json""] if test_config[""deepspeed""] else """"\n        ckpt_num = test_config[\n            ""ckpt_num_layers""] if ""ckpt_num_layers"" in test_config else 1\n        other_args = ""-o "" + test_config[\n            ""other_args""] if ""other_args"" in test_config else """"\n\n        cmd = ""./ds_gpt2_test.sh -m {0} -g {1} -n {2} -b {3} -s {4} -l {5} -h {6} -q {7} -e {8} -c {9} {10} {11}"".format(\n            test_config[""mp""],\n            test_config[""gpus""],\n            test_config[""nodes""],\n            test_config[""bs""],\n            test_config[""steps""],\n            test_config[""layers""],\n            test_config[""hidden_size""],\n            test_config[""seq_length""],\n            test_config[""heads""],\n            ckpt_num,\n            other_args,\n            ds_flag)\n\n        self.ensure_directory_exists(output)\n        with open(output, ""w"") as f:\n            print(cmd)\n            subprocess.run(cmd,\n                           shell=True,\n                           check=False,\n                           executable=\'/bin/bash\',\n                           stdout=f,\n                           stderr=f)\n'"
