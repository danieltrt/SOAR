file_path,api_count,code
LSP_train.py,15,"b'#  Copyright (c) Microsoft Corporation. \n#  Licensed under the MIT license. \n\'\'\'\n * @Desc: train GPT2 from scratch/ fine tuning.\n          Modified based on Huggingface GPT-2 implementation\n\'\'\'\n\nimport json\nimport os\nimport sys\nimport argparse\nimport logging\nimport time\nimport tqdm\nimport datetime\nimport torch\n\nimport numpy as np\n\nfrom os.path import join\nfrom torch.distributed import get_rank, get_world_size\n\nfrom lsp_model import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, Adam\nfrom gpt2_training.train_utils import load_model, boolean_string, set_lr, get_eval_list_same_length\nfrom gpt2_training.eval_utils import eval_model_loss\n\nfrom data_loader import BucketingDataLoader, DynamicBatchingLoader, DistributedBucketingDataLoader\n\n\nfrom gpt2_training.distributed import all_reduce_and_rescale_tensors, all_gather_list\n\n\nlogging.basicConfig(\n    format=\'%(asctime)s - %(levelname)s - %(name)s -   %(message)s\',\n    datefmt=\'%m/%d/%Y %H:%M:%S\', level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nINF = 100000000\nCACHE_EMPTY_STEP = 10000\nEVAL_STEP = 100000\n\n#########################################################################\n# Prepare Parser\n##########################################################################\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--model_name_or_path\', type=str,\n                    help=\'pretrained model name or path to local checkpoint\')\nparser.add_argument(""--seed"", type=int, default=42)\nparser.add_argument(""--max_seq_length"", type=int, default=128)\n\nparser.add_argument(""--skip_eval"", action=\'store_true\',\n                    help=\'If true, skip evaluation.\')\nparser.add_argument(""--init_checkpoint"", type=str)\nparser.add_argument(""--train_input_file"", type=str)\nparser.add_argument(""--eval_input_file"", type=str)\nparser.add_argument(""--continue_from"", type=int, default=0)\n\nparser.add_argument(""--train_batch_size"", type=int, default=4,\n                    help=""batch size now means per GPU per step"")\nparser.add_argument(""--gradient_accumulation_steps"", type=int, default=2,\n                    help=""to increase effective batch size ""\n                         ""and reduce synchronization"")\nparser.add_argument(""--eval_batch_size"", type=int, default=4)\nparser.add_argument(""--learning_rate"", type=float, default=1e-5)\nparser.add_argument(""--num_optim_steps"", type=int, default=1000000,\n                    help=""new API specifies num update steps"")\nparser.add_argument(""--valid_step"", type=int, default=10000,\n                    help=""how many optim steps between validations"")\nparser.add_argument(""--warmup_proportion"", type=float, default=0.1)\nparser.add_argument(""--warmup_steps"", type=int, default=16000)\n\nparser.add_argument(""--normalize_data"", type=boolean_string, default=True)\nparser.add_argument(""--fp16"", type=boolean_string, default=True)\nparser.add_argument(""--lr_schedule"", type=str,\n                    choices=[\'noam\', \'noamwd\', \'BERT\', \'None\'], default=\'noam\')\nparser.add_argument(""--loss_scale"", type=float, default=0)\nparser.add_argument(""--no_token_id"", type=boolean_string, default=True)\n\nparser.add_argument(""--output_dir"", type=str)\nparser.add_argument(""--log_dir"", type=str)\nparser.add_argument(\'--pbar\', type=boolean_string, default=True, help=\'turn on progress bar\')\n\n# distributed\nparser.add_argument(\'--local_rank\', type=int, default=-1,\n                    help=\'for torch.distributed\')\nparser.add_argument(\'--config\', help=\'JSON config file\')\n\n\n# do normal parsing\nargs = parser.parse_args()\n\nif args.config is not None:\n    # override argparse defaults by config JSON\n    opts = json.load(open(args.config))\n    for k, v in opts.items():\n        if isinstance(v, str):\n            # PHILLY ENV special cases\n            if \'PHILLY_JOB_DIRECTORY\' in v:\n                v = v.replace(\'PHILLY_JOB_DIRECTORY\',\n                              os.environ[\'PHILLY_JOB_DIRECTORY\'])\n            elif \'PHILLY_LOG_DIRECTORY\' in v:\n                v = v.replace(\'PHILLY_LOG_DIRECTORY\',\n                              os.environ[\'PHILLY_LOG_DIRECTORY\'])\n        setattr(args, k, v)\n\n    # command line should override config JSON\n    argv = sys.argv[1:]\n    overrides, _ = parser.parse_known_args(argv)\n    for k, v in vars(overrides).items():\n        if f\'--{k}\' in argv:\n            setattr(args, k, v)\n    setattr(args, \'local_rank\', overrides.local_rank)\n\n\nassert args.train_batch_size % args.gradient_accumulation_steps == 0, \\\n    \'batch size % gradient accumulation steps != 0!\'\nargs.train_batch_size = (args.train_batch_size\n                         // args.gradient_accumulation_steps)\nlogger.info(\'train batch size = {}, \'\n            \'new train batch size (after gradient accumulation) = {}\'.format(\n                args.train_batch_size*args.gradient_accumulation_steps,\n                args.train_batch_size))\n\n\nif args.local_rank == -1:\n    logger.info(\'CUDA available? {}\'.format(str(torch.cuda.is_available())))\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    n_gpu = torch.cuda.device_count()\n    args.device, args.n_gpu = device, n_gpu\nelse:\n    # distributed training\n    torch.cuda.set_device(args.local_rank)\n    device = torch.device(""cuda"", args.local_rank)\n    # Initializes the distributed backend which will take care of\n    # sychronizing nodes/GPUs\n    torch.distributed.init_process_group(backend=\'nccl\')\n    n_gpu = torch.distributed.get_world_size()\n    args.device, args.n_gpu = device, 1\n    logger.info(""device: {} n_gpu: {}, distributed training: {}, ""\n                ""16-bits training: {}"".format(\n                    device, n_gpu, bool(args.local_rank != -1), args.fp16))\n\nnp.random.seed(args.seed)\ntorch.random.manual_seed(args.seed)\ntorch.cuda.manual_seed(args.seed)\nif n_gpu > 0:\n    torch.cuda.manual_seed_all(args.seed)\n\ntimestamp = datetime.datetime.now().strftime(\'%Y-%m-%d%H%M%S\')\noutput_dir = join(args.output_dir,\n                  \'GPT2.{}.{}.{}gpu.{}\'.format(args.learning_rate,\n                                               args.train_batch_size, n_gpu,\n                                               timestamp))\nlog_dir = args.log_dir if args.log_dir is not None and len(args.log_dir) > 0 else output_dir\nif args.local_rank == -1 or get_rank() == 0:\n    os.makedirs(output_dir, exist_ok=True)\n\nlogger.info(\'Input Argument Information\')\nargs_dict = vars(args)\nfor a in args_dict:\n    logger.info(\'%-28s  %s\' % (a, args_dict[a]))\n\n\n#########################################################################\n# Prepare Data Set\n##########################################################################\nenc = GPT2Tokenizer.from_pretrained(args.model_name_or_path)\n\nconfig = GPT2Config.from_json_file(\n    join(args.model_name_or_path, \'config.json\'))\n\nif args.local_rank == -1:\n    train_dataloader = BucketingDataLoader(args.train_input_file,\n                                           args.train_batch_size,\n                                           args.max_seq_length)\nelse:\n    train_dataloader = DistributedBucketingDataLoader(\n        get_rank(), get_world_size(),\n        args.train_input_file, args.train_batch_size,\n        args.max_seq_length)\n\neval_dataloader_loss = DynamicBatchingLoader(\n    args.eval_input_file, enc, args.normalize_data,\n    args.eval_batch_size, args.max_seq_length)\n\neval_dataloader_gen = get_eval_list_same_length(\n    args.eval_input_file, enc, args.eval_batch_size, True)\n\n\n#########################################################################\n# Prepare Model and Optimizer\n##########################################################################\nmodel = load_model(GPT2LMHeadModel(config), args.init_checkpoint,\n                   args, verbose=True)\nif args.local_rank != -1:\n    # when from scratch make sure initial models are the same\n    params = [p.data for p in model.parameters()]\n    all_reduce_and_rescale_tensors(\n        params, float(torch.distributed.get_world_size()))\n\nmodel_parameters = filter(lambda p: p.requires_grad, model.parameters())\ntotal_params = sum([np.prod(p.size()) for p in model_parameters])\nlogger.info(\'Number of parameter = {}\'.format(total_params))\n\nparam_optimizer = list(model.named_parameters())\nno_decay = [\'bias\', \'ln\']   # no decay for bias and LayerNorm (ln)\noptimizer_grouped_parameters = [\n    {\'params\': [p for n, p in param_optimizer\n                if not any(nd in n for nd in no_decay)],\n     \'weight_decay\': 0.01},\n    {\'params\': [p for n, p in param_optimizer\n                if any(nd in n for nd in no_decay)], \'weight_decay\': 0.0}\n]\n\nif args.fp16:\n    logger.info(\'in fp16, using FusedAdam\')\n    try:\n        from apex.optimizers import FP16_Optimizer\n        from apex.optimizers import FusedAdam\n    except ImportError:\n        raise ImportError(\n            ""Please install apex from https://www.github.com/nvidia/apex ""\n            ""to use distributed and fp16 training."")\n\n    optimizer = FusedAdam(optimizer_grouped_parameters,\n                          lr=args.learning_rate,\n                          bias_correction=False,\n                          max_grad_norm=1.0)\n    if args.loss_scale == 0:\n        optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True,\n                                   verbose=False)\n    else:\n        optimizer = FP16_Optimizer(optimizer,\n                                   static_loss_scale=args.loss_scale,\n                                   verbose=False)\nelse:\n    optimizer = Adam(optimizer_grouped_parameters, args.learning_rate,\n                     max_grad_norm=1.0)\n\n#########################################################################\n# Training !\n##########################################################################\n\nif args.local_rank == -1 or get_rank() == 0:\n    train_logger = open(join(log_dir, \'train_log.txt\'), \'a+\', buffering=1)\n    eval_logger = open(join(log_dir, \'eval_log.txt\'), \'a+\', buffering=1)\n    print(\'epoch,global_step,step,mean_loss,mean_ppl,n_token_real,\'\n          \'n_token_total,epoch_time\', file=train_logger)\n    print(\'epoch,global_step,step,eval_loss,eval_ppl\', file=eval_logger)\n\nglobal_step = 0\nstep = 0\nepoch = 0\n\nif args.continue_from:\n    global_step = args.continue_from\n    step = global_step*2 - 1\n\n\nif args.local_rank != -1:\n    n_gpu = 1\nif args.local_rank == -1 or get_rank() == 0:\n    if args.pbar:\n        pbar = tqdm.tqdm(total=args.num_optim_steps, desc=f""training"")\n    else:\n        pbar = None\n\nwhile True:\n    model.train()\n    (tr_loss, tr_ppl, mean_ppl, nb_tr_examples, nb_tr_steps) = 0.0, 0.0, 0.0, 0, 0\n    n_token_real, n_token_total = 0, 0\n    train_start_time_epoch = time.time()\n    for batch in train_dataloader:\n        # activate new training mode\n        seq_len = batch[0].shape[1]\n        batch = tuple(t.to(device) for t in batch)\n        input_ids, position_ids, token_ids, label_ids, *_ = batch\n        if args.no_token_id:\n            token_ids = None\n        loss, ppl = model(input_ids, position_ids, token_ids, label_ids)\n\n        if n_gpu > 1:\n            loss = loss.mean()\n            ppl = ppl.mean()\n        loss = loss / (args.train_batch_size / input_ids.shape[0])\n        if args.fp16:\n            optimizer.backward(loss)\n        else:\n            loss.backward()\n\n        tr_loss += float(loss.item()) * (args.train_batch_size / input_ids.shape[0])\n        nb_tr_examples += input_ids.size(0)\n        nb_tr_steps += 1\n        mean_loss = tr_loss / nb_tr_steps\n        if ppl.item() < INF:\n            tr_ppl += ppl.item()\n        else:\n            tr_ppl += mean_ppl\n        mean_ppl = tr_ppl / nb_tr_steps\n\n        n_token_total += input_ids.shape[0] * input_ids.shape[1]\n        n_token_real += (input_ids != 0).sum().item()\n\n        # gradient update\n        step += 1\n        if step % args.gradient_accumulation_steps == 0:\n            set_lr(optimizer, global_step,\n                   args.lr_schedule, args.learning_rate,\n                   args.warmup_steps, args.warmup_proportion,\n                   config.n_embd, args.num_optim_steps)\n\n            if args.local_rank != -1:\n                grads = [p.grad.data for p in model.parameters()\n                         if p.requires_grad and p.grad is not None]\n                all_reduce_and_rescale_tensors(grads, float(1))\n\n            optimizer.step()\n            optimizer.zero_grad()\n            global_step += 1\n\n            # Print log info to file\n            if args.local_rank != -1:\n                mean_loss = sum(all_gather_list(mean_loss)) / get_world_size()\n                mean_ppl = sum(all_gather_list(mean_ppl)) / get_world_size()\n                n_token_real_all_proc = sum(all_gather_list(n_token_real))\n                n_token_total_all_proc = sum(all_gather_list(n_token_total))\n            else:\n                n_token_real_all_proc = n_token_real\n                n_token_total_all_proc = n_token_total\n\n            if args.local_rank == -1 or get_rank() == 0:\n                epoch_time = time.time() - train_start_time_epoch\n                if pbar is not None:\n                    pbar.set_postfix_str(\n                        f""tok/s: {n_token_real_all_proc//epoch_time//1000}k ""\n                        f""ppl: {mean_ppl:.2f} epoch: {epoch}"")\n                    pbar.update(1)\n                print(\'{},{},{},{},{},{},{},{}\'.format(\n                    epoch+1, global_step+1, step+1, mean_loss, mean_ppl,\n                    n_token_real_all_proc, n_token_total_all_proc, epoch_time),\n                    file=train_logger)\n\n            if global_step % args.valid_step == 0:\n                if args.local_rank == -1 or get_rank() == 0:\n                    # only rank 0 process evaluate\n                    torch.save(\n                        {k: (v.cpu() if v is not None else None)  # save to cpu tensors\n                         for k, v in model.state_dict().items()},\n                        join(output_dir,\n                             f\'GP2-pretrain-step-{global_step}.pkl\'))\n\n                    eval_loss, eval_ppl = eval_model_loss(\n                        model, enc, eval_dataloader_loss, epoch, args)\n                    # enable generation step evaluation for now\n                    # gen_response = eval_model_generation(\n                    #     model, enc, eval_dataloader_gen, epoch, args)\n                    \'\'\'\n                    # probably use beam search only for test set\n                    if False:\n                        gen_response_beam = eval_model_generation(\n                            model, enc, eval_dataloader_gen, epoch, args,\n                            use_beam_search=True, beam_width=3)\n                    \'\'\'\n                    print(\'{},{},{},{},{}\'.format(\n                        epoch+1, global_step+1, step+1, eval_loss, eval_ppl),\n                        file=eval_logger)\n                    logger.info(\'current learning rate: \'\n                                + str(optimizer.param_groups[0][\'lr\']))\n                    model.train()\n            if global_step >= args.num_optim_steps:\n                break\n\n        if (step+1) % CACHE_EMPTY_STEP == 0:\n            torch.cuda.empty_cache()\n\n    if global_step >= args.num_optim_steps:\n        break\n    epoch += 1\n\n\nif args.local_rank == -1 or get_rank() == 0:\n    if pbar is not None:\n        pbar.close()\n    train_logger.close()\n    eval_logger.close()\n'"
data_config.py,0,"b'#  Copyright (c) Microsoft Corporation. \r\n#  Licensed under the MIT license. \r\nimport os\r\nfrom . import proj_env\r\n\r\nRAW_DATA_DIR = os.path.join(proj_env.ROOT_DIR, ""raw_data"")\r\nPROCESSED_DATA_DIR = os.path.join(proj_env.ROOT_DIR, ""processed"")\r\nPIPELINE_DATA_DIR = os.path.join(proj_env.ROOT_DIR, ""pipeline_data"")\r\n\r\nTEST_KEY_FN = os.path.join(RAW_DATA_DIR, ""keys.2k.txt"")\r\n\r\n\r\nMAX_LEN = 128 #512\r\nMAX_CONTEXT_LEN = 64#250\r\n\r\nTAG_LIST = [""<p>"", ""<title>"", ""<anchor>""] + [""<h%s>"" % i for i in range(1, 7)]\r\n\r\n'"
data_loader.py,18,"b'#  Copyright (c) Microsoft Corporation. \n#  Licensed under the MIT license. \nimport gzip\nimport json\nimport math\nimport random\nimport shelve\nimport torch\n\nimport subprocess as sp\n\nfrom math import ceil\nfrom torch.utils.data import DataLoader, Sampler, Dataset\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom env import END_OF_TEXT_TOKEN\nfrom gpt2_training.train_utils import (InputFeatures, InputFeatures_train,\n                                       RedditExample)\n\n\nclass BucketSampler(Sampler):\n    """"""\n    this sampler will sort data by sequence length\n    """"""\n    def __init__(self, lens, bucket_size, batch_size,\n                 droplast=False, shuffle=True):\n        self._lens = lens\n        self._batch_size = batch_size\n        self._bucket_size = bucket_size\n        self._droplast = droplast\n        self._shuf = shuffle\n\n    def __iter__(self):\n        ids = list(range(len(self._lens)))\n        if self._shuf:\n            random.shuffle(ids)\n        buckets = [sorted(ids[i:i+self._bucket_size],\n                          key=lambda i: self._lens[i], reverse=True)\n                   for i in range(0, len(ids), self._bucket_size)]\n        batches = [bucket[i:i+self._batch_size]\n                   for bucket in buckets\n                   for i in range(0, len(bucket), self._batch_size)]\n        if self._droplast:\n            batches = [batch for batch in batches\n                       if len(batch) == self._batch_size]\n        if self._shuf:\n            random.shuffle(batches)\n        return iter(batches)\n\n    def __len__(self):\n        bucket_sizes = ([self._bucket_size]\n                        * (len(self._lens) // self._bucket_size)\n                        + [len(self._lens) % self._bucket_size])\n        if self._droplast:\n            return sum(s//self._batch_size for s in bucket_sizes)\n        else:\n            return sum(math.ceil(s/self._batch_size) for s in bucket_sizes)\n\n\nclass GPT2FeatureDataset(Dataset):\n    """""" pytorch dataset for GPT2 training """"""\n    def __init__(self, features, max_len=None):\n        self.features = features\n        self.max_len = max_len  # this max_len do truncate\n\n    def __getitem__(self, i):\n        feat_dict = self.features[i]\n        if self.max_len is not None and feat_dict[\'input_len\'] > self.max_len:\n            # tuncate on the left side (context)\n            feat_dict[\'input_ids\'] = feat_dict[\'input_ids\'][-self.max_len:]\n            feat_dict[\'position_ids\'] = feat_dict[\'position_ids\'][\n                -self.max_len:]\n            feat_dict[\'token_type_ids\'] = feat_dict[\'token_type_ids\'][\n                -self.max_len:]\n            feat_dict[\'lm_labels\'] = feat_dict[\'lm_labels\'][-self.max_len:]\n        try:\n            for s in [\'context_len\', \'response_len\']:\n                if s in feat_dict.keys():\n                    print(""db file missing ""+s)\n                    del feat_dict[s]\n        except Exception:\n            import pdb\n            pdb.set_trace()\n\n        feat = InputFeatures_train(**feat_dict)\n        return feat\n\n    def __len__(self):\n        return len(self.features)\n\n    @staticmethod\n    def collate(features):\n        input_ids = pad_sequence([torch.tensor(f.input_ids, dtype=torch.long)\n                                  for f in features],\n                                 batch_first=True, padding_value=0)\n        position_ids = pad_sequence([torch.tensor(f.position_ids,\n                                                  dtype=torch.long)\n                                     for f in features],\n                                    batch_first=True, padding_value=0)\n        token_type_ids = pad_sequence([torch.tensor(f.token_type_ids,\n                                                    dtype=torch.long)\n                                       for f in features],\n                                      batch_first=True, padding_value=0)\n        labels = pad_sequence([torch.tensor(f.lm_labels, dtype=torch.long)\n                               for f in features],\n                              batch_first=True, padding_value=-1)\n        return (input_ids, position_ids, token_type_ids, labels)\n\n\nclass BucketingDataLoader(object):\n    """""" this loads shelve db chunks and then convert to mini-batch loader""""""\n    def __init__(self, db_name, batch_size, max_seq_len,\n                 bucket=100, shuffle=True):\n        self.db = shelve.open(f\'{db_name}/db\', \'r\')\n        self.batch_size = batch_size\n        self.max_len = max_seq_len\n        self.bucket_size = bucket * batch_size\n        self.shuffle = shuffle\n\n    def _get_keys(self):\n        keys = list(self.db.keys())\n        return keys\n\n    def __iter__(self):\n        keys = self._get_keys()\n        if self.shuffle:\n            random.shuffle(keys)\n        for key in keys:\n            chunk = json.loads(gzip.decompress(self.db[key]).decode(\'utf-8\'))\n            # discard long examples\n            trunc_chunk = []\n            lens = []\n            for feat in chunk:\n                if feat[\'input_len\'] > self.max_len:\n                    continue\n                trunc_chunk.append(feat)\n                lens.append(feat[\'input_len\'])\n\n            dataset = GPT2FeatureDataset(trunc_chunk, self.max_len)\n            sampler = BucketSampler(lens, self.bucket_size, self.batch_size,\n                                    droplast=True, shuffle=self.shuffle)\n            loader = DataLoader(dataset, batch_sampler=sampler,\n                                num_workers=0,  # can test multi-worker\n                                collate_fn=GPT2FeatureDataset.collate)\n            yield from loader\n\n    def __len__(self):\n        raise NotImplementedError()\n\n    def __del__(self):\n        self.db.close()\n\n\nclass DistributedBucketingDataLoader(BucketingDataLoader):\n    """""" distributed version """"""\n    def __init__(self, rank, num_replica, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.rank = rank\n        self.num_replica = num_replica\n\n    def _get_keys(self):\n        keys = list(self.db.keys())[self.rank::self.num_replica]\n        return keys\n\n\ndef convert_examples_to_features_dynamic(examples, tokenizer,\n                                         max_seq_length=512):\n    """"""\n    do not pad\n    """"""\n    def featurize(example):\n        conv_id = example.conv_id\n        context_id = tokenizer.encode(example.context)\n        end_of_text_id = tokenizer.encoder[END_OF_TEXT_TOKEN]\n\n        # response is provided in example\n        response_id = tokenizer.encode(example.response)\n\n        input_ids_len = len(context_id) + len(response_id) + 2\n        if input_ids_len > max_seq_length:\n            if len(context_id) > input_ids_len - max_seq_length:\n                # cut context from beginning if length of context + response is too long\n                # and len of context is long enough to cut\n                context_id = context_id[input_ids_len - max_seq_length:]\n            else:\n                # cut response from end if length of context + response is too long\n                # and len of response is long enough to cut\n                # if no response is available, discard the data\n                if max_seq_length-len(context_id)-2 < 0:\n                    return None\n                response_id = response_id[:max_seq_length-len(context_id)-2]\n\n        input_ids = context_id + [end_of_text_id] + response_id + [end_of_text_id]\n\n        # label simplely is next token in sequences. MASK all context_id tokens except for the last one\n        lm_labels = [-1] * len(context_id) + response_id + [end_of_text_id] + [-1]\n\n        position_ids = list(range(len(input_ids)))\n\n        token_type_id = [0] * len(input_ids)\n\n        return InputFeatures(conv_id, input_ids, position_ids, token_type_id,\n                             lm_labels, len(context_id), len(response_id))\n\n    # discard None feature\n    features = [f for f in [featurize(ex) for ex in examples] if f is not None]\n    return features\n\n\nclass DynamicBatchingLoader(object):\n    """""" this loader takes raw text file, used for validate perplexity """"""\n    def __init__(self, corpus_file, tokenizer, normalize_data,\n                 batch_size, max_seq_length):\n        self.corpus = corpus_file\n        self.toker = tokenizer\n        self.norm = normalize_data\n        self.bs = batch_size\n        self.max_seq_length = max_seq_length\n        self.num_examples = self.get_len(corpus_file)\n\n    def __iter__(self, epoch=1):\n        if epoch > 0:\n            for epoch in range(epoch):\n                yield from self._iter_epoch()\n        else:\n            while True:\n                yield from self._iter_epoch()\n\n    def __len__(self):\n        return ceil(self.num_examples/self.bs)\n\n    def _iter_epoch(self):\n        try:\n            with open(self.corpus, \'r\', encoding=""utf-8"") as corpus:\n                i = 0\n                while True:\n                    examples = []\n                    cur_bs = 0\n                    while True:\n                        line = next(corpus).encode(\'utf-8\').decode(\'utf-8\')\n                        contents = line.split(\'\\t\')\n                        src, tgt_all = contents[0], contents[1:]\n                        for tgt in tgt_all:\n                            if self.norm:\n                                src_line = \' \'.join(src.strip().split())\n                                tgt_line = \' \'.join(tgt.strip().split())\n                            else:\n                                src_line = src.strip()\n                                tgt_line = tgt.strip()\n                            examples.append(\n                                RedditExample(i, src_line, tgt_line),\n                            )\n                            i += 1\n                            cur_bs += 1\n                        if cur_bs >= self.bs:\n                            break\n                    features = convert_examples_to_features_dynamic(\n                        examples, self.toker, self.max_seq_length)\n                    batch = self._batch_feature(features)\n                    yield batch\n        except StopIteration:\n            pass\n\n    def _batch_feature(self, features):\n        input_ids = pad_sequence([torch.tensor(f.choices_features[\'input_ids\'],\n                                               dtype=torch.long)\n                                  for f in features],\n                                 batch_first=True, padding_value=0)\n        position_ids = pad_sequence(\n            [torch.tensor(f.choices_features[\'position_ids\'], dtype=torch.long)\n             for f in features],\n            batch_first=True, padding_value=0)\n        token_type_ids = pad_sequence(\n            [torch.tensor(f.choices_features[\'token_type_ids\'],\n                          dtype=torch.long)\n             for f in features],\n            batch_first=True, padding_value=0)\n        labels = pad_sequence([torch.tensor(f.lm_labels, dtype=torch.long)\n                               for f in features],\n                              batch_first=True, padding_value=-1)\n        context_len = torch.tensor([f.context_len for f in features],\n                                   dtype=torch.long)\n        response_len = torch.tensor([f.response_len for f in features],\n                                    dtype=torch.long)\n        return (input_ids, position_ids, token_type_ids, labels,\n                context_len, response_len)\n\n    def get_len(self, corpus):\n        n_line = int(sp.check_output(f""wc -l {corpus}"".split(),\n                                     universal_newlines=True).split()[0])\n        return n_line\n'"
demo.py,0,"b""#  Copyright (c) Microsoft Corporation.\n#  Licensed under the MIT license.\n#\n# Please assign the DATA_FOLDER before running this scripts, the data, pre-trained model, fine-tuned model will be\n# downloaded automatically to DATA_FOLDER\n\nimport os\nimport sys\nimport logging\nfrom functools import partial\n\nfrom demo_utils import download_model_folder\nimport argparse\nimport subprocess as sp\n\n\nPROJECT_FOLDER = os.path.dirname(os.path.realpath(__file__))\nPYTHON_EXE = 'python'\nMODEL_FOLDER = os.path.join(PROJECT_FOLDER, 'models')\nDATA_FOLDER = os.path.join(PROJECT_FOLDER, 'data')\n\nprint(f'PROJECT_FOLDER = {PROJECT_FOLDER}')\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--data', type=str, default='dummy',\n                    help='choose from dummy, small and full')\ndargs = parser.parse_args()\n\nassert dargs.data == 'dummy' or dargs.data == 'small' or dargs.data == 'full' , \\\n    'The specified data option is not support!'\n\n\nlogging.basicConfig(\n    format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n    datefmt='%m/%d/%Y %H:%M:%S', level=logging.INFO\n)\nlogger = logging.getLogger(__name__)\n\n\nif os.path.exists(MODEL_FOLDER):\n    print(f'Found existing models folder at {MODEL_FOLDER}, skip creating a new one!')\n    os.makedirs(MODEL_FOLDER, exist_ok=True)\nelse:\n    os.makedirs(MODEL_FOLDER)\n\n#########################################################################\n# Download Model\n#########################################################################\nlogger.info('Downloading models...')\ndownload_model = partial(download_model_folder, DATA_FOLDER=MODEL_FOLDER)\n\n# model size:  could be one of 'small' (GPT2 with 117M), 'medium'(345M) or 'large' (1542M)\n# dataset: one of 'multiref' or 'dstc'\n# from_scratch: True : load model trained from scratch or False: load model trained from fine-tuning the GPT-2\ntarget_folder = download_model(model_size='small', dataset='multiref', from_scratch=False)\nlogger.info('Done!\\n')\n\n\n#########################################################################\n# Prepare Data\n#########################################################################\nlogger.info('Downloading and Extracting Data...')\nif dargs.data == 'dummy':\n    cmd = 'bash prepare4db.sh'\n    ret = sp.run(cmd.split(' '), stdout=sp.PIPE, stderr=sp.STDOUT, cwd=DATA_FOLDER)\nelif dargs.data == 'small':\n    myCmd = os.popen('cd reddit_extractor; make -j 8; cd ..').read()\n    cmd = 'gzip -d ./train.tsv.gz'\n    ret = sp.run(cmd.split(' '), stdout=sp.PIPE, stderr=sp.STDOUT, cwd=DATA_FOLDER)\nelif dargs.data == 'full':\n    myCmd = os.popen('cd reddit_extractor; SIZE=full make -j 8; cd ..').read()\n    cmd = 'gzip -d ./train.tsv.gz'\n    ret = sp.run(cmd.split(' '), stdout=sp.PIPE, stderr=sp.STDOUT, cwd=DATA_FOLDER)\nelse:\n    raise ValueError('you need to implement your own data type, or use either dummy, small, or full')\n\nif ret.returncode != 0:\n    print(f'error occurred, {ret.stdout}')\n    sys.exit(ret.returncode)\n\nlogger.info('Preparing Data...')\ndata_path = os.path.join(DATA_FOLDER, 'train.tsv')\nMAX_LEN = 128\ndata_db = f'{data_path[:-4]}.{MAX_LEN}len.db'\nif os.path.isdir(data_db):\n    print(f'{data_db} exists, skip prepro.py')\nelse:\n    cmd = ['prepro.py', '--corpus', data_path, '--max_seq_len', f'{MAX_LEN}']\n    cmd = ' '.join(cmd) #% {'CODE_ROOT': CODE_ROOT}\n    print(cmd)\n    ret = sp.run([PYTHON_EXE] + cmd.split(' '), stdout=sp.PIPE, stderr=sp.STDOUT, cwd=PROJECT_FOLDER)\n    if ret.returncode != 0:\n        print(f'error occurred, {ret.stdout}')\n        sys.exit(ret.returncode)\nlogger.info('Done!\\n')\n\n#########################################################################\n# Train !\n#########################################################################\nlogger.info('Generating training CMD!')\nlogger.info('If there is any problem, please copy (modify) and run command below')\nlogger.info('#########################################################################')\ntrain_cmd = 'LSP_train.py'\nargs = [\n    '--model_name_or_path', target_folder,\n    '--init_checkpoint', os.path.join(target_folder, 'pytorch_model.bin'),\n    '--train_input_file', data_db ,  # file from last step\n    '--eval_input_file', './data/dummy_data.tsv',   # dummy test data\n    '--output_dir', os.path.join(MODEL_FOLDER, 'output_model'),\n    '--seed', '42',\n    '--max_seq_length', '128',\n    '--train_batch_size', '512',\n    '--gradient_accumulation_steps', '8',\n    '--eval_batch_size', '64',\n    '--learning_rate', '1e-5',\n    '--num_optim_steps', '10000',\n    '--valid_step', '5000',\n    '--warmup_steps', '4000',\n    '--normalize_data', 'true',\n    '--fp16', 'true',\n    '--lr_schedule', 'noam',\n    '--loss_scale', '0.0',\n    '--no_token_id', 'true',\n    '--pbar', 'true'\n]\n\narg = ' '.join(args)\ntrain_cmd = train_cmd + ' ' + arg\nprint(PYTHON_EXE + ' ' +train_cmd)\nlogger.info('#########################################################################')\nwith open('./output.log', 'wb') as f: \n    process = sp.Popen([PYTHON_EXE] + train_cmd.split(' '), stdout=sp.PIPE, stderr=sp.STDOUT, cwd=PROJECT_FOLDER)\n    for line in iter(process.stdout.readline, b''): \n        sys.stdout.write(line.decode(sys.stdout.encoding)) \n        f.write(line)\nlogger.info('Done!\\n')\n"""
demo_utils.py,0,"b'#  Copyright (c) Microsoft Corporation.\n#  Licensed under the MIT license.\n\nimport os\nimport logging\n\nfrom pytorch_pretrained_bert.file_utils import http_get\n\n\nlogger = logging.getLogger(__name__)\n\n\n# Note that the model size is roughly half of the GPT model because our model is saved by fp16\nLSP_MODEL_URL = {\n    \'multiref\': {\n        \'large_fs\': \'https://convaisharables.blob.core.windows.net/lsp/multiref/large_fs.pkl\',\n        \'medium_fs\': \'https://convaisharables.blob.core.windows.net/lsp/multiref/medium_fs.pkl\',\n        \'medium_ft\': \'https://convaisharables.blob.core.windows.net/lsp/multiref/medium_ft.pkl\',\n        \'small_fs\': \'https://convaisharables.blob.core.windows.net/lsp/multiref/small_fs.pkl\',\n        \'small_ft\': \'https://convaisharables.blob.core.windows.net/lsp/multiref/small_ft.pkl\'\n    },\n    \'dstc\': {\n        \'medium_ft\': \'https://convaisharables.blob.core.windows.net/lsp/DSTC/medium_ft.pkl\'\n    }\n}\n\n# GPT model could be downloaded from huggingface repo\nGPT2_PRETRAINED_MODEL_ARCHIVE_MAP = {\n    ""small"": ""https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin"",\n    ""medium"": ""https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-pytorch_model.bin"",\n    ""large"": ""https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-pytorch_model.bin""\n}\n\nCONFIG_FILE = {\n    \'small\': \'https://convaisharables.blob.core.windows.net/lsp/117M/config.json\',\n    \'medium\': \'https://convaisharables.blob.core.windows.net/lsp/345M/config.json\',\n    \'large\': \'https://convaisharables.blob.core.windows.net/lsp/1542M/config.json\'\n}\n\nVOCAB_FILE = {\n    \'small\': \'https://convaisharables.blob.core.windows.net/lsp/117M/vocab.json\',\n    \'medium\': \'https://convaisharables.blob.core.windows.net/lsp/345M/vocab.json\',\n    \'large\': \'https://convaisharables.blob.core.windows.net/lsp/1542M/vocab.json\'\n}\n\nMERGE_FILE = {\n    \'small\': \'https://convaisharables.blob.core.windows.net/lsp/117M/merges.txt\',\n    \'medium\': \'https://convaisharables.blob.core.windows.net/lsp/345M/merges.txt\',\n    \'large\': \'https://convaisharables.blob.core.windows.net/lsp/1542M/merges.txt\'\n}\n\n\ndef download_file(url, folder):\n    if not os.path.exists(folder):\n        os.makedirs(folder, exist_ok=True)\n\n    file_name = os.path.basename(url)\n    if \'pytorch_model.bin\' in file_name:\n        file_name = \'pytorch_model.bin\'\n\n    if os.path.isfile(os.path.join(folder, file_name)):\n        logger.info(f\'{os.path.join(folder, file_name)} exists, return!\')\n        return\n\n    with open(os.path.join(folder, file_name), \'wb\') as f:\n        http_get(url, f)\n\n\ndef download_model_folder(model_size, dataset=None, from_scratch=None, DATA_FOLDER=None):\n    assert DATA_FOLDER is not None, \'DATA_FOLDER cannot be None\'\n    assert model_size in [\'small\', \'medium\', \'large\'], \'model size should be one of \\\'small\\\', \\\'medium\\\' or \\\'large\\\'\'\n    target_folder = os.path.join(DATA_FOLDER, model_size)\n    download_file(CONFIG_FILE[model_size], target_folder)\n    download_file(VOCAB_FILE[model_size], target_folder)\n    download_file(MERGE_FILE[model_size], target_folder)\n    download_file(GPT2_PRETRAINED_MODEL_ARCHIVE_MAP[model_size], target_folder)\n    if dataset is not None:\n        assert dataset in [\'multiref\', \'dstc\'], \\\n            \'dataset has to be \\\'multiref\\\' or \\\'dstc\\\'\'\n        assert from_scratch in [True, False], \'from scratch has to be True or False\'\n\n        if from_scratch:\n            model_train_type = model_size + \'_fs\'\n        else:\n            model_train_type = model_size + \'_ft\'\n        if model_train_type not in LSP_MODEL_URL[dataset]:\n            k = \',\'.join(list(LSP_MODEL_URL[dataset].keys()))\n            raise ValueError(f\'\\\'{model_train_type}\\\' not exist for dataset \\\'{dataset}\\\', please choose from [{k}]\')\n        download_file(LSP_MODEL_URL[dataset][model_train_type], target_folder)\n    return target_folder\n\n'"
env.py,0,"b""#  Copyright (c) Microsoft Corporation. \n#  Licensed under the MIT license. \nimport os\n\n\nEND_OF_TURN_TOKEN = '<|endofturn|>'\nEND_OF_TEXT_TOKEN = '<|endoftext|>'\nPROJECT_FOLDER = os.path.dirname(__file__)"""
prepro.py,1,"b'#  Copyright (c) Microsoft Corporation. \n#  Licensed under the MIT license. \n""""""\npreprocess input data into feature and stores binary as python shelve DB\neach chunk is gzipped JSON string\n""""""\nimport argparse\nimport gzip\nimport json\nimport subprocess as sp\nimport shelve\nimport os\nfrom os.path import dirname, exists, join\n\nimport torch\nfrom lsp_model import GPT2Tokenizer\nfrom tqdm import tqdm\n\nfrom env import END_OF_TEXT_TOKEN\nfrom gpt2_training.train_utils import InputFeatures_train as InputFeatures\n\n\ndef _get_file_len(corpus):\n    n_line = int(sp.check_output(f""wc -l {corpus}"".split(),\n                                 universal_newlines=True).split()[0])\n    return n_line\n\n\ndef _norm_text(text):\n    w, *toks = text.strip().split()\n    try:\n        w = float(w)\n    except Exception:\n        toks = [w] + toks\n        w = 1.0\n    return w, \' \'.join(toks)\n\n\ndef _get_inputs_from_text(text, tokenizer):\n    srcs, tgt = text.strip().split(\'\\t\')\n    weights = []\n    inputs = []\n    for src in srcs.split(\' EOS \'):\n        src_weight, src = _norm_text(src)\n        context_id = tokenizer.encode(src)\n        weights.append(src_weight)\n        inputs.append(context_id)\n    tgt_weight, tgt = _norm_text(tgt)\n    if tgt_weight != 0:\n        response_id = tokenizer.encode(tgt)\n        weights.append(tgt_weight)\n        inputs.append(response_id)\n    return weights, inputs\n\n\ndef _make_features(id_, weights, inputs, tokenizer, max_len):\n    end_of_text_id = tokenizer.encoder[END_OF_TEXT_TOKEN]\n    features = []\n    sents = []\n    ws = []\n    len_ = 0\n    i = 0\n    for ids, w in zip(inputs, weights):\n        if len(ids) > max_len:\n            if len(sents) >= 2:\n                feat = _make_feature(id_ + i, sents, ws, end_of_text_id)\n                if feat is not None:\n                    features.append(feat)\n                    i += 1\n            len_ = 0\n            sents = []\n            ws = []\n            continue\n        elif len_ > max_len:\n            feat = _make_feature(id_ + i, sents, ws, end_of_text_id)\n            if feat is not None:\n                features.append(feat)\n                i += 1\n            len_ = len(sents[-1]) + 1\n            sents = sents[-1:]\n            ws = ws[-1:]\n        len_ += (len(ids) + 1)\n        sents.append(ids)\n        ws.append(w)\n    if len(sents) >= 2:\n        feat = _make_feature(id_ + i, sents, ws, end_of_text_id)\n        if feat is not None:\n            features.append(feat)\n\n    return features\n\n\ndef _make_feature(id_, sents, ws, eos):\n    if all(w == 0 for w in ws[1:]):\n        return None\n    input_ids = [i for s in sents for i in s+[eos]][:-1]\n    lm_labels = []\n    weights = []\n    token_type_ids = []  # this becomes round ids\n    for i, (s, w) in enumerate(zip(sents, ws)):\n        if i == 0:\n            lm_labels += [-1] * len(s)\n            weights += [0.0] * len(s)\n            token_type_ids += [0] * len(s)\n            continue\n\n        token_type_ids += [i] * (len(s) + 1)\n        if w == 0.0:\n            lm_labels += [-1] * (len(s) + 1)\n            weights += [0.0] * (len(s) + 1)\n        else:\n            lm_labels += (s + [eos])\n            weights += [w] * (len(s) + 1)\n\n    # handle trailing -1\'s\n    i = len(lm_labels) - 1\n    while i >= 0:\n        if lm_labels[i] != -1:\n            break\n        i -= 1\n    input_ids = input_ids[:i+1]\n    lm_labels = lm_labels[:i+1]\n    weights = weights[:i+1]\n    token_type_ids = token_type_ids[:i+1]\n\n    # pad to multiples of 8\n    while len(input_ids) % 8 != 0:\n        input_ids.append(0)\n        token_type_ids.append(0)\n        lm_labels.append(-1)\n        weights.append(0.0)\n\n    position_ids = list(range(len(input_ids)))\n    assert (len(input_ids) == len(position_ids) == len(token_type_ids)\n            == len(lm_labels) == len(weights))\n    assert len(input_ids) % 8 == 0\n    if len(input_ids) == 0:\n        import pdb\n        pdb.set_trace()\n    feature = InputFeatures(id_, input_ids, position_ids, token_type_ids,\n                            lm_labels, weights)\n    return feature\n\n\ndef main(args):\n    toker = GPT2Tokenizer.from_pretrained(\'gpt2\')\n    attrs = []\n    if args.reverse:\n        attrs.append(\'reverse\')\n    if args.two_turn:\n        attrs.append(\'2turn\')\n    if attrs:\n        db_path = (f\'{args.corpus[:-4]}.{args.max_seq_len}len.\'\n                   f\'{""."".join(attrs)}.db/db\')\n    else:\n        db_path = f\'{args.corpus[:-4]}.{args.max_seq_len}len.db/db\'\n    if exists(dirname(db_path)):\n        raise ValueError(\'Found existing DB, please backup\')\n    else:\n        os.makedirs(dirname(db_path))\n    with open(args.corpus, ""r"", encoding=""utf-8"") as reader, \\\n            shelve.open(db_path, \'n\') as db:\n        chunk = []\n        n_chunk = 0\n        n_example = 0\n        for line in tqdm(reader, total=_get_file_len(args.corpus)):\n            try:\n                if len(chunk) >= args.chunk_size:\n                    # save and renew chunk\n                    db[f\'chunk_{n_chunk}\'] = gzip.compress(\n                        json.dumps(chunk[:args.chunk_size]).encode(\'utf-8\'))\n                    chunk = chunk[args.chunk_size:]\n                    n_chunk += 1\n\n                weights, inputs = _get_inputs_from_text(line, toker)\n                if args.reverse:\n                    weights = list(reversed(weights))\n                    inputs = list(reversed(inputs))\n                if args.two_turn:\n                    weights = weights[:2]\n                    inputs = inputs[:2]\n                if len(weights) < 2:\n                    continue\n                features = _make_features(n_example, weights, inputs,\n                                          toker, args.max_seq_len)\n                for feature in features:\n                    chunk.append(vars(feature))\n                    n_example += 1\n            except Exception as e:\n                print(\'!!! prepro exception !!!\', e)\n                continue\n        # save last chunk\n        db[f\'chunk_{n_chunk}\'] = gzip.compress(\n            json.dumps(chunk).encode(\'utf-8\'))\n    # save relevant information to reproduce\n    meta = {\'n_example\': n_example,\n            \'chunk_size\': args.chunk_size,\n            \'max_seq_len\': args.max_seq_len,\n            \'reverse\': args.reverse,\n            \'two_turn\': args.two_turn}\n    with open(join(dirname(db_path), \'meta.json\'), \'w\') as writer:\n        json.dump(meta, writer, indent=4)\n    torch.save(toker, join(dirname(db_path), \'tokenizer.pt\'))\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--corpus\', required=True,\n                        help=\'file name of training corpus (should be .tsv)\')\n    parser.add_argument(\'--chunk_size\', type=int, default=65536,\n                        help=\'num of data examples in a storing chunk\')\n    parser.add_argument(\'--max_seq_len\', type=int, default=128,\n                        help=\'discard data longer than this\')\n    parser.add_argument(\'--reverse\', action=\'store_true\',\n                        help=\'reverse the src tgt\')\n    parser.add_argument(\'--two_turn\', action=\'store_true\',\n                        help=\'take only the first 2 turns\')\n\n    args = parser.parse_args()\n\n    main(args)\n'"
dstc/batch_eval.py,0,"b'#  Copyright (c) Microsoft Corporation. \r\n#  Licensed under the MIT license. \r\nimport subprocess as sp\r\nimport os\r\nimport argparse\r\nimport glob\r\n\r\nCODE_ROOT = ""./""\r\nPYTHON_EXE = ""python""\r\n\r\n\r\ndef eval(prediction_path, output_path, data_type):\r\n    os.chdir(CODE_ROOT)\r\n    ref_path = f\'{CODE_ROOT}/data/test.refs.txt\'\r\n    key_path = f\'{CODE_ROOT}/data/keys.2k.txt\'\r\n    cmd = [\'dstc.py\',\r\n           prediction_path,\r\n           \'--refs\',\r\n           ref_path,\r\n           \'--keys\',\r\n           key_path,\r\n           \'--clean\']\r\n    cmd = "" "".join(cmd) #% {""CODE_ROOT"": CODE_ROOT}\r\n    print(cmd)\r\n\r\n    ret = sp.run([PYTHON_EXE] + cmd.split("" ""), stdout=sp.PIPE, stderr=sp.STDOUT)\r\n\r\n    with open(output_path, ""w"", encoding=""utf-8"") as out_f:\r\n        out_f.write(ret.stdout.decode(""utf-8""))\r\n\r\n\r\nparser = argparse.ArgumentParser(description=""""""\r\nGiven `input_dir`, create eval directory under it.\r\nfor each *.predicted.txt under `input_dir`, evaluate it, push the evaluation result to files under eval directory.\r\nThe file name is *.eval.txt\r\n"""""")\r\nparser.add_argument(""--input_dir"", type=str, default=""./"")\r\nparser.add_argument(""--data_type"", type=str, default=""test"", help=""could be \'valid\' or \'test\' "")\r\n\r\nargs = parser.parse_args()\r\nassert args.data_type in (""valid"", ""test"")\r\n\r\noutput_dir = os.path.join(args.input_dir, ""eval"")\r\nif not os.path.exists(output_dir):\r\n    os.makedirs(output_dir)\r\n\r\nresp_dir = os.path.join(args.input_dir, ""resp"")\r\nif not os.path.exists(resp_dir):\r\n    os.makedirs(resp_dir)\r\n\r\nfor prediction_path in glob.glob(os.path.join(args.input_dir, ""*.resp.txt"")):\r\n    resp_path = os.path.join(resp_dir , os.path.basename(prediction_path))\r\n    cmd = [\'less\',\r\n           prediction_path,\r\n           \'|grep -v ""^Val""\',\r\n           \'|sed \\\'/^$/d\\\'\',\r\n           \'>\',\r\n           resp_path]\r\n    cmd = "" "".join(cmd)\r\n    print(cmd)\r\n    ret = sp.Popen(cmd, shell=True, stdout=sp.PIPE, stderr=sp.STDOUT)\r\n    output = ret.communicate()[0]\r\n    print(output)\r\n    output_name = os.path.basename(prediction_path).replace("".resp.txt"", "".eval.txt"")\r\n    output_path = os.path.join(output_dir, output_name)\r\n    eval(resp_path, output_path, args.data_type)\r\n\r\n'"
dstc/dstc.py,0,"b'#  Copyright (c) Microsoft Corporation. \n#  Licensed under the MIT license. \n#  Evaluate DSTC-task2 submissions. https://github.com/DSTC-MSR-NLP/DSTC7-End-to-End-Conversation-Modeling\n\nfrom util import *\nfrom metrics import *\nfrom tokenizers import *\n\ndef extract_cells(path_in, path_hash):\n\tkeys = [line.strip(\'\\n\') for line in open(path_hash)]\n\tcells = dict()\n\tfor line in open(path_in, encoding=\'utf-8\'):\n\t\tc = line.strip(\'\\n\').split(\'\\t\')\n\t\tk = c[0]\n\t\tif k in keys:\n\t\t\tcells[k] = c[1:]\n\treturn cells\n\ndef extract_linc_cells(path_in, path_hash):\n\tif ""valid"" in path_hash:\n\t\tcells = dict()\n\t\texternal_keys = [k.strip() for k in open(r""./data/processed/valid.keys.txt"")]\n\t\tfor no, line in enumerate(open(path_in, encoding=\'utf-8\')):\n\t\t\tc = line.strip(\'\\n\')\n\t\t\tk = external_keys[no]\n\t\t\tcells[k] = [c]\n\telse:\n\t\tkeys = set([line.strip(\'\\n\') for line in open(path_hash)])\n\t\tcells = dict()\n\t\texternal_keys = [k.strip() for k in open(r""./data/processed/test_real.keys.txt"")]\n\t\tfor no, line in enumerate(open(path_in, encoding=\'utf-8\')):\n\t\t\tc = line.strip(\'\\n\')\n\t\t\tk = external_keys[no]\n\t\t\tif k in keys:\n\t\t\t\tcells[k] = [c]\n\treturn cells\n\n\ndef extract_hyp_refs(raw_hyp, raw_ref, path_hash, fld_out, n_refs=6, clean=False, vshuman=-1):\n\tcells_hyp = extract_linc_cells(raw_hyp, path_hash)\n\tcells_ref = extract_cells(raw_ref, path_hash)\n\tif not os.path.exists(fld_out):\n\t\tos.makedirs(fld_out)\n\n\tdef _clean(s):\n\t\tif clean:\n\t\t\treturn clean_str(s)\n\t\telse:\n\t\t\treturn s\n\n\tkeys = sorted(cells_hyp.keys())\n\twith open(fld_out + \'/hash.txt\', \'w\', encoding=\'utf-8\') as f:\n\t\tf.write(unicode(\'\\n\'.join(keys)))\n\n\tlines = [_clean(cells_hyp[k][-1]) for k in keys]\n\tpath_hyp = fld_out + \'/hyp.txt\'\n\twith open(path_hyp, \'w\', encoding=\'utf-8\') as f:\n\t\tf.write(unicode(\'\\n\'.join(lines)))\n\t\n\tlines = []\n\tfor _ in range(n_refs):\n\t\tlines.append([])\n\tfor k in keys:\n\t\trefs = cells_ref[k]\n\t\tfor i in range(n_refs):\n\t\t\tidx = i % len(refs)\n\t\t\tif idx == vshuman:\n\t\t\t    idx = (idx + 1) % len(refs)\n\t\t\tif ""|"" in refs[idx]:\n\t\t\t\tfinal_ref = refs[idx].split(\'|\')[1]\n\t\t\telse:\n\t\t\t\tfinal_ref = refs[idx]\n\t\t\tlines[i].append(_clean(final_ref))\n\n\tpath_refs = []\n\tfor i in range(n_refs):\n\t\tpath_ref = fld_out + \'/ref%i.txt\'%i\n\t\twith open(path_ref, \'w\', encoding=\'utf-8\') as f:\n\t\t\tf.write(unicode(\'\\n\'.join(lines[i])))\n\t\tpath_refs.append(path_ref)\n\n\treturn path_hyp, path_refs\n\n\ndef eval_one_system(submitted, keys, multi_ref, n_refs=6, n_lines=None, clean=False, vshuman=-1, PRINT=True):\n\n\tprint(\'evaluating %s\' % submitted)\n\n\tfld_out = submitted.replace(\'.txt\',\'\')\n\tif clean:\n\t\tfld_out += \'_cleaned\'\n\tpath_hyp, path_refs = extract_hyp_refs(submitted, multi_ref, keys, fld_out, n_refs, clean=clean, vshuman=vshuman)\n\tnist, bleu, meteor, entropy, div, avg_len = nlp_metrics(path_refs, path_hyp, fld_out, n_lines=n_lines)\n\t\n\tif n_lines is None:\n\t\tn_lines = len(open(path_hyp, encoding=\'utf-8\').readlines())\n\n\tif PRINT:\n\t\tprint(\'n_lines = \'+str(n_lines))\n\t\tprint(\'NIST = \'+str(nist))\n\t\tprint(\'BLEU = \'+str(bleu))\n\t\tprint(\'METEOR = \'+str(meteor))\n\t\tprint(\'entropy = \'+str(entropy))\n\t\tprint(\'diversity = \' + str(div))\n\t\tprint(\'avg_len = \'+str(avg_len))\n\n\treturn [n_lines] + nist + bleu + [meteor] + entropy + div + [avg_len]\n\n\ndef eval_all_systems(files, path_report, keys, multi_ref, n_refs=6, n_lines=None, clean=False, vshuman=False):\n\t# evaluate all systems (*.txt) in each folder `files`\n\n\twith open(path_report, \'w\') as f:\n\t\tf.write(\'\\t\'.join(\n\t\t\t\t[\'fname\', \'n_lines\'] + \\\n\t\t\t\t[\'nist%i\'%i for i in range(1, 4+1)] + \\\n\t\t\t\t[\'bleu%i\'%i for i in range(1, 4+1)] + \\\n\t\t\t\t[\'meteor\'] + \\\n\t\t\t\t[\'entropy%i\'%i for i in range(1, 4+1)] +\\\n\t\t\t\t[\'div1\',\'div2\',\'avg_len\']\n\t\t\t) + \'\\n\')\n\n\tfor fl in files:\n\t\tif fl.endswith(\'.txt\'):\n\t\t\tsubmitted = fl\n\t\t\tresults = eval_one_system(submitted, keys=keys, multi_ref=multi_ref, n_refs=n_refs, clean=clean, n_lines=n_lines, vshuman=vshuman, PRINT=False)\n\t\t\twith open(path_report, \'a\') as f:\n\t\t\t\tf.write(\'\\t\'.join(map(str, [submitted] + results)) + \'\\n\')\n\t\telse:\n\t\t\tfor fname in os.listdir(fl):\n\t\t\t\tif fname.endswith(\'.txt\'):\n\t\t\t\t\tsubmitted = fl + \'/\' + fname\n\t\t\t\t\tresults = eval_one_system(submitted, keys=keys, multi_ref=multi_ref, n_refs=n_refs, clean=clean, n_lines=n_lines, vshuman=vshuman, PRINT=False)\n\t\t\t\t\twith open(path_report, \'a\') as f:\n\t\t\t\t\t\tf.write(\'\\t\'.join(map(str, [submitted] + results)) + \'\\n\')\n\n\tprint(\'report saved to: \'+path_report, file=sys.stderr)\n\n\nif __name__ == \'__main__\':\n\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\'submitted\')\t# if \'all\' or \'*\', eval all teams listed in dstc/teams.txt\n\t                                    # elif endswith \'.txt\', eval this single file\n\t                                    # else, eval all *.txt in folder `submitted_fld`\n\n\tparser.add_argument(\'--clean\', \'-c\', action=\'store_true\')     # whether to clean ref and hyp before eval\n\tparser.add_argument(\'--n_lines\', \'-n\', type=int, default=-1)  # eval all lines (default) or top n_lines (e.g., for fast debugging)\n\tparser.add_argument(\'--n_refs\', \'-r\', type=int, default=6)    # number of references\n\tparser.add_argument(\'--vshuman\', \'-v\', type=int, default=\'1\') # when evaluating against human performance (N in refN.txt that should be removed) \n\t                                                                      # in which case we need to remove human output from refs\n\tparser.add_argument(\'--refs\', \'-g\', default=\'dstc/test.refs\')\n\tparser.add_argument(\'--keys\', \'-k\', default=\'keys/test.2k.txt\')\n\tparser.add_argument(\'--teams\', \'-i\', type=str, default=\'dstc/teams.txt\')\n\tparser.add_argument(\'--report\', \'-o\', type=str, default=None)\n\targs = parser.parse_args()\n\tprint(\'Args: %s\\n\' % str(args), file=sys.stderr)\n\n\tif args.n_lines < 0:\n\t\tn_lines = None\t# eval all lines\n\telse:\n\t\tn_lines = args.n_lines\t# just eval top n_lines\n\n\tif args.submitted.endswith(\'.txt\'):\n\t\teval_one_system(args.submitted, keys=args.keys, multi_ref=args.refs, clean=args.clean, n_lines=n_lines, n_refs=args.n_refs, vshuman=args.vshuman)\n\telse:\n\t\tfname_report = \'report_ref%i\'%args.n_refs\n\t\tif args.clean:\n\t\t\tfname_report += \'_cleaned\'\n\t\tfname_report += \'.tsv\'\n\t\tif args.submitted == \'all\' or args.submitted == \'*\':\n\t\t\tfiles = [\'dstc/\' + line.strip(\'\\n\') for line in open(args.teams)]\n\t\t\tpath_report = \'dstc/\' + fname_report\n\t\telse:\n\t\t\tfiles = [args.submitted]\n\t\t\tpath_report = args.submitted + \'/\' + fname_report\n\t\tif args.report != None:\n\t\t\tpath_report = args.report\n\t\teval_all_systems(files, path_report, keys=args.keys, multi_ref=args.refs, clean=args.clean, n_lines=n_lines, n_refs=args.n_refs, vshuman=args.vshuman)\n'"
dstc/extract_human.py,0,"b'#  Copyright (c) Microsoft Corporation. \n#  Licensed under the MIT license. \n#  Evaluate DSTC-task2 submissions. https://github.com/DSTC-MSR-NLP/DSTC7-End-to-End-Conversation-Modeling\n\nfrom tokenizers import *\n\nkey_file = ""./data/processed/test_real.keys.txt""\n\nhuman_hyp = ""human.resp.txt""\n\nrefs = ""./data/test.refs.txt""\n\nall_keys = []\n\nwith open(key_file, \'r\') as keys:\n\tfor k in iter(keys):\n\t\tall_keys.append(k)\n\nall_lines = {}\nwith open(refs, \'r\', encoding=\'utf-8\') as all_refs:\n\tfor i,r in enumerate(all_refs):\n\t\t# import pdb; pdb.set_trace()\n\t\tkey = r.split(\'\\t\')[0]\n\t\ttry:\t\n\t\t\tline = r.split(\'\\t\')[2].split(\'|\')[1] #clean_str()\n\t\t\tall_lines[key] = line\n\t\texcept:\n\t\t\tprint(key)\n\t\t\tpass\n\n\t\t\t\nwith open(human_hyp, \'w\') as f:\t\n\tfor key in all_keys:\n\t\tf.write(all_lines[key.strip()] + u\'\\n\')\t\n\n\n'"
dstc/metrics.py,0,"b'#  Copyright (c) Microsoft Corporation. \n#  Licensed under the MIT license. \n\nimport re\nfrom util import *\nfrom collections import defaultdict\n\n\ndef calc_nist_bleu(path_refs, path_hyp, fld_out=\'temp\', n_lines=None):\n\t# call mteval-v14c.pl\n\t# ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v14c.pl\n\t# you may need to cpan install XML:Twig Sort:Naturally String:Util \n\n\tmakedirs(fld_out)\n\n\tif n_lines is None:\n\t\tn_lines = len(open(path_refs[0], encoding=\'utf-8\').readlines())\t\n\t# import pdb; pdb.set_trace()\n\t_write_xml([\'\'], fld_out + \'/src.xml\', \'src\', n_lines=n_lines)\n\t_write_xml([path_hyp], fld_out + \'/hyp.xml\', \'hyp\')#, n_lines=n_lines)\n\t_write_xml(path_refs, fld_out + \'/ref.xml\', \'ref\')#, n_lines=n_lines)\n\n\ttime.sleep(1)\n\tcmd = [\n\t\t\'perl\',\'3rdparty/mteval-v14c.pl\',\n\t\t\'-s\', \'%s/src.xml\'%fld_out,\n\t\t\'-t\', \'%s/hyp.xml\'%fld_out,\n\t\t\'-r\', \'%s/ref.xml\'%fld_out,\n\t\t]\n\tprocess = subprocess.Popen(cmd, stdout=subprocess.PIPE)\n\t# import pdb; pdb.set_trace()\n\toutput, error = process.communicate()\n\n\tlines = output.decode().split(\'\\n\')\n\n\ttry:\n\t\tnist = lines[-6].strip(\'\\r\').split()[1:5]\n\t\tbleu = lines[-4].strip(\'\\r\').split()[1:5]\n\t\treturn [float(x) for x in nist], [float(x) for x in bleu]\n\n\texcept Exception:\n\t\tprint(\'mteval-v14c.pl returns unexpected message\')\n\t\tprint(\'cmd = \'+str(cmd))\n\t\tprint(output.decode())\n\t\tprint(error.decode())\n\t\treturn [-1]*4, [-1]*4\n\n\t\n\n\ndef calc_cum_bleu(path_refs, path_hyp):\n\t# call multi-bleu.pl\n\t# https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.perl\n\t# the 4-gram cum BLEU returned by this one should be very close to calc_nist_bleu\n\t# however multi-bleu.pl doesn\'t return cum BLEU of lower rank, so in nlp_metrics we preferr calc_nist_bleu\n\t# NOTE: this func doesn\'t support n_lines argument and output is not parsed yet\n\n\tprocess = subprocess.Popen(\n\t\t\t[\'perl\', \'3rdparty/multi-bleu.perl\'] + path_refs, \n\t\t\tstdout=subprocess.PIPE, \n\t\t\tstdin=subprocess.PIPE\n\t\t\t)\n\twith open(path_hyp, encoding=\'utf-8\') as f:\n\t\tlines = f.readlines()\n\tfor line in lines:\n\t\tprocess.stdin.write(line.encode())\n\toutput, error = process.communicate()\n\treturn output.decode()\n\n\ndef calc_meteor(path_refs, path_hyp, fld_out=\'temp\', n_lines=None, pretokenized=True):\n\t# Call METEOR code.\n\t# http://www.cs.cmu.edu/~alavie/METEOR/index.html\n\n\tmakedirs(fld_out)\n\tpath_merged_refs = fld_out + \'/refs_merged.txt\'\n\t_write_merged_refs(path_refs, path_merged_refs)\n\tcmd = [\n\t\t\t\'java\', \'-Xmx1g\',\t# heapsize of 1G to avoid OutOfMemoryError\n\t\t\t\'-jar\', \'3rdparty/meteor-1.5/meteor-1.5.jar\', \n\t\t\tpath_hyp, path_merged_refs, \n\t\t\t\'-r\', \'%i\'%len(path_refs), \t# refCount \n\t\t\t\'-l\', \'en\', \'-norm\' \t# also supports language: cz de es fr ar\n\t\t\t]\n\tprint(cmd)\n\tprocess = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\toutput, error = process.communicate()\n\tfor line in output.decode().split(\'\\n\'):\n\t\tif ""Final score:"" in line:\n\t\t\treturn float(line.split()[-1])\n\n\tprint(\'meteor-1.5.jar returns unexpected message\')\n\tprint(""cmd = "" + "" "".join(cmd))\n\tprint(output.decode())\n\tprint(error.decode())\n\treturn -1 \n\n\ndef calc_entropy(path_hyp, n_lines=None):\n\t# based on Yizhe Zhang\'s code\n\tetp_score = [0.0,0.0,0.0,0.0]\n\tcounter = [defaultdict(int),defaultdict(int),defaultdict(int),defaultdict(int)]\n\ti = 0\n\tfor line in open(path_hyp, encoding=\'utf-8\'):\n\t\ti += 1\n\t\twords = line.strip(\'\\n\').split()\n\t\tfor n in range(4):\n\t\t\tfor idx in range(len(words)-n):\n\t\t\t\tngram = \' \'.join(words[idx:idx+n+1])\n\t\t\t\tcounter[n][ngram] += 1\n\t\tif i == n_lines:\n\t\t\tbreak\n\n\tfor n in range(4):\n\t\ttotal = sum(counter[n].values())\n\t\tfor v in counter[n].values():\n\t\t\tetp_score[n] += - v /total * (np.log(v) - np.log(total))\n\n\treturn etp_score\n\n\ndef calc_len(path, n_lines):\n\tl = []\n\tfor line in open(path, encoding=\'utf8\'):\n\t\tl.append(len(line.strip(\'\\n\').split()))\n\t\tif len(l) == n_lines:\n\t\t\tbreak\n\treturn np.mean(l)\n\n\ndef calc_diversity(path_hyp):\n\ttokens = [0.0,0.0]\n\ttypes = [defaultdict(int),defaultdict(int)]\n\tfor line in open(path_hyp, encoding=\'utf-8\'):\n\t\twords = line.strip(\'\\n\').split()\n\t\tfor n in range(2):\n\t\t\tfor idx in range(len(words)-n):\n\t\t\t\tngram = \' \'.join(words[idx:idx+n+1])\n\t\t\t\ttypes[n][ngram] = 1\n\t\t\t\ttokens[n] += 1\n\tdiv1 = len(types[0].keys())/tokens[0]\n\tdiv2 = len(types[1].keys())/tokens[1]\n\treturn [div1, div2]\n\n\ndef nlp_metrics(path_refs, path_hyp, fld_out=\'temp\',  n_lines=None):\n\tnist, bleu = calc_nist_bleu(path_refs, path_hyp, fld_out, n_lines)\n\tmeteor = calc_meteor(path_refs, path_hyp, fld_out, n_lines)\n\tentropy = calc_entropy(path_hyp, n_lines)\n\tdiv = calc_diversity(path_hyp)\n\tavg_len = calc_len(path_hyp, n_lines)\n\treturn nist, bleu, meteor, entropy, div, avg_len\n\n\ndef _write_merged_refs(paths_in, path_out, n_lines=None):\n\t# prepare merged ref file for meteor-1.5.jar (calc_meteor)\n\t# lines[i][j] is the ref from i-th ref set for the j-th query\n\n\tlines = []\n\tfor path_in in paths_in:\n\t\tlines.append([line.strip(\'\\n\') for line in open(path_in, encoding=\'utf-8\')])\n\n\twith open(path_out, \'w\', encoding=\'utf-8\') as f:\n\t\tfor j in range(len(lines[0])):\n\t\t\tfor i in range(len(paths_in)):\n\t\t\t\tf.write(unicode(lines[i][j]) + ""\\n"")\n\n\n\ndef _write_xml(paths_in, path_out, role, n_lines=None):\n\t# prepare .xml files for mteval-v14c.pl (calc_nist_bleu)\n\t# role = \'src\', \'hyp\' or \'ref\'\n\n\tlines = [\n\t\t\'<?xml version=""1.0"" encoding=""UTF-8""?>\',\n\t\t\'<!DOCTYPE mteval SYSTEM """">\',\n\t\t\'<!-- generated by https://github.com/golsun/NLP-tools -->\',\n\t\t\'<!-- from: %s -->\'%paths_in,\n\t\t\'<!-- as inputs for ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v14c.pl -->\',\n\t\t\'<mteval>\',\n\t\t]\n\n\tfor i_in, path_in in enumerate(paths_in):\n\n\t\t# header ----\n\n\t\tif role == \'src\':\n\t\t\tlines.append(\'<srcset setid=""unnamed"" srclang=""src"">\')\n\t\t\tset_ending = \'</srcset>\'\n\t\telif role == \'hyp\':\n\t\t\tlines.append(\'<tstset setid=""unnamed"" srclang=""src"" trglang=""tgt"" sysid=""unnamed"">\')\n\t\t\tset_ending = \'</tstset>\'\n\t\telif role == \'ref\':\n\t\t\tlines.append(\'<refset setid=""unnamed"" srclang=""src"" trglang=""tgt"" refid=""ref%i"">\'%i_in)\n\t\t\tset_ending = \'</refset>\'\n\t\t\n\t\tlines.append(\'<doc docid=""unnamed"" genre=""unnamed"">\')\n\n\t\t# body -----\n\n\t\tif role == \'src\':\n\t\t\tbody = [\'__src__\'] * n_lines\n\t\telse:\n\t\t\twith open(path_in, \'r\', encoding=\'utf-8\') as f:\n\t\t\t\tbody = f.readlines()\n\t\t\tif n_lines is not None:\n\t\t\t\tbody = body[:n_lines]\n\t\t#for i in range(len(body)):\n\t\ti = 0\n\t\tfor b in body:\n\t\t\tline = b.strip(\'\\n\')\n\t\t\tline = line.replace(\'&\',\' \').replace(\'<\',\' \')\t\t# remove illegal xml char\n\t\t\t# if len(line) > 0:\n\t\t\tlines.append(\'<p><seg id=""%i""> %s </seg></p>\'%(i + 1, line))\n\t\t\ti += 1\n\n\t\t# ending -----\n\n\t\tlines.append(\'</doc>\')\n\t\tif role == \'src\':\n\t\t\tlines.append(\'</srcset>\')\n\t\telif role == \'hyp\':\n\t\t\tlines.append(\'</tstset>\')\n\t\telif role == \'ref\':\n\t\t\tlines.append(\'</refset>\')\n\n\tlines.append(\'</mteval>\')\n\twith open(path_out, \'w\', encoding=\'utf-8\') as f:\n\t\tf.write(unicode(\'\\n\'.join(lines)))\n'"
dstc/tokenizers.py,0,"b'#  Copyright (c) Microsoft Corporation. \n#  Licensed under the MIT license. \n\nimport re\nfrom util import *\nfrom nltk.tokenize import TweetTokenizer\n\ndef clean_str(txt):\n\t#print(""in=[%s]"" % txt)\n\ttxt = txt.lower()\n\ttxt = re.sub(\'^\',\' \', txt)\n\ttxt = re.sub(\'$\',\' \', txt)\n\n\t# url and tag\n\twords = []\n\tfor word in txt.split():\n\t\ti = word.find(\'http\') \n\t\tif i >= 0:\n\t\t\tword = word[:i] + \' \' + \'__url__\'\n\t\twords.append(word.strip())\n\ttxt = \' \'.join(words)\n\n\t# remove markdown URL\n\ttxt = re.sub(r\'\\[([^\\]]*)\\] \\( *__url__ *\\)\', r\'\\1\', txt)\n\n\t# remove illegal char\n\ttxt = re.sub(\'__url__\',\'URL\',txt)\n\ttxt = re.sub(r""[^A-Za-z0-9():,.!?\\""\\\']"", "" "", txt)\n\ttxt = re.sub(\'URL\',\'__url__\',txt)\t\n\n\t# contraction\n\tadd_space = [""\'s"", ""\'m"", ""\'re"", ""n\'t"", ""\'ll"",""\'ve"",""\'d"",""\'em""]\n\ttokenizer = TweetTokenizer(preserve_case=False)\n\ttxt = \' \' + \' \'.join(tokenizer.tokenize(txt)) + \' \'\n\ttxt = txt.replace("" won\'t "", "" will n\'t "")\n\ttxt = txt.replace("" can\'t "", "" can n\'t "")\n\tfor a in add_space:\n\t\ttxt = txt.replace(a+\' \', \' \'+a+\' \')\n\n\ttxt = re.sub(r\'^\\s+\', \'\', txt)\n\ttxt = re.sub(r\'\\s+$\', \'\', txt)\n\ttxt = re.sub(r\'\\s+\', \' \', txt) # remove extra spaces\n\t\n\t#print(""out=[%s]"" % txt)\n\treturn txt\n\n\nif __name__ == \'__main__\':\n\tss = [\n\t\t"" I don\'t know:). how about this?https://github.com/golsun/deep-RL-time-series"",\n\t\t""please try [ GitHub ] ( https://github.com )"",\n\t\t]\n\tfor s in ss:\n\t\tprint(s)\n\t\tprint(clean_str(s))\n\t\tprint()\n\t\n'"
dstc/util.py,0,"b""#  Copyright (c) Microsoft Corporation. \n#  Licensed under the MIT license. \nimport os, time, subprocess, io, sys, re, argparse\nimport numpy as np\n\npy_version = sys.version.split('.')[0]\nif py_version == '2':\n\topen = io.open\nelse:\n\tunicode = str\n\ndef makedirs(fld):\n\tif not os.path.exists(fld):\n\t\tos.makedirs(fld)\n\n\ndef str2bool(s):\n\t# to avoid issue like this: https://stackoverflow.com/questions/15008758/parsing-boolean-values-with-argparse\n\tif s.lower() in ['t','true','1','y']:\n\t\treturn True\n\telif s.lower() in ['f','false','0','n']:\n\t\treturn False\n\telse:\n\t\traise ValueError"""
gpt2_training/distributed.py,10,"b'#  Copyright (c) Microsoft Corporation. \n#  Licensed under the MIT license. \n"""""" Pytorch Distributed utils\n    # NOTE: copied from OpenNMT-py\n    This piece of code was heavily inspired by the equivalent of Fairseq-py\n    https://github.com/pytorch/fairseq\n""""""\n\n\nfrom __future__ import print_function\n\nimport math\nimport pickle\nimport torch.distributed\n\n\ndef is_master(opt, device_id):\n    return opt.gpu_ranks[device_id] == 0\n\n\ndef multi_init(opt, device_id, logger=None):\n    dist_init_method = \'tcp://{master_ip}:{master_port}\'.format(\n        master_ip=opt.master_ip,\n        master_port=opt.master_port)\n    dist_world_size = opt.world_size\n    torch.distributed.init_process_group(\n        backend=opt.gpu_backend, init_method=dist_init_method,\n        world_size=dist_world_size, rank=opt.gpu_ranks[device_id])\n    gpu_rank = torch.distributed.get_rank()\n    if not is_master(opt, device_id) and logger is not None:\n        logger.disabled = True\n\n    return gpu_rank\n\n\ndef all_reduce_and_rescale_tensors(tensors, rescale_denom,\n                                   buffer_size=10485760):\n    """"""All-reduce and rescale tensors in chunks of the specified size.\n\n    Args:\n        tensors: list of Tensors to all-reduce\n        rescale_denom: denominator for rescaling summed Tensors\n        buffer_size: all-reduce chunk size in bytes\n    """"""\n    # buffer size in bytes, determine equiv. # of elements based on data type\n    buffer_t = tensors[0].new(\n        math.ceil(buffer_size / tensors[0].element_size())).zero_()\n    buffer = []\n\n    def all_reduce_buffer():\n        # copy tensors into buffer_t\n        offset = 0\n        for t in buffer:\n            numel = t.numel()\n            buffer_t[offset:offset+numel].copy_(t.view(-1))\n            offset += numel\n\n        # all-reduce and rescale\n        torch.distributed.all_reduce(buffer_t[:offset])\n        buffer_t.div_(rescale_denom)\n\n        # copy all-reduced buffer back into tensors\n        offset = 0\n        for t in buffer:\n            numel = t.numel()\n            t.view(-1).copy_(buffer_t[offset:offset+numel])\n            offset += numel\n\n    filled = 0\n    for t in tensors:\n        sz = t.numel() * t.element_size()\n        if sz > buffer_size:\n            # tensor is bigger than buffer, all-reduce and rescale directly\n            torch.distributed.all_reduce(t)\n            t.div_(rescale_denom)\n        elif filled + sz > buffer_size:\n            # buffer is full, all-reduce and replace buffer with grad\n            all_reduce_buffer()\n            buffer = [t]\n            filled = sz\n        else:\n            # add tensor to buffer\n            buffer.append(t)\n            filled += sz\n\n    if len(buffer) > 0:\n        all_reduce_buffer()\n\n\ndef all_gather_list(data, max_size=4096):\n    """"""Gathers arbitrary data from all nodes into a list.""""""\n    world_size = torch.distributed.get_world_size()\n    if not hasattr(all_gather_list, \'_in_buffer\') or \\\n            max_size != all_gather_list._in_buffer.size():\n        all_gather_list._in_buffer = torch.cuda.ByteTensor(max_size)\n        all_gather_list._out_buffers = [\n            torch.cuda.ByteTensor(max_size)\n            for i in range(world_size)\n        ]\n    in_buffer = all_gather_list._in_buffer\n    out_buffers = all_gather_list._out_buffers\n\n    enc = pickle.dumps(data)\n    enc_size = len(enc)\n    if enc_size + 2 > max_size:\n        raise ValueError(\n            \'encoded data exceeds max_size: {}\'.format(enc_size + 2))\n    assert max_size < 255*256\n    in_buffer[0] = enc_size // 255  # this encoding works for max_size < 65k\n    in_buffer[1] = enc_size % 255\n    in_buffer[2:enc_size+2] = torch.ByteTensor(list(enc))\n\n    torch.distributed.all_gather(out_buffers, in_buffer.cuda())\n\n    results = []\n    for i in range(world_size):\n        out_buffer = out_buffers[i]\n        size = (255 * out_buffer[0].item()) + out_buffer[1].item()\n\n        bytes_list = bytes(out_buffer[2:size+2].tolist())\n        result = pickle.loads(bytes_list)\n        results.append(result)\n    return results\n'"
gpt2_training/eval_utils.py,1,"b'#  Copyright (c) Microsoft Corporation. \n#  Licensed under the MIT license. \nimport torch\nimport logging\n\nimport numpy as np\n\nfrom pycocoevalcap.bleu.bleu import Bleu\nfrom collections import defaultdict\n\nlogger = logging.getLogger(__name__)\n\nEOS_ID = 50256\n\n\ndef cal_BLEU_4(generated, reference, is_corpus=False):\n    BLEUscore = [0.0, 0.0, 0.0, 0.0]\n    for idx, g in enumerate(generated):\n        if is_corpus:\n            score, scores = Bleu(4).compute_score(reference, {0: [g]})\n        else:\n            score, scores = Bleu(4).compute_score({0: [reference[0][idx]]},\n                                                  {0: [g]})\n        for i, s in zip([0, 1, 2, 3], score):\n            BLEUscore[i] += s\n    BLEUscore[0] = BLEUscore[0]/len(generated)\n    BLEUscore[1] = BLEUscore[1]/len(generated)\n    BLEUscore[2] = BLEUscore[2]/len(generated)\n    BLEUscore[3] = BLEUscore[3]/len(generated)\n    return BLEUscore\n\n\ndef cal_entropy(generated):\n    etp_score = [0.0, 0.0, 0.0, 0.0]\n    div_score = [0.0, 0.0, 0.0, 0.0]\n    counter = [defaultdict(int), defaultdict(int),\n               defaultdict(int), defaultdict(int)]\n    for gg in generated:\n        g = gg.rstrip().split()\n        for n in range(4):\n            for idx in range(len(g)-n):\n                ngram = \' \'.join(g[idx:idx+n+1])\n                counter[n][ngram] += 1\n    for n in range(4):\n        total = sum(counter[n].values()) + 1e-10\n        for v in counter[n].values():\n            etp_score[n] += - (v+0.0) / total * (np.log(v+0.0) - np.log(total))\n        div_score[n] = (len(counter[n].values())+0.0) / total\n    return etp_score, div_score\n\n\ndef eval_model_loss(model, tokenizer, eval_dataloader, epoch_id, args):\n    # use the same signature with eval_model_generation\n    logger.info(\'compute eval model loss, using eval mode, \'\n                \'please change it back to train after calling this function\')\n    model.eval()\n    tot_loss = []\n    tot_ppl = []\n    tot_sample = []\n    with torch.no_grad():\n        for step, batch in enumerate(eval_dataloader):\n            batch = tuple(t.to(args.device) for t in batch)\n            input_ids, position_ids, token_ids, label_ids, src_len, _ = batch\n            if args.no_token_id:\n                token_ids = None\n            n_sample = input_ids.shape[0]\n            loss, ppl = model(input_ids, position_ids, token_ids, label_ids)\n            tot_loss.append(loss.mean().item() * n_sample)\n            tot_ppl.append(ppl.mean().item() * n_sample)\n            tot_sample.append(n_sample)\n    print(f""\\n Epoch {epoch_id}: Val loss {np.sum(tot_loss) / np.sum(tot_sample)} Val ppl {np.sum(tot_ppl) / np.sum(tot_sample)} "")\n    return np.sum(tot_loss) / np.sum(tot_sample), np.sum(tot_ppl) / np.sum(tot_sample)\n'"
gpt2_training/train_utils.py,15,"b'#  Copyright (c) Microsoft Corporation. \n#  Licensed under the MIT license. \n\nimport os\nimport logging\nimport torch\nfrom collections import defaultdict\n\nfrom env import END_OF_TEXT_TOKEN\nfrom lsp_model.optim import warmup_linear, noam_decay, noamwd_decay\n\n\nlogger = logging.getLogger(__name__)\n\nSEQ_LENGTH_SHRINK_PROP = 0.9\n\n\ndef load_model(model, checkpoint, args, verbose=False):\n    n_gpu = args.n_gpu\n    device = args.device\n    if checkpoint is None or checkpoint == ""None"":\n        if verbose:\n            logger.info(\'no checkpoint provided for %s!\' % model._get_name())\n    else:\n        if not os.path.exists(checkpoint):\n            raise ValueError(\'checkpoint %s not exist\' % checkpoint)\n        if verbose:\n            logger.info(\'loading finetuned model from %s\' % checkpoint)\n        model_state_dict = torch.load(checkpoint)\n\n        model_state_dict = fix_state_dict_namespace(model_state_dict)\n\n        start_model = model\n        if (hasattr(model, ""transformer"")\n            and all(not s.startswith(\'transformer.\')\n                    for s in model_state_dict.keys())):\n            logger.info(\'loading transfomer only\')\n            start_model = model.transformer\n        start_model.load_state_dict(model_state_dict)\n\n    if args.fp16:\n        logger.info(\'in fp16, model.half() activated\')\n        model.half()\n    model.to(device)\n    if n_gpu > 1:\n        logging.info(\'data parallel because more than one gpu\')\n        model = torch.nn.DataParallel(model)\n    return model\n\n\ndef fix_state_dict_namespace(model_state_dict):\n    old_keys = []\n    new_keys = []\n    for t in model_state_dict:\n        new_key = t\n        if t.startswith(\'module.\'):\n            new_key = t.replace(\'module.\', \'\')\n        old_keys.append(t)\n        new_keys.append(new_key)\n\n    for old_key, new_key in zip(old_keys, new_keys):\n        model_state_dict[new_key] = model_state_dict.pop(old_key)\n\n    return model_state_dict\n\n\nclass InputFeatures(object):\n    def __init__(self, conv_id, input_ids, position_ids, token_type_ids,\n                 lm_labels, context_len, response_len):\n        self.conv_id = conv_id\n        self.choices_features = {\n            \'input_ids\': input_ids,\n            \'position_ids\': position_ids,\n            \'token_type_ids\': token_type_ids\n        }\n        self.lm_labels = lm_labels\n        self.context_len = context_len\n        self.response_len = response_len    # in case we need it\n\n\nclass InputFeatures_train(object):\n    def __init__(self, conv_id, input_ids, position_ids, token_type_ids,\n                 lm_labels, weights, input_len=None):\n        self.conv_id = conv_id\n        self.input_ids = input_ids\n        self.position_ids = position_ids\n        self.token_type_ids = token_type_ids\n        self.lm_labels = lm_labels\n        self.weights = weights\n        if input_len is None:\n            self.input_len = len(input_ids)\n        else:\n            self.input_len = input_len\n\n\nclass RedditExample(object):\n    def __init__(self, conv_id, context, response):\n        self.conv_id = conv_id\n        self.context = context\n        self.response = response\n\n    def __repr__(self):\n        return \'conv_id = {}\\ncontext = {}\\nresponse = {}\'.format(\n            self.conv_id, self.context, self.response)\n\n    def __str__(self):\n        return self.__repr__()\n\n\ndef boolean_string(s):\n    if s.lower() not in {\'false\', \'true\'}:\n        raise ValueError(\'Not a valid boolean string\')\n    return s.lower() == \'true\'\n\n\ndef get_eval_list_same_length(input_file, tokenizer, max_batch_size,\n                              norm=True):\n    examples = []\n    with open(input_file, \'r\', encoding=""utf-8"") as f:\n        content = [l.split(\'\\t\') for l in f.read().splitlines()]\n\n    context, response = [c[0] for c in content], [c[1:] for c in content]\n    i = 0\n    for src, tgt_all in zip(context, response):\n        for tgt in tgt_all:\n            if norm:\n                src_line = \' \'.join(src.strip().split())\n                tgt_line = \' \'.join(tgt.strip().split())\n            else:\n                src_line = src.strip()\n                tgt_line = tgt.strip()\n            examples.append(RedditExample(i, src_line, tgt_line))\n            i += 1\n\n    def featurize(example):\n        conv_id = example.conv_id\n        context_id = tokenizer.encode(example.context)\n        end_of_text_id = tokenizer.encoder[END_OF_TEXT_TOKEN]\n\n        response_id = tokenizer.encode(example.response)\n        input_ids = context_id + [end_of_text_id]\n        lm_labels = response_id\n\n        position_ids = list(range(len(input_ids)))\n\n        token_type_id = [0] * len(input_ids)\n\n        return InputFeatures(conv_id, input_ids, position_ids, token_type_id,\n                             lm_labels, len(context_id), len(response_id))\n\n    def batch_feature_same_len(features):\n        input_ids = torch.stack([torch.tensor(f.choices_features[\'input_ids\'],\n                                              dtype=torch.long)\n                                 for f in features])\n        position_ids = torch.stack(\n            [torch.tensor(f.choices_features[\'position_ids\'], dtype=torch.long)\n             for f in features])\n        token_type_ids = torch.stack(\n            [torch.tensor(f.choices_features[\'token_type_ids\'],\n                          dtype=torch.long)\n             for f in features])\n        labels = torch.nn.utils.rnn.pad_sequence(\n            [torch.tensor(f.lm_labels, dtype=torch.long) for f in features],\n            batch_first=True, padding_value=-1)\n\n        context_len = torch.tensor([f.context_len for f in features],\n                                   dtype=torch.long)\n        response_len = torch.tensor([f.response_len for f in features],\n                                    dtype=torch.long)\n        return (input_ids, position_ids, token_type_ids, labels,\n                context_len, response_len)\n\n    features = [featurize(e) for e in examples]\n    dataloader_pre = defaultdict(list)\n    for f in features:\n        dataloader_pre[f.context_len].append(f)\n\n    dataloader = []\n    for l in sorted(dataloader_pre):\n        f = batch_feature_same_len(dataloader_pre[l])\n        if len(f[0]) <= max_batch_size:\n            dataloader.append(f)\n        else:\n            start_index = 0\n            while True:\n                dataloader.append([ff[start_index:start_index + max_batch_size]\n                                   for ff in f])\n                start_index += max_batch_size\n                if start_index >= len(f[0]):\n                    break\n    return dataloader\n\n\ndef set_lr(optimizer, step, schedule, lr,\n           warmup_steps, warmup_proportion, n_embd, tot_steps):\n    if schedule == \'None\':\n        lr_this_step = lr\n    elif schedule == \'noam\':  # transformer like\n        lr_this_step = lr * 1e4 * noam_decay(step+1, warmup_steps, n_embd)\n    elif schedule == \'noamwd\':  # transformer like\n        lr_this_step = lr * 1e4 * noamwd_decay(step+1, warmup_steps, n_embd)\n    else:\n        lr_this_step = lr * warmup_linear(step / tot_steps,\n                                          warmup_proportion)\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr_this_step\n'"
lsp_model/__init__.py,0,"b'__version__ = ""0.0.1""\nfrom pytorch_pretrained_bert.tokenization_gpt2 import GPT2Tokenizer\nfrom pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE, cached_path\nfrom pytorch_pretrained_bert.modeling_gpt2 import GPT2Config, GPT2Model, GPT2Config\nfrom pytorch_pretrained_bert.tokenization_gpt2 import GPT2Tokenizer\n\nfrom .modeling_gpt2 import GPT2LMHeadModel\nfrom .optim import Adam\n\n'"
lsp_model/modeling_gpt2.py,11,"b'# coding=utf-8\n# Copyright 2018 The OpenAI Team Authors and HuggingFace Inc. team.\n# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""PyTorch OpenAI GPT-2 model.""""""\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\n\nimport logging\nimport copy\nimport math\nimport torch\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss\n\nfrom pytorch_pretrained_bert.modeling_gpt2 import GPT2PreTrainedModel, GPT2Model, GPT2LMHead, Attention, Block, \\\n    LayerNorm, MLP\n\nlogger = logging.getLogger(__name__)\n\n\nclass AttentionFP16(Attention):\n    def __init__(self, nx, n_ctx, config, scale=False):\n        super(AttentionFP16, self).__init__(nx, n_ctx, config, scale)\n\n    def _attn(self, q, k, v):\n        w = torch.matmul(q, k)\n        if self.scale:\n            w = w / math.sqrt(v.size(-1))\n        nd, ns = w.size(-2), w.size(-1)\n        b = self.bias[:, :, ns-nd:ns, :ns]\n        w = w * b - 1e4 * (1 - b)    # point out by Yen-Chun, FP16 overflow\n\n        w = nn.Softmax(dim=-1)(w)\n        return torch.matmul(w, v)\n\n\nclass BlockFP16(Block):\n    def __init__(self, n_ctx, config, scale=False):\n        super(BlockFP16, self).__init__(n_ctx, config, scale)\n        nx = config.n_embd\n        self.ln_1 = LayerNorm(nx, eps=config.layer_norm_epsilon)\n        self.attn = AttentionFP16(nx, n_ctx, config, scale)\n        self.ln_2 = LayerNorm(nx, eps=config.layer_norm_epsilon)\n        self.mlp = MLP(4 * nx, config)\n\n\nclass GPT2ModelFP16(GPT2Model):\n    def __init__(self, config):\n        super(GPT2ModelFP16, self).__init__(config)\n        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n        self.wpe = nn.Embedding(config.n_positions, config.n_embd)\n        block = BlockFP16(config.n_ctx, config, scale=True)\n        self.h = nn.ModuleList([copy.deepcopy(block) for _ in range(config.n_layer)])\n        self.ln_f = LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n\n        self.apply(self.init_weights)\n\n\nclass GPT2LMHeadModel(GPT2PreTrainedModel):\n    def __init__(self, config):\n        super(GPT2LMHeadModel, self).__init__(config)\n        self.transformer = GPT2ModelFP16(config)\n        self.lm_head = GPT2LMHead(self.transformer.wte.weight, config)\n        self.apply(self.init_weights)\n\n    def set_tied(self):\n        """""" Make sure we are sharing the embeddings\n        """"""\n        self.lm_head.set_embeddings_weights(self.transformer.wte.weight)\n\n    def forward(self, input_ids, position_ids=None, token_type_ids=None, lm_labels=None, past=None):\n        hidden_states, presents = self.transformer(input_ids, position_ids, token_type_ids, past)\n        # import pdb; pdb.set_trace()\n        lm_logits = self.lm_head(hidden_states)\n        if lm_labels is not None:\n            # loss_fct = CrossEntropyLoss(ignore_index=-1)\n            # loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), lm_labels.view(-1))\n            loss_fct1 = CrossEntropyLoss(ignore_index=-1, reduction=\'none\')\n            loss1 = loss_fct1(lm_logits.view(-1, lm_logits.size(-1)),\n                              lm_labels.view(-1))\n            loss1 = loss1.view(lm_labels.size(0), lm_labels.size(1))\n            label_size = torch.sum(lm_labels != -1, dim=1).type(loss1.type())\n            loss = torch.sum(loss1)/torch.sum(label_size)\n            ppl = torch.exp(torch.mean(torch.sum(loss1, dim=1).float()\n                                       / label_size.float()))\n            # ppl = torch.mean(torch.exp(torch.sum(loss1, dim=1)/label_size))\n            return loss, ppl\n        return lm_logits, presents\n    \n    def forward_pointwise(self, input_ids, position_ids=None, token_type_ids=None, lm_labels=None, past=None):\n        hidden_states, presents = self.transformer(input_ids, position_ids, token_type_ids, past)\n        # import pdb; pdb.set_trace()\n        lm_logits = self.lm_head(hidden_states)\n        if lm_labels is not None:\n            # loss_fct = CrossEntropyLoss(ignore_index=-1)\n            # loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), lm_labels.view(-1))\n            loss_fct1 = CrossEntropyLoss(ignore_index=-1, reduction=\'none\')\n            loss1 = loss_fct1(lm_logits.view(-1, lm_logits.size(-1)),\n                              lm_labels.view(-1))\n            loss1 = loss1.view(lm_labels.size(0), lm_labels.size(1))\n            label_size = torch.sum(lm_labels != -1, dim=1).type(loss1.type())\n            loss1 = torch.sum(loss1, dim=1)/label_size\n            ppl1 = torch.exp(loss1)\n\n            return loss1, ppl1\n        return lm_logits, presents\n'"
lsp_model/optim.py,13,"b'# coding=utf-8\n# Copyright 2018 The Google AI Language Team Authors and The HugginFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n"""""" PyTorch optimization """"""\n\nimport math\nimport torch\nfrom torch.optim import Optimizer\nfrom torch.nn.utils import clip_grad_norm_\n\n\ndef warmup_cosine(x, warmup=0.002):\n    if x < warmup:\n        return x/warmup\n    return 0.5 * (1.0 + torch.cos(math.pi * x))\n\n\ndef warmup_constant(x, warmup=0.002):\n    if x < warmup:\n        return x/warmup\n    return 1.0\n\n\ndef warmup_linear(x, warmup=0.002):\n    if x < warmup:\n        return x/warmup\n    return (1.0 - x)/(1.0 - warmup)\n\n\ndef noam_decay(step, warmup_steps, model_size):\n    """"""Learning rate schedule described in\n    https://arxiv.org/pdf/1706.03762.pdf.\n    """"""\n    return (\n        model_size ** (-0.5) *\n        min(step ** (-0.5), step * warmup_steps**(-1.5)))\n\n\ndef noamwd_decay(step, warmup_steps,\n                 model_size, rate=0.5, decay_steps=1000, start_step=500):\n    """"""Learning rate schedule optimized for huge batches\n    """"""\n    return (\n        model_size ** (-0.5) *\n        min(step ** (-0.5), step * warmup_steps**(-1.5)) *\n        rate ** (max(step - start_step + decay_steps, 0) // decay_steps))\n\n\ndef exponential_decay(step, rate, decay_steps, start_step=0):\n    """"""A standard exponential decay, scaling the learning rate by :obj:`rate`\n    every :obj:`decay_steps` steps.\n    """"""\n    return rate ** (max(step - start_step + decay_steps, 0) // decay_steps)\n\n\ndef rsqrt_decay(step, warmup_steps):\n    """"""Decay based on the reciprocal of the step square root.""""""\n    return 1.0 / math.sqrt(max(step, warmup_steps))\n\n\nSCHEDULES = {\n    \'warmup_cosine\': warmup_cosine,\n    \'warmup_constant\': warmup_constant,\n    \'warmup_linear\': warmup_linear,\n}\n\n\nclass Adam(Optimizer):\n    """"""Implements BERT version of Adam algorithm with weight decay fix (and no ).\n    Params:\n        lr: learning rate\n        warmup: portion of t_total for the warmup, -1  means no warmup. Default: -1\n        t_total: total number of training steps for the learning\n            rate schedule, -1  means constant learning rate. Default: -1\n        schedule: schedule to use for the warmup (see above). Default: \'warmup_linear\'\n        b1: Adams b1. Default: 0.9\n        b2: Adams b2. Default: 0.999\n        e: Adams epsilon. Default: 1e-6\n        weight_decay_rate: Weight decay. Default: 0.01\n        max_grad_norm: Maximum norm for the gradients (-1 means no clipping). Default: 1.0\n    """"""\n    def __init__(self, params, lr, warmup=-1, t_total=-1, schedule=\'warmup_linear\',\n                 b1=0.9, b2=0.999, e=1e-6, weight_decay_rate=0.01,\n                 max_grad_norm=1.0):\n        if not lr >= 0.0:\n            raise ValueError(""Invalid learning rate: {} - should be >= 0.0"".format(lr))\n        if schedule not in SCHEDULES:\n            raise ValueError(""Invalid schedule parameter: {}"".format(schedule))\n        if not 0.0 <= warmup < 1.0 and not warmup == -1:\n            raise ValueError(""Invalid warmup: {} - should be in [0.0, 1.0[ or -1"".format(warmup))\n        if not 0.0 <= b1 < 1.0:\n            raise ValueError(""Invalid b1 parameter: {} - should be in [0.0, 1.0["".format(b1))\n        if not 0.0 <= b2 < 1.0:\n            raise ValueError(""Invalid b2 parameter: {} - should be in [0.0, 1.0["".format(b2))\n        if not e >= 0.0:\n            raise ValueError(""Invalid epsilon value: {} - should be >= 0.0"".format(e))\n        defaults = dict(lr=lr, schedule=schedule, warmup=warmup, t_total=t_total,\n                        b1=b1, b2=b2, e=e, weight_decay_rate=weight_decay_rate,\n                        max_grad_norm=max_grad_norm)\n        super(Adam, self).__init__(params, defaults)\n\n    def get_lr(self):\n        lr = []\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                state = self.state[p]\n                if len(state) == 0:\n                    return [0]\n                if group[\'t_total\'] != -1:\n                    schedule_fct = SCHEDULES[group[\'schedule\']]\n                    lr_scheduled = group[\'lr\'] * schedule_fct(state[\'step\']/group[\'t_total\'], group[\'warmup\'])\n                else:\n                    lr_scheduled = group[\'lr\']\n                lr.append(lr_scheduled)\n        return lr\n\n    def to(self, device):\n        """""" Move the optimizer state to a specified device""""""\n        for state in self.state.values():\n            state[\'exp_avg\'].to(device)\n            state[\'exp_avg_sq\'].to(device)\n\n    def initialize_step(self, initial_step):\n        """"""Initialize state with a defined step (but we don\'t have stored averaged).\n        Arguments:\n            initial_step (int): Initial step number.\n        """"""\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                state = self.state[p]\n                # State initialization\n                state[\'step\'] = initial_step\n                # Exponential moving average of gradient values\n                state[\'exp_avg\'] = torch.zeros_like(p.data)\n                # Exponential moving average of squared gradient values\n                state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\'Adam does not support sparse gradients, please consider SparseAdam instead\')\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'next_m\'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[\'next_v\'] = torch.zeros_like(p.data)\n\n                next_m, next_v = state[\'next_m\'], state[\'next_v\']\n                beta1, beta2 = group[\'b1\'], group[\'b2\']\n\n                # Add grad clipping\n                if group[\'max_grad_norm\'] > 0:\n                    clip_grad_norm_(p, group[\'max_grad_norm\'])\n\n                # Decay the first and second moment running average coefficient\n                # In-place operations to update the averages at the same time\n                next_m.mul_(beta1).add_(1 - beta1, grad)\n                next_v.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                update = next_m / (next_v.sqrt() + group[\'e\'])\n\n                # Just adding the square of the weights to the loss function is *not*\n                # the correct way of using L2 regularization/weight decay with Adam,\n                # since that will interact with the m and v parameters in strange ways.\n                #\n                # Instead we want ot decay the weights in a manner that doesn\'t interact\n                # with the m/v parameters. This is equivalent to adding the square\n                # of the weights to the loss with plain (non-momentum) SGD.\n                if group[\'weight_decay_rate\'] > 0.0:\n                    update += group[\'weight_decay_rate\'] * p.data\n\n                if group[\'t_total\'] != -1:\n                    schedule_fct = SCHEDULES[group[\'schedule\']]\n                    lr_scheduled = group[\'lr\'] * schedule_fct(state[\'step\']/group[\'t_total\'], group[\'warmup\'])\n                else:\n                    lr_scheduled = group[\'lr\']\n\n                update_with_lr = lr_scheduled * update\n                p.data.add_(-update_with_lr)\n\n                state[\'step\'] += 1\n\n                # step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1\n                # bias_correction1 = 1 - beta1 ** state[\'step\']\n                # bias_correction2 = 1 - beta2 ** state[\'step\']\n\n        return loss\n\n\nclass Adamax(Optimizer):\n    """"""Implements BERT version of Adam algorithm with weight decay fix (and no ).\n    Params:\n        lr: learning rate\n        warmup: portion of t_total for the warmup, -1  means no warmup. Default: -1\n        t_total: total number of training steps for the learning\n            rate schedule, -1  means constant learning rate. Default: -1\n        schedule: schedule to use for the warmup (see above). Default: \'warmup_linear\'\n        b1: Adams b1. Default: 0.9\n        b2: Adams b2. Default: 0.999\n        e: Adams epsilon. Default: 1e-6\n        weight_decay_rate: Weight decay. Default: 0.01\n        max_grad_norm: Maximum norm for the gradients (-1 means no clipping). Default: 1.0\n    """"""\n    def __init__(self, params, lr, warmup=-1, t_total=-1, schedule=\'warmup_linear\',\n                 betas=(0.9, 0.999), eps=1e-6, weight_decay_rate=0.01,\n                 max_grad_norm=1.0):\n        if not lr >= 0.0:\n            raise ValueError(""Invalid learning rate: {} - should be >= 0.0"".format(lr))\n        if schedule not in SCHEDULES:\n            raise ValueError(""Invalid schedule parameter: {}"".format(schedule))\n        if not 0.0 <= warmup < 1.0 and not warmup == -1:\n            raise ValueError(""Invalid warmup: {} - should be in [0.0, 1.0[ or -1"".format(warmup))\n        if not 0.0 <= eps:\n            raise ValueError(""Invalid epsilon value: {}"".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(""Invalid beta parameter at index 0: {}"".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(""Invalid beta parameter at index 1: {}"".format(betas[1]))\n        defaults = dict(lr=lr, schedule=schedule, warmup=warmup, t_total=t_total,\n                        betas=betas, eps=eps, weight_decay_rate=weight_decay_rate,\n                        max_grad_norm=max_grad_norm)\n        super(Adamax, self).__init__(params, defaults)\n\n    def get_lr(self):\n        lr = []\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                state = self.state[p]\n                if len(state) == 0:\n                    return [0]\n                if group[\'t_total\'] != -1:\n                    schedule_fct = SCHEDULES[group[\'schedule\']]\n                    lr_scheduled = group[\'lr\'] * schedule_fct(state[\'step\']/group[\'t_total\'], group[\'warmup\'])\n                else:\n                    lr_scheduled = group[\'lr\']\n                lr.append(lr_scheduled)\n        return lr\n\n    def to(self, device):\n        """""" Move the optimizer state to a specified device""""""\n        for state in self.state.values():\n            state[\'exp_avg\'].to(device)\n            state[\'exp_avg_sq\'].to(device)\n\n    def initialize_step(self, initial_step):\n        """"""Initialize state with a defined step (but we don\'t have stored averaged).\n        Arguments:\n            initial_step (int): Initial step number.\n        """"""\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                state = self.state[p]\n                # State initialization\n                state[\'step\'] = initial_step\n                # Exponential moving average of gradient values\n                state[\'exp_avg\'] = torch.zeros_like(p.data)\n                # Exponential moving average of squared gradient values\n                state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\'Adam does not support sparse gradients, please consider SparseAdam instead\')\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'exp_avg\'] = torch.zeros_like(p.data)\n                    state[\'exp_inf\'] = torch.zeros_like(p.data)\n\n                exp_avg, exp_inf = state[\'exp_avg\'], state[\'exp_inf\']\n                beta1, beta2 = group[\'betas\']\n                eps = group[\'eps\']\n                # Add grad clipping\n                if group[\'max_grad_norm\'] > 0:\n                    clip_grad_norm_(p, group[\'max_grad_norm\'])\n\n                # Update biased first moment estimate.\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                # Update the exponentially weighted infinity norm.\n                norm_buf = torch.cat([\n                    exp_inf.mul_(beta2).unsqueeze(0),\n                    grad.abs().add_(eps).unsqueeze_(0)\n                ], 0)\n                torch.max(norm_buf, 0, keepdim=False, out=(exp_inf, exp_inf.new().long()))\n                update = exp_avg / (exp_inf + eps)\n\n                if group[\'weight_decay_rate\'] > 0.0:\n                    update += group[\'weight_decay_rate\'] * p.data\n\n                if group[\'t_total\'] != -1:\n                    schedule_fct = SCHEDULES[group[\'schedule\']]\n                    lr_scheduled = group[\'lr\'] * schedule_fct(state[\'step\']/group[\'t_total\'], group[\'warmup\'])\n                else:\n                    lr_scheduled = group[\'lr\']\n\n\n                # Just adding the square of the weights to the loss function is *not*\n                # the correct way of using L2 regularization/weight decay with Adam,\n                # since that will interact with the m and v parameters in strange ways.\n                #\n                # Instead we want ot decay the weights in a manner that doesn\'t interact\n                # with the m/v parameters. This is equivalent to adding the square\n                # of the weights to the loss with plain (non-momentum) SGD.\n                if group[\'weight_decay_rate\'] > 0.0:\n                    update += group[\'weight_decay_rate\'] * p.data\n\n                if group[\'t_total\'] != -1:\n                    schedule_fct = SCHEDULES[group[\'schedule\']]\n                    lr_scheduled = group[\'lr\'] * schedule_fct(state[\'step\']/group[\'t_total\'], group[\'warmup\'])\n                else:\n                    lr_scheduled = group[\'lr\']\n\n                update_with_lr = lr_scheduled * update\n                p.data.add_(-update_with_lr)\n\n                state[\'step\'] += 1\n\n        return loss\n\n'"
pycocoevalcap/__init__.py,0,"b""__author__ = 'tylin'\n"""
pycocoevalcap/eval.py,0,"b'from __future__ import print_function\n__author__ = \'tylin\'\nfrom tokenizer.ptbtokenizer import PTBTokenizer\nfrom bleu.bleu import Bleu\nfrom meteor.meteor import Meteor\nfrom rouge.rouge import Rouge\nfrom cider.cider import Cider\n\nclass COCOEvalCap:\n    def __init__(self, coco, cocoRes):\n        self.evalImgs = []\n        self.eval = {}\n        self.imgToEval = {}\n        self.coco = coco\n        self.cocoRes = cocoRes\n        self.params = {\'image_id\': coco.getImgIds()}\n\n    def evaluate(self):\n        imgIds = self.params[\'image_id\']\n        # imgIds = self.coco.getImgIds()\n        gts = {}\n        res = {}\n        for imgId in imgIds:\n            gts[imgId] = self.coco.imgToAnns[imgId]\n            res[imgId] = self.cocoRes.imgToAnns[imgId]\n\n        # =================================================\n        # Set up scorers\n        # =================================================\n        print(\'tokenization...\')\n        tokenizer = PTBTokenizer()\n        gts  = tokenizer.tokenize(gts)\n        res = tokenizer.tokenize(res)\n\n        # =================================================\n        # Set up scorers\n        # =================================================\n        print(\'setting up scorers...\')\n        scorers = [\n            (Bleu(4), [""Bleu_1"", ""Bleu_2"", ""Bleu_3"", ""Bleu_4""]),\n            (Meteor(),""METEOR""),\n            (Rouge(), ""ROUGE_L""),\n            (Cider(), ""CIDEr"")\n        ]\n\n        # =================================================\n        # Compute scores\n        # =================================================\n        for scorer, method in scorers:\n            print(\'computing %s score...\'%(scorer.method()))\n            score, scores = scorer.compute_score(gts, res)\n            if type(method) == list:\n                for sc, scs, m in zip(score, scores, method):\n                    self.setEval(sc, m)\n                    self.setImgToEvalImgs(scs, gts.keys(), m)\n                    print(""%s: %0.3f""%(m, sc))\n            else:\n                self.setEval(score, method)\n                self.setImgToEvalImgs(scores, gts.keys(), method)\n                print(""%s: %0.3f""%(method, score))\n        self.setEvalImgs()\n\n    def setEval(self, score, method):\n        self.eval[method] = score\n\n    def setImgToEvalImgs(self, scores, imgIds, method):\n        for imgId, score in zip(imgIds, scores):\n            if not imgId in self.imgToEval:\n                self.imgToEval[imgId] = {}\n                self.imgToEval[imgId][""image_id""] = imgId\n            self.imgToEval[imgId][method] = score\n\n    def setEvalImgs(self):\n        self.evalImgs = [eval for imgId, eval in self.imgToEval.items()]'"
pycocoevalcap/bleu/__init__.py,0,"b""__author__ = 'tylin'\n"""
pycocoevalcap/bleu/bleu.py,0,"b'#!/usr/bin/env python\n# \n# File Name : bleu.py\n#\n# Description : Wrapper for BLEU scorer.\n#\n# Creation Date : 06-01-2015\n# Last Modified : Thu 19 Mar 2015 09:13:28 PM PDT\n# Authors : Hao Fang <hfang@uw.edu> and Tsung-Yi Lin <tl483@cornell.edu>\n\nfrom .bleu_scorer import BleuScorer\n\n\nclass Bleu:\n    def __init__(self, n=4):\n        # default compute Blue score up to 4\n        self._n = n\n        self._hypo_for_image = {}\n        self.ref_for_image = {}\n\n    def compute_score(self, gts, res):\n\n        assert(gts.keys() == res.keys())\n        imgIds = gts.keys()\n\n        bleu_scorer = BleuScorer(n=self._n)\n        for id in imgIds:\n            hypo = res[id]\n            ref = gts[id]\n\n            # Sanity check.\n            assert(type(hypo) is list)\n            assert(len(hypo) == 1)\n            assert(type(ref) is list)\n            #print(ref)\n            #assert(len(ref) > 1)\n\n            bleu_scorer += (hypo[0], ref)\n\n        #score, scores = bleu_scorer.compute_score(option=\'shortest\')\n        score, scores = bleu_scorer.compute_score(option=\'closest\', verbose=1)\n        #score, scores = bleu_scorer.compute_score(option=\'average\', verbose=1)\n\n        # return (bleu, bleu_info)\n        return score, scores\n\n    def method(self):\n        return ""Bleu""\n'"
pycocoevalcap/bleu/bleu_scorer.py,0,"b'#!/usr/bin/env python\n\n# bleu_scorer.py\n# David Chiang <chiang@isi.edu>\n\n# Copyright (c) 2004-2006 University of Maryland. All rights\n# reserved. Do not redistribute without permission from the\n# author. Not for commercial use.\n\n# Modified by: \n# Hao Fang <hfang@uw.edu>\n# Tsung-Yi Lin <tl483@cornell.edu>\n\n\'\'\'Provides:\ncook_refs(refs, n=4): Transform a list of reference sentences as strings into a form usable by cook_test().\ncook_test(test, refs, n=4): Transform a test sentence as a string (together with the cooked reference sentences) into a form usable by score_cooked().\n\'\'\'\n\nimport copy\nimport sys, math, re\nfrom collections import defaultdict\n\ndef precook(s, n=4, out=False):\n    """"""Takes a string as input and returns an object that can be given to\n    either cook_refs or cook_test. This is optional: cook_refs and cook_test\n    can take string arguments as well.""""""\n    words = s.split()\n    counts = defaultdict(int)\n    for k in range(1,n+1):\n        for i in range(len(words)-k+1):\n            ngram = tuple(words[i:i+k])\n            counts[ngram] += 1\n    return (len(words), counts)\n\ndef cook_refs(refs, eff=None, n=4): ## lhuang: oracle will call with ""average""\n    \'\'\'Takes a list of reference sentences for a single segment\n    and returns an object that encapsulates everything that BLEU\n    needs to know about them.\'\'\'\n\n    reflen = []\n    maxcounts = {}\n    for ref in refs:\n        rl, counts = precook(ref, n)\n        reflen.append(rl)\n        for (ngram,count) in counts.items():\n            maxcounts[ngram] = max(maxcounts.get(ngram,0), count)\n\n    # Calculate effective reference sentence length.\n    if eff == ""shortest"":\n        reflen = min(reflen)\n    elif eff == ""average"":\n        reflen = float(sum(reflen))/len(reflen)\n\n    ## lhuang: N.B.: leave reflen computaiton to the very end!!\n    \n    ## lhuang: N.B.: in case of ""closest"", keep a list of reflens!! (bad design)\n\n    return (reflen, maxcounts)\n\ndef cook_test(test, refs , eff=None, n=4):\n    \'\'\'Takes a test sentence and returns an object that\n    encapsulates everything that BLEU needs to know about it.\'\'\'\n    (reflen, refmaxcounts) = refs\n    testlen, counts = precook(test, n, True)\n\n    result = {}\n\n    # Calculate effective reference sentence length.\n    \n    if eff == ""closest"":\n        result[""reflen""] = min((abs(l-testlen), l) for l in reflen)[1]\n    else: ## i.e., ""average"" or ""shortest"" or None\n        result[""reflen""] = reflen\n\n    result[""testlen""] = testlen\n\n    result[""guess""] = [max(0,testlen-k+1) for k in range(1,n+1)]\n\n    result[\'correct\'] = [0]*n\n    for (ngram, count) in counts.items():\n        result[""correct""][len(ngram)-1] += min(refmaxcounts.get(ngram,0), count)\n\n    return result\n\nclass BleuScorer(object):\n    """"""Bleu scorer.\n    """"""\n\n    __slots__ = ""n"", ""crefs"", ""ctest"", ""_score"", ""_ratio"", ""_testlen"", ""_reflen"", ""special_reflen""\n    # special_reflen is used in oracle (proportional effective ref len for a node).\n\n    def copy(self):\n        \'\'\' copy the refs.\'\'\'\n        new = BleuScorer(n=self.n)\n        new.ctest = copy.copy(self.ctest)\n        new.crefs = copy.copy(self.crefs)\n        new._score = None\n        return new\n\n    def __init__(self, test=None, refs=None, n=4, special_reflen=None):\n        \'\'\' singular instance \'\'\'\n\n        self.n = n\n        self.crefs = []\n        self.ctest = []\n        self.cook_append(test, refs)\n        self.special_reflen = special_reflen\n\n    def cook_append(self, test, refs):\n        \'\'\'called by constructor and __iadd__ to avoid creating new instances.\'\'\'\n        \n        if refs is not None:\n            self.crefs.append(cook_refs(refs))\n            if test is not None:\n                cooked_test = cook_test(test, self.crefs[-1])\n                self.ctest.append(cooked_test) ## N.B.: -1\n            else:\n                self.ctest.append(None) # lens of crefs and ctest have to match\n\n        self._score = None ## need to recompute\n\n    def ratio(self, option=None):\n        self.compute_score(option=option)\n        return self._ratio\n\n    def score_ratio(self, option=None):\n        \'\'\'return (bleu, len_ratio) pair\'\'\'\n        return (self.fscore(option=option), self.ratio(option=option))\n\n    def score_ratio_str(self, option=None):\n        return ""%.4f (%.2f)"" % self.score_ratio(option)\n\n    def reflen(self, option=None):\n        self.compute_score(option=option)\n        return self._reflen\n\n    def testlen(self, option=None):\n        self.compute_score(option=option)\n        return self._testlen        \n\n    def retest(self, new_test):\n        if type(new_test) is str:\n            new_test = [new_test]\n        assert len(new_test) == len(self.crefs), new_test\n        self.ctest = []\n        for t, rs in zip(new_test, self.crefs):\n            self.ctest.append(cook_test(t, rs))\n        self._score = None\n\n        return self\n\n    def rescore(self, new_test):\n        \'\'\' replace test(s) with new test(s), and returns the new score.\'\'\'\n        \n        return self.retest(new_test).compute_score()\n\n    def size(self):\n        assert len(self.crefs) == len(self.ctest), ""refs/test mismatch! %d<>%d"" % (len(self.crefs), len(self.ctest))\n        return len(self.crefs)\n\n    def __iadd__(self, other):\n        \'\'\'add an instance (e.g., from another sentence).\'\'\'\n\n        if type(other) is tuple:\n            ## avoid creating new BleuScorer instances\n            self.cook_append(other[0], other[1])\n        else:\n            assert self.compatible(other), ""incompatible BLEUs.""\n            self.ctest.extend(other.ctest)\n            self.crefs.extend(other.crefs)\n            self._score = None ## need to recompute\n\n        return self        \n\n    def compatible(self, other):\n        return isinstance(other, BleuScorer) and self.n == other.n\n\n    def single_reflen(self, option=""average""):\n        return self._single_reflen(self.crefs[0][0], option)\n\n    def _single_reflen(self, reflens, option=None, testlen=None):\n        \n        if option == ""shortest"":\n            reflen = min(reflens)\n        elif option == ""average"":\n            reflen = float(sum(reflens))/len(reflens)\n        elif option == ""closest"":\n            reflen = min((abs(l-testlen), l) for l in reflens)[1]\n        else:\n            assert False, ""unsupported reflen option %s"" % option\n\n        return reflen\n\n    def recompute_score(self, option=None, verbose=0):\n        self._score = None\n        return self.compute_score(option, verbose)\n        \n    def compute_score(self, option=None, verbose=0):\n        n = self.n\n        small = 1e-9\n        tiny = 1e-15 ## so that if guess is 0 still return 0\n        bleu_list = [[] for _ in range(n)]\n\n        if self._score is not None:\n            return self._score\n\n        if option is None:\n            option = ""average"" if len(self.crefs) == 1 else ""closest""\n\n        self._testlen = 0\n        self._reflen = 0\n        totalcomps = {\'testlen\':0, \'reflen\':0, \'guess\':[0]*n, \'correct\':[0]*n}\n\n        # for each sentence\n        for comps in self.ctest:            \n            testlen = comps[\'testlen\']\n            self._testlen += testlen\n\n            if self.special_reflen is None: ## need computation\n                reflen = self._single_reflen(comps[\'reflen\'], option, testlen)\n            else:\n                reflen = self.special_reflen\n\n            self._reflen += reflen\n                \n            for key in [\'guess\',\'correct\']:\n                for k in range(n):\n                    totalcomps[key][k] += comps[key][k]\n\n            # append per image bleu score\n            bleu = 1.\n            for k in range(n):\n                bleu *= (float(comps[\'correct\'][k]) + tiny) \\\n                        /(float(comps[\'guess\'][k]) + small) \n                bleu_list[k].append(bleu ** (1./(k+1)))\n            ratio = (testlen + tiny) / (reflen + small) ## N.B.: avoid zero division\n            if ratio < 1:\n                for k in range(n):\n                    bleu_list[k][-1] *= math.exp(1 - 1/ratio)\n\n            if verbose > 1:\n                print(comps, reflen)\n\n        totalcomps[\'reflen\'] = self._reflen\n        totalcomps[\'testlen\'] = self._testlen\n\n        bleus = []\n        bleu = 1.\n        for k in range(n):\n            bleu *= float(totalcomps[\'correct\'][k] + tiny) \\\n                    / (totalcomps[\'guess\'][k] + small)\n            bleus.append(bleu ** (1./(k+1)))\n        ratio = (self._testlen + tiny) / (self._reflen + small) ## N.B.: avoid zero division\n        if ratio < 1:\n            for k in range(n):\n                bleus[k] *= math.exp(1 - 1/ratio)\n\n        #if verbose > 0:\n        #    print totalcomps\n        #    print ""ratio:"", ratio\n\n        self._score = bleus\n        return self._score, bleu_list\n'"
pycocoevalcap/cider/__init__.py,0,"b""__author__ = 'tylin'\n"""
pycocoevalcap/cider/cider.py,0,"b'# Filename: cider.py\n#\n# Description: Describes the class to compute the CIDEr (Consensus-Based Image Description Evaluation) Metric \n#               by Vedantam, Zitnick, and Parikh (http://arxiv.org/abs/1411.5726)\n#\n# Creation Date: Sun Feb  8 14:16:54 2015\n#\n# Authors: Ramakrishna Vedantam <vrama91@vt.edu> and Tsung-Yi Lin <tl483@cornell.edu>\n\nfrom cider_scorer import CiderScorer\nimport pdb\n\nclass Cider:\n    """"""\n    Main Class to compute the CIDEr metric \n\n    """"""\n    def __init__(self, test=None, refs=None, n=4, sigma=6.0):\n        # set cider to sum over 1 to 4-grams\n        self._n = n\n        # set the standard deviation parameter for gaussian penalty\n        self._sigma = sigma\n\n    def compute_score(self, gts, res):\n        """"""\n        Main function to compute CIDEr score\n        :param  hypo_for_image (dict) : dictionary with key <image> and value <tokenized hypothesis / candidate sentence>\n                ref_for_image (dict)  : dictionary with key <image> and value <tokenized reference sentence>\n        :return: cider (float) : computed CIDEr score for the corpus \n        """"""\n\n        assert(gts.keys() == res.keys())\n        imgIds = gts.keys()\n\n        cider_scorer = CiderScorer(n=self._n, sigma=self._sigma)\n\n        for id in imgIds:\n            hypo = res[id]\n            ref = gts[id]\n\n            # Sanity check.\n            assert(type(hypo) is list)\n            assert(len(hypo) == 1)\n            assert(type(ref) is list)\n            assert(len(ref) > 0)\n\n            cider_scorer += (hypo[0], ref)\n\n        (score, scores) = cider_scorer.compute_score()\n\n        return score, scores\n\n    def method(self):\n        return ""CIDEr""'"
pycocoevalcap/cider/cider_scorer.py,0,"b'#!/usr/bin/env python\n# Tsung-Yi Lin <tl483@cornell.edu>\n# Ramakrishna Vedantam <vrama91@vt.edu>\n\nimport copy\nfrom collections import defaultdict\nimport numpy as np\nimport pdb\nimport math\n\ndef precook(s, n=4, out=False):\n    """"""\n    Takes a string as input and returns an object that can be given to\n    either cook_refs or cook_test. This is optional: cook_refs and cook_test\n    can take string arguments as well.\n    :param s: string : sentence to be converted into ngrams\n    :param n: int    : number of ngrams for which representation is calculated\n    :return: term frequency vector for occuring ngrams\n    """"""\n    words = s.split()\n    counts = defaultdict(int)\n    for k in range(1,n+1):\n        for i in range(len(words)-k+1):\n            ngram = tuple(words[i:i+k])\n            counts[ngram] += 1\n    return counts\n\ndef cook_refs(refs, n=4): ## lhuang: oracle will call with ""average""\n    \'\'\'Takes a list of reference sentences for a single segment\n    and returns an object that encapsulates everything that BLEU\n    needs to know about them.\n    :param refs: list of string : reference sentences for some image\n    :param n: int : number of ngrams for which (ngram) representation is calculated\n    :return: result (list of dict)\n    \'\'\'\n    return [precook(ref, n) for ref in refs]\n\ndef cook_test(test, n=4):\n    \'\'\'Takes a test sentence and returns an object that\n    encapsulates everything that BLEU needs to know about it.\n    :param test: list of string : hypothesis sentence for some image\n    :param n: int : number of ngrams for which (ngram) representation is calculated\n    :return: result (dict)\n    \'\'\'\n    return precook(test, n, True)\n\nclass CiderScorer(object):\n    """"""CIDEr scorer.\n    """"""\n\n    def copy(self):\n        \'\'\' copy the refs.\'\'\'\n        new = CiderScorer(n=self.n)\n        new.ctest = copy.copy(self.ctest)\n        new.crefs = copy.copy(self.crefs)\n        return new\n\n    def __init__(self, test=None, refs=None, n=4, sigma=6.0):\n        \'\'\' singular instance \'\'\'\n        self.n = n\n        self.sigma = sigma\n        self.crefs = []\n        self.ctest = []\n        self.document_frequency = defaultdict(float)\n        self.cook_append(test, refs)\n        self.ref_len = None\n\n    def cook_append(self, test, refs):\n        \'\'\'called by constructor and __iadd__ to avoid creating new instances.\'\'\'\n\n        if refs is not None:\n            self.crefs.append(cook_refs(refs))\n            if test is not None:\n                self.ctest.append(cook_test(test)) ## N.B.: -1\n            else:\n                self.ctest.append(None) # lens of crefs and ctest have to match\n\n    def size(self):\n        assert len(self.crefs) == len(self.ctest), ""refs/test mismatch! %d<>%d"" % (len(self.crefs), len(self.ctest))\n        return len(self.crefs)\n\n    def __iadd__(self, other):\n        \'\'\'add an instance (e.g., from another sentence).\'\'\'\n\n        if type(other) is tuple:\n            ## avoid creating new CiderScorer instances\n            self.cook_append(other[0], other[1])\n        else:\n            self.ctest.extend(other.ctest)\n            self.crefs.extend(other.crefs)\n\n        return self\n    def compute_doc_freq(self):\n        \'\'\'\n        Compute term frequency for reference data.\n        This will be used to compute idf (inverse document frequency later)\n        The term frequency is stored in the object\n        :return: None\n        \'\'\'\n        for refs in self.crefs:\n            # refs, k ref captions of one image\n            for ngram in set([ngram for ref in refs for (ngram,count) in ref.items()]):\n                self.document_frequency[ngram] += 1\n            # maxcounts[ngram] = max(maxcounts.get(ngram,0), count)\n\n    def compute_cider(self):\n        def counts2vec(cnts):\n            """"""\n            Function maps counts of ngram to vector of tfidf weights.\n            The function returns vec, an array of dictionary that store mapping of n-gram and tf-idf weights.\n            The n-th entry of array denotes length of n-grams.\n            :param cnts:\n            :return: vec (array of dict), norm (array of float), length (int)\n            """"""\n            vec = [defaultdict(float) for _ in range(self.n)]\n            length = 0\n            norm = [0.0 for _ in range(self.n)]\n            for (ngram,term_freq) in cnts.items():\n                # give word count 1 if it doesn\'t appear in reference corpus\n                df = np.log(max(1.0, self.document_frequency[ngram]))\n                # ngram index\n                n = len(ngram)-1\n                # tf (term_freq) * idf (precomputed idf) for n-grams\n                vec[n][ngram] = float(term_freq)*(self.ref_len - df)\n                # compute norm for the vector.  the norm will be used for computing similarity\n                norm[n] += pow(vec[n][ngram], 2)\n\n                if n == 1:\n                    length += term_freq\n            norm = [np.sqrt(n) for n in norm]\n            return vec, norm, length\n\n        def sim(vec_hyp, vec_ref, norm_hyp, norm_ref, length_hyp, length_ref):\n            \'\'\'\n            Compute the cosine similarity of two vectors.\n            :param vec_hyp: array of dictionary for vector corresponding to hypothesis\n            :param vec_ref: array of dictionary for vector corresponding to reference\n            :param norm_hyp: array of float for vector corresponding to hypothesis\n            :param norm_ref: array of float for vector corresponding to reference\n            :param length_hyp: int containing length of hypothesis\n            :param length_ref: int containing length of reference\n            :return: array of score for each n-grams cosine similarity\n            \'\'\'\n            delta = float(length_hyp - length_ref)\n            # measure consine similarity\n            val = np.array([0.0 for _ in range(self.n)])\n            for n in range(self.n):\n                # ngram\n                for (ngram,count) in vec_hyp[n].items():\n                    # vrama91 : added clipping\n                    val[n] += min(vec_hyp[n][ngram], vec_ref[n][ngram]) * vec_ref[n][ngram]\n\n                if (norm_hyp[n] != 0) and (norm_ref[n] != 0):\n                    val[n] /= (norm_hyp[n]*norm_ref[n])\n\n                assert(not math.isnan(val[n]))\n                # vrama91: added a length based gaussian penalty\n                val[n] *= np.e**(-(delta**2)/(2*self.sigma**2))\n            return val\n\n        # compute log reference length\n        self.ref_len = np.log(float(len(self.crefs)))\n\n        scores = []\n        for test, refs in zip(self.ctest, self.crefs):\n            # compute vector for test captions\n            vec, norm, length = counts2vec(test)\n            # compute vector for ref captions\n            score = np.array([0.0 for _ in range(self.n)])\n            for ref in refs:\n                vec_ref, norm_ref, length_ref = counts2vec(ref)\n                score += sim(vec, vec_ref, norm, norm_ref, length, length_ref)\n            # change by vrama91 - mean of ngram scores, instead of sum\n            score_avg = np.mean(score)\n            # divide by number of references\n            score_avg /= len(refs)\n            # multiply score by 10\n            score_avg *= 10.0\n            # append score of an image to the score list\n            scores.append(score_avg)\n        return scores\n\n    def compute_score(self, option=None, verbose=0):\n        # compute idf\n        self.compute_doc_freq()\n        # assert to check document frequency\n        assert(len(self.ctest) >= max(self.document_frequency.values()))\n        # compute cider score\n        score = self.compute_cider()\n        # debug\n        # print score\n        return np.mean(np.array(score)), np.array(score)'"
pycocoevalcap/meteor/__init__.py,0,"b""__author__ = 'tylin'\n"""
pycocoevalcap/meteor/meteor.py,0,"b'#!/usr/bin/env python\n\n# Python wrapper for METEOR implementation, by Xinlei Chen\n# Acknowledge Michael Denkowski for the generous discussion and help \n\nimport os\nimport sys\nimport subprocess\nimport threading\n\n# Assumes meteor-1.5.jar is in the same directory as meteor.py.  Change as needed.\nMETEOR_JAR = \'meteor-1.5.jar\'\n# print METEOR_JAR\n\nclass Meteor:\n\n    def __init__(self):\n        self.meteor_cmd = [\'java\', \'-jar\', \'-Xmx2G\', METEOR_JAR, \\\n                \'-\', \'-\', \'-stdio\', \'-l\', \'en\', \'-norm\']\n        self.meteor_p = subprocess.Popen(self.meteor_cmd, \\\n                cwd=os.path.dirname(os.path.abspath(__file__)), \\\n                stdin=subprocess.PIPE, \\\n                stdout=subprocess.PIPE, \\\n                stderr=subprocess.PIPE)\n        # Used to guarantee thread safety\n        self.lock = threading.Lock()\n\n    def compute_score(self, gts, res):\n        assert(gts.keys() == res.keys())\n        imgIds = gts.keys()\n        scores = []\n\n        eval_line = \'EVAL\'\n        self.lock.acquire()\n        for i in imgIds:\n            assert(len(res[i]) == 1)\n            stat = self._stat(res[i][0], gts[i])\n            eval_line += \' ||| {}\'.format(stat)\n\n        self.meteor_p.stdin.write(\'{}\\n\'.format(eval_line))\n        for i in range(0,len(imgIds)):\n            scores.append(float(self.meteor_p.stdout.readline().strip()))\n        score = float(self.meteor_p.stdout.readline().strip())\n        self.lock.release()\n\n        return score, scores\n\n    def method(self):\n        return ""METEOR""\n\n    def _stat(self, hypothesis_str, reference_list):\n        # SCORE ||| reference 1 words ||| reference n words ||| hypothesis words\n        hypothesis_str = hypothesis_str.replace(\'|||\',\'\').replace(\'  \',\' \')\n        score_line = \' ||| \'.join((\'SCORE\', \' ||| \'.join(reference_list), hypothesis_str))\n        self.meteor_p.stdin.write(\'{}\\n\'.format(score_line))\n        return self.meteor_p.stdout.readline().strip()\n\n    def _score(self, hypothesis_str, reference_list):\n        self.lock.acquire()\n        # SCORE ||| reference 1 words ||| reference n words ||| hypothesis words\n        hypothesis_str = hypothesis_str.replace(\'|||\',\'\').replace(\'  \',\' \')\n        score_line = \' ||| \'.join((\'SCORE\', \' ||| \'.join(reference_list), hypothesis_str))\n        self.meteor_p.stdin.write(\'{}\\n\'.format(score_line))\n        stats = self.meteor_p.stdout.readline().strip()\n        eval_line = \'EVAL ||| {}\'.format(stats)\n        # EVAL ||| stats \n        self.meteor_p.stdin.write(\'{}\\n\'.format(eval_line))\n        score = float(self.meteor_p.stdout.readline().strip())\n        # bug fix: there are two values returned by the jar file, one average, and one all, so do it twice\n        # thanks for Andrej for pointing this out\n        score = float(self.meteor_p.stdout.readline().strip())\n        self.lock.release()\n        return score\n \n    def __exit__(self):\n        self.lock.acquire()\n        self.meteor_p.stdin.close()\n        self.meteor_p.kill()\n        self.meteor_p.wait()\n        self.lock.release()\n'"
pycocoevalcap/rouge/__init__.py,0,"b""__author__ = 'vrama91'\n"""
pycocoevalcap/rouge/rouge.py,0,"b'#!/usr/bin/env python\n# \n# File Name : rouge.py\n#\n# Description : Computes ROUGE-L metric as described by Lin and Hovey (2004)\n#\n# Creation Date : 2015-01-07 06:03\n# Author : Ramakrishna Vedantam <vrama91@vt.edu>\n\nimport numpy as np\nimport pdb\n\ndef my_lcs(string, sub):\n    """"""\n    Calculates longest common subsequence for a pair of tokenized strings\n    :param string : list of str : tokens from a string split using whitespace\n    :param sub : list of str : shorter string, also split using whitespace\n    :returns: length (list of int): length of the longest common subsequence between the two strings\n\n    Note: my_lcs only gives length of the longest common subsequence, not the actual LCS\n    """"""\n    if(len(string)< len(sub)):\n        sub, string = string, sub\n\n    lengths = [[0 for i in range(0,len(sub)+1)] for j in range(0,len(string)+1)]\n\n    for j in range(1,len(sub)+1):\n        for i in range(1,len(string)+1):\n            if(string[i-1] == sub[j-1]):\n                lengths[i][j] = lengths[i-1][j-1] + 1\n            else:\n                lengths[i][j] = max(lengths[i-1][j] , lengths[i][j-1])\n\n    return lengths[len(string)][len(sub)]\n\nclass Rouge():\n    \'\'\'\n    Class for computing ROUGE-L score for a set of candidate sentences for the MS COCO test set\n\n    \'\'\'\n    def __init__(self):\n        # vrama91: updated the value below based on discussion with Hovey\n        self.beta = 1.2\n\n    def calc_score(self, candidate, refs):\n        """"""\n        Compute ROUGE-L score given one candidate and references for an image\n        :param candidate: str : candidate sentence to be evaluated\n        :param refs: list of str : COCO reference sentences for the particular image to be evaluated\n        :returns score: int (ROUGE-L score for the candidate evaluated against references)\n        """"""\n        assert(len(candidate)==1)\t\n        assert(len(refs)>0)         \n        prec = []\n        rec = []\n\n        # split into tokens\n        token_c = candidate[0].split("" "")\n    \t\n        for reference in refs:\n            # split into tokens\n            token_r = reference.split("" "")\n            # compute the longest common subsequence\n            lcs = my_lcs(token_r, token_c)\n            prec.append(lcs/float(len(token_c)))\n            rec.append(lcs/float(len(token_r)))\n\n        prec_max = max(prec)\n        rec_max = max(rec)\n\n        if(prec_max!=0 and rec_max !=0):\n            score = ((1 + self.beta**2)*prec_max*rec_max)/float(rec_max + self.beta**2*prec_max)\n        else:\n            score = 0.0\n        return score\n\n    def compute_score(self, gts, res):\n        """"""\n        Computes Rouge-L score given a set of reference and candidate sentences for the dataset\n        Invoked by evaluate_captions.py \n        :param hypo_for_image: dict : candidate / test sentences with ""image name"" key and ""tokenized sentences"" as values \n        :param ref_for_image: dict : reference MS-COCO sentences with ""image name"" key and ""tokenized sentences"" as values\n        :returns: average_score: float (mean ROUGE-L score computed by averaging scores for all the images)\n        """"""\n        assert(gts.keys() == res.keys())\n        imgIds = gts.keys()\n\n        score = []\n        for id in imgIds:\n            hypo = res[id]\n            ref  = gts[id]\n\n            score.append(self.calc_score(hypo, ref))\n\n            # Sanity check.\n            assert(type(hypo) is list)\n            assert(len(hypo) == 1)\n            assert(type(ref) is list)\n            assert(len(ref) > 0)\n\n        average_score = np.mean(np.array(score))\n        return average_score, np.array(score)\n\n    def method(self):\n        return ""Rouge""\n'"
pycocoevalcap/tokenizer/__init__.py,0,"b""__author__ = 'hfang'\n"""
pycocoevalcap/tokenizer/ptbtokenizer.py,0,"b'#!/usr/bin/env python\n# \n# File Name : ptbtokenizer.py\n#\n# Description : Do the PTB Tokenization and remove punctuations.\n#\n# Creation Date : 29-12-2014\n# Last Modified : Thu Mar 19 09:53:35 2015\n# Authors : Hao Fang <hfang@uw.edu> and Tsung-Yi Lin <tl483@cornell.edu>\n\nimport os\nimport sys\nimport subprocess\nimport tempfile\nimport itertools\n\n# path to the stanford corenlp jar\nSTANFORD_CORENLP_3_4_1_JAR = \'stanford-corenlp-3.4.1.jar\'\n\n# punctuations to be removed from the sentences\nPUNCTUATIONS = [""\'\'"", ""\'"", ""``"", ""`"", ""-LRB-"", ""-RRB-"", ""-LCB-"", ""-RCB-"", \\\n        ""."", ""?"", ""!"", "","", "":"", ""-"", ""--"", ""..."", "";""] \n\nclass PTBTokenizer:\n    """"""Python wrapper of Stanford PTBTokenizer""""""\n\n    def tokenize(self, captions_for_image):\n        cmd = [\'java\', \'-cp\', STANFORD_CORENLP_3_4_1_JAR, \\\n                \'edu.stanford.nlp.process.PTBTokenizer\', \\\n                \'-preserveLines\', \'-lowerCase\']\n\n        # ======================================================\n        # prepare data for PTB Tokenizer\n        # ======================================================\n        final_tokenized_captions_for_image = {}\n        image_id = [k for k, v in captions_for_image.items() for _ in range(len(v))]\n        sentences = \'\\n\'.join([c[\'caption\'].replace(\'\\n\', \' \') for k, v in captions_for_image.items() for c in v])\n\n        # ======================================================\n        # save sentences to temporary file\n        # ======================================================\n        path_to_jar_dirname=os.path.dirname(os.path.abspath(__file__))\n        tmp_file = tempfile.NamedTemporaryFile(delete=False, dir=path_to_jar_dirname)\n        tmp_file.write(sentences)\n        tmp_file.close()\n\n        # ======================================================\n        # tokenize sentence\n        # ======================================================\n        cmd.append(os.path.basename(tmp_file.name))\n        p_tokenizer = subprocess.Popen(cmd, cwd=path_to_jar_dirname, \\\n                stdout=subprocess.PIPE)\n        token_lines = p_tokenizer.communicate(input=sentences.rstrip())[0]\n        lines = token_lines.split(\'\\n\')\n        # remove temp file\n        os.remove(tmp_file.name)\n\n        # ======================================================\n        # create dictionary for tokenized captions\n        # ======================================================\n        for k, line in zip(image_id, lines):\n            if not k in final_tokenized_captions_for_image:\n                final_tokenized_captions_for_image[k] = []\n            tokenized_caption = \' \'.join([w for w in line.rstrip().split(\' \') \\\n                    if w not in PUNCTUATIONS])\n            final_tokenized_captions_for_image[k].append(tokenized_caption)\n\n        return final_tokenized_captions_for_image\n'"
reddit_extractor/src/create-multiref.py,0,"b'#!/usr/bin/env python\n#  Copyright (c) Microsoft Corporation. \n#  Licensed under the MIT license. \n\nimport sys\nimport gzip\nimport argparse\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(""--data"", required=True, help=""gz file containing test data"")\nparser.add_argument(""--testids"", required=True, help=""multi-ref test set with string replaced with IDs"")\nparser.add_argument(""--out"", required=True, help=""output multi-ref file"")\n\nargs = parser.parse_args()\n\ndata = {}\n\nwith gzip.open(args.data, \'rt\', encoding=""utf-8"") as f:\n\tfor line in f:\n\t\tline = line.rstrip()\n\t\tline = line.replace(\'\xe2\x80\x9c\',\'""\')\n\t\tline = line.replace(\'\xe2\x80\x9d\',\'""\')\n\t\tkeys, src, tgt = line.split(\'\\t\')\n\t\tdata[keys] = (src, tgt)\n\nwith open(args.out, \'wt\', encoding=""utf-8"") as fo:\n\twith open(args.testids, \'rt\', encoding=""utf-8"") as fi:\n\t\tfor line in fi:\n\t\t\tline = line.rstrip()\n\t\t\tels = line.split(\'\\t\')\n\t\t\theader = els[0]\n\t\t\tif header != \'multiref\':\n\t\t\t\tprint(""Ignoring line: "" + line, file=sys.stderr)\n\t\t\trscore1, rids1 = els[1].split(\',\', 1)\n\t\t\tif rids1 not in data.keys():\n\t\t\t\tprint(""Error: can\'t find data for ref ID: %s"" % rids1, file=sys.stderr)\n\t\t\t\tcontinue\n\t\t\tsrc, r1 = data[rids1]\n\t\t\tscores = [ rscore1 ]\n\t\t\trefs = [ r1 ]\n\t\t\tfor el in els[2:]:\n\t\t\t\trscoreI, ridsI = el.split(\',\', 1)\n\t\t\t\tif ridsI not in data.keys():\n\t\t\t\t\tprint(""Error: can\'t find data for ref ID: %s"" % ridsI, file=sys.stderr)\n\t\t\t\telse:\n\t\t\t\t\tsrcI, rI = data[ridsI]\n\t\t\t\t\tif srcI != src:\n\t\t\t\t\t\tprint(""Error: mismatch source for ref ID: %s"" % ridsI, file=sys.stderr)\n\t\t\t\t\telse:\n\t\t\t\t\t\tscores.append(rscoreI)\n\t\t\t\t\t\trefs.append(rI)\n\n\t\t\t# Write multi-ref instance:\n\t\t\tfo.write(\'%s\' % src)\n\t\t\tfor i in range(len(scores)):\n\t\t\t\tfo.write(\'\\t%s|%s\' % (scores[i], refs[i]))\n\t\t\tfo.write(\'\\n\')\n'"
reddit_extractor/src/reddit.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#  Copyright (c) Microsoft Corporation. \n#  Licensed under the MIT license. \n\nimport sys\nimport time\nimport os.path\nimport math\nimport re\nimport argparse\nimport traceback\nimport json\nimport bz2\nimport gzip\nfrom nltk.tokenize import TweetTokenizer\nfrom flashtext import KeywordProcessor\nimport hashlib\n\ndef makedirs(fld):\n\tif not os.path.exists(fld):\n\t\tos.makedirs(fld)\n\nPICKLE_MAX_LEN = 1e4\nTAG_COMMENT = \'t1_\'\nTAG_SUBMISSION = \'t3_\'\ndontuse = \'__dontuse__\'\nurl_str = \'__url__\'\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(""dump_name"", help=""YYYY-MM, dumped files to be loaded"")\nparser.add_argument(""--bl_words"", help=""list of offensive words, to avoid in responses"")\nparser.add_argument(""--ignore_keys"", default=False, type=bool, help=""If true ignore any keys provided as arguments"")\nparser.add_argument(""--keep_keys"", help=""hashes of instances to keep"")\nparser.add_argument(""--discard_tgt_keys"", help=""hashes of targets to discard"")\nparser.add_argument(""--freq_words"", help=""words sorted by their corpus frequencies"")\nparser.add_argument(""--bl_subreddits"", help=""blocklist of offensive subreddits"")\nparser.add_argument(""--wl_subreddits"", help=""whitelist of relatively safe subreddits"")\nparser.add_argument(""--reddit_input"", default=""d:/data/reddit/bz2/"", help=""Location of the input reddit data (bz2 files)"")\nparser.add_argument(""--reddit_output"", default=""d:/data/reddit/"", help=""Location of the output reddit data (conversations)"")\nparser.add_argument(""--max_len"", default=30, type=int)\n# 30 words means roughly 70 characters on average for Reddit\nparser.add_argument(""--max_len_type"", default=\'w\') # w for words, c for chars\nparser.add_argument(""--min_depth"", default=2, type=int)\nparser.add_argument(""--max_depth"", default=10, type=int)\nparser.add_argument(""--min_score"", default=0, type=int)\nparser.add_argument(""--use_title"", default=1, type=int)\nparser.add_argument(""--leaves_only"", default=0, type=int)\nparser.add_argument(""--split_size"", default=int(5e5), type=int)\nparser.add_argument(""--task"", default=\'conv\')\nparser.add_argument(""--parallel"", default=False, type=bool)\nparser.add_argument(""--pre_tok"", default=False, type=bool, help=""whether to tokenize during the extract step"")\nparser.add_argument(""--clean"", default=False, type=bool, help=""apply some filters to significantly reduce number of instances"")\n\nargs = parser.parse_args()\nprint(""Args: %s"" % args, file=sys.stderr)\n\nfields_subm = [ ""id"", ""score"", ""num_comments"", ""domain"", ""permalink"", ""title"" ]\nfields_comm = [ ""id"", ""author"", ""parent_id"", ""link_id"", ""score"", ""n_char"", ""body""]\n\nbl_words = KeywordProcessor()\nbl_subreddits = {}\nwl_subreddits = {}\nkeys = {}\nkeys_rm = {}\n\n\ndef get_submission_id(submission):\n\treturn TAG_SUBMISSION + submission[""id""]\n\n\ndef get_comment_id(comment):\n\treturn TAG_COMMENT + comment[""id""]\n\n\ndef norm_sentence(txt, is_extract):\n\tif is_extract:\n\t\treturn minimal_norm_sentence(txt)\n\telse:\n\t\treturn gpt_norm_sentence(txt)\n\n\ndef minimal_norm_sentence(txt):\n\ttxt = txt.replace(chr(92),\'\') # chr(92) = \'\\\'. as twitter has \'b\\/c\' rather than \'b/c\'\n\ttxt = txt.replace(\'\\n\', \' \')\n\ttxt = txt.replace(\'\\r\', \' \')\n\ttxt = txt.replace(\'\\t\', \' \')\n\t#print (""Tokenized: [%s]"" % txt, file=sys.stderr)\n\treturn txt\n\n\ndef gpt_norm_sentence(txt):\n\t# url and tag\n\twords = []\n\tfor word in txt.split():\n\t\tif word[0] == \'#\': # don\'t allow tag\n\t\t\tcontinue\n\t\ti = word.lower().find(\'http\')\n\t\tif i >= 0:\n\t\t\tword = word[:i] + \' \' + \'__url__\'\n\t\twords.append(word.strip())\n\ttxt = \' \'.join(words)\n\n\t# remove illegal char\n\ttxt = txt.replace(chr(92),\'\') # chr(92) = \'\\\'. as twitter has \'b\\/c\' rather than \'b/c\'\n\ttxt = txt.replace(""b/c"",""because"").replace(\'j/k\',\'just kidding\').replace(\'w/o\',\'without\').replace(\'w/\',\'with\')\n\ttxt = re.sub(\'__mention__\',\'MENTION\',txt)\n\ttxt = re.sub(\'__url__\',\'URL\',txt)\n\ttxt = re.sub(r""[^A-Za-z0-9()\\[\\]:,.!?\'\xe2\x80\x9c\xe2\x80\x9d ]"", "" "", txt)\n\ttxt = re.sub(\'MENTION\',\'__mention__\',txt)\n\ttxt = re.sub(\'URL\',\'__url__\',txt)\n\n\ttokenizer = TweetTokenizer(preserve_case=True)\n\ttxt = \' \' + \' \'.join(tokenizer.tokenize(txt)) + \' \'\n\n\t# remove un-necessary space\n\treturn \' \'.join(txt.split())\n\n\ndef extract_submissions(fld_bz2, fld_split, size=2e5):\n\tpath_in = fld_bz2 + \'/RS_%s.bz2\'%args.dump_name\n\tn = 0\n\tm = 0\n\tsub = 0\n\tsid = []\n\tsids = []\n\tlines = []\n\twith bz2.open(path_in, \'rt\', encoding=""utf-8"") as f:\n\t\tfor line in f:\n\t\t\tn += 1\n\t\t\tif n%1e4 == 0:\n\t\t\t\tprint(\'[%s] selected %.3fM from %.2fM submissions\'%(\n\t\t\t\t\targs.dump_name, m/1e6, n/1e6))\n\t\t\ttry:\n\t\t\t\tsubmission = json.loads(line)\n\t\t\t\tif int(submission[\'num_comments\']) < 2: # filter 1\n\t\t\t\t\tcontinue\n\t\t\t\tsubmission[\'title\'] = norm_sentence(submission[\'title\'], True)\n\t\t\t\tlines.append(\'\\t\'.join([str(submission[k]) for k in fields_subm]))\n\t\t\t\tm += 1\n\t\t\t\tsid.append(get_submission_id(submission))\n\n\t\t\texcept Exception:\n\t\t\t\ttraceback.print_exc()\n\t\t\t\tcontinue\n\n\t\t\tif len(sid) == size:\n\t\t\t\tprint(\'writing submissions_sub%i\'%sub)\n\t\t\t\tsids.append(set(sid))\n\t\t\t\twith open(fld_split + \'/rs_sub%i.tsv\'%sub, \'w\', encoding=\'utf-8\') as f:\n\t\t\t\t\tf.write(\'\\n\'.join(lines))\n\t\t\t\tsid = []\n\t\t\t\tlines = []\n\t\t\t\tsub += 1\n\n\tprint(\'writing submissions_sub%i\'%sub)\n\tsids.append(set(sid))\n\twith open(fld_split + \'/rs_sub%i.tsv\'%sub, \'w\', encoding=\'utf-8\') as f:\n\t\tf.write(\'\\n\'.join(lines))\n\tprint(\'extract_submissions done.\\n\')\n\treturn sids, m, n\n\n\ndef extract_comments(fld_bz2, fld_split, sids):\n\tpath_in = fld_bz2 + \'/RC_%s.bz2\'%args.dump_name\n\tn = 0\n\tm = 0\n\tn_sub = len(sids)\n\tlines = [[] for i in range(n_sub)]\n\tfor sub in range(n_sub):\n\t\topen(fld_split + \'/rc_sub%i.tsv\'%sub, \'w\')\n\n\twith bz2.open(path_in, \'rt\', encoding=""utf-8"") as f:\n\t\tfor line in f:\n\t\t\tn += 1\n\t\t\tif n%1e4 == 0:\n\t\t\t\tprint(\'[%s] selected %.3fM from %.2fM comments\'%(\n\t\t\t\t\targs.dump_name, m/1e6, n/1e6))\n\n\t\t\t\tfor sub in range(n_sub):\n\t\t\t\t\tprint(\'    sub %i: %i\'%(sub, len(lines[sub])))\n\t\t\t\t\tif len(lines[sub]) > 0:\n\t\t\t\t\t\twith open(fld_split + \'/rc_sub%i.tsv\'%sub, \'a\', encoding=\'utf-8\') as f:\n\t\t\t\t\t\t\tf.write(\'\\n\'.join(lines[sub]) + \'\\n\')\n\t\t\t\t\t\tlines[sub] = []\n\t\t\ttry:\n\t\t\t\tcomment = json.loads(line)\n\t\t\t\tif args.keep_keys:\n\t\t\t\t\tk = \'\\t\'.join([comment[\'link_id\'], get_comment_id(comment), \'dep\'])\n\t\t\t\t\tif k not in keys.keys():\n\t\t\t\t\t\tcontinue\n\t\t\t\tif comment[\'body\'] == \'[deleted]\': # filter 1\n\t\t\t\t\tcontinue\n\t\t\t\tif \'>\' in comment[\'body\'] or \'&gt;\' in comment[\'body\']: # filter 3: \'&gt;\' means \'>\'\n\t\t\t\t\tcontinue\n\t\t\t\tsid = comment[\'link_id\']\n\t\t\t\tfor sub in range(n_sub):\n\t\t\t\t\tif sid in sids[sub]:\n\t\t\t\t\t\tcomment[\'n_char\'] = len(comment[\'body\'])\n\t\t\t\t\t\tcomment[\'body\'] = norm_sentence(comment[\'body\'], True)\n\t\t\t\t\t\tif len(comment[\'body\'].split()) < 2: # filter 2\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\tlines[sub].append(\'\\t\'.join([str(comment[k]) for k in fields_comm]))\n\t\t\t\t\t\tm += 1\n\t\t\t\t\t\tbreak\n\n\t\t\texcept Exception:\n\t\t\t\ttraceback.print_exc()\n\n\tprint(\'the rest...\')\n\tfor sub in range(n_sub):\n\t\tprint(\'    sub %i: %i\'%(sub, len(lines[sub])))\n\t\twith open(fld_split + \'/rc_sub%i.tsv\'%sub, \'a\', encoding=\'utf-8\') as f:\n\t\t\tf.write(\'\\n\'.join(lines[sub]))\n\n\tprint(\'extract_comments done.\\n\')\n\treturn m, n\n\n\ndef get_convo(sid, rootid, cid, submissions, comments, depth=args.max_depth):\n\tif depth == 0:\n\t\treturn []\n\tc = comments[cid]\n\tif args.max_len_type == \'w\' and len(c[\'body\'].split()) > args.max_len: # len filter\n\t\treturn []\n\tif args.max_len_type == \'c\' and int(c[\'n_char\']) > args.max_len:\n\t\treturn []\n\n\tpid = c[\'parent_id\']\n\tif args.use_title and pid.startswith(TAG_SUBMISSION):\n\t\ttxts = [ ""title: "" + submissions[c[\'link_id\']][\'title\'] ]\n\telif pid in comments:\n\t\ttxts = get_convo(sid, rootid, pid, submissions, comments, depth-1)\n\telse:\n\t\ttxts = []\n\ttxts.append(c[\'body\'])\n\treturn txts\n\n\ndef filter_instance(src, tgt, info):\n\t# Remove offensive words:\n\tif args.bl_words and not args.leaves_only:\n\t\tbad_words = bl_words.extract_keywords(tgt)\n\t\tif bad_words:\n\t\t\tprint(""skip\\toffensive\\t%s\\t%s\\tbad word(s): %s"" % (info, tgt, bad_words), file=sys.stderr)\n\t\t\treturn True\n\n\t# Remove empty targets:\n\ttgttoks = tgt.split()\n\tif len(tgttoks) <= 1: # 1 means there is only a weight, and 0 means there\'s a bug..\n\t\tprint(""skip\\temptytarget\\t%s\\t%s"" % (info, tgt), file=sys.stderr)\n\t\treturn True\n\n\t# Skip if word too long:\n\ttoolong = False\n\tfor w in tgttoks:\n\t\tif len(w) > 30:\n\t\t\ttoolong = True\n\t\t\tbreak\n\tif toolong:\n\t\tprint(""skip\\tlongword\\t%s\\t%s\\tword too long"" % (info, tgt), file=sys.stderr)\n\t\treturn True\n\n\tsrctoks = src.split()\n\t# Remove empty sources: (should probably uncomment, but left for reproducibility)\n\t#if len(srctoks) <= 1: # 1 means there is only a weight, and 0 means there\'s a bug..\n\t#\tprint(""skip\\temptysource\\t%s\\t%s"" % (info, src), file=sys.stderr)\n\t#\treturn True\n\n\t# Remove too long turns:\n\tnsrctgt = len(srctoks) + len(tgttoks)\n\tif nsrctgt > 200:\n\t\tprint(""skip\\ttoolong\\t%s\\t%s\\tsrc+tgt too long, src=[%s]"" % (info, tgt, src), file=sys.stderr)\n\t\treturn True\n\n\t# Skip turns with URLs:\n\tsrctgt = src + "" "" + tgt\n\tif ""__url__"" in srctgt:\n\t\tprint(""skip\\turl\\t%s\\t%s\\turl in tgt, or src =[%s]"" % (info, tgt, src), file=sys.stderr)\n\t\treturn True\n\n\t# Skip responses with meta data:\n\tif re.search(""[\\[\\]\\(\\)]"", srctgt) != None:\n\t\tprint(""skip\\ttags\\t%s\\t%s\\ttag in tgt (or src: [%s])"" % (info, tgt, src), file=sys.stderr)\n\t\treturn True\n\n\t# Skip yelling:\n\tif re.search(""[A-Z]{5,}"", srctgt) != None:\n\t\tprint(""skip\\tallcaps\\t%s\\t%s\\tall caps in tgt (or src: [%s])"" % (info, tgt, src), file=sys.stderr)\n\t\treturn True\n\n\t# Skip word repetitions:\n\treps = False\n\tfor i in range(2, len(tgttoks)):\n\t\tif tgttoks[i-2] == tgttoks[i] and tgttoks[i-1] == tgttoks[i]:\n\t\t\treps = True\n\t\t\tbreak\n\tif reps:\n\t\tprint(""skip\\trepetitions\\t%s\\t%s\\ttoo many repetitions"" % (info, tgt), file=sys.stderr)\n\t\treturn True\n\n\treturn False\n\n\ndef save_convo(path_rs, path_rc, path_out):\n\tprint(\'reading submissions...\')\n\tsubmissions = dict()\n\twith gzip.open(path_rs, mode=\'rt\', encoding=\'utf-8\') as f:\n\t\tfor line in f:\n\t\t\tcells = line.strip(\'\\n\').strip().split(\'\\t\')\n\t\t\ttry:\n\t\t\t\tsubmission = dict([(fields_subm[i], cells[i]) for i in range(len(fields_subm))])\n\t\t\texcept Exception:\n\t\t\t\t#traceback.print_exc()\n\t\t\t\tcontinue\n\t\t\tsubmissions[get_submission_id(submission)] = submission\n\n\tprint(\'reading comments...\')\n\tcomments = dict()\n\twith gzip.open(path_rc, mode=\'rt\', encoding=\'utf-8\') as f:\n\t\tfor line in f:\n\t\t\tcells = line.strip(\'\\n\').strip().split(\'\\t\')\n\t\t\ttry:\n\t\t\t\tcomment = dict([(fields_comm[i], cells[i]) for i in range(len(fields_comm))])\n\t\t\texcept Exception:\n\t\t\t\ttraceback.print_exc()\n\t\t\t\tcontinue\n\t\t\tcomments[get_comment_id(comment)] = comment\n\n\tsorted_id = sorted([(\n\t\t\t\t\tcomments[cid][\'link_id\'],\n\t\t\t\t\tcomments[cid][\'parent_id\'],\n\t\t\t\t\tcid\n\t\t\t\t\t) for cid in comments])\n\n\tn = len(comments)\n\tprint(\'total comments: %i\'%n)\n\n\ti = 0\n\tm = 0\n\tlines = []\n\tsum_resp_len = 0\n\n\tskip_id = {}\n\tif args.leaves_only:\n\t\tfor _, pid, _ in sorted_id:\n\t\t\tskip_id[pid] = 1\n\t\tprint(""leaves ratio : %f"" % (len(skip_id) / len(sorted_id)), file=sys.stderr)\n\n\tfor sid, pid, cid in sorted_id:\n\t\tif args.keep_keys:\n\t\t\tk = \'\\t\'.join([sid, cid, \'keep\'])\n\t\t\tif k not in keys.keys():\n\t\t\t\tcontinue\n\t\tif cid in skip_id:\n\t\t\tcontinue\n\t\ti += 1\n\t\tif i%1e5 == 0:\n\t\t\tprint(\'selected %.2fM from %.1f/%.1fM comments\'%(m/1e6, i/1e6, n/1e6), file=sys.stderr)\n\t\t\tif len(lines) > 0:\n\t\t\t\twith open(path_out, \'a\', encoding=""utf-8"") as f:\n\t\t\t\t\tf.write(\'\\n\'.join(lines) + \'\\n\')\n\t\t\tlines = []\n\n\t\tsubreddit = \'\'\n\t\tdomain = \'\'\n\t\tif sid in submissions.keys():\n\t\t\tsubreddit = submissions[sid][\'permalink\'].split(\'/\')[2].lower()\n\t\t\tdomain = submissions[sid][\'domain\'].lower()\n\t\tinfo = subreddit + \'\\t\' + domain\n\n\t\tif args.bl_subreddits:\n\t\t\tif not subreddit:\n\t\t\t\tprint(""skip\\tmissing\\t%s\\tN/A\\tmissing submission: %s"" % (info, sid), file=sys.stderr)\n\t\t\t\tcontinue\n\t\t\tif subreddit in bl_subreddits:\n\t\t\t\tprint(""skip\\tbad_subreddit\\t%s\\tN/A\\toffensive subreddit: %s"" % (info, subreddit), file=sys.stderr)\n\t\t\t\tcontinue\n\n\t\tcomment = comments[cid]\n\t\tif comment[\'score\'] == \'None\':\n\t\t\tscore = 0\n\t\telse:\n\t\t\tscore = int(comment[\'score\'])\n\t\tif score < args.min_score: # filter 1\n\t\t\tprint(""skip\\tlow_score\\t%s\\t%s\\tscore %d < %d"" % (info, comment[\'body\'], score, args.min_score), file=sys.stderr)\n\t\t\tcontinue\n\t\ttry:\n\t\t\ttxts = get_convo(sid, cid, cid, submissions, comments) # filter 2\n\t\texcept Exception:\n\t\t\tprint(""skip\\texception\\t%s\\t%s\\texception"" % (info, comment[\'body\']), file=sys.stderr)\n\t\t\tcontinue\n\t\tif len(txts) < args.min_depth: # filter 3\n\t\t\tprint(""skip\\tmin_depth\\t%s\\t%s\\tdepth %d < %d: %s"" % (info, comment[\'body\'], len(txts), args.min_depth, ""|"".join(txts)), file=sys.stderr)\n\t\t\tcontinue\n\n\t\tfor i in range(len(txts)):\n\t\t\ttxts[i] = norm_sentence(txts[i], False)\n\t\t\tif args.leaves_only and args.clean:\n\t\t\t\tsc = \'1.0\'\n\t\t\t\tskip_target = False\n\t\t\t\tif args.discard_tgt_keys:\n\t\t\t\t\ttgt_h = hashlib.sha224(txts[i].encode(""utf-8"")).hexdigest()\n\t\t\t\t\tif tgt_h in keys_rm.keys():\n\t\t\t\t\t\tskip_target = True\n\t\t\t\tif bl_words.extract_keywords(txts[i]) or skip_target:\n\t\t\t\t\tsc = \'0.0\'\n\t\t\t\ttxts[i] = sc + \' \' + txts[i]\n\n\t\tsrc = \' EOS \'.join(txts[:-1])\n\t\ttgt = txts[-1]\n\n\t\tif args.clean and filter_instance(src, tgt, info):\n\t\t\tcontinue\n\n\t\theader = \',\'.join([sid, pid, cid])\n\t\tlines.append(header + \'\\t\' + src + \'\\t\' + tgt)\n\t\tsum_resp_len += len(tgt.split())\n\t\tm += 1\n\n\tavg_len = sum_resp_len/m\n\twith open(path_out, \'a\', encoding=""utf-8"") as f:\n\t\tf.write(\'\\n\'.join(lines) + \'\\n\')\n\tprint(\'finally selected %i/%i, avg len = %.2f\'%(m, n, avg_len))\n\treturn m, n, avg_len\n\n\ndef extract():\n\tmakedirs(fld_split)\n\tsids, ms, ns = extract_submissions(fld_root_in, fld_split, size=args.split_size)\n\tmc, nc = extract_comments(fld_root_in, fld_split, sids)\n\twith open(fld_split + \'/stat.tsv\', \'a\') as f:\n\t\tf.write(\'\\t\'.join(map(str, [args.dump_name, mc, nc, ms, ns])) + \'\\n\')\n\n\ndef build_conv(fld_out):\n\tmakedirs(fld_out)\n\tpath_out = fld_out + \'/%s.tsv\'%args.dump_name\n\tprint(path_out)\n\n\tif args.parallel:\n\t\tfs = open(fld_out + \'/\' + args.dump_name + \'.stat.tsv\', \'w\')\n\telse:\n\t\tfs = open(fld_out + \'/stat.tsv\', \'a\')\n\n\tsub = 0\n\tsum_m = 0\n\tsum_n = 0\n\twhile True:\n\t\tpath_rs = fld_split + \'/rs_sub%i.tsv.gz\'%sub\n\t\tif not os.path.exists(path_rs):\n\t\t\tif sub == 0:\n\t\t\t\tprint(\'no such file: \'+path_rs)\n\t\t\tbreak\n\t\tprint(\'-\'*10 + \' sub%i \'%sub + \'-\'*10)\n\t\tpath_rc = path_rs.replace(\'/rs_\', \'/rc_\')\n\t\tm, n, avg_len = save_convo(path_rs, path_rc, path_out)\n\t\tfs.write(\'\\t\'.join([args.dump_name, str(sub), str(m), str(n), \'%.2f\'%avg_len]) + \'\\n\')\n\t\tsum_m += m\n\t\tsum_n += n\n\t\tsub += 1\n\n\tfs.write(\'\\t\'.join([args.dump_name, \'all\', str(sum_m), str(sum_n), \'\']) + \'\\n\')\n\tfs.close()\n\n\ndef load_keys(key_file):\n\td = {}\n\twith gzip.open(key_file, \'rt\', encoding=""utf-8"") as f:\n\t\tfor line in f:\n\t\t\tk = line.rstrip()\n\t\t\tif args.task == \'conv\' and k.endswith(\'\\tdep\'):\n\t\t\t\tcontinue\n\t\t\td[k] = 1\n\treturn d\n\n\nif args.freq_words:\n\twith open(args.freq_words, \'rt\', encoding=""utf-8"") as f:\n\t\tn = 0\n\t\tfor line in f:\n\t\t\tn += 1\n\t\t\tw = line.rstrip().lower()\n\t\t\targs.freq_words[w] = n\n\nif args.bl_words:\n\twith open(args.bl_words, \'rt\', encoding=""utf-8"") as f:\n\t\tfor line in f:\n\t\t\tif line[0] == \'#\':\n\t\t\t\tcontinue\n\t\t\tw = line.rstrip()\n\t\t\tbl_words.add_keyword(w)\n\nif args.bl_subreddits:\n\twith open(args.bl_subreddits, \'rt\', encoding=""utf-8"") as f:\n\t\tfor line in f:\n\t\t\tif line[0] == \'#\':\n\t\t\t\tcontinue\n\t\t\ts = line.rstrip().lower()\n\t\t\tbl_subreddits[s] = 1\n\nif args.ignore_keys:\n\targs.keep_keys = None\n\targs.discard_tgt_keys = None\nelse:\n\tif args.keep_keys:\n\t\tkeys = load_keys(args.keep_keys)\n\tif args.discard_tgt_keys:\n\t\tkeys_rm = load_keys(args.discard_tgt_keys)\n\nfld_root_in = args.reddit_input\nfld_root_out = args.reddit_output\nfld_split = fld_root_out + \'/extract/%s\'%(args.dump_name)\n\nif args.task == \'extract\':\n\textract()\nelif args.task == \'conv\':\n\tfld_out = fld_root_out + \'/conv\'\n\tbuild_conv(fld_out)\nelse:\n\tprint(""Unknown task: %s"" % args.task, file=sys.stderr)\n'"
