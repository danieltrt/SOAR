file_path,api_count,code
pytorch/setup.py,3,"b'import setuptools #enables develop\nimport os\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\nfrom edgeml_pytorch.utils import findCUDA\n\nif findCUDA() is not None:\n    setuptools.setup(\n        name=\'fastgrnn_cuda\',\n        ext_modules=[\n            CUDAExtension(\'fastgrnn_cuda\', [\n                \'edgeml_pytorch/cuda/fastgrnn_cuda.cpp\',\n                \'edgeml_pytorch/cuda/fastgrnn_cuda_kernel.cu\',\n            ]),\n        ],\n        cmdclass={\n            \'build_ext\': BuildExtension\n        }\n    )\n\nsetuptools.setup(\n    name=\'edgeml\',\n    version=\'0.3.0\',\n    description=\'PyTorch code for ML algorithms for edge devices developed at Microsoft Research India.\',\n    author_email=""edgeml@microsoft.com"",\n    packages=[\'edgeml_pytorch\', \'edgeml_pytorch.trainer\', \'edgeml_pytorch.graph\'],\n    license=\'MIT License\',\n    long_description=open(\'README.md\').read(),\n    url=\'https://github.com/Microsoft/EdgeML\',\n)\n'"
tf/setup.py,0,"b""from distutils.core import setup\n\nsetup(\n    name='edgeml_tf',\n    version='0.3.0',\n    packages=['edgeml_tf' ],\n    license='MIT License',\n    long_description=open('README.md').read(),\n)\n"""
pytorch/edgeml_pytorch/__init__.py,0,b''
pytorch/edgeml_pytorch/utils.py,13,"b""# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\nimport sys\nimport os\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport subprocess\nimport glob\n\ndef findCUDA():\n    '''Finds the CUDA install path.'''\n    # Guess #1\n    IS_WINDOWS = sys.platform == 'win32'\n    cuda_home = os.environ.get('CUDA_HOME') or os.environ.get('CUDA_PATH')\n    if cuda_home is None:\n        # Guess #2\n        try:\n            which = 'where' if IS_WINDOWS else 'which'\n            nvcc = subprocess.check_output(\n                [which, 'nvcc']).decode().rstrip('\\r\\n')\n            cuda_home = os.path.dirname(os.path.dirname(nvcc))\n        except Exception:\n            # Guess #3\n            if IS_WINDOWS:\n                cuda_homes = glob.glob(\n                    'C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v*.*')\n                if len(cuda_homes) == 0:\n                    cuda_home = ''\n                else:\n                    cuda_home = cuda_homes[0]\n            else:\n                cuda_home = '/usr/local/cuda'\n            if not os.path.exists(cuda_home):\n                cuda_home = None\n    return cuda_home\n\ndef multiClassHingeLoss(logits, labels):\n    '''\n    MultiClassHingeLoss to match C++ Version - No pytorch internal version\n    '''\n    flatLogits = torch.reshape(logits, [-1, ])\n    labels_ = labels.argmax(dim=1)\n\n    correctId = torch.arange(labels.shape[0]).to(\n        logits.device) * labels.shape[1] + labels_\n    correctLogit = torch.gather(flatLogits, 0, correctId)\n\n    maxLabel = logits.argmax(dim=1)\n    top2, _ = torch.topk(logits, k=2, sorted=True)\n\n    wrongMaxLogit = torch.where((maxLabel == labels_), top2[:, 1], top2[:, 0])\n\n    return torch.mean(F.relu(1. + wrongMaxLogit - correctLogit))\n\n\ndef crossEntropyLoss(logits, labels):\n    '''\n    Cross Entropy loss for MultiClass case in joint training for\n    faster convergence\n    '''\n    return F.cross_entropy(logits, labels.argmax(dim=1))\n\n\ndef binaryHingeLoss(logits, labels):\n    '''\n    BinaryHingeLoss to match C++ Version - No pytorch internal version\n    '''\n    return torch.mean(F.relu(1.0 - (2 * labels - 1) * logits))\n\n\ndef hardThreshold(A: torch.Tensor, s):\n    '''\n    Hard thresholds and modifies in-palce nn.Parameter A with sparsity s \n    '''\n    #PyTorch disallows numpy access/copy to tensors in graph.\n    #.detach() creates a new tensor not attached to the graph.\n    A_ = A.data.cpu().detach().numpy().ravel()    \n    if len(A_) > 0:\n        th = np.percentile(np.abs(A_), (1 - s) * 100.0, interpolation='higher')\n        A_[np.abs(A_) < th] = 0.0\n    A_ = A_.reshape(A.shape)\n    return torch.tensor(A_, requires_grad=True)\n\ndef supportBasedThreshold(dst: torch.Tensor, src: torch.Tensor):\n    '''\n    zero out entries in dst.data that are zeros in src tensor\n    '''\n    return copySupport(src, dst.data)\n\ndef copySupport(src, dst):\n    '''\n    zero out entries in dst.data that are zeros in src tensor\n    '''\n    zeroSupport = (src.view(-1) == 0.0).nonzero()\n    dst = dst.reshape(-1)\n    dst[zeroSupport] = 0\n    dst = dst.reshape(src.shape)\n    del zeroSupport\n    return dst\n\n\ndef estimateNNZ(A, s, bytesPerVar=4):\n    '''\n    Returns # of non-zeros and representative size of the tensor\n    Uses dense for s >= 0.5 - 4 byte\n    Else uses sparse - 8 byte\n    '''\n    params = 1\n    hasSparse = False\n    for i in range(0, len(A.shape)):\n        params *= int(A.shape[i])\n    if s < 0.5:\n        nnZ = np.ceil(params * s)\n        hasSparse = True\n        return nnZ, nnZ * 2 * bytesPerVar, hasSparse\n    else:\n        nnZ = params\n        return nnZ, nnZ * bytesPerVar, hasSparse\n\n\ndef countNNZ(A: torch.nn.Parameter, isSparse):\n    '''\n    Returns # of non-zeros \n    '''\n    A_ = A.detach().numpy()\n    if isSparse:\n        return np.count_nonzero(A_)\n    else:\n        nnzs = 1\n        for i in range(0, len(A.shape)):\n            nnzs *= int(A.shape[i])\n        return nnzs\n\ndef restructreMatrixBonsaiSeeDot(A, nClasses, nNodes):\n    '''\n    Restructures a matrix from [nNodes*nClasses, Proj] to\n    [nClasses*nNodes, Proj] for SeeDot\n    '''\n    tempMatrix = np.zeros(A.shape)\n    rowIndex = 0\n\n    for i in range(0, nClasses):\n        for j in range(0, nNodes):\n            tempMatrix[rowIndex] = A[j * nClasses + i]\n            rowIndex += 1\n\n    return tempMatrix\n\nclass TriangularLR(optim.lr_scheduler._LRScheduler):\n    def __init__(self, optimizer, stepsize, lr_min, lr_max, gamma):\n        self.stepsize = stepsize\n        self.lr_min = lr_min\n        self.lr_max = lr_max\n        self.gamma = gamma\n        super(TriangularLR, self).__init__(optimizer)\n\n    def get_lr(self):\n        it = self.last_epoch\n        cycle = math.floor(1 + it / (2 * self.stepsize))\n        x = abs(it / self.stepsize - 2 * cycle + 1)\n        decayed_range = (self.lr_max - self.lr_min) * self.gamma ** (it / 3)\n        lr = self.lr_min + decayed_range * x\n        return [lr]\n\nclass ExponentialResettingLR(optim.lr_scheduler._LRScheduler):\n    def __init__(self, optimizer, gamma, reset_epoch):\n        self.gamma = gamma\n        self.reset_epoch = int(reset_epoch)\n        super(ExponentialResettingLR, self).__init__(optimizer)\n\n    def get_lr(self):\n        epoch = self.last_epoch\n        if epoch > self.reset_epoch:\n            epoch -= self.reset_epoch\n        return [base_lr * self.gamma ** epoch\n                for base_lr in self.base_lrs]\n"""
tf/edgeml_tf/__init__.py,0,"b""# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n'''\npackage edgeml\n\nProvides: Bonsai, ProtoNN and BasicTrainer routines\n    for both\n'''\n\n# TODO Override the __all__ variable for the package\n# and limit the functions that are exposed.\n# Do not expose functions in utils - can be dangerous\n"""
tf/edgeml_tf/utils.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nfrom __future__ import print_function\nimport tensorflow as tf\nimport numpy as np\nimport scipy.cluster\nimport scipy.spatial\nimport os\n\n\ndef medianHeuristic(data, projectionDimension, numPrototypes, W_init=None):\n    \'\'\'\n    This method can be used to estimate gamma for ProtoNN. An approximation to\n    median heuristic is used here.\n    1. First the data is collapsed into the projectionDimension by W_init. If\n    W_init is not provided, it is initialized from a random normal(0, 1). Hence\n    data normalization is essential.\n    2. Prototype are computed by running a  k-means clustering on the projected\n    data.\n    3. The median distance is then estimated by calculating median distance\n    between prototypes and projected data points.\n\n    data needs to be [-1, numFeats]\n    If using this method to initialize gamma, please use the W and B as well.\n\n    TODO: Return estimate of Z (prototype labels) based on cluster centroids\n    andand labels\n\n    TODO: Clustering fails due to singularity error if projecting upwards\n\n    W [dxd_cap]\n    B [d_cap, m]\n    returns gamma, W, B\n    \'\'\'\n    assert data.ndim == 2\n    X = data\n    featDim = data.shape[1]\n    if projectionDimension > featDim:\n        print(""Warning: Projection dimension > feature dimension. Gamma"")\n        print(""\\t estimation due to median heuristic could fail."")\n        print(""\\tTo retain the projection dataDimension, provide"")\n        print(""\\ta value for gamma."")\n\n    if W_init is None:\n        W_init = np.random.normal(size=[featDim, projectionDimension])\n    W = W_init\n    XW = np.matmul(X, W)\n    assert XW.shape[1] == projectionDimension\n    assert XW.shape[0] == len(X)\n    # Requires [N x d_cap] data matrix of N observations of d_cap-dimension and\n    # the number of centroids m. Returns, [n x d_cap] centroids and\n    # elementwise center information.\n    B, centers = scipy.cluster.vq.kmeans2(XW, numPrototypes)\n    # Requires two matrices. Number of observations x dimension of observation\n    # space. Distances[i,j] is the distance between XW[i] and B[j]\n    distances = scipy.spatial.distance.cdist(XW, B, metric=\'euclidean\')\n    distances = np.reshape(distances, [-1])\n    gamma = np.median(distances)\n    gamma = 1 / (2.5 * gamma)\n    return gamma.astype(\'float32\'), W.astype(\'float32\'), B.T.astype(\'float32\')\n\n\ndef multiClassHingeLoss(logits, label, batch_th):\n    \'\'\'\n    MultiClassHingeLoss to match C++ Version - No TF internal version\n    \'\'\'\n    flatLogits = tf.reshape(logits, [-1, ])\n    label_ = tf.argmax(label, 1)\n\n    correctId = tf.range(0, batch_th) * label.shape[1] + label_\n    correctLogit = tf.gather(flatLogits, correctId)\n\n    maxLabel = tf.argmax(logits, 1)\n    top2, _ = tf.nn.top_k(logits, k=2, sorted=True)\n\n    wrongMaxLogit = tf.where(\n        tf.equal(maxLabel, label_), top2[:, 1], top2[:, 0])\n\n    return tf.reduce_mean(tf.nn.relu(1. + wrongMaxLogit - correctLogit))\n\n\ndef crossEntropyLoss(logits, label):\n    \'\'\'\n    Cross Entropy loss for MultiClass case in joint training for\n    faster convergence\n    \'\'\'\n    return tf.reduce_mean(\n        tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n                                                   labels=tf.stop_gradient(label)))\n\n\ndef mean_absolute_error(logits, label):\n    \'\'\'\n    Function to compute the mean absolute error.\n    \'\'\'\n    return tf.reduce_mean(tf.abs(tf.subtract(logits, label)))\n\n\ndef hardThreshold(A, s):\n    \'\'\'\n    Hard thresholding function on Tensor A with sparsity s\n    \'\'\'\n    A_ = np.copy(A)\n    A_ = A_.ravel()\n    if len(A_) > 0:\n        th = np.percentile(np.abs(A_), (1 - s) * 100.0, interpolation=\'higher\')\n        A_[np.abs(A_) < th] = 0.0\n    A_ = A_.reshape(A.shape)\n    return A_\n\n\ndef copySupport(src, dest):\n    \'\'\'\n    copy support of src tensor to dest tensor\n    \'\'\'\n    support = np.nonzero(src)\n    dest_ = dest\n    dest = np.zeros(dest_.shape)\n    dest[support] = dest_[support]\n    return dest\n\n\ndef countnnZ(A, s, bytesPerVar=4):\n    \'\'\'\n    Returns # of non-zeros and representative size of the tensor\n    Uses dense for s >= 0.5 - 4 byte\n    Else uses sparse - 8 byte\n    \'\'\'\n    params = 1\n    hasSparse = False\n    for i in range(0, len(A.shape)):\n        params *= int(A.shape[i])\n    if s < 0.5:\n        nnZ = np.ceil(params * s)\n        hasSparse = True\n        return nnZ, nnZ * 2 * bytesPerVar, hasSparse\n    else:\n        nnZ = params\n        return nnZ, nnZ * bytesPerVar, hasSparse\n\n\ndef getConfusionMatrix(predicted, target, numClasses):\n    \'\'\'\n    Returns a confusion matrix for a multiclass classification\n    problem. `predicted` is a 1-D array of integers representing\n    the predicted classes and `target` is the target classes.\n\n    confusion[i][j]: Number of elements of class j\n        predicted as class i\n    Labels are assumed to be in range(0, numClasses)\n    Use`printFormattedConfusionMatrix` to echo the confusion matrix\n    in a user friendly form.\n    \'\'\'\n    assert(predicted.ndim == 1)\n    assert(target.ndim == 1)\n    arr = np.zeros([numClasses, numClasses])\n\n    for i in range(len(predicted)):\n        arr[predicted[i]][target[i]] += 1\n    return arr\n\n\ndef printFormattedConfusionMatrix(matrix):\n    \'\'\'\n    Given a 2D confusion matrix, prints it in a human readable way.\n    The confusion matrix is expected to be a 2D numpy array with\n    square dimensions\n    \'\'\'\n    assert(matrix.ndim == 2)\n    assert(matrix.shape[0] == matrix.shape[1])\n    RECALL = \'Recall\'\n    PRECISION = \'PRECISION\'\n    print(""|%s|"" % (\'True->\'), end=\'\')\n    for i in range(matrix.shape[0]):\n        print(""%7d|"" % i, end=\'\')\n    print(""%s|"" % \'Precision\')\n\n    print(""|%s|"" % (\'-\' * len(RECALL)), end=\'\')\n    for i in range(matrix.shape[0]):\n        print(""%s|"" % (\'-\' * 7), end=\'\')\n    print(""%s|"" % (\'-\' * len(PRECISION)))\n\n    precisionlist = np.sum(matrix, axis=1)\n    recalllist = np.sum(matrix, axis=0)\n    precisionlist = [matrix[i][i] / x if x !=\n                     0 else -1 for i, x in enumerate(precisionlist)]\n    recalllist = [matrix[i][i] / x if x !=\n                  0 else -1 for i, x in enumerate(recalllist)]\n    for i in range(matrix.shape[0]):\n        # len recall = 6\n        print(""|%6d|"" % (i), end=\'\')\n        for j in range(matrix.shape[0]):\n            print(""%7d|"" % (matrix[i][j]), end=\'\')\n        print(""%s"" % ("" "" * (len(PRECISION) - 7)), end=\'\')\n        if precisionlist[i] != -1:\n            print(""%1.5f|"" % precisionlist[i])\n        else:\n            print(""%7s|"" % ""nan"")\n\n    print(""|%s|"" % (\'-\' * len(RECALL)), end=\'\')\n    for i in range(matrix.shape[0]):\n        print(""%s|"" % (\'-\' * 7), end=\'\')\n    print(""%s|"" % (\'-\' * len(PRECISION)))\n    print(""|%s|"" % (\'Recall\'), end=\'\')\n\n    for i in range(matrix.shape[0]):\n        if recalllist[i] != -1:\n            print(""%1.5f|"" % (recalllist[i]), end=\'\')\n        else:\n            print(""%7s|"" % ""nan"", end=\'\')\n\n    print(\'%s|\' % (\' \' * len(PRECISION)))\n\n\ndef getPrecisionRecall(cmatrix, label=1):\n    trueP = cmatrix[label][label]\n    denom = np.sum(cmatrix, axis=0)[label]\n    if denom == 0:\n        denom = 1\n    recall = trueP / denom\n    denom = np.sum(cmatrix, axis=1)[label]\n    if denom == 0:\n        denom = 1\n    precision = trueP / denom\n    return precision, recall\n\n\ndef getMacroPrecisionRecall(cmatrix):\n    # TP + FP\n    precisionlist = np.sum(cmatrix, axis=1)\n    # TP + FN\n    recalllist = np.sum(cmatrix, axis=0)\n    precisionlist__ = [cmatrix[i][i] / x if x !=\n                       0 else 0 for i, x in enumerate(precisionlist)]\n    recalllist__ = [cmatrix[i][i] / x if x !=\n                    0 else 0 for i, x in enumerate(recalllist)]\n    precision = np.sum(precisionlist__)\n    precision /= len(precisionlist__)\n    recall = np.sum(recalllist__)\n    recall /= len(recalllist__)\n    return precision, recall\n\n\ndef getMicroPrecisionRecall(cmatrix):\n    # TP + FP\n    precisionlist = np.sum(cmatrix, axis=1)\n    # TP + FN\n    recalllist = np.sum(cmatrix, axis=0)\n    num = 0.0\n    for i in range(len(cmatrix)):\n        num += cmatrix[i][i]\n\n    precision = num / np.sum(precisionlist)\n    recall = num / np.sum(recalllist)\n    return precision, recall\n\n\ndef getMacroMicroFScore(cmatrix):\n    \'\'\'\n    Returns macro and micro f-scores.\n    Refer: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.104.8244&rep=rep1&type=pdf\n    \'\'\'\n    precisionlist = np.sum(cmatrix, axis=1)\n    recalllist = np.sum(cmatrix, axis=0)\n    precisionlist__ = [cmatrix[i][i] / x if x !=\n                       0 else 0 for i, x in enumerate(precisionlist)]\n    recalllist__ = [cmatrix[i][i] / x if x !=\n                    0 else 0 for i, x in enumerate(recalllist)]\n    macro = 0.0\n    for i in range(len(precisionlist)):\n        denom = precisionlist__[i] + recalllist__[i]\n        numer = precisionlist__[i] * recalllist__[i] * 2\n        if denom == 0:\n            denom = 1\n        macro += numer / denom\n    macro /= len(precisionlist)\n\n    num = 0.0\n    for i in range(len(precisionlist)):\n        num += cmatrix[i][i]\n\n    denom1 = np.sum(precisionlist)\n    denom2 = np.sum(recalllist)\n    pi = num / denom1\n    rho = num / denom2\n    denom = pi + rho\n    if denom == 0:\n        denom = 1\n    micro = 2 * pi * rho / denom\n    return macro, micro\n\n\ndef restructreMatrixBonsaiSeeDot(A, nClasses, nNodes):\n    \'\'\'\n    Restructures a matrix from [nNodes*nClasses, Proj] to \n    [nClasses*nNodes, Proj] for SeeDot\n    \'\'\'\n    tempMatrix = np.zeros(A.shape)\n    rowIndex = 0\n\n    for i in range(0, nClasses):\n        for j in range(0, nNodes):\n            tempMatrix[rowIndex] = A[j * nClasses + i]\n            rowIndex += 1\n\n    return tempMatrix\n\n\nclass GraphManager:\n    \'\'\'\n    Manages saving and restoring graphs. Designed to be used with EMI-RNN\n    though is general enough to be useful otherwise as well.\n    \'\'\'\n\n    def __init__(self):\n        pass\n\n    def checkpointModel(self, saver, sess, modelPrefix,\n                        globalStep=1000, redirFile=None):\n        saver.save(sess, modelPrefix, global_step=globalStep)\n        print(\'Model saved to %s, global_step %d\' % (modelPrefix, globalStep),\n              file=redirFile)\n\n    def loadCheckpoint(self, sess, modelPrefix, globalStep,\n                       redirFile=None):\n        metaname = modelPrefix + \'-%d.meta\' % globalStep\n        basename = os.path.basename(metaname)\n        fileList = os.listdir(os.path.dirname(modelPrefix))\n        fileList = [x for x in fileList if x.startswith(basename)]\n        assert len(fileList) > 0, \'Checkpoint file not found\'\n        msg = \'Too many or too few checkpoint files for globalStep: %d\' % globalStep\n        assert len(fileList) is 1, msg\n        chkpt = basename + \'/\' + fileList[0]\n        saver = tf.train.import_meta_graph(metaname)\n        metaname = metaname[:-5]\n        saver.restore(sess, metaname)\n        graph = tf.get_default_graph()\n        return graph\n'"
tools/SeeDot/SeeDot.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport argparse\nimport datetime\nfrom distutils.dir_util import copy_tree\nimport os\nimport shutil\nimport operator\nimport tempfile\nimport traceback\n\nimport seedot.common as Common\nfrom seedot.main import Main\nimport seedot.util as Util\n\n\nclass MainDriver:\n\n    def parseArgs(self):\n        parser = argparse.ArgumentParser()\n\n        parser.add_argument(""-a"", ""--algo"", choices=Common.Algo.All,\n                            metavar=\'\', help=""Algorithm to run (\'bonsai\' or \'protonn\')"")\n        parser.add_argument(""--train"", required=True,\n                            metavar=\'\', help=""Training set file"")\n        parser.add_argument(""--test"", required=True,\n                            metavar=\'\', help=""Testing set file"")\n        parser.add_argument(""--model"", required=True, metavar=\'\',\n                            help=""Directory containing trained model (output from Bonsai/ProtoNN trainer)"")\n        #parser.add_argument(""-v"", ""--version"", default=Common.Version.Fixed, choices=Common.Version.All, metavar=\'\',\n        #                    help=""Datatype of the generated code (fixed-point or floating-point)"")\n        parser.add_argument(""--tempdir"", metavar=\'\',\n                            help=""Scratch directory for intermediate files"")\n        parser.add_argument(""-o"", ""--outdir"", metavar=\'\',\n                            help=""Directory to output the generated Arduino sketch"")\n\n        self.args = parser.parse_args()\n\n        # Verify the input files and directory exists\n        assert os.path.isfile(self.args.train), ""Training set doesn\'t exist""\n        assert os.path.isfile(self.args.test), ""Testing set doesn\'t exist""\n        assert os.path.isdir(self.args.model), ""Model directory doesn\'t exist""\n\n        if self.args.tempdir is not None:\n            assert os.path.isdir(\n                self.args.tempdir), ""Scratch directory doesn\'t exist""\n            Common.tempdir = self.args.tempdir\n        else:\n            Common.tempdir = os.path.join(tempfile.gettempdir(\n            ), ""SeeDot"", datetime.datetime.now().strftime(\'%Y-%m-%d_%H-%M-%S\'))\n            os.makedirs(Common.tempdir, exist_ok=True)\n\n        if self.args.outdir is not None:\n            assert os.path.isdir(\n                self.args.outdir), ""Output directory doesn\'t exist""\n            Common.outdir = self.args.outdir\n        else:\n            Common.outdir = os.path.join(Common.tempdir, ""arduino"")\n            os.makedirs(Common.outdir, exist_ok=True)\n\n    def checkMSBuildPath(self):\n        found = False\n        for path in Common.msbuildPathOptions:\n            if os.path.isfile(path):\n                found = True\n                Common.msbuildPath = path\n\n        if not found:\n            raise Exception(""Msbuild.exe not found at the following locations:\\n%s\\nPlease change the path and run again"" % (\n                Common.msbuildPathOptions))\n\n    def run(self):\n        if Util.windows():\n            self.checkMSBuildPath()\n\n        algo, version, trainingInput, testingInput, modelDir = self.args.algo, Common.Version.Fixed, self.args.train, self.args.test, self.args.model\n\n        print(""\\n================================"")\n        print(""Executing on %s for Arduino"" % (algo))\n        print(""--------------------------------"")\n        print(""Train file: %s"" % (trainingInput))\n        print(""Test file: %s"" % (testingInput))\n        print(""Model directory: %s"" % (modelDir))\n        print(""================================\\n"")\n\n        obj = Main(algo, version, Common.Target.Arduino,\n                   trainingInput, testingInput, modelDir, None)\n        obj.run()\n\nif __name__ == ""__main__"":\n    obj = MainDriver()\n    obj.parseArgs()\n    obj.run()\n'"
Applications/GesturePod/training/csvFromSerial.py,0,"b'\'\'\'\nCopyright (c) Microsoft Corporation. All rights reserved.\nLicensed under the MIT license.\n \ncsvFromSerial.py reads data from Serial COM port and stores it as a csv. \nThis script is used for collecting raw data.\n\'\'\'\n\nimport serial\nimport sys\nfrom threading import Thread\nimport os\n\nTOGGLESTATUS = \'OFF\'\nEXIT = False\n\n\'\'\'\nToggle can be used to mark the approximate beginng and end boundries for labels.\n\'\'\'\ndef threadToggle():\n    global TOGGLESTATUS\n    global EXIT\n    while not EXIT:\n        try:\n            line = input()\n            if \' \' in line:\n                if TOGGLESTATUS == \'OFF\':\n                    TOGGLESTATUS = \'ON\'\n                else:\n                    TOGGLESTATUS = \'OFF\'\n            if \'q\' in line:\n                EXIT = True\n                break\n        except KeyboardInterrupt:\n            print(""\\nExiting."")\n\n\ndef main():\n    global EXIT\n    global TOGGLESTATUS\n    sessionKey = \'\'\n    outFile = \'\'\n    comPort = \'COM13\'\n    baudRate = 115200\n    if len(sys.argv) < 2:\n        print(""Usage: %s SESSION_KEY [COM Port]"" % (sys.argv[0]))\n        EXIT = True\n        return\n    else:\n        sessionKey = sys.argv[1]\n        if len(sys.argv) > 2:\n            comPort = sys.argv[2]\n        outFile = sessionKey + \'.csv\'\n\n    ser = serial.Serial(port=comPort, baudrate=baudRate, timeout=1)\n    testChar = ser.read()\n    if len(testChar) < 1:\n        print(""No bytes received. Exiting!"")\n        return\n    # Ignore what ever remains of the first line\n    for x in range(0, 5):\n        a = ser.readline()\n    # Create directory\n    if not os.path.exists(\'data\'):\n        os.mkdir(\'data\')\n    if not os.path.exists(\'data/raw_data\'):\n        os.mkdir(\'data/raw_data\')\n    outFile = \'./data/raw_data/\' + outFile\n    fout = open(outFile, \'w\')\n    fout.write(""millis,ax,ay,az,gx,gy,gz,toggle\\n"")\n    print(""Starting fetch"")\n    print(""Use spacebar to toggle label On/Off"")\n    linesWritten = 0\n    while not EXIT:\n        try:\n            print(\'\\r%-3s\' % (TOGGLESTATUS), end=\'\')\n            a = ser.readline().decode(\'utf-8\')\n            a = a[:-3]\n            tokens = a.split(\',\')\n            if len(tokens) != 7:\n                continue\n            if TOGGLESTATUS == \'OFF\':\n                toggle = 0\n            else:\n                toggle = 1\n            a += \',\' + str(toggle)\n            a += \'\\n\'\n            fout.write(a)\n            linesWritten += 1\n            if linesWritten % 500 is 0:\n                print(""\\r%-3s%5d"" % (TOGGLESTATUS, linesWritten), end=\'\')\n        except KeyboardInterrupt:\n            ser.close()\n            fout.close()\n            print(""\\nExiting."")\n            EXIT = True\n            break\n\n\nif __name__ == \'__main__\':\n    thread1 = Thread(target = threadToggle)\n    thread2 = Thread(target = main)\n    thread1.start()\n    thread2.start()\n    thread1.join()\n    thread2.join()\n'"
Applications/GesturePod/training/dataFileTemplate.py,0,"b""'''\nCopyright (c) Microsoft Corporation. All rights reserved.\nLicensed under the MIT license.\n\ndataFileTemplate.py has the template for data.h \nDO NOT MODIFY THIS\n\n'''\ndef populateDataFileTemplate(valueDict):\n    gamma = valueDict['gamma']\n    featDim = valueDict['featDim']\n    ldDim = valueDict['ldDim']\n    ldProjectionMatrix = valueDict['ldProjectionMatrix']\n    numPrototypes = valueDict['numPrototypes']\n    prototypeMatrix = valueDict['prototypeMatrix']\n    numLabels = valueDict['numLabels']\n    prototypeLabelMatrix = valueDict['prototypeLabelMatrix']\n    \n    template = '''/*\n * This is an autogenerated file. Modifications\n * might not be persistent.\n */\nnamespace protoNNParam {\n\n#ifndef __TEST_PROTONN__\n    /** Gamma for gaussian kernel */\n    const PROGMEM float gamma = %s;\n    /** Low Dimensional Projection Matrix */\n    const PROGMEM unsigned int featDim = %s;\n    const PROGMEM unsigned int ldDim = %s;\n\n    /**\n    * Projectino Matrix (W)\n    * d_cap x d flattened (dimension of 2D array)\n    * ldDim x featDim\n    */\n    const PROGMEM  float ldProjectionMatrix[]  = {%s};\n    ''' % (gamma, featDim, ldDim, ldProjectionMatrix)\n\n    template += '''\n    /**\n     * Prototypes (B)\n     * m x d_cap flattened (dimension of 2D array)\n     * numPrototypes x d_cap\n     */\n    const PROGMEM float prototypeMatrix[] = {%s};\n    /** Number of prototypes (m) */\n    const PROGMEM unsigned int numPrototypes = %s;\n    ''' % (prototypeMatrix, numPrototypes)\n\n    template += '''\n    /**\n     * Prototype Lables (Z)\n     * m x L (dimension of 2D array)\n     * numLabels x numPrototypes\n     */\n    const PROGMEM float prototypeLabelMatrix[] = {%s};\n    /** Number of output labels, (L). */\n    const PROGMEM unsigned int numLabels = %s; // 0,1,2,3,4,5\n    ''' % (prototypeLabelMatrix, numLabels)\n\n    template += '''\n#else\n    const PROGMEM float = 1.0;\n    const PROGMEM unsigned int featDim = 10;\n    const PROGMEM unsigned int ldDim = 5;\n    // Row Major (X.x)\n    const PROGMEM  float ldProjectionMatrix[]  = {\n        0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,\n        1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,\n        2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,\n        3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,\n        4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,\n    };\n    // Column Major\n    const PROGMEM unsigned int numPrototypes = 3;\n    const PROGMEM float prototypeMatrix[] = {\n        -1.0,-0.5,0.0,0.5,1.0,\n        -2.0,-1.0,0.0,1.0,2.0,\n        -7.51,-7.51,-7.51,-7.51,-7.51,\n    };\n    // column major\n    const PROGMEM unsigned int numLabels = 4;\n    const PROGMEM float prototypeLabelMatrix[] = {\n        0.96,0.01,0.01,0.02,\n        0.02,0.94,0.02,0.02,\n        0.10,0.15,0.55,0.20,\n    };\n#endif\n};\n    '''\n    return template\n"""
Applications/GesturePod/training/genDataHeader.py,0,"b""'''\nCopyright (c) Microsoft Corporation. All rights reserved.\nLicensed under the MIT license.\n\ngenDataHeader.py generates data.h\n'''\n\nimport numpy as np\nimport pandas as pd\nfrom dataFileTemplate import populateDataFileTemplate\nimport sys\n\n\ndef loadTLCMatrices(dfolder):\n    '''\n    Loads Matrices B, W and Z from TLC format\n    '''\n    # W is stored as d_cap x d\n    df = pd.read_csv(dfolder + 'W', sep='\\t', header=None)\n    W = np.matrix(df)\n    d_cap = W.shape[0]\n\n    # B is stored as d_cap x m\n    df = pd.read_csv(dfolder + 'B', sep='\\t', header=None)\n    B = np.matrix(df)\n    m = B.shape[1]\n    assert(d_cap == B.shape[0])\n\n    # Z is stored as L x m\n    df = pd.read_csv(dfolder + 'Z', sep='\\t', header=None)\n    Z = np.matrix(df)\n    assert(Z.shape[1] == m)\n\n    return W, B, Z\n\n\ndef createDataFile(W, B, Z, gamma, outfile='data.h'):\n    '''\n    Exports the provided matrices loaded, into a temp file\n    so that they can be directly copied onto the MKR1000\n    board's data.h file.\n    Required input matrix dimensions:\n    W: d_cap x d\n    B: d_cap x m\n    Z: L x m\n\n    Created output dimensions:\n    W: d_cap x d\n    B: m x d_cap (i.e column major)\n    Z: m x L (i.e. column major)\n    '''\n    valueDict = {}\n    valueDict['gamma'] = gamma\n    d_cap = W.shape[0]\n    d = W.shape[1]\n    valueDict['featDim'] = '%d' % (d)\n    valueDict['ldDim'] = '%d' % (d_cap)\n    WStr = '\\n\\t\\t'\n    for i in range(0, d_cap):\n        for j in range(0, d):\n            WStr += str(W[i, j]) + ','\n        WStr += '\\n\\t\\t'\n    valueDict['ldProjectionMatrix'] = WStr[:-1]\n\n    assert(B.shape[0] == d_cap)\n    m = B.shape[1]\n    valueDict['numPrototypes'] = '%d' % (m)\n    BStr = '\\n\\t\\t'\n    for i in range(0, m):\n        for j in range(0, d_cap):\n            # Column major (j, i)\n            BStr += '%f' % (B[j, i]) + ','\n        BStr += '\\n\\t\\t'\n    valueDict['prototypeMatrix'] = BStr[:-1]\n    assert(Z.shape[1] == m)\n    L = Z.shape[0]\n    valueDict['numLabels'] = L\n    ZStr = '\\n\\t\\t'\n    for i in range(0, m):\n        for j in range(0, L):\n            # Columns major\n            ZStr += '%f' % (Z[j, i]) + ','\n        ZStr += '\\n\\t\\t'\n    valueDict['prototypeLabelMatrix'] = ZStr[:-2]\n    template = populateDataFileTemplate(valueDict)\n    fin = open(outfile, 'w')\n    fin.write(template)\n    fin.close()\n\n\n\ndef automaticExport():\n    # Copy W, B, Z from Debug\n    gamma = sys.argv[1]\n    dfolder = './'\n    W, B, Z = loadTLCMatrices(dfolder)\n    # Create a new file with gamma in it\n    createDataFile(W, B, Z, gamma, 'data.h')\n\n\nif __name__ == '__main__':\n    automaticExport()\n"""
Applications/GesturePod/training/generateFeatures.py,0,"b'\'\'\'\nCopyright (c) Microsoft Corporation. All rights reserved.\nLicensed under the MIT license.\n\nfeatureExtraction.py extracts features from labelled data files.\n\nASSUMPTIONS:\n\n- Negatives in general have to be specified by 1 and should\nbe at least 400 wide. In fact all non zero classes have to be\nat least 400 wide. Any more, and I can\'t gaurentee anything.\nThis is for things like pertubations and all to work properly\n\n- Do not change multi-threads to multi-process. It will not \nwork with the current formulation.\n\'\'\'\nimport pandas as pd\nimport time\nimport numpy as np\nimport re\nimport threading\nimport os\n\n\nlabelledFileList = [\n    # Should not contain files with only noise.\n    # They are delt with separately - allNoiseFileList.\n    \n    \'foo.csv\',\n]\n\nallNoiseFileList = [\n    # Files containing only noise - walking, climbing stairs, etc.\n    # Note - This requires raw files, *NOT* labelled files\n\n    \'bar.csv\',\n\n    \n]\n\n# Not tested for more than 1 thread\nNUM_THREADS = 1\n\ndef collapseLabel(dataFrame, hyperParams):\n    \'\'\'\n    if [start, end) is == 1\n    then end - 1 is set to 1 and everything else\n    is zero\n    \'\'\'\n    pertubations = hyperParams[\'pertubations\']\n    windowWidth = hyperParams[\'windowWidth\']\n    assert(\'mlabel\' in dataFrame.columns)\n    dataFrame[\'label\'] = 0\n    labelIndex = dataFrame.columns.get_loc(\'label\')\n    startIndex = 0\n    endIndex = windowWidth\n    allVals = dataFrame[\'mlabel\'].values\n    while endIndex < len(dataFrame):\n        # check if all values in current window\n        # are non zero and equal\n        currVals = allVals[startIndex:endIndex]\n        pureWindow = True\n        base = currVals[0]\n        if base == 0:\n            pureWindow = False\n        else:\n            tot = np.sum(currVals)\n            # not fool proof but good enough\n            if tot != base * windowWidth:\n                pureWindow = False\n        if pureWindow:\n            minInd = max(0, endIndex - 1 - pertubations)\n            maxInd = min(endIndex + pertubations, len(dataFrame))\n            dataFrame.iloc[minInd:maxInd, labelIndex] = base\n        startIndex += 1\n        endIndex += 1\n    return dataFrame\n\n\ndef indicesMaxMin(__list, hyperParams):\n    \'\'\'\n    This function is called by longestPosNegFeatures()\n    This takes a 400 long list of values and returns the \n    index of longest max and min values, along with the \n    measure of longest such instances.\n    \'\'\'\n    i = imax = imin = maxval = minval = maxCount = minCount = 0\n    thresholdCount = 3\n    windowWidth = hyperParams[\'windowWidth\']\n    length = len(__list)\n    assert(length == windowWidth)\n    while(i < length):\n        if(__list[i] > 0.62):\n            maxCount = 0\n            postemp = i\n            while(i < length and __list[i] > 0.62):\n                i = i + 1\n                maxCount = maxCount + 1\n            if(maxCount > maxval):\n                maxval = maxCount\n                imax = postemp\n        elif(i < length and __list[i] < 0.32):\n            minCount = 0\n            negtemp = i\n            while(i < length and __list[i] < 0.32):\n                i = i + 1\n                minCount = minCount + 1\n            if(minCount > minval):\n                minval = minCount\n                imin = negtemp\n        else:\n            i = i + 1\n    if(maxval < thresholdCount):\n        imax = -1\n        maxval = 0\n    if(minval < thresholdCount):\n        imin = -1\n        minval = 0\n    maxmin = [imax, maxval, minval, imin]\n    return maxmin\n\n\ndef longestPosNegFeatures(dataFrame, hyperParams):\n    \'\'\'\n    This function calls the entire data frame, uses\n    indicesMaxMin() function to iteratively obtain the\n    features.\n    \'\'\'\n    assert(\'norm_gy\' in dataFrame.columns)\n    windowWidth = hyperParams[\'windowWidth\']\n    windowStride = hyperParams[\'windowStride\']\n    assert(windowStride == 1)\n    dataFrame[\'longestPosEdge\'] = 0\n    dataFrame[\'longestNegEdge\'] = 0\n    dataFrame[\'longestPosCount\'] = 0\n    dataFrame[\'longestNegCount\'] = 0\n    colIndex = dataFrame.columns.get_loc(\'norm_gy\')\n    colIndexPos = dataFrame.columns.get_loc(\'longestPosEdge\')\n    colIndexNeg = dataFrame.columns.get_loc(\'longestNegEdge\')\n    colIndexPosCount = dataFrame.columns.get_loc(\'longestPosCount\')\n    colIndexNegCount = dataFrame.columns.get_loc(\'longestNegCount\')\n    startWindow = 0\n    endWindow = windowWidth\n    lengthDF = len(dataFrame)\n    while endWindow < lengthDF:\n        values = dataFrame.iloc[startWindow:endWindow, colIndex].values\n        listMaxMinIndex = indicesMaxMin(values, hyperParams)\n        dataFrame.iloc[endWindow - 1, colIndexPos] = listMaxMinIndex[0]\n        dataFrame.iloc[endWindow - 1, colIndexNeg] = listMaxMinIndex[-1]\n        dataFrame.iloc[endWindow - 1, colIndexPosCount] = listMaxMinIndex[1]\n        dataFrame.iloc[endWindow - 1, colIndexNegCount] = listMaxMinIndex[-2]\n        startWindow = startWindow + windowStride\n        endWindow = startWindow + windowWidth\n    return dataFrame\n\n\ndef binningFeatures(dataFrame, hyperParams):\n    rawColumns = hyperParams[\'rawColumns\']\n    normColumns = [\'norm_\' + x for x in rawColumns]\n    for col in normColumns:\n        assert (col in normColumns)\n    windowWidth = hyperParams[\'windowWidth\']\n    numbins = hyperParams[\'numHistogramBins\']\n    for col in normColumns:\n        for i in range(0, numbins):\n            dataFrame[\'bin_\' + str(i) + \'_\' + col] = 0\n    oldBinBoundaries = None\n    binBoundaries = []\n    for col in normColumns:\n        colIndex = dataFrame.columns.get_loc(col)\n        startWindow = 0\n        endWindow = windowWidth\n        values = dataFrame.iloc[startWindow:endWindow, colIndex]\n        binF, binBoundaries = np.histogram(values,\n                                           bins=numbins,\n                                           range=(0.0, 1.0))\n        if oldBinBoundaries is None:\n            oldBinBoundaries = binBoundaries\n        else:\n            for i in range(0, len(binBoundaries)):\n                assert binBoundaries[i] == oldBinBoundaries[i]\n        tot = 0\n        for i in range(0, numbins):\n            colname = \'bin_\' + str(i) + \'_\' + col\n            colIndex = dataFrame.columns.get_loc(colname)\n            dataFrame.iloc[endWindow - 1, colIndex] = binF[i]\n            tot += binF[i]\n\n    def findIndex(val, boundaryDict):\n        for i in range(1, len(boundaryDict)):\n            if val < boundaryDict[i]:\n                return i - 1\n        # too big, goes to last bin\n        return len(boundaryDict) - 2\n\n    lengthDF = len(dataFrame)\n    for col in normColumns:\n        allValues = dataFrame[col].values\n        freqDict = {}\n        for i in range(0, numbins):\n            colname = \'bin_\' + str(i) + \'_\' + col\n            colIndex = dataFrame.columns.get_loc(colname)\n            freqDict[colname] = [0] * (lengthDF)\n            val = dataFrame.iloc[windowWidth - 1, colIndex]\n            freqDict[colname][windowWidth - 1] = val\n\n        startWindow = 0\n        endWindow = windowWidth\n        while endWindow < lengthDF:\n            for i in range(0, numbins):\n                colname = \'bin_\' + str(i) + \'_\' + col\n                freqDict[colname][endWindow] = freqDict[colname][endWindow - 1]\n\n            valueSub = allValues[startWindow]\n            valueAdd = allValues[endWindow]\n            inS = findIndex(valueSub, oldBinBoundaries)\n            colIndex = \'bin_\' + str(inS) + \'_\' + col\n            freqDict[colIndex][endWindow] -= 1\n            inA = findIndex(valueAdd, oldBinBoundaries)\n            colIndex = \'bin_\' + str(inA) + \'_\' + col\n            freqDict[colIndex][endWindow] += 1\n            startWindow += 1\n            endWindow += 1\n        for i in range(int(windowWidth - 1), lengthDF):\n            tot = 0\n            for key in freqDict:\n                tot += freqDict[key][i]\n            assert tot == 400, i\n\n        for key in freqDict:\n            colIndex = dataFrame.columns.get_loc(key)\n            dataFrame.iloc[windowWidth:,\n                           colIndex] = freqDict[key][windowWidth:]\n\n    return dataFrame\n\n\ndef normalizeDF(df, hyperParams):\n    \'\'\'\n    min max and mean for each of the raw columns should\n    be provided.\n    \'\'\'\n    rawColumns = hyperParams[\'rawColumns\']\n    for col in rawColumns:\n        temp = \'norm_\' + col\n        df[temp] = 0\n    minMaxDict = hyperParams[\'minMaxDict\']\n    for col in rawColumns:\n        temp = \'norm_\' + col\n        assert(temp in df.columns)\n        _min_ = minMaxDict[col][\'min\']\n        _max_ = minMaxDict[col][\'max\']\n        assert(_max_ - _min_ is not 0)\n        df[temp] = (df[col] - _min_) / (_max_ - _min_)\n    return df\n\n\ndef featureExtractor(dataFrame, hyperParams, isDebug,\n                     collapse = True):\n    \'\'\'\n    Pick one file at a time\n    Make basic assertions\n    make sure infile is part of the data\n    Start sliding by 400 wide window from the top\n    and come down while extracting featuers.\n    \'\'\'\n    assert(\'mlabel\' in dataFrame.columns)\n    assert(\'infile\' in dataFrame.columns)\n    assert(\'millis\' in dataFrame.columns)\n    assert(len(dataFrame.infile.unique()) == 1)\n    assert(dataFrame.mlabel.unique()[0] in [0, 1, 3, 4, 5, 7, 9])\n    assert(hyperParams[\'windowStride\'] == 1)\n    # STEP 0: Loading the data\n    # The cleaning process should ensure that the data is sorted. The following\n    # assertion will verify this.\n    assertMsg = \'Monotonicity in millis not maintained.\'\n    i = 1\n    while i < len(dataFrame.millis):\n        assert dataFrame.millis[i - 1] <= dataFrame.millis[i], assertMsg\n        i += 1\n    # STEP 1: Normalization\n    oldColOrder = dataFrame.columns\n    dataFrame = normalizeDF(dataFrame, hyperParams)\n    for i in range(0, len(oldColOrder)):\n        assert(dataFrame.columns[i] == oldColOrder[i])\n    # STEP 2: Binning Feature\n    oldColOrder = dataFrame.columns\n    dataFrame = binningFeatures(dataFrame, hyperParams)\n    for i in range(0, len(oldColOrder)):\n        assert(dataFrame.columns[i] == oldColOrder[i])\n    # STEP 3: Index of longest positive and longest negative\n    dataFrame = longestPosNegFeatures(dataFrame, hyperParams)\n    # STEP 4: Collapse labels\n    if collapse:\n        oldColOrder = dataFrame.columns\n        dataFrame = collapseLabel(dataFrame, hyperParams)\n        for i in range(0, len(oldColOrder)):\n            assert(dataFrame.columns[i] == oldColOrder[i])\n    else:\n        dataFrame = dataFrame.iloc[400:]\n        dataFrame[\'label\'] = 0\n        dataFrame.iloc[:, dataFrame.columns.get_loc(\'label\')] = 1\n\n\ndef threadFeatureExtractor(df, hyperParams, isDebug,\n                           collapse=True, NUM_THREADS = 1):\n    \'\'\'\n    Takes the given data frame and breaks it down into \n    smaller chunks\n    \'\'\'\n    # split dataframe\n    print(""\\n Starting splitting of data frame"")\n    numDataFrames = NUM_THREADS\n    dataFrames = np.array_split(df, numDataFrames)\n    threads = [None for i in range(numDataFrames)]\n    print(""Beginning Spawning Threads"")\n    # begin threads\n    for threadCount in range(0, numDataFrames):\n        tempDf = dataFrames[threadCount]\n        threads[threadCount] = threading.Thread(target=featureExtractor, args=(tempDf, hyperParams, isDebug,))\n    print(""Starting Threads"")\n    for threadCount in range(0, numDataFrames):\n        print(""Starting thread:"", threadCount)\n        threads[threadCount].start()\n    print(""Joining Threads"")\n    for threadCount in range(0, numDataFrames):\n        threads[threadCount].join()\n        print(""Joined thread: "", threadCount)\n    print(""Concatenating dataframes"")\n    # Concatinating Dataframes\n    df = pd.concat(dataFrames)\n    return df\n\n\ndef main(inputFolder, outputFolder, fileList,\n         isDebug=False, collapse=True):\n    rawColumns = [\'ax\', \'ay\', \'az\', \'gx\', \'gy\', \'gz\']\n    hyperParams = {\n        \'windowWidth\': 400,\n        \'windowStride\': 1,\n        \'numHistogramBins\': 20,\n        \'rawColumns\': rawColumns,\n        # for N pertubations (including original), set below value to N/2\n        # (True - N, True + N]\n        \'pertubations\': 5,\n        \'minMaxDict\': {\n            \'ax\': {\'min\': -16384, \'max\': 16384},\n            \'ay\': {\'min\': -16384, \'max\': 16384},\n            \'az\': {\'min\': -16384, \'max\': 16384},\n            \'gx\': {\'min\': -512, \'max\': 512},\n            \'gy\': {\'min\': -2048, \'max\': 2048},\n            \'gz\': {\'min\': -512, \'max\': 512},\n        },\n    }\n    for key in hyperParams:\n        print(\'%15s: %s\' % (key, str(hyperParams[key])))\n\n    startTime = time.time()\n    currentFile = 1\n    oldColOrder = None\n    for __file in fileList:\n        inpFile = inputFolder + \'/\' + __file\n        msg = \'\\rFile: %3d/%-3d \' % (currentFile, len(fileList))\n        msg += ""(%2.2f%%) %-20s"" % ((currentFile / len(fileList) * 100), __file)\n        print(msg, end = \'\')\n        currentFile += 1\n\n        df = pd.read_csv(inpFile)\n        df[\'infile\'] = __file\n        ret = threadFeatureExtractor(df, hyperParams, isDebug, collapse, NUM_THREADS)\n        print(""Feature extractor done"")\n        if oldColOrder is None:\n            oldColOrder = ret.columns\n        for i in range(0, len(oldColOrder)):\n            assert(oldColOrder[i] == ret.columns[i])\n        outputName = outputFolder + \'/\' + __file[:-4] + \'_extracted.csv\'\n        print(""Starting to write to csv"")\n        ret.to_csv(outputName, index=False)\n    endTime = time.time()\n    print(""\\nElapsed feature Extraction: %ds"" % (endTime - startTime))\n\n\ndef exportTLCTrainTest(inputFolder, outputFolder, fileList):\n    dataFrame = None\n    for __file in fileList:\n        inpFile = inputFolder + \'/\' + __file[:-4] + \'_extracted.csv\'\n        ret = pd.read_csv(inpFile)\n        if dataFrame is None:\n            dataFrame = ret\n        else:\n            dataFrame = pd.concat([dataFrame, ret])\n    dataFrame = dataFrame.sample(frac=1).reset_index(drop=True)\n    binCol = [x for x in dataFrame.columns if x.startswith(\'bin\')]\n    allCol = [\'label\', \'longestPosEdge\', \'longestPosCount\',\n              \'longestNegCount\', \'longestNegEdge\'] + binCol\n    df = dataFrame[allCol]\n    df = df[df.label != 0]\n    df = df.reset_index(drop=True)\n    # Train test split\n    traindf = df.sample(frac=0.8, random_state=42)\n    testdf = df.drop(traindf.index)\n    traindf.to_csv(outputFolder + \'/\' + \'_train.csv\', index=False)\n    testdf.to_csv(outputFolder + \'/\' + \'_test.csv\', index=False)\n\n\ndef labelAllAsNoise(inputFolder, outputFolder, fileList):\n    for f in fileList:\n        df = pd.read_csv(inputFolder + \'/\' + f)\n        df[\'mlabel\'] = 0\n        colIndex = df.columns.get_loc(\'mlabel\')\n        df.iloc[:, colIndex] = 1\n        df.iloc[0, colIndex] = 0\n        outName = f[:-4] + \'_labelled.csv\'\n        df.to_csv(outputFolder + \'/\' + outName, index=False)\n\n\ndef processNoiseData(allNoiseFileList):\n    \'\'\'\n    WARNING: This takes RAW files and labels them. This does not take\n    labeled files like the other methods\n    \'\'\'\n    rawSource = \'./data/raw_data/\'\n    labeledOutput = \'./data/labelled_data/\'\n    if not os.path.exists(\'data/labelled_data\'):\n        os.mkdir(\'data/labelled_data\')\n    extractedOutput = \'./data/extracted_data/\'\n    if not os.path.exists(\'data/extracted_data\'):\n        os.mkdir(\'data/extracted_data\')\n    labelAllAsNoise(rawSource,\n                    labeledOutput, allNoiseFileList)\n    labelledFileList = [x[:-4] + \'_labelled.csv\' for x in allNoiseFileList]\n    main(labeledOutput, extractedOutput,\n         labelledFileList, isDebug=False, collapse=False)\n\n\ndef processLabelledData(labelledFileList):\n    if not os.path.exists(\'data/extracted_data\'):\n        os.mkdir(\'data/extracted_data\')\n    labelledFileList = [x[:-4] + \'_labelled.csv\' for x in labelledFileList]\n    main(\'./data/labelled_data/\', \'./data/extracted_data/\',\n         labelledFileList, isDebug=False, collapse=True)\n\n\nif __name__ == \'__main__\':\n    print(""Starting Feature Extraction"")\n    # The below 2 methods help in feature extraction.\n    processLabelledData(labelledFileList)\n    # processNoiseData(allNoiseFileList)\n    # The below method generates train and test split.\n    filesForExport = [x[:-4] + \'_labelled.csv\' for x in labelledFileList]\n    filesForExport += [x[:-4] + \'_labelled.csv\' for x in allNoiseFileList]\n    print(""Generating training and test data from %d files"" % \n                                    (len(filesForExport)))\n    exportTLCTrainTest(\'./data/extracted_data/\',\n                       \'./\', filesForExport)\n    print(""Done!"")\n'"
Applications/GesturePod/training/labelData.py,0,"b'\'\'\'\nCopyright (c) Microsoft Corporation. All rights reserved.\nLicensed under the MIT license.\n\nlabelData.py used to label (sliding) window of data.\n\'\'\'\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport sys\nimport os\n\nmodule_path = os.path.abspath(os.path.join(\'../\'))\nif module_path not in sys.path:\n    sys.path.append(module_path)\nfrom timestep.plotterobjects import LinePlotter, BarPlotter\nfrom timestep.plotterobjects import BooleanPlotter, StatusBox\nfrom timestep.eventhandler import BasicEventHandler\n\nSILENCE = \'0\'\nNOISE = \'1\'\nDTAP = \'3\'\nRIGHT_TWIST = \'4\'\nLEFT_TWIST = \'5\'\nTWIRL = \'7\'\nDOUBLE_SWIPE = \'9\'\n\ndef defaultCB(event, eventHandlerObj):\n    eobj = eventHandlerObj\n    if eobj.CURRENT_STATE is not \'PAUSED\':\n        eobj.startIndex += eobj.stride\n        eobj.endIndex += eobj.stride\n    if eobj.startIndex < 0:\n        eobj.startIndex = 0\n        eobj.endIndex = eobj.length\n    dataFrame = eventHandlerObj.dataFrame\n    if eobj.endIndex >= len(dataFrame):\n        eobj.endIndex = len(dataFrame)\n        eobj.startIndex = eobj.endIndex - eobj.length\n        eventHandlerObj.statusMessage += ""End of file\\n""\n\n\ndef leftRightCB(event, eventHandlerObj):\n    eobj = eventHandlerObj\n    if event == \'left\':\n        eobj.startIndex -= eobj.stride\n        eobj.endIndex -= eobj.stride\n    elif event == \'right\':\n        eobj.startIndex += eobj.stride\n        eobj.endIndex += eobj.stride\n\n    dataFrame = eventHandlerObj.dataFrame\n    if eobj.startIndex < 0:\n        eobj.startIndex = 0\n        eobj.endIndex = eobj.length\n    if eobj.endIndex > len(dataFrame):\n        eobj.endIndex = len(dataFrame)\n        eobj.startIndex = eobj.endIndex - eobj.length\n\n\ndef quitCB(event, eventHandlerObj):\n    global fileName\n    if not os.path.exists(\'data/labelled_data\'):\n        os.mkdir(\'data/labelled_data\')\n    outfilename = ""./data/labelled_data/"" + fileName[:-4] + \'_labelled.csv\'\n    print(""Saving and exiting"")\n    print(""Outputfile: %s"" % outfilename)\n    eventHandlerObj.dataFrame.to_csv(outfilename, index=False)\n    exit()\n\n\ndef labelCB(event, eventHandlerObj):\n    eobj = eventHandlerObj\n    if eobj.CURRENT_STATE is not \'PAUSED\':\n        eobj.statusMessage += ""Labelling allowed only when paused.\\n""\n        return\n    df = eobj.dataFrame\n    s = eobj.startIndex\n    e = eobj.endIndex\n    labelCol = eobj.labelCol\n    labelCol = df.columns.get_loc(labelCol)\n    eobj.dataFrame.iloc[s:e, labelCol] = int(event)\n\n\ndef spacePressCB(event, eventHandlerObj):\n    CURRENT_STATE = eventHandlerObj.CURRENT_STATE\n    if CURRENT_STATE == \'PAUSED\':\n        eventHandlerObj.CURRENT_STATE = \'PLAY\'\n    else:\n        eventHandlerObj.CURRENT_STATE = \'PAUSED\'\n\n\ndef run(dataFrame, length, stride):\n    fig = plt.figure()\n    #AX\n    accXPlotter = LinePlotter(length=length, subplotIndex=331,\n                              ylim=[-20000, 5000])\n    accXPlotter.ax.set_title(""acc_x"")\n    #AY\n    accYPlotter = LinePlotter(length=length, subplotIndex=332,\n                              ylim=[-20000, 5000])\n    accYPlotter.ax.set_title(""acc_y"")\n    #AZ\n    accZPlotter = LinePlotter(length=length, subplotIndex=333,\n                              ylim=[-20000, 5000])\n    accZPlotter.ax.set_title(""acc_z"")\n    #GX\n    gyrXPlotter = LinePlotter(length=length, subplotIndex=334,\n                              ylim=[-600, 600])\n    gyrXPlotter.ax.set_title(""gyr_x"")\n    #GY\n    gyrYPlotter = LinePlotter(length=length, subplotIndex=335,\n                              ylim=[-600, 600])\n    gyrYPlotter.ax.set_title(""gyr_y"")\n    #GZ\n    gyrZPlotter = LinePlotter(length=length, subplotIndex=336,\n                              ylim=[-600, 600])\n    gyrZPlotter.ax.set_title(""gyr_z"")\n    #Manual Label\n    manualLabelPlotter = LinePlotter(length=length, subplotIndex=337,\n                                     ylim=[-0.2, 9.2],\n                                     c=\'r\')\n    manualLabelPlotter.ax.set_title(""label"")\n    #Toggle\n    togglePlotter = LinePlotter(length=length, subplotIndex=338,\n                                ylim=[-0.2, 9.2],\n                                c=\'r\')\n    togglePlotter.ax.set_title(""toggle"")\n    statusBox = StatusBox(fig, initText=\'PLAY\')\n    plotterList = [accYPlotter, gyrYPlotter, manualLabelPlotter]\n    plotterList += [togglePlotter, statusBox]\n    eventHandler = BasicEventHandler(plotterList, fig)\n    eventHandler.CURRENT_STATE = \'PLAY\'\n    eventHandler.startIndex = 0\n    eventHandler.endIndex = length\n    eventHandler.length = length\n    eventHandler.stride = stride\n    eventHandler.statusMessage = """"\n    eventHandler.maxLength = len(dataFrame)\n    eventHandler.registerEvent(\' \', spacePressCB)\n    eventHandler.deregisterEvent(\'close_event\')\n    eventHandler.registerEvent(\'close_event\', quitCB)\n    eventHandler.registerEvent(\'q\', quitCB)\n    eventHandler.registerEvent(\'default\', defaultCB)\n    eventHandler.registerEvent(\'right\', leftRightCB)\n    eventHandler.registerEvent(\'left\', leftRightCB)\n    # Labelling events\n    eventHandler.registerEvent(DTAP, labelCB)\n    eventHandler.registerEvent(LEFT_TWIST, labelCB)\n    eventHandler.registerEvent(RIGHT_TWIST, labelCB)\n    eventHandler.registerEvent(NOISE, labelCB)\n    eventHandler.registerEvent(SILENCE, labelCB)\n    eventHandler.registerEvent(TWIRL, labelCB)\n    eventHandler.registerEvent(DOUBLE_SWIPE, labelCB)\n    labelCol = \'mlabel\'\n    if labelCol not in dataFrame:\n        dataFrame[labelCol] = 0\n    eventHandler.dataFrame = dataFrame\n    eventHandler.labelCol = labelCol\n    labelColIndex = dataFrame.columns.get_loc(labelCol)\n    axColIndex = dataFrame.columns.get_loc(\'ax\')\n    ayColIndex = dataFrame.columns.get_loc(\'ay\')\n    azColIndex = dataFrame.columns.get_loc(\'az\')\n    gxColIndex = dataFrame.columns.get_loc(\'gx\')\n    gyColIndex = dataFrame.columns.get_loc(\'gy\')\n    gzColIndex = dataFrame.columns.get_loc(\'gz\')\n    toggleIndex = dataFrame.columns.get_loc(\'toggle\')\n    while True:\n        startIndex = eventHandler.startIndex\n        endIndex = eventHandler.endIndex\n        CURRENT_STATE = eventHandler.CURRENT_STATE\n        # AX\n        sensorValues = dataFrame.iloc[startIndex:endIndex, axColIndex]\n        accXPlotter.setValues(sensorValues)\n        # AY\n        sensorValues = dataFrame.iloc[startIndex:endIndex, ayColIndex]\n        accYPlotter.setValues(sensorValues)\n        # Az\n        sensorValues = dataFrame.iloc[startIndex:endIndex, azColIndex]\n        accZPlotter.setValues(sensorValues)\n        # GX\n        sensorValues = dataFrame.iloc[startIndex:endIndex, gxColIndex]\n        gyrXPlotter.setValues(sensorValues)\n        # GY\n        sensorValues = dataFrame.iloc[startIndex:endIndex, gyColIndex]\n        gyrYPlotter.setValues(sensorValues)\n        # GZ\n        sensorValues = dataFrame.iloc[startIndex:endIndex, gzColIndex]\n        gyrZPlotter.setValues(sensorValues)\n        # Labelling\n        label = dataFrame.iloc[startIndex:endIndex, labelColIndex]\n        manualLabelPlotter.setValues(label)\n        # Toggle info\n        toggleVals = dataFrame.iloc[startIndex:endIndex, toggleIndex]\n        togglePlotter.setValues(toggleVals)\n        # Update graph\n        eventHandler.handleQueuedEvents()\n        statusMessage = eventHandler.statusMessage\n        statusMessage += CURRENT_STATE\n        statusBox.setText(statusMessage)\n        eventHandler.updatePlot()\n        eventHandler.statusMessage = """"\n    return dataFrame\n\nfileName = fileName  = sys.argv[1]\ndef main():\n    global fileName\n    file      = ""./data/raw_data/"" + fileName \n    dataFrame = pd.read_csv(file)\n    run(dataFrame, 400, 20)\n\n\nif __name__==""__main__"":\n    main()\n'"
cpp/eigen/scripts/relicense.py,0,"b'# This file is part of Eigen, a lightweight C++ template library\n# for linear algebra.\n#\n# Copyright (C) 2012 Keir Mierle <mierle@gmail.com>\n#\n# This Source Code Form is subject to the terms of the Mozilla\n# Public License v. 2.0. If a copy of the MPL was not distributed\n# with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n#\n# Author: mierle@gmail.com (Keir Mierle)\n#\n# Make the long-awaited conversion to MPL.\n\nlgpl3_header = \'\'\'\n// Eigen is free software; you can redistribute it and/or\n// modify it under the terms of the GNU Lesser General Public\n// License as published by the Free Software Foundation; either\n// version 3 of the License, or (at your option) any later version.\n//\n// Alternatively, you can redistribute it and/or\n// modify it under the terms of the GNU General Public License as\n// published by the Free Software Foundation; either version 2 of\n// the License, or (at your option) any later version.\n//\n// Eigen is distributed in the hope that it will be useful, but WITHOUT ANY\n// WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n// FOR A PARTICULAR PURPOSE. See the GNU Lesser General Public License or the\n// GNU General Public License for more details.\n//\n// You should have received a copy of the GNU Lesser General Public\n// License and a copy of the GNU General Public License along with\n// Eigen. If not, see <http://www.gnu.org/licenses/>.\n\'\'\'\n\nmpl2_header = """"""\n// This Source Code Form is subject to the terms of the Mozilla\n// Public License v. 2.0. If a copy of the MPL was not distributed\n// with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n""""""\n\nimport os\nimport sys\n\nexclusions = set([\'relicense.py\'])\n\ndef update(text):\n  if text.find(lgpl3_header) == -1:\n    return text, False\n  return text.replace(lgpl3_header, mpl2_header), True\n\nrootdir = sys.argv[1]\nfor root, sub_folders, files in os.walk(rootdir):\n    for basename in files:\n        if basename in exclusions:\n          print \'SKIPPED\', filename\n          continue\n        filename = os.path.join(root, basename)\n        fo = file(filename)\n        text = fo.read()\n        fo.close()\n\n        text, updated = update(text)\n        if updated:\n          fo = file(filename, ""w"")\n          fo.write(text)\n          fo.close()\n          print \'UPDATED\', filename\n        else:\n          print \'       \', filename\n'"
examples/pytorch/Bonsai/bonsai_example.py,8,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport helpermethods\nimport numpy as np\nimport sys\nfrom edgeml_pytorch.trainer.bonsaiTrainer import BonsaiTrainer\nfrom edgeml_pytorch.graph.bonsai import Bonsai\nimport torch\n\n\ndef main():\n    # change cuda:0 to cuda:gpuid for specific allocation\n    device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n    # Fixing seeds for reproducibility\n    torch.manual_seed(42)\n    np.random.seed(42)\n\n    # Hyper Param pre-processing\n    args = helpermethods.getArgs()\n\n    sigma = args.sigma\n    depth = args.depth\n\n    projectionDimension = args.proj_dim\n    regZ = args.rZ\n    regT = args.rT\n    regW = args.rW\n    regV = args.rV\n\n    totalEpochs = args.epochs\n\n    learningRate = args.learning_rate\n\n    dataDir = args.data_dir\n\n    outFile = args.output_file\n\n    (dataDimension, numClasses, Xtrain, Ytrain, Xtest, Ytest,\n     mean, std) = helpermethods.preProcessData(dataDir)\n\n    sparZ = args.sZ\n\n    if numClasses > 2:\n        sparW = 0.2\n        sparV = 0.2\n        sparT = 0.2\n    else:\n        sparW = 1\n        sparV = 1\n        sparT = 1\n\n    if args.sW is not None:\n        sparW = args.sW\n    if args.sV is not None:\n        sparV = args.sV\n    if args.sT is not None:\n        sparT = args.sT\n\n    if args.batch_size is None:\n        batchSize = np.maximum(100, int(np.ceil(np.sqrt(Ytrain.shape[0]))))\n    else:\n        batchSize = args.batch_size\n\n    useMCHLoss = True\n\n    if numClasses == 2:\n        numClasses = 1\n\n    currDir = helpermethods.createTimeStampDir(dataDir)\n\n    helpermethods.dumpCommand(sys.argv, currDir)\n    helpermethods.saveMeanStd(mean, std, currDir)\n\n    # numClasses = 1 for binary case\n    bonsaiObj = Bonsai(numClasses, dataDimension,\n                       projectionDimension, depth, sigma).to(device)\n\n    bonsaiTrainer = BonsaiTrainer(bonsaiObj,\n                                  regW, regT, regV, regZ,\n                                  sparW, sparT, sparV, sparZ,\n                                  learningRate, useMCHLoss, outFile, device)\n\n    bonsaiTrainer.train(batchSize, totalEpochs,\n                        torch.from_numpy(Xtrain.astype(np.float32)),\n                        torch.from_numpy(Xtest.astype(np.float32)),\n                        torch.from_numpy(Ytrain.astype(np.float32)),\n                        torch.from_numpy(Ytest.astype(np.float32)),\n                        dataDir, currDir)\n    sys.stdout.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/pytorch/Bonsai/fetch_usps.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n#\n# Setting up the USPS Data.\n\nimport subprocess\nimport os\nimport numpy as np\nfrom sklearn.datasets import load_svmlight_file\nimport sys\n\ndef downloadData(workingDir, downloadDir, linkTrain, linkTest):\n    def runcommand(command):\n        p = subprocess.Popen(command.split(), stdout=subprocess.PIPE)\n        output, error = p.communicate()\n        assert(p.returncode == 0), \'Command failed: %s\' % command\n\n    path = workingDir + \'/\' + downloadDir\n    path = os.path.abspath(path)\n    try:\n        os.mkdir(path)\n    except OSError:\n        print(""Could not create %s. Make sure the path does"" % path)\n        print(""not already exist and you have permisions to create it."")\n        return False\n    cwd = os.getcwd()\n    os.chdir(path)\n    print(""Downloading data"")\n    command = \'wget %s\' % linkTrain\n    runcommand(command)\n    command = \'wget %s\' % linkTest\n    runcommand(command)\n    print(""Extracting data"")\n    command = \'bzip2 -d usps.bz2\'\n    runcommand(command)\n    command = \'bzip2 -d usps.t.bz2\'\n    runcommand(command)\n    command = \'mv usps train.txt\'\n    runcommand(command)\n    command = \'mv usps.t test.txt\'\n    runcommand(command)\n    os.chdir(cwd)\n    return True\n\nif __name__ == \'__main__\':\n    workingDir = \'./\'\n    downloadDir = \'usps10\'\n    linkTrain = \'http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/usps.bz2\'\n    linkTest = \'http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/usps.t.bz2\'\n    failureMsg = \'\'\'\nDownload Failed!\nTo manually perform the download\n\\t1. Create a new empty directory named `usps10`.\n\\t2. Download the data from the following links into the usps10 directory.\n\\t\\tTest: %s\n\\t\\tTrain: %s\n\\t3. Extract the downloaded files.\n\\t4. Rename `usps` to `train.txt` and,\n\\t5. Rename `usps.t` to `test.txt\n\'\'\' % (linkTrain, linkTest)\n\n    if not downloadData(workingDir, downloadDir, linkTrain, linkTest):\n        exit(failureMsg)\n    print(""Done"")\n'"
examples/pytorch/Bonsai/helpermethods.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport argparse\nimport datetime\nimport os\nimport numpy as np\n\n\'\'\'\n Functions to check sanity of input arguments\n for the example script.\n\'\'\'\n\n\ndef checkIntPos(value):\n    ivalue = int(value)\n    if ivalue <= 0:\n        raise argparse.ArgumentTypeError(\n            ""%s is an invalid positive int value"" % value)\n    return ivalue\n\n\ndef checkIntNneg(value):\n    ivalue = int(value)\n    if ivalue < 0:\n        raise argparse.ArgumentTypeError(\n            ""%s is an invalid non-neg int value"" % value)\n    return ivalue\n\n\ndef checkFloatNneg(value):\n    fvalue = float(value)\n    if fvalue < 0:\n        raise argparse.ArgumentTypeError(\n            ""%s is an invalid non-neg float value"" % value)\n    return fvalue\n\n\ndef checkFloatPos(value):\n    fvalue = float(value)\n    if fvalue <= 0:\n        raise argparse.ArgumentTypeError(\n            ""%s is an invalid positive float value"" % value)\n    return fvalue\n\n\ndef str2bool(v):\n    if v.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\n        return True\n    elif v.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\'Boolean value expected.\')\n\n\ndef getArgs():\n    \'\'\'\n    Function to parse arguments for Bonsai Algorithm\n    \'\'\'\n    parser = argparse.ArgumentParser(\n        description=\'HyperParams for Bonsai Algorithm\')\n    parser.add_argument(\'-dir\', \'--data-dir\', required=True,\n                        help=\'Data directory containing\' +\n                        \'train.npy and test.npy\')\n\n    parser.add_argument(\'-d\', \'--depth\', type=checkIntNneg, default=2,\n                        help=\'Depth of Bonsai Tree \' +\n                        \'(default: 2 try: [0, 1, 3])\')\n    parser.add_argument(\'-p\', \'--proj-dim\', type=checkIntPos, default=10,\n                        help=\'Projection Dimension \' +\n                        \'(default: 20 try: [5, 10, 30])\')\n    parser.add_argument(\'-s\', \'--sigma\', type=float, default=1.0,\n                        help=\'Parameter for sigmoid sharpness \' +\n                        \'(default: 1.0 try: [3.0, 0.05, 0.1]\')\n    parser.add_argument(\'-e\', \'--epochs\', type=checkIntPos, default=42,\n                        help=\'Total Epochs (default: 42 try:[100, 150, 60])\')\n    parser.add_argument(\'-b\', \'--batch-size\', type=checkIntPos,\n                        help=\'Batch Size to be used \' +\n                        \'(default: max(100, sqrt(train_samples)))\')\n    parser.add_argument(\'-lr\', \'--learning-rate\', type=checkFloatPos,\n                        default=0.01, help=\'Initial Learning rate for \' +\n                        \'Adam Optimizer (default: 0.01)\')\n\n    parser.add_argument(\'-rW\', type=float, default=0.0001,\n                        help=\'Regularizer for predictor parameter W  \' +\n                        \'(default: 0.0001 try: [0.01, 0.001, 0.00001])\')\n    parser.add_argument(\'-rV\', type=float, default=0.0001,\n                        help=\'Regularizer for predictor parameter V  \' +\n                        \'(default: 0.0001 try: [0.01, 0.001, 0.00001])\')\n    parser.add_argument(\'-rT\', type=float, default=0.0001,\n                        help=\'Regularizer for branching parameter Theta  \' +\n                        \'(default: 0.0001 try: [0.01, 0.001, 0.00001])\')\n    parser.add_argument(\'-rZ\', type=float, default=0.00001,\n                        help=\'Regularizer for projection parameter Z  \' +\n                        \'(default: 0.00001 try: [0.001, 0.0001, 0.000001])\')\n\n    parser.add_argument(\'-sW\', type=checkFloatPos,\n                        help=\'Sparsity for predictor parameter W  \' +\n                        \'(default: For Binary classification 1.0 else 0.2 \' +\n                        \'try: [0.1, 0.3, 0.5])\')\n    parser.add_argument(\'-sV\', type=checkFloatPos,\n                        help=\'Sparsity for predictor parameter V  \' +\n                        \'(default: For Binary classification 1.0 else 0.2 \' +\n                        \'try: [0.1, 0.3, 0.5])\')\n    parser.add_argument(\'-sT\', type=checkFloatPos,\n                        help=\'Sparsity for branching parameter Theta  \' +\n                        \'(default: For Binary classification 1.0 else 0.2 \' +\n                        \'try: [0.1, 0.3, 0.5])\')\n    parser.add_argument(\'-sZ\', type=checkFloatPos, default=0.2,\n                        help=\'Sparsity for projection parameter Z  \' +\n                        \'(default: 0.2 try: [0.1, 0.3, 0.5])\')\n    parser.add_argument(\'-oF\', \'--output-file\', default=None,\n                        help=\'Output file for dumping the program output, \' +\n                        \'(default: stdout)\')\n\n    return parser.parse_args()\n\n\ndef getQuantArgs():\n    \'\'\'\n    Function to parse arguments for Model Quantisation\n    \'\'\'\n    parser = argparse.ArgumentParser(\n        description=\'Arguments for quantizing Fast models. \' +\n        \'Works only for piece-wise linear non-linearities, \' +\n        \'like relu, quantTanh, quantSigm (check rnn.py for the definitions)\')\n    parser.add_argument(\'-dir\', \'--model-dir\', required=True,\n                        help=\'model directory containing\' +\n                        \'*.npy weight files dumped from the trained model\')\n    parser.add_argument(\'-m\', \'--max-val\', type=checkIntNneg, default=127,\n                        help=\'this represents the maximum possible value \' +\n                        \'in model, essentially the byte complexity, \' +\n                        \'127=> 1 byte is default\')\n\n    return parser.parse_args()\n\n\ndef createTimeStampDir(dataDir):\n    \'\'\'\n    Creates a Directory with timestamp as it\'s name\n    \'\'\'\n    if os.path.isdir(dataDir + \'/PyTorchBonsaiResults\') is False:\n        try:\n            os.mkdir(dataDir + \'/PyTorchBonsaiResults\')\n        except OSError:\n            print(""Creation of the directory %s failed"" %\n                  dataDir + \'/PyTorchBonsaiResults\')\n\n    currDir = \'PyTorchBonsaiResults/\' + \\\n        datetime.datetime.now().strftime(""%H_%M_%S_%d_%m_%y"")\n    if os.path.isdir(dataDir + \'/\' + currDir) is False:\n        try:\n            os.mkdir(dataDir + \'/\' + currDir)\n        except OSError:\n            print(""Creation of the directory %s failed"" %\n                  dataDir + \'/\' + currDir)\n        else:\n            return (dataDir + \'/\' + currDir)\n    return None\n\n\ndef preProcessData(dataDir):\n    \'\'\'\n    Function to pre-process input data\n    Expects a .npy file of form [lbl feats] for each datapoint\n    Outputs a train and test set datapoints appended with 1 for Bias induction\n    dataDimension, numClasses are inferred directly\n    \'\'\'\n    train = np.load(dataDir + \'/train.npy\')\n    test = np.load(dataDir + \'/test.npy\')\n\n    dataDimension = int(train.shape[1]) - 1\n\n    Xtrain = train[:, 1:dataDimension + 1]\n    Ytrain_ = train[:, 0]\n\n    Xtest = test[:, 1:dataDimension + 1]\n    Ytest_ = test[:, 0]\n\n    # Mean Var Normalisation\n    mean = np.mean(Xtrain, 0)\n    std = np.std(Xtrain, 0)\n    std[std[:] < 0.000001] = 1\n    Xtrain = (Xtrain - mean) / std\n    Xtest = (Xtest - mean) / std\n    # End Mean Var normalisation\n\n    # Classification.\n\n    numClasses = max(Ytrain_) - min(Ytrain_) + 1\n    numClasses = int(max(numClasses, max(Ytest_) - min(Ytest_) + 1))\n\n    lab = Ytrain_.astype(\'uint8\')\n    lab = np.array(lab) - min(lab)\n\n    lab_ = np.zeros((Xtrain.shape[0], numClasses))\n    lab_[np.arange(Xtrain.shape[0]), lab] = 1\n    if (numClasses == 2):\n        Ytrain = np.reshape(lab, [-1, 1])\n    else:\n        Ytrain = lab_\n\n    lab = Ytest_.astype(\'uint8\')\n    lab = np.array(lab) - min(lab)\n\n    lab_ = np.zeros((Xtest.shape[0], numClasses))\n    lab_[np.arange(Xtest.shape[0]), lab] = 1\n    if (numClasses == 2):\n        Ytest = np.reshape(lab, [-1, 1])\n    else:\n        Ytest = lab_\n\n    trainBias = np.ones([Xtrain.shape[0], 1])\n    Xtrain = np.append(Xtrain, trainBias, axis=1)\n    testBias = np.ones([Xtest.shape[0], 1])\n    Xtest = np.append(Xtest, testBias, axis=1)\n\n    mean = np.append(mean, np.array([0]))\n    std = np.append(std, np.array([1]))\n\n    return dataDimension + 1, numClasses, Xtrain, Ytrain, Xtest, Ytest, mean, std\n\n\ndef dumpCommand(list, currDir):\n    \'\'\'\n    Dumps the current command to a file for further use\n    \'\'\'\n    commandFile = open(currDir + \'/command.txt\', \'w\')\n    command = ""python""\n\n    command = command + "" "" + \' \'.join(list)\n    commandFile.write(command)\n\n    commandFile.flush()\n    commandFile.close()\n\n\ndef saveMeanStd(mean, std, currDir):\n    \'\'\'\n    Function to save Mean and Std vectors\n    \'\'\'\n    np.save(currDir + \'/mean.npy\', mean)\n    np.save(currDir + \'/std.npy\', std)\n    saveMeanStdSeeDot(mean, std, currDir + ""/SeeDot"")\n\n\ndef saveMeanStdSeeDot(mean, std, seeDotDir):\n    \'\'\'\n    Function to save Mean and Std vectors\n    \'\'\'\n    if os.path.isdir(seeDotDir) is False:\n        try:\n            os.mkdir(seeDotDir)\n        except OSError:\n            print(""Creation of the directory %s failed"" %\n                  seeDotDir)\n    np.savetxt(seeDotDir + \'/Mean\', mean, delimiter=""\\t"")\n    np.savetxt(seeDotDir + \'/Std\', std, delimiter=""\\t"")\n'"
examples/pytorch/Bonsai/process_usps.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n#\n# Processing the USPS Data. It is assumed that the data is already\n# downloaded.\n\nimport subprocess\nimport os\nimport numpy as np\nfrom sklearn.datasets import load_svmlight_file\nimport sys\n\ndef processData(workingDir, downloadDir):\n    def loadLibSVMFile(file):\n        data = load_svmlight_file(file)\n        features = data[0]\n        labels = data[1]\n        retMat = np.zeros([features.shape[0], features.shape[1] + 1])\n        retMat[:, 0] = labels\n        retMat[:, 1:] = features.todense()\n        return retMat\n\n    path = workingDir + \'/\' + downloadDir\n    path = os.path.abspath(path)\n    trf = path + \'/train.txt\'\n    tsf = path + \'/test.txt\'\n    assert os.path.isfile(trf), \'File not found: %s\' % trf\n    assert os.path.isfile(tsf), \'File not found: %s\' % tsf\n    train = loadLibSVMFile(trf)\n    test = loadLibSVMFile(tsf)\n\n    # Convert the labels from 0 to numClasses-1\n    y_train = train[:, 0]\n    y_test = test[:, 0]\n\n    lab = y_train.astype(\'uint8\')\n    lab = np.array(lab) - min(lab)\n    train[:, 0] = lab\n\n    lab = y_test.astype(\'uint8\')\n    lab = np.array(lab) - min(lab)\n    test[:, 0] = lab\n\n    np.save(path + \'/train.npy\', train)\n    np.save(path + \'/test.npy\', test)\n\nif __name__ == \'__main__\':\n    # Configuration\n    workingDir = \'./\'\n    downloadDir = \'usps10\'\n    # End config\n    print(""Processing data"")\n    processData(workingDir, downloadDir)\n    print(""Done"")\n'"
examples/pytorch/Bonsai/quantizeBonsaiModels.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport helpermethods\nimport os\nimport numpy as np\n\n\ndef min_max(A, name):\n    print(name + "" has max: "" + str(np.max(A)) + "" min: "" + str(np.min(A)))\n    return np.max([np.abs(np.max(A)), np.abs(np.min(A))])\n\n\ndef quantizeBonsaiModels(modelDir, maxValue=127, scalarScaleFactor=1000):\n    ls = os.listdir(modelDir)\n    paramNameList = []\n    paramWeightList = []\n    paramLimitList = []\n\n    for file in ls:\n        if file.endswith(""npy""):\n            if file.startswith(""mean"") or file.startswith(""std"") or file.startswith(""hyperParam""):\n                continue\n            else:\n                paramNameList.append(file)\n                temp = np.load(modelDir + ""/"" + file)\n                paramWeightList.append(temp)\n                paramLimitList.append(min_max(temp, file))\n\n    paramLimit = np.max(paramLimitList)\n\n    paramScaleFactor = np.round((2.0 * maxValue + 1.0) / (2.0 * paramLimit))\n\n    quantParamWeights = []\n    for param in paramWeightList:\n        temp = np.round(paramScaleFactor * param)\n        temp[temp[:] > maxValue] = maxValue\n        temp[temp[:] < -maxValue] = -1 * (maxValue + 1)\n\n        if maxValue <= 127:\n            temp = temp.astype(\'int8\')\n        elif maxValue <= 32767:\n            temp = temp.astype(\'int16\')\n        else:\n            temp = temp.astype(\'int32\')\n\n        quantParamWeights.append(temp)\n\n    if os.path.isdir(modelDir + \'/QuantizedPyTorchBonsaiModel\') is False:\n        try:\n            os.mkdir(modelDir + \'/QuantizedPyTorchBonsaiModel\')\n            quantModelDir = modelDir + \'/QuantizedPyTorchBonsaiModel\'\n        except OSError:\n            print(""Creation of the directory %s failed"" %\n                  modelDir + \'/QuantizedPyTorchBonsaiModel\')\n\n    np.save(quantModelDir + ""/paramScaleFactor.npy"",\n            paramScaleFactor.astype(\'int32\'))\n\n    for i in range(len(paramNameList)):\n        np.save(quantModelDir + ""/q"" + paramNameList[i], quantParamWeights[i])\n\n    print(""\\n\\nQuantized Model Dir: "" + quantModelDir)\n\n\ndef main():\n    args = helpermethods.getQuantArgs()\n    quantizeBonsaiModels(args.model_dir, int(args.max_val))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/pytorch/FastCells/fastcell_example.py,8,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport helpermethods\nimport torch\nimport numpy as np\nimport sys\nfrom edgeml_pytorch.graph.rnn import *\nfrom edgeml_pytorch.trainer.fastTrainer import FastTrainer\n\n\ndef main():\n    # change cuda:0 to cuda:gpuid for specific allocation\n    device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n    # Fixing seeds for reproducibility\n    torch.manual_seed(42)\n    np.random.seed(42)\n\n    # Hyper Param pre-processing\n    args = helpermethods.getArgs()\n\n    dataDir = args.data_dir\n    cell = args.cell\n    inputDims = args.input_dim\n    batch_first = args.batch_first\n    hiddenDims = args.hidden_dim\n\n    totalEpochs = args.epochs\n    learningRate = args.learning_rate\n    outFile = args.output_file\n    batchSize = args.batch_size\n    decayStep = args.decay_step\n    decayRate = args.decay_rate\n\n    wRank = args.wRank\n    uRank = args.uRank\n\n    sW = args.sW\n    sU = args.sU\n\n    update_non_linearity = args.update_nl\n    gate_non_linearity = args.gate_nl\n\n    (dataDimension, numClasses, Xtrain, Ytrain, Xtest, Ytest,\n     mean, std) = helpermethods.preProcessData(dataDir)\n\n    assert dataDimension % inputDims == 0, ""Infeasible per step input, "" + \\\n        ""Timesteps have to be integer""\n\n    currDir = helpermethods.createTimeStampDir(dataDir, cell)\n\n    helpermethods.dumpCommand(sys.argv, currDir)\n    helpermethods.saveMeanStd(mean, std, currDir)\n\n    if cell == ""FastGRNN"":\n        FastCell = FastGRNNCell(inputDims, hiddenDims,\n                                gate_nonlinearity=gate_non_linearity,\n                                update_nonlinearity=update_non_linearity,\n                                wRank=wRank, uRank=uRank)\n    elif cell == ""FastRNN"":\n        FastCell = FastRNNCell(inputDims, hiddenDims,\n                               update_nonlinearity=update_non_linearity,\n                               wRank=wRank, uRank=uRank)\n    elif cell == ""UGRNN"":\n        FastCell = UGRNNLRCell(inputDims, hiddenDims,\n                               update_nonlinearity=update_non_linearity,\n                               wRank=wRank, uRank=uRank)\n    elif cell == ""GRU"":\n        FastCell = GRULRCell(inputDims, hiddenDims,\n                             update_nonlinearity=update_non_linearity,\n                             wRank=wRank, uRank=uRank)\n    elif cell == ""LSTM"":\n        FastCell = LSTMLRCell(inputDims, hiddenDims,\n                              update_nonlinearity=update_non_linearity,\n                              wRank=wRank, uRank=uRank)\n    else:\n        sys.exit(\'Exiting: No Such Cell as \' + cell)\n\n    FastCellTrainer = FastTrainer(FastCell, numClasses, sW=sW, sU=sU,\n                                  learningRate=learningRate, outFile=outFile,\n                                  device=device, batch_first=batch_first)\n\n    FastCellTrainer.train(batchSize, totalEpochs,\n                          torch.from_numpy(Xtrain.astype(np.float32)),\n                          torch.from_numpy(Xtest.astype(np.float32)),\n                          torch.from_numpy(Ytrain.astype(np.float32)),\n                          torch.from_numpy(Ytest.astype(np.float32)),\n                          decayStep, decayRate, dataDir, currDir)\n\n\nif __name__ == \'__main__\':\n\n    main()\n'"
examples/pytorch/FastCells/fetch_usps.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n#\n# Setting up the USPS Data.\n\nimport bz2\nimport os\nimport subprocess\nimport sys\n\nimport requests\nimport numpy as np\nfrom sklearn.datasets import load_svmlight_file\nfrom helpermethods import download_file, decompress\n\n\n\ndef downloadData(workingDir, downloadDir, linkTrain, linkTest):\n    path = workingDir + \'/\' + downloadDir\n    path = os.path.abspath(path)\n    try:\n        os.makedirs(path, exist_ok=True)\n    except OSError:\n        print(""Could not create %s. Make sure the path does"" % path)\n        print(""not already exist and you have permissions to create it."")\n        return False\n\n    training_data_bz2 = download_file(linkTrain, path)\n    test_data_bz2 = download_file(linkTest, path)\n\n    training_data = decompress(training_data_bz2)\n    test_data = decompress(test_data_bz2)\n    \n    train = os.path.join(path, ""train.txt"")\n    test = os.path.join(path, ""test.txt"")\n    if os.path.isfile(train):\n        os.remove(train)\n    if os.path.isfile(test):\n        os.remove(test)\n\n    os.rename(training_data, train)\n    os.rename(test_data, test)\n    os.remove(training_data_bz2)\n    os.remove(test_data_bz2)\n    return True\n\nif __name__ == \'__main__\':\n    workingDir = \'./\'\n    downloadDir = \'usps10\'\n    linkTrain = \'http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/usps.bz2\'\n    linkTest = \'http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/usps.t.bz2\'\n    failureMsg = \'\'\'\nDownload Failed!\nTo manually perform the download\n\\t1. Create a new empty directory named `usps10`.\n\\t2. Download the data from the following links into the usps10 directory.\n\\t\\tTest: %s\n\\t\\tTrain: %s\n\\t3. Extract the downloaded files.\n\\t4. Rename `usps` to `train.txt` and,\n\\t5. Rename `usps.t` to `test.txt\n\'\'\' % (linkTrain, linkTest)\n\n    if not downloadData(workingDir, downloadDir, linkTrain, linkTest):\n        exit(failureMsg)\n    print(""Done: see "", downloadDir)\n'"
examples/pytorch/FastCells/helpermethods.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n\'\'\'\n Functions to check sanity of input arguments\n for the example script.\n\'\'\'\nimport argparse\nimport bz2\nimport datetime\nimport json\nimport os\n\nimport numpy as np\nimport requests\n\n\ndef decompress(filepath):\n    print(""extracting: "", filepath)\n    zipfile = bz2.BZ2File(filepath)  # open the file\n    data = zipfile.read()  # get the decompressed data\n    newfilepath = os.path.splitext(filepath)[0]  # assuming the filepath ends with .bz2\n    with open(newfilepath, \'wb\') as f:\n        f.write(data)  # write a uncompressed file\n    return newfilepath\n\n\ndef download_file(url, local_folder=None):\n    """"""Downloads file pointed to by `url`.\n    If `local_folder` is not supplied, downloads to the current folder.\n    """"""\n    filename = os.path.basename(url)\n    if local_folder:\n        filename = os.path.join(local_folder, filename)\n\n    # Download the file\n    print(""Downloading: "" + url)\n    response = requests.get(url, stream=True)\n    if response.status_code != 200:\n        raise Exception(""download file failed with status code: %d, fetching url \'%s\'"" % (response.status_code, url))\n\n    # Write the file to disk\n    with open(filename, ""wb"") as handle:\n        handle.write(response.content)\n    return filename\n\n\ndef checkIntPos(value):\n    ivalue = int(value)\n    if ivalue <= 0:\n        raise argparse.ArgumentTypeError(\n            ""%s is an invalid positive int value"" % value)\n    return ivalue\n\n\ndef checkIntNneg(value):\n    ivalue = int(value)\n    if ivalue < 0:\n        raise argparse.ArgumentTypeError(\n            ""%s is an invalid non-neg int value"" % value)\n    return ivalue\n\n\ndef checkFloatNneg(value):\n    fvalue = float(value)\n    if fvalue < 0:\n        raise argparse.ArgumentTypeError(\n            ""%s is an invalid non-neg float value"" % value)\n    return fvalue\n\n\ndef checkFloatPos(value):\n    fvalue = float(value)\n    if fvalue <= 0:\n        raise argparse.ArgumentTypeError(\n            ""%s is an invalid positive float value"" % value)\n    return fvalue\n\n\ndef getArgs():\n    \'\'\'\n    Function to parse arguments for FastCells\n    \'\'\'\n    parser = argparse.ArgumentParser(\n        description=\'HyperParams for Fast(G)RNN\')\n    parser.add_argument(\'-dir\', \'--data-dir\', required=True,\n                        help=\'Data directory containing\' +\n                        \'train.npy and test.npy\')\n\n    parser.add_argument(\'-c\', \'--cell\', type=str, default=""FastGRNN"",\n                        help=\'Choose between [FastGRNN, FastRNN, UGRNN\' +\n                        \', GRU, LSTM], default: FastGRNN\')\n\n    parser.add_argument(\'-id\', \'--input-dim\', type=checkIntNneg, required=True,\n                        help=\'Input Dimension of RNN, each timestep will \' +\n                        \'feed input-dim features to RNN. \' +\n                        \'Total Feature length = Input Dim * Total Timestep\')\n\n    parser.add_argument(\'-bf\', \'--batch_first\', type=bool, default=False,\n                        help=\'Whether batch is first dim or the second dim\')\n\n    parser.add_argument(\'-hd\', \'--hidden-dim\', type=checkIntNneg,\n                        required=True, help=\'Hidden Dimension of RNN\')\n\n    parser.add_argument(\'-e\', \'--epochs\', type=checkIntPos, default=300,\n                        help=\'Total Epochs (default: 300 try:[100, 150, 600])\')\n    parser.add_argument(\'-b\', \'--batch-size\', type=checkIntPos, default=100,\n                        help=\'Batch Size to be used (default: 100)\')\n    parser.add_argument(\'-lr\', \'--learning-rate\', type=checkFloatPos,\n                        default=0.01, help=\'Initial Learning rate for \' +\n                        \'Adam Optimizer (default: 0.01)\')\n\n    parser.add_argument(\'-rW\', \'--wRank\', type=checkIntPos, default=None,\n                        help=\'Rank for the low-rank parameterisation of W, \' +\n                        \'None => Full Rank\')\n    parser.add_argument(\'-rU\', \'--uRank\', type=checkIntPos, default=None,\n                        help=\'Rank for the low-rank parameterisation of U, \' +\n                        \'None => Full Rank\')\n\n    parser.add_argument(\'-sW\', type=checkFloatPos, default=1.0,\n                        help=\'Sparsity for predictor parameter W(and both \' +\n                        \'W1 and W2 in low-rank)  \' +\n                        \'(default: 1.0(Dense) try: [0.1, 0.2, 0.3])\')\n    parser.add_argument(\'-sU\', type=checkFloatPos, default=1.0,\n                        help=\'Sparsity for predictor parameter U(and both \' +\n                        \'U1 and U2 in low-rank)  \' +\n                        \'(default: 1.0(Dense) try: [0.1, 0.2, 0.3])\')\n\n    parser.add_argument(\'-unl\', \'--update-nl\', type=str, default=""tanh"",\n                        help=\'Update non linearity. Choose between \' +\n                        \'[tanh, sigmoid, relu, quantTanh, quantSigm]. \' +\n                        \'default => tanh. Can add more in edgeml/graph/rnn.py\')\n    parser.add_argument(\'-gnl\', \'--gate-nl\', type=str, default=""sigmoid"",\n                        help=\'Gate non linearity. Choose between \' +\n                        \'[tanh, sigmoid, relu, quantTanh, quantSigm]. \' +\n                        \'default => sigmoid. Can add more in \' +\n                        \'edgeml/graph/rnn.py. Only Applicable to FastGRNN\')\n\n    parser.add_argument(\'-dS\', \'--decay-step\', type=checkIntPos, default=200,\n                        help=\'The interval (in epochs) after which the \' +\n                        \'learning rate should decay. \' +\n                        \'Default is 200 for 300 epochs\')\n\n    parser.add_argument(\'-dR\', \'--decay-rate\', type=checkFloatPos, default=0.1,\n                        help=\'The factor by which learning rate \' +\n                        \'should decay after each interval. Default 0.1\')\n\n    parser.add_argument(\'-oF\', \'--output-file\', default=None,\n                        help=\'Output file for dumping the program output, \' +\n                        \'(default: stdout)\')\n\n    return parser.parse_args()\n\n\ndef getQuantArgs():\n    \'\'\'\n    Function to parse arguments for Model Quantisation\n    \'\'\'\n    parser = argparse.ArgumentParser(\n        description=\'Arguments for quantizing Fast models. \' +\n        \'Works only for piece-wise linear non-linearities, \' +\n        \'like relu, quantTanh, quantSigm (check rnn.py for the definitions)\')\n    parser.add_argument(\'-dir\', \'--model-dir\', required=True,\n                        help=\'model directory containing\' +\n                        \'*.npy weight files dumped from the trained model\')\n    parser.add_argument(\'-m\', \'--max-val\', type=checkIntNneg, default=127,\n                        help=\'this represents the maximum possible value \' +\n                        \'in model, essentially the byte complexity, \' +\n                        \'127=> 1 byte is default\')\n    parser.add_argument(\'-s\', \'--scalar-scale\', type=checkIntNneg,\n                        default=1000, help=\'maximum granularity/decimals \' +\n                        \'you wish to get when quantising simple sclars \' +\n                        \'involved. Default is 1000\')\n\n    return parser.parse_args()\n\n\ndef createTimeStampDir(dataDir, cell):\n    \'\'\'\n    Creates a Directory with timestamp as it\'s name\n    \'\'\'\n    if os.path.isdir(os.path.join(dataDir, str(cell) + \'Results\')) is False:\n        try:\n            os.mkdir(os.path.join(dataDir, str(cell) + \'Results\'))\n        except OSError:\n            print(""Creation of the directory %s failed"" %\n                  os.path.join(dataDir, str(cell) + \'Results\'))\n\n    currDir = os.path.join(str(cell) + \'Results\',\n        datetime.datetime.now().strftime(""%Y-%m-%dT%H-%M-%S""))\n    if os.path.isdir(os.path.join(dataDir, currDir)) is False:\n        try:\n            os.mkdir(os.path.join(dataDir, currDir))\n        except OSError:\n            print(""Creation of the directory %s failed"" %\n                  os.path.join(dataDir, currDir))\n        else:\n            return (os.path.join(dataDir, currDir))\n    return None\n\n\ndef preProcessData(dataDir):\n    \'\'\'\n    Function to pre-process input data\n\n    Expects a .npy file of form [lbl feats] for each datapoint,\n    feats is timesteps*inputDims, flattened across timestep dimension.\n    So input of 1st timestep followed by second and so on.\n\n    Outputs train and test set datapoints\n    dataDimension, numClasses are inferred directly\n    \'\'\'\n    train = np.load(os.path.join(dataDir, \'train.npy\'))\n    test = np.load(os.path.join(dataDir, \'test.npy\'))\n\n    dataDimension = int(train.shape[1]) - 1\n\n    Xtrain = train[:, 1:dataDimension + 1]\n    Ytrain_ = train[:, 0]\n    numClasses = max(Ytrain_) - min(Ytrain_) + 1\n\n    Xtest = test[:, 1:dataDimension + 1]\n    Ytest_ = test[:, 0]\n\n    numClasses = int(max(numClasses, max(Ytest_) - min(Ytest_) + 1))\n\n    # Mean Var Normalisation\n    mean = np.mean(Xtrain, 0)\n    std = np.std(Xtrain, 0)\n    std[std[:] < 0.000001] = 1\n    Xtrain = (Xtrain - mean) / std\n\n    Xtest = (Xtest - mean) / std\n    # End Mean Var normalisation\n\n    lab = Ytrain_.astype(\'uint8\')\n    lab = np.array(lab) - min(lab)\n\n    lab_ = np.zeros((Xtrain.shape[0], numClasses))\n    lab_[np.arange(Xtrain.shape[0]), lab] = 1\n    Ytrain = lab_\n\n    lab = Ytest_.astype(\'uint8\')\n    lab = np.array(lab) - min(lab)\n\n    lab_ = np.zeros((Xtest.shape[0], numClasses))\n    lab_[np.arange(Xtest.shape[0]), lab] = 1\n    Ytest = lab_\n\n    return dataDimension, numClasses, Xtrain, Ytrain, Xtest, Ytest, mean, std\n\n\ndef dumpCommand(list, currDir):\n    \'\'\'\n    Dumps the current command to a file for further use\n    \'\'\'\n    commandFile = open(os.path.join(currDir, \'command.txt\'), \'w\')\n    command = ""python""\n\n    command = command + "" "" + \' \'.join(list)\n    commandFile.write(command)\n\n    commandFile.flush()\n    commandFile.close()\n\n\ndef saveMeanStd(mean, std, currDir):\n    \'\'\'\n    Function to save Mean and Std vectors\n    \'\'\'\n    np.save(os.path.join(currDir, \'mean.npy\'), mean)\n    np.save(os.path.join(currDir, \'std.npy\'), std)\n\n\ndef saveJSon(data, filename):\n    with open(filename, ""w"") as outfile:\n        json.dump(data, outfile, indent=2)\n'"
examples/pytorch/FastCells/process_usps.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n#\n# Processing the USPS Data. It is assumed that the data is already\n# downloaded.\n\nimport subprocess\nimport os\nimport numpy as np\nfrom sklearn.datasets import load_svmlight_file\nimport sys\n\ndef processData(workingDir, downloadDir):\n    def loadLibSVMFile(file):\n        data = load_svmlight_file(file)\n        features = data[0]\n        labels = data[1]\n        retMat = np.zeros([features.shape[0], features.shape[1] + 1])\n        retMat[:, 0] = labels\n        retMat[:, 1:] = features.todense()\n        return retMat\n\n    path = workingDir + \'/\' + downloadDir\n    path = os.path.abspath(path)\n    trf = path + \'/train.txt\'\n    tsf = path + \'/test.txt\'\n    assert os.path.isfile(trf), \'File not found: %s\' % trf\n    assert os.path.isfile(tsf), \'File not found: %s\' % tsf\n    train = loadLibSVMFile(trf)\n    test = loadLibSVMFile(tsf)\n    np.save(path + \'/train.npy\', train)\n    np.save(path + \'/test.npy\', test)\n\nif __name__ == \'__main__\':\n    # Configuration\n    workingDir = \'./\'\n    downloadDir = \'usps10\'\n    # End config\n    print(""Processing data"")\n    processData(workingDir, downloadDir)\n    print(""Done"")\n'"
examples/pytorch/FastCells/quantizeFastModels.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport helpermethods\nimport os\nimport numpy as np\n\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n\ndef min_max(A, name):\n    print(name + "" has max: "" + str(np.max(A)) + "" min: "" + str(np.min(A)))\n    return np.max([np.abs(np.max(A)), np.abs(np.min(A))])\n\n\ndef quantizeFastModels(modelDir, maxValue=127, scalarScaleFactor=1000):\n    ls = os.listdir(modelDir)\n    paramNameList = []\n    paramWeightList = []\n    paramLimitList = []\n\n    classifierNameList = []\n    classifierWeightList = []\n    classifierLimitList = []\n\n    scalarNameList = []\n    scalarWeightList = []\n\n    for file in ls:\n        if file.endswith(""npy""):\n            if file.startswith(""W""):\n                paramNameList.append(file)\n                temp = np.load(os.path.join(modelDir, file))\n                paramWeightList.append(temp)\n                paramLimitList.append(min_max(temp, file))\n            elif file.startswith(""U""):\n                paramNameList.append(file)\n                temp = np.load(os.path.join(modelDir, file))\n                paramWeightList.append(temp)\n                paramLimitList.append(min_max(temp, file))\n            elif file.startswith(""B""):\n                paramNameList.append(file)\n                temp = np.load(os.path.join(modelDir, file))\n                paramWeightList.append(temp)\n                paramLimitList.append(min_max(temp, file))\n            elif file.startswith(""FC""):\n                classifierNameList.append(file)\n                temp = np.load(os.path.join(modelDir, file))\n                classifierWeightList.append(temp)\n                classifierLimitList.append(min_max(temp, file))\n            elif file.startswith(""mean"") or file.startswith(""std""):\n                continue\n            else:\n                scalarNameList.append(file)\n                scalarWeightList.append(np.load(os.path.join(modelDir, file)))\n\n    paramLimit = np.max(paramLimitList)\n    classifierLimit = np.max(classifierLimitList)\n\n    paramScaleFactor = np.round((2.0 * maxValue + 1.0) / (2.0 * paramLimit))\n    classifierScaleFactor = (2.0 * maxValue + 1.0) / (2.0 * classifierLimit)\n\n    quantParamWeights = []\n    for param in paramWeightList:\n        temp = np.round(paramScaleFactor * param)\n        temp[temp[:] > maxValue] = maxValue\n        temp[temp[:] < -maxValue] = -1 * (maxValue + 1)\n\n        if maxValue <= 127:\n            temp = temp.astype(\'int8\')\n        elif maxValue <= 32767:\n            temp = temp.astype(\'int16\')\n        else:\n            temp = temp.astype(\'int32\')\n\n        quantParamWeights.append(temp)\n\n    quantClassifierWeights = []\n    for param in classifierWeightList:\n        temp = np.round(classifierScaleFactor * param)\n        temp[temp[:] > maxValue] = maxValue\n        temp[temp[:] < -maxValue] = -1 * (maxValue + 1)\n\n        if maxValue <= 127:\n            temp = temp.astype(\'int8\')\n        elif maxValue <= 32767:\n            temp = temp.astype(\'int16\')\n        else:\n            temp = temp.astype(\'int32\')\n\n        quantClassifierWeights.append(temp)\n\n    quantScalarWeights = []\n    for scalar in scalarWeightList:\n        quantScalarWeights.append(\n            np.round(scalarScaleFactor * sigmoid(scalar)).astype(\'int32\'))\n\n    quantModelDir = os.path.join(modelDir, \'QuantizedModel\')\n    if not os.path.isdir(quantModelDir):\n        try:\n            os.makedirs(quantModelDir, exist_ok=True)\n        except OSError:\n            print(""Creation of the directory %s failed"" % quantModelDir)\n\n    np.save(os.path.join(quantModelDir, ""paramScaleFactor.npy""),\n            paramScaleFactor.astype(\'int32\'))\n    np.save(os.path.join(quantModelDir, ""classifierScaleFactor.npy""),\n            classifierScaleFactor)\n    np.save(os.path.join(quantModelDir, ""scalarScaleFactor""), scalarScaleFactor)\n\n    for i in range(0, len(scalarNameList)):\n        np.save(os.path.join(quantModelDir, ""q"" +\n                scalarNameList[i]), quantScalarWeights[i])\n\n    for i in range(len(classifierNameList)):\n        np.save(os.path.join(quantModelDir, ""q"" +\n                classifierNameList[i]), quantClassifierWeights[i])\n\n    for i in range(len(paramNameList)):\n        np.save(os.path.join(quantModelDir, ""q"" + paramNameList[i]),\n                quantParamWeights[i])\n\n    print(""\\n\\nQuantized Model Dir: "" + quantModelDir)\n\n\ndef main():\n    args = helpermethods.getQuantArgs()\n    quantizeFastModels(args.model_dir, int(\n        args.max_val), int(args.scalar_scale))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/pytorch/ProtoNN/fetch_usps.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n#\n# Setting up the USPS Data.\n\nimport subprocess\nimport os\nimport numpy as np\nfrom sklearn.datasets import load_svmlight_file\nimport sys\n\ndef downloadData(workingDir, downloadDir, linkTrain, linkTest):\n    def runcommand(command):\n        p = subprocess.Popen(command.split(), stdout=subprocess.PIPE)\n        output, error = p.communicate()\n        assert(p.returncode == 0), \'Command failed: %s\' % command\n\n    path = workingDir + \'/\' + downloadDir\n    path = os.path.abspath(path)\n    try:\n        os.mkdir(path)\n    except OSError:\n        print(""Could not create %s. Make sure the path does"" % path)\n        print(""not already exist and you have permisions to create it."")\n        return False\n    cwd = os.getcwd()\n    os.chdir(path)\n    print(""Downloading data"")\n    command = \'wget %s\' % linkTrain\n    runcommand(command)\n    command = \'wget %s\' % linkTest\n    runcommand(command)\n    print(""Extracting data"")\n    command = \'bzip2 -d usps.bz2\'\n    runcommand(command)\n    command = \'bzip2 -d usps.t.bz2\'\n    runcommand(command)\n    command = \'mv usps train.txt\'\n    runcommand(command)\n    command = \'mv usps.t test.txt\'\n    runcommand(command)\n    os.chdir(cwd)\n    return True\n\nif __name__ == \'__main__\':\n    workingDir = \'./\'\n    downloadDir = \'usps10\'\n    linkTrain = \'http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/usps.bz2\'\n    linkTest = \'http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/usps.t.bz2\'\n    failureMsg = \'\'\'\nDownload Failed!\nTo manually perform the download\n\\t1. Create a new empty directory named `usps10`.\n\\t2. Download the data from the following links into the usps10 directory.\n\\t\\tTest: %s\n\\t\\tTrain: %s\n\\t3. Extract the downloaded files.\n\\t4. Rename `usps` to `train.txt` and,\n\\t5. Rename `usps.t` to `test.txt\n\'\'\' % (linkTrain, linkTest)\n\n    if not downloadData(workingDir, downloadDir, linkTrain, linkTest):\n        exit(failureMsg)\n    print(""Done"")\n'"
examples/pytorch/ProtoNN/helpermethods.py,1,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nfrom __future__ import print_function\nimport sys\nimport os\nimport numpy as np\nimport edgeml_pytorch.utils as utils\nimport argparse\n\n\ndef getModelSize(matrixList, sparcityList, expected=True, bytesPerVar=4):\n    \'\'\'\n    expected: Expected size according to the parameters set. The number of\n        zeros could actually be more than that is required to satisfy the\n        sparsity constraint.\n    \'\'\'\n    nnzList, sizeList, isSparseList = [], [], []\n    hasSparse = False\n    for i in range(len(matrixList)):\n        A, s = matrixList[i], sparcityList[i]\n        assert A.ndim == 2\n        assert s >= 0\n        assert s <= 1\n        nnz, size, sparse = utils.estimateNNZ(A, s, bytesPerVar=bytesPerVar)\n        nnzList.append(nnz)\n        sizeList.append(size)\n        hasSparse = (hasSparse or sparse)\n\n    totalnnZ = np.sum(nnzList)\n    totalSize = np.sum(sizeList)\n    if expected:\n        return totalnnZ, totalSize, hasSparse\n    numNonZero = 0\n    totalSize = 0\n    hasSparse = False\n    for i in range(len(matrixList)):\n        A, s = matrixList[i], sparcityList[i]\n        numNonZero_ = np.count_nonzero(A)\n        numNonZero += numNonZero_\n        hasSparse = (hasSparse or (s < 0.5))\n        if s <= 0.5:\n            totalSize += numNonZero_ * 2 * bytesPerVar\n        else:\n            totalSize += A.size * bytesPerVar\n    return numNonZero, totalSize, hasSparse\n\n\ndef getGamma(gammaInit, projectionDim, dataDim, numPrototypes, x_train):\n    if gammaInit is None:\n        print(""Using median heuristic to estimate gamma."")\n        gamma, W, B = utils.medianHeuristic(x_train, projectionDim,\n                                            numPrototypes)\n        print(""Gamma estimate is: %f"" % gamma)\n        return W, B, gamma\n    return None, None, gammaInit\n\ndef to_onehot(y, numClasses, minlabel = None):\n    \'\'\'\n    If the y labelling does not contain the minimum label info, use min-label to\n    provide this value.\n    \'\'\'\n    lab = y.astype(\'uint8\')\n    if minlabel is None:\n        minlabel = np.min(lab)\n    minlabel = int(minlabel)\n    lab = np.array(lab) - minlabel\n    lab_ = np.zeros((y.shape[0], numClasses))\n    lab_[np.arange(y.shape[0]), lab] = 1\n    return lab_\n\ndef preprocessData(train, test):\n    \'\'\'\n    Loads data from the dataDir and does some initial preprocessing\n    steps. Data is assumed to be contained in two files,\n    train.npy and test.npy. Each containing a 2D numpy array of dimension\n    [numberOfExamples, numberOfFeatures + 1]. The first column of each\n    matrix is assumed to contain label information.\n\n    For an N-Class problem, we assume the labels are integers from 0 through\n    N-1.\n    \'\'\'\n    dataDimension = int(train.shape[1]) - 1\n    x_train = train[:, 1:dataDimension + 1]\n    y_train_ = train[:, 0]\n    x_test = test[:, 1:dataDimension + 1]\n    y_test_ = test[:, 0]\n\n    numClasses = max(y_train_) - min(y_train_) + 1\n    numClasses = max(numClasses, max(y_test_) - min(y_test_) + 1)\n    numClasses = int(numClasses)\n\n    # mean-var\n    mean = np.mean(x_train, 0)\n    std = np.std(x_train, 0)\n    std[std[:] < 0.000001] = 1\n    x_train = (x_train - mean) / std\n    x_test = (x_test - mean) / std\n\n    # one hot y-train\n    lab = y_train_.astype(\'uint8\')\n    lab = np.array(lab) - min(lab)\n    lab_ = np.zeros((x_train.shape[0], numClasses))\n    lab_[np.arange(x_train.shape[0]), lab] = 1\n    y_train = lab_\n\n    # one hot y-test\n    lab = y_test_.astype(\'uint8\')\n    lab = np.array(lab) - min(lab)\n    lab_ = np.zeros((x_test.shape[0], numClasses))\n    lab_[np.arange(x_test.shape[0]), lab] = 1\n    y_test = lab_\n\n    return dataDimension, numClasses, x_train, y_train, x_test, y_test\n\n\n\ndef getProtoNNArgs():\n    def checkIntPos(value):\n        ivalue = int(value)\n        if ivalue <= 0:\n            raise argparse.ArgumentTypeError(\n                ""%s is an invalid positive int value"" % value)\n        return ivalue\n\n    def checkIntNneg(value):\n        ivalue = int(value)\n        if ivalue < 0:\n            raise argparse.ArgumentTypeError(\n                ""%s is an invalid non-neg int value"" % value)\n        return ivalue\n\n    def checkFloatNneg(value):\n        fvalue = float(value)\n        if fvalue < 0:\n            raise argparse.ArgumentTypeError(\n                ""%s is an invalid non-neg float value"" % value)\n        return fvalue\n\n    def checkFloatPos(value):\n        fvalue = float(value)\n        if fvalue <= 0:\n            raise argparse.ArgumentTypeError(\n                ""%s is an invalid positive float value"" % value)\n        return fvalue\n\n    \'\'\'\n    Parse protoNN commandline arguments\n    \'\'\'\n    parser = argparse.ArgumentParser(\n        description=\'Hyperparameters for ProtoNN Algorithm\')\n\n    msg = \'Data directory containing train and test data. The \'\n    msg += \'data is assumed to be saved as 2-D numpy matrices with \'\n    msg += \'names `train.npy` and `test.npy`, of dimensions\\n\'\n    msg += \'\\t[numberOfInstances, numberOfFeatures + 1].\\n\'\n    msg += \'The first column of each file is assumed to contain label information.\'\n    msg += \' For a N-class problem, labels are assumed to be integers from 0 to\'\n    msg += \' N-1 (inclusive).\'\n    parser.add_argument(\'-d\', \'--data-dir\', required=True, help=msg)\n    parser.add_argument(\'-l\', \'--projection-dim\', type=checkIntPos, default=10,\n                        help=\'Projection Dimension.\')\n    parser.add_argument(\'-p\', \'--num-prototypes\', type=checkIntPos, default=20,\n                        help=\'Number of prototypes.\')\n    parser.add_argument(\'-g\', \'--gamma\', type=checkFloatPos, default=None,\n                        help=\'Gamma for Gaussian kernel. If not provided, \' +\n                        \'median heuristic will be used to estimate gamma.\')\n\n    parser.add_argument(\'-e\', \'--epochs\', type=checkIntPos, default=100,\n                        help=\'Total training epochs.\')\n    parser.add_argument(\'-b\', \'--batch-size\', type=checkIntPos, default=32,\n                        help=\'Batch size for each pass.\')\n    parser.add_argument(\'-r\', \'--learning-rate\', type=checkFloatPos,\n                        default=0.001,\n                        help=\'Initial Learning rate for ADAM Optimizer.\')\n\n    parser.add_argument(\'-rW\', type=float, default=0.000,\n                        help=\'Coefficient for l2 regularizer for predictor\' +\n                        \' parameter W \' + \'(default = 0.0).\')\n    parser.add_argument(\'-rB\', type=float, default=0.00,\n                        help=\'Coefficient for l2 regularizer for predictor\' +\n                        \' parameter B \' + \'(default = 0.0).\')\n    parser.add_argument(\'-rZ\', type=float, default=0.00,\n                        help=\'Coefficient for l2 regularizer for predictor\' +\n                        \'parameter Z \' +\n                        \'(default = 0.0).\')\n\n    parser.add_argument(\'-sW\', type=float, default=1.000,\n                        help=\'Sparsity constraint for predictor parameter W \' +\n                        \'(default = 1.0, i.e. dense matrix).\')\n    parser.add_argument(\'-sB\', type=float, default=1.00,\n                        help=\'Sparsity constraint for predictor parameter B \' +\n                        \'(default = 1.0, i.e. dense matrix).\')\n    parser.add_argument(\'-sZ\', type=float, default=1.00,\n                        help=\'Sparsity constraint for predictor parameter Z \' +\n                        \'(default = 1.0, i.e. dense matrix).\')\n    parser.add_argument(\'-pS\', \'--print-step\', type=int, default=200,\n                        help=\'The number of update steps between print \' +\n                        \'calls to console.\')\n    parser.add_argument(\'-vS\', \'--val-step\', type=int, default=3,\n                        help=\'The number of epochs between validation\' +\n                        \'performance evaluation\')\n    parser.add_argument(\'-o\', \'--output-dir\', type=str, default=\'./\',\n                        help=\'Output directory to dump model matrices.\')\n    return parser.parse_args()\n'"
examples/pytorch/ProtoNN/process_usps.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n#\n# Processing the USPS Data. It is assumed that the data is already\n# downloaded.\n\nimport subprocess\nimport os\nimport numpy as np\nfrom sklearn.datasets import load_svmlight_file\nimport sys\nfrom helpermethods import preprocessData\n\ndef processData(workingDir, downloadDir):\n    def loadLibSVMFile(file):\n        data = load_svmlight_file(file)\n        features = data[0]\n        labels = data[1]\n        retMat = np.zeros([features.shape[0], features.shape[1] + 1])\n        retMat[:, 0] = labels\n        retMat[:, 1:] = features.todense()\n        return retMat\n\n    path = workingDir + \'/\' + downloadDir\n    path = os.path.abspath(path)\n    trf = path + \'/train.txt\'\n    tsf = path + \'/test.txt\'\n    assert os.path.isfile(trf), \'File not found: %s\' % trf\n    assert os.path.isfile(tsf), \'File not found: %s\' % tsf\n    train = loadLibSVMFile(trf)\n    test = loadLibSVMFile(tsf)\n    np.save(path + \'/train_unnormalized.npy\', train)\n    np.save(path + \'/test_unnormalized.npy\', test)\n    _, _, x_train, y_train, x_test, y_test = preprocessData(train, test)\n\n    y_ = np.expand_dims(np.argmax(y_train, axis=1), axis=1)\n    train = np.concatenate([y_, x_train], axis=1)\n    np.save(path + \'/train.npy\', train)\n    y_ = np.expand_dims(np.argmax(y_test, axis=1), axis=1)\n    test = np.concatenate([y_, x_test], axis=1)\n    np.save(path + \'/test.npy\', test)\n\n\nif __name__ == \'__main__\':\n    # Configuration\n    workingDir = \'./\'\n    downloadDir = \'usps10\'\n    # End config\n    print(""Processing data"")\n    processData(workingDir, downloadDir)\n    print(""Done"")\n'"
examples/pytorch/ProtoNN/protoNN_example.py,7,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nfrom __future__ import print_function\nimport sys\nimport os\nimport numpy as np\nfrom edgeml_pytorch.trainer.protoNNTrainer import ProtoNNTrainer\nfrom edgeml_pytorch.graph.protoNN import ProtoNN\nimport edgeml_pytorch.utils as utils\nimport helpermethods as helper\nimport torch\n\ndef main():\n    # change cuda:0 to cuda:gpu_id for using particular gpu\n    device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n    config = helper.getProtoNNArgs()\n    # Get hyper parameters\n    DATA_DIR = config.data_dir\n    PROJECTION_DIM = config.projection_dim\n    NUM_PROTOTYPES = config.num_prototypes\n    REG_W = config.rW\n    REG_B = config.rB\n    REG_Z = config.rZ\n    SPAR_W = config.sW\n    SPAR_B = config.sB\n    SPAR_Z = config.sZ\n    LEARNING_RATE = config.learning_rate\n    NUM_EPOCHS = config.epochs\n    BATCH_SIZE = config.batch_size\n    PRINT_STEP = config.print_step\n    VAL_STEP = config.val_step\n    OUT_DIR = config.output_dir\n\n    # Load data\n    train = np.load(DATA_DIR + \'/train.npy\')\n    test = np.load(DATA_DIR + \'/test.npy\')\n    x_train, y_train = train[:, 1:], train[:, 0]\n    x_test, y_test = test[:, 1:], test[:, 0]\n    # Convert y to one-hot\n    minval = min(min(y_train), min(y_test))\n    numClasses = max(y_train) - min(y_train) + 1\n    numClasses = max(numClasses, max(y_test) - min(y_test) + 1)\n    numClasses = int(numClasses)\n    y_train = helper.to_onehot(y_train, numClasses, minlabel=minval)\n    y_test = helper.to_onehot(y_test, numClasses, minlabel=minval)\n    dataDimension = x_train.shape[1]\n\n    W, B, gamma = helper.getGamma(config.gamma, PROJECTION_DIM, dataDimension,\n                                  NUM_PROTOTYPES, x_train)\n\n    # Setup input and train protoNN\n    protoNN = ProtoNN(dataDimension, PROJECTION_DIM,\n                      NUM_PROTOTYPES, numClasses,\n                      gamma, W=W, B=B).to(device)\n\n    trainer = ProtoNNTrainer(protoNN, REG_W, REG_B, REG_Z,\n                             SPAR_W, SPAR_B, SPAR_Z,\n                             LEARNING_RATE, lossType=\'xentropy\', device=device)\n    # Train the protoNN object\n    trainer.train(BATCH_SIZE, NUM_EPOCHS, x_train, x_test,\n                  y_train, y_test, printStep=PRINT_STEP, valStep=VAL_STEP)\n\n    # Print some summary metrics\n    x_, y_= (torch.Tensor(x_test)).to(device), (torch.Tensor(y_test)).to(device)\n\n    logits = protoNN.forward(x_)\n    _, predictions = torch.max(logits, dim=1)\n    _, target = torch.max(y_, dim=1)\n    acc, count = trainer.accuracy(predictions, target)\n    #Model needs to be on cpu for numpy operations below\n    protoNN = protoNN.cpu()\n    W, B, Z, gamma  = protoNN.getModelMatrices()\n    matrixList = [W, B, Z]\n    matrixList = [x.detach().numpy() for x in matrixList]\n    sparcityList = [SPAR_W, SPAR_B, SPAR_Z]\n    nnz, size, sparse = helper.getModelSize(matrixList, sparcityList)\n    print(""Final test accuracy"", acc)\n    print(""Model size constraint (Bytes): "", size)\n    print(""Number of non-zeros: "", nnz)\n    nnz, size, sparse = helper.getModelSize(matrixList, sparcityList,\n                                            expected=False)\n    print(""Actual model size: "", size)\n    print(""Actual non-zeros: "", nnz)\n    print(""Saving model matrices to: "", OUT_DIR)\n    np.save(OUT_DIR + \'/W.npy\', matrixList[0])\n    np.save(OUT_DIR + \'/B.npy\', matrixList[1])\n    np.save(OUT_DIR + \'/Z.npy\', matrixList[2])\n    np.save(OUT_DIR + \'/gamma.npy\', gamma)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/pytorch/SRNN/SRNN_Example.py,5,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nfrom __future__ import print_function\nimport sys\nimport os\nimport numpy as np\nimport torch\n\nfrom edgeml_pytorch.graph.rnn import SRNN2\nfrom edgeml_pytorch.trainer.srnnTrainer import SRNNTrainer\nimport edgeml_pytorch.utils as utils\nimport helpermethods as helper\n\nconfig = helper.getSRNN2Args()\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\nDATA_DIR = config.data_dir\nx_train_ = np.load(DATA_DIR + \'x_train.npy\')\ny_train = np.load(DATA_DIR + \'y_train.npy\')\nx_val_ = np.load(DATA_DIR + \'x_val.npy\')\ny_val = np.load(DATA_DIR + \'y_val.npy\')\nx_test_ = np.load(DATA_DIR + \'x_test.npy\')\ny_test = np.load(DATA_DIR + \'y_test.npy\')\n\n# Mean-var normalize\nmean = np.mean(np.reshape(x_train_, [-1, x_train_.shape[-1]]), axis=0)\nstd = np.std(np.reshape(x_train_, [-1, x_train_.shape[-1]]), axis=0)\nstd[std[:] < 0.000001] = 1\nx_train_ = (x_train_ - mean) / std\nx_val_ = (x_val_ - mean) / std\nx_test_ = (x_test_ - mean) / std\nnp.save(\'mean.npy\', mean)\nnp.save(\'std.npy\', std)\n\nx_train = np.swapaxes(x_train_, 0, 1)\nx_val = np.swapaxes(x_val_, 0, 1)\nx_test = np.swapaxes(x_test_, 0, 1)\nprint(""Train shape"", x_train.shape, y_train.shape)\nprint(""Val shape"", x_val.shape, y_val.shape)\nprint(""Test shape"", x_test.shape, y_test.shape)\n\nnumTimeSteps = x_train.shape[0]\nnumInput = x_train.shape[-1]\nnumClasses = y_train.shape[1]\n\nhiddenDim0 = config.hidden_dim0\nbrickSize = config.brick_size\nhiddenDim1 = config.hidden_dim1\ncellType = config.cell_type\nlearningRate = config.learning_rate\nbatchSize = config.batch_size\nepochs = config.epochs\nprintStep = config.print_step\nvalStep = config.val_step\ndropoutProbability_l0 = 0.2\ndropoutProbability_l1 = 0.2\n\nprint(cellType)\n\'\'\'\ncellArgs (optional) will be passed to the respective cell\n\nExample OPTIONAL args for FastGRNNCell\ncellArgs = {\'gate_non_linearity\':""sigmoid"",\'update_non_linearity\':""tanh"",\n\t\t\t\t\'wRank\':None, \'uRank\':None,\'zetaInit\':1.0, \'nuInit\':-4.0, \n\t\t\t\t\'batch_first\':False}\n\n\'\'\'\ncellArgs = {}\n\nsrnn2 = SRNN2(numInput, numClasses, hiddenDim0, hiddenDim1, cellType,\n\t\t\t dropoutProbability_l0, dropoutProbability_l1,\n\t\t\t **cellArgs).to(device)  \ntrainer = SRNNTrainer(srnn2, learningRate, lossType=\'xentropy\', device=device)\n\ntrainer.train(brickSize, batchSize, epochs, x_train, x_val, y_train, y_val,\n              printStep=printStep, valStep=valStep)\n\nprint(\'Saving trained model:\')\ntorch.save(srnn2.state_dict(), \'model_srnn.pt\')\n'"
examples/pytorch/SRNN/helpermethods.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport argparse\n\ndef getSRNN2Args():\n    def checkIntPos(value):\n        ivalue = int(value)\n        if ivalue <= 0:\n            raise argparse.ArgumentTypeError(\n                ""%s is an invalid positive int value"" % value)\n        return ivalue\n\n    def checkIntNneg(value):\n        ivalue = int(value)\n        if ivalue < 0:\n            raise argparse.ArgumentTypeError(\n                ""%s is an invalid non-neg int value"" % value)\n        return ivalue\n\n    def checkFloatNneg(value):\n        fvalue = float(value)\n        if fvalue < 0:\n            raise argparse.ArgumentTypeError(\n                ""%s is an invalid non-neg float value"" % value)\n        return fvalue\n\n    def checkFloatPos(value):\n        fvalue = float(value)\n        if fvalue <= 0:\n            raise argparse.ArgumentTypeError(\n                ""%s is an invalid positive float value"" % value)\n        return fvalue\n\n    parser = argparse.ArgumentParser(\n        description=\'Hyperparameters for 2 layer SRNN Algorithm\')\n\n    parser.add_argument(\'-d\', \'--data-dir\', required=True,\n                        help=\'Directory containing processed data.\')\n    parser.add_argument(\'-h0\', \'--hidden-dim0\', type=checkIntPos, default=64,\n                        help=\'Hidden dimension of lower layer RNN cell.\')\n    parser.add_argument(\'-h1\', \'--hidden-dim1\', type=checkIntPos, default=32,\n                        help=\'Hidden dimension of upper layer RNN cell.\')\n    parser.add_argument(\'-bz\', \'--brick-size\', type=checkIntPos, required=True,\n                        help=\'Brick size to be used at the lower layer.\')\n    parser.add_argument(\'-c\', \'--cell-type\', default=\'LSTM\',\n                        help=\'Type of RNN cell to use among [LSTM, FastRNN, \' +\n                        \'FastGRNN\')\n\n    parser.add_argument(\'-p\', \'--num-prototypes\', type=checkIntPos, default=20,\n                        help=\'Number of prototypes.\')\n    parser.add_argument(\'-g\', \'--gamma\', type=checkFloatPos, default=None,\n                        help=\'Gamma for Gaussian kernel. If not provided, \' +\n                        \'median heuristic will be used to estimate gamma.\')\n\n    parser.add_argument(\'-e\', \'--epochs\', type=checkIntPos, default=10,\n                        help=\'Total training epochs.\')\n    parser.add_argument(\'-b\', \'--batch-size\', type=checkIntPos, default=128,\n                        help=\'Batch size for each pass.\')\n    parser.add_argument(\'-r\', \'--learning-rate\', type=checkFloatPos,\n                        default=0.01,\n                        help=\'Learning rate for ADAM Optimizer.\')\n\n    parser.add_argument(\'-pS\', \'--print-step\', type=int, default=200,\n                        help=\'The number of update steps between print \' +\n                        \'calls to console.\')\n    parser.add_argument(\'-vS\', \'--val-step\', type=int, default=5,\n                        help=\'The number of epochs between validation\' +\n                        \'performance evaluation\')\n    return parser.parse_args()\n'"
examples/pytorch/SRNN/process_google.py,0,"b'\n# Google Speech data feature extraction\n\n# Note that the \'testing_list.txt\' and \'validation_list.txt\'\n# that provided is used to create test and validation\n# sets. Everything that is not in these sets is considered\n# for training.\n\n# The testing_list and validation_list and by extension\n# the training set has the following property.\n\n#     If one audio sample of a user is in either one of these\n#     sets, then all audio samples of that user will also be\n#     in that set.\n\n#     As long as the same methodology of creating testing\n#     and validation set that google used - as outlined in\n#     their README is used, the testing and validation set\n#     will be consistent. That is, the will always contain\n#     the same set of examples\n\n# Sampling is not supported yet.\n\nfrom python_speech_features import fbank\nimport os\nimport glob\nimport numpy as np\nimport scipy.io.wavfile as r\nimport random\n\n\n# Various version can be created depending on which labels are chosen and which\n# are moved to the negative (noise) set. We use LABELMAP13 for most of our\n# experiments.\nLABELMAP30 = {\n    \'_background_noise_\': 1, \'bed\': 2, \'bird\': 3,\n    \'cat\': 4, \'dog\': 5, \'down\': 6, \'eight\': 7,\n    \'five\': 8, \'four\': 9, \'go\': 10, \'happy\': 11,\n    \'house\': 12, \'left\': 13, \'marvin\': 14, \'nine\': 15,\n    \'no\': 16, \'off\': 17, \'on\': 18, \'one\': 19,\n    \'right\': 20, \'seven\': 21, \'sheila\': 22, \'six\': 23,\n    \'stop\': 24, \'three\': 25, \'tree\': 26, \'two\': 27,\n    \'up\': 28, \'wow\': 29, \'yes\': 30, \'zero\': 31\n}\n\n\nLABELMAP13 = {\n    \'go\': 1, \'no\': 2, \'on\': 3, \'up\': 4, \'bed\': 5, \'cat\': 6,\n    \'dog\': 7, \'off\': 8, \'one\': 9, \'six\': 10, \'two\': 11,\n    \'yes\': 12,\n    \'wow\': 0, \'bird\': 0, \'down\': 0, \'five\': 0, \'four\': 0,\n    \'left\': 0, \'nine\': 0, \'stop\': 0, \'tree\': 0, \'zero\': 0,\n    \'eight\': 0, \'happy\': 0, \'house\': 0, \'right\': 0, \'seven\': 0,\n    \'three\': 0, \'marvin\': 0, \'sheila\': 0, \'_background_noise_\': 0\n}\n\nLABELMAP12 = {\n    \'yes\': 1, \'no\': 2, \'up\': 3, \'down\': 4, \'left\': 5, \'right\': 6,\n    \'on\': 7, \'off\': 8, \'stop\': 9, \'go\': 10,\n    \'bed\':0, \'cat\':0, \'dog\':0, \'one\':0, \'six\':0, \'two\':0,\n    \'wow\':0, \'bird\':0, \'five\':0, \'four\':0, \'nine\':0, \'tree\':0,\n    \'zero\':0, \'eight\':0, \'happy\':0, \'house\':0, \'seven\':0, \'three\':0,\n    \'marvin\':0, \'sheila\':0, \'_background_noise_\':0\n}\n\ndef createFileList(audioFileDir, testingList,\n                   validationList, outPrefix,\n                   labelMap):\n    \'\'\'\n    audioFileDir: The directory containing the directories\n        with audio files.\n    testingList: the `testing_list.txt` file\n    validationList: the `validation_list.txt` file\n\n    Reads all the files in audioFileDir and creates\n    a list of files that are not part of testingList\n    or validationList.\n\n    WARNING: _background_noise_ is ignored\n\n    Then testingList, validationList and trainginList\n    are converted into numpy arrays with their labels\n\n    This is written as\n        outPrefix + \'_testList.npy\'\n        outPrefix + \'_trainList.npy\'\n        outPrefix + \'_validationList.npy\'\n    \'\'\'\n    dirs = os.listdir(audioFileDir)\n    dirs = [x for x in dirs if os.path.isdir(os.path.join(audioFileDir, x))]\n    assert(len(dirs) == 31), (len(dirs))\n    for x in dirs:\n        msg = \'%s found without label map\' % x\n        assert x in labelMap, msg\n\n    allFileList = []\n    for fol in dirs:\n        if fol == \'_background_noise_\':\n            print(""Ignoring %s"" % fol)\n            continue\n        path = audioFileDir + \'/\' + fol + \'/\'\n        files = []\n        for w in os.listdir(path):\n            if not w.endswith(\'.wav\'):\n                print(""Ignoring %s"" % w)\n                continue\n            files.append(fol + \'/\' + w)\n        allFileList.extend(files)\n    assert(len(allFileList) == len(set(allFileList)))\n\n    fil = open(testingList, \'r\')\n    testingList = fil.readlines()\n    testingList = [x.strip() for x in testingList]\n    fil.close()\n    fil = open(validationList, \'r\')\n    validationList = fil.readlines()\n    validationList = [x.strip() for x in validationList]\n    originalLen = len(allFileList)\n    allFileList = set(allFileList) - set(validationList)\n    assert len(allFileList) < originalLen\n    assert originalLen == len(allFileList) + len(validationList)\n    originalLen = len(allFileList)\n    allFileList = set(allFileList) - set(testingList)\n    assert len(allFileList) < originalLen\n    assert originalLen == len(allFileList) + len(testingList)\n\n    trainingList = list(allFileList)\n    testingList = list(testingList)\n    validationList = list(validationList)\n    np.save(outPrefix + \'file_train.npy\', trainingList)\n    np.save(outPrefix + \'file_test.npy\', testingList)\n    np.save(outPrefix + \'file_val.npy\', validationList)\n\n\ndef extractFeatures(fileList, LABELMAP, maxlen, numFilt, samplerate, winlen,\n                    winstep):\n    \'\'\'\n    Reads audio from files specified in fileList, extracts features and assigns\n    labels to them.\n\n    fileList: List of audio file names.\n    LABELMAP: The label map to use.\n    maxlen: maximum length of the audio file. Every other\n        files is zero padded to maxlen\n    numFilt: number of filters to use in MFCC\n    samplerate: sample rate of the audio file. All files are\n        assumed to be of same sample rate\n    winLen: winLen to use for fbank in seconds\n    winstep: winstep for fbank in seconds\n    \'\'\'\n    def __extractFeatures(stackedWav, numSteps, numFilt,\n                          samplerate, winlen, winstep):\n        \'\'\'\n        [number of waves, Len(wave)]\n        returns [number of waves, numSteps, numFilt]\n        All waves are assumed to be of fixed length\n        \'\'\'\n        assert stackedWav.ndim == 2, \'Should be [number of waves, len(wav)]\'\n        extractedList = []\n        eps = 1e-10\n        for sample in stackedWav:\n            temp, _ = fbank(sample, samplerate=samplerate, winlen=winlen,\n                            winstep=winstep, nfilt=numFilt,\n                            winfunc=np.hamming)\n            temp = np.log(temp + eps)\n            assert temp.ndim == 2, \'Should be [numSteps, numFilt]\'\n            assert temp.shape[0] == numSteps, \'Should be [numSteps, numFilt]\'\n            extractedList.append(temp)\n        return np.array(extractedList)\n\n    fileList = np.array(fileList)\n    assert(fileList.ndim == 1)\n    allSamples = np.zeros((len(fileList), maxlen))\n    i = 0\n    for i,file in enumerate(fileList):\n        _, x = r.read(file)\n        assert(len(x) <= maxlen)\n        allSamples[i, maxlen - len(x):maxlen] += x\n        i += 1\n    assert allSamples.ndim == 2\n    winstepSamples = winstep * samplerate\n    winlenSamples = winlen * samplerate\n    assert(winstepSamples.is_integer())\n    assert(winlenSamples.is_integer())\n    numSteps = int(np.ceil((maxlen - winlenSamples)/winstepSamples) + 1)\n    x = __extractFeatures(allSamples, numSteps, numFilt, samplerate, winlen,\n                          winstep)\n    y_ = [t.split(\'/\') for t in fileList]\n    y_ = [t[-2] for t in y_]\n    y = []\n    for t in y_:\n        assert t in LABELMAP\n        y.append(LABELMAP[t])\n\n    def to_onehot(indices, numClasses):\n        assert indices.ndim == 1\n        n = max(indices) + 1\n        assert numClasses <= n\n        b = np.zeros((len(indices), numClasses))\n        b[np.arange(len(indices)), indices] = 1\n        return b\n    y = to_onehot(np.array(y), np.max(y) + 1)\n    return x, y\n\nif __name__==\'__main__\':\n    # ----------------------------------------- #\n    # Configuration\n    # ----------------------------------------- #\n    seed = 42\n    maxlen = 16000\n    numFilt = 32\n    samplerate = 16000\n    winlen = 0.025\n    winstep = 0.010\n    # 13 for google 13, 11 for google 12\n    numLabels = 13 # 0 not assigned\n    samplerate=16000\n    # For creation of training file list, testing file list\n    # and validation list. \n    audioFileDir = \'./GoogleSpeech/Raw/\'\n    testingList = \'./GoogleSpeech/Raw/testing_list.txt\'\n    validationList = \'./GoogleSpeech/Raw/validation_list.txt\'\n    outDir = \'./GoogleSpeech/Extracted/\'\n    # ----------------------------------------- #\n    np.random.seed(seed)\n    random.seed(seed)\n    assert(numLabels in [13, 11])\n    if numLabels == 13:\n        values = [LABELMAP13[x] for x in LABELMAP13]\n        values = set(values)\n        assert(len(values) == 13)\n        LABELMAP = LABELMAP13\n    if numLabels == 11:\n        values = [LABELMAP12[x] for x in LABELMAP12]\n        values = set(values)\n        assert(len(values) == 11)\n        LABELMAP = LABELMAP12\n\n    print(""Peforming file creation"")\n    createFileList(audioFileDir, testingList, validationList,\n                   outDir, LABELMAP)\n    trainFileList = np.load(outDir + \'file_train.npy\')\n    testFileList = np.load(outDir + \'file_test.npy\')\n    valFileList = np.load(outDir + \'file_val.npy\')\n    print(""Number of train files:"", len(trainFileList))\n    print(""Number of test files"", len(testFileList))\n    print(""Number of val files"", len(valFileList))\n    print(""Performing feature extraction"")\n    trainFileList_ = [audioFileDir + x for x in trainFileList]\n    valFileList_ = [audioFileDir + x for x in valFileList]\n    testFileList_ = [audioFileDir + x for x in testFileList]\n    x_test, y_test = extractFeatures(testFileList_, LABELMAP, maxlen, numFilt,\n                                     samplerate, winlen, winstep)\n    x_val, y_val = extractFeatures(valFileList_, LABELMAP, maxlen, numFilt,\n                                   samplerate, winlen, winstep)\n    x_train, y_train = extractFeatures(trainFileList_, LABELMAP, maxlen,\n                                       numFilt, samplerate, winlen, winstep)\n    np.save(outDir + \'x_train\', x_train);np.save(outDir + \'y_train\', y_train)\n    np.save(outDir + \'x_test\', x_test);np.save(outDir + \'y_test\', y_test)\n    np.save(outDir + \'x_val\', x_val);np.save(outDir + \'y_val\', y_val)\n    print(""Shape train"", x_train.shape, y_train.shape)\n    print(""Shape test"", x_test.shape, y_test.shape)\n    print(""Shape val"", x_val.shape, y_val.shape)\n\n\n'"
examples/tf/Bonsai/bonsai_example.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport helpermethods\nimport tensorflow as tf\nimport numpy as np\nimport sys\nfrom edgeml_tf.trainer.bonsaiTrainer import BonsaiTrainer\nfrom edgeml_tf.graph.bonsai import Bonsai\n\n\ndef main():\n    # Fixing seeds for reproducibility\n    tf.set_random_seed(42)\n    np.random.seed(42)\n\n    # Hyper Param pre-processing\n    args = helpermethods.getArgs()\n\n    # Set \'isRegression\' to be True, for regression. Default is \'False\'.\n    isRegression = args.regression\n\n    sigma = args.sigma\n    depth = args.depth\n\n    projectionDimension = args.proj_dim\n    regZ = args.rZ\n    regT = args.rT\n    regW = args.rW\n    regV = args.rV\n\n    totalEpochs = args.epochs\n\n    learningRate = args.learning_rate\n\n    dataDir = args.data_dir\n\n    outFile = args.output_file\n\n    (dataDimension, numClasses, Xtrain, Ytrain, Xtest, Ytest,\n     mean, std) = helpermethods.preProcessData(dataDir, isRegression)\n\n    sparZ = args.sZ\n\n    if numClasses > 2:\n        sparW = 0.2\n        sparV = 0.2\n        sparT = 0.2\n    else:\n        sparW = 1\n        sparV = 1\n        sparT = 1\n\n    if args.sW is not None:\n        sparW = args.sW\n    if args.sV is not None:\n        sparV = args.sV\n    if args.sT is not None:\n        sparT = args.sT\n\n    if args.batch_size is None:\n        batchSize = np.maximum(100, int(np.ceil(np.sqrt(Ytrain.shape[0]))))\n    else:\n        batchSize = args.batch_size\n\n    useMCHLoss = True\n\n    if numClasses == 2:\n        numClasses = 1\n\n    X = tf.placeholder(""float32"", [None, dataDimension])\n    Y = tf.placeholder(""float32"", [None, numClasses])\n\n    currDir = helpermethods.createTimeStampDir(dataDir)\n\n    helpermethods.dumpCommand(sys.argv, currDir)\n    helpermethods.saveMeanStd(mean, std, currDir)\n\n    # numClasses = 1 for binary case\n    bonsaiObj = Bonsai(numClasses, dataDimension,\n                       projectionDimension, depth, sigma, isRegression)\n\n    bonsaiTrainer = BonsaiTrainer(bonsaiObj,\n                                  regW, regT, regV, regZ,\n                                  sparW, sparT, sparV, sparZ,\n                                  learningRate, X, Y, useMCHLoss, outFile)\n\n    sess = tf.InteractiveSession()\n\n    sess.run(tf.global_variables_initializer())\n\n    bonsaiTrainer.train(batchSize, totalEpochs, sess,\n                        Xtrain, Xtest, Ytrain, Ytest, dataDir, currDir)\n\n    sess.close()\n    sys.stdout.close()\n\n\nif __name__ == \'__main__\':\n    main()\n\n'"
examples/tf/Bonsai/fetch_usps.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n#\n# Setting up the USPS Data.\n\nimport subprocess\nimport os\nimport numpy as np\nfrom sklearn.datasets import load_svmlight_file\nimport sys\n\ndef downloadData(workingDir, downloadDir, linkTrain, linkTest):\n    def runcommand(command):\n        p = subprocess.Popen(command.split(), stdout=subprocess.PIPE)\n        output, error = p.communicate()\n        assert(p.returncode == 0), \'Command failed: %s\' % command\n\n    path = workingDir + \'/\' + downloadDir\n    path = os.path.abspath(path)\n    try:\n        os.mkdir(path)\n    except OSError:\n        print(""Could not create %s. Make sure the path does"" % path)\n        print(""not already exist and you have permisions to create it."")\n        return False\n    cwd = os.getcwd()\n    os.chdir(path)\n    print(""Downloading data"")\n    command = \'wget %s\' % linkTrain\n    runcommand(command)\n    command = \'wget %s\' % linkTest\n    runcommand(command)\n    print(""Extracting data"")\n    command = \'bzip2 -d usps.bz2\'\n    runcommand(command)\n    command = \'bzip2 -d usps.t.bz2\'\n    runcommand(command)\n    command = \'mv usps train.txt\'\n    runcommand(command)\n    command = \'mv usps.t test.txt\'\n    runcommand(command)\n    os.chdir(cwd)\n    return True\n\nif __name__ == \'__main__\':\n    workingDir = \'./\'\n    downloadDir = \'usps10\'\n    linkTrain = \'http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/usps.bz2\'\n    linkTest = \'http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/usps.t.bz2\'\n    failureMsg = \'\'\'\nDownload Failed!\nTo manually perform the download\n\\t1. Create a new empty directory named `usps10`.\n\\t2. Download the data from the following links into the usps10 directory.\n\\t\\tTest: %s\n\\t\\tTrain: %s\n\\t3. Extract the downloaded files.\n\\t4. Rename `usps` to `train.txt` and,\n\\t5. Rename `usps.t` to `test.txt\n\'\'\' % (linkTrain, linkTest)\n\n    if not downloadData(workingDir, downloadDir, linkTrain, linkTest):\n        exit(failureMsg)\n    print(""Done"")\n'"
examples/tf/Bonsai/helpermethods.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n\'\'\'\n Functions to check sanity of input arguments\n for the example script.\n\'\'\'\nimport argparse\nimport datetime\nimport os\nimport numpy as np\n\n\ndef checkIntPos(value):\n    ivalue = int(value)\n    if ivalue <= 0:\n        raise argparse.ArgumentTypeError(\n            ""%s is an invalid positive int value"" % value)\n    return ivalue\n\n\ndef checkIntNneg(value):\n    ivalue = int(value)\n    if ivalue < 0:\n        raise argparse.ArgumentTypeError(\n            ""%s is an invalid non-neg int value"" % value)\n    return ivalue\n\n\ndef checkFloatNneg(value):\n    fvalue = float(value)\n    if fvalue < 0:\n        raise argparse.ArgumentTypeError(\n            ""%s is an invalid non-neg float value"" % value)\n    return fvalue\n\n\ndef checkFloatPos(value):\n    fvalue = float(value)\n    if fvalue <= 0:\n        raise argparse.ArgumentTypeError(\n            ""%s is an invalid positive float value"" % value)\n    return fvalue\n\n\ndef str2bool(v):\n    if v.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\n        return True\n    elif v.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\n        return False\n    else:\n        raise argparse.ArgumentTypeError(\'Boolean value expected.\')\n\n\ndef getArgs():\n    \'\'\'\n    Function to parse arguments for Bonsai Algorithm\n    \'\'\'\n    parser = argparse.ArgumentParser(\n        description=\'HyperParams for Bonsai Algorithm\')\n    parser.add_argument(\'-dir\', \'--data-dir\', required=True,\n                        help=\'Data directory containing\' +\n                        \'train.npy and test.npy\')\n\n    parser.add_argument(\'-d\', \'--depth\', type=checkIntNneg, default=2,\n                        help=\'Depth of Bonsai Tree \' +\n                        \'(default: 2 try: [0, 1, 3])\')\n    parser.add_argument(\'-p\', \'--proj-dim\', type=checkIntPos, default=10,\n                        help=\'Projection Dimension \' +\n                        \'(default: 20 try: [5, 10, 30])\')\n    parser.add_argument(\'-s\', \'--sigma\', type=float, default=1.0,\n                        help=\'Parameter for sigmoid sharpness \' +\n                        \'(default: 1.0 try: [3.0, 0.05, 0.1]\')\n    parser.add_argument(\'-e\', \'--epochs\', type=checkIntPos, default=42,\n                        help=\'Total Epochs (default: 42 try:[100, 150, 60])\')\n    parser.add_argument(\'-b\', \'--batch-size\', type=checkIntPos,\n                        help=\'Batch Size to be used \' +\n                        \'(default: max(100, sqrt(train_samples)))\')\n    parser.add_argument(\'-lr\', \'--learning-rate\', type=checkFloatPos,\n                        default=0.01, help=\'Initial Learning rate for \' +\n                        \'Adam Optimizer (default: 0.01)\')\n\n    parser.add_argument(\'-rW\', type=float, default=0.0001,\n                        help=\'Regularizer for predictor parameter W  \' +\n                        \'(default: 0.0001 try: [0.01, 0.001, 0.00001])\')\n    parser.add_argument(\'-rV\', type=float, default=0.0001,\n                        help=\'Regularizer for predictor parameter V  \' +\n                        \'(default: 0.0001 try: [0.01, 0.001, 0.00001])\')\n    parser.add_argument(\'-rT\', type=float, default=0.0001,\n                        help=\'Regularizer for branching parameter Theta  \' +\n                        \'(default: 0.0001 try: [0.01, 0.001, 0.00001])\')\n    parser.add_argument(\'-rZ\', type=float, default=0.00001,\n                        help=\'Regularizer for projection parameter Z  \' +\n                        \'(default: 0.00001 try: [0.001, 0.0001, 0.000001])\')\n\n    parser.add_argument(\'-sW\', type=checkFloatPos,\n                        help=\'Sparsity for predictor parameter W  \' +\n                        \'(default: For Binary classification 1.0 else 0.2 \' +\n                        \'try: [0.1, 0.3, 0.5])\')\n    parser.add_argument(\'-sV\', type=checkFloatPos,\n                        help=\'Sparsity for predictor parameter V  \' +\n                        \'(default: For Binary classification 1.0 else 0.2 \' +\n                        \'try: [0.1, 0.3, 0.5])\')\n    parser.add_argument(\'-sT\', type=checkFloatPos,\n                        help=\'Sparsity for branching parameter Theta  \' +\n                        \'(default: For Binary classification 1.0 else 0.2 \' +\n                        \'try: [0.1, 0.3, 0.5])\')\n    parser.add_argument(\'-sZ\', type=checkFloatPos, default=0.2,\n                        help=\'Sparsity for projection parameter Z  \' +\n                        \'(default: 0.2 try: [0.1, 0.3, 0.5])\')\n    parser.add_argument(\'-oF\', \'--output-file\', default=None,\n                        help=\'Output file for dumping the program output, \' +\n                        \'(default: stdout)\')\n\n    parser.add_argument(\'-regression\', type=str2bool, default=False,\n                        help=\'boolean argument which controls whether to perform \' +\n                        \'regression or classification.\' +\n                        \'default : False (Classification) values: [True, False]\')\n\n    return parser.parse_args()\n\n\ndef getQuantArgs():\n    \'\'\'\n    Function to parse arguments for Model Quantisation\n    \'\'\'\n    parser = argparse.ArgumentParser(\n        description=\'Arguments for quantizing Fast models. \' +\n        \'Works only for piece-wise linear non-linearities, \' +\n        \'like relu, quantTanh, quantSigm (check rnn.py for the definitions)\')\n    parser.add_argument(\'-dir\', \'--model-dir\', required=True,\n                        help=\'model directory containing\' +\n                        \'*.npy weight files dumped from the trained model\')\n    parser.add_argument(\'-m\', \'--max-val\', type=checkIntNneg, default=127,\n                        help=\'this represents the maximum possible value \' +\n                        \'in model, essentially the byte complexity, \' +\n                        \'127=> 1 byte is default\')\n\n    return parser.parse_args()\n\n\ndef createTimeStampDir(dataDir):\n    \'\'\'\n    Creates a Directory with timestamp as it\'s name\n    \'\'\'\n    if os.path.isdir(dataDir + \'/TFBonsaiResults\') is False:\n        try:\n            os.mkdir(dataDir + \'/TFBonsaiResults\')\n        except OSError:\n            print(""Creation of the directory %s failed"" %\n                  dataDir + \'/TFBonsaiResults\')\n\n    currDir = \'TFBonsaiResults/\' + datetime.datetime.now().strftime(""%H_%M_%S_%d_%m_%y"")\n    if os.path.isdir(dataDir + \'/\' + currDir) is False:\n        try:\n            os.mkdir(dataDir + \'/\' + currDir)\n        except OSError:\n            print(""Creation of the directory %s failed"" %\n                  dataDir + \'/\' + currDir)\n        else:\n            return (dataDir + \'/\' + currDir)\n    return None\n\n\ndef preProcessData(dataDir, isRegression=False):\n    \'\'\'\n    Function to pre-process input data\n    Expects a .npy file of form [lbl feats] for each datapoint\n    Outputs a train and test set datapoints appended with 1 for Bias induction\n    dataDimension, numClasses are inferred directly\n    \'\'\'\n    train = np.load(dataDir + \'/train.npy\')\n    test = np.load(dataDir + \'/test.npy\')\n\n    dataDimension = int(train.shape[1]) - 1\n\n    Xtrain = train[:, 1:dataDimension + 1]\n    Ytrain_ = train[:, 0]\n\n    Xtest = test[:, 1:dataDimension + 1]\n    Ytest_ = test[:, 0]\n\n    # Mean Var Normalisation\n    mean = np.mean(Xtrain, 0)\n    std = np.std(Xtrain, 0)\n    std[std[:] < 0.000001] = 1\n    Xtrain = (Xtrain - mean) / std\n    Xtest = (Xtest - mean) / std\n    # End Mean Var normalisation\n\n    # Classification.\n    if (isRegression == False):\n        numClasses = max(Ytrain_) - min(Ytrain_) + 1\n        numClasses = int(max(numClasses, max(Ytest_) - min(Ytest_) + 1))\n\n        lab = Ytrain_.astype(\'uint8\')\n        lab = np.array(lab) - min(lab)\n\n        lab_ = np.zeros((Xtrain.shape[0], numClasses))\n        lab_[np.arange(Xtrain.shape[0]), lab] = 1\n        if (numClasses == 2):\n            Ytrain = np.reshape(lab, [-1, 1])\n        else:\n            Ytrain = lab_\n\n        lab = Ytest_.astype(\'uint8\')\n        lab = np.array(lab) - min(lab)\n\n        lab_ = np.zeros((Xtest.shape[0], numClasses))\n        lab_[np.arange(Xtest.shape[0]), lab] = 1\n        if (numClasses == 2):\n            Ytest = np.reshape(lab, [-1, 1])\n        else:\n            Ytest = lab_\n\n    elif (isRegression == True):\n        # The number of classes is always 1, for regression.\n        numClasses = 1\n        Ytrain = Ytrain_\n        Ytest = Ytest_\n\n    trainBias = np.ones([Xtrain.shape[0], 1])\n    Xtrain = np.append(Xtrain, trainBias, axis=1)\n    testBias = np.ones([Xtest.shape[0], 1])\n    Xtest = np.append(Xtest, testBias, axis=1)\n\n    mean = np.append(mean, np.array([0]))\n    std = np.append(std, np.array([1]))\n\n    if (isRegression == False):\n        return dataDimension + 1, numClasses, Xtrain, Ytrain, Xtest, Ytest, mean, std\n    elif (isRegression == True):\n        return dataDimension + 1, numClasses, Xtrain, Ytrain.reshape((-1, 1)), Xtest, Ytest.reshape((-1, 1)), mean, std\n\n\ndef dumpCommand(list, currDir):\n    \'\'\'\n    Dumps the current command to a file for further use\n    \'\'\'\n    commandFile = open(currDir + \'/command.txt\', \'w\')\n    command = ""python""\n\n    command = command + "" "" + \' \'.join(list)\n    commandFile.write(command)\n\n    commandFile.flush()\n    commandFile.close()\n\n\ndef saveMeanStd(mean, std, currDir):\n    \'\'\'\n    Function to save Mean and Std vectors\n    \'\'\'\n    np.save(currDir + \'/mean.npy\', mean)\n    np.save(currDir + \'/std.npy\', std)\n    saveMeanStdSeeDot(mean, std, currDir + ""/SeeDot"")\n\n\ndef saveMeanStdSeeDot(mean, std, seeDotDir):\n    \'\'\'\n    Function to save Mean and Std vectors\n    \'\'\'\n    if os.path.isdir(seeDotDir) is False:\n        try:\n            os.mkdir(seeDotDir)\n        except OSError:\n            print(""Creation of the directory %s failed"" %\n                  seeDotDir)\n    np.savetxt(seeDotDir + \'/Mean\', mean, delimiter=""\\t"")\n    np.savetxt(seeDotDir + \'/Std\', std, delimiter=""\\t"")\n'"
examples/tf/Bonsai/process_usps.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n#\n# Processing the USPS Data. It is assumed that the data is already\n# downloaded.\n\nimport subprocess\nimport os\nimport numpy as np\nfrom sklearn.datasets import load_svmlight_file\nimport sys\n\ndef processData(workingDir, downloadDir):\n    def loadLibSVMFile(file):\n        data = load_svmlight_file(file)\n        features = data[0]\n        labels = data[1]\n        retMat = np.zeros([features.shape[0], features.shape[1] + 1])\n        retMat[:, 0] = labels\n        retMat[:, 1:] = features.todense()\n        return retMat\n\n    path = workingDir + \'/\' + downloadDir\n    path = os.path.abspath(path)\n    trf = path + \'/train.txt\'\n    tsf = path + \'/test.txt\'\n    assert os.path.isfile(trf), \'File not found: %s\' % trf\n    assert os.path.isfile(tsf), \'File not found: %s\' % tsf\n    train = loadLibSVMFile(trf)\n    test = loadLibSVMFile(tsf)\n\n    # Convert the labels from 0 to numClasses-1\n    y_train = train[:, 0]\n    y_test = test[:, 0]\n\n    lab = y_train.astype(\'uint8\')\n    lab = np.array(lab) - min(lab)\n    train[:, 0] = lab\n\n    lab = y_test.astype(\'uint8\')\n    lab = np.array(lab) - min(lab)\n    test[:, 0] = lab\n\n    np.save(path + \'/train.npy\', train)\n    np.save(path + \'/test.npy\', test)\n\nif __name__ == \'__main__\':\n    # Configuration\n    workingDir = \'./\'\n    downloadDir = \'usps10\'\n    # End config\n    print(""Processing data"")\n    processData(workingDir, downloadDir)\n    print(""Done"")\n'"
examples/tf/Bonsai/quantizeBonsaiModels.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport helpermethods\nimport os\nimport numpy as np\n\n\ndef min_max(A, name):\n    print(name + "" has max: "" + str(np.max(A)) + "" min: "" + str(np.min(A)))\n    return np.max([np.abs(np.max(A)), np.abs(np.min(A))])\n\n\ndef quantizeBonsaiModels(modelDir, maxValue=127, scalarScaleFactor=1000):\n    ls = os.listdir(modelDir)\n    paramNameList = []\n    paramWeightList = []\n    paramLimitList = []\n\n    for file in ls:\n        if file.endswith(""npy""):\n            if file.startswith(""mean"") or file.startswith(""std"") or file.startswith(""hyperParam""):\n                continue\n            else:\n                paramNameList.append(file)\n                temp = np.load(modelDir + ""/"" + file)\n                paramWeightList.append(temp)\n                paramLimitList.append(min_max(temp, file))\n\n    paramLimit = np.max(paramLimitList)\n\n    paramScaleFactor = np.round((2.0 * maxValue + 1.0) / (2.0 * paramLimit))\n\n    quantParamWeights = []\n    for param in paramWeightList:\n        temp = np.round(paramScaleFactor * param)\n        temp[temp[:] > maxValue] = maxValue\n        temp[temp[:] < -maxValue] = -1 * (maxValue + 1)\n\n        if maxValue <= 127:\n            temp = temp.astype(\'int8\')\n        elif maxValue <= 32767:\n            temp = temp.astype(\'int16\')\n        else:\n            temp = temp.astype(\'int32\')\n\n        quantParamWeights.append(temp)\n\n    if os.path.isdir(modelDir + \'/QuantizedTFBonsaiModel\') is False:\n        try:\n            os.mkdir(modelDir + \'/QuantizedTFBonsaiModel\')\n            quantModelDir = modelDir + \'/QuantizedTFBonsaiModel\'\n        except OSError:\n            print(""Creation of the directory %s failed"" %\n                  modelDir + \'/QuantizedTFBonsaiModel\')\n\n    np.save(quantModelDir + ""/paramScaleFactor.npy"",\n            paramScaleFactor.astype(\'int32\'))\n\n    for i in range(len(paramNameList)):\n        np.save(quantModelDir + ""/q"" + paramNameList[i], quantParamWeights[i])\n\n    print(""\\n\\nQuantized Model Dir: "" + quantModelDir)\n\n\ndef main():\n    args = helpermethods.getQuantArgs()\n    quantizeBonsaiModels(args.model_dir, int(args.max_val))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/tf/EMI-RNN/fetch_har.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n# \n# Script to fetch the HAR data\n\nimport subprocess\nimport os\nimport numpy as np\nimport sys\nfrom helpermethods import *\n\n\ndef downloadData(workingDir, downloadDir):\n    def runcommand(command, splitChar=\' \'):\n        p = subprocess.Popen(command.split(splitChar), stdout=subprocess.PIPE)\n        output, error = p.communicate()\n        assert(p.returncode == 0), \'Command failed: %s\' % command\n\n    path = workingDir + \'/\' + downloadDir\n    path = os.path.abspath(path)\n    try:\n        os.mkdir(path)\n    except OSError:\n        print(""Could not create %s. Make sure the path does"" % path)\n        print(""not already exist and you have permisions to create it."")\n        return False\n    cwd = os.getcwd()\n    os.chdir(path)\n    print(""Downloading data"")\n    command = \'wget %s\' % linkData\n    runcommand(command)\n    print(""Extracting data"")\n    print(os.getcwd())\n    runcommand(\'ls\')\n    command = ""unzip#UCI HAR Dataset.zip""\n    runcommand(command, splitChar=\'#\')\n    os.chdir(cwd)\n    return True\n\n\nif __name__ == \'__main__\':\n    workingDir = \'./\'\n    downloadDir = \'HAR\'\n    linkData = \'https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip\'\n    exitMessage =\'\'\'\nDownload Failed!\n\nTo manually download the HAR data,\n\\t1. Download the compressed folder with the data from:\n\\t\\t%s\n\\t2. Extract the downloaded files into a directory named `HAR`\n\'\'\' % linkData\n    if not downloadData(workingDir, downloadDir):\n        exit(\'Download failed\')\n    print(""Done"")\n'"
examples/tf/EMI-RNN/helpermethods.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n# \n# Helper methods to generate training, validation and test splits from the UCI HAR dataset. \n# Each split consists of a separate set of users.\n# Reference : https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition\n\nimport numpy as np\nimport os\n\ndef generateIndicesForSplits(path=\'./HAR/UCI HAR Dataset/train/subject_train.txt\'):\n    f = open(path)\n    subjects = []\n    for line in f:\n        subject = line.strip().split()\n        subjects.append(int(subject[0]))\n    subjects = np.array(subjects)\n\n    # get unique subjects\n    numSubjects = np.unique(subjects)\n\n    # shuffle amongst train subjects so that difficult/easy subjects spread in both val and train\n    np.random.shuffle(numSubjects)\n\n    l = len(numSubjects)\n\n    splitRatio = 0.1\n    valSplit = int(l * splitRatio + 1)\n\n    valSubjects = numSubjects[:valSplit]\n    trainSubjects = numSubjects[valSplit:]\n\n    trainSubjectIndices = []\n    valSubjectIndices = []\n\n    for i, subject in enumerate(subjects):\n        if subject in trainSubjects:\n            trainSubjectIndices.append(i)\n        elif subject in valSubjects:\n            valSubjectIndices.append(i)\n        else:\n            raise Exception(""some bug in your code"")\n\n    # assert that train/val different\n    for x in trainSubjectIndices:\n        assert x not in valSubjectIndices\n\n    trainSubjectIndices = np.array(trainSubjectIndices)\n    valSubjectIndices = np.array(valSubjectIndices)\n\n    # shuffle more, so that readings not grouped by a subject\n    # therefore, no need to shuffle after slicing from read dataset, as we are shuffling here\n    idx = np.arange(len(trainSubjectIndices))\n    np.random.shuffle(idx)\n    trainSubjectIndices = trainSubjectIndices[idx]\n\n    idx = np.arange(len(valSubjectIndices))\n    np.random.shuffle(idx)\n    valSubjectIndices = valSubjectIndices[idx]\n\n    assert len(trainSubjectIndices) + len(valSubjectIndices) == len(subjects)\n\n    return trainSubjectIndices, valSubjectIndices\n\ndef readData(extractedDir):\n    INPUT_SIGNAL_TYPES = [\n    ""body_acc_x_"",\n    ""body_acc_y_"",\n    ""body_acc_z_"",\n    ""body_gyro_x_"",\n    ""body_gyro_y_"",\n    ""body_gyro_z_"",\n    ""total_acc_x_"",\n    ""total_acc_y_"",\n    ""total_acc_z_""\n    ]\n\n    # Output classes to learn how to classify\n    LABELS = [\n        ""WALKING"", \n        ""WALKING_UPSTAIRS"", \n        ""WALKING_DOWNSTAIRS"", \n        ""SITTING"", \n        ""STANDING"", \n        ""LAYING""\n    ] \n    DATASET_PATH = extractedDir + ""/UCI HAR Dataset/""\n    TRAIN = ""train/""\n    TEST = ""test/""\n    # Load ""X"" (the neural network\'s training and testing inputs)\n\n    def load_X(X_signals_paths):\n        X_signals = []\n        \n        for signal_type_path in X_signals_paths:\n            file = open(signal_type_path, \'r\')\n            # Read dataset from disk, dealing with text files\' syntax\n            X_signals.append(\n                [np.array(serie, dtype=np.float32) for serie in [\n                    row.replace(\'  \', \' \').strip().split(\' \') for row in file\n                ]]\n            )\n            file.close()\n        \n        return np.transpose(np.array(X_signals), (1, 2, 0))\n\n    X_train_signals_paths = [\n        DATASET_PATH + TRAIN + ""Inertial Signals/"" + signal + ""train.txt"" for signal in INPUT_SIGNAL_TYPES\n    ]\n    X_test_signals_paths = [\n        DATASET_PATH + TEST + ""Inertial Signals/"" + signal + ""test.txt"" for signal in INPUT_SIGNAL_TYPES\n    ]\n\n    x_train_val_combined = load_X(X_train_signals_paths)\n    x_test = load_X(X_test_signals_paths)\n\n\n    # Load ""y"" (the neural network\'s training and testing outputs)\n\n    def load_y(y_path):\n        file = open(y_path, \'r\')\n        # Read dataset from disk, dealing with text file\'s syntax\n        y_ = np.array(\n            [elem for elem in [\n                row.replace(\'  \', \' \').strip().split(\' \') for row in file\n            ]], \n            dtype=np.int32\n        )\n        file.close()\n        \n        # Substract 1 to each output class for friendly 0-based indexing \n        return y_ - 1\n\n    y_train_path = DATASET_PATH + TRAIN + ""y_train.txt""\n    y_test_path = DATASET_PATH + TEST + ""y_test.txt""\n\n    y_train_val_combined = load_y(y_train_path)\n    y_test = load_y(y_test_path)\n\n    return x_train_val_combined, y_train_val_combined, x_test, y_test\n\ndef one_hot(y, numOutput):\n    y = np.reshape(y, [-1])\n    ret = np.zeros([y.shape[0], numOutput])\n    for i, label in enumerate(y):\n        ret[i, label] = 1\n    return ret\n\n\ndef generateData(extractedDir):\n    x_train_val_combined, y_train_val_combined, x_test, y_test = readData(extractedDir)\n    timesteps = x_train_val_combined.shape[-2]\n    feats = x_train_val_combined.shape[-1]\n\n    trainSubjectIndices, valSubjectIndices = generateIndicesForSplits()\n\n    x_train = x_train_val_combined[trainSubjectIndices]\n    y_train = y_train_val_combined[trainSubjectIndices]\n    x_val = x_train_val_combined[valSubjectIndices]\n    y_val = y_train_val_combined[valSubjectIndices]\n\n    # normalization\n    x_train = np.reshape(x_train, [-1, feats])\n    mean = np.mean(x_train, axis=0)\n    std = np.std(x_train, axis=0)\n\n    # normalize train\n    x_train = x_train - mean\n    x_train = x_train / std\n    x_train = np.reshape(x_train, [-1, timesteps, feats])\n\n    # normalize val\n    x_val = np.reshape(x_val, [-1, feats])\n    x_val = x_val - mean\n    x_val = x_val / std\n    x_val = np.reshape(x_val, [-1, timesteps, feats])\n\n    # normalize test\n    x_test = np.reshape(x_test, [-1, feats])\n    x_test = x_test - mean\n    x_test = x_test / std\n    x_test = np.reshape(x_test, [-1, timesteps, feats])\n\n    # shuffle test, as this was remaining\n    idx = np.arange(len(x_test))\n    np.random.shuffle(idx)\n    x_test = x_test[idx]\n    y_test = y_test[idx]\n\n    # one-hot encoding of labels\n    numOutput = 6\n    y_train = one_hot(y_train, numOutput)\n    y_val = one_hot(y_val, numOutput)\n    y_test = one_hot(y_test, numOutput)\n    extractedDir += \'/\'\n    try:\n        os.mkdir(extractedDir + \'RAW\')\n    except OSError:\n        exit(""Could not create %s"" % extractedDir + \'RAW\')\n    np.save(extractedDir + ""RAW/x_train"", x_train)\n    np.save(extractedDir + ""RAW/y_train"", y_train)\n    np.save(extractedDir + ""RAW/x_test"", x_test)\n    np.save(extractedDir + ""RAW/y_test"", y_test)\n    np.save(extractedDir + ""RAW/x_val"", x_val)\n    np.save(extractedDir + ""RAW/y_val"", y_val)\n    return extractedDir\n\ndef loadData(dirname):\n    x_train = np.load(dirname + \'/\' + \'x_train.npy\')\n    y_train = np.load(dirname + \'/\' + \'y_train.npy\')\n    x_test = np.load(dirname + \'/\' + \'x_test.npy\')\n    y_test = np.load(dirname + \'/\' + \'y_test.npy\')\n    x_val = np.load(dirname + \'/\' + \'x_val.npy\')\n    y_val = np.load(dirname + \'/\' + \'y_val.npy\')\n    return x_train, y_train, x_test, y_test, x_val, y_val\n\n\ndef bagData(X, Y, subinstanceLen, subinstanceStride):\n    \'\'\'\n    Takes x and y of shape\n    [-1, 128, 9] and [-1, 6] respectively and converts it into bags of instances.\n    returns [-1, numInstance, ]\n    \'\'\'\n    numClass = 6\n    numSteps = 128\n    numFeats = 9\n    assert X.ndim == 3\n    assert X.shape[1] == numSteps\n    assert X.shape[2] == numFeats\n    assert subinstanceLen <= numSteps\n    assert subinstanceLen > 0\n    assert subinstanceStride <= numSteps\n    assert subinstanceStride >= 0\n    assert len(X) == len(Y)\n    assert Y.ndim == 2\n    assert Y.shape[1] == numClass\n    x_bagged = []\n    y_bagged = []\n    for i, point in enumerate(X[:, :, :]):\n        instanceList = []\n        start = 0\n        end = subinstanceLen\n        while True:\n            x = point[start:end, :]\n            if len(x) < subinstanceLen:\n                x_ = np.zeros([subinstanceLen, x.shape[1]])\n                x_[:len(x), :] = x[:, :]\n                x = x_\n            instanceList.append(x)\n            if end >= numSteps:\n                break\n            start += subinstanceStride\n            end += subinstanceStride\n        bag = np.array(instanceList)\n        numSubinstance = bag.shape[0]\n        label = Y[i]\n        label = np.argmax(label)\n        labelBag = np.zeros([numSubinstance, numClass])\n        labelBag[:, label] = 1\n        x_bagged.append(bag)\n        label = np.array(labelBag)\n        y_bagged.append(label)\n    return np.array(x_bagged), np.array(y_bagged)\n\n\ndef makeEMIData(subinstanceLen, subinstanceStride, sourceDir, outDir):\n    x_train, y_train, x_test, y_test, x_val, y_val = loadData(sourceDir)\n    x, y = bagData(x_train, y_train, subinstanceLen, subinstanceStride)\n    np.save(outDir + \'/x_train.npy\', x)\n    np.save(outDir + \'/y_train.npy\', y)\n    print(\'Num train %d\' % len(x))\n    x, y = bagData(x_test, y_test, subinstanceLen, subinstanceStride)\n    np.save(outDir + \'/x_test.npy\', x)\n    np.save(outDir + \'/y_test.npy\', y)\n    print(\'Num test %d\' % len(x))\n    x, y = bagData(x_val, y_val, subinstanceLen, subinstanceStride)\n    np.save(outDir + \'/x_val.npy\', x)\n    np.save(outDir + \'/y_val.npy\', y)\n    print(\'Num val %d\' % len(x))\n'"
examples/tf/EMI-RNN/process_har.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n#\n# Setting up the HAR data for EMI-RNN. It is assumed that the data has already\n# been downloaded and is placed in downloadDir.\n\nimport subprocess\nimport os\nimport numpy as np\nimport sys\nfrom helpermethods import *\n\n\nif __name__ == \'__main__\':\n    ## Config\n    workingDir = \'./\'\n    downloadDir = \'HAR\'\n    subinstanceLen = 48\n    subinstanceStride = 16\n    ## End Config\n\n    print(""Processing data"")\n    path = workingDir + \'/\' + downloadDir\n    path = os.path.abspath(path)\n    extractedDir = generateData(path)\n\n    rawDir = extractedDir + \'/RAW/\'\n    print(""Extracting features"")\n    sourceDir = rawDir\n    outDir = extractedDir + \'/%d_%d/\' % (subinstanceLen, subinstanceStride)\n    if subinstanceLen == 128:\n        outDir = extractedDir + \'/%d/\' % (subinstanceLen)\n    print(\'subinstanceLen\', subinstanceLen)\n    print(\'subinstanceStride\', subinstanceStride)\n    print(\'sourceDir\', sourceDir)\n    print(\'outDir\', outDir)\n    try:\n        os.mkdir(outDir)\n    except OSError:\n        exit(""Could not create %s"" % outDir)\n    assert len(os.listdir(outDir)) == 0\n    makeEMIData(subinstanceLen, subinstanceStride, sourceDir, outDir)\n    print(""Done"")\n\n'"
examples/tf/FastCells/fastcell_example.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport helpermethods\nimport tensorflow as tf\nimport numpy as np\nimport sys\n\nfrom edgeml_tf.trainer.fastTrainer import FastTrainer\nfrom edgeml_tf.graph.rnn import FastGRNNCell\nfrom edgeml_tf.graph.rnn import FastRNNCell\nfrom edgeml_tf.graph.rnn import UGRNNLRCell\nfrom edgeml_tf.graph.rnn import GRULRCell\nfrom edgeml_tf.graph.rnn import LSTMLRCell\n\n\ndef main():\n    # Fixing seeds for reproducibility\n    tf.set_random_seed(42)\n    np.random.seed(42)\n\n    # Hyper Param pre-processing\n    args = helpermethods.getArgs()\n\n    dataDir = args.data_dir\n    cell = args.cell\n    inputDims = args.input_dim\n    hiddenDims = args.hidden_dim\n\n    totalEpochs = args.epochs\n    learningRate = args.learning_rate\n    outFile = args.output_file\n    batchSize = args.batch_size\n    decayStep = args.decay_step\n    decayRate = args.decay_rate\n\n    wRank = args.wRank\n    uRank = args.uRank\n\n    sW = args.sW\n    sU = args.sU\n\n    update_non_linearity = args.update_nl\n    gate_non_linearity = args.gate_nl\n\n    (dataDimension, numClasses, Xtrain, Ytrain, Xtest, Ytest,\n     mean, std) = helpermethods.preProcessData(dataDir)\n\n    assert dataDimension % inputDims == 0, ""Infeasible per step input, "" + \\\n        ""Timesteps have to be integer""\n\n    X = tf.placeholder(\n        ""float"", [None, int(dataDimension / inputDims), inputDims])\n    Y = tf.placeholder(""float"", [None, numClasses])\n\n    currDir = helpermethods.createTimeStampDir(dataDir, cell)\n\n    helpermethods.dumpCommand(sys.argv, currDir)\n    helpermethods.saveMeanStd(mean, std, currDir)\n\n    if cell == ""FastGRNN"":\n        FastCell = FastGRNNCell(hiddenDims,\n                                gate_non_linearity=gate_non_linearity,\n                                update_non_linearity=update_non_linearity,\n                                wRank=wRank, uRank=uRank)\n    elif cell == ""FastRNN"":\n        FastCell = FastRNNCell(hiddenDims,\n                               update_non_linearity=update_non_linearity,\n                               wRank=wRank, uRank=uRank)\n    elif cell == ""UGRNN"":\n        FastCell = UGRNNLRCell(hiddenDims,\n                               update_non_linearity=update_non_linearity,\n                               wRank=wRank, uRank=uRank)\n    elif cell == ""GRU"":\n        FastCell = GRULRCell(hiddenDims,\n                             update_non_linearity=update_non_linearity,\n                             wRank=wRank, uRank=uRank)\n    elif cell == ""LSTM"":\n        FastCell = LSTMLRCell(hiddenDims,\n                              update_non_linearity=update_non_linearity,\n                              wRank=wRank, uRank=uRank)\n    else:\n        sys.exit(\'Exiting: No Such Cell as \' + cell)\n\n    FastCellTrainer = FastTrainer(\n        FastCell, X, Y, sW=sW, sU=sU,\n        learningRate=learningRate, outFile=outFile)\n\n    sess = tf.InteractiveSession()\n    sess.run(tf.global_variables_initializer())\n\n    FastCellTrainer.train(batchSize, totalEpochs, sess, Xtrain, Xtest,\n                          Ytrain, Ytest, decayStep, decayRate,\n                          dataDir, currDir)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/tf/FastCells/fetch_usps.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n#\n# Setting up the USPS Data.\n\nimport bz2\nimport os\nimport subprocess\nimport sys\n\nimport requests\nimport numpy as np\nfrom sklearn.datasets import load_svmlight_file\nfrom helpermethods import download_file, decompress\n\n\n\ndef downloadData(workingDir, downloadDir, linkTrain, linkTest):\n    path = workingDir + \'/\' + downloadDir\n    path = os.path.abspath(path)\n    try:\n        os.makedirs(path, exist_ok=True)\n    except OSError:\n        print(""Could not create %s. Make sure the path does"" % path)\n        print(""not already exist and you have permissions to create it."")\n        return False\n\n    training_data_bz2 = download_file(linkTrain, path)\n    test_data_bz2 = download_file(linkTest, path)\n\n    training_data = decompress(training_data_bz2)\n    test_data = decompress(test_data_bz2)\n    \n    train = os.path.join(path, ""train.txt"")\n    test = os.path.join(path, ""test.txt"")\n    if os.path.isfile(train):\n        os.remove(train)\n    if os.path.isfile(test):\n        os.remove(test)\n\n    os.rename(training_data, train)\n    os.rename(test_data, test)\n    os.remove(training_data_bz2)\n    os.remove(test_data_bz2)\n    return True\n\nif __name__ == \'__main__\':\n    workingDir = \'./\'\n    downloadDir = \'usps10\'\n    linkTrain = \'http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/usps.bz2\'\n    linkTest = \'http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/usps.t.bz2\'\n    failureMsg = \'\'\'\nDownload Failed!\nTo manually perform the download\n\\t1. Create a new empty directory named `usps10`.\n\\t2. Download the data from the following links into the usps10 directory.\n\\t\\tTest: %s\n\\t\\tTrain: %s\n\\t3. Extract the downloaded files.\n\\t4. Rename `usps` to `train.txt` and,\n\\t5. Rename `usps.t` to `test.txt\n\'\'\' % (linkTrain, linkTest)\n\n    if not downloadData(workingDir, downloadDir, linkTrain, linkTest):\n        exit(failureMsg)\n    print(""Done: see "", downloadDir)\n'"
examples/tf/FastCells/helpermethods.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n\'\'\'\n Functions to check sanity of input arguments\n for the example script.\n\'\'\'\nimport argparse\nimport bz2\nimport datetime\nimport json\nimport os\n\nimport numpy as np\nimport requests\n\n\ndef decompress(filepath):\n    print(""extracting: "", filepath)\n    zipfile = bz2.BZ2File(filepath)  # open the file\n    data = zipfile.read()  # get the decompressed data\n    newfilepath = os.path.splitext(filepath)[0]  # assuming the filepath ends with .bz2\n    with open(newfilepath, \'wb\') as f:\n        f.write(data)  # write a uncompressed file\n    return newfilepath\n\n\ndef download_file(url, local_folder=None):\n    """"""Downloads file pointed to by `url`.\n    If `local_folder` is not supplied, downloads to the current folder.\n    """"""\n    filename = os.path.basename(url)\n    if local_folder:\n        filename = os.path.join(local_folder, filename)\n\n    # Download the file\n    print(""Downloading: "" + url)\n    response = requests.get(url, stream=True)\n    if response.status_code != 200:\n        raise Exception(""download file failed with status code: %d, fetching url \'%s\'"" % (response.status_code, url))\n\n    # Write the file to disk\n    with open(filename, ""wb"") as handle:\n        handle.write(response.content)\n    return filename\n\n\ndef checkIntPos(value):\n    ivalue = int(value)\n    if ivalue <= 0:\n        raise argparse.ArgumentTypeError(\n            ""%s is an invalid positive int value"" % value)\n    return ivalue\n\n\ndef checkIntNneg(value):\n    ivalue = int(value)\n    if ivalue < 0:\n        raise argparse.ArgumentTypeError(\n            ""%s is an invalid non-neg int value"" % value)\n    return ivalue\n\n\ndef checkFloatNneg(value):\n    fvalue = float(value)\n    if fvalue < 0:\n        raise argparse.ArgumentTypeError(\n            ""%s is an invalid non-neg float value"" % value)\n    return fvalue\n\n\ndef checkFloatPos(value):\n    fvalue = float(value)\n    if fvalue <= 0:\n        raise argparse.ArgumentTypeError(\n            ""%s is an invalid positive float value"" % value)\n    return fvalue\n\n\ndef getArgs():\n    \'\'\'\n    Function to parse arguments for FastCells\n    \'\'\'\n    parser = argparse.ArgumentParser(\n        description=\'HyperParams for Fast(G)RNN\')\n    parser.add_argument(\'-dir\', \'--data-dir\', required=True,\n                        help=\'Data directory containing\' +\n                        \'train.npy and test.npy\')\n\n    parser.add_argument(\'-c\', \'--cell\', type=str, default=""FastGRNN"",\n                        help=\'Choose between [FastGRNN, FastRNN, UGRNN\' +\n                        \', GRU, LSTM], default: FastGRNN\')\n\n    parser.add_argument(\'-id\', \'--input-dim\', type=checkIntNneg, required=True,\n                        help=\'Input Dimension of RNN, each timestep will \' +\n                        \'feed input-dim features to RNN. \' +\n                        \'Total Feature length = Input Dim * Total Timestep\')\n    parser.add_argument(\'-hd\', \'--hidden-dim\', type=checkIntNneg,\n                        required=True, help=\'Hidden Dimension of RNN\')\n\n    parser.add_argument(\'-e\', \'--epochs\', type=checkIntPos, default=300,\n                        help=\'Total Epochs (default: 300 try:[100, 150, 600])\')\n    parser.add_argument(\'-b\', \'--batch-size\', type=checkIntPos, default=100,\n                        help=\'Batch Size to be used (default: 100)\')\n    parser.add_argument(\'-lr\', \'--learning-rate\', type=checkFloatPos,\n                        default=0.01, help=\'Initial Learning rate for \' +\n                        \'Adam Optimizer (default: 0.01)\')\n\n    parser.add_argument(\'-rW\', \'--wRank\', type=checkIntPos, default=None,\n                        help=\'Rank for the low-rank parameterisation of W, \' +\n                        \'None => Full Rank\')\n    parser.add_argument(\'-rU\', \'--uRank\', type=checkIntPos, default=None,\n                        help=\'Rank for the low-rank parameterisation of U, \' +\n                        \'None => Full Rank\')\n\n    parser.add_argument(\'-sW\', type=checkFloatPos, default=1.0,\n                        help=\'Sparsity for predictor parameter W(and both \' +\n                        \'W1 and W2 in low-rank)  \' +\n                        \'(default: 1.0(Dense) try: [0.1, 0.2, 0.3])\')\n    parser.add_argument(\'-sU\', type=checkFloatPos, default=1.0,\n                        help=\'Sparsity for predictor parameter U(and both \' +\n                        \'U1 and U2 in low-rank)  \' +\n                        \'(default: 1.0(Dense) try: [0.1, 0.2, 0.3])\')\n\n    parser.add_argument(\'-unl\', \'--update-nl\', type=str, default=""tanh"",\n                        help=\'Update non linearity. Choose between \' +\n                        \'[tanh, sigmoid, relu, quantTanh, quantSigm]. \' +\n                        \'default => tanh. Can add more in edgeml/graph/rnn.py\')\n    parser.add_argument(\'-gnl\', \'--gate-nl\', type=str, default=""sigmoid"",\n                        help=\'Gate non linearity. Choose between \' +\n                        \'[tanh, sigmoid, relu, quantTanh, quantSigm]. \' +\n                        \'default => sigmoid. Can add more in \' +\n                        \'edgeml/graph/rnn.py. Only Applicable to FastGRNN\')\n\n    parser.add_argument(\'-dS\', \'--decay-step\', type=checkIntPos, default=200,\n                        help=\'The interval (in epochs) after which the \' +\n                        \'learning rate should decay. \' +\n                        \'Default is 200 for 300 epochs\')\n\n    parser.add_argument(\'-dR\', \'--decay-rate\', type=checkFloatPos, default=0.1,\n                        help=\'The factor by which learning rate \' +\n                        \'should decay after each interval. Default 0.1\')\n\n    parser.add_argument(\'-oF\', \'--output-file\', default=None,\n                        help=\'Output file for dumping the program output, \' +\n                        \'(default: stdout)\')\n\n    return parser.parse_args()\n\n\ndef getQuantArgs():\n    \'\'\'\n    Function to parse arguments for Model Quantisation\n    \'\'\'\n    parser = argparse.ArgumentParser(\n        description=\'Arguments for quantizing Fast models. \' +\n        \'Works only for piece-wise linear non-linearities, \' +\n        \'like relu, quantTanh, quantSigm (check rnn.py for the definitions)\')\n    parser.add_argument(\'-dir\', \'--model-dir\', required=True,\n                        help=\'model directory containing\' +\n                        \'*.npy weight files dumped from the trained model\')\n    parser.add_argument(\'-m\', \'--max-val\', type=checkIntNneg, default=127,\n                        help=\'this represents the maximum possible value \' +\n                        \'in model, essentially the byte complexity, \' +\n                        \'127=> 1 byte is default\')\n    parser.add_argument(\'-s\', \'--scalar-scale\', type=checkIntNneg,\n                        default=1000, help=\'maximum granularity/decimals \' +\n                        \'you wish to get when quantising simple sclars \' +\n                        \'involved. Default is 1000\')\n\n    return parser.parse_args()\n\n\ndef createTimeStampDir(dataDir, cell):\n    \'\'\'\n    Creates a Directory with timestamp as it\'s name\n    \'\'\'\n    if os.path.isdir(os.path.join(dataDir, str(cell) + \'Results\')) is False:\n        try:\n            os.mkdir(os.path.join(dataDir, str(cell) + \'Results\'))\n        except OSError:\n            print(""Creation of the directory %s failed"" %\n                  os.path.join(dataDir, str(cell) + \'Results\'))\n\n    currDir = os.path.join(str(cell) + \'Results\',\n        datetime.datetime.now().strftime(""%Y-%m-%dT%H-%M-%S""))\n    if os.path.isdir(os.path.join(dataDir, currDir)) is False:\n        try:\n            os.mkdir(os.path.join(dataDir, currDir))\n        except OSError:\n            print(""Creation of the directory %s failed"" %\n                  os.path.join(dataDir, currDir))\n        else:\n            return (os.path.join(dataDir, currDir))\n    return None\n\n\ndef preProcessData(dataDir):\n    \'\'\'\n    Function to pre-process input data\n\n    Expects a .npy file of form [lbl feats] for each datapoint,\n    feats is timesteps*inputDims, flattened across timestep dimension.\n    So input of 1st timestep followed by second and so on.\n\n    Outputs train and test set datapoints\n    dataDimension, numClasses are inferred directly\n    \'\'\'\n    train = np.load(os.path.join(dataDir, \'train.npy\'))\n    test = np.load(os.path.join(dataDir, \'test.npy\'))\n\n    dataDimension = int(train.shape[1]) - 1\n\n    Xtrain = train[:, 1:dataDimension + 1]\n    Ytrain_ = train[:, 0]\n    numClasses = max(Ytrain_) - min(Ytrain_) + 1\n\n    Xtest = test[:, 1:dataDimension + 1]\n    Ytest_ = test[:, 0]\n\n    numClasses = int(max(numClasses, max(Ytest_) - min(Ytest_) + 1))\n\n    # Mean Var Normalisation\n    mean = np.mean(Xtrain, 0)\n    std = np.std(Xtrain, 0)\n    std[std[:] < 0.000001] = 1\n    Xtrain = (Xtrain - mean) / std\n\n    Xtest = (Xtest - mean) / std\n    # End Mean Var normalisation\n\n    lab = Ytrain_.astype(\'uint8\')\n    lab = np.array(lab) - min(lab)\n\n    lab_ = np.zeros((Xtrain.shape[0], numClasses))\n    lab_[np.arange(Xtrain.shape[0]), lab] = 1\n    Ytrain = lab_\n\n    lab = Ytest_.astype(\'uint8\')\n    lab = np.array(lab) - min(lab)\n\n    lab_ = np.zeros((Xtest.shape[0], numClasses))\n    lab_[np.arange(Xtest.shape[0]), lab] = 1\n    Ytest = lab_\n\n    return dataDimension, numClasses, Xtrain, Ytrain, Xtest, Ytest, mean, std\n\n\ndef dumpCommand(list, currDir):\n    \'\'\'\n    Dumps the current command to a file for further use\n    \'\'\'\n    commandFile = open(os.path.join(currDir, \'command.txt\'), \'w\')\n    command = ""python""\n\n    command = command + "" "" + \' \'.join(list)\n    commandFile.write(command)\n\n    commandFile.flush()\n    commandFile.close()\n\n\ndef saveMeanStd(mean, std, currDir):\n    \'\'\'\n    Function to save Mean and Std vectors\n    \'\'\'\n    np.save(os.path.join(currDir, \'mean.npy\'), mean)\n    np.save(os.path.join(currDir, \'std.npy\'), std)\n\n\ndef saveJSon(data, filename):\n    with open(filename, ""w"") as outfile:\n        json.dump(data, outfile, indent=2)\n'"
examples/tf/FastCells/process_usps.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n#\n# Processing the USPS Data. It is assumed that the data is already\n# downloaded.\n\nimport subprocess\nimport os\nimport numpy as np\nfrom sklearn.datasets import load_svmlight_file\nimport sys\n\ndef processData(workingDir, downloadDir):\n    def loadLibSVMFile(file):\n        data = load_svmlight_file(file)\n        features = data[0]\n        labels = data[1]\n        retMat = np.zeros([features.shape[0], features.shape[1] + 1])\n        retMat[:, 0] = labels\n        retMat[:, 1:] = features.todense()\n        return retMat\n\n    path = workingDir + \'/\' + downloadDir\n    path = os.path.abspath(path)\n    trf = path + \'/train.txt\'\n    tsf = path + \'/test.txt\'\n    assert os.path.isfile(trf), \'File not found: %s\' % trf\n    assert os.path.isfile(tsf), \'File not found: %s\' % tsf\n    train = loadLibSVMFile(trf)\n    test = loadLibSVMFile(tsf)\n    np.save(path + \'/train.npy\', train)\n    np.save(path + \'/test.npy\', test)\n\nif __name__ == \'__main__\':\n    # Configuration\n    workingDir = \'./\'\n    downloadDir = \'usps10\'\n    # End config\n    print(""Processing data"")\n    processData(workingDir, downloadDir)\n    print(""Done"")\n'"
examples/tf/FastCells/quantizeFastModels.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport helpermethods\nimport os\nimport numpy as np\n\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n\ndef min_max(A, name):\n    print(name + "" has max: "" + str(np.max(A)) + "" min: "" + str(np.min(A)))\n    return np.max([np.abs(np.max(A)), np.abs(np.min(A))])\n\n\ndef quantizeFastModels(modelDir, maxValue=127, scalarScaleFactor=1000):\n    ls = os.listdir(modelDir)\n    paramNameList = []\n    paramWeightList = []\n    paramLimitList = []\n\n    classifierNameList = []\n    classifierWeightList = []\n    classifierLimitList = []\n\n    scalarNameList = []\n    scalarWeightList = []\n\n    for file in ls:\n        if file.endswith(""npy""):\n            if file.startswith(""W""):\n                paramNameList.append(file)\n                temp = np.load(os.path.join(modelDir, file))\n                paramWeightList.append(temp)\n                paramLimitList.append(min_max(temp, file))\n            elif file.startswith(""U""):\n                paramNameList.append(file)\n                temp = np.load(os.path.join(modelDir, file))\n                paramWeightList.append(temp)\n                paramLimitList.append(min_max(temp, file))\n            elif file.startswith(""B""):\n                paramNameList.append(file)\n                temp = np.load(os.path.join(modelDir, file))\n                paramWeightList.append(temp)\n                paramLimitList.append(min_max(temp, file))\n            elif file.startswith(""FC""):\n                classifierNameList.append(file)\n                temp = np.load(os.path.join(modelDir, file))\n                classifierWeightList.append(temp)\n                classifierLimitList.append(min_max(temp, file))\n            elif file.startswith(""mean"") or file.startswith(""std""):\n                continue\n            else:\n                scalarNameList.append(file)\n                scalarWeightList.append(np.load(os.path.join(modelDir, file)))\n\n    paramLimit = np.max(paramLimitList)\n    classifierLimit = np.max(classifierLimitList)\n\n    paramScaleFactor = np.round((2.0 * maxValue + 1.0) / (2.0 * paramLimit))\n    classifierScaleFactor = (2.0 * maxValue + 1.0) / (2.0 * classifierLimit)\n\n    quantParamWeights = []\n    for param in paramWeightList:\n        temp = np.round(paramScaleFactor * param)\n        temp[temp[:] > maxValue] = maxValue\n        temp[temp[:] < -maxValue] = -1 * (maxValue + 1)\n\n        if maxValue <= 127:\n            temp = temp.astype(\'int8\')\n        elif maxValue <= 32767:\n            temp = temp.astype(\'int16\')\n        else:\n            temp = temp.astype(\'int32\')\n\n        quantParamWeights.append(temp)\n\n    quantClassifierWeights = []\n    for param in classifierWeightList:\n        temp = np.round(classifierScaleFactor * param)\n        temp[temp[:] > maxValue] = maxValue\n        temp[temp[:] < -maxValue] = -1 * (maxValue + 1)\n\n        if maxValue <= 127:\n            temp = temp.astype(\'int8\')\n        elif maxValue <= 32767:\n            temp = temp.astype(\'int16\')\n        else:\n            temp = temp.astype(\'int32\')\n\n        quantClassifierWeights.append(temp)\n\n    quantScalarWeights = []\n    for scalar in scalarWeightList:\n        quantScalarWeights.append(\n            np.round(scalarScaleFactor * sigmoid(scalar)).astype(\'int32\'))\n\n    quantModelDir = os.path.join(modelDir, \'QuantizedModel\')\n    if not os.path.isdir(quantModelDir):\n        try:\n            os.makedirs(quantModelDir, exist_ok=True)\n        except OSError:\n            print(""Creation of the directory %s failed"" % quantModelDir)\n\n    np.save(os.path.join(quantModelDir, ""paramScaleFactor.npy""),\n            paramScaleFactor.astype(\'int32\'))\n    np.save(os.path.join(quantModelDir, ""classifierScaleFactor.npy""),\n            classifierScaleFactor)\n    np.save(os.path.join(quantModelDir, ""scalarScaleFactor""), scalarScaleFactor)\n\n    for i in range(0, len(scalarNameList)):\n        np.save(os.path.join(quantModelDir, ""q"" +\n                scalarNameList[i]), quantScalarWeights[i])\n\n    for i in range(len(classifierNameList)):\n        np.save(os.path.join(quantModelDir, ""q"" +\n                classifierNameList[i]), quantClassifierWeights[i])\n\n    for i in range(len(paramNameList)):\n        np.save(os.path.join(quantModelDir, ""q"" + paramNameList[i]),\n                quantParamWeights[i])\n\n    print(""\\n\\nQuantized Model Dir: "" + quantModelDir)\n\n\ndef main():\n    args = helpermethods.getQuantArgs()\n    quantizeFastModels(args.model_dir, int(\n        args.max_val), int(args.scalar_scale))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/tf/ProtoNN/fetch_usps.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n#\n# Setting up the USPS Data.\n\nimport subprocess\nimport os\nimport numpy as np\nfrom sklearn.datasets import load_svmlight_file\nimport sys\n\ndef downloadData(workingDir, downloadDir, linkTrain, linkTest):\n    def runcommand(command):\n        p = subprocess.Popen(command.split(), stdout=subprocess.PIPE)\n        output, error = p.communicate()\n        assert(p.returncode == 0), \'Command failed: %s\' % command\n\n    path = workingDir + \'/\' + downloadDir\n    path = os.path.abspath(path)\n    try:\n        os.mkdir(path)\n    except OSError:\n        print(""Could not create %s. Make sure the path does"" % path)\n        print(""not already exist and you have permisions to create it."")\n        return False\n    cwd = os.getcwd()\n    os.chdir(path)\n    print(""Downloading data"")\n    command = \'wget %s\' % linkTrain\n    runcommand(command)\n    command = \'wget %s\' % linkTest\n    runcommand(command)\n    print(""Extracting data"")\n    command = \'bzip2 -d usps.bz2\'\n    runcommand(command)\n    command = \'bzip2 -d usps.t.bz2\'\n    runcommand(command)\n    command = \'mv usps train.txt\'\n    runcommand(command)\n    command = \'mv usps.t test.txt\'\n    runcommand(command)\n    os.chdir(cwd)\n    return True\n\nif __name__ == \'__main__\':\n    workingDir = \'./\'\n    downloadDir = \'usps10\'\n    linkTrain = \'http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/usps.bz2\'\n    linkTest = \'http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/usps.t.bz2\'\n    failureMsg = \'\'\'\nDownload Failed!\nTo manually perform the download\n\\t1. Create a new empty directory named `usps10`.\n\\t2. Download the data from the following links into the usps10 directory.\n\\t\\tTest: %s\n\\t\\tTrain: %s\n\\t3. Extract the downloaded files.\n\\t4. Rename `usps` to `train.txt` and,\n\\t5. Rename `usps.t` to `test.txt\n\'\'\' % (linkTrain, linkTest)\n\n    if not downloadData(workingDir, downloadDir, linkTrain, linkTest):\n        exit(failureMsg)\n    print(""Done"")\n'"
examples/tf/ProtoNN/helpermethods.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nfrom __future__ import print_function\nimport sys\nimport os\nimport numpy as np\nimport tensorflow as tf\nimport edgeml_tf.utils as utils\nimport argparse\n\n\ndef getModelSize(matrixList, sparcityList, expected=True, bytesPerVar=4):\n    \'\'\'\n    expected: Expected size according to the parameters set. The number of\n        zeros could actually be more than that is required to satisfy the\n        sparsity constraint.\n    \'\'\'\n    nnzList, sizeList, isSparseList = [], [], []\n    hasSparse = False\n    for i in range(len(matrixList)):\n        A, s = matrixList[i], sparcityList[i]\n        assert A.ndim == 2\n        assert s >= 0\n        assert s <= 1\n        nnz, size, sparse = utils.countnnZ(A, s, bytesPerVar=bytesPerVar)\n        nnzList.append(nnz)\n        sizeList.append(size)\n        hasSparse = (hasSparse or sparse)\n\n    totalnnZ = np.sum(nnzList)\n    totalSize = np.sum(sizeList)\n    if expected:\n        return totalnnZ, totalSize, hasSparse\n    numNonZero = 0\n    totalSize = 0\n    hasSparse = False\n    for i in range(len(matrixList)):\n        A, s = matrixList[i], sparcityList[i]\n        numNonZero_ = np.count_nonzero(A)\n        numNonZero += numNonZero_\n        hasSparse = (hasSparse or (s < 0.5))\n        if s <= 0.5:\n            totalSize += numNonZero_ * 2 * bytesPerVar\n        else:\n            totalSize += A.size * bytesPerVar\n    return numNonZero, totalSize, hasSparse\n\n\ndef getGamma(gammaInit, projectionDim, dataDim, numPrototypes, x_train):\n    if gammaInit is None:\n        print(""Using median heuristic to estimate gamma."")\n        gamma, W, B = utils.medianHeuristic(x_train, projectionDim,\n                                            numPrototypes)\n        print(""Gamma estimate is: %f"" % gamma)\n        return W, B, gamma\n    return None, None, gammaInit\n\ndef to_onehot(y, numClasses, minlabel = None):\n    \'\'\'\n    If the y labelling does not contain the minimum label info, use min-label to\n    provide this value.\n    \'\'\'\n    lab = y.astype(\'uint8\')\n    if minlabel is None:\n        minlabel = np.min(lab)\n    minlabel = int(minlabel)\n    lab = np.array(lab) - minlabel\n    lab_ = np.zeros((y.shape[0], numClasses))\n    lab_[np.arange(y.shape[0]), lab] = 1\n    return lab_\n\ndef preprocessData(train, test):\n    \'\'\'\n    Loads data from the dataDir and does some initial preprocessing\n    steps. Data is assumed to be contained in two files,\n    train.npy and test.npy. Each containing a 2D numpy array of dimension\n    [numberOfExamples, numberOfFeatures + 1]. The first column of each\n    matrix is assumed to contain label information.\n\n    For an N-Class problem, we assume the labels are integers from 0 through\n    N-1.\n    \'\'\'\n    dataDimension = int(train.shape[1]) - 1\n    x_train = train[:, 1:dataDimension + 1]\n    y_train_ = train[:, 0]\n    x_test = test[:, 1:dataDimension + 1]\n    y_test_ = test[:, 0]\n\n    numClasses = max(y_train_) - min(y_train_) + 1\n    numClasses = max(numClasses, max(y_test_) - min(y_test_) + 1)\n    numClasses = int(numClasses)\n\n    # mean-var\n    mean = np.mean(x_train, 0)\n    std = np.std(x_train, 0)\n    std[std[:] < 0.000001] = 1\n    x_train = (x_train - mean) / std\n    x_test = (x_test - mean) / std\n\n    # one hot y-train\n    lab = y_train_.astype(\'uint8\')\n    lab = np.array(lab) - min(lab)\n    lab_ = np.zeros((x_train.shape[0], numClasses))\n    lab_[np.arange(x_train.shape[0]), lab] = 1\n    y_train = lab_\n\n    # one hot y-test\n    lab = y_test_.astype(\'uint8\')\n    lab = np.array(lab) - min(lab)\n    lab_ = np.zeros((x_test.shape[0], numClasses))\n    lab_[np.arange(x_test.shape[0]), lab] = 1\n    y_test = lab_\n\n    return dataDimension, numClasses, x_train, y_train, x_test, y_test\n\n\n\ndef getProtoNNArgs():\n    def checkIntPos(value):\n        ivalue = int(value)\n        if ivalue <= 0:\n            raise argparse.ArgumentTypeError(\n                ""%s is an invalid positive int value"" % value)\n        return ivalue\n\n    def checkIntNneg(value):\n        ivalue = int(value)\n        if ivalue < 0:\n            raise argparse.ArgumentTypeError(\n                ""%s is an invalid non-neg int value"" % value)\n        return ivalue\n\n    def checkFloatNneg(value):\n        fvalue = float(value)\n        if fvalue < 0:\n            raise argparse.ArgumentTypeError(\n                ""%s is an invalid non-neg float value"" % value)\n        return fvalue\n\n    def checkFloatPos(value):\n        fvalue = float(value)\n        if fvalue <= 0:\n            raise argparse.ArgumentTypeError(\n                ""%s is an invalid positive float value"" % value)\n        return fvalue\n\n    \'\'\'\n    Parse protoNN commandline arguments\n    \'\'\'\n    parser = argparse.ArgumentParser(\n        description=\'Hyperparameters for ProtoNN Algorithm\')\n\n    msg = \'Data directory containing train and test data. The \'\n    msg += \'data is assumed to be saved as 2-D numpy matrices with \'\n    msg += \'names `train.npy` and `test.npy`, of dimensions\\n\'\n    msg += \'\\t[numberOfInstances, numberOfFeatures + 1].\\n\'\n    msg += \'The first column of each file is assumed to contain label information.\'\n    msg += \' For a N-class problem, labels are assumed to be integers from 0 to\'\n    msg += \' N-1 (inclusive).\'\n    parser.add_argument(\'-d\', \'--data-dir\', required=True, help=msg)\n    parser.add_argument(\'-l\', \'--projection-dim\', type=checkIntPos, default=10,\n                        help=\'Projection Dimension.\')\n    parser.add_argument(\'-p\', \'--num-prototypes\', type=checkIntPos, default=20,\n                        help=\'Number of prototypes.\')\n    parser.add_argument(\'-g\', \'--gamma\', type=checkFloatPos, default=None,\n                        help=\'Gamma for Gaussian kernel. If not provided, \' +\n                        \'median heuristic will be used to estimate gamma.\')\n\n    parser.add_argument(\'-e\', \'--epochs\', type=checkIntPos, default=100,\n                        help=\'Total training epochs.\')\n    parser.add_argument(\'-b\', \'--batch-size\', type=checkIntPos, default=32,\n                        help=\'Batch size for each pass.\')\n    parser.add_argument(\'-r\', \'--learning-rate\', type=checkFloatPos,\n                        default=0.001,\n                        help=\'Initial Learning rate for ADAM Optimizer.\')\n\n    parser.add_argument(\'-rW\', type=float, default=0.000,\n                        help=\'Coefficient for l2 regularizer for predictor\' +\n                        \' parameter W \' + \'(default = 0.0).\')\n    parser.add_argument(\'-rB\', type=float, default=0.00,\n                        help=\'Coefficient for l2 regularizer for predictor\' +\n                        \' parameter B \' + \'(default = 0.0).\')\n    parser.add_argument(\'-rZ\', type=float, default=0.00,\n                        help=\'Coefficient for l2 regularizer for predictor\' +\n                        \'parameter Z \' +\n                        \'(default = 0.0).\')\n\n    parser.add_argument(\'-sW\', type=float, default=1.000,\n                        help=\'Sparsity constraint for predictor parameter W \' +\n                        \'(default = 1.0, i.e. dense matrix).\')\n    parser.add_argument(\'-sB\', type=float, default=1.00,\n                        help=\'Sparsity constraint for predictor parameter B \' +\n                        \'(default = 1.0, i.e. dense matrix).\')\n    parser.add_argument(\'-sZ\', type=float, default=1.00,\n                        help=\'Sparsity constraint for predictor parameter Z \' +\n                        \'(default = 1.0, i.e. dense matrix).\')\n    parser.add_argument(\'-pS\', \'--print-step\', type=int, default=200,\n                        help=\'The number of update steps between print \' +\n                        \'calls to console.\')\n    parser.add_argument(\'-vS\', \'--val-step\', type=int, default=3,\n                        help=\'The number of epochs between validation\' +\n                        \'performance evaluation\')\n    parser.add_argument(\'-o\', \'--output-dir\', type=str, default=\'./\',\n                        help=\'Output directory to dump model matrices.\')\n    return parser.parse_args()\n'"
examples/tf/ProtoNN/process_usps.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n#\n# Processing the USPS Data. It is assumed that the data is already\n# downloaded.\n\nimport subprocess\nimport os\nimport numpy as np\nfrom sklearn.datasets import load_svmlight_file\nimport sys\nfrom helpermethods import preprocessData\n\ndef processData(workingDir, downloadDir):\n    def loadLibSVMFile(file):\n        data = load_svmlight_file(file)\n        features = data[0]\n        labels = data[1]\n        retMat = np.zeros([features.shape[0], features.shape[1] + 1])\n        retMat[:, 0] = labels\n        retMat[:, 1:] = features.todense()\n        return retMat\n\n    path = workingDir + \'/\' + downloadDir\n    path = os.path.abspath(path)\n    trf = path + \'/train.txt\'\n    tsf = path + \'/test.txt\'\n    assert os.path.isfile(trf), \'File not found: %s\' % trf\n    assert os.path.isfile(tsf), \'File not found: %s\' % tsf\n    train = loadLibSVMFile(trf)\n    test = loadLibSVMFile(tsf)\n    np.save(path + \'/train_unnormalized.npy\', train)\n    np.save(path + \'/test_unnormalized.npy\', test)\n    _, _, x_train, y_train, x_test, y_test = preprocessData(train, test)\n\n    y_ = np.expand_dims(np.argmax(y_train, axis=1), axis=1)\n    train = np.concatenate([y_, x_train], axis=1)\n    np.save(path + \'/train.npy\', train)\n    y_ = np.expand_dims(np.argmax(y_test, axis=1), axis=1)\n    test = np.concatenate([y_, x_test], axis=1)\n    np.save(path + \'/test.npy\', test)\n\n\nif __name__ == \'__main__\':\n    # Configuration\n    workingDir = \'./\'\n    downloadDir = \'usps10\'\n    # End config\n    print(""Processing data"")\n    processData(workingDir, downloadDir)\n    print(""Done"")\n'"
examples/tf/ProtoNN/protoNN_example.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nfrom __future__ import print_function\nimport sys\nimport os\nimport numpy as np\nimport tensorflow as tf\nfrom edgeml_tf.trainer.protoNNTrainer import ProtoNNTrainer\nfrom edgeml_tf.graph.protoNN import ProtoNN\nimport edgeml_tf.utils as utils\nimport helpermethods as helper\n\n\ndef main():\n    config = helper.getProtoNNArgs()\n    # Get hyper parameters\n    DATA_DIR = config.data_dir\n    PROJECTION_DIM = config.projection_dim\n    NUM_PROTOTYPES = config.num_prototypes\n    REG_W = config.rW\n    REG_B = config.rB\n    REG_Z = config.rZ\n    SPAR_W = config.sW\n    SPAR_B = config.sB\n    SPAR_Z = config.sZ\n    LEARNING_RATE = config.learning_rate\n    NUM_EPOCHS = config.epochs\n    BATCH_SIZE = config.batch_size\n    PRINT_STEP = config.print_step\n    VAL_STEP = config.val_step\n    OUT_DIR = config.output_dir\n\n    # Load data\n    train = np.load(DATA_DIR + \'/train.npy\')\n    test = np.load(DATA_DIR + \'/test.npy\')\n    x_train, y_train = train[:, 1:], train[:, 0]\n    x_test, y_test = test[:, 1:], test[:, 0]\n    # Convert y to one-hot\n    minval = min(min(y_train), min(y_test))\n    numClasses = max(y_train) - min(y_train) + 1\n    numClasses = max(numClasses, max(y_test) - min(y_test) + 1)\n    numClasses = int(numClasses)\n    y_train = helper.to_onehot(y_train, numClasses, minlabel=minval)\n    y_test = helper.to_onehot(y_test, numClasses, minlabel=minval)\n    dataDimension = x_train.shape[1]\n\n    W, B, gamma = helper.getGamma(config.gamma, PROJECTION_DIM, dataDimension,\n                                  NUM_PROTOTYPES, x_train)\n\n    # Setup input and train protoNN\n    X = tf.placeholder(tf.float32, [None, dataDimension], name=\'X\')\n    Y = tf.placeholder(tf.float32, [None, numClasses], name=\'Y\')\n    protoNN = ProtoNN(dataDimension, PROJECTION_DIM,\n                      NUM_PROTOTYPES, numClasses,\n                      gamma, W=W, B=B)\n    trainer = ProtoNNTrainer(protoNN, REG_W, REG_B, REG_Z,\n                             SPAR_W, SPAR_B, SPAR_Z,\n                             LEARNING_RATE, X, Y, lossType=\'xentropy\')\n    sess = tf.Session()\n    trainer.train(BATCH_SIZE, NUM_EPOCHS, sess, x_train, x_test,\n                  y_train, y_test, printStep=PRINT_STEP, valStep=VAL_STEP)\n\n    # Print some summary metrics\n    acc = sess.run(protoNN.accuracy, feed_dict={X: x_test, Y: y_test})\n    # W, B, Z are tensorflow graph nodes\n    W, B, Z, gamma  = protoNN.getModelMatrices()\n    matrixList = sess.run([W, B, Z])\n    gamma = sess.run(gamma)\n    sparcityList = [SPAR_W, SPAR_B, SPAR_Z]\n    nnz, size, sparse = helper.getModelSize(matrixList, sparcityList)\n    print(""Final test accuracy"", acc)\n    print(""Model size constraint (Bytes): "", size)\n    print(""Number of non-zeros: "", nnz)\n    nnz, size, sparse = helper.getModelSize(matrixList, sparcityList,\n                                            expected=False)\n    print(""Actual model size: "", size)\n    print(""Actual non-zeros: "", nnz)\n    print(""Saving model matrices to: "", OUT_DIR)\n    np.save(OUT_DIR + \'/W.npy\', matrixList[0])\n    np.save(OUT_DIR + \'/B.npy\', matrixList[1])\n    np.save(OUT_DIR + \'/Z.npy\', matrixList[2])\n    np.save(OUT_DIR + \'/gamma.npy\', gamma)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pytorch/edgeml_pytorch/graph/__init__.py,0,b''
pytorch/edgeml_pytorch/graph/bonsai.py,19,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\n\nclass Bonsai(nn.Module):\n\n    def __init__(self, numClasses, dataDimension, projectionDimension,\n                 treeDepth, sigma, W=None, T=None, V=None, Z=None):\n        super(Bonsai, self).__init__()\n        \'\'\'\n        Expected Dimensions:\n\n        Bonsai Params // Optional\n        W [numClasses*totalNodes, projectionDimension]\n        V [numClasses*totalNodes, projectionDimension]\n        Z [projectionDimension, dataDimension + 1]\n        T [internalNodes, projectionDimension]\n\n        internalNodes = 2**treeDepth - 1\n        totalNodes = 2*internalNodes + 1\n\n        sigma - tanh non-linearity\n        sigmaI - Indicator function for node probabilities\n        sigmaI - has to be set to infinity(1e9 for practice)\n        while doing testing/inference\n        numClasses will be reset to 1 in binary case\n        \'\'\'\n\n        self.dataDimension = dataDimension\n        self.projectionDimension = projectionDimension\n\n        if numClasses == 2:\n            self.numClasses = 1\n        else:\n            self.numClasses = numClasses\n\n        self.treeDepth = treeDepth\n        self.sigma = sigma\n\n        self.internalNodes = 2**self.treeDepth - 1\n        self.totalNodes = 2 * self.internalNodes + 1\n\n        self.W = self.initW(W)\n        self.V = self.initV(V)\n        self.T = self.initT(T)\n        self.Z = self.initZ(Z)\n\n        self.assertInit()\n\n    def initZ(self, Z):\n        if Z is None:\n            Z = torch.randn([self.projectionDimension, self.dataDimension])\n            Z = nn.Parameter(Z)\n        else:\n            Z = torch.from_numpy(Z.astype(np.float32))\n            Z = nn.Parameter(Z)\n        return Z\n\n    def initW(self, W):\n        if W is None:\n            W = torch.randn(\n                [self.numClasses * self.totalNodes, self.projectionDimension])\n            W = nn.Parameter(W)\n        else:\n            W = torch.from_numpy(W.astype(np.float32))\n            W = nn.Parameter(W)\n        return W\n\n    def initV(self, V):\n        if V is None:\n            V = torch.randn(\n                [self.numClasses * self.totalNodes, self.projectionDimension])\n            V = nn.Parameter(V)\n        else:\n            V = torch.from_numpy(V.astype(np.float32))\n            V = nn.Parameter(V)\n        return V\n\n    def initT(self, T):\n        if T is None:\n            T = torch.randn([self.internalNodes, self.projectionDimension])\n            T = nn.Parameter(T)\n        else:\n            T = torch.from_numpy(T.astype(np.float32))\n            T = nn.Parameter(T)\n        return T\n\n    def forward(self, X, sigmaI):\n        \'\'\'\n        Function to build/exxecute the Bonsai Tree graph\n        Expected Dimensions\n\n        X is [batchSize, self.dataDimension]\n        sigmaI is constant\n        \'\'\'\n        X_ = torch.matmul(self.Z, torch.t(X)) / self.projectionDimension\n        W_ = self.W[0:(self.numClasses)]\n        V_ = self.V[0:(self.numClasses)]\n        self.__nodeProb = []\n        self.__nodeProb.append(1)\n        score_ = self.__nodeProb[0] * (torch.matmul(W_, X_) *\n                                       torch.tanh(self.sigma *\n                                                  torch.matmul(V_, X_)))\n        for i in range(1, self.totalNodes):\n            W_ = self.W[i * self.numClasses:((i + 1) * self.numClasses)]\n            V_ = self.V[i * self.numClasses:((i + 1) * self.numClasses)]\n\n            T_ = torch.reshape(self.T[int(np.ceil(i / 2.0) - 1.0)],\n                               [-1, self.projectionDimension])\n            prob = (1 + ((-1)**(i + 1)) *\n                    torch.tanh(sigmaI * torch.matmul(T_, X_)))\n\n            prob = prob / 2.0\n            prob = self.__nodeProb[int(np.ceil(i / 2.0) - 1.0)] * prob\n            self.__nodeProb.append(prob)\n            score_ += self.__nodeProb[i] * (torch.matmul(W_, X_) *\n                                            torch.tanh(self.sigma *\n                                                       torch.matmul(V_, X_)))\n\n        self.score = score_\n        self.X_ = X_\n        return torch.t(self.score), self.X_\n\n    def assertInit(self):\n        errRank = ""All Parameters must has only two dimensions shape = [a, b]""\n        assert len(self.W.shape) == len(self.Z.shape), errRank\n        assert len(self.W.shape) == len(self.T.shape), errRank\n        assert len(self.W.shape) == 2, errRank\n        msg = ""W and V should be of same Dimensions""\n        assert self.W.shape == self.V.shape, msg\n        errW = ""W and V are [numClasses*totalNodes, projectionDimension]""\n        assert self.W.shape[0] == self.numClasses * self.totalNodes, errW\n        assert self.W.shape[1] == self.projectionDimension, errW\n        errZ = ""Z is [projectionDimension, dataDimension]""\n        assert self.Z.shape[0] == self.projectionDimension, errZ\n        assert self.Z.shape[1] == self.dataDimension, errZ\n        errT = ""T is [internalNodes, projectionDimension]""\n        assert self.T.shape[0] == self.internalNodes, errT\n        assert self.T.shape[1] == self.projectionDimension, errT\n        assert int(self.numClasses) > 0, ""numClasses should be > 1""\n        msg = ""# of features in data should be > 0""\n        assert int(self.dataDimension) > 0, msg\n        msg = ""Projection should be  > 0 dims""\n        assert int(self.projectionDimension) > 0, msg\n        msg = ""treeDepth should be >= 0""\n        assert int(self.treeDepth) >= 0, msg\n'"
pytorch/edgeml_pytorch/graph/protoNN.py,15,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\n\nclass ProtoNN(nn.Module):\n    def __init__(self, inputDimension, projectionDimension, numPrototypes,\n                 numOutputLabels, gamma, W=None, B=None, Z=None):\n        \'\'\'\n        Forward computation graph for ProtoNN.\n\n        inputDimension: Input data dimension or feature dimension.\n        projectionDimension: hyperparameter\n        numPrototypes: hyperparameter\n        numOutputLabels: The number of output labels or classes\n        W, B, Z: Numpy matrices that can be used to initialize\n            projection matrix(W), prototype matrix (B) and prototype labels\n            matrix (B).\n            Expected Dimensions:\n                W   inputDimension (d) x projectionDimension (d_cap)\n                B   projectionDimension (d_cap) x numPrototypes (m)\n                Z   numOutputLabels (L) x numPrototypes (m)\n        \'\'\'\n        super(ProtoNN, self).__init__()\n        self.__d = inputDimension\n        self.__d_cap = projectionDimension\n        self.__m = numPrototypes\n        self.__L = numOutputLabels\n\n        self.W, self.B, self.Z = None, None, None\n        self.gamma = gamma\n\n        self.__validInit = False\n        self.__initWBZ(W, B, Z)\n        self.__validateInit()\n\n    def __validateInit(self):\n        self.__validinit = False\n        errmsg = ""Dimensions mismatch! Should be W[d, d_cap]""\n        errmsg+= "", B[d_cap, m] and Z[L, m]""\n        d, d_cap, m, L, _ = self.getHyperParams()\n        assert self.W.shape[0] == d, errmsg\n        assert self.W.shape[1] == d_cap, errmsg\n        assert self.B.shape[0] == d_cap, errmsg\n        assert self.B.shape[1] == m, errmsg\n        assert self.Z.shape[0] == L, errmsg\n        assert self.Z.shape[1] == m, errmsg\n        self.__validInit = True\n\n    def __initWBZ(self, inW, inB, inZ):\n        if inW is None:\n            self.W = torch.randn([self.__d, self.__d_cap])\n            self.W = nn.Parameter(self.W)\n        else:\n            self.W = nn.Parameter(torch.from_numpy(inW.astype(np.float32)))\n\n        if inB is None:\n            self.B = torch.randn([self.__d_cap, self.__m])\n            self.B = nn.Parameter(self.B)\n        else:\n            self.B = nn.Parameter(torch.from_numpy(inB.astype(np.float32)))\n\n        if inZ is None:\n            self.Z = torch.randn([self.__L, self.__m])\n            self.Z = nn.Parameter(self.Z)\n        else:\n            self.Z = nn.Parameter(torch.from_numpy(inZ.astype(np.float32)))\n\n    def getHyperParams(self):\n        \'\'\'\n        Returns the model hyperparameters:\n            [inputDimension, projectionDimension, numPrototypes,\n            numOutputLabels, gamma]\n        \'\'\'\n        d =  self.__d\n        dcap = self.__d_cap\n        m = self.__m\n        L = self.__L\n        return d, dcap, m, L, self.gamma\n\n    def getModelMatrices(self):\n        \'\'\'\n        Returns model matrices, which can then be evaluated to obtain\n        corresponding numpy arrays.  These can then be exported as part of\n        other implementations of ProtonNN, for instance a C++ implementation or\n        pure python implementation.\n        Returns\n            [ProjectionMatrix (W), prototypeMatrix (B),\n             prototypeLabelsMatrix (Z), gamma]\n        \'\'\'\n        return self.W, self.B, self.Z, self.gamma\n\n    def forward(self, X):\n        \'\'\'\n        This method is responsible for construction of the forward computation\n        graph. The end point of the computation graph, or in other words the\n        output operator for the forward computation is returned.\n\n        X: Input of shape [-1, inputDimension]\n        returns: The forward computation outputs, self.protoNNOut\n        \'\'\'\n        assert self.__validInit is True, ""Initialization failed!""\n\n        W, B, Z, gamma = self.W, self.B, self.Z, self.gamma\n        WX = torch.matmul(X, W)\n        dim = [-1, WX.shape[1], 1]\n        WX = torch.reshape(WX, dim)\n        dim = [1, B.shape[0], -1]\n        B_ = torch.reshape(B, dim)\n        l2sim = B_ - WX\n        l2sim = torch.pow(l2sim, 2)\n        l2sim = torch.sum(l2sim, dim=1, keepdim=True)\n        self.l2sim = l2sim\n        gammal2sim = (-1 * gamma * gamma) * l2sim\n        M = torch.exp(gammal2sim)\n        dim = [1] + list(Z.shape)\n        Z_ = torch.reshape(Z, dim)\n        y = Z_ * M\n        y = torch.sum(y, dim=2)\n        return y\n\n\n'"
pytorch/edgeml_pytorch/graph/rnn.py,203,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport os\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Function\nimport numpy as np\n\nimport edgeml_pytorch.utils as utils\n\nif utils.findCUDA() is not None:\n    import fastgrnn_cuda\n\n\n# All the matrix vector computations of the form Wx are done \n# in the form of xW (with appropriate changes in shapes) to \n# be consistent with tesnorflow and pytorch internal implementations\n\n\ndef onnx_exportable_rnn(input, fargs, cell, output):\n    class RNNSymbolic(Function):\n        @staticmethod\n        def symbolic(g, *fargs):\n            # NOTE: args/kwargs contain RNN parameters\n            return g.op(cell.name, *fargs,\n                        outputs=1, hidden_size_i=cell.state_size,\n                        wRank_i=cell.wRank, uRank_i=cell.uRank,\n                        gate_nonlinearity_s=cell.gate_nonlinearity,\n                        update_nonlinearity_s=cell.update_nonlinearity)\n\n        @staticmethod\n        def forward(ctx, *fargs):\n            return output\n\n        @staticmethod\n        def backward(ctx, *gargs, **gkwargs):\n            raise RuntimeError(""FIXME: Traced RNNs don\'t support backward"")\n\n    return RNNSymbolic.apply(input, *fargs)\n\ndef gen_nonlinearity(A, nonlinearity):\n    \'\'\'\n    Returns required activation for a tensor based on the inputs\n\n    nonlinearity is either a callable or a value in\n        [\'tanh\', \'sigmoid\', \'relu\', \'quantTanh\', \'quantSigm\', \'quantSigm4\']\n    \'\'\'\n    if nonlinearity == ""tanh"":\n        return torch.tanh(A)\n    elif nonlinearity == ""sigmoid"":\n        return torch.sigmoid(A)\n    elif nonlinearity == ""relu"":\n        return torch.relu(A, 0.0)\n    elif nonlinearity == ""quantTanh"":\n        return torch.max(torch.min(A, torch.ones_like(A)), -1.0 * torch.ones_like(A))\n    elif nonlinearity == ""quantSigm"":\n        A = (A + 1.0) / 2.0\n        return torch.max(torch.min(A, torch.ones_like(A)), torch.zeros_like(A))\n    elif nonlinearity == ""quantSigm4"":\n        A = (A + 2.0) / 4.0\n        return torch.max(torch.min(A, torch.ones_like(A)), torch.zeros_like(A))\n    else:\n        # nonlinearity is a user specified function\n        if not callable(nonlinearity):\n            raise ValueError(""nonlinearity is either a callable or a value "" +\n                             ""[\'tanh\', \'sigmoid\', \'relu\', \'quantTanh\', "" +\n                             ""\'quantSigm\'"")\n        return nonlinearity(A)\n\n\nclass RNNCell(nn.Module):\n    def __init__(self, input_size, hidden_size,\n                 gate_nonlinearity, update_nonlinearity,\n                 num_W_matrices, num_U_matrices, num_biases,\n                 wRank=None, uRank=None,\n                 wSparsity=1.0, uSparsity=1.0):\n        super(RNNCell, self).__init__()\n        self._input_size = input_size\n        self._hidden_size = hidden_size\n        self._gate_nonlinearity = gate_nonlinearity\n        self._update_nonlinearity = update_nonlinearity\n        self._num_W_matrices = num_W_matrices\n        self._num_U_matrices = num_U_matrices\n        self._num_biases = num_biases\n        self._num_weight_matrices = [self._num_W_matrices, self._num_U_matrices,\n                                     self._num_biases]\n        self._wRank = wRank\n        self._uRank = uRank\n        self._wSparsity = wSparsity\n        self._uSparsity = uSparsity\n        self.oldmats = []\n\n\n    @property\n    def state_size(self):\n        return self._hidden_size\n\n    @property\n    def input_size(self):\n        return self._input_size\n\n    @property\n    def output_size(self):\n        return self._hidden_size\n\n    @property\n    def gate_nonlinearity(self):\n        return self._gate_nonlinearity\n\n    @property\n    def update_nonlinearity(self):\n        return self._update_nonlinearity\n\n    @property\n    def wRank(self):\n        return self._wRank\n\n    @property\n    def uRank(self):\n        return self._uRank\n\n    @property\n    def num_W_matrices(self):\n        return self._num_W_matrices\n\n    @property\n    def num_U_matrices(self):\n        return self._num_U_matrices\n\n    @property\n    def num_weight_matrices(self):\n        return self._num_weight_matrices\n\n    @property\n    def name(self):\n        raise NotImplementedError()\n\n    def forward(self, input, state):\n        raise NotImplementedError()\n\n    def getVars(self):\n        raise NotImplementedError()\n\n    def get_model_size(self):\n        \'\'\'\n\t\tFunction to get aimed model size\n\t\t\'\'\'\n        mats = self.getVars()\n        endW = self._num_W_matrices\n        endU = endW + self._num_U_matrices\n\n        totalnnz = 2  # For Zeta and Nu\n        for i in range(0, endW):\n            device = mats[i].device\n            totalnnz += utils.countNNZ(mats[i].cpu(), self._wSparsity)\n            mats[i].to(device)\n        for i in range(endW, endU):\n            device = mats[i].device\n            totalnnz += utils.countNNZ(mats[i].cpu(), self._uSparsity)\n            mats[i].to(device)\n        for i in range(endU, len(mats)):\n            device = mats[i].device\n            totalnnz += utils.countNNZ(mats[i].cpu(), False)\n            mats[i].to(device)\n        return totalnnz * 4\n\n    def copy_previous_UW(self):\n        mats = self.getVars()\n        num_mats = self._num_W_matrices + self._num_U_matrices\n        if len(self.oldmats) != num_mats:\n            for i in range(num_mats):\n                self.oldmats.append(torch.FloatTensor())\n        for i in range(num_mats):\n            self.oldmats[i] = torch.FloatTensor(mats[i].detach().clone().to(mats[i].device))\n\n    def sparsify(self):\n        mats = self.getVars()\n        endW = self._num_W_matrices\n        endU = endW + self._num_U_matrices\n        for i in range(0, endW):\n            mats[i] = utils.hardThreshold(mats[i], self._wSparsity)\n        for i in range(endW, endU):\n            mats[i] = utils.hardThreshold(mats[i], self._uSparsity)\n        self.copy_previous_UW()\n\n    def sparsifyWithSupport(self):\n        mats = self.getVars()\n        endU = self._num_W_matrices + self._num_U_matrices\n        for i in range(0, endU):\n            mats[i] = utils.supportBasedThreshold(mats[i], self.oldmats[i])\n\nclass FastGRNNCell(RNNCell):\n    \'\'\'\n    FastGRNN Cell with Both Full Rank and Low Rank Formulations\n    Has multiple activation functions for the gates\n    hidden_size = # hidden units\n\n    gate_nonlinearity = nonlinearity for the gate can be chosen from\n    [tanh, sigmoid, relu, quantTanh, quantSigm]\n    update_nonlinearity = nonlinearity for final rnn update\n    can be chosen from [tanh, sigmoid, relu, quantTanh, quantSigm]\n\n    wRank = rank of W matrix (creates two matrices if not None)\n    uRank = rank of U matrix (creates two matrices if not None)\n\n    wSparsity = intended sparsity of W matrix(ces)\n    uSparsity = intended sparsity of U matrix(ces)\n    Warning:\n    The Cell will not automatically sparsify.\n    The user must invoke .sparsify to hard threshold.\n\n    zetaInit = init for zeta, the scale param\n    nuInit = init for nu, the translation param\n\n    FastGRNN architecture and compression techniques are found in\n    FastGRNN(LINK) paper\n\n    Basic architecture is like:\n\n    z_t = gate_nl(Wx_t + Uh_{t-1} + B_g)\n    h_t^ = update_nl(Wx_t + Uh_{t-1} + B_h)\n    h_t = z_t*h_{t-1} + (sigmoid(zeta)(1-z_t) + sigmoid(nu))*h_t^\n\n    W and U can further parameterised into low rank version by\n    W = matmul(W_1, W_2) and U = matmul(U_1, U_2)\n    \'\'\'\n\n    def __init__(self, input_size, hidden_size, gate_nonlinearity=""sigmoid"",\n                 update_nonlinearity=""tanh"", wRank=None, uRank=None,\n                 wSparsity=1.0, uSparsity=1.0, zetaInit=1.0, nuInit=-4.0,\n                 name=""FastGRNN""):\n        super(FastGRNNCell, self).__init__(input_size, hidden_size,\n                                          gate_nonlinearity, update_nonlinearity,\n                                          1, 1, 2, wRank, uRank, wSparsity,\n                                          uSparsity)\n        self._zetaInit = zetaInit\n        self._nuInit = nuInit\n        if wRank is not None:\n            self._num_W_matrices += 1\n            self._num_weight_matrices[0] = self._num_W_matrices\n        if uRank is not None:\n            self._num_U_matrices += 1\n            self._num_weight_matrices[1] = self._num_U_matrices\n        self._name = name\n\n        if wRank is None:\n            self.W = nn.Parameter(0.1 * torch.randn([input_size, hidden_size]))\n        else:\n            self.W1 = nn.Parameter(0.1 * torch.randn([input_size, wRank]))\n            self.W2 = nn.Parameter(0.1 * torch.randn([wRank, hidden_size]))\n\n        if uRank is None:\n            self.U = nn.Parameter(0.1 * torch.randn([hidden_size, hidden_size]))\n        else:\n            self.U1 = nn.Parameter(0.1 * torch.randn([hidden_size, uRank]))\n            self.U2 = nn.Parameter(0.1 * torch.randn([uRank, hidden_size]))\n\n        self.bias_gate = nn.Parameter(torch.ones([1, hidden_size]))\n        self.bias_update = nn.Parameter(torch.ones([1, hidden_size]))\n        self.zeta = nn.Parameter(self._zetaInit * torch.ones([1, 1]))\n        self.nu = nn.Parameter(self._nuInit * torch.ones([1, 1]))\n\n        self.copy_previous_UW()\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def cellType(self):\n        return ""FastGRNN""\n\n    def forward(self, input, state):\n        if self._wRank is None:\n            wComp = torch.matmul(input, self.W)\n        else:\n            wComp = torch.matmul(\n                torch.matmul(input, self.W1), self.W2)\n\n        if self._uRank is None:\n            uComp = torch.matmul(state, self.U)\n        else:\n            uComp = torch.matmul(\n                torch.matmul(state, self.U1), self.U2)\n\n        pre_comp = wComp + uComp\n        z = gen_nonlinearity(pre_comp + self.bias_gate,\n                              self._gate_nonlinearity)\n        c = gen_nonlinearity(pre_comp + self.bias_update,\n                              self._update_nonlinearity)\n        new_h = z * state + (torch.sigmoid(self.zeta) *\n                             (1.0 - z) + torch.sigmoid(self.nu)) * c\n\n        return new_h\n\n    def getVars(self):\n        Vars = []\n        if self._num_W_matrices == 1:\n            Vars.append(self.W)\n        else:\n            Vars.extend([self.W1, self.W2])\n\n        if self._num_U_matrices == 1:\n            Vars.append(self.U)\n        else:\n            Vars.extend([self.U1, self.U2])\n\n        Vars.extend([self.bias_gate, self.bias_update])\n        Vars.extend([self.zeta, self.nu])\n        return Vars\n\nclass FastGRNNCUDACell(RNNCell):\n    \'\'\'\n    A CUDA implementation of FastGRNN Cell with Full Rank Support\n    hidden_size = # hidden units\n\n    zetaInit = init for zeta, the scale param\n    nuInit = init for nu, the translation param\n\n    FastGRNN architecture and compression techniques are found in\n    FastGRNN(LINK) paper\n\n    Basic architecture is like:\n\n    z_t = non_linearity(Wx_t + Uh_{t-1} + B_g)\n    h_t^ = tanh(Wx_t + Uh_{t-1} + B_h)\n    h_t = z_t*h_{t-1} + (sigmoid(zeta)(1-z_t) + sigmoid(nu))*h_t^\n\n    \'\'\'\n    def __init__(self, input_size, hidden_size, gate_nonlinearity=""sigmoid"", \n    update_nonlinearity=""tanh"", wRank=None, uRank=None, zetaInit=1.0, nuInit=-4.0, wSparsity=1.0, uSparsity=1.0, name=""FastGRNNCUDACell""):\n        super(FastGRNNCUDACell, self).__init__(input_size, hidden_size, gate_non_linearity, update_nonlinearity, \n                                                1, 1, 2, wRank, uRank, wSparsity, uSparsity)\n        if utils.findCUDA() is None:\n            raise Exception(\'FastGRNNCUDA is supported only on GPU devices.\')\n        NON_LINEARITY = {""sigmoid"": 0, ""relu"": 1, ""tanh"": 2}\n        self._input_size = input_size\n        self._hidden_size = hidden_size\n        self._zetaInit = zetaInit\n        self._nuInit = nuInit\n        self._name = name\n        self.device = torch.device(""cuda"")\n\n        if wRank is not None:\n            self._num_W_matrices += 1\n            self._num_weight_matrices[0] = self._num_W_matrices\n        if uRank is not None:\n            self._num_U_matrices += 1\n            self._num_weight_matrices[1] = self._num_U_matrices\n        self._name = name\n\n        if wRank is None:\n            self.W = nn.Parameter(0.1 * torch.randn([hidden_size, input_size], self.device))\n            self.W1 = torch.empty(0)\n            self.W2 = torch.empty(0)\n        else:\n            self.W = torch.empty(0)\n            self.W1 = nn.Parameter(0.1 * torch.randn([wRank, input_size], self.device))\n            self.W2 = nn.Parameter(0.1 * torch.randn([hidden_size, wRank], self.device))\n\n        if uRank is None:\n            self.U = nn.Parameter(0.1 * torch.randn([hidden_size, hidden_size], self.device))\n            self.U1 = torch.empty(0)\n            self.U2 = torch.empty(0)\n        else:\n            self.U = torch.empty(0)\n            self.U1 = nn.Parameter(0.1 * torch.randn([uRank, hidden_size], self.device))\n            self.U2 = nn.Parameter(0.1 * torch.randn([hidden_size, uRank], self.device))\n\n        self._gate_non_linearity = NON_LINEARITY[gate_nonlinearity]\n\n        self.bias_gate = nn.Parameter(torch.ones([1, hidden_size], self.device))\n        self.bias_update = nn.Parameter(torch.ones([1, hidden_size], self.device))\n        self.zeta = nn.Parameter(self._zetaInit * torch.ones([1, 1], self.device))\n        self.nu = nn.Parameter(self._nuInit * torch.ones([1, 1], self.device))\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def cellType(self):\n        return ""FastGRNNCUDACell""\n\n    def forward(self, input, state):\n        # Calls the custom autograd function while invokes the CUDA implementation\n        if not input.is_cuda:\n            input.to(self.device)\n        if not state.is_cuda:\n            state.to(self.device)\n        return FastGRNNFunction.apply(input, self.bias_gate, self.bias_update, self.zeta, self.nu, state,\n            self.W, self.U, self.W1, self.W2, self.U1, self.U2, self._gate_non_linearity)\n\n    def getVars(self):\n        Vars = []\n        if self._num_W_matrices == 1:\n            Vars.append(self.W)\n        else:\n            Vars.extend([self.W1, self.W2])\n\n        if self._num_U_matrices == 1:\n            Vars.append(self.U)\n        else:\n            Vars.extend([self.U1, self.U2])\n\n        Vars.extend([self.bias_gate, self.bias_update, self.zeta, self.nu])\n        return Vars\n\nclass FastRNNCell(RNNCell):\n    \'\'\'\n    FastRNN Cell with Both Full Rank and Low Rank Formulations\n    Has multiple activation functions for the gates\n    hidden_size = # hidden units\n\n    update_nonlinearity = nonlinearity for final rnn update\n    can be chosen from [tanh, sigmoid, relu, quantTanh, quantSigm]\n\n    wRank = rank of W matrix (creates two matrices if not None)\n    uRank = rank of U matrix (creates two matrices if not None)\n\n    wSparsity = intended sparsity of W matrix(ces)\n    uSparsity = intended sparsity of U matrix(ces)\n    Warning:\n    The Cell will not automatically sparsify.\n    The user must invoke .sparsify to hard threshold.\n\n    alphaInit = init for alpha, the update scalar\n    betaInit = init for beta, the weight for previous state\n\n    FastRNN architecture and compression techniques are found in\n    FastGRNN(LINK) paper\n\n    Basic architecture is like:\n\n    h_t^ = update_nl(Wx_t + Uh_{t-1} + B_h)\n    h_t = sigmoid(beta)*h_{t-1} + sigmoid(alpha)*h_t^\n\n    W and U can further parameterised into low rank version by\n    W = matmul(W_1, W_2) and U = matmul(U_1, U_2)\n    \'\'\'\n\n    def __init__(self, input_size, hidden_size,\n                 update_nonlinearity=""tanh"", wRank=None, uRank=None,\n                 wSparsity=1.0, uSparsity=1.0, alphaInit=-3.0, betaInit=3.0,\n                 name=""FastRNN""):\n        super(FastRNNCell, self).__init__(input_size, hidden_size,\n                                           None, update_nonlinearity,\n                                           1, 1, 1, wRank, uRank, wSparsity,\n                                           uSparsity)\n\n        self._alphaInit = alphaInit\n        self._betaInit = betaInit\n        if wRank is not None:\n            self._num_W_matrices += 1\n            self._num_weight_matrices[0] = self._num_W_matrices\n        if uRank is not None:\n            self._num_U_matrices += 1\n            self._num_weight_matrices[1] = self._num_U_matrices\n        self._name = name\n\n        if wRank is None:\n            self.W = nn.Parameter(0.1 * torch.randn([input_size, hidden_size]))\n        else:\n            self.W1 = nn.Parameter(0.1 * torch.randn([input_size, wRank]))\n            self.W2 = nn.Parameter(0.1 * torch.randn([wRank, hidden_size]))\n\n        if uRank is None:\n            self.U = nn.Parameter(\n                0.1 * torch.randn([hidden_size, hidden_size]))\n        else:\n            self.U1 = nn.Parameter(0.1 * torch.randn([hidden_size, uRank]))\n            self.U2 = nn.Parameter(0.1 * torch.randn([uRank, hidden_size]))\n\n        self.bias_update = nn.Parameter(torch.ones([1, hidden_size]))\n        self.alpha = nn.Parameter(self._alphaInit * torch.ones([1, 1]))\n        self.beta = nn.Parameter(self._betaInit * torch.ones([1, 1]))\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def cellType(self):\n        return ""FastRNN""\n\n    def forward(self, input, state):\n        if self._wRank is None:\n            wComp = torch.matmul(input, self.W)\n        else:\n            wComp = torch.matmul(\n                torch.matmul(input, self.W1), self.W2)\n\n        if self._uRank is None:\n            uComp = torch.matmul(state, self.U)\n        else:\n            uComp = torch.matmul(\n                torch.matmul(state, self.U1), self.U2)\n\n        pre_comp = wComp + uComp\n\n        c = gen_nonlinearity(pre_comp + self.bias_update,\n                              self._update_nonlinearity)\n        new_h = torch.sigmoid(self.beta) * state + \\\n            torch.sigmoid(self.alpha) * c\n\n        return new_h\n\n    def getVars(self):\n        Vars = []\n        if self._num_W_matrices == 1:\n            Vars.append(self.W)\n        else:\n            Vars.extend([self.W1, self.W2])\n\n        if self._num_U_matrices == 1:\n            Vars.append(self.U)\n        else:\n            Vars.extend([self.U1, self.U2])\n\n        Vars.extend([self.bias_update])\n        Vars.extend([self.alpha, self.beta])\n\n        return Vars\n\n\nclass LSTMLRCell(RNNCell):\n    \'\'\'\n    LR - Low Rank\n    LSTM LR Cell with Both Full Rank and Low Rank Formulations\n    Has multiple activation functions for the gates\n    hidden_size = # hidden units\n\n    gate_nonlinearity = nonlinearity for the gate can be chosen from\n    [tanh, sigmoid, relu, quantTanh, quantSigm]\n    update_nonlinearity = nonlinearity for final rnn update\n    can be chosen from [tanh, sigmoid, relu, quantTanh, quantSigm]\n\n    wRank = rank of all W matrices\n    (creates 5 matrices if not None else creates 4 matrices)\n    uRank = rank of all U matrices\n    (creates 5 matrices if not None else creates 4 matrices)\n\n    LSTM architecture and compression techniques are found in\n    LSTM paper\n\n    Basic architecture:\n\n    f_t = gate_nl(W1x_t + U1h_{t-1} + B_f)\n    i_t = gate_nl(W2x_t + U2h_{t-1} + B_i)\n    C_t^ = update_nl(W3x_t + U3h_{t-1} + B_c)\n    o_t = gate_nl(W4x_t + U4h_{t-1} + B_o)\n    C_t = f_t*C_{t-1} + i_t*C_t^\n    h_t = o_t*update_nl(C_t)\n\n    Wi and Ui can further parameterised into low rank version by\n    Wi = matmul(W, W_i) and Ui = matmul(U, U_i)\n    \'\'\'\n\n    def __init__(self, input_size, hidden_size, gate_nonlinearity=""sigmoid"",\n                 update_nonlinearity=""tanh"", wRank=None, uRank=None,\n                 wSparsity=1.0, uSparsity=1.0, name=""LSTMLR""):\n        super(LSTMLRCell, self).__init__(input_size, hidden_size,\n                                          gate_nonlinearity, update_nonlinearity,\n                                          4, 4, 4, wRank, uRank, wSparsity,\n                                          uSparsity)\n\n        if wRank is not None:\n            self._num_W_matrices += 1\n            self._num_weight_matrices[0] = self._num_W_matrices\n        if uRank is not None:\n            self._num_U_matrices += 1\n            self._num_weight_matrices[1] = self._num_U_matrices\n        self._name = name\n\n        if wRank is None:\n            self.W1 = nn.Parameter(\n                0.1 * torch.randn([input_size, hidden_size]))\n            self.W2 = nn.Parameter(\n                0.1 * torch.randn([input_size, hidden_size]))\n            self.W3 = nn.Parameter(\n                0.1 * torch.randn([input_size, hidden_size]))\n            self.W4 = nn.Parameter(\n                0.1 * torch.randn([input_size, hidden_size]))\n        else:\n            self.W = nn.Parameter(0.1 * torch.randn([input_size, wRank]))\n            self.W1 = nn.Parameter(0.1 * torch.randn([wRank, hidden_size]))\n            self.W2 = nn.Parameter(0.1 * torch.randn([wRank, hidden_size]))\n            self.W3 = nn.Parameter(0.1 * torch.randn([wRank, hidden_size]))\n            self.W4 = nn.Parameter(0.1 * torch.randn([wRank, hidden_size]))\n\n        if uRank is None:\n            self.U1 = nn.Parameter(\n                0.1 * torch.randn([hidden_size, hidden_size]))\n            self.U2 = nn.Parameter(\n                0.1 * torch.randn([hidden_size, hidden_size]))\n            self.U3 = nn.Parameter(\n                0.1 * torch.randn([hidden_size, hidden_size]))\n            self.U4 = nn.Parameter(\n                0.1 * torch.randn([hidden_size, hidden_size]))\n        else:\n            self.U = nn.Parameter(0.1 * torch.randn([hidden_size, uRank]))\n            self.U1 = nn.Parameter(0.1 * torch.randn([uRank, hidden_size]))\n            self.U2 = nn.Parameter(0.1 * torch.randn([uRank, hidden_size]))\n            self.U3 = nn.Parameter(0.1 * torch.randn([uRank, hidden_size]))\n            self.U4 = nn.Parameter(0.1 * torch.randn([uRank, hidden_size]))\n\n        self.bias_f = nn.Parameter(torch.ones([1, hidden_size]))\n        self.bias_i = nn.Parameter(torch.ones([1, hidden_size]))\n        self.bias_c = nn.Parameter(torch.ones([1, hidden_size]))\n        self.bias_o = nn.Parameter(torch.ones([1, hidden_size]))\n\n    @property\n    def gate_nonlinearity(self):\n        return self._gate_nonlinearity\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def cellType(self):\n        return ""LSTMLR""\n\n    def forward(self, input, hiddenStates):\n        (h, c) = hiddenStates\n\n        if self._wRank is None:\n            wComp1 = torch.matmul(input, self.W1)\n            wComp2 = torch.matmul(input, self.W2)\n            wComp3 = torch.matmul(input, self.W3)\n            wComp4 = torch.matmul(input, self.W4)\n        else:\n            wComp1 = torch.matmul(\n                torch.matmul(input, self.W), self.W1)\n            wComp2 = torch.matmul(\n                torch.matmul(input, self.W), self.W2)\n            wComp3 = torch.matmul(\n                torch.matmul(input, self.W), self.W3)\n            wComp4 = torch.matmul(\n                torch.matmul(input, self.W), self.W4)\n\n        if self._uRank is None:\n            uComp1 = torch.matmul(h, self.U1)\n            uComp2 = torch.matmul(h, self.U2)\n            uComp3 = torch.matmul(h, self.U3)\n            uComp4 = torch.matmul(h, self.U4)\n        else:\n            uComp1 = torch.matmul(\n                torch.matmul(h, self.U), self.U1)\n            uComp2 = torch.matmul(\n                torch.matmul(h, self.U), self.U2)\n            uComp3 = torch.matmul(\n                torch.matmul(h, self.U), self.U3)\n            uComp4 = torch.matmul(\n                torch.matmul(h, self.U), self.U4)\n        pre_comp1 = wComp1 + uComp1\n        pre_comp2 = wComp2 + uComp2\n        pre_comp3 = wComp3 + uComp3\n        pre_comp4 = wComp4 + uComp4\n\n        i = gen_nonlinearity(pre_comp1 + self.bias_i,\n                              self._gate_nonlinearity)\n        f = gen_nonlinearity(pre_comp2 + self.bias_f,\n                              self._gate_nonlinearity)\n        o = gen_nonlinearity(pre_comp4 + self.bias_o,\n                              self._gate_nonlinearity)\n\n        c_ = gen_nonlinearity(pre_comp3 + self.bias_c,\n                               self._update_nonlinearity)\n\n        new_c = f * c + i * c_\n        new_h = o * gen_nonlinearity(new_c, self._update_nonlinearity)\n        return new_h, new_c\n\n    def getVars(self):\n        Vars = []\n        if self._num_W_matrices == 4:\n            Vars.extend([self.W1, self.W2, self.W3, self.W4])\n        else:\n            Vars.extend([self.W, self.W1, self.W2, self.W3, self.W4])\n\n        if self._num_U_matrices == 4:\n            Vars.extend([self.U1, self.U2, self.U3, self.U4])\n        else:\n            Vars.extend([self.U, self.U1, self.U2, self.U3, self.U4])\n\n        Vars.extend([self.bias_f, self.bias_i, self.bias_c, self.bias_o])\n\n        return Vars\n\n\nclass GRULRCell(RNNCell):\n    \'\'\'\n    GRU LR Cell with Both Full Rank and Low Rank Formulations\n    Has multiple activation functions for the gates\n    hidden_size = # hidden units\n\n    gate_nonlinearity = nonlinearity for the gate can be chosen from\n    [tanh, sigmoid, relu, quantTanh, quantSigm]\n    update_nonlinearity = nonlinearity for final rnn update\n    can be chosen from [tanh, sigmoid, relu, quantTanh, quantSigm]\n\n    wRank = rank of W matrix\n    (creates 4 matrices if not None else creates 3 matrices)\n    uRank = rank of U matrix\n    (creates 4 matrices if not None else creates 3 matrices)\n\n    GRU architecture and compression techniques are found in\n    GRU(LINK) paper\n\n    Basic architecture is like:\n\n    r_t = gate_nl(W1x_t + U1h_{t-1} + B_r)\n    z_t = gate_nl(W2x_t + U2h_{t-1} + B_g)\n    h_t^ = update_nl(W3x_t + r_t*U3(h_{t-1}) + B_h)\n    h_t = z_t*h_{t-1} + (1-z_t)*h_t^\n\n    Wi and Ui can further parameterised into low rank version by\n    Wi = matmul(W, W_i) and Ui = matmul(U, U_i)\n    \'\'\'\n\n    def __init__(self, input_size, hidden_size, gate_nonlinearity=""sigmoid"",\n                 update_nonlinearity=""tanh"", wRank=None, uRank=None,\n                 wSparsity=1.0, uSparsity=1.0, name=""GRULR""):\n        super(GRULRCell, self).__init__(input_size, hidden_size,\n                                           gate_nonlinearity, update_nonlinearity,\n                                           3, 3, 3, wRank, uRank, wSparsity,\n                                           uSparsity)\n\n        if wRank is not None:\n            self._num_W_matrices += 1\n            self._num_weight_matrices[0] = self._num_W_matrices\n        if uRank is not None:\n            self._num_U_matrices += 1\n            self._num_weight_matrices[1] = self._num_U_matrices\n        self._name = name\n\n        if wRank is None:\n            self.W1 = nn.Parameter(\n                0.1 * torch.randn([input_size, hidden_size]))\n            self.W2 = nn.Parameter(\n                0.1 * torch.randn([input_size, hidden_size]))\n            self.W3 = nn.Parameter(\n                0.1 * torch.randn([input_size, hidden_size]))\n        else:\n            self.W = nn.Parameter(0.1 * torch.randn([input_size, wRank]))\n            self.W1 = nn.Parameter(0.1 * torch.randn([wRank, hidden_size]))\n            self.W2 = nn.Parameter(0.1 * torch.randn([wRank, hidden_size]))\n            self.W3 = nn.Parameter(0.1 * torch.randn([wRank, hidden_size]))\n\n        if uRank is None:\n            self.U1 = nn.Parameter(\n                0.1 * torch.randn([hidden_size, hidden_size]))\n            self.U2 = nn.Parameter(\n                0.1 * torch.randn([hidden_size, hidden_size]))\n            self.U3 = nn.Parameter(\n                0.1 * torch.randn([hidden_size, hidden_size]))\n        else:\n            self.U = nn.Parameter(0.1 * torch.randn([hidden_size, uRank]))\n            self.U1 = nn.Parameter(0.1 * torch.randn([uRank, hidden_size]))\n            self.U2 = nn.Parameter(0.1 * torch.randn([uRank, hidden_size]))\n            self.U3 = nn.Parameter(0.1 * torch.randn([uRank, hidden_size]))\n\n        self.bias_r = nn.Parameter(torch.ones([1, hidden_size]))\n        self.bias_gate = nn.Parameter(torch.ones([1, hidden_size]))\n        self.bias_update = nn.Parameter(torch.ones([1, hidden_size]))\n        self._device = self.bias_update.device\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def cellType(self):\n        return ""GRULR""\n\n    def forward(self, input, state):\n        if self._wRank is None:\n            wComp1 = torch.matmul(input, self.W1)\n            wComp2 = torch.matmul(input, self.W2)\n            wComp3 = torch.matmul(input, self.W3)\n        else:\n            wComp1 = torch.matmul(\n                torch.matmul(input, self.W), self.W1)\n            wComp2 = torch.matmul(\n                torch.matmul(input, self.W), self.W2)\n            wComp3 = torch.matmul(\n                torch.matmul(input, self.W), self.W3)\n\n        if self._uRank is None:\n            uComp1 = torch.matmul(state, self.U1)\n            uComp2 = torch.matmul(state, self.U2)\n        else:\n            uComp1 = torch.matmul(\n                torch.matmul(state, self.U), self.U1)\n            uComp2 = torch.matmul(\n                torch.matmul(state, self.U), self.U2)\n\n        pre_comp1 = wComp1 + uComp1\n        pre_comp2 = wComp2 + uComp2\n\n        r = gen_nonlinearity(pre_comp1 + self.bias_r,\n                              self._gate_nonlinearity)\n        z = gen_nonlinearity(pre_comp2 + self.bias_gate,\n                              self._gate_nonlinearity)\n\n        if self._uRank is None:\n            pre_comp3 = wComp3 + torch.matmul(r * state, self.U3)\n        else:\n            pre_comp3 = wComp3 + \\\n                torch.matmul(torch.matmul(r * state, self.U), self.U3)\n\n        c = gen_nonlinearity(pre_comp3 + self.bias_update,\n                              self._update_nonlinearity)\n\n        new_h = z * state + (1.0 - z) * c\n        return new_h\n\n    def getVars(self):\n        Vars = []\n        if self._num_W_matrices == 3:\n            Vars.extend([self.W1, self.W2, self.W3])\n        else:\n            Vars.extend([self.W, self.W1, self.W2, self.W3])\n\n        if self._num_U_matrices == 3:\n            Vars.extend([self.U1, self.U2, self.U3])\n        else:\n            Vars.extend([self.U, self.U1, self.U2, self.U3])\n\n        Vars.extend([self.bias_r, self.bias_gate, self.bias_update])\n\n        return Vars\n\n\nclass UGRNNLRCell(RNNCell):\n    \'\'\'\n    UGRNN LR Cell with Both Full Rank and Low Rank Formulations\n    Has multiple activation functions for the gates\n    hidden_size = # hidden units\n\n    gate_nonlinearity = nonlinearity for the gate can be chosen from\n    [tanh, sigmoid, relu, quantTanh, quantSigm]\n    update_nonlinearity = nonlinearity for final rnn update\n    can be chosen from [tanh, sigmoid, relu, quantTanh, quantSigm]\n\n    wRank = rank of W matrix\n    (creates 3 matrices if not None else creates 2 matrices)\n    uRank = rank of U matrix\n    (creates 3 matrices if not None else creates 2 matrices)\n\n    UGRNN architecture and compression techniques are found in\n    UGRNN(LINK) paper\n\n    Basic architecture is like:\n\n    z_t = gate_nl(W1x_t + U1h_{t-1} + B_g)\n    h_t^ = update_nl(W1x_t + U1h_{t-1} + B_h)\n    h_t = z_t*h_{t-1} + (1-z_t)*h_t^\n\n    Wi and Ui can further parameterised into low rank version by\n    Wi = matmul(W, W_i) and Ui = matmul(U, U_i)\n    \'\'\'\n\n    def __init__(self, input_size, hidden_size, gate_nonlinearity=""sigmoid"",\n                 update_nonlinearity=""tanh"", wRank=None, uRank=None,\n                 wSparsity=1.0, uSparsity=1.0, name=""UGRNNLR""):\n        super(UGRNNLRCell, self).__init__(input_size, hidden_size,\n                                          gate_nonlinearity, update_nonlinearity,\n                                          2, 2, 2, wRank, uRank, wSparsity, uSparsity)\n\n        if wRank is not None:\n            self._num_W_matrices += 1\n            self._num_weight_matrices[0] = self._num_W_matrices\n        if uRank is not None:\n            self._num_U_matrices += 1\n            self._num_weight_matrices[1] = self._num_U_matrices\n        self._name = name\n\n        if wRank is None:\n            self.W1 = nn.Parameter(\n                0.1 * torch.randn([input_size, hidden_size]))\n            self.W2 = nn.Parameter(\n                0.1 * torch.randn([input_size, hidden_size]))\n        else:\n            self.W = nn.Parameter(0.1 * torch.randn([input_size, wRank]))\n            self.W1 = nn.Parameter(0.1 * torch.randn([wRank, hidden_size]))\n            self.W2 = nn.Parameter(0.1 * torch.randn([wRank, hidden_size]))\n\n        if uRank is None:\n            self.U1 = nn.Parameter(\n                0.1 * torch.randn([hidden_size, hidden_size]))\n            self.U2 = nn.Parameter(\n                0.1 * torch.randn([hidden_size, hidden_size]))\n        else:\n            self.U = nn.Parameter(0.1 * torch.randn([hidden_size, uRank]))\n            self.U1 = nn.Parameter(0.1 * torch.randn([uRank, hidden_size]))\n            self.U2 = nn.Parameter(0.1 * torch.randn([uRank, hidden_size]))\n\n        self.bias_gate = nn.Parameter(torch.ones([1, hidden_size]))\n        self.bias_update = nn.Parameter(torch.ones([1, hidden_size]))\n        self._device = self.bias_update.device\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def cellType(self):\n        return ""UGRNNLR""\n\n    def forward(self, input, state):\n        if self._wRank is None:\n            wComp1 = torch.matmul(input, self.W1)\n            wComp2 = torch.matmul(input, self.W2)\n        else:\n            wComp1 = torch.matmul(\n                torch.matmul(input, self.W), self.W1)\n            wComp2 = torch.matmul(\n                torch.matmul(input, self.W), self.W2)\n\n        if self._uRank is None:\n            uComp1 = torch.matmul(state, self.U1)\n            uComp2 = torch.matmul(state, self.U2)\n        else:\n            uComp1 = torch.matmul(\n                torch.matmul(state, self.U), self.U1)\n            uComp2 = torch.matmul(\n                torch.matmul(state, self.U), self.U2)\n\n        pre_comp1 = wComp1 + uComp1\n        pre_comp2 = wComp2 + uComp2\n\n        z = gen_nonlinearity(pre_comp1 + self.bias_gate,\n                              self._gate_nonlinearity)\n        c = gen_nonlinearity(pre_comp2 + self.bias_update,\n                              self._update_nonlinearity)\n\n        new_h = z * state + (1.0 - z) * c\n        return new_h\n\n    def getVars(self):\n        Vars = []\n        if self._num_W_matrices == 2:\n            Vars.extend([self.W1, self.W2])\n        else:\n            Vars.extend([self.W, self.W1, self.W2])\n\n        if self._num_U_matrices == 2:\n            Vars.extend([self.U1, self.U2])\n        else:\n            Vars.extend([self.U, self.U1, self.U2])\n\n        Vars.extend([self.bias_gate, self.bias_update])\n\n        return Vars\n\n\nclass BaseRNN(nn.Module):\n    \'\'\'\n    Generic equivalent of static_rnn in tf\n    Used to unroll all the cell written in this file\n    We assume batch_first to be False by default \n    (following the convention in pytorch) ie.,\n    [timeSteps, batchSize, inputDims] else\n    [batchSize, timeSteps, inputDims]\n    \'\'\'\n\n    def __init__(self, cell: RNNCell, batch_first=False):\n        super(BaseRNN, self).__init__()\n        self._RNNCell = cell\n        self._batch_first = batch_first\n\n    def getVars(self):\n        return self._RNNCell.getVars()\n\n    def forward(self, input, hiddenState=None,\n                cellState=None):\n        self.device = input.device\n        hiddenStates = torch.zeros(\n                [input.shape[0], input.shape[1],\n                 self._RNNCell.output_size]).to(self.device)\n        if hiddenState is None:\n                hiddenState = torch.zeros(\n                    [input.shape[0] if self._batch_first else input.shape[1],\n                    self._RNNCell.output_size]).to(self.device)\n\n        if self._batch_first is True:\n            if self._RNNCell.cellType == ""LSTMLR"":\n                cellStates = torch.zeros(\n                    [input.shape[0], input.shape[1],\n                     self._RNNCell.output_size]).to(self.device)\n                if cellState is None:\n                    cellState = torch.zeros(\n                        [input.shape[0], self._RNNCell.output_size]).to(self.device)\n                for i in range(0, input.shape[1]):\n                    hiddenState, cellState = self._RNNCell(\n                        input[:, i, :], (hiddenState, cellState))\n                    hiddenStates[:, i, :] = hiddenState\n                    cellStates[:, i, :] = cellState\n                return hiddenStates, cellStates\n            else:\n                for i in range(0, input.shape[1]):\n                    hiddenState = self._RNNCell(input[:, i, :], hiddenState)\n                    hiddenStates[:, i, :] = hiddenState\n                return hiddenStates\n        else:\n            if self._RNNCell.cellType == ""LSTMLR"":\n                cellStates = torch.zeros(\n                    [input.shape[0], input.shape[1],\n                     self._RNNCell.output_size]).to(self.device)\n                if cellState is None:\n                    cellState = torch.zeros(\n                        [input.shape[1], self._RNNCell.output_size]).to(self.device)\n                for i in range(0, input.shape[0]):\n                    hiddenState, cellState = self._RNNCell(\n                        input[i, :, :], (hiddenState, cellState))\n                    hiddenStates[i, :, :] = hiddenState\n                    cellStates[i, :, :] = cellState\n                return hiddenStates, cellStates\n            else:\n                for i in range(0, input.shape[0]):\n                    hiddenState = self._RNNCell(input[i, :, :], hiddenState)\n                    hiddenStates[i, :, :] = hiddenState\n                return hiddenStates\n\n\nclass LSTM(nn.Module):\n    """"""Equivalent to nn.LSTM using LSTMLRCell""""""\n\n    def __init__(self, input_size, hidden_size, gate_nonlinearity=""sigmoid"",\n                 update_nonlinearity=""tanh"", wRank=None, uRank=None,\n                 wSparsity=1.0, uSparsity=1.0, batch_first=False):\n        super(LSTM, self).__init__()\n        self.cell = LSTMLRCell(input_size, hidden_size,\n                               gate_nonlinearity=gate_nonlinearity,\n                               update_nonlinearity=update_nonlinearity,\n                               wRank=wRank, uRank=uRank,\n                               wSparsity=wSparsity, uSparsity=uSparsity)\n        self.unrollRNN = BaseRNN(self.cell, batch_first=batch_first)\n\n    def forward(self, input, hiddenState=None, cellState=None):\n        return self.unrollRNN(input, hiddenState, cellState)\n\n\nclass GRU(nn.Module):\n    """"""Equivalent to nn.GRU using GRULRCell""""""\n\n    def __init__(self, input_size, hidden_size, gate_nonlinearity=""sigmoid"",\n                 update_nonlinearity=""tanh"", wRank=None, uRank=None,\n                 wSparsity=1.0, uSparsity=1.0, batch_first=False):\n        super(GRU, self).__init__()\n        self.cell = GRULRCell(input_size, hidden_size,\n                              gate_nonlinearity=gate_nonlinearity,\n                              update_nonlinearity=update_nonlinearity,\n                              wRank=wRank, uRank=uRank,\n                              wSparsity=wSparsity, uSparsity=uSparsity)\n        self.unrollRNN = BaseRNN(self.cell, batch_first=batch_first)\n\n    def forward(self, input, hiddenState=None, cellState=None):\n        return self.unrollRNN(input, hiddenState, cellState)\n\n\nclass UGRNN(nn.Module):\n    """"""Equivalent to nn.UGRNN using UGRNNLRCell""""""\n\n    def __init__(self, input_size, hidden_size, gate_nonlinearity=""sigmoid"",\n                 update_nonlinearity=""tanh"", wRank=None, uRank=None,\n                 wSparsity=1.0, uSparsity=1.0, batch_first=False):\n        super(UGRNN, self).__init__()\n        self.cell = UGRNNLRCell(input_size, hidden_size,\n                                gate_nonlinearity=gate_nonlinearity,\n                                update_nonlinearity=update_nonlinearity,\n                                wRank=wRank, uRank=uRank,\n                                wSparsity=wSparsity, uSparsity=uSparsity)\n        self.unrollRNN = BaseRNN(self.cell, batch_first=batch_first)\n\n    def forward(self, input, hiddenState=None, cellState=None):\n        return self.unrollRNN(input, hiddenState, cellState)\n\n\nclass FastRNN(nn.Module):\n    """"""Equivalent to nn.FastRNN using FastRNNCell""""""\n\n    def __init__(self, input_size, hidden_size, gate_nonlinearity=""sigmoid"",\n                 update_nonlinearity=""tanh"", wRank=None, uRank=None,\n                 wSparsity=1.0, uSparsity=1.0, alphaInit=-3.0, betaInit=3.0, batch_first=False):\n        super(FastRNN, self).__init__()\n        self.cell = FastRNNCell(input_size, hidden_size,\n                                gate_nonlinearity=gate_nonlinearity,\n                                update_nonlinearity=update_nonlinearity,\n                                wRank=wRank, uRank=uRank,\n                                wSparsity=wSparsity, uSparsity=uSparsity,\n                                alphaInit=alphaInit, betaInit=betaInit)\n        self.unrollRNN = BaseRNN(self.cell, batch_first=batch_first)\n\n    def forward(self, input, hiddenState=None, cellState=None):\n        return self.unrollRNN(input, hiddenState, cellState)\n\n\nclass FastGRNN(nn.Module):\n    """"""Equivalent to nn.FastGRNN using FastGRNNCell""""""\n\n    def __init__(self, input_size, hidden_size, gate_nonlinearity=""sigmoid"",\n                 update_nonlinearity=""tanh"", wRank=None, uRank=None,\n                 wSparsity=1.0, uSparsity=1.0, zetaInit=1.0, nuInit=-4.0,\n                 batch_first=False):\n        super(FastGRNN, self).__init__()\n        self.cell = FastGRNNCell(input_size, hidden_size,\n                                 gate_nonlinearity=gate_nonlinearity,\n                                 update_nonlinearity=update_nonlinearity,\n                                 wRank=wRank, uRank=uRank,\n                                 wSparsity=wSparsity, uSparsity=uSparsity,\n                                 zetaInit=zetaInit, nuInit=nuInit)\n        self.unrollRNN = BaseRNN(self.cell, batch_first=batch_first)\n\n    def getVars(self):\n        return self.unrollRNN.getVars()\n\n    def forward(self, input, hiddenState=None, cellState=None):\n        return self.unrollRNN(input, hiddenState, cellState)\n\nclass FastGRNNCUDA(nn.Module):\n    """"""\n        Unrolled implementation of the FastGRNNCUDACell\n        Note: update_nonlinearity is fixed to tanh, only gate_nonlinearity\n        is configurable.\n    """"""\n    def __init__(self, input_size, hidden_size, gate_nonlinearity=""sigmoid"",\n                 update_nonlinearity=""tanh"", wRank=None, uRank=None, \n                 wSparsity=1.0, uSparsity=1.0, zetaInit=1.0, nuInit=-4.0,\n                 batch_first=False, name=""FastGRNNCUDA""):\n        super(FastGRNNCUDA, self).__init__()\n        if utils.findCUDA() is None:\n            raise Exception(\'FastGRNNCUDA is supported only on GPU devices.\')\n        NON_LINEARITY = {""sigmoid"": 0, ""relu"": 1, ""tanh"": 2}\n        self._input_size = input_size\n        self._hidden_size = hidden_size\n        self._zetaInit = zetaInit\n        self._nuInit = nuInit\n        self._name = name\n        self._num_W_matrices = 1\n        self._num_U_matrices = 1\n        self._num_biases = 2\n        self._num_weight_matrices = [self._num_W_matrices, self._num_U_matrices, self._num_biases]\n        self._wRank = wRank\n        self._uRank = uRank\n        self._wSparsity = wSparsity\n        self._uSparsity = uSparsity\n        self.oldmats = []\n        self.device = torch.device(""cuda"")\n        self.batch_first = batch_first\n        if wRank is not None:\n            self._num_W_matrices += 1\n            self._num_weight_matrices[0] = self._num_W_matrices\n        if uRank is not None:\n            self._num_U_matrices += 1\n            self._num_weight_matrices[1] = self._num_U_matrices\n        self._name = name\n\n        if wRank is None:\n            self.W = nn.Parameter(0.1 * torch.randn([hidden_size, input_size], device=self.device))\n            self.W1 = torch.empty(0)\n            self.W2 = torch.empty(0)\n        else:\n            self.W = torch.empty(0)\n            self.W1 = nn.Parameter(0.1 * torch.randn([wRank, input_size], device=self.device))\n            self.W2 = nn.Parameter(0.1 * torch.randn([hidden_size, wRank], device=self.device))\n\n        if uRank is None:\n            self.U = nn.Parameter(0.1 * torch.randn([hidden_size, hidden_size], device=self.device))\n            self.U1 = torch.empty(0)\n            self.U2 = torch.empty(0)\n        else:\n            self.U = torch.empty(0)\n            self.U1 = nn.Parameter(0.1 * torch.randn([uRank, hidden_size], device=self.device))\n            self.U2 = nn.Parameter(0.1 * torch.randn([hidden_size, uRank], device=self.device))\n\n        self._gate_non_linearity = NON_LINEARITY[gate_nonlinearity]\n\n        self.bias_gate = nn.Parameter(torch.ones([1, hidden_size], device=self.device))\n        self.bias_update = nn.Parameter(torch.ones([1, hidden_size], device=self.device))\n        self.zeta = nn.Parameter(self._zetaInit * torch.ones([1, 1], device=self.device))\n        self.nu = nn.Parameter(self._nuInit * torch.ones([1, 1], device=self.device))\n\n    def forward(self, input, hiddenState=None, cell_state=None):\n        \'\'\'\n            input: [timesteps, batch, features]; hiddenState: [batch, state_size]\n            hiddenState is set to zeros if not provided. \n        \'\'\'\n        if self.batch_first is True:\n            input = input.transpose(0, 1).contiguous()\n        if not input.is_cuda:\n            input = input.to(self.device)\n        if hiddenState is None:\n            hiddenState = torch.zeros(\n                [input.shape[1], self._hidden_size]).to(self.device)\n        if not hiddenState.is_cuda:\n            hiddenState = hiddenState.to(self.device)\n        result = FastGRNNUnrollFunction.apply(input, self.bias_gate, self.bias_update, self.zeta, self.nu, hiddenState,\n            self.W, self.U, self.W1, self.W2, self.U1, self.U2, self._gate_non_linearity)\n        if self.batch_first is True:\n            return result.transpose(0, 1)\n        else:\n            return result\n\n    def getVars(self):\n        Vars = []\n        if self._num_W_matrices == 1:\n            Vars.append(self.W)\n        else:\n            Vars.extend([self.W1, self.W2])\n\n        if self._num_U_matrices == 1:\n            Vars.append(self.U)\n        else:\n            Vars.extend([self.U1, self.U2])\n\n        Vars.extend([self.bias_gate, self.bias_update, self.zeta, self.nu])\n        return Vars\n\n    def get_model_size(self):\n        \'\'\'\n\t\tFunction to get aimed model size\n\t\t\'\'\'\n        mats = self.getVars()\n        endW = self._num_W_matrices\n        endU = endW + self._num_U_matrices\n\n        totalnnz = 2  # For Zeta and Nu\n        for i in range(0, endW):\n            device = mats[i].device\n            totalnnz += utils.countNNZ(mats[i].cpu(), self._wSparsity)\n            mats[i].to(device)\n        for i in range(endW, endU):\n            device = mats[i].device\n            totalnnz += utils.countNNZ(mats[i].cpu(), self._uSparsity)\n            mats[i].to(device)\n        for i in range(endU, len(mats)):\n            device = mats[i].device\n            totalnnz += utils.countNNZ(mats[i].cpu(), False)\n            mats[i].to(device)\n        return totalnnz * 4\n\n    def copy_previous_UW(self):\n        mats = self.getVars()\n        num_mats = self._num_W_matrices + self._num_U_matrices\n        if len(self.oldmats) != num_mats:\n            for i in range(num_mats):\n                self.oldmats.append(torch.FloatTensor())\n        for i in range(num_mats):\n            self.oldmats[i] = torch.FloatTensor(mats[i].detach().clone().to(mats[i].device))\n\n    def sparsify(self):\n        mats = self.getVars()\n        endW = self._num_W_matrices\n        endU = endW + self._num_U_matrices\n        for i in range(0, endW):\n            mats[i] = utils.hardThreshold(mats[i], self._wSparsity)\n        for i in range(endW, endU):\n            mats[i] = utils.hardThreshold(mats[i], self._uSparsity)\n        self.copy_previous_UW()\n\n    def sparsifyWithSupport(self):\n        mats = self.getVars()\n        endU = self._num_W_matrices + self._num_U_matrices\n        for i in range(0, endU):\n            mats[i] = utils.supportBasedThreshold(mats[i], self.oldmats[i])\n\nclass SRNN2(nn.Module):\n\n    def __init__(self, inputDim, outputDim, hiddenDim0, hiddenDim1, cellType,\n                 dropoutProbability0 = None, dropoutProbability1 = None,\n                 **cellArgs):\n        \'\'\'\n        A 2 Layer Shallow RNN.\n\n        inputDim: Input data\'s feature dimension.\n        hiddenDim0: Hidden state dimension of the lower layer RNN cell.\n        hiddenDim1: Hidden state dimension of the second layer RNN cell.\n        cellType: The type of RNN cell to use. Options are [\'LSTM\', \'FastRNNCell\',\n        \'FastGRNNCell\', \'GRULRCell\']\n        \'\'\'\n        super(SRNN2, self).__init__()\n\n        # Create two RNN Cells\n        self.inputDim = inputDim\n        self.hiddenDim0 = hiddenDim0\n        self.hiddenDim1 = hiddenDim1\n        self.outputDim = outputDim\n        self.dropoutProbability0 = dropoutProbability0\n        self.dropoutProbability1 = dropoutProbability1\n        if dropoutProbability0 != None:\n            assert 0 < dropoutProbability0 <= 1.0\n        if dropoutProbability1 != None:\n            assert 0 < dropoutProbability1 <= 1.0\n        # Setting batch_first = False to ensure compatibility of parameters across nn.LSTM and the\n        # other low-rank implementations\n        self.cellArgs = {\n            \'batch_first\': False\n        }\n        self.cellArgs.update(cellArgs)\n        supportedCells = [\'LSTM\', \'FastRNNCell\', \'FastGRNNCell\', \'GRULRCell\']\n        assert cellType in supportedCells, \'Currently supported cells: %r\' % supportedCells\n        self.cellType = cellType\n\n        if self.cellType == \'LSTM\':\n            self.rnnClass = nn.LSTM\n        elif self.cellType == \'FastRNNCell\':\n            self.rnnClass = FastRNN\n        elif self.cellType == \'FastGRNNCell\':\n            self.rnnClass = FastGRNN\n        else:\n            self.rnnClass = GRU\n\n        self.rnn0 = self.rnnClass(input_size=inputDim, hidden_size=hiddenDim0, **self.cellArgs)\n        self.rnn1 = self.rnnClass(input_size=hiddenDim0, hidden_size=hiddenDim1, **self.cellArgs)\n        self.W = torch.randn([self.hiddenDim1, self.outputDim])\n        self.W = nn.Parameter(self.W)\n        self.B = torch.randn([self.outputDim])\n        self.B = nn.Parameter(self.B)\n\n    def getBrickedData(self, x, brickSize):\n        \'\'\'\n        Takes x of shape [timeSteps, batchSize, featureDim] and returns bricked\n        x of shape [numBricks, brickSize, batchSize, featureDim] by chunking\n        along 0-th axes.\n        \'\'\'\n        timeSteps = list(x.size())[0]\n        numSplits = int(timeSteps / brickSize)\n        batchSize = list(x.size())[1]\n        featureDim = list(x.size())[2]\n        numBricks = int(timeSteps/brickSize)\n        eqlen = numSplits * brickSize\n        x = x[:eqlen]\n        x_bricked = torch.split(x, numSplits, dim = 0)\n        x_bricked_batched = torch.cat(x_bricked)\n        x_bricked_batched = torch.reshape(x_bricked_batched, (numBricks,brickSize,batchSize,featureDim))\n        return x_bricked_batched\n\n    def forward(self, x, brickSize):\n        \'\'\'\n        x: Input data in numpy. Expected to be a 3D tensor  with shape\n            [timeStep, batchSize, featureDim]. Note that this is different from\n            the convention followed in the TF codebase.\n        brickSize: The brick size for the lower dimension. The input data will\n            be divided into bricks along the timeStep axis (axis=0) internally\n            and fed into the lowest layer RNN. Note that if the last brick has\n            fewer than \'brickSize\' steps, it will be ignored (no internal\n            padding is done).\n        \'\'\'\n        assert x.ndimension() == 3\n        assert list(x.size())[2] == self.inputDim\n        x_bricks = self.getBrickedData(x, brickSize)\n        # x bricks: [numBricks, brickSize, batchSize, featureDim]\n        x_bricks = x_bricks.permute(1,0,2,3)\n        # x bricks: [brickSize, numBricks, batchSize, featureDim]\n        oldShape = list(x_bricks.size())\n        x_bricks = torch.reshape(x_bricks, [oldShape[0], oldShape[1] * oldShape[2], oldShape[3]])\n        # x bricks: [brickSize, numBricks * batchSize, featureDim]\n        # x_bricks = torch.Tensor(x_bricks)\n\n        self.dropoutLayer0 = None\n        self.dropoutLayer1 = None\n\n        if self.cellType == \'LSTM\':\n            hidd0, out0 = self.rnn0(x_bricks)\n        else:\n            hidd0 = self.rnn0(x_bricks)\n\n        if self.dropoutProbability0 != None:\n            self.dropoutLayer0 = nn.Dropout(p=self.dropoutProbability0)\n            hidd0 = self.dropoutLayer0(hidd0)\n        hidd0 = torch.squeeze(hidd0[-1])\n        # [numBricks * batchSize, hiddenDim0]\n        inp1 = hidd0.view(oldShape[1], oldShape[2], self.hiddenDim0)\n        # [numBricks, batchSize, hiddenDim0]\n        if self.cellType == \'LSTM\':\n            hidd1, out1 = self.rnn1(inp1)\n        else:\n            hidd1 = self.rnn1(inp1)\n        if self.dropoutProbability1 != None:\n            self.dropoutLayer1 = nn.Dropout(p=self.dropoutProbability1)\n            hidd1 = self.dropoutLayer1(hidd1)\n        hidd1 = torch.squeeze(hidd1[-1])\n        out = torch.matmul(hidd1, self.W) + self.B\n        return out\n\nclass FastGRNNFunction(Function):\n    @staticmethod\n    def forward(ctx, input, bias_gate, bias_update, zeta, nu, old_h, w, u, w1, w2, u1, u2, gate_non_linearity):\n        outputs = fastgrnn_cuda.forward(input, w, u, bias_gate, bias_update, zeta, nu, old_h, gate_non_linearity, w1, w2, u1, u2)\n        new_h = outputs[0]\n        variables = [input, old_h, zeta, nu, w, u] + outputs[1:] + [w1, w2, u1, u2]\n        ctx.save_for_backward(*variables)\n        ctx.non_linearity = gate_non_linearity\n        return new_h\n\n    @staticmethod\n    def backward(ctx, grad_h):\n        outputs = fastgrnn_cuda.backward(\n            grad_h.contiguous(), *ctx.saved_variables, ctx.non_linearity)\n        return tuple(outputs + [None])\n\nclass FastGRNNUnrollFunction(Function):\n    @staticmethod\n    def forward(ctx, input, bias_gate, bias_update, zeta, nu, old_h, w, u, w1, w2, u1, u2, gate_non_linearity):\n        outputs = fastgrnn_cuda.forward_unroll(input, w, u, bias_gate, bias_update, zeta, nu, old_h, gate_non_linearity, w1, w2, u1, u2)\n        hidden_states = outputs[0]\n        variables = [input, hidden_states, zeta, nu, w, u] + outputs[1:] + [old_h, w1, w2, u1, u2]\n        ctx.save_for_backward(*variables)\n        ctx.gate_non_linearity = gate_non_linearity\n        return hidden_states\n\n    @staticmethod\n    def backward(ctx, grad_h):\n        outputs = fastgrnn_cuda.backward_unroll(\n            grad_h.contiguous(), *ctx.saved_variables, ctx.gate_non_linearity)\n        return tuple(outputs + [None])\n'"
pytorch/edgeml_pytorch/trainer/__init__.py,0,b''
pytorch/edgeml_pytorch/trainer/bonsaiTrainer.py,18,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport numpy as np\nimport os\nimport sys\nimport edgeml_pytorch.utils as utils\n\n\nclass BonsaiTrainer:\n\n    def __init__(self, bonsaiObj, lW, lT, lV, lZ, sW, sT, sV, sZ,\n                 learningRate, useMCHLoss=False, outFile=None, device=None):\n        \'\'\'\n        bonsaiObj - Initialised Bonsai Object and Graph\n        lW, lT, lV and lZ are regularisers to Bonsai Params\n        sW, sT, sV and sZ are sparsity factors to Bonsai Params\n        learningRate - learningRate for optimizer\n        useMCHLoss - For choice between HingeLoss vs CrossEntropy\n        useMCHLoss - True - MultiClass - multiClassHingeLoss\n        useMCHLoss - False - MultiClass - crossEntropyLoss\n        \'\'\'\n\n        self.bonsaiObj = bonsaiObj\n\n        self.lW = lW\n        self.lV = lV\n        self.lT = lT\n        self.lZ = lZ\n\n        self.sW = sW\n        self.sV = sV\n        self.sT = sT\n        self.sZ = sZ\n\n        if device is None:\n            self.device = ""cpu""\n        else:\n            self.device = device\n\n        self.useMCHLoss = useMCHLoss\n\n        if outFile is not None:\n            print(""Outfile : "", outFile)\n            self.outFile = open(outFile, \'w\')\n        else:\n            self.outFile = sys.stdout\n\n        self.learningRate = learningRate\n\n        self.assertInit()\n\n        self.optimizer = self.optimizer()\n\n        if self.sW > 0.99 and self.sV > 0.99 and self.sZ > 0.99 and self.sT > 0.99:\n            self.isDenseTraining = True\n        else:\n            self.isDenseTraining = False\n\n    def loss(self, logits, labels):\n        \'\'\'\n        Loss function for given Bonsai Obj\n        \'\'\'\n        regLoss = 0.5 * (self.lZ * (torch.norm(self.bonsaiObj.Z)**2) +\n                         self.lW * (torch.norm(self.bonsaiObj.W)**2) +\n                         self.lV * (torch.norm(self.bonsaiObj.V)**2) +\n                         self.lT * (torch.norm(self.bonsaiObj.T))**2)\n\n        if (self.bonsaiObj.numClasses > 2):\n            if self.useMCHLoss is True:\n                marginLoss = utils.multiClassHingeLoss(logits, labels)\n            else:\n                marginLoss = utils.crossEntropyLoss(logits, labels)\n            loss = marginLoss + regLoss\n        else:\n            marginLoss = utils.binaryHingeLoss(logits, labels)\n            loss = marginLoss + regLoss\n\n        return loss, marginLoss, regLoss\n\n    def optimizer(self):\n        \'\'\'\n        Optimizer for Bonsai Params\n        \'\'\'\n        optimizer = torch.optim.Adam(\n            self.bonsaiObj.parameters(), lr=self.learningRate)\n\n        return optimizer\n\n    def accuracy(self, logits, labels):\n        \'\'\'\n        Accuracy fucntion to evaluate accuracy when needed\n        \'\'\'\n        if (self.bonsaiObj.numClasses > 2):\n            correctPredictions = (logits.argmax(dim=1) == labels.argmax(dim=1))\n            accuracy = torch.mean(correctPredictions.float())\n        else:\n            pred = (torch.cat((torch.zeros(logits.shape),\n                               logits), 1)).argmax(dim=1)\n            accuracy = torch.mean((labels.view(-1).long() == pred).float())\n\n        return accuracy\n\n    def runHardThrsd(self):\n        \'\'\'\n        Function to run the IHT routine on Bonsai Obj\n        \'\'\'\n        currW = self.bonsaiObj.W.data\n        currV = self.bonsaiObj.V.data\n        currZ = self.bonsaiObj.Z.data\n        currT = self.bonsaiObj.T.data\n\n        __thrsdW = utils.hardThreshold(currW.cpu(), self.sW)\n        __thrsdV = utils.hardThreshold(currV.cpu(), self.sV)\n        __thrsdZ = utils.hardThreshold(currZ.cpu(), self.sZ)\n        __thrsdT = utils.hardThreshold(currT.cpu(), self.sT)\n\n        self.bonsaiObj.W.data = torch.FloatTensor(\n            __thrsdW).to(self.device)\n        self.bonsaiObj.V.data = torch.FloatTensor(\n            __thrsdV).to(self.device)\n        self.bonsaiObj.Z.data = torch.FloatTensor(\n            __thrsdZ).to(self.device)\n        self.bonsaiObj.T.data = torch.FloatTensor(\n            __thrsdT).to(self.device)\n\n        self.__thrsdW = torch.FloatTensor(\n            __thrsdW.detach().clone()).to(self.device)\n        self.__thrsdV = torch.FloatTensor(\n            __thrsdV.detach().clone()).to(self.device)\n        self.__thrsdZ = torch.FloatTensor(\n            __thrsdZ.detach().clone()).to(self.device)\n        self.__thrsdT = torch.FloatTensor(\n            __thrsdT.detach().clone()).to(self.device)\n\n    def runSparseTraining(self):\n        \'\'\'\n        Function to run the Sparse Retraining routine on Bonsai Obj\n        \'\'\'\n        currW = self.bonsaiObj.W.data\n        currV = self.bonsaiObj.V.data\n        currZ = self.bonsaiObj.Z.data\n        currT = self.bonsaiObj.T.data\n\n        newW = utils.copySupport(self.__thrsdW, currW)\n        newV = utils.copySupport(self.__thrsdV, currV)\n        newZ = utils.copySupport(self.__thrsdZ, currZ)\n        newT = utils.copySupport(self.__thrsdT, currT)\n\n        self.bonsaiObj.W.data = newW\n        self.bonsaiObj.V.data = newV\n        self.bonsaiObj.Z.data = newZ\n        self.bonsaiObj.T.data = newT\n\n    def assertInit(self):\n        err = ""sparsity must be between 0 and 1""\n        assert self.sW >= 0 and self.sW <= 1, ""W "" + err\n        assert self.sV >= 0 and self.sV <= 1, ""V "" + err\n        assert self.sZ >= 0 and self.sZ <= 1, ""Z "" + err\n        assert self.sT >= 0 and self.sT <= 1, ""T "" + err\n\n    def saveParams(self, currDir):\n        \'\'\'\n        Function to save Parameter matrices into a given folder\n        \'\'\'\n        paramDir = currDir + \'/\'\n        np.save(paramDir + ""W.npy"", self.bonsaiObj.W.data.cpu())\n        np.save(paramDir + ""V.npy"", self.bonsaiObj.V.data.cpu())\n        np.save(paramDir + ""T.npy"", self.bonsaiObj.T.data.cpu())\n        np.save(paramDir + ""Z.npy"", self.bonsaiObj.Z.data.cpu())\n        hyperParamDict = {\'dataDim\': self.bonsaiObj.dataDimension,\n                          \'projDim\': self.bonsaiObj.projectionDimension,\n                          \'numClasses\': self.bonsaiObj.numClasses,\n                          \'depth\': self.bonsaiObj.treeDepth,\n                          \'sigma\': self.bonsaiObj.sigma}\n        hyperParamFile = paramDir + \'hyperParam.npy\'\n        np.save(hyperParamFile, hyperParamDict)\n\n    def saveParamsForSeeDot(self, currDir):\n        \'\'\'\n        Function to save Parameter matrices into a given folder for SeeDot compiler\n        \'\'\'\n        seeDotDir = currDir + \'/SeeDot/\'\n\n        if os.path.isdir(seeDotDir) is False:\n            try:\n                os.mkdir(seeDotDir)\n            except OSError:\n                print(""Creation of the directory %s failed"" %\n                      seeDotDir)\n\n        np.savetxt(seeDotDir + ""W"",\n                   utils.restructreMatrixBonsaiSeeDot(self.bonsaiObj.W.data.cpu(),\n                                                      self.bonsaiObj.numClasses,\n                                                      self.bonsaiObj.totalNodes),\n                   delimiter=""\\t"")\n        np.savetxt(seeDotDir + ""V"",\n                   utils.restructreMatrixBonsaiSeeDot(self.bonsaiObj.V.data.cpu(),\n                                                      self.bonsaiObj.numClasses,\n                                                      self.bonsaiObj.totalNodes),\n                   delimiter=""\\t"")\n        np.savetxt(seeDotDir + ""T"", self.bonsaiObj.T.data.cpu(), delimiter=""\\t"")\n        np.savetxt(seeDotDir + ""Z"", self.bonsaiObj.Z.data.cpu(), delimiter=""\\t"")\n        np.savetxt(seeDotDir + ""Sigma"",\n                   np.array([self.bonsaiObj.sigma]), delimiter=""\\t"")\n\n    def loadModel(self, currDir):\n        \'\'\'\n        Load the Saved model and load it to the model using constructor\n        Returns two dict one for params and other for hyperParams\n        \'\'\'\n        paramDir = currDir + \'/\'\n        paramDict = {}\n        paramDict[\'W\'] = np.load(paramDir + ""W.npy"")\n        paramDict[\'V\'] = np.load(paramDir + ""V.npy"")\n        paramDict[\'T\'] = np.load(paramDir + ""T.npy"")\n        paramDict[\'Z\'] = np.load(paramDir + ""Z.npy"")\n        hyperParamDict = np.load(paramDir + ""hyperParam.npy"").item()\n        return paramDict, hyperParamDict\n\n    # Function to get aimed model size\n    def getModelSize(self):\n        \'\'\'\n        Function to get aimed model size\n        \'\'\'\n        nnzZ, sizeZ, sparseZ = utils.estimateNNZ(self.bonsaiObj.Z, self.sZ)\n        nnzW, sizeW, sparseW = utils.estimateNNZ(self.bonsaiObj.W, self.sW)\n        nnzV, sizeV, sparseV = utils.estimateNNZ(self.bonsaiObj.V, self.sV)\n        nnzT, sizeT, sparseT = utils.estimateNNZ(self.bonsaiObj.T, self.sT)\n\n        totalnnZ = (nnzZ + nnzT + nnzV + nnzW)\n        totalSize = (sizeZ + sizeW + sizeV + sizeT)\n        hasSparse = (sparseW or sparseV or sparseT or sparseZ)\n        return totalnnZ, totalSize, hasSparse\n\n    def train(self, batchSize, totalEpochs,\n              Xtrain, Xtest, Ytrain, Ytest, dataDir, currDir):\n        \'\'\'\n        The Dense - IHT - Sparse Retrain Routine for Bonsai Training\n        \'\'\'\n        resultFile = open(dataDir + \'/PyTorchBonsaiResults.txt\', \'a+\')\n        numIters = Xtrain.shape[0] / batchSize\n\n        totalBatches = numIters * totalEpochs\n\n        self.sigmaI = 1\n\n        counter = 0\n        if self.bonsaiObj.numClasses > 2:\n            trimlevel = 15\n        else:\n            trimlevel = 5\n        ihtDone = 0\n\n        maxTestAcc = -10000\n        if self.isDenseTraining is True:\n            ihtDone = 1\n            self.sigmaI = 1\n            itersInPhase = 0\n\n        header = \'*\' * 20\n        for i in range(totalEpochs):\n            print(""\\nEpoch Number: "" + str(i), file=self.outFile)\n\n            \'\'\'\n            trainAcc -> For Classification, it is \'Accuracy\'.\n            \'\'\'\n            trainAcc = 0.0\n            trainLoss = 0.0\n\n            numIters = int(numIters)\n            for j in range(numIters):\n\n                if counter == 0:\n                    msg = "" Dense Training Phase Started ""\n                    print(""\\n%s%s%s\\n"" %\n                          (header, msg, header), file=self.outFile)\n\n                # Updating the indicator sigma\n                if ((counter == 0) or (counter == int(totalBatches / 3.0)) or\n                        (counter == int(2 * totalBatches / 3.0))) and (self.isDenseTraining is False):\n                    self.sigmaI = 1\n                    itersInPhase = 0\n\n                elif (itersInPhase % 100 == 0):\n                    indices = np.random.choice(Xtrain.shape[0], 100)\n                    batchX = Xtrain[indices, :]\n                    batchY = Ytrain[indices, :]\n                    batchY = np.reshape(\n                        batchY, [-1, self.bonsaiObj.numClasses])\n\n                    Teval = self.bonsaiObj.T.data\n                    Xcapeval = (torch.matmul(self.bonsaiObj.Z, torch.t(\n                        batchX.to(self.device))) / self.bonsaiObj.projectionDimension).data\n\n                    sum_tr = 0.0\n                    for k in range(0, self.bonsaiObj.internalNodes):\n                        sum_tr += (\n                            np.sum(np.abs(np.dot(Teval[k].cpu(), Xcapeval.cpu()))))\n\n                    if(self.bonsaiObj.internalNodes > 0):\n                        sum_tr /= (100 * self.bonsaiObj.internalNodes)\n                        sum_tr = 0.1 / sum_tr\n                    else:\n                        sum_tr = 0.1\n                    sum_tr = min(\n                        1000, sum_tr * (2**(float(itersInPhase) /\n                                            (float(totalBatches) / 30.0))))\n\n                    self.sigmaI = sum_tr\n\n                itersInPhase += 1\n                batchX = Xtrain[j * batchSize:(j + 1) * batchSize]\n                batchY = Ytrain[j * batchSize:(j + 1) * batchSize]\n                batchY = np.reshape(\n                    batchY, [-1, self.bonsaiObj.numClasses])\n\n                self.optimizer.zero_grad()\n                logits, _ = self.bonsaiObj(batchX.to(self.device), self.sigmaI)\n                batchLoss, _, _ = self.loss(logits, batchY.to(self.device))\n                batchAcc = self.accuracy(logits, batchY.to(self.device))\n\n                batchLoss.backward()\n                self.optimizer.step()\n\n                # Classification.\n\n                trainAcc += batchAcc.item()\n                trainLoss += batchLoss.item()\n\n                # Training routine involving IHT and sparse retraining\n                if (counter >= int(totalBatches / 3.0) and\n                    (counter < int(2 * totalBatches / 3.0)) and\n                    counter % trimlevel == 0 and\n                        self.isDenseTraining is False):\n                    self.runHardThrsd()\n                    if ihtDone == 0:\n                        msg = "" IHT Phase Started ""\n                        print(""\\n%s%s%s\\n"" %\n                              (header, msg, header), file=self.outFile)\n                    ihtDone = 1\n                elif ((ihtDone == 1 and counter >= int(totalBatches / 3.0) and\n                       (counter < int(2 * totalBatches / 3.0)) and\n                       counter % trimlevel != 0 and\n                       self.isDenseTraining is False) or\n                        (counter >= int(2 * totalBatches / 3.0) and\n                            self.isDenseTraining is False)):\n                    self.runSparseTraining()\n                    if counter == int(2 * totalBatches / 3.0):\n                        msg = "" Sparse Retraining Phase Started ""\n                        print(""\\n%s%s%s\\n"" %\n                              (header, msg, header), file=self.outFile)\n                counter += 1\n\n            print(""\\nClassification Train Loss: "" + str(trainLoss / numIters) +\n                  ""\\nTraining accuracy (Classification): "" +\n                  str(trainAcc / numIters),\n                  file=self.outFile)\n\n            oldSigmaI = self.sigmaI\n            self.sigmaI = 1e9\n            logits, _ = self.bonsaiObj(Xtest.to(self.device), self.sigmaI)\n            testLoss, marginLoss, regLoss = self.loss(\n                logits, Ytest.to(self.device))\n            testAcc = self.accuracy(logits, Ytest.to(self.device)).item()\n\n            if ihtDone == 0:\n                maxTestAcc = -10000\n                maxTestAccEpoch = i\n            else:\n                if maxTestAcc <= testAcc:\n                    maxTestAccEpoch = i\n                    maxTestAcc = testAcc\n                    self.saveParams(currDir)\n                    self.saveParamsForSeeDot(currDir)\n\n            print(""Test accuracy %g"" % testAcc, file=self.outFile)\n\n            testAcc = testAcc\n            maxTestAcc = maxTestAcc\n\n            print(""MarginLoss + RegLoss: "" + str(marginLoss.item()) + "" + "" +\n                  str(regLoss.item()) + "" = "" + str(testLoss.item()) + ""\\n"",\n                  file=self.outFile)\n            self.outFile.flush()\n\n            self.sigmaI = oldSigmaI\n\n        # sigmaI has to be set to infinity to ensure\n        # only a single path is used in inference\n        self.sigmaI = 1e9\n        print(""\\nNon-Zero : "" + str(self.getModelSize()[0]) + "" Model Size: "" +\n              str(float(self.getModelSize()[1]) / 1024.0) + "" KB hasSparse: "" +\n              str(self.getModelSize()[2]) + ""\\n"", file=self.outFile)\n\n        print(""For Classification, Maximum Test accuracy at compressed"" +\n              "" model size(including early stopping): "" +\n              str(maxTestAcc) + "" at Epoch: "" +\n              str(maxTestAccEpoch + 1) + ""\\nFinal Test"" +\n              "" Accuracy: "" + str(testAcc), file=self.outFile)\n\n        resultFile.write(""MaxTestAcc: "" + str(maxTestAcc) +\n                         "" at Epoch(totalEpochs): "" +\n                         str(maxTestAccEpoch + 1) +\n                         ""("" + str(totalEpochs) + "")"" + "" ModelSize: "" +\n                         str(float(self.getModelSize()[1]) / 1024.0) +\n                         "" KB hasSparse: "" + str(self.getModelSize()[2]) +\n                         "" Param Directory: "" +\n                         str(os.path.abspath(currDir)) + ""\\n"")\n        print(""The Model Directory: "" + currDir + ""\\n"")\n\n        resultFile.close()\n        self.outFile.flush()\n\n        if self.outFile is not sys.stdout:\n            self.outFile.close()\n'"
pytorch/edgeml_pytorch/trainer/fastTrainer.py,12,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport os\nimport sys\nimport torch\nimport torch.nn as nn\nimport edgeml_pytorch.utils as utils\nfrom edgeml_pytorch.graph.rnn import *\nimport numpy as np\n\n\nclass FastTrainer:\n\n    def __init__(self, FastObj, numClasses, sW=1.0, sU=1.0,\n                 learningRate=0.01, outFile=None, device=None, batch_first=False):\n        \'\'\'\n        FastObj - Can be either FastRNN or FastGRNN or any of the RNN cells \n        in graph.rnn with proper initialisations\n        numClasses is the # of classes\n        sW and sU are the sparsity factors for Fast parameters\n        batchSize is the batchSize\n        learningRate is the initial learning rate\n        \'\'\'\n        self.FastObj = FastObj\n        self.batch_first = batch_first\n        self.sW = sW\n        self.sU = sU\n\n        self.numClasses = numClasses\n        self.inputDims = self.FastObj.input_size\n        if device is None:\n            self.device = torch.device(""cpu"")\n        else:\n            self.device = device\n\n        self.learningRate = learningRate\n\n        if outFile is not None:\n            self.outFile = open(outFile, \'w\')\n        else:\n            self.outFile = sys.stdout\n\n        if self.sW > 0.99 and self.sU > 0.99:\n            self.isDenseTraining = True\n        else:\n            self.isDenseTraining = False\n\n        self.assertInit()\n        self.numMatrices = self.FastObj.num_weight_matrices\n        self.totalMatrices = self.numMatrices[0] + self.numMatrices[1]\n\n        self.optimizer = self.optimizer()\n\n        self.RNN = BaseRNN(self.FastObj, batch_first=self.batch_first).to(self.device)\n\n        self.FC = nn.Parameter(torch.randn(\n            [self.FastObj.output_size, self.numClasses])).to(self.device)\n        self.FCbias = nn.Parameter(torch.randn(\n            [self.numClasses])).to(self.device)\n\n        self.FastParams = self.FastObj.getVars()\n\n    def classifier(self, feats):\n        \'\'\'\n        Can be raplaced by any classifier\n        TODO: Make this a separate class if needed\n        \'\'\'\n        return torch.matmul(feats, self.FC) + self.FCbias\n\n    def computeLogits(self, input):\n        \'\'\'\n        Compute graph to unroll and predict on the FastObj\n        \'\'\'\n        if self.FastObj.cellType == ""LSTMLR"":\n            feats, _ = self.RNN(input)\n            logits = self.classifier(feats[-1, :])\n        else:\n            feats = self.RNN(input)\n            logits = self.classifier(feats[-1, :])\n\n        return logits, feats[:, -1]\n\n    def optimizer(self):\n        \'\'\'\n        Optimizer for FastObj Params\n        \'\'\'\n        optimizer = torch.optim.Adam(\n            self.FastObj.parameters(), lr=self.learningRate)\n\n        return optimizer\n\n    def loss(self, logits, labels):\n        \'\'\'\n        Loss function for given FastObj\n        \'\'\'\n        loss = utils.crossEntropyLoss(logits, labels)\n\n        return loss\n\n    def accuracy(self, logits, labels):\n        \'\'\'\n        Accuracy fucntion to evaluate accuracy when needed\n        \'\'\'\n        correctPredictions = (logits.argmax(dim=1) == labels.argmax(dim=1))\n        accuracy = torch.mean(correctPredictions.float())\n\n        return accuracy\n\n    def assertInit(self):\n        err = ""sparsity must be between 0 and 1""\n        assert self.sW >= 0 and self.sW <= 1, ""W "" + err\n        assert self.sU >= 0 and self.sU <= 1, ""U "" + err\n\n    def runHardThrsd(self):\n        \'\'\'\n        Function to run the IHT routine on FastObj\n        \'\'\'\n        self.thrsdParams = []\n        thrsdParams = []\n        for i in range(0, self.numMatrices[0]):\n            thrsdParams.append(\n                utils.hardThreshold(self.FastParams[i].data.cpu(), self.sW))\n        for i in range(self.numMatrices[0], self.totalMatrices):\n            thrsdParams.append(\n                utils.hardThreshold(self.FastParams[i].data.cpu(), self.sU))\n        for i in range(0, self.totalMatrices):\n            self.FastParams[i].data = torch.FloatTensor(\n                thrsdParams[i]).to(self.device)\n        for i in range(0, self.totalMatrices):\n            self.thrsdParams.append(torch.FloatTensor(\n                np.copy(thrsdParams[i].detach())).to(self.device))\n\n    def runSparseTraining(self):\n        \'\'\'\n        Function to run the Sparse Retraining routine on FastObj\n        \'\'\'\n        self.reTrainParams = []\n        for i in range(0, self.totalMatrices):\n            self.reTrainParams.append(\n                utils.copySupport(self.thrsdParams[i],\n                                  self.FastParams[i].data))\n        for i in range(0, self.totalMatrices):\n            self.FastParams[i].data = self.reTrainParams[i]\n\n    def getModelSize(self):\n        \'\'\'\n        Function to get aimed model size\n        \'\'\'\n        totalnnZ = 0\n        totalSize = 0\n        hasSparse = False\n        for i in range(0, self.numMatrices[0]):\n            nnz, size, sparseFlag = utils.estimateNNZ(self.FastParams[i], self.sW)\n            totalnnZ += nnz\n            totalSize += size\n            hasSparse = hasSparse or sparseFlag\n\n        for i in range(self.numMatrices[0], self.totalMatrices):\n            nnz, size, sparseFlag = utils.estimateNNZ(self.FastParams[i], self.sU)\n            totalnnZ += nnz\n            totalSize += size\n            hasSparse = hasSparse or sparseFlag\n        for i in range(self.totalMatrices, len(self.FastParams)):\n            nnz, size, sparseFlag = utils.estimateNNZ(self.FastParams[i], 1.0)\n            totalnnZ += nnz\n            totalSize += size\n            hasSparse = hasSparse or sparseFlag\n\n        # Replace this with classifier class call\n        nnz, size, sparseFlag = utils.estimateNNZ(self.FC, 1.0)\n        totalnnZ += nnz\n        totalSize += size\n        hasSparse = hasSparse or sparseFlag\n\n        nnz, size, sparseFlag = utils.estimateNNZ(self.FCbias, 1.0)\n        totalnnZ += nnz\n        totalSize += size\n        hasSparse = hasSparse or sparseFlag\n\n        return totalnnZ, totalSize, hasSparse\n\n    def saveParams(self, currDir):\n        \'\'\'\n        Function to save Parameter matrices\n        \'\'\'\n        if self.numMatrices[0] == 1:\n            np.save(os.path.join(currDir, ""W.npy""),\n                    self.FastParams[0].data.cpu())\n        elif self.FastObj.wRank is None:\n            if self.numMatrices[0] == 2:\n                np.save(os.path.join(currDir, ""W1.npy""),\n                        self.FastParams[0].data.cpu())\n                np.save(os.path.join(currDir, ""W2.npy""),\n                        self.FastParams[1].data.cpu())\n            if self.numMatrices[0] == 3:\n                np.save(os.path.join(currDir, ""W1.npy""),\n                        self.FastParams[0].data.cpu())\n                np.save(os.path.join(currDir, ""W2.npy""),\n                        self.FastParams[1].data.cpu())\n                np.save(os.path.join(currDir, ""W3.npy""),\n                        self.FastParams[2].data.cpu())\n            if self.numMatrices[0] == 4:\n                np.save(os.path.join(currDir, ""W1.npy""),\n                        self.FastParams[0].data.cpu())\n                np.save(os.path.join(currDir, ""W2.npy""),\n                        self.FastParams[1].data.cpu())\n                np.save(os.path.join(currDir, ""W3.npy""),\n                        self.FastParams[2].data.cpu())\n                np.save(os.path.join(currDir, ""W4.npy""),\n                        self.FastParams[3].data.cpu())\n        elif self.FastObj.wRank is not None:\n            if self.numMatrices[0] == 2:\n                np.save(os.path.join(currDir, ""W1.npy""),\n                        self.FastParams[0].data.cpu())\n                np.save(os.path.join(currDir, ""W2.npy""),\n                        self.FastParams[1].data.cpu())\n            if self.numMatrices[0] == 3:\n                np.save(os.path.join(currDir, ""W.npy""),\n                        self.FastParams[0].data.cpu())\n                np.save(os.path.join(currDir, ""W1.npy""),\n                        self.FastParams[1].data.cpu())\n                np.save(os.path.join(currDir, ""W2.npy""),\n                        self.FastParams[2].data.cpu())\n            if self.numMatrices[0] == 4:\n                np.save(os.path.join(currDir, ""W.npy""),\n                        self.FastParams[0].data.cpu())\n                np.save(os.path.join(currDir, ""W1.npy""),\n                        self.FastParams[1].data.cpu())\n                np.save(os.path.join(currDir, ""W2.npy""),\n                        self.FastParams[2].data.cpu())\n                np.save(os.path.join(currDir, ""W3.npy""),\n                        self.FastParams[3].data.cpu())\n            if self.numMatrices[0] == 5:\n                np.save(os.path.join(currDir, ""W.npy""),\n                        self.FastParams[0].data.cpu())\n                np.save(os.path.join(currDir, ""W1.npy""),\n                        self.FastParams[1].data.cpu())\n                np.save(os.path.join(currDir, ""W2.npy""),\n                        self.FastParams[2].data.cpu())\n                np.save(os.path.join(currDir, ""W3.npy""),\n                        self.FastParams[3].data.cpu())\n                np.save(os.path.join(currDir, ""W4.npy""),\n                        self.FastParams[4].data.cpu())\n\n        idx = self.numMatrices[0]\n        if self.numMatrices[1] == 1:\n            np.save(os.path.join(currDir, ""U.npy""),\n                    self.FastParams[idx + 0].data.cpu())\n        elif self.FastObj.uRank is None:\n            if self.numMatrices[1] == 2:\n                np.save(os.path.join(currDir, ""U1.npy""),\n                        self.FastParams[idx + 0].data.cpu())\n                np.save(os.path.join(currDir, ""U2.npy""),\n                        self.FastParams[idx + 1].data.cpu())\n            if self.numMatrices[1] == 3:\n                np.save(os.path.join(currDir, ""U1.npy""),\n                        self.FastParams[idx + 0].data.cpu())\n                np.save(os.path.join(currDir, ""U2.npy""),\n                        self.FastParams[idx + 1].data.cpu())\n                np.save(os.path.join(currDir, ""U3.npy""),\n                        self.FastParams[idx + 2].data.cpu())\n            if self.numMatrices[1] == 4:\n                np.save(os.path.join(currDir, ""U1.npy""),\n                        self.FastParams[idx + 0].data.cpu())\n                np.save(os.path.join(currDir, ""U2.npy""),\n                        self.FastParams[idx + 1].data.cpu())\n                np.save(os.path.join(currDir, ""U3.npy""),\n                        self.FastParams[idx + 2].data.cpu())\n                np.save(os.path.join(currDir, ""U4.npy""),\n                        self.FastParams[idx + 3].data.cpu())\n        elif self.FastObj.uRank is not None:\n            if self.numMatrices[1] == 2:\n                np.save(os.path.join(currDir, ""U1.npy""),\n                        self.FastParams[idx + 0].data.cpu())\n                np.save(os.path.join(currDir, ""U2.npy""),\n                        self.FastParams[idx + 1].data.cpu())\n            if self.numMatrices[1] == 3:\n                np.save(os.path.join(currDir, ""U.npy""),\n                        self.FastParams[idx + 0].data.cpu())\n                np.save(os.path.join(currDir, ""U1.npy""),\n                        self.FastParams[idx + 1].data.cpu())\n                np.save(os.path.join(currDir, ""U2.npy""),\n                        self.FastParams[idx + 2].data.cpu())\n            if self.numMatrices[1] == 4:\n                np.save(os.path.join(currDir, ""U.npy""),\n                        self.FastParams[idx + 0].data.cpu())\n                np.save(os.path.join(currDir, ""U1.npy""),\n                        self.FastParams[idx + 1].data.cpu())\n                np.save(os.path.join(currDir, ""U2.npy""),\n                        self.FastParams[idx + 2].data.cpu())\n                np.save(os.path.join(currDir, ""U3.npy""),\n                        self.FastParams[idx + 3].data.cpu())\n            if self.numMatrices[1] == 5:\n                np.save(os.path.join(currDir, ""U.npy""),\n                        self.FastParams[idx + 0].data.cpu())\n                np.save(os.path.join(currDir, ""U1.npy""),\n                        self.FastParams[idx + 1].data.cpu())\n                np.save(os.path.join(currDir, ""U2.npy""),\n                        self.FastParams[idx + 2].data.cpu())\n                np.save(os.path.join(currDir, ""U3.npy""),\n                        self.FastParams[idx + 3].data.cpu())\n                np.save(os.path.join(currDir, ""U4.npy""),\n                        self.FastParams[idx + 4].data.cpu())\n\n        if self.FastObj.cellType == ""FastGRNN"":\n            np.save(os.path.join(currDir, ""Bg.npy""),\n                    self.FastParams[self.totalMatrices].data.cpu())\n            np.save(os.path.join(currDir, ""Bh.npy""),\n                    self.FastParams[self.totalMatrices + 1].data.cpu())\n            np.save(os.path.join(currDir, ""zeta.npy""),\n                    self.FastParams[self.totalMatrices + 2].data.cpu())\n            np.save(os.path.join(currDir, ""nu.npy""),\n                    self.FastParams[self.totalMatrices + 3].data.cpu())\n        elif self.FastObj.cellType == ""FastRNN"":\n            np.save(os.path.join(currDir, ""B.npy""),\n                    self.FastParams[self.totalMatrices].data.cpu())\n            np.save(os.path.join(currDir, ""alpha.npy""), self.FastParams[\n                    self.totalMatrices + 1].data.cpu())\n            np.save(os.path.join(currDir, ""beta.npy""),\n                    self.FastParams[self.totalMatrices + 2].data.cpu())\n        elif self.FastObj.cellType == ""UGRNNLR"":\n            np.save(os.path.join(currDir, ""Bg.npy""),\n                    self.FastParams[self.totalMatrices].data.cpu())\n            np.save(os.path.join(currDir, ""Bh.npy""),\n                    self.FastParams[self.totalMatrices + 1].data.cpu())\n        elif self.FastObj.cellType == ""GRULR"":\n            np.save(os.path.join(currDir, ""Br.npy""),\n                    self.FastParams[self.totalMatrices].data.cpu())\n            np.save(os.path.join(currDir, ""Bg.npy""),\n                    self.FastParams[self.totalMatrices + 1].data.cpu())\n            np.save(os.path.join(currDir, ""Bh.npy""),\n                    self.FastParams[self.totalMatrices + 2].data.cpu())\n        elif self.FastObj.cellType == ""LSTMLR"":\n            np.save(os.path.join(currDir, ""Bf.npy""),\n                    self.FastParams[self.totalMatrices].data.cpu())\n            np.save(os.path.join(currDir, ""Bi.npy""),\n                    self.FastParams[self.totalMatrices + 1].data.cpu())\n            np.save(os.path.join(currDir, ""Bc.npy""),\n                    self.FastParams[self.totalMatrices + 2].data.cpu())\n            np.save(os.path.join(currDir, ""Bo.npy""),\n                    self.FastParams[self.totalMatrices + 3].data.cpu())\n\n        np.save(os.path.join(currDir, ""FC.npy""), self.FC.data.cpu())\n        np.save(os.path.join(currDir, ""FCbias.npy""), self.FCbias.data.cpu())\n\n    def train(self, batchSize, totalEpochs, Xtrain, Xtest, Ytrain, Ytest,\n              decayStep, decayRate, dataDir, currDir):\n        \'\'\'\n        The Dense - IHT - Sparse Retrain Routine for FastCell Training\n        \'\'\'\n        fileName = str(self.FastObj.cellType) + \'Results_pytorch.txt\'\n        resultFile = open(os.path.join(dataDir, fileName), \'a+\')\n        numIters = int(np.ceil(float(Xtrain.shape[0]) / float(batchSize)))\n        totalBatches = numIters * totalEpochs\n\n        counter = 0\n        trimlevel = 15\n        ihtDone = 0\n        maxTestAcc = -10000\n        if self.isDenseTraining is True:\n            ihtDone = 1\n            maxTestAcc = -10000\n        header = \'*\' * 20\n        self.timeSteps = int(Xtest.shape[1] / self.inputDims)\n        Xtest = Xtest.reshape((-1, self.timeSteps, self.inputDims))\n        Xtest = np.swapaxes(Xtest, 0, 1)\n        Xtrain = Xtrain.reshape((-1, self.timeSteps, self.inputDims))\n        Xtrain = np.swapaxes(Xtrain, 0, 1)\n\n        for i in range(0, totalEpochs):\n            print(""\\nEpoch Number: "" + str(i), file=self.outFile)\n\n            if i % decayStep == 0 and i != 0:\n                self.learningRate = self.learningRate * decayRate\n                for param_group in self.optimizer.param_groups:\n                    param_group[\'lr\'] = self.learningRate\n\n            shuffled = list(range(Xtrain.shape[1]))\n            np.random.shuffle(shuffled)\n            trainAcc = 0.0\n            trainLoss = 0.0\n            numIters = int(numIters)\n            for j in range(0, numIters):\n\n                if counter == 0:\n                    msg = "" Dense Training Phase Started ""\n                    print(""\\n%s%s%s\\n"" %\n                          (header, msg, header), file=self.outFile)\n\n                k = shuffled[j * batchSize:(j + 1) * batchSize]\n                batchX = Xtrain[:, k, :]\n                batchY = Ytrain[k]\n\n                self.optimizer.zero_grad()\n                logits, _ = self.computeLogits(batchX.to(self.device))\n                batchLoss = self.loss(logits, batchY.to(self.device))\n                batchAcc = self.accuracy(logits, batchY.to(self.device))\n                batchLoss.backward()\n                self.optimizer.step()\n\n                del batchX, batchY\n\n                trainAcc += batchAcc.item()\n                trainLoss += batchLoss.item()\n\n                if (counter >= int(totalBatches / 3.0) and\n                        (counter < int(2 * totalBatches / 3.0)) and\n                        counter % trimlevel == 0 and\n                        self.isDenseTraining is False):\n                    self.runHardThrsd()\n                    if ihtDone == 0:\n                        msg = "" IHT Phase Started ""\n                        print(""\\n%s%s%s\\n"" %\n                              (header, msg, header), file=self.outFile)\n                    ihtDone = 1\n                elif ((ihtDone == 1 and counter >= int(totalBatches / 3.0) and\n                       (counter < int(2 * totalBatches / 3.0)) and\n                       counter % trimlevel != 0 and\n                       self.isDenseTraining is False) or\n                        (counter >= int(2 * totalBatches / 3.0) and\n                            self.isDenseTraining is False)):\n                    self.runSparseTraining()\n                    if counter == int(2 * totalBatches / 3.0):\n                        msg = "" Sprase Retraining Phase Started ""\n                        print(""\\n%s%s%s\\n"" %\n                              (header, msg, header), file=self.outFile)\n                counter += 1\n\n            trainLoss /= numIters\n            trainAcc /= numIters\n            print(""Train Loss: "" + str(trainLoss) +\n                  "" Train Accuracy: "" + str(trainAcc),\n                  file=self.outFile)\n\n            logits, _ = self.computeLogits(Xtest.to(self.device))\n            testLoss = self.loss(logits, Ytest.to(self.device)).item()\n            testAcc = self.accuracy(logits, Ytest.to(self.device)).item()\n\n            if ihtDone == 0:\n                maxTestAcc = -10000\n                maxTestAccEpoch = i\n            else:\n                if maxTestAcc <= testAcc:\n                    maxTestAccEpoch = i\n                    maxTestAcc = testAcc\n                    self.saveParams(currDir)\n\n            print(""Test Loss: "" + str(testLoss) +\n                  "" Test Accuracy: "" + str(testAcc), file=self.outFile)\n            self.outFile.flush()\n\n        print(""\\nMaximum Test accuracy at compressed"" +\n              "" model size(including early stopping): "" +\n              str(maxTestAcc) + "" at Epoch: "" +\n              str(maxTestAccEpoch + 1) + ""\\nFinal Test"" +\n              "" Accuracy: "" + str(testAcc), file=self.outFile)\n        print(""\\n\\nNon-Zeros: "" + str(self.getModelSize()[0]) +\n              "" Model Size: "" + str(float(self.getModelSize()[1]) / 1024.0) +\n              "" KB hasSparse: "" + str(self.getModelSize()[2]) + ""\\n"",\n              file=self.outFile)\n\n        resultFile.write(""MaxTestAcc: "" + str(maxTestAcc) +\n                         "" at Epoch(totalEpochs): "" +\n                         str(maxTestAccEpoch + 1) +\n                         ""("" + str(totalEpochs) + "")"" + "" ModelSize: "" +\n                         str(float(self.getModelSize()[1]) / 1024.0) +\n                         "" KB hasSparse: "" + str(self.getModelSize()[2]) +\n                         "" Param Directory: "" +\n                         str(os.path.abspath(currDir)) + ""\\n"")\n\n        print(""The Model Directory: "" + currDir + ""\\n"")\n\n        resultFile.close()\n        self.outFile.flush()\n        if self.outFile is not sys.stdout:\n            self.outFile.close()\n'"
pytorch/edgeml_pytorch/trainer/fastmodel.py,8,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport sys\nfrom edgeml_pytorch.graph.rnn import *\n\ndef get_model_class(inheritance_class=nn.Module):\n    class RNNClassifierModel(inheritance_class):\n        """"""This class is a PyTorch Module that implements a 1, 2 or 3 layer\n           RNN-based classifier\n        """"""\n\n        def __init__(self, rnn_name, input_dim, num_layers, hidden_units_list,\n                     wRank_list, uRank_list, wSparsity_list, uSparsity_list,\n                     gate_nonlinearity, update_nonlinearity, num_classes=None,\n                     linear=True, batch_first=False, apply_softmax=True):\n            """"""\n            Initialize the KeywordSpotter with the following parameters:\n            input_dim - the size of the input audio frame in # samples.\n            hidden_units - the size of the hidden state of the FastGrnn nodes\n            num_keywords - the number of predictions to come out of the model.\n            num_layers - the number of FastGrnn layers to use (1, 2 or 3)\n            """"""\n            self.input_dim = input_dim\n            self.hidden_units_list = hidden_units_list\n            self.num_layers = num_layers\n            self.num_classes = num_classes\n            self.wRank_list = wRank_list\n            self.uRank_list = uRank_list\n            self.wSparsity_list = wSparsity_list\n            self.uSparsity_list = uSparsity_list\n            self.gate_nonlinearity = gate_nonlinearity\n            self.update_nonlinearity = update_nonlinearity\n            self.linear = linear\n            self.batch_first = batch_first\n            self.apply_softmax = apply_softmax\n            self.rnn_name = rnn_name\n\n            if self.linear:\n                if not self.num_classes:\n                    raise Exception(""num_classes need to be specified if linear is True"")\n\n            super(RNNClassifierModel, self).__init__()\n\n            RNN = getattr(getattr(getattr(__import__(\'edgeml_pytorch\'), \'graph\'), \'rnn\'), rnn_name)\n            self.rnn_list = nn.ModuleList([\n                RNN(self.input_dim if l==0 else self.hidden_units_list[l-1], \n                            self.hidden_units_list[l], \n                            gate_nonlinearity=self.gate_nonlinearity,\n                            update_nonlinearity=self.update_nonlinearity,\n                            wRank=self.wRank_list[l], uRank=self.uRank_list[l],\n                            wSparsity=self.wSparsity_list[l],\n                            uSparsity=self.uSparsity_list[l],\n                            batch_first = self.batch_first)\n                for l in range(self.num_layers)])\n\n            if rnn_name == ""FastGRNNCUDA"":\n                RNN_ = getattr(getattr(getattr(__import__(\'edgeml_pytorch\'), \'graph\'), \'rnn\'), \'FastGRNN\')\n                self.rnn_list_ = nn.ModuleList([\n                    RNN_(self.input_dim if l==0 else self.hidden_units_list[l-1],\n                        self.hidden_units_list[l],\n                        gate_nonlinearity=self.gate_nonlinearity,\n                        update_nonlinearity=self.update_nonlinearity,\n                        wRank=self.wRank_list[l], uRank=self.uRank_list[l],\n                        wSparsity=self.wSparsity_list[l],\n                        uSparsity=self.uSparsity_list[l],\n                        batch_first = self.batch_first)\n                    for l in range(self.num_layers)])\n            # The linear layer is a fully connected layer that maps from hidden state space\n            # to number of expected keywords\n            if self.linear:\n                last_output_size = self.hidden_units_list[self.num_layers-1]\n                self.hidden2keyword = nn.Linear(last_output_size, num_classes)\n            self.init_hidden()\n\n        def sparsify(self):\n            for rnn in self.rnn_list:\n                if self.rnn_name is ""FastGRNNCUDA"":\n                    rnn.to(torch.device(""cpu""))\n                    rnn.sparsify()\n                    rnn.to(torch.device(""cuda""))\n                else:\n                    rnn.cell.sparsify()\n\n        def sparsifyWithSupport(self):\n            for rnn in self.rnn_list:\n                if self.rnn_name is ""FastGRNNCUDA"":\n                    rnn.to(torch.device(""cpu""))\n                    rnn.sparsifyWithSupport()\n                    rnn.to(torch.device(""cuda""))\n                else:\n                    rnn.cell.sparsifyWithSupport()\n\n        def get_model_size(self):\n            total_size = 4 * self.hidden_units_list[self.num_layers-1] * self.num_classes\n            print(self.rnn_name)\n            for rnn in self.rnn_list:\n                if self.rnn_name == ""FastGRNNCUDA"":\n                    total_size += rnn.get_model_size()\n                else:\n                    total_size += rnn.cell.get_model_size()\n            return total_size\n\n        def normalize(self, mean, std):\n            self.mean = mean\n            self.std = std\n        \n        def name(self):\n            return ""{} layer FastGRNN"".format(self.num_layers)\n\n        def move_to(self, device):\n            for rnn in self.rnn_list:\n                rnn.to(device)\n            if hasattr(self, \'hidden2keyword\'):\n                self.hidden2keyword.to(device)\n\n        def init_hidden_bag(self, hidden_bag_size, device):\n            self.hidden_bag_size = hidden_bag_size\n            self.device = device\n            self.hidden_bags_list = []\n\n            for l in range(self.num_layers):\n                self.hidden_bags_list.append(\n                   torch.from_numpy(np.zeros([self.hidden_bag_size, self.hidden_units_list[l]],\n                                              dtype=np.float32)).to(self.device))\n\n        def rolling_step(self):\n            shuffled_indices = list(range(self.hidden_bag_size))\n            np.random.shuffle(shuffled_indices)\n            # if self.hidden1 is not None:\n            #     batch_size = self.hidden1.shape[0]\n            if self.hidden_states[0] is not None:\n                batch_size = self.hidden_states[0].shape[0]\n                temp_indices = shuffled_indices[:batch_size]\n                for l in range(self.num_layers):\n                    bag = self.hidden_bags_list[l]\n                    bag[temp_indices, :] = self.hidden_states[l]\n                    self.hidden_states[l] = bag[0:batch_size, :]\n\n        def init_hidden(self):\n            """""" Clear the hidden state for the GRU nodes """"""\n            # self.hidden1 = None\n            # self.hidden2 = None\n            # self.hidden3 = None\n            self.hidden_states = []\n            for l in range(self.num_layers):\n                self.hidden_states.append(None)\n\n        def forward(self, input):\n            """""" Perform the forward processing of the given input and return the prediction """"""\n            # input is shape: [seq,batch,feature]\n            if self.mean is not None:\n                input = (input - self.mean) / self.std\n\n            rnn_in = input\n            if self.rnn_name == ""FastGRNNCUDA"":\n                if self.tracking:\n                    for l in range(self.num_layers):\n                        print(""Layer: "", l)\n                        rnn_ = self.rnn_list_[l]\n                        model_output = rnn_(rnn_in, hiddenState=self.hidden_states[l])\n                        self.hidden_states[l] = model_output.detach()[-1, :, :]\n                        weights = self.rnn_list[l].getVars()\n                        weights = [weight.clone() for weight in weights]\n                        model_output = onnx_exportable_rnn(rnn_in, weights, rnn_.cell, output=model_output)\n                        rnn_in = model_output\n                else:\n                    for l in range(self.num_layers):\n                        rnn = self.rnn_list[l]\n                        model_output = rnn(rnn_in, hiddenState=self.hidden_states[l])\n                        self.hidden_states[l] = model_output.detach()[-1, :, :]\n                        rnn_in = model_output\n            else:\n                for l in range(self.num_layers):\n                    rnn = self.rnn_list[l]\n                    model_output = rnn(rnn_in, hiddenState=self.hidden_states[l])\n                    self.hidden_states[l] = model_output.detach()[-1, :, :]\n                    if self.tracking:\n                        weights = rnn.getVars()\n                        model_output = onnx_exportable_rnn(rnn_in, weights, rnn.cell, output=model_output)\n                    rnn_in = model_output\n\n            if self.linear:\n                model_output = self.hidden2keyword(model_output[-1, :, :])\n            if self.apply_softmax:\n                model_output = F.log_softmax(model_output, dim=1)\n            return model_output\n    return RNNClassifierModel\n'"
pytorch/edgeml_pytorch/trainer/protoNNTrainer.py,19,"b'# Copyright (c) Microsoft Corporation. All rights reserved.abs\n# Licensed under the MIT license.\n\nimport torch\nimport numpy as np\nimport os\nimport sys\nimport edgeml_pytorch.utils as utils\n\n\nclass ProtoNNTrainer:\n\n    def __init__(self, protoNNObj, regW, regB, regZ, sparcityW, sparcityB,\n                 sparcityZ, learningRate, lossType=\'l2\', device=None):\n        \'\'\'\n        A wrapper for the various techniques used for training ProtoNN. This\n        subsumes both the responsibility of loss graph construction and\n        performing training. The original training routine that is part of the\n        C++ implementation of EdgeML used iterative hard thresholding (IHT),\n        gamma estimation through median heuristic and other tricks for\n        training ProtoNN. This module implements the same in pytorch\n        and python.\n\n        protoNNObj: An instance of ProtoNN class defining the forward\n            computation graph. The loss functions and training routines will be\n            attached to this instance.\n        regW, regB, regZ: Regularization constants for W, B, and\n            Z matrices of protoNN.\n        sparcityW, sparcityB, sparcityZ: Sparsity constraints\n            for W, B and Z matrices. A value between 0 (exclusive) and 1\n            (inclusive) is expected. A value of 1 indicates dense training.\n        learningRate: Initial learning rate for ADAM optimizer.\n        X, Y : Placeholders for data and labels.\n            X [-1, featureDimension]\n            Y [-1, num Labels]\n        lossType: [\'l2\', \'xentropy\']\n\n        \'\'\'\n        self.protoNNObj = protoNNObj\n        self.__regW = regW\n        self.__regB = regB\n        self.__regZ = regZ\n        self.__sW = sparcityW\n        self.__sB = sparcityB\n        self.__sZ = sparcityZ\n        self.__lR = learningRate\n        self.sparseTraining = True\n        if (sparcityW == 1.0) and (sparcityB == 1.0) and (sparcityZ == 1.0):\n            self.sparseTraining = False\n            print(""Sparse training disabled."", file=sys.stderr)\n        self.W_th = None\n        self.B_th = None\n        self.Z_th = None\n        self.__lossType = lossType\n        self.optimizer = self.__optimizer()\n        self.lossCriterion = None\n        assert lossType in [\'l2\', \'xentropy\']\n        if lossType == \'l2\':\n            self.lossCriterion = torch.nn.MSELoss()\n            print(""Using L2 (MSE) loss"")\n        else :\n            self.lossCriterion = torch.nn.CrossEntropyLoss()\n            print(""Using x-entropy loss"")\n        self.__validInit = False\n        self.__validInit = self.__validateInit()\n        if device is None:\n            self.device = ""cpu""\n        else:\n            self.device = device\n\n    def __validateInit(self):\n        assert self.__validInit == False\n        msg = ""Sparsity values should be between 0 and 1 (both inclusive)""\n        assert 0 <= self.__sW <= 1, msg\n        assert 0 <= self.__sB <= 1, msg\n        assert 0 <= self.__sZ <= 1, msg\n        return True\n\n    def __optimizer(self):\n        optimizer = torch.optim.Adam(self.protoNNObj.parameters(),\n                                     lr=self.__lR)\n        return optimizer\n\n    def loss(self, logits, labels_or_target):\n        labels = labels_or_target\n        assert len(logits) == len(labels)\n        assert len(labels.shape) == 2\n        assert len(logits.shape) == 2\n        regLoss = (self.__regW * (torch.norm(self.protoNNObj.W)**2) +\n                   self.__regB * (torch.norm(self.protoNNObj.B)**2) +\n                   self.__regZ * (torch.norm(self.protoNNObj.Z)**2))\n        if self.__lossType == \'xentropy\':\n            _, labels = torch.max(labels, dim=1)\n            assert len(labels.shape)== 1\n        loss = self.lossCriterion(logits, labels) + regLoss\n        return loss\n\n    def accuracy(self, predictions, labels):\n        \'\'\'\n        Returns accuracy and number of correct predictions.\n        \'\'\'\n        assert len(predictions.shape) == 1\n        assert len(labels.shape) == 1\n        assert len(predictions) == len(labels)\n        correct = (predictions == labels).double()\n        numCorrect = torch.sum(correct)\n        acc = torch.mean(correct)\n        return acc, numCorrect\n\n    def hardThreshold(self):\n        prtn = self.protoNNObj\n        W, B, Z = prtn.W.data, prtn.B.data, prtn.Z.data\n        newW = utils.hardThreshold(W, self.__sW)\n        newB = utils.hardThreshold(B, self.__sB)\n        newZ = utils.hardThreshold(Z, self.__sZ)\n        prtn.W.data = torch.FloatTensor(newW).to(self.device)\n        prtn.B.data = torch.FloatTensor(newB).to(self.device)\n        prtn.Z.data = torch.FloatTensor(newZ).to(self.device)\n\n    def train(self, batchSize, epochs, x_train, x_val, y_train, y_val,\n              printStep=10, valStep=1):\n        \'\'\'\n        Performs dense training of ProtoNN followed by iterative hard\n        thresholding to enforce sparsity constraints.\n\n        batchSize: Batch size per update\n        epochs : The number of epochs to run training for. One epoch is\n            defined as one pass over the entire training data.\n        x_train, x_val, y_train, y_val: The numpy array containing train and\n            validation data. x data is assumed to in of shape [-1,\n            featureDimension] while y should have shape [-1, numberLabels].\n        printStep: Number of batches between echoing of loss and train accuracy.\n        valStep: Number of epochs between evaluations on validation set.\n        \'\'\'\n        d, dcap, m, L, _ = self.protoNNObj.getHyperParams()\n        assert batchSize >= 1, \'Batch size should be positive integer\'\n        assert epochs >= 1, \'Total epochs should be positive integer\'\n        assert x_train.ndim == 2, \'Expected training data to be of rank 2\'\n        assert x_train.shape[1] == d, \'Expected x_train to be [-1, %d]\' % d\n        assert x_val.ndim == 2, \'Expected validation data to be of rank 2\'\n        assert x_val.shape[1] == d, \'Expected x_val to be [-1, %d]\' % d\n        assert y_train.ndim == 2, \'Expected training labels to be of rank 2\'\n        assert y_train.shape[1] == L, \'Expected y_train to be [-1, %d]\' % L\n        assert y_val.ndim == 2, \'Expected validation labels to be of rank 2\'\n        assert y_val.shape[1] == L, \'Expected y_val to be [-1, %d]\' % L\n\n        trainNumBatches = int(np.ceil(len(x_train) / batchSize))\n        valNumBatches = int(np.ceil(len(x_val) / batchSize))\n        x_train_batches = np.array_split(x_train, trainNumBatches)\n        y_train_batches = np.array_split(y_train, trainNumBatches)\n        x_val_batches = np.array_split(x_val, valNumBatches)\n        y_val_batches = np.array_split(y_val, valNumBatches)\n\n        for epoch in range(epochs):\n            for i in range(len(x_train_batches)):\n                x_batch, y_batch = x_train_batches[i], y_train_batches[i]\n                x_batch, y_batch = torch.Tensor(x_batch), torch.Tensor(y_batch)\n                x_batch, y_batch = x_batch.to(self.device), y_batch.to(self.device)\n                self.optimizer.zero_grad()\n                logits = self.protoNNObj.forward(x_batch)\n                loss = self.loss(logits, y_batch)\n                loss.backward()\n                self.optimizer.step()\n                _, predictions = torch.max(logits, dim=1)\n                _, target = torch.max(y_batch, dim=1)\n                acc, _ = self.accuracy(predictions, target)\n                if i % printStep == 0:\n                    print(""Epoch %d batch %d loss %f acc %f"" % (epoch, i, loss,\n                                                               acc))\n            # Perform IHT Here.\n            if self.sparseTraining:\n                self.hardThreshold()\n            # Perform validation set evaluation\n            if (epoch + 1) % valStep == 0:\n                numCorrect = 0\n                for i in range(len(x_val_batches)):\n                    x_batch, y_batch = x_val_batches[i], y_val_batches[i]\n                    x_batch, y_batch = torch.Tensor(x_batch), torch.Tensor(y_batch)\n                    x_batch, y_batch = x_batch.to(self.device), y_batch.to(self.device)\n                    logits = self.protoNNObj.forward(x_batch)\n                    _, predictions = torch.max(logits, dim=1)\n                    _, target = torch.max(y_batch, dim=1)\n                    _, count = self.accuracy(predictions, target)\n                    numCorrect += count\n                print(""Validation accuracy: %f"" % (numCorrect / len(x_val)))\n\n'"
pytorch/edgeml_pytorch/trainer/srnnTrainer.py,15,"b'# Copyright (c) Microsoft Corporation. All rights reserved.abs\n# Licensed under the MIT license.\n\nimport torch\nimport numpy as np\nimport os\nimport sys\nimport edgeml_pytorch.utils as utils\n\n\nclass SRNNTrainer:\n\n    def __init__(self, srnnObj, learningRate, lossType=\'l2\', device = None):\n        \'\'\'\n        A simple trainer for SRNN\n        \'\'\'\n\n        self.srnnObj = srnnObj\n        self.__lR = learningRate\n        self.lossType = lossType\n        self.optimizer = self.__optimizer()\n        self.lossCriterion = None\n        assert lossType in [\'l2\', \'xentropy\']\n        if lossType == \'l2\':\n            self.lossCriterion = torch.nn.MSELoss()\n            print(""Using L2 (MSE) loss"")\n        else :\n            self.lossCriterion = torch.nn.CrossEntropyLoss()\n            print(""Using x-entropy loss"")\n\n        if device is None:\n            self.device = ""cpu""\n        else:\n            self.device = device\n\n    def __optimizer(self):\n        optimizer = torch.optim.Adam(self.srnnObj.parameters(),\n                                     lr=self.__lR)\n        return optimizer\n\n    def loss(self, logits, labels_or_target):\n        labels = labels_or_target\n        assert len(logits) == len(labels)\n        assert len(labels.shape) == 2\n        assert len(logits.shape) == 2\n        if self.lossType == \'xentropy\':\n            _, labels = torch.max(labels, dim=1)\n            assert len(labels.shape)== 1\n        loss = self.lossCriterion(logits, labels)\n        return loss\n\n    def accuracy(self, predictions, labels):\n        \'\'\'\n        Returns accuracy and number of correct predictions.\n        \'\'\'\n        assert len(predictions.shape) == 1\n        assert len(labels.shape) == 1\n        assert len(predictions) == len(labels)\n        correct = (predictions == labels).double()\n        numCorrect = torch.sum(correct)\n        acc = torch.mean(correct)\n        return acc, numCorrect\n\n    def train(self, brickSize, batchSize, epochs, x_train, x_val, y_train, y_val,\n              printStep=10, valStep=1):\n        \'\'\'\n        Performs training of SRNN.\n\n        batchSize: Batch size per update\n        epochs : The number of epochs to run training for. One epoch is\n            defined as one pass over the entire training data.\n        x_train, x_val, y_train, y_val: The numpy array containing train and\n            validation data. x data is assumed to in of shape [timeSteps,\n            -1, featureDimension] while y should have shape [-1, numberLabels].\n        printStep: Number of batches between echoing of loss and train accuracy.\n        valStep: Number of epochs between evaluations on validation set.\n        \'\'\'\n        L = self.srnnObj.outputDim\n        assert batchSize >= 1, \'Batch size should be positive integer\'\n        assert epochs >= 1, \'Total epochs should be positive integer\'\n        assert x_train.ndim == 3, \'Expected training data to be of rank 3\'\n        assert x_val.ndim == 3, \'Expected validation data to be of rank 3\'\n        assert y_train.ndim == 2, \'Expected training labels to be of rank 2\'\n        assert y_train.shape[1] == L, \'Expected y_train to be [-1, %d]\' % L\n        assert y_val.ndim == 2, \'Expected validation labels to be of rank 2\'\n        assert y_val.shape[1] == L, \'Expected y_val to be [-1, %d]\' % L\n\n        trainNumBatches = int(np.ceil((x_train.shape[1]) / batchSize))\n        valNumBatches = int(np.ceil((x_val.shape[1]) / batchSize))\n        x_train_batches = np.array_split(x_train, trainNumBatches, axis=1)\n        y_train_batches = np.array_split(y_train, trainNumBatches)\n        x_val_batches = np.array_split(x_val, valNumBatches, axis=1)\n        y_val_batches = np.array_split(y_val, valNumBatches)\n\n        for epoch in range(epochs):\n            for i in range(len(x_train_batches)):\n                x_batch, y_batch = x_train_batches[i], y_train_batches[i]\n                x_batch = torch.Tensor(x_batch)\n                y_batch = torch.Tensor(y_batch)\n                x_batch, y_batch = x_batch.to(self.device), y_batch.to(self.device)\n                self.optimizer.zero_grad()\n                logits = self.srnnObj.forward(x_batch, brickSize)\n                loss = self.loss(logits, y_batch)\n                loss.backward()\n                self.optimizer.step()\n                _, predictions = torch.max(logits, dim=1)\n                _, target = torch.max(y_batch, dim=1)\n                acc, _ = self.accuracy(predictions, target)\n                if i % printStep == 0:\n                    print(""Epoch %d batch %d loss %f acc %f"" % (epoch, i, loss,\n                                                               acc))\n            # Perform validation set evaluation\n            if (epoch + 1) % valStep == 0 or (epoch == epochs - 1):\n                numCorrect = 0\n                for i in range(len(x_val_batches)):\n                    x_batch, y_batch = x_val_batches[i], y_val_batches[i]\n                    x_batch = torch.Tensor(x_batch)\n                    y_batch = torch.Tensor(y_batch)\n                    x_batch, y_batch = x_batch.to(self.device), y_batch.to(self.device)\n                    logits = self.srnnObj.forward(x_batch, brickSize)\n                    _, predictions = torch.max(logits, dim=1)\n                    _, target = torch.max(y_batch, dim=1)\n                    _, count = self.accuracy(predictions, target)\n                    numCorrect += count\n                print(""Validation accuracy: %f"" % (numCorrect / x_val.shape[1]))\n\n'"
tf/edgeml_tf/graph/__init__.py,0,b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n'
tf/edgeml_tf/graph/bonsai.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport tensorflow as tf\nimport numpy as np\nimport warnings\n\n\nclass Bonsai:\n    def __init__(self, numClasses, dataDimension, projectionDimension,\n                 treeDepth, sigma,\n                 isRegression=False, W=None, T=None, V=None, Z=None):\n        \'\'\'\n        Expected Dimensions:\n\n        Bonsai Params // Optional\n        W [numClasses*totalNodes, projectionDimension]\n        V [numClasses*totalNodes, projectionDimension]\n        Z [projectionDimension, dataDimension + 1]\n        T [internalNodes, projectionDimension]\n\n        internalNodes = 2**treeDepth - 1\n        totalNodes = 2*internalNodes + 1\n\n        sigma - tanh non-linearity\n        sigmaI - Indicator function for node probabilities\n        sigmaI - has to be set to infinity(1e9 for practicality)\n        while doing testing/inference\n        numClasses will be reset to 1 in binary case\n        \'\'\'\n        self.dataDimension = dataDimension\n        self.projectionDimension = projectionDimension\n        self.isRegression = isRegression\n\n        if ((self.isRegression == True) & (numClasses != 1)):\n            warnings.warn(""Number of classes cannot be greater than 1 for regression"")\n            self.numClasses = 1\n\n        if numClasses == 2:\n            self.numClasses = 1\n        else:\n            self.numClasses = numClasses\n\n        self.treeDepth = treeDepth\n        self.sigma = sigma\n\n        self.internalNodes = 2**self.treeDepth - 1\n        self.totalNodes = 2 * self.internalNodes + 1\n\n        self.W = self.initW(W)\n        self.V = self.initV(V)\n        self.T = self.initT(T)\n        self.Z = self.initZ(Z)\n\n        self.assertInit()\n\n        self.score = None\n        self.X_ = None\n        self.prediction = None\n\n    def initZ(self, Z):\n        if Z is None:\n            Z = tf.random_normal(\n                [self.projectionDimension, self.dataDimension])\n        Z = tf.Variable(Z, name=\'Z\', dtype=tf.float32)\n        return Z\n\n    def initW(self, W):\n        if W is None:\n            W = tf.random_normal(\n                [self.numClasses * self.totalNodes, self.projectionDimension])\n        W = tf.Variable(W, name=\'W\', dtype=tf.float32)\n        return W\n\n    def initV(self, V):\n        if V is None:\n            V = tf.random_normal(\n                [self.numClasses * self.totalNodes, self.projectionDimension])\n        V = tf.Variable(V, name=\'V\', dtype=tf.float32)\n        return V\n\n    def initT(self, T):\n        if T is None:\n            T = tf.random_normal(\n                [self.internalNodes, self.projectionDimension])\n        T = tf.Variable(T, name=\'T\', dtype=tf.float32)\n        return T\n\n    def __call__(self, X, sigmaI):\n        \'\'\'\n        Function to build the Bonsai Tree graph\n        Expected Dimensions\n\n        X is [_, self.dataDimension]\n        \'\'\'\n        errmsg = ""Dimension Mismatch, X is [_, self.dataDimension]""\n        assert (len(X.shape) == 2 and int(\n            X.shape[1]) == self.dataDimension), errmsg\n        if self.score is not None:\n            return self.score, self.X_\n\n        X_ = tf.divide(tf.matmul(self.Z, X, transpose_b=True),\n                       self.projectionDimension)\n\n        W_ = self.W[0:(self.numClasses)]\n        V_ = self.V[0:(self.numClasses)]\n\n        self.__nodeProb = []\n        self.__nodeProb.append(1)\n\n        score_ = self.__nodeProb[0] * tf.multiply(\n            tf.matmul(W_, X_), tf.tanh(self.sigma * tf.matmul(V_, X_)))\n        for i in range(1, self.totalNodes):\n            W_ = self.W[i * self.numClasses:((i + 1) * self.numClasses)]\n            V_ = self.V[i * self.numClasses:((i + 1) * self.numClasses)]\n\n            T_ = tf.reshape(self.T[int(np.ceil(i / 2.0) - 1.0)],\n                            [-1, self.projectionDimension])\n            prob = (1 + ((-1)**(i + 1)) *\n                    tf.tanh(tf.multiply(sigmaI, tf.matmul(T_, X_))))\n\n            prob = tf.divide(prob, 2.0)\n            prob = self.__nodeProb[int(np.ceil(i / 2.0) - 1.0)] * prob\n            self.__nodeProb.append(prob)\n            score_ += self.__nodeProb[i] * tf.multiply(\n                tf.matmul(W_, X_), tf.tanh(self.sigma * tf.matmul(V_, X_)))\n\n        self.score = score_\n        self.X_ = X_\n        return self.score, self.X_\n\n    def getPrediction(self):\n        \'\'\'\n        Takes in a score tensor and outputs a integer class for each data point\n        \'\'\'\n\n        # Classification.\n        if (self.isRegression == False):\n            if self.prediction is not None:\n                return self.prediction\n\n            if self.numClasses > 2:\n                self.prediction = tf.argmax(tf.transpose(self.score), 1)\n            else:\n                self.prediction = tf.argmax(\n                    tf.concat([tf.transpose(self.score),\n                               0 * tf.transpose(self.score)], 1), 1)\n        # Regression.\n        elif (self.isRegression == True):\n            # For regression , scores are the actual predictions, just return them.\n            self.prediction = self.score\n\n        return self.prediction\n\n    def assertInit(self):\n        errmsg = ""Number of Classes for regression can only be 1.""\n        if (self.isRegression == True):\n            assert (self.numClasses == 1), errmsg\n        errRank = ""All Parameters must has only two dimensions shape = [a, b]""\n        assert len(self.W.shape) == len(self.Z.shape), errRank\n        assert len(self.W.shape) == len(self.T.shape), errRank\n        assert len(self.W.shape) == 2, errRank\n        msg = ""W and V should be of same Dimensions""\n        assert self.W.shape == self.V.shape, msg\n        errW = ""W and V are [numClasses*totalNodes, projectionDimension]""\n        assert self.W.shape[0] == self.numClasses * self.totalNodes, errW\n        assert self.W.shape[1] == self.projectionDimension, errW\n        errZ = ""Z is [projectionDimension, dataDimension]""\n        assert self.Z.shape[0] == self.projectionDimension, errZ\n        assert self.Z.shape[1] == self.dataDimension, errZ\n        errT = ""T is [internalNodes, projectionDimension]""\n        assert self.T.shape[0] == self.internalNodes, errT\n        assert self.T.shape[1] == self.projectionDimension, errT\n        assert int(self.numClasses) > 0, ""numClasses should be > 1""\n        msg = ""# of features in data should be > 0""\n        assert int(self.dataDimension) > 0, msg\n        msg = ""Projection should be  > 0 dims""\n        assert int(self.projectionDimension) > 0, msg\n        msg = ""treeDepth should be >= 0""\n        assert int(self.treeDepth) >= 0, msg\n'"
tf/edgeml_tf/graph/protoNN.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport numpy as np\nimport tensorflow as tf\n\n\nclass ProtoNN:\n    def __init__(self, inputDimension, projectionDimension, numPrototypes,\n                 numOutputLabels, gamma,\n                 W = None, B = None, Z = None):\n        \'\'\'\n        Forward computation graph for ProtoNN.\n\n        inputDimension: Input data dimension or feature dimension.\n        projectionDimension: hyperparameter\n        numPrototypes: hyperparameter\n        numOutputLabels: The number of output labels or classes\n        W, B, Z: Numpy matrices that can be used to initialize\n            projection matrix(W), prototype matrix (B) and prototype labels\n            matrix (B).\n            Expected Dimensions:\n                W   inputDimension (d) x projectionDimension (d_cap)\n                B   projectionDimension (d_cap) x numPrototypes (m)\n                Z   numOutputLabels (L) x numPrototypes (m)\n        \'\'\'\n        with tf.name_scope(\'protoNN\') as ns:\n            self.__nscope = ns\n        self.__d = inputDimension\n        self.__d_cap = projectionDimension\n        self.__m = numPrototypes\n        self.__L = numOutputLabels\n\n        self.__inW = W\n        self.__inB = B\n        self.__inZ = Z\n        self.__inGamma = gamma\n        self.W, self.B, self.Z = None, None, None\n        self.gamma = None\n\n        self.__validInit = False\n        self.__initWBZ()\n        self.__initGamma()\n        self.__validateInit()\n        self.protoNNOut = None\n        self.predictions = None\n        self.accuracy = None\n\n    def __validateInit(self):\n        self.__validInit = False\n        errmsg = ""Dimensions mismatch! Should be W[d, d_cap]""\n        errmsg += "", B[d_cap, m] and Z[L, m]""\n        d, d_cap, m, L, _ = self.getHyperParams()\n        assert self.W.shape[0] == d, errmsg\n        assert self.W.shape[1] == d_cap, errmsg\n        assert self.B.shape[0] == d_cap, errmsg\n        assert self.B.shape[1] == m, errmsg\n        assert self.Z.shape[0] == L, errmsg\n        assert self.Z.shape[1] == m, errmsg\n        self.__validInit = True\n\n    def __initWBZ(self):\n        with tf.name_scope(self.__nscope):\n            W = self.__inW\n            if W is None:\n                W = tf.random_normal_initializer()\n                W = W([self.__d, self.__d_cap])\n            self.W = tf.Variable(W, name=\'W\', dtype=tf.float32)\n\n            B = self.__inB\n            if B is None:\n                B = tf.random_uniform_initializer()\n                B = B([self.__d_cap, self.__m])\n            self.B = tf.Variable(B, name=\'B\', dtype=tf.float32)\n\n            Z = self.__inZ\n            if Z is None:\n                Z = tf.random_normal_initializer()\n                Z = Z([self.__L, self.__m])\n            Z = tf.Variable(Z, name=\'Z\', dtype=tf.float32)\n            self.Z = Z\n        return self.W, self.B, self.Z\n\n    def __initGamma(self):\n        with tf.name_scope(self.__nscope):\n            gamma = self.__inGamma\n            self.gamma = tf.constant(gamma, name=\'gamma\')\n\n    def getHyperParams(self):\n        \'\'\'\n        Returns the model hyperparameters:\n            [inputDimension, projectionDimension,\n            numPrototypes, numOutputLabels, gamma]\n        \'\'\'\n        d = self.__d\n        dcap = self.__d_cap\n        m = self.__m\n        L = self.__L\n        return d, dcap, m, L, self.gamma\n\n    def getModelMatrices(self):\n        \'\'\'\n        Returns Tensorflow tensors of the model matrices, which\n        can then be evaluated to obtain corresponding numpy arrays.\n\n        These can then be exported as part of other implementations of\n        ProtonNN, for instance a C++ implementation or pure python\n        implementation.\n        Returns\n            [ProjectionMatrix (W), prototypeMatrix (B),\n             prototypeLabelsMatrix (Z), gamma]\n        \'\'\'\n        return self.W, self.B, self.Z, self.gamma\n\n    def __call__(self, X, Y=None):\n        \'\'\'\n        This method is responsible for construction of the forward computation\n        graph. The end point of the computation graph, or in other words the\n        output operator for the forward computation is returned. Additionally,\n        if the argument Y is provided, a classification accuracy operator with\n        Y as target will also be created. For this, Y is assumed to in one-hot\n        encoded format and the class with the maximum prediction score is\n        compared to the encoded class in Y.  This accuracy operator is returned\n        by getAccuracyOp() method. If a different accuracyOp is required, it\n        can be defined by overriding the createAccOp(protoNNScoresOut, Y)\n        method.\n\n        X: Input tensor or placeholder of shape [-1, inputDimension]\n        Y: Optional tensor or placeholder for targets (labels or classes).\n            Expected shape is [-1, numOutputLabels].\n        returns: The forward computation outputs, self.protoNNOut\n        \'\'\'\n        # This should never execute\n        assert self.__validInit is True, ""Initialization failed!""\n        if self.protoNNOut is not None:\n            return self.protoNNOut\n\n        W, B, Z, gamma = self.W, self.B, self.Z, self.gamma\n        with tf.name_scope(self.__nscope):\n            WX = tf.matmul(X, W)\n            # Convert WX to tensor so that broadcasting can work\n            dim = [-1, WX.shape.as_list()[1], 1]\n            WX = tf.reshape(WX, dim)\n            dim = [1, B.shape.as_list()[0], -1]\n            B_ = tf.reshape(B, dim)\n            l2sim = B_ - WX\n            l2sim = tf.pow(l2sim, 2)\n            l2sim = tf.reduce_sum(l2sim, 1, keepdims=True)\n            self.l2sim = l2sim\n            gammal2sim = (-1 * gamma * gamma) * l2sim\n            M = tf.exp(gammal2sim)\n            dim = [1] + Z.shape.as_list()\n            Z_ = tf.reshape(Z, dim)\n            y = tf.multiply(Z_, M)\n            y = tf.reduce_sum(y, 2, name=\'protoNNScoreOut\')\n            self.protoNNOut = y\n            self.predictions = tf.argmax(y, 1, name=\'protoNNPredictions\')\n            if Y is not None:\n                self.createAccOp(self.protoNNOut, Y)\n        return y\n\n    def createAccOp(self, outputs, target):\n        \'\'\'\n        Define an accuracy operation on ProtoNN\'s output scores and targets.\n        Here a simple classification accuracy operator is defined. More\n        complicated operators (for multiple label problems and so forth) can be\n        defined by overriding this method\n        \'\'\'\n        assert self.predictions is not None\n        target = tf.argmax(target, 1)\n        correctPrediction = tf.equal(self.predictions, target)\n        acc = tf.reduce_mean(tf.cast(correctPrediction, tf.float32),\n                             name=\'protoNNAccuracy\')\n        self.accuracy = acc\n\n    def getPredictionsOp(self):\n        \'\'\'\n        The predictions operator is defined as argmax(protoNNScores) for each\n        prediction.\n        \'\'\'\n        return self.predictions\n\n    def getAccuracyOp(self):\n        \'\'\'\n        returns accuracyOp as defined by createAccOp. It defaults to\n        multi-class classification accuracy.\n        \'\'\'\n        msg = ""Accuracy operator not defined in graph. Did you provide Y as an""\n        msg += "" argument to _call_?""\n        assert self.accuracy is not None, msg\n        return self.accuracy\n'"
tf/edgeml_tf/graph/rnn.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport tensorflow as tf\nfrom tensorflow.python.ops import init_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import variable_scope as vs\nfrom tensorflow.python.ops import gen_math_ops\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops.rnn_cell_impl import RNNCell\n\n\ndef gen_non_linearity(A, non_linearity):\n    \'\'\'\n    Returns required activation for a tensor based on the inputs\n\n    non_linearity is either a callable or a value in\n        [\'tanh\', \'sigmoid\', \'relu\', \'quantTanh\', \'quantSigm\', \'quantSigm4\']\n    \'\'\'\n    if non_linearity == ""tanh"":\n        return math_ops.tanh(A)\n    elif non_linearity == ""sigmoid"":\n        return math_ops.sigmoid(A)\n    elif non_linearity == ""relu"":\n        return gen_math_ops.maximum(A, 0.0)\n    elif non_linearity == ""quantTanh"":\n        return gen_math_ops.maximum(gen_math_ops.minimum(A, 1.0), -1.0)\n    elif non_linearity == ""quantSigm"":\n        A = (A + 1.0) / 2.0\n        return gen_math_ops.maximum(gen_math_ops.minimum(A, 1.0), 0.0)\n    elif non_linearity == ""quantSigm4"":\n        A = (A + 2.0) / 4.0\n        return gen_math_ops.maximum(gen_math_ops.minimum(A, 1.0), 0.0)\n    else:\n        # non_linearity is a user specified function\n        if not callable(non_linearity):\n            raise ValueError(""non_linearity is either a callable or a value "" +\n                             + ""[\'tanh\', \'sigmoid\', \'relu\', \'quantTanh\', "" +\n                             ""\'quantSigm\'"")\n        return non_linearity(A)\n\n\nclass FastGRNNCell(RNNCell):\n    \'\'\'\n    FastGRNN Cell with Both Full Rank and Low Rank Formulations\n    Has multiple activation functions for the gates\n    hidden_size = # hidden units\n\n    gate_non_linearity = nonlinearity for the gate can be chosen from\n    [tanh, sigmoid, relu, quantTanh, quantSigm]\n    update_non_linearity = nonlinearity for final rnn update\n    can be chosen from [tanh, sigmoid, relu, quantTanh, quantSigm]\n\n    wRank = rank of W matrix (creates two matrices if not None)\n    uRank = rank of U matrix (creates two matrices if not None)\n    zetaInit = init for zeta, the scale param\n    nuInit = init for nu, the translation param\n\n    FastGRNN architecture and compression techniques are found in\n    FastGRNN(LINK) paper\n\n    Basic architecture is like:\n\n    z_t = gate_nl(Wx_t + Uh_{t-1} + B_g)\n    h_t^ = update_nl(Wx_t + Uh_{t-1} + B_h)\n    h_t = z_t*h_{t-1} + (sigmoid(zeta)(1-z_t) + sigmoid(nu))*h_t^\n\n    W and U can further parameterised into low rank version by\n    W = matmul(W_1, W_2) and U = matmul(U_1, U_2)\n    \'\'\'\n\n    def __init__(self, hidden_size, gate_non_linearity=""sigmoid"",\n                 update_non_linearity=""tanh"", wRank=None, uRank=None,\n                 zetaInit=1.0, nuInit=-4.0, name=""FastGRNN"", reuse=None):\n        super(FastGRNNCell, self).__init__(_reuse=reuse)\n        self._hidden_size = hidden_size\n        self._gate_non_linearity = gate_non_linearity\n        self._update_non_linearity = update_non_linearity\n        self._num_weight_matrices = [1, 1]\n        self._wRank = wRank\n        self._uRank = uRank\n        self._zetaInit = zetaInit\n        self._nuInit = nuInit\n        if wRank is not None:\n            self._num_weight_matrices[0] += 1\n        if uRank is not None:\n            self._num_weight_matrices[1] += 1\n        self._name = name\n        self._reuse = reuse\n\n    @property\n    def state_size(self):\n        return self._hidden_size\n\n    @property\n    def output_size(self):\n        return self._hidden_size\n\n    @property\n    def gate_non_linearity(self):\n        return self._gate_non_linearity\n\n    @property\n    def update_non_linearity(self):\n        return self._update_non_linearity\n\n    @property\n    def wRank(self):\n        return self._wRank\n\n    @property\n    def uRank(self):\n        return self._uRank\n\n    @property\n    def num_weight_matrices(self):\n        return self._num_weight_matrices\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def cellType(self):\n        return ""FastGRNN""\n\n    def call(self, inputs, state):\n        with vs.variable_scope(self._name + ""/FastGRNNcell""):\n\n            if self._wRank is None:\n                W_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.W = vs.get_variable(\n                    ""W"", [inputs.get_shape()[-1], self._hidden_size],\n                    initializer=W_matrix_init)\n                wComp = math_ops.matmul(inputs, self.W)\n            else:\n                W_matrix_1_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.W1 = vs.get_variable(\n                    ""W1"", [inputs.get_shape()[-1], self._wRank],\n                    initializer=W_matrix_1_init)\n                W_matrix_2_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.W2 = vs.get_variable(\n                    ""W2"", [self._wRank, self._hidden_size],\n                    initializer=W_matrix_2_init)\n                wComp = math_ops.matmul(\n                    math_ops.matmul(inputs, self.W1), self.W2)\n\n            if self._uRank is None:\n                U_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.U = vs.get_variable(\n                    ""U"", [self._hidden_size, self._hidden_size],\n                    initializer=U_matrix_init)\n                uComp = math_ops.matmul(state, self.U)\n            else:\n                U_matrix_1_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.U1 = vs.get_variable(\n                    ""U1"", [self._hidden_size, self._uRank],\n                    initializer=U_matrix_1_init)\n                U_matrix_2_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.U2 = vs.get_variable(\n                    ""U2"", [self._uRank, self._hidden_size],\n                    initializer=U_matrix_2_init)\n                uComp = math_ops.matmul(\n                    math_ops.matmul(state, self.U1), self.U2)\n            # Init zeta to 6.0 and nu to -6.0 if this doesn\'t give good\n            # results. The inits are hyper-params.\n            zeta_init = init_ops.constant_initializer(\n                self._zetaInit, dtype=tf.float32)\n            self.zeta = vs.get_variable(""zeta"", [1, 1], initializer=zeta_init)\n\n            nu_init = init_ops.constant_initializer(\n                self._nuInit, dtype=tf.float32)\n            self.nu = vs.get_variable(""nu"", [1, 1], initializer=nu_init)\n\n            pre_comp = wComp + uComp\n\n            bias_gate_init = init_ops.constant_initializer(\n                1.0, dtype=tf.float32)\n            self.bias_gate = vs.get_variable(\n                ""B_g"", [1, self._hidden_size], initializer=bias_gate_init)\n            z = gen_non_linearity(pre_comp + self.bias_gate,\n                                  self._gate_non_linearity)\n\n            bias_update_init = init_ops.constant_initializer(\n                1.0, dtype=tf.float32)\n            self.bias_update = vs.get_variable(\n                ""B_h"", [1, self._hidden_size], initializer=bias_update_init)\n            c = gen_non_linearity(\n                pre_comp + self.bias_update, self._update_non_linearity)\n            new_h = z * state + (math_ops.sigmoid(self.zeta) * (1.0 - z) +\n                                 math_ops.sigmoid(self.nu)) * c\n        return new_h, new_h\n\n    def getVars(self):\n        Vars = []\n        if self._num_weight_matrices[0] == 1:\n            Vars.append(self.W)\n        else:\n            Vars.extend([self.W1, self.W2])\n\n        if self._num_weight_matrices[1] == 1:\n            Vars.append(self.U)\n        else:\n            Vars.extend([self.U1, self.U2])\n\n        Vars.extend([self.bias_gate, self.bias_update])\n        Vars.extend([self.zeta, self.nu])\n\n        return Vars\n\n\nclass FastRNNCell(RNNCell):\n    \'\'\'\n    FastRNN Cell with Both Full Rank and Low Rank Formulations\n    Has multiple activation functions for the gates\n    hidden_size = # hidden units\n\n    update_non_linearity = nonlinearity for final rnn update\n    can be chosen from [tanh, sigmoid, relu, quantTanh, quantSigm]\n\n    wRank = rank of W matrix (creates two matrices if not None)\n    uRank = rank of U matrix (creates two matrices if not None)\n    alphaInit = init for alpha, the update scalar\n    betaInit = init for beta, the weight for previous state\n\n    FastRNN architecture and compression techniques are found in\n    FastGRNN(LINK) paper\n\n    Basic architecture is like:\n\n    h_t^ = update_nl(Wx_t + Uh_{t-1} + B_h)\n    h_t = sigmoid(beta)*h_{t-1} + sigmoid(alpha)*h_t^\n\n    W and U can further parameterised into low rank version by\n    W = matmul(W_1, W_2) and U = matmul(U_1, U_2)\n    \'\'\'\n\n    def __init__(self, hidden_size, update_non_linearity=""tanh"",\n                 wRank=None, uRank=None, alphaInit=-3.0, betaInit=3.0,\n                 name=""FastRNN"", reuse=None):\n        super(FastRNNCell, self).__init__(_reuse=reuse)\n        self._hidden_size = hidden_size\n        self._update_non_linearity = update_non_linearity\n        self._num_weight_matrices = [1, 1]\n        self._wRank = wRank\n        self._uRank = uRank\n        self._alphaInit = alphaInit\n        self._betaInit = betaInit\n        if wRank is not None:\n            self._num_weight_matrices[0] += 1\n        if uRank is not None:\n            self._num_weight_matrices[1] += 1\n        self._name = name\n        self._reuse = reuse\n\n    @property\n    def state_size(self):\n        return self._hidden_size\n\n    @property\n    def output_size(self):\n        return self._hidden_size\n\n    @property\n    def update_non_linearity(self):\n        return self._update_non_linearity\n\n    @property\n    def wRank(self):\n        return self._wRank\n\n    @property\n    def uRank(self):\n        return self._uRank\n\n    @property\n    def num_weight_matrices(self):\n        return self._num_weight_matrices\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def cellType(self):\n        return ""FastRNN""\n\n    def call(self, inputs, state):\n        with vs.variable_scope(self._name + ""/FastRNNcell""):\n\n            if self._wRank is None:\n                W_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.W = vs.get_variable(\n                    ""W"", [inputs.get_shape()[-1], self._hidden_size],\n                    initializer=W_matrix_init)\n                wComp = math_ops.matmul(inputs, self.W)\n            else:\n                W_matrix_1_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.W1 = vs.get_variable(\n                    ""W1"", [inputs.get_shape()[-1], self._wRank],\n                    initializer=W_matrix_1_init)\n                W_matrix_2_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.W2 = vs.get_variable(\n                    ""W2"", [self._wRank, self._hidden_size],\n                    initializer=W_matrix_2_init)\n                wComp = math_ops.matmul(\n                    math_ops.matmul(inputs, self.W1), self.W2)\n\n            if self._uRank is None:\n                U_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.U = vs.get_variable(\n                    ""U"", [self._hidden_size, self._hidden_size],\n                    initializer=U_matrix_init)\n                uComp = math_ops.matmul(state, self.U)\n            else:\n                U_matrix_1_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.U1 = vs.get_variable(\n                    ""U1"", [self._hidden_size, self._uRank],\n                    initializer=U_matrix_1_init)\n                U_matrix_2_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.U2 = vs.get_variable(\n                    ""U2"", [self._uRank, self._hidden_size],\n                    initializer=U_matrix_2_init)\n                uComp = math_ops.matmul(\n                    math_ops.matmul(state, self.U1), self.U2)\n\n            alpha_init = init_ops.constant_initializer(\n                self._alphaInit, dtype=tf.float32)\n            self.alpha = vs.get_variable(\n                ""alpha"", [1, 1], initializer=alpha_init)\n\n            beta_init = init_ops.constant_initializer(\n                self._betaInit, dtype=tf.float32)\n            self.beta = vs.get_variable(""beta"", [1, 1], initializer=beta_init)\n\n            pre_comp = wComp + uComp\n\n            bias_update_init = init_ops.constant_initializer(\n                1.0, dtype=tf.float32)\n            self.bias_update = vs.get_variable(\n                ""B_h"", [1, self._hidden_size], initializer=bias_update_init)\n            c = gen_non_linearity(\n                pre_comp + self.bias_update, self._update_non_linearity)\n\n            new_h = math_ops.sigmoid(self.beta) * \\\n                state + math_ops.sigmoid(self.alpha) * c\n        return new_h, new_h\n\n    def getVars(self):\n        Vars = []\n        if self._num_weight_matrices[0] == 1:\n            Vars.append(self.W)\n        else:\n            Vars.extend([self.W1, self.W2])\n\n        if self._num_weight_matrices[1] == 1:\n            Vars.append(self.U)\n        else:\n            Vars.extend([self.U1, self.U2])\n\n        Vars.extend([self.bias_update])\n        Vars.extend([self.alpha, self.beta])\n\n        return Vars\n\n\nclass LSTMLRCell(RNNCell):\n    \'\'\'\n    LR - Low Rank\n    LSTM LR Cell with Both Full Rank and Low Rank Formulations\n    Has multiple activation functions for the gates\n    hidden_size = # hidden units\n\n    gate_non_linearity = nonlinearity for the gate can be chosen from\n    [tanh, sigmoid, relu, quantTanh, quantSigm]\n    update_non_linearity = nonlinearity for final rnn update\n    can be chosen from [tanh, sigmoid, relu, quantTanh, quantSigm]\n\n    wRank = rank of all W matrices\n    (creates 5 matrices if not None else creates 4 matrices)\n    uRank = rank of all U matrices\n    (creates 5 matrices if not None else creates 4 matrices)\n\n    LSTM architecture and compression techniques are found in\n    LSTM paper\n\n    Basic architecture is like:\n\n    f_t = gate_nl(W1x_t + U1h_{t-1} + B_f)\n    i_t = gate_nl(W2x_t + U2h_{t-1} + B_i)\n    C_t^ = update_nl(W3x_t + U3h_{t-1} + B_c)\n    o_t = gate_nl(W4x_t + U4h_{t-1} + B_o)\n    C_t = f_t*C_{t-1} + i_t*C_t^\n    h_t = o_t*update_nl(C_t)\n\n    Wi and Ui can further parameterised into low rank version by\n    Wi = matmul(W, W_i) and Ui = matmul(U, U_i)\n    \'\'\'\n\n    def __init__(self, hidden_size, gate_non_linearity=""sigmoid"",\n                 update_non_linearity=""tanh"", wRank=None, uRank=None,\n                 name=""LSTMLR"", reuse=None):\n        super(LSTMLRCell, self).__init__(_reuse=reuse)\n        self._hidden_size = hidden_size\n        self._gate_non_linearity = gate_non_linearity\n        self._update_non_linearity = update_non_linearity\n        self._num_weight_matrices = [4, 4]\n        self._wRank = wRank\n        self._uRank = uRank\n        if wRank is not None:\n            self._num_weight_matrices[0] += 1\n        if uRank is not None:\n            self._num_weight_matrices[1] += 1\n        self._name = name\n        self._reuse = reuse\n\n    @property\n    def state_size(self):\n        return 2 * self._hidden_size\n\n    @property\n    def output_size(self):\n        return self._hidden_size\n\n    @property\n    def gate_non_linearity(self):\n        return self._gate_non_linearity\n\n    @property\n    def update_non_linearity(self):\n        return self._update_non_linearity\n\n    @property\n    def wRank(self):\n        return self._wRank\n\n    @property\n    def uRank(self):\n        return self._uRank\n\n    @property\n    def num_weight_matrices(self):\n        return self._num_weight_matrices\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def cellType(self):\n        return ""LSTMLR""\n\n    def call(self, inputs, state):\n        c, h = array_ops.split(value=state, num_or_size_splits=2, axis=1)\n        with vs.variable_scope(self._name + ""/LSTMLRCell""):\n\n            if self._wRank is None:\n                W1_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.W1 = vs.get_variable(\n                    ""W1"", [inputs.get_shape()[-1], self._hidden_size],\n                    initializer=W1_matrix_init)\n                W2_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.W2 = vs.get_variable(\n                    ""W2"", [inputs.get_shape()[-1], self._hidden_size],\n                    initializer=W2_matrix_init)\n                W3_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.W3 = vs.get_variable(\n                    ""W3"", [inputs.get_shape()[-1], self._hidden_size],\n                    initializer=W3_matrix_init)\n                W4_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.W4 = vs.get_variable(\n                    ""W4"", [inputs.get_shape()[-1], self._hidden_size],\n                    initializer=W4_matrix_init)\n                wComp1 = math_ops.matmul(inputs, self.W1)\n                wComp2 = math_ops.matmul(inputs, self.W2)\n                wComp3 = math_ops.matmul(inputs, self.W3)\n                wComp4 = math_ops.matmul(inputs, self.W4)\n            else:\n                W_matrix_r_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.W = vs.get_variable(\n                    ""W"", [inputs.get_shape()[-1], self._wRank],\n                    initializer=W_matrix_r_init)\n                W1_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.W1 = vs.get_variable(\n                    ""W1"", [self._wRank, self._hidden_size],\n                    initializer=W1_matrix_init)\n                W2_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.W2 = vs.get_variable(\n                    ""W2"", [self._wRank, self._hidden_size],\n                    initializer=W2_matrix_init)\n                W3_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.W3 = vs.get_variable(\n                    ""W3"", [self._wRank, self._hidden_size],\n                    initializer=W3_matrix_init)\n                W4_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.W4 = vs.get_variable(\n                    ""W4"", [self._wRank, self._hidden_size],\n                    initializer=W4_matrix_init)\n                wComp1 = math_ops.matmul(\n                    math_ops.matmul(inputs, self.W), self.W1)\n                wComp2 = math_ops.matmul(\n                    math_ops.matmul(inputs, self.W), self.W2)\n                wComp3 = math_ops.matmul(\n                    math_ops.matmul(inputs, self.W), self.W3)\n                wComp4 = math_ops.matmul(\n                    math_ops.matmul(inputs, self.W), self.W4)\n            if self._uRank is None:\n                U1_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.U1 = vs.get_variable(\n                    ""U1"", [self._hidden_size, self._hidden_size],\n                    initializer=U1_matrix_init)\n                U2_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.U2 = vs.get_variable(\n                    ""U2"", [self._hidden_size, self._hidden_size],\n                    initializer=U2_matrix_init)\n                U3_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.U3 = vs.get_variable(\n                    ""U3"", [self._hidden_size, self._hidden_size],\n                    initializer=U3_matrix_init)\n                U4_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.U4 = vs.get_variable(\n                    ""U4"", [self._hidden_size, self._hidden_size],\n                    initializer=U4_matrix_init)\n                uComp1 = math_ops.matmul(h, self.U1)\n                uComp2 = math_ops.matmul(h, self.U2)\n                uComp3 = math_ops.matmul(h, self.U3)\n                uComp4 = math_ops.matmul(h, self.U4)\n            else:\n                U_matrix_r_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.U = vs.get_variable(\n                    ""U"", [self._hidden_size, self._uRank],\n                    initializer=U_matrix_r_init)\n                U1_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.U1 = vs.get_variable(\n                    ""U1"", [self._uRank, self._hidden_size],\n                    initializer=U1_matrix_init)\n                U2_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.U2 = vs.get_variable(\n                    ""U2"", [self._uRank, self._hidden_size],\n                    initializer=U2_matrix_init)\n                U3_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.U3 = vs.get_variable(\n                    ""U3"", [self._uRank, self._hidden_size],\n                    initializer=U3_matrix_init)\n                U4_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.U4 = vs.get_variable(\n                    ""U4"", [self._uRank, self._hidden_size],\n                    initializer=U4_matrix_init)\n\n                uComp1 = math_ops.matmul(\n                    math_ops.matmul(h, self.U), self.U1)\n                uComp2 = math_ops.matmul(\n                    math_ops.matmul(h, self.U), self.U2)\n                uComp3 = math_ops.matmul(\n                    math_ops.matmul(h, self.U), self.U3)\n                uComp4 = math_ops.matmul(\n                    math_ops.matmul(h, self.U), self.U4)\n\n            pre_comp1 = wComp1 + uComp1\n            pre_comp2 = wComp2 + uComp2\n            pre_comp3 = wComp3 + uComp3\n            pre_comp4 = wComp4 + uComp4\n\n            bias_gate_init = init_ops.constant_initializer(\n                1.0, dtype=tf.float32)\n            self.bias_f = vs.get_variable(\n                ""B_f"", [1, self._hidden_size], initializer=bias_gate_init)\n            self.bias_i = vs.get_variable(\n                ""B_i"", [1, self._hidden_size], initializer=bias_gate_init)\n            self.bias_c = vs.get_variable(\n                ""B_c"", [1, self._hidden_size], initializer=bias_gate_init)\n            self.bias_o = vs.get_variable(\n                ""B_o"", [1, self._hidden_size], initializer=bias_gate_init)\n\n            f = gen_non_linearity(pre_comp1 + self.bias_f,\n                                  self._gate_non_linearity)\n            i = gen_non_linearity(pre_comp2 + self.bias_i,\n                                  self._gate_non_linearity)\n            o = gen_non_linearity(pre_comp4 + self.bias_o,\n                                  self._gate_non_linearity)\n\n            c_ = gen_non_linearity(\n                pre_comp3 + self.bias_c, self._update_non_linearity)\n\n            new_c = f * c + i * c_\n            new_h = o * gen_non_linearity(new_c, self._update_non_linearity)\n            new_state = array_ops.concat([new_c, new_h], 1)\n\n        return new_h, new_state\n\n    def getVars(self):\n        Vars = []\n        if self._num_weight_matrices[0] == 4:\n            Vars.extend([self.W1, self.W2, self.W3, self.W4])\n        else:\n            Vars.extend([self.W, self.W1, self.W2, self.W3, self.W4])\n\n        if self._num_weight_matrices[1] == 4:\n            Vars.extend([self.U1, self.U2, self.U3, self.U4])\n        else:\n            Vars.extend([self.U, self.U1, self.U2, self.U3, self.U4])\n\n        Vars.extend([self.bias_f, self.bias_i, self.bias_c, self.bias_o])\n\n        return Vars\n\n\nclass GRULRCell(RNNCell):\n    \'\'\'\n    GRU LR Cell with Both Full Rank and Low Rank Formulations\n    Has multiple activation functions for the gates\n    hidden_size = # hidden units\n\n    gate_non_linearity = nonlinearity for the gate can be chosen from\n    [tanh, sigmoid, relu, quantTanh, quantSigm]\n    update_non_linearity = nonlinearity for final rnn update\n    can be chosen from [tanh, sigmoid, relu, quantTanh, quantSigm]\n\n    wRank = rank of W matrix\n    (creates 4 matrices if not None else creates 3 matrices)\n    uRank = rank of U matrix\n    (creates 4 matrices if not None else creates 3 matrices)\n\n    GRU architecture and compression techniques are found in\n    GRU(LINK) paper\n\n    Basic architecture is like:\n\n    r_t = gate_nl(W1x_t + U1h_{t-1} + B_r)\n    z_t = gate_nl(W2x_t + U2h_{t-1} + B_g)\n    h_t^ = update_nl(W3x_t + r_t*U3(h_{t-1}) + B_h)\n    h_t = z_t*h_{t-1} + (1-z_t)*h_t^\n\n    Wi and Ui can further parameterised into low rank version by\n    Wi = matmul(W, W_i) and Ui = matmul(U, U_i)\n    \'\'\'\n\n    def __init__(self, hidden_size, gate_non_linearity=""sigmoid"",\n                 update_non_linearity=""tanh"", wRank=None, uRank=None,\n                 name=""GRULR"", reuse=None):\n        super(GRULRCell, self).__init__(_reuse=reuse)\n        self._hidden_size = hidden_size\n        self._gate_non_linearity = gate_non_linearity\n        self._update_non_linearity = update_non_linearity\n        self._num_weight_matrices = [3, 3]\n        self._wRank = wRank\n        self._uRank = uRank\n        if wRank is not None:\n            self._num_weight_matrices[0] += 1\n        if uRank is not None:\n            self._num_weight_matrices[1] += 1\n        self._name = name\n        self._reuse = reuse\n\n    @property\n    def state_size(self):\n        return self._hidden_size\n\n    @property\n    def output_size(self):\n        return self._hidden_size\n\n    @property\n    def gate_non_linearity(self):\n        return self._gate_non_linearity\n\n    @property\n    def update_non_linearity(self):\n        return self._update_non_linearity\n\n    @property\n    def wRank(self):\n        return self._wRank\n\n    @property\n    def uRank(self):\n        return self._uRank\n\n    @property\n    def num_weight_matrices(self):\n        return self._num_weight_matrices\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def cellType(self):\n        return ""GRULR""\n\n    def call(self, inputs, state):\n        with vs.variable_scope(self._name + ""/GRULRCell""):\n\n            if self._wRank is None:\n                W1_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.W1 = vs.get_variable(\n                    ""W1"", [inputs.get_shape()[-1], self._hidden_size],\n                    initializer=W1_matrix_init)\n                W2_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.W2 = vs.get_variable(\n                    ""W2"", [inputs.get_shape()[-1], self._hidden_size],\n                    initializer=W2_matrix_init)\n                W3_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.W3 = vs.get_variable(\n                    ""W3"", [inputs.get_shape()[-1], self._hidden_size],\n                    initializer=W3_matrix_init)\n                wComp1 = math_ops.matmul(inputs, self.W1)\n                wComp2 = math_ops.matmul(inputs, self.W2)\n                wComp3 = math_ops.matmul(inputs, self.W3)\n            else:\n                W_matrix_r_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.W = vs.get_variable(\n                    ""W"", [inputs.get_shape()[-1], self._wRank],\n                    initializer=W_matrix_r_init)\n                W1_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.W1 = vs.get_variable(\n                    ""W1"", [self._wRank, self._hidden_size],\n                    initializer=W1_matrix_init)\n                W2_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.W2 = vs.get_variable(\n                    ""W2"", [self._wRank, self._hidden_size],\n                    initializer=W2_matrix_init)\n                W3_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.W3 = vs.get_variable(\n                    ""W3"", [self._wRank, self._hidden_size],\n                    initializer=W3_matrix_init)\n                wComp1 = math_ops.matmul(\n                    math_ops.matmul(inputs, self.W), self.W1)\n                wComp2 = math_ops.matmul(\n                    math_ops.matmul(inputs, self.W), self.W2)\n                wComp3 = math_ops.matmul(\n                    math_ops.matmul(inputs, self.W), self.W3)\n\n            if self._uRank is None:\n                U1_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.U1 = vs.get_variable(\n                    ""U1"", [self._hidden_size, self._hidden_size],\n                    initializer=U1_matrix_init)\n                U2_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.U2 = vs.get_variable(\n                    ""U2"", [self._hidden_size, self._hidden_size],\n                    initializer=U2_matrix_init)\n                U3_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.U3 = vs.get_variable(\n                    ""U3"", [self._hidden_size, self._hidden_size],\n                    initializer=U3_matrix_init)\n                uComp1 = math_ops.matmul(state, self.U1)\n                uComp2 = math_ops.matmul(state, self.U2)\n            else:\n                U_matrix_r_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.U = vs.get_variable(\n                    ""U"", [self._hidden_size, self._uRank],\n                    initializer=U_matrix_r_init)\n                U1_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.U1 = vs.get_variable(\n                    ""U1"", [self._uRank, self._hidden_size],\n                    initializer=U1_matrix_init)\n                U2_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.U2 = vs.get_variable(\n                    ""U2"", [self._uRank, self._hidden_size],\n                    initializer=U2_matrix_init)\n                U3_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.U3 = vs.get_variable(\n                    ""U3"", [self._uRank, self._hidden_size],\n                    initializer=U3_matrix_init)\n                uComp1 = math_ops.matmul(\n                    math_ops.matmul(state, self.U), self.U1)\n                uComp2 = math_ops.matmul(\n                    math_ops.matmul(state, self.U), self.U2)\n\n            pre_comp1 = wComp1 + uComp1\n            pre_comp2 = wComp2 + uComp2\n\n            bias_r_init = init_ops.constant_initializer(\n                1.0, dtype=tf.float32)\n            self.bias_r = vs.get_variable(\n                ""B_r"", [1, self._hidden_size], initializer=bias_r_init)\n            r = gen_non_linearity(pre_comp1 + self.bias_r,\n                                  self._gate_non_linearity)\n\n            bias_gate_init = init_ops.constant_initializer(\n                1.0, dtype=tf.float32)\n            self.bias_gate = vs.get_variable(\n                ""B_g"", [1, self._hidden_size], initializer=bias_gate_init)\n            z = gen_non_linearity(pre_comp2 + self.bias_gate,\n                                  self._gate_non_linearity)\n\n            if self._uRank is None:\n                pre_comp3 = wComp3 + math_ops.matmul(r * state, self.U3)\n            else:\n                pre_comp3 = wComp3 + \\\n                    math_ops.matmul(math_ops.matmul(\n                        r * state, self.U), self.U3)\n\n            bias_update_init = init_ops.constant_initializer(\n                1.0, dtype=tf.float32)\n            self.bias_update = vs.get_variable(\n                ""B_h"", [1, self._hidden_size], initializer=bias_update_init)\n            c = gen_non_linearity(\n                pre_comp3 + self.bias_update, self._update_non_linearity)\n\n            new_h = z * state + (1.0 - z) * c\n\n        return new_h, new_h\n\n    def getVars(self):\n        Vars = []\n        if self._num_weight_matrices[0] == 3:\n            Vars.extend([self.W1, self.W2, self.W3])\n        else:\n            Vars.extend([self.W, self.W1, self.W2, self.W3])\n\n        if self._num_weight_matrices[1] == 3:\n            Vars.extend([self.U1, self.U2, self.U3])\n        else:\n            Vars.extend([self.U, self.U1, self.U2, self.U3])\n\n        Vars.extend([self.bias_r, self.bias_gate, self.bias_update])\n\n        return Vars\n\n\nclass UGRNNLRCell(RNNCell):\n    \'\'\'\n    UGRNN LR Cell with Both Full Rank and Low Rank Formulations\n    Has multiple activation functions for the gates\n    hidden_size = # hidden units\n\n    gate_non_linearity = nonlinearity for the gate can be chosen from\n    [tanh, sigmoid, relu, quantTanh, quantSigm]\n    update_non_linearity = nonlinearity for final rnn update\n    can be chosen from [tanh, sigmoid, relu, quantTanh, quantSigm]\n\n    wRank = rank of W matrix\n    (creates 3 matrices if not None else creates 2 matrices)\n    uRank = rank of U matrix\n    (creates 3 matrices if not None else creates 2 matrices)\n\n    UGRNN architecture and compression techniques are found in\n    UGRNN(LINK) paper\n\n    Basic architecture is like:\n\n    z_t = gate_nl(W1x_t + U1h_{t-1} + B_g)\n    h_t^ = update_nl(W1x_t + U1h_{t-1} + B_h)\n    h_t = z_t*h_{t-1} + (1-z_t)*h_t^\n\n    Wi and Ui can further parameterised into low rank version by\n    Wi = matmul(W, W_i) and Ui = matmul(U, U_i)\n    \'\'\'\n\n    def __init__(self, hidden_size, gate_non_linearity=""sigmoid"",\n                 update_non_linearity=""tanh"", wRank=None, uRank=None,\n                 name=""UGRNNLR"", reuse=None):\n        super(UGRNNLRCell, self).__init__(_reuse=reuse)\n        self._hidden_size = hidden_size\n        self._gate_non_linearity = gate_non_linearity\n        self._update_non_linearity = update_non_linearity\n        self._num_weight_matrices = [2, 2]\n        self._wRank = wRank\n        self._uRank = uRank\n        if wRank is not None:\n            self._num_weight_matrices[0] += 1\n        if uRank is not None:\n            self._num_weight_matrices[1] += 1\n        self._name = name\n        self._reuse = reuse\n\n    @property\n    def state_size(self):\n        return self._hidden_size\n\n    @property\n    def output_size(self):\n        return self._hidden_size\n\n    @property\n    def gate_non_linearity(self):\n        return self._gate_non_linearity\n\n    @property\n    def update_non_linearity(self):\n        return self._update_non_linearity\n\n    @property\n    def wRank(self):\n        return self._wRank\n\n    @property\n    def uRank(self):\n        return self._uRank\n\n    @property\n    def num_weight_matrices(self):\n        return self._num_weight_matrices\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def cellType(self):\n        return ""UGRNNLR""\n\n    def call(self, inputs, state):\n        with vs.variable_scope(self._name + ""/UGRNNLRCell""):\n\n            if self._wRank is None:\n                W1_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.W1 = vs.get_variable(\n                    ""W1"", [inputs.get_shape()[-1], self._hidden_size],\n                    initializer=W1_matrix_init)\n                W2_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.W2 = vs.get_variable(\n                    ""W2"", [inputs.get_shape()[-1], self._hidden_size],\n                    initializer=W2_matrix_init)\n                wComp1 = math_ops.matmul(inputs, self.W1)\n                wComp2 = math_ops.matmul(inputs, self.W2)\n            else:\n                W_matrix_r_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.W = vs.get_variable(\n                    ""W"", [inputs.get_shape()[-1], self._wRank],\n                    initializer=W_matrix_r_init)\n                W1_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.W1 = vs.get_variable(\n                    ""W1"", [self._wRank, self._hidden_size],\n                    initializer=W1_matrix_init)\n                W2_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.W2 = vs.get_variable(\n                    ""W2"", [self._wRank, self._hidden_size],\n                    initializer=W2_matrix_init)\n                wComp1 = math_ops.matmul(\n                    math_ops.matmul(inputs, self.W), self.W1)\n                wComp2 = math_ops.matmul(\n                    math_ops.matmul(inputs, self.W), self.W2)\n\n            if self._uRank is None:\n                U1_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.U1 = vs.get_variable(\n                    ""U1"", [self._hidden_size, self._hidden_size],\n                    initializer=U1_matrix_init)\n                U2_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.U2 = vs.get_variable(\n                    ""U2"", [self._hidden_size, self._hidden_size],\n                    initializer=U2_matrix_init)\n                uComp1 = math_ops.matmul(state, self.U1)\n                uComp2 = math_ops.matmul(state, self.U2)\n            else:\n                U_matrix_r_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.U = vs.get_variable(\n                    ""U"", [self._hidden_size, self._uRank],\n                    initializer=U_matrix_r_init)\n                U1_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.U1 = vs.get_variable(\n                    ""U1"", [self._uRank, self._hidden_size],\n                    initializer=U1_matrix_init)\n                U2_matrix_init = init_ops.random_normal_initializer(\n                    mean=0.0, stddev=0.1, dtype=tf.float32)\n                self.U2 = vs.get_variable(\n                    ""U2"", [self._uRank, self._hidden_size],\n                    initializer=U2_matrix_init)\n                uComp1 = math_ops.matmul(\n                    math_ops.matmul(state, self.U), self.U1)\n                uComp2 = math_ops.matmul(\n                    math_ops.matmul(state, self.U), self.U2)\n\n            pre_comp1 = wComp1 + uComp1\n            pre_comp2 = wComp2 + uComp2\n\n            bias_gate_init = init_ops.constant_initializer(\n                1.0, dtype=tf.float32)\n            self.bias_gate = vs.get_variable(\n                ""B_g"", [1, self._hidden_size], initializer=bias_gate_init)\n            z = gen_non_linearity(pre_comp1 + self.bias_gate,\n                                  self._gate_non_linearity)\n\n            bias_update_init = init_ops.constant_initializer(\n                1.0, dtype=tf.float32)\n            self.bias_update = vs.get_variable(\n                ""B_h"", [1, self._hidden_size], initializer=bias_update_init)\n            c = gen_non_linearity(\n                pre_comp2 + self.bias_update, self._update_non_linearity)\n\n            new_h = z * state + (1.0 - z) * c\n\n        return new_h, new_h\n\n    def getVars(self):\n        Vars = []\n        if self._num_weight_matrices[0] == 2:\n            Vars.extend([self.W1, self.W2])\n        else:\n            Vars.extend([self.W, self.W1, self.W2])\n\n        if self._num_weight_matrices[1] == 2:\n            Vars.extend([self.U1, self.U2])\n        else:\n            Vars.extend([self.U, self.U1, self.U2])\n\n        Vars.extend([self.bias_gate, self.bias_update])\n\n        return Vars\n\n\nclass EMI_DataPipeline():\n    \'\'\'\n    The data input block for EMI-RNN training. Since EMI-RNN is an expensive\n    algorithm due to the multiple rounds of updates that are to be performed,\n    we avoid using feed dict to feed data into tensorflow and rather,\n    exploit the dataset API. This class abstracts away most of the dataset API\n    implementation details and provides a module that ingests data in numpy\n    matrices and serves them to the remainder of the computation graph.\n\n    This class uses reinitializable iterators. Please refer to the dataset API\n    docs for more information.\n\n    This class supports resuming from checkpoint files. Provide the restored\n    meta graph as an argument to __init__ to enable this behaviour.\n\n    Usage:\n        Step 1: Create a data input pipeline object and obtain the x_batch and\n        y_batch tensors. These should be fed to other parts of the graph that\n        are supposed to act on the input data.\n        ```\n            inputPipeline = EMI_DataPipeline(NUM_SUBINSTANCE, NUM_TIMESTEPS,\n                                             NUM_FEATS, NUM_OUTPUT)\n            x_batch, y_batch = inputPipeline()\n            # feed to emiLSTM or some other computation subgraph\n            y_cap = emiLSTM(x_batch)\n        ```\n\n        Step 2:  Create other parts of the computation graph (loss operations,\n        training ops etc). After the graph construction is complete and after\n        initializing the Tensorflow graph with global_variables_initializer,\n        initialize the iterator with the input data by calling:\n            inputPipeline.runInitializer(x_train, y_trian, ...)\n\n        Step 3: You can now iterate over batches by running some computation\n        operation as you would normally do in seesion.run(..). Att the end of\n        the data, tf.errors.OutOfRangeError will be\n        thrown.\n        ```\n        while True:\n            try:\n                sess.run(y_cap)\n            except tf.errors.OutOfRangeError:\n                break\n        ```\n    \'\'\'\n\n    def __init__(self, numSubinstance, numTimesteps, numFeats, numOutput,\n                 graph=None, prefetchNum=5):\n        \'\'\'\n        numSubinstance, numTimeSteps, numFeats, numOutput:\n            Dataset characteristics. Please refer to the data preparation\n            documentation for more information provided in `examples/EMI-RNN`\n        graph: This module supports resuming/restoring from a saved metagraph. To\n            enable this behaviour, pass the restored graph as an argument. A\n            saved metagraph can be restored using the edgeml.utils.GraphManager\n            module.\n        prefetchNum: The number of asynchronous prefetch to do when iterating over\n            the data. Please refer to \'prefetching\' in Tensorflow dataset API\n        \'\'\'\n\n        self.numSubinstance = numSubinstance\n        self.numTimesteps = numTimesteps\n        self.numFeats = numFeats\n        self.graph = graph\n        self.prefetchNum = prefetchNum\n        self.numOutput = numOutput\n        self.graphCreated = False\n        # Either restore or create the following\n        self.X = None\n        self.Y = None\n        self.batchSize = None\n        self.numEpochs = None\n        self.dataset_init = None\n        self.x_batch = None\n        self.y_batch = None\n        # Internal\n        self.scope = \'EMI/\'\n\n    def _createGraph(self):\n        assert self.graphCreated is False\n        dim = [None, self.numSubinstance, self.numTimesteps, self.numFeats]\n        scope = self.scope + \'input-pipeline/\'\n        with tf.name_scope(scope):\n            X = tf.placeholder(tf.float32, dim, name=\'inpX\')\n            Y = tf.placeholder(tf.float32, [None, self.numSubinstance,\n                                            self.numOutput], name=\'inpY\')\n            batchSize = tf.placeholder(tf.int64, name=\'batch-size\')\n            numEpochs = tf.placeholder(tf.int64, name=\'num-epochs\')\n\n            dataset_x_target = tf.data.Dataset.from_tensor_slices(X)\n            dataset_y_target = tf.data.Dataset.from_tensor_slices(Y)\n            couple = (dataset_x_target, dataset_y_target)\n            ds_target = tf.data.Dataset.zip(couple).repeat(numEpochs)\n            ds_target = ds_target.batch(batchSize)\n            ds_target = ds_target.prefetch(self.prefetchNum)\n            ds_iterator_target = tf.data.Iterator.from_structure(ds_target.output_types,\n                                                                 ds_target.output_shapes)\n            ds_next_target = ds_iterator_target\n            ds_init_target = ds_iterator_target.make_initializer(ds_target,\n                                                                 name=\'dataset-init\')\n            x_batch, y_batch = ds_iterator_target.get_next()\n            tf.add_to_collection(\'next-x-batch\', x_batch)\n            tf.add_to_collection(\'next-y-batch\', y_batch)\n        self.X = X\n        self.Y = Y\n        self.batchSize = batchSize\n        self.numEpochs = numEpochs\n        self.dataset_init = ds_init_target\n        self.x_batch, self.y_batch = x_batch, y_batch\n        self.graphCreated = True\n\n    def _restoreGraph(self, graph):\n        assert self.graphCreated is False\n        scope = \'EMI/input-pipeline/\'\n        self.X = graph.get_tensor_by_name(scope + ""inpX:0"")\n        self.Y = graph.get_tensor_by_name(scope + ""inpY:0"")\n        self.batchSize = graph.get_tensor_by_name(scope + ""batch-size:0"")\n        self.numEpochs = graph.get_tensor_by_name(scope + ""num-epochs:0"")\n        self.dataset_init = graph.get_operation_by_name(scope + ""dataset-init"")\n        self.x_batch = graph.get_collection(\'next-x-batch\')\n        self.y_batch = graph.get_collection(\'next-y-batch\')\n        msg = \'More than one tensor named next-x-batch/next-y-batch. \'\n        msg += \'Are you not resetting your graph?\'\n        assert len(self.x_batch) == 1, msg\n        assert len(self.y_batch) == 1, msg\n        self.x_batch = self.x_batch[0]\n        self.y_batch = self.y_batch[0]\n        self.graphCreated = True\n\n    def __call__(self):\n        \'\'\'\n        The call method performs graph construction either by\n        creating a new graph or, if a restored meta graph is provided, by\n        restoring operators from this meta graph.\n\n        returns iterators (x_batch, y_batch)\n        \'\'\'\n        if self.graphCreated is True:\n            return self.x_batch, self.y_batch\n        if self.graph is None:\n            self._createGraph()\n        else:\n            self._restoreGraph(self.graph)\n        assert self.graphCreated is True\n        return self.x_batch, self.y_batch\n\n    def restoreFromGraph(self, graph, *args, **kwargs):\n        \'\'\'\n        This method provides an alternate way of restoring\n        from a saved meta graph - without having to provide the restored meta\n        graph as a parameter to __init__. This is useful when, in between\n        training, you want to reset the entire computation graph and reload a\n        new meta graph from disk. This method allows you to attach to this\n        newly loaded meta graph without having to create a new EMI_DataPipeline\n        object. Use this method only when you want to clear/reset the existing\n        computational graph.\n        \'\'\'\n        self.graphCreated = False\n        self.graph = graph\n        self._restoreGraph(graph)\n        assert self.graphCreated is True\n\n    def runInitializer(self, sess, x_data, y_data, batchSize, numEpochs):\n        \'\'\'\n        This method is used to ingest data by the dataset API. Call this method\n        with the data matrices after the graph has been initialized.\n\n        x_data, y_data, batchSize: Self explanatory.\n        numEpochs: The Tensorflow dataset API implements iteration over epochs\n            by appending the data to itself numEpochs times and then iterating\n            over the resulting data as if it was a single data set.\n        \'\'\'\n        assert self.graphCreated is True\n        msg = \'X shape should be [-1, numSubinstance, numTimesteps, numFeats]\'\n        assert x_data.ndim == 4, msg\n        assert x_data.shape[1] == self.numSubinstance, msg\n        assert x_data.shape[2] == self.numTimesteps, msg\n        assert x_data.shape[3] == self.numFeats, msg\n        msg = \'X and Y sould have same first dimension\'\n        assert y_data.shape[0] == x_data.shape[0], msg\n        msg = \'Y shape should be [-1, numSubinstance, numOutput]\'\n        assert y_data.shape[1] == self.numSubinstance, msg\n        assert y_data.shape[2] == self.numOutput, msg\n        feed_dict = {\n            self.X: x_data,\n            self.Y: y_data,\n            self.batchSize: batchSize,\n            self.numEpochs: numEpochs\n        }\n        assert self.dataset_init is not None, \'Internal error!\'\n        sess.run(self.dataset_init, feed_dict=feed_dict)\n\n\nclass EMI_RNN():\n\n    def __init__(self, *args, **kwargs):\n        """"""\n        Abstract base class for RNN architectures compatible with EMI-RNN.\n        This class is extended by specific architectures like LSTM/GRU/FastGRNN\n        etc.\n\n        Note: We are not using the PEP recommended abc module since it is\n        difficult to support in both python 2 and 3 """"""\n        self.graphCreated = False\n        # Model specific matrices, parameter should be saved\n        self.graph = None\n        self.varList = []\n        self.output = None\n        self.assignOps = []\n        raise NotImplementedError(""This is intended to act similar to an "" +\n                                  ""abstract class. Instantiating is not "" +\n                                  ""allowed."")\n\n    def __call__(self, x_batch, **kwargs):\n        \'\'\'\n        The call method performs graph construction either by\n        creating a new graph or, if a restored meta graph is provided, by\n        restoring operators from this meta graph.\n\n        x_batch: Dataset API iterators to the data.\n\n        returns forward computation output tensor\n        \'\'\'\n        if self.graphCreated is True:\n            assert self.output is not None\n            return self.output\n        if self.graph is None:\n            output = self._createBaseGraph(x_batch, **kwargs)\n            assert self.graphCreated is False\n            self._createExtendedGraph(output, **kwargs)\n        else:\n            self._restoreBaseGraph(self.graph, **kwargs)\n            assert self.graphCreated is False\n            self._restoreExtendedGraph(self.graph, **kwargs)\n        assert self.graphCreated is True\n        return self.output\n\n    def restoreFromGraph(self, graph, **kwargs):\n        \'\'\'\n        This method provides an alternate way of restoring\n        from a saved meta graph - without having to provide the restored meta\n        graph as a parameter to __init__. This is useful when, in between\n        training, you want to reset the entire computation graph and reload a\n        new meta graph from disk. This method allows you to attach to this\n        newly loaded meta graph without having to create a new EMI_DataPipeline\n        object. Use this method only when you want to clear/reset the existing\n        computational graph.\n        \'\'\'\n        self.graphCreated = False\n        self.varList = []\n        self.output = None\n        self.assignOps = []\n        self.graph = graph\n        self._restoreBaseGraph(self.graph, **kwargs)\n        assert self.graphCreated is False\n        self._restoreExtendedGraph(self.graph, **kwargs)\n        assert self.graphCreated is True\n\n    def getModelParams(self):\n        raise NotImplementedError(""Subclass does not implement this method"")\n\n    def _createBaseGraph(self, x_batch, **kwargs):\n        raise NotImplementedError(""Subclass does not implement this method"")\n\n    def _createExtendedGraph(self, baseOutput, **kwargs):\n        raise NotImplementedError(""Subclass does not implement this method"")\n\n    def _restoreBaseGraph(self, graph, **kwargs):\n        raise NotImplementedError(""Subclass does not implement this method"")\n\n    def _restoreExtendedGraph(self, graph, **kwargs):\n        raise NotImplementedError(""Subclass does not implement this method"")\n\n    def addBaseAssignOps(self, graph, initVarList, **kwargs):\n        raise NotImplementedError(""Subclass does not implement this method"")\n\n    def addExtendedAssignOps(self, graph, **kwargs):\n        raise NotImplementedError(""Subclass does not implement this method"")\n\n\nclass EMI_BasicLSTM(EMI_RNN):\n\n    def __init__(self, numSubinstance, numHidden, numTimeSteps,\n                 numFeats, graph=None, forgetBias=1.0, useDropout=False):\n        \'\'\'\n        EMI-RNN using LSTM cell. The architecture consists of a single LSTM\n        layer followed by a secondary classifier. The secondary classifier is\n        not defined as part of this module and is left for the user to define,\n        through the redefinition of the \'_createExtendedGraph\' and\n        \'_restoreExtendedGraph\' methods.\n\n        This class supports restoring from a meta-graph. Provide the restored\n        graph as an argument to the graph keyword to enable this behaviour.\n\n        numSubinstance: Number of sub-instance.\n        numHidden: The dimension of the hidden state.\n        numTimeSteps: The number of time steps of the RNN.\n        numFeats: The feature vector dimension for each time step.\n        graph: A restored metagraph. Provide a graph if restoring form a meta\n            graph is required.\n        forgetBias: Bias for the forget gate of the LSTM.\n        useDropout: Set to True if a dropout layer is to be added between\n            inputs and outputs to the LSTM.\n        \'\'\'\n        self.numHidden = numHidden\n        self.numTimeSteps = numTimeSteps\n        self.numFeats = numFeats\n        self.useDropout = useDropout\n        self.forgetBias = forgetBias\n        self.numSubinstance = numSubinstance\n        self.graph = graph\n        self.graphCreated = False\n        # Restore or initialize\n        self.keep_prob = None\n        self.varList = []\n        self.output = None\n        self.assignOps = []\n        # Internal\n        self._scope = \'EMI/BasicLSTM/\'\n\n    def _createBaseGraph(self, X, **kwargs):\n        assert self.graphCreated is False\n        msg = \'X should be of form [-1, numSubinstance, numTimeSteps, numFeatures]\'\n        assert X.get_shape().ndims == 4, msg\n        assert X.shape[1] == self.numSubinstance\n        assert X.shape[2] == self.numTimeSteps\n        assert X.shape[3] == self.numFeats\n        # Reshape into 3D such that the first dimension is -1 * numSubinstance\n        # where each numSubinstance segment corresponds to one bag\n        # then shape it back in into 4D\n        scope = self._scope\n        keep_prob = None\n        with tf.name_scope(scope):\n            x = tf.reshape(X, [-1, self.numTimeSteps, self.numFeats])\n            x = tf.unstack(x, num=self.numTimeSteps, axis=1)\n            # Get the LSTM output\n            cell = tf.nn.rnn_cell.BasicLSTMCell(self.numHidden,\n                                                forget_bias=self.forgetBias,\n                                                name=\'EMI-LSTM-Cell\')\n            wrapped_cell = cell\n            if self.useDropout is True:\n                keep_prob = tf.placeholder(dtype=tf.float32, name=\'keep-prob\')\n                wrapped_cell = tf.contrib.rnn.DropoutWrapper(cell,\n                                                             input_keep_prob=keep_prob,\n                                                             output_keep_prob=keep_prob)\n            outputs__, states = tf.nn.static_rnn(\n                wrapped_cell, x, dtype=tf.float32)\n            outputs = []\n            for output in outputs__:\n                outputs.append(tf.expand_dims(output, axis=1))\n            # Convert back to bag form\n            outputs = tf.concat(outputs, axis=1, name=\'concat-output\')\n            dims = [-1, self.numSubinstance, self.numTimeSteps, self.numHidden]\n            output = tf.reshape(outputs, dims, name=\'bag-output\')\n\n        LSTMVars = cell.variables\n        self.varList.extend(LSTMVars)\n        if self.useDropout:\n            self.keep_prob = keep_prob\n        self.output = output\n        return self.output\n\n    def _restoreBaseGraph(self, graph, **kwargs):\n        assert self.graphCreated is False\n        assert self.graph is not None\n        scope = self._scope\n        if self.useDropout:\n            self.keep_prob = graph.get_tensor_by_name(scope + \'keep-prob:0\')\n        self.output = graph.get_tensor_by_name(scope + \'bag-output:0\')\n        kernel = graph.get_tensor_by_name(""rnn/EMI-LSTM-Cell/kernel:0"")\n        bias = graph.get_tensor_by_name(""rnn/EMI-LSTM-Cell/bias:0"")\n        assert len(self.varList) is 0\n        self.varList = [kernel, bias]\n\n    def getModelParams(self):\n        \'\'\'\n        Returns the LSTM kernel and bias tensors.\n        returns [kernel, bias]\n        \'\'\'\n        assert self.graphCreated is True, ""Graph is not created""\n        assert len(self.varList) == 2\n        return self.varList\n\n    def addBaseAssignOps(self, graph, initVarList, **kwargs):\n        \'\'\'\n        Adds Tensorflow assignment operations to all of the model tensors.\n        These operations can then be used to initialize these tensors from\n        numpy matrices by running these operators\n\n        initVarList: A list of numpy matrices that will be used for\n            initialization by the assignment operation. For EMI_BasicLSTM, this\n            should be [kernel, bias] matrices.\n        \'\'\'\n        assert initVarList is not None\n        assert len(initVarList) == 2\n        k_ = graph.get_tensor_by_name(\'rnn/EMI-LSTM-Cell/kernel:0\')\n        b_ = graph.get_tensor_by_name(\'rnn/EMI-LSTM-Cell/bias:0\')\n        kernel, bias = initVarList[-2], initVarList[-1]\n        k_op = tf.assign(k_, kernel)\n        b_op = tf.assign(b_, bias)\n        self.assignOps.extend([k_op, b_op])\n\n\nclass EMI_GRU(EMI_RNN):\n\n    def __init__(self, numSubinstance, numHidden, numTimeSteps,\n                 numFeats, graph=None, useDropout=False):\n        \'\'\'\n        EMI-RNN using GRU cell. The architecture consists of a single GRU\n        layer followed by a secondary classifier. The secondary classifier is\n        not defined as part of this module and is left for the user to define,\n        through the redefinition of the \'_createExtendedGraph\' and\n        \'_restoreExtendedGraph\' methods.\n\n        This class supports restoring from a meta-graph. Provide the restored\n        graph as value to the graph keyword to enable this behaviour.\n\n        numSubinstance: Number of sub-instance.\n        numHidden: The dimension of the hidden state.\n        numTimeSteps: The number of time steps of the RNN.\n        numFeats: The feature vector dimension for each time step.\n        graph: A restored metagraph. Provide a graph if restoring form a meta\n            graph is required.\n        useDropout: Set to True if a dropout layer is to be added between\n            inputs and outputs to the RNN.\n        \'\'\'\n        self.numHidden = numHidden\n        self.numTimeSteps = numTimeSteps\n        self.numFeats = numFeats\n        self.useDropout = useDropout\n        self.numSubinstance = numSubinstance\n        self.graph = graph\n        self.graphCreated = False\n        # Restore or initialize\n        self.keep_prob = None\n        self.varList = []\n        self.output = None\n        self.assignOps = []\n        # Internal\n        self._scope = \'EMI/GRU/\'\n\n    def _createBaseGraph(self, X, **kwargs):\n        assert self.graphCreated is False\n        msg = \'X should be of form [-1, numSubinstance, numTimeSteps, numFeatures]\'\n        assert X.get_shape().ndims == 4, msg\n        assert X.shape[1] == self.numSubinstance\n        assert X.shape[2] == self.numTimeSteps\n        assert X.shape[3] == self.numFeats\n        # Reshape into 3D suself.h that the first dimension is -1 * numSubinstance\n        # where each numSubinstance segment corresponds to one bag\n        # then shape it back in into 4D\n        scope = self._scope\n        keep_prob = None\n        with tf.name_scope(scope):\n            x = tf.reshape(X, [-1, self.numTimeSteps, self.numFeats])\n            x = tf.unstack(x, num=self.numTimeSteps, axis=1)\n            # Get the GRU output\n            cell = tf.nn.rnn_cell.GRUCell(self.numHidden, name=\'EMI-GRU-Cell\')\n            wrapped_cell = cell\n            if self.useDropout is True:\n                keep_prob = tf.placeholder(dtype=tf.float32, name=\'keep-prob\')\n                wrapped_cell = tf.contrib.rnn.DropoutWrapper(cell,\n                                                             input_keep_prob=keep_prob,\n                                                             output_keep_prob=keep_prob)\n            outputs__, states = tf.nn.static_rnn(\n                wrapped_cell, x, dtype=tf.float32)\n            outputs = []\n            for output in outputs__:\n                outputs.append(tf.expand_dims(output, axis=1))\n            # Convert back to bag form\n            outputs = tf.concat(outputs, axis=1, name=\'concat-output\')\n            dims = [-1, self.numSubinstance, self.numTimeSteps, self.numHidden]\n            output = tf.reshape(outputs, dims, name=\'bag-output\')\n\n        GRUVars = cell.variables\n        self.varList.extend(GRUVars)\n        if self.useDropout:\n            self.keep_prob = keep_prob\n        self.output = output\n        return self.output\n\n    def _restoreBaseGraph(self, graph, **kwargs):\n        assert self.graphCreated is False\n        assert self.graph is not None\n        scope = self._scope\n        if self.useDropout:\n            self.keep_prob = graph.get_tensor_by_name(scope + \'keep-prob:0\')\n        self.output = graph.get_tensor_by_name(scope + \'bag-output:0\')\n        kernel1 = graph.get_tensor_by_name(""rnn/EMI-GRU-Cell/gates/kernel:0"")\n        bias1 = graph.get_tensor_by_name(""rnn/EMI-GRU-Cell/gates/bias:0"")\n        kernel2 = graph.get_tensor_by_name(\n            ""rnn/EMI-GRU-Cell/candidate/kernel:0"")\n        bias2 = graph.get_tensor_by_name(""rnn/EMI-GRU-Cell/candidate/bias:0"")\n        assert len(self.varList) is 0\n        self.varList = [kernel1, bias1, kernel2, bias2]\n\n    def getModelParams(self):\n        \'\'\'\n        Returns the GRU kernel and bias tensors.\n        returns [kernel1, bias1, kernel2, bias2]\n        \'\'\'\n        assert self.graphCreated is True, ""Graph is not created""\n        assert len(self.varList) == 4\n        return self.varList\n\n    def addBaseAssignOps(self, graph, initVarList, **kwargs):\n        \'\'\'\n        Adds Tensorflow assignment operations to all of the model tensors.\n        These operations can then be used to initialize these tensors from\n        numpy matrices by running these operators\n\n        initVarList: A list of numpy matrices that will be used for\n            initialization by the assignment operation. For EMI_GRU, this\n            should be list of numpy matrices corresponding to  [kernel1, bias1,\n            kernel2, bias2]\n        \'\'\'\n        assert initVarList is not None\n        assert len(initVarList) == 2\n        kernel1_ = graph.get_tensor_by_name(""rnn/EMI-GRU-Cell/gates/kernel:0"")\n        bias1_ = graph.get_tensor_by_name(""rnn/EMI-GRU-Cell/gates/bias:0"")\n        kernel2_ = graph.get_tensor_by_name(\n            ""rnn/EMI-GRU-Cell/candidate/kernel:0"")\n        bias2_ = graph.get_tensor_by_name(""rnn/EMI-GRU-Cell/candidate/bias:0"")\n        kernel1, bias1, kernel2, bias2 = initVarList[\n            0], initVarList[1], initVarList[2], initVarList[3]\n        kernel1_op = tf.assign(kernel1_, kernel1)\n        bias1_op = tf.assign(bias1_, bias1)\n        kernel2_op = tf.assign(kernel2_, kernel2)\n        bias2_op = tf.assign(bias2_, bias2)\n        self.assignOps.extend([kernel1_op, bias1_op, kernel2_op, bias2_op])\n\n\nclass EMI_FastRNN(EMI_RNN):\n\n    def __init__(self, numSubinstance, numHidden, numTimeSteps,\n                 numFeats, graph=None, useDropout=False,\n                 update_non_linearity=""tanh"", wRank=None,\n                 uRank=None, alphaInit=-3.0, betaInit=3.0):\n        \'\'\'\n        EMI-RNN using FastRNN cell. The architecture consists of a single\n        FastRNN layer followed by a secondary classifier. The secondary\n        classifier is not defined as part of this module and is left for the\n        user to define, through the redefinition of the \'_createExtendedGraph\'\n        and \'_restoreExtendedGraph\' methods.\n\n        This class supports restoring from a meta-graph. Provide the restored\n        graph as value to the graph keyword to enable this behaviour.\n\n        numSubinstance: Number of sub-instance.\n        numHidden: The dimension of the hidden state.\n        numTimeSteps: The number of time steps of the RNN.\n        numFeats: The feature vector dimension for each time step.\n        graph: A restored metagraph. Provide a graph if restoring form a meta\n            graph is required.\n        useDropout: Set to True if a dropout layer is to be added\n            between inputs and outputs to the RNN.\n        update_non_linearity, wRank, uRank, _alphaInit, betaInit:\n            These are FastRNN parameters. Please refer to FastRNN documentation\n            for more information.\n        \'\'\'\n        self.numHidden = numHidden\n        self.numTimeSteps = numTimeSteps\n        self.numFeats = numFeats\n        self.useDropout = useDropout\n        self.numSubinstance = numSubinstance\n        self.graph = graph\n        self.update_non_linearity = update_non_linearity\n        self.wRank = wRank\n        self.uRank = uRank\n        self.alphaInit = alphaInit\n        self.betaInit = betaInit\n        self.graphCreated = False\n        # Restore or initialize\n        self.keep_prob = None\n        self.varList = []\n        self.output = None\n        self.assignOps = []\n        # Internal\n        self._scope = \'EMI/FastRNN/\'\n\n    def _createBaseGraph(self, X, **kwargs):\n        assert self.graphCreated is False\n        msg = \'X should be of form [-1, numSubinstance, numTimeSteps,\'\n        msg += \' numFeatures]\'\n        assert X.get_shape().ndims == 4, msg\n        assert X.shape[1] == self.numSubinstance\n        assert X.shape[2] == self.numTimeSteps\n        assert X.shape[3] == self.numFeats\n        # Reshape into 3D suself.h that the first dimension is -1 *\n        # numSubinstance where each numSubinstance segment corresponds to one\n        # bag then shape it back in into 4D\n        scope = self._scope\n        keep_prob = None\n        with tf.name_scope(scope):\n            x = tf.reshape(X, [-1, self.numTimeSteps, self.numFeats])\n            x = tf.unstack(x, num=self.numTimeSteps, axis=1)\n            # Get the FastRNN output\n            cell = FastRNNCell(self.numHidden, self.update_non_linearity,\n                               self.wRank, self.uRank, self.alphaInit,\n                               self.betaInit, name=\'EMI-FastRNN-Cell\')\n            wrapped_cell = cell\n            if self.useDropout is True:\n                keep_prob = tf.placeholder(dtype=tf.float32, name=\'keep-prob\')\n                wrapped_cell = tf.contrib.rnn.DropoutWrapper(cell,\n                                                             input_keep_prob=keep_prob,\n                                                             output_keep_prob=keep_prob)\n            outputs__, states = tf.nn.static_rnn(wrapped_cell, x,\n                                                 dtype=tf.float32)\n            outputs = []\n            for output in outputs__:\n                outputs.append(tf.expand_dims(output, axis=1))\n            # Convert back to bag form\n            outputs = tf.concat(outputs, axis=1, name=\'concat-output\')\n            dims = [-1, self.numSubinstance, self.numTimeSteps, self.numHidden]\n            output = tf.reshape(outputs, dims, name=\'bag-output\')\n\n        FastRNNVars = cell.variables\n        self.varList.extend(FastRNNVars)\n        if self.useDropout:\n            self.keep_prob = keep_prob\n        self.output = output\n        return self.output\n\n    def _restoreBaseGraph(self, graph, **kwargs):\n        assert self.graphCreated is False\n        assert self.graph is not None\n        scope = self._scope\n        if self.useDropout:\n            self.keep_prob = graph.get_tensor_by_name(scope + \'keep-prob:0\')\n        self.output = graph.get_tensor_by_name(scope + \'bag-output:0\')\n\n        assert len(self.varList) is 0\n        if self.wRank is None:\n            W = graph.get_tensor_by_name(\n                ""rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/W:0"")\n            self.varList = [W]\n        else:\n            W1 = graph.get_tensor_by_name(\n                ""rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/W1:0"")\n            W2 = graph.get_tensor_by_name(\n                ""rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/W2:0"")\n            self.varList = [W1, W2]\n\n        if self.uRank is None:\n            U = graph.get_tensor_by_name(\n                ""rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/U:0"")\n            self.varList.extend([U])\n        else:\n            U1 = graph.get_tensor_by_name(\n                ""rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/U1:0"")\n            U2 = graph.get_tensor_by_name(\n                ""rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/U2:0"")\n            self.varList.extend([U1, U2])\n\n        alpha = graph.get_tensor_by_name(\n            ""rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/alpha:0"")\n        beta = graph.get_tensor_by_name(\n            ""rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/beta:0"")\n        bias = graph.get_tensor_by_name(\n            ""rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/B_h:0"")\n        self.varList.extend([alpha, beta, bias])\n\n    def getModelParams(self):\n        \'\'\'\n        Returns the FastRNN model tensors.\n        In the order of  [W(W1, W2), U(U1,U2), alpha, beta, B_h]\n        () implies that the matrix can be replaced with the matrices inside.\n        \'\'\'\n        assert self.graphCreated is True, ""Graph is not created""\n        return self.varList\n\n    def addBaseAssignOps(self, graph, initVarList, **kwargs):\n        \'\'\'\n        Adds Tensorflow assignment operations to all of the model tensors.\n        These operations can then be used to initialize these tensors from\n        numpy matrices by running these operators\n\n        initVarList: A list of numpy matrices that will be used for\n            initialization by the assignment operation. For EMI_FastRNN, this\n            should be list of numpy matrices corresponding to  [W(W1, W2),\n            U(U1,U2), alpha, beta, B_h]\n        \'\'\'\n        assert initVarList is not None\n        index = 0\n        if self.wRank is None:\n            W_ = graph.get_tensor_by_name(\n                ""rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/W:0"")\n            W = initVarList[0]\n            w_op = tf.assign(W_, W)\n            self.assignOps.extend([w_op])\n            index += 1\n        else:\n            W1_ = graph.get_tensor_by_name(\n                ""rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/W1:0"")\n            W2_ = graph.get_tensor_by_name(\n                ""rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/W2:0"")\n            W1, W2 = initVarList[0], initVarList[1]\n            w1_op = tf.assign(W1_, W1)\n            w2_op = tf.assign(W2_, W2)\n            self.assignOps.extend([w1_op, w2_op])\n            index += 2\n\n        if self.uRank is None:\n            U_ = graph.get_tensor_by_name(\n                ""rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/U:0"")\n            U = initVarList[index]\n            u_op = tf.assign(U_, U)\n            self.assignOps.extend([u_op])\n            index += 1\n        else:\n            U1_ = graph.get_tensor_by_name(\n                ""rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/U1:0"")\n            U2_ = graph.get_tensor_by_name(\n                ""rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/U2:0"")\n            U1, U2 = initVarList[index], initVarList[index + 1]\n            u1_op = tf.assign(U1_, U1)\n            u2_op = tf.assign(U2_, U2)\n            self.assignOps.extend([u1_op, u2_op])\n            index += 2\n\n        alpha_ = graph.get_tensor_by_name(\n            ""rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/alpha:0"")\n        beta_ = graph.get_tensor_by_name(\n            ""rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/beta:0"")\n        bias_ = graph.get_tensor_by_name(\n            ""rnn/fast_rnn_cell/EMI-FastRNN-Cell/FastRNNcell/B_h:0"")\n\n        alpha, beta, bias = initVarList[index], initVarList[\n            index + 1], initVarList[index + 2]\n        alpha_op = tf.assign(alpha_, alpha)\n        beta_op = tf.assign(beta_, beta)\n        bias_op = tf.assign(bias_, bias)\n\n        self.assignOps.extend([alpha_op, beta_op, bias_op])\n\n\nclass EMI_UGRNN(EMI_RNN):\n\n    def __init__(self, numSubinstance, numHidden, numTimeSteps,\n                 numFeats, graph=None, forgetBias=1.0, useDropout=False):\n        \'\'\'\n        EMI-RNN using UGRNN cell. The architecture consists of a single UGRNN\n        layer followed by a secondary classifier. The secondary classifier is\n        not defined as part of this module and is left for the user to define,\n        through the redefinition of the \'_createExtendedGraph\' and\n        \'_restoreExtendedGraph\' methods.\n\n        This class supports restoring from a meta-graph. Provide the restored\n        graph as value to the graph keyword to enable this behaviour.\n\n        numSubinstance: Number of sub-instance.\n        numHidden: The dimension of the hidden state.\n        numTimeSteps: The number of time steps of the RNN.\n        numFeats: The feature vector dimension for each time step.\n        graph: A restored metagraph. Provide a graph if restoring form a meta\n            graph is required.\n        forgetBias: Bias for the forget gate of the UGRNN.\n        useDropout: Set to True if a dropout layer is to be added between\n            inputs and outputs to the RNN.\n        \'\'\'\n        self.numHidden = numHidden\n        self.numTimeSteps = numTimeSteps\n        self.numFeats = numFeats\n        self.useDropout = useDropout\n        self.forgetBias = forgetBias\n        self.numSubinstance = numSubinstance\n        self.graph = graph\n        self.graphCreated = False\n        # Restore or initialize\n        self.keep_prob = None\n        self.varList = []\n        self.output = None\n        self.assignOps = []\n        # Internal\n        self._scope = \'EMI/UGRNN/\'\n\n    def _createBaseGraph(self, X, **kwargs):\n        assert self.graphCreated is False\n        msg = \'X should be of form [-1, numSubinstance, numTimeSteps, numFeatures]\'\n        assert X.get_shape().ndims == 4, msg\n        assert X.shape[1] == self.numSubinstance\n        assert X.shape[2] == self.numTimeSteps\n        assert X.shape[3] == self.numFeats\n        # Reshape into 3D such that the first dimension is -1 * numSubinstance\n        # where each numSubinstance segment corresponds to one bag\n        # then shape it back in into 4D\n        scope = self._scope\n        keep_prob = None\n        with tf.name_scope(scope):\n            x = tf.reshape(X, [-1, self.numTimeSteps, self.numFeats])\n            x = tf.unstack(x, num=self.numTimeSteps, axis=1)\n            # Get the UGRNN output\n            cell = tf.contrib.rnn.UGRNNCell(self.numHidden,\n                                            forget_bias=self.forgetBias)\n            wrapped_cell = cell\n            if self.useDropout is True:\n                keep_prob = tf.placeholder(dtype=tf.float32, name=\'keep-prob\')\n                wrapped_cell = tf.contrib.rnn.DropoutWrapper(cell,\n                                                             input_keep_prob=keep_prob,\n                                                             output_keep_prob=keep_prob)\n            outputs__, states = tf.nn.static_rnn(\n                wrapped_cell, x, dtype=tf.float32)\n            outputs = []\n            for output in outputs__:\n                outputs.append(tf.expand_dims(output, axis=1))\n            # Convert back to bag form\n            outputs = tf.concat(outputs, axis=1, name=\'concat-output\')\n            dims = [-1, self.numSubinstance, self.numTimeSteps, self.numHidden]\n            output = tf.reshape(outputs, dims, name=\'bag-output\')\n\n        UGRNNVars = cell.variables\n        self.varList.extend(UGRNNVars)\n        if self.useDropout:\n            self.keep_prob = keep_prob\n        self.output = output\n        return self.output\n\n    def _restoreBaseGraph(self, graph, **kwargs):\n        assert self.graphCreated is False\n        assert self.graph is not None\n        scope = self._scope\n        if self.useDropout:\n            self.keep_prob = graph.get_tensor_by_name(scope + \'keep-prob:0\')\n        self.output = graph.get_tensor_by_name(scope + \'bag-output:0\')\n        kernel = graph.get_tensor_by_name(""rnn/ugrnn_cell/kernel:0"")\n        bias = graph.get_tensor_by_name(""rnn/ugrnn_cell/bias:0"")\n        assert len(self.varList) is 0\n        self.varList = [kernel, bias]\n\n    def getModelParams(self):\n        \'\'\'\n        Returns the FastRRNN model tensors.\n        returns [kernel, bias]\n        \'\'\'\n        assert self.graphCreated is True, ""Graph is not created""\n        assert len(self.varList) == 2\n        return self.varList\n\n    def addBaseAssignOps(self, graph, initVarList, **kwargs):\n        \'\'\'\n        Adds Tensorflow assignment operations to all of the model tensors.\n        These operations can then be used to initialize these tensors from\n        numpy matrices by running these operators\n\n        initVarList: A list of numpy matrices that will be used for\n            initialization by the assignment operation. For EMI_UGRNN, this\n            should be list of numpy matrices corresponding to  [kernel, bias]\n        \'\'\'\n        assert initVarList is not None\n        assert len(initVarList) == 2\n        k_ = graph.get_tensor_by_name(\'rnn/ugrnn_cell/kernel:0\')\n        b_ = graph.get_tensor_by_name(\'rnn/ugrnn_cell/bias:0\')\n        kernel, bias = initVarList[-2], initVarList[-1]\n        k_op = tf.assign(k_, kernel)\n        b_op = tf.assign(b_, bias)\n        self.assignOps.extend([k_op, b_op])\n\n\nclass EMI_FastGRNN(EMI_RNN):\n\n    def __init__(self, numSubinstance, numHidden, numTimeSteps, numFeats,\n                 graph=None, useDropout=False, gate_non_linearity=""sigmoid"",\n                 update_non_linearity=""tanh"", wRank=None, uRank=None,\n                 zetaInit=1.0, nuInit=-4.0):\n        \'\'\'\n        EMI-RNN using FastGRNN cell. The architecture consists of a single\n        FastGRNN layer followed by a secondary classifier. The secondary\n        classifier is not defined as part of this module and is left for the\n        user to define, through the redefinition of the \'_createExtendedGraph\'\n        and \'_restoreExtendedGraph\' methods.\n\n        This class supports restoring from a meta-graph. Provide the restored\n        graph as value to the graph keyword to enable this behaviour.\n\n        numSubinstance: Number of sub-instance.\n        numHidden: The dimension of the hidden state.\n        numTimeSteps: The number of time steps of the RNN.\n        numFeats: The feature vector dimension for each time step.\n        graph: A restored metagraph. Provide a graph if restoring form a meta\n            graph is required.\n        useDropout: Set to True if a dropout layer is to be added\n            between inputs and outputs to the RNN.\n\n        gate_non_linearity, update_non_linearity, wRank, uRank, zetaInit,\n        nuInit:\n            These are FastGRNN parameters. Please refer to FastGRNN documentation\n            for more information.\n        \'\'\'\n        self.numHidden = numHidden\n        self.numTimeSteps = numTimeSteps\n        self.numFeats = numFeats\n        self.useDropout = useDropout\n        self.numSubinstance = numSubinstance\n        self.graph = graph\n\n        self.gate_non_linearity = gate_non_linearity\n        self.update_non_linearity = update_non_linearity\n        self.wRank = wRank\n        self.uRank = uRank\n        self.zetaInit = zetaInit\n        self.nuInit = nuInit\n\n        self.graphCreated = False\n        # Restore or initialize\n        self.keep_prob = None\n        self.varList = []\n        self.output = None\n        self.assignOps = []\n        # Internal\n        self._scope = \'EMI/FastGRNN/\'\n\n    def _createBaseGraph(self, X, **kwargs):\n        assert self.graphCreated is False\n        msg = \'X should be of form [-1, numSubinstance, numTimeSteps, numFeatures]\'\n        assert X.get_shape().ndims == 4, msg\n        assert X.shape[1] == self.numSubinstance\n        assert X.shape[2] == self.numTimeSteps\n        assert X.shape[3] == self.numFeats\n        # Reshape into 3D suself.h that the first dimension is -1 * numSubinstance\n        # where each numSubinstance segment corresponds to one bag\n        # then shape it back in into 4D\n        scope = self._scope\n        keep_prob = None\n        with tf.name_scope(scope):\n            x = tf.reshape(X, [-1, self.numTimeSteps, self.numFeats])\n            x = tf.unstack(x, num=self.numTimeSteps, axis=1)\n            # Get the FastGRNN output\n            cell = FastGRNNCell(self.numHidden, self.gate_non_linearity,\n                                self.update_non_linearity, self.wRank,\n                                self.uRank, self.zetaInit, self.nuInit,\n                                name=\'EMI-FastGRNN-Cell\')\n            wrapped_cell = cell\n            if self.useDropout is True:\n                keep_prob = tf.placeholder(dtype=tf.float32, name=\'keep-prob\')\n                wrapped_cell = tf.contrib.rnn.DropoutWrapper(cell,\n                                                             input_keep_prob=keep_prob,\n                                                             output_keep_prob=keep_prob)\n            outputs__, states = tf.nn.static_rnn(\n                wrapped_cell, x, dtype=tf.float32)\n            outputs = []\n            for output in outputs__:\n                outputs.append(tf.expand_dims(output, axis=1))\n            # Convert back to bag form\n            outputs = tf.concat(outputs, axis=1, name=\'concat-output\')\n            dims = [-1, self.numSubinstance, self.numTimeSteps, self.numHidden]\n            output = tf.reshape(outputs, dims, name=\'bag-output\')\n\n        FastGRNNVars = cell.variables\n        self.varList.extend(FastGRNNVars)\n        if self.useDropout:\n            self.keep_prob = keep_prob\n        self.output = output\n        return self.output\n\n    def _restoreBaseGraph(self, graph, **kwargs):\n        assert self.graphCreated is False\n        assert self.graph is not None\n        scope = self._scope\n        if self.useDropout:\n            self.keep_prob = graph.get_tensor_by_name(scope + \'keep-prob:0\')\n        self.output = graph.get_tensor_by_name(scope + \'bag-output:0\')\n\n        assert len(self.varList) is 0\n        if self.wRank is None:\n            W = graph.get_tensor_by_name(\n                ""rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/W:0"")\n            self.varList = [W]\n        else:\n            W1 = graph.get_tensor_by_name(\n                ""rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/W1:0"")\n            W2 = graph.get_tensor_by_name(\n                ""rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/W2:0"")\n            self.varList = [W1, W2]\n\n        if self.uRank is None:\n            U = graph.get_tensor_by_name(\n                ""rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/U:0"")\n            self.varList.extend([U])\n        else:\n            U1 = graph.get_tensor_by_name(\n                ""rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/U1:0"")\n            U2 = graph.get_tensor_by_name(\n                ""rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/U2:0"")\n            self.varList.extend([U1, U2])\n\n        zeta = graph.get_tensor_by_name(\n            ""rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/zeta:0"")\n        nu = graph.get_tensor_by_name(\n            ""rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/nu:0"")\n        gate_bias = graph.get_tensor_by_name(\n            ""rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/B_g:0"")\n        update_bias = graph.get_tensor_by_name(\n            ""rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/B_h:0"")\n        self.varList.extend([zeta, nu, gate_bias, update_bias])\n\n    def getModelParams(self):\n        \'\'\'\n        Returns the FastGRNN model tensors.\n        In the order of  [W(W1, W2), U(U1,U2), zeta, nu, B_g, B_h]\n        () implies that the matrix can be replaced with the matrices inside.\n        \'\'\'\n        assert self.graphCreated is True, ""Graph is not created""\n        return self.varList\n\n    def addBaseAssignOps(self, graph, initVarList, **kwargs):\n        \'\'\'\n        Adds Tensorflow assignment operations to all of the model tensors.\n        These operations can then be used to initialize these tensors from\n        numpy matrices by running these operators\n\n        initVarList: A list of numpy matrices that will be used for\n            initialization by the assignment operation. For EMI_FastGRNN, this\n            should be list of numpy matrices corresponding to  [W(W1, W2),\n            U(U1,U2), zeta, nu, B_g, B_h]\n        \'\'\'\n        assert initVarList is not None\n        index = 0\n        if self.wRank is None:\n            W_ = graph.get_tensor_by_name(\n                ""rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/W:0"")\n            W = initVarList[0]\n            w_op = tf.assign(W_, W)\n            self.assignOps.extend([w_op])\n            index += 1\n        else:\n            W1_ = graph.get_tensor_by_name(\n                ""rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/W1:0"")\n            W2_ = graph.get_tensor_by_name(\n                ""rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/W2:0"")\n            W1, W2 = initVarList[0], initVarList[1]\n            w1_op = tf.assign(W1_, W1)\n            w2_op = tf.assign(W2_, W2)\n            self.assignOps.extend([w1_op, w2_op])\n            index += 2\n\n        if self.uRank is None:\n            U_ = graph.get_tensor_by_name(\n                ""rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/U:0"")\n            U = initVarList[index]\n            u_op = tf.assign(U_, U)\n            self.assignOps.extend([u_op])\n            index += 1\n        else:\n            U1_ = graph.get_tensor_by_name(\n                ""rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/U1:0"")\n            U2_ = graph.get_tensor_by_name(\n                ""rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/U2:0"")\n            U1, U2 = initVarList[index], initVarList[index + 1]\n            u1_op = tf.assign(U1_, U1)\n            u2_op = tf.assign(U2_, U2)\n            self.assignOps.extend([u1_op, u2_op])\n            index += 2\n\n        zeta_ = graph.get_tensor_by_name(\n            ""rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/zeta:0"")\n        nu_ = graph.get_tensor_by_name(\n            ""rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/nu:0"")\n        gate_bias_ = graph.get_tensor_by_name(\n            ""rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/B_g:0"")\n        update_bias_ = graph.get_tensor_by_name(\n            ""rnn/fast_grnn_cell/EMI-FastGRNN-Cell/FastGRNNcell/B_h:0"")\n\n        zeta, nu, gate_bias, update_bias = initVarList[index], initVarList[\n            index + 1], initVarList[index + 2], initVarList[index + 3]\n        zeta_op = tf.assign(zeta_, zeta)\n        nu_op = tf.assign(nu_, nu)\n        gate_bias_op = tf.assign(gate_bias_, gate_bias)\n        update_bias_op = tf.assign(update_bias_, update_bias)\n\n        self.assignOps.extend([zeta_op, nu_op, gate_bias_op, update_bias_op])\n'"
tf/edgeml_tf/trainer/__init__.py,0,b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n'
tf/edgeml_tf/trainer/bonsaiTrainer.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nfrom __future__ import print_function\nimport tensorflow as tf\nimport edgeml_tf.utils as utils\nimport numpy as np\nimport os\nimport sys\n\n\nclass BonsaiTrainer:\n\n    def __init__(self, bonsaiObj, lW, lT, lV, lZ, sW, sT, sV, sZ,\n                 learningRate, X, Y, useMCHLoss=False, outFile=None, regLoss=\'huber\'):\n        \'\'\'\n        bonsaiObj - Initialised Bonsai Object and Graph\n        lW, lT, lV and lZ are regularisers to Bonsai Params\n        sW, sT, sV and sZ are sparsity factors to Bonsai Params\n        learningRate - learningRate fro optimizer\n        X is the Data Placeholder - Dims [_, dataDimension]\n        Y - Label placeholder for loss computation\n        useMCHLoss - For choice between HingeLoss vs CrossEntropy\n        useMCHLoss - True - MultiClass - multiClassHingeLoss\n        useMCHLoss - False - MultiClass - crossEntropyLoss\n        \'\'\'\n\n        self.bonsaiObj = bonsaiObj\n        self.regressionLoss = regLoss\n\n        self.lW = lW\n        self.lV = lV\n        self.lT = lT\n        self.lZ = lZ\n\n        self.sW = sW\n        self.sV = sV\n        self.sT = sT\n        self.sZ = sZ\n\n        self.Y = Y\n        self.X = X\n\n        self.useMCHLoss = useMCHLoss\n\n        if outFile is not None:\n            print(""Outfile : "", outFile)\n            self.outFile = open(outFile, \'w\')\n        else:\n            self.outFile = sys.stdout\n\n        self.learningRate = learningRate\n\n        self.assertInit()\n\n        self.sigmaI = tf.placeholder(tf.float32, name=\'sigmaI\')\n\n        self.score, self.X_ = self.bonsaiObj(self.X, self.sigmaI)\n\n        self.loss, self.marginLoss, self.regLoss = self.lossGraph()\n\n        self.trainStep = self.trainGraph()\n        \'\'\'\n        self.accuracy -> \'MAE\' for Regression.\n        self.accuracy -> \'Accuracy\' for Classification.\n        \'\'\'\n        self.accuracy = self.accuracyGraph()\n        self.prediction = self.bonsaiObj.getPrediction()\n\n        if self.sW > 0.99 and self.sV > 0.99 and self.sZ > 0.99 and self.sT > 0.99:\n            self.isDenseTraining = True\n        else:\n            self.isDenseTraining = False\n\n        self.hardThrsd()\n        self.sparseTraining()\n\n    def lossGraph(self):\n        \'\'\'\n        Loss Graph for given Bonsai Obj\n        \'\'\'\n        self.regLoss = 0.5 * (self.lZ * tf.square(tf.norm(self.bonsaiObj.Z)) +\n                              self.lW * tf.square(tf.norm(self.bonsaiObj.W)) +\n                              self.lV * tf.square(tf.norm(self.bonsaiObj.V)) +\n                              self.lT * tf.square(tf.norm(self.bonsaiObj.T)))\n\n        # Loss functions for classification.\n        if (self.bonsaiObj.isRegression is False):\n            if (self.bonsaiObj.numClasses > 2):\n                if self.useMCHLoss is True:\n                    self.batch_th = tf.placeholder(tf.int64, name=\'batch_th\')\n                    self.marginLoss = utils.multiClassHingeLoss(\n                        tf.transpose(self.score), self.Y,\n                        self.batch_th)\n                else:\n                    self.marginLoss = utils.crossEntropyLoss(\n                        tf.transpose(self.score), self.Y)\n                self.loss = self.marginLoss + self.regLoss\n            else:\n                self.marginLoss = tf.reduce_mean(tf.nn.relu(\n                    1.0 - (2 * self.Y - 1) * tf.transpose(self.score)))\n                self.loss = self.marginLoss + self.regLoss\n\n        # Loss functions for regression.\n        elif (self.bonsaiObj.isRegression is True):\n            if(self.regressionLoss == \'huber\'):\n                # Use of Huber Loss , because it is more robust to outliers.\n                self.marginLoss = tf.losses.huber_loss(\n                    self.Y, tf.transpose(self.score))\n                self.loss = self.marginLoss + self.regLoss\n            elif (self.regressionLoss == \'l2\'):\n                # L2 loss function.\n                self.marginLoss = tf.nn.l2_loss(\n                    self.Y - tf.transpose(self.score))\n                self.loss = self.marginLoss + self.regLoss\n\n        return self.loss, self.marginLoss, self.regLoss\n\n    def trainGraph(self):\n        \'\'\'\n        Train Graph for the loss generated by Bonsai\n        \'\'\'\n        self.bonsaiObj.TrainStep = tf.train.AdamOptimizer(\n            self.learningRate).minimize(self.loss)\n\n        return self.bonsaiObj.TrainStep\n\n    def accuracyGraph(self):\n        \'\'\'\n        Accuracy Graph to evaluate accuracy when needed\n        \'\'\'\n        if(self.bonsaiObj.isRegression is False):\n            if (self.bonsaiObj.numClasses > 2):\n                correctPrediction = tf.equal(\n                    tf.argmax(tf.transpose(self.score), 1), tf.argmax(self.Y, 1))\n                self.accuracy = tf.reduce_mean(\n                    tf.cast(correctPrediction, tf.float32))\n            else:\n                y_ = self.Y * 2 - 1\n                correctPrediction = tf.multiply(tf.transpose(self.score), y_)\n                correctPrediction = tf.nn.relu(correctPrediction)\n                correctPrediction = tf.ceil(tf.tanh(correctPrediction))\n                self.accuracy = tf.reduce_mean(\n                    tf.cast(correctPrediction, tf.float32))\n\n        elif (self.bonsaiObj.isRegression is True):\n            # Accuracy for regression , in terms of mean absolute error.\n            self.accuracy = utils.mean_absolute_error(tf.reshape(\n                self.score, [-1, 1]), tf.reshape(self.Y, [-1, 1]))\n        return self.accuracy\n\n    def hardThrsd(self):\n        \'\'\'\n        Set up for hard Thresholding Functionality\n        \'\'\'\n        self.__Wth = tf.placeholder(tf.float32, name=\'Wth\')\n        self.__Vth = tf.placeholder(tf.float32, name=\'Vth\')\n        self.__Zth = tf.placeholder(tf.float32, name=\'Zth\')\n        self.__Tth = tf.placeholder(tf.float32, name=\'Tth\')\n\n        self.__Woph = self.bonsaiObj.W.assign(self.__Wth)\n        self.__Voph = self.bonsaiObj.V.assign(self.__Vth)\n        self.__Toph = self.bonsaiObj.T.assign(self.__Tth)\n        self.__Zoph = self.bonsaiObj.Z.assign(self.__Zth)\n\n        self.hardThresholdGroup = tf.group(\n            self.__Woph, self.__Voph, self.__Toph, self.__Zoph)\n\n    def sparseTraining(self):\n        \'\'\'\n        Set up for Sparse Retraining Functionality\n        \'\'\'\n        self.__Wops = self.bonsaiObj.W.assign(self.__Wth)\n        self.__Vops = self.bonsaiObj.V.assign(self.__Vth)\n        self.__Zops = self.bonsaiObj.Z.assign(self.__Zth)\n        self.__Tops = self.bonsaiObj.T.assign(self.__Tth)\n\n        self.sparseRetrainGroup = tf.group(\n            self.__Wops, self.__Vops, self.__Tops, self.__Zops)\n\n    def runHardThrsd(self, sess):\n        \'\'\'\n        Function to run the IHT routine on Bonsai Obj\n        \'\'\'\n        currW = self.bonsaiObj.W.eval()\n        currV = self.bonsaiObj.V.eval()\n        currZ = self.bonsaiObj.Z.eval()\n        currT = self.bonsaiObj.T.eval()\n\n        self.__thrsdW = utils.hardThreshold(currW, self.sW)\n        self.__thrsdV = utils.hardThreshold(currV, self.sV)\n        self.__thrsdZ = utils.hardThreshold(currZ, self.sZ)\n        self.__thrsdT = utils.hardThreshold(currT, self.sT)\n\n        fd_thrsd = {self.__Wth: self.__thrsdW, self.__Vth: self.__thrsdV,\n                    self.__Zth: self.__thrsdZ, self.__Tth: self.__thrsdT}\n        sess.run(self.hardThresholdGroup, feed_dict=fd_thrsd)\n\n    def runSparseTraining(self, sess):\n        \'\'\'\n        Function to run the Sparse Retraining routine on Bonsai Obj\n        \'\'\'\n        currW = self.bonsaiObj.W.eval()\n        currV = self.bonsaiObj.V.eval()\n        currZ = self.bonsaiObj.Z.eval()\n        currT = self.bonsaiObj.T.eval()\n\n        newW = utils.copySupport(self.__thrsdW, currW)\n        newV = utils.copySupport(self.__thrsdV, currV)\n        newZ = utils.copySupport(self.__thrsdZ, currZ)\n        newT = utils.copySupport(self.__thrsdT, currT)\n\n        fd_st = {self.__Wth: newW, self.__Vth: newV,\n                 self.__Zth: newZ, self.__Tth: newT}\n        sess.run(self.sparseRetrainGroup, feed_dict=fd_st)\n\n    def assertInit(self):\n        err = ""sparsity must be between 0 and 1""\n        assert self.sW >= 0 and self.sW <= 1, ""W "" + err\n        assert self.sV >= 0 and self.sV <= 1, ""V "" + err\n        assert self.sZ >= 0 and self.sZ <= 1, ""Z "" + err\n        assert self.sT >= 0 and self.sT <= 1, ""T "" + err\n        errMsg = ""Dimension Mismatch, Y has to be [_, "" + \\\n            str(self.bonsaiObj.numClasses) + ""]""\n        errCont = "" numClasses are 1 in case of Binary case by design""\n        assert (len(self.Y.shape) == 2 and\n                self.Y.shape[1] == self.bonsaiObj.numClasses), errMsg + errCont\n\n    def saveParams(self, currDir):\n        \'\'\'\n        Function to save Parameter matrices into a given folder\n        \'\'\'\n        paramDir = currDir + \'/\'\n        np.save(paramDir + ""W.npy"", self.bonsaiObj.W.eval())\n        np.save(paramDir + ""V.npy"", self.bonsaiObj.V.eval())\n        np.save(paramDir + ""T.npy"", self.bonsaiObj.T.eval())\n        np.save(paramDir + ""Z.npy"", self.bonsaiObj.Z.eval())\n        hyperParamDict = {\'dataDim\': self.bonsaiObj.dataDimension,\n                          \'projDim\': self.bonsaiObj.projectionDimension,\n                          \'numClasses\': self.bonsaiObj.numClasses,\n                          \'depth\': self.bonsaiObj.treeDepth,\n                          \'sigma\': self.bonsaiObj.sigma}\n        hyperParamFile = paramDir + \'hyperParam.npy\'\n        np.save(hyperParamFile, hyperParamDict)\n\n    def saveParamsForSeeDot(self, currDir):\n        \'\'\'\n        Function to save Parameter matrices into a given folder for SeeDot compiler\n        \'\'\'\n        seeDotDir = currDir + \'/SeeDot/\'\n\n        if os.path.isdir(seeDotDir) is False:\n            try:\n                os.mkdir(seeDotDir)\n            except OSError:\n                print(""Creation of the directory %s failed"" %\n                      seeDotDir)\n\n        np.savetxt(seeDotDir + ""W"",\n                   utils.restructreMatrixBonsaiSeeDot(self.bonsaiObj.W.eval(),\n                                                      self.bonsaiObj.numClasses,\n                                                      self.bonsaiObj.totalNodes),\n                   delimiter=""\\t"")\n        np.savetxt(seeDotDir + ""V"",\n                   utils.restructreMatrixBonsaiSeeDot(self.bonsaiObj.V.eval(),\n                                                      self.bonsaiObj.numClasses,\n                                                      self.bonsaiObj.totalNodes),\n                   delimiter=""\\t"")\n        np.savetxt(seeDotDir + ""T"", self.bonsaiObj.T.eval(), delimiter=""\\t"")\n        np.savetxt(seeDotDir + ""Z"", self.bonsaiObj.Z.eval(), delimiter=""\\t"")\n        np.savetxt(seeDotDir + ""Sigma"",\n                   np.array([self.bonsaiObj.sigma]), delimiter=""\\t"")\n\n    def loadModel(self, currDir):\n        \'\'\'\n        Load the Saved model and load it to the model using constructor\n        Returns two dict one for params and other for hyperParams\n        \'\'\'\n        paramDir = currDir + \'/\'\n        paramDict = {}\n        paramDict[\'W\'] = np.load(paramDir + ""W.npy"")\n        paramDict[\'V\'] = np.load(paramDir + ""V.npy"")\n        paramDict[\'T\'] = np.load(paramDir + ""T.npy"")\n        paramDict[\'Z\'] = np.load(paramDir + ""Z.npy"")\n        hyperParamDict = np.load(paramDir + ""hyperParam.npy"").item()\n        return paramDict, hyperParamDict\n\n    # Function to get aimed model size\n    def getModelSize(self):\n        \'\'\'\n        Function to get aimed model size\n        \'\'\'\n        nnzZ, sizeZ, sparseZ = utils.countnnZ(self.bonsaiObj.Z, self.sZ)\n        nnzW, sizeW, sparseW = utils.countnnZ(self.bonsaiObj.W, self.sW)\n        nnzV, sizeV, sparseV = utils.countnnZ(self.bonsaiObj.V, self.sV)\n        nnzT, sizeT, sparseT = utils.countnnZ(self.bonsaiObj.T, self.sT)\n\n        totalnnZ = (nnzZ + nnzT + nnzV + nnzW)\n        totalSize = (sizeZ + sizeW + sizeV + sizeT)\n        hasSparse = (sparseW or sparseV or sparseT or sparseZ)\n        return totalnnZ, totalSize, hasSparse\n\n    def train(self, batchSize, totalEpochs, sess,\n              Xtrain, Xtest, Ytrain, Ytest, dataDir, currDir):\n        \'\'\'\n        The Dense - IHT - Sparse Retrain Routine for Bonsai Training\n        \'\'\'\n        resultFile = open(dataDir + \'/TFBonsaiResults.txt\', \'a+\')\n        numIters = Xtrain.shape[0] / batchSize\n\n        totalBatches = numIters * totalEpochs\n\n        bonsaiObjSigmaI = 1\n\n        counter = 0\n        if self.bonsaiObj.numClasses > 2:\n            trimlevel = 15\n        else:\n            trimlevel = 5\n        ihtDone = 0\n        if (self.bonsaiObj.isRegression is True):\n            maxTestAcc = 100000007\n        else:\n            maxTestAcc = -10000\n        if self.isDenseTraining is True:\n            ihtDone = 1\n            bonsaiObjSigmaI = 1\n            itersInPhase = 0\n\n        header = \'*\' * 20\n        for i in range(totalEpochs):\n            print(""\\nEpoch Number: "" + str(i), file=self.outFile)\n\n            \'\'\'\n            trainAcc -> For Regression, it is \'Mean Absolute Error\'.\n            trainAcc -> For Classification, it is \'Accuracy\'.\n            \'\'\'\n            trainAcc = 0.0\n            trainLoss = 0.0\n\n            numIters = int(numIters)\n            for j in range(numIters):\n\n                if counter == 0:\n                    msg = "" Dense Training Phase Started ""\n                    print(""\\n%s%s%s\\n"" %\n                          (header, msg, header), file=self.outFile)\n\n                # Updating the indicator sigma\n                if ((counter == 0) or (counter == int(totalBatches / 3.0)) or\n                        (counter == int(2 * totalBatches / 3.0))) and (self.isDenseTraining is False):\n                    bonsaiObjSigmaI = 1\n                    itersInPhase = 0\n\n                elif (itersInPhase % 100 == 0):\n                    indices = np.random.choice(Xtrain.shape[0], 100)\n                    batchX = Xtrain[indices, :]\n                    batchY = Ytrain[indices, :]\n                    batchY = np.reshape(\n                        batchY, [-1, self.bonsaiObj.numClasses])\n\n                    _feed_dict = {self.X: batchX}\n                    Xcapeval = self.X_.eval(feed_dict=_feed_dict)\n                    Teval = self.bonsaiObj.T.eval()\n\n                    sum_tr = 0.0\n                    for k in range(0, self.bonsaiObj.internalNodes):\n                        sum_tr += (np.sum(np.abs(np.dot(Teval[k], Xcapeval))))\n\n                    if(self.bonsaiObj.internalNodes > 0):\n                        sum_tr /= (100 * self.bonsaiObj.internalNodes)\n                        sum_tr = 0.1 / sum_tr\n                    else:\n                        sum_tr = 0.1\n                    sum_tr = min(\n                        1000, sum_tr * (2**(float(itersInPhase) /\n                                            (float(totalBatches) / 30.0))))\n\n                    bonsaiObjSigmaI = sum_tr\n\n                itersInPhase += 1\n                batchX = Xtrain[j * batchSize:(j + 1) * batchSize]\n                batchY = Ytrain[j * batchSize:(j + 1) * batchSize]\n                batchY = np.reshape(\n                    batchY, [-1, self.bonsaiObj.numClasses])\n\n                if self.bonsaiObj.numClasses > 2:\n                    if self.useMCHLoss is True:\n                        _feed_dict = {self.X: batchX, self.Y: batchY,\n                                      self.batch_th: batchY.shape[0],\n                                      self.sigmaI: bonsaiObjSigmaI}\n                    else:\n                        _feed_dict = {self.X: batchX, self.Y: batchY,\n                                      self.sigmaI: bonsaiObjSigmaI}\n                else:\n                    _feed_dict = {self.X: batchX, self.Y: batchY,\n                                  self.sigmaI: bonsaiObjSigmaI}\n\n                # Mini-batch training\n                _, batchLoss, batchAcc = sess.run(\n                    [self.trainStep, self.loss, self.accuracy],\n                    feed_dict=_feed_dict)\n\n                # Classification.\n                if (self.bonsaiObj.isRegression is False):\n                    trainAcc += batchAcc\n                    trainLoss += batchLoss\n                # Regression.\n                else:\n                    trainAcc += np.mean(batchAcc)\n                    trainLoss += np.mean(batchLoss)\n\n                # Training routine involving IHT and sparse retraining\n                if (counter >= int(totalBatches / 3.0) and\n                    (counter < int(2 * totalBatches / 3.0)) and\n                    counter % trimlevel == 0 and\n                        self.isDenseTraining is False):\n                    self.runHardThrsd(sess)\n                    if ihtDone == 0:\n                        msg = "" IHT Phase Started ""\n                        print(""\\n%s%s%s\\n"" %\n                              (header, msg, header), file=self.outFile)\n                    ihtDone = 1\n                elif ((ihtDone == 1 and counter >= int(totalBatches / 3.0) and\n                       (counter < int(2 * totalBatches / 3.0)) and\n                       counter % trimlevel != 0 and\n                       self.isDenseTraining is False) or\n                        (counter >= int(2 * totalBatches / 3.0) and\n                            self.isDenseTraining is False)):\n                    self.runSparseTraining(sess)\n                    if counter == int(2 * totalBatches / 3.0):\n                        msg = "" Sparse Retraining Phase Started ""\n                        print(""\\n%s%s%s\\n"" %\n                              (header, msg, header), file=self.outFile)\n                counter += 1\n            try:\n                if (self.bonsaiObj.isRegression is True):\n                    print(""\\nRegression Train Loss: "" + str(trainLoss / numIters) +\n                          ""\\nTraining MAE (Regression): "" +\n                          str(trainAcc / numIters),\n                          file=self.outFile)\n                else:\n                    print(""\\nClassification Train Loss: "" + str(trainLoss / numIters) +\n                          ""\\nTraining accuracy (Classification): "" +\n                          str(trainAcc / numIters),\n                          file=self.outFile)\n            except:\n                continue\n\n            oldSigmaI = bonsaiObjSigmaI\n            bonsaiObjSigmaI = 1e9\n\n            if self.bonsaiObj.numClasses > 2:\n                if self.useMCHLoss is True:\n                    _feed_dict = {self.X: Xtest, self.Y: Ytest,\n                                  self.batch_th: Ytest.shape[0],\n                                  self.sigmaI: bonsaiObjSigmaI}\n                else:\n                    _feed_dict = {self.X: Xtest, self.Y: Ytest,\n                                  self.sigmaI: bonsaiObjSigmaI}\n            else:\n                _feed_dict = {self.X: Xtest, self.Y: Ytest,\n                              self.sigmaI: bonsaiObjSigmaI}\n\n            # This helps in direct testing instead of extracting the model out\n\n            testAcc, testLoss, regTestLoss, pred = sess.run(\n                [self.accuracy, self.loss, self.regLoss, self.prediction], feed_dict=_feed_dict)\n\n            if ihtDone == 0:\n                if (self.bonsaiObj.isRegression is False):\n                    maxTestAcc = -10000\n                    maxTestAccEpoch = i\n                elif (self.bonsaiObj.isRegression is True):\n                    maxTestAcc = testAcc\n                    maxTestAccEpoch = i\n\n            else:\n                if (self.bonsaiObj.isRegression is False):\n                    if maxTestAcc <= testAcc:\n                        maxTestAccEpoch = i\n                        maxTestAcc = testAcc\n                        self.saveParams(currDir)\n                        self.saveParamsForSeeDot(currDir)\n                elif (self.bonsaiObj.isRegression is True):\n                    print(""Minimum Training MAE : "", np.mean(maxTestAcc))\n                    if maxTestAcc >= testAcc:\n                        # For regression , we\'re more interested in the minimum\n                        # MAE.\n                        maxTestAccEpoch = i\n                        maxTestAcc = testAcc\n                        self.saveParams(currDir)\n                        self.saveParamsForSeeDot(currDir)\n\n            if (self.bonsaiObj.isRegression is True):\n                print(""Testing MAE %g"" % np.mean(testAcc), file=self.outFile)\n            else:\n                print(""Test accuracy %g"" % np.mean(testAcc), file=self.outFile)\n\n            if (self.bonsaiObj.isRegression is True):\n                testAcc = np.mean(testAcc)\n            else:\n                testAcc = testAcc\n                maxTestAcc = maxTestAcc\n\n            print(""MarginLoss + RegLoss: "" + str(testLoss - regTestLoss) +\n                  "" + "" + str(regTestLoss) + "" = "" + str(testLoss) + ""\\n"",\n                  file=self.outFile)\n            self.outFile.flush()\n\n            bonsaiObjSigmaI = oldSigmaI\n\n        # sigmaI has to be set to infinity to ensure\n        # only a single path is used in inference\n        bonsaiObjSigmaI = 1e9\n        print(""\\nNon-Zero : "" + str(self.getModelSize()[0]) + "" Model Size: "" +\n              str(float(self.getModelSize()[1]) / 1024.0) + "" KB hasSparse: "" +\n              str(self.getModelSize()[2]) + ""\\n"", file=self.outFile)\n\n        if (self.bonsaiObj.isRegression is True):\n            maxTestAcc = np.mean(maxTestAcc)\n\n        if (self.bonsaiObj.isRegression is True):\n            print(""For Regression, Minimum MAE at compressed"" +\n                  "" model size(including early stopping): "" +\n                  str(maxTestAcc) + "" at Epoch: "" +\n                  str(maxTestAccEpoch + 1) + ""\\nFinal Test"" +\n                  "" MAE: "" + str(testAcc), file=self.outFile)\n\n            resultFile.write(""MinTestMAE: "" + str(maxTestAcc) +\n                             "" at Epoch(totalEpochs): "" +\n                             str(maxTestAccEpoch + 1) +\n                             ""("" + str(totalEpochs) + "")"" + "" ModelSize: "" +\n                             str(float(self.getModelSize()[1]) / 1024.0) +\n                             "" KB hasSparse: "" + str(self.getModelSize()[2]) +\n                             "" Param Directory: "" +\n                             str(os.path.abspath(currDir)) + ""\\n"")\n\n        elif (self.bonsaiObj.isRegression is False):\n            print(""For Classification, Maximum Test accuracy at compressed"" +\n                  "" model size(including early stopping): "" +\n                  str(maxTestAcc) + "" at Epoch: "" +\n                  str(maxTestAccEpoch + 1) + ""\\nFinal Test"" +\n                  "" Accuracy: "" + str(testAcc), file=self.outFile)\n\n            resultFile.write(""MaxTestAcc: "" + str(maxTestAcc) +\n                             "" at Epoch(totalEpochs): "" +\n                             str(maxTestAccEpoch + 1) +\n                             ""("" + str(totalEpochs) + "")"" + "" ModelSize: "" +\n                             str(float(self.getModelSize()[1]) / 1024.0) +\n                             "" KB hasSparse: "" + str(self.getModelSize()[2]) +\n                             "" Param Directory: "" +\n                             str(os.path.abspath(currDir)) + ""\\n"")\n        print(""The Model Directory: "" + currDir + ""\\n"")\n\n        resultFile.close()\n        self.outFile.flush()\n\n        if self.outFile is not sys.stdout:\n            self.outFile.close()\n'"
tf/edgeml_tf/trainer/emirnnTrainer.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nfrom __future__ import print_function\nimport tensorflow as tf\nimport numpy as np\nimport sys\nimport edgeml_tf.utils as utils\nimport pandas as pd\n\nclass EMI_Trainer:\n    def __init__(self, numTimeSteps, numOutput, graph=None,\n                 stepSize=0.001, lossType=\'l2\', optimizer=\'Adam\',\n                 automode=True):\n        \'\'\'\n        The EMI-RNN trainer. This classes attaches loss functions and training\n        operations to the forward EMI-RNN graph. Currently, simple softmax loss\n        and l2 loss are supported on the outputs. For optimizers, only ADAM\n        optimizer is available.\n\n        numTimesteps: Number of time steps of the RNN model\n        numOutput: Number of output classes\n        graph: This module supports restoring from a meta graph. Provide the\n            meta graph as an argument to enable this behaviour.\n        lossType: A valid loss type string in [\'l2\', \'xentropy\'].\n        optimizer: A valid optimizer string in [\'Adam\'].\n        automode: Disable or enable the automode behaviour.\n        This module takes care of all of the training procedure automatically,\n        and the default behaviour is suitable for most cases. In certain cases\n        though, the user would want to change certain aspects of the graph;\n        specifically, he would want to change to loss operation by, say, adding\n        regularization terms for the model matrices. To enable this behaviour,\n        the user can perform the following steps:\n            1. Disable automode. That is, when initializing, set automode=False\n            2. After the __call__ method has been invoked to create the loss\n            operation, the user can access the self.lossOp attribute and modify\n            it by adding regularization or other terms.\n            3. After the modification has been performed, the user needs to\n            call the `createOpCollections()` method so that the newly edited\n            operations can be added to Tensorflow collections.\n        \'\'\'\n        self.numTimeSteps = numTimeSteps\n        self.numOutput = numOutput\n        self.graph = graph\n        self.stepSize = stepSize\n        self.lossType = lossType\n        self.optimizer = optimizer\n        self.automode = automode\n        self.__validInit = False\n        self.graphCreated = False\n        # Operations to be restored\n        self.lossOp = None\n        self.trainOp = None\n        self.softmaxPredictions = None\n        self.accTilda = None\n        self.equalTilda = None\n        self.lossIndicatorTensor = None\n        self.lossIndicatorPlaceholder = None\n        self.lossIndicatorAssignOp = None\n        # Input validation\n        self.supportedLosses = [\'xentropy\', \'l2\']\n        self.supportedOptimizers = [\'Adam\']\n        assert lossType in self.supportedLosses\n        assert optimizer in self.supportedOptimizers\n        # Internal\n        self.scope = \'EMI/Trainer/\'\n\n    def __validateInit(self, predicted, target):\n        msg = \'Predicted/Target tensors have incorrect dimension\'\n        assert len(predicted.shape) == 4, msg\n        assert predicted.shape[3] == self.numOutput, msg\n        assert predicted.shape[2] == self.numTimeSteps, msg\n        assert predicted.shape[1] == target.shape[1], msg\n        assert len(target.shape) == 3\n        assert target.shape[2] == self.numOutput\n        self.__validInit = True\n\n    def __call__(self, predicted, target):\n        \'\'\'\n        Constructs the loss and train operations. If already created, returns\n        the created operators.\n\n        predicted: The prediction scores outputed from the forward computation\n            graph. Expects a 4 dimensional tensor with shape [-1,\n            numSubinstance, numTimeSteps, numClass].\n        target: The target labels in one hot-encoding. Expects [-1,\n            numSubinstance, numClass]\n        \'\'\'\n        if self.graphCreated:\n            # TODO: These statements are redundant after self.validInit call\n            # A simple check to self.__validInit should suffice. Test this.\n            assert self.lossOp is not None\n            assert self.trainOp is not None\n            return self.lossOp, self.trainOp\n        self.__validateInit(predicted, target)\n        assert self.__validInit is True\n        if self.graph is None:\n            self._createGraph(predicted, target)\n        else:\n            self._restoreGraph(predicted, target)\n        assert self.graphCreated == True\n        return self.lossOp, self.trainOp\n\n    def __transformY(self, target):\n        \'\'\'\n        Because we need output from each step and not just the last step.\n        Currently we just tile the target to each step. This method can be\n        exteneded/overridden to allow more complex behaviours\n        \'\'\'\n        with tf.name_scope(self.scope):\n            A_ = tf.expand_dims(target, axis=2)\n            A__ = tf.tile(A_, [1, 1, self.numTimeSteps, 1])\n        return A__\n\n    def createLossOp(self, predicted, target):\n        assert self.__validInit is True, \'Initialization failure\'\n        with tf.name_scope(self.scope):\n            # Loss indicator tensor\n            li = np.zeros([self.numTimeSteps, self.numOutput])\n            li[-1, :] = 1\n            liTensor = tf.Variable(li.astype(\'float32\'),\n                                   name=\'loss-indicator\',\n                                   trainable=False)\n            name=\'loss-indicator-placeholder\'\n            liPlaceholder = tf.placeholder(tf.float32,\n                                           name=name)\n            liAssignOp = tf.assign(liTensor, liPlaceholder,\n                                   name=\'loss-indicator-assign-op\')\n            self.lossIndicatorTensor = liTensor\n            self.lossIndicatorPlaceholder = liPlaceholder\n            self.lossIndicatorAssignOp = liAssignOp\n            # predicted of dim [-1, numSubinstance, numTimeSteps, numOutput]\n            dims = [-1, self.numTimeSteps, self.numOutput]\n            logits__ = tf.reshape(predicted, dims)\n            labels__ = tf.reshape(target, dims)\n            diff = (logits__ - labels__)\n            diff = tf.multiply(self.lossIndicatorTensor, diff)\n            # take loss only for the timesteps indicated by lossIndicator for softmax\n            logits__ = tf.multiply(self.lossIndicatorTensor, logits__)\n            labels__ = tf.multiply(self.lossIndicatorTensor, labels__)\n            logits__ = tf.reshape(logits__, [-1, self.numOutput])\n            labels__ = tf.reshape(labels__, [-1, self.numOutput])\n            # Regular softmax\n            if self.lossType == \'xentropy\':\n                softmax1 = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels__,\n                                                                      logits=logits__)\n                lossOp = tf.reduce_mean(softmax1, name=\'xentropy-loss\')\n            elif self.lossType == \'l2\':\n                lossOp = tf.nn.l2_loss(diff, name=\'l2-loss\')\n        return lossOp\n\n    def createTrainOp(self):\n        with tf.name_scope(self.scope):\n            tst = tf.train.AdamOptimizer(self.stepSize).minimize(self.lossOp)\n        return tst\n\n    def _createGraph(self, predicted, target):\n        target = self.__transformY(target)\n        assert self.__validInit is True\n        with tf.name_scope(self.scope):\n            self.softmaxPredictions = tf.nn.softmax(predicted, axis=3,\n                                                    name=\'softmaxed-prediction\')\n            pred = self.softmaxPredictions[:, :, -1, :]\n            actu = target[:, :, -1, :]\n            resPred = tf.reshape(pred, [-1, self.numOutput])\n            resActu = tf.reshape(actu, [-1, self.numOutput])\n            maxPred = tf.argmax(resPred, axis=1)\n            maxActu = tf.argmax(resActu, axis=1)\n            equal = tf.equal(maxPred, maxActu)\n            self.equalTilda = tf.cast(equal, tf.float32, name=\'equal-tilda\')\n            self.accTilda = tf.reduce_mean(self.equalTilda, name=\'acc-tilda\')\n\n        self.lossOp = self.createLossOp(predicted, target)\n        self.trainOp = self.createTrainOp()\n        self.createOpCollections()\n        self.graphCreated = True\n\n    def _restoreGraph(self, predicted, target):\n        assert self.graphCreated is False\n        scope = self.scope\n        graph = self.graph\n        self.trainOp = tf.get_collection(\'EMI-train-op\')\n        self.lossOp = tf.get_collection(\'EMI-loss-op\')\n        msg0 = \'Operator or tensor not found\'\n        msg1 = \'Multiple tensors with the same name in the graph. Are you not\'\n        msg1 +=\' resetting your graph?\'\n        assert len(self.trainOp) != 0, msg0\n        assert len(self.lossOp) != 0, msg0\n        assert len(self.trainOp) == 1, msg1\n        assert len(self.lossOp) == 1, msg1\n        self.trainOp = self.trainOp[0]\n        self.lossOp = self.lossOp[0]\n        self.lossIndicatorTensor = graph.get_tensor_by_name(scope +\n                                                            \'loss-indicator:0\')\n        name = \'loss-indicator-placeholder:0\'\n        self.lossIndicatorPlaceholder = graph.get_tensor_by_name(scope + name)\n        name = \'loss-indicator-assign-op:0\'\n        self.lossIndicatorAssignOp = graph.get_tensor_by_name(scope + name)\n        name = scope + \'softmaxed-prediction:0\'\n        self.softmaxPredictions = graph.get_tensor_by_name(name)\n        name = scope + \'acc-tilda:0\'\n        self.accTilda = graph.get_tensor_by_name(name)\n        name = scope + \'equal-tilda:0\'\n        self.equalTilda = graph.get_tensor_by_name(name)\n        self.graphCreated = True\n        self.__validInit = True\n\n    def createOpCollections(self):\n        \'\'\'\n        Adds the trainOp and lossOp to Tensorflow collections. This enables us\n        to restore these operations from saved metagraphs.\n        \'\'\'\n        tf.add_to_collection(\'EMI-train-op\', self.trainOp)\n        tf.add_to_collection(\'EMI-loss-op\', self.lossOp)\n\n    def __echoCB(self, sess, feedDict, currentBatch, redirFile, **kwargs):\n        _, loss = sess.run([self.trainOp, self.lossOp],\n                                feed_dict=feedDict)\n        print(""\\rBatch %5d Loss %2.5f"" % (currentBatch, loss),\n              end=\'\', file=redirFile)\n\n    def trainModel(self, sess, redirFile=None, echoInterval=15,\n                   echoCB=None, feedDict=None, **kwargs):\n        \'\'\'\n        The training routine.\n\n        sess: The Tensorflow session associated with the computation graph.\n        redirFile: Output from the training routine can be redirected to a file\n            on the disk. Please provide the file pointer to said file to enable\n            this behaviour. Defaults to STDOUT. To disable outputs all\n            together, please pass a file pointer to DEVNULL or equivalent as an\n            argument.\n        echoInterval: The number of batch updates between calls to echoCB.\n        echoCB: This call back method is used for printing intermittent\n            training stats such as validation accuracy or loss value. By default,\n            it defaults to self.__echoCB. The signature of the method is,\n\n            echoCB(self, session, feedDict, currentBatch, redirFile, **kwargs)\n\n            Please refer to the __echoCB implementation for a simple example.\n            A more complex example can be found in the EMI_Driver.\n        feedDict: feedDict, that is required for the session.run() calls. Will\n            be directly passed to the sess.run() calls.\n        **kwargs: Additional args to echoCB.\n        \'\'\'\n        if echoCB is None:\n            echoCB = self.__echoCB\n        currentBatch = 0\n        while True:\n            try:\n                if currentBatch % echoInterval == 0:\n                    echoCB(sess, feedDict, currentBatch, redirFile, **kwargs)\n                else:\n                    sess.run([self.trainOp], feed_dict=feedDict)\n                currentBatch += 1\n            except tf.errors.OutOfRangeError:\n                break\n\n    def restoreFromGraph(self, graph):\n        \'\'\'\n        This method provides an alternate way of restoring\n        from a saved meta graph - without having to provide the restored meta\n        graph as a parameter to __init__. This is useful when, in between\n        training, you want to reset the entire computation graph and reload a\n        new meta graph from disk. This method allows you to attach to this\n        newly loaded meta graph without having to create a new EMI_Trainer\n        object. Use this method only when you want to clear/reset the existing\n        computational graph.\n        \'\'\'\n        self.graphCreated = False\n        self.lossOp = None\n        self.trainOp = None\n        self.lossIndicatorTensor = None\n        self.softmaxPredictions = None\n        self.accTilda = None\n        self.graph = graph\n        self.__validInit = True\n        assert self.graphCreated is False\n        self._restoreGraph(None, None)\n        assert self.graphCreated is True\n\n\nclass EMI_Driver:\n    def __init__(self, emiDataPipeline, emiGraph, emiTrainer,\n                 max_to_keep=1000, globalStepStart=1000):\n        \'\'\'\n        The driver class that takes care of training an EMI RNN graph. The EMI\n        RNN graph consists of three parts - a data input pipeline\n        (EMI_DataPipeline), the forward computation graph (EMI-RNN) and the\n        loss graph (EMI_Trainer). After the three parts of been created and\n        connected, they should be passed as arguments to this module.\n\n        Since EMI-RNN training requires careful handling of Sessions and\n        graphs, these details are wrapped inside EMI_Driver. For an external\n        method to access the current session, please make sure to use\n        getCurrentSession() method defined in EMI_Driver. Note that this has to\n        be done every time a reference to the current session is required.\n        (Internally, sessions are closed and opened when new models are loaded\n        from the disk, so technically, as long as no new model has been loaded\n        from disk, there is no need to call getCurrentSession() again - it is\n        better to be safe though).\n\n        emiDataPipeline: An EMI_DataPipeline object.\n        emiGraph: An EMI_RNN object.\n        emiTrainer: An EMI_Trainer object.\n        max_to_keep: Maximum number of model checkpoints to keep. Make sure\n            that this is more than [number of iterations] * [number of rounds].\n        globalStepStart: The global step  value is used as a key for naming\n        saved meta graphs. Meta graphs and checkpoints will be named from\n        globalStepStart through globalStepStart  + max_to_keep.\n        \'\'\'\n        self._dataPipe = emiDataPipeline\n        self._emiGraph = emiGraph\n        self._emiTrainer = emiTrainer\n        msg = \'Have you invoked __call__()\'\n        assert self._dataPipe.graphCreated is True, msg\n        assert self._emiGraph.graphCreated is True, msg\n        assert self._emiTrainer.graphCreated is True, msg\n        self.__globalStep = globalStepStart\n        self.__saver = tf.train.Saver(max_to_keep=max_to_keep,\n                                      save_relative_paths=True)\n        self.__graphManager = utils.GraphManager()\n        self.__sess = None\n\n    def fancyEcho(self, sess, feedDict, currentBatch, redirFile,\n                  numBatches=None):\n        \'\'\'\n        A callable that is passed as argument - echoCB - to\n        EMI_Trainer.train() method.\n        \'\'\'\n        _, loss, acc = sess.run([self._emiTrainer.trainOp,\n                                 self._emiTrainer.lossOp,\n                                 self._emiTrainer.accTilda],\n                                feed_dict=feedDict)\n        epoch = int(currentBatch /  numBatches)\n        batch = int(currentBatch % max(numBatches, 1))\n        print(""\\rEpoch %3d Batch %5d (%5d) Loss %2.5f Acc %2.5f |"" %\n              (epoch, batch, currentBatch, loss, acc),\n              end=\'\', file=redirFile)\n\n    def assignToGraph(self, initVarList):\n        \'\'\'\n        This method should deal with restoring the entire graph\n        now\'\'\'\n        raise NotImplementedError()\n\n    def initializeSession(self, graph, reuse=False, feedDict=None):\n        \'\'\'\n        Initialize a new session with the computation graph provided in graph.\n\n        graph: The computation graph needed to be used for the current session.\n        reuse: If True, global_variables_initializer will not be invoked and\n            the graph will retain the current tensor states/values.\n        feedDict: Not used\n        \'\'\'\n        sess = self.__sess\n        if sess is not None:\n           sess.close()\n        with graph.as_default():\n            sess = tf.Session()\n        if reuse is False:\n            with graph.as_default():\n                init = tf.global_variables_initializer()\n            sess.run(init)\n        self.__sess = sess\n\n    def getCurrentSession(self):\n        \'\'\'\n        Returns the current tf.Session()\n        \'\'\'\n        return self.__sess\n\n    def setSession(self, sess):\n        \'\'\'\n        Sets sess as the session to be used by the driver. Experimental and not\n        recommended.\n        \'\'\'\n        self.__sess = sess\n\n    def runOps(self, opList, X, Y, batchSize, feedDict=None, **kwargs):\n        \'\'\'\n        Run tensorflow operations provided in opList on data X, Y.\n\n        opList: A list of operations.\n        X, Y: Numpy matrices of the data.\n        batchSize: batch size\n        feedDict: Feed dict required, if any, by the provided ops.\n\n        returns a  list of batchwise results of sess.run(opList) on the\n        provided data.\n        \'\'\'\n        sess = self.__sess\n        if feedDict is None:\n            feedDict = self.feedDictFunc(**kwargs)\n        self._dataPipe.runInitializer(sess, X, Y, batchSize,\n                                       numEpochs=1)\n        outList = []\n        while True:\n            try:\n                resList = sess.run(opList, feed_dict=feedDict)\n                outList.append(resList)\n            except tf.errors.OutOfRangeError:\n                break\n        return outList\n\n    def run(self, numClasses, x_train, y_train, bag_train, x_val, y_val,\n            bag_val, numIter, numRounds, batchSize, numEpochs, echoCB=None,\n            redirFile=None, modelPrefix=\'/tmp/model\', updatePolicy=\'top-k\',\n            fracEMI=0.3, lossIndicator=None, *args, **kwargs):\n        \'\'\'\n        Performs the EMI-RNN training routine.\n\n        numClasses: Number of output classes.\n        x_train, y_train, bag_train, x_val, y_val, bag_val: data matrices for\n            test and validation sets. Please refer to the data preparation\n            document for more information.\n        numIter: Number of iterations. Each iteration consists of numEpochs\n            passes of the data. A model check point is created after each\n            iteration.\n        numRounds: Number of rounds of label updates to perform. Each round\n            consists of numIter iterations of numEpochs passes over the data.\n        batchSize: Batch Size.\n        numEpochs: Number of epochs per iteration. A model checkpoint is\n            created after evey numEpochs passes over the data.\n        feedDict: Feed dict for training procedure (optional).\n        echoCB: The echo function (print function) that is passed to the\n            EMI_Trainer.trian() method. Defaults to self.fancyEcho()\n        redirFile: Provide a file pointer to redirect output to if required.\n        modelPrefix: Output directory/prefix for checkpoints and metagraphs.\n        updatePolicy: Supported values are \'top-k\' and \'prune-ends\'. Refer to\n            the update policy documentation for more information.\n        fracEMI: Fraction of the total rounds that use EMI-RNN loss. The\n            initial (1-fracEMI) rounds will use regular MI-RNN loss. To perform\n            only MI-RNN training, set this to 0.0.\n        lossIndicator: NotImplemented\n        *args, **kwargs: Additional arguments passed to callback methods and\n            update policy methods.\n\n        returns the updated instance level labels on the training set and a\n            list of model stats after each round.\n        \'\'\'\n        assert self.__sess is not None, \'No sessions initialized\'\n        sess = self.__sess\n        assert updatePolicy in [\'prune-ends\', \'top-k\']\n        if updatePolicy == \'top-k\':\n            print(""Update policy: top-k"", file=redirFile)\n            updatePolicyFunc = self.__policyTopK\n        else:\n            print(""Update policy: prune-ends"", file=redirFile)\n            updatePolicyFunc = self.__policyPrune\n\n        curr_y = np.array(y_train)\n        assert fracEMI >= 0\n        assert fracEMI <= 1\n        emiSteps = int(fracEMI * numRounds)\n        emiStep = numRounds - emiSteps\n        print(""Training with MI-RNN loss for %d rounds"" % emiStep,\n              file=redirFile)\n        modelStats = []\n        for cround in range(numRounds):\n            feedDict = self.feedDictFunc(inference=False, **kwargs)\n            print(""Round: %d"" % cround, file=redirFile)\n            if cround == emiStep:\n                print(""Switching to EMI-Loss function"", file=redirFile)\n                if lossIndicator is not None:\n                    raise NotImplementedError(\'TODO\')\n                else:\n                    nTs = self._emiTrainer.numTimeSteps\n                    nOut = self._emiTrainer.numOutput\n                    lossIndicator = np.ones([nTs, nOut])\n                    sess.run(self._emiTrainer.lossIndicatorAssignOp,\n                         feed_dict={self._emiTrainer.lossIndicatorPlaceholder:\n                                    lossIndicator})\n            valAccList, globalStepList = [], []\n            # Train the best model for the current round\n            for citer in range(numIter):\n                self._dataPipe.runInitializer(sess, x_train, curr_y,\n                                               batchSize, numEpochs)\n                numBatches = int(np.ceil(len(x_train) / batchSize))\n                self._emiTrainer.trainModel(sess, echoCB=self.fancyEcho,\n                                             numBatches=numBatches,\n                                             feedDict=feedDict,\n                                             redirFile=redirFile)\n                if self._emiGraph.useDropout is True:\n                    ret = self.getInstancePredictions(x_val, y_val,\n                                                      self.__nonEarlyInstancePrediction,\n                                                      keep_prob=1.0)\n                else:\n                    ret = self.getInstancePredictions(x_val, y_val,\n                                                      self.__nonEarlyInstancePrediction)\n                predictions = ret[0]\n                numSubinstance = x_val.shape[1]\n                numOutput = self._emiTrainer.numOutput\n                df = self.analyseModel(predictions, bag_val, numSubinstance,\n                                       numOutput, silent=True)\n                acc = np.max(df[\'acc\'].values)\n                print("" Val acc %2.5f | "" % acc, end=\'\', file=redirFile)\n                self.__graphManager.checkpointModel(self.__saver, sess,\n                                                    modelPrefix,\n                                                    self.__globalStep,\n                                                    redirFile=redirFile)\n                valAccList.append(acc)\n                globalStepList.append((modelPrefix, self.__globalStep))\n                self.__globalStep += 1\n\n            # Update y for the current round\n            ## Load the best val-acc model\n            argAcc = np.argmax(valAccList)\n            resPrefix, resStep = globalStepList[argAcc]\n            modelStats.append((cround, np.max(valAccList),\n                               resPrefix, resStep))\n            self.loadSavedGraphToNewSession(resPrefix, resStep, redirFile)\n            sess = self.getCurrentSession()\n            feedDict = self.feedDictFunc(inference=True, **kwargs)\n            smxOut = self.runOps([self._emiTrainer.softmaxPredictions],\n                                     x_train, y_train, batchSize, feedDict)\n            smxOut= [np.array(smxOut[i][0]) for i in range(len(smxOut))]\n            smxOut = np.concatenate(smxOut)[:, :, -1, :]\n            newY = updatePolicyFunc(curr_y, smxOut, bag_train,\n                                    numClasses, **kwargs)\n            currY = newY\n        return currY, modelStats\n\n    def loadSavedGraphToNewSession(self, modelPrefix, globalStep,\n                                      redirFile=None):\n        self.__sess.close()\n        tf.reset_default_graph()\n        sess = tf.Session()\n        graph = self.__graphManager.loadCheckpoint(sess, modelPrefix,\n                                                   globalStep=globalStep,\n                                                   redirFile=redirFile)\n        # return graph\n        self._dataPipe.restoreFromGraph(graph)\n        self._emiGraph.restoreFromGraph(graph)\n        self._emiTrainer.restoreFromGraph(graph)\n        self.__sess = sess\n        return graph\n\n    def updateLabel(self, Y, policy, softmaxOut, bagLabel, numClasses, **kwargs):\n        \'\'\'\n        Updates the current label information based on policy and the predicted\n        outputs.\n\n        Y: numpy array of current label information.\n        policy: The update policy to use. Currently supports [\'top-k\',\n            \'prune-ends\']\n        softmaxOut: The predicted instance level output from the soft-max\n            layer.\n        bagLabel: A numpy array with bag level label information.\n        numClasses: Number of output classes.\n        **kwargs: Additional keyword arguments to the update policy\n        \'\'\'\n        assert policy in [\'prune-ends\', \'top-k\']\n        if policy == \'top-k\':\n            updatePolicyFunc = self.__policyTopK\n        else:\n            updatePolicyFunc = self.__policyPrune\n        Y_ = np.array(Y)\n        newY = updatePolicyFunc(Y_, softmaxOut, bagLabel, numClasses, **kwargs)\n        return newY\n\n    def analyseModel(self, predictions, Y_bag, numSubinstance, numClass,\n                     redirFile=None, verbose=False, silent=False):\n        \'\'\'\n        Some basic analysis on predictions and true labels.\n\n        predictions: [-1, numsubinstance] is the instance level prediction.\n        Y_Bag: [-1] is the bag level labels.\n        numSubinstace: Number of sub-instance.\n        numClass: Number of classes.\n        redirFile: To redirect output to a file, provide the file pointer.\n\n        verbose: Prints verbose data frame. Includes additionally, precision\n            and recall information.\n\n        silent: Disable output to console or file pointer.\n        \'\'\'\n        assert (predictions.ndim == 2)\n        assert (predictions.shape[1] == numSubinstance)\n        assert (Y_bag.ndim == 1)\n        assert (len(Y_bag) == len(predictions))\n        pholder = [0.0] * numSubinstance\n        df = pd.DataFrame()\n        df[\'len\'] = np.arange(1, numSubinstance + 1)\n        df[\'acc\'] = pholder\n        df[\'macro-fsc\'] = pholder\n        df[\'macro-pre\'] = pholder\n        df[\'macro-rec\'] = pholder\n\n        df[\'micro-fsc\'] = pholder\n        df[\'micro-pre\'] = pholder\n        df[\'micro-rec\'] = pholder\n        colList = []\n        colList.append(\'acc\')\n        colList.append(\'macro-fsc\')\n        colList.append(\'macro-pre\')\n        colList.append(\'macro-rec\')\n\n        colList.append(\'micro-fsc\')\n        colList.append(\'micro-pre\')\n        colList.append(\'micro-rec\')\n        for i in range(0, numClass):\n            pre = \'pre_%02d\' % i\n            rec = \'rec_%02d\' % i\n            df[pre] = pholder\n            df[rec] = pholder\n            colList.append(pre)\n            colList.append(rec)\n\n        for i in range(1, numSubinstance + 1):\n            pred_ = self.getBagPredictions(predictions, numClass=numClass,\n                                           minSubsequenceLen=i,\n                                           redirFile = redirFile)\n            correct = (pred_ == Y_bag).astype(\'int\')\n            trueAcc = np.mean(correct)\n            cmatrix = utils.getConfusionMatrix(pred_, Y_bag, numClass)\n            df.iloc[i-1, df.columns.get_loc(\'acc\')] = trueAcc\n\n            macro, micro = utils.getMacroMicroFScore(cmatrix)\n            df.iloc[i-1, df.columns.get_loc(\'macro-fsc\')] = macro\n            df.iloc[i-1, df.columns.get_loc(\'micro-fsc\')] = micro\n\n            pre, rec = utils.getMacroPrecisionRecall(cmatrix)\n            df.iloc[i-1, df.columns.get_loc(\'macro-pre\')] = pre\n            df.iloc[i-1, df.columns.get_loc(\'macro-rec\')] = rec\n\n            pre, rec = utils.getMicroPrecisionRecall(cmatrix)\n            df.iloc[i-1, df.columns.get_loc(\'micro-pre\')] = pre\n            df.iloc[i-1, df.columns.get_loc(\'micro-rec\')] = rec\n            for j in range(numClass):\n                pre, rec = utils.getPrecisionRecall(cmatrix, label=j)\n                pre_ = df.columns.get_loc(\'pre_%02d\' % j)\n                rec_ = df.columns.get_loc(\'rec_%02d\' % j)\n                df.iloc[i-1, pre_ ] = pre\n                df.iloc[i-1, rec_ ] = rec\n\n        df.set_index(\'len\')\n        # Comment this line to include all columns\n        colList = [\'len\', \'acc\', \'macro-fsc\', \'macro-pre\', \'macro-rec\']\n        colList += [\'micro-fsc\', \'micro-pre\', \'micro-rec\']\n        if verbose:\n            for col in df.columns:\n                if col not in colList:\n                    colList.append(col)\n        if numClass == 2:\n            precisionList = df[\'pre_01\'].values\n            recallList = df[\'rec_01\'].values\n            denom = precisionList + recallList\n            denom[denom == 0] = 1\n            numer = 2 * precisionList * recallList\n            f_ = numer / denom\n            df[\'fscore_01\'] = f_\n            colList.append(\'fscore_01\')\n\n        df = df[colList]\n        if silent is True:\n            return df\n\n        with pd.option_context(\'display.max_rows\', 100,\n                               \'display.max_columns\', 100,\n                               \'expand_frame_repr\', True):\n            print(df, file=redirFile)\n\n        idx = np.argmax(df[\'acc\'].values)\n        val = np.max(df[\'acc\'].values)\n        print(""Max accuracy %f at subsequencelength %d"" % (val, idx + 1),\n              file=redirFile)\n        val = np.max(df[\'micro-fsc\'].values)\n        idx = np.argmax(df[\'micro-fsc\'].values)\n        print(""Max micro-f %f at subsequencelength %d"" % (val, idx + 1),\n              file=redirFile)\n        val = df[\'micro-pre\'].values[idx]\n        print(""Micro-precision %f at subsequencelength %d"" % (val, idx + 1),\n              file=redirFile)\n        val = df[\'micro-rec\'].values[idx]\n        print(""Micro-recall %f at subsequencelength %d"" % (val, idx + 1),\n              file=redirFile)\n\n        idx = np.argmax(df[\'macro-fsc\'].values)\n        val = np.max(df[\'macro-fsc\'].values)\n        print(""Max macro-f %f at subsequencelength %d"" % (val, idx + 1),\n              file=redirFile)\n        val = df[\'macro-pre\'].values[idx]\n        print(""macro-precision %f at subsequencelength %d"" % (val, idx + 1),\n              file=redirFile)\n        val = df[\'macro-rec\'].values[idx]\n        print(""macro-recall %f at subsequencelength %d"" % (val, idx + 1),\n              file=redirFile)\n        if numClass == 2 and verbose:\n            idx = np.argmax(df[\'fscore_01\'].values)\n            val = np.max(df[\'fscore_01\'].values)\n            print(\'Max fscore %f at subsequencelength %d\' % (val, idx + 1),\n                  file=redirFile)\n            print(\'Precision %f at subsequencelength %d\' %\n                  (df[\'pre_01\'].values[idx], idx + 1), file=redirFile)\n            print(\'Recall %f at subsequencelength %d\' %\n                  (df[\'rec_01\'].values[idx], idx + 1), file=redirFile)\n        return df\n\n    def __nonEarlyInstancePrediction(self, instanceOut, **kwargs):\n        \'\'\'\n        A prediction policy used internally. No early prediction is performed\n        and the class with max prob at the last step of the RNN is returned.\n        \'\'\'\n        assert instanceOut.ndim == 2\n        retclass = np.argmax(instanceOut[-1])\n        step = len(instanceOut) - 1\n        return retclass, step\n\n    def getInstancePredictions(self, x, y, earlyPolicy, batchSize=1024,\n                               feedDict=None, **kwargs):\n\n        \'\'\'\n        Returns instance level predictions for data (x, y).\n\n        Takes the softmax outputs from the joint trained model and, applies\n        earlyPolicy() on each instance and returns the instance level\n        prediction as well as the step at which this prediction was made.\n\n        softmaxOut: [-1, numSubinstance, numTimeSteps, numClass]\n        earlyPolicy: callable,\n            def earlyPolicy(subinstacePrediction):\n                subinstacePrediction: [numTimeSteps, numClass]\n                ...\n                return predictedClass, predictedStep\n\n        returns: predictions, predictionStep\n            predictions: [-1, numSubinstance]\n            predictionStep: [-1, numSubinstance]\n        \'\'\'\n        opList = self._emiTrainer.softmaxPredictions\n        if \'keep_prob\' in kwargs:\n            assert kwargs[\'keep_prob\'] == 1, \'Keep prob should be 1.0\'\n        smxOut = self.runOps(opList, x, y, batchSize, feedDict=feedDict,\n                             **kwargs)\n        softmaxOut = np.concatenate(smxOut, axis=0)\n        assert softmaxOut.ndim == 4\n        numSubinstance, numTimeSteps, numClass = softmaxOut.shape[1:]\n        softmaxOutFlat = np.reshape(softmaxOut, [-1, numTimeSteps, numClass])\n        flatLen = len(softmaxOutFlat)\n        predictions = np.zeros(flatLen)\n        predictionStep = np.zeros(flatLen)\n        for i, instance in enumerate(softmaxOutFlat):\n            # instance is [numTimeSteps, numClass]\n            assert instance.ndim == 2\n            assert instance.shape[0] == numTimeSteps\n            assert instance.shape[1] == numClass\n            predictedClass, predictedStep = earlyPolicy(instance, **kwargs)\n            predictions[i] = predictedClass\n            predictionStep[i] = predictedStep\n        predictions = np.reshape(predictions, [-1, numSubinstance])\n        predictionStep = np.reshape(predictionStep, [-1, numSubinstance])\n        return predictions, predictionStep\n\n    def getBagPredictions(self, Y_predicted, minSubsequenceLen = 4,\n                          numClass=2, redirFile = None):\n        \'\'\'\n        Returns bag level predictions given instance level predictions.\n\n        A bag is considered to belong to a non-zero class if\n        minSubsequenceLen is satisfied. Otherwise, it is assumed\n        to belong to class 0. class 0 is negative by default. If\n        minSubsequenceLen is satisfied by multiple classes, the smaller of the\n        two is returned\n\n        Y_predicted is the predicted instance level results\n        [-1, numsubinstance]\n        Y True is the correct instance level label\n        [-1, numsubinstance]\n        \'\'\'\n        assert(Y_predicted.ndim == 2)\n        scoreList = []\n        for x in range(1, numClass):\n            scores = self.__getLengthScores(Y_predicted, val=x)\n            length = np.max(scores, axis=1)\n            scoreList.append(length)\n        scoreList = np.array(scoreList)\n        scoreList = scoreList.T\n        assert(scoreList.ndim == 2)\n        assert(scoreList.shape[0] == Y_predicted.shape[0])\n        assert(scoreList.shape[1] == numClass - 1)\n        length = np.max(scoreList, axis=1)\n        assert(length.ndim == 1)\n        assert(length.shape[0] == Y_predicted.shape[0])\n        predictionIndex = (length >= minSubsequenceLen)\n        prediction = np.zeros((Y_predicted.shape[0]))\n        labels = np.argmax(scoreList, axis=1) + 1\n        prediction[predictionIndex] = labels[predictionIndex]\n        return prediction.astype(int)\n\n    def __getLengthScores(self, Y_predicted, val=1):\n        \'\'\'\n        Returns an matrix which contains the length of the longest positive\n        subsequence of val ending at that index.\n        Y_predicted: [-1, numSubinstance] Is the instance level class\n            labels.\n        \'\'\'\n        scores = np.zeros(Y_predicted.shape)\n        for i, bag in enumerate(Y_predicted):\n            for j, instance in enumerate(bag):\n                prev = 0\n                if j > 0:\n                    prev = scores[i, j-1]\n                if instance == val:\n                    scores[i, j] = prev + 1\n                else:\n                    scores[i, j] = 0\n        return scores\n\n    def __policyPrune(self, currentY, softmaxOut, bagLabel, numClasses,\n                      minNegativeProb=0.0, updatesPerCall=3,\n                      maxAllowedUpdates=3, **kwargs):\n        \'\'\'\n        CurrentY: [-1, numsubinstance, numClass]\n        softmaxOut: [-1, numsubinstance, numClass]\n        bagLabel: [-1]\n        numClasses: Number of output classes\n        minNegativeProb: A instance predicted as negative is labeled as\n            negative iff prob. negative >= minNegativeProb\n        updatesPerCall: At most number of updates to per function call\n        maxAllowedUpdates: Total updates on positive bag cannot exceed\n            maxAllowedUpdate.\n\n        This policy incrementally increases the prefix/suffix of negative\n        labels in currentY.  An instance is labelled as a negative if:\n\n            1. All the instances preceding it in case of a prefix and all\n            instances succeeding it in case of a continuous prefix and/or\n            suffix of negative is labeled as a negative.\n            2. The probability of the instance being negative > negativeProb.\n            3. The instance is indeed predicted as negative (i.e. prob class 0\n            is max)\n            4. If the sequence length is less than maxSamples.\n\n        All four conditions must hold. In case of a tie between instances near\n        the suffix and prefix, the one with maximum probability is updated. If\n        probabilities are same, then the left prefix is updated.\n\n        CLASS 0 is assumed to be negative class\n        \'\'\'\n        assert currentY.ndim == 3\n        assert softmaxOut.ndim == 3\n        assert bagLabel.ndim == 1\n        assert len(currentY) == len(softmaxOut)\n        assert len(softmaxOut) == len(bagLabel)\n        numSubinstance = currentY.shape[1]\n        assert maxAllowedUpdates < numSubinstance\n        assert softmaxOut.shape[1] == numSubinstance\n\n        index = (bagLabel != 0)\n        indexList = np.where(bagLabel)[0]\n        newY = np.array(currentY)\n        for i in indexList:\n            currLabel = currentY[i]\n            currProbabilities = softmaxOut[i]\n            prevPrefix = 0\n            prevSuffix = 0\n            for inst in currLabel:\n                if np.argmax(inst) == 0:\n                    prevPrefix += 1\n                else:\n                    break\n            for inst in reversed(currLabel):\n                if np.argmax(inst) == 0:\n                    prevSuffix += 1\n                else:\n                    break\n            assert (prevPrefix + prevSuffix <= maxAllowedUpdates)\n            leftIdx = int(prevPrefix)\n            rightIdx = numSubinstance - int(prevSuffix) - 1\n            possibleUpdates = min(updatesPerCall, maxAllowedUpdates - prevPrefix - prevSuffix)\n            while (possibleUpdates > 0):\n                assert leftIdx < numSubinstance\n                assert leftIdx >= 0\n                assert rightIdx < numSubinstance\n                assert rightIdx >= 0\n                leftLbl = np.argmax(currProbabilities[leftIdx])\n                leftProb = np.max(currProbabilities[leftIdx])\n                rightLbl = np.argmax(currProbabilities[rightIdx])\n                rightProb = np.max(currProbabilities[rightIdx])\n                if (leftLbl != 0 and rightLbl !=0):\n                    break\n                elif (leftLbl == 0 and rightLbl != 0):\n                    if leftProb >= minNegativeProb:\n                        newY[i, leftIdx, :] = 0\n                        newY[i, leftIdx, 0] = 1\n                        leftIdx += 1\n                    else:\n                        break\n                elif (leftLbl != 0 and rightLbl == 0):\n                    if rightProb >= minNegativeProb:\n                        newY[i, rightIdx, :] = 0\n                        newY[i, rightIdx, 0] = 1\n                        rightIdx -= 1\n                    else:\n                        break\n                elif leftProb >= rightProb:\n                    if leftProb >= minNegativeProb:\n                        newY[i, leftIdx, :] = 0\n                        newY[i, leftIdx, 0] = 1\n                        leftIdx += 1\n                    else:\n                        break\n                elif rightProb > leftProb:\n                    if rightProb >= minNegativeProb:\n                        newY[i, rightIdx, :] = 0\n                        newY[i, rightIdx, 0] = 1\n                        rightIdx -= 1\n                    else:\n                        break\n                possibleUpdates -= 1\n        return newY\n\n    def __policyTopK(self, currentY, softmaxOut, bagLabel, numClasses, k=1,\n                     **kwargs):\n        \'\'\'\n        currentY: [-1, numsubinstance, numClass]\n        softmaxOut: [-1, numsubinstance, numClass]\n        bagLabel [-1]\n        k: minimum length of continuous non-zero examples\n\n        Algorithm:\n            For each bag:\n                1. Find the longest continuous subsequence of a label.\n                2. If this label is the same as the bagLabel, and if the length\n                of the subsequence is at least k:\n                    2.1 Set the label of these instances as the bagLabel.\n                    2.2 Set all other labels as 0\n        \'\'\'\n        assert currentY.ndim == 3\n        assert k <= currentY.shape[1]\n        assert k > 0\n        # predicted label for each instance is max of softmax\n        predictedLabels = np.argmax(softmaxOut, axis=2)\n        scoreList = []\n        # classScores[i] is a 2d array where a[j,k] is the longest\n        # string of consecutive class labels i in bag j ending at instance k\n        classScores = [-1]\n        for i in range(1, numClasses):\n            scores = self.__getLengthScores(predictedLabels, val=i)\n            classScores.append(scores)\n            length = np.max(scores, axis=1)\n            scoreList.append(length)\n        scoreList = np.array(scoreList)\n        scoreList = scoreList.T\n        # longestContinuousClass[i] is the class label having\n        # longest substring in bag i\n        longestContinuousClass = np.argmax(scoreList, axis=1) + 1\n        # longestContinuousClassLength[i] is length of\n        # longest class substring in bag i\n        longestContinuousClassLength = np.max(scoreList, axis=1)\n        assert longestContinuousClass.ndim == 1\n        assert longestContinuousClass.shape[0] == bagLabel.shape[0]\n        assert longestContinuousClassLength.ndim == 1\n        assert longestContinuousClassLength.shape[0] == bagLabel.shape[0]\n        newY = np.array(currentY)\n        index = (bagLabel != 0)\n        indexList = np.where(index)[0]\n        # iterate through all non-zero bags\n        for i in indexList:\n            # longest continuous class for this bag\n            lcc = longestContinuousClass[i]\n            # length of longest continuous class for this bag\n            lccl = int(longestContinuousClassLength[i])\n            # if bagLabel is not the same as longest continuous\n            # class, don\'t update\n            if lcc != bagLabel[i]:\n                continue\n            # we check for longest string to be at least k\n            if lccl < k:\n                continue\n            lengths = classScores[lcc][i]\n            assert np.max(lengths) == lccl\n            possibleCandidates = np.where(lengths == lccl)[0]\n            # stores (candidateIndex, sum of probabilities\n            # over window for this index) pairs\n            sumProbsAcrossLongest = {}\n            for candidate in possibleCandidates:\n                sumProbsAcrossLongest[candidate] = 0.0\n                # sum the probabilities over the continuous substring\n                for j in range(0, lccl):\n                    sumProbsAcrossLongest[candidate] += softmaxOut[i, candidate-j, lcc]\n            # we want only the one with maximum sum of\n            # probabilities; sort dict by value\n            sortedProbs = sorted(sumProbsAcrossLongest.items(),key=lambda x: x[1], reverse=True)\n            bestCandidate = sortedProbs[0][0]\n            # apart from (bestCanditate-lcc,bestCandidate] label\n            # everything else as 0\n            newY[i, :, :] = 0\n            newY[i, :, 0] = 1\n            newY[i, bestCandidate-lccl+1:bestCandidate+1, 0] = 0\n            newY[i, bestCandidate-lccl+1:bestCandidate+1, lcc] = 1\n        return newY\n\n    def feedDictFunc(self, **kwargs):\n        \'\'\'\n        Construct feed dict from graph objects\n        \'\'\'\n        return None\n'"
tf/edgeml_tf/trainer/fastTrainer.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nfrom __future__ import print_function\nimport os\nimport sys\nimport tensorflow as tf\nimport edgeml_tf.utils as utils\nimport numpy as np\n\n\nclass FastTrainer:\n\n    def __init__(self, FastObj, X, Y, sW=1.0, sU=1.0, learningRate=0.01,\n                 outFile=None):\n        \'\'\'\n        FastObj - Can be either FastRNN or FastGRNN with proper initialisations\n        sW and sU are the sparsity factors for Fast parameters\n        X is the Data Placeholder - Dims [_, timesteps, input_dims]\n        Y is the label placeholder for loss computation - Dims [_, num_classes]\n        batchSize is the batchSize\n        learningRate is the initial learning rate\n        \'\'\'\n\n        self.FastObj = FastObj\n        self.history = []\n\n        self.sW = sW\n        self.sU = sU\n\n        self.Y = Y\n        self.X = X\n\n        self.numClasses = int(self.Y.shape[1])\n        self.timeSteps = int(self.X.shape[1])\n        self.inputDims = int(self.X.shape[2])\n\n        self.learningRate = learningRate\n\n        if outFile is not None:\n            self.outFile = open(outFile, \'w\')\n        else:\n            self.outFile = sys.stdout\n\n        self.lr = tf.placeholder(""float"", name=""lr"")\n\n        self.logits, self.finalHiddenState, self.predictions = self.computeGraph()\n\n        self.lossOp = self.lossGraph(self.logits, self.Y)\n        self.trainOp = self.trainGraph(self.lossOp, self.lr)\n\n        self.correctPredictions, self.accuracy = self.accuracyGraph(\n            self.predictions, self.Y)\n\n        self.numMatrices = self.FastObj.num_weight_matrices\n        self.totalMatrices = self.numMatrices[0] + self.numMatrices[1]\n\n        self.FastParams = self.FastObj.getVars()\n\n        if self.sW > 0.99 and self.sU > 0.99:\n            self.isDenseTraining = True\n        else:\n            self.isDenseTraining = False\n\n        self.hardThrsdGraph()\n        self.sparseTrainingGraph()\n\n    def RNN(self, x, timeSteps, FastObj):\n        \'\'\'\n        Unrolls and adds linear classifier\n        \'\'\'\n        x = tf.unstack(x, timeSteps, 1)\n        outputs, states = tf.nn.static_rnn(FastObj, x, dtype=tf.float32)\n        return outputs[-1]\n\n    def computeGraph(self):\n        \'\'\'\n        Compute graph to unroll and predict on the FastObj\n        \'\'\'\n        finalHiddenState = self.RNN(self.X, self.timeSteps, self.FastObj)\n\n        logits = self.classifier(finalHiddenState)\n        predictions = tf.nn.softmax(logits, name=\'predictions\')\n\n        return logits, finalHiddenState, predictions\n\n    def classifier(self, feats):\n        \'\'\'\n        Can be raplaced by any classifier\n        TODO: Make this a separate class if needed\n        \'\'\'\n        self.FC = tf.Variable(tf.random_normal(\n            [self.FastObj.output_size, self.numClasses]), name=\'FC\')\n        self.FCbias = tf.Variable(tf.random_normal(\n            [self.numClasses]), name=\'FCbias\')\n\n        return tf.matmul(feats, self.FC) + self.FCbias\n\n    def lossGraph(self, logits, Y):\n        \'\'\'\n        Loss Graph for given FastObj\n        \'\'\'\n        lossOp = utils.crossEntropyLoss(logits, Y)\n        return lossOp\n\n    def trainGraph(self, lossOp, lr):\n        \'\'\'\n        Train Graph for the loss generated by Bonsai\n        \'\'\'\n        optimizer = tf.train.AdamOptimizer(lr)\n        trainOp = optimizer.minimize(lossOp)\n        return trainOp\n\n    def accuracyGraph(self, predictions, Y):\n        \'\'\'\n        Accuracy Graph to evaluate accuracy when needed\n        \'\'\'\n        correctPredictions = tf.equal(\n            tf.argmax(predictions, 1), tf.argmax(Y, 1))\n        accuracy = tf.reduce_mean(tf.cast(correctPredictions, tf.float32))\n        return correctPredictions, accuracy\n\n    def assertInit(self):\n        err = ""sparsity must be between 0 and 1""\n        assert self.sW >= 0 and self.sW <= 1, ""W "" + err\n        assert self.sU >= 0 and self.sU <= 1, ""U "" + err\n\n    def hardThrsdGraph(self):\n        \'\'\'\n        Set up for hard Thresholding Functionality\n        \'\'\'\n        self.paramPlaceholders = []\n        self.htOps = []\n        for i in range(0, self.numMatrices[0]):\n            self.paramPlaceholders.append(tf.placeholder(\n                tf.float32, name=""Wth_"" + str(i)))\n        for i in range(self.numMatrices[0], self.totalMatrices):\n            self.paramPlaceholders.append(tf.placeholder(\n                tf.float32, name=""Uth_"" + str(i)))\n\n        for i in range(0, self.numMatrices[0]):\n            self.htOps.append(\n                self.FastParams[i].assign(self.paramPlaceholders[i]))\n        for i in range(self.numMatrices[0], self.totalMatrices):\n            self.htOps.append(\n                self.FastParams[i].assign(self.paramPlaceholders[i]))\n\n        self.hardThresholdGroup = tf.group(*self.htOps)\n\n    def sparseTrainingGraph(self):\n        \'\'\'\n        Set up for Sparse Retraining Functionality\n        \'\'\'\n        self.stOps = []\n\n        for i in range(0, self.numMatrices[0]):\n            self.stOps.append(\n                self.FastParams[i].assign(self.paramPlaceholders[i]))\n        for i in range(self.numMatrices[0], self.totalMatrices):\n            self.stOps.append(\n                self.FastParams[i].assign(self.paramPlaceholders[i]))\n\n        self.sparseRetrainGroup = tf.group(*self.stOps)\n\n    def runHardThrsd(self, sess):\n        \'\'\'\n        Function to run the IHT routine on FastObj\n        \'\'\'\n        self.thrsdParams = []\n        for i in range(0, self.numMatrices[0]):\n            self.thrsdParams.append(\n                utils.hardThreshold(self.FastParams[i].eval(), self.sW))\n        for i in range(self.numMatrices[0], self.totalMatrices):\n            self.thrsdParams.append(\n                utils.hardThreshold(self.FastParams[i].eval(), self.sU))\n\n        fd_thrsd = {}\n        for i in range(0, self.totalMatrices):\n            fd_thrsd[self.paramPlaceholders[i]] = self.thrsdParams[i]\n        sess.run(self.hardThresholdGroup, feed_dict=fd_thrsd)\n\n    def runSparseTraining(self, sess):\n        \'\'\'\n        Function to run the Sparse Retraining routine on FastObj\n        \'\'\'\n        self.reTrainParams = []\n        for i in range(0, self.totalMatrices):\n            self.reTrainParams.append(\n                utils.copySupport(self.thrsdParams[i], self.FastParams[i].eval()))\n\n        fd_st = {}\n        for i in range(0, self.totalMatrices):\n            fd_st[self.paramPlaceholders[i]] = self.reTrainParams[i]\n        sess.run(self.sparseRetrainGroup, feed_dict=fd_st)\n\n    def getModelSize(self):\n        \'\'\'\n        Function to get aimed model size\n        \'\'\'\n        totalnnZ = 0\n        totalSize = 0\n        hasSparse = False\n        for i in range(0, self.numMatrices[0]):\n            nnz, size, sparseFlag = utils.countnnZ(self.FastParams[i], self.sW)\n            totalnnZ += nnz\n            totalSize += size\n            hasSparse = hasSparse or sparseFlag\n\n        for i in range(self.numMatrices[0], self.totalMatrices):\n            nnz, size, sparseFlag = utils.countnnZ(self.FastParams[i], self.sU)\n            totalnnZ += nnz\n            totalSize += size\n            hasSparse = hasSparse or sparseFlag\n        for i in range(self.totalMatrices, len(self.FastParams)):\n            nnz, size, sparseFlag = utils.countnnZ(self.FastParams[i], 1.0)\n            totalnnZ += nnz\n            totalSize += size\n            hasSparse = hasSparse or sparseFlag\n\n        # Replace this with classifier class call\n        nnz, size, sparseFlag = utils.countnnZ(self.FC, 1.0)\n        totalnnZ += nnz\n        totalSize += size\n        hasSparse = hasSparse or sparseFlag\n\n        nnz, size, sparseFlag = utils.countnnZ(self.FCbias, 1.0)\n        totalnnZ += nnz\n        totalSize += size\n        hasSparse = hasSparse or sparseFlag\n\n        return totalnnZ, totalSize, hasSparse\n\n    def saveParams(self, currDir):\n        \'\'\'\n        Function to save Parameter matrices\n        \'\'\'\n        if self.numMatrices[0] == 1:\n            np.save(os.path.join(currDir, ""W.npy""), self.FastParams[0].eval())\n        elif self.FastObj.wRank is None:\n            if self.numMatrices[0] == 2:\n                np.save(os.path.join(currDir, ""W1.npy""),\n                        self.FastParams[0].eval())\n                np.save(os.path.join(currDir, ""W2.npy""),\n                        self.FastParams[1].eval())\n            if self.numMatrices[0] == 3:\n                np.save(os.path.join(currDir, ""W1.npy""),\n                        self.FastParams[0].eval())\n                np.save(os.path.join(currDir, ""W2.npy""),\n                        self.FastParams[1].eval())\n                np.save(os.path.join(currDir, ""W3.npy""),\n                        self.FastParams[2].eval())\n            if self.numMatrices[0] == 4:\n                np.save(os.path.join(currDir, ""W1.npy""),\n                        self.FastParams[0].eval())\n                np.save(os.path.join(currDir, ""W2.npy""),\n                        self.FastParams[1].eval())\n                np.save(os.path.join(currDir, ""W3.npy""),\n                        self.FastParams[2].eval())\n                np.save(os.path.join(currDir, ""W4.npy""),\n                        self.FastParams[3].eval())\n        elif self.FastObj.wRank is not None:\n            if self.numMatrices[0] == 2:\n                np.save(os.path.join(currDir, ""W1.npy""),\n                        self.FastParams[0].eval())\n                np.save(os.path.join(currDir, ""W2.npy""),\n                        self.FastParams[1].eval())\n            if self.numMatrices[0] == 3:\n                np.save(os.path.join(currDir, ""W.npy""),\n                        self.FastParams[0].eval())\n                np.save(os.path.join(currDir, ""W1.npy""),\n                        self.FastParams[1].eval())\n                np.save(os.path.join(currDir, ""W2.npy""),\n                        self.FastParams[2].eval())\n            if self.numMatrices[0] == 4:\n                np.save(os.path.join(currDir, ""W.npy""),\n                        self.FastParams[0].eval())\n                np.save(os.path.join(currDir, ""W1.npy""),\n                        self.FastParams[1].eval())\n                np.save(os.path.join(currDir, ""W2.npy""),\n                        self.FastParams[2].eval())\n                np.save(os.path.join(currDir, ""W3.npy""),\n                        self.FastParams[3].eval())\n            if self.numMatrices[0] == 5:\n                np.save(os.path.join(currDir, ""W.npy""),\n                        self.FastParams[0].eval())\n                np.save(os.path.join(currDir, ""W1.npy""),\n                        self.FastParams[1].eval())\n                np.save(os.path.join(currDir, ""W2.npy""),\n                        self.FastParams[2].eval())\n                np.save(os.path.join(currDir, ""W3.npy""),\n                        self.FastParams[3].eval())\n                np.save(os.path.join(currDir, ""W4.npy""),\n                        self.FastParams[4].eval())\n\n        idx = self.numMatrices[0]\n        if self.numMatrices[1] == 1:\n            np.save(os.path.join(currDir, ""U.npy""), self.FastParams[idx + 0].eval())\n        elif self.FastObj.uRank is None:\n            if self.numMatrices[1] == 2:\n                np.save(os.path.join(currDir, ""U1.npy""),\n                        self.FastParams[idx + 0].eval())\n                np.save(os.path.join(currDir, ""U2.npy""),\n                        self.FastParams[idx + 1].eval())\n            if self.numMatrices[1] == 3:\n                np.save(os.path.join(currDir, ""U1.npy""),\n                        self.FastParams[idx + 0].eval())\n                np.save(os.path.join(currDir, ""U2.npy""),\n                        self.FastParams[idx + 1].eval())\n                np.save(os.path.join(currDir, ""U3.npy""),\n                        self.FastParams[idx + 2].eval())\n            if self.numMatrices[1] == 4:\n                np.save(os.path.join(currDir, ""U1.npy""),\n                        self.FastParams[idx + 0].eval())\n                np.save(os.path.join(currDir, ""U2.npy""),\n                        self.FastParams[idx + 1].eval())\n                np.save(os.path.join(currDir, ""U3.npy""),\n                        self.FastParams[idx + 2].eval())\n                np.save(os.path.join(currDir, ""U4.npy""),\n                        self.FastParams[idx + 3].eval())\n        elif self.FastObj.uRank is not None:\n            if self.numMatrices[1] == 2:\n                np.save(os.path.join(currDir, ""U1.npy""),\n                        self.FastParams[idx + 0].eval())\n                np.save(os.path.join(currDir, ""U2.npy""),\n                        self.FastParams[idx + 1].eval())\n            if self.numMatrices[1] == 3:\n                np.save(os.path.join(currDir, ""U.npy""),\n                        self.FastParams[idx + 0].eval())\n                np.save(os.path.join(currDir, ""U1.npy""),\n                        self.FastParams[idx + 1].eval())\n                np.save(os.path.join(currDir, ""U2.npy""),\n                        self.FastParams[idx + 2].eval())\n            if self.numMatrices[1] == 4:\n                np.save(os.path.join(currDir, ""U.npy""),\n                        self.FastParams[idx + 0].eval())\n                np.save(os.path.join(currDir, ""U1.npy""),\n                        self.FastParams[idx + 1].eval())\n                np.save(os.path.join(currDir, ""U2.npy""),\n                        self.FastParams[idx + 2].eval())\n                np.save(os.path.join(currDir, ""U3.npy""),\n                        self.FastParams[idx + 3].eval())\n            if self.numMatrices[1] == 5:\n                np.save(os.path.join(currDir, ""U.npy""),\n                        self.FastParams[idx + 0].eval())\n                np.save(os.path.join(currDir, ""U1.npy""),\n                        self.FastParams[idx + 1].eval())\n                np.save(os.path.join(currDir, ""U2.npy""),\n                        self.FastParams[idx + 2].eval())\n                np.save(os.path.join(currDir, ""U3.npy""),\n                        self.FastParams[idx + 3].eval())\n                np.save(os.path.join(currDir, ""U4.npy""),\n                        self.FastParams[idx + 4].eval())\n\n        if self.FastObj.cellType == ""FastGRNN"":\n            np.save(os.path.join(currDir, ""Bg.npy""),\n                    self.FastParams[self.totalMatrices].eval())\n            np.save(os.path.join(currDir, ""Bh.npy""),\n                    self.FastParams[self.totalMatrices + 1].eval())\n            np.save(os.path.join(currDir, ""zeta.npy""),\n                    self.FastParams[self.totalMatrices + 2].eval())\n            np.save(os.path.join(currDir, ""nu.npy""),\n                    self.FastParams[self.totalMatrices + 3].eval())\n        elif self.FastObj.cellType == ""FastRNN"":\n            np.save(os.path.join(currDir, ""B.npy""),\n                    self.FastParams[self.totalMatrices].eval())\n            np.save(os.path.join(currDir, ""alpha.npy""), self.FastParams[\n                    self.totalMatrices + 1].eval())\n            np.save(os.path.join(currDir, ""beta.npy""),\n                    self.FastParams[self.totalMatrices + 2].eval())\n        elif self.FastObj.cellType == ""UGRNNLR"":\n            np.save(os.path.join(currDir, ""Bg.npy""),\n                    self.FastParams[self.totalMatrices].eval())\n            np.save(os.path.join(currDir, ""Bh.npy""),\n                    self.FastParams[self.totalMatrices + 1].eval())\n        elif self.FastObj.cellType == ""GRULR"":\n            np.save(os.path.join(currDir, ""Br.npy""),\n                    self.FastParams[self.totalMatrices].eval())\n            np.save(os.path.join(currDir, ""Bg.npy""),\n                    self.FastParams[self.totalMatrices + 1].eval())\n            np.save(os.path.join(currDir, ""Bh.npy""),\n                    self.FastParams[self.totalMatrices + 2].eval())\n        elif self.FastObj.cellType == ""LSTMLR"":\n            np.save(os.path.join(currDir, ""Bf.npy""),\n                    self.FastParams[self.totalMatrices].eval())\n            np.save(os.path.join(currDir, ""Bi.npy""),\n                    self.FastParams[self.totalMatrices + 1].eval())\n            np.save(os.path.join(currDir, ""Bc.npy""),\n                    self.FastParams[self.totalMatrices + 2].eval())\n            np.save(os.path.join(currDir, ""Bo.npy""),\n                    self.FastParams[self.totalMatrices + 3].eval())\n\n        np.save(os.path.join(currDir, ""FC.npy""), self.FC.eval())\n        np.save(os.path.join(currDir, ""FCbias.npy""), self.FCbias.eval())\n\n    def train(self, batchSize, totalEpochs, sess,\n              Xtrain, Xtest, Ytrain, Ytest,\n              decayStep, decayRate, dataDir, currDir):\n        \'\'\'\n        The Dense - IHT - Sparse Retrain Routine for FastCell Training\n        \'\'\'\n        fileName = str(self.FastObj.cellType) + \'Results.txt\'\n        resultFile = open(os.path.join(dataDir, fileName), \'a+\')\n        numIters = int(np.ceil(float(Xtrain.shape[0]) / float(batchSize)))\n        totalBatches = numIters * totalEpochs\n\n        counter = 0\n        trimlevel = 15\n        ihtDone = 0\n        maxTestAcc = -10000\n        if self.isDenseTraining is True:\n            ihtDone = 1\n            maxTestAcc = -10000\n        header = \'*\' * 20\n\n        Xtest = Xtest.reshape((-1, self.timeSteps, self.inputDims))\n        Xtrain = Xtrain.reshape((-1, self.timeSteps, self.inputDims))\n\n        self.history = []\n\n        for i in range(0, totalEpochs):\n            print(""\\nEpoch Number: "" + str(i), file=self.outFile)\n\n            if i % decayStep == 0 and i != 0:\n                self.learningRate = self.learningRate * decayRate\n\n            shuffled = list(range(Xtrain.shape[0]))\n            np.random.shuffle(shuffled)\n            trainAcc = 0.0\n            trainLoss = 0.0\n\n            numIters = int(numIters)\n            for j in range(0, numIters):\n\n                if counter == 0:\n                    msg = "" Dense Training Phase Started ""\n                    print(""\\n%s%s%s\\n"" %\n                          (header, msg, header), file=self.outFile)\n\n                k = shuffled[j * batchSize:(j + 1) * batchSize]\n                batchX = Xtrain[k]\n                batchY = Ytrain[k]\n\n                # Mini-batch training\n                _, batchLoss, batchAcc = sess.run([self.trainOp, self.lossOp, self.accuracy], feed_dict={\n                                                  self.X: batchX, self.Y: batchY, self.lr: self.learningRate})\n\n                trainAcc += batchAcc\n                trainLoss += batchLoss\n\n                # Training routine involving IHT and sparse retraining\n                if (counter >= int(totalBatches / 3.0) and\n                        (counter < int(2 * totalBatches / 3.0)) and\n                        counter % trimlevel == 0 and\n                        self.isDenseTraining is False):\n                    self.runHardThrsd(sess)\n                    if ihtDone == 0:\n                        msg = "" IHT Phase Started ""\n                        print(""\\n%s%s%s\\n"" %\n                              (header, msg, header), file=self.outFile)\n                    ihtDone = 1\n                elif ((ihtDone == 1 and counter >= int(totalBatches / 3.0) and\n                       (counter < int(2 * totalBatches / 3.0)) and\n                       counter % trimlevel != 0 and\n                       self.isDenseTraining is False) or\n                        (counter >= int(2 * totalBatches / 3.0) and\n                            self.isDenseTraining is False)):\n                    self.runSparseTraining(sess)\n                    if counter == int(2 * totalBatches / 3.0):\n                        msg = "" Sprase Retraining Phase Started ""\n                        print(""\\n%s%s%s\\n"" %\n                              (header, msg, header), file=self.outFile)\n                counter += 1\n\n            trainLoss /= numIters\n            trainAcc /= numIters\n            print(""Train Loss: "" + str(trainLoss) +\n                  "" Train Accuracy: "" + str(trainAcc),\n                  file=self.outFile)\n\n            testAcc, testLoss = sess.run([self.accuracy, self.lossOp], feed_dict={\n                                         self.X: Xtest, self.Y: Ytest})\n\n            self.history += [\n                {\n                    ""epoch"": i,\n                    ""trainAcc"": trainAcc,\n                    ""trainLoss"": trainLoss,\n                    ""testAcc"": testAcc,\n                    ""testLoss"": testLoss\n                }\n            ]\n\n            if ihtDone == 0:\n                maxTestAcc = -10000\n                maxTestAccEpoch = i\n            else:\n                if maxTestAcc <= testAcc:\n                    maxTestAccEpoch = i\n                    maxTestAcc = testAcc\n                    self.saveParams(currDir)\n\n            print(""Test Loss: "" + str(testLoss) +\n                  "" Test Accuracy: "" + str(testAcc), file=self.outFile)\n            self.outFile.flush()\n\n        print(""\\nMaximum Test accuracy at compressed"" +\n              "" model size(including early stopping): "" +\n              str(maxTestAcc) + "" at Epoch: "" +\n              str(maxTestAccEpoch + 1) + ""\\nFinal Test"" +\n              "" Accuracy: "" + str(testAcc), file=self.outFile)\n        print(""\\n\\nNon-Zeros: "" + str(self.getModelSize()[0]) +\n              "" Model Size: "" + str(float(self.getModelSize()[1]) / 1024.0) +\n              "" KB hasSparse: "" + str(self.getModelSize()[2]) + ""\\n"",\n              file=self.outFile)\n\n        resultFile.write(""MaxTestAcc: "" + str(maxTestAcc) +\n                         "" at Epoch(totalEpochs): "" +\n                         str(maxTestAccEpoch + 1) +\n                         ""("" + str(totalEpochs) + "")"" + "" ModelSize: "" +\n                         str(float(self.getModelSize()[1]) / 1024.0) +\n                         "" KB hasSparse: "" + str(self.getModelSize()[2]) +\n                         "" Param Directory: "" +\n                         str(os.path.abspath(currDir)) + ""\\n"")\n\n        print(""The Model Directory: "" + currDir + ""\\n"")\n\n        # output the tensorflow model\n        # model_dir = os.path.join(currDir, ""model"")\n        # os.makedirs(model_dir, exist_ok=True)\n\n        resultFile.close()\n        self.outFile.flush()\n        if self.outFile is not sys.stdout:\n            self.outFile.close()\n\n    def getAccuracyLog(self):\n        return self.history\n'"
tf/edgeml_tf/trainer/protoNNTrainer.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nfrom __future__ import print_function\nimport tensorflow as tf\nimport numpy as np\nimport sys\nimport edgeml_tf.utils as utils\n\n\nclass ProtoNNTrainer:\n    def __init__(self, protoNNObj, regW, regB, regZ,\n                 sparcityW, sparcityB, sparcityZ,\n                 learningRate, X, Y, lossType=\'l2\'):\n        \'\'\'\n        A wrapper for the various techniques used for training ProtoNN. This\n        subsumes both the responsibility of loss graph construction and\n        performing training. The original training routine that is part of the\n        C++ implementation of EdgeML used iterative hard thresholding (IHT),\n        gamma estimation through median heuristic and other tricks for\n        training ProtoNN. This module implements the same in Tensorflow\n        and python.\n\n        protoNNObj: An instance of ProtoNN class defining the forward\n            computation graph. The loss functions and training routines will be\n            attached to this instance.\n        regW, regB, regZ: Regularization constants for W, B, and\n            Z matrices of protoNN.\n        sparcityW, sparcityB, sparcityZ: Sparsity constraints\n            for W, B and Z matrices. A value between 0 (exclusive) and 1\n            (inclusive) is expected. A value of 1 indicates dense training.\n        learningRate: Initial learning rate for ADAM optimizer.\n        X, Y : Placeholders for data and labels.\n            X [-1, featureDimension]\n            Y [-1, num Labels]\n        lossType: [\'l2\', \'xentropy\']\n        \'\'\'\n        self.protoNNObj = protoNNObj\n        self.__regW = regW\n        self.__regB = regB\n        self.__regZ = regZ\n        self.__sW = sparcityW\n        self.__sB = sparcityB\n        self.__sZ = sparcityZ\n        self.__lR = learningRate\n        self.X = X\n        self.Y = Y\n        self.sparseTraining = True\n        if (sparcityW == 1.0) and (sparcityB == 1.0) and (sparcityZ == 1.0):\n            self.sparseTraining = False\n            print(""Sparse training disabled."", file=sys.stderr)\n        # Define placeholders for sparse training\n        self.W_th = None\n        self.B_th = None\n        self.Z_th = None\n        self.__lossType = lossType\n        self.__validInit = False\n        self.__validInit = self.__validateInit()\n        self.__protoNNOut = protoNNObj(X, Y)\n        self.loss = self.__lossGraph()\n        self.trainStep = self.__trainGraph()\n        self.__hthOp = self.__getHardThresholdOp()\n        self.accuracy = protoNNObj.getAccuracyOp()\n\n    def __validateInit(self):\n        self.__validInit = False\n        msg = ""Sparsity value should be between""\n        msg += "" 0 and 1 (both inclusive).""\n        assert self.__sW >= 0. and self.__sW <= 1., \'W:\' + msg\n        assert self.__sB >= 0. and self.__sB <= 1., \'B:\' + msg\n        assert self.__sZ >= 0. and self.__sZ <= 1., \'Z:\' + msg\n        d, dcap, m, L, _ = self.protoNNObj.getHyperParams()\n        msg = \'Y should be of dimension [-1, num labels/classes]\'\n        msg += \' specified as part of ProtoNN object.\'\n        assert (len(self.Y.shape)) == 2, msg\n        assert (self.Y.shape[1] == L), msg\n        msg = \'X should be of dimension [-1, featureDimension]\'\n        msg += \' specified as part of ProtoNN object.\'\n        assert (len(self.X.shape) == 2), msg\n        assert (self.X.shape[1] == d), msg\n        self.__validInit = True\n        msg = \'Values can be \\\'l2\\\', or \\\'xentropy\\\'\'\n        if self.__lossType not in [\'l2\', \'xentropy\']:\n            raise ValueError(msg)\n        return True\n\n    def __lossGraph(self):\n        pnnOut = self.__protoNNOut\n        l1, l2, l3 = self.__regW, self.__regB, self.__regZ\n        W, B, Z, _ = self.protoNNObj.getModelMatrices()\n        if self.__lossType == \'l2\':\n            with tf.name_scope(\'protonn-l2-loss\'):\n                loss_0 = tf.nn.l2_loss(self.Y - pnnOut)\n                reg = l1 * tf.nn.l2_loss(W) + l2 * tf.nn.l2_loss(B)\n                reg += l3 * tf.nn.l2_loss(Z)\n                loss = loss_0 + reg\n        elif self.__lossType == \'xentropy\':\n            with tf.name_scope(\'protonn-xentropy-loss\'):\n                loss_0 = tf.nn.softmax_cross_entropy_with_logits_v2(logits=pnnOut,\n                                                         labels=tf.stop_gradient(self.Y))\n                loss_0 = tf.reduce_mean(loss_0)\n                reg = l1 * tf.nn.l2_loss(W) + l2 * tf.nn.l2_loss(B)\n                reg += l3 * tf.nn.l2_loss(Z)\n                loss = loss_0 + reg\n        return loss\n\n    def __trainGraph(self):\n        with tf.name_scope(\'protonn-gradient-adam\'):\n            trainStep = tf.train.AdamOptimizer(self.__lR)\n            trainStep = trainStep.minimize(self.loss)\n        return trainStep\n\n    def __getHardThresholdOp(self):\n        W, B, Z, _ = self.protoNNObj.getModelMatrices()\n        self.W_th = tf.placeholder(tf.float32, name=\'W_th\')\n        self.B_th = tf.placeholder(tf.float32, name=\'B_th\')\n        self.Z_th = tf.placeholder(tf.float32, name=\'Z_th\')\n        with tf.name_scope(\'hard-threshold-assignments\'):\n            # hard_thrsd_W = W.assign(self.W_th)\n            # hard_thrsd_B = B.assign(self.B_th)\n            # hard_thrsd_Z = Z.assign(self.Z_th)\n            # Code changes for tf 1.11\n            hard_thrsd_W = tf.assign(W, self.W_th)\n            hard_thrsd_B = tf.assign(B, self.B_th)\n            hard_thrsd_Z = tf.assign(Z, self.Z_th)\n            hard_thrsd_op = tf.group(hard_thrsd_W, hard_thrsd_B, hard_thrsd_Z)\n        return hard_thrsd_op\n\n    def train(self, batchSize, totalEpochs, sess,\n              x_train, x_val, y_train, y_val, noInit=False,\n              redirFile=None, printStep=10, valStep=3):\n        \'\'\'\n        Performs dense training of ProtoNN followed by iterative hard\n        thresholding to enforce sparsity constraints.\n\n        batchSize: Batch size per update\n        totalEpochs: The number of epochs to run training for. One epoch is\n            defined as one pass over the entire training data.\n        sess: The Tensorflow session to use for running various graph\n            operators.\n        x_train, x_val, y_train, y_val: The numpy array containing train and\n            validation data. x data is assumed to in of shape [-1,\n            featureDimension] while y should have shape [-1, numberLabels].\n        noInit: By default, all the tensors of the computation graph are\n        initialized at the start of the training session. Set noInit=False to\n        disable this behaviour.\n        printStep: Number of batches between echoing of loss and train accuracy.\n        valStep: Number of epochs between evolutions on validation set.\n        \'\'\'\n        d, d_cap, m, L, gamma = self.protoNNObj.getHyperParams()\n        assert batchSize >= 1, \'Batch size should be positive integer\'\n        assert totalEpochs >= 1, \'Total epochs should be positive integer\'\n        assert x_train.ndim == 2, \'Expected training data to be of rank 2\'\n        assert x_train.shape[1] == d, \'Expected x_train to be [-1, %d]\' % d\n        assert x_val.ndim == 2, \'Expected validation data to be of rank 2\'\n        assert x_val.shape[1] == d, \'Expected x_val to be [-1, %d]\' % d\n        assert y_train.ndim == 2, \'Expected training labels to be of rank 2\'\n        assert y_train.shape[1] == L, \'Expected y_train to be [-1, %d]\' % L\n        assert y_val.ndim == 2, \'Expected validation labels to be of rank 2\'\n        assert y_val.shape[1] == L, \'Expected y_val to be [-1, %d]\' % L\n\n        # Numpy will throw asserts for arrays\n        if sess is None:\n            raise ValueError(\'sess must be valid Tensorflow session.\')\n\n        trainNumBatches = int(np.ceil(len(x_train) / batchSize))\n        valNumBatches = int(np.ceil(len(x_val) / batchSize))\n        x_train_batches = np.array_split(x_train, trainNumBatches)\n        y_train_batches = np.array_split(y_train, trainNumBatches)\n        x_val_batches = np.array_split(x_val, valNumBatches)\n        y_val_batches = np.array_split(y_val, valNumBatches)\n        if not noInit:\n            sess.run(tf.global_variables_initializer())\n        X, Y = self.X, self.Y\n        W, B, Z, _ = self.protoNNObj.getModelMatrices()\n        for epoch in range(totalEpochs):\n            for i in range(len(x_train_batches)):\n                batch_x = x_train_batches[i]\n                batch_y = y_train_batches[i]\n                feed_dict = {\n                    X: batch_x,\n                    Y: batch_y\n                }\n                sess.run(self.trainStep, feed_dict=feed_dict)\n                if i % printStep == 0:\n                    loss, acc = sess.run([self.loss, self.accuracy],\n                                         feed_dict=feed_dict)\n                    msg = ""Epoch: %3d Batch: %3d"" % (epoch, i)\n                    msg += "" Loss: %3.5f Accuracy: %2.5f"" % (loss, acc)\n                    print(msg, file=redirFile)\n\n            # Perform Hard thresholding\n            if self.sparseTraining:\n                W_, B_, Z_ = sess.run([W, B, Z])\n                fd_thrsd = {\n                    self.W_th: utils.hardThreshold(W_, self.__sW),\n                    self.B_th: utils.hardThreshold(B_, self.__sB),\n                    self.Z_th: utils.hardThreshold(Z_, self.__sZ)\n                }\n                sess.run(self.__hthOp, feed_dict=fd_thrsd)\n\n            if (epoch + 1) % valStep  == 0:\n                acc = 0.0\n                loss = 0.0\n                for j in range(len(x_val_batches)):\n                    batch_x = x_val_batches[j]\n                    batch_y = y_val_batches[j]\n                    feed_dict = {\n                        X: batch_x,\n                        Y: batch_y\n                    }\n                    acc_, loss_ = sess.run([self.accuracy, self.loss],\n                                           feed_dict=feed_dict)\n                    acc += acc_\n                    loss += loss_\n                acc /= len(y_val_batches)\n                loss /= len(y_val_batches)\n                print(""Test Loss: %2.5f Accuracy: %2.5f"" % (loss, acc))\n\n'"
tools/SeeDot/seedot/common.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n# Target word length. Currently set to match the word length of Arduino (2\n# bytes)\nwordLength = 16\n\ninputFileType = ""npy""\n\n# Range of max scale factor used for exploration\nmaxScaleRange = 0, -wordLength\n\n# tanh approximation limit\ntanh_limit = 1.0\n\n# MSBuild location\n# Edit the path if not present at the following location\nmsbuildPathOptions = [r""C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\MSBuild\\15.0\\Bin\\MSBuild.exe"",\n                      r""C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\MSBuild\\15.0\\Bin\\MSBuild.exe"",\n                      r""C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Professional\\MSBuild\\15.0\\Bin\\MSBuild.exe""\n                      ]\n\n\nclass Algo:\n    Bonsai = ""bonsai""\n    Protonn = ""protonn""\n    Default = [Bonsai, Protonn]\n    All = [Bonsai, Protonn]\n\n\nclass Version:\n    Fixed = ""fixed""\n    Float = ""float""\n    All = [Fixed, Float]\n\n\nclass DatasetType:\n    Training = ""training""\n    Testing = ""testing""\n    Default = Testing\n    All = [Training, Testing]\n\n\nclass Target:\n    Arduino = ""arduino""\n    X86 = ""x86""\n    Default = Arduino\n    All = [Arduino, X86]\n'"
tools/SeeDot/seedot/main.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport argparse\nimport datetime\nfrom distutils.dir_util import copy_tree\nimport os\nimport shutil\nimport operator\nimport tempfile\nimport traceback\n\nfrom seedot.compiler.converter.converter import Converter\n\nimport seedot.common as Common\nfrom seedot.compiler.compiler import Compiler\nfrom seedot.predictor import Predictor\nimport seedot.util as Util\n\n\nclass Main:\n\n    def __init__(self, algo, version, target, trainingFile, testingFile, modelDir, sf):\n        self.algo, self.version, self.target = algo, version, target\n        self.trainingFile, self.testingFile, self.modelDir = trainingFile, testingFile, modelDir\n        self.sf = sf\n        self.accuracy = {}\n\n    def setup(self):\n        curr_dir = os.path.dirname(os.path.realpath(__file__))\n        \n        copy_tree(os.path.join(curr_dir, ""Predictor""), Common.tempdir)\n\n        for fileName in [""arduino.ino"", ""config.h"", ""library.h"", ""predict.h""]:\n            srcFile = os.path.join(curr_dir, ""arduino"", fileName)\n            destFile = os.path.join(Common.outdir, fileName)\n            shutil.copyfile(srcFile, destFile)\n\n    # Generate the fixed-point code using the input generated from the\n    # Converter project\n    def compile(self, target, sf):\n        print(""Generating code..."", end=\'\')\n\n        # Set input and output files\n        inputFile = os.path.join(Common.tempdir, ""input.sd"")\n        profileLogFile = os.path.join(\n            Common.tempdir, ""output"", self.algo + ""-float"", ""profile.txt"")\n\n        if target == Common.Target.Arduino:\n            outputFile = os.path.join(Common.outdir, ""predict.cpp"")\n        elif target == Common.Target.X86:\n            outputFile = os.path.join(Common.tempdir, ""seedot_fixed.cpp"")\n\n        try:\n            obj = Compiler(self.algo, target, inputFile,\n                           outputFile, profileLogFile, sf)\n            obj.run()\n        except:\n            print(""failed!\\n"")\n            #traceback.print_exc()\n            return False\n\n        print(""completed"")\n        return True\n\n    # Run the converter project to generate the input files using reading the\n    # training model\n    def convert(self, version, datasetType, target):\n        print(""Generating input files for %s %s dataset..."" %\n              (version, datasetType), end=\'\')\n\n        # Create output dirs\n        if target == Common.Target.Arduino:\n            outputDir = os.path.join(Common.outdir, ""input"")\n            datasetOutputDir = outputDir\n        elif target == Common.Target.X86:\n            outputDir = Common.tempdir\n            datasetOutputDir = os.path.join(Common.tempdir, ""input"")\n        else:\n            assert False\n\n        os.makedirs(datasetOutputDir, exist_ok=True)\n        os.makedirs(outputDir, exist_ok=True)\n\n        try:\n            obj = Converter(self.algo, version, datasetType, target,\n                            datasetOutputDir, outputDir)\n            obj.setInput(self.modelDir, self.trainingFile, self.testingFile)\n            obj.run()\n        except Exception as e:\n            traceback.print_exc()\n            return False\n\n        print(""done\\n"")\n        return True\n\n    # Build and run the Predictor project\n    def predict(self, version, datasetType):\n        outputDir = os.path.join(\n            Common.tempdir, ""output"", self.algo + ""-"" + version)\n\n        curDir = os.getcwd()\n        os.chdir(Common.tempdir)\n\n        obj = Predictor(self.algo, version, datasetType, outputDir)\n        acc = obj.run()\n\n        os.chdir(curDir)\n\n        return acc\n\n    # Compile and run the generated code once for a given scaling factor\n    def runOnce(self, version, datasetType, target, sf):\n        res = self.compile(target, sf)\n        if res == False:\n            return False, False\n\n        acc = self.predict(version, datasetType)\n        if acc == None:\n            return False, True\n\n        self.accuracy[sf] = acc\n        print(""Accuracy is %.3f%%\\n"" % (acc))\n\n        return True, False\n\n    # Iterate over multiple scaling factors and store their accuracies\n    def performSearch(self):\n        start, end = Common.maxScaleRange\n        searching = False\n\n        for i in range(start, end, -1):\n            print(""Testing with max scale factor of "" + str(i))\n\n            res, exit = self.runOnce(\n                Common.Version.Fixed, Common.DatasetType.Training, Common.Target.X86, i)\n\n            if exit == True:\n                return False\n\n            # The iterator logic is as follows:\n            # Search begins when the first valid scaling factor is found (runOnce returns True)\n            # Search ends when the execution fails on a particular scaling factor (runOnce returns False)\n            # This is the window where valid scaling factors exist and we\n            # select the one with the best accuracy\n            if res == True:\n                searching = True\n            elif searching == True:\n                break\n\n        # If search didn\'t begin at all, something went wrong\n        if searching == False:\n            return False\n\n        print(""\\nSearch completed\\n"")\n        print(""----------------------------------------------"")\n        print(""Best performing scaling factors with accuracy:"")\n\n        self.sf = self.getBestScale()\n\n        return True\n\n    # Reverse sort the accuracies, print the top 5 accuracies and return the\n    # best scaling factor\n    def getBestScale(self):\n        sorted_accuracy = dict(\n            sorted(self.accuracy.items(), key=operator.itemgetter(1), reverse=True)[:5])\n        print(sorted_accuracy)\n        return next(iter(sorted_accuracy))\n\n    # Find the scaling factor which works best on the training dataset and\n    # predict on the testing dataset\n    def findBestScalingFactor(self):\n        print(""-------------------------------------------------"")\n        print(""Performing search to find the best scaling factor"")\n        print(""-------------------------------------------------\\n"")\n\n        # Generate input files for training dataset\n        res = self.convert(Common.Version.Fixed,\n                           Common.DatasetType.Training, Common.Target.X86)\n        if res == False:\n            return False\n\n        # Search for the best scaling factor\n        res = self.performSearch()\n        if res == False:\n            return False\n\n        print(""Best scaling factor = %d"" % (self.sf))\n\n        return True\n\n    def runOnTestingDataset(self):\n        print(""\\n-------------------------------"")\n        print(""Prediction on testing dataset"")\n        print(""-------------------------------\\n"")\n\n        print(""Setting max scaling factor to %d\\n"" % (self.sf))\n\n        # Generate files for the testing dataset\n        res = self.convert(Common.Version.Fixed,\n                           Common.DatasetType.Testing, Common.Target.X86)\n        if res == False:\n            return False\n\n        # Compile and run code using the best scaling factor\n        res = self.runOnce(\n            Common.Version.Fixed, Common.DatasetType.Testing, Common.Target.X86, self.sf)\n        if res == False:\n            return False\n\n        return True\n\n    # Generate files for training dataset and perform a profiled execution\n    def collectProfileData(self):\n        print(""-----------------------"")\n        print(""Collecting profile data"")\n        print(""-----------------------"")\n\n        res = self.convert(Common.Version.Float,\n                           Common.DatasetType.Training, Common.Target.X86)\n        if res == False:\n            return False\n\n        acc = self.predict(Common.Version.Float, Common.DatasetType.Training)\n        if acc == None:\n            return False\n\n        print(""Accuracy is %.3f%%\\n"" % (acc))\n\n    # Generate code for Arduino\n    def compileForTarget(self):\n        print(""------------------------------"")\n        print(""Generating code for %s..."" % (self.target))\n        print(""------------------------------\\n"")\n\n        res = self.convert(Common.Version.Fixed,\n                           Common.DatasetType.Testing, self.target)\n        if res == False:\n            return False\n\n        # Copy file\n        srcFile = os.path.join(Common.outdir, ""input"", ""seedot_fixed_model.h"")\n        destFile = os.path.join(Common.outdir, ""model.h"")\n        shutil.copyfile(srcFile, destFile)\n\n        res = self.compile(self.target, self.sf)\n        if res == False:\n            return False\n\n    def runForFixed(self):\n        # Collect runtime profile for ProtoNN\n        if self.algo == Common.Algo.Protonn:\n            res = self.collectProfileData()\n            if res == False:\n                return False\n\n        # Obtain best scaling factor\n        if self.sf == None:\n            res = self.findBestScalingFactor()\n            if res == False:\n                return False\n\n        res = self.runOnTestingDataset()\n        if res == False:\n            return False\n        else:\n            self.testingAccuracy = self.accuracy[self.sf]\n\n        # Generate code for target\n        self.compileForTarget()\n\n        print(""\\nArduino sketch dumped in the folder %s\\n"" % (Common.outdir))\n\n        return True\n\n    def runForFloat(self):\n        print(""---------------------------"")\n        print(""Executing for X86 target..."")\n        print(""---------------------------\\n"")\n\n        res = self.convert(Common.Version.Float,\n                           Common.DatasetType.Testing, Common.Target.X86)\n        if res == False:\n            return False\n\n        acc = self.predict(Common.Version.Float, Common.DatasetType.Testing)\n        if acc == None:\n            return False\n        else:\n            self.testingAccuracy = acc\n\n        print(""Accuracy is %.3f%%\\n"" % (acc))\n\n        print(""------------------------------"")\n        print(""Generating code for Arduino..."")\n        print(""------------------------------\\n"")\n\n        res = self.convert(Common.Version.Float,\n                           Common.DatasetType.Testing, Common.Target.Arduino)\n        if res == False:\n            return False\n\n        # Copy model.h\n        srcFile = os.path.join(Common.outdir, ""input"",\n                               self.algo + ""_"" + ""float_model.h"")\n        destFile = os.path.join(Common.outdir, ""model.h"")\n        shutil.copyfile(srcFile, destFile)\n\n        curr_dir = os.path.dirname(os.path.realpath(__file__))\n\n        # Copy predict.cpp\n        srcFile = os.path.join(\n            curr_dir, ""arduino"", ""floating-point"", self.algo + ""_float.cpp"")\n        destFile = os.path.join(Common.outdir, ""predict.cpp"")\n        shutil.copyfile(srcFile, destFile)\n\n        return True\n\n    def run(self):\n\n        self.setup()\n\n        if self.version == Common.Version.Fixed:\n            return self.runForFixed()\n        else:\n            return self.runForFloat()\n\n\n\nclass MainDriver:\n\n    def parseArgs(self):\n        parser = argparse.ArgumentParser()\n\n        parser.add_argument(""-a"", ""--algo"", choices=Common.Algo.All,\n                            metavar=\'\', help=""Algorithm to run"")\n        parser.add_argument(""--train"", required=True,\n                            metavar=\'\', help=""Training set file"")\n        parser.add_argument(""--test"", required=True,\n                            metavar=\'\', help=""Testing set file"")\n        parser.add_argument(""--model"", required=True, metavar=\'\',\n                            help=""Directory containing trained model"")\n        parser.add_argument(""--tempdir"", metavar=\'\', help=""Scratch directory"")\n        parser.add_argument(""-o"", ""--outdir"", metavar=\'\',\n                            help=""Directory to output the generated Arduino sketch"")\n\n        self.args = parser.parse_args()\n\n        # Verify the input files and directory exists\n        assert os.path.isfile(self.args.train), ""Training set doesn\'t exist""\n        assert os.path.isfile(self.args.test), ""Testing set doesn\'t exist""\n        assert os.path.isdir(self.args.model), ""Model directory doesn\'t exist""\n\n        if self.args.tempdir is not None:\n            assert os.path.isdir(\n                self.args.tempdir), ""Scratch directory doesn\'t exist""\n            Common.tempdir = self.args.tempdir\n        else:\n            Common.tempdir = os.path.join(tempfile.gettempdir(\n            ), ""SeeDot"", datetime.datetime.now().strftime(\'%Y-%m-%d_%H-%M-%S\'))\n            os.makedirs(Common.tempdir, exist_ok=True)\n\n        if self.args.outdir is not None:\n            assert os.path.isdir(\n                self.args.outdir), ""Output directory doesn\'t exist""\n            Common.outdir = self.args.outdir\n        else:\n            Common.outdir = os.path.join(Common.tempdir, ""arduino"")\n            os.makedirs(Common.outdir, exist_ok=True)\n\n    def checkMSBuildPath(self):\n        found = False\n        for path in Common.msbuildPathOptions:\n            if os.path.isfile(path):\n                found = True\n                Common.msbuildPath = path\n\n        if not found:\n            raise Exception(""Msbuild.exe not found at the following locations:\\n%s\\nPlease change the path and run again"" % (\n                Common.msbuildPathOptions))\n\n    def run(self):\n        if Util.windows():\n            self.checkMSBuildPath()\n\n        algo, trainingInput, testingInput, modelDir = self.args.algo, self.args.train, self.args.test, self.args.model\n\n        print(""\\n================================"")\n        print(""Executing on %s for Arduino"" % (algo))\n        print(""--------------------------------"")\n        print(""Train file: %s"" % (trainingInput))\n        print(""Test file: %s"" % (testingInput))\n        print(""Model directory: %s"" % (modelDir))\n        print(""================================\\n"")\n\n        obj = Main(algo, Common.Version.Fixed, Common.Target.Arduino,\n                   trainingInput, testingInput, modelDir, None)\n        obj.run()\n\nif __name__ == ""__main__"":\n    obj = MainDriver()\n    obj.parseArgs()\n    obj.run()\n'"
tools/SeeDot/seedot/predictor.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport os\nimport subprocess\n\nimport seedot.common as Common\nimport seedot.util as Util\n\n# Program to build and run the predictor project using msbuild\n# The accuracy and other statistics are written to the output file specified\n\n\nclass Predictor:\n\n    def __init__(self, algo, version, datasetType, outputDir):\n        self.algo, self.version, self.datasetType = algo, version, datasetType\n\n        self.outputDir = outputDir\n        os.makedirs(self.outputDir, exist_ok=True)\n\n    def buildForWindows(self):\n        \'\'\'\n        Builds using the Predictor.vcxproj project file and creates the executable\n        The target platform is currently set to x64\n        \'\'\'\n        print(""Build..."", end=\'\')\n\n        projFile = ""Predictor.vcxproj""\n        args = [Common.msbuildPath, projFile, r""/t:Build"",\n                r""/p:Configuration=Release"", r""/p:Platform=x64""]\n\n        logFile = os.path.join(self.outputDir, ""msbuild.txt"")\n        with open(logFile, \'w\') as file:\n            process = subprocess.call(args, stdout=file)\n\n        if process == 1:\n            print(""FAILED!!\\n"")\n            return False\n        else:\n            print(""success"")\n            return True\n\n    def buildForLinux(self):\n        print(""Build..."", end=\'\')\n\n        args = [""make""]\n\n        logFile = os.path.join(self.outputDir, ""msbuild.txt"")\n        with open(logFile, \'w\') as file:\n            process = subprocess.call(args, stdout=file)\n\n        if process == 1:\n            print(""FAILED!!\\n"")\n            return False\n        else:\n            print(""success"")\n            return True\n\n    def build(self):\n        if Util.windows():\n            return self.buildForWindows()\n        else:\n            return self.buildForLinux()\n\n    def executeForWindows(self):\n        \'\'\'\n        Invokes the executable with arguments\n        \'\'\'\n        print(""Execution..."", end=\'\')\n\n        exeFile = os.path.join(""x64"", ""Release"", ""Predictor.exe"")\n        args = [exeFile, self.algo, self.version, self.datasetType]\n\n        logFile = os.path.join(self.outputDir, ""exec.txt"")\n        with open(logFile, \'w\') as file:\n            process = subprocess.call(args, stdout=file)\n\n        if process == 1:\n            print(""FAILED!!\\n"")\n            return None\n        else:\n            print(""success"")\n            acc = self.readStatsFile()\n            return acc\n\n    def executeForLinux(self):\n        print(""Execution..."", end=\'\')\n\n        exeFile = os.path.join(""./Predictor"")\n        args = [exeFile, self.algo, self.version, self.datasetType]\n\n        logFile = os.path.join(self.outputDir, ""exec.txt"")\n        with open(logFile, \'w\') as file:\n            process = subprocess.call(args, stdout=file)\n\n        if process == 1:\n            print(""FAILED!!\\n"")\n            return None\n        else:\n            print(""success"")\n            acc = self.readStatsFile()\n            return acc\n\n    def execute(self):\n        if Util.windows():\n            return self.executeForWindows()\n        else:\n            return self.executeForLinux()\n\n    # Read statistics of execution (currently only accuracy)\n    def readStatsFile(self):\n        statsFile = os.path.join(\n            ""output"", self.algo + ""-"" + self.version, ""stats-"" + self.datasetType + "".txt"")\n\n        with open(statsFile, \'r\') as file:\n            content = file.readlines()\n\n        stats = [x.strip() for x in content]\n\n        return float(stats[0])\n\n    def run(self):\n        res = self.build()\n        if res == False:\n            return None\n\n        acc = self.execute()\n\n        return acc\n'"
tools/SeeDot/seedot/util.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport platform\n\nimport seedot.common as Common\n\n\nclass Config:\n    expBigLength = 6\n    exp = ""table""  # ""table"" ""math""\n    codegen = ""funcCall""  # ""funcCall"" ""inline""\n\n\ndef windows():\n    return platform.system() == ""Windows""\n\n\ndef linux():\n    return platform.system() == ""Linux""\n\n\ndef getAlgo():\n    return Config.algo\n\n\ndef setAlgo(algo: str):\n    Config.algo = algo\n\n\ndef getTarget():\n    return Config.target\n\n\ndef setTarget(target: str):\n    Config.target = target\n\n\ndef forArduino():\n    return Config.target == Common.Target.Arduino\n\n\ndef forX86():\n    return Config.target == Common.Target.X86\n\n\ndef getProfileLogFile():\n    return Config.profileLogFile\n\n\ndef setProfileLogFile(file):\n    Config.profileLogFile = file\n\n\ndef getExpBitLength():\n    return Config.expBigLength\n\n\ndef getMaxScale():\n    return Config.maxExpnt\n\n\ndef setMaxExpnt(x: int):\n    Config.maxExpnt = x\n\n\ndef getShrType():\n    # ""shr"" ""shr+"" ""div"" ""negate""\n    return ""div""\n\n\ndef useMathExp():\n    return Config.exp == ""math""\n\n\ndef useTableExp():\n    return Config.exp == ""table""\n\n\ndef genFuncCalls():\n    return Config.codegen == ""funcCall""\n\n\ndef copy_dict(dict_src: dict, diff={}):\n    dict_res = dict(dict_src)\n    dict_res.update(diff)\n    return dict_res\n\n\n# z = [y1,y2,..] = [[x1,..], [x2,..], ..] --> [x1,.., x2,.., ..]\ndef flatten(z: list):\n    return [x for y in z for x in y]\n'"
tools/SeeDot/seedot/writer.py,0,"b""# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n\nclass Writer:\n\n    def __init__(self, fileName):\n        self.file = open(fileName, 'w')\n        self.indentLevel = 0\n\n    def printf(self, str, *args, indent=False):\n        if indent:\n            self.file.write('\\t' * self.indentLevel)\n        self.file.write(str % args)\n\n    def increaseIndent(self):\n        self.indentLevel += 1\n\n    def decreaseIndent(self):\n        self.indentLevel -= 1\n\n    def close(self):\n        self.file.close()\n"""
Applications/GesturePod/training/timestep/__init__.py,0,b''
Applications/GesturePod/training/timestep/eventhandler.py,0,"b'\'\'\'\nCopyright (c) Microsoft Corporation. All rights reserved.\nLicensed under the MIT license.\n\'\'\'\n\nimport matplotlib.pyplot as plt\nfrom timestep.plotterobjects import LinePlotter\nimport numpy as np\n\n\nclass BasicEventHandler:\n    \'\'\'\n    Multiple instances of the event handler can\'t\n    be used and is not intended to be used as such.\n    \'\'\'\n\n    def __init__(self, plotterList, fig, delay=0.00000001):\n        self.plotterList = plotterList\n        self.eventList = []\n        self.fig = fig\n        self.fig.canvas.mpl_connect(\'key_press_event\', self.keyPress)\n        self.fig.canvas.mpl_connect(\'close_event\', self.closeEvent)\n        self.eventCallbackDict = {\n            \'close_event\': self.closeEventCB,\n        }\n        self.exitOnCloseEvent = True\n        self.delay = delay\n\n    def registerEvent(self, key, callback):\n        if key in self.eventCallbackDict:\n            raise ValueError(""An event already registerd for %s"" % key)\n        self.eventCallbackDict[key] = callback\n\n    def deregisterEvent(self, key):\n        if key not in self.eventCallbackDict:\n            return\n        del self.eventCallbackDict[key]\n\n    def updatePlot(self):\n        plotterList = self.plotterList\n        for plotter in plotterList:\n            plotter.updatePlot()\n        plt.draw()\n        plt.pause(self.delay)\n\n    def setExitOnCloseEvent(self, val):\n        if val:\n            self.exitOnCloseEvent = True\n        else:\n            self.exitOnCloseEvent = False\n        return self.exitOnCloseEvent\n\n    def handleQueuedEvents(self):\n        while len(self.eventList) > 0:\n            event = self.eventList[0]\n            del self.eventList[0]\n            if event not in self.eventCallbackDict:\n                continue\n            handler = self.eventCallbackDict[event]\n            handler(event, self)\n        if \'default\' in self.eventCallbackDict:\n            handler = self.eventCallbackDict[\'default\']\n            handler(\'default\', self)\n\n    def keyPress(self, event):\n        self.eventList.append(event.key)\n\n    def closeEvent(self, event):\n        self.eventList.append(event.name)\n\n    def closeEventCB(self, event, obj):\n        if self.exitOnCloseEvent:\n            exit()\n        self.fig.close()\n\n\ndef test():\n    fig = plt.figure()\n    l2 = LinePlotter(10, 111)\n    eventHandler = BasicEventHandler([l2], fig, currStat=\'1\')\n    eventHandler.setExitOnCloseEvent(True)\n    while True:\n        values = np.random.rand(10)\n        l2.setValues(values)\n        eventHandler.updatePlot()\n\n'"
Applications/GesturePod/training/timestep/plotterobjects.py,0,"b'\'\'\'\nCopyright (c) Microsoft Corporation. All rights reserved.\nLicensed under the MIT license.\n\'\'\'\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nclass LinePlotter():\n\n    def __init__(self, length, subplotIndex=None,\n                 ax=None, initValues = None,\n                 ylim=None, **kwargs):\n        self.length = length\n        if all(v is None for v in {subplotIndex, ax}):\n            raise ValueError(""Expected either ax or subplotIndex"")\n        if ax is None:\n            self.subplotIndex = subplotIndex\n            self.ax = plt.subplot(subplotIndex)\n        else:\n            self.ax = ax\n        if ylim is not None:\n            self.ax.set_ylim(ylim)\n        if initValues is None:\n            initValues = np.random.rand(length)\n        self.setValues(initValues)\n        self.line, = self.ax.plot(self.values, **kwargs)\n\n    def updatePlot(self):\n        self.line.set_ydata(self.values)\n\n    def setValues(self, values):\n        assert(len(values) == self.length)\n        self.values = values\n\n\nclass BarPlotter():\n    def __init__(self, numBars, subplotIndex=None,\n                 ax = None, ticks=None,\n                 initValues=None, ylim=None, **kwargs):\n        self.length = numBars\n        if all(v is None for v in {subplotIndex, ax}):\n            raise ValueError(""Expected either ax or subplotIndex"")\n        if ax is None:\n            self.subplotIndex = subplotIndex\n            self.ax = plt.subplot(subplotIndex)\n        else:\n            self.ax = ax\n        if ticks is None:\n            ticks = np.arange(numBars)\n        if ylim is not None:\n            self.ax.set_ylim(ylim)\n        if initValues is None:\n            initValues = np.random.rand(numBars)\n        self.setValues(initValues)\n        self.line = self.ax.bar(ticks, self.values, **kwargs)\n\n    def updatePlot(self):\n        for i in range(len(self.line.patches)):\n            self.ax.patches[i].set_height(self.values[i])\n\n    def setValues(self, values):\n        assert(len(values) == self.length)\n        self.values = values\n\n\nclass BooleanPlotter():\n    def __init__(self, length, subplotIndex=None,\n                 ax = None, initValues = None,\n                 ylim=None, **kwargs):\n        self.length = length\n        if all(v is None for v in {subplotIndex, ax}):\n            raise ValueError(""Expected either ax or subplotIndex"")\n        if ax is None:\n            self.subplotIndex = subplotIndex\n            self.ax = plt.subplot(subplotIndex)\n        else:\n            self.ax = ax\n        if ylim is not None:\n            self.ax.set_ylim(ylim)\n        else:\n            self.ax.set_ylim([-0.1, 1.1])\n        if initValues is None:\n            initValues = np.random.rand(length)\n        self.setValues(initValues)\n        if not (\'linestyle\' in kwargs or \'ls\' in kwargs):\n            kwargs[\'linestyle\'] = \'None\'\n        if not (\'markersize\' in kwargs or \'ms\' in kwargs):\n            kwargs[\'markersize\'] = 10.00\n        if not (\'marker\' in kwargs):\n            kwargs[\'marker\'] = \'o\'\n        if not (\'color\' in kwargs or \'c\' in kwargs):\n            kwargs[\'color\'] = \'r\'\n        self.line, = self.ax.plot(self.values, **kwargs)\n\n    def updatePlot(self):\n        self.line.set_ydata(self.values)\n\n    def setValues(self, values):\n        assert(len(values) == self.length)\n        self.values = values\n\n\nclass StatusBox:\n    def __init__(self, fig, x=None, y=None, initText=None):\n        if x is None:\n            x = 0.01\n            y = 0.01\n        self.x = x\n        self.y = y\n        if initText is None:\n            initText = \'Status:\'\n        self.text = initText\n        self.textbox = fig.text(self.x, self.y, initText)\n\n    def setText(self, s):\n        self.text = s\n\n    def updatePlot(self):\n        self.textbox.set_text(self.text)\n'"
examples/pytorch/FastCells/KWS-training/train_classifier.py,18,"b'#!/usr/bin/env python3\n###################################################################################################\n#\n#  Project:  Embedded Learning Library (ELL)\n#  File:     train_classifier.py\n#  Authors:  Chris Lovett\n#\n#  Requires: Python 3.x\n#\n###################################################################################################\n\nimport argparse\nimport json\nimport math\nimport os\nimport sys\nimport time\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.onnx\nimport random\n\nfrom torch.autograd import Variable, Function\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom training_config import TrainingConfig\nfrom edgeml_pytorch.trainer.fastmodel import *\n\nclass KeywordSpotter(nn.Module):\n    """""" This baseclass provides the PyTorch Module pattern for defining and training keyword spotters """"""\n\n    def __init__(self):\n        """"""\n        Initialize the KeywordSpotter with the following parameters:\n        input_dim - the size of the input audio frame in # samples\n        num_keywords - the number of predictions to come out of the model.\n        """"""\n        super(KeywordSpotter, self).__init__()\n\n        self.training = False\n        self.tracking = False\n\n        self.init_hidden()\n\n    def name(self):\n        return ""KeywordSpotter""\n\n    def init_hidden(self):\n        """""" Clear any  hidden state """"""\n        pass\n\n    def forward(self, input):\n        """""" Perform the forward processing of the given input and return the prediction """"""\n        raise Exception(""need to implement the forward method"")\n\n    def export(self, name, device):\n        """""" Export the model to the ONNX file format """"""\n        self.init_hidden()\n        self.tracking = True\n        dummy_input = Variable(torch.randn(1, 1, self.input_dim))\n        if device:\n            dummy_input = dummy_input.to(device)\n        torch.onnx.export(self, dummy_input, name, verbose=True)\n        self.tracking = False\n\n    def batch_accuracy(self, scores, labels):\n        """""" Compute the training accuracy of the results of a single mini-batch """"""\n        batch_size = scores.shape[0]\n        passed = 0\n        results = []\n        for i in range(batch_size):\n            expected = labels[i]\n            actual = scores[i].argmax()\n            results += [int(actual)]\n            if expected == actual:\n                passed += 1\n        return (float(passed) * 100.0 / float(batch_size), passed, results)\n\n    def configure_optimizer(self, options):\n        initial_rate = options.learning_rate\n        oo = options.optimizer_options\n\n        if options.optimizer == ""Adadelta"":\n            optimizer = optim.Adadelta(self.parameters(), lr=initial_rate, weight_decay=oo.weight_decay,\n                                       rho=oo.rho, eps=oo.eps)\n        elif options.optimizer == ""Adagrad"":\n            optimizer = optim.Adagrad(self.parameters(), lr=initial_rate, weight_decay=oo.weight_decay,\n                                      lr_decay=oo.lr_decay)\n        elif options.optimizer == ""Adam"":\n            optimizer = optim.Adam(self.parameters(), lr=initial_rate, weight_decay=oo.weight_decay,\n                                   betas=oo.betas, eps=oo.eps)\n        elif options.optimizer == ""Adamax"":\n            optimizer = optim.Adamax(self.parameters(), lr=initial_rate, weight_decay=oo.weight_decay,\n                                     betas=oo.betas, eps=oo.eps)\n        elif options.optimizer == ""ASGD"":\n            optimizer = optim.ASGD(self.parameters(), lr=initial_rate, weight_decay=oo.weight_decay,\n                                   lambd=oo.lambd, alpha=oo.alpha, t0=oo.t0)\n        elif options.optimizer == ""RMSprop"":\n            optimizer = optim.RMSprop(self.parameters(), lr=initial_rate, weight_decay=oo.weight_decay,\n                                      eps=oo.eps, alpha=oo.alpha, momentum=oo.momentum, centered=oo.centered)\n        elif options.optimizer == ""Rprop"":\n            optimizer = optim.Rprop(self.parameters(), lr=initial_rate, etas=oo.etas,\n                                    step_sizes=oo.step_sizes)\n        elif options.optimizer == ""SGD"":\n            optimizer = optim.SGD(self.parameters(), lr=initial_rate, weight_decay=oo.weight_decay,\n                                  momentum=oo.momentum, dampening=oo.dampening, nesterov=oo.nesterov)\n        return optimizer\n\n    def configure_lr(self, options, optimizer, ticks, total_iterations):\n        num_epochs = options.max_epochs\n        learning_rate = options.learning_rate\n        lr_scheduler = options.lr_scheduler\n        lr_min = options.lr_min\n        lr_peaks = options.lr_peaks\n        gamma = options.lr_gamma\n        if not lr_min:\n            lr_min = learning_rate\n        scheduler = None\n        if lr_scheduler == ""TriangleLR"":\n            steps = lr_peaks * 2 + 1\n            stepsize = num_epochs / steps\n            scheduler = TriangularLR(optimizer, stepsize * ticks, lr_min, learning_rate, gamma)\n        elif lr_scheduler == ""CosineAnnealingLR"":\n            # divide by odd number to finish on the minimum learning rate\n            cycles = lr_peaks * 2 + 1\n            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_iterations / cycles,\n                                                             eta_min=lr_min)\n        elif lr_scheduler == ""ExponentialLR"":\n            scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma)\n        elif lr_scheduler == ""StepLR"":\n            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=options.lr_step_size, gamma=gamma)\n        elif lr_scheduler == ""ExponentialResettingLR"":\n            reset = (num_epochs * ticks) / 3  # reset at the 1/3 mark.\n            scheduler = ExponentialResettingLR(optimizer, gamma, reset)\n        return scheduler\n\n    def fit(self, training_data, validation_data, options, sparsify=False, device=None, detail=False, run=None):\n        """"""\n        Perform the training.  This is not called ""train"" because\n        the base class already defines that method with a different meaning.\n        The base class ""train"" method puts the Module into ""training mode"".\n        """"""\n        print(""Training {} using {} rows of featurized training input..."".format(self.name(), training_data.num_rows))\n\n        if training_data.mean is not None:\n            mean = torch.from_numpy(np.array([[training_data.mean]])).to(device)\n            std = torch.from_numpy(np.array([[training_data.std]])).to(device)\n        else:\n            mean = None\n            std = None\n\n        self.normalize(mean, std)\n\n        self.training = True\n        start = time.time()\n        loss_function = nn.NLLLoss()\n        optimizer = self.configure_optimizer(options)\n        print(optimizer)\n\n        num_epochs = options.max_epochs\n        batch_size = options.batch_size\n        trim_level = options.trim_level\n        \n        ticks = training_data.num_rows / batch_size  # iterations per epoch\n        \n        # Calculation of total iterations in non-rolling vs rolling training\n        # ticks = num_rows/batch_size (total number of iterations per epoch)\n        # Non-Rolling Training:\n        # Total Iteration = num_epochs * ticks\n        # Rolling Training:\n        # irl = Initial_rolling_length (We are using 2)\n        # If num_epochs <=  max_rolling_length:\n        # Total Iterations = sum(range(irl, irl + num_epochs))\n        # If num_epochs > max_rolling_length:\n        # Total Iterations = sum(range(irl, irl + max_rolling_length)) + (num_epochs - max_rolling_length)*ticks\n        if options.rolling:\n            rolling_length = 2\n            max_rolling_length = int(ticks)\n            if max_rolling_length > options.max_rolling_length + rolling_length:\n                max_rolling_length = options.max_rolling_length + rolling_length\n            bag_count = 100\n            hidden_bag_size = batch_size * bag_count\n            if num_epochs + rolling_length < max_rolling_length:\n                max_rolling_length = num_epochs + rolling_length\n            total_iterations = sum(range(rolling_length, max_rolling_length))\n            if num_epochs + rolling_length > max_rolling_length:\n                epochs_remaining = num_epochs + rolling_length - max_rolling_length\n                total_iterations += epochs_remaining * training_data.num_rows / batch_size\n            ticks = total_iterations / num_epochs\n        else:\n            total_iterations = ticks * num_epochs\n\n        scheduler = self.configure_lr(options, optimizer, ticks, total_iterations)\n\n        # optimizer = optim.Adam(model.parameters(), lr=0.0001)\n        log = []\n\n        for epoch in range(num_epochs):\n            self.train()\n            if options.rolling:\n                rolling_length += 1\n                if rolling_length <= max_rolling_length:\n                    self.init_hidden_bag(hidden_bag_size, device)\n            for i_batch, (audio, labels) in enumerate(training_data.get_data_loader(batch_size)):\n                if not self.batch_first:\n                    audio = audio.transpose(1, 0)  # GRU wants seq,batch,feature\n\n                if device:\n                    self.move_to(device)\n                    audio = audio.to(device)\n                    labels = labels.to(device)\n\n                # Also, we need to clear out the hidden state,\n                # detaching it from its history on the last instance.\n                if options.rolling:\n                    if rolling_length <= max_rolling_length:\n                        if (i_batch + 1) % rolling_length == 0:\n                            self.init_hidden()\n                            break\n\n                    self.rolling_step()\n                else:\n                    self.init_hidden()\n\n                self.to(device) # sparsify routines might move param matrices to cpu    \n\n                # Before the backward pass, use the optimizer object to zero all of the\n                # gradients for the variables it will update (which are the learnable\n                # weights of the model). This is because by default, gradients are\n                # accumulated in buffers( i.e, not overwritten) whenever .backward()\n                # is called. Checkout docs of torch.autograd.backward for more details.\n                optimizer.zero_grad()\n\n                # Run our forward pass.\n                keyword_scores = self(audio)\n\n                # Compute the loss, gradients\n                loss = loss_function(keyword_scores, labels)\n\n                # Backward pass: compute gradient of the loss with respect to all the learnable\n                # parameters of the model. Internally, the parameters of each Module are stored\n                # in Tensors with requires_grad=True, so this call will compute gradients for\n                # all learnable parameters in the model.\n                loss.backward()\n                # move to next learning rate\n                if scheduler:\n                    scheduler.step()\n\n                # Calling the step function on an Optimizer makes an update to its parameters\n                # applying the gradients we computed during back propagation\n                optimizer.step()\n\n                if sparsify:\n                    if epoch >= num_epochs/3:\n                        if epoch < (2*num_epochs)/3:\n                            if i_batch % trim_level == 0:\n                                self.sparsify()\n                            else:\n                                self.sparsifyWithSupport()\n                        else:\n                            self.sparsifyWithSupport()\n                    self.to(device) # sparsify routines might move param matrices to cpu\n\n                learning_rate = optimizer.param_groups[0][\'lr\']\n                if detail:\n                    learning_rate = optimizer.param_groups[0][\'lr\']\n                    log += [{\'iteration\': iteration, \'loss\': loss.item(), \'learning_rate\': learning_rate}]\n            # Find the best prediction in each sequence and return it\'s accuracy\n            passed, total, rate = self.evaluate(validation_data, batch_size, device)\n            learning_rate = optimizer.param_groups[0][\'lr\']\n            current_loss = float(loss.item())\n            print(""Epoch {}, Loss {:.3f}, Validation Accuracy {:.3f}, Learning Rate {}"".format(\n                  epoch, current_loss, rate * 100, learning_rate))\n            log += [{\'epoch\': epoch, \'loss\': current_loss, \'accuracy\': rate, \'learning_rate\': learning_rate}]\n            if run is not None:\n                run.log(\'progress\', epoch / num_epochs)\n                run.log(\'epoch\', epoch)\n                run.log(\'accuracy\', rate)\n                run.log(\'loss\', current_loss)\n                run.log(\'learning_rate\', learning_rate)\n\n        end = time.time()\n        self.training = False\n        print(""Trained in {:.2f} seconds"".format(end - start))\n        print(""Model size {}"".format(self.get_model_size()))\n        return log\n\n    def evaluate(self, test_data, batch_size, device=None, outfile=None):\n        """"""\n        Evaluate the given test data and print the pass rate\n        """"""\n        self.eval()\n        passed = 0\n        total = 0\n\n        self.zero_grad()\n        results = []\n        with torch.no_grad():\n            for i_batch, (audio, labels) in enumerate(test_data.get_data_loader(batch_size)):\n                batch_size = audio.shape[0]\n                audio = audio.transpose(1, 0)  # GRU wants seq,batch,feature\n                if device:\n                    audio = audio.to(device)\n                    labels = labels.to(device)\n                total += batch_size\n                self.init_hidden()\n                keyword_scores = self(audio)\n                last_accuracy, ok, actual = self.batch_accuracy(keyword_scores, labels)\n                results += actual\n                passed += ok\n\n        if outfile:\n            print(""Saving evaluation results in \'{}\'"".format(outfile))\n            with open(outfile, ""w"") as f:\n                json.dump(results, f)\n\n        return (passed, total, passed / total)\n\n\nclass AudioDataset(Dataset):\n    """"""\n    Featurized Audio in PyTorch Dataset so we can get a DataLoader that is needed for\n    mini-batch training.\n    """"""\n\n    def __init__(self, filename, config, keywords, training=False):\n        """""" Initialize the AudioDataset from the given *.npz file """"""\n        self.dataset = np.load(filename)\n\n        # get parameters saved by make_dataset.py\n        parameters = self.dataset[""parameters""]\n        self.sample_rate = int(parameters[0])\n        self.audio_size = int(parameters[1])\n        self.input_size = int(parameters[2])\n        self.window_size = int(parameters[3])\n        self.shift = int(parameters[4])\n        self.features = self.dataset[""features""].astype(np.float32)\n        self.num_rows = len(self.features)\n        self.features = self.features.reshape((self.num_rows, self.window_size, self.input_size))\n\n        if config.normalize:\n            mean = self.features.mean(axis=0)\n            std = self.features.std(axis=0)\n            self.mean = mean.mean(axis=0).astype(np.float32)\n            std = std.mean(axis=0)\n            # self.std is a divisor, so make sure it contains no zeros\n            self.std = np.array(np.where(std == 0, 1, std)).astype(np.float32)\n        else:\n            self.mean = None\n            self.std = None\n\n        self.label_names = self.dataset[""labels""]\n        self.keywords = keywords\n        self.num_keywords = len(self.keywords)\n        self.labels = self.to_long_vector()\n        \n        self.keywords_idx = None\n        self.non_keywords_idx = None\n        if training and config.sample_non_kw is not None:\n            self.keywords_idx, self.non_keywords_idx = self.get_keyword_idx(config.sample_non_kw)\n            self.sample_non_kw_probability = config.sample_non_kw_probability\n\n        msg = ""Loaded dataset {} and found sample rate {}, audio_size {}, input_size {}, window_size {} and shift {}""\n        print(msg.format(os.path.basename(filename), self.sample_rate, self.audio_size, self.input_size,\n                         self.window_size, self.shift))\n\n    def get_data_loader(self, batch_size):\n        """""" Get a DataLoader that can enumerate shuffled batches of data in this dataset """"""\n        return DataLoader(self, batch_size=batch_size, shuffle=True, drop_last=True)\n    \n    def to_long_vector(self):\n        """""" convert the expected labels to a list of integer indexes into the array of keywords """"""\n        indexer = [(0 if x == ""<null>"" else self.keywords.index(x)) for x in self.label_names]\n        return np.array(indexer, dtype=np.longlong)\n\n    def get_keyword_idx(self, non_kw_label):\n        """""" find the keywords and store there index """"""\n        indexer = [ids for ids, label in enumerate(self.label_names) if label != non_kw_label]\n        non_indexer = [ids for ids, label in enumerate(self.label_names) if label == non_kw_label]\n        return (np.array(indexer, dtype=np.longlong), np.array(non_indexer, dtype=np.longlong))\n\n    def __len__(self):\n        """""" Return the number of rows in this Dataset """"""\n        if self.non_keywords_idx is None:\n            return self.num_rows\n        else:\n            return int(len(self.keywords_idx) / (1-self.sample_non_kw_probability))\n\n    def __getitem__(self, idx):\n        """""" Return a single labelled sample here as a tuple """"""\n        if self.non_keywords_idx is None:\n            updated_idx=idx\n        else:\n            if idx < len(self.keywords_idx):\n                updated_idx=self.keywords_idx[idx]\n            else:\n                updated_idx=np.random.choice(self.non_keywords_idx)\n        audio = self.features[updated_idx]  # batch index is second dimension\n        label = self.labels[updated_idx]\n        sample = (audio, label)\n        return sample\n\n            \n\ndef create_model(model_config, input_size, num_keywords):\n    ModelClass = get_model_class(KeywordSpotter)\n    hidden_units_list = [model_config.hidden_units1, model_config.hidden_units2, model_config.hidden_units3]\n    wRank_list = [model_config.wRank1, model_config.wRank2, model_config.wRank3]\n    uRank_list = [model_config.uRank1, model_config.uRank2, model_config.uRank3]\n    wSparsity_list = [model_config.wSparsity, model_config.wSparsity, model_config.wSparsity]\n    uSparsity_list = [model_config.uSparsity, model_config.uSparsity, model_config.uSparsity]\n    print(model_config.gate_nonlinearity, model_config.update_nonlinearity)\n    return ModelClass(model_config.architecture, input_size, model_config.num_layers,\n                      hidden_units_list, wRank_list, uRank_list, wSparsity_list,\n                      uSparsity_list, model_config.gate_nonlinearity, \n                      model_config.update_nonlinearity, num_keywords)\n\ndef save_json(obj, filename):\n    with open(filename, ""w"") as f:\n        json.dump(obj, f, indent=2)\n\n\ndef train(config, evaluate_only=False, outdir=""."", detail=False, azureml=False):\n\n    filename = config.model.filename\n    categories_file = config.dataset.categories\n    wav_directory = config.dataset.path\n    batch_size = config.training.batch_size\n    hidden_units = config.model.hidden_units\n    architecture = config.model.architecture\n    num_layers = config.model.num_layers\n    use_gpu = config.training.use_gpu\n\n    run = None\n\n    if azureml:\n        from azureml.core.run import Run\n        run = Run.get_context()\n        if run is None:\n            print(""### Run.get_context() returned None"")\n        else:\n            print(""### Running in Azure Context"")\n\n    valid_layers = [1, 2, 3]\n    if num_layers not in valid_layers:\n        raise Exception(""--num_layers can only be one of these values {}"".format(valid_layers))\n\n    if not os.path.isdir(outdir):\n        os.makedirs(outdir)\n\n    if not filename:\n        filename = ""{}{}KeywordSpotter.pt"".format(architecture, hidden_units)\n        config.model.filename = filename\n\n    # load the featurized data\n    if not os.path.isdir(wav_directory):\n        print(""### Error: please specify valid --dataset folder location: {}"".format(wav_directory))\n        sys.exit(1)\n\n    if not categories_file:\n        categories_file = os.path.join(wav_directory, ""categories.txt"")\n\n    with open(categories_file, ""r"") as f:\n        keywords = [x.strip() for x in f.readlines()]\n\n    training_file = os.path.join(wav_directory, ""training_list.npz"")\n    testing_file = os.path.join(wav_directory, ""testing_list.npz"")\n    validation_file = os.path.join(wav_directory, ""validation_list.npz"")\n\n    if not os.path.isfile(training_file):\n        print(""Missing file {}"".format(training_file))\n        print(""Please run make_datasets.py"")\n        sys.exit(1)\n    if not os.path.isfile(validation_file):\n        print(""Missing file {}"".format(validation_file))\n        print(""Please run make_datasets.py"")\n        sys.exit(1)\n    if not os.path.isfile(testing_file):\n        print(""Missing file {}"".format(testing_file))\n        print(""Please run make_datasets.py"")\n        sys.exit(1)\n\n    model = None\n\n    device = torch.device(""cpu"")\n    if use_gpu:\n        if torch.cuda.is_available():\n            device = torch.device(""cuda"")\n        else:\n            print(""### CUDA not available!!"")\n\n    print(""Loading {}..."".format(testing_file))\n    test_data = AudioDataset(testing_file, config.dataset, keywords)\n\n    log = None\n    if not evaluate_only:\n        print(""Loading {}..."".format(training_file))\n        training_data = AudioDataset(training_file, config.dataset, keywords, training=True)\n\n        print(""Loading {}..."".format(validation_file))\n        validation_data = AudioDataset(validation_file, config.dataset, keywords)\n\n        if training_data.mean is not None:\n            fname = os.path.join(outdir, ""mean.npy"")\n            print(""Saving {}"".format(fname))\n            np.save(fname, training_data.mean)\n            fname = os.path.join(outdir, ""std.npy"")\n            print(""Saving {}"".format(fname))\n            np.save(fname, training_data.std)\n\n            # use the training_data mean and std variation\n            test_data.mean = training_data.mean\n            test_data.std = training_data.std\n            validation_data.mean = training_data.mean\n            validation_data.std = training_data.std\n\n        print(""Training model {}"".format(filename))\n        model = create_model(config.model, training_data.input_size, training_data.num_keywords)\n        if device.type == \'cuda\':\n            model.cuda()  # move the processing to GPU\n\n        start = time.time()\n        log = model.fit(training_data, validation_data, config.training,\n                       config.model.sparsify, device, detail, run)\n        end = time.time()\n\n        passed, total, rate = model.evaluate(training_data, batch_size, device)\n        print(""Training accuracy = {:.3f} %"".format(rate * 100))\n\n        torch.save(model.state_dict(), os.path.join(outdir, filename))\n\n    print(""Evaluating {} keyword spotter using {} rows of featurized test audio..."".format(\n          architecture, test_data.num_rows))\n    if model is None:\n        msg = ""Loading trained model with input size {}, hidden units {} and num keywords {}""\n        print(msg.format(test_data.input_size, hidden_units, test_data.num_keywords))\n        model = create_model(config.model, test_data.input_size, test_data.num_keywords)\n        model.load_dict(torch.load(filename))\n        if model and device.type == \'cuda\':\n            model.cuda()  # move the processing to GPU\n\n    results_file = os.path.join(outdir, ""results.txt"")\n    passed, total, rate = model.evaluate(test_data, batch_size, device, results_file)\n    print(""Testing accuracy = {:.3f} %"".format(rate * 100))\n\n    if not evaluate_only:\n        name = os.path.splitext(filename)[0] + "".onnx""\n        print(""saving onnx file: {}"".format(name))\n        model.export(os.path.join(outdir, name), device)\n\n        config.dataset.sample_rate = test_data.sample_rate\n        config.dataset.input_size = test_data.audio_size\n        config.dataset.num_filters = test_data.input_size\n        config.dataset.window_size = test_data.window_size\n        config.dataset.shift = test_data.shift\n\n        logdata = {\n            ""accuracy_val"": rate,\n            ""training_time"": end - start,\n            ""log"": log\n        }\n        d = TrainingConfig.to_dict(config)\n        logdata.update(d)\n\n        logname = os.path.join(outdir, ""train_results.json"")\n        save_json(logdata, logname)\n\n    return rate, log\n\n\ndef str2bool(v):\n    if v is None:\n        return False\n    lower = v.lower()\n    return lower in [""t"", ""1"", ""true"", ""yes""]\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(""train a RNN based neural network for keyword spotting"")\n\n    # all the training parameters\n    parser.add_argument(""--epochs"", help=""Number of epochs to train"", type=int)\n    parser.add_argument(""--trim_level"", help=""Number of batches before sparse support is updated in IHT"", type=int)\n    parser.add_argument(""--lr_scheduler"", help=""Type of learning rate scheduler (None, TriangleLR, CosineAnnealingLR,""\n                                               "" ExponentialLR, ExponentialResettingLR)"")\n    parser.add_argument(""--learning_rate"", help=""Default learning rate, and maximum for schedulers"", type=float)\n    parser.add_argument(""--lr_min"", help=""Minimum learning rate for the schedulers"", type=float)\n    parser.add_argument(""--lr_peaks"", help=""Number of peaks for triangle and cosine schedules"", type=float)\n    parser.add_argument(""--batch_size"", ""-bs"", help=""Batch size of training"", type=int)\n    parser.add_argument(""--architecture"", help=""Specify model architecture (FastGRNN)"")\n    parser.add_argument(""--num_layers"", type=int, help=""Number of RNN layers (1, 2 or 3)"")\n    parser.add_argument(""--hidden_units"", ""-hu"", type=int, help=""Number of hidden units in the FastGRNN layers"")\n    parser.add_argument(""--hidden_units1"", ""-hu1"", type=int, help=""Number of hidden units in the FastGRNN 1st layer"")\n    parser.add_argument(""--hidden_units2"", ""-hu2"", type=int, help=""Number of hidden units in the FastGRNN 2nd layer"")\n    parser.add_argument(""--hidden_units3"", ""-hu3"", type=int, help=""Number of hidden units in the FastGRNN 3rd layer"")\n    parser.add_argument(""--use_gpu"", help=""Whether to use fastGRNN for training"", action=""store_true"")\n    parser.add_argument(""--normalize"", help=""Whether to normalize audio dataset"", action=""store_true"")\n    parser.add_argument(""--rolling"", help=""Whether to train model in rolling fashion or not"", action=""store_true"")\n    parser.add_argument(""--max_rolling_length"", help=""Max number of epochs you want to roll the rolling training""\n                        "" default is 100"", type=int)\n    parser.add_argument(""--sample_non_kw"", ""-sl"", type=str, help=""Sample data for this label with probability sample_prob"")\n    parser.add_argument(""--sample_non_kw_probability"", ""-spr"", type=float, help=""Sample from scl with this probability"")\n\n    # arguments for fastgrnn\n    parser.add_argument(""--wRank"", ""-wr"", help=""Rank of W in 1st layer of FastGRNN default is None"", type=int)\n    parser.add_argument(""--uRank"", ""-ur"", help=""Rank of U in 1st layer of FastGRNN default is None"", type=int)\n    parser.add_argument(""--wRank1"", ""-wr1"", help=""Rank of W in 1st layer of FastGRNN default is None"", type=int)\n    parser.add_argument(""--uRank1"", ""-ur1"", help=""Rank of U in 1st layer of FastGRNN default is None"", type=int)\n    parser.add_argument(""--wRank2"", ""-wr2"", help=""Rank of W in 2nd layer of FastGRNN default is None"", type=int)\n    parser.add_argument(""--uRank2"", ""-ur2"", help=""Rank of U in 2nd layer of FastGRNN default is None"", type=int)\n    parser.add_argument(""--wRank3"", ""-wr3"", help=""Rank of W in 3rd layer of FastGRNN default is None"", type=int)\n    parser.add_argument(""--uRank3"", ""-ur3"", help=""Rank of U in 3rd layer of FastGRNN default is None"", type=int)\n    parser.add_argument(""--wSparsity"", ""-wsp"", help=""Sparsity of W matrices"", type=float)\n    parser.add_argument(""--uSparsity"", ""-usp"", help=""Sparsity of U matrices"", type=float)\n    parser.add_argument(""--gate_nonlinearity"", ""-gnl"", help=""Gate Non-Linearity in FastGRNN default is sigmoid""\n                        "" use between [sigmoid, quantSigmoid, tanh, quantTanh]"")\n    parser.add_argument(""--update_nonlinearity"", ""-unl"", help=""Update Non-Linearity in FastGRNN default is Tanh""\n                        "" use between [sigmoid, quantSigmoid, tanh, quantTanh]"")\n\n    # or you can just specify an options file.\n    parser.add_argument(""--config"", help=""Use json file containing all these options (as per \'training_config.py\')"")\n\n    # and some additional stuff ...\n    parser.add_argument(""--azureml"", help=""Tells script we are running in Azure ML context"")\n    parser.add_argument(""--eval"", ""-e"", help=""No training, just evaluate existing model"", action=\'store_true\')\n    parser.add_argument(""--filename"", ""-o"", help=""Name of model file to generate"")\n    parser.add_argument(""--categories"", ""-c"", help=""Name of file containing keywords"")\n    parser.add_argument(""--dataset"", ""-a"", help=""Path to the audio folder containing \'training.npz\' file"")\n    parser.add_argument(""--outdir"", help=""Folder in which to store output file and log files"")\n    parser.add_argument(""--detail"", ""-d"", help=""Save loss info for every iteration not just every epoch"",\n                        action=""store_true"")\n    args = parser.parse_args()\n\n    config = TrainingConfig()\n    if args.config:\n        config.load(args.config)\n\n    azureml = str2bool(args.azureml)\n\n    # then any user defined options overrides these defaults\n    if args.epochs:\n        config.training.max_epochs = args.epochs\n    if args.trim_level:\n        config.training.trim_level = args.trim_level\n    else:\n        config.training.trim_level = 15\n    if args.learning_rate:\n        config.training.learning_rate = args.learning_rate\n    if args.lr_min:\n        config.training.lr_min = args.lr_min\n    if args.lr_peaks:\n        config.training.lr_peaks = args.lr_peaks\n    if args.lr_scheduler:\n        config.training.lr_scheduler = args.lr_scheduler\n    if args.batch_size:\n        config.training.batch_size = args.batch_size\n    if args.rolling:\n        config.training.rolling = args.rolling\n    if args.max_rolling_length:\n        config.training.max_rolling_length = args.max_rolling_length\n    if args.architecture:\n        config.model.architecture = args.architecture\n    if args.num_layers:\n        config.model.num_layers = args.num_layers\n    if args.hidden_units:\n        config.model.hidden_units = args.hidden_units\n    if args.hidden_units1:\n        config.model.hidden_units = args.hidden_units\n    if args.hidden_units2:\n        config.model.hidden_units = args.hidden_units\n    if args.hidden_units3:\n        config.model.hidden_units = args.hidden_units\n    if config.model.num_layers >= 1:\n        if config.model.hidden_units1 is None:\n            config.model.hidden_units1 = config.model.hidden_units\n    if config.model.num_layers >= 2:\n        if config.model.hidden_units2 is None:\n            config.model.hidden_units2 = config.model.hidden_units1\n    if config.model.num_layers == 3:\n        if config.model.hidden_units3 is None:\n            config.model.hidden_units3 = config.model.hidden_units2\n    if args.filename:\n        config.model.filename = args.filename\n    if args.use_gpu:\n        config.training.use_gpu = args.use_gpu\n    if args.normalize:\n        config.dataset.normalize = args.normalize\n    if args.categories:\n        config.dataset.categories = args.categories\n    if args.dataset:\n        config.dataset.path = args.dataset\n    if args.sample_non_kw:\n        config.dataset.sample_non_kw = args.sample_non_kw\n        if args.sample_non_kw_probability is None:\n            config.dataset.sample_non_kw_probability = 0.5\n        else:\n            config.dataset.sample_non_kw_probability = args.sample_non_kw_probability\n    else:\n        config.dataset.sample_non_kw = None\n\n    if args.wRank:\n        config.model.wRank = args.wRank\n    if args.uRank:\n        config.model.uRank = args.wRank\n    if args.wRank1:\n        config.model.wRank1 = args.wRank1\n    if args.uRank1:\n        config.model.uRank1 = args.wRank1\n    if config.model.wRank1 is None:\n        if config.model.wRank is not None:\n            config.model.wRank1 = config.model.wRank\n    if config.model.uRank1 is None:\n        if config.model.uRank is not None:\n            config.model.uRank1 = config.model.uRank\n    if args.wRank2:\n        config.model.wRank2 = args.wRank2\n    if args.uRank2:\n        config.model.uRank2 = args.wRank2\n    if args.wRank3:\n        config.model.wRank3 = args.wRank3\n    if args.uRank3:\n        config.model.uRank3 = args.wRank3\n    if args.wSparsity:\n        config.model.wSparsity = args.wSparsity\n    else:\n        config.model.wSparsity = 1.0\n    if args.uSparsity:\n        config.model.uSparsity = args.uSparsity\n    else:\n        config.model.uSparsity = 1.0\n    if config.model.uSparsity < 1.0 or config.model.wSparsity < 1.0:\n        config.model.sparsify = True\n    else:\n        config.model.sparsify = False\n    if args.gate_nonlinearity:\n        config.model.gate_nonlinearity = args.gate_nonlinearity\n    if args.update_nonlinearity:\n        config.model.update_nonlinearity = args.update_nonlinearity\n\n    if not os.path.isfile(""config.json""):\n        config.save(""config.json"")\n\n    train(config, args.eval, args.outdir, args.detail, azureml)\n'"
examples/pytorch/FastCells/KWS-training/training_config.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n# config file for train_classifier.py\n\nimport json\nimport os\n\nclass ModelOptions:\n    def __init__(self):\n        self.architecture = ""FastGRNN""\n        self.num_layers = 1\n        self.hidden_units = None\n        self.hidden_units1 = None\n        self.hidden_units2 = None\n        self.hidden_units3 = None\n        self.filename = """"\n        self.wRank = None\n        self.uRank = None\n        self.wRank1 = None\n        self.uRank1 = None\n        self.wRank2 = None\n        self.uRank2 = None\n        self.wRank3 = None\n        self.uRank3 = None\n        self.gate_nonlinearity = ""sigmoid""\n        self.update_nonlinearity = ""tanh""\n\n\nclass DatasetOptions:\n    def __init__(self):\n        self.name = ""speechcommandsv01""\n        self.featurizer = ""featurizer_mel_16000_512_512_80_40_log""\n        self.categories = ""categories.txt""\n        self.path = """"\n        self.auto_scale = False\n        self.normalize = False\n\n\nclass OptimizerOptions:\n    def __init__(self):\n        self.weight_decay = 1e-5\n        self.momentum = 0.9  # RMSprop\n        self.centered = False  # RMSprop\n        self.alpha = 0  # ASGD, RMSprop\n        self.eps = 1e-8\n        self.rho = 0  # Adadelta\n        self.lr_decay = 0  # Adagrad\n        self.betas = (0.9, 0.999)  # Adam, SparseAdam, Adamax\n        self.lambd = 0.0001  # ASGD\n        self.t0 = 1000000.0  # ASGD\n        self.etas = (0.5, 1.2)  # Rprop\n        self.dampening = 0  # SGD\n        self.step_sizes = (1e-06, 50)  # Rprop\n        self.nesterov = True # SGD\n\n\nclass TrainingOptions:\n    def __init__(self):\n        self.max_epochs = 30\n        self.learning_rate = 1e-2\n        self.lr_scheduler = None\n        self.lr_peaks = 1\n        self.lr_min = 1e-5\n        self.lr_gamma = 1\n        self.lr_step_size = 1\n        self.batch_size = 128\n        self.optimizer = ""SGD""\n        self.optimizer_options = OptimizerOptions()\n        self.use_gpu = False\n        self.rolling = False\n        self.max_rolling_length = 100\n        self.decay_step = 200\n        self.decay_rate = 0.1\n\n\nclass TrainingConfig:\n    def __init__(self):\n        self.name = """"\n        self.description = """"\n        self.folder = None\n\n        self.model = ModelOptions()\n        self.dataset = DatasetOptions()\n        self.training = TrainingOptions()\n\n        self.job_id = None\n        self.status = None\n        self.downloaded = False\n        self.last_modified = 0\n        self.retries = 0\n        self.filename = None\n        self.sweep = None\n\n    def set(self, name, value):\n        if name not in self.__dict__:\n            self.__dict__[name] = value\n        else:\n            t = self.__dict__[name]\n            if type(t) == int:\n                self.__dict__[name] = int(value)\n            if type(t) == float:\n                self.__dict__[name] = float(value)\n            if type(t) == str:\n                self.__dict__[name] = str(value)\n            else:\n                self.__dict__[name] = value\n\n    def load(self, filename):\n        with open(filename, ""r"") as f:\n            data = json.load(f)\n        TrainingConfig.from_dict(self, data)\n        self.filename = filename\n        self.last_modified = os.path.getmtime(self.filename)\n\n    def save(self, filename):\n        """""" save an options.json file in the self.filename location """"""\n        self.filename = filename\n        data = TrainingConfig.to_dict(self)\n        data[""model""] = self.model.__dict__\n        with open(filename, ""w"") as f:\n            json.dump(data, f, indent=2, sort_keys=True)\n        self.last_modified = os.path.getmtime(self.filename)\n\n    @staticmethod\n    def to_dict(obj):\n        data = dict(obj.__dict__)\n        for k in data:\n            o = data[k]\n            if hasattr(o, ""__dict__""):\n                data[k] = TrainingConfig.to_dict(o)\n        return data\n\n    @staticmethod\n    def from_dict(obj, data):\n        for k in data:\n            v = data[k]\n            if not hasattr(obj, k):\n                setattr(obj, k, v)\n            else:\n                if isinstance(v, dict):\n                    TrainingConfig.from_dict(getattr(obj, k), v)\n                elif isinstance(getattr(obj, k), tuple):\n                    setattr(obj, k, tuple(v))\n                else:\n                    setattr(obj, k, v)\n'"
tools/SeeDot/seedot/compiler/__init__.py,0,b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n'
tools/SeeDot/seedot/compiler/compiler.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nfrom antlr4 import *\nimport argparse\nimport os\n\nfrom seedot.compiler.antlr.seedotLexer import seedotLexer as SeeDotLexer\nfrom seedot.compiler.antlr.seedotParser import seedotParser as SeeDotParser\n\nimport seedot.compiler.ast.ast as AST\nimport seedot.compiler.ast.astBuilder as ASTBuilder\nfrom seedot.compiler.ast.printAST import PrintAST\n\nfrom seedot.compiler.codegen.arduino import Arduino as ArduinoCodegen\nfrom seedot.compiler.codegen.x86 import X86 as X86Codegen\n\nfrom seedot.compiler.ir.irBuilder import IRBuilder\nimport seedot.compiler.ir.irUtil as IRUtil\n\nfrom seedot.compiler.type import InferType\nfrom seedot.util import *\nfrom seedot.writer import Writer\n\n\nclass Compiler:\n\n    def __init__(self, algo, target, inputFile, outputFile, profileLogFile, maxExpnt):\n        if os.path.isfile(inputFile) == False:\n            raise Exception(""Input file doesn\'t exist"")\n\n        setAlgo(algo)\n        setTarget(target)\n        self.input = FileStream(inputFile)\n        self.outputFile = outputFile\n        setProfileLogFile(profileLogFile)\n        setMaxExpnt(maxExpnt)\n\n    def run(self):\n        # Parse and generate CST for the input\n        lexer = SeeDotLexer(self.input)\n        tokens = CommonTokenStream(lexer)\n        parser = SeeDotParser(tokens)\n        tree = parser.expr()\n\n        # Generate AST\n        ast = ASTBuilder.ASTBuilder().visit(tree)\n\n        # Pretty printing AST\n        # PrintAST().visit(ast)\n\n        # Perform type inference\n        InferType().visit(ast)\n\n        IRUtil.init()\n\n        res, state = self.compile(ast)\n\n        writer = Writer(self.outputFile)\n\n        if forArduino():\n            codegen = ArduinoCodegen(writer, *state)\n        elif forX86():\n            codegen = X86Codegen(writer, *state)\n        else:\n            assert False\n\n        codegen.printAll(*res)\n\n        writer.close()\n\n    def compile(self, ast):\n        return self.genCodeWithFuncCalls(ast)\n\n    def genCodeWithFuncCalls(self, ast):\n\n        compiler = IRBuilder()\n\n        res = compiler.visit(ast)\n\n        state = compiler.decls, compiler.scales, compiler.intvs, compiler.cnsts, compiler.expTables, compiler.globalVars\n\n        return res, state\n'"
tools/SeeDot/seedot/compiler/type.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nfrom functools import reduce\nimport operator\n\nfrom seedot.compiler.antlr.seedotParser import seedotParser as SeeDotParser\n\nimport seedot.compiler.ast.ast as AST\nfrom seedot.compiler.ast.astVisitor import ASTVisitor\n\n\nclass Type:\n    pass\n\n\nclass Int(Type):\n    pass\n\n\nclass Tensor(Type):\n\n    def __init__(self, shape: list):\n        self.shape = shape\n        self.dim = len(shape)\n\n    def size(self):\n        return reduce(operator.mul, self.shape, 1)\n\n    # Tensor without any dimension (float) or a tensor with all dimensions\n    # equal to 1\n    def isShapeOne(self):\n        return self.dim == 0 or self.size() == 1\n\n\ndef isInt(type: Type):\n    return isinstance(type, Int)\n\n\ndef isTensor(type: Type):\n    return isinstance(type, Tensor)\n\n\ndef isEqual(type1: Type, type2: Type):\n    if isInt(type1) and isInt(type2):\n        return True\n    elif isTensor(type1) and isTensor(type2):\n        if type1.dim != type2.dim:\n            return False\n        return type1.shape == type2.shape\n    else:\n        assert False\n\n\nclass InferType(ASTVisitor):\n\n    def visitInt(self, node: AST.Int):\n        node.type = Int()\n        return node.type\n\n    # Float is represented as a tensor with 0 dimension\n    def visitFloat(self, node: AST.Float):\n        node.type = Tensor([])\n        return node.type\n\n    def visitId(self, node: AST.ID):\n        node.type = node.gamma[node.name]\n        return node.type\n\n    def visitDecl(self, node: AST.Decl):\n        node.type = Tensor(node.shape)\n        return node.type\n\n    # Matrix transpose\n    def visitTransp(self, node: AST.Transp):\n        node.expr.gamma = dict(node.gamma)\n        exprType = self.visit(node.expr)\n\n        assert isTensor(exprType) and exprType.dim == 2\n\n        [m, n] = exprType.shape\n        node.type = Tensor([n, m])\n\n        return node.type\n\n    # Reshape the tensor with custom dimensions\n    def visitReshape(self, node: AST.Reshape):\n        node.expr.gamma = dict(node.gamma)\n        exprType = self.visit(node.expr)\n\n        assert isTensor(exprType) and exprType.dim >= 1\n\n        # Reshape is valid if the total number of elements remain same after\n        # reshape\n        assert reduce(operator.mul, exprType.shape, 1) == reduce(\n            operator.mul, node.shape, 1)\n        node.type = Tensor(node.shape)\n\n        return node.type\n\n    # Reduces the shape of a tensor by choosing the maximum from a filter\n    def visitMaxpool(self, node: AST.Maxpool):\n        node.expr.gamma = dict(node.gamma)\n        exprType = self.visit(node.expr)\n\n        [n1, n2, n3, n4] = exprType.shape\n\n        # Implementation only performs maxpool over a 4D input\n        assert isTensor(exprType) and exprType.dim == 4\n\n        # Implementation needs node.dim to exactly divide matrix dimensions\n        assert n2 % node.dim == 0 and n3 % node.dim == 0\n\n        shape = [n1, n2 // node.dim, n3 // node.dim, n4]\n        node.type = Tensor(shape)\n\n        return node.type\n\n    # Indexing a tensor\n    def visitIndex(self, node: AST.Index):\n        node.expr.gamma = dict(node.gamma)\n        exprType = self.visit(node.expr)\n\n        assert isTensor(exprType) and exprType.dim >= 1\n\n        node.index.gamma = dict(node.gamma)\n        indexType = self.visit(node.index)\n\n        assert isInt(indexType)\n\n        shape = exprType.shape[1:]\n        node.type = Tensor(shape)\n\n        return node.type\n\n    # Currently assuming that the type of each expr is same\n    def visitFuncCall(self, node: AST.FuncCall):\n        type = None\n        for expr in node.exprList:\n            expr.gamma = dict(node.gamma)\n            currType = self.visit(expr)\n\n            if type != None:\n                assert isEqual(type, currType)\n            else:\n                type = currType\n\n        node.type = type\n\n        return node.type\n\n    def visitUop(self, node: AST.Uop):\n        node.expr.gamma = dict(node.gamma)\n        node.type = self.visit(node.expr)\n        return node.type\n\n    # e BINOP f\n    def visitBop1(self, node: AST.Bop1):\n        node.expr1.gamma = dict(node.gamma)\n        eType = self.visit(node.expr1)\n\n        node.expr2.gamma = dict(node.gamma)\n        fType = self.visit(node.expr2)\n\n        if node.op == SeeDotParser.MUL or node.op == SeeDotParser.SPARSEMUL:\n            return self.visitBopMul(node, eType, fType)\n        elif node.op == SeeDotParser.ADDCIR or node.op == SeeDotParser.SUBCIR:\n            return self.visitBopAddOrSubCir(node, eType, fType)\n        elif node.op == SeeDotParser.MULCIR:\n            return self.visitBopMulCir(node, eType, fType)\n        elif node.op == SeeDotParser.CONV:\n            return self.visitBopConv(node, eType, fType)\n        else:\n            assert False\n\n    # e * f OR e |*| f\n    def visitBopMul(self, node: AST.Bop1, eType: Type, fType: Type):\n        if isInt(eType) and isInt(fType):\n            node.type = Int()\n        elif isTensor(eType) and isTensor(fType):\n            # Tensor() * Tensor(...)\n            if eType.dim == 0:\n                node.type = fType\n            elif fType.dim == 0:\n                node.type = eType\n\n            # Tensor(...) * Tensor(...)\n            else:\n                assert eType.dim == 2 and fType.dim == 2\n\n                [n1, n2] = eType.shape\n                [n3, n4] = fType.shape\n                assert n2 == n3\n\n                node.type = Tensor([n1, n4])\n        else:\n            assert False\n\n        return node.type\n\n    # e <+> f OR e <-> f\n    def visitBopAddOrSubCir(self, node: AST.Bop1, eType: Type, fType: Type):\n        assert isTensor(eType) and isTensor(fType)\n        assert eType.dim >= fType.dim\n        assert fType.dim == 1\n        assert eType.shape[-1] == fType.shape[-1]\n\n        shape = eType.shape\n        node.type = Tensor(shape)\n        return node.type\n\n    # e <*> f - Point-wise multiplication\n    def visitBopMulCir(self, node: AST.Bop1, eType: Type, fType: Type):\n        assert isTensor(eType) and isTensor(fType)\n        assert eType.dim >= 1\n        assert eType.shape == fType.shape\n\n        node.type = eType\n        return node.type\n\n    # e # f\n    def visitBopConv(self, node: AST.Bop1, eType: Type, fType: Type):\n        assert isTensor(eType) and isTensor(fType)\n        assert eType.dim == 4 and fType.dim == 4\n\n        # Implementation does Conv on 4D input on 4D filter\n        # Input is padded with 0s to ensure that the output dimension of the\n        # matrix is same as the input\n        [n, h, w, cin] = eType.shape\n        [hf, wf, cin_, cout] = fType.shape\n        assert cin == cin_\n\n        shape = [n, h, w, cout]\n        node.type = Tensor(shape)\n        return node.type\n\n    # e + f OR e - f\n    def visitBop2(self, node: AST.Bop2):\n        node.expr1.gamma = dict(node.gamma)\n        eType = self.visit(node.expr1)\n\n        node.expr2.gamma = dict(node.gamma)\n        fType = self.visit(node.expr2)\n\n        if isInt(eType) and isInt(fType):\n            pass\n        elif isTensor(eType) and isTensor(fType):\n            assert eType.shape == fType.shape\n        else:\n            assert False\n\n        node.type = eType\n        return node.type\n\n    def visitFunc(self, node: AST.Func):\n        node.expr.gamma = dict(node.gamma)\n        eType = self.visit(node.expr)\n\n        # relu(e)\n        if node.op == SeeDotParser.RELU:\n            assert isTensor(eType) and eType.dim >= 1\n            node.type = eType\n\n        # exp(e)\n        elif node.op == SeeDotParser.EXP:\n            # Currently supports exp() on a tensor with single element\n            assert isTensor(eType) and eType.isShapeOne()\n            node.type = eType\n\n        # argmax(e)\n        elif node.op == SeeDotParser.ARGMAX:\n            assert isTensor(eType) and eType.dim >= 1\n            node.type = Int()\n\n        # sgn(e)\n        elif node.op == SeeDotParser.SGN:\n            assert isTensor(eType) and eType.isShapeOne()\n            node.type = Int()\n\n        # tanh(e)\n        elif node.op == SeeDotParser.TANH:\n            assert isTensor(eType) and eType.dim == 2\n            node.type = eType\n\n        else:\n            assert False\n\n        return node.type\n\n    # $(x=[1:5]) e\n    def visitSum(self, node: AST.Sum):\n        node.expr.gamma = dict(node.gamma)\n        node.expr.gamma[node.name] = Int()\n        eType = self.visit(node.expr)\n\n        assert isTensor(eType)\n        node.type = eType\n\n        return node.type\n\n    # e >= 0?  f : g\n    def visitCond(self, node: AST.Cond):\n        node.expr.gamma = dict(node.gamma)\n        eType = self.visit(node.expr)\n\n        node.trueBlock.gamma = dict(node.gamma)\n        fType = self.visit(node.trueBlock)\n\n        node.falseBlock.gamma = dict(node.gamma)\n        gType = self.visit(node.falseBlock)\n\n        assert isInt(eType) or (isTensor(eType) and eType.isShapeOne())\n        assert (isInt(fType) and isInt(gType)) or (isTensor(fType)\n                                                   and isTensor(gType) and fType.shape == gType.shape)\n\n        node.type = fType\n        return node.type\n\n    # Let x = e in f\n    def visitLet(self, node: AST.Let):\n        node.decl.gamma = dict(node.gamma)\n        eType = self.visit(node.decl)\n\n        node.expr.gamma = dict(node.gamma)\n        node.expr.gamma[node.name] = eType\n        fType = self.visit(node.expr)\n\n        node.type = fType\n        return node.type\n'"
tools/SeeDot/seedot/compiler/antlr/__init__.py,0,b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n'
tools/SeeDot/seedot/compiler/antlr/seedotLexer.py,0,"b'# Generated from seedot.g4 by ANTLR 4.7\nfrom antlr4 import *\nfrom io import StringIO\nfrom typing.io import TextIO\nimport sys\n\n\ndef serializedATN():\n    with StringIO() as buf:\n        buf.write(""\\3\\u608b\\ua72a\\u8133\\ub9ed\\u417c\\u3be7\\u7786\\u5964\\2#"")\n        buf.write(""\\u0107\\b\\1\\4\\2\\t\\2\\4\\3\\t\\3\\4\\4\\t\\4\\4\\5\\t\\5\\4\\6\\t\\6\\4\\7"")\n        buf.write(""\\t\\7\\4\\b\\t\\b\\4\\t\\t\\t\\4\\n\\t\\n\\4\\13\\t\\13\\4\\f\\t\\f\\4\\r\\t\\r"")\n        buf.write(""\\4\\16\\t\\16\\4\\17\\t\\17\\4\\20\\t\\20\\4\\21\\t\\21\\4\\22\\t\\22\\4\\23"")\n        buf.write(""\\t\\23\\4\\24\\t\\24\\4\\25\\t\\25\\4\\26\\t\\26\\4\\27\\t\\27\\4\\30\\t\\30"")\n        buf.write(""\\4\\31\\t\\31\\4\\32\\t\\32\\4\\33\\t\\33\\4\\34\\t\\34\\4\\35\\t\\35\\4\\36"")\n        buf.write(""\\t\\36\\4\\37\\t\\37\\4 \\t \\4!\\t!\\4\\""\\t\\""\\4#\\t#\\4$\\t$\\4%\\t%"")\n        buf.write(""\\4&\\t&\\4\\\'\\t\\\'\\3\\2\\3\\2\\3\\3\\3\\3\\3\\4\\3\\4\\3\\5\\3\\5\\3\\6\\3\\6"")\n        buf.write(""\\3\\7\\3\\7\\3\\7\\3\\b\\3\\b\\3\\t\\3\\t\\3\\n\\3\\n\\3\\n\\3\\13\\3\\13\\3\\f"")\n        buf.write(""\\3\\f\\3\\r\\3\\r\\3\\16\\3\\16\\3\\17\\3\\17\\3\\17\\3\\17\\3\\20\\3\\20\\3"")\n        buf.write(""\\20\\3\\20\\3\\21\\3\\21\\3\\22\\3\\22\\3\\22\\3\\22\\3\\23\\3\\23\\3\\23"")\n        buf.write(""\\3\\23\\3\\24\\3\\24\\3\\24\\3\\24\\3\\24\\3\\25\\3\\25\\3\\25\\3\\25\\3\\26"")\n        buf.write(""\\3\\26\\3\\26\\3\\26\\3\\26\\3\\26\\3\\26\\3\\27\\3\\27\\3\\27\\3\\27\\3\\30"")\n        buf.write(""\\3\\30\\3\\30\\3\\30\\3\\30\\3\\31\\3\\31\\3\\31\\3\\31\\3\\31\\3\\31\\3\\31"")\n        buf.write(""\\3\\31\\3\\32\\3\\32\\3\\32\\3\\32\\3\\32\\3\\32\\3\\32\\3\\32\\3\\33\\3\\33"")\n        buf.write(""\\3\\34\\3\\34\\3\\34\\3\\34\\3\\35\\3\\35\\3\\35\\3\\36\\3\\36\\3\\36\\3\\36"")\n        buf.write(""\\7\\36\\u00b4\\n\\36\\f\\36\\16\\36\\u00b7\\13\\36\\3\\37\\3\\37\\3 \\6"")\n        buf.write("" \\u00bc\\n \\r \\16 \\u00bd\\3!\\3!\\3\\""\\5\\""\\u00c3\\n\\""\\3\\""\\3"")\n        buf.write(""\\""\\5\\""\\u00c7\\n\\""\\3\\""\\5\\""\\u00ca\\n\\""\\3\\""\\6\\""\\u00cd\\n\\""\\r"")\n        buf.write(""\\""\\16\\""\\u00ce\\3\\""\\3\\""\\5\\""\\u00d3\\n\\""\\3#\\6#\\u00d6\\n#\\r#"")\n        buf.write(""\\16#\\u00d7\\5#\\u00da\\n#\\3#\\3#\\6#\\u00de\\n#\\r#\\16#\\u00df"")\n        buf.write(""\\3#\\6#\\u00e3\\n#\\r#\\16#\\u00e4\\3#\\3#\\5#\\u00e9\\n#\\3$\\3$\\5"")\n        buf.write(""$\\u00ed\\n$\\3$\\6$\\u00f0\\n$\\r$\\16$\\u00f1\\3%\\3%\\3&\\6&\\u00f7"")\n        buf.write(""\\n&\\r&\\16&\\u00f8\\3&\\3&\\3\\\'\\3\\\'\\3\\\'\\3\\\'\\7\\\'\\u0101\\n\\\'\\f"")\n        buf.write(""\\\'\\16\\\'\\u0104\\13\\\'\\3\\\'\\3\\\'\\2\\2(\\3\\3\\5\\4\\7\\5\\t\\6\\13\\7\\r"")\n        buf.write(""\\b\\17\\t\\21\\n\\23\\13\\25\\f\\27\\r\\31\\16\\33\\17\\35\\20\\37\\21!"")\n        buf.write(""\\22#\\23%\\24\\\'\\25)\\26+\\27-\\30/\\31\\61\\32\\63\\33\\65\\34\\67"")\n        buf.write(""\\359\\36;\\37=\\2? A\\2C!E\\2G\\2I\\2K\\""M#\\3\\2\\b\\5\\2C\\\\aac|\\3"")\n        buf.write(""\\2\\62;\\4\\2GGgg\\4\\2--//\\5\\2\\13\\f\\17\\17\\""\\""\\4\\2\\f\\f\\17\\17"")\n        buf.write(""\\2\\u0113\\2\\3\\3\\2\\2\\2\\2\\5\\3\\2\\2\\2\\2\\7\\3\\2\\2\\2\\2\\t\\3\\2\\2"")\n        buf.write(""\\2\\2\\13\\3\\2\\2\\2\\2\\r\\3\\2\\2\\2\\2\\17\\3\\2\\2\\2\\2\\21\\3\\2\\2\\2"")\n        buf.write(""\\2\\23\\3\\2\\2\\2\\2\\25\\3\\2\\2\\2\\2\\27\\3\\2\\2\\2\\2\\31\\3\\2\\2\\2\\2"")\n        buf.write(""\\33\\3\\2\\2\\2\\2\\35\\3\\2\\2\\2\\2\\37\\3\\2\\2\\2\\2!\\3\\2\\2\\2\\2#\\3"")\n        buf.write(""\\2\\2\\2\\2%\\3\\2\\2\\2\\2\\\'\\3\\2\\2\\2\\2)\\3\\2\\2\\2\\2+\\3\\2\\2\\2\\2"")\n        buf.write(""-\\3\\2\\2\\2\\2/\\3\\2\\2\\2\\2\\61\\3\\2\\2\\2\\2\\63\\3\\2\\2\\2\\2\\65\\3"")\n        buf.write(""\\2\\2\\2\\2\\67\\3\\2\\2\\2\\29\\3\\2\\2\\2\\2;\\3\\2\\2\\2\\2?\\3\\2\\2\\2\\2"")\n        buf.write(""C\\3\\2\\2\\2\\2K\\3\\2\\2\\2\\2M\\3\\2\\2\\2\\3O\\3\\2\\2\\2\\5Q\\3\\2\\2\\2"")\n        buf.write(""\\7S\\3\\2\\2\\2\\tU\\3\\2\\2\\2\\13W\\3\\2\\2\\2\\rY\\3\\2\\2\\2\\17\\\\\\3\\2"")\n        buf.write(""\\2\\2\\21^\\3\\2\\2\\2\\23`\\3\\2\\2\\2\\25c\\3\\2\\2\\2\\27e\\3\\2\\2\\2\\31"")\n        buf.write(""g\\3\\2\\2\\2\\33i\\3\\2\\2\\2\\35k\\3\\2\\2\\2\\37o\\3\\2\\2\\2!s\\3\\2\\2"")\n        buf.write(""\\2#u\\3\\2\\2\\2%y\\3\\2\\2\\2\\\'}\\3\\2\\2\\2)\\u0082\\3\\2\\2\\2+\\u0086"")\n        buf.write(""\\3\\2\\2\\2-\\u008d\\3\\2\\2\\2/\\u0091\\3\\2\\2\\2\\61\\u0096\\3\\2\\2"")\n        buf.write(""\\2\\63\\u009e\\3\\2\\2\\2\\65\\u00a6\\3\\2\\2\\2\\67\\u00a8\\3\\2\\2\\2"")\n        buf.write(""9\\u00ac\\3\\2\\2\\2;\\u00af\\3\\2\\2\\2=\\u00b8\\3\\2\\2\\2?\\u00bb\\3"")\n        buf.write(""\\2\\2\\2A\\u00bf\\3\\2\\2\\2C\\u00d2\\3\\2\\2\\2E\\u00e8\\3\\2\\2\\2G\\u00ea"")\n        buf.write(""\\3\\2\\2\\2I\\u00f3\\3\\2\\2\\2K\\u00f6\\3\\2\\2\\2M\\u00fc\\3\\2\\2\\2"")\n        buf.write(""OP\\7*\\2\\2P\\4\\3\\2\\2\\2QR\\7+\\2\\2R\\6\\3\\2\\2\\2ST\\7]\\2\\2T\\b\\3"")\n        buf.write(""\\2\\2\\2UV\\7.\\2\\2V\\n\\3\\2\\2\\2WX\\7_\\2\\2X\\f\\3\\2\\2\\2YZ\\7`\\2"")\n        buf.write(""\\2Z[\\7V\\2\\2[\\16\\3\\2\\2\\2\\\\]\\7?\\2\\2]\\20\\3\\2\\2\\2^_\\7<\\2\\2"")\n        buf.write(""_\\22\\3\\2\\2\\2`a\\7@\\2\\2ab\\7?\\2\\2b\\24\\3\\2\\2\\2cd\\7A\\2\\2d\\26"")\n        buf.write(""\\3\\2\\2\\2ef\\7-\\2\\2f\\30\\3\\2\\2\\2gh\\7/\\2\\2h\\32\\3\\2\\2\\2ij\\7"")\n        buf.write("",\\2\\2j\\34\\3\\2\\2\\2kl\\7~\\2\\2lm\\7,\\2\\2mn\\7~\\2\\2n\\36\\3\\2\\2"")\n        buf.write(""\\2op\\7>\\2\\2pq\\7,\\2\\2qr\\7@\\2\\2r \\3\\2\\2\\2st\\7%\\2\\2t\\""\\3"")\n        buf.write(""\\2\\2\\2uv\\7>\\2\\2vw\\7-\\2\\2wx\\7@\\2\\2x$\\3\\2\\2\\2yz\\7>\\2\\2z"")\n        buf.write(""{\\7/\\2\\2{|\\7@\\2\\2|&\\3\\2\\2\\2}~\\7t\\2\\2~\\177\\7g\\2\\2\\177\\u0080"")\n        buf.write(""\\7n\\2\\2\\u0080\\u0081\\7w\\2\\2\\u0081(\\3\\2\\2\\2\\u0082\\u0083"")\n        buf.write(""\\7g\\2\\2\\u0083\\u0084\\7z\\2\\2\\u0084\\u0085\\7r\\2\\2\\u0085*\\3"")\n        buf.write(""\\2\\2\\2\\u0086\\u0087\\7c\\2\\2\\u0087\\u0088\\7t\\2\\2\\u0088\\u0089"")\n        buf.write(""\\7i\\2\\2\\u0089\\u008a\\7o\\2\\2\\u008a\\u008b\\7c\\2\\2\\u008b\\u008c"")\n        buf.write(""\\7z\\2\\2\\u008c,\\3\\2\\2\\2\\u008d\\u008e\\7u\\2\\2\\u008e\\u008f"")\n        buf.write(""\\7i\\2\\2\\u008f\\u0090\\7p\\2\\2\\u0090.\\3\\2\\2\\2\\u0091\\u0092"")\n        buf.write(""\\7v\\2\\2\\u0092\\u0093\\7c\\2\\2\\u0093\\u0094\\7p\\2\\2\\u0094\\u0095"")\n        buf.write(""\\7j\\2\\2\\u0095\\60\\3\\2\\2\\2\\u0096\\u0097\\7t\\2\\2\\u0097\\u0098"")\n        buf.write(""\\7g\\2\\2\\u0098\\u0099\\7u\\2\\2\\u0099\\u009a\\7j\\2\\2\\u009a\\u009b"")\n        buf.write(""\\7c\\2\\2\\u009b\\u009c\\7r\\2\\2\\u009c\\u009d\\7g\\2\\2\\u009d\\62"")\n        buf.write(""\\3\\2\\2\\2\\u009e\\u009f\\7o\\2\\2\\u009f\\u00a0\\7c\\2\\2\\u00a0\\u00a1"")\n        buf.write(""\\7z\\2\\2\\u00a1\\u00a2\\7r\\2\\2\\u00a2\\u00a3\\7q\\2\\2\\u00a3\\u00a4"")\n        buf.write(""\\7q\\2\\2\\u00a4\\u00a5\\7n\\2\\2\\u00a5\\64\\3\\2\\2\\2\\u00a6\\u00a7"")\n        buf.write(""\\7&\\2\\2\\u00a7\\66\\3\\2\\2\\2\\u00a8\\u00a9\\7n\\2\\2\\u00a9\\u00aa"")\n        buf.write(""\\7g\\2\\2\\u00aa\\u00ab\\7v\\2\\2\\u00ab8\\3\\2\\2\\2\\u00ac\\u00ad"")\n        buf.write(""\\7k\\2\\2\\u00ad\\u00ae\\7p\\2\\2\\u00ae:\\3\\2\\2\\2\\u00af\\u00b5"")\n        buf.write(""\\5=\\37\\2\\u00b0\\u00b4\\5=\\37\\2\\u00b1\\u00b4\\5A!\\2\\u00b2\\u00b4"")\n        buf.write(""\\7)\\2\\2\\u00b3\\u00b0\\3\\2\\2\\2\\u00b3\\u00b1\\3\\2\\2\\2\\u00b3"")\n        buf.write(""\\u00b2\\3\\2\\2\\2\\u00b4\\u00b7\\3\\2\\2\\2\\u00b5\\u00b3\\3\\2\\2\\2"")\n        buf.write(""\\u00b5\\u00b6\\3\\2\\2\\2\\u00b6<\\3\\2\\2\\2\\u00b7\\u00b5\\3\\2\\2"")\n        buf.write(""\\2\\u00b8\\u00b9\\t\\2\\2\\2\\u00b9>\\3\\2\\2\\2\\u00ba\\u00bc\\5A!"")\n        buf.write(""\\2\\u00bb\\u00ba\\3\\2\\2\\2\\u00bc\\u00bd\\3\\2\\2\\2\\u00bd\\u00bb"")\n        buf.write(""\\3\\2\\2\\2\\u00bd\\u00be\\3\\2\\2\\2\\u00be@\\3\\2\\2\\2\\u00bf\\u00c0"")\n        buf.write(""\\t\\3\\2\\2\\u00c0B\\3\\2\\2\\2\\u00c1\\u00c3\\5I%\\2\\u00c2\\u00c1"")\n        buf.write(""\\3\\2\\2\\2\\u00c2\\u00c3\\3\\2\\2\\2\\u00c3\\u00c4\\3\\2\\2\\2\\u00c4"")\n        buf.write(""\\u00c6\\5E#\\2\\u00c5\\u00c7\\5G$\\2\\u00c6\\u00c5\\3\\2\\2\\2\\u00c6"")\n        buf.write(""\\u00c7\\3\\2\\2\\2\\u00c7\\u00d3\\3\\2\\2\\2\\u00c8\\u00ca\\5I%\\2\\u00c9"")\n        buf.write(""\\u00c8\\3\\2\\2\\2\\u00c9\\u00ca\\3\\2\\2\\2\\u00ca\\u00cc\\3\\2\\2\\2"")\n        buf.write(""\\u00cb\\u00cd\\5A!\\2\\u00cc\\u00cb\\3\\2\\2\\2\\u00cd\\u00ce\\3\\2"")\n        buf.write(""\\2\\2\\u00ce\\u00cc\\3\\2\\2\\2\\u00ce\\u00cf\\3\\2\\2\\2\\u00cf\\u00d0"")\n        buf.write(""\\3\\2\\2\\2\\u00d0\\u00d1\\5G$\\2\\u00d1\\u00d3\\3\\2\\2\\2\\u00d2\\u00c2"")\n        buf.write(""\\3\\2\\2\\2\\u00d2\\u00c9\\3\\2\\2\\2\\u00d3D\\3\\2\\2\\2\\u00d4\\u00d6"")\n        buf.write(""\\5A!\\2\\u00d5\\u00d4\\3\\2\\2\\2\\u00d6\\u00d7\\3\\2\\2\\2\\u00d7\\u00d5"")\n        buf.write(""\\3\\2\\2\\2\\u00d7\\u00d8\\3\\2\\2\\2\\u00d8\\u00da\\3\\2\\2\\2\\u00d9"")\n        buf.write(""\\u00d5\\3\\2\\2\\2\\u00d9\\u00da\\3\\2\\2\\2\\u00da\\u00db\\3\\2\\2\\2"")\n        buf.write(""\\u00db\\u00dd\\7\\60\\2\\2\\u00dc\\u00de\\5A!\\2\\u00dd\\u00dc\\3"")\n        buf.write(""\\2\\2\\2\\u00de\\u00df\\3\\2\\2\\2\\u00df\\u00dd\\3\\2\\2\\2\\u00df\\u00e0"")\n        buf.write(""\\3\\2\\2\\2\\u00e0\\u00e9\\3\\2\\2\\2\\u00e1\\u00e3\\5A!\\2\\u00e2\\u00e1"")\n        buf.write(""\\3\\2\\2\\2\\u00e3\\u00e4\\3\\2\\2\\2\\u00e4\\u00e2\\3\\2\\2\\2\\u00e4"")\n        buf.write(""\\u00e5\\3\\2\\2\\2\\u00e5\\u00e6\\3\\2\\2\\2\\u00e6\\u00e7\\7\\60\\2"")\n        buf.write(""\\2\\u00e7\\u00e9\\3\\2\\2\\2\\u00e8\\u00d9\\3\\2\\2\\2\\u00e8\\u00e2"")\n        buf.write(""\\3\\2\\2\\2\\u00e9F\\3\\2\\2\\2\\u00ea\\u00ec\\t\\4\\2\\2\\u00eb\\u00ed"")\n        buf.write(""\\5I%\\2\\u00ec\\u00eb\\3\\2\\2\\2\\u00ec\\u00ed\\3\\2\\2\\2\\u00ed\\u00ef"")\n        buf.write(""\\3\\2\\2\\2\\u00ee\\u00f0\\5A!\\2\\u00ef\\u00ee\\3\\2\\2\\2\\u00f0\\u00f1"")\n        buf.write(""\\3\\2\\2\\2\\u00f1\\u00ef\\3\\2\\2\\2\\u00f1\\u00f2\\3\\2\\2\\2\\u00f2"")\n        buf.write(""H\\3\\2\\2\\2\\u00f3\\u00f4\\t\\5\\2\\2\\u00f4J\\3\\2\\2\\2\\u00f5\\u00f7"")\n        buf.write(""\\t\\6\\2\\2\\u00f6\\u00f5\\3\\2\\2\\2\\u00f7\\u00f8\\3\\2\\2\\2\\u00f8"")\n        buf.write(""\\u00f6\\3\\2\\2\\2\\u00f8\\u00f9\\3\\2\\2\\2\\u00f9\\u00fa\\3\\2\\2\\2"")\n        buf.write(""\\u00fa\\u00fb\\b&\\2\\2\\u00fbL\\3\\2\\2\\2\\u00fc\\u00fd\\7\\61\\2"")\n        buf.write(""\\2\\u00fd\\u00fe\\7\\61\\2\\2\\u00fe\\u0102\\3\\2\\2\\2\\u00ff\\u0101"")\n        buf.write(""\\n\\7\\2\\2\\u0100\\u00ff\\3\\2\\2\\2\\u0101\\u0104\\3\\2\\2\\2\\u0102"")\n        buf.write(""\\u0100\\3\\2\\2\\2\\u0102\\u0103\\3\\2\\2\\2\\u0103\\u0105\\3\\2\\2\\2"")\n        buf.write(""\\u0104\\u0102\\3\\2\\2\\2\\u0105\\u0106\\b\\\'\\3\\2\\u0106N\\3\\2\\2"")\n        buf.write(""\\2\\24\\2\\u00b3\\u00b5\\u00bd\\u00c2\\u00c6\\u00c9\\u00ce\\u00d2"")\n        buf.write(""\\u00d7\\u00d9\\u00df\\u00e4\\u00e8\\u00ec\\u00f1\\u00f8\\u0102"")\n        buf.write(""\\4\\b\\2\\2\\2\\3\\2"")\n        return buf.getvalue()\n\n\nclass seedotLexer(Lexer):\n\n    atn = ATNDeserializer().deserialize(serializedATN())\n\n    decisionsToDFA = [ DFA(ds, i) for i, ds in enumerate(atn.decisionToState) ]\n\n    T__0 = 1\n    T__1 = 2\n    T__2 = 3\n    T__3 = 4\n    T__4 = 5\n    T__5 = 6\n    T__6 = 7\n    T__7 = 8\n    T__8 = 9\n    T__9 = 10\n    ADD = 11\n    SUB = 12\n    MUL = 13\n    SPARSEMUL = 14\n    MULCIR = 15\n    CONV = 16\n    ADDCIR = 17\n    SUBCIR = 18\n    RELU = 19\n    EXP = 20\n    ARGMAX = 21\n    SGN = 22\n    TANH = 23\n    Reshape = 24\n    Maxpool = 25\n    Sum = 26\n    Let = 27\n    In = 28\n    Id = 29\n    IntConst = 30\n    FloatConst = 31\n    WS = 32\n    LineComment = 33\n\n    channelNames = [ u""DEFAULT_TOKEN_CHANNEL"", u""HIDDEN"" ]\n\n    modeNames = [ ""DEFAULT_MODE"" ]\n\n    literalNames = [ ""<INVALID>"",\n            ""\'(\'"", ""\')\'"", ""\'[\'"", ""\',\'"", ""\']\'"", ""\'^T\'"", ""\'=\'"", ""\':\'"", ""\'>=\'"", \n            ""\'?\'"", ""\'+\'"", ""\'-\'"", ""\'*\'"", ""\'|*|\'"", ""\'<*>\'"", ""\'#\'"", ""\'<+>\'"", \n            ""\'<->\'"", ""\'relu\'"", ""\'exp\'"", ""\'argmax\'"", ""\'sgn\'"", ""\'tanh\'"", ""\'reshape\'"", \n            ""\'maxpool\'"", ""\'$\'"", ""\'let\'"", ""\'in\'"" ]\n\n    symbolicNames = [ ""<INVALID>"",\n            ""ADD"", ""SUB"", ""MUL"", ""SPARSEMUL"", ""MULCIR"", ""CONV"", ""ADDCIR"", \n            ""SUBCIR"", ""RELU"", ""EXP"", ""ARGMAX"", ""SGN"", ""TANH"", ""Reshape"", \n            ""Maxpool"", ""Sum"", ""Let"", ""In"", ""Id"", ""IntConst"", ""FloatConst"", \n            ""WS"", ""LineComment"" ]\n\n    ruleNames = [ ""T__0"", ""T__1"", ""T__2"", ""T__3"", ""T__4"", ""T__5"", ""T__6"", \n                  ""T__7"", ""T__8"", ""T__9"", ""ADD"", ""SUB"", ""MUL"", ""SPARSEMUL"", \n                  ""MULCIR"", ""CONV"", ""ADDCIR"", ""SUBCIR"", ""RELU"", ""EXP"", ""ARGMAX"", \n                  ""SGN"", ""TANH"", ""Reshape"", ""Maxpool"", ""Sum"", ""Let"", ""In"", \n                  ""Id"", ""Nondigit"", ""IntConst"", ""Digit"", ""FloatConst"", ""FracConst"", \n                  ""ExpntPart"", ""Sign"", ""WS"", ""LineComment"" ]\n\n    grammarFileName = ""seedot.g4""\n\n    def __init__(self, input=None, output:TextIO = sys.stdout):\n        super().__init__(input, output)\n        self.checkVersion(""4.7"")\n        self._interp = LexerATNSimulator(self, self.atn, self.decisionsToDFA, PredictionContextCache())\n        self._actions = None\n        self._predicates = None\n\n\n'"
tools/SeeDot/seedot/compiler/antlr/seedotParser.py,0,"b'# Generated from seedot.g4 by ANTLR 4.7\n# encoding: utf-8\nfrom antlr4 import *\nfrom io import StringIO\nfrom typing.io import TextIO\nimport sys\n\ndef serializedATN():\n    with StringIO() as buf:\n        buf.write(""\\3\\u608b\\ua72a\\u8133\\ub9ed\\u417c\\u3be7\\u7786\\u5964\\3#"")\n        buf.write(""\\u0085\\4\\2\\t\\2\\4\\3\\t\\3\\4\\4\\t\\4\\4\\5\\t\\5\\4\\6\\t\\6\\3\\2\\3\\2"")\n        buf.write(""\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3"")\n        buf.write(""\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2"")\n        buf.write(""\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\7\\2\\64\\n\\2\\f"")\n        buf.write(""\\2\\16\\2\\67\\13\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3"")\n        buf.write(""\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2"")\n        buf.write(""\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\5\\2Y\\n\\2\\3\\2\\3\\2"")\n        buf.write(""\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3"")\n        buf.write(""\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\3\\2\\7\\2r\\n\\2\\f\\2\\16\\2u\\13\\2"")\n        buf.write(""\\3\\3\\3\\3\\3\\4\\3\\4\\3\\5\\3\\5\\3\\6\\3\\6\\3\\6\\7\\6\\u0080\\n\\6\\f\\6"")\n        buf.write(""\\16\\6\\u0083\\13\\6\\3\\6\\2\\3\\2\\7\\2\\4\\6\\b\\n\\2\\5\\3\\2\\r\\16\\3"")\n        buf.write(""\\2\\17\\24\\3\\2\\25\\31\\2\\u0091\\2X\\3\\2\\2\\2\\4v\\3\\2\\2\\2\\6x\\3"")\n        buf.write(""\\2\\2\\2\\bz\\3\\2\\2\\2\\n|\\3\\2\\2\\2\\f\\r\\b\\2\\1\\2\\rY\\7 \\2\\2\\16"")\n        buf.write(""Y\\7!\\2\\2\\17Y\\7\\37\\2\\2\\20\\21\\7\\3\\2\\2\\21\\22\\5\\n\\6\\2\\22\\23"")\n        buf.write(""\\7\\4\\2\\2\\23\\24\\7\\36\\2\\2\\24\\25\\7\\5\\2\\2\\25\\26\\7!\\2\\2\\26"")\n        buf.write(""\\27\\7\\6\\2\\2\\27\\30\\7!\\2\\2\\30\\31\\7\\7\\2\\2\\31Y\\3\\2\\2\\2\\32"")\n        buf.write(""\\33\\7\\32\\2\\2\\33\\34\\7\\3\\2\\2\\34\\35\\5\\2\\2\\2\\35\\36\\7\\6\\2\\2"")\n        buf.write(""\\36\\37\\7\\3\\2\\2\\37 \\5\\n\\6\\2 !\\7\\4\\2\\2!\\""\\7\\6\\2\\2\\""#\\7\\3"")\n        buf.write(""\\2\\2#$\\5\\n\\6\\2$%\\7\\4\\2\\2%&\\7\\4\\2\\2&Y\\3\\2\\2\\2\\\'(\\7\\33\\2"")\n        buf.write(""\\2()\\7\\3\\2\\2)*\\5\\2\\2\\2*+\\7\\6\\2\\2+,\\7 \\2\\2,-\\7\\4\\2\\2-Y"")\n        buf.write(""\\3\\2\\2\\2./\\7\\37\\2\\2/\\60\\7\\3\\2\\2\\60\\65\\5\\2\\2\\2\\61\\62\\7"")\n        buf.write(""\\6\\2\\2\\62\\64\\5\\2\\2\\2\\63\\61\\3\\2\\2\\2\\64\\67\\3\\2\\2\\2\\65\\63"")\n        buf.write(""\\3\\2\\2\\2\\65\\66\\3\\2\\2\\2\\668\\3\\2\\2\\2\\67\\65\\3\\2\\2\\289\\7\\4"")\n        buf.write(""\\2\\29Y\\3\\2\\2\\2:;\\5\\4\\3\\2;<\\5\\2\\2\\n<Y\\3\\2\\2\\2=>\\5\\b\\5\\2"")\n        buf.write("">?\\7\\3\\2\\2?@\\5\\2\\2\\2@A\\7\\4\\2\\2AY\\3\\2\\2\\2BC\\7\\34\\2\\2CD"")\n        buf.write(""\\7\\3\\2\\2DE\\7\\37\\2\\2EF\\7\\t\\2\\2FG\\7\\5\\2\\2GH\\7 \\2\\2HI\\7\\n"")\n        buf.write(""\\2\\2IJ\\7 \\2\\2JK\\7\\7\\2\\2KL\\7\\4\\2\\2LY\\5\\2\\2\\6MN\\7\\35\\2\\2"")\n        buf.write(""NO\\7\\37\\2\\2OP\\7\\t\\2\\2PQ\\5\\2\\2\\2QR\\7\\36\\2\\2RS\\5\\2\\2\\4S"")\n        buf.write(""Y\\3\\2\\2\\2TU\\7\\3\\2\\2UV\\5\\2\\2\\2VW\\7\\4\\2\\2WY\\3\\2\\2\\2X\\f\\3"")\n        buf.write(""\\2\\2\\2X\\16\\3\\2\\2\\2X\\17\\3\\2\\2\\2X\\20\\3\\2\\2\\2X\\32\\3\\2\\2\\2"")\n        buf.write(""X\\\'\\3\\2\\2\\2X.\\3\\2\\2\\2X:\\3\\2\\2\\2X=\\3\\2\\2\\2XB\\3\\2\\2\\2XM"")\n        buf.write(""\\3\\2\\2\\2XT\\3\\2\\2\\2Ys\\3\\2\\2\\2Z[\\f\\t\\2\\2[\\\\\\5\\6\\4\\2\\\\]\\5"")\n        buf.write(""\\2\\2\\n]r\\3\\2\\2\\2^_\\f\\b\\2\\2_`\\5\\4\\3\\2`a\\5\\2\\2\\tar\\3\\2\\2"")\n        buf.write(""\\2bc\\f\\5\\2\\2cd\\7\\13\\2\\2de\\7 \\2\\2ef\\7\\f\\2\\2fg\\5\\2\\2\\2g"")\n        buf.write(""h\\7\\n\\2\\2hi\\5\\2\\2\\6ir\\3\\2\\2\\2jk\\f\\17\\2\\2kr\\7\\b\\2\\2lm\\f"")\n        buf.write(""\\f\\2\\2mn\\7\\5\\2\\2no\\5\\2\\2\\2op\\7\\7\\2\\2pr\\3\\2\\2\\2qZ\\3\\2\\2"")\n        buf.write(""\\2q^\\3\\2\\2\\2qb\\3\\2\\2\\2qj\\3\\2\\2\\2ql\\3\\2\\2\\2ru\\3\\2\\2\\2s"")\n        buf.write(""q\\3\\2\\2\\2st\\3\\2\\2\\2t\\3\\3\\2\\2\\2us\\3\\2\\2\\2vw\\t\\2\\2\\2w\\5"")\n        buf.write(""\\3\\2\\2\\2xy\\t\\3\\2\\2y\\7\\3\\2\\2\\2z{\\t\\4\\2\\2{\\t\\3\\2\\2\\2|\\u0081"")\n        buf.write(""\\7 \\2\\2}~\\7\\6\\2\\2~\\u0080\\7 \\2\\2\\177}\\3\\2\\2\\2\\u0080\\u0083"")\n        buf.write(""\\3\\2\\2\\2\\u0081\\177\\3\\2\\2\\2\\u0081\\u0082\\3\\2\\2\\2\\u0082\\13"")\n        buf.write(""\\3\\2\\2\\2\\u0083\\u0081\\3\\2\\2\\2\\7\\65Xqs\\u0081"")\n        return buf.getvalue()\n\n\nclass seedotParser ( Parser ):\n\n    grammarFileName = ""seedot.g4""\n\n    atn = ATNDeserializer().deserialize(serializedATN())\n\n    decisionsToDFA = [ DFA(ds, i) for i, ds in enumerate(atn.decisionToState) ]\n\n    sharedContextCache = PredictionContextCache()\n\n    literalNames = [ ""<INVALID>"", ""\'(\'"", ""\')\'"", ""\'[\'"", ""\',\'"", ""\']\'"", ""\'^T\'"", \n                     ""\'=\'"", ""\':\'"", ""\'>=\'"", ""\'?\'"", ""\'+\'"", ""\'-\'"", ""\'*\'"", ""\'|*|\'"", \n                     ""\'<*>\'"", ""\'#\'"", ""\'<+>\'"", ""\'<->\'"", ""\'relu\'"", ""\'exp\'"", \n                     ""\'argmax\'"", ""\'sgn\'"", ""\'tanh\'"", ""\'reshape\'"", ""\'maxpool\'"", \n                     ""\'$\'"", ""\'let\'"", ""\'in\'"" ]\n\n    symbolicNames = [ ""<INVALID>"", ""<INVALID>"", ""<INVALID>"", ""<INVALID>"", \n                      ""<INVALID>"", ""<INVALID>"", ""<INVALID>"", ""<INVALID>"", \n                      ""<INVALID>"", ""<INVALID>"", ""<INVALID>"", ""ADD"", ""SUB"", \n                      ""MUL"", ""SPARSEMUL"", ""MULCIR"", ""CONV"", ""ADDCIR"", ""SUBCIR"", \n                      ""RELU"", ""EXP"", ""ARGMAX"", ""SGN"", ""TANH"", ""Reshape"", \n                      ""Maxpool"", ""Sum"", ""Let"", ""In"", ""Id"", ""IntConst"", ""FloatConst"", \n                      ""WS"", ""LineComment"" ]\n\n    RULE_expr = 0\n    RULE_addOp = 1\n    RULE_binOp = 2\n    RULE_specialFunc = 3\n    RULE_intConstList = 4\n\n    ruleNames =  [ ""expr"", ""addOp"", ""binOp"", ""specialFunc"", ""intConstList"" ]\n\n    EOF = Token.EOF\n    T__0=1\n    T__1=2\n    T__2=3\n    T__3=4\n    T__4=5\n    T__5=6\n    T__6=7\n    T__7=8\n    T__8=9\n    T__9=10\n    ADD=11\n    SUB=12\n    MUL=13\n    SPARSEMUL=14\n    MULCIR=15\n    CONV=16\n    ADDCIR=17\n    SUBCIR=18\n    RELU=19\n    EXP=20\n    ARGMAX=21\n    SGN=22\n    TANH=23\n    Reshape=24\n    Maxpool=25\n    Sum=26\n    Let=27\n    In=28\n    Id=29\n    IntConst=30\n    FloatConst=31\n    WS=32\n    LineComment=33\n\n    def __init__(self, input:TokenStream, output:TextIO = sys.stdout):\n        super().__init__(input, output)\n        self.checkVersion(""4.7"")\n        self._interp = ParserATNSimulator(self, self.atn, self.decisionsToDFA, self.sharedContextCache)\n        self._predicates = None\n\n\n\n    class ExprContext(ParserRuleContext):\n\n        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):\n            super().__init__(parent, invokingState)\n            self.parser = parser\n\n\n        def getRuleIndex(self):\n            return seedotParser.RULE_expr\n\n     \n        def copyFrom(self, ctx:ParserRuleContext):\n            super().copyFrom(ctx)\n\n\n    class Bop1Context(ExprContext):\n\n        def __init__(self, parser, ctx:ParserRuleContext): # actually a seedotParser.ExprContext\n            super().__init__(parser)\n            self.copyFrom(ctx)\n\n        def expr(self, i:int=None):\n            if i is None:\n                return self.getTypedRuleContexts(seedotParser.ExprContext)\n            else:\n                return self.getTypedRuleContext(seedotParser.ExprContext,i)\n\n        def binOp(self):\n            return self.getTypedRuleContext(seedotParser.BinOpContext,0)\n\n\n        def accept(self, visitor:ParseTreeVisitor):\n            if hasattr( visitor, ""visitBop1"" ):\n                return visitor.visitBop1(self)\n            else:\n                return visitor.visitChildren(self)\n\n\n    class Bop2Context(ExprContext):\n\n        def __init__(self, parser, ctx:ParserRuleContext): # actually a seedotParser.ExprContext\n            super().__init__(parser)\n            self.copyFrom(ctx)\n\n        def expr(self, i:int=None):\n            if i is None:\n                return self.getTypedRuleContexts(seedotParser.ExprContext)\n            else:\n                return self.getTypedRuleContext(seedotParser.ExprContext,i)\n\n        def addOp(self):\n            return self.getTypedRuleContext(seedotParser.AddOpContext,0)\n\n\n        def accept(self, visitor:ParseTreeVisitor):\n            if hasattr( visitor, ""visitBop2"" ):\n                return visitor.visitBop2(self)\n            else:\n                return visitor.visitChildren(self)\n\n\n    class DeclContext(ExprContext):\n\n        def __init__(self, parser, ctx:ParserRuleContext): # actually a seedotParser.ExprContext\n            super().__init__(parser)\n            self.copyFrom(ctx)\n\n        def intConstList(self):\n            return self.getTypedRuleContext(seedotParser.IntConstListContext,0)\n\n        def In(self):\n            return self.getToken(seedotParser.In, 0)\n        def FloatConst(self, i:int=None):\n            if i is None:\n                return self.getTokens(seedotParser.FloatConst)\n            else:\n                return self.getToken(seedotParser.FloatConst, i)\n\n        def accept(self, visitor:ParseTreeVisitor):\n            if hasattr( visitor, ""visitDecl"" ):\n                return visitor.visitDecl(self)\n            else:\n                return visitor.visitChildren(self)\n\n\n    class IndexContext(ExprContext):\n\n        def __init__(self, parser, ctx:ParserRuleContext): # actually a seedotParser.ExprContext\n            super().__init__(parser)\n            self.copyFrom(ctx)\n\n        def expr(self, i:int=None):\n            if i is None:\n                return self.getTypedRuleContexts(seedotParser.ExprContext)\n            else:\n                return self.getTypedRuleContext(seedotParser.ExprContext,i)\n\n\n        def accept(self, visitor:ParseTreeVisitor):\n            if hasattr( visitor, ""visitIndex"" ):\n                return visitor.visitIndex(self)\n            else:\n                return visitor.visitChildren(self)\n\n\n    class SumContext(ExprContext):\n\n        def __init__(self, parser, ctx:ParserRuleContext): # actually a seedotParser.ExprContext\n            super().__init__(parser)\n            self.copyFrom(ctx)\n\n        def Sum(self):\n            return self.getToken(seedotParser.Sum, 0)\n        def Id(self):\n            return self.getToken(seedotParser.Id, 0)\n        def IntConst(self, i:int=None):\n            if i is None:\n                return self.getTokens(seedotParser.IntConst)\n            else:\n                return self.getToken(seedotParser.IntConst, i)\n        def expr(self):\n            return self.getTypedRuleContext(seedotParser.ExprContext,0)\n\n\n        def accept(self, visitor:ParseTreeVisitor):\n            if hasattr( visitor, ""visitSum"" ):\n                return visitor.visitSum(self)\n            else:\n                return visitor.visitChildren(self)\n\n\n    class ReshapeContext(ExprContext):\n\n        def __init__(self, parser, ctx:ParserRuleContext): # actually a seedotParser.ExprContext\n            super().__init__(parser)\n            self.copyFrom(ctx)\n\n        def Reshape(self):\n            return self.getToken(seedotParser.Reshape, 0)\n        def expr(self):\n            return self.getTypedRuleContext(seedotParser.ExprContext,0)\n\n        def intConstList(self, i:int=None):\n            if i is None:\n                return self.getTypedRuleContexts(seedotParser.IntConstListContext)\n            else:\n                return self.getTypedRuleContext(seedotParser.IntConstListContext,i)\n\n\n        def accept(self, visitor:ParseTreeVisitor):\n            if hasattr( visitor, ""visitReshape"" ):\n                return visitor.visitReshape(self)\n            else:\n                return visitor.visitChildren(self)\n\n\n    class FloatContext(ExprContext):\n\n        def __init__(self, parser, ctx:ParserRuleContext): # actually a seedotParser.ExprContext\n            super().__init__(parser)\n            self.copyFrom(ctx)\n\n        def FloatConst(self):\n            return self.getToken(seedotParser.FloatConst, 0)\n\n        def accept(self, visitor:ParseTreeVisitor):\n            if hasattr( visitor, ""visitFloat"" ):\n                return visitor.visitFloat(self)\n            else:\n                return visitor.visitChildren(self)\n\n\n    class CondContext(ExprContext):\n\n        def __init__(self, parser, ctx:ParserRuleContext): # actually a seedotParser.ExprContext\n            super().__init__(parser)\n            self.copyFrom(ctx)\n\n        def expr(self, i:int=None):\n            if i is None:\n                return self.getTypedRuleContexts(seedotParser.ExprContext)\n            else:\n                return self.getTypedRuleContext(seedotParser.ExprContext,i)\n\n        def IntConst(self):\n            return self.getToken(seedotParser.IntConst, 0)\n\n        def accept(self, visitor:ParseTreeVisitor):\n            if hasattr( visitor, ""visitCond"" ):\n                return visitor.visitCond(self)\n            else:\n                return visitor.visitChildren(self)\n\n\n    class IntContext(ExprContext):\n\n        def __init__(self, parser, ctx:ParserRuleContext): # actually a seedotParser.ExprContext\n            super().__init__(parser)\n            self.copyFrom(ctx)\n\n        def IntConst(self):\n            return self.getToken(seedotParser.IntConst, 0)\n\n        def accept(self, visitor:ParseTreeVisitor):\n            if hasattr( visitor, ""visitInt"" ):\n                return visitor.visitInt(self)\n            else:\n                return visitor.visitChildren(self)\n\n\n    class TranspContext(ExprContext):\n\n        def __init__(self, parser, ctx:ParserRuleContext): # actually a seedotParser.ExprContext\n            super().__init__(parser)\n            self.copyFrom(ctx)\n\n        def expr(self):\n            return self.getTypedRuleContext(seedotParser.ExprContext,0)\n\n\n        def accept(self, visitor:ParseTreeVisitor):\n            if hasattr( visitor, ""visitTransp"" ):\n                return visitor.visitTransp(self)\n            else:\n                return visitor.visitChildren(self)\n\n\n    class ParenContext(ExprContext):\n\n        def __init__(self, parser, ctx:ParserRuleContext): # actually a seedotParser.ExprContext\n            super().__init__(parser)\n            self.copyFrom(ctx)\n\n        def expr(self):\n            return self.getTypedRuleContext(seedotParser.ExprContext,0)\n\n\n        def accept(self, visitor:ParseTreeVisitor):\n            if hasattr( visitor, ""visitParen"" ):\n                return visitor.visitParen(self)\n            else:\n                return visitor.visitChildren(self)\n\n\n    class FuncContext(ExprContext):\n\n        def __init__(self, parser, ctx:ParserRuleContext): # actually a seedotParser.ExprContext\n            super().__init__(parser)\n            self.copyFrom(ctx)\n\n        def specialFunc(self):\n            return self.getTypedRuleContext(seedotParser.SpecialFuncContext,0)\n\n        def expr(self):\n            return self.getTypedRuleContext(seedotParser.ExprContext,0)\n\n\n        def accept(self, visitor:ParseTreeVisitor):\n            if hasattr( visitor, ""visitFunc"" ):\n                return visitor.visitFunc(self)\n            else:\n                return visitor.visitChildren(self)\n\n\n    class UopContext(ExprContext):\n\n        def __init__(self, parser, ctx:ParserRuleContext): # actually a seedotParser.ExprContext\n            super().__init__(parser)\n            self.copyFrom(ctx)\n\n        def addOp(self):\n            return self.getTypedRuleContext(seedotParser.AddOpContext,0)\n\n        def expr(self):\n            return self.getTypedRuleContext(seedotParser.ExprContext,0)\n\n\n        def accept(self, visitor:ParseTreeVisitor):\n            if hasattr( visitor, ""visitUop"" ):\n                return visitor.visitUop(self)\n            else:\n                return visitor.visitChildren(self)\n\n\n    class LetContext(ExprContext):\n\n        def __init__(self, parser, ctx:ParserRuleContext): # actually a seedotParser.ExprContext\n            super().__init__(parser)\n            self.copyFrom(ctx)\n\n        def Let(self):\n            return self.getToken(seedotParser.Let, 0)\n        def Id(self):\n            return self.getToken(seedotParser.Id, 0)\n        def expr(self, i:int=None):\n            if i is None:\n                return self.getTypedRuleContexts(seedotParser.ExprContext)\n            else:\n                return self.getTypedRuleContext(seedotParser.ExprContext,i)\n\n        def In(self):\n            return self.getToken(seedotParser.In, 0)\n\n        def accept(self, visitor:ParseTreeVisitor):\n            if hasattr( visitor, ""visitLet"" ):\n                return visitor.visitLet(self)\n            else:\n                return visitor.visitChildren(self)\n\n\n    class IdContext(ExprContext):\n\n        def __init__(self, parser, ctx:ParserRuleContext): # actually a seedotParser.ExprContext\n            super().__init__(parser)\n            self.copyFrom(ctx)\n\n        def Id(self):\n            return self.getToken(seedotParser.Id, 0)\n\n        def accept(self, visitor:ParseTreeVisitor):\n            if hasattr( visitor, ""visitId"" ):\n                return visitor.visitId(self)\n            else:\n                return visitor.visitChildren(self)\n\n\n    class FuncCallContext(ExprContext):\n\n        def __init__(self, parser, ctx:ParserRuleContext): # actually a seedotParser.ExprContext\n            super().__init__(parser)\n            self.copyFrom(ctx)\n\n        def Id(self):\n            return self.getToken(seedotParser.Id, 0)\n        def expr(self, i:int=None):\n            if i is None:\n                return self.getTypedRuleContexts(seedotParser.ExprContext)\n            else:\n                return self.getTypedRuleContext(seedotParser.ExprContext,i)\n\n\n        def accept(self, visitor:ParseTreeVisitor):\n            if hasattr( visitor, ""visitFuncCall"" ):\n                return visitor.visitFuncCall(self)\n            else:\n                return visitor.visitChildren(self)\n\n\n    class MaxpoolContext(ExprContext):\n\n        def __init__(self, parser, ctx:ParserRuleContext): # actually a seedotParser.ExprContext\n            super().__init__(parser)\n            self.copyFrom(ctx)\n\n        def Maxpool(self):\n            return self.getToken(seedotParser.Maxpool, 0)\n        def expr(self):\n            return self.getTypedRuleContext(seedotParser.ExprContext,0)\n\n        def IntConst(self):\n            return self.getToken(seedotParser.IntConst, 0)\n\n        def accept(self, visitor:ParseTreeVisitor):\n            if hasattr( visitor, ""visitMaxpool"" ):\n                return visitor.visitMaxpool(self)\n            else:\n                return visitor.visitChildren(self)\n\n\n\n    def expr(self, _p:int=0):\n        _parentctx = self._ctx\n        _parentState = self.state\n        localctx = seedotParser.ExprContext(self, self._ctx, _parentState)\n        _prevctx = localctx\n        _startState = 0\n        self.enterRecursionRule(localctx, 0, self.RULE_expr, _p)\n        self._la = 0 # Token type\n        try:\n            self.enterOuterAlt(localctx, 1)\n            self.state = 86\n            self._errHandler.sync(self)\n            la_ = self._interp.adaptivePredict(self._input,1,self._ctx)\n            if la_ == 1:\n                localctx = seedotParser.IntContext(self, localctx)\n                self._ctx = localctx\n                _prevctx = localctx\n\n                self.state = 11\n                self.match(seedotParser.IntConst)\n                pass\n\n            elif la_ == 2:\n                localctx = seedotParser.FloatContext(self, localctx)\n                self._ctx = localctx\n                _prevctx = localctx\n                self.state = 12\n                self.match(seedotParser.FloatConst)\n                pass\n\n            elif la_ == 3:\n                localctx = seedotParser.IdContext(self, localctx)\n                self._ctx = localctx\n                _prevctx = localctx\n                self.state = 13\n                self.match(seedotParser.Id)\n                pass\n\n            elif la_ == 4:\n                localctx = seedotParser.DeclContext(self, localctx)\n                self._ctx = localctx\n                _prevctx = localctx\n                self.state = 14\n                self.match(seedotParser.T__0)\n                self.state = 15\n                self.intConstList()\n                self.state = 16\n                self.match(seedotParser.T__1)\n                self.state = 17\n                self.match(seedotParser.In)\n                self.state = 18\n                self.match(seedotParser.T__2)\n                self.state = 19\n                self.match(seedotParser.FloatConst)\n                self.state = 20\n                self.match(seedotParser.T__3)\n                self.state = 21\n                self.match(seedotParser.FloatConst)\n                self.state = 22\n                self.match(seedotParser.T__4)\n                pass\n\n            elif la_ == 5:\n                localctx = seedotParser.ReshapeContext(self, localctx)\n                self._ctx = localctx\n                _prevctx = localctx\n                self.state = 24\n                self.match(seedotParser.Reshape)\n                self.state = 25\n                self.match(seedotParser.T__0)\n                self.state = 26\n                self.expr(0)\n                self.state = 27\n                self.match(seedotParser.T__3)\n                self.state = 28\n                self.match(seedotParser.T__0)\n                self.state = 29\n                self.intConstList()\n                self.state = 30\n                self.match(seedotParser.T__1)\n                self.state = 31\n                self.match(seedotParser.T__3)\n                self.state = 32\n                self.match(seedotParser.T__0)\n                self.state = 33\n                self.intConstList()\n                self.state = 34\n                self.match(seedotParser.T__1)\n                self.state = 35\n                self.match(seedotParser.T__1)\n                pass\n\n            elif la_ == 6:\n                localctx = seedotParser.MaxpoolContext(self, localctx)\n                self._ctx = localctx\n                _prevctx = localctx\n                self.state = 37\n                self.match(seedotParser.Maxpool)\n                self.state = 38\n                self.match(seedotParser.T__0)\n                self.state = 39\n                self.expr(0)\n                self.state = 40\n                self.match(seedotParser.T__3)\n                self.state = 41\n                self.match(seedotParser.IntConst)\n                self.state = 42\n                self.match(seedotParser.T__1)\n                pass\n\n            elif la_ == 7:\n                localctx = seedotParser.FuncCallContext(self, localctx)\n                self._ctx = localctx\n                _prevctx = localctx\n                self.state = 44\n                self.match(seedotParser.Id)\n                self.state = 45\n                self.match(seedotParser.T__0)\n                self.state = 46\n                self.expr(0)\n                self.state = 51\n                self._errHandler.sync(self)\n                _la = self._input.LA(1)\n                while _la==seedotParser.T__3:\n                    self.state = 47\n                    self.match(seedotParser.T__3)\n                    self.state = 48\n                    self.expr(0)\n                    self.state = 53\n                    self._errHandler.sync(self)\n                    _la = self._input.LA(1)\n\n                self.state = 54\n                self.match(seedotParser.T__1)\n                pass\n\n            elif la_ == 8:\n                localctx = seedotParser.UopContext(self, localctx)\n                self._ctx = localctx\n                _prevctx = localctx\n                self.state = 56\n                self.addOp()\n                self.state = 57\n                self.expr(8)\n                pass\n\n            elif la_ == 9:\n                localctx = seedotParser.FuncContext(self, localctx)\n                self._ctx = localctx\n                _prevctx = localctx\n                self.state = 59\n                self.specialFunc()\n                self.state = 60\n                self.match(seedotParser.T__0)\n                self.state = 61\n                self.expr(0)\n                self.state = 62\n                self.match(seedotParser.T__1)\n                pass\n\n            elif la_ == 10:\n                localctx = seedotParser.SumContext(self, localctx)\n                self._ctx = localctx\n                _prevctx = localctx\n                self.state = 64\n                self.match(seedotParser.Sum)\n                self.state = 65\n                self.match(seedotParser.T__0)\n                self.state = 66\n                self.match(seedotParser.Id)\n                self.state = 67\n                self.match(seedotParser.T__6)\n                self.state = 68\n                self.match(seedotParser.T__2)\n                self.state = 69\n                self.match(seedotParser.IntConst)\n                self.state = 70\n                self.match(seedotParser.T__7)\n                self.state = 71\n                self.match(seedotParser.IntConst)\n                self.state = 72\n                self.match(seedotParser.T__4)\n                self.state = 73\n                self.match(seedotParser.T__1)\n                self.state = 74\n                self.expr(4)\n                pass\n\n            elif la_ == 11:\n                localctx = seedotParser.LetContext(self, localctx)\n                self._ctx = localctx\n                _prevctx = localctx\n                self.state = 75\n                self.match(seedotParser.Let)\n                self.state = 76\n                self.match(seedotParser.Id)\n                self.state = 77\n                self.match(seedotParser.T__6)\n                self.state = 78\n                self.expr(0)\n                self.state = 79\n                self.match(seedotParser.In)\n                self.state = 80\n                self.expr(2)\n                pass\n\n            elif la_ == 12:\n                localctx = seedotParser.ParenContext(self, localctx)\n                self._ctx = localctx\n                _prevctx = localctx\n                self.state = 82\n                self.match(seedotParser.T__0)\n                self.state = 83\n                self.expr(0)\n                self.state = 84\n                self.match(seedotParser.T__1)\n                pass\n\n\n            self._ctx.stop = self._input.LT(-1)\n            self.state = 113\n            self._errHandler.sync(self)\n            _alt = self._interp.adaptivePredict(self._input,3,self._ctx)\n            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:\n                if _alt==1:\n                    if self._parseListeners is not None:\n                        self.triggerExitRuleEvent()\n                    _prevctx = localctx\n                    self.state = 111\n                    self._errHandler.sync(self)\n                    la_ = self._interp.adaptivePredict(self._input,2,self._ctx)\n                    if la_ == 1:\n                        localctx = seedotParser.Bop1Context(self, seedotParser.ExprContext(self, _parentctx, _parentState))\n                        self.pushNewRecursionContext(localctx, _startState, self.RULE_expr)\n                        self.state = 88\n                        if not self.precpred(self._ctx, 7):\n                            from antlr4.error.Errors import FailedPredicateException\n                            raise FailedPredicateException(self, ""self.precpred(self._ctx, 7)"")\n                        self.state = 89\n                        self.binOp()\n                        self.state = 90\n                        self.expr(8)\n                        pass\n\n                    elif la_ == 2:\n                        localctx = seedotParser.Bop2Context(self, seedotParser.ExprContext(self, _parentctx, _parentState))\n                        self.pushNewRecursionContext(localctx, _startState, self.RULE_expr)\n                        self.state = 92\n                        if not self.precpred(self._ctx, 6):\n                            from antlr4.error.Errors import FailedPredicateException\n                            raise FailedPredicateException(self, ""self.precpred(self._ctx, 6)"")\n                        self.state = 93\n                        self.addOp()\n                        self.state = 94\n                        self.expr(7)\n                        pass\n\n                    elif la_ == 3:\n                        localctx = seedotParser.CondContext(self, seedotParser.ExprContext(self, _parentctx, _parentState))\n                        self.pushNewRecursionContext(localctx, _startState, self.RULE_expr)\n                        self.state = 96\n                        if not self.precpred(self._ctx, 3):\n                            from antlr4.error.Errors import FailedPredicateException\n                            raise FailedPredicateException(self, ""self.precpred(self._ctx, 3)"")\n                        self.state = 97\n                        self.match(seedotParser.T__8)\n                        self.state = 98\n                        self.match(seedotParser.IntConst)\n                        self.state = 99\n                        self.match(seedotParser.T__9)\n                        self.state = 100\n                        self.expr(0)\n                        self.state = 101\n                        self.match(seedotParser.T__7)\n                        self.state = 102\n                        self.expr(4)\n                        pass\n\n                    elif la_ == 4:\n                        localctx = seedotParser.TranspContext(self, seedotParser.ExprContext(self, _parentctx, _parentState))\n                        self.pushNewRecursionContext(localctx, _startState, self.RULE_expr)\n                        self.state = 104\n                        if not self.precpred(self._ctx, 13):\n                            from antlr4.error.Errors import FailedPredicateException\n                            raise FailedPredicateException(self, ""self.precpred(self._ctx, 13)"")\n                        self.state = 105\n                        self.match(seedotParser.T__5)\n                        pass\n\n                    elif la_ == 5:\n                        localctx = seedotParser.IndexContext(self, seedotParser.ExprContext(self, _parentctx, _parentState))\n                        self.pushNewRecursionContext(localctx, _startState, self.RULE_expr)\n                        self.state = 106\n                        if not self.precpred(self._ctx, 10):\n                            from antlr4.error.Errors import FailedPredicateException\n                            raise FailedPredicateException(self, ""self.precpred(self._ctx, 10)"")\n                        self.state = 107\n                        self.match(seedotParser.T__2)\n                        self.state = 108\n                        self.expr(0)\n                        self.state = 109\n                        self.match(seedotParser.T__4)\n                        pass\n\n             \n                self.state = 115\n                self._errHandler.sync(self)\n                _alt = self._interp.adaptivePredict(self._input,3,self._ctx)\n\n        except RecognitionException as re:\n            localctx.exception = re\n            self._errHandler.reportError(self, re)\n            self._errHandler.recover(self, re)\n        finally:\n            self.unrollRecursionContexts(_parentctx)\n        return localctx\n\n    class AddOpContext(ParserRuleContext):\n\n        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):\n            super().__init__(parent, invokingState)\n            self.parser = parser\n\n        def ADD(self):\n            return self.getToken(seedotParser.ADD, 0)\n\n        def SUB(self):\n            return self.getToken(seedotParser.SUB, 0)\n\n        def getRuleIndex(self):\n            return seedotParser.RULE_addOp\n\n        def accept(self, visitor:ParseTreeVisitor):\n            if hasattr( visitor, ""visitAddOp"" ):\n                return visitor.visitAddOp(self)\n            else:\n                return visitor.visitChildren(self)\n\n\n\n\n    def addOp(self):\n\n        localctx = seedotParser.AddOpContext(self, self._ctx, self.state)\n        self.enterRule(localctx, 2, self.RULE_addOp)\n        self._la = 0 # Token type\n        try:\n            self.enterOuterAlt(localctx, 1)\n            self.state = 116\n            _la = self._input.LA(1)\n            if not(_la==seedotParser.ADD or _la==seedotParser.SUB):\n                self._errHandler.recoverInline(self)\n            else:\n                self._errHandler.reportMatch(self)\n                self.consume()\n        except RecognitionException as re:\n            localctx.exception = re\n            self._errHandler.reportError(self, re)\n            self._errHandler.recover(self, re)\n        finally:\n            self.exitRule()\n        return localctx\n\n    class BinOpContext(ParserRuleContext):\n\n        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):\n            super().__init__(parent, invokingState)\n            self.parser = parser\n\n        def MUL(self):\n            return self.getToken(seedotParser.MUL, 0)\n\n        def SPARSEMUL(self):\n            return self.getToken(seedotParser.SPARSEMUL, 0)\n\n        def MULCIR(self):\n            return self.getToken(seedotParser.MULCIR, 0)\n\n        def CONV(self):\n            return self.getToken(seedotParser.CONV, 0)\n\n        def ADDCIR(self):\n            return self.getToken(seedotParser.ADDCIR, 0)\n\n        def SUBCIR(self):\n            return self.getToken(seedotParser.SUBCIR, 0)\n\n        def getRuleIndex(self):\n            return seedotParser.RULE_binOp\n\n        def accept(self, visitor:ParseTreeVisitor):\n            if hasattr( visitor, ""visitBinOp"" ):\n                return visitor.visitBinOp(self)\n            else:\n                return visitor.visitChildren(self)\n\n\n\n\n    def binOp(self):\n\n        localctx = seedotParser.BinOpContext(self, self._ctx, self.state)\n        self.enterRule(localctx, 4, self.RULE_binOp)\n        self._la = 0 # Token type\n        try:\n            self.enterOuterAlt(localctx, 1)\n            self.state = 118\n            _la = self._input.LA(1)\n            if not((((_la) & ~0x3f) == 0 and ((1 << _la) & ((1 << seedotParser.MUL) | (1 << seedotParser.SPARSEMUL) | (1 << seedotParser.MULCIR) | (1 << seedotParser.CONV) | (1 << seedotParser.ADDCIR) | (1 << seedotParser.SUBCIR))) != 0)):\n                self._errHandler.recoverInline(self)\n            else:\n                self._errHandler.reportMatch(self)\n                self.consume()\n        except RecognitionException as re:\n            localctx.exception = re\n            self._errHandler.reportError(self, re)\n            self._errHandler.recover(self, re)\n        finally:\n            self.exitRule()\n        return localctx\n\n    class SpecialFuncContext(ParserRuleContext):\n\n        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):\n            super().__init__(parent, invokingState)\n            self.parser = parser\n\n        def RELU(self):\n            return self.getToken(seedotParser.RELU, 0)\n\n        def EXP(self):\n            return self.getToken(seedotParser.EXP, 0)\n\n        def ARGMAX(self):\n            return self.getToken(seedotParser.ARGMAX, 0)\n\n        def SGN(self):\n            return self.getToken(seedotParser.SGN, 0)\n\n        def TANH(self):\n            return self.getToken(seedotParser.TANH, 0)\n\n        def getRuleIndex(self):\n            return seedotParser.RULE_specialFunc\n\n        def accept(self, visitor:ParseTreeVisitor):\n            if hasattr( visitor, ""visitSpecialFunc"" ):\n                return visitor.visitSpecialFunc(self)\n            else:\n                return visitor.visitChildren(self)\n\n\n\n\n    def specialFunc(self):\n\n        localctx = seedotParser.SpecialFuncContext(self, self._ctx, self.state)\n        self.enterRule(localctx, 6, self.RULE_specialFunc)\n        self._la = 0 # Token type\n        try:\n            self.enterOuterAlt(localctx, 1)\n            self.state = 120\n            _la = self._input.LA(1)\n            if not((((_la) & ~0x3f) == 0 and ((1 << _la) & ((1 << seedotParser.RELU) | (1 << seedotParser.EXP) | (1 << seedotParser.ARGMAX) | (1 << seedotParser.SGN) | (1 << seedotParser.TANH))) != 0)):\n                self._errHandler.recoverInline(self)\n            else:\n                self._errHandler.reportMatch(self)\n                self.consume()\n        except RecognitionException as re:\n            localctx.exception = re\n            self._errHandler.reportError(self, re)\n            self._errHandler.recover(self, re)\n        finally:\n            self.exitRule()\n        return localctx\n\n    class IntConstListContext(ParserRuleContext):\n\n        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):\n            super().__init__(parent, invokingState)\n            self.parser = parser\n\n        def IntConst(self, i:int=None):\n            if i is None:\n                return self.getTokens(seedotParser.IntConst)\n            else:\n                return self.getToken(seedotParser.IntConst, i)\n\n        def getRuleIndex(self):\n            return seedotParser.RULE_intConstList\n\n        def accept(self, visitor:ParseTreeVisitor):\n            if hasattr( visitor, ""visitIntConstList"" ):\n                return visitor.visitIntConstList(self)\n            else:\n                return visitor.visitChildren(self)\n\n\n\n\n    def intConstList(self):\n\n        localctx = seedotParser.IntConstListContext(self, self._ctx, self.state)\n        self.enterRule(localctx, 8, self.RULE_intConstList)\n        self._la = 0 # Token type\n        try:\n            self.enterOuterAlt(localctx, 1)\n            self.state = 122\n            self.match(seedotParser.IntConst)\n            self.state = 127\n            self._errHandler.sync(self)\n            _la = self._input.LA(1)\n            while _la==seedotParser.T__3:\n                self.state = 123\n                self.match(seedotParser.T__3)\n                self.state = 124\n                self.match(seedotParser.IntConst)\n                self.state = 129\n                self._errHandler.sync(self)\n                _la = self._input.LA(1)\n\n        except RecognitionException as re:\n            localctx.exception = re\n            self._errHandler.reportError(self, re)\n            self._errHandler.recover(self, re)\n        finally:\n            self.exitRule()\n        return localctx\n\n\n\n    def sempred(self, localctx:RuleContext, ruleIndex:int, predIndex:int):\n        if self._predicates == None:\n            self._predicates = dict()\n        self._predicates[0] = self.expr_sempred\n        pred = self._predicates.get(ruleIndex, None)\n        if pred is None:\n            raise Exception(""No predicate with index:"" + str(ruleIndex))\n        else:\n            return pred(localctx, predIndex)\n\n    def expr_sempred(self, localctx:ExprContext, predIndex:int):\n            if predIndex == 0:\n                return self.precpred(self._ctx, 7)\n         \n\n            if predIndex == 1:\n                return self.precpred(self._ctx, 6)\n         \n\n            if predIndex == 2:\n                return self.precpred(self._ctx, 3)\n         \n\n            if predIndex == 3:\n                return self.precpred(self._ctx, 13)\n         \n\n            if predIndex == 4:\n                return self.precpred(self._ctx, 10)\n         \n\n\n\n\n'"
tools/SeeDot/seedot/compiler/antlr/seedotVisitor.py,0,"b'# Generated from seedot.g4 by ANTLR 4.7\nfrom antlr4 import *\nif __name__ is not None and ""."" in __name__:\n    from .seedotParser import seedotParser\nelse:\n    from seedotParser import seedotParser\n\n# This class defines a complete generic visitor for a parse tree produced by seedotParser.\n\nclass seedotVisitor(ParseTreeVisitor):\n\n    # Visit a parse tree produced by seedotParser#bop1.\n    def visitBop1(self, ctx:seedotParser.Bop1Context):\n        return self.visitChildren(ctx)\n\n\n    # Visit a parse tree produced by seedotParser#bop2.\n    def visitBop2(self, ctx:seedotParser.Bop2Context):\n        return self.visitChildren(ctx)\n\n\n    # Visit a parse tree produced by seedotParser#decl.\n    def visitDecl(self, ctx:seedotParser.DeclContext):\n        return self.visitChildren(ctx)\n\n\n    # Visit a parse tree produced by seedotParser#index.\n    def visitIndex(self, ctx:seedotParser.IndexContext):\n        return self.visitChildren(ctx)\n\n\n    # Visit a parse tree produced by seedotParser#sum.\n    def visitSum(self, ctx:seedotParser.SumContext):\n        return self.visitChildren(ctx)\n\n\n    # Visit a parse tree produced by seedotParser#reshape.\n    def visitReshape(self, ctx:seedotParser.ReshapeContext):\n        return self.visitChildren(ctx)\n\n\n    # Visit a parse tree produced by seedotParser#float.\n    def visitFloat(self, ctx:seedotParser.FloatContext):\n        return self.visitChildren(ctx)\n\n\n    # Visit a parse tree produced by seedotParser#cond.\n    def visitCond(self, ctx:seedotParser.CondContext):\n        return self.visitChildren(ctx)\n\n\n    # Visit a parse tree produced by seedotParser#int.\n    def visitInt(self, ctx:seedotParser.IntContext):\n        return self.visitChildren(ctx)\n\n\n    # Visit a parse tree produced by seedotParser#transp.\n    def visitTransp(self, ctx:seedotParser.TranspContext):\n        return self.visitChildren(ctx)\n\n\n    # Visit a parse tree produced by seedotParser#paren.\n    def visitParen(self, ctx:seedotParser.ParenContext):\n        return self.visitChildren(ctx)\n\n\n    # Visit a parse tree produced by seedotParser#func.\n    def visitFunc(self, ctx:seedotParser.FuncContext):\n        return self.visitChildren(ctx)\n\n\n    # Visit a parse tree produced by seedotParser#uop.\n    def visitUop(self, ctx:seedotParser.UopContext):\n        return self.visitChildren(ctx)\n\n\n    # Visit a parse tree produced by seedotParser#let.\n    def visitLet(self, ctx:seedotParser.LetContext):\n        return self.visitChildren(ctx)\n\n\n    # Visit a parse tree produced by seedotParser#id.\n    def visitId(self, ctx:seedotParser.IdContext):\n        return self.visitChildren(ctx)\n\n\n    # Visit a parse tree produced by seedotParser#funcCall.\n    def visitFuncCall(self, ctx:seedotParser.FuncCallContext):\n        return self.visitChildren(ctx)\n\n\n    # Visit a parse tree produced by seedotParser#maxpool.\n    def visitMaxpool(self, ctx:seedotParser.MaxpoolContext):\n        return self.visitChildren(ctx)\n\n\n    # Visit a parse tree produced by seedotParser#addOp.\n    def visitAddOp(self, ctx:seedotParser.AddOpContext):\n        return self.visitChildren(ctx)\n\n\n    # Visit a parse tree produced by seedotParser#binOp.\n    def visitBinOp(self, ctx:seedotParser.BinOpContext):\n        return self.visitChildren(ctx)\n\n\n    # Visit a parse tree produced by seedotParser#specialFunc.\n    def visitSpecialFunc(self, ctx:seedotParser.SpecialFuncContext):\n        return self.visitChildren(ctx)\n\n\n    # Visit a parse tree produced by seedotParser#intConstList.\n    def visitIntConstList(self, ctx:seedotParser.IntConstListContext):\n        return self.visitChildren(ctx)\n\n\n\ndel seedotParser'"
tools/SeeDot/seedot/compiler/ast/__init__.py,0,b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n'
tools/SeeDot/seedot/compiler/ast/ast.py,0,"b""# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n'''\nThis file contains the definitions of various nodes in the Abstract Syntax Tree (AST).\nFor a given program, the nodes of the AST is created based on the operators present in the input program.\n'''\n\n\nclass ASTNode:\n\n    def __init__(self):\n        self.printLevel = 0\n        self.gamma = {}\n\n\nclass Int(ASTNode):\n\n    def __init__(self, value):\n        super().__init__()\n        self.value = value\n\n\nclass Float(ASTNode):\n\n    def __init__(self, value):\n        super().__init__()\n        self.value = value\n\n\nclass ID(ASTNode):\n\n    def __init__(self, name: str):\n        super().__init__()\n        self.name = name\n\n\nclass Decl(ASTNode):\n\n    def __init__(self, shape: list, range: tuple):\n        super().__init__()\n        self.shape = shape\n        self.range = range\n\n\nclass Transp(ASTNode):\n\n    def __init__(self, expr):\n        super().__init__()\n        self.expr = expr\n\n\nclass Reshape(ASTNode):\n\n    def __init__(self, expr, shape, order):\n        super().__init__()\n        self.expr = expr\n        self.shape = shape\n        self.order = order\n\n\nclass Maxpool(ASTNode):\n\n    def __init__(self, expr, dim: int):\n        super().__init__()\n        self.expr = expr\n        self.dim = dim\n\n\nclass Index(ASTNode):\n\n    def __init__(self, expr, index):\n        super().__init__()\n        self.expr = expr\n        self.index = index\n\n\nclass FuncCall(ASTNode):\n\n    def __init__(self, name: str, exprList):\n        super().__init__()\n        self.name = name\n        self.exprList = exprList\n\n\nclass Uop(ASTNode):\n\n    def __init__(self, op, expr):\n        super().__init__()\n        self.op = op\n        self.expr = expr\n\n\nclass Bop1(ASTNode):\n\n    def __init__(self, expr1, op, expr2):\n        super().__init__()\n        self.expr1 = expr1\n        self.op = op\n        self.expr2 = expr2\n\n\nclass Bop2(ASTNode):\n\n    def __init__(self, expr1, op, expr2):\n        super().__init__()\n        self.expr1 = expr1\n        self.op = op\n        self.expr2 = expr2\n\n\nclass Func(ASTNode):\n\n    def __init__(self, op, expr):\n        super().__init__()\n        self.op = op\n        self.expr = expr\n\n\nclass Sum(ASTNode):\n\n    def __init__(self, name, start, end, expr):\n        super().__init__()\n        self.name = name\n        self.start = start\n        self.end = end\n        self.expr = expr\n\n\nclass Cond(ASTNode):\n\n    def __init__(self, expr, num, trueBlock, falseBlock):\n        super().__init__()\n        self.expr = expr\n        self.num = num\n        self.trueBlock = trueBlock\n        self.falseBlock = falseBlock\n\n\nclass Let(ASTNode):\n\n    def __init__(self, name, decl, expr):\n        super().__init__()\n        self.name = name\n        self.decl = decl\n        self.expr = expr\n"""
tools/SeeDot/seedot/compiler/ast/astBuilder.py,0,"b""# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n'''\nASTBuilder creates the AST for a given SeeDot program.\n'''\n\nfrom seedot.compiler.antlr.seedotParser import seedotParser as SeeDotParser\nfrom seedot.compiler.antlr.seedotVisitor import seedotVisitor as SeeDotVisitor\n\nimport seedot.compiler.ast.ast as AST\n\n\nclass ASTBuilder(SeeDotVisitor):\n\n    def visitInt(self, ctx: SeeDotParser.IntContext):\n        value = int(ctx.IntConst().getText())\n        return AST.Int(value)\n\n    def visitFloat(self, ctx: SeeDotParser.FloatContext):\n        value = float(ctx.FloatConst().getText())\n        return AST.Float(value)\n\n    def visitId(self, ctx: SeeDotParser.IdContext):\n        name = ctx.Id().getText()\n        return AST.ID(name)\n\n    def visitDecl(self, ctx: SeeDotParser.DeclContext):\n        shape = [int(IntConst.getText())\n                 for IntConst in ctx.intConstList().IntConst()]\n        range = float(ctx.FloatConst(0).getText()), float(\n            ctx.FloatConst(1).getText())\n        return AST.Decl(shape, range)\n\n    def visitTransp(self, ctx: SeeDotParser.TranspContext):\n        expr = self.visit(ctx.expr())\n        return AST.Transp(expr)\n\n    def visitReshape(self, ctx: SeeDotParser.ReshapeContext):\n        expr = self.visit(ctx.expr())\n        shape = [int(IntConst.getText())\n                 for IntConst in ctx.intConstList(0).IntConst()]\n        order = [int(IntConst.getText())\n                 for IntConst in ctx.intConstList(1).IntConst()]\n        return AST.Reshape(expr, shape, order)\n\n    def visitMaxpool(self, ctx: SeeDotParser.MaxpoolContext):\n        expr = self.visit(ctx.expr())\n        dim = int(ctx.IntConst().getText())\n        return AST.Maxpool(expr, dim)\n\n    def visitIndex(self, ctx: SeeDotParser.IndexContext):\n        expr = self.visit(ctx.expr(0))\n        index = self.visit(ctx.expr(1))\n        return AST.Index(expr, index)\n\n    def visitFuncCall(self, ctx: SeeDotParser.FuncCallContext):\n        name = ctx.Id().getText()\n        exprList = [self.visit(expr) for expr in ctx.expr()]\n        return AST.FuncCall(name, exprList)\n\n    def visitUop(self, ctx: SeeDotParser.UopContext):\n        op = ctx.addOp().getChild(0).symbol.type\n        expr = self.visit(ctx.expr())\n        return AST.Uop(op, expr)\n\n    def visitBop1(self, ctx: SeeDotParser.Bop1Context):\n        expr1 = self.visit(ctx.expr(0))\n        op = ctx.binOp().getChild(0).symbol.type\n        expr2 = self.visit(ctx.expr(1))\n        return AST.Bop1(expr1, op, expr2)\n\n    def visitBop2(self, ctx: SeeDotParser.Bop2Context):\n        expr1 = self.visit(ctx.expr(0))\n        op = ctx.addOp().getChild(0).symbol.type\n        expr2 = self.visit(ctx.expr(1))\n        return AST.Bop2(expr1, op, expr2)\n\n    def visitFunc(self, ctx: SeeDotParser.FuncContext):\n        op = ctx.specialFunc().getChild(0).symbol.type\n        expr = self.visit(ctx.expr())\n        return AST.Func(op, expr)\n\n    def visitSum(self, ctx: SeeDotParser.SumContext):\n        name = ctx.Id().getText()\n        start = int(ctx.IntConst(0).getText())\n        end = int(ctx.IntConst(1).getText())\n        expr = self.visit(ctx.expr())\n        return AST.Sum(name, start, end, expr)\n\n    def visitCond(self, ctx: SeeDotParser.CondContext):\n        expr = self.visit(ctx.expr(0))\n        # Currently Cond node is used only to check sign of the expression\n        # Hence the rhs of the conditional is supposed to be zero\n        assert ctx.IntConst().getText() == '0'\n        num = 0\n        trueBlock = self.visit(ctx.expr(1))\n        falseBlock = self.visit(ctx.expr(2))\n        return AST.Cond(expr, num, trueBlock, falseBlock)\n\n    def visitLet(self, ctx: SeeDotParser.LetContext):\n        name = ctx.Id().getText()\n        decl = self.visit(ctx.expr(0))\n        expr = self.visit(ctx.expr(1))\n        return AST.Let(name, decl, expr)\n\n    def visitParen(self, ctx: SeeDotParser.ParenContext):\n        expr = self.visit(ctx.expr())\n        return expr\n"""
tools/SeeDot/seedot/compiler/ast/astVisitor.py,0,"b""# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n'''\nASTVisitor contains the dynamic dispatcher used by printAST.py\n'''\n\nimport seedot.compiler.ast.ast as AST\n\n\nclass ASTVisitor:\n\n    def visit(self, node):\n        if isinstance(node, AST.Int):\n            return self.visitInt(node)\n        elif isinstance(node, AST.Float):\n            return self.visitFloat(node)\n        elif isinstance(node, AST.ID):\n            return self.visitId(node)\n        elif isinstance(node, AST.Decl):\n            return self.visitDecl(node)\n        elif isinstance(node, AST.Transp):\n            return self.visitTransp(node)\n        elif isinstance(node, AST.Reshape):\n            return self.visitReshape(node)\n        elif isinstance(node, AST.Maxpool):\n            return self.visitMaxpool(node)\n        elif isinstance(node, AST.Index):\n            return self.visitIndex(node)\n        elif isinstance(node, AST.FuncCall):\n            return self.visitFuncCall(node)\n        elif isinstance(node, AST.Uop):\n            return self.visitUop(node)\n        elif isinstance(node, AST.Bop1):\n            return self.visitBop1(node)\n        elif isinstance(node, AST.Bop2):\n            return self.visitBop2(node)\n        elif isinstance(node, AST.Func):\n            return self.visitFunc(node)\n        elif isinstance(node, AST.Sum):\n            return self.visitSum(node)\n        elif isinstance(node, AST.Cond):\n            return self.visitCond(node)\n        elif isinstance(node, AST.Let):\n            return self.visitLet(node)\n        else:\n            assert False\n"""
tools/SeeDot/seedot/compiler/ast/printAST.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n\'\'\'\nPrintAST can be used to print the generated AST for a given SeeDot program.\n\'\'\'\n\nfrom seedot.compiler.antlr.seedotParser import seedotParser as SeeDotParser\n\nimport seedot.compiler.ast.ast as AST\nfrom seedot.compiler.ast.astVisitor import ASTVisitor\n\nindent = ""  ""\n\n\nclass PrintAST(ASTVisitor):\n\n    def visitInt(self, node: AST.Int):\n        print(indent * node.printLevel, node.value)\n\n    def visitFloat(self, node: AST.Float):\n        print(indent * node.printLevel, node.value)\n\n    def visitId(self, node: AST.ID):\n        print(indent * node.printLevel, node.name)\n\n    def visitDecl(self, node: AST.Decl):\n        print(indent * node.printLevel, node.shape, ""in"", node.range)\n\n    def visitTransp(self, node: AST.Transp):\n        node.expr.printLevel = node.printLevel + 1\n        self.visit(node.expr)\n        print(indent * node.printLevel, ""^T"")\n\n    def visitReshape(self, node: AST.Reshape):\n        node.expr.printLevel = node.printLevel + 1\n        print(indent * node.printLevel, ""reshape"")\n        self.visit(node.expr)\n        print(indent * node.printLevel, node.shape, ""order"", node.order)\n\n    def visitMaxpool(self, node: AST.Maxpool):\n        node.expr.printLevel = node.printLevel + 1\n        print(indent * node.printLevel, ""maxpool"")\n        self.visit(node.expr)\n        print(indent * node.printLevel, node.dim)\n\n    def visitIndex(self, node: AST.Index):\n        node.expr.printLevel = node.index.printLevel = node.printLevel + 1\n        self.visit(node.expr)\n        print(indent * node.printLevel, ""["")\n        self.visit(node.index)\n        print(indent * node.printLevel, ""]"")\n\n    def visitFuncCall(self, node: AST.Index):\n        for expr in node.exprList:\n            expr.printLevel = node.printLevel + 1\n        print(indent * node.printLevel, node.name + "" ("")\n        for expr in node.exprList:\n            self.visit(expr)\n            print(indent * node.printLevel, "","")\n        print(indent * node.printLevel, "")"")\n\n    def visitUop(self, node: AST.Uop):\n        node.expr.printLevel = node.printLevel + 1\n        print(indent * node.printLevel, SeeDotParser.literalNames[node.op])\n        self.visit(node.expr)\n\n    def visitBop1(self, node: AST.Bop1):\n        node.expr1.printLevel = node.expr2.printLevel = node.printLevel + 1\n        self.visit(node.expr1)\n        print(indent * node.printLevel, SeeDotParser.literalNames[node.op])\n        self.visit(node.expr2)\n\n    def visitBop2(self, node: AST.Bop2):\n        node.expr1.printLevel = node.expr2.printLevel = node.printLevel + 1\n        self.visit(node.expr1)\n        print(indent * node.printLevel, SeeDotParser.literalNames[node.op])\n        self.visit(node.expr2)\n\n    def visitFunc(self, node: AST.Func):\n        print(indent * node.printLevel, SeeDotParser.literalNames[node.op])\n        node.expr.printLevel = node.printLevel + 1\n        self.visit(node.expr)\n\n    def visitSum(self, node: AST.Sum):\n        print(indent * node.printLevel, node.name,\n              str(node.start), str(node.end))\n        node.expr.printLevel = node.printLevel + 1\n        self.visit(node.expr)\n\n    def visitCond(self, node: AST.Cond):\n        node.expr.printLevel = node.trueBlock.printLevel = node.falseBlock.printLevel = node.printLevel + 1\n        self.visit(node.expr)\n        print(indent * node.printLevel, "">"", str(node.num))\n        self.visit(node.trueBlock)\n        print(indent * node.printLevel, "":"")\n        self.visit(node.falseBlock)\n\n    def visitLet(self, node: AST.Let):\n        node.decl.printLevel = node.expr.printLevel = node.printLevel + 1\n        print(indent * node.printLevel, ""let"", node.name, ""="")\n        self.visit(node.decl)\n        print(indent * node.printLevel, ""in"")\n        self.visit(node.expr)\n'"
tools/SeeDot/seedot/compiler/codegen/__init__.py,0,b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n'
tools/SeeDot/seedot/compiler/codegen/arduino.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n\'\'\'\nArduino backend handles the automatic Arduino sketch generation.\nIt adds the appropriate header files to the sketch and makes it easy to \'compile and upload\' the sketch to a device.\nMost of the routines in the base class CodegenBase are unchanged.\n\'\'\'\n\nimport numpy as np\n\nfrom seedot.compiler.codegen.codegenBase import CodegenBase\n\nimport seedot.compiler.ir.ir as IR\nimport seedot.compiler.ir.irUtil as IRUtil\n\nimport seedot.compiler.type as Type\nfrom seedot.util import *\n\n\nclass Arduino(CodegenBase):\n\n    def __init__(self, writer, decls, scales, intvs, cnsts, expTables, globalVars):\n        self.out = writer\n        self.decls = decls\n        self.scales = scales\n        self.intvs = intvs\n        self.cnsts = cnsts\n        self.expTables = expTables\n        self.globalVars = globalVars\n\n    def printPrefix(self):\n        self.printArduinoIncludes()\n\n        self.printExpTables()\n\n        self.printArduinoHeader()\n\n        self.printVarDecls()\n\n        self.printConstDecls()\n\n        self.out.printf(\'\\n\')\n\n    def printArduinoIncludes(self):\n        self.out.printf(\'#include <Arduino.h>\\n\\n\', indent=True)\n        self.out.printf(\'#include ""config.h""\\n\', indent=True)\n        self.out.printf(\'#include ""predict.h""\\n\', indent=True)\n        self.out.printf(\'#include ""library.h""\\n\', indent=True)\n        self.out.printf(\'#include ""model.h""\\n\\n\', indent=True)\n        self.out.printf(\'using namespace model;\\n\\n\', indent=True)\n\n    # Dumps the generated look-up table for computing exponentials.\n    def printExpTables(self):\n        for exp, [table, [tableVarA, tableVarB]] in self.expTables.items():\n            self.printExpTable(table[0], tableVarA)\n            self.printExpTable(table[1], tableVarB)\n            self.out.printf(\'\\n\')\n\n    def printExpTable(self, table_row, var):\n        self.out.printf(\'const PROGMEM MYINT %s[%d] = {\\n\' % (\n            var.idf, len(table_row)), indent=True)\n        self.out.increaseIndent()\n        self.out.printf(\'\', indent=True)\n        for i in range(len(table_row)):\n            self.out.printf(\'%d, \' % table_row[i])\n        self.out.decreaseIndent()\n        self.out.printf(\'\\n};\\n\')\n\n    def printArduinoHeader(self):\n        self.out.printf(\'int predict() {\\n\', indent=True)\n        self.out.increaseIndent()\n\n    # Generate the appropriate return experssion\n    # If integer, return the integer\n    # If tensor of size 0, convert the fixed-point integer to float and return the float value.\n    # If tensor of size >0, convert the tensor to fixed-point integer, print\n    # it to the serial port, and return void.\n    def printSuffix(self, expr: IR.Expr):\n        self.out.printf(\'\\n\')\n\n        type = self.decls[expr.idf]\n\n        if Type.isInt(type):\n            self.out.printf(\'return \', indent=True)\n            self.print(expr)\n            self.out.printf(\';\\n\')\n        elif Type.isTensor(type):\n            idfr = expr.idf\n            exponent = self.scales[expr.idf]\n            num = 2 ** exponent\n\n            if type.dim == 0:\n                self.out.printf(\'Serial.println(\', indent=True)\n                self.out.printf(\'float(\' + idfr + \')*\' + str(num))\n                self.out.printf(\', 6);\\n\')\n            else:\n                iters = []\n                for i in range(type.dim):\n                    s = chr(ord(\'i\') + i)\n                    tempVar = IR.Var(s)\n                    iters.append(tempVar)\n                expr_1 = IRUtil.addIndex(expr, iters)\n                cmds = IRUtil.loop(type.shape, iters, [\n                                   IR.PrintAsFloat(expr_1, exponent)])\n                self.print(IR.Prog(cmds))\n        else:\n            assert False\n\n        self.out.decreaseIndent()\n        self.out.printf(\'}\\n\', indent=True)\n\n    \'\'\'\n    Below functions are overriding their corresponding definitions in codegenBase.py.\n    These function have arduino-specific print functions.\n    \'\'\'\n\n    # Print the variable with pragmas\n    def printVar(self, ir):\n        if ir.inputVar:\n            if Common.wordLength == 16:\n                self.out.printf(\'((MYINT) pgm_read_word_near(&\')\n            elif Common.wordLength == 32:\n                self.out.printf(\'((MYINT) pgm_read_dword_near(&\')\n            else:\n                assert False\n        self.out.printf(\'%s\', ir.idf)\n        for e in ir.idx:\n            self.out.printf(\'[\')\n            self.print(e)\n            self.out.printf(\']\')\n        if ir.inputVar:\n            self.out.printf(\'))\')\n\n    # The variable X is used to define the data point.\n    # It is either read from the serial port or from the device\'s memory based on the operating mode.\n    # The getIntFeature() function reads the appropriate value of X based on the mode.\n    def printAssn(self, ir):\n        if isinstance(ir.e, IR.Var) and ir.e.idf == ""X"":\n            self.out.printf("""", indent=True)\n            self.print(ir.var)\n            self.out.printf("" = getIntFeature(i0);\\n"")\n        else:\n            super().printAssn(ir)\n\n    def printFuncCall(self, ir):\n        self.out.printf(""%s("" % ir.name, indent=True)\n        keys = list(ir.argList)\n        for i in range(len(keys)):\n            arg = keys[i]\n\n            # Do not print the \'X\' variable as it will be read from the getIntFeature() function.\n            if isinstance(arg, IR.Var) and arg.idf == \'X\':\n                continue\n\n            # The value of x in the below code is the number of special characters (& and []) around the variable in the function call.\n            # This number depends on the shape of the variable.\n            # Example: A[10][10] is written as &A[0][0]. The value of x in this case is 2.\n            # x is 0 for constants\n            # x is -1 for integer variables where only & is printed and not []\n            if isinstance(arg, IR.Var) and arg.idf in self.decls.keys() and not arg.idf == \'X\':\n                type = self.decls[arg.idf]\n                if isinstance(type, Type.Tensor):\n                    if type.dim == 0:\n                        x = -1\n                    else:\n                        x = type.dim - len(arg.idx)\n                else:\n                    x = -1\n            else:\n                x = 0\n            \n            if x != 0:\n                self.out.printf(""&"")\n\n            self.print(arg)\n            \n            if x != 0 and x != -1:\n                self.out.printf(""[0]"" * x)\n            if i != len(keys) - 1:\n                self.out.printf("", "")\n        \n        self.out.printf("");\\n\\n"")\n\n    def printPrint(self, ir):\n        self.out.printf(\'Serial.println(\', indent=True)\n        self.print(ir.expr)\n        self.out.printf(\');\\n\')\n\n    def printPrintAsFloat(self, ir):\n        self.out.printf(\'Serial.println(float(\', indent=True)\n        self.print(ir.expr)\n        self.out.printf(\') * \' + str(2 ** ir.expnt) + \', 6);\')\n'"
tools/SeeDot/seedot/compiler/codegen/codegenBase.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n\'\'\'\nCodegenBase has print functions for the IR classes defined in IR.py\n\'\'\'\n\nimport numpy as np\n\nimport seedot.compiler.ir.ir as IR\n\nimport seedot.common as Common\nimport seedot.compiler.type as Type\nfrom seedot.util import *\n\n\nclass CodegenBase:\n\n    def __init__(self, writer):\n        self.out = writer\n\n    def printOp(self, ir):\n        self.out.printf(\'%s\', ir.name)\n\n    def printInt(self, ir):\n        if np.iinfo(np.int16).min <= ir.n <= np.iinfo(np.int16).max:\n            self.out.printf(\'%d\', ir.n)\n        elif np.iinfo(np.int32).min <= ir.n <= np.iinfo(np.int32).max:\n            self.out.printf(\'%dL\', ir.n)\n        elif np.iinfo(np.int64).min <= ir.n <= np.iinfo(np.int64).max:\n            self.out.printf(\'%dLL\', ir.n)\n        else:\n            assert False\n\n    def printVar(self, ir):\n        self.out.printf(\'%s\', ir.idf)\n        for e in ir.idx:\n            self.out.printf(\'[\')\n            self.print(e)\n            self.out.printf(\']\')\n\n    def printBool(self, ir):\n        self.out.printf({True: \'true\', False: \'false\'}[ir.b])\n\n    def printIntUop(self, ir):\n        self.out.printf(\'(\')\n        self.print(ir.op)\n        self.print(ir.e)\n        self.out.printf(\')\')\n\n    def printIntBop(self, ir):\n        self.out.printf(\'(\')\n        self.print(ir.e1)\n        self.out.printf(\' \')\n        self.print(ir.op)\n        self.out.printf(\' \')\n        self.print(ir.e2)\n        self.out.printf(\')\')\n\n    def printBoolUop(self, ir):\n        self.out.printf(\'(\')\n        self.print(ir.op)\n        self.print(ir.e)\n        self.out.printf(\')\')\n\n    def printBoolBop(self, ir):\n        self.out.printf(\'(\')\n        self.print(ir.e1)\n        self.out.printf(\' \')\n        self.print(ir.op)\n        self.out.printf(\' \')\n        self.print(ir.e2)\n        self.out.printf(\')\')\n\n    def printBoolCop(self, ir):\n        self.out.printf(\'(\')\n        self.print(ir.e1)\n        self.out.printf(\' \')\n        self.print(ir.op)\n        self.out.printf(\' \')\n        self.print(ir.e2)\n        self.out.printf(\')\')\n\n    def printCExpr(self, ir):\n        self.out.printf(\'(\')\n        self.print(ir.cond)\n        self.out.printf(\' ? \')\n        self.print(ir.et)\n        self.out.printf(\' : \')\n        self.print(ir.ef)\n        self.out.printf(\')\')\n\n    def printExp(self, ir):\n        self.out.printf(\'(exp(\')\n        self.print(ir.e)\n        self.out.printf(\'))\')\n\n    def printTypeCast(self, ir):\n        self.out.printf(\'(\')\n        self.out.printf(\'(\' + ir.type + \')\')\n        self.print(ir.expr)\n        self.out.printf(\')\')\n\n    def printAssn(self, ir):\n        self.out.printf(\'\', indent=True)\n        self.print(ir.var)\n        self.out.printf(\' = \')\n        self.print(ir.e)\n        self.out.printf(\';\\n\')\n\n    def printIf(self, ir):\n        self.out.printf(\'if (\', indent=True)\n        self.print(ir.cond)\n        self.out.printf(\') {\\n\')\n\n        self.out.increaseIndent()\n        for cmd in ir.trueCmds:\n            self.print(cmd)\n        self.out.decreaseIndent()\n\n        if len(ir.falseCmds) == 0:\n            self.out.printf(\'}\\n\', indent=True)\n            return\n\n        self.out.printf(\'} else {\\n\', indent=True)\n\n        self.out.increaseIndent()\n        for cmd in ir.falseCmds:\n            self.print(cmd)\n        self.out.decreaseIndent()\n\n        self.out.printf(\'}\\n\', indent=True)\n\n    def printFor(self, ir):\n        self.printForHeader(ir)\n        self.out.increaseIndent()\n        for cmd in ir.cmd_l:\n            self.print(cmd)\n        self.out.decreaseIndent()\n        self.out.printf(\'}\\n\', indent=True)\n\n    def printForHeader(self, ir):\n        self.out.printf(\'for (%s \', IR.DataType.getIntStr(), indent=True)\n        self.print(ir.var)\n        self.out.printf(\' = %d; \', ir.st)\n        self.print(ir.cond)\n        self.out.printf(\'; \')\n        self.print(ir.var)\n        self.out.printf(\'++) {\\n\')\n\n    def printWhile(self, ir):\n        self.out.printf(\'while (\', indent=True)\n        self.print(ir.expr)\n        self.out.printf(\') {\\n\')\n        self.out.increaseIndent()\n        for cmd in ir.cmds:\n            self.print(cmd)\n        self.out.decreaseIndent()\n        self.out.printf(\'}\\n\', indent=True)\n\n    def printFuncCall(self, ir):\n        self.out.printf(""%s("" % ir.name, indent=True)\n        keys = list(ir.argList)\n        for i in range(len(keys)):\n            arg = keys[i]\n            if isinstance(arg, IR.Var) and arg.idf in self.decls.keys() and not arg.idf == \'X\':\n                type = self.decls[arg.idf]\n                if isinstance(type, Type.Tensor):\n                    if type.dim == 0:\n                        x = -1\n                    else:\n                        x = type.dim - len(arg.idx)\n                else:\n                    x = -1\n            else:\n                x = 0\n            if x != 0:\n                self.out.printf(""&"")\n            self.print(arg)\n            if x != 0 and x != -1:\n                self.out.printf(""[0]"" * x)\n            if i != len(keys) - 1:\n                self.out.printf("", "")\n        self.out.printf("");\\n\\n"")\n\n    def printMemset(self, ir):\n        self.out.printf(\'memset(\', indent=True)\n        self.print(ir.e)\n        self.out.printf(\', 0, sizeof(%s) * %d);\\n\' %\n                        (IR.DataType.getIntStr(), ir.len))\n\n    def printPrint(self, ir):\n        self.out.printf(\'cout << \', indent=True)\n        self.print(ir.expr)\n        self.out.printf(\' << endl;\\n\')\n\n    def printPrintAsFloat(self, ir):\n        self.out.printf(\'cout << ((float)(\', indent=True)\n        self.print(ir.expr)\n        self.out.printf(\')) * \' + str(2 ** ir.expnt) + \' << """";\\n\')\n\n    def printPragmas(self, ir):\n        if ir.vital == 1:\n            self.out.printf(\'\\n\')\n            self.out.printf(ir.msg + \'\\n\', indent=True)\n\n    def printComment(self, ir):\n        self.out.printf(\'\\n\')\n        self.out.printf(\'// \' + ir.msg + \'\\n\', indent=True)\n\n    def printProg(self, ir):\n        for cmd in ir.cmd_l:\n            self.print(cmd)\n\n    def print(self, ir):\n        if isinstance(ir, IR.Int):\n            return self.printInt(ir)\n        elif isinstance(ir, IR.Var):\n            return self.printVar(ir)\n        elif isinstance(ir, IR.Bool):\n            return self.printBool(ir)\n        elif isinstance(ir, IR.IntUop):\n            return self.printIntUop(ir)\n        elif isinstance(ir, IR.IntBop):\n            return self.printIntBop(ir)\n        elif isinstance(ir, IR.BoolUop):\n            return self.printBoolUop(ir)\n        elif isinstance(ir, IR.BoolBop):\n            return self.printBoolBop(ir)\n        elif isinstance(ir, IR.BoolCop):\n            return self.printBoolCop(ir)\n        elif isinstance(ir, IR.CExpr):\n            return self.printCExpr(ir)\n        elif isinstance(ir, IR.Exp):\n            return self.printExp(ir)\n        elif isinstance(ir, IR.TypeCast):\n            return self.printTypeCast(ir)\n        elif isinstance(ir, IR.Assn):\n            return self.printAssn(ir)\n        elif isinstance(ir, IR.If):\n            return self.printIf(ir)\n        elif isinstance(ir, IR.For):\n            return self.printFor(ir)\n        elif isinstance(ir, IR.While):\n            return self.printWhile(ir)\n        elif isinstance(ir, IR.FuncCall):\n            return self.printFuncCall(ir)\n        elif isinstance(ir, IR.Memset):\n            return self.printMemset(ir)\n        elif isinstance(ir, IR.Print):\n            return self.printPrint(ir)\n        elif isinstance(ir, IR.PrintAsFloat):\n            return self.printPrintAsFloat(ir)\n        elif isinstance(ir, IR.Comment):\n            return self.printComment(ir)\n        elif isinstance(ir, IR.Prog):\n            return self.printProg(ir)\n        elif isinstance(ir, IR.Op.Op):\n            return self.printOp(ir)\n        else:\n            assert False\n\n    def printAll(self, prog: IR.Prog, expr: IR.Expr):\n        self.printPrefix()\n        self.print(prog)\n        self.printSuffix(expr)\n\n    def printVarDecls(self):\n        for decl in self.decls:\n            if decl in self.globalVars:\n                continue\n            typ_str = IR.DataType.getIntStr()\n            idf_str = decl\n            type = self.decls[decl]\n            if Type.isInt(type):\n                shape_str = \'\'\n            elif Type.isTensor(type):\n                shape_str = \'\'.join([\'[\' + str(n) + \']\' for n in type.shape])\n            self.out.printf(\'%s %s%s;\\n\', typ_str, idf_str,\n                            shape_str, indent=True)\n        self.out.printf(\'\\n\')\n\n    def printConstDecls(self):\n        for cnst in self.cnsts:\n            var, num = cnst, self.cnsts[cnst]\n            if np.iinfo(np.int16).min <= num <= np.iinfo(np.int16).max:\n                self.out.printf(\'%s = %d;\\n\', var, num, indent=True)\n            elif np.iinfo(np.int32).min <= num <= np.iinfo(np.int32).max:\n                self.out.printf(\'%s = %dL;\\n\', var, num, indent=True)\n            elif np.iinfo(np.int64).min <= num <= np.iinfo(np.int64).max:\n                self.out.printf(\'%s = %dLL;\\n\', var, num, indent=True)\n            else:\n                assert False\n'"
tools/SeeDot/seedot/compiler/codegen/x86.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport numpy as np\n\nfrom seedot.compiler.codegen.codegenBase import CodegenBase\n\nimport seedot.compiler.ir.ir as IR\nimport seedot.compiler.ir.irUtil as IRUtil\n\nimport seedot.compiler.type as Type\nfrom seedot.util import *\n\n\nclass X86(CodegenBase):\n\n    def __init__(self, writer, decls, scales, intvs, cnsts, expTables, globalVars):\n        self.out = writer\n        self.decls = decls\n        self.scales = scales\n        self.intvs = intvs\n        self.cnsts = cnsts\n        self.expTables = expTables\n        self.globalVars = globalVars\n\n    def printPrefix(self):\n        self.printCincludes()\n\n        self.printExpTables()\n\n        self.printCHeader()\n\n        self.printVarDecls()\n\n        self.printConstDecls()\n\n        self.out.printf(\'\\n\')\n\n    def printCincludes(self):\n        self.out.printf(\'#include <iostream>\\n\', indent=True)\n        self.out.printf(\'#include <cstring>\\n\', indent=True)\n        self.out.printf(\'#include <cmath>\\n\\n\', indent=True)\n        self.out.printf(\'#include ""datatypes.h""\\n\', indent=True)\n        self.out.printf(\'#include ""predictors.h""\\n\', indent=True)\n        self.out.printf(\'#include ""library.h""\\n\', indent=True)\n        self.out.printf(\'#include ""seedot_fixed_model.h""\\n\\n\', indent=True)\n        self.out.printf(\'using namespace std;\\n\', indent=True)\n        self.out.printf(\'using namespace %s_fixed;\\n\\n\' %\n                        (getAlgo()), indent=True)\n\n    def printExpTables(self):\n        for exp, [table, [tableVarA, tableVarB]] in self.expTables.items():\n            self.printExpTable(table[0], tableVarA)\n            self.printExpTable(table[1], tableVarB)\n            self.out.printf(\'\\n\')\n\n    def printExpTable(self, table_row, var):\n        self.out.printf(\'const MYINT %s[%d] = {\\n\' % (\n            var.idf, len(table_row)), indent=True)\n        self.out.increaseIndent()\n        self.out.printf(\'\', indent=True)\n        for i in range(len(table_row)):\n            self.out.printf(\'%d, \' % table_row[i])\n        self.out.decreaseIndent()\n        self.out.printf(\'\\n};\\n\')\n\n    def printCHeader(self):\n        self.out.printf(\'int seedotFixed(MYINT **X) {\\n\', indent=True)\n        self.out.increaseIndent()\n\n    def printSuffix(self, expr: IR.Expr):\n        self.out.printf(\'\\n\')\n\n        type = self.decls[expr.idf]\n\n        if Type.isInt(type):\n            self.out.printf(\'return \', indent=True)\n            self.print(expr)\n            self.out.printf(\';\\n\')\n        elif Type.isTensor(type):\n            idfr = expr.idf\n            exponent = self.scales[expr.idf]\n            num = 2 ** exponent\n\n            if type.dim == 0:\n                self.out.printf(\'cout << \', indent=True)\n                self.out.printf(\'float(\' + idfr + \')*\' + str(num))\n                self.out.printf(\' << endl;\\n\')\n            else:\n                iters = []\n                for i in range(type.dim):\n                    s = chr(ord(\'i\') + i)\n                    tempVar = IR.Var(s)\n                    iters.append(tempVar)\n                expr_1 = IRUtil.addIndex(expr, iters)\n                cmds = IRUtil.loop(type.shape, iters, [\n                                   IR.PrintAsFloat(expr_1, exponent)])\n                self.print(IR.Prog(cmds))\n        else:\n            assert False\n\n        self.out.decreaseIndent()\n        self.out.printf(\'}\\n\', indent=True)\n'"
tools/SeeDot/seedot/compiler/converter/__init__.py,0,b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n'
tools/SeeDot/seedot/compiler/converter/bonsai.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport math\nimport os\n\nfrom seedot.compiler.converter.util import *\n\nimport seedot.common as Common\n\n# Class to read Bonsai model dumps and generate input files (C header file and the compiler input)\n# The two classes BonsaiFixed and BonsaiFloat are for generating fixed model and float model respectively\n# The baseclass Bonsai collects some of the common functions between them.\n\n\nclass Bonsai:\n\n    def readDataset(self):\n        self.X, self.Y = readXandY()\n\n    def formatDataset(self):\n        assert len(self.X) == len(self.Y)\n        for i in range(len(self.X)):\n            self.X[i].append(1)\n\n    def writeDataset(self):\n        writeMatAsCSV(self.X, os.path.join(getDatasetOutputDir(), ""X.csv""))\n        writeMatAsCSV(self.Y, os.path.join(getDatasetOutputDir(), ""Y.csv""))\n\n    def processDataset(self):\n        self.readDataset()\n        self.formatDataset()\n        self.transformDataset()\n        self.writeDataset()\n\n    def readTheta(self):\n        if os.path.isfile(os.path.join(getModelDir(), ""T"")):\n            return readFileAsMat(os.path.join(getModelDir(), ""T""), ""\\t"", float)\n        elif os.path.isfile(os.path.join(getModelDir(), ""Theta"")):\n            return readFileAsMat(os.path.join(getModelDir(), ""Theta""), ""\\t"", float)\n        else:\n            assert False\n\n    def readModel(self):\n        self.Z = readFileAsMat(os.path.join(getModelDir(), ""Z""), ""\\t"", float)\n        self.W = readFileAsMat(os.path.join(getModelDir(), ""W""), ""\\t"", float)\n        self.V = readFileAsMat(os.path.join(getModelDir(), ""V""), ""\\t"", float)\n        self.T = self.readTheta()\n        self.Sigma = readFileAsMat(os.path.join(\n            getModelDir(), ""Sigma""), ""\\t"", float)\n        self.Mean = readFileAsMat(os.path.join(\n            getModelDir(), ""Mean""), ""\\t"", float)\n        self.Variance = readFileAsMat(os.path.join(\n            getModelDir(), ""Std""), ""\\t"", float)\n\n    def validateModel(self):\n        Z_m, Z_n = matShape(self.Z)\n        W_m, W_n = matShape(self.W)\n        V_m, V_n = matShape(self.V)\n        # If T is empty\n        if len(self.T) == 0:\n            T_m, T_n = 0, Z_m\n        else:\n            T_m, T_n = matShape(self.T)\n        Sigma_m, Sigma_n = matShape(self.Sigma)\n        Mean_m, Mean_n = matShape(self.Mean)\n        Variance_m, Variance_n = matShape(self.Variance)\n\n        assert Z_n == Mean_m == Variance_m\n        assert Z_m == W_n == V_n == T_n\n        assert W_m == V_m\n        assert Sigma_m == Sigma_n == 1\n        assert Mean_n == Variance_n == 1\n\n    # Restructure the W and V matrix to be node ID major instead of class ID\n    # major\n    def rearrangeWandV(self, mat):\n        matNew = [[] for _ in range(len(mat))]\n        for i in range(self.numClasses):\n            for j in range(self.totalNodes):\n                matNew[i + j * self.numClasses] = mat[i * self.totalNodes + j]\n        return matNew\n\n    # Note: Following computations assume that the tree is always complete\n    # (which is a property of Bonsai algorithm)\n    def computeVars(self):\n        self.D = len(self.Z[0])\n        self.d = len(self.Z)\n        self.internalNodes = len(self.T)\n        self.depth = int(math.log2(self.internalNodes + 1))\n        self.totalNodes = 2 ** (self.depth + 1) - 1\n        self.numClasses = len(self.W) // self.totalNodes\n\n    def formatModel(self):\n        # Manually set mean and variance of bias to 0 and 1 respectively\n        self.Mean[len(self.Mean) - 1] = [0]\n        self.Variance[len(self.Variance) - 1] = [1]\n\n        sigma = self.Sigma[0][0]\n\n        self.computeVars()\n\n        # Precompute some values to speedup prediction\n        # Precompute V*sigma and Z/d\n        self.V = [[x * sigma for x in y] for y in self.V]\n        self.Z = [[x / self.d for x in y] for y in self.Z]\n\n        # Precompute Z*(X-m)/v by absorbing m, v into Z\n        self.Z = [[x / v[0] for x, v in zip(y, self.Variance)] for y in self.Z]\n\n        self.mean = matMul(self.Z, self.Mean)\n        assert len(self.mean[0]) == 1\n        self.mean = [x[0] for x in self.mean]\n\n        # Restructure W and V\n        self.W = self.rearrangeWandV(self.W)\n        self.V = self.rearrangeWandV(self.V)\n\n    def verifyModel(self):\n        # If T is empty, it is replaced with a matrix containing 2 elements\n        # (which is never used during prediction)\n        if len(self.T) == 0:\n            print(""Warning: Empty matrix T\\n"")\n            self.T = [[-0.000001, 0.000001]]\n\n    def computeModelSize(self):\n        mean_num = len(self.mean)\n        if useSparseMat():\n            Z_transp = matTranspose(self.Z)\n            Zval, Zidx = convertToSparse(Z_transp)\n            Z_num = len(Zval)\n            Zidx_num = len(Zidx)\n        else:\n            Z_num = len(self.Z) * len(self.Z[0])\n            Zidx_num = 0\n        W_num = len(self.W) * len(self.W[0])\n        V_num = len(self.V) * len(self.V[0])\n        T_num = len(self.T) * len(self.T[0])\n\n        total_num = mean_num + Z_num + W_num + V_num + T_num\n\n        with open(self.infoFile, \'a\') as file:\n            file.write(""nnz values: %d\\n"" % (total_num))\n            file.write(""# indexes: %d\\n\\n"" % (Zidx_num))\n            file.write(""---------------------\\n"")\n            file.write(""Model size comparison\\n"")\n            file.write(""---------------------\\n"")\n            file.write(""32-bit floating-points (in KB): %.3f\\n"" %\n                       (((total_num * 4) + (Zidx_num * 2)) / 1024))\n            file.write(\n                ""(assuming 4 bytes for values and 2 bytes for indices)\\n\\n"")\n            file.write(""16-bit fixed-points (in KB): %.3f\\n"" %\n                       (((total_num * 2) + (Zidx_num * 2)) / 1024))\n            file.write(\n                ""(assuming 2 bytes for values and 2 bytes for indices)\\n\\n"")\n            file.write(""32-bit fixed-points (in KB): %.3f\\n"" %\n                       (((total_num * 4) + (Zidx_num * 2)) / 1024))\n            file.write(\n                ""(assuming 4 bytes for values and 2 bytes for indices)\\n"")\n            file.write(""--------------------------------------------\\n\\n"")\n\n    # Write macros and namespace declarations\n    def writeHeader(self):\n        with open(self.headerFile, \'a\') as file:\n            file.write(""#pragma once\\n\\n"")\n            if useSparseMat():\n                file.write(""#define B_SPARSE_Z 1\\n\\n"")\n            else:\n                file.write(""#define B_SPARSE_Z 0\\n\\n"")\n\n            if forArduino():\n                s = \'model\'\n            else:\n                s = \'bonsai_\' + getVersion()\n            file.write(""namespace %s {\\n\\n"" % (s))\n\n    def writeFooter(self):\n        with open(self.headerFile, \'a\') as file:\n            file.write(""}\\n"")\n\n    def processModel(self):\n        self.readModel()\n        self.validateModel()\n        self.formatModel()\n        self.verifyModel()\n        self.transformModel()\n        self.computeModelSize()\n        self.writeModel()\n\n    def run(self):\n        if getVersion() == Common.Version.Float:\n            self.headerFile = os.path.join(\n                getOutputDir(), ""bonsai_float_model.h"")\n        else:\n            self.headerFile = os.path.join(\n                getOutputDir(), ""seedot_fixed_model.h"")\n        self.inputFile = os.path.join(getOutputDir(), ""input.sd"")\n        self.infoFile = os.path.join(getOutputDir(), ""info.txt"")\n\n        open(self.headerFile, \'w\').close()\n        open(self.infoFile, \'w\').close()\n\n        if dumpDataset():\n            self.processDataset()\n\n        self.processModel()\n\n        if dumpDataset():\n            assert len(self.X[0]) == len(self.Z[0])\n\n\nclass BonsaiFixed(Bonsai):\n\n    # The X matrix is quantized using a scale factor computed from the training dataset.\n    # The range of X_train is used to compute the scale factor.\n    # Since the range of X_train depends on its distribution, the scale computed may be imprecise.\n    # To avoid this, any outliers in X_train is trimmed off using a threshold\n    # to get a more precise range and a more precise scale.\n    def transformDataset(self):\n        # If X itself is X_train, reuse it. Otherwise, read it from file\n        if usingTrainingDataset():\n            self.X_train = list(self.X)\n        else:\n            self.X_train, _ = readXandY(useTrainingSet=True)\n\n        # Trim some data points from X_train\n        self.X_train, _ = trimMatrix(self.X_train)\n\n        # Compute range and scale and quantize X\n        testDatasetRange = matRange(self.X)\n        self.trainDatasetRange = matRange(self.X_train)\n\n        scale = computeScale(*self.trainDatasetRange)\n        self.X, _ = scaleMat(self.X, scale)\n\n        with open(self.infoFile, \'a\') as file:\n            file.write(""Range of test dataset: [%.6f, %.6f]\\n"" % (\n                testDatasetRange))\n            file.write(""Range of training dataset: [%.6f, %.6f]\\n"" % (\n                self.trainDatasetRange))\n            file.write(""Test dataset scaled by: %d\\n\\n"" % (scale))\n\n    # Write the Bonsai algorithm in terms of the compiler DSL.\n    # Compiler takes this as input and generates fixed-point code.\n    def genInputForCompiler(self):\n        # Threshold for mean\n        m_mean, M_mean = listRange(self.mean)\n        if abs(m_mean) < 0.0000005:\n            m_mean = -0.000001\n        if abs(M_mean) < 0.0000005:\n            M_mean = 0.000001\n\n        with open(self.inputFile, \'w\') as file:\n            # Matrix declarations\n            file.write(""let X   = (%d, 1)   in [%.6f, %.6f] in\\n"" % (\n                (len(self.X[0]),) + self.trainDatasetRange))\n            file.write(""let Z   = (%d, %d)  in [%.6f, %.6f] in\\n"" % (\n                (self.d, self.D) + matRange(self.Z)))\n            file.write(""let W   = (%d, %d, %d) in [%.6f, %.6f] in\\n"" % (\n                (self.totalNodes, self.numClasses, self.d) + matRange(self.W)))\n            file.write(""let V   = (%d, %d, %d) in [%.6f, %.6f] in\\n"" % (\n                (self.totalNodes, self.numClasses, self.d) + matRange(self.V)))\n            file.write(""let T   = (%d, 1, %d) in [%.6f, %.6f] in\\n"" % (\n                (self.internalNodes, self.d) + matRange(self.T)))\n            file.write(""let mean = (%d, 1) in [%.6f, %.6f] in\\n\\n"" % (\n                self.d, m_mean, M_mean))\n\n            if useSparseMat():\n                file.write(""let ZX = Z |*| X - mean in\\n\\n"")\n            else:\n                file.write(""let ZX = Z * X - mean in\\n\\n"")\n\n            # Computing score for depth == 0\n            file.write(""// depth 0\\n"")\n            file.write(""let node0   = 0    in\\n"")\n            file.write(""let W0      = W[node0] * ZX in\\n"")\n            file.write(""let V0      = V[node0] * ZX in\\n"")\n            file.write(""let V0_tanh = tanh(V0) in\\n"")\n            file.write(""let score0  = W0 <*> V0_tanh in\\n\\n"")\n\n            # Computing score for depth > 0\n            for i in range(1, self.depth + 1):\n                file.write(""// depth %d\\n"" % (i))\n                file.write(\n                    ""let node%d   = (T[node%d] * ZX) >= 0? 2 * node%d + 1 : 2 * node%d + 2 in\\n"" % (i, i - 1, i - 1, i - 1))\n                file.write(""let W%d      = W[node%d] * ZX in\\n"" % (i, i))\n                file.write(""let V%d      = V[node%d] * ZX in\\n"" % (i, i))\n                file.write(""let V%d_tanh = tanh(V%d) in\\n"" % (i, i))\n                file.write(\n                    ""let score%d  = score%d + W%d <*> V%d_tanh in\\n\\n"" % (i, i - 1, i, i))\n\n            # Predicting the class\n            if self.numClasses <= 2:\n                file.write(""sgn(score%d)\\n"" % (self.depth))\n            else:\n                file.write(""argmax(score%d)\\n"" % (self.depth))\n\n    # Quantize the matrices\n    def transformModel(self):\n        if dumpDataset():\n            self.genInputForCompiler()\n\n        self.Z, _ = scaleMat(self.Z)\n        self.W, _ = scaleMat(self.W)\n        self.V, _ = scaleMat(self.V)\n        self.T, _ = scaleMat(self.T)\n        self.mean, _ = scaleList(self.mean)\n\n    # Writing the model as a bunch of variables, arrays and matrices to a file\n    def writeModel(self):\n        self.writeHeader()\n\n        if forArduino():\n            writeListAsArray(self.X[0], \'X\', self.headerFile)\n            writeVars({\'Y\': self.Y[0][0]}, self.headerFile)\n\n        writeListAsArray(self.mean, \'mean\', self.headerFile,\n                         shapeStr=""[%d]"" * 2 % (self.d, 1))\n\n        # Sparse matrices are converted in to two arrays containing values and\n        # indices to reduce space\n        if useSparseMat():\n            Z_transp = matTranspose(self.Z)\n            Zval, Zidx = convertToSparse(Z_transp)\n            writeListsAsArray({\'Zval\': Zval, \'Zidx\': Zidx}, self.headerFile)\n        else:\n            writeMatAsArray(self.Z, \'Z\', self.headerFile)\n\n        # If T_m is 0, the generated code will throw an error\n        # Hence, setting it to 1\n        T_m = self.internalNodes\n        if T_m == 0:\n            T_m = 1\n\n        writeMatAsArray(self.W, \'W\', self.headerFile,\n                        shapeStr=""[%d]"" * 3 % (self.totalNodes, self.numClasses, self.d))\n        writeMatAsArray(self.V, \'V\', self.headerFile,\n                        shapeStr=""[%d]"" * 3 % (self.totalNodes, self.numClasses, self.d))\n        writeMatAsArray(self.T, \'T\', self.headerFile,\n                        shapeStr=""[%d]"" * 3 % (T_m, 1, self.d))\n\n        self.writeFooter()\n\n\nclass BonsaiFloat(Bonsai):\n\n    # Float model is generated for for training dataset to profile the prediction\n    # Hence, X is trimmed down to remove outliers. Prediction profiling is\n    # performed on the trimmed X to generate more precise profile data\n    def transformDataset(self):\n        if usingTrainingDataset():\n            beforeLen = len(self.X)\n            beforeRange = matRange(self.X)\n\n            self.X, self.Y = trimMatrix(self.X, self.Y)\n\n            afterLen = len(self.X)\n            afterRange = matRange(self.X)\n\n            with open(self.infoFile, \'a\') as file:\n                file.write(""Old range of X: [%.6f, %.6f]\\n"" % (beforeRange))\n                file.write(""Trimmed the dataset from %d to %d data points; %.3f%%\\n"" % (\n                    beforeLen, afterLen, float(beforeLen - afterLen) / beforeLen * 100))\n                file.write(""New range of X: [%.6f, %.6f]\\n"" % (afterRange))\n\n    def transformModel(self):\n        pass\n\n    # Writing the model as a bunch of variables, arrays and matrices to a file\n    def writeModel(self):\n        lists = {\'mean\': self.mean}\n        mats = {}\n\n        # Sparse matrices are converted in to two arrays containing values and\n        # indices to reduce space\n        if useSparseMat():\n            Z_transp = matTranspose(self.Z)\n            Zval, Zidx = convertToSparse(Z_transp)\n            lists.update({\'Zval\': Zval, \'Zidx\': Zidx})\n        else:\n            mats[\'Z\'] = self.Z\n\n        mats.update({\'W\': self.W, \'V\': self.V, \'T\': self.T})\n\n        self.writeHeader()\n        writeVars({\'D\': self.D, \'d\': self.d, \'c\': self.numClasses, \'depth\': self.depth,\n                   \'totalNodes\': self.totalNodes, \'internalNodes\': self.internalNodes,\n                   \'tanh_limit\': Common.tanh_limit}, self.headerFile)\n\n        if forArduino():\n            writeListAsArray(self.X[0], \'X\', self.headerFile)\n            writeVars({\'Y\': self.Y[0][0]}, self.headerFile)\n\n        writeListsAsArray(lists, self.headerFile)\n        writeMatsAsArray(mats, self.headerFile)\n        self.writeFooter()\n'"
tools/SeeDot/seedot/compiler/converter/converter.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport os\n\nfrom seedot.compiler.converter.bonsai import *\nfrom seedot.compiler.converter.protonn import *\nfrom seedot.compiler.converter.util import *\n\nimport seedot.common as Common\n\n# Main file which sets the configurations and creates the corresponding object\n\n\nclass Converter:\n\n    def __init__(self, algo, version, datasetType, target, datasetOutputDir, outputDir):\n        setAlgo(algo)\n        setVersion(version)\n        setDatasetType(datasetType)\n        setTarget(target)\n\n        # Set output directories\n        setDatasetOutputDir(datasetOutputDir)\n        setOutputDir(outputDir)\n\n    def setInput(self, modelDir, trainingInput, testingInput):\n        setModelDir(modelDir)\n\n        # Type of normalization: 0 - No norm, 1 - MinMax norm, 2 - L2 norm, 3 -\n        # MeanVar norm\n        if os.path.isfile(os.path.join(modelDir, ""minMaxParams"")):\n            setNormType(1)\n        else:\n            setNormType(0)\n\n        setDatasetInput(trainingInput, testingInput)\n\n        self.inputSet = True\n\n    def run(self):\n        if self.inputSet != True:\n            raise Exception(""Set input paths before running Converter"")\n\n        algo, version = getAlgo(), getVersion()\n\n        if algo == Common.Algo.Bonsai and version == Common.Version.Fixed:\n            obj = BonsaiFixed()\n        elif algo == Common.Algo.Bonsai and version == Common.Version.Float:\n            obj = BonsaiFloat()\n        elif algo == Common.Algo.Protonn and version == Common.Version.Fixed:\n            obj = ProtonnFixed()\n        elif algo == Common.Algo.Protonn and version == Common.Version.Float:\n            obj = ProtonnFloat()\n        else:\n            assert False\n\n        obj.run()\n'"
tools/SeeDot/seedot/compiler/converter/protonn.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport os\n\nfrom seedot.compiler.converter.util import *\n\n# Class to read ProtoNN model dumps and generate input files (C header file and the compiler input)\n# The two classes ProtonnFixed and ProtonnFloat are for generating fixed model and float model respectively\n# The baseclass Protonn collects some of the common functions between them.\n\n\nclass Protonn:\n\n    def readDataset(self):\n        self.X, self.Y = readXandY()\n\n    def writeDataset(self):\n        writeMatAsCSV(self.X, os.path.join(getDatasetOutputDir(), ""X.csv""))\n        writeMatAsCSV(self.Y, os.path.join(getDatasetOutputDir(), ""Y.csv""))\n\n    def processDataset(self):\n        self.readDataset()\n        assert len(self.X) == len(self.Y)\n        self.transformDataset()\n        self.writeDataset()\n\n    def readNormFile(self):\n        if noNorm():\n            pass\n        elif minMaxNorm():\n            self.MinMax = readFileAsMat(os.path.join(\n                getModelDir(), ""minMaxParams""), ""\\t"", float)\n        else:\n            assert False\n\n    def readModel(self):\n        if os.path.isfile(os.path.join(getModelDir(), ""W"")):\n            return self.readModelAsTxt()\n        elif os.path.isfile(os.path.join(getModelDir(), ""W.npy"")):\n            return self.readModelAsNpy()\n        else:\n            assert False\n\n    def readModelAsNpy(self):\n        self.W = np.load(os.path.join(getModelDir(), ""W.npy""))\n        self.W = self.W.transpose().tolist()\n        self.B = np.load(os.path.join(getModelDir(), ""B.npy"")).tolist()\n        self.Z = np.load(os.path.join(getModelDir(), ""Z.npy"")).tolist()\n        self.gamma = np.load(os.path.join(getModelDir(), ""gamma.npy"")).tolist()\n        self.gamma = [[self.gamma]]\n        self.readNormFile()\n\n    def readModelAsTxt(self):\n        self.W = readFileAsMat(os.path.join(getModelDir(), ""W""), ""\\t"", float)\n        self.B = readFileAsMat(os.path.join(getModelDir(), ""B""), ""\\t"", float)\n        self.Z = readFileAsMat(os.path.join(getModelDir(), ""Z""), ""\\t"", float)\n        self.gamma = readFileAsMat(os.path.join(\n            getModelDir(), ""gamma""), ""\\t"", float)\n        self.readNormFile()\n\n    def validateNormFile(self):\n        if noNorm():\n            pass\n        elif minMaxNorm():\n            MinMax_m, MinMax_n = matShape(self.MinMax)\n            W_m, W_n = matShape(self.W)\n\n            assert MinMax_m == 2\n            assert MinMax_n == W_n\n        else:\n            assert False\n\n    def validateModel(self):\n        W_m, W_n = matShape(self.W)\n        B_m, B_n = matShape(self.B)\n        Z_m, Z_n = matShape(self.Z)\n        gamma_m, gamma_n = matShape(self.gamma)\n\n        assert W_m == B_m\n        assert B_n == Z_n\n        assert gamma_m == gamma_n == 1\n\n        self.validateNormFile()\n\n    def computeVars(self):\n        self.D = len(self.W[0])\n        self.d = len(self.W)\n        self.p = len(self.B[0])\n        self.c = len(self.Z)\n\n    # Precompute some values to speedup prediction\n    def formatModel(self):\n        # Precompute g2\n        self.g2 = self.gamma[0][0] * self.gamma[0][0]\n\n        if noNorm():\n            self.Norm = [1 for _ in range(len(self.W))]\n        elif minMaxNorm():\n            # Extract Min and Max\n            Min = []\n            Max = []\n            for i in range(len(self.MinMax[0])):\n                Min.append([self.MinMax[0][i]])\n                Max.append([self.MinMax[1][i]])\n\n            # Precompute W * (X-m)/(M-m) by absorbing m, M into W\n            for i in range(len(self.W)):\n                for j in range(len(self.W[0])):\n                    self.W[i][j] = self.W[i][j] / (Max[j][0] - Min[j][0])\n\n            self.Norm = matMul(self.W, Min)\n\n            assert len(self.Norm[0]) == 1\n            self.Norm = [x[0] for x in self.Norm]\n        else:\n            assert False\n\n        self.computeVars()\n\n    def computeModelSize(self):\n        if noNorm():\n            norm_num = 0\n        else:\n            norm_num = len(self.Norm)\n\n        if useSparseMat():\n            W_transp = matTranspose(self.W)\n            Wval, Widx = convertToSparse(W_transp)\n            W_num = len(Wval)\n            Widx_num = len(Widx)\n        else:\n            W_num = len(self.W) * len(self.W[0])\n            Widx_num = 0\n\n        B_num = len(self.B) * len(self.B[0])\n        Z_num = len(self.Z) * len(self.Z[0])\n\n        total_num = norm_num + W_num + B_num + Z_num\n\n        with open(self.infoFile, \'a\') as file:\n            file.write(""nnz values: %d\\n"" % (total_num))\n            file.write(""# indexes: %d\\n\\n"" % (Widx_num))\n            file.write(""---------------------\\n"")\n            file.write(""Model size comparison\\n"")\n            file.write(""---------------------\\n"")\n            file.write(""32-bit floating-points (in KB): %.3f\\n"" %\n                       (((total_num * 4) + (Widx_num * 2)) / 1024))\n            file.write(\n                ""(assuming 4 bytes for values and 2 bytes for indices)\\n\\n"")\n            file.write(""16-bit fixed-points (in KB): %.3f\\n"" %\n                       (((total_num * 2) + (Widx_num * 2)) / 1024))\n            file.write(\n                ""(assuming 2 bytes for values and 2 bytes for indices)\\n\\n"")\n            file.write(""32-bit fixed-points (in KB): %.3f\\n"" %\n                       (((total_num * 4) + (Widx_num * 2)) / 1024))\n            file.write(\n                ""(assuming 4 bytes for values and 2 bytes for indices)\\n"")\n            file.write(""--------------------------------------------\\n\\n"")\n\n    # Write macros and namespace declarations\n    def writeHeader(self):\n        with open(self.headerFile, \'a\') as file:\n            file.write(""#pragma once\\n\\n"")\n            if useSparseMat():\n                file.write(""#define P_SPARSE_W 1\\n"")\n            else:\n                file.write(""#define P_SPARSE_W 0\\n"")\n\n            if noNorm():\n                file.write(""#define P_NORM 0\\n\\n"")\n            elif minMaxNorm():\n                file.write(""#define P_NORM 1\\n\\n"")\n            else:\n                assert False\n\n            if forArduino():\n                s = \'model\'\n            else:\n                s = \'protonn_\' + getVersion()\n            file.write(""namespace %s {\\n\\n"" % (s))\n\n    def writeFooter(self):\n        with open(self.headerFile, \'a\') as file:\n            file.write(""}\\n"")\n\n    def processModel(self):\n        self.readModel()\n        self.validateModel()\n        self.formatModel()\n        self.transformModel()\n        self.computeModelSize()\n        self.writeModel()\n\n    def run(self):\n        if getVersion() == Common.Version.Float:\n            self.headerFile = os.path.join(\n                getOutputDir(), ""protonn_float_model.h"")\n        else:\n            self.headerFile = os.path.join(\n                getOutputDir(), ""seedot_fixed_model.h"")\n        self.inputFile = os.path.join(getOutputDir(), ""input.sd"")\n        self.infoFile = os.path.join(getOutputDir(), ""info.txt"")\n\n        open(self.headerFile, \'w\').close()\n        open(self.infoFile, \'w\').close()\n\n        if dumpDataset():\n            self.processDataset()\n\n        self.processModel()\n\n        if dumpDataset():\n            assert len(self.X[0]) == len(self.W[0])\n\n\nclass ProtonnFixed(Protonn):\n\n    # The X matrix is quantized using a scale factor computed from the training dataset.\n    # The range of X_train is used to compute the scale factor.\n    # Since the range of X_train depends on its distribution, the scale computed may be imprecise.\n    # To avoid this, any outliers in X_train is trimmed off using a threshold\n    # to get a more precise range and a more precise scale.\n    def transformDataset(self):\n        # If X itself is X_train, reuse it. Otherwise, read it from file\n        if usingTrainingDataset():\n            self.X_train = list(self.X)\n        else:\n            self.X_train, _ = readXandY(useTrainingSet=True)\n\n        # Trim some data points from X_train\n        self.X_train, _ = trimMatrix(self.X_train)\n\n        # Compute range and scale and quantize X\n        testDatasetRange = matRange(self.X)\n        self.trainDatasetRange = matRange(self.X_train)\n\n        scale = computeScale(*self.trainDatasetRange)\n        self.X, _ = scaleMat(self.X, scale)\n\n        with open(self.infoFile, \'a\') as file:\n            file.write(""Range of test dataset: [%.6f, %.6f]\\n"" % (\n                testDatasetRange))\n            file.write(""Range of training dataset: [%.6f, %.6f]\\n"" % (\n                self.trainDatasetRange))\n            file.write(""Test dataset scaled by: %d\\n\\n"" % (scale))\n\n    # Write the ProtoNN algorithm in terms of the compiler DSL.\n    # Compiler takes this as input and generates fixed-point code.\n    def genInputForCompiler(self):\n        with open(self.inputFile, \'w\') as file:\n            # Matrix declarations\n            file.write(""let X   = (%d, 1)   in [%.6f, %.6f] in\\n"" % (\n                (len(self.X[0]),) + self.trainDatasetRange))\n            file.write(""let W  = (%d, %d)    in [%.6f, %.6f] in\\n"" % (\n                (self.d, self.D) + matRange(self.W)))\n            file.write(""let B  = (%d, %d, 1) in [%.6f, %.6f] in\\n"" % (\n                (self.p, self.d) + matRange(self.B)))\n            file.write(""let Z  = (%d, %d, 1) in [%.6f, %.6f] in\\n"" % (\n                (self.p, self.c) + matRange(self.Z)))\n            if noNorm() == False:\n                file.write(""let norm = (%d, 1)   in [%.6f, %.6f] in\\n"" % (\n                    (self.d,) + listRange(self.Norm)))\n            file.write(""let g2 = %.6f in\\n\\n"" % (self.g2))\n\n            # Algorithm\n            if useSparseMat():\n                s = ""W |*| X""\n            else:\n                s = ""W * X""\n\n            if noNorm() == False:\n                s = s + "" - norm""\n\n            file.write(""let WX = %s in\\n"" % (s))\n            file.write(""let res = $(i = [0:%d])\\n"" % (self.p))\n            file.write(""(\\n"")\n            file.write(""\\tlet del = WX - B[i] in\\n"")\n            file.write(""\\tZ[i] * exp(-g2 * (del^T * del))\\n"")\n            file.write("") in\\n"")\n            file.write(""argmax(res)\\n"")\n\n    # Quantize the matrices\n    def transformModel(self):\n        if dumpDataset():\n            self.genInputForCompiler()\n\n        self.W, _ = scaleMat(self.W)\n        self.B, _ = scaleMat(self.B)\n        self.Z, _ = scaleMat(self.Z)\n        self.Norm, _ = scaleList(self.Norm)\n\n    # Writing the model as a bunch of variables, arrays and matrices to a file\n    def writeModel(self):\n        self.writeHeader()\n\n        if forArduino():\n            writeListAsArray(self.X[0], \'X\', self.headerFile)\n            writeVars({\'Y\': self.Y[0][0]}, self.headerFile)\n\n        if noNorm() == False:\n            writeListAsArray(self.Norm, \'norm\', self.headerFile,\n                             shapeStr=""[%d]"" * 2 % (self.d, 1))\n\n        # Sparse matrices are converted in to two arrays containing values and\n        # indices to reduce space\n        if useSparseMat():\n            W_transp = matTranspose(self.W)\n            Wval, Widx = convertToSparse(W_transp)\n            writeListsAsArray({\'Wval\': Wval, \'Widx\': Widx}, self.headerFile)\n        else:\n            writeMatAsArray(self.W, \'W\', self.headerFile)\n\n        # Transpose B and Z to satisfy the declarations in the generated DSL\n        # input\n        B_transp = matTranspose(self.B)\n        Z_transp = matTranspose(self.Z)\n\n        writeMatAsArray(B_transp, \'B\', self.headerFile,\n                        shapeStr=""[%d]"" * 3 % (self.p, self.d, 1))\n        writeMatAsArray(Z_transp, \'Z\', self.headerFile,\n                        shapeStr=""[%d]"" * 3 % (self.p, self.c, 1))\n\n        self.writeFooter()\n\n\nclass ProtonnFloat(Protonn):\n\n    # Float model is generated for for training dataset to profile the prediction\n    # Hence, X is trimmed down to remove outliers. Prediction profiling is\n    # performed on the trimmed X to generate more precise profile data\n    def transformDataset(self):\n        if usingTrainingDataset():\n            beforeLen = len(self.X)\n            beforeRange = matRange(self.X)\n\n            self.X, self.Y = trimMatrix(self.X, self.Y)\n\n            afterLen = len(self.X)\n            afterRange = matRange(self.X)\n\n            with open(self.infoFile, \'a\') as file:\n                file.write(""Old range of X: [%.6f, %.6f]\\n"" % (beforeRange))\n                file.write(""Trimmed the dataset from %d to %d data points; %.3f%%\\n"" % (\n                    beforeLen, afterLen, float(beforeLen - afterLen) / beforeLen * 100))\n                file.write(""New range of X: [%.6f, %.6f]\\n"" % (afterRange))\n\n    def transformModel(self):\n        pass\n\n    # Writing the model as a bunch of variables, arrays and matrices to a file\n    def writeModel(self):\n        mats = {}\n        lists = {}\n\n        if noNorm() == False:\n            lists[\'norm\'] = self.Norm\n\n        # Sparse matrices are converted in to two arrays containing values and\n        # indices to reduce space\n        if useSparseMat():\n            W_transp = matTranspose(self.W)\n            Wval, Widx = convertToSparse(W_transp)\n            lists.update({\'Wval\': Wval, \'Widx\': Widx})\n        else:\n            mats[\'W\'] = self.W\n\n        mats.update({\'B\': self.B, \'Z\': self.Z})\n\n        self.writeHeader()\n\n        if forArduino():\n            writeListAsArray(self.X[0], \'X\', self.headerFile)\n            writeVars({\'Y\': self.Y[0][0]}, self.headerFile)\n\n        writeVars({\'D\': self.D, \'d\': self.d, \'p\': self.p,\n                   \'c\': self.c, \'g2\': self.g2}, self.headerFile)\n        writeListsAsArray(lists, self.headerFile)\n        writeMatsAsArray(mats, self.headerFile)\n        self.writeFooter()\n'"
tools/SeeDot/seedot/compiler/converter/util.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport math\nimport numpy as np\nimport os\nfrom sklearn.datasets import load_svmlight_file\n\nimport seedot.common as Common\n\n# Utility functions commonly used by both Bonsai and Protonn\n\n# Configurations class which can be modified based on the requirement\n\n\nclass Config:\n    # If False, datasets are not generated again which reduces the processing\n    # time\n    dumpDataset = True\n    # To use sparse matrix representation whenever required\n    sparseMat = True\n\n\n# Bonsai or Protonn\ndef getAlgo():\n    return Config.algo\n\n\ndef setAlgo(algo: str):\n    Config.algo = algo\n\n\n# Fixed-point or float-point\ndef getVersion():\n    return Config.version\n\n\ndef setVersion(version: str):\n    Config.version = version\n\n\n# training or testing dataset\ndef getDatasetType():\n    return Config.datasetType\n\n\ndef setDatasetType(datasetType: str):\n    Config.datasetType = datasetType\n\n\ndef usingTrainingDataset():\n    return getDatasetType() == Common.DatasetType.Training\n\n\n# Arduino code or desktop code (aka plain C++ code)\ndef getTarget():\n    return Config.target\n\n\ndef setTarget(target: str):\n    Config.target = target\n\n\ndef forArduino():\n    return getTarget() == Common.Target.Arduino\n\n\ndef getDatasetOutputDir():\n    return Config.datasetOuputDir\n\n\ndef setDatasetOutputDir(datasetOuputDir):\n    Config.datasetOuputDir = datasetOuputDir\n\n\ndef getOutputDir():\n    return Config.outputDir\n\n\ndef setOutputDir(outputDir: str):\n    Config.outputDir = outputDir\n\n\ndef getModelDir():\n    return Config.modelDir\n\n\ndef setModelDir(modelDir):\n    Config.modelDir = modelDir\n\n\ndef setDatasetInput(trainingFile, testingFile):\n    Config.trainingFile = trainingFile\n    Config.testingFile = testingFile\n\n\ndef usingLibSVM():\n    return Common.inputFileType == ""libsvm""\n\n\ndef usingTSV():\n    return Common.inputFileType == ""tsv""\n\n\ndef usingCSV():\n    return Common.inputFileType == ""csv""\n\n\ndef usingNPY():\n    return Common.inputFileType == ""npy""\n\n\ndef dumpDataset():\n    return Config.dumpDataset\n\n\ndef useSparseMat():\n    return Config.sparseMat\n\n\ndef getNormType():\n    return Config.norm\n\n\ndef setNormType(normType):\n    Config.norm = normType\n\n\ndef noNorm():\n    return getNormType() == 0\n\n\ndef minMaxNorm():\n    return getNormType() == 1\n\n\ndef l2Norm():\n    return getNormType() == 2\n\n\ndef meanVarNorm():\n    return getNormType() == 3\n\n\ndef getMaxInt():\n    return (2 ** (Common.wordLength - 1)) - 1\n\n\n# Format specifiers for various datatypes\ndef getDataType(num):\n    if isinstance(num, int):\n        return \'MYINT\', \'%d\'\n    elif isinstance(num, float):\n        return \'float\', \'%.6f\'\n    else:\n        raise Exception(\n            ""Format specifier not found for the type: "" + str(type(num)))\n\n\ndef matMin(mat):\n    return min([min(x) for x in mat])\n\n\ndef matMax(mat):\n    return max([max(x) for x in mat])\n\n\ndef matRange(mat):\n    return matMin(mat), matMax(mat)\n\n\ndef matShape(mat):\n    return len(mat), len(mat[0])\n\n\ndef listRange(list):\n    return min(list), max(list)\n\n\ndef readXandY(useTrainingSet=False):\n    train_ext = os.path.splitext(Config.trainingFile)[1]\n    test_ext = os.path.splitext(Config.testingFile)[1]\n\n    if train_ext == test_ext == "".npy"":\n        return readXandYasNPY(useTrainingSet)\n    elif train_ext == test_ext == "".tsv"":\n        return readXandYasTSV(useTrainingSet)\n    elif train_ext == test_ext == "".csv"":\n        return readXandYasCSV(useTrainingSet)\n    elif train_ext == test_ext == "".txt"":\n        return readXandYasLibSVM(useTrainingSet)\n    else:\n        assert False\n\n\ndef zeroIndexLabels(Y):\n    lab = np.array(Y)\n    lab = lab.astype(\'uint8\')\n    lab = np.array(lab) - min(lab)\n    return lab.tolist()\n\n\ndef readXandYasLibSVM(trainingDataset):\n    if trainingDataset == True or usingTrainingDataset() == True:\n        inputFile = Config.trainingFile\n    else:\n        inputFile = Config.testingFile\n\n    data = load_svmlight_file(inputFile)\n\n    X = data[0].todense().tolist()\n\n    Y = data[1].tolist()\n    Y = list(map(int, Y))\n    Y = [[classID] for classID in Y]\n\n    Y = zeroIndexLabels(Y)\n\n    return X, Y\n\n\ndef readXandYasTSV(trainingDataset):\n    \'\'\'\n    In TSV format, the input is a file containing tab seperated values.\n    In each row of the TSV file, the class ID will be the first entry followed by the feature vector of the data point\n    The file is initially read as a matrix and later X and Y are extracted\n    \'\'\'\n    if trainingDataset == True or usingTrainingDataset() == True:\n        mat = readFileAsMat(Config.trainingFile, ""\\t"", float)\n    else:\n        mat = readFileAsMat(Config.testingFile, ""\\t"", float)\n    X, Y = extractXandYfromMat(mat)\n\n    Y = zeroIndexLabels(Y)\n\n    return X, Y\n\n\ndef extractXandYfromMat(mat):\n    \'\'\'\n    The first entry is cast to int (since it is the class ID) and used as Y\n    The remaining entries are part of X\n    \'\'\'\n    X = []\n    Y = []\n    for i in range(len(mat)):\n        classID = int(mat[i][0])\n\n        temp = mat[i]\n        temp.pop(0)\n\n        X.append(temp)\n        Y.append([classID])\n    return X, Y\n\n\ndef readXandYasCSV(trainingDataset):\n    \'\'\'\n    In CSV format, the input is a folder containing two files ""X.csv"" and ""Y.csv""\n    Each file contains comma seperated values.\n    X contains feature vector and Y contains the class ID of each data point\n    \'\'\'\n    if trainingDataset == True or usingTrainingDataset() == True:\n        X = readFileAsMat(os.path.join(\n            Config.trainingFile, ""X.csv""), "", "", float)\n        Y = readFileAsMat(os.path.join(\n            Config.trainingFile, ""Y.csv""), "", "", int)\n    else:\n        X = readFileAsMat(os.path.join(\n            Config.testingFile, ""X.csv""), "", "", float)\n        Y = readFileAsMat(os.path.join(Config.testingFile, ""Y.csv""), "", "", int)\n\n    Y = zeroIndexLabels(Y)\n\n    return X, Y\n\n\ndef readXandYasNPY(trainingDataset):\n    \'\'\'\n    In TSV format, the input is a file containing tab seperated values.\n    In each row of the TSV file, the class ID will be the first entry followed by the feature vector of the data point\n    The file is initially read as a matrix and later X and Y are extracted\n    \'\'\'\n    if trainingDataset == True or usingTrainingDataset() == True:\n        mat = np.load(Config.trainingFile).tolist()\n    else:\n        mat = np.load(Config.testingFile).tolist()\n    X, Y = extractXandYfromMat(mat)\n\n    Y = zeroIndexLabels(Y)\n\n    return X, Y\n\n\n# Parse the file using the delimited and store it as a matrix\ndef readFileAsMat(fileName: str, delimiter: str, dataType):\n    mat = []\n    rowLength = -1\n\n    with open(fileName, \'r\') as f:\n        for line in f:\n            # If the delimiter is \' \', use split() without parameters to parse\n            # the line even if there are consecutive spaces\n            if delimiter == "" "":\n                entries = line.strip().split()\n            else:\n                entries = line.strip().split(delimiter)\n\n            if rowLength == -1:\n                rowLength = len(entries)\n\n            assert rowLength == len(entries)\n\n            # Cast each entry to the datatype specified\n            row = list(map(dataType, entries))\n            mat.append(row)\n    return mat\n\n\n# Write the matrix as a CSV file\ndef writeMatAsCSV(mat, fileName: str):\n    m, n = matShape(mat)\n    _, formatSpecifier = getDataType(mat[0][0])\n\n    with open(fileName, \'w\') as file:\n        for i in range(m):\n            for j in range(n):\n                file.write(formatSpecifier % mat[i][j])\n                if j != (n - 1):\n                    file.write("", "")\n            file.write(""\\n"")\n\n\ndef writeMatsAsArray(mats: dict, fileName: str, shapeStr=None):\n    for key in mats:\n        writeMatAsArray(mats[key], key, fileName, shapeStr)\n\n\ndef writeMatAsArray(mat, name: str, fileName: str, shapeStr=None):\n    m, n = matShape(mat)\n\n    dataType, formatSpecifier = getDataType(mat[0][0])\n\n    # Add the \'f\' specifier for each float to supress compiler warnings\n    if dataType == ""float"":\n        formatSpecifier += \'f\'\n\n    # If custom matrix shape is not specified, use default\n    if shapeStr == None:\n        shapeStr = ""[%d]"" * 2 % (m, n)\n\n    # Use Arduino pragma\n    arduinoStr = """"\n    if forArduino():\n        arduinoStr = ""PROGMEM ""\n\n    with open(fileName, \'a\') as file:\n        file.write(\'const %s%s %s%s = {\\n\' %\n                   (arduinoStr, dataType, name, shapeStr))\n\n        for row in mat:\n            file.write(\'\\t\')\n            for cell in row:\n                file.write((formatSpecifier + "", "") % cell)\n            file.write(\'\\n\')\n        file.write(\'};\\n\\n\')\n\n\ndef writeListsAsArray(lists: dict, fileName: str, shapeStr=None):\n    for key in lists:\n        writeListAsArray(lists[key], key, fileName, shapeStr)\n\n\ndef writeListAsArray(list, name: str, fileName: str, shapeStr=None):\n    n = len(list)\n\n    dataType, formatSpecifier = getDataType(list[0])\n\n    # Add the \'f\' specifier for each float to supress compiler warnings\n    if dataType == ""float"":\n        formatSpecifier += \'f\'\n\n    # If custom matrix shape is not specified, use default\n    if shapeStr == None:\n        shapeStr = ""[%d]"" % (n)\n\n    # Use Arduino pragma\n    arduinoStr = """"\n    if forArduino():\n        arduinoStr = ""PROGMEM ""\n\n    with open(fileName, \'a\') as file:\n        file.write(\'const %s%s %s%s = {\\n\' %\n                   (arduinoStr, dataType, name, shapeStr))\n\n        file.write(\'\\t\')\n        for cell in list:\n            file.write((formatSpecifier + "", "") % cell)\n        file.write(\'\\n};\\n\\n\')\n\n\ndef hex2(n):\n    return hex(n & 0xffffffff)\n\n\ndef writeListsAsLUTs(lists: dict, dirName: str):\n    os.makedirs(dirName, exist_ok=True)\n\n    for key in lists:\n        fileName = os.path.join(dirName, (key + \'.lut\'))\n        writeListAsLUT(lists[key], key, fileName)\n\n\ndef writeListAsLUT(list, name: str, fileName: str):\n    n = len(list)\n    file = open(fileName, ""w"")\n    for i in list:\n        file.write(hex2(i)[2:])\n        file.write(\'\\n\')\n    file.close()\n\n\ndef writeVars(vars: dict, fileName: str):\n    with open(fileName, \'a\') as file:\n        for key in vars:\n            dataType, formatSpecifier = getDataType(vars[key])\n\n            # Add the \'f\' specifier for each float to supress compiler warnings\n            if dataType == ""float"":\n                formatSpecifier += \'f\'\n                file.write((""const %s %s = "" + formatSpecifier + "";\\n"") %\n                           (dataType, key, vars[key]))\n            # todo: Why is this required when I am already taking care of\n            # writing the datatype?\n            else:\n                file.write((""const %s %s = "" + formatSpecifier + "";\\n"") %\n                           (""int"", key, vars[key]))\n        file.write(""\\n"")\n\n\ndef matMul(X, Y):\n    X_m, X_n = matShape(X)\n    Y_m, Y_n = matShape(Y)\n\n    assert X_n == Y_m\n\n    Z = [[0 for _ in range(Y_n)] for _ in range(X_m)]\n\n    for i in range(X_m):\n        for j in range(Y_n):\n            sum = 0\n            for k in range(X_n):\n                sum += X[i][k] * Y[k][j]\n            Z[i][j] = sum\n    return Z\n\n\ndef matTranspose(mat):\n    m, n = matShape(mat)\n    transp = [[0 for _ in range(m)] for _ in range(n)]\n\n    for i in range(m):\n        for j in range(n):\n            transp[j][i] = mat[i][j]\n    return transp\n\n\ndef convertToSparse(mat):\n    \'\'\'\n    Convert a sparse matrix into two arrays M_val and M_idx\n    M_val contains all the non-zero elements in the matrix\n    M_idx contains the row index of each non-zero element in the matrix which are delimited at each column using \'0\'\n    \'\'\'\n    m, n = matShape(mat)\n\n    matVal = []\n    matIdx = []\n\n    for i in range(m):\n        for j in range(n):\n            if mat[i][j] != 0:\n                matVal.append(mat[i][j])\n                matIdx.append(int(j + 1))\n        matIdx.append(int(0))\n\n    return matVal, matIdx\n\n\n# Custom function to compute the maximum scaling factor which can fit M\n# into an integer of Common.wordLength length\ndef computeScale(m, M):\n    maxAbs = max(abs(m), abs(M))\n    return int(math.ceil(math.log2(maxAbs) - math.log2((1 << (Common.wordLength - 2)) - 1)))\n\n\n# Scaling the matrix using the scaling factor computed\ndef scaleMat(mat, scale=None):\n    if scale == None:\n        scale = computeScale(*matRange(mat))\n\n    scaledMat = [[int(math.ldexp(cell, -scale))\n                  for cell in row] for row in mat]\n\n    return scaledMat, scale\n\n\n# Scaling an array using the scaling factor computed\ndef scaleList(list, scale=None):\n    if scale == None:\n        scale = computeScale(*listRange(list))\n\n    scaledList = [int(math.ldexp(cell, -scale)) for cell in list]\n\n    return scaledList, scale\n\n\n# Remove some data points in X whose value is an outlier compared to the\n# distribution of X\ndef trimMatrix(X, Y=None):\n    # The matrix is trimmed only if the range of the matrix is more than this threshold\n    # Used to skip trimming when the range is already low\n    matThreshold = 2.1\n\n    # The percentage of data points used to performa trimming\n    ratio = 0.9\n\n    # Skip trimming if within the threshold\n    matMin, matmax = matRange(X)\n    if abs(matmax - matMin) < matThreshold:\n        return X, Y\n\n    # Find the max of each data point\n    rowMax = []\n    for i in range(len(X)):\n        m, M = listRange(X[i])\n        maxAbs = max(abs(m), abs(M))\n        rowMax.append(maxAbs)\n\n    # Sort and find the trim threshold\n    rowMaxSorted = list(rowMax)\n    rowMaxSorted.sort()\n    trimThreshold = rowMaxSorted[int(len(X) * ratio) - 1]\n\n    # Only store data points which are beyond the threshold\n    X_trim = []\n    Y_trim = []\n    for i in range(len(rowMax)):\n        if rowMax[i] < trimThreshold:\n            X_trim.append(X[i])\n            if Y != None:\n                Y_trim.append(Y[i])\n\n    return X_trim, Y_trim\n'"
tools/SeeDot/seedot/compiler/ir/__init__.py,0,b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n'
tools/SeeDot/seedot/compiler/ir/ir.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nfrom enum import Enum\nimport numpy as np\n\nimport seedot.common as Common\nfrom seedot.util import *\n\n\nclass Op:\n    Op = Enum(\'Op\', \'+ - * / << >> & | ^ ~ ! && || < <= > >= == !=\')\n    Op.print = lambda self, writer: writer.printf(\'%s\', self.name)\n    Op.op_list = lambda op_str: list(map(lambda x: Op.Op[x], op_str.split()))\n\n\nclass Expr:\n    pass\n\n\nclass IntExpr(Expr):\n    pass\n\n\nclass BoolExpr(Expr):\n    pass\n\n\nclass Int(IntExpr):\n\n    @staticmethod\n    def max():\n        return DataType.getMax()\n\n    @staticmethod\n    def negMax():\n        return DataType.getNegMax()\n\n    def __init__(self, n: int):\n        self.n = DataType.getInt(n)\n\n    def subst(self, from_idf: str, to_e: Expr):\n        return Int(self.n)\n\n\nclass Var(IntExpr):\n\n    def __init__(self, idf: str, idx: list=[], inputVar=False):\n        self.idf = idf\n        self.idx = idx\n        self.inputVar = inputVar\n\n    def subst(self, from_idf: str, to_e: Expr):\n        idx_new = list(map(lambda e: e.subst(from_idf, to_e), self.idx))\n        if self.idf != from_idf:\n            return Var(self.idf, idx_new, self.inputVar)\n        else:\n            if isinstance(to_e, Var):\n                return Var(to_e.idf, to_e.idx + idx_new, to_e.inputVar and self.inputVar)\n            elif isinstance(to_e, Int):\n                return to_e\n            else:\n                assert False\n\n\nclass Bool(BoolExpr):\n\n    def __init__(self, b: bool):\n        self.b = b\n\n    def subst(self, from_idf: str, to_e: Expr):\n        return Bool(self.b)\n\n\nclass IntUop(IntExpr):\n\n    def __init__(self, op: Op.Op, e: IntExpr):\n        assert op in Op.Op.op_list(\'- ~\')\n        self.op = op\n        self.e = e\n\n    def subst(self, from_idf: str, to_e: Expr):\n        return IntUop(self.op, self.e.subst(from_idf, to_e))\n\n\nclass IntBop(IntExpr):\n\n    def __init__(self, e1: IntExpr, op: Op.Op, e2: IntExpr):\n        assert op in Op.Op.op_list(\'+ - * / << >> & | ^\')\n        self.e1 = e1\n        self.op = op\n        self.e2 = e2\n\n    def subst(self, from_idf: str, to_e: Expr):\n        return IntBop(self.e1.subst(from_idf, to_e), self.op, self.e2.subst(from_idf, to_e))\n\n\nclass BoolUop(BoolExpr):\n\n    def __init__(self, op: Op.Op, e: BoolExpr):\n        assert op in Op.Op.op_list(\'\')\n        self.op = op\n        self.e = e\n\n    def subst(self, from_idf: str, to_e: Expr):\n        return BoolUop(self.op, self.e.subst(from_idf, to_e))\n\n\nclass BoolBop(BoolExpr):\n\n    def __init__(self, e1: BoolExpr, op: Op.Op, e2: BoolExpr):\n        assert op in Op.Op.op_list(\'&& ||\')\n        self.e1 = e1\n        self.op = op\n        self.e2 = e2\n\n    def subst(self, from_idf: str, to_e: Expr):\n        return BoolBop(self.e1.subst(from_idf, to_e), self.op, self.e2.subst(from_idf, to_e))\n\n\nclass BoolCop(BoolExpr):\n\n    def __init__(self, e1: IntExpr, op: Op.Op, e2: IntExpr):\n        assert op in Op.Op.op_list(\'< <= > >= == !=\')\n        self.e1 = e1\n        self.op = op\n        self.e2 = e2\n\n    def subst(self, from_idf: str, to_e: Expr):\n        return BoolCop(self.e1.subst(from_idf, to_e), self.op, self.e2.subst(from_idf, to_e))\n\n\nclass CExpr(Expr):\n\n    def __init__(self, cond: BoolExpr, et: Expr, ef: Expr):\n        self.cond = cond\n        self.et = et\n        self.ef = ef\n\n    def subst(self, from_idf: str, to_e: Expr):\n        return CExpr(self.cond.subst(from_idf, to_e), self.et.subst(from_idf, to_e), self.ef.subst(from_idf, to_e))\n\n\nclass Exp(IntExpr):\n\n    def __init__(self, e: IntExpr):\n        self.e = e\n\n    def subst(self, from_idf: str, to_e: Expr):\n        return Exp(self.e.subst(from_idf, to_e))\n\n\nclass TypeCast(IntExpr):\n\n    def __init__(self, type, expr: Expr):\n        self.type = type\n        self.expr = expr\n\n    def subst(self, from_idf: str, to_e: Expr):\n        return TypeCast(self.type, self.expr.subst(from_idf, to_e))\n\n\nclass Cmd:\n    pass\n\n\nclass CmdList:\n    pass\n\n\nclass Assn(Cmd):\n\n    def __init__(self, var: Var, e: Expr):\n        self.var = var\n        self.e = e\n\n    def subst(self, from_idf: str, to_e: Expr):\n        return Assn(self.var.subst(from_idf, to_e), self.e.subst(from_idf, to_e))\n\n\nclass If(Cmd):\n\n    def __init__(self, cond: Expr, trueCmds: CmdList, falseCmds: CmdList=[]):\n        self.cond = cond\n        self.trueCmds = trueCmds\n        self.falseCmds = falseCmds\n\n    def subst(self, from_idf: str, to_e: Expr):\n        trueCmdsNew = list(\n            map(lambda cmd: cmd.subst(from_idf, to_e), self.trueCmds))\n        falseCmdsNew = list(\n            map(lambda cmd: cmd.subst(from_idf, to_e), self.falseCmds))\n        return If(self.cond.subst(from_idf, to_e), trueCmdsNew, falseCmdsNew)\n\n\nclass For(Cmd):\n\n    def __init__(self, var: Var, st: int, cond: Expr, cmd_l: CmdList, fac=0):\n        self.var = var\n        self.st = DataType.getInt(st)\n        self.cond = cond\n        self.cmd_l = cmd_l\n        self.factor = fac\n\n    def subst(self, from_idf: str, to_e: Expr):\n        cmd_l_new = list(\n            map(lambda cmd: cmd.subst(from_idf, to_e), self.cmd_l))\n        return For(self.var, self.st, self.cond.subst(from_idf, to_e), cmd_l_new, self.factor)\n\n\nclass While(Cmd):\n\n    def __init__(self, expr: BoolExpr, cmds: CmdList):\n        self.expr = expr\n        self.cmds = cmds\n\n    def subst(self, from_idf: str, to_e: Expr):\n        cmds_new = list(map(lambda cmd: cmd.subst(from_idf, to_e), self.cmds))\n        return While(self.expr.subst(from_idf, to_e), cmds_new)\n\n\nclass FuncCall(Cmd):\n\n    def __init__(self, name, argList):\n        self.name = name\n        self.argList = argList\n\n    def subst(self, from_idf: str, to_e: Expr):\n        argList_new = dict(\n            map(lambda cmd: (cmd[0].subst(from_idf, to_e), cmd[1]), self.argList.items()))\n        return FuncCall(self.name, argList_new)\n\n\nclass Memset(Cmd):\n\n    def __init__(self, e: Var, len: int, dim=1, lens=[]):\n        self.e = e\n        self.len = len\n        self.dim = dim\n        self.lens = lens\n\n    def subst(self, from_idf: str, to_e: Expr):\n        return Memset(self.e.subst(from_idf, to_e), self.len)\n\n\nclass Print(Cmd):\n\n    def __init__(self, expr: Expr):\n        self.expr = expr\n\n    def subst(self, from_idf: str, to_e: Expr):\n        return Print(self.expr.subst(from_idf, to_e))\n\n\nclass PrintAsFloat(Cmd):\n\n    def __init__(self, expr: Expr, expnt: int):\n        self.expr = expr\n        self.expnt = expnt\n\n    def subst(self, from_idf: str, to_e: Expr):\n        return PrintAsFloat(self.expr.subst(from_idf, to_e), self.expnt)\n\n\nclass Comment(Cmd):\n\n    def __init__(self, msg):\n        self.msg = msg\n\n    def subst(self, from_idf: str, to_e: Expr):\n        return Comment(self.msg)\n\n\nclass Prog:\n\n    def __init__(self, cmd_l: CmdList, resource=0):\n        self.cmd_l = cmd_l\n        self.resource = resource\n\n    def subst(self, from_idf: str, to_e: Expr):\n        cmd_l_new = list(\n            map(lambda cmd: cmd.subst(from_idf, to_e), self.cmd_l))\n        return Prog(cmd_l_new, self.resource)\n\n\nclass DataType:\n    intType = {Common.Target.Arduino: {8: np.int8, 16: np.int16, 32: np.int32, 64: np.int64},\n               Common.Target.X86: {8: np.int8, 16: np.int16, 32: np.int32, 64: np.int64}\n               }\n    intStr = {Common.Target.Arduino: \'MYINT\',\n              Common.Target.X86: \'MYINT\'\n              }\n    floatStr = ""float""\n\n    @staticmethod\n    def getInt(x: int):\n        target = getTarget()\n        wordLen = Common.wordLength\n        return DataType.intType[target][wordLen](x)\n\n    @staticmethod\n    def getIntClass():\n        target = getTarget()\n        wordLen = Common.wordLength\n        return DataType.intType[target][wordLen]\n\n    @staticmethod\n    def getIntStr():\n        target = getTarget()\n        return DataType.intStr[target]\n\n    @staticmethod\n    def getFloatStr():\n        return DataType.floatStr\n\n    @staticmethod\n    def getMax():\n        intClass = DataType.getIntClass()\n        return intClass(np.iinfo(intClass).max)\n\n    @staticmethod\n    def getNegMax():\n        intClass = DataType.getIntClass()\n        return intClass(np.iinfo(intClass).min)\n'"
tools/SeeDot/seedot/compiler/ir/irBuilder.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport numpy as np\nimport operator\nimport os\n\nfrom seedot.compiler.antlr.seedotParser import seedotParser as SeeDotParser\n\nimport seedot.compiler.ast.ast as AST\nfrom seedot.compiler.ast.astVisitor import ASTVisitor\n\nimport seedot.compiler.ir.ir as IR\nimport seedot.compiler.ir.irUtil as IRUtil\n\nimport seedot.common as Common\nimport seedot.compiler.type as Type\nfrom seedot.util import *\n\n\nclass IRBuilder(ASTVisitor):\n\n    def __init__(self):\n\n        self.profileLoaded = False\n\n        if getMaxScale() == None:\n            # data-driven parameters\n            inputFile = getProfileLogFile()\n\n            assert os.path.isfile(inputFile)\n\n            data = []\n            with open(inputFile, \'r\') as f:\n                for line in f:\n                    entries = line.strip().split("", "")\n                    row = list(map(float, entries))\n                    data.append(row)\n\n            [min_all, max_all] = data[0]\n\n            self.MAX_SCALE = self.getScale(max(abs(min_all), abs(max_all)))\n        else:\n            self.MAX_SCALE = getMaxScale()\n\n        self.expTables = {}\n\n        # Counter for temp variables\n        self.counter_var = 0\n        self.counter_iter = 0\n\n        # idf of vars that need to be init\'ed\n        self.globalVars = []\n\n        # Global variables\n        self.decls = {}\n        self.scales = {}\n        self.intvs = {}\n        self.cnsts = {}\n\n    def readProfileFile(self):\n        if self.profileLoaded == True:\n            return\n\n        self.profileLoaded = True\n\n        # data-driven parameters\n        inputFile = getProfileLogFile()\n\n        data = []\n        with open(inputFile, \'r\') as f:\n            for line in f:\n                entries = line.strip().split("", "")\n                row = list(map(float, entries))\n                data.append(row)\n\n        [min_exp, max_exp] = data[1]\n        #[min_exp, max_exp] = [0.022, 15.012]\n\n        expB = getExpBitLength()\n\n        # Data for computing exp\n        self.expRange = [min_exp, max_exp]\n        self.expB = expB\n        self.expTableShape = [2, 2 ** self.expB]\n\n        self.MAX_VAL_EXP = max_exp\n\n    def visitInt(self, node: AST.Int):\n        val = node.value\n\n        prog = IR.Prog([])\n        expr = IR.Int(val)\n\n        return (prog, expr)\n\n    def visitFloat(self, node: AST.Float):\n        val = node.value\n        scale = self.getScale(abs(val))\n        intv = self.getInterval(scale, val, val)\n        val_int = IR.DataType.getInt(np.ldexp(val, -scale))\n\n        prog = IR.Prog([])\n        expr = self.getTempVar()\n\n        self.decls[expr.idf] = node.type\n        self.scales[expr.idf] = scale\n        self.intvs[expr.idf] = intv\n        self.cnsts[expr.idf] = val_int\n\n        return (prog, expr)\n\n    def visitId(self, node: AST.ID):\n        idf = node.name\n\n        prog = IR.Prog([])\n\n        expr = IR.Var(idf, inputVar=True if idf in self.globalVars else False)\n\n        return (prog, expr)\n\n    def visitDecl(self, node: AST.Decl):\n        minVal, maxVal = node.range\n\n        scale = self.getScale(max(abs(minVal), abs(maxVal)))\n        intv = self.getInterval(scale, minVal, maxVal)\n\n        prog = IR.Prog([])\n        expr = self.getTempVar()\n        expr.inputVar = True\n\n        self.scales[expr.idf] = scale\n        self.intvs[expr.idf] = intv\n\n        return (prog, expr)\n\n    # out = in ^ T\n    def visitTransp(self, node: AST.Transp):\n\n        (prog_in, expr_in) = self.visit(node.expr)\n\n        expr_out = self.getTempVar()\n\n        type_out = node.type\n        [I, J] = type_out.shape\n\n        scale_out = self.scales[expr_in.idf]\n        intv_out = self.intvs[expr_in.idf]\n\n        expr_in.inputVar = False\n        expr_out.inputVar = False\n\n        cmd0 = IR.Comment(expr_in.idf + ""^T"")\n\n        funcCall = IR.FuncCall(""Transpose"", {\n            expr_in: ""A"",\n            expr_out: ""B"",\n            IR.Int(I): ""I"",\n            IR.Int(J): ""J""\n        })\n\n        prog_transp = IR.Prog([cmd0, funcCall])\n\n        prog_out = IRUtil.concatPrograms(prog_in, prog_transp)\n\n        self.decls[expr_out.idf] = type_out\n        self.scales[expr_out.idf] = scale_out\n        self.intvs[expr_out.idf] = intv_out\n\n        return (prog_out, expr_out)\n\n    # out = reshape(in, shape, order)\n    def visitReshape(self, node: AST.Reshape):\n\n        (prog_in, expr_in) = self.visit(node.expr)\n\n        \'\'\'\n\t\treshape(A, n, h, w)\n\n\t\tcmd1:  t1 = t2 = 0;\n\t\tloop2: for n in 0:N:\n\t\t         for h in 0:H:\n\t\t           for w in 0:W:\n\t\tcmd3:        B[n][h][w] = A[t1][t2][t3]\n\t\tcmd4:        t3++;\n\t\tcmd5:        if (t3 == WW)\n\t\t               t3 = 0;\n\t\t               t2++;\n\t\t               if (t2 == HH)\n\t\t                 t2 = 0;\n\t\t                 t1++;\n\t\t\'\'\'\n\n        type_in = node.expr.type\n        type_out = node.type\n\n        # Compute scaling factors\n        scale_out = self.scales[expr_in.idf]\n        intv_out = self.intvs[expr_in.idf]\n\n        # Declare variables\n        expr_out = self.getTempVar()\n\n        iters_in = self.getTempIterators(type_in.dim)\n        iters_out = self.getTempVars(type_out.dim)\n\n        # Initialize to 0\n        cmd1 = [IR.Assn(var, IRUtil.zero) for var in iters_out]\n\n        # Incrementing the first index\n        first_iter = iters_out[0]\n        cmd4 = IRUtil.incCmd(first_iter)\n\n        # Incrementing other indices using a loop\n        cmd5 = [cmd4]\n        for i in range(1, type_out.dim):\n            curr_iter = iters_out[i]\n            curr_size = IR.Int(type_out.shape[i])\n            cmd5 = [IRUtil.incCmd(curr_iter), IR.If(IRUtil.eq(curr_iter, curr_size), [\n                IRUtil.initVarToZero(curr_iter)] + cmd5)]\n\n        # Outer loop\n        loopShape = []\n        loopIters = []\n        for order in node.order:\n            order = order - 1\n            loopShape.append(type_in.shape[order])\n            loopIters.append(iters_in[order])\n\n        loop2 = IRUtil.loop(loopShape, loopIters, [IR.Assn(IRUtil.addIndex(\n            expr_out, iters_out), IRUtil.addIndex(expr_in, iters_in))] + cmd5)\n\n        # Finalize\n        comment = IR.Comment(\n            ""reshape("" + expr_in.idf + "", "" + \', \'.join(str(e) for e in type_out.shape) + "")"")\n        prog_reshape = IR.Prog([comment] + cmd1 + loop2)\n\n        prog_out = IRUtil.concatPrograms(prog_in, prog_reshape)\n\n        # Update context\n        self.decls[expr_out.idf] = type_out\n        self.scales[expr_out.idf] = scale_out\n        self.intvs[expr_out.idf] = intv_out\n\n        # Update declarations\n        self.decls.update(dict((var.idf, Type.Int()) for var in iters_out))\n\n        return (prog_out, expr_out)\n\n    # out = maxpool(in, stride)\n    def visitMaxpool(self, node: AST.Maxpool):\n\n        (prog_in, expr_in) = self.visit(node.expr)\n\n        type_out = node.type\n        stride = node.dim\n\n        # Compute scaling factor\n        scale_out = self.scales[expr_in.idf]\n        intv_out = self.intvs[expr_in.idf]\n\n        # Declare variables\n        expr_out = self.getTempVar()\n\n        [N, H, W, C] = node.expr.type.shape\n\n        expr_in.inputVar = False\n        expr_out.inputVar = False\n\n        cmd0 = IR.Comment(""maxpool("" + expr_in.idf + "", "" + str(stride) + "")"")\n\n        funcCall = IR.FuncCall(""Maxpool"", {\n            expr_in: ""A"",\n            expr_out: ""B"",\n            IR.Int(N): ""N"",\n            IR.Int(H): ""H"",\n            IR.Int(W): ""W"",\n            IR.Int(C): ""C"",\n            IR.Int(stride): ""stride""\n        })\n\n        prog_maxpool = IR.Prog([cmd0, funcCall])\n\n        prog_out = IRUtil.concatPrograms(prog_in, prog_maxpool)\n\n        # Update declarations\n        self.decls[expr_out.idf] = type_out\n        self.scales[expr_out.idf] = scale_out\n        self.intvs[expr_out.idf] = intv_out\n\n        return (prog_out, expr_out)\n\n    # out = in[index]\n    def visitIndex(self, node: AST.Index):\n\n        (prog_in, expr_in) = self.visit(node.expr)\n        (prog_idx, expr_idx) = self.visit(node.index)\n\n        prog_out = IRUtil.concatPrograms(prog_in, prog_idx)\n        expr_out = IRUtil.addIndex(expr_in, [expr_idx])\n\n        return (prog_out, expr_out)\n\n    # out = func(in_A, in_B, in_C, ...)\n    def visitFuncCall(self, node: AST.FuncCall):\n        # Assumes that the output of the uninterpretted function call is stored in one of the arguments\n        # Also assumes that the scale of the output is equal to the scale of\n        # the first argument\n\n        progs = []\n        exprs = []\n        for expr in node.exprList:\n            (prog_in, expr_in) = self.visit(expr)\n            progs.append(prog_in)\n            exprs.append(expr_in)\n\n        prog_out = IR.Prog([])\n        for prog_funcCall in progs:\n            prog_out = IRUtil.concatPrograms(prog_out, prog_funcCall)\n\n        expr_out = self.getTempVar()\n\n        args = dict()\n        ch = \'A\'\n        for expr in exprs:\n            args[expr] = ch\n            ch = chr(ord(ch) + 1)\n        args[expr_out] = expr_out.idf\n\n        ch = \'I\'\n        for i in node.type.shape:\n            args[IR.Int(i)] = ch\n            ch = chr(ord(ch) + 1)\n\n        str = [expr.idf for expr in exprs]\n        cmd0 = IR.Comment(node.name + \'(\' + \', \'.join(str) + \')\')\n\n        funcCall = IR.FuncCall(node.name, args)\n\n        prog_funcCall = IR.Prog([cmd0, funcCall])\n\n        prog_out = IRUtil.concatPrograms(prog_out, prog_funcCall)\n\n        self.decls[expr_out.idf] = node.type\n        self.scales[expr_out.idf] = self.scales[exprs[0].idf]\n        self.intvs[expr_out.idf] = self.intvs[exprs[0].idf]\n\n        return (prog_out, expr_out)\n\n    # out = +- in\n    def visitUop(self, node: AST.Uop):\n\n        (prog_in, expr_in) = self.visit(node.expr)\n\n        op = node.op\n\n        if op == SeeDotParser.ADD:\n            return (prog_in, expr_in)\n        assert op == SeeDotParser.SUB\n\n        type_out = node.type\n\n        # e : Int\n        if Type.isInt(type_out):\n            prog_out = prog_in\n            expr_out = IRUtil.negate(expr_in)\n\n        # e: Tensor(), or Tensor(..)\n        else:\n            expr_out = self.getTempVar()\n            iters = self.getTempIterators(type_out.dim)\n\n            scale_out = self.scales[expr_in.idf]\n            (m, M) = self.intvs[expr_in.idf]\n            intv_out = (-M, -m)\n\n            expr_in_idx = IRUtil.addIndex(expr_in, iters)\n            expr_out_idx = IRUtil.addIndex(expr_out, iters)\n            rhs = IRUtil.negate(expr_in_idx)\n            loop = IRUtil.loop(type_out.shape, iters, [\n                               IR.Assn(expr_out_idx, rhs)])\n            prog_uop = IR.Prog(loop)\n\n            prog_out = IRUtil.concatPrograms(prog_in, prog_uop)\n\n            self.decls[expr_out.idf] = type_out\n            self.scales[expr_out.idf] = scale_out\n            self.intvs[expr_out.idf] = intv_out\n\n        return (prog_out, expr_out)\n\n    # out = in_A op in_B\n    def visitBop1(self, node: AST.Bop1):\n        op = node.op\n\n        if op == SeeDotParser.MUL:\n            return self.visitBopMul(node)\n        elif op == SeeDotParser.SPARSEMUL:\n            return self.visitBopSparseMul(node)\n        elif op == SeeDotParser.MULCIR:\n            return self.visitBopMulCir(node)\n        elif op == SeeDotParser.CONV:\n            return self.visitBopConv(node)\n        elif op == SeeDotParser.ADDCIR:\n            return self.visitBopAddOrSubCir(node)\n        elif op == SeeDotParser.SUBCIR:\n            return self.visitBopAddOrSubCir(node)\n        else:\n            assert False\n\n    # out = in_A * in_B\n    def visitBopMul(self, node: AST.Bop1):\n        type_in_A = node.expr1.type\n        type_in_B = node.expr2.type\n        type_out = node.type\n\n        if Type.isInt(type_out):\n            return self.visitBopMulInt(node)\n        elif type_in_A.dim == 0:\n            return self.visitBopMul1DTensor(node)\n        elif type_in_B.dim == 0:\n            return self.visitBopMul1DTensor(node)\n        else:\n            return self.visitBopMul2DTensor(node)\n\n    # out = in_A * in_B\n    def visitBopMulInt(self, node: AST.Bop1):\n\n        (prog_in_A, expr_in_A) = self.visit(node.expr1)\n\n        (prog_in_B, expr_in_B) = self.visit(node.expr2)\n\n        prog_out = IRUtil.concatPrograms(prog_in_A, prog_in_B)\n        expr_out = IRUtil.mul(expr_in_A, expr_in_B)\n\n        return (prog_out, expr_out)\n\n    # out = in_A * in_B\n    def visitBopMul1DTensor(self, node: AST.Bop1):\n\n        (prog_in_A, expr_in_A) = self.visit(node.expr1)\n\n        (prog_in_B, expr_in_B) = self.visit(node.expr2)\n\n        type_in_A, type_in_B = node.expr1.type, node.expr2.type\n        type_out = node.type\n\n        expr_out = self.getTempVar()\n\n        scale_in_A, scale_in_B = self.scales[\n            expr_in_A.idf], self.scales[expr_in_B.idf]\n        intv_in_A, intv_in_B = self.intvs[\n            expr_in_A.idf], self.intvs[expr_in_B.idf]\n\n        [shr_A, shr_B] = self.getShrForMul(scale_in_A, scale_in_B)\n\n        scale_out = self.getScaleForMul(scale_in_A, shr_A, scale_in_B, shr_B)\n        intv_out = self.getIntvervalForMul(intv_in_A, shr_A, intv_in_B, shr_B)\n\n        if type_in_A.dim == 0:\n            a, b = expr_in_A, expr_in_B\n            [I, J] = type_in_B.shape\n            shr_a, shr_b = shr_A, shr_B\n        else:\n            a, b = expr_in_B, expr_in_A\n            [I, J] = type_in_A.shape\n            shr_a, shr_b = shr_B, shr_A\n\n        shr_a = self.formatShr(shr_a)\n        shr_b = self.formatShr(shr_b)\n\n        a.inputVar = False\n        b.inputVar = False\n        expr_out.inputVar = False\n\n        cmd0 = IR.Comment(expr_in_A.idf + \' * \' + expr_in_B.idf)\n\n        funcCall = IR.FuncCall(""ScalarMul"", {\n            a: ""A"",\n            b: ""B"",\n            expr_out: ""C"",\n            IR.Int(I): ""I"",\n            IR.Int(J): ""J"",\n            shr_a: ""shr1"",\n            shr_b: ""shr2""\n        })\n\n        prog_mul = IR.Prog([cmd0, funcCall])\n\n        prog_out = IRUtil.concatPrograms(prog_in_A, prog_in_B, prog_mul)\n\n        self.decls[expr_out.idf] = type_out\n        self.scales[expr_out.idf] = scale_out\n        self.intvs[expr_out.idf] = intv_out\n\n        return (prog_out, expr_out)\n\n    # out = in_A * in_B\n    def visitBopMul2DTensor(self, node: AST.Bop1):\n\n        (prog_in_A, expr_in_A) = self.visit(node.expr1)\n\n        (prog_in_B, expr_in_B) = self.visit(node.expr2)\n\n        expr_treeSum = self.getTempVar()\n        expr_out = self.getTempVar()\n\n        # Compute scales\n        scale_in_A, scale_in_B = self.scales[\n            expr_in_A.idf], self.scales[expr_in_B.idf]\n        intv_in_A, intv_in_B = self.intvs[\n            expr_in_A.idf], self.intvs[expr_in_B.idf]\n\n        [shr_A, shr_B] = self.getShrForMul(scale_in_A, scale_in_B)\n\n        type_in_A, type_in_B = node.expr1.type, node.expr2.type\n        type_out = node.type\n\n        [I, J] = type_in_A.shape\n        [J, K] = type_in_B.shape\n        type_treeSum = Type.Tensor([J])\n\n        scale_treeSum = self.getScaleForMul(\n            scale_in_A, shr_A, scale_in_B, shr_B)\n        intv_treeSum = self.getIntvervalForMul(\n            intv_in_A, shr_A, intv_in_B, shr_B)\n\n        (scale_out, height_shr, height_noshr) = self.getScaleForTreeSum(\n            scale_treeSum, J)\n        intv_out = self.getIntervalForTreeSum(intv_treeSum, J)\n\n        shr_A = self.formatShr(shr_A)\n        shr_B = self.formatShr(shr_B)\n\n        c = \'\'\n        if expr_in_A.idf in self.globalVars:\n            c += \'C\'\n        else:\n            c += \'N\'\n        if expr_in_B.idf in self.globalVars:\n            c += \'C\'\n        else:\n            c += \'N\'\n\n        expr_in_A.inputVar = False\n        expr_in_B.inputVar = False\n        expr_out.inputVar = False\n        expr_treeSum.inputVar = False\n\n        cmd0 = IR.Comment(expr_in_A.idf + \' * \' + expr_in_B.idf)\n\n        funcCall = IR.FuncCall(""MatMul"" + c, {\n            expr_in_A: ""A"",\n            expr_in_B: ""B"",\n            expr_out: ""C"",\n            expr_treeSum: ""T"",\n            IR.Int(I): ""I"",\n            IR.Int(J): ""J"",\n            IR.Int(K): ""K"",\n            shr_A: ""shr1"",\n            shr_B: ""shr2"",\n            IR.Int(height_shr): ""H1"",\n            IR.Int(height_noshr): ""H2""\n        })\n\n        prog_mul = IR.Prog([cmd0, funcCall])\n\n        prog_out = IRUtil.concatPrograms(prog_in_A, prog_in_B, prog_mul)\n\n        self.decls[expr_out.idf] = type_out\n        self.scales[expr_out.idf] = scale_out\n        self.intvs[expr_out.idf] = intv_out\n\n        self.decls[expr_treeSum.idf] = type_treeSum\n\n        return (prog_out, expr_out)\n\n    # out = in_A |*| in_B\n    def visitBopSparseMul(self, node: AST.Bop1):\n\n        (prog_in_A, expr_in_A) = self.visit(node.expr1)\n\n        (prog_in_B, expr_in_B) = self.visit(node.expr2)\n\n        [P, Q] = node.expr1.type.shape\n        [Q, R] = node.expr2.type.shape\n        assert R == 1\n\n        expr_out = self.getTempVar()\n        type_out = node.type\n\n        scale_in_A, scale_in_B = self.scales[\n            expr_in_A.idf], self.scales[expr_in_B.idf]\n        intv_in_A, intv_in_B = self.intvs[\n            expr_in_A.idf], self.intvs[expr_in_B.idf]\n\n        [shr_A, shr_B] = self.getShrForMul(scale_in_A, scale_in_B)\n\n        scale_treeSum = self.getScaleForMul(\n            scale_in_A, shr_A, scale_in_B, shr_B)\n        intv_treeSum = self.getIntvervalForMul(\n            intv_in_A, shr_A, intv_in_B, shr_B)\n\n        (scale_out, height_shr, height_noshr) = self.getScaleForTreeSum(\n            scale_treeSum, Q)\n        intv_out = self.getIntervalForTreeSum(intv_treeSum, Q)\n\n        in_A_idx = IR.Var(expr_in_A.idf[0] +\n                          \'idx\', expr_in_A.idx, inputVar=True)\n        in_A_val = IR.Var(expr_in_A.idf[0] +\n                          \'val\', expr_in_A.idx, inputVar=True)\n\n        shr_A = self.formatShr(shr_A)\n        shr_B = self.formatShr(shr_B)\n        height_shr = self.formatShr(height_shr)\n\n        in_A_idx.inputVar = False\n        in_A_val.inputVar = False\n        expr_in_B.inputVar = False\n        expr_out.inputVar = False\n\n        cmd0 = IR.Comment(expr_in_A.idf + \' |*| \' + expr_in_B.idf)\n        cmd1 = IR.Memset(expr_out, type_out.size())\n\n        funcCall = IR.FuncCall(""SparseMatMul"", {\n            in_A_idx: ""Aidx"",\n            in_A_val: ""Aval"",\n            expr_in_B: ""B"",\n            expr_out: ""C"",\n            IR.Int(Q): ""K"",\n            shr_A: ""shrA"",\n            shr_B: ""shrB"",\n            height_shr: ""shrC""\n        })\n\n        prog_mul = IR.Prog([cmd0, cmd1, funcCall])\n\n        prog_out = IRUtil.concatPrograms(prog_in_A, prog_in_B, prog_mul)\n\n        self.decls[expr_out.idf] = type_out\n        self.scales[expr_out.idf] = scale_out\n        self.intvs[expr_out.idf] = intv_out\n\n        # Length of Aidx and Aval hard coded to 100\n        # This is safe as it will be ignored in the generated code\n        self.decls.update({in_A_idx.idf: Type.Tensor([100]),\n                           in_A_val.idf: Type.Tensor([100]),\n                           })\n        self.globalVars.append(in_A_idx.idf)\n        self.globalVars.append(in_A_val.idf)\n\n        return (prog_out, expr_out)\n\n    # out = in_A <*> in_B\n    def visitBopMulCir(self, node: AST.Bop1):\n\n        (prog_in_A, expr_in_A) = self.visit(node.expr1)\n\n        (prog_in_B, expr_in_B) = self.visit(node.expr2)\n\n        type_in_A, type_in_B = node.expr1.type, node.expr2.type\n        type_out = node.type\n\n        expr_out = self.getTempVar()\n\n        assert type_out.dim == 2\n\n        [I, J] = type_out.shape\n\n        scale_in_A, scale_in_B = self.scales[\n            expr_in_A.idf], self.scales[expr_in_B.idf]\n        intv_in_A, intv_in_B = self.intvs[\n            expr_in_A.idf], self.intvs[expr_in_B.idf]\n\n        [shr_A, shr_B] = self.getShrForMul(scale_in_A, scale_in_B)\n\n        scale_out = self.getScaleForMul(scale_in_A, shr_A, scale_in_B, shr_B)\n        intv_out = self.getIntvervalForMul(intv_in_A, shr_A, intv_in_B, shr_B)\n\n        shr_A = self.formatShr(shr_A)\n        shr_B = self.formatShr(shr_B)\n\n        expr_in_A.inputVar = False\n        expr_in_B.inputVar = False\n        expr_out.inputVar = False\n\n        cmd0 = IR.Comment(expr_in_A.idf + \' <*> \' + expr_in_B.idf)\n\n        funcCall = IR.FuncCall(""MulCir"", {\n            expr_in_A: ""A"",\n            expr_in_B: ""B"",\n            expr_out: ""C"",\n            IR.Int(I): ""I"",\n            IR.Int(J): ""J"",\n            shr_A: ""shrA"",\n            shr_B: ""shrB""\n        })\n\n        prog_mul = IR.Prog([cmd0, funcCall])\n\n        prog_out = IRUtil.concatPrograms(prog_in_A, prog_in_B, prog_mul)\n\n        self.decls[expr_out.idf] = type_out\n        self.scales[expr_out.idf] = scale_out\n        self.intvs[expr_out.idf] = intv_out\n\n        return (prog_out, expr_out)\n\n    # out = in_A # in_B\n    def visitBopConv(self, node: AST.Bop1):\n\n        (prog_in_A, expr_in_A) = self.visit(node.expr1)\n\n        (prog_in_B, expr_in_B) = self.visit(node.expr2)\n\n        [N, H, W, CI] = node.expr1.type.shape\n        [HF, WF, CI, CO] = node.expr2.type.shape\n\n        type_treeSum = Type.Tensor([HF * WF * CI])\n        type_out = node.type\n\n        # Compute padding\n        padH = (HF - 1) // 2\n        padW = (WF - 1) // 2\n\n        # Declare variables\n        [expr_treeSum, expr_out] = self.getTempVars(2)\n\n        # Compute scale reductions and new scaling factors\n        scale_in_A, scale_in_B = self.scales[\n            expr_in_A.idf], self.scales[expr_in_B.idf]\n        intv_in_A, intv_in_B = self.intvs[\n            expr_in_A.idf], self.intvs[expr_in_B.idf]\n\n        [shr_A, shr_B] = self.getShrForMul(scale_in_A, scale_in_B)\n\n        scale_treeSum = self.getScaleForMul(\n            scale_in_A, shr_A, scale_in_B, shr_B)\n        intv_treeSum = self.getIntvervalForMul(\n            intv_in_A, shr_A, intv_in_B, shr_B)\n\n        (scale_out, height_shr, height_noshr) = self.getScaleForTreeSum(\n            scale_treeSum, HF * WF * CI)\n        intv_out = self.getIntervalForTreeSum(intv_treeSum, HF * WF * CI)\n\n        shr_A = self.formatShr(shr_A)\n        shr_B = self.formatShr(shr_B)\n\n        expr_in_A.inputVar = False\n        expr_in_B.inputVar = False\n        expr_out.inputVar = False\n\n        cmd0 = IR.Comment(expr_in_A.idf + \' # \' + expr_in_B.idf)\n\n        funcCall = IR.FuncCall(""Conv"", {\n            expr_in_A: ""A"",\n            expr_in_B: ""B"",\n            expr_out: ""C"",\n            expr_treeSum: ""tmp"",\n            IR.Int(N): ""N"",\n            IR.Int(H): ""H"",\n            IR.Int(W): ""W"",\n            IR.Int(CI): ""CI"",\n            IR.Int(HF): ""HF"",\n            IR.Int(WF): ""WF"",\n            IR.Int(CO): ""CO"",\n            shr_A: ""shrA"",\n            shr_B: ""shrB"",\n            IR.Int(height_shr): ""H1"",\n            IR.Int(height_noshr): ""H2""\n        })\n\n        prog_conv = IR.Prog([cmd0, funcCall])\n\n        prog_out = IRUtil.concatPrograms(prog_in_A, prog_in_B, prog_conv)\n\n        # Update context for output variable\n        self.decls[expr_out.idf] = type_out\n        self.scales[expr_out.idf] = scale_out\n        self.intvs[expr_out.idf] = intv_out\n\n        # Update declarations\n        self.decls[expr_treeSum.idf] = type_treeSum\n\n        return (prog_out, expr_out)\n\n    # out = in_A <+-> in_B\n    def visitBopAddOrSubCir(self, node: AST.Bop1):\n\n        (prog_in_A, expr_in_A) = self.visit(node.expr1)\n\n        (prog_in_B, expr_in_B) = self.visit(node.expr2)\n\n        op = node.op\n        type_in_A, type_in_B = node.expr1.type, node.expr2.type\n        type_out = node.type\n\n        if op == SeeDotParser.ADDCIR:\n            (op_ir, op_fn) = (IR.Op.Op[\'+\'], operator.add)\n            add = True\n        elif op == SeeDotParser.SUBCIR:\n            (op_ir, op_fn) = (IR.Op.Op[\'-\'], operator.sub)\n            add = False\n\n        assert add == True\n\n        scale_in_A, scale_in_B = self.scales[\n            expr_in_A.idf], self.scales[expr_in_B.idf]\n        intv_in_A, intv_in_B = self.intvs[\n            expr_in_A.idf], self.intvs[expr_in_B.idf]\n\n        (scale_out, intv_out, [shr_A, shr_B, shr_out]) = self.getScaleAndIntervalForAdd(\n            scale_in_A, scale_in_B, intv_in_A, intv_in_B, op_fn)\n\n        shr_A = self.formatShr(shr_A)\n        shr_B = self.formatShr(shr_B)\n        shr_out = self.formatShr(shr_out)\n\n        expr_in_A.inputVar = False\n        expr_in_B.inputVar = False\n\n        cmd0 = IR.Comment(expr_in_A.idf + "" <"" +\n                          op_ir.name + ""> "" + expr_in_B.idf)\n\n        if node.type.dim == 4:\n            [N, H, W, C] = node.type.shape\n            funcCall = IR.FuncCall(""AddOrSubCir4D"", {\n                expr_in_A: ""A"",\n                expr_in_B: ""B"",\n                IR.Int(N): ""N"",\n                IR.Int(H): ""H"",\n                IR.Int(W): ""W"",\n                IR.Int(C): ""C"",\n                shr_A: ""shrA"",\n                shr_B: ""shrB"",\n                shr_out: ""shrC"",\n                IR.Bool(True): ""add""\n            })\n        elif node.type.dim == 2:\n            [H, W] = node.type.shape\n            funcCall = IR.FuncCall(""AddOrSubCir2D"", {\n                expr_in_A: ""A"",\n                expr_in_B: ""B"",\n                IR.Int(H): ""H"",\n                IR.Int(W): ""W"",\n                shr_A: ""shrA"",\n                shr_B: ""shrB"",\n                shr_out: ""shrC"",\n                IR.Bool(True): ""add""\n            })\n        else:\n            assert False\n\n        prog_cir = IR.Prog([cmd0, funcCall])\n\n        prog_out = IRUtil.concatPrograms(prog_in_A, prog_in_B, prog_cir)\n\n        self.scales[expr_in_A.idf] = scale_out\n        self.intvs[expr_in_A.idf] = intv_out\n\n        return (prog_out, expr_in_A)\n\n    # out = in_A \'op\' in_B\n    def visitBop2(self, node: AST.Bop2):\n\n        (prog_in_A, expr_in_A) = self.visit(node.expr1)\n\n        (prog_in_B, expr_in_B) = self.visit(node.expr2)\n\n        op = node.op\n        if op == SeeDotParser.ADD:\n            (op_ir, op_fn) = (IR.Op.Op[\'+\'], operator.add)\n            funcName = ""MatAdd""\n        elif op == SeeDotParser.SUB:\n            (op_ir, op_fn) = (IR.Op.Op[\'-\'], operator.sub)\n            funcName = ""MatSub""\n\n        type_out = node.type\n\n        # e : Int\n        if Type.isInt(type_out):\n            prog_out = IRUtil.concatPrograms(prog_in_A, prog_in_B)\n            expr_out = IR.IntBop(expr_in_A, op_ir, expr_in_B)\n\n        # e : Tensor(), or Tensor(..)\n        else:\n            expr_out = self.getTempVar()\n\n            scale_in_A, scale_in_B = self.scales[\n                expr_in_A.idf], self.scales[expr_in_B.idf]\n            intv_in_A, intv_in_B = self.intvs[\n                expr_in_A.idf], self.intvs[expr_in_B.idf]\n\n            (scale_out, intv_out, [shr_A, shr_B, shr_out]) = self.getScaleAndIntervalForAdd(\n                scale_in_A, scale_in_B, intv_in_A, intv_in_B, op_fn)\n\n            assert type_out.dim == 2\n\n            [I, J] = type_out.shape\n\n            shr_A = self.formatShr(shr_A)\n            shr_B = self.formatShr(shr_B)\n            shr_out = self.formatShr(shr_out)\n\n            expr_in_A.inputVar = False\n            expr_in_B.inputVar = False\n            expr_out.inputVar = False\n\n            cmd0 = IR.Comment(expr_in_A.idf + \' \' +\n                              op_ir.name + \' \' + expr_in_B.idf)\n\n            funcCall = IR.FuncCall(funcName, {\n                expr_in_A: ""A"",\n                expr_in_B: ""B"",\n                expr_out: ""C"",\n                IR.Int(I): ""I"",\n                IR.Int(J): ""J"",\n                shr_A: ""shrA"",\n                shr_B: ""shrB"",\n                shr_out: ""shrC""\n            })\n\n            prog_bop = IR.Prog([cmd0, funcCall])\n\n            prog_out = IRUtil.concatPrograms(prog_in_A, prog_in_B, prog_bop)\n\n            self.decls[expr_out.idf] = type_out\n            self.scales[expr_out.idf] = scale_out\n            self.intvs[expr_out.idf] = intv_out\n\n        return (prog_out, expr_out)\n\n    # out = func(in)\n    def visitFunc(self, node: AST.Func):\n        op = node.op\n\n        if op == SeeDotParser.RELU:\n            return self.visitRelu(node)\n        elif op == SeeDotParser.EXP:\n            return self.visitExp(node)\n        elif op == SeeDotParser.ARGMAX:\n            return self.visitArgMax(node)\n        elif op == SeeDotParser.SGN:\n            return self.visitSgn(node)\n        elif op == SeeDotParser.TANH:\n            return self.visitTanh(node)\n        else:\n            assert False\n\n    # out = relu(in)\n    def visitRelu(self, node: AST.Func):\n\n        (prog_in, expr_in) = self.visit(node.expr)\n\n        type_out = node.expr.type\n\n        (m, M) = self.intvs[expr_in.idf]\n        if m < 0:\n            m = 0\n        if M < 0:\n            M = 0\n        intv_out = (m, M)\n\n        expr_in.inputVar = False\n\n        cmd0 = IR.Comment(""relu("" + expr_in.idf + "")"")\n\n        if node.type.dim == 4:\n            [N, H, W, C] = node.type.shape\n            funcCall = IR.FuncCall(""Relu4D"", {\n                expr_in: ""A"",\n                IR.Int(N): ""N"",\n                IR.Int(H): ""H"",\n                IR.Int(W): ""W"",\n                IR.Int(C): ""C""\n            })\n        elif node.type.dim == 2:\n            [H, W] = node.type.shape\n            funcCall = IR.FuncCall(""Relu2D"", {\n                expr_in: ""A"",\n                IR.Int(H): ""H"",\n                IR.Int(W): ""W""\n            })\n        else:\n            assert False\n\n        prog_relu = IR.Prog([cmd0, funcCall])\n\n        prog_out = IRUtil.concatPrograms(prog_in, prog_relu)\n\n        self.intvs[expr_in.idf] = intv_out\n\n        return (prog_out, expr_in)\n\n    # out = exp(in)\n    def visitExp(self, node: AST.Func):\n\n        self.readProfileFile()\n\n        if useMathExp():\n            return self.visitMathExp(node)\n        elif useTableExp():\n            return self.visitTableExp(node)\n        else:\n            assert False\n\n    # Note: We assume e<=0 for exp(e)\n    def visitMathExp(self, node: AST.Func):\n\n        # Tunable parameter\n        MIN = 0.1\n\n        (prog_in, expr_in) = self.visit(node.expr)\n\n        type_in = node.expr.type\n\n        scale_in = self.scales[expr_in.idf]\n        intv_in = self.intvs[expr_in.idf]\n\n        \'\'\'\n\t\t1.  y = ((int) (exp(((float)e) / shr1) * shr2))\n\t\t\'\'\'\n\n        maxExp = np.exp(-MIN)\n\n        expr_out = self.getTempVar()\n\n        scale_out = self.getScale(maxExp)\n        intv_out = self.getInterval(scale_out, maxExp, maxExp)\n\n        shr1 = IR.Int(2 ** -scale_in)\n        shr2 = IR.Int(2 ** -scale_out)\n\n        expr_in_idx = IRUtil.addIndex(expr_in, [IRUtil.zero] * type_in.dim)\n        expr_out_idx = IRUtil.addIndex(expr_out, [IRUtil.zero] * type_in.dim)\n\n        cmd0 = IR.Comment(\'exp(\' + expr_in.idf + \')\')\n\n        cmd_assn = IR.Assn(expr_out_idx, IRUtil.castToInt(IRUtil.mul(\n            IR.Exp(IRUtil.div(IRUtil.castToFloat(expr_in_idx), shr1)), shr2)))\n\n        prog_exp = IR.Prog([cmd0, cmd_assn])\n\n        prog_out = IRUtil.concatPrograms(prog_in, prog_exp)\n\n        self.decls[expr_out.idf] = type_in\n        self.scales[expr_out.idf] = scale_out\n        self.intvs[expr_out.idf] = intv_out\n\n        return (prog_out, expr_out)\n\n    # Note: We assume e<=0 for exp(e)\n    def visitTableExp(self, node: AST.Func):\n\n        (prog_in, expr_in) = self.visit(node.expr)\n\n        # TODO: use MAX_VAL_EXP\n        type_in = node.expr.type\n\n        scale_in = self.scales[expr_in.idf]\n        intv_in = self.intvs[expr_in.idf]\n\n        [m, M] = self.expRange\n        [m_scale, M_scale] = [\n            int(np.ldexp(m, -scale_in)), int(np.ldexp(M, -scale_in))]\n\n        max = int(np.ldexp(M - m, -scale_in))\n        shl = self.getShl(max)\n\n        input = self.getTempVar()\n        [i, j] = self.getTempVars(2)\n        expr_out = self.getTempVar()\n\n        \'\'\'\n\t\t1.  if ((-x) < min) {\n\t\t2.  \ti = 0;\n\t\t3.  \tj = 0;\n\t\t4.  }\n\t\t5.  else {\n\t\t6.  \ty = ((-x) - min) << shl\n\t\t7.  \ti = (y >> shrI) & (2^b-1)\n\t\t8.  \tj = (y >> shrJ) & (2^b-1)\n\t\t9.  }\n\t\t10. ans = T[i] * U[j]\n\t\t\'\'\'\n\n        mask = IR.Int(2 ** self.expB - 1)\n        shrI = Common.wordLength - self.expB\n        shrJ = Common.wordLength - self.expB * 2\n        table = self.getExpTable(scale_in)\n\n        scale1 = self.getScale(1)\n        scale2 = self.getScale(abs(np.exp(-m)))\n\n        [shr1, shr2] = self.getShrForMul(scale1, scale2)\n\n        expr_1_elt = IRUtil.addIndex(expr_in, [IRUtil.zero] * type_in.dim)\n        expr_2_elt = IRUtil.addIndex(expr_out, [IRUtil.zero] * type_in.dim)\n\n        cond = IRUtil.lt(IRUtil.negate(expr_1_elt), IR.Int(m_scale))\n\n        cmd2 = IR.Assn(i, IR.Int(0))\n        cmd3 = IR.Assn(j, IR.Int(0))\n\n        cmd6 = IR.Assn(input, IRUtil.shl(IRUtil.sub(\n            IRUtil.negate(expr_1_elt), IR.Int(m_scale)), shl))\n        cmd7 = IR.Assn(i, IRUtil.bitAnd(IRUtil.shrUint(input, shrI), mask))\n        cmd8 = IR.Assn(j, IRUtil.bitAnd(IRUtil.shrUint(input, shrJ), mask))\n\n        cmd1 = IR.If(cond, [cmd2, cmd3], [cmd6, cmd7, cmd8])\n        cmd10 = IR.Assn(expr_2_elt, IRUtil.mul(IRUtil.shrUint(IRUtil.addIndex(\n            table[0], [i]), shr1), IRUtil.shrUint(IRUtil.addIndex(table[1], [j]), shr2)))\n\n        scale_out = self.getScaleForExp(scale1, shr1, scale2, shr2)\n        intv_out = self.getIntervalForExp(scale_out, [-m_scale, -M_scale])\n\n        cmd0 = IR.Comment(\'exp(\' + expr_in.idf + \')\')\n\n        prog_exp = IR.Prog([cmd0, cmd1, cmd10])\n\n        prog_out = IRUtil.concatPrograms(prog_in, prog_exp)\n\n        self.decls[expr_out.idf] = type_in\n        self.scales[expr_out.idf] = scale_out\n        self.intvs[expr_out.idf] = intv_out\n\n        self.decls.update(dict((var.idf, Type.Int()) for var in [input, i, j]))\n\n        return (prog_out, expr_out)\n\n    def getShl(self, n: int):\n        assert n != 0\n\n        shl = 0\n        while(n != 0):\n            n = n >> 1\n            shl += 1\n        return min(Common.wordLength - shl, Common.wordLength - self.expB * 2)\n\n    def getExpTable(self, p):\n        table = self.expTables.get(p)\n        if table == None:\n            table = self.populateExpTable(p)\n            self.expTables[p] = table\n\n        return table[1]\n\n    def populateExpTable(self, p):\n        [table_m, table_n] = self.expTableShape\n        b = np.log2(table_n)\n\n        # Currently looking at only 2D arrays\n        assert table_m == 2\n\n        [m, M] = self.expRange\n        max = int(np.ldexp(M - m, -p))\n        shl = self.getShl(max)\n\n        #alpha_count = self.getAlphaCount(max, shl)\n        alpha_count = table_n\n        beta_count = table_n\n\n        table = [[0 for _ in range(alpha_count)], [\n            0 for _ in range(beta_count)]]\n\n        alpha = Common.wordLength - shl - b\n        pRes = self.getScale(1)\n        for i in range(alpha_count):\n            num = i * 2 ** (alpha + p)\n            exp = np.exp(-num)\n            table[0][i] = int(np.ldexp(exp, -pRes))\n\n        beta = alpha - b\n        pRes = self.getScale(abs(np.exp(-m)))\n        for i in range(beta_count):\n            num = m + i * 2 ** (beta + p)\n            exp = np.exp(-num)\n            table[1][i] = int(np.ldexp(exp, -pRes))\n\n        tableVar = [IR.Var(\'EXP\' + str(abs(p)) + \'A\', inputVar=True),\n                    IR.Var(\'EXP\' + str(abs(p)) + \'B\', inputVar=True)]\n\n        return [table, tableVar]\n\n    def getAlphaCount(self, max, shl):\n        mask = 2 ** self.expB - 1\n        shr = Common.wordLength - shl - self.expB\n        return ((max >> shr) & mask) + 1\n\n    # out = argmax(in)\n    def visitArgMax(self, node: AST.Func):\n\n        (prog_in, expr_in) = self.visit(node.expr)\n\n        type_out = node.expr.type\n\n        assert type_out.dim == 2\n\n        [I, J] = type_out.shape\n\n        expr_out = self.getTempVar()\n\n        expr_in.inputVar = False\n\n        cmd0 = IR.Comment(\'argmax(\' + expr_in.idf + \')\')\n\n        funcCall = IR.FuncCall(""ArgMax"", {\n            expr_in: ""A"",\n            IR.Int(I): ""I"",\n            IR.Int(J): ""J"",\n            expr_out: ""index""\n        })\n\n        prog_argmax = IR.Prog([cmd0, funcCall])\n\n        prog_out = IRUtil.concatPrograms(prog_in, prog_argmax)\n\n        self.decls[expr_out.idf] = Type.Int()\n\n        return (prog_out, expr_out)\n\n    # out = sgn(in)\n    def visitSgn(self, node: AST.Func):\n\n        (prog_in, expr_in) = self.visit(node.expr)\n\n        expr_out = self.getTempVar()\n        type_in = node.expr.type\n\n        expr_in_idx = IRUtil.addIndex(expr_in, [IRUtil.zero] * type_in.dim)\n\n        cmd0 = IR.Comment(\'sgn(\' + expr_in.idf + \')\')\n        cmd1 = IR.Assn(expr_out, IRUtil.cond_zero(\n            expr_in_idx, IRUtil.one, IRUtil.zero))\n\n        prog_sgn = IR.Prog([cmd0, cmd1])\n\n        prog_out = IRUtil.concatPrograms(prog_in, prog_sgn)\n\n        self.decls[expr_out.idf] = Type.Int()\n\n        return (prog_out, expr_out)\n\n    # out = tanh(in)\n    def visitTanh(self, node: AST.Func):\n\n        (prog_in, expr_in) = self.visit(node.expr)\n\n        type_in = node.expr.type\n        [I, J] = type_in.shape\n\n        scale_in = self.scales[expr_in.idf]\n        intv_in = self.intvs[expr_in.idf]\n\n        # Scale tanh limit\n        tanh_limit = int(np.ldexp(Common.tanh_limit, -scale_in))\n        assert tanh_limit < np.iinfo(IR.DataType.getIntClass()).max\n        tanh_limit = IR.DataType.getInt(tanh_limit)\n\n        tanh_intv = self.getInterval(\n            scale_in, Common.tanh_limit, Common.tanh_limit)\n        intv_out = self.updateTanhIntv(intv_in, tanh_intv)\n\n        expr_in.inputVar = False\n\n        cmd0 = IR.Comment(""tanh("" + expr_in.idf + "")"")\n\n        funcCall = IR.FuncCall(""TanH"", {\n            expr_in: ""A"",\n            IR.Int(I): ""I"",\n            IR.Int(J): ""J"",\n            IR.Int(tanh_limit): ""threshold""\n        })\n\n        prog_tanh = IR.Prog([cmd0, funcCall])\n\n        prog_out = IRUtil.concatPrograms(prog_in, prog_tanh)\n\n        self.intvs[expr_in.idf] = intv_out\n        expr_out = expr_in\n\n        return (prog_out, expr_out)\n\n    # out = $x[start:end] in\n    def visitSum(self, node: AST.Sum):\n        \'\'\'\n        expr_out\n        i = 0\n        for (j = 0; j < n; j++)\n          expr_in = prog_in\n          expr_out = expr_out + expr_in\n          i++\n\n        1.  for i in [0, C]:\n        2.    expr_out[i] = expr_out[i] + shr(expr_in[i])\n        \'\'\'\n\n        var_idf = node.name\n        self.decls[var_idf] = Type.Int()\n\n        (prog_in, expr_in) = self.visit(node.expr)\n\n        start, end = node.start, node.end\n\n        expr_out = self.getTempVar()\n        type_out = node.type\n\n        var = IR.Var(var_idf)\n        var_iter = self.getTempIterator()\n        iters = self.getTempIterators(type_out.dim)\n\n        (scale_out, height_shr, height_noshr) = self.getScaleForTreeSum(\n            self.scales[expr_in.idf], end - start)\n        intv_out = self.getIntervalForTreeSum(\n            self.intvs[expr_in.idf], end - start)\n\n        # Tree sum to sum output of each iteration\n        expr_in_idx = IRUtil.addIndex(expr_in, iters)\n        expr_out_idx = IRUtil.addIndex(expr_out, iters)\n\n        cmd1 = IR.Memset(expr_out, type_out.size())\n        cmd2 = IR.Assn(expr_out_idx, IRUtil.add(\n            expr_out_idx, IRUtil.shr(expr_in_idx, height_shr)))\n        treeSum = IRUtil.loop(type_out.shape, iters, [cmd2])\n\n        # Final program to sum output of each iteration\n        prog_sum = [cmd1,\n                    IR.Assn(var, IR.Int(start)),\n                    IR.For(var_iter, 0, IRUtil.lt(var_iter, IR.Int(end - start)),\n                           prog_in.cmd_l + treeSum +\n                           [IR.Assn(var, IRUtil.inc(var))])]\n\n        prog_out = IR.Prog(prog_sum)\n\n        self.decls[expr_out.idf] = type_out\n        self.scales[expr_out.idf] = scale_out\n        self.intvs[expr_out.idf] = intv_out\n\n        return (prog_out, expr_out)\n\n    # out = in_cond > 0? in_A: in_B\n    def visitCond(self, node: AST.Cond):\n\n        (prog_in_cond, expr_in_cond) = self.visit(node.expr)\n\n        (prog_in_A, expr_in_A) = self.visit(node.trueBlock)\n\n        (prog_in_B, expr_in_B) = self.visit(node.falseBlock)\n\n        type_in_cond = node.expr.type\n        type_in_A = node.trueBlock.type\n\n        if Type.isInt(type_in_cond):\n            expr_in_cond_idx = expr_in_cond\n        else:\n            expr_in_cond_idx = IRUtil.addIndex(\n                expr_in_cond, [IRUtil.zero] * type_in_cond.dim)\n\n        # e2,e3 : Int\n        if Type.isInt(type_in_A):\n            # TODO: Update the scale and intv of expr_out based on in_A and\n            # in_B\n            prog_out = IRUtil.concatPrograms(\n                prog_in_cond, prog_in_A, prog_in_B)\n            expr_out = IRUtil.cond_zero(expr_in_cond_idx, expr_in_A, expr_in_B)\n\n        # e2,e3 : Tensor(), or Tensor(..)\n        else:\n            expr_out = self.getTempVar()\n            iters = self.getTempIterators(type_in_A.dim)\n\n            scale_in_A, scale_in_B = self.scales[\n                expr_in_A.idf], self.scales[expr_in_B.idf]\n            intv_in_A, intv_in_B = self.intvs[\n                expr_in_A.idf], self.intvs[expr_in_B.idf]\n            m_2, M_2 = intv_in_A\n            m_3, M_3 = intv_in_B\n\n            if scale_in_A >= scale_in_B:\n                shr_n_2, shr_n_3 = 0, scale_in_A - scale_in_B\n            else:\n                shr_n_2, shr_n_3 = scale_in_B - scale_in_A, 0\n\n            scale_out = max(scale_in_A, scale_in_B)\n            intv_out = (min(m_2 >> shr_n_2, m_3 >> shr_n_3),\n                        max(M_2 >> shr_n_2, M_3 >> shr_n_3))\n\n            # prog_assn\n            expr_in_A_idx = IRUtil.addIndex(expr_in_A, iters)\n            expr_in_B_idx = IRUtil.addIndex(expr_in_B, iters)\n            expr_out_idx = IRUtil.addIndex(expr_out, iters)\n            rhs = IRUtil.cond_zero(expr_in_cond_idx,\n                                   IRUtil.shr(expr_in_A_idx, shr_n_2),\n                                   IRUtil.shr(expr_in_B_idx, shr_n_3))\n            cmdl_assn = IRUtil.loop(type_in_A.shape, iters, [\n                                    IR.Assn(expr_out_idx, rhs)])\n            prog_cond = IR.Prog(cmdl_assn)\n\n            prog_out = IRUtil.concatPrograms(\n                prog_in_cond, prog_in_A, prog_in_B, prog_cond)\n\n            self.decls[expr_out.idf] = type_in_A\n            self.scales[expr_out.idf] = scale_out\n            self.intvs[expr_out.idf] = intv_out\n\n        return (prog_out, expr_out)\n\n    # let idf = decl \'in\' in\n    def visitLet(self, node: AST.Let):\n\n        (prog_decl, expr_decl) = self.visit(node.decl)\n        type_decl = node.decl.type\n\n        idf = node.name\n\n        # e1 : Int\n        if Type.isInt(type_decl):\n            self.decls[idf] = Type.Int()\n\n            (prog_in, expr_in) = self.visit(node.expr)\n\n            cmd = IR.Assn(IR.Var(idf), expr_decl)\n            prog_let = IR.Prog([cmd])\n\n            prog_out = IRUtil.concatPrograms(prog_decl, prog_let, prog_in)\n\n            return (prog_out, expr_in)\n\n        # e1 : Tensor{(),(..)}\n        else:\n            self.scales[idf] = self.scales[expr_decl.idf]\n            self.intvs[idf] = self.intvs[expr_decl.idf]\n\n            if isinstance(node.decl, AST.Decl):\n                self.globalVars.append(idf)\n                self.decls[idf] = node.decl.type\n                expr_decl.idf = idf\n                expr_decl.inputVar = True\n\n            (prog_in, expr_in) = self.visit(node.expr)\n\n            prog_in = prog_in.subst(idf, expr_decl)\n            expr_in = expr_in.subst(idf, expr_decl)\n\n            prog_out = IRUtil.concatPrograms(prog_decl, prog_in)\n\n            return (prog_out, expr_in)\n\n    # Computing exponent and intervals\n    def getScale(self, maxabs: float):  # -> int\n        return int(np.ceil(np.log2(maxabs) - np.log2((1 << (Common.wordLength - 2)) - 1)))\n\n    # Takes range [r1, r2] and returns the interval scaled by p\n    def getInterval(self, p: int, r1: float, r2: float):\n        return (int(np.ldexp(r1, -p)), int(np.ldexp(r2, -p)))\n\n    def getScaleForMul(self, p1: int, shr1: int, p2: int, shr2: int) -> int:\n        return (p1 + shr1) + (p2 + shr2)\n\n    # int^2 * int^2 -> int^2\n    def getIntvervalForMul(self, intv_1, shr1: int, intv_2, shr2: int):\n        (m_1, M_1) = intv_1\n        (m_1, M_1) = (m_1 >> shr1, M_1 >> shr1)\n\n        (m_2, M_2) = intv_2\n        (m_2, M_2) = (m_2 >> shr2, M_2 >> shr2)\n\n        m = min([m_1 * m_2, m_1 * M_2, M_1 * m_2, M_1 * M_2])\n        M = max([m_1 * m_2, m_1 * M_2, M_1 * m_2, M_1 * M_2])\n\n        return (m, M)\n\n    def getScaleForTreeSum(self, p: int, n: int):\n        H_tot = int(np.ceil(np.log2(n)))\n        if p >= self.MAX_SCALE:\n            p_res = p\n        else:\n            p_res = min(p + H_tot, self.MAX_SCALE)\n        H_1 = p_res - p\n        assert H_1 >= 0\n        H_2 = H_tot - H_1\n        assert H_2 >= 0\n        return (p_res, H_1, H_2)\n\n    def getIntervalForTreeSum(self, intv, n: int):\n        max_abs = (1 << Common.wordLength - 2) - 1\n        (m, M) = intv\n        m = max(n * m, -max_abs)\n        M = min(n * M,  max_abs)\n        return (m, M)\n\n    def getScaleAndIntervalForAdd(self, p_1: int, p_2: int, intv_1, intv_2, op_fn):\n        (m_1, M_1) = intv_1\n        (m_2, M_2) = intv_2\n\n        if p_1 >= p_2:\n            shr_n = [0, p_1 - p_2, 0]\n            p = p_1\n        else:\n            shr_n = [p_2 - p_1, 0, 0]\n            p = p_2\n        m = op_fn(m_1 >> shr_n[0], m_2 >> shr_n[1])\n        M = op_fn(M_1 >> shr_n[0], M_2 >> shr_n[1])\n\n        if max(abs(m), abs(M)) >= (1 << (Common.wordLength - 2)) and p < self.MAX_SCALE:\n            shr_n[2] = 1\n            p += 1\n        max_abs = (1 << Common.wordLength - 2) - 1\n        m = max(m >> shr_n[2], -max_abs)\n        M = min(M >> shr_n[2],  max_abs)\n\n        return (p, (m, M), shr_n)\n\n    def getScaleForExp(self, p1: int, shr1: int, p2: int, shr2: int):\n        return (p1 + shr1) + (p2 + shr2)\n\n    def getIntervalForExp(self, p: int, intv):  # int^2 -> int^2\n        (m, M) = intv\n        assert m < np.ldexp(self.MAX_VAL_EXP, -p)\n        M = min(M, np.ldexp(self.MAX_VAL_EXP, -p))\n        return self.getInterval(p, np.exp(np.ldexp(m, p)), np.exp(np.ldexp(M, p)))\n\n    def getShrForMul(self, p1, p2):\n        shr = (Common.wordLength - 2) // 2\n        pRes = (p1 + shr) + (p2 + shr)\n        if pRes < self.MAX_SCALE:\n            return [shr, shr]\n        else:\n            save = abs(abs(pRes) - abs(self.MAX_SCALE))\n            save1 = save // 2\n            save2 = save - save1\n            shr1 = max(shr - save1, 0)\n            shr2 = max(shr - save2, 0)\n            return [shr1, shr2]\n\n    def updateTanhIntv(self, intv_1, intv_tanh):\n        m_e, M_e = intv_1\n        m_t, M_t = intv_tanh\n        return min(m_e, m_t), min(M_e, M_t)\n\n    # Variable and iterators creation\n    def getTempVars(self, n: int):\n        return [self.getTempVar() for i in range(n)]\n\n    def getTempVar(self):\n        var = IR.Var(\'tmp\' + str(self.counter_var))\n        self.counter_var += 1\n        return var\n\n    def getTempIterators(self, n: int):\n        return [self.getTempIterator() for i in range(n)]\n\n    def getTempIterator(self):\n        var = IR.Var(\'i\' + str(self.counter_iter))\n        self.counter_iter += 1\n        return var\n\n    def formatShr(self, n):\n        assert n >= 0\n\n        shrType = getShrType()\n\n        if shrType == ""shr"" or shrType == ""shr+"":\n            return IR.Int(n)\n        elif shrType == ""div"":\n            intVar = IR.Int(2 ** n)\n            if intVar.n == 0:\n                return IR.Int(IR.Int.max())\n            return intVar\n        else:\n            assert False\n'"
tools/SeeDot/seedot/compiler/ir/irUtil.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport numpy as np\n\nfrom seedot.compiler.ir.ir import *\n\nimport seedot.common as Common\nfrom seedot.util import *\n\n\ndef init():\n    global zero, one, negone, negmax\n\n    zero = Int(0)\n    one = Int(1)\n    negone = Int(-1)\n    negmax = Int.negMax()\n\n\ndef add(e1: Expr, e2: Expr) -> Expr: return IntBop(e1, Op.Op[\'+\'], e2)\n\n\ndef sub(e1: Expr, e2: Expr) -> Expr: return IntBop(e1, Op.Op[\'-\'], e2)\n\n\ndef mul(e1: Expr, e2: Expr) -> Expr: return IntBop(e1, Op.Op[\'*\'], e2)\n\n\ndef div(e1: Expr, e2: Expr) -> Expr: return IntBop(e1, Op.Op[\'/\'], e2)\n\n\ndef inc(e: Expr) -> Expr: return add(e, one)\n\n\ndef dec(e: Expr) -> Expr: return sub(e, one)\n\n\ndef andd(e1: Expr, e2: Expr) -> Expr: return BoolBop(e1, Op.Op[\'&&\'], e2)\n\n\ndef orr(e1: Expr, e2: Expr) -> Expr: return BoolBop(e1, Op.Op[\'||\'], e2)\n\n\ndef eq(e1: Expr, e2: Expr) -> Expr: return BoolCop(e1, Op.Op[\'==\'], e2)\n\n\ndef neq(e1: Expr, e2: Expr) -> Expr: return BoolCop(e1, Op.Op[\'!=\'], e2)\n\n\ndef lt(e1: Expr, e2: Expr) -> Expr: return BoolCop(e1, Op.Op[\'<\'],  e2)\n\n\ndef lte(e1: Expr, e2: Expr) -> Expr: return BoolCop(e1, Op.Op[\'<=\'], e2)\n\n\ndef gt(e1: Expr, e2: Expr) -> Expr: return BoolCop(e1, Op.Op[\'>\'],  e2)\n\n\ndef gte(e1: Expr, e2: Expr) -> Expr: return BoolCop(e1, Op.Op[\'>=\'], e2)\n\n\ndef bitAnd(e1: Expr, e2: Expr) -> Expr: return IntBop(e1, Op.Op[\'&\'], e2)\n\n\ndef max(e1: Expr, e2: Expr) -> Expr:\n    return CExpr(BoolCop(e1, Op.Op[\'>\'], e2), e1, e2)\n\n\ndef max_uint(e1: Expr, e2: Expr) -> Expr:\n    return max(e1, e2)\n\n\ndef max_sint(e1: Expr, e2: Expr) -> Expr:\n    return cond_zero(e1, cond_zero(e2, max_uint(e1, e2), e1), cond_zero(e2, e2, max_uint(e1, e2)))\n\n\ndef negate(e: Expr) -> Expr:\n    return IntUop(Op.Op[\'-\'], e)\n\n\ndef shl(e: Expr, n: int) -> Expr:\n    assert n >= 0\n    if n == 0:\n        return e\n    return IntBop(e, Op.Op[\'<<\'], Int(n))\n\n\ndef shrUint(e: Expr, n: int) -> Expr:\n    assert n >= 0\n    if n == 0:\n        return e\n    return IntBop(e, Op.Op[\'>>\'], Int(n))\n\n\ndef shr(e: Expr, n: int) -> Expr:\n    assert n >= 0\n    if n == 0:\n        return e\n\n    if getShrType() == ""shr"":\n        return cond_zero(e, IntBop(e, Op.Op[\'>>\'], Int(n)), IntUop(Op.Op[\'-\'], IntBop(IntUop(Op.Op[\'-\'], e), Op.Op[\'>>\'], Int(n))))\n    elif getShrType() == ""shr+"":\n        mask = Int((2 ** n) - 1)\n        return cond_zero(e, IntBop(e, Op.Op[\'>>\'], Int(n)), IntBop(IntBop(e, Op.Op[\'+\'], mask), Op.Op[\'>>\'], Int(n)))\n    elif getShrType() == ""div"":\n        intVar = Int(2 ** n)\n        if intVar.n == 0:\n            return zero\n        return div(e, intVar)\n    elif getShrType() == ""negate"":\n        return cond_zero(e, IntBop(e, Op.Op[\'>>\'], Int(n)), IntBop(IntBop(IntBop(e, Op.Op[\'^\'], negone), Op.Op[\'>>\'], Int(n)), Op.Op[\'^\'], negone))\n    else:\n        assert False\n\n\ndef shrVar(e: Expr, n: Var) -> Expr:\n    # Using ""shr"" version\n    return cond_zero(e, IntBop(e, Op.Op[\'>>\'], n), IntUop(Op.Op[\'-\'], IntBop(IntUop(Op.Op[\'-\'], e), Op.Op[\'>>\'], n)))\n\n\ndef castToInt(e: Expr):\n    return TypeCast(DataType.getIntStr(), e)\n\n\ndef castToFloat(e: Expr):\n    return TypeCast(DataType.getFloatStr(), e)\n\n\ndef addIndex(var: Var, indices: list, prefix: bool=False) -> Var:\n    if prefix == False:\n        return Var(var.idf, var.idx + indices, var.inputVar)\n    else:\n        return Var(var.idf, indices + var.idx, var.inputVar)\n\n\ndef cond_zero(e: Expr, et: Expr, ef: Expr) -> Expr:\n    return CExpr(BoolCop(e, Op.Op[\'>\'], zero), et, ef)\n\n\ndef relu(e: Expr): return cond_zero(e, e, zero)\n\n\ndef loop_shr(lhs: Expr, rhs: Expr, shape: list, iters: list, n: int) -> CmdList:\n    lhs_elt = addIndex(lhs, iters)\n    rhs_elt = addIndex(rhs, iters)\n    return loop(shape, iters, [Assn(lhs_elt, shr(rhs_elt, n))])\n\n\ndef initVarToZero(e: Expr) -> Cmd: return Assn(e, Int(0))\n\n\ndef incCmd(e: Var) -> Cmd: return Assn(e, inc(e))\n\n\ndef decCmd(e: Var) -> Cmd: return Assn(e, dec(e))\n\n\ndef concatPrograms(*prog_l, resource=0):\n    cmd_l = flatten([prog.cmd_l for prog in prog_l])\n    Res = 0\n    for x in prog_l:\n        Res = Res + x.resource\n    return Prog(cmd_l, resource=Res)\n\n# iteration\ndef loop(shape: list, iters: list, cmdl_body: CmdList, factor=0) -> CmdList:\n    cmdl_for = cmdl_body\n    for i in reversed(range(len(shape))):\n        cmdl_for = [\n            For(iters[i], 0, lt(iters[i], Int(shape[i])), cmdl_for, factor)]\n    return cmdl_for\n\n\ndef print_loop(shape: list, iters: list, cmdl_body: CmdList, factor=0) -> CmdList:\n    cmdl_for = cmdl_body\n    for i in reversed(range(len(shape))):\n        cmdl_for = [For(iters[i], 0, lt(iters[i], Int(shape[i])),\n                        cmdl_for, factor), Print(Var(\'""""\'))]\n    return cmdl_for\n'"
