file_path,api_count,code
LearningMachine.py,24,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\n\nimport os\nimport time\nimport numpy as np\nfrom tqdm import tqdm\nimport random\nimport codecs\nimport pickle as pkl\n\nfrom utils.common_utils import dump_to_pkl, load_from_pkl, get_param_num, get_trainable_param_num, \\\n    transfer_to_gpu, transform_params2tensors, get_layer_class, load_from_json, dump_to_json\nfrom utils.philly_utils import HDFSDirectTransferer, open_and_move, convert_to_tmppath, \\\n    convert_to_hdfspath, move_from_local_to_hdfs\nfrom Model import Model\nimport logging\nfrom metrics.Evaluator import Evaluator\nfrom utils.corpus_utils import get_batches\nfrom core.StreamingRecorder import StreamingRecorder\nfrom core.LRScheduler import LRScheduler\nfrom settings import ProblemTypes, Setting as st\nfrom block_zoo import Linear\nfrom block_zoo import CRF\nfrom losses.CRFLoss import CRFLoss\n\n\nclass LearningMachine(object):\n    def __init__(self, phase, conf, problem, vocab_info=None, initialize=True, use_gpu=False, **kwargs):\n        if initialize is True:\n            assert vocab_info is not None\n            self.model = Model(conf, problem, vocab_info, use_gpu)\n            if use_gpu is True:\n                self.model = nn.DataParallel(self.model)\n                self.model = transfer_to_gpu(self.model)\n            # judge the embedding matrix weight\'s device\n            emb_weight_device = list(self.model.module.layers.embedding.embeddings.values())[0].weight.device.type if isinstance(self.model, nn.DataParallel) \\\n                else list(self.model.layers.embedding.embeddings.values())[0].weight.device.type\n            device = \'GPU\' if \'cuda\' in emb_weight_device else \'CPU\'\n            logging.info(\n                ""The embedding matrix is on %s now, you can modify the weight_on_gpu parameter to change embeddings weight device."" % device)\n            logging.info(""=""*100 + \'\\n\' + ""*""*15 + ""Model Achitecture"" + ""*""*15)\n            logging.info(self.model)\n            #logging.info(""Total parameters: %d; trainable parameters: %d"" % (get_param_num(self.model), get_trainable_param_num(self.model)))\n            logging.info(""Total trainable parameters: %d"" % (get_trainable_param_num(self.model)))\n            logging.info(""Model built!"")\n        else:\n            self.model = None\n\n        self.conf = conf\n        self.problem = problem\n        self.phase = phase\n        self.use_gpu = use_gpu\n\n        # if it is a 2-class classification problem, figure out the real positive label\n        # CAUTION: multi-class classification\n        if phase != \'predict\':\n            if \'auc\' in conf.metrics:\n                if not hasattr(self.conf, \'pos_label\') or self.conf.pos_label is None:\n                    if problem.output_dict.cell_num() == 2 and \\\n                        problem.output_dict.has_cell(""0"") and problem.output_dict.has_cell(""1""):\n                        self.conf.pos_label = problem.output_dict.id(""1"")\n                        logging.debug(""Postive label (target index): %d"" % self.conf.pos_label)\n                    else:\n                        # default\n                        raise Exception(\'Please configure the positive label for auc metric at inputs/positive_label in the configuration file\')\n                else:\n                    self.conf.pos_label = problem.output_dict.id(self.conf.pos_label)\n            else:\n                self.conf.pos_label = 1  # whatever\n\n            self.metrics = conf.metrics\n            if ProblemTypes[self.problem.problem_type] == ProblemTypes.classification \\\n                or ProblemTypes[self.problem.problem_type] == ProblemTypes.sequence_tagging:\n                self.evaluator = Evaluator(metrics=self.metrics, pos_label=self.conf.pos_label, tagging_scheme=problem.tagging_scheme, label_indices=self.problem.output_dict.cell_id_map)\n            elif ProblemTypes[self.problem.problem_type] == ProblemTypes.regression:\n                self.evaluator = Evaluator(metrics=self.metrics, pos_label=self.conf.pos_label, tagging_scheme=problem.tagging_scheme, label_indices=None)\n            elif ProblemTypes[self.problem.problem_type] == ProblemTypes.mrc:\n                curr_mrc_metric = []\n                for single_mrc_metric in self.metrics:\n                    if \'mrc\' in single_mrc_metric.lower():\n                        curr_mrc_metric.append(single_mrc_metric.lower())\n                    else:\n                        curr_mrc_metric.append(\'mrc_\' + single_mrc_metric.lower())\n                self.evaluator = Evaluator(metrics=curr_mrc_metric, pos_label=self.conf.pos_label, tagging_scheme=problem.tagging_scheme, label_indices=None)\n        self.use_gpu = use_gpu\n\n        self.best_test_result = ""(No best test result yet)""\n\n    def train(self, optimizer, loss_fn):\n        self.model.train()\n        logging.info(""=""*100 + \'\\n\' + ""*""*15 + \'Prepare data for training\' + ""*""*15)\n\n        valid_data, valid_length, valid_target = self.problem.encode(self.conf.valid_data_path, self.conf.file_columns,\n            self.conf.input_types, self.conf.file_with_col_header, self.conf.object_inputs, self.conf.answer_column_name, max_lengths=self.conf.max_lengths,\n            min_sentence_len = self.conf.min_sentence_len, extra_feature = self.conf.extra_feature,fixed_lengths=self.conf.fixed_lengths, file_format=\'tsv\',\n            show_progress=True if self.conf.mode == \'normal\' else False, cpu_num_workers=self.conf.cpu_num_workers, chunk_size=self.conf.chunk_size)\n\n        if self.conf.test_data_path is not None:\n            test_data, test_length, test_target = self.problem.encode(self.conf.test_data_path, self.conf.file_columns, \n            self.conf.input_types, self.conf.file_with_col_header, self.conf.object_inputs, self.conf.answer_column_name, max_lengths=self.conf.max_lengths,\n            min_sentence_len = self.conf.min_sentence_len, extra_feature = self.conf.extra_feature,fixed_lengths=self.conf.fixed_lengths, file_format=\'tsv\', \n            show_progress=True if self.conf.mode == \'normal\' else False, cpu_num_workers=self.conf.cpu_num_workers, chunk_size=self.conf.chunk_size)\n\n        stop_training = False\n        epoch = 1\n        best_result = None\n        show_result_cnt = 0\n        lr_scheduler = LRScheduler(optimizer, self.conf.lr_decay, self.conf.minimum_lr, self.conf.epoch_start_lr_decay)\n\n        if ProblemTypes[self.problem.problem_type] == ProblemTypes.classification:\n            streaming_recoder = StreamingRecorder([\'prediction\', \'pred_scores\', \'pred_scores_all\', \'target\'])\n        elif ProblemTypes[self.problem.problem_type] == ProblemTypes.sequence_tagging:\n            streaming_recoder = StreamingRecorder([\'prediction\', \'pred_scores\', \'target\'])\n        elif ProblemTypes[self.problem.problem_type] == ProblemTypes.regression:\n            streaming_recoder = StreamingRecorder([\'prediction\', \'target\'])\n        elif ProblemTypes[self.problem.problem_type] == ProblemTypes.mrc:\n            streaming_recoder = StreamingRecorder([\'prediction\', \'answer_text\'])\n\n        logging.info(""="" * 100 + \'\\n\' + ""*"" * 15 + \'Start training\' + ""*"" * 15)\n        while not stop_training and epoch <= self.conf.max_epoch:\n            logging.info(\'Training: Epoch \' + str(epoch))\n            train_data_generator = self._get_training_data_generator()\n            part_index = 1\n            for train_data, train_length, train_target in train_data_generator:\n                logging.debug(\'Training: Epoch %s Part %s\'%(epoch, part_index))\n                part_index += 1\n                data_batches, length_batches, target_batches = \\\n                    get_batches(self.problem, train_data, train_length, train_target, self.conf.batch_size_total,\n                        self.conf.input_types, None, permutate=True, transform_tensor=True)\n\n                whole_batch_num = len(target_batches)\n                valid_batch_num = min(self.conf.steps_per_validation, whole_batch_num)\n                small_batch_num = whole_batch_num\n                valid_batch_num_show = valid_batch_num\n                batch_num_to_show_results = self.conf.batch_num_to_show_results\n                if torch.cuda.device_count() > 1:\n                    batch_num_to_show_results *= torch.cuda.device_count() # total batch num overall all the gpus to log \n                    small_batch_num *= torch.cuda.device_count()       # total batch num over all the gpus\n                    valid_batch_num_show *= torch.cuda.device_count()      # total batch num over all the gpus to do validation\n                \n                streaming_recoder.clear_records()\n                all_costs = []\n\n                logging.info(\'There are %d batches during current period; validation are conducted every %d batch\' % (small_batch_num, valid_batch_num_show))\n\n                if self.conf.mode == \'normal\':\n                    progress = tqdm(range(len(target_batches)))\n                elif self.conf.mode == \'philly\':\n                    progress = range(len(target_batches))\n                for i in progress:\n                    # the result shape: for classification: [batch_size, # of classes]; for sequence tagging: [batch_size, seq_len, # of tags]\n                    param_list, inputs_desc, length_desc = transform_params2tensors(data_batches[i], length_batches[i])\n                    logits = self.model(inputs_desc, length_desc, *param_list)\n\n                    logits_softmax = {}\n                    if isinstance(self.model, nn.DataParallel):\n                        for tmp_output_layer_id in self.model.module.output_layer_id:\n                            if isinstance(self.model.module.layers[tmp_output_layer_id], Linear) and \\\n                                    (not self.model.module.layers[tmp_output_layer_id].layer_conf.last_hidden_softmax):\n                                logits_softmax[tmp_output_layer_id] = nn.functional.softmax(\n                                    logits[tmp_output_layer_id], dim=-1)\n                            elif isinstance(get_layer_class(self.model, tmp_output_layer_id), CRF):\n                                pass\n                            else:\n                                logits_softmax[tmp_output_layer_id] = logits[tmp_output_layer_id]\n                    else:\n                        for tmp_output_layer_id in self.model.output_layer_id:\n                            if isinstance(self.model.layers[tmp_output_layer_id], Linear) and \\\n                                    (not self.model.layers[tmp_output_layer_id].layer_conf.last_hidden_softmax):\n                                logits_softmax[tmp_output_layer_id] = nn.functional.softmax(\n                                    logits[tmp_output_layer_id], dim=-1)\n                            elif isinstance(get_layer_class(self.model, tmp_output_layer_id), CRF):\n                                pass\n                            else:\n                                logits_softmax[tmp_output_layer_id] = logits[tmp_output_layer_id]\n\n                    # check the output\n                    if ProblemTypes[self.problem.problem_type] == ProblemTypes.classification:\n                        logits = list(logits.values())[0]\n                        logits_softmax = list(logits_softmax.values())[0]\n                        assert len(logits_softmax.shape) == 2, \'The dimension of your output is %s, but we need [batch_size*GPUs, class num]\' % (str(list(logits_softmax.shape)))\n                        assert logits_softmax.shape[1] == self.problem.output_target_num(), \'The dimension of your output layer %d is inconsistent with your type number %d!\' % (logits_softmax.shape[1], self.problem.output_target_num())\n                        # for auc metric\n                        prediction_scores = logits_softmax[:, self.conf.pos_label].cpu().data.numpy()\n                        if self.evaluator.has_auc_type_specific:\n                            prediction_scores_all = logits_softmax.cpu().data.numpy()\n                        else:\n                            prediction_scores_all = None\n                    elif ProblemTypes[self.problem.problem_type] == ProblemTypes.sequence_tagging:\n                        logits = list(logits.values())[0]\n                        if not isinstance(get_layer_class(self.model, tmp_output_layer_id), CRF):\n                            logits_softmax = list(logits_softmax.values())[0]\n                            assert len(logits_softmax.shape) == 3, \'The dimension of your output is %s, but we need [batch_size*GPUs, sequence length, representation dim]\' % (str(list(logits_softmax.shape)), )\n                        prediction_scores = None\n                        prediction_scores_all = None\n                    elif ProblemTypes[self.problem.problem_type] == ProblemTypes.regression:\n                        logits = list(logits.values())[0]\n                        logits_softmax = list(logits_softmax.values())[0]\n                        assert len(logits_softmax.shape) == 2 and logits_softmax.shape[1] == 1, \'The dimension of your output is %s, but we need [batch_size*GPUs, 1]\' % (str(list(logits_softmax.shape)))\n                        prediction_scores = None\n                        prediction_scores_all = None\n                    elif ProblemTypes[self.problem.problem_type] == ProblemTypes.mrc:\n                        for single_value in logits_softmax.values():\n                            assert len(single_value.shape) == 3, \'The dimension of your output is %s, but we need [batch_size*GPUs, sequence_len, 1]\' % (str(list(single_value.shape)))\n                        prediction_scores = None\n                        prediction_scores_all = None\n\n                    logits_flat = dict()\n                    if ProblemTypes[self.problem.problem_type] == ProblemTypes.sequence_tagging:\n                        # Transform output shapes for metric evaluation\n                        # for seq_tag_f1 metric\n                        if isinstance(get_layer_class(self.model, tmp_output_layer_id), CRF):\n                            forward_score, scores, masks, tag_seq, transitions, layer_conf = logits\n                            prediction_indices = tag_seq.cpu().numpy()\n                            streaming_recoder.record_one_row([self.problem.decode(prediction_indices, length_batches[i][\'target\'][self.conf.answer_column_name[0]].numpy()),\n                                                            prediction_scores, self.problem.decode(\n                                    target_batches[i][self.conf.answer_column_name[0]],\n                                    length_batches[i][\'target\'][self.conf.answer_column_name[0]].numpy())], keep_dim=False)\n\n                        else:\n                            prediction_indices = logits_softmax.data.max(2)[1].cpu().numpy()    # [batch_size, seq_len]\n                            # pytorch\'s CrossEntropyLoss only support this\n                            logits_flat[self.conf.output_layer_id[0]] = logits.view(-1, logits.size(2))  # [batch_size * seq_len, # of tags]\n                            streaming_recoder.record_one_row([self.problem.decode(prediction_indices, length_batches[i][\'target\'][self.conf.answer_column_name[0]].numpy()),\n                                                            prediction_scores, self.problem.decode(\n                                    target_batches[i][self.conf.answer_column_name[0]],\n                                    length_batches[i][\'target\'][self.conf.answer_column_name[0]].numpy())], keep_dim=False)\n\n                            target_batches[i][self.conf.answer_column_name[0]] = target_batches[i][\n                                self.conf.answer_column_name[0]].reshape(-1)\n\n                    elif ProblemTypes[self.problem.problem_type] == ProblemTypes.classification:\n                        prediction_indices = logits_softmax.detach().max(1)[1].cpu().numpy()\n                        # Should not decode!\n                        streaming_recoder.record_one_row([prediction_indices, prediction_scores, prediction_scores_all, target_batches[i][self.conf.answer_column_name[0]].numpy()])\n                        logits_flat[self.conf.output_layer_id[0]] = logits\n                    elif ProblemTypes[self.problem.problem_type] == ProblemTypes.regression:\n                        temp_logits_flat = logits.squeeze(1)\n                        prediction_scores = temp_logits_flat.detach().cpu().numpy()\n                        streaming_recoder.record_one_row([prediction_scores, target_batches[i][self.conf.answer_column_name[0]].numpy()])\n                        logits_flat[self.conf.output_layer_id[0]] = temp_logits_flat\n                    elif ProblemTypes[self.problem.problem_type] == ProblemTypes.mrc:\n                        for key, value in logits.items():\n                            logits[key] = value.squeeze()\n                        for key, value in logits_softmax.items():\n                            logits_softmax[key] = value.squeeze()\n                        passage_identify = None\n                        for type_key in data_batches[i].keys():\n                            if \'p\' in type_key.lower():\n                                passage_identify = type_key\n                                break\n                        if not passage_identify:\n                            raise Exception(\'MRC task need passage information.\')\n                        prediction = self.problem.decode(logits_softmax, lengths=length_batches[i][passage_identify],\n                                                        batch_data=data_batches[i][passage_identify])\n                        logits_flat = logits\n                        mrc_answer_target = None\n                        for single_target in target_batches[i]:\n                            if isinstance(target_batches[i][single_target][0], str):\n                                mrc_answer_target = target_batches[i][single_target]\n                        streaming_recoder.record_one_row([prediction, mrc_answer_target])\n\n                    if self.use_gpu:\n                        for single_target in self.conf.answer_column_name:\n                            if isinstance(target_batches[i][single_target], torch.Tensor):\n                                target_batches[i][single_target] = transfer_to_gpu(target_batches[i][single_target])\n                    if isinstance(loss_fn.loss_fn[0], CRFLoss):\n                        loss = loss_fn.loss_fn[0](forward_score, scores, masks, list(target_batches[i].values())[0], transitions, layer_conf)\n                    else:\n                        loss = loss_fn(logits_flat, target_batches[i])\n\n                    all_costs.append(loss.item())\n                    optimizer.zero_grad()\n                    loss.backward()\n                    if self.conf.clip_grad_norm_max_norm != -1:\n                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.conf.clip_grad_norm_max_norm)\n                        if isinstance(self.model, nn.DataParallel):\n                            torch.nn.utils.clip_grad_norm_(self.model.module.layers[\'embedding\'].get_parameters(), self.conf.clip_grad_norm_max_norm)\n                        else:\n                            torch.nn.utils.clip_grad_norm_(self.model.layers[\'embedding\'].get_parameters(), self.conf.clip_grad_norm_max_norm)\n                    optimizer.step()\n\n                    del loss, logits, logits_softmax, logits_flat\n                    del prediction_scores\n                    if ProblemTypes[self.problem.problem_type] == ProblemTypes.sequence_tagging \\\n                            or ProblemTypes[self.problem.problem_type] == ProblemTypes.classification:\n                        del prediction_indices\n\n                    if show_result_cnt == batch_num_to_show_results:\n                        if ProblemTypes[self.problem.problem_type] == ProblemTypes.classification:\n                            result = self.evaluator.evaluate(streaming_recoder.get(\'target\'),\n                                streaming_recoder.get(\'prediction\'), y_pred_pos_score=streaming_recoder.get(\'pred_scores\'),\n                                y_pred_scores_all=streaming_recoder.get(\'pred_scores_all\'), formatting=True)\n                        elif ProblemTypes[self.problem.problem_type] == ProblemTypes.sequence_tagging:\n                            result = self.evaluator.evaluate(streaming_recoder.get(\'target\'),\n                                streaming_recoder.get(\'prediction\'), y_pred_pos_score=streaming_recoder.get(\'pred_scores\'),\n                                formatting=True)\n                        elif ProblemTypes[self.problem.problem_type] == ProblemTypes.regression:\n                            result = self.evaluator.evaluate(streaming_recoder.get(\'target\'),\n                                streaming_recoder.get(\'prediction\'), y_pred_pos_score=None, y_pred_scores_all=None, formatting=True)\n                        elif ProblemTypes[self.problem.problem_type] == ProblemTypes.mrc:\n                            result = self.evaluator.evaluate(streaming_recoder.get(\'answer_text\'), streaming_recoder.get(\'prediction\'),\n                                                                y_pred_pos_score=None, y_pred_scores_all=None, formatting=True)\n\n                        if torch.cuda.device_count() > 1:\n                            logging.info(""Epoch %d batch idx: %d; lr: %f; since last log, loss=%f; %s"" % \\\n                                (epoch, i * torch.cuda.device_count(), lr_scheduler.get_lr(), np.mean(all_costs), result))\n                        else:\n                            logging.info(""Epoch %d batch idx: %d; lr: %f; since last log, loss=%f; %s"" % \\\n                                (epoch, i, lr_scheduler.get_lr(), np.mean(all_costs), result))\n                        show_result_cnt = 0\n                        # The loss and other metrics printed during a training epoch are just the result of part of the training data.\n                        all_costs = []\n                        streaming_recoder.clear_records()\n\n                    if (i != 0 and i % valid_batch_num == 0) or i == len(target_batches) - 1:\n                        torch.cuda.empty_cache()    # actually useless\n                        logging.info(\'Valid & Test : Epoch \' + str(epoch))\n                        new_result = self.evaluate(valid_data, valid_length, valid_target,\n                            self.conf.input_types, self.evaluator, loss_fn, pad_ids=None, cur_best_result=best_result,\n                            model_save_path=self.conf.model_save_path, phase=""valid"", epoch=epoch)\n                        renew_flag = best_result != new_result\n                        best_result = new_result\n\n                        if renew_flag and self.conf.test_data_path is not None:\n                            self.evaluate(test_data, test_length, test_target,\n                                self.conf.input_types, self.evaluator, loss_fn, pad_ids=None, phase=""test"", epoch=epoch)\n                        self.model.train()\n                    show_result_cnt += 1\n\n                del data_batches, length_batches, target_batches\n            lr_scheduler.step()\n            epoch += 1\n\n    def test(self, loss_fn, test_data_path=None, predict_output_path=None):\n        if test_data_path is None:\n            # test_data_path in the parameter is prior to self.conf.test_data_path\n            test_data_path = self.conf.test_data_path\n\n        if not test_data_path.endswith(\'.pkl\'):\n            test_data, test_length, test_target = self.problem.encode(test_data_path, self.conf.file_columns, self.conf.input_types,\n                self.conf.file_with_col_header, self.conf.object_inputs, self.conf.answer_column_name, max_lengths=self.conf.max_lengths,\n                min_sentence_len = self.conf.min_sentence_len, extra_feature = self.conf.extra_feature,fixed_lengths=self.conf.fixed_lengths, file_format=\'tsv\',\n                show_progress=True if self.conf.mode == \'normal\' else False, cpu_num_workers=self.conf.cpu_num_workers, chunk_size=self.conf.chunk_size)\n        else:\n            test_pkl_data = load_from_pkl(test_data_path)\n            test_data, test_length, test_target = test_pkl_data[\'data\'], test_pkl_data[\'length\'], test_pkl_data[\'target\']\n\n        if not predict_output_path:\n            self.evaluate(test_data, test_length, test_target,\n                self.conf.input_types, self.evaluator, loss_fn, pad_ids=None, phase=""test"")\n        else:\n            self.evaluate(test_data, test_length, test_target,\n                self.conf.input_types, self.evaluator, loss_fn, pad_ids=None, phase=""test"",\n                origin_data_path=test_data_path, predict_output_path=predict_output_path)\n\n    def evaluate(self, data, length, target, input_types, evaluator,\n                 loss_fn, pad_ids=None, cur_best_result=None, model_save_path=None, phase="""", epoch=None, origin_data_path=None, predict_output_path=None):\n        """"""\n\n        Args:\n            qp_net:\n            epoch:\n            data:\n                    {\n                    \'string1\': {\n                        \'word1\': [...],\n                        \'postage_feature1\': [..]\n                        }\n                    \'string2\': {\n                        \'word1\': [...],\n                        \'postage_feature1\': [..]\n                    }\n            lengths:\n                    {\n                    \'string1\':   [...],\n                    \'string2\':   [...]\n                    }\n            target:  [...]\n            input_types:  {\n                      ""word"": {\n                        ""cols"": [\n                          ""word1"",\n                          ""word2""\n                        ],\n                        ""dim"": 300\n                      },\n                      ""postag"": {\n                        ""cols"": [""postag_feature1"", ""postag_feature2""],\n                        ""dim"": 20\n                      }\n            origin_data_path:\n            predict_output_path: if predict_output_path exists, output the prediction result.\n\n        Returns:\n\n        """"""\n        assert not (predict_output_path and not origin_data_path)\n        if predict_output_path:\n            to_predict = True\n        else:\n            to_predict = False\n\n        logging.info(""Starting %s ..."" % phase)\n        self.model.eval()\n        with torch.no_grad():\n            data_batches, length_batches, target_batches = \\\n                get_batches(self.problem, data, length, target, self.conf.batch_size_total, input_types, pad_ids, permutate=False, transform_tensor=True)\n\n            if ProblemTypes[self.problem.problem_type] == ProblemTypes.classification:\n                streaming_recoder = StreamingRecorder([\'prediction\', \'pred_scores\', \'pred_scores_all\', \'target\'])\n            elif ProblemTypes[self.problem.problem_type] == ProblemTypes.sequence_tagging:\n                streaming_recoder = StreamingRecorder([\'prediction\', \'pred_scores\', \'target\'])\n            elif ProblemTypes[self.problem.problem_type] == ProblemTypes.regression:\n                streaming_recoder = StreamingRecorder([\'prediction\', \'target\'])\n            elif ProblemTypes[self.problem.problem_type] == ProblemTypes.mrc:\n                streaming_recoder = StreamingRecorder([\'prediction\', \'answer_text\'])\n\n            if to_predict:\n                predict_stream_recoder = StreamingRecorder(self.conf.predict_fields)\n                fin = open(origin_data_path, \'r\', encoding=\'utf-8\')\n                if predict_output_path.startswith(\'/hdfs/\'):\n                    direct_hdfs_path = convert_to_hdfspath(predict_output_path)\n                    local_tmp_path = convert_to_tmppath(predict_output_path)\n                    fout = open(local_tmp_path, \'w\', encoding=\'utf-8\')\n                else:\n                    direct_hdfs_path = None\n                    fout = open(predict_output_path, \'w\', encoding=\'utf-8\')\n                if self.conf.file_with_col_header:\n                    title_line = fin.readline()\n                    fout.write(title_line)\n\n            temp_key_list = list(length_batches[0].keys())\n            if \'target\' in temp_key_list:\n                temp_key_list.remove(\'target\')\n            key_random = random.choice(temp_key_list)\n            loss_recoder = StreamingRecorder([\'loss\'])\n            if self.conf.mode == \'normal\':\n                progress = tqdm(range(len(target_batches)))\n            elif self.conf.mode == \'philly\':\n                progress = range(len(target_batches))\n            for i in progress:\n                # batch_size_actual = target_batches[i].size(0)\n\n                param_list, inputs_desc, length_desc = transform_params2tensors(data_batches[i], length_batches[i])\n                logits = self.model(inputs_desc, length_desc, *param_list)\n\n                logits_softmax = {}\n                if isinstance(self.model, nn.DataParallel):\n                    for tmp_output_layer_id in self.model.module.output_layer_id:\n                        if isinstance(self.model.module.layers[tmp_output_layer_id], Linear) and \\\n                                (not self.model.module.layers[tmp_output_layer_id].layer_conf.last_hidden_softmax):\n                            logits_softmax[tmp_output_layer_id] = nn.functional.softmax(\n                                logits[tmp_output_layer_id], dim=-1)\n                        else:\n                            logits_softmax[tmp_output_layer_id] = logits[tmp_output_layer_id]\n                else:\n                    for tmp_output_layer_id in self.model.output_layer_id:\n                        if isinstance(self.model.layers[tmp_output_layer_id], Linear) and \\\n                                (not self.model.layers[tmp_output_layer_id].layer_conf.last_hidden_softmax):\n                            logits_softmax[tmp_output_layer_id] = nn.functional.softmax(\n                                logits[tmp_output_layer_id], dim=-1)\n                        else:\n                            logits_softmax[tmp_output_layer_id] = logits[tmp_output_layer_id]\n\n                if ProblemTypes[self.problem.problem_type] == ProblemTypes.classification:\n                    logits = list(logits.values())[0]\n                    logits_softmax = list(logits_softmax.values())[0]\n                    # for auc metric\n                    prediction_pos_scores = logits_softmax[:, self.conf.pos_label].cpu().data.numpy()\n                    if self.evaluator.has_auc_type_specific:\n                        prediction_scores_all = logits_softmax.cpu().data.numpy()\n                    else:\n                        prediction_scores_all = None\n                else:\n                    prediction_pos_scores = None\n                    prediction_scores_all = None\n\n                logits_flat = {}\n                if ProblemTypes[self.problem.problem_type] == ProblemTypes.sequence_tagging:\n                    logits = list(logits.values())[0]\n                    if isinstance(get_layer_class(self.model, tmp_output_layer_id), CRF):\n                        forward_score, scores, masks, tag_seq, transitions, layer_conf = logits\n                        prediction_indices = tag_seq.cpu().numpy()\n                        streaming_recoder.record_one_row(\n                            [self.problem.decode(prediction_indices, length_batches[i][\'target\'][self.conf.answer_column_name[0]].numpy()),\n                             prediction_pos_scores,\n                             self.problem.decode(target_batches[i], length_batches[i][\'target\'][self.conf.answer_column_name[0]].numpy())],\n                            keep_dim=False)\n                    else:\n                        logits_softmax = list(logits_softmax.values())[0]\n                        # Transform output shapes for metric evaluation\n                        # for seq_tag_f1 metric\n                        prediction_indices = logits_softmax.data.max(2)[1].cpu().numpy()  # [batch_size, seq_len]\n                        # pytorch\'s CrossEntropyLoss only support this\n                        logits_flat[self.conf.output_layer_id[0]] = logits.view(-1, logits.size(2))  # [batch_size * seq_len, # of tags]\n                        streaming_recoder.record_one_row(\n                            [self.problem.decode(prediction_indices, length_batches[i][\'target\'][self.conf.answer_column_name[0]].numpy()),\n                             prediction_pos_scores,\n                             self.problem.decode(target_batches[i], length_batches[i][\'target\'][self.conf.answer_column_name[0]].numpy())],\n                            keep_dim=False)\n\n                        target_batches[i][self.conf.answer_column_name[0]] = target_batches[i][\n                            self.conf.answer_column_name[0]].reshape(-1)  # [batch_size * seq_len]\n\n                    if to_predict:\n                        prediction_batch = self.problem.decode(prediction_indices, length_batches[i][key_random].numpy())\n                        for prediction_sample in prediction_batch:\n                            predict_stream_recoder.record(\'prediction\', "" "".join(prediction_sample))\n\n                elif ProblemTypes[self.problem.problem_type] == ProblemTypes.classification:\n                    prediction_indices = logits_softmax.data.max(1)[1].cpu().numpy()\n                    # Should not decode!\n                    streaming_recoder.record_one_row([prediction_indices, prediction_pos_scores, prediction_scores_all, target_batches[i][self.conf.answer_column_name[0]].numpy()])\n                    logits_flat[self.conf.output_layer_id[0]] = logits\n\n                    if to_predict:\n                        for field in self.conf.predict_fields:\n                            if field == \'prediction\':\n                                predict_stream_recoder.record(field, self.problem.decode(prediction_indices, length_batches[i][key_random].numpy()))\n                            elif field == \'confidence\':\n                                prediction_scores = logits_softmax.cpu().data.numpy()\n                                for prediction_score, prediction_idx in zip(prediction_scores, prediction_indices):\n                                    predict_stream_recoder.record(field, prediction_score[prediction_idx])\n                            elif field.startswith(\'confidence\') and field.find(\'@\') != -1:\n                                label_specified = field.split(\'@\')[1]\n                                label_specified_idx = self.problem.output_dict.id(label_specified)\n                                confidence_specified = torch.index_select(logits_softmax.cpu(), 1, torch.tensor([label_specified_idx], dtype=torch.long)).squeeze(1)\n                                predict_stream_recoder.record(field, confidence_specified.data.numpy())\n\n                elif ProblemTypes[self.problem.problem_type] == ProblemTypes.regression:\n                    logits = list(logits.values())[0]\n                    # logits_softmax is unuseful for regression task!\n                    logits_softmax = list(logits_softmax.values())[0]\n                    temp_logits_flat = logits.squeeze(1)\n                    prediction_scores = temp_logits_flat.detach().cpu().numpy()\n                    streaming_recoder.record_one_row([prediction_scores, target_batches[i][self.conf.answer_column_name[0]].numpy()])\n                    logits_flat[self.conf.output_layer_id[0]] = temp_logits_flat\n                    if to_predict:\n                        predict_stream_recoder.record_one_row([prediction_scores])\n\n                elif ProblemTypes[self.problem.problem_type] == ProblemTypes.mrc:\n                    for key, value in logits.items():\n                        logits[key] = value.squeeze()\n                    for key, value in logits_softmax.items():\n                        logits_softmax[key] = value.squeeze()\n                    passage_identify = None\n                    for type_key in data_batches[i].keys():\n                        if \'p\' in type_key.lower():\n                            passage_identify = type_key\n                            break\n                    if not passage_identify:\n                        raise Exception(\'MRC task need passage information.\')\n                    prediction = self.problem.decode(logits_softmax, lengths=length_batches[i][passage_identify],\n                                                     batch_data=data_batches[i][passage_identify])\n                    logits_flat = logits\n                    mrc_answer_target = None\n                    for single_target in target_batches[i]:\n                        if isinstance(target_batches[i][single_target][0], str):\n                            mrc_answer_target = target_batches[i][single_target]\n                    streaming_recoder.record_one_row([prediction, mrc_answer_target])\n\n                    if to_predict:\n                        predict_stream_recoder.record_one_row([prediction])\n\n                if to_predict:\n                    if ProblemTypes[self.problem.problem_type] == ProblemTypes.mrc:\n                        logits_len = len(list(logits.values())[0])\n                    elif ProblemTypes[self.problem.problem_type] == ProblemTypes.sequence_tagging and isinstance(get_layer_class(self.model, tmp_output_layer_id), CRF):\n                        # for sequence_tagging task, logits is tuple type which index 3 is tag_seq [batch_size*seq_len]\n                        logits_len = logits[3].size(0)\n                    else:\n                        logits_len = len(logits)\n                    for sample_idx in range(logits_len):\n                        while True:\n                            sample = fin.readline().rstrip()\n                            line_split = list(filter(lambda x: len(x) > 0, sample.rstrip().split(\'\\t\')))\n                            if self.problem.file_column_num is None or len(line_split) == self.problem.file_column_num:\n                                break\n\n                        fout.write(""%s\\t%s\\n"" % (sample,\n                            ""\\t"".join([str(predict_stream_recoder.get(field)[sample_idx]) for field in self.conf.predict_fields])))\n                    predict_stream_recoder.clear_records()\n\n                if self.use_gpu:\n                    for single_target in self.conf.answer_column_name:\n                        if isinstance(target_batches[i][single_target], torch.Tensor):\n                            target_batches[i][single_target] = transfer_to_gpu(target_batches[i][single_target])\n                if isinstance(loss_fn.loss_fn[0], CRFLoss):\n                    loss = loss_fn.loss_fn[0](forward_score, scores, masks, list(target_batches[i].values())[0], transitions, layer_conf)\n                else:\n                    loss = loss_fn(logits_flat, target_batches[i])\n                loss_recoder.record(\'loss\', loss.item())\n\n                del loss, logits, logits_softmax, logits_flat\n                del prediction_pos_scores\n                if ProblemTypes[self.problem.problem_type] == ProblemTypes.sequence_tagging or ProblemTypes[self.problem.problem_type] == ProblemTypes.classification:\n                    del prediction_indices\n\n            del data_batches, length_batches, target_batches\n\n            if ProblemTypes[self.problem.problem_type] == ProblemTypes.classification:\n                result = self.evaluator.evaluate(streaming_recoder.get(\'target\'), streaming_recoder.get(\'prediction\'),\n                    y_pred_pos_score=streaming_recoder.get(\'pred_scores\'),\n                    y_pred_scores_all=streaming_recoder.get(\'pred_scores_all\'), formatting=True)\n            elif ProblemTypes[self.problem.problem_type] == ProblemTypes.sequence_tagging:\n                result = self.evaluator.evaluate(streaming_recoder.get(\'target\'), streaming_recoder.get(\'prediction\'), y_pred_pos_score=streaming_recoder.get(\'pred_scores\'), formatting=True)\n            elif ProblemTypes[self.problem.problem_type] == ProblemTypes.regression:\n                result = self.evaluator.evaluate(streaming_recoder.get(\'target\'), streaming_recoder.get(\'prediction\'), y_pred_pos_score=None, formatting=True)\n            elif ProblemTypes[self.problem.problem_type] == ProblemTypes.mrc:\n                result = self.evaluator.evaluate(streaming_recoder.get(\'answer_text\'), streaming_recoder.get(\'prediction\'),\n                                                 y_pred_pos_score=None, y_pred_scores_all=None, formatting=True)\n\n            if epoch:\n                logging.info(""Epoch %d, %s %s loss: %f"" % (epoch, phase, result, loss_recoder.get(\'loss\', \'mean\')))\n            else:\n                logging.info(""%s %s loss: %f"" % (phase, result, loss_recoder.get(\'loss\', \'mean\')))\n\n            if phase == \'valid\':\n                cur_result = evaluator.get_first_metric_result()\n                if self.evaluator.compare(cur_result, cur_best_result) == 1:\n                    logging.info(\n                        \'Cur result %f is better than previous best result %s, renew the best model now...\' % (cur_result, ""%f"" % cur_best_result if cur_best_result else ""None""))\n                    if model_save_path is not None:\n                        if self.conf.mode == \'philly\' and model_save_path.startswith(\'/hdfs/\'):\n                            with HDFSDirectTransferer(model_save_path, with_hdfs_command=True) as transferer:\n                                if isinstance(self.model, nn.DataParallel):\n                                    transferer.torch_save(self.model.module)\n                                else:\n                                    transferer.torch_save(self.model)\n                        else:\n                            if not os.path.exists(os.path.dirname(model_save_path)):\n                                os.makedirs(os.path.dirname(model_save_path))\n                            if isinstance(self.model, nn.DataParallel):\n                                torch.save(self.model.module, model_save_path, pickle_protocol=pkl.HIGHEST_PROTOCOL)\n                            else:\n                                torch.save(self.model, model_save_path, pickle_protocol=pkl.HIGHEST_PROTOCOL)\n                        logging.info(""Best model saved to %s"" % model_save_path)\n                    cur_best_result = cur_result\n                else:\n                    logging.info(\'Cur result %f is no better than previous best result %f\' % (cur_result, cur_best_result))\n\n        if to_predict:\n            fin.close()\n            fout.close()\n            if direct_hdfs_path:\n                move_from_local_to_hdfs(local_tmp_path, direct_hdfs_path)\n\n        return cur_best_result\n\n    def predict(self, predict_data_path, output_path, file_columns, predict_fields=[\'prediction\']):\n        """""" prediction\n\n        Args:\n            predict_data_path:\n            predict_fields: default: only prediction. For classification and regression tasks, prediction_confidence is also supported.\n\n        Returns:\n\n        """"""\n        if predict_data_path is None:\n            predict_data_path = self.conf.predict_data_path\n\n        predict_data, predict_length, _ = self.problem.encode(predict_data_path, file_columns, self.conf.input_types,\n            self.conf.file_with_col_header,self.conf.object_inputs, None, min_sentence_len=self.conf.min_sentence_len,\n            extra_feature=self.conf.extra_feature,max_lengths=self.conf.max_lengths, fixed_lengths=self.conf.fixed_lengths,\n            file_format=\'tsv\', show_progress=True if self.conf.mode == \'normal\' else False, \n            cpu_num_workers=self.conf.cpu_num_workers, chunk_size=self.conf.chunk_size)\n\n        logging.info(""Starting predict ..."")\n        self.model.eval()\n        with torch.no_grad():\n            data_batches, length_batches, _ = \\\n                get_batches(self.problem, predict_data, predict_length, None, self.conf.batch_size_total,\n                    self.conf.input_types, None, permutate=False, transform_tensor=True)\n\n            streaming_recoder = StreamingRecorder(predict_fields)\n\n            fin = open(predict_data_path, \'r\', encoding=\'utf-8\')\n            with open_and_move(output_path) as fout:\n                if self.conf.file_with_col_header:\n                    title_line = fin.readline()\n                    fout.write(title_line)\n                key_random = random.choice(list(length_batches[0].keys()).remove(\'target\') if \'target\' in list(length_batches[0].keys()) else list(length_batches[0].keys()))\n                if self.conf.mode == \'normal\':\n                    progress = tqdm(range(len(data_batches)))\n                elif self.conf.mode == \'philly\':\n                    progress = range(len(data_batches))\n                for i in progress:\n                    # batch_size_actual = target_batches[i].size(0)\n                    param_list, inputs_desc, length_desc = transform_params2tensors(data_batches[i], length_batches[i])\n                    logits = self.model(inputs_desc, length_desc, *param_list)\n\n                    logits_softmax = {}\n                    if isinstance(self.model, nn.DataParallel):\n                        for tmp_output_layer_id in self.model.module.output_layer_id:\n                            if isinstance(self.model.module.layers[tmp_output_layer_id], Linear) and \\\n                                    (not self.model.module.layers[tmp_output_layer_id].layer_conf.last_hidden_softmax):\n                                logits_softmax[tmp_output_layer_id] = nn.functional.softmax(\n                                    logits[tmp_output_layer_id], dim=-1)\n                            else:\n                                logits_softmax[tmp_output_layer_id] = logits[tmp_output_layer_id]\n                    else:\n                        for tmp_output_layer_id in self.model.output_layer_id:\n                            if isinstance(self.model.layers[tmp_output_layer_id], Linear) and \\\n                                    (not self.model.layers[tmp_output_layer_id].layer_conf.last_hidden_softmax):\n                                logits_softmax[tmp_output_layer_id] = nn.functional.softmax(\n                                    logits[tmp_output_layer_id], dim=-1)\n                            else:\n                                logits_softmax[tmp_output_layer_id] = logits[tmp_output_layer_id]\n\n                    if ProblemTypes[self.problem.problem_type] == ProblemTypes.sequence_tagging:\n                        logits = list(logits.values())[0]\n                        if isinstance(get_layer_class(self.model, tmp_output_layer_id), CRF):\n                            forward_score, scores, masks, tag_seq, transitions, layer_conf = logits\n                            prediction_indices = tag_seq.cpu().numpy()\n                        else:\n                            logits_softmax = list(logits_softmax.values())[0]\n                            # Transform output shapes for metric evaluation\n                            # for seq_tag_f1 metric\n                            prediction_indices = logits_softmax.data.max(2)[1].cpu().numpy()  # [batch_size, seq_len]\n                        prediction_batch = self.problem.decode(prediction_indices, length_batches[i][key_random].numpy())\n                        for prediction_sample in prediction_batch:\n                            streaming_recoder.record(\'prediction\', "" "".join(prediction_sample))\n                    elif ProblemTypes[self.problem.problem_type] == ProblemTypes.classification:\n                        logits = list(logits.values())[0]\n                        logits_softmax = list(logits_softmax.values())[0]\n                        prediction_indices = logits_softmax.data.max(1)[1].cpu().numpy()\n\n                        for field in predict_fields:\n                            if field == \'prediction\':\n                                streaming_recoder.record(field,\n                                    self.problem.decode(prediction_indices, length_batches[i][key_random].numpy()))\n                            elif field == \'confidence\':\n                                prediction_scores = logits_softmax.cpu().data.numpy()\n                                for prediction_score, prediction_idx in zip(prediction_scores, prediction_indices):\n                                    streaming_recoder.record(field, prediction_score[prediction_idx])\n                            elif field.startswith(\'confidence\') and field.find(\'@\') != -1:\n                                label_specified = field.split(\'@\')[1]\n                                label_specified_idx = self.problem.output_dict.id(label_specified)\n                                confidence_specified = torch.index_select(logits_softmax.cpu(), 1,\n                                        torch.tensor([label_specified_idx], dtype=torch.long)).squeeze(1)\n                                streaming_recoder.record(field, confidence_specified.data.numpy())\n                    elif ProblemTypes[self.problem.problem_type] == ProblemTypes.regression:\n                        logits = list(logits.values())[0]\n                        # logits_softmax is unuseful for regression task!\n                        logits_softmax = list(logits_softmax.values())[0]\n                        logits_flat = logits.squeeze(1)\n                        prediction_scores = logits_flat.detach().cpu().numpy()\n                        streaming_recoder.record_one_row([prediction_scores])\n                    elif ProblemTypes[self.problem.problem_type] == ProblemTypes.mrc:\n                        for key, value in logits.items():\n                            logits[key] = value.squeeze()\n                        for key, value in logits_softmax.items():\n                            logits_softmax[key] = value.squeeze()\n                        passage_identify = None\n                        for type_key in data_batches[i].keys():\n                            if \'p\' in type_key.lower():\n                                passage_identify = type_key\n                                break\n                        if not passage_identify:\n                            raise Exception(\'MRC task need passage information.\')\n                        prediction = self.problem.decode(logits_softmax, lengths=length_batches[i][passage_identify],\n                                                         batch_data=data_batches[i][passage_identify])\n                        streaming_recoder.record_one_row([prediction])\n\n                    logits_len = len(list(logits.values())[0]) \\\n                        if ProblemTypes[self.problem.problem_type] == ProblemTypes.mrc else len(logits)\n                    for sample_idx in range(logits_len):\n                        sample = fin.readline().rstrip()\n                        fout.write(""%s\\t%s\\n"" % (sample,\n                            ""\\t"".join([str(streaming_recoder.get(field)[sample_idx]) for field in predict_fields])))\n                    streaming_recoder.clear_records()\n\n                    del logits, logits_softmax\n\n        fin.close()\n\n    def interactive(self, sample, file_columns, predict_fields=[\'prediction\'], predict_mode=\'batch\'):\n        """""" interactive prediction\n\n         Args:\n            file_columns: representation the columns of sample\n            predict_mode: interactive|batch(need a predict file)\n        """"""\n        predict_data, predict_length, _, _, _ = \\\n            self.problem.encode_data_list(sample, file_columns, self.conf.input_types, self.conf.object_inputs, None,\n                                          self.conf.min_sentence_len, self.conf.extra_feature, self.conf.max_lengths,\n                                          self.conf.fixed_lengths, predict_mode=predict_mode)\n        if predict_data is None:\n            return \'Wrong Case!\'\n        self.model.eval()\n        with torch.no_grad():\n            data_batches, length_batches, _ = \\\n                get_batches(self.problem, predict_data, predict_length, None, 1,\n                            self.conf.input_types, None, permutate=False, transform_tensor=True, predict_mode=predict_mode)\n            streaming_recoder = StreamingRecorder(predict_fields)\n\n            key_random = random.choice(\n                list(length_batches[0].keys()).remove(\'target\') if \'target\' in list(length_batches[0].keys()) else\n                list(length_batches[0].keys()))\n            param_list, inputs_desc, length_desc = transform_params2tensors(data_batches[0], length_batches[0])\n            logits = self.model(inputs_desc, length_desc, *param_list)\n\n            logits_softmax = {}\n            if isinstance(self.model, nn.DataParallel):\n                for tmp_output_layer_id in self.model.module.output_layer_id:\n                    if isinstance(self.model.module.layers[tmp_output_layer_id], Linear) and \\\n                            (not self.model.module.layers[tmp_output_layer_id].layer_conf.last_hidden_softmax):\n                        logits_softmax[tmp_output_layer_id] = nn.functional.softmax(\n                            logits[tmp_output_layer_id], dim=-1)\n                    else:\n                        logits_softmax[tmp_output_layer_id] = logits[tmp_output_layer_id]\n            else:\n                for tmp_output_layer_id in self.model.output_layer_id:\n                    if isinstance(self.model.layers[tmp_output_layer_id], Linear) and \\\n                            (not self.model.layers[tmp_output_layer_id].layer_conf.last_hidden_softmax):\n                        logits_softmax[tmp_output_layer_id] = nn.functional.softmax(\n                            logits[tmp_output_layer_id], dim=-1)\n                    else:\n                        logits_softmax[tmp_output_layer_id] = logits[tmp_output_layer_id]\n\n            if ProblemTypes[self.problem.problem_type] == ProblemTypes.sequence_tagging:\n                logits = list(logits.values())[0]\n                if isinstance(get_layer_class(self.model, tmp_output_layer_id), CRF):\n                    forward_score, scores, masks, tag_seq, transitions, layer_conf = logits\n                    prediction_indices = tag_seq.cpu().numpy()\n                else:\n                    logits_softmax = list(logits_softmax.values())[0]\n                    # Transform output shapes for metric evaluation\n                    # for seq_tag_f1 metric\n                    prediction_indices = logits_softmax.data.max(2)[1].cpu().numpy()  # [batch_size, seq_len]\n                prediction_batch = self.problem.decode(prediction_indices, length_batches[0][key_random].numpy())\n                for prediction_sample in prediction_batch:\n                    streaming_recoder.record(\'prediction\', "" "".join(prediction_sample))\n            elif ProblemTypes[self.problem.problem_type] == ProblemTypes.classification:\n                logits = list(logits.values())[0]\n                logits_softmax = list(logits_softmax.values())[0]\n                prediction_indices = logits_softmax.data.max(1)[1].cpu().numpy()\n\n                for field in predict_fields:\n                    if field == \'prediction\':\n                        streaming_recoder.record(field,\n                                                 self.problem.decode(prediction_indices,\n                                                                     length_batches[0][key_random].numpy()))\n                    elif field == \'confidence\':\n                        prediction_scores = logits_softmax.cpu().data.numpy()\n                        for prediction_score, prediction_idx in zip(prediction_scores, prediction_indices):\n                            streaming_recoder.record(field, prediction_score[prediction_idx])\n                    elif field.startswith(\'confidence\') and field.find(\'@\') != -1:\n                        label_specified = field.split(\'@\')[1]\n                        label_specified_idx = self.problem.output_dict.id(label_specified)\n                        confidence_specified = torch.index_select(logits_softmax.cpu(), 1, torch.tensor([label_specified_idx], dtype=torch.long)).squeeze(1)\n                        streaming_recoder.record(field, confidence_specified.data.numpy())\n            elif ProblemTypes[self.problem.problem_type] == ProblemTypes.regression:\n                logits = list(logits.values())[0]\n                # logits_softmax is unuseful for regression task!\n                logits_softmax = list(logits_softmax.values())[0]\n                logits_flat = logits.squeeze(1)\n                prediction_scores = logits_flat.detach().cpu().numpy()\n                streaming_recoder.record_one_row([prediction_scores])\n            elif ProblemTypes[self.problem.problem_type] == ProblemTypes.mrc:\n                for key, value in logits.items():\n                    logits[key] = value.squeeze()\n                for key, value in logits_softmax.items():\n                    logits_softmax[key] = value.squeeze()\n                passage_identify = None\n                for type_key in data_batches[0].keys():\n                    if \'p\' in type_key.lower():\n                        passage_identify = type_key\n                        break\n                if not passage_identify:\n                    raise Exception(\'MRC task need passage information.\')\n                prediction = self.problem.decode(logits_softmax, lengths=length_batches[0][passage_identify],\n                                                 batch_data=data_batches[0][passage_identify])\n                streaming_recoder.record_one_row([prediction])\n\n            return ""\\t"".join([str(streaming_recoder.get(field)[0]) for field in predict_fields])\n\n    def load_model(self, model_path):\n        if self.use_gpu is True:\n            self.model = torch.load(model_path)\n            if isinstance(self.model, nn.DataParallel):\n                self.model = self.model.module\n            self.model.update_use_gpu(self.use_gpu)\n            self.model.cuda()\n            self.model = nn.DataParallel(self.model)\n        else:\n            self.model = torch.load(model_path, map_location=\'cpu\')\n            if isinstance(self.model, nn.DataParallel):\n                self.model = self.model.module\n            self.model.update_use_gpu(self.use_gpu)\n\n        logging.info(""Model %s loaded!"" % model_path)\n        logging.info(""Total trainable parameters: %d"" % (get_trainable_param_num(self.model)))\n\n    def _get_training_data_generator(self):\n        if not self.conf.use_cache:\n            return self.problem.get_encode_generator(self.conf, build_cache=False)\n        if not self.conf.encoding_file_index:\n            return self._get_save_encode_generator()\n        assert self.conf.load_encoding_cache_generator, \'function conf.load_encoding_cache_generator is not defined\'\n        return self.conf.load_encoding_cache_generator(self.conf.encoding_cache_dir, self.conf.encoding_file_index)\n\n    def _get_save_encode_generator(self):\n        load_save_encode_generator = self.problem.get_encode_generator(self.conf, build_cache=True)\n        for data, lengths, target in load_save_encode_generator:\n            yield data, lengths, target\n        cache_index = load_from_json(self.conf.encoding_cache_index_file_path)\n        self.conf.encoding_file_index = cache_index[st.cencoding_key_index]\n\n\n'"
Model.py,1,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nfrom block_zoo import *\nimport copy\nimport logging\nfrom utils.exceptions import ConfigurationError, LayerUndefinedError, LayerConfigUndefinedError\nfrom queue import Queue\nfrom utils.common_utils import transform_tensors2params, transfer_to_gpu\n\nfrom block_zoo.Embedding import *\n\nEMBED_LAYER_NAME = \'Embedding\'\nEMBED_LAYER_ID = \'embedding\'\n\n\ndef get_conf(layer_id, layer_name, input_layer_ids, all_layer_configs, model_input_ids, use_gpu,\n        conf_dict=None, shared_conf=None, succeed_embedding_flag=False, output_layer_flag=False,\n        target_num=None, fixed_lengths=None, target_dict=None):\n    """""" get layer configuration\n\n    Args\n        layer_id: layer identifier\n        layer_name: name of layer such as BiLSTM\n        input_layer_ids (list): the inputs of current layer\n        all_layer_configs (dict): records the conf class of each layer.\n        model_input_ids (set): the inputs of the model, e.g. [\'query\', \'passage\']\n        use_gpu:\n        conf_dict:\n        shared_conf: if fixed_lengths is not None, the output_dim of shared_conf should be corrected!\n        flag:\n        output_layer_flag:\n        target_num: used for inference the dimension of output space if someone declare a dimension of -1\n        fixed_lengths\n    Returns:\n        configuration class coresponds to the layer\n\n    """"""\n    if shared_conf:\n        conf = copy.deepcopy(shared_conf)\n    else:\n        try:\n            conf_dict[\'use_gpu\'] = use_gpu\n\n            # for Embedding layer, add weight_on_gpu parameters\n            if layer_id == EMBED_LAYER_ID:\n                conf_dict[\'weight_on_gpu\'] = conf_dict[\'conf\'][\'weight_on_gpu\']\n                del conf_dict[\'conf\'][\'weight_on_gpu\']\n\n            # for classification tasks, we usually add a Linear layer to project the output to dimension of number of classes. If we don\'t know the #classes, we can use \'-1\' instead and we would calculate the number of classes from the corpus.\n            if layer_name == \'Linear\':\n                if isinstance(conf_dict[\'hidden_dim\'], list):\n                    if conf_dict[\'hidden_dim\'][-1] == -1:\n                        assert output_layer_flag is True, ""Only in the last layer, hidden_dim == -1 is allowed!""\n                        assert target_num is not None, ""Number of targets should be given!""\n                        conf_dict[\'hidden_dim\'][-1] = target_num\n                    elif conf_dict[\'hidden_dim\'][-1] == \'#target#\':\n                        logging.info(\'#target# position will be replace by target num: %d\' % target_num)\n                        conf_dict[\'hidden_dim\'][-1] = target_num\n                elif isinstance(conf_dict[\'hidden_dim\'], int) and conf_dict[\'hidden_dim\'] == -1:\n                    assert output_layer_flag is True, ""Only in the last layer, hidden_dim == -1 is allowed!""\n                    assert target_num is not None, ""Number of targets should be given!""\n                    conf_dict[\'hidden_dim\'] = target_num\n                elif isinstance(conf_dict[\'hidden_dim\'], str) and conf_dict[\'hidden_dim\'] == \'#target#\':\n                    logging.info(\'#target# position will be replace by target num: %d\' % target_num)\n                    conf_dict[\'hidden_dim\'] = target_num\n            # add some necessary attribute for CRF layer\n            if layer_name == \'CRF\':\n                conf_dict[\'target_dict\'] = target_dict\n\n            conf = eval(layer_name + ""Conf"")(**conf_dict)\n        except NameError as e:\n            raise LayerConfigUndefinedError(""\\""%sConf\\"" has not been defined"" % layer_name)\n\n    # verify the rank consistence of joint layers\n    if layer_name == EMBED_LAYER_NAME:\n        # the embedding layer\n        pass\n    else:\n        # make sure all the inputs to current layer exist\n        for input_layer_id in input_layer_ids:\n            if not (input_layer_id in all_layer_configs or input_layer_id in model_input_ids):\n                raise ConfigurationError(""The input %s of layer %s does not exist. Please define it before ""\n                    ""defining layer %s!"" % (input_layer_id, layer_id, layer_id))\n\n        former_output_ranks = [all_layer_configs[input_layer_id].output_rank if input_layer_id in all_layer_configs else all_layer_configs[EMBED_LAYER_ID].output_rank for input_layer_id in input_layer_ids]\n        # inference input_dim\n        conf.input_dims = [all_layer_configs[input_layer_id].output_dim if input_layer_id in all_layer_configs else all_layer_configs[EMBED_LAYER_ID].output_dim for input_layer_id in input_layer_ids]\n\n        # If the inputs come from embedding layer and fixed_lengths exist, set the length to input_dims\n        if len(input_layer_ids) == 1 and input_layer_ids[0] in model_input_ids and fixed_lengths:\n            conf.input_dims[0][1] = fixed_lengths[input_layer_ids[0]]\n\n        # check and verify input ranks\n        if conf.num_of_inputs > 0:\n            if conf.num_of_inputs != len(input_layer_ids):\n                raise ConfigurationError(""%s only accept %d inputs but you feed %d inputs to it!"" % \\\n                        (layer_name, conf.num_of_inputs, len(input_layer_ids)))\n        elif conf.num_of_inputs == -1:\n            conf.num_of_inputs = len(input_layer_ids)\n            if isinstance(conf.input_ranks, list):\n                conf.input_ranks = conf.input_ranks * conf.num_of_inputs\n            else:\n                logging.warning(""[For developer of %s] The input_ranks attribute should be a list!"" % (layer_name))\n                [conf.input_ranks] * conf.num_of_inputs\n\n        for input_rank, former_output_rank in zip(conf.input_ranks, former_output_ranks):\n            if input_rank != -1 and input_rank != former_output_rank:\n                raise ConfigurationError(""Input ranks of %s are inconsistent with former layers"" % layer_id)\n        conf.input_ranks = copy.deepcopy(former_output_ranks)\n\n    # inference and varification inside the layer\n    conf.inference()        # update some attributes which relies on input dimension or something else\n    conf.verify()           # verify if the configuration is legal\n    former_conf = None if len(all_layer_configs) == 0 else list(all_layer_configs.values())[-1]\n    conf.verify_former_block(former_conf)  # check if has special attribute rely on former layer\n\n    logging.debug(\'Layer id: %s; name: %s; input_dims: %s; input_ranks: %s; output_dim: %s; output_rank: %s\' % (layer_id, layer_name, conf.input_dims if layer_id != \'embedding\' else \'None\', conf.input_ranks, conf.output_dim, conf.output_rank))\n\n    return conf\n\n\ndef get_layer(layer_name, conf):\n    """"""\n\n    Args:\n        layer_name:\n        conf:  configuration class\n\n    Returns:\n        specific layer\n\n    """"""\n    try:\n        layer = eval(layer_name)(conf)\n    except NameError as e:\n        raise Exception(""%s; Layer \\""%s\\"" has not been defined"" % (str(e), layer_name))\n    return layer\n\n\nclass Model(nn.Module):\n    def __init__(self, conf, problem, vocab_info, use_gpu):\n        """"""\n\n        Args:\n            inputs: [\'string1\', \'string2\']\n            layer_archs:  The layers must produce tensors with similar shapes. The layers may be nested.\n                [\n                    {\n                    \'layer\': Layer name,\n                    \'conf\': {xxxx}\n                    },\n                    [\n                        {\n                        \'layer\': Layer name,\n                        \'conf\': {},\n                        },\n                        {\n                        \'layer\': Layer name,\n                        \'conf\': {},\n                        }\n                    ]\n                ]\n            vocab_info:\n                {\n                    \'word\':  {\n                        \'vocab_size\': xxx,\n                        \'init_weights\': np matrix\n                        }\n                    \'postag\': {\n                        \'vocab_size\': xxx,\n                        \'init_weights\': None\n                        }\n                }\n        """"""\n        super(Model, self).__init__()\n\n        inputs = conf.object_inputs_names\n        layer_archs = conf.architecture\n        target_num = problem.output_target_num()\n\n        # correct the real fixed length if begin/end of sentence are added\n        if conf.fixed_lengths:\n            fixed_lengths_corrected = copy.deepcopy(conf.fixed_lengths)\n            for seq in fixed_lengths_corrected:\n                if problem.with_bos_eos:\n                    fixed_lengths_corrected[seq] += 2\n        else:\n            fixed_lengths_corrected = None\n\n        self.use_gpu = use_gpu\n\n        all_layer_configs = dict()\n        self.layers = nn.ModuleDict()\n        self.layer_inputs = dict()\n        self.layer_dependencies = dict()\n        self.layer_dependencies[EMBED_LAYER_ID] = set()\n        # change output_layer_id to list for support multi_output\n        self.output_layer_id = []\n\n        for layer_index, layer_arch in enumerate(layer_archs):\n            output_layer_flag = True if \'output_layer_flag\' in layer_arch and layer_arch[\'output_layer_flag\'] is True else False\n            succeed_embedding_flag = True if layer_index > 0 and \'inputs\' in layer_arch and \\\n                    [input in inputs for input in layer_arch[\'inputs\']].count(True) == len(layer_arch[\'inputs\']) else False\n\n            if output_layer_flag:\n                self.output_layer_id.append(layer_arch[\'layer_id\'])\n                # if hasattr(self, \'output_layer_id\'):\n                #     raise ConfigurationError(""There should be only one output!"")\n                # else:\n                #     self.output_layer_id = layer_arch[\'layer_id\']\n\n            if layer_index == 0:\n                # embedding layer\n                emb_conf = copy.deepcopy(vocab_info)\n                for input_cluster in emb_conf:\n                    emb_conf[input_cluster][\'dim\'] = layer_arch[\'conf\'][input_cluster][\'dim\']\n                    emb_conf[input_cluster][\'fix_weight\'] = layer_arch[\'conf\'][input_cluster].get(\'fix_weight\', False)\n                emb_conf[\'weight_on_gpu\'] = layer_arch.get(\'weight_on_gpu\', True)\n\n                all_layer_configs[EMBED_LAYER_ID] = get_conf(EMBED_LAYER_ID, layer_arch[\'layer\'],\n                    None, all_layer_configs, inputs, self.use_gpu, conf_dict={\'conf\': emb_conf},\n                    shared_conf=None, succeed_embedding_flag=False, output_layer_flag=output_layer_flag,\n                    target_num=target_num, fixed_lengths=fixed_lengths_corrected, target_dict=problem.output_dict)\n                self.add_layer(EMBED_LAYER_ID, get_layer(layer_arch[\'layer\'], all_layer_configs[EMBED_LAYER_ID]))\n            else:\n                if layer_arch[\'layer\'] in self.layers and not \'conf\' in layer_arch:\n                    # reuse formly defined layers (share the same parameters)\n                    logging.debug(""Layer id: %s; Sharing configuration with layer %s"" % (layer_arch[\'layer_id\'], layer_arch[\'layer\']))\n                    conf_dict = None\n                    shared_conf = all_layer_configs[layer_arch[\'layer\']]\n                else:\n                    conf_dict = layer_arch[\'conf\']\n                    shared_conf = None\n\n                # if the layer is EncoderDecoder, inference the vocab size\n                if layer_arch[\'layer\'] == \'EncoderDecoder\':\n                        layer_arch[\'conf\'][\'decoder_conf\'][\'decoder_vocab_size\'] = target_num\n                all_layer_configs[layer_arch[\'layer_id\']] = get_conf(layer_arch[\'layer_id\'], layer_arch[\'layer\'],\n                    layer_arch[\'inputs\'], all_layer_configs, inputs, self.use_gpu, conf_dict=conf_dict,\n                    shared_conf=shared_conf, succeed_embedding_flag=succeed_embedding_flag,\n                    output_layer_flag=output_layer_flag, target_num=target_num,\n                    fixed_lengths=fixed_lengths_corrected, target_dict=problem.output_dict)\n\n                if layer_arch[\'layer\'] in self.layers and not \'conf\' in layer_arch:\n                    self.add_layer(layer_arch[\'layer_id\'], self.layers[layer_arch[\'layer\']])\n                else:\n                    self.add_layer(layer_arch[\'layer_id\'], get_layer(layer_arch[\'layer\'], all_layer_configs[layer_arch[\'layer_id\']]))\n\n                self.layer_inputs[layer_arch[\'layer_id\']] = layer_arch[\'inputs\']\n\n                # register dependencies, except embeddings\n                cur_layer_depend = set()\n                for layer_depend_id in layer_arch[\'inputs\']:\n                    if not layer_depend_id in inputs:\n                        cur_layer_depend.add(layer_depend_id)\n                self.add_dependency(layer_arch[\'layer_id\'], cur_layer_depend)\n\n        logging.debug(""Layer dependencies: %s"" % repr(self.layer_dependencies))\n\n        if not hasattr(self, \'output_layer_id\'):\n            raise ConfigurationError(""Please define an output layer"")\n\n        self.layer_topological_sequence = self.get_topological_sequence()\n\n    def add_layer(self, layer_id, layer):\n        """""" register a layer\n\n        Args:\n            layer_id:\n            layer:\n\n        Returns:\n\n        """"""\n        if layer_id in self.layers:\n            raise ConfigurationError(""The layer id %s is not unique!"")\n        else:\n            self.layers[layer_id] = layer\n\n    def add_dependency(self, layer_id, depend_layer_id):\n        """""" add the layers have to be proceed before layer_id\n\n        Args:\n            layer_id:\n            depend_layer_id:\n\n        Returns:\n\n        """"""\n        if not layer_id in self.layer_dependencies:\n            self.layer_dependencies[layer_id] = set()\n\n        if isinstance(depend_layer_id, int):\n            self.layer_dependencies[layer_id].add(depend_layer_id)\n        else:\n            self.layer_dependencies[layer_id] |= set(depend_layer_id)\n\n    def remove_dependency(self, depend_layer_id):\n        """""" remove dependencies on layer_id\n\n        Args:\n            layer_id:\n\n        Returns:\n\n        """"""\n        for layer_id in self.layer_dependencies:\n            self.layer_dependencies[layer_id].remove(depend_layer_id)\n\n    def get_topological_sequence(self):\n        """""" get topological sequence of nodes in the model\n\n        Returns:\n\n        """"""\n        total_layer_ids = Queue()\n        for layer_id in self.layers.keys():\n            if layer_id != EMBED_LAYER_ID:\n                total_layer_ids.put(layer_id)\n\n        topological_list = []\n        circular_cnt = 0     # used for checking if there is at least one legal topological sorting\n        while not total_layer_ids.empty():\n            layer_id = total_layer_ids.get()\n            if len(self.layer_dependencies[layer_id]) == 0:\n                for layer_id2 in self.layer_dependencies:\n                    if layer_id in self.layer_dependencies[layer_id2]:\n                        self.layer_dependencies[layer_id2].remove(layer_id)\n                circular_cnt = 0\n                topological_list.append(layer_id)\n            else:\n                total_layer_ids.put(layer_id)\n                circular_cnt += 1\n                if circular_cnt >= total_layer_ids.qsize():\n                    rest_layers = []\n                    while not total_layer_ids.empty():\n                        rest_layers.append(total_layer_ids.get())\n                    raise ConfigurationError(""The model architecture is illegal because there is a circular dependency ""\n                        ""or there are some isolated layers. The layers can not be resolved: [%s]"" % ("", "".join(rest_layers)))\n\n        logging.debug(""Topological sequence of nodes: %s"" % ("","".join(topological_list)))\n        return topological_list\n\n    def forward(self, inputs_desc, length_desc, *param_list):\n        """"""\n\n        Args:\n            with the help of transform_tensors2params(inputs_desc, length_desc, param_list), we can get the below inputs and lengths\n\n            inputs: dict.\n                {\n                    ""string1"":{\n                        \'word\': word ids, [batch size, seq len]\n                        \'postag\': postag ids,[batch size, seq len]\n                        ...\n                    }\n                    ""string2"":{\n                        \'word\': word ids,[batch size, seq len]\n                        \'postag\': postag ids,[batch size, seq len]\n                        ...\n                    }\n                }\n            lengths: dict.\n                {\n                    ""string1"": [...]\n                    ""string2"": [...]\n                }\n\n        Returns:\n\n        """"""\n        inputs, lengths = transform_tensors2params(inputs_desc, length_desc, param_list)\n\n        representation = dict()\n        representation[EMBED_LAYER_ID] = dict()\n        repre_lengths = dict()\n        repre_lengths[EMBED_LAYER_ID] = dict()\n\n        for input in inputs:\n            representation[input] = self.layers[EMBED_LAYER_ID](inputs[input], use_gpu=self.is_cuda())\n            if self.use_gpu:\n                repre_lengths[input] = transfer_to_gpu(lengths[input])\n            else:\n                repre_lengths[input] = lengths[input]\n\n        for layer_id in self.layer_topological_sequence:\n            #logging.debug(""To proces layer %s"" % layer_id)\n            input_params = []\n            for input_layer_id in self.layer_inputs[layer_id]:\n                input_params.append(representation[input_layer_id])\n                input_params.append(repre_lengths[input_layer_id])\n\n            representation[layer_id], repre_lengths[layer_id] = self.layers[layer_id](*input_params)\n            #logging.debug(""Layer %s processed. output size: %s"" % (layer_id, representation[layer_id].size()))\n\n        # for support multi_output\n        representation_output = dict()\n        for single_output_layer_id in self.output_layer_id:\n            representation_output[single_output_layer_id] = representation[single_output_layer_id]\n        return representation_output\n\n    def is_cuda(self):\n        return list(self.parameters())[-1].data.is_cuda\n\n    def update_use_gpu(self, new_use_gpu):\n        self.use_gpu = new_use_gpu\n        for layer_id in self.layers.keys():\n            if isinstance(self.layers[layer_id], Embedding):\n                for input_cluster in self.layers[layer_id].embeddings:\n                    if isinstance(self.layers[layer_id].embeddings[input_cluster], CNNCharEmbedding):\n                        self.layers[layer_id].embeddings[input_cluster].layer_conf.use_gpu = new_use_gpu\n            elif isinstance(self.layers[layer_id], EncoderDecoder):\n                self.layers[layer_id].encoder.layer_conf.use_gpu = new_use_gpu\n                self.layers[layer_id].decoder.layer_conf.use_gpu = new_use_gpu\n            else:\n                self.layers[layer_id].layer_conf.use_gpu = new_use_gpu\n\n\n\n'"
ModelConf.py,5,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport codecs\nimport json\nimport os\nimport tempfile\nimport random\nimport string\nimport copy\nimport torch\nimport logging\nimport shutil\n\nfrom losses.BaseLossConf import BaseLossConf\n#import traceback\nfrom settings import LanguageTypes, ProblemTypes, TaggingSchemes, SupportedMetrics, PredictionTypes, DefaultPredictionFields, ConstantStatic\nfrom utils.common_utils import log_set, prepare_dir, md5, load_from_json, dump_to_json\nfrom utils.exceptions import ConfigurationError\nimport numpy as np\n\nclass ConstantStaticItems(ConstantStatic):\n    @staticmethod\n    def concat_key_desc(key_prefix_desc, key):\n        return key_prefix_desc + \'.\' + key\n\n    @staticmethod\n    def get_value_by_key(json, key, key_prefix=\'\', use_default=False, default=None):\n        """"""\n        Args:\n            json: a json object\n            key: a key pointing to the value wanted to acquire\n            use_default: if you really want to use default value when key can not be found in json object, set use_default=True\n            default: if key is not found and default is None, we would raise an Exception, except that use_default is True\n        Returns:\n            value: \n        """"""\n        try:\n            value = json[key]\n        except:\n            if not use_default:\n                raise ConfigurationError(""key[%s] can not be found in configuration file"" % (key_prefix + key))\n            else:\n                value = default\n        return value\n\n    @staticmethod\n    def add_item(item_name, use_default=False, default=None):\n        def add_item_loading_func(use_default, default, func_get_value_by_key):\n            @classmethod\n            def load_data(cls, obj, json, key_prefix_desc=\'\', use_default=use_default, default=default, func_get_value_by_key=func_get_value_by_key):\n                obj.__dict__[cls.__name__] = func_get_value_by_key(json, cls.__name__, key_prefix_desc, use_default, default)\n                return obj\n            return load_data\n        return type(item_name, (ConstantStatic, ), dict(load_data=add_item_loading_func(use_default, default, __class__.get_value_by_key)))\n\n    @classmethod\n    def load_data(cls, obj, json, key_prefix_desc=\'\'):\n        if cls.__name__ in json.keys():\n            json = json[cls.__name__]\n        for key in cls.__dict__.keys():\n            if not hasattr(cls.__dict__[key], \'load_data\'):\n                continue\n            item = cls.__dict__[key]\n            obj = item.load_data(obj, json, cls.concat_key_desc(key_prefix_desc, item.__name__))\n        return obj\n\nclass ModelConf(object):\n    def __init__(self, phase, conf_path, nb_version, params=None, mode=\'normal\'):\n        """""" loading configuration from configuration file and argparse parameters\n\n        Args:\n            phase: train/test/predict/cache\n                specially, \'cache\' phase is used for verifying old cache\n            conf_path:\n            params:\n            mode: \'normal\', \'philly\'\n        """"""\n        self.phase = phase\n        assert self.phase in set([\'train\', \'test\', \'predict\', \'cache\'])\n        self.conf_path = conf_path\n        self.params = params\n        self.mode = mode.lower()\n        assert self.mode in set([\'normal\', \'philly\']), \'Your mode %s is illegal, supported modes are: normal and philly!\'\n        \n        self.load_from_file(conf_path)\n\n        self.check_version_compat(nb_version, self.tool_version)\n\n        if phase != \'cache\':\n            self.check_conf()\n\n        logging.debug(\'Print ModelConf below:\')\n        logging.debug(\'=\' * 80)\n        # print ModelConf\n        for name, value in vars(self).items():\n            if name.startswith(""__"") is False:\n                logging.debug(\'%s: %s\' % (str(name), str(value)))\n        logging.debug(\'=\' * 80)\n\n    class Conf(ConstantStaticItems):\n        license = ConstantStaticItems.add_item(\'license\')\n        tool_version = ConstantStaticItems.add_item(\'tool_version\')\n        model_description = ConstantStaticItems.add_item(\'model_description\')\n        language = ConstantStaticItems.add_item(\'language\', use_default=True, default=\'english\')\n\n        class inputs(ConstantStaticItems):\n            use_cache = ConstantStaticItems.add_item(\'use_cache\', use_default=True, default=True)\n            dataset_type =  ConstantStaticItems.add_item(\'dataset_type\')\n            tagging_scheme = ConstantStaticItems.add_item(\'tagging_scheme\', use_default=True, default=None)\n\n            class data_paths(ConstantStaticItems):\n                train_data_path = ConstantStaticItems.add_item(\'train_data_path\', use_default=True, default=None)\n                valid_data_path = ConstantStaticItems.add_item(\'valid_data_path\', use_default=True, default=None)\n                test_data_path = ConstantStaticItems.add_item(\'test_data_path\', use_default=True, default=None)\n                predict_data_path = ConstantStaticItems.add_item(\'predict_data_path\', use_default=True, default=None)\n                pre_trained_emb = ConstantStaticItems.add_item(\'pre_trained_emb\', use_default=True, default=None)\n                pretrained_model_path = ConstantStaticItems.add_item(\'pretrained_model_path\', use_default=True, default=None)\n\n            file_with_col_header = ConstantStaticItems.add_item(\'file_with_col_header\', use_default=True, default=False)\n            pretrained_emb_type = ConstantStaticItems.add_item(\'pretrained_emb_type\', use_default=True, default=\'glove\')\n            pretrained_emb_binary_or_text = ConstantStaticItems.add_item(\'pretrained_emb_binary_or_text\', use_default=True, default=\'text\')\n            involve_all_words_in_pretrained_emb = ConstantStaticItems.add_item(\'involve_all_words_in_pretrained_emb\', use_default=True, default=False)\n            add_start_end_for_seq = ConstantStaticItems.add_item(\'add_start_end_for_seq\', use_default=True, default=False)\n            file_header = ConstantStaticItems.add_item(\'file_header\', use_default=True, default=None)\n            predict_file_header = ConstantStaticItems.add_item(\'predict_file_header\', use_default=True, default=None)\n            model_inputs = ConstantStaticItems.add_item(\'model_inputs\')\n            target = ConstantStaticItems.add_item(\'target\', use_default=True, default=None)\n            positive_label = ConstantStaticItems.add_item(\'positive_label\', use_default=True, default=None)\n        \n        class outputs(ConstantStaticItems):\n            save_base_dir = ConstantStaticItems.add_item(\'save_base_dir\', use_default=True, default=None)\n            model_name = ConstantStaticItems.add_item(\'model_name\')\n        \n            train_log_name = ConstantStaticItems.add_item(\'train_log_name\', use_default=True, default=None)\n            test_log_name = ConstantStaticItems.add_item(\'test_log_name\', use_default=True, default=None)\n            predict_log_name = ConstantStaticItems.add_item(\'predict_log_name\', use_default=True, default=None)\n            predict_fields = ConstantStaticItems.add_item(\'predict_fields\', use_default=True, default=None)\n            predict_output_name = ConstantStaticItems.add_item(\'predict_output_name\', use_default=True, default=\'predict.tsv\')\n            cache_dir = ConstantStaticItems.add_item(\'cache_dir\', use_default=True, default=None)\n        \n        class training_params(ConstantStaticItems):\n            class vocabulary(ConstantStaticItems):\n                min_word_frequency = ConstantStaticItems.add_item(\'min_word_frequency\', use_default=True, default=3)\n                max_vocabulary = ConstantStaticItems.add_item(\'max_vocabulary\', use_default=True, default=800 * 1000)\n                max_building_lines = ConstantStaticItems.add_item(\'max_building_lines\', use_default=True, default=1000 * 1000)\n            \n            optimizer = ConstantStaticItems.add_item(\'optimizer\', use_default=True, default=None)\n            clip_grad_norm_max_norm = ConstantStaticItems.add_item(\'clip_grad_norm_max_norm\', use_default=True, default=-1)\n            chunk_size = ConstantStaticItems.add_item(\'chunk_size\', use_default=True, default=1000 * 1000)\n            lr_decay = ConstantStaticItems.add_item(\'lr_decay\', use_default=True, default=1)\n            minimum_lr = ConstantStaticItems.add_item(\'minimum_lr\', use_default=True, default=0)\n            epoch_start_lr_decay = ConstantStaticItems.add_item(\'epoch_start_lr_decay\', use_default=True, default=1)\n            use_gpu = ConstantStaticItems.add_item(\'use_gpu\', use_default=True, default=False)\n            cpu_num_workers = ConstantStaticItems.add_item(\'cpu_num_workers\', use_default=True, default=-1) #by default, use all workers cpu supports\n            batch_size = ConstantStaticItems.add_item(\'batch_size\', use_default=True, default=1)\n            batch_num_to_show_results = ConstantStaticItems.add_item(\'batch_num_to_show_results\', use_default=True, default=10)\n            max_epoch = ConstantStaticItems.add_item(\'max_epoch\', use_default=True, default=float(\'inf\'))\n            valid_times_per_epoch = ConstantStaticItems.add_item(\'valid_times_per_epoch\', use_default=True, default=None)\n            steps_per_validation = ConstantStaticItems.add_item(\'steps_per_validation\', use_default=True, default=10)\n            text_preprocessing = ConstantStaticItems.add_item(\'text_preprocessing\', use_default=True, default=list())\n            max_lengths = ConstantStaticItems.add_item(\'max_lengths\', use_default=True, default=None)\n            fixed_lengths = ConstantStaticItems.add_item(\'fixed_lengths\', use_default=True, default=None)\n            tokenizer = ConstantStaticItems.add_item(\'tokenizer\', use_default=True, default=None)\n\n        architecture = ConstantStaticItems.add_item(\'architecture\')\n        loss = ConstantStaticItems.add_item(\'loss\', use_default=True, default=None)\n        metrics = ConstantStaticItems.add_item(\'metrics\', use_default=True, default=None)\n\n    def raise_configuration_error(self, key):\n        raise ConfigurationError(\n            ""The configuration file %s is illegal. the item [%s] is not found."" % (self.conf_path,  key))\n\n    def load_from_file(self, conf_path):\n        # load file\n        self.conf = load_from_json(conf_path, debug=False)\n        self = self.Conf.load_data(self, {\'Conf\' : self.conf}, key_prefix_desc=\'Conf\')\n        self.language = self.language.lower()\n        self.configurate_outputs()\n        self.configurate_inputs()\n        self.configurate_training_params()\n        self.configurate_architecture()\n        self.configurate_loss()\n        self.configurate_cache()\n\n    def configurate_outputs(self):\n        def configurate_logger(self):\n            if self.phase == \'cache\':\n                return\n\n            # dir\n            if hasattr(self.params, \'log_dir\') and self.params.log_dir:\n                self.log_dir = self.params.log_dir\n                prepare_dir(self.log_dir, True, allow_overwrite=True)\n            else:\n                self.log_dir = self.save_base_dir\n            \n            # path\n            self.train_log_path = os.path.join(self.log_dir, self.train_log_name)\n            self.test_log_path = os.path.join(self.log_dir, self.test_log_name)\n            self.predict_log_path = os.path.join(self.log_dir, self.predict_log_name)\n            if self.phase == \'train\':\n                log_path = self.train_log_path\n            elif self.phase == \'test\':\n                log_path = self.test_log_path\n            elif self.phase == \'predict\':\n                log_path =  self.predict_log_path\n            if log_path is None:\n                self.raise_configuration_error(self.phase + \'_log_name\')\n\n            # log level\n            if self.mode == \'philly\' or self.params.debug:\n                log_set(log_path, console_level=\'DEBUG\', console_detailed=True, disable_log_file=self.params.disable_log_file)\n            else:\n                log_set(log_path, disable_log_file=self.params.disable_log_file)\n\n        # save base dir\n        if hasattr(self.params, \'model_save_dir\') and self.params.model_save_dir:\n            self.save_base_dir = self.params.model_save_dir\n        elif self.save_base_dir is None:\n            self.raise_configuration_error(\'save_base_dir\')\n\n        # prepare save base dir \n        if self.phase != \'cache\':\n            prepare_dir(self.save_base_dir, True, allow_overwrite=self.params.force or self.mode == \'philly\',\n                        extra_info=\'will overwrite model file and train.log\' if self.phase==\'train\' else \'will add %s.log and predict file\'%self.phase)\n\n        # logger\n        configurate_logger(self)\n\n        # predict output path\n        if self.phase != \'cache\':\n            if self.params.predict_output_path:\n                self.predict_output_path = self.params.predict_output_path\n            else:\n                self.predict_output_path = os.path.join(self.save_base_dir, self.predict_output_name)\n            logging.debug(\'Prepare dir for: %s\' % self.predict_output_path)\n            prepare_dir(self.predict_output_path, False, allow_overwrite=self.params.force or self.mode == \'philly\')\n\n        if self.predict_fields is None:\n            self.predict_fields = DefaultPredictionFields[ProblemTypes[self.problem_type]]\n\n        self.model_save_path = os.path.join(self.save_base_dir, self.model_name)\n\n    def configurate_inputs(self):\n\n        def configurate_data_path(self):\n            self.pretrained_emb_path =self.pre_trained_emb\n\n            if self.mode != ""normal"":\n                self.train_data_path = None\n                self.valid_data_path = None\n                self.test_data_path = None\n                self.predict_data_path = None\n                self.pretrained_emb_path = None\n\n            if hasattr(self.params, \'train_data_path\') and self.params.train_data_path:\n                self.train_data_path = self.params.train_data_path\n            if hasattr(self.params, \'valid_data_path\') and self.params.valid_data_path:\n                self.valid_data_path = self.params.valid_data_path\n            if hasattr(self.params, \'test_data_path\') and self.params.test_data_path:\n                self.test_data_path = self.params.test_data_path\n            if hasattr(self.params, \'predict_data_path\') and self.params.predict_data_path:\n                self.predict_data_path = self.params.predict_data_path\n            if hasattr(self.params, \'pretrained_emb_path\') and self.params.pretrained_emb_path:\n                self.pretrained_emb_path = self.params.pretrained_emb_path\n\n            if self.phase == \'train\' or self.phase == \'cache\':\n                if self.valid_data_path is None and self.test_data_path is not None:\n                    # We support test_data_path == None, if someone set valid_data_path to None while test_data_path is not None,\n                    # swap the valid_data_path and test_data_path\n                    self.valid_data_path = self.test_data_path\n                    self.test_data_path = None\n            elif self.phase == \'predict\':\n                if self.predict_data_path is None and self.test_data_path is not None:\n                    self.predict_data_path = self.test_data_path\n                    self.test_data_path = None\n            \n            return self\n\n        def configurate_data_format(self):\n            # file columns\n            if self.phase == \'train\' or self.phase == \'test\' or self.phase == \'cache\':\n                self.file_columns = self.file_header\n                if self.file_columns is None:\n                    self.raise_configuration_error(\'file_columns\')\n            if self.phase == \'predict\':\n                self.file_columns, self.predict_file_columns = self.file_header, self.predict_file_header\n                if self.file_columns is None and self.predict_file_columns is None:\n                    self.raise_configuration_error(\'predict_file_columns\')\n                if self.file_columns and self.predict_file_columns is None:\n                    self.predict_file_columns = self.file_columns\n\n            # target\n            if self.phase != \'predict\':\n                self.answer_column_name = self.target\n                if self.target is None and self.phase != \'cache\':\n                    self.raise_configuration_error(\'target\')\n\n            if ProblemTypes[self.problem_type] == ProblemTypes.sequence_tagging and self.add_start_end_for_seq is None:\n                self.add_start_end_for_seq = True\n\n            # pretrained embedding\n            if \'word\' in self.architecture[0][\'conf\'] and self.pretrained_emb_path:\n                if hasattr(self.params, \'involve_all_words_in_pretrained_emb\') and self.params.involve_all_words_in_pretrained_emb:\n                    self.involve_all_words_in_pretrained_emb = self.params.involve_all_words_in_pretrained_emb\n                if hasattr(self.params, \'pretrained_emb_type\') and self.params.pretrained_emb_type:\n                    self.pretrained_emb_type = self.params.pretrained_emb_type\n                if hasattr(self.params, \'pretrained_emb_binary_or_text\') and self.params.pretrained_emb_binary_or_text:\n                    self.pretrained_emb_binary_or_text = self.params.pretrained_emb_binary_or_text\n                self.pretrained_emb_dim = self.architecture[0][\'conf\'][\'word\'][\'dim\']\n            else:\n                self.pretrained_emb_path = None\n                self.involve_all_words_in_pretrained_emb = None\n                self.pretrained_emb_type = None\n                self.pretrained_emb_binary_or_text = None\n                self.pretrained_emb_dim = None\n            \n            return self\n\n        def configurate_model_input(self):\n            self.object_inputs = self.model_inputs\n            self.object_inputs_names = [name for name in self.object_inputs]\n\n            return self\n\n        self.problem_type = self.dataset_type.lower()\n\n        # previous model path\n        if hasattr(self.params, \'previous_model_path\') and self.params.previous_model_path:\n            self.previous_model_path = self.params.previous_model_path\n        else:\n            self.previous_model_path = os.path.join(self.save_base_dir, self.model_name)\n\n        # pretrained model path\n        if hasattr(self.params, \'pretrained_model_path\') and self.params.pretrained_model_path:\n            self.pretrained_model_path = self.params.pretrained_model_path\n\n        # saved problem path\n        model_path = None\n        if self.phase == \'train\':\n            model_path = self.pretrained_model_path\n        elif self.phase == \'test\' or self.phase == \'predict\':\n            model_path = self.previous_model_path\n        if model_path:\n            model_path_dir = os.path.dirname(model_path)\n            self.saved_problem_path = os.path.join(model_path_dir, \'.necessary_cache\', \'problem.pkl\')\n            if not os.path.isfile(self.saved_problem_path):\n                self.saved_problem_path = os.path.join(model_path_dir, \'necessary_cache\', \'problem.pkl\')\n            if not (os.path.isfile(model_path) and os.path.isfile(self.saved_problem_path)):\n                raise Exception(\'Previous trained model %s or its dictionaries %s does not exist!\' % (model_path, self.saved_problem_path))\n\n        configurate_data_path(self)\n        configurate_data_format(self)\n        configurate_model_input(self) \n\n    def configurate_training_params(self):\n        # optimizer\n        if self.phase == \'train\':\n            if self.optimizer is None:\n                self.raise_configuration_error(\'training_params.optimizer\')\n            if \'name\' not in self.optimizer.keys():\n                self.raise_configuration_error(\'training_params.optimizer.name\')\n            self.optimizer_name = self.optimizer[\'name\']\n            if \'params\' not in self.optimizer.keys():\n                self.raise_configuration_error(\'training_params.optimizer.params\')\n            self.optimizer_params = self.optimizer[\'params\']\n            if hasattr(self.params, \'learning_rate\') and self.params.learning_rate:\n                self.optimizer_params[\'lr\'] = self.params.learning_rate\n        \n        # batch size\n        self.batch_size_each_gpu = self.batch_size # the batch_size in conf file is the batch_size on each GPU\n        if hasattr(self.params, \'batch_size\') and self.params.batch_size:\n            self.batch_size_each_gpu = self.params.batch_size\n        if self.batch_size_each_gpu is None:\n            self.raise_configuration_error(\'training_params.batch_size\')\n        self.batch_size_total = self.batch_size_each_gpu\n        if torch.cuda.device_count() > 1:\n            self.batch_size_total = torch.cuda.device_count() * self.batch_size_each_gpu\n            self.batch_num_to_show_results = self.batch_num_to_show_results // torch.cuda.device_count()\n\n    \n        if hasattr(self.params, \'max_epoch\') and self.params.max_epoch:\n            self.max_epoch = self.params.max_epoch\n        \n        if self.valid_times_per_epoch is not None:\n            logging.info(""configuration[training_params][valid_times_per_epoch] is deprecated, please use configuration[training_params][steps_per_validation] instead"")\n        \n        # sequence length\n        if self.fixed_lengths:\n            self.max_lengths = None\n        if ProblemTypes[self.problem_type] == ProblemTypes.sequence_tagging:\n            self.fixed_lengths = None\n            self.max_lengths = None\n\n        # text preprocessing\n        self.__text_preprocessing = self.text_preprocessing\n        self.DBC2SBC = True if \'DBC2SBC\' in self.__text_preprocessing else False\n        self.unicode_fix = True if \'unicode_fix\' in self.__text_preprocessing else False\n        self.remove_stopwords = True if \'remove_stopwords\' in self.__text_preprocessing else False\n\n        # tokenzier\n        if self.tokenizer is None:\n            self.tokenizer = \'jieba\' if self.language == \'chinese\' else \'nltk\'\n        \n        # GPU/CPU\n        if self.phase != \'cache\':\n            if torch.cuda.is_available() and torch.cuda.device_count() > 0 and self.use_gpu:\n                logging.info(""Activating GPU mode, there are %d GPUs available"" % torch.cuda.device_count())\n            else:\n                self.use_gpu = False\n                logging.info(""Activating CPU mode"")\n\n    def configurate_architecture(self):\n        self.input_types = self.architecture[0][\'conf\']\n        \n        # extra feature\n        feature_all = set([_.lower() for _ in self.input_types.keys()])\n        formal_feature = set([\'word\', \'char\'])\n        extra_feature_num = feature_all - formal_feature\n        self.extra_feature = len(extra_feature_num) != 0\n        if self.extra_feature:\n            if self.DBC2SBC:\n                logging.warning(""Detect the extra feature %s, set the DBC2sbc is False."" % \'\'.join(list(extra_feature_num)))\n            if self.unicode_fix:\n                logging.warning(""Detect the extra feature %s, set the unicode_fix is False."" % \'\'.join(list(extra_feature_num)))\n            if self.remove_stopwords:\n                logging.warning(""Detect the extra feature %s, set the remove_stopwords is False."" % \'\'.join(list(extra_feature_num)))\n\n        # output layer\n        self.output_layer_id = []\n        for single_layer in self.architecture:\n            if \'output_layer_flag\' in single_layer and single_layer[\'output_layer_flag\']:\n                self.output_layer_id.append(single_layer[\'layer_id\'])\n\n        # check CNN layer & change min sentence length\n        cnn_rele_layers = [\'Conv\', \'ConvPooling\']\n        self.min_sentence_len = 0\n        for layer_index, single_layer in enumerate(self.architecture):\n            if layer_index == 0:\n                continue\n            if sum([_ == single_layer[\'layer\'] for _ in cnn_rele_layers]):\n                # get window_size conf: type maybe int or list\n                for single_conf, single_conf_value in single_layer[\'conf\'].items():\n                    if \'window\' in single_conf.lower():\n                        self.min_sentence_len = max(self.min_sentence_len, np.max(np.array([single_conf_value])))\n                        break\n\n    def configurate_loss(self):\n        if self.phase != \'train\' and self.phase != \'test\':\n            return\n        \n        if self.loss is None or self.metrics is None:\n            self.raise_configuration_error(\'loss/metrics\')\n        self.loss = BaseLossConf.get_conf(**self.loss)\n\n        if \'auc\' in self.metrics and ProblemTypes[self.problem_type] == ProblemTypes.classification:\n            self.pos_label = self.positive_label\n\n    def configurate_cache(self):\n        # whether use cache\n        if self.mode == \'philly\':\n            self.use_cache = True\n\n        # cache dir\n        if self.phase == \'train\':\n            if hasattr(self.params, \'cache_dir\') and self.params.cache_dir:\n                self.cache_dir = self.params.cache_dir\n            else:\n                if self.mode == \'normal\':\n                    if self.use_cache is False:\n                        self.cache_dir = os.path.join(tempfile.gettempdir(), \'neuron_blocks\', \'\'.join(random.sample(string.ascii_letters+string.digits, 16)))\n                else:\n                    # for philly mode, we can only save files in model_path or scratch_path\n                    self.cache_dir = os.path.join(self.save_base_dir, \'cache\')\n\n            self.problem_path = os.path.join(self.cache_dir, \'problem.pkl\')\n            if self.pretrained_emb_path is not None:\n                self.emb_pkl_path = os.path.join(self.cache_dir, \'emb.pkl\')\n            else:\n                self.emb_pkl_path = None\n        else:\n            tmp_problem_path = os.path.join(self.save_base_dir, \'.necessary_cache\', \'problem.pkl\')\n            self.problem_path = tmp_problem_path if os.path.isfile(tmp_problem_path) else os.path.join(self.save_base_dir, \'necessary_cache\', \'problem.pkl\')\n        \n        # md5 of training data and problem\n        self.train_data_md5 = None\n        if self.phase == \'train\' and self.train_data_path:\n            logging.info(""Calculating the md5 of traing data ..."")\n            self.train_data_md5 = md5([self.train_data_path])\n            logging.info(""the md5 of traing data is %s""%(self.train_data_md5))\n        self.problem_md5 = None\n\n        # encoding \n        self.encoding_cache_dir = None\n        self.encoding_cache_index_file_path = None\n        self.encoding_cache_index_file_md5_path = None\n        self.encoding_file_index = None\n        self.encoding_cache_legal_line_cnt = 0\n        self.encoding_cache_illegal_line_cnt = 0\n        self.load_encoding_cache_generator = None\n\n    def check_conf(self):\n        """""" verify if the configuration is legal or not\n\n        Returns:\n\n        """"""\n        # In philly mode, ensure the data and model etc. are not the local paths defined in configuration file.\n        if self.mode == \'philly\':\n            assert not (hasattr(self.params, \'train_data_path\') and self.params.train_data_path is None and hasattr(self, \'train_data_path\') and self.train_data_path), \'In philly mode, but you define a local train_data_path:%s in your configuration file\' % self.train_data_path\n            assert not (hasattr(self.params, \'valid_data_path\') and self.params.valid_data_path is None and hasattr(self, \'valid_data_path\') and self.valid_data_path), \'In philly mode, but you define a local valid_data_path:%s in your configuration file\' % self.valid_data_path\n            assert not (hasattr(self.params, \'test_data_path\') and self.params.test_data_path is None and hasattr(self, \'test_data_path\') and self.test_data_path), \'In philly mode, but you define a local test_data_path:%s in your configuration file\' % self.test_data_path\n            if self.phase == \'train\':\n                assert hasattr(self.params, \'model_save_dir\') and self.params.model_save_dir, \'In philly mode, you must define a model save dir through the training params\'\n                assert not (self.params.pretrained_model_path is None and self.pretrained_model_path), \'In philly mode, but you define a local pretrained model path:%s in your configuration file\' % self.pretrained_model_path\n                assert not (self.pretrained_model_path is None and self.params.pretrained_emb_path is None and self.pretrained_emb_path), \'In philly mode, but you define a local pretrained embedding:%s in your configuration file\' % self.pretrained_emb_path\n            elif self.phase == \'test\' or self.phase == \'predict\':\n                assert not (self.params.previous_model_path is None and self.previous_model_path), \'In philly mode, but you define a local model trained previously %s in your configuration file\' % self.previous_model_path\n\n        # check inputs\n        # it seems that os.path.isfile cannot detect hdfs files\n        if self.phase == \'train\':\n            assert self.train_data_path is not None, ""Please define train_data_path""\n            assert os.path.isfile(self.train_data_path), ""Training data %s does not exist!"" % self.train_data_path\n            assert self.valid_data_path is not None, ""Please define valid_data_path""\n            assert os.path.isfile(self.valid_data_path), ""Training data %s does not exist!"" % self.valid_data_path\n\n            if hasattr(self, \'pretrained_emb_type\') and self.pretrained_emb_type:\n                assert self.pretrained_emb_type in set([\'glove\', \'word2vec\', \'fasttext\']), \'Embedding type %s is not supported! We support glove, word2vec, fasttext now.\'\n\n            if hasattr(self, \'pretrained_emb_binary_or_text\') and self.pretrained_emb_binary_or_text:\n                assert self.pretrained_emb_binary_or_text in set([\'text\', \'binary\']), \'Embedding file type %s is not supported! We support text and binary.\'\n\n\n        elif self.phase == \'test\':\n            assert self.test_data_path is not None, ""Please define test_data_path""\n            assert os.path.isfile(self.test_data_path), ""Training data %s does not exist!"" % self.test_data_path\n        elif self.phase == \'predict\':\n            assert self.predict_data_path is not None, ""Please define predict_data_path""\n            assert os.path.isfile(self.predict_data_path), ""Training data %s does not exist!"" % self.predict_data_path\n\n        # check language types\n        SUPPORTED_LANGUAGES = set(LanguageTypes._member_names_)\n        assert self.language in SUPPORTED_LANGUAGES, ""Language type %s is not supported now. Supported types: %s"" % (self.language, "","".join(SUPPORTED_LANGUAGES))\n\n        # check problem types\n        SUPPORTED_PROBLEMS = set(ProblemTypes._member_names_)\n        assert self.problem_type in SUPPORTED_PROBLEMS, ""Data type %s is not supported now. Supported types: %s"" % (self.problem_type, "","".join(SUPPORTED_PROBLEMS))\n\n        if ProblemTypes[self.problem_type] == ProblemTypes.sequence_tagging:\n            SUPPORTED_TAGGING_SCHEMES = set(TaggingSchemes._member_names_)\n            assert self.tagging_scheme is not None, ""For sequence tagging proble, tagging scheme must be defined at configuration[\\\'inputs\\\'][\\\'tagging_scheme\\\']!""\n            assert self.tagging_scheme in SUPPORTED_TAGGING_SCHEMES, ""Tagging scheme %s is not supported now. Supported schemes: %s"" % (self.tagging_scheme, "","".join(SUPPORTED_TAGGING_SCHEMES))\n\n            # the max_lengths of all the inputs and targets should be consistent\n            if self.max_lengths:\n                max_lengths = list(self.max_lengths.values())\n                for i in range(len(max_lengths) - 1):\n                    assert max_lengths[i] == max_lengths[i + 1], ""For sequence tagging tasks, the max_lengths of all the inputs and targets should be consistent!""\n\n        # check appliable metrics\n        if self.phase == \'train\' or self.phase == \'test\':\n            self.metrics_post_check = set() # saved to check later\n            diff = set(self.metrics) - SupportedMetrics[ProblemTypes[self.problem_type]]\n            illegal_metrics = []\n            for diff_metric in diff:\n                if diff_metric.find(\'@\') != -1:\n                    field, target = diff_metric.split(\'@\')\n                    #if not field in PredictionTypes[ProblemTypes[self.problem_type]]:\n                    if field != \'auc\':\n                        illegal_metrics.append(diff_metric)\n                    else:\n                        if target != \'average\':\n                            self.metrics_post_check.add(diff_metric)\n            if len(illegal_metrics) > 0:\n                raise Exception(""Metrics %s are not supported for %s tasks!"" % ("","".join(list(illegal_metrics)), self.problem_type))\n\n        # check predict fields\n        if self.phase == \'predict\':\n            self.predict_fields_post_check = set() # saved to check later\n            diff = set(self.predict_fields) - PredictionTypes[ProblemTypes[self.problem_type]]\n            illegal_fields = []\n            for diff_field in diff:\n                if diff_field.find(\'@\') != -1 and diff_field.startswith(\'confidence\'):\n                    field, target = diff_field.split(\'@\')\n                    #if not field in PredictionTypes[ProblemTypes[self.problem_type]]:\n                    if field != \'confidence\':\n                        illegal_fields.append(diff_field)\n                    else:\n                        # don\'t know if the target exists in the output dictionary, check after problem loaded\n                        self.predict_fields_post_check.add(diff_field)\n                else:\n                    illegal_fields.append(diff_field)\n            if len(illegal_fields) > 0:\n                raise Exception(""The prediction fields %s is/are not supported!"" % "","".join(illegal_fields))\n\n    def check_version_compat(self, nb_version, conf_version):\n        """""" check if the version of toolkit and configuration file is compatible\n\n        Args:\n            nb_version: x.y.z\n            conf_version: x.y.z\n\n        Returns:\n            If the x field and y field are both the same, return True, else return False\n\n        """"""\n        nb_version_split = nb_version.split(\'.\')\n        conf_version_split = conf_version.split(\'.\')\n        if len(nb_version_split) != len(conf_version_split):\n            raise ConfigurationError(\'The tool_version field of your configuration is illegal!\')\n        if not (nb_version_split[0] == conf_version_split[0] and nb_version_split[1] == conf_version_split[1]):\n            raise ConfigurationError(\'The NeuronBlocks version is %s, but the configuration version is %s, please update your configuration to %s.%s.X\' % (nb_version, conf_version, nb_version_split[0], nb_version_split[1]))\n\n    def back_up(self, params):\n        shutil.copy(params.conf_path, self.save_base_dir)\n        logging.info(\'Configuration file is backed up to %s\' % (self.save_base_dir))\n'"
predict.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nfrom settings import ProblemTypes, version\n\nimport os\nimport argparse\nimport logging\n\nfrom ModelConf import ModelConf\nfrom problem import Problem\n\nfrom LearningMachine import LearningMachine\n\ndef main(params):\n    conf = ModelConf(\'predict\', params.conf_path, version, params, mode=params.mode)\n    problem = Problem(\'predict\', conf.problem_type, conf.input_types, None,\n        with_bos_eos=conf.add_start_end_for_seq, tagging_scheme=conf.tagging_scheme, tokenizer=conf.tokenizer,\n        remove_stopwords=conf.remove_stopwords, DBC2SBC=conf.DBC2SBC, unicode_fix=conf.unicode_fix)\n        \n    if os.path.isfile(conf.saved_problem_path):\n        problem.load_problem(conf.saved_problem_path)\n        logging.info(""Problem loaded!"")\n        logging.debug(""Problem loaded from %s"" % conf.saved_problem_path)\n    else:\n        raise Exception(""Problem does not exist!"")\n\n    if len(conf.predict_fields_post_check) > 0:\n        for field_to_chk in conf.predict_fields_post_check:\n            field, target = field_to_chk.split(\'@\')\n            if not problem.output_dict.has_cell(target):\n                raise Exception(""The target %s of %s does not exist in the training data."" % (target, field_to_chk))\n\n    lm = LearningMachine(\'predict\', conf, problem, vocab_info=None, initialize=False, use_gpu=conf.use_gpu)\n    lm.load_model(conf.previous_model_path)\n\n    if params.predict_mode == \'batch\':\n        logging.info(\'Predicting %s with the model saved at %s\' % (conf.predict_data_path, conf.previous_model_path))\n    if params.predict_mode == \'batch\':\n        lm.predict(conf.predict_data_path, conf.predict_output_path, conf.predict_file_columns, conf.predict_fields)\n        logging.info(""Predict done! The predict result: %s"" % conf.predict_output_path)\n    elif params.predict_mode == \'interactive\':\n        print(\'=\'*80)\n        task_type = str(ProblemTypes[problem.problem_type]).split(\'.\')[1]\n        sample_format = list(conf.predict_file_columns.keys())\n        target_ = conf.conf[\'inputs\'].get(\'target\', None)\n        target_list = list(target_) if target_ else []\n        for single_element in sample_format[:]:\n            if single_element in target_list:\n                sample_format.remove(single_element)\n        predict_file_columns = {}\n        for index, single in enumerate(sample_format):\n            predict_file_columns[single] = index\n        print(\'Enabling Interactive Inference Mode for %s Task...\' % (task_type.upper()))\n        print(\'%s Task Interactive. The sample format is <%s>\' % (task_type.upper(), \', \'.join(sample_format)))\n        case_cnt = 1\n        while True:\n            print(\'Case%d:\' % case_cnt)\n            sample = []\n            for single in sample_format:\n                temp_ = input(\'\\t%s: \' % single)\n                if temp_.lower() == \'exit\':\n                    exit(0)\n                sample.append(temp_)\n            sample = \'\\t\'.join(sample)\n            result = lm.interactive([sample], predict_file_columns, conf.predict_fields, params.predict_mode)\n            print(\'\\tInference result: %s\' % result)\n            case_cnt += 1\n    else:\n        raise Exception(\'Predict mode support interactive|batch, get %s\' % params.predict_mode)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=\'Prediction\')\n    parser.add_argument(""--conf_path"", type=str, help=""configuration path"")\n    parser.add_argument(""--predict_mode"", type=str, default=\'batch\', help=\'interactive|batch\')\n    parser.add_argument(""--predict_data_path"", type=str, help=\'specify another predict data path, instead of the one defined in configuration file\')\n    parser.add_argument(""--previous_model_path"", type=str, help=\'load model trained previously.\')\n    parser.add_argument(""--predict_output_path"", type=str, help=\'specify another prediction output path, instead of conf[outputs][save_base_dir] + conf[outputs][predict_output_name] defined in configuration file\')\n    parser.add_argument(""--log_dir"", type=str)\n    parser.add_argument(""--batch_size"", type=int, help=\'batch_size of each gpu\')\n    parser.add_argument(""--mode"", type=str, default=\'normal\', help=\'normal|philly\')\n    parser.add_argument(""--force"", type=bool, default=False, help=\'Allow overwriting if some files or directories already exist.\')\n    parser.add_argument(""--disable_log_file"", type=bool, default=False, help=\'If True, disable log file\')\n    parser.add_argument(""--debug"", type=bool, default=False)\n    params, _ = parser.parse_known_args()\n\n    assert params.conf_path, \'Please specify a configuration path via --conf_path\'\n    if params.debug is True:\n        import debugger\n    main(params)'"
problem.py,4,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport logging\nimport numpy as np\nfrom core.CellDict import CellDict\nfrom tqdm import tqdm\nfrom utils.corpus_utils import load_embedding\nimport nltk\nnltk.download(\'punkt\', quiet=True)\nnltk.download(\'stopwords\', quiet=True)\nfrom utils.BPEEncoder import BPEEncoder\nimport os\nimport pickle as pkl\nfrom utils.common_utils import load_from_pkl, dump_to_pkl, load_from_json, dump_to_json, prepare_dir, md5\n\nfrom settings import ProblemTypes, Setting as st\nimport math\nfrom utils.ProcessorsScheduler import ProcessorsScheduler\n\nfrom core.EnglishTokenizer import EnglishTokenizer\nfrom core.ChineseTokenizer import ChineseTokenizer\nfrom core.EnglishTextPreprocessor import EnglishTextPreprocessor\nfrom utils.exceptions import PreprocessError\nimport torch\nimport torch.nn as nn\n\nclass Problem():\n    \n    def __init__(self, phase, problem_type, input_types, answer_column_name=None, lowercase=False, with_bos_eos=True,\n            tagging_scheme=None, tokenizer=""nltk"", remove_stopwords=False, DBC2SBC=True, unicode_fix=True):\n        """"""\n\n        Args:\n            input_types: {\n                  ""word"": [""word1"", ""word1""],\n                  ""postag"": [""postag_feature1"", ""postag_feature2""]\n                }\n            answer_column_name: ""label"" after v1.0.0 answer_column_name change to list\n            source_with_start:\n            source_with_end:\n            source_with_unk:\n            source_with_pad:\n            target_with_start:\n            target_with_end:\n            target_with_unk:\n            target_with_pad:\n            same_length:\n            with_bos_eos: whether to add bos and eos when encoding\n        """"""\n\n        # init\n        source_with_start, source_with_end, source_with_unk, source_with_pad, \\\n        target_with_start, target_with_end, target_with_unk, target_with_pad, \\\n        same_length = (True, ) * 9\n        if ProblemTypes[problem_type] == ProblemTypes.sequence_tagging:\n            pass\n        elif \\\n           ProblemTypes[problem_type] == ProblemTypes.classification or \\\n           ProblemTypes[problem_type] == ProblemTypes.regression:\n           target_with_start, target_with_end, target_with_unk, target_with_pad, same_length = (False, ) * 5\n           if phase != \'train\':\n                same_length = True\n        elif ProblemTypes[problem_type] == ProblemTypes.mrc:\n            target_with_start, target_with_end, target_with_unk, target_with_pad, same_length = (False, ) * 5\n            with_bos_eos = False\n\n        if ProblemTypes[problem_type] == ProblemTypes.sequence_tagging:\n            target_with_start = False\n            target_with_end = False\n            target_with_unk = False\n\n        self.lowercase = lowercase\n        self.problem_type = problem_type\n        self.tagging_scheme = tagging_scheme\n        self.with_bos_eos = with_bos_eos\n        self.source_with_start = source_with_start\n        self.source_with_end = source_with_end\n        self.source_with_unk = source_with_unk\n        self.source_with_pad = source_with_pad\n        self.target_with_start = target_with_start\n        self.target_with_end = target_with_end\n        self.target_with_unk = target_with_unk\n        self.target_with_pad = target_with_pad\n\n        self.input_dicts = dict()\n        for input_type in input_types:\n           self.input_dicts[input_type] = CellDict(with_unk=source_with_unk, with_pad=source_with_pad,\n                                        with_start=source_with_start, with_end=source_with_end)\n        if ProblemTypes[self.problem_type] == ProblemTypes.sequence_tagging or \\\n                ProblemTypes[self.problem_type] == ProblemTypes.classification :\n            self.output_dict = CellDict(with_unk=target_with_unk, with_pad=target_with_pad,\n                                    with_start=target_with_start, with_end=target_with_end)\n        elif ProblemTypes[self.problem_type] == ProblemTypes.regression or \\\n                ProblemTypes[self.problem_type] == ProblemTypes.mrc:\n            self.output_dict = None\n\n        self.file_column_num = None\n\n        if tokenizer in [\'nltk\']:\n            self.tokenizer = EnglishTokenizer(tokenizer=tokenizer, remove_stopwords=remove_stopwords)\n        elif tokenizer in [\'jieba\']:\n            self.tokenizer = ChineseTokenizer(tokenizer=tokenizer, remove_stopwords=remove_stopwords)\n        self.text_preprocessor = EnglishTextPreprocessor(DBC2SBC=DBC2SBC, unicode_fix=unicode_fix)\n\n    def input_word_num(self):\n        return self.input_word_dict.cell_num()\n\n    def output_target_num(self):\n        if ProblemTypes[self.problem_type] == ProblemTypes.sequence_tagging or ProblemTypes[self.problem_type] == ProblemTypes.classification:\n            return self.output_dict.cell_num()\n        else:\n            return None\n\n    def get_data_generator_from_file(self, data_path, file_with_col_header, chunk_size=1000000):\n        data_list = list()\n        with open(data_path, ""r"", encoding=\'utf-8\') as f:\n            if file_with_col_header:\n                f.readline()\n            for index, line in enumerate(f):\n                line = line.rstrip()\n                if not line:\n                    break\n                data_list.append(line)\n                if (index + 1) % chunk_size == 0:\n                    yield data_list\n                    data_list = list()\n            if len(data_list) > 0:\n                yield data_list\n\n    def build_training_data_list(self, training_data_list, file_columns, input_types, answer_column_name, bpe_encoder=None):\n        docs = dict()           # docs of each type of input\n        col_index_types = dict()        # input type of each column, col_index_types[0] = \'word\'/\'postag\'\n\n        target_docs = {}  # after v1.0.0, the target_docs change to dict for support multi_label\n        columns_to_target = {}\n        for single_target in answer_column_name:\n            target_docs[single_target] = []\n            columns_to_target[file_columns[single_target]] = single_target\n\n        for input_type in input_types:\n            docs[input_type] = []\n            # char is not in file_columns\n            if input_type == \'char\':\n                continue\n            for col in input_types[input_type][\'cols\']:\n                col_index_types[file_columns[col]] = input_type\n\n        cnt_legal = 0\n        cnt_illegal = 0\n        for line in training_data_list:\n            # line_split = list(filter(lambda x: len(x) > 0, line.rstrip().split(\'\\t\')))\n            line_split = line.rstrip().split(\'\\t\')\n            if len(line_split) != len(file_columns):\n                logging.warning(""Current line is inconsistent with configuration/inputs/file_header. Ingore now. %s"" % line)\n                cnt_illegal += 1\n                continue\n            cnt_legal += 1\n\n            for i in range(len(line_split)):\n                if i in col_index_types:\n                    if self.lowercase:\n                        line_split[i] = line_split[i].lower()\n                    line_split[i] = self.text_preprocessor.preprocess(line_split[i])\n\n                    if col_index_types[i] == \'word\':\n                        if ProblemTypes[self.problem_type] == ProblemTypes.sequence_tagging:\n                            token_list = line_split[i].split("" "")\n                        else:\n                            token_list = self.tokenizer.tokenize(line_split[i])\n                        docs[col_index_types[i]].append(token_list)\n                        if \'char\' in docs:\n                            # add char\n                            docs[\'char\'].append([single_char for single_char in \'\'.join(token_list)])\n                    elif col_index_types[i] == \'bpe\':\n                        bpe_tokens = []\n                        for token in self.tokenizer.tokenize(line_split[i]):\n                            bpe_tokens.extend(bpe_encoder.bpe(token))\n                        docs[col_index_types[i]].append(bpe_tokens)\n                    else:\n                        docs[col_index_types[i]].append(line_split[i].split("" ""))\n                # target_docs change to dict\n                elif i in columns_to_target.keys():\n                    curr_target = columns_to_target[i]\n                    if ProblemTypes[self.problem_type] == ProblemTypes.classification:\n                        target_docs[curr_target].append(line_split[i])\n                    elif ProblemTypes[self.problem_type] == ProblemTypes.sequence_tagging:\n                        target_docs[curr_target].append(line_split[i].split("" ""))\n                    elif ProblemTypes[self.problem_type] == ProblemTypes.regression or \\\n                            ProblemTypes[self.problem_type] == ProblemTypes.mrc:\n                        pass\n        return docs, target_docs, cnt_legal, cnt_illegal\n\n    def build_training_multi_processor(self, training_data_generator, cpu_num_workers, file_columns, input_types, answer_column_name, bpe_encoder=None):\n        for data in training_data_generator:\n            # multi-Processing\n            scheduler = ProcessorsScheduler(cpu_num_workers)\n            func_args = (data, file_columns, input_types, answer_column_name, bpe_encoder)\n            res = scheduler.run_data_parallel(self.build_training_data_list, func_args)\n            # aggregate\n            docs = dict()           # docs of each type of input\n            target_docs = []\n            cnt_legal = 0\n            cnt_illegal = 0\n            for (index, j) in res:\n                #logging.info(""collect proccesor %d result"" % index)\n                tmp_docs, tmp_target_docs, tmp_cnt_legal, tmp_cnt_illegal = j.get()\n                if len(docs) == 0:\n                    docs = tmp_docs\n                else:\n                    for key, value in tmp_docs.items():\n                        docs[key].extend(value)\n                if len(target_docs) == 0:\n                    target_docs = tmp_target_docs\n                else:\n                    for single_type in tmp_target_docs:\n                        target_docs[single_type].extend(tmp_target_docs[single_type])\n                # target_docs.extend(tmp_target_docs)\n                cnt_legal += tmp_cnt_legal\n                cnt_illegal += tmp_cnt_illegal\n\n            yield docs, target_docs, cnt_legal, cnt_illegal\n\n    def build(self, data_path_list, file_columns, input_types, file_with_col_header, answer_column_name, word2vec_path=None, word_emb_dim=None,\n              format=None, file_type=None, involve_all_words=None, file_format=""tsv"", show_progress=True,\n              cpu_num_workers=-1, max_vocabulary=800000, word_frequency=3, max_building_lines=1000*1000):\n        """"""\n\n        Args:\n            data_path_list:\n            file_columns: {\n                  ""word1"": 0,\n                  ""word2"": 1,\n                  ""label"":   2,\n                  ""postag_feature1"": 3,\n                  ""postag_feature2"": 4\n                },\n            input_types:\n                e.g.\n                {\n                  ""word"": {\n                    ""cols"": [""word1"", ""word2""],\n                    ""dim"": 300\n                  },\n                  ""postag"": {\n                    ""cols"": [""postag_feature1"", ""postag_feature2""],\n                    ""dim"": 20\n                  },\n                }\n                or\n                {\n                  ""bpe"": {\n                    ""cols"": [""word1"", ""word2""],\n                    ""dim"": 100\n                    ""bpe_path"": ""xxx.bpe""\n                  }\n                }\n\n            word2vec_path:\n            word_emb_dim:\n            involve_all_word: involve all words that show up in the pretrained embedding\n            file_format: ""tsv"", or ""json"". Note ""json"" means each sample is represented by a json string.\n\n        Returns:\n\n        """"""\n        # parameter check\n        bpe_encoder = self._check_bpe_encoder(input_types)  \n        self.file_column_num = len(file_columns)\n\n        for data_path in data_path_list:\n            if data_path:\n                progress = self.get_data_generator_from_file(data_path, file_with_col_header, chunk_size=max_building_lines)\n                preprocessed_data_generator= self.build_training_multi_processor(progress, cpu_num_workers, file_columns, input_types, answer_column_name, bpe_encoder=bpe_encoder)\n        \n                # update symbol universe\n                docs, target_docs, cnt_legal, cnt_illegal = next(preprocessed_data_generator)\n\n                # input_type\n                for input_type in input_types:\n                    self.input_dicts[input_type].update(docs[input_type])\n        \n                # problem_type\n                if ProblemTypes[self.problem_type] == ProblemTypes.classification or \\\n                    ProblemTypes[self.problem_type] == ProblemTypes.sequence_tagging:\n                    self.output_dict.update(list(target_docs.values())[0])\n                elif ProblemTypes[self.problem_type] == ProblemTypes.regression or \\\n                        ProblemTypes[self.problem_type] == ProblemTypes.mrc:\n                    pass\n                logging.info(""[Building Dictionary] in %s at most %d lines imported: %d legal lines, %d illegal lines."" % (data_path, max_building_lines, cnt_legal, cnt_illegal))\n     \n        # build dictionary\n        for input_type in input_types:\n            self.input_dicts[input_type].build(threshold=word_frequency, max_vocabulary_num=max_vocabulary)\n            logging.info(""%d types in %s column"" % (self.input_dicts[input_type].cell_num(), input_type))\n        if self.output_dict:\n            self.output_dict.build(threshold=0)\n            if ProblemTypes[self.problem_type] == ProblemTypes.sequence_tagging:\n                self.output_dict.cell_id_map[""<start>""] = len(self.output_dict.cell_id_map)\n                self.output_dict.id_cell_map[len(self.output_dict.id_cell_map)] = ""<start>""\n                self.output_dict.cell_id_map[""<eos>""] = len(self.output_dict.cell_id_map)\n                self.output_dict.id_cell_map[len(self.output_dict.id_cell_map)] = ""<eos>""\n            logging.info(""%d types in target column"" % (self.output_dict.cell_num()))\n        logging.debug(""training data dict built"")\n\n        # embedding\n        word_emb_matrix = None\n        if word2vec_path:\n            logging.info(""Getting pre-trained embeddings..."")\n            word_emb_dict = None\n            if involve_all_words is True:\n                word_emb_dict = load_embedding(word2vec_path, word_emb_dim, format, file_type, with_head=False, word_set=None)\n                self.input_dicts[\'word\'].update([list(word_emb_dict.keys())])\n                self.input_dicts[\'word\'].build(threshold=0, max_vocabulary_num=len(word_emb_dict))\n            else:\n                extend_vocabulary = set()\n                for single_word in self.input_dicts[\'word\'].cell_id_map.keys():\n                    extend_vocabulary.add(single_word)\n                    if single_word.lower() != single_word:\n                        extend_vocabulary.add(single_word.lower())\n                word_emb_dict = load_embedding(word2vec_path, word_emb_dim, format, file_type, with_head=False, word_set=extend_vocabulary)\n\n            for word in word_emb_dict:\n                loaded_emb_dim = len(word_emb_dict[word])\n                break\n\n            assert loaded_emb_dim == word_emb_dim, ""The dimension of defined word embedding is inconsistent with the pretrained embedding provided!""\n\n            logging.info(""constructing embedding table"")\n            if self.input_dicts[\'word\'].with_unk:\n                word_emb_dict[\'<unk>\'] = np.random.random(size=word_emb_dim)\n            if self.input_dicts[\'word\'].with_pad:\n                word_emb_dict[\'<pad>\'] = np.random.random(size=word_emb_dim)\n\n            word_emb_matrix = []\n            unknown_word_count = 0\n            scale = np.sqrt(3.0 / word_emb_dim)\n            for i in range(self.input_dicts[\'word\'].cell_num()):\n                single_word = self.input_dicts[\'word\'].id_cell_map[i]\n                if single_word in word_emb_dict:\n                    word_emb_matrix.append(word_emb_dict[single_word])\n                elif single_word.lower() in word_emb_dict:\n                    word_emb_matrix.append(word_emb_dict[single_word.lower()])\n                else:\n                    word_emb_matrix.append(np.random.uniform(-scale, scale, word_emb_dim))\n                    unknown_word_count += 1\n            word_emb_matrix = np.array(word_emb_matrix)\n            logging.info(""word embedding matrix shape:(%d, %d); unknown word count: %d;"" %\n                         (len(word_emb_matrix), len(word_emb_matrix[0]), unknown_word_count))\n            logging.info(""Word embedding loaded"")\n\n        return word_emb_matrix\n    \n    @staticmethod\n    def _merge_encode_data(dest_dict, src_dict):\n        if len(dest_dict) == 0:\n            dest_dict = src_dict\n        else:\n            for branch in src_dict:\n                for input_type in dest_dict[branch]:\n                    dest_dict[branch][input_type].extend(src_dict[branch][input_type])\n        return dest_dict \n\n    @staticmethod\n    def _merge_encode_lengths(dest_dict, src_dict):\n        def judge_dict(obj):\n            return True if isinstance(obj, dict) else False\n\n        if len(dest_dict) == 0:\n            dest_dict = src_dict\n        else:\n            for branch in src_dict:\n                if judge_dict(src_dict[branch]):\n                    for type_branch in src_dict[branch]:\n                        dest_dict[branch][type_branch].extend(src_dict[branch][type_branch])\n                else:\n                    dest_dict[branch].extend(src_dict[branch])\n        return dest_dict\n\n    @staticmethod \n    def _merge_target(dest_dict, src_dict):\n        if not src_dict:\n            return src_dict\n\n        if len(dest_dict) == 0:\n            dest_dict = src_dict\n        else:\n            for single_type in src_dict:\n                dest_dict[single_type].extend(src_dict[single_type])\n        return dest_dict\n\n    def encode_data_multi_processor(self, data_generator, cpu_num_workers, file_columns, input_types, object_inputs,\n                answer_column_name, min_sentence_len, extra_feature, max_lengths=None, fixed_lengths=None, file_format=""tsv"", bpe_encoder=None):\n        for data in data_generator:\n            scheduler = ProcessorsScheduler(cpu_num_workers)\n            func_args = (data, file_columns, input_types, object_inputs,\n                        answer_column_name, min_sentence_len, extra_feature, max_lengths, fixed_lengths, file_format, bpe_encoder)\n            res = scheduler.run_data_parallel(self.encode_data_list, func_args)\n            \n            output_data, lengths, target = dict(), dict(), dict()\n            cnt_legal, cnt_illegal = 0, 0\n            for (index, j) in res:\n                # logging.info(""collect proccesor %d result""%index)\n                tmp_data, tmp_lengths, tmp_target, tmp_cnt_legal, tmp_cnt_illegal = j.get()\n                output_data = self._merge_encode_data(output_data, tmp_data)\n                lengths = self._merge_encode_lengths(lengths, tmp_lengths)\n                target = self._merge_target(target, tmp_target)   \n                cnt_legal += tmp_cnt_legal\n                cnt_illegal += tmp_cnt_illegal\n            yield output_data, lengths, target, cnt_legal, cnt_illegal\n\n    def encode_data_list(self, data_list, file_columns, input_types, object_inputs, answer_column_name, min_sentence_len,\n                         extra_feature, max_lengths=None, fixed_lengths=None, file_format=""tsv"", bpe_encoder=None, predict_mode=\'batch\'):\n        data = dict()\n        lengths = dict()\n        char_emb = True if \'char\' in [single_input_type.lower() for single_input_type in input_types] else False\n        if answer_column_name is not None and len(answer_column_name)>0:\n            target = {}\n            lengths[\'target\'] = {}\n            columns_to_target = {}\n            for single_target in answer_column_name:\n                target[single_target] = []\n                columns_to_target[file_columns[single_target]] = single_target\n                lengths[\'target\'][single_target] = []\n        else:\n            target = None\n\n        col_index_types = dict()        # input type of each column, namely the inverse of file_columns, e.g. col_index_types[0] = \'query_index\'\n        type2cluster = dict()           # e.g. type2cluster[\'query_index\'] = \'word\'\n\n        type_branches = dict()            # branch of input type, e.g. type_branches[\'query_index\'] = \'query\'\n\n        # for char: don\'t split these word\n        word_no_split = [\'<start>\', \'<pad>\', \'<eos>\', \'<unk>\']\n        \n        for branch in object_inputs:\n            data[branch] = dict()\n            lengths[branch] = dict()\n            lengths[branch][\'sentence_length\'] = []\n            temp_branch_char = False\n            for input_type in object_inputs[branch]:\n                type_branches[input_type] = branch\n                data[branch][input_type] = []\n                if \'char\' in input_type.lower():\n                    temp_branch_char = True\n            if char_emb and temp_branch_char:\n                lengths[branch][\'word_length\'] = []\n        # for extra_info for mrc task\n        if ProblemTypes[self.problem_type] == ProblemTypes.mrc:\n            extra_info_type = \'passage\'\n            if extra_info_type not in object_inputs:\n                raise Exception(\'MRC task need passage for model_inputs, given: {0}\'.format(\';\'.join(list(object_inputs.keys()))))\n            data[extra_info_type][\'extra_passage_text\'] = []\n            data[extra_info_type][\'extra_passage_token_offsets\'] = []\n\n        for input_type in input_types:\n            for col_name in input_types[input_type][\'cols\']:\n                type2cluster[col_name] = input_type\n                if col_name in file_columns:\n                    col_index_types[file_columns[col_name]] = col_name\n\n\n        cnt_legal = 0\n        cnt_illegal = 0\n\n        # cnt_length_unconsistent = 0\n        cnt_all = 0\n\n        for line in data_list:\n            # line_split = list(filter(lambda x: len(x) > 0, line.rstrip().split(\'\\t\')))\n            line_split = line.rstrip().split(\'\\t\')\n            cnt_all += 1\n            if len(line_split) != len(file_columns):\n                if predict_mode == \'batch\':\n                    cnt_illegal += 1\n                    if cnt_illegal / cnt_all > 0.33:\n                        raise PreprocessError(\'The illegal data is too much. Please check the number of data columns or text token version.\')\n                    continue\n                else:\n                    print(\'\\tThe case is illegal! Please check your case and input again!\')\n                    return [None]*5\n            # cnt_legal += 1\n            length_appended_set = set()  # to store branches whose length have been appended to lengths[branch]\n\n            if ProblemTypes[self.problem_type] == ProblemTypes.mrc:\n                passage_token_offsets = None\n\n            for i in range(len(line_split)):\n                line_split[i] = line_split[i].strip()\n                if i in col_index_types:\n                    # these are data\n                    branch = type_branches[col_index_types[i]]\n                    input_type = []\n                    input_type.append(col_index_types[i])\n                    if(type2cluster[col_index_types[i]] == \'word\' and char_emb):\n                        temp_col_char = col_index_types[i].split(\'_\')[0] + \'_\' + \'char\'\n                        if temp_col_char in input_types[\'char\'][\'cols\']:\n                            input_type.append(temp_col_char)\n                    if type2cluster[col_index_types[i]] == \'word\' or type2cluster[col_index_types[i]] == \'bpe\':\n                        if self.lowercase:\n                            line_split[i] = line_split[i].lower()\n                        line_split[i] = self.text_preprocessor.preprocess(line_split[i])\n                    if type2cluster[col_index_types[i]] == \'word\':\n                        if ProblemTypes[self.problem_type] == ProblemTypes.mrc:\n                            token_offsets = self.tokenizer.span_tokenize(line_split[i])\n                            tokens = [line_split[i][span[0]:span[1]] for span in token_offsets]\n                            if branch == \'passage\':\n                                passage_token_offsets = token_offsets\n                                data[extra_info_type][\'extra_passage_text\'].append(line_split[i])\n                                data[extra_info_type][\'extra_passage_token_offsets\'].append(passage_token_offsets)\n                        else:\n                            if extra_feature == False and ProblemTypes[self.problem_type] != ProblemTypes.sequence_tagging:\n                                tokens = self.tokenizer.tokenize(line_split[i])\n                            else:\n                                tokens = line_split[i].split(\' \')\n                    elif type2cluster[col_index_types[i]] == \'bpe\':\n                        tokens = bpe_encoder.encode(line_split[i])\n                    else:\n                        tokens = line_split[i].split(\' \')\n\n                    # for sequence labeling task, the length must be record the corpus truth length\n                    if ProblemTypes[self.problem_type] == ProblemTypes.sequence_tagging:\n                        if not branch in length_appended_set:\n                            lengths[branch][\'sentence_length\'].append(len(tokens))\n                            length_appended_set.add(branch)\n                        else:\n                            if len(tokens) != lengths[branch][\'sentence_length\'][-1]:\n                                # logging.warning(\n                                #     ""The length of inputs are not consistent. Ingore now. %s"" % line)\n                                cnt_illegal += 1\n                                if cnt_illegal / cnt_all > 0.33:\n                                    raise PreprocessError(\n                                        ""The illegal data is too much. Please check the number of data columns or text token version."")\n                                lengths[branch][\'sentence_length\'].pop()\n                                true_len = len(lengths[branch][\'sentence_length\'])\n                                # need delete the last example\n                                check_list = [\'data\', \'lengths\', \'target\']\n                                for single_check in check_list:\n                                    single_check = eval(single_check)\n                                    self.delete_example(single_check, true_len)\n                                break\n\n                    if fixed_lengths and type_branches[input_type[0]] in fixed_lengths:\n                        if len(tokens) >= fixed_lengths[type_branches[input_type[0]]]:\n                            tokens = tokens[:fixed_lengths[type_branches[input_type[0]]]]\n                        else:\n                            tokens = tokens + [\'<pad>\'] * (fixed_lengths[type_branches[input_type[0]]] - len(tokens))\n                    else:\n                        if max_lengths and type_branches[input_type[0]] in max_lengths:  # cut sequences which are too long\n                            tokens = tokens[:max_lengths[type_branches[input_type[0]]]]\n\n                    if len(tokens) < min_sentence_len:\n                        tokens = tokens + [\'<pad>\'] * (min_sentence_len - len(tokens))\n\n                    if self.with_bos_eos is True:\n                        tokens = [\'<start>\'] + tokens + [\'<eos>\']  # so that source_with_start && source_with_end should be True\n\n                    # for other tasks, length must be same as data length because fix/max_length operation\n                    if not ProblemTypes[self.problem_type] == ProblemTypes.sequence_tagging:\n                        if not branch in length_appended_set:\n                            lengths[branch][\'sentence_length\'].append(len(tokens))\n                            length_appended_set.add(branch)\n                        else:\n                            if len(tokens) != lengths[branch][\'sentence_length\'][-1]:\n                                # logging.warning(\n                                #     ""The length of inputs are not consistent. Ingore now. %s"" % line)\n                                cnt_illegal += 1\n                                if cnt_illegal / cnt_all > 0.33:\n                                    raise PreprocessError(\n                                        ""The illegal data is too much. Please check the number of data columns or text token version."")\n                                lengths[branch][\'sentence_length\'].pop()\n                                true_len = len(lengths[branch][\'sentence_length\'])\n                                # need delete the last example\n                                check_list = [\'data\', \'lengths\', \'target\']\n                                for single_check in check_list:\n                                    single_check = eval(single_check)\n                                    self.delete_example(single_check, true_len)\n                                break\n\n                    for single_input_type in input_type:\n                        if \'char\' in single_input_type:\n                            temp_word_char = []\n                            temp_word_length = []\n                            for single_token in tokens:\n                                if single_token in word_no_split:\n                                    # temp_word_length.append(1)\n                                    temp_id = [self.input_dicts[type2cluster[single_input_type]].id(single_token)]\n                                else:\n                                    temp_id = self.input_dicts[type2cluster[single_input_type]].lookup(single_token)\n                                if fixed_lengths and \'word\' in fixed_lengths:\n                                    if len(temp_id) >= fixed_lengths[\'word\']:\n                                        temp_id = temp_id[:fixed_lengths[\'word\']]\n                                    else:\n                                        temp_id = temp_id + [self.input_dicts[type2cluster[single_input_type]].id(\'<pad>\')] * (fixed_lengths[\'word\'] - len(temp_id))\n                                temp_word_char.append(temp_id)\n                                temp_word_length.append(len(temp_id))\n                            data[branch][single_input_type].append(temp_word_char)\n                            lengths[branch][\'word_length\'].append(temp_word_length)\n                        else:\n                            data[branch][single_input_type].\\\n                                append(self.input_dicts[type2cluster[single_input_type]].lookup(tokens))\n\n                else:\n                    # judge target\n                    if answer_column_name is not None and len(answer_column_name) > 0:\n                        if i in columns_to_target.keys():\n                            # this is target\n                            curr_target = columns_to_target[i]\n                            if ProblemTypes[self.problem_type] == ProblemTypes.mrc:\n                                try:\n                                    trans2int = int(line_split[i])\n                                except(ValueError):\n                                    target[curr_target].append(line_split[i])\n                                else:\n                                    target[curr_target].append(trans2int)\n                                lengths[\'target\'][curr_target].append(1)\n                            if ProblemTypes[self.problem_type] == ProblemTypes.sequence_tagging:\n                                target_tags = line_split[i].split("" "")\n                                if fixed_lengths and ""target"" in fixed_lengths:\n                                    if len(target_tags) >= fixed_lengths[type_branches[input_type[0]]]:\n                                        target_tags = target_tags[:fixed_lengths[type_branches[input_type[0]]]]\n                                    else:\n                                        target_tags = target_tags + [\'<pad>\'] * (fixed_lengths[type_branches[input_type[0]]] - len(target_tags))\n                                else:\n                                    if max_lengths and ""target"" in max_lengths:  # cut sequences which are too long\n                                        target_tags = target_tags[:max_lengths[""target""]]\n\n                                if self.with_bos_eos is True:\n                                    target_tags = [\'O\'] + target_tags + [\'O\']\n                                target[curr_target].append(self.output_dict.lookup(target_tags))\n                                lengths[\'target\'][curr_target].append(len(target_tags))\n                            elif ProblemTypes[self.problem_type] == ProblemTypes.classification:\n                                target[curr_target].append(self.output_dict.id(line_split[i]))\n                                lengths[\'target\'][curr_target].append(1)\n                            elif ProblemTypes[self.problem_type] == ProblemTypes.regression:\n                                target[curr_target].append(float(line_split[i]))\n                                lengths[\'target\'][curr_target].append(1)\n                        else:\n                            # these columns are useless in the configuration\n                            pass\n\n            cnt_legal += 1\n            if ProblemTypes[self.problem_type] == ProblemTypes.mrc and target is not None:\n                if passage_token_offsets:\n                    if \'start_label\' not in target or \'end_label\' not in target:\n                        raise Exception(\'MRC task need start_label and end_label.\')\n                    start_char_label = target[\'start_label\'][-1]\n                    end_char_label = target[\'end_label\'][-1]\n                    start_word_label = 0\n                    end_word_label = len(passage_token_offsets) - 1\n                    # for i in range(len(passage_token_offsets)):\n                    #     token_s, token_e = passage_token_offsets[i]\n                    #     if token_s > start_char_label:\n                    #         break\n                    #     start_word_label = i\n                    # for i in range(len(passage_token_offsets)):\n                    #     token_s, token_e = passage_token_offsets[i]\n                    #     end_word_label = i\n                    #     if token_e >= end_char_label:\n                    #         break\n                    for i in range(len(passage_token_offsets)):\n                        token_s, token_e = passage_token_offsets[i]\n                        if token_s <= start_char_label <= token_e:\n                            start_word_label = i\n                        if token_s <= end_char_label - 1 <= token_e:\n                            end_word_label = i\n                    target[\'start_label\'][-1] = start_word_label\n                    target[\'end_label\'][-1] = end_word_label\n                else:\n                    raise Exception(\'MRC task need passage.\')\n\n        return data, lengths, target, cnt_legal, cnt_illegal\n\n    def encode(self, data_path, file_columns, input_types, file_with_col_header, object_inputs, answer_column_name,\n               min_sentence_len, extra_feature, max_lengths=None, fixed_lengths=None, file_format=""tsv"", show_progress=True,\n               cpu_num_workers=-1, chunk_size=1000*1000):\n        """"""\n\n        Args:\n            data_path:\n            file_columns: {\n                  ""word1"": 0,\n                  ""word2"": 1,\n                  ""label"":   2,\n                  ""postag_feature1"": 3,\n                  ""postag_feature2"": 4\n                },\n            input_types:\n                {\n                  ""word"": {\n                    ""cols"": [\n                      ""word1"",\n                      ""word2""\n                    ],\n                    ""dim"": 300\n                  },\n                  ""postag"": {\n                    ""cols"": [""postag_feature1"", ""postag_feature2""],\n                    ""dim"": 20\n                  }\n                }\n                or\n                {\n                  ""bpe"": {\n                    ""cols"": [""word1"", ""word2""],\n                    ""dim"": 100\n                    ""bpe_path"": ""xxx.bpe""\n                  }\n                }\n            object_inputs: {\n              ""string1"": [\n                ""word1"",\n                ""postag_feature1""\n              ],\n              ""string2"": [\n                ""word2"",\n                ""postag_feature2""\n              ]\n            },\n            answer_column_name: \'label\' / None. None means there is no target and it is used for prediction only.\n            max_lengths: if it is a dict, firstly cut the sequences if they exceed the max length. Then, pad all the sequences to the length of longest string.\n                {\n                    ""string1"": 25,\n                    ""string2"": 100\n                }\n            fixed_lengths: if it is a dict, cut or pad the sequences to the fixed lengths.\n                {\n                    ""string1"": 25,\n                    ""string2"": 100\n                }\n            file_format:\n\n        Returns:\n            data: indices, padded\n                {\n                \'string1\': {\n                    \'word1\': [...],\n                    \'postage_feature1\': [..]\n                    }\n                \'string2\': {\n                    \'word1\': [...],\n                    \'postage_feature1\': [..]\n                }\n            lengths: real length of data\n                {\n                \'string1\':   [...],\n                \'string2\':   [...]\n                }\n            target: [...]\n\n        """"""\n        bpe_encoder = self._check_bpe_encoder(input_types)  \n\n        progress = self.get_data_generator_from_file(data_path, file_with_col_header, chunk_size=chunk_size)\n        encode_generator = self.encode_data_multi_processor(progress, cpu_num_workers,\n                    file_columns, input_types, object_inputs, answer_column_name, min_sentence_len, extra_feature, max_lengths,\n                    fixed_lengths, file_format, bpe_encoder=bpe_encoder)\n        \n        data, lengths, target = dict(), dict(), dict()\n        cnt_legal, cnt_illegal = 0, 0\n        for temp_data, temp_lengths, temp_target, temp_cnt_legal, temp_cnt_illegal in tqdm(encode_generator):\n            data = self._merge_encode_data(data, temp_data)\n            lengths = self._merge_encode_lengths(lengths, temp_lengths)\n            target = self._merge_target(target, temp_target)   \n            cnt_legal += temp_cnt_legal\n            cnt_illegal += temp_cnt_illegal\n\n        logging.info(""%s: %d legal samples, %d illegal samples"" % (data_path, cnt_legal, cnt_illegal))\n        return data, lengths, target\n\n    def build_encode_cache(self, conf, file_format=""tsv""):\n        logging.info(""[Cache] building encoding cache"") \n        build_encode_cache_generator = self.get_encode_generator(conf, build_cache=True, file_format=file_format)\n        for _ in build_encode_cache_generator:\n            continue\n        logging.info(""[Cache] encoding is saved to %s"" % conf.encoding_cache_dir)    \n        \n    def get_encode_generator(self, conf, build_cache=True, file_format=""tsv""):\n        # parameter check\n        if build_cache:\n            assert conf.encoding_cache_dir, \'There is no property encoding_cache_dir in object conf\'\n            assert conf.encoding_cache_index_file_path, \'There is no property encoding_cache_index_file_path in object conf\'\n            assert conf.encoding_cache_index_file_md5_path, \'There is no property encoding_cache_index_file_md5_path in object conf\'\n\n        bpe_encoder = self._check_bpe_encoder(conf.input_types)   \n        data_generator = self.get_data_generator_from_file(conf.train_data_path, conf.file_with_col_header, chunk_size=conf.chunk_size)\n        encode_generator = self.encode_data_multi_processor(data_generator, conf.cpu_num_workers,\n                    conf.file_columns, conf.input_types, conf.object_inputs, conf.answer_column_name, \n                    conf.min_sentence_len, conf.extra_feature, conf.max_lengths,\n                    conf.fixed_lengths, file_format, bpe_encoder=bpe_encoder)\n      \n        file_index = []\n        total_cnt_legal, total_cnt_illegal = 0, 0\n        for part_number, encode_data in enumerate(encode_generator):\n            data, lengths, target, cnt_legal, cnt_illegal = encode_data\n            if build_cache:\n                total_cnt_legal = total_cnt_legal + cnt_legal\n                total_cnt_illegal = total_cnt_illegal + cnt_illegal\n                file_name = st.cencoding_file_name_pattern % (part_number)\n                file_path = os.path.join(conf.encoding_cache_dir, file_name)\n                dump_to_pkl((data, lengths, target), file_path)\n                file_index.append([file_name, md5([file_path])])\n                logging.info(""Up to now, in %s: %d legal samples, %d illegal samples"" % (conf.train_data_path, total_cnt_legal, total_cnt_illegal))\n            yield data, lengths, target\n        \n        if build_cache:\n            cache_index = dict()\n            cache_index[st.cencoding_key_index] = file_index\n            cache_index[st.cencoding_key_legal_cnt] = total_cnt_legal\n            cache_index[st.cencoding_key_illegal_cnt] = total_cnt_illegal\n            dump_to_json(cache_index, conf.encoding_cache_index_file_path)\n            dump_to_json(md5([conf.encoding_cache_index_file_path]), conf.encoding_cache_index_file_md5_path)            \n            \n    @staticmethod\n    def _check_bpe_encoder(input_types):\n        bpe_encoder = None\n        if \'bpe\' in input_types:\n            try:\n                bpe_encoder = BPEEncoder(input_types[\'bpe\'][\'bpe_path\'])\n            except KeyError:\n                raise Exception(\'Please define a bpe path at the embedding layer.\')\n        return bpe_encoder\n\n    def decode(self, model_output, lengths=None, batch_data=None):\n        """""" decode the model output, either a batch of output or a single output\n\n        Args:\n            model_output: target indices.\n                if is 1d array, it is an output of a sample;\n                if is 2d array, it is outputs of a batch of samples;\n            lengths: if not None, the shape of length should be consistent with model_output.\n\n        Returns:\n            the original output\n\n        """"""\n        if ProblemTypes[self.problem_type] == ProblemTypes.classification:\n            if isinstance(model_output, int):       # output of a sample\n                return self.output_dict.cell(model_output)\n            else:   # output of a batch\n                return self.output_dict.decode(model_output)\n        elif ProblemTypes[self.problem_type] == ProblemTypes.sequence_tagging:\n            if isinstance(model_output, dict):\n                model_output = list(model_output.values())[0]\n            if not isinstance(model_output, np.ndarray):\n                model_output = np.array(model_output)\n            if len(model_output.shape) == 1:        # output of a sample\n                if lengths is None:\n                    outputs = np.array(self.output_dict.decode(model_output))\n                else:\n                    outputs = np.array(self.output_dict.decode(model_output[:lengths]))\n                if self.with_bos_eos:\n                    outputs = outputs[1:-1]\n\n            elif len(model_output.shape) == 2:      # output of a batch of sequence\n                outputs = []\n                if lengths is None:\n                    for sample in model_output:\n                        if self.with_bos_eos:\n                            outputs.append(self.output_dict.decode(sample[1:-1]))\n                        else:\n                            outputs.append(self.output_dict.decode(sample))\n                else:\n                    for sample, length in zip(model_output, lengths):\n                        if self.with_bos_eos:\n                            outputs.append(self.output_dict.decode(sample[:length][1:-1]))\n                        else:\n                            outputs.append(self.output_dict.decode(sample[:length]))\n            return outputs\n        elif ProblemTypes[self.problem_type] == ProblemTypes.mrc:\n            # for mrc, model_output is dict\n            answers = []\n            p1, p2 = list(model_output.values())[0], list(model_output.values())[1]\n            batch_size, c_len = p1.size()\n            passage_length = lengths.numpy()\n            padding_mask = np.ones((batch_size, c_len))\n            for i, single_len in enumerate(passage_length):\n                padding_mask[i][:single_len] = 0\n            device = p1.device\n            padding_mask = torch.from_numpy(padding_mask).byte().to(device)\n            p1.data.masked_fill_(padding_mask.data, float(\'-inf\'))\n            p2.data.masked_fill_(padding_mask.data, float(\'-inf\'))\n            ls = nn.LogSoftmax(dim=1)\n            mask = (torch.ones(c_len, c_len) * float(\'-inf\')).to(device).tril(-1).unsqueeze(0).expand(batch_size, -1, -1)\n            score = (ls(p1).unsqueeze(2) + ls(p2).unsqueeze(1)) + mask\n            score, s_idx = score.max(dim=1)\n            score, e_idx = score.max(dim=1)\n            s_idx = torch.gather(s_idx, 1, e_idx.view(-1, 1)).squeeze()\n            # encode mrc answer text\n            passage_text = \'extra_passage_text\'\n            passage_token_offsets = \'extra_passage_token_offsets\'\n            for i in range(batch_size):\n                char_s_idx, _ = batch_data[passage_token_offsets][i][s_idx[i]]\n                _, char_e_idx = batch_data[passage_token_offsets][i][e_idx[i]]\n                answer = batch_data[passage_text][i][char_s_idx:char_e_idx]\n                answers.append(answer)\n            return answers\n\n    def get_vocab_sizes(self):\n        """""" get size of vocabs: including word embedding, postagging ...\n\n        Returns:\n            {\n                \'word\':  xxx,\n                \'postag\': xxx,\n            }\n\n        """"""\n        vocab_sizes = dict()\n        for input in self.input_dicts:\n            vocab_sizes[input] = self.input_dicts[input].cell_num()\n        return vocab_sizes\n\n    def export_problem(self, save_path, ret_without_save=False):\n        if not os.path.exists(os.path.dirname(save_path)):\n            os.makedirs(os.path.dirname(save_path))\n\n        problem = dict()\n        for name, value in vars(self).items():\n            if name.startswith(""__"") is False:\n                if isinstance(value, CellDict):\n                    problem[name] = value.export_cell_dict()\n                else:\n                    problem[name] = value\n\n        if ret_without_save is False:\n            with open(save_path, \'wb\') as fout:\n                pkl.dump(problem, fout, protocol=pkl.HIGHEST_PROTOCOL)\n            logging.debug(""Problem saved to %s"" % save_path)\n            return None\n        else:\n            return problem\n\n    def load_problem(self, problem_path):\n        info_dict = load_from_pkl(problem_path)\n        for name in info_dict:\n            if isinstance(getattr(self, name), CellDict):\n                getattr(self, name).load_cell_dict(info_dict[name])\n\n            else:\n                setattr(self, name, info_dict[name])\n            # the type of input_dicts is dict\n            # elif name == \'input_dicts\' and isinstance(getattr(self, name), type(info_dict[name])):\n            #     setattr(self, name, info_dict[name])\n        logging.debug(""Problem loaded"")\n\n    def delete_example(self, data, true_len):\n        if isinstance(data, list):\n            if len(data)>true_len:\n                data.pop()\n        else:\n            # data is dict\n            for single_value in data.values():\n                self.delete_example(single_value, true_len)\n'"
register_block.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport os\nimport argparse\n\n\ndef get_block_path(block_name, path=\'./block_zoo\'):\n    \'\'\' find the block_name.py file in block_zoo\n    Args:\n         block_name: the name need to be registered. eg. BiLSTM/ CRF\n    \'\'\'\n    get_dir = os.listdir(path)\n    for single in get_dir:\n        sub_dir = os.path.join(path, single)\n        if os.path.isdir(sub_dir):\n            result = get_block_path(block_name, path=sub_dir)\n            if result:\n                return result\n        else:\n            if block_name + \'.py\' == single:\n                return sub_dir\n    return None\n\n\ndef write_file(new_block_path, file_path):\n    init_path = os.path.join(file_path, \'__init__.py\')\n    diff = new_block_path[len(file_path):].split(\'/\')\n    if diff[0] == \'\':\n        diff.pop(0)\n    # delete \'.py\' in the last str\n    diff[-1] = diff[-1][:-3]\n    line = \'from .\' + diff[0] + \' import \' + diff[-1] + \', \' + diff[-1] + \'Conf\'\n    with open(init_path, \'a\', encoding=\'utf-8\') as fin:\n        fin.write(\'\\n\' + line + \'\\n\')\n\n\ndef register(block_name, new_block_path):\n    \'\'\' Add import code in the corresponding file. eg. block_zoo/__init__.py or block_zoo/subdir/__init__.py\n\n    \'\'\'\n    # check if block exist or not\n    if new_block_path:\n        block_path_split = new_block_path.split(\'/\')\n        for i in range(len(block_path_split)-1, 1, -1):\n            # need_add_file.append(os.path.join(\'/\'.join(block_path_split[:i])))\n            write_file(new_block_path, os.path.join(\'/\'.join(block_path_split[:i])))\n        print(\'The block %s is registered successfully.\' % block_name)\n    else:\n        raise Exception(\'The %s.py file does not exist! Please check your program or file name.\' % block_name)\n\n\ndef main(params):\n    new_block_path = get_block_path(params.block_name)\n    register(params.block_name, new_block_path)\n\n\nif __name__ == \'__main__\':\n    parse = argparse.ArgumentParser(description=\'Register Block\')\n    parse.add_argument(""--block_name"", type=str, help=""block name want to be registered"")\n    params, _ = parse.parse_known_args()\n    assert params.block_name, \'Please specify a block_name via --block_name\'\n    main(params)\n'"
settings.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n# add the project root to python path\nimport os\nimport sys\nsys.path.append(sys.path[0])\nfrom enum import Enum\nimport nltk\n\n\nversion = \'1.1.0\'\n\n# Supported languages\nLanguageTypes = Enum(\'LanguageTypes\', (\'english\', \'chinese\'))\n\n# Supported problems\nProblemTypes = Enum(\'ProblemTypes\', (\'sequence_tagging\', \'classification\', \'regression\', \'mrc\'))\n\n# Supported sequence tagging scheme\nTaggingSchemes = Enum(\'TaggingSchemes\', (\'BIO\', \'BIOES\'))\n\n# supported metrics\nSupportedMetrics = {\n    ProblemTypes.sequence_tagging: set([\'seq_tag_f1\', \'seq_tag_accuracy\']),\n    ProblemTypes.classification: set([\'auc\', \'accuracy\', \'f1\', \'macro_f1\', \'macro_precision\', \'macro_recall\', \'micro_f1\', \'micro_precision\', \'micro_recall\', \'weighted_f1\', \'weighted_precision\', \'weighted_recall\']),\n    # In addition, for auc in multi-type classification,\n    # if there is a type named 1, auc@1 means use 1 as the positive label\n    # auc@average means enumerate all the types as the positive label and obtain the average auc.\n    ProblemTypes.regression: set([\'MSE\', \'RMSE\']),\n    ProblemTypes.mrc: set([\'f1\', \'em\']),\n}\n\n# Supported prediction types\nPredictionTypes = {\n    ProblemTypes.sequence_tagging: set([\'prediction\']),\n    ProblemTypes.classification: set([\'prediction\', \'confidence\']),     # In addition, if there is a type named positive, confidence@positive means the confidence of positive\n    ProblemTypes.regression: set([\'prediction\']),\n    ProblemTypes.mrc: set([\'prediction\']),\n}\n\n# Supported multi_loss operation\nLossOperationType = Enum(\'LossOperationType\', (\'weighted_sum\'))\n\n# If prediction_field is not defined, use the default fields below\nDefaultPredictionFields = {\n    ProblemTypes.sequence_tagging: [\'prediction\'],\n    ProblemTypes.classification: [\'prediction\', \'confidence\'],\n    ProblemTypes.regression: [\'prediction\'],\n    ProblemTypes.mrc: [\'prediction\'],\n}\n\n# nltk\'s models\nnltk.data.path.append(os.path.join(os.getcwd(), \'dataset\', \'nltk_data\'))\n\n\nclass Constant(type):\n    def __setattr__(self, name, value):\n        raise AttributeError(""Class %s can not be modified""%(self.__name__))\n\nclass ConstantStatic(metaclass=Constant):\n    def __init__(self, *args,**kwargs):\n        raise Exception(""Class %s can not be instantiated""%(self.__class__.__name__))\n\n\nclass Setting(ConstantStatic):\n    # cache\n\n    ## cencoding (cache_encoding)\n    cencodig_index_file_name = \'index.json\'\n    cencoding_index_md5_file_name = \'index_md5.json\'\n    cencoding_file_name_pattern = \'encoding_cache_%s.pkl\'\n    cencoding_key_finish = \'finish\'\n    cencoding_key_index = \'index\'\n    cencoding_key_legal_cnt = \'legal_line_cnt\'\n    cencoding_key_illegal_cnt = \'illegal_line_cnt\'\n\n\n\n'"
test.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nfrom settings import ProblemTypes, version\n\nimport os\nimport argparse\nimport logging\n\nfrom ModelConf import ModelConf\nfrom problem import Problem\nfrom losses import *\n\nfrom LearningMachine import LearningMachine\n\n\ndef main(params):\n    conf = ModelConf(""test"", params.conf_path, version, params, mode=params.mode)\n    problem = Problem(""test"", conf.problem_type, conf.input_types, conf.answer_column_name,\n        with_bos_eos=conf.add_start_end_for_seq, tagging_scheme=conf.tagging_scheme, tokenizer=conf.tokenizer,\n        remove_stopwords=conf.remove_stopwords, DBC2SBC=conf.DBC2SBC, unicode_fix=conf.unicode_fix)\n\n    if os.path.isfile(conf.saved_problem_path):\n        problem.load_problem(conf.saved_problem_path)\n        logging.info(""Problem loaded!"")\n        logging.debug(""Problem loaded from %s"" % conf.saved_problem_path)\n    else:\n        raise Exception(""Problem does not exist!"")\n\n    if len(conf.metrics_post_check) > 0:\n        for metric_to_chk in conf.metrics_post_check:\n            metric, target = metric_to_chk.split(\'@\')\n            if not problem.output_dict.has_cell(target):\n                raise Exception(""The target %s of %s does not exist in the training data."" % (target, metric_to_chk))\n\n    lm = LearningMachine(\'test\', conf, problem, vocab_info=None, initialize=False, use_gpu=conf.use_gpu)\n    lm.load_model(conf.previous_model_path)\n\n    loss_conf = conf.loss\n    # loss_fn = eval(loss_conf[\'type\'])(**loss_conf[\'conf\'])\n    loss_conf[\'output_layer_id\'] = conf.output_layer_id\n    loss_conf[\'answer_column_name\'] = conf.answer_column_name\n    loss_fn = Loss(**loss_conf)\n    if conf.use_gpu is True:\n        loss_fn.cuda()\n\n    test_path = params.test_data_path\n    if conf.test_data_path is not None:\n        test_path = conf.test_data_path\n    elif conf.valid_data_path is not None:\n        test_path = conf.valid_data_path\n\n    logging.info(\'Testing the best model saved at %s, with %s\' % (conf.previous_model_path, test_path))\n    if not test_path.endswith(\'pkl\'):\n        lm.test(loss_fn, test_path, predict_output_path=conf.predict_output_path)\n    else:\n        lm.test(loss_fn, test_path)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=\'testing\')\n    parser.add_argument(""--conf_path"", type=str, help=""configuration path"")\n    parser.add_argument(""--test_data_path"", type=str, help=\'specify another test data path, instead of the one defined in configuration file\')\n    parser.add_argument(""--previous_model_path"", type=str, help=\'load model trained previously.\')\n    parser.add_argument(""--predict_output_path"", type=str, help=\'specify another prediction output path, instead of conf[outputs][save_base_dir] + conf[outputs][predict_output_name] defined in configuration file\')\n    parser.add_argument(""--log_dir"", type=str)\n    parser.add_argument(""--batch_size"", type=int, help=\'batch_size of each gpu\')\n    parser.add_argument(""--mode"", type=str, default=\'normal\', help=\'normal|philly\')\n    parser.add_argument(""--force"", type=bool, default=False, help=\'Allow overwriting if some files or directories already exist.\')\n    parser.add_argument(""--disable_log_file"", type=bool, default=False, help=\'If True, disable log file\')\n    parser.add_argument(""--debug"", type=bool, default=False)\n    params, _ = parser.parse_known_args()\n\n    assert params.conf_path, \'Please specify a configuration path via --conf_path\'\n    if params.debug is True:\n        import debugger\n    main(params)'"
train.py,1,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nfrom settings import ProblemTypes, version, Setting as st\n\nimport os\nimport argparse\nimport logging\nimport shutil\nimport time\nimport numpy as np\nimport copy\n\nimport torch\nimport torch.nn as nn\nfrom ModelConf import ModelConf\nfrom problem import Problem\nfrom utils.common_utils import dump_to_pkl, load_from_pkl, load_from_json, dump_to_json, prepare_dir, md5\nfrom utils.philly_utils import HDFSDirectTransferer\nfrom losses import *\nfrom optimizers import *\n\nfrom LearningMachine import LearningMachine\n\nclass Cache:\n    def __init__(self):\n        self.dictionary_invalid = True\n        self.embedding_invalid = True\n        self.encoding_invalid = True\n    \n    def _check_dictionary(self, conf, params):\n        # init status\n        self.dictionary_invalid = True\n        self.embedding_invalid = True\n\n        if not conf.pretrained_model_path:\n            # cache_conf\n            cache_conf = None\n            cache_conf_path = os.path.join(conf.cache_dir, \'conf_cache.json\')\n            if os.path.isfile(cache_conf_path):\n                params_cache = copy.deepcopy(params)\n                try:\n                    cache_conf = ModelConf(\'cache\', cache_conf_path, version, params_cache)\n                except Exception as e:\n                    cache_conf = None\n            if cache_conf is None or not self._verify_conf(cache_conf, conf):\n                return False\n            \n            # problem\n            if not os.path.isfile(conf.problem_path):\n                return False\n\n            # embedding\n            if conf.emb_pkl_path:\n                if not os.path.isfile(conf.emb_pkl_path):\n                    return False\n                self.embedding_invalid = False\n        \n            self.dictionary_invalid = False\n            logging.info(\'[Cache] dictionary found\')\n        return True\n        \n    def _check_encoding(self, conf):\n        self.encoding_invalid = True\n        if not conf.pretrained_model_path and self.dictionary_invalid:\n            return False\n        \n        # Calculate the MD5 of problem\n        problem_path = conf.problem_path if not conf.pretrained_model_path else conf.saved_problem_path\n        try:\n            conf.problem_md5 = md5([problem_path])\n        except Exception as e:\n            conf.problem_md5 = None\n            logging.info(\'Can not calculate md5 of problem.pkl from %s\'%(problem_path))\n            return False\n        \n        # check the valid of encoding cache\n        ## encoding cache dir\n        conf.encoding_cache_dir = os.path.join(conf.cache_dir, conf.train_data_md5 + conf.problem_md5)\n        logging.debug(\'[Cache] conf.encoding_cache_dir %s\' % (conf.encoding_cache_dir))\n        if not os.path.exists(conf.encoding_cache_dir):\n            return False\n        \n        ## encoding cache index \n        conf.encoding_cache_index_file_path = os.path.join(conf.encoding_cache_dir, st.cencodig_index_file_name)\n        conf.encoding_cache_index_file_md5_path = os.path.join(conf.encoding_cache_dir, st.cencoding_index_md5_file_name)\n        if not os.path.exists(conf.encoding_cache_index_file_path) or not os.path.exists(conf.encoding_cache_index_file_md5_path):\n            return False\n        if md5([conf.encoding_cache_index_file_path]) != load_from_json(conf.encoding_cache_index_file_md5_path):\n            return False\n        cache_index = load_from_json(conf.encoding_cache_index_file_path)\n\n        ## encoding cache content\n        for index in cache_index[st.cencoding_key_index]:\n            file_name, file_md5 = index[0], index[1]\n            if file_md5 != md5([os.path.join(conf.encoding_cache_dir, file_name)]):\n                return False\n        \n        if (st.cencoding_key_legal_cnt in cache_index) and (st.cencoding_key_illegal_cnt in cache_index):\n            conf.encoding_cache_legal_line_cnt = cache_index[st.cencoding_key_legal_cnt]\n            conf.encoding_cache_illegal_line_cnt = cache_index[st.cencoding_key_illegal_cnt]\n        \n        self.encoding_invalid = False\n        logging.info(\'[Cache] encoding found\')\n        logging.info(\'%s: %d legal samples, %d illegal samples\' % (conf.train_data_path, conf.encoding_cache_legal_line_cnt, conf.encoding_cache_illegal_line_cnt))\n        return True\n\n    def check(self, conf, params):\n        # dictionary\n        if not self._check_dictionary(conf, params):\n            self._renew_cache(params, conf.cache_dir)\n            return\n        # encoding\n        if not self._check_encoding(conf):\n            self._renew_cache(params, conf.encoding_cache_dir)\n\n    def load(self, conf, problem, emb_matrix):\n        # load dictionary when (not finetune) and (cache valid)\n        if not conf.pretrained_model_path and not self.dictionary_invalid:\n            problem.load_problem(conf.problem_path)\n            if not self.embedding_invalid:\n                emb_matrix = np.array(load_from_pkl(conf.emb_pkl_path))\n            logging.info(\'[Cache] loading dictionary successfully\')\n        \n        if not self.encoding_invalid:\n            self._prepare_encoding_cache(conf, problem, build=False)\n            logging.info(\'[Cache] preparing encoding successfully\')\n        return problem, emb_matrix\n\n    def save(self, conf, params, problem, emb_matrix):\n        # make cache dir\n        if not os.path.exists(conf.cache_dir):\n            os.makedirs(conf.cache_dir)\n        shutil.copy(params.conf_path, os.path.join(conf.cache_dir, \'conf_cache.json\'))\n\n        # dictionary\n        if self.dictionary_invalid:\n            if conf.mode == \'philly\' and conf.emb_pkl_path.startswith(\'/hdfs/\'):\n                with HDFSDirectTransferer(conf.problem_path, with_hdfs_command=True) as transferer:\n                    transferer.pkl_dump(problem.export_problem(conf.problem_path, ret_without_save=True))\n            else:\n                problem.export_problem(conf.problem_path)\n            logging.info(""[Cache] problem is saved to %s"" % conf.problem_path)\n            if emb_matrix is not None and conf.emb_pkl_path is not None:\n                if conf.mode == \'philly\' and conf.emb_pkl_path.startswith(\'/hdfs/\'):\n                    with HDFSDirectTransferer(conf.emb_pkl_path, with_hdfs_command=True) as transferer:\n                        transferer.pkl_dump(emb_matrix)\n                else:\n                    dump_to_pkl(emb_matrix, conf.emb_pkl_path)\n            logging.info(""[Cache] Embedding matrix saved to %s"" % conf.emb_pkl_path)\n        \n        # encoding\n        if self.encoding_invalid:\n            self._prepare_encoding_cache(conf, problem, build=params.make_cache_only) \n\n    def back_up(self, conf, problem):\n        cache_bakup_path = os.path.join(conf.save_base_dir, \'necessary_cache/\')\n        logging.debug(\'Prepare dir: %s\' % cache_bakup_path)\n        prepare_dir(cache_bakup_path, True, allow_overwrite=True, clear_dir_if_exist=True)\n\n        problem.export_problem(cache_bakup_path+\'problem.pkl\')\n        logging.debug(""Problem %s is backed up to %s"" % (conf.problem_path, cache_bakup_path))\n\n    def _renew_cache(self, params, cache_path):\n        if not os.path.exists(cache_path):\n            return\n        logging.info(\'Found cache that is ineffective\')\n        renew_option = \'yes\'\n        if params.mode != \'philly\' and params.force is not True:\n            renew_option = input(\'There exists ineffective cache %s for old models. Input ""yes"" to renew cache and ""no"" to exit. (default:no): \' % os.path.abspath(cache_path))\n        if renew_option.lower() != \'yes\':\n            exit(0)\n        else:\n            shutil.rmtree(cache_path)\n            time.sleep(2)  # sleep 2 seconds since the deleting is asynchronous\n            logging.info(\'Old cache is deleted\')\n\n    def _verify_conf(self, cache_conf, cur_conf):\n        """""" To verify if the cache is appliable to current configuration\n\n        Args:\n            cache_conf (ModelConf):\n            cur_conf (ModelConf):\n\n        Returns:\n\n        """"""\n        if cache_conf.tool_version != cur_conf.tool_version:\n            return False\n\n        attribute_to_cmp = [\'file_columns\', \'object_inputs\', \'answer_column_name\', \'input_types\', \'language\']\n\n        flag = True\n        for attr in attribute_to_cmp:\n            if not (hasattr(cache_conf, attr) and hasattr(cur_conf, attr) and getattr(cache_conf, attr) == getattr(cur_conf, attr)):\n                logging.error(\'configuration %s is inconsistent with the old cache\' % attr)\n                flag = False\n        return flag\n\n    def _prepare_encoding_cache(self, conf, problem, build=False):\n        # encoding cache dir\n        problem_path = conf.problem_path if not conf.pretrained_model_path else conf.saved_problem_path\n        conf.problem_md5 = md5([problem_path])\n        conf.encoding_cache_dir = os.path.join(conf.cache_dir, conf.train_data_md5 + conf.problem_md5)\n        if not os.path.exists(conf.encoding_cache_dir):\n            os.makedirs(conf.encoding_cache_dir)\n        \n        # encoding cache files\n        conf.encoding_cache_index_file_path = os.path.join(conf.encoding_cache_dir, st.cencodig_index_file_name)\n        conf.encoding_cache_index_file_md5_path = os.path.join(conf.encoding_cache_dir, st.cencoding_index_md5_file_name) \n        conf.load_encoding_cache_generator = self._load_encoding_cache_generator\n        \n        if build:\n            prepare_dir(conf.encoding_cache_dir, True, allow_overwrite=True, clear_dir_if_exist=True)\n            problem.build_encode_cache(conf)\n            self.encoding_invalid = False\n\n        if not self.encoding_invalid:\n            cache_index = load_from_json(conf.encoding_cache_index_file_path)\n            conf.encoding_file_index = cache_index[st.cencoding_key_index]\n\n    @staticmethod\n    def _load_encoding_cache_generator(cache_dir, file_index):\n        for index in file_index:\n            file_path = os.path.join(cache_dir, index[0])\n            yield load_from_pkl(file_path)\n    \ndef main(params):\n    # init\n    conf = ModelConf(""train"", params.conf_path, version, params, mode=params.mode)\n    problem = Problem(""train"", conf.problem_type, conf.input_types, conf.answer_column_name,\n        with_bos_eos=conf.add_start_end_for_seq, tagging_scheme=conf.tagging_scheme, tokenizer=conf.tokenizer,\n        remove_stopwords=conf.remove_stopwords, DBC2SBC=conf.DBC2SBC, unicode_fix=conf.unicode_fix)\n    if conf.pretrained_model_path:\n        ### when finetuning, load previous saved problem\n        problem.load_problem(conf.saved_problem_path)\n   \n    # cache verification\n    emb_matrix = None\n    cache = Cache()\n    if conf.use_cache:\n        ## check\n        cache.check(conf, params)\n        ## load\n        problem, emb_matrix = cache.load(conf, problem, emb_matrix)\n\n    # data preprocessing\n    ## build dictionary when (not in finetune model) and (not use cache or cache invalid)\n    if (not conf.pretrained_model_path) and ((conf.use_cache == False) or cache.dictionary_invalid):\n        logging.info(""=""*100)\n        logging.info(""Preprocessing... Depending on your corpus size, this step may take a while."")\n        # modify train_data_path to [train_data_path, valid_data_path, test_data_path]\n        # remember the test_data may be None\n        data_path_list = [conf.train_data_path, conf.valid_data_path, conf.test_data_path]\n        emb_matrix = problem.build(data_path_list, conf.file_columns, conf.input_types, conf.file_with_col_header,\n                                    conf.answer_column_name, word2vec_path=conf.pretrained_emb_path,\n                                    word_emb_dim=conf.pretrained_emb_dim, format=conf.pretrained_emb_type,\n                                    file_type=conf.pretrained_emb_binary_or_text, involve_all_words=conf.involve_all_words_in_pretrained_emb,\n                                    show_progress=True if params.mode == \'normal\' else False, cpu_num_workers = conf.cpu_num_workers,\n                                    max_vocabulary=conf.max_vocabulary, word_frequency=conf.min_word_frequency, max_building_lines=conf.max_building_lines)\n\n    # environment preparing\n    ## cache save\n    if conf.use_cache:\n        cache.save(conf, params, problem, emb_matrix)\n\n    if params.make_cache_only:\n        if conf.use_cache:\n            logging.info(""Finish building cache!"")\n        else:\n            logging.info(\'Please set parameters ""use_cache"" is true\')\n        return\n\n    ## back up the problem.pkl to save_base_dir/.necessary_cache. \n    ## During test phase, we would load cache from save_base_dir/.necessary_cache/problem.pkl\n    conf.back_up(params) \n    cache.back_up(conf, problem)\n    if problem.output_dict:\n        logging.debug(""Problem target cell dict: %s"" % (problem.output_dict.cell_id_map))\n    \n    # train phase\n    ## init \n    ### model\n    vocab_info, initialize = None, False\n    if not conf.pretrained_model_path:\n        vocab_info, initialize = get_vocab_info(conf, problem, emb_matrix), True\n  \n    lm = LearningMachine(\'train\', conf, problem, vocab_info=vocab_info, initialize=initialize, use_gpu=conf.use_gpu)\n    if conf.pretrained_model_path:\n        logging.info(\'Loading the pretrained model: %s...\' % conf.pretrained_model_path)\n        lm.load_model(conf.pretrained_model_path)\n\n    ### loss\n    if len(conf.metrics_post_check) > 0:\n        for metric_to_chk in conf.metrics_post_check:\n            metric, target = metric_to_chk.split(\'@\')\n            if not problem.output_dict.has_cell(target):\n                raise Exception(""The target %s of %s does not exist in the training data."" % (target, metric_to_chk))\n    loss_conf = conf.loss\n    loss_conf[\'output_layer_id\'] = conf.output_layer_id\n    loss_conf[\'answer_column_name\'] = conf.answer_column_name\n    # loss_fn = eval(loss_conf[\'type\'])(**loss_conf[\'conf\'])\n    loss_fn = Loss(**loss_conf)\n    if conf.use_gpu is True:\n        loss_fn.cuda()\n\n    ### optimizer\n    if isinstance(lm.model, nn.DataParallel):\n        if isinstance(lm.model.module.layers[\'embedding\'].embeddings, nn.ModuleDict):\n            optimizer = eval(conf.optimizer_name)(list(lm.model.parameters()), **conf.optimizer_params)\n        else:\n            optimizer = eval(conf.optimizer_name)(\n                list(lm.model.parameters()) + list(lm.model.module.layers[\'embedding\'].get_parameters()),\n                **conf.optimizer_params)\n    else:\n        if isinstance(lm.model.layers[\'embedding\'].embeddings, nn.ModuleDict):\n            optimizer = eval(conf.optimizer_name)(\n                list(lm.model.parameters()), **conf.optimizer_params)\n        else:\n            optimizer = eval(conf.optimizer_name)(\n                list(lm.model.parameters()) + list(lm.model.layers[\'embedding\'].get_parameters()),\n                **conf.optimizer_params)\n\n    ## train\n    lm.train(optimizer, loss_fn)\n\n    ## test the best model with the best model saved\n    lm.load_model(conf.model_save_path)\n    if conf.test_data_path is not None:\n        test_path = conf.test_data_path\n    elif conf.valid_data_path is not None:\n        test_path = conf.valid_data_path\n    logging.info(\'Testing the best model saved at %s, with %s\' % (conf.model_save_path, test_path))\n    if not test_path.endswith(\'pkl\'):\n        lm.test(loss_fn, test_path, predict_output_path=conf.predict_output_path)\n    else:\n        lm.test(loss_fn, test_path)\n\ndef get_vocab_info(conf, problem, emb_matrix):\n    vocab_info = dict() # include input_type\'s vocab_size & init_emd_matrix\n    vocab_sizes = problem.get_vocab_sizes()\n    for input_cluster in vocab_sizes:\n        vocab_info[input_cluster] = dict()\n        vocab_info[input_cluster][\'vocab_size\'] = vocab_sizes[input_cluster]\n        # add extra info for char_emb\n        if input_cluster.lower() == \'char\':\n            for key, value in conf.input_types[input_cluster].items():\n                if key != \'cols\':\n                    vocab_info[input_cluster][key] = value\n        if input_cluster == \'word\' and emb_matrix is not None:\n            vocab_info[input_cluster][\'init_weights\'] = emb_matrix\n        else:\n            vocab_info[input_cluster][\'init_weights\'] = None\n    return vocab_info\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=\'Training\')\n    parser.add_argument(""--conf_path"", type=str, help=""configuration path"")\n    parser.add_argument(""--train_data_path"", type=str)\n    parser.add_argument(""--valid_data_path"", type=str)\n    parser.add_argument(""--test_data_path"", type=str)\n    parser.add_argument(""--pretrained_emb_path"", type=str)\n    parser.add_argument(""--pretrained_emb_type"", type=str, default=\'glove\', help=\'glove|word2vec|fasttext\')\n    parser.add_argument(""--pretrained_emb_binary_or_text"", type=str, default=\'text\', help=\'text|binary\')\n    parser.add_argument(""--involve_all_words_in_pretrained_emb"", type=bool, default=False, help=\'By default, only words that show up in the training data are involved.\')\n    parser.add_argument(""--pretrained_model_path"", type=str, help=\'load pretrained model, and then finetune it.\')\n    parser.add_argument(""--cache_dir"", type=str, help=\'where stores the built problem.pkl where there are dictionaries like word2id, id2word. CAUTION: if there is a previous model, the dictionaries would be loaded from os.path.dir(previous_model_path)/.necessary_cache/problem.pkl\')\n    parser.add_argument(""--model_save_dir"", type=str, help=\'where to store models\')\n    parser.add_argument(""--predict_output_path"", type=str, help=\'specify another prediction output path, instead of conf[outputs][save_base_dir] + conf[outputs][predict_output_name] defined in configuration file\')\n    parser.add_argument(""--log_dir"", type=str, help=\'If not specified, logs would be stored in conf_bilstmlast.json/outputs/save_base_dir\')\n    parser.add_argument(""--make_cache_only"", type=bool, default=False, help=\'make cache without training\')\n    parser.add_argument(""--max_epoch"", type=int, help=\'maximum number of epochs\')\n    parser.add_argument(""--batch_size"", type=int, help=\'batch_size of each gpu\')\n    parser.add_argument(""--learning_rate"", type=float, help=\'learning rate\')\n    parser.add_argument(""--mode"", type=str, default=\'normal\', help=\'normal|philly\')\n    parser.add_argument(""--force"", type=bool, default=False, help=\'Allow overwriting if some files or directories already exist.\')\n    parser.add_argument(""--disable_log_file"", type=bool, default=False, help=\'If True, disable log file\')\n    parser.add_argument(""--debug"", type=bool, default=False)\n\n    params, _ = parser.parse_known_args()\n    # use for debug, remember delete\n    # params.conf_path = \'configs_example/conf_debug_charemb.json\'\n\n    assert params.conf_path, \'Please specify a configuration path via --conf_path\'\n    if params.pretrained_emb_path and not os.path.isabs(params.pretrained_emb_path):\n        params.pretrained_emb_path = os.path.join(os.getcwd(), params.pretrained_emb_path)\n    if params.debug is True:\n        import debugger\n    main(params)\n'"
block_zoo/BaseLayer.py,2,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport codecs\nimport logging\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom abc import ABC, abstractmethod\n\nfrom utils.exceptions import LayerDefineError, ConfigurationError\n\n\nclass BaseConf(ABC):\n    """"""Basic configuration\n\n    Args:\n        input_dim (int): the dimension of input.\n        hidden_dim (int): the dimension of hidden state.\n        dropout (float): dropout rate.\n        (others)...\n    """"""\n    def __init__(self, **kwargs):\n        self.default()\n\n        for key in kwargs:\n            setattr(self, key, kwargs[key])\n\n        self.declare()\n\n        self.verify_before_inference()\n\n        # Moved to get_conf() in Model.py\n        #self.inference()\n        #self.verify()\n\n    def default(self):\n        """""" Define the default hyper parameters here. You can define these hyper parameters in your configuration file as well.\n\n        Returns:\n            None\n\n        """"""\n        #self.input_dims = [xxx, xxx]  would be inferenced automatically\n        self.hidden_dim = 10\n\n\n    @abstractmethod\n    def declare(self):\n        """""" Define things like ""input_ranks"" and ""num_of_inputs"", which are certain with regard to your layer\n\n            num_of_input is N(N>0) means this layer accepts N inputs;\n\n            num_of_input is -1 means this layer accepts any number of inputs;\n\n            The rank here is not the same as matrix rank:\n\n              For a scalar, its rank is 0;\\n\n              For a vector, its rank is 1;\\n\n              For a matrix, its rank is 2;\\n\n              For a cube of numbers, its rank is 3.\\n\n            ...\n            For instance, the rank of (batch size, sequence length, hidden_dim) is 3.\n\n            if num_of_input > 0:\n\n              len(input_ranks) should be equal to num_of_input\n\n            elif num_of_input == -1:\n\n              input_ranks should be a list with only one element and the rank of all the inputs should be equal to that element.\n\n            NOTE: when we build the model, if num_of_input is -1, we would replace it with the real number of inputs and replace input_ranks with a list of real input_ranks.\n\n        Returns:\n            None\n\n        """"""\n        self.num_of_inputs = 1\n        self.input_ranks = [3]\n\n    @abstractmethod\n    def inference(self):\n        """""" Inference things like output_dim, which may relies on defined hyper parameter such as hidden dim and input_dim\n\n        Returns:\n            None\n\n        """"""\n        # inference the output dim and output rank from inputs. Here are some examples:\n        #self.output_dim = copy.deepcopy(self.input_dims[0])\n        #self.output_dim[-1] = sum([input_dim[-1] for input_dim in self.input_dims])\n\n        self.output_rank = len(self.output_dim)  # DON\'T MODIFY THIS\n\n    def verify_before_inference(self):\n        """""" Some conditions must be fulfilled, otherwise there would be errors when calling inference()\n\n        The difference between verify_before_inference() and verify() is that:\n            verify_before_inference() is called before inference() while verify() is called after inference().\n\n        Returns:\n            None\n\n        """"""\n        necessary_attrs_for_dev = [\'num_of_inputs\', \'input_ranks\']\n        for attr in necessary_attrs_for_dev:\n            self.add_attr_exist_assertion_for_dev(attr)\n\n        type_checks = [(\'num_of_inputs\', int),\n                       (\'input_ranks\', list)]\n        for attr, attr_type in type_checks:\n            self.add_attr_type_assertion(attr, attr_type)\n\n\n    def verify(self):\n        """""" Define some necessary varification for your layer when we define the model.\n\n        If you define your own layer and rewrite this funciton, please add ""super(YourLayerConf, self).verify()"" at the beginning\n\n        Returns:\n            None\n\n        """"""\n        self.verify_before_inference()      # prevent to be neglected, verify again\n\n        necessary_attrs_for_user = []\n        for attr in necessary_attrs_for_user:\n            self.add_attr_exist_assertion_for_user(attr)\n\n        necessary_attrs_for_dev = [\'input_dims\', \'output_dim\', \'output_rank\', \'use_gpu\']\n        for attr in necessary_attrs_for_dev:\n            self.add_attr_exist_assertion_for_dev(attr)\n\n        type_checks = [(\'output_dim\', list),\n                       (\'input_dims\', list),\n                       (\'output_rank\', int)]\n        for attr, attr_type in type_checks:\n            self.add_attr_type_assertion(attr, attr_type)\n\n        range_checks = [(\'dropout\', (0, 1), (True, True))]\n        for attr, ranges, bound_legal in range_checks:\n            self.add_attr_range_assertion(attr, ranges, bound_legal)\n\n        # demonstration for value checks\n        \'\'\'\n        value_checks = [(\'some_attr\', [\'legal_value1\', \'legal_value2\', \'legal_value3\'])]\n        for attr, legal_values in value_checks:\n            self.add_attr_value_assertion(attr, legal_values)\n        \'\'\'\n\n        # To check if deepcopy is applied\n        assert id(self.output_dim) != id(self.input_dims[0]), \'Please use copy.deepcopy to copy the input_dim to output_dim\'\n\n    def verify_former_block(self, former_conf):\n        """"""check if has special attribute rely on former layer\n\n        """"""\n        return True\n\n    def add_attr_type_assertion(self, attr, specified_type):\n        """""" check if the types of attributes are legal\n\n        Args:\n            attr (str): the attribution name\n            specified_type (None/str/list): one specified_type of a list of specified_type(including None)\n\n        Returns:\n            None\n\n        """"""\n        if not hasattr(self, attr):\n            return\n\n        if isinstance(specified_type, list):\n            hit_flag = False\n            for specified_type_ in specified_type:\n                if specified_type_ is None:\n                    if getattr(self, attr) is None:\n                        hit_flag = True\n                        break\n                else:\n                    if isinstance(getattr(self, attr), specified_type_):\n                        hit_flag = True\n                        break\n\n            if hit_flag is False:\n                raise Exception(""For layer %s, the attribute %s should be one of [%s]!"" % (\n                    type(self).__name__, attr, "", "".join(specified_type_.__name__ if specified_type_ is not None else ""None"" for specified_type_ in specified_type)))\n        else:\n            if not (getattr(self, attr) is None and specified_type is None or isinstance(getattr(self, attr), specified_type)):\n                raise LayerDefineError(""For layer %s, the attribute %s should be a/an %s!"" %\n                            (type(self).__name__, attr, specified_type.__name__))\n\n    def add_attr_range_assertion(self, attr, range, bounds_legal=(True, True)):\n        """""" check if attribute falls into the legal range\n\n        Args:\n            attr (str): the attribution name\n            range (tuple): (num/float(\'-inf\')/None, num/float(\'inf\')/None), None means -inf or inf.\n            bounds_legal (tuple): (bool, bool), if the left/right bound is legal\n\n        Returns:\n            None\n\n        """"""\n        if not hasattr(self, attr):\n            return\n\n        value = getattr(self, attr)\n        range = list(range)\n        bounds_legal = list(bounds_legal)\n        if range[0] is None:\n            range[0] = float(\'-inf\')\n        if range[1] is None:\n            range[1] = float(\'inf\')\n        if range[0] == float(\'-inf\'):\n            bounds_legal[0] = False\n        if range[1] == float(\'inf\'):\n            bounds_legal[1] = False\n\n        left_bound_ch = \'[\' if bounds_legal[0] else \'(\'\n        right_bound_ch = \']\' if bounds_legal[1] else \')\'\n        if not ((bounds_legal[0] and value >= range[0] or bounds_legal[0] is False and value > range[0]) and (\n                bounds_legal[1] and value <= range[1] or bounds_legal[1] is False and value < range[1])):\n            raise Exception(""For layer %s, the legal range of attribute %s is %s%f, %f%s"" % (\n                type(self).__name__, attr, left_bound_ch, range[0], range[1], right_bound_ch))\n\n    def add_attr_exist_assertion_for_dev(self, attr):\n        """""" check if there are some attributes being forgot by developers\n\n        Args:\n            attr (str): the attribution name\n\n        Returns:\n            None\n\n        """"""\n        if not hasattr(self, attr):\n            raise LayerDefineError(""For layer %s, please define %s attribute in declare() or inference()!"" % (type(self).__name__, attr))\n\n    def add_attr_exist_assertion_for_user(self, attr):\n        """""" check if there are some attributes being forgot by users\n\n        Args:\n            attr (str): the attribution name\n\n        Returns:\n            None\n\n        """"""\n        if not hasattr(self, attr):\n            raise ConfigurationError(""For layer %s, please configure %s attribute for %s in the configuration file!"" % (type(self).__name__, attr, type(self).__name__))\n\n    def add_attr_value_assertion(self, attr, legal_values):\n        """""" check if attr equals to one of the legal values\n\n        Args:\n            attr (str): the attribution name\n            legal_values (list): include the legal value\n\n        Returns:\n            None\n\n        """"""\n        if not hasattr(self, attr):\n            return\n        hit_flag = False\n        for legal_value in legal_values:\n            if getattr(self, attr) == legal_value:\n                hit_flag = True\n                break\n        if hit_flag is False:\n            raise Exception(""For layer %s, attribute %s should be one of [%s], but you give %s."" % \\\n                (type(self).__name__, attr, "", "".join(str(legal_value) for legal_value in legal_values), str(getattr(self, attr))))\n\n\nclass BaseLayer(nn.Module):\n    """"""The base class of layers\n\n    Args:\n        layer_conf (BaseConf): configuration of a layer\n    """"""\n    def __init__(self, layer_conf):\n        super(BaseLayer, self).__init__()\n        self.layer_conf = layer_conf\n\n    def forward(self, *args):\n        """"""\n\n        Args:\n            *args (list): a list of args in which arg should be a pair of (representation, length)\n\n        Returns:\n            None\n\n        """"""\n        pass\n\n    def is_cuda(self):\n        """""" To judge if the layer is on CUDA\n        if there are parameters in this layer, judge according to the parameters;\n        else: judge according to the self.layer_conf.use_gpu\n\n        Returns:\n            bool: whether to use gpu\n\n        """"""\n        try:\n            # In case someone forget to check use_gpu flag, this function would first detect if the parameters are on cuda\n            # ret = next(self.parameters()).data.is_cuda\n            ret = self.layer_conf.use_gpu\n        except StopIteration as e:\n            if not hasattr(self, \'layer_conf\'):\n                logging.error(\'Layer.layer_conf must be defined!\')\n            else:\n                logging.error(e)\n\n        return ret\n\n'"
block_zoo/BiGRU.py,3,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport copy\n\nfrom block_zoo.BaseLayer import BaseLayer, BaseConf\nfrom utils.DocInherit import DocInherit\nfrom utils.common_utils import transfer_to_gpu\n\n\nclass BiGRUConf(BaseConf):\n    """"""Configuration of BiGRU\n\n    Args:\n        hidden_dim (int): dimension of hidden state\n        dropout (float): dropout rate\n\n    """"""\n    def __init__(self, **kwargs):\n        super(BiGRUConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        self.hidden_dim = 128\n        self.dropout = 0.0\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 1\n        self.input_ranks = [3]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = copy.deepcopy(self.input_dims[0])\n        self.output_dim[-1] = 2 * self.hidden_dim\n        super(BiGRUConf, self).inference()  # PUT THIS LINE AT THE END OF inference()\n\n    @DocInherit\n    def verify(self):\n        super(BiGRUConf, self).verify()\n        assert hasattr(self, \'hidden_dim\'), ""Please define hidden_dim attribute of BiGRUConf in default() or the configuration file""\n        assert hasattr(self, \'dropout\'), ""Please define dropout attribute of BiGRUConf in default() or the configuration file""\n\n\nclass BiGRU(BaseLayer):\n    """"""Bidirectional GRU\n\n    Args:\n        layer_conf (BiGRUConf): configuration of a layer\n    """"""\n    def __init__(self, layer_conf):\n        super(BiGRU, self).__init__(layer_conf)\n        self.GRU = nn.GRU(layer_conf.input_dims[0][-1], layer_conf.hidden_dim, 1, bidirectional=True,\n            dropout=layer_conf.dropout, batch_first=True)\n\n    def forward(self, string, string_len):\n        """""" process inputs\n\n        Args:\n            string (Tensor): [batch_size, seq_len, dim]\n            string_len (Tensor): [batch_size]\n\n        Returns:\n            Tensor: [batch_size, seq_len, 2 * hidden_dim]\n\n        """"""\n\n        padded_seq_len = string.shape[1]\n        self.init_GRU = torch.FloatTensor(2, string.size(0), self.layer_conf.hidden_dim).zero_()\n        if self.is_cuda():\n            self.init_GRU = transfer_to_gpu(self.init_GRU)\n\n        # Sort by length (keep idx)\n        str_len, idx_sort = (-string_len).sort()\n        str_len = -str_len\n        idx_unsort = idx_sort.sort()[1]\n\n        string = string.index_select(0, idx_sort)\n\n        # Handling padding in Recurrent Networks\n        string_packed = nn.utils.rnn.pack_padded_sequence(string, str_len, batch_first=True)\n        self.GRU.flatten_parameters()\n        string_output, hn = self.GRU(string_packed, self.init_GRU)  # seqlen x batch x 2*nhid\n        string_output = nn.utils.rnn.pad_packed_sequence(string_output, batch_first=True, total_length=padded_seq_len)[0]\n\n        # Un-sort by length\n        string_output = string_output.index_select(0, idx_unsort)\n\n        return string_output, string_len\n\n'"
block_zoo/BiGRULast.py,4,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np\n\nfrom block_zoo.BaseLayer import BaseLayer, BaseConf\nfrom utils.DocInherit import DocInherit\nfrom utils.common_utils import transfer_to_gpu\n\nclass BiGRULastConf(BaseConf):\n    """""" Configuration of the layer BiGRULast\n\n    Args:\n        hidden_dim (int): dimension of hidden state\n        dropout (float): dropout rate\n    """"""\n    def __init__(self, **kwargs):\n\n        super(BiGRULastConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        self.hidden_dim = 128\n        self.dropout = 0.0\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 1\n        self.input_ranks = [3]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = [-1]\n        self.output_dim.append(2 * self.hidden_dim)\n\n        super(BiGRULastConf, self).inference()  # PUT THIS LINE AT THE END OF inference()\n\n    @DocInherit\n    def verify_before_inference(self):\n        super(BiGRULastConf, self).verify_before_inference()\n        necessary_attrs_for_user = [\'hidden_dim\']\n        for attr in necessary_attrs_for_user:\n            self.add_attr_exist_assertion_for_user(attr)\n\n    @DocInherit\n    def verify(self):\n        super(BiGRULastConf, self).verify()\n        necessary_attrs_for_user = [\'hidden_dim\', \'dropout\']\n        for attr in necessary_attrs_for_user:\n            self.add_attr_exist_assertion_for_user(attr)\n\n\nclass BiGRULast(BaseLayer):\n    """""" Get the last hidden state of Bi GRU\n\n    Args:\n        layer_conf (BiGRULastConf): configuration of a layer\n    """"""\n    def __init__(self, layer_conf):\n        super(BiGRULast, self).__init__(layer_conf)\n        self.GRU = nn.GRU(layer_conf.input_dims[0][-1], layer_conf.hidden_dim, 1, bidirectional=True,\n            dropout=layer_conf.dropout, batch_first=True)\n\n    def forward(self, string, string_len):\n        """""" process inputs\n\n        Args:\n            string (Tensor): [batch_size, seq_len, dim]\n            string_len (Tensor): [batch_size]\n\n        Returns:\n            Tensor: [batch_size, 2 * hidden_dim]\n        """"""\n        #string = string.permute([1, 0, 2])\n        self.init_GRU = torch.FloatTensor(2, string.size(0), self.layer_conf.hidden_dim).zero_()\n        if self.is_cuda():\n            self.init_GRU = transfer_to_gpu(self.init_GRU)\n\n        # Sort by length (keep idx)\n        str_len, idx_sort = (-string_len).sort()\n        str_len = -str_len\n        idx_unsort = idx_sort.sort()[1]\n\n        string = string.index_select(0, idx_sort)\n\n        # Handling padding in Recurrent Networks\n        string_packed = nn.utils.rnn.pack_padded_sequence(string, str_len, batch_first=True)\n        self.GRU.flatten_parameters()\n        string_output, hn = self.GRU(string_packed, self.init_GRU)  # seqlen x batch x 2*nhid\n\n        emb = torch.cat((hn[0], hn[1]), 1)  # batch x 2*nhid\n\n        emb = emb.index_select(0, idx_unsort)\n        return emb, string_len\n'"
block_zoo/BiLSTM.py,2,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np\nfrom block_zoo.BaseLayer import BaseLayer, BaseConf\nfrom utils.DocInherit import DocInherit\nimport copy\n\nclass BiLSTMConf(BaseConf):\n    """""" Configuration of BiLSTM\n\n    Args:\n        hidden_dim (int): dimension of hidden state\n        dropout (float): dropout rate\n        num_layers (int): number of BiLSTM layers\n    """"""\n    def __init__(self, **kwargs):\n        super(BiLSTMConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        self.hidden_dim = 128\n        self.dropout = 0.0\n        self.num_layers = 1\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 1\n        self.input_ranks = [3]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = copy.deepcopy(self.input_dims[0])\n        self.output_dim[-1] = 2 * self.hidden_dim\n\n        super(BiLSTMConf, self).inference()      # PUT THIS LINE AT THE END OF inference()\n\n    @DocInherit\n    def verify(self):\n        super(BiLSTMConf, self).verify()\n\n        necessary_attrs_for_user = [\'hidden_dim\', \'dropout\', \'num_layers\']\n        for attr in necessary_attrs_for_user:\n            self.add_attr_exist_assertion_for_user(attr)\n\n\nclass BiLSTM(BaseLayer):\n    """""" Bidrectional LSTM\n\n    Args:\n        layer_conf (BiLSTMConf): configuration of a layer\n    """"""\n    def __init__(self, layer_conf):\n        super(BiLSTM, self).__init__(layer_conf)\n        self.lstm = nn.LSTM(layer_conf.input_dims[0][-1], layer_conf.hidden_dim, layer_conf.num_layers, bidirectional=True,\n            dropout=layer_conf.dropout, batch_first=True)\n\n    def forward(self, string, string_len):\n        """""" process inputs\n\n        Args:\n            string (Tensor): [batch_size, seq_len, dim]\n            string_len (Tensor): [batch_size]\n\n        Returns:\n            Tensor: [batch_size, seq_len, 2 * hidden_dim]\n\n        """"""\n        padded_seq_len = string.shape[1]\n\n        # Sort by length (keep idx)\n        str_len, idx_sort = (-string_len).sort()\n        str_len = -str_len\n        idx_unsort = idx_sort.sort()[1]\n\n        string = string.index_select(0, idx_sort)\n\n        # Handling padding in Recurrent Networks\n        string_packed = nn.utils.rnn.pack_padded_sequence(string, str_len, batch_first=True)\n        self.lstm.flatten_parameters()\n        string_output = self.lstm(string_packed)[0]  # seqlen x batch x 2*nhid\n        string_output = nn.utils.rnn.pad_packed_sequence(string_output, batch_first=True, total_length=padded_seq_len)[0]\n\n        # Un-sort by length\n        string_output = string_output.index_select(0, idx_unsort)\n\n        return string_output, string_len\n'"
block_zoo/BiLSTMAtt.py,3,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport copy\nimport numpy as np\n\nfrom block_zoo.BaseLayer import BaseLayer, BaseConf\nfrom utils.DocInherit import DocInherit\n\n\nclass BiLSTMAttConf(BaseConf):\n    """""" Configuration of BiLSTMAtt layer\n\n    Args:\n        hidden_dim (int): dimension of hidden state\n        dropout (float): dropout rate\n        num_layers (int): number of BiLSTM layers\n    """"""\n    def __init__(self, **kwargs):\n        super(BiLSTMAttConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        self.hidden_dim = 128\n        self.dropout = 0.0\n        self.num_layers = 1\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 1\n        self.input_ranks = [3]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = copy.deepcopy(self.input_dims[0])\n        self.output_dim[-1] = 2 * self.hidden_dim\n\n        self.attention_dim = 2 * self.hidden_dim\n\n        super(BiLSTMAttConf, self).inference()  # PUT THIS LINE AT THE END OF inference()\n\n    @DocInherit\n    def verify_before_inference(self):\n        super(BiLSTMAttConf, self).verify_before_inference()\n        necessary_attrs_for_user = [\'hidden_dim\']\n        for attr in necessary_attrs_for_user:\n            self.add_attr_exist_assertion_for_user(attr)\n\n    @DocInherit\n    def verify(self):\n        super(BiLSTMAttConf, self).verify()\n\n        necessary_attrs_for_user = [\'dropout\', \'attention_dim\']\n        for attr in necessary_attrs_for_user:\n            self.add_attr_exist_assertion_for_user(attr)\n\n\nclass BiLSTMAtt(BaseLayer):\n    """""" BiLSTM with self attention\n\n    Args:\n        layer_conf (BiLSTMAttConf): configuration of a layer\n    """"""\n    def __init__(self, layer_conf):\n        super(BiLSTMAtt, self).__init__(layer_conf)\n        self.lstm = nn.LSTM(layer_conf.input_dims[0][-1], layer_conf.hidden_dim, layer_conf.num_layers, bidirectional=True,\n            dropout=layer_conf.dropout, batch_first=True)\n        self.att = nn.Parameter(torch.randn(layer_conf.attention_dim, layer_conf.attention_dim), requires_grad=True)\n        nn.init.uniform_(self.att, a=0, b=1)\n        self.softmax = nn.Softmax()\n\n    def forward(self, string, string_len):\n        """""" process inputs\n\n        Args:\n            string (Tensor): [batch_size, seq_len, dim]\n            string_len (Tensor): [batch_size]\n\n        Returns:\n            Tensor: [batch_size, seq_len, 2 * hidden_dim]\n\n        """"""\n        padded_seq_len = string.shape[1]\n\n        # Sort by length (keep idx)\n        string_len_sorted, idx_sort = (-string_len).sort()\n        string_len_sorted = -string_len_sorted\n        idx_unsort = idx_sort.sort()[1]\n\n        bsize = string.shape[0]\n\n        string = string.index_select(0, idx_sort)\n\n        # Handling padding in Recurrent Networks\n        string_packed = nn.utils.rnn.pack_padded_sequence(string, string_len_sorted, batch_first=True)\n        self.lstm.flatten_parameters()\n        string_output = self.lstm(string_packed)[0]  # seqlen x batch x 2*nhid\n        string_output = nn.utils.rnn.pad_packed_sequence(string_output, batch_first=True, total_length=padded_seq_len)[0]\n\n        # Un-sort by length\n        string_output = string_output.index_select(0, idx_unsort).contiguous()   # [batch, seqlen, 2*nhid]\n\n        # Self Attention\n        alphas = string_output.matmul(self.att).bmm(string_output.transpose(1, 2).contiguous())  # [batch, seqlen, seqlen]\n\n        # Set probas of padding to zero in softmax\n        alphas = alphas + ((alphas == 0).float() * -10000)\n\n        # softmax\n        alphas = self.softmax(alphas.view(-1, int(padded_seq_len)))  # [batch*seglen, seqlen]\n\n        alphas = alphas.view(bsize, -1, int(padded_seq_len))  # [batch, seglen, seq_len]\n\n        string_output = alphas.bmm(string_output)  # [batch, seglen, 2*nhid]\n\n        return string_output, string_len\n'"
block_zoo/BiLSTMLast.py,3,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np\nfrom block_zoo.BaseLayer import BaseLayer, BaseConf\nfrom utils.DocInherit import DocInherit\n\nclass BiLSTMLastConf(BaseConf):\n    """""" Configuration of BiLSTMLast\n\n    Args:\n        hidden_dim (int): dimension of hidden state\n        dropout (float): dropout rate\n        num_layers (int): number of BiLSTM layers\n    """"""\n    def __init__(self, **kwargs):\n        super(BiLSTMLastConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        self.hidden_dim = 128\n        self.dropout = 0.0\n        self.num_layers = 1\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 1\n        self.input_ranks = [3]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = [-1]\n        self.output_dim.append(2 * self.hidden_dim)\n\n        super(BiLSTMLastConf, self).inference()  # PUT THIS LINE AT THE END OF inference()\n\n    @DocInherit\n    def verify(self):\n        super(BiLSTMLastConf, self).verify()\n\n        necessary_attrs_for_user = [\'hidden_dim\', \'dropout\', \'num_layers\']\n        for attr in necessary_attrs_for_user:\n            self.add_attr_exist_assertion_for_user(attr)\n\n\nclass BiLSTMLast(BaseLayer):\n    """""" get last hidden states of Bidrectional LSTM\n\n    Args:\n        layer_conf (BiLSTMConf): configuration of a layer\n    """"""\n    def __init__(self, layer_conf):\n        super(BiLSTMLast, self).__init__(layer_conf)\n        self.lstm = nn.LSTM(layer_conf.input_dims[0][-1], layer_conf.hidden_dim, layer_conf.num_layers, bidirectional=True,\n            dropout=layer_conf.dropout, batch_first=True)\n\n    def forward(self, string, string_len):\n        """""" process inputs\n\n        Args:\n            string (Tensor): [batch_size, seq_len, dim]\n            string_len (Tensor): [batch_size]\n\n        Returns:\n            Tensor: [batch_size, 2 * hidden_dim]\n\n        """"""\n        str_len, idx_sort = (-string_len).sort()\n        str_len = -str_len\n        idx_unsort = idx_sort.sort()[1]\n\n        string = string.index_select(0, idx_sort)\n\n        # Handling padding in Recurrent Networks\n        string_packed = nn.utils.rnn.pack_padded_sequence(string, str_len, batch_first=True)\n        self.lstm.flatten_parameters()\n        string_output, (hn, cn) = self.lstm(string_packed)  # seqlen x batch x 2*nhid\n\n        emb = torch.cat((hn[0], hn[1]), 1)  # batch x 2*nhid\n        emb = emb.index_select(0, idx_unsort)\n\n        return emb, string_len\n'"
block_zoo/BiQRNN.py,18,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\n\nimport numpy as np\nfrom block_zoo.BaseLayer import BaseLayer, BaseConf\nfrom utils.DocInherit import DocInherit\nimport copy\n\n\nclass ForgetMult(torch.nn.Module):\n    """"""ForgetMult computes a simple recurrent equation:\n    h_t = f_t * x_t + (1 - f_t) * h_{t-1}\n\n    This equation is equivalent to dynamic weighted averaging.\n\n    Inputs: X, hidden\n        - X (seq_len, batch, input_size): tensor containing the features of the input sequence.\n        - F (seq_len, batch, input_size): tensor containing the forget gate values, assumed in range [0, 1].\n        - hidden_init (batch, input_size): tensor containing the initial hidden state for the recurrence (h_{t-1}).\n    """"""\n\n    def __init__(self):\n        super(ForgetMult, self).__init__()\n\n    def forward(self, f, x, hidden_init=None):\n        result = []\n        forgets = f.split(1, dim=0)\n        prev_h = hidden_init\n        for i, h in enumerate((f * x).split(1, dim=0)):\n            if prev_h is not None: h = h + (1 - forgets[i]) * prev_h\n            # h is (1, batch, hidden) when it needs to be (batch_hidden)\n            # Calling squeeze will result in badness if batch size is 1\n            h = h.view(h.size()[1:])\n            result.append(h)\n            prev_h = h\n        return torch.stack(result)\n\n\nclass QRNNLayer(nn.Module):\n    """"""Applies a single layer Quasi-Recurrent Neural Network (QRNN) to an input sequence.\n\n    Args:\n        input_size: The number of expected features in the input x.\n        hidden_size: The number of features in the hidden state h. If not specified, the input size is used.\n        save_prev_x: Whether to store previous inputs for use in future convolutional windows (i.e. for a continuing sequence such as in language modeling). If true, you must call reset to remove cached previous values of x. Default: False.\n        window: Defines the size of the convolutional window (how many previous tokens to look when computing the QRNN values). Supports 1 and 2. Default: 1.\n        zoneout: Whether to apply zoneout (i.e. failing to update elements in the hidden state) to the hidden state updates. Default: 0.\n        output_gate: If True, performs QRNN-fo (applying an output gate to the output). If False, performs QRNN-f. Default: True.\n\n    Inputs: X, hidden\n        - X (seq_len, batch, input_size): tensor containing the features of the input sequence.\n        - hidden (batch, hidden_size): tensor containing the initial hidden state for the QRNN.\n\n    Outputs: output, h_n\n        - output (seq_len, batch, hidden_size): tensor containing the output of the QRNN for each timestep.\n        - h_n (1, batch, hidden_size): tensor containing the hidden state for t=seq_len\n    """"""\n\n    def __init__(self, input_size, hidden_size=None, save_prev_x=False, zoneout=0, window=1, output_gate=True):\n        super(QRNNLayer, self).__init__()\n\n        assert window in [1, 2], ""This QRNN implementation currently only handles convolutional window of size 1 or size 2""\n        self.window = window\n        self.input_size = input_size\n        self.hidden_size = hidden_size if hidden_size else input_size\n        self.zoneout = zoneout\n        self.save_prev_x = save_prev_x\n        self.prevX = None\n        self.output_gate = output_gate\n\n        # One large matmul with concat is faster than N small matmuls and no concat\n        self.linear = nn.Linear(self.window * self.input_size, 3 * self.hidden_size if self.output_gate else 2 * self.hidden_size)\n\n    def reset(self):\n        # If you are saving the previous value of x, you should call this when starting with a new state\n        self.prevX = None\n\n    def forward(self, X, hidden=None):\n        seq_len, batch_size, _ = X.size()\n\n        source = None\n        if self.window == 1:\n            source = X\n        elif self.window == 2:\n            # Construct the x_{t-1} tensor with optional x_{-1}, otherwise a zeroed out value for x_{-1}\n            Xm1 = []\n            Xm1.append(self.prevX if self.prevX is not None else X[:1, :, :] * 0)\n            # Note: in case of len(X) == 1, X[:-1, :, :] results in slicing of empty tensor == bad\n            if len(X) > 1:\n                Xm1.append(X[:-1, :, :])\n            Xm1 = torch.cat(Xm1, 0)\n            # Convert two (seq_len, batch_size, hidden) tensors to (seq_len, batch_size, 2 * hidden)\n            source = torch.cat([X, Xm1], 2)\n\n        # Matrix multiplication for the three outputs: Z, F, O\n        Y = self.linear(source)\n        # Convert the tensor back to (batch, seq_len, len([Z, F, O]) * hidden_size)\n        if self.output_gate:\n            Y = Y.view(seq_len, batch_size, 3 * self.hidden_size)\n            Z, F, O = Y.chunk(3, dim=2)\n        else:\n            Y = Y.view(seq_len, batch_size, 2 * self.hidden_size)\n            Z, F = Y.chunk(2, dim=2)\n        ###\n        Z = torch.tanh(Z)\n        F = torch.sigmoid(F)\n\n        # If zoneout is specified, we perform dropout on the forget gates in F\n        # If an element of F is zero, that means the corresponding neuron keeps the old value\n        if self.zoneout:\n            if self.training:\n                # mask = Variable(F.data.new(*F.size()).bernoulli_(1 - self.zoneout), requires_grad=False)\n                mask = F.new_empty(F.size(), requires_grad=False).bernoulli_(1 - self.zoneout)\n                F = F * mask\n            else:\n                F *= 1 - self.zoneout\n\n        # Forget Mult\n        C = ForgetMult()(F, Z, hidden)\n\n        # Apply (potentially optional) output gate\n        if self.output_gate:\n            H = torch.sigmoid(O) * C\n        else:\n            H = C\n\n        # In an optimal world we may want to backprop to x_{t-1} but ...\n        if self.window > 1 and self.save_prev_x:\n            # self.prevX = Variable(X[-1:, :, :].data, requires_grad=False)\n            self.prevX = X[-1:, :, :].detach()\n\n        return H, C[-1:, :, :]\n\n\nclass QRNN(torch.nn.Module):\n    """"""Applies a multiple layer Quasi-Recurrent Neural Network (QRNN) to an input sequence.\n\n    Args:\n        input_size: The number of expected features in the input x.\n        hidden_size: The number of features in the hidden state h. If not specified, the input size is used.\n        num_layers: The number of QRNN layers to produce.\n        dropout: Whether to use dropout between QRNN layers. Default: 0.\n        bidirectional: If True, becomes a bidirectional QRNN. Default: False.\n        save_prev_x: Whether to store previous inputs for use in future convolutional windows (i.e. for a continuing sequence such as in language modeling). If true, you must call reset to remove cached previous values of x. Default: False.\n        window: Defines the size of the convolutional window (how many previous tokens to look when computing the QRNN values). Supports 1 and 2. Default: 1.\n        zoneout: Whether to apply zoneout (i.e. failing to update elements in the hidden state) to the hidden state updates. Default: 0.\n        output_gate: If True, performs QRNN-fo (applying an output gate to the output). If False, performs QRNN-f. Default: True.\n\n    Inputs: X, hidden\n        - X (seq_len, batch, input_size): tensor containing the features of the input sequence.\n        - hidden (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for the QRNN.\n\n    Outputs: output, h_n\n        - output (seq_len, batch, hidden_size * num_directions): tensor containing the output of the QRNN for each timestep.\n        - h_n (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t=seq_len\n    """"""\n\n    def __init__(self, input_size, hidden_size,\n                 num_layers=1, bias=True, batch_first=False,\n                 dropout=0.0, bidirectional=False, **kwargs):\n        # assert bidirectional == False, \'Bidirectional QRNN is not yet supported\'\n        assert batch_first == False, \'Batch first mode is not yet supported\'\n        assert bias == True, \'Removing underlying bias is not yet supported\'\n\n        super(QRNN, self).__init__()\n\n        # self.layers = torch.nn.ModuleList(layers if layers else [QRNNLayer(input_size if l == 0 else hidden_size, hidden_size, **kwargs) for l in range(num_layers)])\n        if bidirectional:\n            self.layers = torch.nn.ModuleList(\n                [QRNNLayer(input_size if l < 2 else hidden_size * 2, hidden_size, **kwargs) for l in\n                 range(num_layers * 2)])\n        else:\n            self.layers = torch.nn.ModuleList(\n                [QRNNLayer(input_size if l == 0 else hidden_size, hidden_size, **kwargs) for l in\n                 range(num_layers)])\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.bias = bias\n        self.batch_first = batch_first\n        self.dropout = dropout\n        self.bidirectional = bidirectional\n        self.num_directions = 2 if bidirectional else 1\n        assert len(self.layers) == self.num_layers * self.num_directions\n\n    def tensor_reverse(self, tensor):\n        # idx = [i for i in range(tensor.size(0) - 1, -1, -1)]\n        # idx = torch.LongTensor(idx)\n        # inverted_tensor = tensor.index_select(0, idx)\n        return tensor.flip(0)\n\n    def reset(self):\n        r\'\'\'If your convolutional window is greater than 1, you must reset at the beginning of each new sequence\'\'\'\n        [layer.reset() for layer in self.layers]\n\n    def forward(self, input, hidden=None):\n        next_hidden = []\n        for i in range(self.num_layers):\n            all_output = []\n            for j in range(self.num_directions):\n                l = i * self.num_directions + j\n                layer = self.layers[l]\n                if j == 1:\n                    input = self.tensor_reverse(input)  # reverse\n                output, hn = layer(input, None if hidden is None else hidden[l])\n                next_hidden.append(hn)\n                if j == 1:\n                    output = self.tensor_reverse(output)    # reverse\n                all_output.append(output)\n\n            input = torch.cat(all_output, input.dim() - 1)\n            if self.dropout != 0 and i < self.num_layers - 1:\n                input = torch.nn.functional.dropout(input, p=self.dropout, training=self.training, inplace=False)\n\n        next_hidden = torch.cat(next_hidden, 0).view(self.num_layers * self.num_directions, *next_hidden[0].size()[-2:])\n\n        # for i, layer in enumerate(self.layers):\n        #     input, hn = layer(input, None if hidden is None else hidden[i])\n        #     next_hidden.append(hn)\n        #\n        #     if self.dropout != 0 and i < len(self.layers) - 1:\n        #         input = torch.nn.functional.dropout(input, p=self.dropout, training=self.training, inplace=False)\n        #\n        # next_hidden = torch.cat(next_hidden, 0).view(self.num_layers, *next_hidden[0].size()[-2:])\n\n        return input, next_hidden\n\n\nclass BiQRNNConf(BaseConf):\n    """""" Configuration of BiQRNN\n\n    Args:\n        hidden_dim (int): dimension of hidden state\n        window: the size of the convolutional window. Supports 1 and 2. Default: 1\n        zoneout: Whether to apply zoneout (failing to update elements in the hidden state). Default: 0\n        dropout (float): dropout rate bewteen BiQRNN layers\n        num_layers (int): number of BiQRNN layers\n    """"""\n    def __init__(self, **kwargs):\n        super(BiQRNNConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        self.hidden_dim = 128\n        self.window = 1\n        self.zoneout = 0.0\n        self.dropout = 0.0\n        self.num_layers = 1\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 1\n        self.input_ranks = [3]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = copy.deepcopy(self.input_dims[0])\n        self.output_dim[-1] = 2 * self.hidden_dim\n\n        super(BiQRNNConf, self).inference()      # PUT THIS LINE AT THE END OF inference()\n\n    @DocInherit\n    def verify(self):\n        super(BiQRNNConf, self).verify()\n\n        necessary_attrs_for_user = [\'hidden_dim\', \'window\', \'zoneout\', \'dropout\', \'num_layers\']\n        for attr in necessary_attrs_for_user:\n            self.add_attr_exist_assertion_for_user(attr)\n\n\nclass BiQRNN(BaseLayer):\n    """""" Bidrectional QRNN\n\n    Args:\n        layer_conf (BiQRNNConf): configuration of a layer\n    """"""\n    def __init__(self, layer_conf):\n        super(BiQRNN, self).__init__(layer_conf)\n        self.qrnn = QRNN(layer_conf.input_dims[0][-1], layer_conf.hidden_dim, layer_conf.num_layers,\n                         window=layer_conf.window, zoneout=layer_conf.zoneout, dropout=layer_conf.dropout,\n                         bidirectional=True)\n\n    def forward(self, string, string_len):\n        """""" process inputs\n\n        Args:\n            string (Tensor): [batch_size, seq_len, dim]\n            string_len (Tensor): [batch_size]\n\n        Returns:\n            Tensor: [batch_size, seq_len, 2 * hidden_dim]\n\n        """"""\n        string = string.transpose(0, 1)\n        string_output = self.qrnn(string)[0]\n        string_output = string_output.transpose(0, 1)\n\n        return string_output, string_len\n'"
block_zoo/CRF.py,18,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nfrom block_zoo.BaseLayer import BaseLayer, BaseConf\nfrom utils.DocInherit import DocInherit\nimport torch\nimport torch.nn as nn\nfrom copy import deepcopy\nimport torch.autograd as autograd\n\n\ndef argmax(vec):\n    # return the argmax as a python int\n    _, idx = torch.max(vec, 1)\n    return idx.item()\n\n\ndef log_sum_exp(vec, m_size):\n    """"""\n    calculate log of exp sum\n    args:\n        vec (batch_size, vanishing_dim, hidden_dim) : input tensor\n        m_size : hidden_dim\n    return:\n        batch_size, hidden_dim\n    """"""\n    _, idx = torch.max(vec, 1)  # B * 1 * M\n    max_score = torch.gather(vec, 1, idx.view(-1, 1, m_size)).view(-1, 1, m_size)  # B * M\n    return max_score.view(-1, m_size) + torch.log(torch.sum(torch.exp(vec - max_score.expand_as(vec)), 1)).view(-1, m_size)  # B * M\n\n\nclass CRFConf(BaseConf):\n    """"""\n    Configuration of CRF layer\n\n    Args:\n\n    """"""\n    def __init__(self, **kwargs):\n        super(CRFConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        self.START_TAG = ""<start>""\n        self.STOP_TAG = ""<eos>""\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 1\n        self.input_ranks = [3]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = [1]\n        # add target dict judgement start or end\n        self.target_dict = deepcopy(self.target_dict.cell_id_map)\n        if not self.target_dict.get(self.START_TAG):\n            self.target_dict[self.START_TAG] = len(self.target_dict)\n        if not self.target_dict.get(self.STOP_TAG):\n            self.target_dict[self.STOP_TAG] = len(self.target_dict)\n\n        super(CRFConf, self).inference()\n\n    @DocInherit\n    def verify(self):\n        super(CRFConf, self).verify()\n\n\nclass CRF(BaseLayer):\n    """""" Conditional Random Field layer\n\n    Args:\n        layer_conf(CRFConf): configuration of CRF layer\n    """"""\n    def __init__(self, layer_conf):\n        super(CRF, self).__init__(layer_conf)\n        self.target_size = len(self.layer_conf.target_dict)\n\n        init_transitions = torch.zeros(self.target_size, self.target_size)\n        init_transitions[:, self.layer_conf.target_dict[self.layer_conf.START_TAG]] = -10000.0\n        init_transitions[self.layer_conf.target_dict[self.layer_conf.STOP_TAG], :] = -10000.0\n        init_transitions[:, 0] = -10000.0\n        init_transitions[0, :] = -10000.0\n\n        if self.layer_conf.use_gpu:\n            init_transitions = init_transitions.cuda()\n        self.transitions = nn.Parameter(init_transitions)\n\n    def _calculate_forward(self, feats, mask):\n        """"""\n            input:\n                feats: (batch, seq_len, self.tag_size)\n                masks: (batch, seq_len)\n        """"""\n        batch_size = feats.size(0)\n        seq_len = feats.size(1)\n        tag_size = feats.size(2)\n\n        mask = mask.transpose(1, 0).contiguous()\n        ins_num = seq_len * batch_size\n        # be careful the view shape, it is .view(ins_num, 1, tag_size) but not .view(ins_num, tag_size, 1)\n        feats = feats.transpose(1, 0).contiguous().view(ins_num, 1, tag_size).expand(ins_num, tag_size, tag_size)\n        # need to consider start\n        scores = feats + self.transitions.view(1, tag_size, tag_size).expand(ins_num, tag_size, tag_size)\n        scores = scores.view(seq_len, batch_size, tag_size, tag_size)\n        # build iter\n        seq_iter = enumerate(scores)\n        _, inivalues = next(seq_iter)  # bat_size * from_target_size * to_target_size\n        # only need start from start_tag\n        partition = inivalues[:, self.layer_conf.target_dict[self.layer_conf.START_TAG], :].clone().view(batch_size, tag_size, 1)  # bat_size * to_target_size\n\n        for idx, cur_values in seq_iter:\n            # previous to_target is current from_target\n            # partition: previous results log(exp(from_target)), #(batch_size * from_target)\n            # cur_values: bat_size * from_target * to_target\n\n            cur_values = cur_values + partition.contiguous().view(batch_size, tag_size, 1).expand(batch_size, tag_size, tag_size)\n            cur_partition = log_sum_exp(cur_values, tag_size)\n\n            # (bat_size * from_target * to_target) -> (bat_size * to_target)\n            # partition = utils.switch(partition, cur_partition, mask[idx].view(bat_size, 1).expand(bat_size, self.tagset_size)).view(bat_size, -1)\n            mask_idx = mask[idx, :].view(batch_size, 1).expand(batch_size, tag_size)\n\n            # effective updated partition part, only keep the partition value of mask value = 1\n            masked_cur_partition = cur_partition.masked_select(mask_idx)\n            # let mask_idx broadcastable, to disable warning\n            mask_idx = mask_idx.contiguous().view(batch_size, tag_size, 1)\n\n            # replace the partition where the maskvalue=1, other partition value keeps the same\n            partition.masked_scatter_(mask_idx, masked_cur_partition)\n        # until the last state, add transition score for all partition (and do log_sum_exp) then select the value in STOP_TAG\n        cur_values = self.transitions.view(1, tag_size, tag_size).expand(batch_size, tag_size, tag_size) + partition.contiguous().view(batch_size, tag_size, 1).expand(batch_size, tag_size, tag_size)\n        cur_partition = log_sum_exp(cur_values, tag_size)\n        final_partition = cur_partition[:, self.layer_conf.target_dict[self.layer_conf.STOP_TAG]]\n        return final_partition.sum(), scores\n\n    def _viterbi_decode(self, feats, mask):\n        """"""\n            input:\n                feats: (batch, seq_len, self.tag_size)\n                mask: (batch, seq_len)\n            output:\n                decode_idx: (batch, seq_len) decoded sequence\n                path_score: (batch, 1) corresponding score for each sequence\n        """"""\n        batch_size = feats.size(0)\n        seq_len = feats.size(1)\n        tag_size = feats.size(2)\n\n        # calculate sentence length for each sentence\n        length_mask = torch.sum(mask.long(), dim=1).view(batch_size, 1).long()\n        # mask to (seq_len, batch_size)\n        mask = mask.transpose(1, 0).contiguous()\n        ins_num = seq_len * batch_size\n        # be careful the view shape, it is .view(ins_num, 1, tag_size) but not .view(ins_num, tag_size, 1)\n        feats = feats.transpose(1, 0).contiguous().view(ins_num, 1, tag_size).expand(ins_num, tag_size, tag_size)\n        # need to consider start\n        scores = feats + self.transitions.view(1, tag_size, tag_size).expand(ins_num, tag_size, tag_size)\n        scores = scores.view(seq_len, batch_size, tag_size, tag_size)\n\n        # build iter\n        seq_iter = enumerate(scores)\n        # record the position of best score\n        back_points = list()\n        partition_history = list()\n        #  reverse mask (bug for mask = 1- mask, use this as alternative choice)\n        mask = (1 - mask.long()).byte()\n        _, inivalues = next(seq_iter)  # bat_size * from_target_size * to_target_size\n        # only need start from start_tag\n        partition = inivalues[:, self.layer_conf.target_dict[self.layer_conf.START_TAG], :].clone().view(batch_size, tag_size)  # bat_size * to_target_size\n        # print ""init part:"",partition.size()\n        partition_history.append(partition)\n        # iter over last scores\n        for idx, cur_values in seq_iter:\n            # previous to_target is current from_target\n            # partition: previous results log(exp(from_target)), #(batch_size * from_target)\n            # cur_values: batch_size * from_target * to_target\n            cur_values = cur_values + partition.contiguous().view(batch_size, tag_size, 1).expand(batch_size, tag_size, tag_size)\n            partition, cur_bp = torch.max(cur_values, 1)\n            partition_history.append(partition)\n            # cur_bp: (batch_size, tag_size) max source score position in current tag\n            # set padded label as 0, which will be filtered in post processing\n            cur_bp.masked_fill_(mask[idx].view(batch_size, 1).expand(batch_size, tag_size), 0)\n            back_points.append(cur_bp)\n        # add score to final STOP_TAG\n        partition_history = torch.cat(partition_history, 0).view(seq_len, batch_size, -1).transpose(1, 0).contiguous() # (batch_size, seq_len. tag_size)\n        # get the last position for each setences, and select the last partitions using gather()\n        last_position = length_mask.view(batch_size, 1, 1).expand(batch_size, 1, tag_size) - 1\n        last_partition = torch.gather(partition_history, 1, last_position).view(batch_size,tag_size,1)\n        # calculate the score from last partition to end state (and then select the STOP_TAG from it)\n        last_values = last_partition.expand(batch_size, tag_size, tag_size) + self.transitions.view(1, tag_size, tag_size).expand(batch_size, tag_size, tag_size)\n        _, last_bp = torch.max(last_values, 1)\n        pad_zero = autograd.Variable(torch.zeros(batch_size, tag_size)).long()\n        if self.layer_conf.use_gpu:\n            pad_zero = pad_zero.cuda()\n        back_points.append(pad_zero)\n        back_points = torch.cat(back_points).view(seq_len, batch_size, tag_size)\n\n        # select end ids in STOP_TAG\n        pointer = last_bp[:, self.layer_conf.target_dict[self.layer_conf.STOP_TAG]]\n        insert_last = pointer.contiguous().view(batch_size, 1, 1).expand(batch_size, 1, tag_size)\n        back_points = back_points.transpose(1, 0).contiguous()\n        # move the end ids(expand to tag_size) to the corresponding position of back_points to replace the 0 values\n        back_points.scatter_(1, last_position, insert_last)\n        back_points = back_points.transpose(1, 0).contiguous()\n        # decode from the end, padded position ids are 0, which will be filtered if following evaluation\n        decode_idx = autograd.Variable(torch.LongTensor(seq_len, batch_size))\n        if self.layer_conf.use_gpu:\n            decode_idx = decode_idx.cuda()\n        decode_idx[-1] = pointer.detach()\n        for idx in range(len(back_points)-2, -1, -1):\n            pointer = torch.gather(back_points[idx], 1, pointer.contiguous().view(batch_size, 1))\n            decode_idx[idx] = pointer.detach().view(batch_size)\n        path_score = None\n        decode_idx = decode_idx.transpose(1, 0)\n        return path_score, decode_idx\n\n    def forward(self, string, string_len):\n        """"""\n        CRF layer process: include use transition matrix compute score and  viterbi decode\n\n        Args:\n            string(Tensor): [batch_size, seq_len, target_num]\n            string_len(Tensor): [batch_size]\n\n        Returns:\n            score: the score by CRF inference\n            best_path: the best bath of viterbi decode\n        """"""\n        assert string_len is not None, ""CRF layer need string length for mask.""\n        masks = []\n        string_len_val = string_len.cpu().data.numpy()\n        for i in range(len(string_len)):\n            masks.append(\n                torch.cat([torch.ones(string_len_val[i]), torch.zeros(string.shape[1] - string_len_val[i])]))\n        masks = torch.stack(masks).view(string.shape[0], string.shape[1]).byte()\n        if self.layer_conf.use_gpu:\n            masks = masks.cuda()\n\n        forward_score, scores = self._calculate_forward(string, masks)\n\n        _, tag_seq = self._viterbi_decode(string, masks)\n\n        return (forward_score, scores, masks, tag_seq, self.transitions, self.layer_conf), string_len\n'"
block_zoo/Conv.py,5,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import OrderedDict\n\nfrom block_zoo.BaseLayer import BaseLayer, BaseConf\nfrom utils.DocInherit import DocInherit\n\n\nclass ConvConf(BaseConf):\n    """""" Configuration of Conv\n\n    Args:\n        stride (int): the stride of the convolving kernel. Can be a single number or a tuple (sH, sW). Default: 1\n        padding (int): implicit zero paddings on both sides of the input. Can be a single number or a tuple (padH, padW). Default: 0\n        window_size (int): actually, the window size is (window_size, feature_dim), because for NLP tasks, 1d convolution is more commonly used.\n        input_channel_num (int): for NLP tasks, input_channel_num would always be 1\n        output_channel_num (int): number of feature maps\n        batch_norm (bool): If True, apply batch normalization before activation\n        activation (string): activation functions, e.g. ReLU\n\n    """"""\n    def __int__(self, **kwargs):\n        super(ConvConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        self.stride = 1\n        self.padding = 0\n        self.window_size = 3\n        self.input_channel_num = 1      # for NLP tasks, input_channel_num would always be 1\n        self.output_channel_num = 16\n        self.batch_norm = True\n        self.activation = \'ReLU\'\n        self.padding_type = \'VALID\'\n        self.dropout = 0\n        self.remind_lengths = True\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 1\n        self.input_ranks = [3]\n\n    @DocInherit\n    def inference(self):\n\n        if self.padding_type == \'SAME\':\n            self.padding = int((self.window_size-1)/2)\n\n        self.output_dim = [-1]\n        if self.input_dims[0][1] != -1:\n            if self.padding_type == \'SAME\':\n                self.output_dim.append(self.input_dims[0][1])\n            else:\n                self.output_dim.append((self.input_dims[0][1] - self.window_size) // self.stride + 1)\n        else:\n            self.output_dim.append(-1)\n        self.output_dim.append(self.output_channel_num)\n\n        super(ConvConf, self).inference()  # PUT THIS LINE AT THE END OF inference()\n\n    @DocInherit\n    def verify_before_inference(self):\n        super(ConvConf, self).verify_before_inference()\n        necessary_attrs_for_user = [\'output_channel_num\']\n        for attr in necessary_attrs_for_user:\n            self.add_attr_exist_assertion_for_user(attr)\n\n    @DocInherit\n    def verify(self):\n        super(ConvConf, self).verify()\n\n        necessary_attrs_for_user = [\'stride\', \'padding\', \'window_size\', \'input_channel_num\', \'output_channel_num\', \'activation\']\n        for attr in necessary_attrs_for_user:\n            self.add_attr_exist_assertion_for_user(attr)\n\n    @DocInherit\n    def verify_former_block(self, former_conf):\n        if \'conv\' in str(type(former_conf)).lower():\n            self.mask = False\n        else:\n            self.mask = True\n\n\nclass Conv(BaseLayer):\n    """""" Convolution along just 1 direction\n\n    Args:\n        layer_conf (ConvConf): configuration of a layer\n    """"""\n    def __init__(self, layer_conf):\n        super(Conv, self).__init__(layer_conf)\n        self.layer_conf = layer_conf\n        if layer_conf.activation:\n            self.activation = eval(""nn."" + self.layer_conf.activation)()\n        else:\n            self.activation = None\n\n        self.conv = nn.Conv1d(layer_conf.input_dims[0][-1], layer_conf.output_channel_num, kernel_size=layer_conf.window_size, padding=layer_conf.padding)\n\n        if layer_conf.batch_norm:\n            # self.batch_norm = nn.BatchNorm2d(layer_conf.output_channel_num)    # the output_chanel of Conv is the input_channel of BN\n            self.batch_norm = nn.BatchNorm1d(layer_conf.output_channel_num)\n        else:\n            self.batch_norm = None\n\n        if layer_conf.dropout > 0:\n            self.cov_dropout = nn.Dropout(layer_conf.dropout)\n        else:\n            self.cov_dropout = None\n\n        if layer_conf.use_gpu:\n            self.conv = self.conv.cuda()\n            if self.batch_norm:\n                self.batch_norm = self.batch_norm.cuda()\n            if self.cov_dropout:\n                self.cov_dropout = self.cov_dropout.cuda()\n            if self.activation:\n                self.activation = self.activation.cuda()\n\n    def forward(self, string, string_len):\n        """""" process inputs\n\n        Args:\n            string (Tensor): tensor with shape: [batch_size, seq_len, feature_dim]\n            string_len (Tensor):  [batch_size]\n\n        Returns:\n            Tensor: shape: [batch_size, (seq_len - conv_window_size) // stride + 1, output_channel_num]\n\n        """"""\n        if string_len is not None and self.layer_conf.mask:\n            string_len_val = string_len.cpu().data.numpy()\n            masks = []\n            for i in range(len(string_len)):\n                masks.append(torch.cat([torch.ones(string_len_val[i]), torch.zeros(string.shape[1] - string_len_val[i])]))\n            masks = torch.stack(masks).view(string.shape[0], string.shape[1], 1).expand_as(string)\n            if self.is_cuda():\n                device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n                masks = masks.to(device)\n            string = string * masks\n\n        string_ = string.transpose(2, 1).contiguous()\n        string_out = self.conv(string_)\n\n        if self.activation:\n            string_out = self.activation(string_out)\n\n        if self.cov_dropout:\n            string_out = self.cov_dropout(string_out)\n\n        if self.batch_norm:\n            string_out = self.batch_norm(string_out)\n\n        string_out = string_out.transpose(2, 1).contiguous()\n\n        string_len_out = None\n        if string_len is not None and self.layer_conf.remind_lengths:\n            string_len_out = string_len\n        return string_out, string_len_out\n'"
block_zoo/Conv2D.py,2,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import OrderedDict\n\nfrom block_zoo.BaseLayer import BaseLayer, BaseConf\nfrom utils.DocInherit import DocInherit\n\n\nclass Conv2DConf(BaseConf):\n    """""" Configuration of Conv\n\n    Args:\n        stride (int): the stride of the convolving kernel. Can be a single number or a tuple (sH, sW). Default: 1\n        padding (int): implicit zero paddings on both sides of the input. Can be a single number or a tuple (padH, padW). Default: 0\n        window_size (int): actually, the window size is (window_sizeH, window_sizeW), because for NLP tasks, 1d convolution is more commonly used.\n        output_channel_num (int): number of feature maps\n        batch_norm (bool): If True, apply batch normalization before activation\n        activation (string): activation functions, e.g. ReLU\n\n    """"""\n    def __int__(self, **kwargs):\n        super(Conv2DConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        self.stride = 1\n        self.padding = 0\n        self.window_size = 3\n        self.output_channel_num = 16\n        self.batch_norm = True\n        self.activation = \'ReLU\'\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 1\n        self.input_ranks = [4]\n    \n    def check_size(self, value, attr):\n        res = value\n        if isinstance(value,int):\n            res = [value, value]\n        elif (isinstance(self.window_size, tuple) or isinstance(self.window_size, list)) and len(value)==2:\n            res = list(value)\n        else:\n            raise AttributeError(""The Atrribute `%s\' should be given an integer or a list/tuple with length of 2, instead of %s."" %(attr,str(value)))\n        return res\n\n    @DocInherit\n    def inference(self):\n        self.window_size = self.check_size(self.window_size, ""window_size"")\n        self.stride = self.check_size(self.stride, ""stride"")\n        self.padding = self.check_size(self.padding, ""padding"")\n        \n        self.input_channel_num = self.input_dims[0][-1]\n\n        self.output_dim = [self.input_dims[0][0]]\n        if self.input_dims[0][1] != -1:\n            self.output_dim.append((self.input_dims[0][1] + 2 * self.padding[0] - self.window_size[0]) // self.stride[0] + 1)\n        else:\n            self.output_dim.append(-1)\n        if self.input_dims[0][2] != -1:\n            self.output_dim.append((self.input_dims[0][2] + 2 * self.padding[1] - self.window_size[1]) // self.stride[1] + 1)\n        else:\n            self.output_dim.append(-1)\n        self.output_dim.append(self.output_channel_num)\n\n        super(Conv2DConf, self).inference()  # PUT THIS LINE AT THE END OF inference()\n\n\n    @DocInherit\n    def verify_before_inference(self):\n        super(Conv2DConf, self).verify_before_inference()\n        necessary_attrs_for_user = [\'output_channel_num\']\n        for attr in necessary_attrs_for_user:\n            self.add_attr_exist_assertion_for_user(attr)\n\n    @DocInherit\n    def verify(self):\n        super(Conv2DConf, self).verify()\n\n        necessary_attrs_for_user = [\'stride\', \'padding\', \'window_size\', \'input_channel_num\', \'output_channel_num\', \'activation\']\n        for attr in necessary_attrs_for_user:\n            self.add_attr_exist_assertion_for_user(attr)\n\n\nclass Conv2D(BaseLayer):\n    """""" Convolution along just 1 direction\n\n    Args:\n        layer_conf (ConvConf): configuration of a layer\n    """"""\n    def __init__(self, layer_conf):\n        super(Conv2D, self).__init__(layer_conf)\n        self.layer_conf = layer_conf\n        if layer_conf.activation:\n            self.activation = eval(""nn."" + self.layer_conf.activation)()\n        else:\n            self.activation = None\n        \n        self.cnn = nn.Conv2d(in_channels=layer_conf.input_channel_num, out_channels=layer_conf.output_channel_num,kernel_size=layer_conf.window_size,stride=layer_conf.stride,padding=layer_conf.padding)\n\n        if layer_conf.batch_norm:\n            self.batch_norm = nn.BatchNorm2d(layer_conf.output_channel_num)    # the output_chanel of Conv is the input_channel of BN\n        else:\n            self.batch_norm = None\n\n    def forward(self, string, string_len=None):\n        """""" process inputs\n\n        Args:\n            string (Tensor): tensor with shape: [batch_size, length, width, feature_dim]\n            string_len (Tensor):  [batch_size]\n\n        Returns:\n            Tensor: shape: [batch_size, (length - conv_window_size) // stride + 1, (width - conv_window_size) // stride + 1, output_channel_num]\n\n        """"""\n\n        string = string.permute([0,3,1,2]).contiguous()\n        string_out = self.cnn(string)\n        if hasattr(self, \'batch_norms\') and self.batch_norm:\n            string_out = self.batch_norm(string_out)\n\n        string_out = string_out.permute([0,2,3,1]).contiguous()\n\n        if self.activation:\n            string_out = self.activation(string_out)\n        if string_len is not None:\n            string_len_out = (string_len - self.layer_conf.window_size[0]) // self.layer_conf.stride[0] + 1\n        else:\n            string_len_out = None\n        return string_out, string_len_out\n'"
block_zoo/ConvPooling.py,16,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import OrderedDict\n\nfrom block_zoo.BaseLayer import BaseLayer, BaseConf\nfrom utils.DocInherit import DocInherit\n\n\nclass ConvPoolingConf(BaseConf):\n    """""" Configuration of Conv + Pooling architecture\n\n    Args:\n        stride (int): the stride of the convolving kernel. Can be a single number or a tuple (sH, sW). Default: 1\n        padding (int): implicit zero paddings on both sides of the input. Can be a single number or a tuple (padH, padW). Default: 0\n        window_sizes (list): for each window_size, the actual window size is (window_size, feature_dim), because for NLP tasks, 1d convolution is more commonly used.\n        input_channel_num (int): for NLP tasks, input_channel_num would always be 1\n        output_channel_num (int): number of feature maps\n        batch_norm (bool): If True, apply batch normalization before activation\n        activation (string): activation functions, e.g. ReLU\n\n    """"""\n    def __int__(self, **kwargs):\n        super(ConvPoolingConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        self.stride = 1\n        self.padding = 0\n        self.window_sizes = [1, 2, 3]\n        self.input_channel_num = 1      # for NLP tasks, input_channel_num would always be 1\n        self.output_channel_num = 16\n        self.batch_norm = True\n        self.activation = \'ReLU\'\n        self.pool_type = \'max\'  # Supported: [\'max\', mean\']\n        self.pool_axis = 1\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 1\n        self.input_ranks = [3]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = [-1]\n        self.output_dim.append(self.output_channel_num * len(self.window_sizes))\n\n        super(ConvPoolingConf, self).inference()  # PUT THIS LINE AT THE END OF inference()\n    \n    @DocInherit\n    def verify_before_inference(self):\n        super(ConvPoolingConf, self).verify_before_inference()\n        necessary_attrs_for_user = [\'output_channel_num\']\n        for attr in necessary_attrs_for_user:\n            self.add_attr_exist_assertion_for_user(attr)\n\n    @DocInherit\n    def verify(self):\n        super(ConvPoolingConf, self).verify()\n\n        necessary_attrs_for_user = [\'stride\', \'padding\', \'window_sizes\', \'input_channel_num\', \'output_channel_num\', \'activation\', \'pool_type\', \'pool_axis\']\n        for attr in necessary_attrs_for_user:\n            self.add_attr_exist_assertion_for_user(attr)\n\n\nclass ConvPooling(BaseLayer):\n    """""" Convolution along just 1 direction\n\n    Args:\n        layer_conf (ConvConf): configuration of a layer\n    """"""\n    def __init__(self, layer_conf):\n        super(ConvPooling, self).__init__(layer_conf)\n        self.layer_conf = layer_conf\n\n        self.filters = nn.ParameterList()\n        if layer_conf.batch_norm:\n            self.batch_norms = nn.ModuleList()\n        else:\n            self.batch_norms = None\n\n        #device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n        for i in range(len(layer_conf.window_sizes)):\n            self.filters.append(nn.Parameter(torch.randn(layer_conf.output_channel_num, layer_conf.input_channel_num, layer_conf.window_sizes[i], layer_conf.input_dims[0][2], requires_grad=True).float()))\n            if layer_conf.batch_norm:\n                self.batch_norms.append(nn.BatchNorm2d(layer_conf.output_channel_num))\n\n        if layer_conf.activation:\n            self.activation = eval(""nn."" + self.layer_conf.activation)()\n        else:\n            self.activation = None\n\n    def forward(self, string, string_len=None):\n        """""" process inputs\n\n        Args:\n            string (Tensor): tensor with shape: [batch_size, seq_len, feature_dim]\n            string_len (Tensor):  [batch_size]\n\n        Returns:\n            Tensor: shape: [batch_size, (seq_len - conv_window_size) // stride + 1, output_channel_num]\n\n        """"""\n        if string_len is not None:\n            string_len_val = string_len.cpu().data.numpy()\n            masks = []\n            for i in range(len(string_len)):\n                masks.append(torch.cat([torch.ones(string_len_val[i]), torch.zeros(string.shape[1] - string_len_val[i])]))\n            masks = torch.stack(masks).view(string.shape[0], string.shape[1], 1).expand_as(string)\n            if self.is_cuda():\n                device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n                masks = masks.to(device)\n            string = string * masks\n\n        string = torch.unsqueeze(string, 1)     # [batch_size, input_channel_num=1, seq_len, feature_dim]\n\n        outputs = []\n        for idx, (filter, window_size) in enumerate(zip(self.filters, self.layer_conf.window_sizes)):\n            string_out = F.conv2d(string, filter, stride=self.layer_conf.stride, padding=self.layer_conf.padding)\n            if hasattr(self, \'batch_norms\') and self.batch_norms:     #hasattr(self, \'batch_norms\') enable NB to be compatible to models trained previously\n                string_out = self.batch_norms[idx](string_out)\n            string_out = torch.squeeze(string_out, 3).permute(0, 2, 1)\n            if self.activation:\n                string_out = self.activation(string_out)\n\n            if string_len is not None:\n                string_len_out = (string_len - window_size) // self.layer_conf.stride + 1\n            else:\n                string_len_out = None\n\n            if self.layer_conf.pool_type == ""mean"":\n                assert not string_len_out is None, ""Parameter string_len should not be None!""\n                string_out = torch.sum(string_out, self.layer_conf.pool_axis).squeeze(self.layer_conf.pool_axis)\n                #if not isinstance(string_len_out, torch.Tensor):\n                if not torch.is_tensor(string_len_out):\n                    string_len_out = torch.FloatTensor(string_len_out)\n                string_len_out = string_len_out.unsqueeze(1)\n                if self.is_cuda():\n                    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n                    string_len_out = string_len_out.to(device)\n                output = string_out / string_len_out.expand_as(string_out)\n            elif self.layer_conf.pool_type == ""max"":\n                output = torch.max(string_out, self.layer_conf.pool_axis)[0]\n\n            outputs.append(output)\n\n        if len(outputs) > 1:\n            string_output = torch.cat(outputs, 1)\n        else:\n            string_output = outputs[0]\n\n        return string_output, None\n\n\n\n'"
block_zoo/Dropout.py,2,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import OrderedDict\nimport copy\n\nfrom block_zoo.BaseLayer import BaseLayer, BaseConf\nfrom utils.DocInherit import DocInherit\n\nclass DropoutConf(BaseConf):\n    """""" Configuration for Dropout\n\n    Args:\n        dropout (float): dropout rate, probability of an element to be zeroed\n\n    Returns:\n\n    """"""\n    def __int__(self, **kwargs):\n\n        super(DropoutConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        self.dropout = 0.5\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 1\n        self.input_ranks = [-1]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = copy.deepcopy(self.input_dims[0])\n\n        super(DropoutConf, self).inference()  # PUT THIS LINE AT THE END OF inference()\n\n    @DocInherit\n    def verify(self):\n        super(DropoutConf, self).verify()\n\n        necessary_attrs_for_user = [\'dropout\']\n        for attr in necessary_attrs_for_user:\n            self.add_attr_exist_assertion_for_user(attr)\n\n        range_checks = [(\'dropout\', (0, 1), (True, True))]\n        for attr, ranges, bound_legal in range_checks:\n            self.add_attr_range_assertion(attr, ranges, bound_legal)\n\n\nclass Dropout(BaseLayer):\n    """""" Dropout\n\n    Args:\n        layer_conf (DropoutConf): configuration of a layer\n    """"""\n    def __init__(self, layer_conf):\n        super(Dropout, self).__init__(layer_conf)\n        self.dropout_layer = nn.Dropout(layer_conf.dropout)\n\n    def forward(self, string, string_len=None):\n        """""" process inputs\n\n        Args:\n            string (Tensor): any shape.\n            string_len (Tensor): [batch_size], default is None.\n\n        Returns:\n            Tensor: has the same shape as string.\n        """"""\n        string_out = self.dropout_layer(string)\n        return string_out, string_len\n\n'"
block_zoo/Embedding.py,5,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import autograd\nfrom block_zoo.BaseLayer import BaseLayer, BaseConf\nimport numpy as np\nfrom utils.DocInherit import DocInherit\nfrom block_zoo.embedding import *\nimport copy\nimport logging\n\nclass EmbeddingConf(BaseConf):\n    """""" Configuration for Embedding\n\n    Args:\n        conf:a dictionary. The key is embedding type, such as word embedding, char embedding, Part-of-Speech embedding and so on.\n\n    Example::\n\n        ""conf"": {\n          ""word"": {\n            ""cols"": [""question_text"", ""answer_text""],\n            ""dim"": 300,\n            ""fix_weight"": true\n          },\n          ""postag"": {\n            ""cols"": [""question_postag"",""answer_postag""],\n            ""dim"": 20\n          },\n          ""char"": {\n            ""cols"": [""question_char"", ""answer_char""],\n            ""type"": ""CNNCharEmbedding"",\n            ""dropout"": 0.2,\n            ""dim"": 30,\n            ""embedding_matrix_dim"": 8,\n            ""stride"":1,\n            ""window_size"": 5,\n            ""activation"": null\n          }\n        }\n    """"""\n    def __init__(self, **kwargs):\n        super(EmbeddingConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        self.conf = {\n            \'word\': {\n                \'vocab_size\': 1000,\n                \'dim\': 300,\n                \'init_weights\': np.random.randn(1000, 300)      # you can give a initial weight here like this or assign it to None\n            }\n        }\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 1\n        self.input_ranks = [2]         #[batch size, sequence length]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = [-1, -1, 0]\n        for emb_type in self.conf:\n            if emb_type == \'position\':\n                continue\n            if isinstance(self.conf[emb_type][\'dim\'], list):\n                self.output_dim[2] += sum(self.conf[emb_type][\'dim\'])\n            else:\n                self.output_dim[2] += self.conf[emb_type][\'dim\']\n\n        super(EmbeddingConf, self).inference()\n\n    @DocInherit\n    def verify_before_inference(self):\n        necessary_attrs_for_user = [\'conf\']\n        for attr in necessary_attrs_for_user:\n            self.add_attr_exist_assertion_for_user(attr)\n\n        necessary_attrs_for_dev = [\'num_of_inputs\', \'input_ranks\']\n        for attr in necessary_attrs_for_dev:\n            self.add_attr_exist_assertion_for_dev(attr)\n\n        type_checks = [(\'conf\', dict),\n                       (\'num_of_inputs\', int),\n                       (\'input_ranks\', list)]\n        for attr, attr_type in type_checks:\n            self.add_attr_type_assertion(attr, attr_type)\n\n\n    @DocInherit\n    def verify(self):\n        #super(EmbeddingConf, self).verify()\n\n        necessary_attrs_for_dev = [\'output_dim\', \'output_rank\']\n        for attr in necessary_attrs_for_dev:\n            self.add_attr_exist_assertion_for_dev(attr)\n\n        type_checks = [(\'output_dim\', list),\n                       (\'output_rank\', int)]\n        for attr, attr_type in type_checks:\n            self.add_attr_type_assertion(attr, attr_type)\n\n\nclass Embedding(BaseLayer):\n    """""" Embedding layer\n\n    Args:\n        layer_conf (EmbeddingConf): configuration of a layer\n    """"""\n    def __init__(self, layer_conf):\n\n        super(Embedding, self).__init__(layer_conf)\n        self.layer_conf = layer_conf\n\n        self.embeddings = nn.ModuleDict() if layer_conf.weight_on_gpu else dict()\n        self.char_embeddings = nn.ModuleDict()\n        for input_cluster in layer_conf.conf:\n            if \'type\' in layer_conf.conf[input_cluster]:\n                # char embedding\n                char_emb_conf_dict = copy.deepcopy(layer_conf.conf[input_cluster])\n                # del char_emb_conf_dict[\'cols\'], char_emb_conf_dict[\'type\']\n                char_emb_conf_dict[\'use_gpu\'] = layer_conf.use_gpu\n                char_emb_conf = eval(layer_conf.conf[input_cluster][\'type\'] + ""Conf"")(** char_emb_conf_dict)\n                char_emb_conf.inference()\n                char_emb_conf.verify()\n                self.char_embeddings[input_cluster] = eval(layer_conf.conf[input_cluster][\'type\'])(char_emb_conf)\n            else:\n                # word embedding, postag embedding, and so on\n                self.embeddings[input_cluster] = nn.Embedding(layer_conf.conf[input_cluster][\'vocab_size\'], layer_conf.conf[input_cluster][\'dim\'], padding_idx=0)\n                if \'init_weights\' in layer_conf.conf[input_cluster] and layer_conf.conf[input_cluster][\'init_weights\'] is not None:\n                    self.embeddings[input_cluster].weight = nn.Parameter(torch.from_numpy(layer_conf.conf[input_cluster][\'init_weights\']))\n\n                # judge if fix the embedding weight\n                if layer_conf.conf[input_cluster][\'fix_weight\']:\n                    self.embeddings[input_cluster].weight.requires_grad = False\n                    logging.info(""The Embedding[%s][fix_weight] is true, fix the embeddings[%s]\'s weight"" % (input_cluster, input_cluster))\n\n    def forward(self, inputs, use_gpu=False):\n        """""" process inputs\n\n        Args:\n            inputs (dict): a dictionary to describe each transformer_model inputs. e.g.:\\n\n                        char_emb\': [[char ids of word1], [char ids of word2], [...], ...], shape: [batch_size, seq_len, word character num]\\n\n                        \'word\': word ids (Variable), shape:[batch_size, seq_len],\\n\n                        \'postag\': postag ids (Variable), shape: [batch_size, seq_len],\\n\n                        ...\n            use_gpu (bool): put embedding matrix on GPU (True) or not (False)\n\n        Returns:\n            Variable: the embedding representation with shape [batch_size, seq_len, emb_dim]\n\n        """"""\n        features = []\n\n        for input_cluster in inputs:\n            if \'extra\' in input_cluster:\n                continue\n            input = inputs[input_cluster]\n            if input_cluster == \'char\':\n                emb = self.char_embeddings[input_cluster](input).float()\n            else:\n                if list(self.embeddings[input_cluster].parameters())[0].device.type == \'cpu\':\n                    emb = self.embeddings[input_cluster](input.cpu()).float()\n                else:\n                    emb = self.embeddings[input_cluster](input).float()\n            if use_gpu is True:\n                device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n                emb = emb.to(device)\n            features.append(emb)\n\n        if len(features) > 1:\n            return torch.cat(features, 2)\n        else:\n            return features[0]\n\n    def get_parameters(self):\n        for sub_emb in self.embeddings:\n            for param in self.embeddings[sub_emb].parameters():\n                yield param\n\n\n\n\n'"
block_zoo/EncoderDecoder.py,2,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np\nfrom block_zoo.BaseLayer import BaseLayer, BaseConf\nfrom .encoder_decoder import *\nfrom utils.DocInherit import DocInherit\nfrom utils.exceptions import ConfigurationError\nimport copy\n\nclass EncoderDecoderConf(BaseConf):\n    """""" Configuration of Encoder-Decoder\n\n    Args:\n        encoder (str): encoder name\n        encoder_conf (dict): configurations of encoder\n        decoder (str): decoder name\n        decoder_conf (dict): configurations of decoder\n\n    """"""\n    def __init__(self, **kwargs):\n        self.encoder_conf_cls = eval(kwargs[\'encoder\'] + ""Conf"")(**kwargs[\'encoder_conf\'])\n        self.decoder_conf_cls = eval(kwargs[\'decoder\'] + ""Conf"")(**kwargs[\'decoder_conf\'])\n\n        super(EncoderDecoderConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        self.encoder_name = ""SLUEncoder""\n        self.decoder_name = ""SLUDecoder""\n\n        self.encoder_conf = dict()\n        self.decoder_conf = dict()\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 1\n        self.input_ranks = [3]\n\n    @DocInherit\n    def inference(self):\n        """""" Dimension inference of encoder and decoder is conducted here, but not in the Model.\n\n        Returns:\n\n        """"""\n        self.encoder_conf_cls.use_gpu = self.use_gpu\n        self.decoder_conf_cls.use_gpu = self.use_gpu\n\n        # inference inside the encoder and decoder\n        self.encoder_conf_cls.input_dims = copy.deepcopy(self.input_dims)\n        self.encoder_conf_cls.inference()\n\n        # rank varification between encoder and decoder\n        former_output_ranks = [self.encoder_conf_cls.output_rank]\n        for input_rank, former_output_rank in zip(self.decoder_conf_cls.input_ranks, former_output_ranks):\n            if input_rank != -1 and input_rank != former_output_rank:\n                raise ConfigurationError(""Input ranks of decoder %s are inconsistent with former encoder %s"" %\n                         (self.decoder_name, self.encoder_name))\n        self.decoder_conf_cls.input_ranks = copy.deepcopy(former_output_ranks)\n\n        # some dimension of decoder are inferenced from encoder\n        self.decoder_conf_cls.input_dims = [self.encoder_conf_cls.output_dim]\n        self.decoder_conf_cls.input_context_dims = [self.encoder_conf_cls.output_context_dim]\n        self.decoder_conf_cls.inference()\n\n        self.output_dim = self.decoder_conf_cls.output_dim\n        self.output_rank = 3\n\n    @DocInherit\n    def verify_before_inference(self):\n        super(EncoderDecoderConf, self).verify_before_inference()\n        self.encoder_conf_cls.verify_before_inference()\n        self.decoder_conf_cls.verify_before_inference()\n\n        necessary_attrs_for_user = [\'encoder\', \'encoder_conf\', \'decoder\', \'decoder_conf\', \'encoder_name\', \'decoder_name\']\n        for attr in necessary_attrs_for_user:\n            self.add_attr_exist_assertion_for_user(attr)\n\n    @DocInherit\n    def verify(self):\n        super(EncoderDecoderConf, self).verify()\n        self.encoder_conf_cls.verify()\n        self.decoder_conf_cls.verify()\n\n\nclass EncoderDecoder(BaseLayer):\n    """""" The encoder decoder framework\n\n    Args:\n        layer_conf (EncoderDecoderConf): configuration of a layer\n    """"""\n    def __init__(self, layer_conf):\n        super(EncoderDecoder, self).__init__(layer_conf)\n        self.layer_conf = layer_conf\n\n        self.encoder = eval(layer_conf.encoder_name)(layer_conf.encoder_conf_cls)\n        self.decoder = eval(layer_conf.decoder_name)(layer_conf.decoder_conf_cls)\n\n    def forward(self, string, string_len):\n        """""" process inputs with encoder & decoder\n\n        Args:\n            string (Variable): [batch_size, seq_len, dim]\n            string_len (ndarray): [batch_size]\n\n        Returns:\n            Variable : decode scores with shape [batch_size, seq_len, decoder_vocab_size]\n        """"""\n        encoder_output, encoder_context = self.encoder(string, string_len)\n        decoder_scores = self.decoder(string, string_len, encoder_context, encoder_output)\n        return decoder_scores, string_len\n\n\n'"
block_zoo/HighwayLinear.py,4,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np\nimport copy\nfrom block_zoo.BaseLayer import BaseLayer, BaseConf\nfrom utils.DocInherit import DocInherit\n\n\nclass HighwayLinearConf(BaseConf):\n    """""" Configuration of BiLSTM\n\n    Args:\n        hidden_dim (int): dimension of hidden state\n        dropout (float): dropout rate\n        num_layers (int): number of BiLSTM layers\n    """"""\n    def __init__(self, **kwargs):\n        super(HighwayLinearConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        self.num_layers = 1\n        self.activation = \'PReLU\'\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 1\n        self.input_ranks = [-1]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = copy.deepcopy(self.input_dims[0])\n        super(HighwayLinearConf, self).inference()  # PUT THIS LINE AT THE END OF inference()\n\n    @DocInherit\n    def verify(self):\n        super(HighwayLinearConf, self).verify()\n\n        necessary_attrs_for_user = [\'num_layers\', \'activation\']\n        for attr in necessary_attrs_for_user:\n            self.add_attr_exist_assertion_for_user(attr)\n\n\nclass HighwayLinear(BaseLayer):\n    """""" A `Highway layer <https://arxiv.org/abs/1505.00387>`_ does a gated combination of a linear\n    transformation and a non-linear transformation of its input.  :math:`y = g * x + (1 - g) *\n    f(A(x))`, where :math:`A` is a linear transformation, :math:`f` is an element-wise\n    non-linearity, and :math:`g` is an element-wise gate, computed as :math:`sigmoid(B(x))`.\n    This module will apply a fixed number of highway layers to its input, returning the final\n    result.\n\n    Args:\n        layer_conf (HighwayLinearConf): configuration of a layer\n    """"""\n    def __init__(self, layer_conf):\n        super(HighwayLinear, self).__init__(layer_conf)\n        self.layer_conf = layer_conf\n        self.layers = torch.nn.ModuleList([torch.nn.Linear(layer_conf.input_dims[0][-1], layer_conf.input_dims[0][-1] * 2) for _ in range(layer_conf.num_layers)])\n        self.activation = eval(""nn."" + layer_conf.activation)()\n\n    def forward(self, string, string_len):\n        """""" process inputs\n\n        Args:\n            string (Tensor): [batch_size, seq_len, dim]\n            string_len (Tensor): [batch_size]\n\n        Returns:\n            Tensor: [batch_size, seq_len, 2 * hidden_dim]\n\n        """"""\n        current_input = string\n        for layer in self.layers:\n            projected_input = layer(current_input)\n            linear_part = current_input\n            # NOTE: if you modify this, think about whether you should modify the initialization above, too.\n            nonlinear_part, gate = projected_input.chunk(2, dim=-1)\n            nonlinear_part = self.activation(nonlinear_part)\n            gate = torch.sigmoid(gate)\n            current_input = gate * linear_part + (1 - gate) * nonlinear_part\n        return current_input, string_len\n\n'"
block_zoo/Linear.py,7,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import OrderedDict\n\nfrom block_zoo.BaseLayer import BaseLayer, BaseConf\nfrom utils.DocInherit import DocInherit\nimport logging\nimport copy\n\n\nclass LinearConf(BaseConf):\n    """"""Configuration for Linear layer\n\n    Args:\n        hidden_dim (int or list): if is int, it means there is one linear layer and the hidden_dim is the dimension of that layer.\n                if is list of int, it means there is multiple linear layer and hidden_dim are the dimensions of these layers.\n        activation (str): Name of activation function. All the non-linear activations in http://pytorch.org/docs/0.3.1/nn.html#non-linear-activations are supported, such as \'Tanh\', \'ReLU\', \'PReLU\', \'ReLU6\' and \'LeakyReLU\'. Default is None.\n        last_hidden_activation (bool): [Optional], whether to add nonlinearity to the last linear layer\'s output. Default is True.\n        last_hidden_softmax (bool): [Optional], whether to add softmax to the last linear layer\'s output. Default is False.\n    """"""\n    def __int__(self, **kwargs):\n        super(LinearConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        self.hidden_dim = 128\n        self.batch_norm = True     # currently, batch_norm for rank 3 inputs is disabled\n        self.activation = \'PReLU\'\n        self.last_hidden_activation = True\n        self.last_hidden_softmax = False\n        self.keep_dim = True       # for exmaple if the output shape is [?, len, 1]. you want to squeeze it, set keep_dim=False, the the output shape is [?, len]\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 1\n        self.input_ranks = [-1]\n\n    @DocInherit\n    def inference(self):\n        if isinstance(self.hidden_dim, int):\n            self.output_dim = copy.deepcopy(self.input_dims[0])\n            if not self.keep_dim and self.hidden_dim == 1:\n                self.output_dim.pop()\n            else:\n                self.output_dim[-1] = self.hidden_dim\n        elif isinstance(self.hidden_dim, list):\n            self.output_dim = copy.deepcopy(self.input_dims[0])\n            if not self.keep_dim and self.hidden_dim[-1] == 1:\n                self.output_dim.pop()\n            else:\n                self.output_dim[-1] = self.hidden_dim[-1]\n\n        super(LinearConf, self).inference()  # PUT THIS LINE AT THE END OF inference()\n\n    @DocInherit\n    def verify_before_inference(self):\n        super(LinearConf, self).verify_before_inference()\n        necessary_attrs_for_user = [\'hidden_dim\']\n        for attr in necessary_attrs_for_user:\n            self.add_attr_exist_assertion_for_user(attr)\n\n    @DocInherit\n    def verify(self):\n        super(LinearConf, self).verify()\n        necessary_attrs_for_user = [\'activation\', \'last_hidden_activation\', \'last_hidden_softmax\']\n        for attr in necessary_attrs_for_user:\n            self.add_attr_exist_assertion_for_user(attr)\n\n        type_checks = [(\'activation\', [None, str]),\n                       (\'last_hidden_activation\', bool),\n                       (\'last_hidden_softmax\', bool)]\n        for attr, attr_type in type_checks:\n            self.add_attr_type_assertion(attr, attr_type)\n\n        # supported activation of PyTorch now:\n        supported_activation_pytorch = [None, \'Sigmoid\', \'Tanh\', \'ReLU\', \'PReLU\', \'ReLU6\', \'LeakyReLU\', \'LogSigmoid\', \'ELU\',\n                \'SELU\', \'Threshold\', \'Hardtanh\', \'Softplus\', \'Softshrink\', \'Softsign\', \'Tanhshrink\', \'Softmin\',\n                \'Softmax\', \'Softmax2d\', \'LogSoftmax\']\n        value_checks = [(\'activation\', supported_activation_pytorch)]\n        for attr, legal_values in value_checks:\n            self.add_attr_value_assertion(attr, legal_values)\n\n\nclass Linear(BaseLayer):\n    """""" Linear layer\n\n    Args:\n        layer_conf (LinearConf): configuration of a layer\n    """"""\n    def __init__(self, layer_conf):\n\n        super(Linear, self).__init__(layer_conf)\n        self.layer_conf = layer_conf\n\n        if layer_conf.input_ranks[0] == 3 and layer_conf.batch_norm is True:\n            layer_conf.batch_norm = False\n            logging.warning(\'Batch normalization for dense layers of which the rank is 3 is not available now. Batch norm is set to False now.\')\n\n        if isinstance(layer_conf.hidden_dim, int):\n            layer_conf.hidden_dim = [layer_conf.hidden_dim]\n\n        layers = OrderedDict()\n        former_dim = layer_conf.input_dims[0][-1]\n        for i in range(len(layer_conf.hidden_dim)):\n            #cur_layer_name = \'linear_%d\' % len(layers)\n            layers[\'linear_%d\' % len(layers)] = nn.Linear(former_dim, layer_conf.hidden_dim[i])\n            if layer_conf.activation is not None and \\\n                    (layer_conf.last_hidden_activation is True or (i != len(layer_conf.hidden_dim) - 1)):\n                try:\n                    if layer_conf.batch_norm:\n                        layers[\'batch_norm_%d\' % len(layers)] = nn.BatchNorm1d(layer_conf.hidden_dim[i])\n                    layers[\'linear_activate_%d\' % len(layers)] = eval(""nn."" + layer_conf.activation)()\n                except NameError as e:\n                    raise Exception(""%s; Activation layer \\""nn.%s\\"""" % (str(e), layer_conf.activation))\n\n            if layer_conf.last_hidden_softmax is True and i == len(layer_conf.hidden_dim) - 1:\n                layers[\'linear_softmax_%d\' % len(layers)] = nn.Softmax(layer_conf.output_rank - 1)\n\n            former_dim = layer_conf.hidden_dim[i]\n\n        self.linear = nn.Sequential(layers)\n\n    def forward(self, string, string_len=None):\n        """""" process inputs\n\n        Args:\n            string (Tensor): any shape.\n            string_len (Tensor): [batch_size], default is None.\n\n        Returns:\n            Tensor: has the same shape as string.\n        """"""\n        if self.layer_conf.input_ranks[0] == 3 and string_len is not None:\n            # need padding mask\n            string_len_val = string_len.cpu().data.numpy()\n            masks = []\n            for i in range(len(string_len)):\n                masks.append(\n                    torch.cat([torch.ones(string_len_val[i]), torch.zeros(string.shape[1] - string_len_val[i])]))\n            masks = torch.stack(masks).view(string.shape[0], string.shape[1], 1).expand_as(string)\n            if self.is_cuda():\n                device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n                masks = masks.to(device)\n            string = string * masks\n        string_out = self.linear(string.float())\n        if not self.layer_conf.keep_dim:\n            string_out = torch.squeeze(string_out, -1)\n        return string_out, string_len\n\n\n'"
block_zoo/Pooling.py,7,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np\n\nfrom block_zoo.BaseLayer import BaseLayer, BaseConf\nfrom utils.DocInherit import DocInherit\n\n\nclass PoolingConf(BaseConf):\n    """"""\n\n    Args:\n        pool_type (str): \'max\' or \'mean\', default is \'max\'.\n        pool_axis (int): which axis to conduct pooling, default is 1.\n    """"""\n    def __init__(self, **kwargs):\n        super(PoolingConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        #self.input_dim = 128\n        self.pool_type = \'max\'  # Supported: [\'max\', mean\']\n        self.pool_axis = 1\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 1\n        self.input_ranks = [-1]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = []\n        for idx, dim in enumerate(self.input_dims[0]):\n            if idx != self.pool_axis:\n                self.output_dim.append(dim)\n\n        # DON\'T MODIFY THIS\n        self.output_rank = len(self.output_dim)\n\n    @DocInherit\n    def verify(self):\n        super(PoolingConf, self).verify()\n\n        necessary_attrs_for_user = [\'pool_type\', \'pool_axis\']\n        for attr in necessary_attrs_for_user:\n            self.add_attr_exist_assertion_for_user(attr)\n\n        self.add_attr_value_assertion(\'pool_type\', [\'max\', \'mean\'])\n\n        assert all([input_rank >= 3 for input_rank in self.input_ranks]), ""Cannot apply a pooling layer on a tensor of which the rank is less than 3. Usually, a tensor whose rank is at least 3, e.g. [batch size, sequence length, feature]""\n\n        assert self.output_dim[-1] != -1, ""Pooling on the axis %d while the input shape is %s requires that the sequence lengths should be fixed! Please set it on conf/training_params/fixed_lengths"" % (self.pool_axis, str(self.input_dims[0]))\n\nclass Pooling(BaseLayer):\n    """""" Pooling layer\n\n    Args:\n        layer_conf (PoolingConf): configuration of a layer\n    """"""\n    def __init__(self, layer_conf):\n        super(Pooling, self).__init__(layer_conf)\n\n    def forward(self, string, string_len=None):\n        """""" process inputs\n\n        Args:\n            string (Tensor): any shape.\n            string_len (Tensor): [batch_size], default is None.\n\n        Returns:\n            Tensor: Pooling result of string\n\n        """"""\n        if self.layer_conf.pool_type == ""mean"":\n            assert not string_len is None, ""Parameter string_len should not be None!""\n            string = torch.sum(string, self.layer_conf.pool_axis).squeeze(self.layer_conf.pool_axis)\n            if not torch.is_tensor(string_len):\n                string_len = torch.FloatTensor(string_len).unsqueeze(1)\n            if self.is_cuda():\n                device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n                string_len = string_len.to(device)\n            string_len = string_len.unsqueeze(1)\n            output = string / string_len.expand_as(string).float()\n        elif self.layer_conf.pool_type == ""max"":\n            output = torch.max(string, self.layer_conf.pool_axis)[0]\n\n        return output, string_len\n\n\n'"
block_zoo/Pooling1D.py,2,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np\n\nfrom block_zoo.BaseLayer import BaseLayer, BaseConf\nfrom utils.DocInherit import DocInherit\n\n\nclass Pooling1DConf(BaseConf):\n    """"""\n\n    Args:\n        pool_type (str): \'max\' or \'mean\', default is \'max\'.\n        stride (int): which axis to conduct pooling, default is 1.\n        padding (int): implicit zero paddings on both sides of the input. Can be a single number or a tuple (padH, padW). Default: 0\n        window_size (int): the size of the pooling\n\n    """"""\n\n    def __init__(self, **kwargs):\n        super(Pooling1DConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        self.pool_type = \'max\'  # Supported: [\'max\', mean\']\n        self.stride = 1\n        self.padding = 0\n        self.window_size = 3\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 1\n        self.input_ranks = [3]\n\n\n    @DocInherit\n    def inference(self):\n\n        self.output_dim = [self.input_dims[0][0]]\n        if self.input_dims[0][1] != -1:\n            self.output_dim.append(\n                (self.input_dims[0][1] + 2 * self.padding - self.window_size) // self.stride + 1)\n        else:\n            self.output_dim.append(-1)\n\n        self.output_dim.append(self.input_dims[0][-1])\n        # DON\'T MODIFY THIS\n        self.output_rank = len(self.output_dim)\n\n    @DocInherit\n    def verify(self):\n        super(Pooling1DConf, self).verify()\n\n        necessary_attrs_for_user = [\'pool_type\']\n        for attr in necessary_attrs_for_user:\n            self.add_attr_exist_assertion_for_user(attr)\n\n        self.add_attr_value_assertion(\'pool_type\', [\'max\', \'mean\'])\n\n        assert self.output_dim[\n                   -1] != -1, ""The shape of input is %s , and the input channel number of pooling should not be -1."" % (\n            str(self.input_dims[0]))\n\n\nclass Pooling1D(BaseLayer):\n    """""" Pooling layer\n\n    Args:\n        layer_conf (PoolingConf): configuration of a layer\n    """"""\n\n    def __init__(self, layer_conf):\n        super(Pooling1D, self).__init__(layer_conf)\n        self.pool = None\n        if layer_conf.pool_type == ""max"":\n            self.pool = nn.MaxPool1d(kernel_size=layer_conf.window_size, stride=layer_conf.stride,\n                                     padding=layer_conf.padding)\n        elif layer_conf.pool_type == ""mean"":\n            self.pool = nn.AvgPool1d(kernel_size=layer_conf.window_size, stride=layer_conf.stride,\n                                     padding=layer_conf.padding)\n\n    def forward(self, string, string_len=None):\n        """""" process inputs\n\n        Args:\n            string (Tensor): tensor with shape: [batch_size, length, feature_dim]\n            string_len (Tensor): [batch_size], default is None.\n\n        Returns:\n            Tensor: Pooling result of string\n\n        """"""\n\n        string = string.permute([0, 2, 1]).contiguous()\n        string = self.pool(string)\n        string = string.permute([0, 2, 1]).contiguous()\n        return string, string_len\n\n\n'"
block_zoo/Pooling2D.py,2,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np\n\nfrom block_zoo.BaseLayer import BaseLayer, BaseConf\nfrom utils.DocInherit import DocInherit\n\n\nclass Pooling2DConf(BaseConf):\n    """"""\n\n    Args:\n        pool_type (str): \'max\' or \'mean\', default is \'max\'.\n        stride (int): which axis to conduct pooling, default is 1.\n        padding (int): implicit zero paddings on both sides of the input. Can be a single number or a tuple (padH, padW). Default: 0\n        window_size (int): the size of the pooling\n\n    """"""\n    def __init__(self, **kwargs):\n        super(Pooling2DConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        self.pool_type = \'max\'  # Supported: [\'max\', mean\']\n        self.stride = 1\n        self.padding = 0\n        # self.window_size = [self.input_dims[0][1], self.input_dims[0][2]]\n        \n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 1\n        self.input_ranks = [4]\n    \n    def check_size(self, value, attr):\n        res = value\n        if isinstance(value, int):\n            res = [value, value]\n        elif (isinstance(self.window_size, tuple) or isinstance(self.window_size, list)) and len(value)==2:\n            res = list(value)\n        else:\n            raise AttributeError(""The Atrribute `%s\' should be given an integer or a list/tuple with length of 2, instead of %s."" %(attr,str(value)))\n        return res\n            \n    @DocInherit\n    def inference(self):\n\n        if not hasattr(self, ""window_size""):\n            self.window_size = [self.input_dims[0][1], self.input_dims[0][2]]\n        \n        self.window_size = self.check_size(self.window_size, ""window_size"")\n        self.stride = self.check_size(self.stride, ""stride"")\n        self.padding = self.check_size(self.padding, ""padding"")\n        \n        self.output_dim = [self.input_dims[0][0]]\n        if self.input_dims[0][1] != -1:\n            self.output_dim.append((self.input_dims[0][1] + 2 * self.padding[0] - self.window_size[0]) // self.stride[0] + 1)\n        else:\n            self.output_dim.append(-1)\n        if self.input_dims[0][2] != -1:\n            self.output_dim.append((self.input_dims[0][2] + 2 * self.padding[1] - self.window_size[1]) // self.stride[1] + 1)\n        else:\n            self.output_dim.append(-1)\n        # print(""pool"",self.output_dim)\n        self.input_channel_num = self.input_dims[0][-1]\n\n        self.output_dim.append(self.input_dims[0][-1])\n\n        # DON\'T MODIFY THIS\n        self.output_rank = len(self.output_dim)\n\n    @DocInherit\n    def verify(self):\n        super(Pooling2DConf, self).verify()\n\n        necessary_attrs_for_user = [\'pool_type\']\n        for attr in necessary_attrs_for_user:\n            self.add_attr_exist_assertion_for_user(attr)\n\n        self.add_attr_value_assertion(\'pool_type\', [\'max\', \'mean\'])\n\n        assert all([input_rank >= 4 for input_rank in self.input_ranks]), ""Cannot apply a pooling layer on a tensor of which the rank is less than 4. Usually, a tensor whose rank is at least 4, e.g. [batch size, length, width, feature]""\n\n        assert self.output_dim[-1] != -1, ""The shape of input is %s , and the input channel number of pooling should not be -1."" % (str(self.input_dims[0]))\n\nclass Pooling2D(BaseLayer):\n    """""" Pooling layer\n\n    Args:\n        layer_conf (PoolingConf): configuration of a layer\n    """"""\n    def __init__(self, layer_conf):\n        super(Pooling2D, self).__init__(layer_conf)\n        self.pool = None\n        if layer_conf.pool_type == ""max"":\n            self.pool = nn.MaxPool2d(kernel_size=layer_conf.window_size,stride=layer_conf.stride,padding=layer_conf.padding)\n        elif layer_conf.pool_type == ""mean"":\n            self.pool = nn.AvgPool2d(kernel_size=layer_conf.window_size,stride=layer_conf.stride,padding=layer_conf.padding)\n\n    def forward(self, string, string_len=None):\n        """""" process inputs\n\n        Args:\n            string (Tensor): tensor with shape: [batch_size, length, width, feature_dim]\n            string_len (Tensor): [batch_size], default is None.\n\n        Returns:\n            Tensor: Pooling result of string\n\n        """"""\n\n        string = string.permute([0,3,1,2]).contiguous()\n\n        string = self.pool(string)\n\n        string = string.permute([0,2,3,1]).contiguous()\n\n        return string, string_len\n\n\n'"
block_zoo/Transformer.py,1,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\n\nfrom block_zoo.BaseLayer import BaseLayer, BaseConf\nfrom block_zoo.transformer.MLP import MLP, MLPConf\nfrom block_zoo.transformer.MultiHeadAttention import MultiHeadAttention, MultiHeadAttentionConf\nfrom block_zoo.normalizations.LayerNorm import LayerNorm, LayerNormConf\nfrom utils.DocInherit import DocInherit\nimport copy\n\nclass TransformerConf(BaseConf):\n    """""" Configuration of Transformer\n\n    Args:\n        attention (str): attention name\n        attention_conf (dict): configurations of attention\n        layernorm1 (str): layernorm1 name\n        layernorm1_conf (dict): configurations of layernorm1\n        mlp (str): mlp name\n        mlp_conf (dict): configuration of mlp\n        layernorm2 (str): layernorm2 name\n        layernorm2_conf (dict): configurations of layernorm2\n        n_layer (int) layer num of transformer\n\n    """"""\n    def __init__(self, **kwargs):\n        self.attention_conf_cls = eval(kwargs[\'attention\'] + ""Conf"")(**kwargs[\'attention_conf\'])\n        self.layernorm1_conf_cls = eval(kwargs[\'layernorm_1\'] + ""Conf"")(**kwargs[\'layernorm1_conf\'])\n        self.mlp_conf_cls = eval(kwargs[\'mlp\'] + ""Conf"")(**kwargs[\'mlp_conf\'])\n        self.layernorm2_conf_cls = eval(kwargs[\'layernorm_2\'] + ""Conf"")(**kwargs[\'layernorm2_conf\'])\n\n        super(TransformerConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        self.attention_name = ""MultiHeadAttention""\n        self.layernorm1_name = ""LayerNorm""\n        self.mlp_name = ""MLP""\n        self.layernorm2_name = ""LayerNorm""\n\n        self.attention_conf = dict()\n        self.layernorm1_conf = dict()\n        self.mlp_conf = dict()\n        self.layernorm2_conf = dict()\n\n        self.n_layer = 12\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 1\n        self.input_ranks = [3]\n\n    @DocInherit\n    def inference(self):\n        self.attention_conf_cls.use_gpu = self.use_gpu\n        self.layernorm1_conf_cls.use_gpu = self.use_gpu\n        self.mlp_conf_cls.use_gpu = self.use_gpu\n        self.layernorm2_conf_cls.use_gpu = self.use_gpu\n\n        self.attention_conf_cls.input_dims = copy.deepcopy(self.input_dims)\n        self.attention_conf_cls.inference()\n\n        self.layernorm1_conf_cls.input_dims = [self.attention_conf_cls.output_dim]\n        self.layernorm1_conf_cls.inference()\n\n        self.mlp_conf_cls.input_dims = [self.layernorm1_conf_cls.output_dim]\n        self.mlp_conf_cls.inference()\n\n        self.layernorm2_conf_cls.input_dims = [self.mlp_conf_cls.output_dim]\n        self.layernorm2_conf_cls.inference()\n\n        self.output_dim = self.layernorm2_conf_cls.output_dim\n\n        super(TransformerConf, self).inference()\n\n    @DocInherit\n    def verify(self):\n        super(TransformerConf, self).verify()\n        self.attention_conf_cls.verify()\n        self.layernorm1_conf_cls.verify()\n        self.layernorm2_conf_cls.verify()\n        self.mlp_conf_cls.verify()\n\nclass Transformer(nn.Module):\n    """""" Transformer layer\n\n    Args:\n        layer_conf (TransformerConf): configuration of a layer\n    """"""\n    def __init__(self, layer_conf):\n        super(Transformer, self).__init__()\n        self.layer_conf = layer_conf\n\n        self.transformer_layer = nn.ModuleList([copy.deepcopy(nn.ModuleList([eval(layer_conf.attention_name)(layer_conf.attention_conf_cls),\n                                    eval(layer_conf.layernorm1_name)(layer_conf.layernorm1_conf_cls),\n                                    eval(layer_conf.mlp_name)(layer_conf.mlp_conf_cls),\n                                    eval(layer_conf.layernorm2_name)(layer_conf.layernorm2_conf_cls)])) for _ in range(self.layer_conf.n_layer)])\n        # self.attention_layers = nn.ModuleList()\n        # self.layernorm1_layers = nn.ModuleList()\n        # self.mlp_layers = nn.ModuleList()\n        # self.layernorm2_layers = nn.ModuleList()\n        #\n        # for i in range(self.layer_conf.n_layer):\n        #     self.attention_layers.append(eval(layer_conf.attention_name)(layer_conf.attention_conf_cls))\n        #     self.layernorm1_layers.append(eval(layer_conf.layernorm1_name)(layer_conf.layernorm1_conf_cls))\n        #     self.mlp_layers.append(eval(layer_conf.mlp_name)(layer_conf.mlp_conf_cls))\n        #     self.layernorm2_layers.append(eval(layer_conf.layernorm2_name)(layer_conf.layernorm2_conf_cls))\n\n        # self.attention = eval(layer_conf.attention_name)(layer_conf.attention_conf_cls)\n        # self.layernorm1 = eval(layer_conf.layernorm1_name)(layer_conf.layernorm1_conf_cls)\n        # self.mlp = eval(layer_conf.mlp_name)(layer_conf.mlp_conf_cls)\n        # self.layernorm2 = eval(layer_conf.layernorm2_name)(layer_conf.layernorm2_conf_cls)\n\n    def forward(self, string, string_len):\n        """""" process input\n\n        Args:\n            string (Tensor): [batch_size, seq_len, dim]\n            string_len (Tensor): [batch_size]\n        Returns:\n            Tensor : [batch_size, seq_len, output_dim], [batch_size]\n        """"""\n        h = string\n        l = string_len\n        # for i in range(self.layer_conf.n_layer):\n        #     a, a_len = self.attention_layers[i](h,l)\n        #     n, n_len = self.layernorm1_layers[i](a+h, a_len)\n        #     m, m_len = self.mlp_layers[i](n, n_len)\n        #     h, l = self.layernorm2_layers[i](m+n, m_len)\n        for block in self.transformer_layer:\n            a, a_len = block[0](h,1)\n            n, n_len = block[1](a+h, a_len)\n            m, m_len = block[2](n, n_len)\n            h, l = block[3](m + n, m_len)\n        return h, l\n\n\n\n'"
block_zoo/__init__.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\nfrom .Embedding import Embedding, EmbeddingConf\nfrom .BiLSTM import BiLSTM, BiLSTMConf\nfrom .BiLSTMLast import BiLSTMLast, BiLSTMLastConf\nfrom .BiGRU import BiGRU, BiGRUConf\nfrom .BiGRULast import BiGRULast, BiGRULastConf\nfrom .Linear import Linear, LinearConf\nfrom .BaseLayer import BaseLayer, BaseConf\nfrom .BiLSTMAtt import BiLSTMAtt, BiLSTMAttConf\nfrom .BiQRNN import BiQRNN, BiQRNNConf\nfrom .Conv import Conv, ConvConf\nfrom .Pooling import Pooling, PoolingConf\nfrom .ConvPooling import ConvPooling, ConvPoolingConf\n\nfrom .Dropout import Dropout, DropoutConf\n\nfrom .Conv2D import Conv2D, Conv2DConf\nfrom .Pooling1D import Pooling1D, Pooling1DConf\nfrom .Pooling2D import Pooling2D, Pooling2DConf\n\nfrom .embedding import CNNCharEmbedding, CNNCharEmbeddingConf\nfrom .embedding import LSTMCharEmbedding, LSTMCharEmbeddingConf\n\nfrom .CRF import CRFConf, CRF\n\nfrom .attentions import FullAttention, FullAttentionConf\nfrom .attentions import Seq2SeqAttention, Seq2SeqAttentionConf\nfrom .attentions import LinearAttention, LinearAttentionConf       # The output rank of this layer can be either unchanged or reduced\nfrom .attentions import BiAttFlow, BiAttFlowConf\nfrom .attentions import MatchAttention, MatchAttentionConf\nfrom .attentions import Attention, AttentionConf\nfrom .attentions import BilinearAttention, BilinearAttentionConf\nfrom .attentions import Interaction, InteractionConf\n\n# Operators\nfrom .op import *\n\n# Math operations\nfrom .math import Add2D, Add2DConf\nfrom .math import Add3D, Add3DConf\nfrom .math import Minus2D, Minus2DConf\nfrom .math import Minus3D, Minus3DConf\nfrom .math import ElementWisedMultiply2D, ElementWisedMultiply2DConf\nfrom .math import ElementWisedMultiply3D, ElementWisedMultiply3DConf\nfrom .math import MatrixMultiply, MatrixMultiplyConf\n\n# Transformer layer\nfrom .Transformer import Transformer, TransformerConf\n\n# Encoder Decoder classes\nfrom .EncoderDecoder import EncoderDecoder, EncoderDecoderConf\n\nfrom .normalizations import LayerNorm, LayerNormConf\n\nfrom .HighwayLinear import HighwayLinear, HighwayLinearConf\n\n'"
core/CellDict.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport logging\nfrom collections import Counter\nimport os\nimport pickle as pkl\n\nclass CellDict(object):\n    def __init__(self, with_start=False, with_end=False, with_unk=False, with_pad=True):\n\n        self.cell_id_map = dict()\n        self.id_cell_map = dict()\n\n        self.cell_doc_count = Counter()\n\n        self.with_start = with_start\n        self.with_end = with_end\n        self.with_unk = with_unk\n        self.with_pad = with_pad\n\n        if with_pad:\n            # 0 is reserved for padding\n            self.cell_id_map[""<pad>""] = 0\n            self.id_cell_map[0] = ""<pad>""\n\n        if with_start:\n            id = len(self.cell_id_map)\n            self.cell_id_map[""<start>""] = id\n            self.id_cell_map[id] = ""<start>""\n\n        if with_end:\n            id = len(self.cell_id_map)\n            self.cell_id_map[""<eos>""] = id\n            self.id_cell_map[id] = ""<eos>""\n\n        if with_unk:\n            id = len(self.cell_id_map)\n            self.cell_id_map[""<unk>""] = id\n            self.id_cell_map[id] = ""<unk>""\n\n    def iter_ids(self):\n\n        for id in self.id_cell_map:\n            yield id\n\n    def iter_cells(self):\n\n        for cell in self.cell_id_map:\n            yield cell\n\n    def id(self, cell):\n        return self.cell_id_map[cell]\n\n    def cell(self, id):\n        return self.id_cell_map[id]\n\n    def cell_num(self):\n        return len(self.id_cell_map)\n\n    def has_id(self, id):\n\n        return id in self.id_cell_map\n\n    def has_cell(self, cell):\n\n        return cell in self.cell_id_map\n\n    def update(self, docs):\n        for doc in docs:\n            # add type judge\n            if isinstance(doc, list):\n                self.cell_doc_count.update(set(doc))\n            else:\n                self.cell_doc_count.update([doc])\n        \n    def build(self, threshold, max_vocabulary_num=800000):\n        # restrict the vocabulary size to prevent embedding dict weight size\n        self.cell_doc_count = Counter(dict(self.cell_doc_count.most_common(max_vocabulary_num)))\n        for cell in self.cell_doc_count:\n            if cell not in self.cell_id_map and self.cell_doc_count[cell] >= threshold:\n                id = len(self.cell_id_map)\n                self.cell_id_map[cell] = id\n                self.id_cell_map[id] = cell\n        self.cell_doc_count = Counter()\n\n    def id_unk(self, cell):\n        """"""\n        return <unk> in cells not in cell_id_map dict\n        :param cell:\n        :return:\n        """"""\n        if cell in self.cell_id_map:\n            return self.cell_id_map[cell]\n        else:\n            return self.cell_id_map[\'<unk>\']\n\n    def load_from_exist_dict(self, word_dict,\n                             start_str=None,\n                             end_str=None,\n                             unk_str=None,\n                             pad_str=None):\n        """"""\n        build cells from exist dicts\n        :param word_dict:\n        :param start_str: start single in word_dict\n        :param end_str: end single in word_dict\n        :param unk_str:\n        :param pad_str:\n        :return:\n        """"""\n        id2word_dict = dict()\n        if pad_str:\n            # 0 is reserved for padding\n            id = word_dict[pad_str]\n            word_dict[""<pad>""] = id\n            del (word_dict[pad_str])\n            self.with_pad = True\n\n        if start_str:\n            id = word_dict[start_str]\n            word_dict[""<start>""] = id\n            del(word_dict[start_str])\n            self.with_start = True\n\n        if end_str:\n            id = word_dict[end_str]\n            word_dict[""<eos>""] = id\n            del(word_dict[end_str])\n            self.with_end = True\n\n        if unk_str:\n            id = word_dict[unk_str]\n            word_dict[""<unk>""] = id\n            del(word_dict[unk_str])\n            self.with_unk = True\n\n        for word, id in word_dict.iteritems():\n            id2word_dict[id] = word\n        self.cell_id_map = word_dict\n        self.id_cell_map = id2word_dict\n\n    def add_cells(self, cells):\n\n        for cell in cells:\n            if cell not in self.cell_id_map:\n                id = len(self.cell_id_map)\n                self.cell_id_map[cell] = id\n                self.id_cell_map[id] = cell\n\n    def lookup(self, cells):\n        """""" get id of cells. if cell in dict, return id, else update the dict and return id\n\n        Args:\n            cells:\n\n        Returns:\n\n        """"""\n        cur_corpus = [0] * len(cells)\n\n        for idx, cell in enumerate(cells):\n\n            if cell not in self.cell_id_map:\n\n                if self.with_unk:\n                    id = self.id(""<unk>"")\n                else:\n                    logging.error(\n                        u""Unknown cells {0} found. the repo ""\n                        u""should build with with_unk = True."".format(\n                            cell).encode(\'utf8\'))\n                    raise Exception(""Unknown cells found with with_unk=False"")\n            else:\n                id = self.cell_id_map[cell]\n\n            cur_corpus[idx] = id\n\n        return cur_corpus\n\n    def decode(self, ids):\n        """""" look up the cell for a list of ids\n\n        Args:\n            ids: list or 1d np array\n\n        Returns:\n\n        """"""\n        return [self.cell(id) for id in ids]\n\n\n    def export_cell_dict(self, save_path=None):\n        if save_path is not None and not os.path.exists(os.path.dirname(save_path)):\n            os.makedirs(os.path.dirname(save_path))\n\n        cell_dict = dict()\n        for name, value in vars(self).items():\n            if name.startswith(""__"") is False:\n                cell_dict[name] = value\n\n        if save_path != None:\n            with open(save_path, \'wb\') as fout:\n                pkl.dump(cell_dict, fout, protocol=pkl.HIGHEST_PROTOCOL)\n            logging.debug(""Cell dict saved to %s"" % save_path)\n        return cell_dict\n\n    def load_cell_dict(self, info_dict):\n        for name in info_dict:\n            setattr(self, name, info_dict[name])\n        logging.debug(""Cell dict loaded"")\n\n\n\n'"
core/ChineseTokenizer.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\r\n# Licensed under the MIT license.\r\n\r\nimport jieba\r\nimport logging\r\njieba.setLogLevel(logging.INFO)\r\nfrom nltk.tokenize.util import align_tokens\r\nfrom .Stopwords import Stopwords\r\n\r\nclass ChineseTokenizer(object):\r\n    def __init__(self, tokenizer=\'jieba\', remove_stopwords=False):\r\n        self.__tokenizer = tokenizer\r\n        self.__remove_stopwords = remove_stopwords\r\n        if self.__remove_stopwords:\r\n            self.__stop_words = Stopwords.chinese_stopwords\r\n        else:\r\n            self.__stop_words = None\r\n\r\n    def tokenize(self, string):\r\n        if self.__tokenizer == \'jieba\':\r\n            tokens = list(jieba.cut(string))\r\n\r\n        if self.__remove_stopwords:\r\n            tokens = [word for word in tokens if word not in self.__stop_words]\r\n        return tokens\r\n\r\n    def span_tokenize(self, string):\r\n        if self.__tokenizer == \'jieba\':\r\n            tokens = self.tokenize(string)\r\n            spans = align_tokens(tokens, string)\r\n        return spans\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    import os\r\n    # nltk.data.path.append(r""C:\\Users\\wutlin\\OneDrive - Microsoft\\workspace\\DNNMatchingToolkit\\dataset\\nltk_data"")\r\n    tokenizer = ChineseTokenizer(tokenizer=\'jieba\', remove_stopwords=True)\r\n    print(tokenizer.tokenize(""\xe6\x88\x91\xe7\x88\xb1\xe5\x8c\x97\xe4\xba\xac\xe5\xa4\xa9\xe5\xae\x89\xe9\x97\xa8\xef\xbc\x8c\xe5\xa4\xa9\xe5\xae\x89\xe9\x97\xa8\xe4\xb8\x8a\xe5\xa4\xaa\xe9\x98\xb3\xe5\x8d\x87\xe3\x80\x82""))\r\n    print(tokenizer.span_tokenize(""\xe6\x88\x91\xe7\x88\xb1\xe5\x8c\x97\xe4\xba\xac\xe5\xa4\xa9\xe5\xae\x89\xe9\x97\xa8\xef\xbc\x8c\xe5\xa4\xa9\xe5\xae\x89\xe9\x97\xa8\xe4\xb8\x8a\xe5\xa4\xaa\xe9\x98\xb3\xe5\x8d\x87\xe3\x80\x82""))\r\n    print(tokenizer.tokenize(""\xe7\xbb\x99\xe6\xaf\x8f\xe4\xb8\x80\xe6\x9d\xa1\xe6\xb2\xb3\xe6\xaf\x8f\xe4\xb8\x80\xe5\xba\xa7\xe5\xb1\xb1\xe5\x8f\x96\xe4\xb8\x80\xe4\xb8\xaa\xe6\xb8\xa9\xe6\x9a\x96\xe7\x9a\x84\xe5\x90\x8d\xe5\xad\x97\xef\xbc\x9b\xe9\x99\x8c\xe7\x94\x9f\xe4\xba\xba\xef\xbc\x8c\xe6\x88\x91\xe4\xb9\x9f\xe4\xb8\xba\xe4\xbd\xa0\xe7\xa5\x9d\xe7\xa6\x8f\xef\xbc\x9b\xe6\x84\xbf\xe4\xbd\xa0\xe6\x9c\x89\xe4\xb8\x80\xe4\xb8\xaa\xe7\x81\xbf\xe7\x83\x82\xe7\x9a\x84\xe5\x89\x8d\xe7\xa8\x8b\xef\xbc\x9b\xe6\x84\xbf\xe4\xbd\xa0\xe6\x9c\x89\xe6\x83\x85\xe4\xba\xba\xe7\xbb\x88\xe6\x88\x90\xe7\x9c\xb7\xe5\xb1\x9e\xef\xbc\x9b\xe6\x84\xbf\xe4\xbd\xa0\xe5\x9c\xa8\xe5\xb0\x98\xe4\xb8\x96\xe8\x8e\xb7\xe5\xbe\x97\xe5\xb9\xb8\xe7\xa6\x8f\xef\xbc\x9b\xe6\x88\x91\xe5\x8f\xaa\xe6\x84\xbf\xe9\x9d\xa2\xe6\x9c\x9d\xe5\xa4\xa7\xe6\xb5\xb7\xef\xbc\x8c\xe6\x98\xa5\xe6\x9a\x96\xe8\x8a\xb1\xe5\xbc\x80\xe3\x80\x82""))\r\n'"
core/EnglishPOSTagger.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport os\n\'\'\'\nif not (""STANFORD_MODELS"" in os.environ and ""STANFORD_POSTAGGER_PATH"" in os.environ \\\n        and ""CLASSPATH"" in os.environ  \\\n        and os.environ[\'CLASSPATH\'].find(\'stanford-postagger.jar\') != -1):\n    raise Exception(""To use Stanford POS tagger, please set the corresponding environment ""\n        ""variables first"")\nfrom nltk.tag import StanfordPOSTagger\n\'\'\'\n#from nltk.tag import pos_tag, pos_tag_sents\nfrom nltk.tag.perceptron import PerceptronTagger\nimport nltk\nclass EnglishPOSTagger(object):\n    def __init__(self, model_type=\'english-bidirectional-distsim.tagger\'):\n        """"""\n        Args:\n            model:  model available in $STANFORD_MODELS:\n                english-bidirectional-distsim.tagger\n                english-caseless-left3words-distsim.tagger\n                english-left3words-distsim.tagger\n        """"""\n        #self.eng_tagger = StanfordPOSTagger(model_type, java_options=\'-mx16000m\')\n        self.eng_tagger = PerceptronTagger()\n\n    def postag(self, word_list):\n        """"""\n        Args:\n            word_list:  word list\n        Returns:\n            pos tag list\n        """"""\n        #word_pos_pairs = self.eng_tagger.tag(word_list)\n        \n        #word_pos_pairs = pos_tag(word_list)\n        word_pos_pairs = nltk.tag._pos_tag(word_list, None, self.eng_tagger)\n        pos_list = [pos for (word, pos) in word_pos_pairs]\n        return pos_list\n\n    def postag_multi(self, multi_sentence):\n        """""" tag multiple sentences one time\n        RECOMMAND! Because the efficiency of stanford pos tagger in NLTK is too slow.\n        Args:\n            multi_sentence: [[token1, token2], ..., [...]]\n        Returns:\n        """"""\n        #word_pos_pairs_multi_sent = self.eng_tagger.tag_sents(multi_sentence)\n        \'\'\'\n        word_pos_pairs_multi_sent = pos_tag_sents(multi_sentence)\n        pos_lists = []\n        for word_pos_pairs in word_pos_pairs_multi_sent:\n            pos_lists.append([pos for (word, pos) in word_pos_pairs])\n        return pos_lists\n        \'\'\'\n        return [self.postag(sent) for sent in multi_sentence]\n\n'"
core/EnglishTextPreprocessor.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport ftfy\n\n\nclass EnglishTextPreprocessor(object):\n    def __init__(self, DBC2SBC=True, unicode_fix=True):\n        self.__DBC2SBC = DBC2SBC\n        self.__unicode_fix = unicode_fix\n\n    def preprocess(self, string):\n        if self.__unicode_fix:\n            string = ftfy.fix_text(string)\n        if self.__DBC2SBC:\n            string = self.DBC2SBC(string)\n        return string\n\n    def DBC2SBC(self, ustring):\n        """""" DBC characters to SBC\n\n        Args:\n            ustring:\n\n        Returns:\n\n        """"""\n        rstring = """"\n        for uchar in ustring:\n            inside_code = ord(uchar)\n            if inside_code == 0x3000:\n                inside_code = 0x0020\n            else:\n                inside_code -= 0xfee0\n            if not (0x0021 <= inside_code and inside_code <= 0x7e):\n                rstring += uchar\n                continue\n            rstring += chr(inside_code)\n        return rstring\n\n    def SBC2DBC(ustring):\n        """""" SBC to DBC\n\n        Returns:\n\n        """"""\n        rstring = """"\n        for uchar in ustring:\n            inside_code = ord(uchar)\n            if inside_code == 0x0020:\n                inside_code = 0x3000\n            else:\n                if not (0x0021 <= inside_code and inside_code <= 0x7e):\n                    rstring += uchar\n                    continue\n                inside_code += 0xfee0\n            rstring += chr(inside_code)\n        return rstring'"
core/EnglishTokenizer.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport nltk\nimport re\nfrom nltk.tokenize.util import align_tokens\nfrom .Stopwords import Stopwords\n\nclass EnglishTokenizer(object):\n    def __init__(self, tokenizer=\'nltk\', remove_stopwords=False):\n        self.__tokenizer = tokenizer\n        self.__remove_stopwords = remove_stopwords\n        if self.__remove_stopwords:\n            self.__stop_words = Stopwords.english_stopwords\n        else:\n            self.__stop_words = None\n\n    def tokenize(self, string):\n        if self.__tokenizer == \'nltk\':\n            tokens = nltk.word_tokenize(string)\n\n        if self.__remove_stopwords:\n            tokens = [word for word in tokens if word not in self.__stop_words]\n        return tokens\n\n    def span_tokenize(self, string):\n        if self.__tokenizer == \'nltk\':\n            raw_tokens = nltk.word_tokenize(string)\n            if (\'""\' in string) or (""\'\'"" in string):\n                matched = [m.group() for m in re.finditer(r""``|\'{2}|\\"""", string)]\n                tokens = [matched.pop(0) if tok in [\'""\', ""``"", ""\'\'""] else tok for tok in raw_tokens]\n            else:\n                tokens = raw_tokens\n            spans = align_tokens(tokens, string)\n        return spans\n\n\nif __name__ == \'__main__\':\n    import os\n    # nltk.data.path.append(r""C:\\Users\\wutlin\\OneDrive - Microsoft\\workspace\\DNNMatchingToolkit\\dataset\\nltk_data"")\n    tokenizer = EnglishTokenizer(tokenizer=\'nltk\', remove_stopwords=True)\n    print(tokenizer.span_tokenize(""""""What singer did Beyonce record a song with for the movie, \'\'The Best Man""?""""""))'"
core/LRScheduler.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport numpy as np\n\nclass LRScheduler():\n    def __init__(self, optimizer, decay_rate=1, minimum_lr=0, epoch_start_decay=1):\n        """"""\n\n        Args:\n            optimizer:\n            decay_rate:\n            minimum_lr: if lr < minimum_lr, stop lr decay\n        """"""\n        self.optimizer = optimizer\n        self.decay_rate = decay_rate\n        self.minimum_lr = minimum_lr\n        self.epoch_cnt = 0\n        self.epoch_start_decay = epoch_start_decay\n\n    def step(self):\n        """""" adjust learning rate\n\n        Args:\n            optimizer:\n            decay_rate:\n            minimum_lr:\n\n        Returns:\n            None\n\n        """"""\n        self.epoch_cnt += 1\n\n        if self.epoch_cnt >= self.epoch_start_decay:\n            for param_group in self.optimizer.param_groups:\n                if param_group[\'lr\'] * self.decay_rate >= self.minimum_lr:\n                    param_group[\'lr\'] = param_group[\'lr\'] * self.decay_rate\n                else:\n                    param_group[\'lr\'] = self.minimum_lr\n\n\n    def get_lr(self):\n        """""" get average learning rate of optimizer.param_groups\n\n        Args:\n            optimizer:\n\n        Returns:\n\n        """"""\n        lr_total = []\n        for param_group in self.optimizer.param_groups:\n            lr_total.append(param_group[\'lr\'])\n        return np.mean(lr_total)\n'"
core/Stopwords.py,0,"b""# Copyright (c) Microsoft Corporation. All rights reserved.\r\n# Licensed under the MIT license.\r\n\r\nfrom nltk.corpus import stopwords\r\n\r\n\r\nclass Stopwords:\r\n    english_stopwords = stopwords.words('english')\r\n    chinese_stopwords = ['$', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '?', '_', '\xe2\x80\x9c', '\xe2\x80\x9d', '\xe3\x80\x81', '\xe3\x80\x82', '\xe3\x80\x8a', '\xe3\x80\x8b', '\xe4\xb8\x80', '\xe4\xb8\x80\xe4\xba\x9b', '\xe4\xb8\x80\xe4\xbd\x95', '\xe4\xb8\x80\xe5\x88\x87', '\xe4\xb8\x80\xe5\x88\x99', '\xe4\xb8\x80\xe6\x96\xb9\xe9\x9d\xa2', '\xe4\xb8\x80\xe6\x97\xa6', '\xe4\xb8\x80\xe6\x9d\xa5', '\xe4\xb8\x80\xe6\xa0\xb7', '\xe4\xb8\x80\xe8\x88\xac', '\xe4\xb8\x80\xe8\xbd\xac\xe7\x9c\xbc', '\xe4\xb8\x87\xe4\xb8\x80', '\xe4\xb8\x8a', '\xe4\xb8\x8a\xe4\xb8\x8b', '\xe4\xb8\x8b', '\xe4\xb8\x8d', '\xe4\xb8\x8d\xe4\xbb\x85', '\xe4\xb8\x8d\xe4\xbd\x86', '\xe4\xb8\x8d\xe5\x85\x89', '\xe4\xb8\x8d\xe5\x8d\x95', '\xe4\xb8\x8d\xe5\x8f\xaa', '\xe4\xb8\x8d\xe5\xa4\x96\xe4\xb9\x8e', '\xe4\xb8\x8d\xe5\xa6\x82', '\xe4\xb8\x8d\xe5\xa6\xa8', '\xe4\xb8\x8d\xe5\xb0\xbd', '\xe4\xb8\x8d\xe5\xb0\xbd\xe7\x84\xb6', '\xe4\xb8\x8d\xe5\xbe\x97', '\xe4\xb8\x8d\xe6\x80\x95', '\xe4\xb8\x8d\xe6\x83\x9f', '\xe4\xb8\x8d\xe6\x88\x90', '\xe4\xb8\x8d\xe6\x8b\x98', '\xe4\xb8\x8d\xe6\x96\x99', '\xe4\xb8\x8d\xe6\x98\xaf', '\xe4\xb8\x8d\xe6\xaf\x94', '\xe4\xb8\x8d\xe7\x84\xb6', '\xe4\xb8\x8d\xe7\x89\xb9', '\xe4\xb8\x8d\xe7\x8b\xac', '\xe4\xb8\x8d\xe7\xae\xa1', '\xe4\xb8\x8d\xe8\x87\xb3\xe4\xba\x8e', '\xe4\xb8\x8d\xe8\x8b\xa5', '\xe4\xb8\x8d\xe8\xae\xba', '\xe4\xb8\x8d\xe8\xbf\x87', '\xe4\xb8\x8d\xe9\x97\xae', '\xe4\xb8\x8e', '\xe4\xb8\x8e\xe5\x85\xb6', '\xe4\xb8\x8e\xe5\x85\xb6\xe8\xaf\xb4', '\xe4\xb8\x8e\xe5\x90\xa6', '\xe4\xb8\x8e\xe6\xad\xa4\xe5\x90\x8c\xe6\x97\xb6', '\xe4\xb8\x94', '\xe4\xb8\x94\xe4\xb8\x8d\xe8\xaf\xb4', ' \xe4\xb8\x94\xe8\xaf\xb4', '\xe4\xb8\xa4\xe8\x80\x85', '\xe4\xb8\xaa', '\xe4\xb8\xaa\xe5\x88\xab', '\xe4\xb8\xb4', '\xe4\xb8\xba', '\xe4\xb8\xba\xe4\xba\x86', '\xe4\xb8\xba\xe4\xbb\x80\xe4\xb9\x88', '\xe4\xb8\xba\xe4\xbd\x95', '\xe4\xb8\xba\xe6\xad\xa2', '\xe4\xb8\xba\xe6\xad\xa4', '\xe4\xb8\xba\xe7\x9d\x80', '\xe4\xb9\x83', '\xe4\xb9\x83\xe8\x87\xb3', '\xe4\xb9\x83\xe8\x87\xb3\xe4\xba\x8e', '\xe4\xb9\x88', '\xe4\xb9\x8b', '\xe4\xb9\x8b\xe4\xb8\x80', '\xe4\xb9\x8b\xe6\x89\x80\xe4\xbb\xa5', '\xe4\xb9\x8b\xe7\xb1\xbb', '\xe4\xb9\x8c\xe4\xb9\x8e', '\xe4\xb9\x8e', '\xe4\xb9\x98', '\xe4\xb9\x9f', '\xe4\xb9\x9f\xe5\xa5\xbd', '\xe4\xb9\x9f\xe7\xbd\xa2', '\xe4\xba\x86', '\xe4\xba\x8c\xe6\x9d\xa5', '\xe4\xba\x8e', '\xe4\xba\x8e\xe6\x98\xaf', '\xe4\xba\x8e\xe6\x98\xaf\xe4\xb9\x8e', '\xe4\xba\x91\xe4\xba\x91', '\xe4\xba\x91\xe5\xb0\x94', '\xe4\xba\x9b', '\xe4\xba\xa6', '\xe4\xba\xba', '\xe4\xba\xba\xe4\xbb\xac', '\xe4\xba\xba\xe5\xae\xb6', '\xe4\xbb\x80\xe4\xb9\x88', '\xe4\xbb\x80\xe4\xb9\x88\xe6\xa0\xb7', '\xe4\xbb\x8a', '\xe4\xbb\x8b\xe4\xba\x8e', '\xe4\xbb\x8d', '\xe4\xbb\x8d\xe6\x97\xa7', '\xe4\xbb\x8e', '\xe4\xbb\x8e\xe6\xad\xa4', '\xe4\xbb\x8e\xe8\x80\x8c', '\xe4\xbb\x96', '\xe4\xbb\x96\xe4\xba\xba', '\xe4\xbb\x96\xe4\xbb\xac', '\xe4\xbb\xa5', ' \xe4\xbb\xa5\xe4\xb8\x8a', '\xe4\xbb\xa5\xe4\xb8\xba', '\xe4\xbb\xa5\xe4\xbe\xbf', '\xe4\xbb\xa5\xe5\x85\x8d', '\xe4\xbb\xa5\xe5\x8f\x8a', '\xe4\xbb\xa5\xe6\x95\x85', '\xe4\xbb\xa5\xe6\x9c\x9f', '\xe4\xbb\xa5\xe6\x9d\xa5', '\xe4\xbb\xa5\xe8\x87\xb3', '\xe4\xbb\xa5\xe8\x87\xb3\xe4\xba\x8e', '\xe4\xbb\xa5\xe8\x87\xb4', '\xe4\xbb\xac', '\xe4\xbb\xbb', '\xe4\xbb\xbb\xe4\xbd\x95', '\xe4\xbb\xbb\xe5\x87\xad', '\xe4\xbc\xbc\xe7\x9a\x84', ' \xe4\xbd\x86', '\xe4\xbd\x86\xe5\x87\xa1', '\xe4\xbd\x86\xe6\x98\xaf', '\xe4\xbd\x95', '\xe4\xbd\x95\xe4\xbb\xa5', '\xe4\xbd\x95\xe5\x86\xb5', '\xe4\xbd\x95\xe5\xa4\x84', '\xe4\xbd\x95\xe6\x97\xb6', '\xe4\xbd\x99\xe5\xa4\x96', '\xe4\xbd\x9c\xe4\xb8\xba', '\xe4\xbd\xa0', '\xe4\xbd\xa0\xe4\xbb\xac', '\xe4\xbd\xbf', '\xe4\xbd\xbf\xe5\xbe\x97', '\xe4\xbe\x8b\xe5\xa6\x82', '\xe4\xbe\x9d', '\xe4\xbe\x9d\xe6\x8d\xae', ' \xe4\xbe\x9d\xe7\x85\xa7', '\xe4\xbe\xbf\xe4\xba\x8e', '\xe4\xbf\xba', '\xe4\xbf\xba\xe4\xbb\xac', '\xe5\x80\x98', '\xe5\x80\x98\xe4\xbd\xbf', '\xe5\x80\x98\xe6\x88\x96', '\xe5\x80\x98\xe7\x84\xb6', '\xe5\x80\x98\xe8\x8b\xa5', '\xe5\x80\x9f', '\xe5\x81\x87\xe4\xbd\xbf', '\xe5\x81\x87\xe5\xa6\x82', '\xe5\x81\x87\xe8\x8b\xa5', '\xe5\x82\xa5\xe7\x84\xb6', '\xe5\x83\x8f', '\xe5\x84\xbf', '\xe5\x85\x88\xe4\xb8\x8d\xe5\x85\x88', '\xe5\x85\x89\xe6\x98\xaf', '\xe5\x85\xa8\xe4\xbd\x93', '\xe5\x85\xa8\xe9\x83\xa8', '\xe5\x85\xae', '\xe5\x85\xb3\xe4\xba\x8e', '\xe5\x85\xb6', '\xe5\x85\xb6\xe4\xb8\x80', '\xe5\x85\xb6\xe4\xb8\xad', '\xe5\x85\xb6\xe4\xba\x8c', '\xe5\x85\xb6\xe4\xbb\x96', '\xe5\x85\xb6\xe4\xbd\x99', '\xe5\x85\xb6\xe5\xae\x83', '\xe5\x85\xb6\xe6\xac\xa1', '\xe5\x85\xb7\xe4\xbd\x93\xe5\x9c\xb0\xe8\xaf\xb4', '\xe5\x85\xb7\xe4\xbd\x93\xe8\xaf\xb4\xe6\x9d\xa5', '\xe5\x85\xbc\xe4\xb9\x8b', '\xe5\x86\x85', '\xe5\x86\x8d', '\xe5\x86\x8d\xe5\x85\xb6\xe6\xac\xa1', '\xe5\x86\x8d\xe5\x88\x99', '\xe5\x86\x8d\xe6\x9c\x89', '\xe5\x86\x8d\xe8\x80\x85', '\xe5\x86\x8d\xe8\x80\x85\xe8\xaf\xb4', '\xe5\x86\x8d\xe8\xaf\xb4', '\xe5\x86\x92', '\xe5\x86\xb2', '\xe5\x86\xb5\xe4\xb8\x94', '\xe5\x87\xa0', '\xe5\x87\xa0\xe6\x97\xb6', '\xe5\x87\xa1', '\xe5\x87\xa1\xe6\x98\xaf', '\xe5\x87\xad', '\xe5\x87\xad \xe5\x80\x9f', '\xe5\x87\xba\xe4\xba\x8e', '\xe5\x87\xba\xe6\x9d\xa5', '\xe5\x88\x86\xe5\x88\xab', '\xe5\x88\x99', '\xe5\x88\x99\xe7\x94\x9a', '\xe5\x88\xab', '\xe5\x88\xab\xe4\xba\xba', '\xe5\x88\xab\xe5\xa4\x84', '\xe5\x88\xab\xe6\x98\xaf', '\xe5\x88\xab\xe7\x9a\x84', '\xe5\x88\xab\xe7\xae\xa1', '\xe5\x88\xab\xe8\xaf\xb4', '\xe5\x88\xb0', '\xe5\x89\x8d\xe5\x90\x8e', '\xe5\x89\x8d\xe6\xad\xa4', '\xe5\x89\x8d\xe8\x80\x85', '\xe5\x8a\xa0\xe4\xb9\x8b', '\xe5\x8a\xa0\xe4\xbb\xa5', '\xe5\x8d\xb3', '\xe5\x8d\xb3\xe4\xbb\xa4', '\xe5\x8d\xb3\xe4\xbd\xbf', '\xe5\x8d\xb3\xe4\xbe\xbf', '\xe5\x8d\xb3\xe5\xa6\x82', '\xe5\x8d\xb3\xe6\x88\x96', '\xe5\x8d\xb3\xe8\x8b\xa5', '\xe5\x8d\xb4', '\xe5\x8e\xbb', '\xe5\x8f\x88', '\xe5\x8f\x88\xe5\x8f\x8a', '\xe5\x8f\x8a', '\xe5\x8f\x8a\xe5\x85\xb6', '\xe5\x8f\x8a\xe8\x87\xb3', '\xe5\x8f\x8d\xe4\xb9\x8b', '\xe5\x8f\x8d\xe8\x80\x8c', '\xe5\x8f\x8d\xe8\xbf\x87\xe6\x9d\xa5', '\xe5\x8f\x8d\xe8\xbf\x87\xe6\x9d\xa5\xe8\xaf\xb4', '\xe5\x8f\x97\xe5\x88\xb0', '\xe5\x8f\xa6', '\xe5\x8f\xa6\xe4\xb8\x80\xe6\x96\xb9\xe9\x9d\xa2', '\xe5\x8f\xa6\xe5\xa4\x96', '\xe5\x8f\xa6\xe6\x82\x89', '\xe5\x8f\xaa', '\xe5\x8f\xaa\xe5\xbd\x93', '\xe5\x8f\xaa\xe6\x80\x95', '\xe5\x8f\xaa\xe6\x98\xaf', '\xe5\x8f\xaa\xe6\x9c\x89', '\xe5\x8f\xaa\xe6\xb6\x88', '\xe5\x8f\xaa\xe8\xa6\x81', ' \xe5\x8f\xaa\xe9\x99\x90', '\xe5\x8f\xab', '\xe5\x8f\xae\xe5\x92\x9a', '\xe5\x8f\xaf', '\xe5\x8f\xaf\xe4\xbb\xa5', '\xe5\x8f\xaf\xe6\x98\xaf', '\xe5\x8f\xaf\xe8\xa7\x81', '\xe5\x90\x84', '\xe5\x90\x84\xe4\xb8\xaa', '\xe5\x90\x84\xe4\xbd\x8d', '\xe5\x90\x84\xe7\xa7\x8d', '\xe5\x90\x84\xe8\x87\xaa', '\xe5\x90\x8c', '\xe5\x90\x8c\xe6\x97\xb6', '\xe5\x90\x8e', '\xe5\x90\x8e\xe8\x80\x85', '\xe5\x90\x91', '\xe5\x90\x91 \xe4\xbd\xbf', '\xe5\x90\x91\xe7\x9d\x80', '\xe5\x90\x93', '\xe5\x90\x97', '\xe5\x90\xa6\xe5\x88\x99', '\xe5\x90\xa7', '\xe5\x90\xa7\xe5\x93\x92', '\xe5\x90\xb1', '\xe5\x91\x80', '\xe5\x91\x83', '\xe5\x91\x95', '\xe5\x91\x97', '\xe5\x91\x9c', '\xe5\x91\x9c\xe5\x91\xbc', '\xe5\x91\xa2', '\xe5\x91\xb5', '\xe5\x91\xb5\xe5\x91\xb5', '\xe5\x91\xb8', '\xe5\x91\xbc\xe5\x93\xa7', ' \xe5\x92\x8b', '\xe5\x92\x8c', '\xe5\x92\x9a', '\xe5\x92\xa6', '\xe5\x92\xa7', '\xe5\x92\xb1', '\xe5\x92\xb1\xe4\xbb\xac', '\xe5\x92\xb3', '\xe5\x93\x87', '\xe5\x93\x88', '\xe5\x93\x88\xe5\x93\x88', '\xe5\x93\x89', '\xe5\x93\x8e', '\xe5\x93\x8e\xe5\x91\x80', '\xe5\x93\x8e\xe5\x93\x9f', '\xe5\x93\x97', '\xe5\x93\x9f', '\xe5\x93\xa6', '\xe5\x93\xa9', '\xe5\x93\xaa', '\xe5\x93\xaa\xe4\xb8\xaa', '\xe5\x93\xaa\xe4\xba\x9b', '\xe5\x93\xaa\xe5\x84\xbf', '\xe5\x93\xaa\xe5\xa4\xa9', '\xe5\x93\xaa\xe5\xb9\xb4', '\xe5\x93\xaa\xe6\x80\x95', '\xe5\x93\xaa\xe6\xa0\xb7', '\xe5\x93\xaa\xe8\xbe\xb9', '\xe5\x93\xaa\xe9\x87\x8c', '\xe5\x93\xbc', '\xe5\x93\xbc\xe5\x94\xb7', '\xe5\x94\x89', '\xe5\x94\xaf\xe6\x9c\x89', '\xe5\x95\x8a', '\xe5\x95\x90', '\xe5\x95\xa5', '\xe5\x95\xa6', '\xe5\x95\xaa\xe8\xbe\xbe', '\xe5\x95\xb7\xe5\xbd\x93', '\xe5\x96\x82', '\xe5\x96\x8f', '\xe5\x96\x94\xe5\x94\xb7', '\xe5\x96\xbd', '\xe5\x97\xa1', '\xe5\x97\xa1\xe5\x97\xa1', '\xe5\x97\xac', '\xe5\x97\xaf', '\xe5\x97\xb3', '\xe5\x98\x8e', '\xe5\x98\x8e\xe7\x99\xbb', '\xe5\x98\x98', '\xe5\x98\x9b', '\xe5\x98\xbb', '\xe5\x98\xbf', '\xe5\x98\xbf\xe5\x98\xbf', '\xe5\x9b\xa0', '\xe5\x9b\xa0 \xe4\xb8\xba', '\xe5\x9b\xa0\xe4\xba\x86', '\xe5\x9b\xa0\xe6\xad\xa4', '\xe5\x9b\xa0\xe7\x9d\x80', '\xe5\x9b\xa0\xe8\x80\x8c', '\xe5\x9b\xba\xe7\x84\xb6', '\xe5\x9c\xa8', '\xe5\x9c\xa8\xe4\xb8\x8b', '\xe5\x9c\xa8\xe4\xba\x8e', '\xe5\x9c\xb0', '\xe5\x9f\xba\xe4\xba\x8e', '\xe5\xa4\x84\xe5\x9c\xa8', '\xe5\xa4\x9a', '\xe5\xa4\x9a\xe4\xb9\x88', '\xe5\xa4\x9a\xe5\xb0\x91', '\xe5\xa4\xa7', '\xe5\xa4\xa7\xe5\xae\xb6', ' \xe5\xa5\xb9', '\xe5\xa5\xb9\xe4\xbb\xac', '\xe5\xa5\xbd', '\xe5\xa6\x82', '\xe5\xa6\x82\xe4\xb8\x8a', '\xe5\xa6\x82\xe4\xb8\x8a\xe6\x89\x80\xe8\xbf\xb0', '\xe5\xa6\x82\xe4\xb8\x8b', '\xe5\xa6\x82\xe4\xbd\x95', '\xe5\xa6\x82\xe5\x85\xb6', '\xe5\xa6\x82\xe5\x90\x8c', '\xe5\xa6\x82\xe6\x98\xaf', '\xe5\xa6\x82\xe6\x9e\x9c', '\xe5\xa6\x82\xe6\xad\xa4', '\xe5\xa6\x82\xe8\x8b\xa5', '\xe5\xa7\x8b\xe8\x80\x8c', '\xe5\xad\xb0\xe6\x96\x99', ' \xe5\xad\xb0\xe7\x9f\xa5', '\xe5\xae\x81', '\xe5\xae\x81\xe5\x8f\xaf', '\xe5\xae\x81\xe6\x84\xbf', '\xe5\xae\x81\xe8\x82\xaf', '\xe5\xae\x83', '\xe5\xae\x83\xe4\xbb\xac', '\xe5\xaf\xb9', '\xe5\xaf\xb9\xe4\xba\x8e', '\xe5\xaf\xb9\xe5\xbe\x85', '\xe5\xaf\xb9\xe6\x96\xb9', '\xe5\xaf\xb9\xe6\xaf\x94', '\xe5\xb0\x86', '\xe5\xb0\x8f', '\xe5\xb0\x94', '\xe5\xb0\x94\xe5\x90\x8e', '\xe5\xb0\x94\xe5\xb0\x94', '\xe5\xb0\x9a \xe4\xb8\x94', '\xe5\xb0\xb1', '\xe5\xb0\xb1\xe6\x98\xaf', '\xe5\xb0\xb1\xe6\x98\xaf\xe4\xba\x86', '\xe5\xb0\xb1\xe6\x98\xaf\xe8\xaf\xb4', '\xe5\xb0\xb1\xe7\xae\x97', '\xe5\xb0\xb1\xe8\xa6\x81', '\xe5\xb0\xbd', '\xe5\xb0\xbd\xe7\xae\xa1', '\xe5\xb0\xbd\xe7\xae\xa1\xe5\xa6\x82\xe6\xad\xa4', '\xe5\xb2\x82\xe4\xbd\x86', '\xe5\xb7\xb1', '\xe5\xb7\xb2', '\xe5\xb7\xb2\xe7\x9f\xa3', '\xe5\xb7\xb4', '\xe5\xb7\xb4\xe5\xb7\xb4', '\xe5\xb9\xb6', '\xe5\xb9\xb6\xe4\xb8\x94', '\xe5\xb9\xb6\xe9\x9d\x9e', '\xe5\xba\xb6\xe4\xb9\x8e', '\xe5\xba\xb6\xe5\x87\xa0', '\xe5\xbc\x80\xe5\xa4\x96', '\xe5\xbc\x80\xe5\xa7\x8b', '\xe5\xbd\x92', '\xe5\xbd\x92\xe9\xbd\x90', '\xe5\xbd\x93', '\xe5\xbd\x93\xe5\x9c\xb0', '\xe5\xbd\x93\xe7\x84\xb6', '\xe5\xbd\x93\xe7\x9d\x80', '\xe5\xbd\xbc', '\xe5\xbd\xbc\xe6\x97\xb6', '\xe5\xbd\xbc\xe6\xad\xa4', '\xe5\xbe\x80', '\xe5\xbe\x85', '\xe5\xbe\x88', '\xe5\xbe\x97', '\xe5\xbe\x97\xe4\xba\x86', '\xe6\x80\x8e', '\xe6\x80\x8e\xe4\xb9\x88', '\xe6\x80\x8e\xe4\xb9\x88\xe5\x8a\x9e', '\xe6\x80\x8e\xe4\xb9\x88\xe6\xa0\xb7', '\xe6\x80\x8e\xe5\xa5\x88', '\xe6\x80\x8e\xe6\xa0\xb7', '\xe6\x80\xbb\xe4\xb9\x8b', '\xe6\x80\xbb\xe7\x9a\x84\xe6\x9d\xa5\xe7\x9c\x8b', '\xe6\x80\xbb\xe7\x9a\x84\xe6\x9d\xa5\xe8\xaf\xb4', '\xe6\x80\xbb\xe7\x9a\x84\xe8\xaf\xb4\xe6\x9d\xa5', '\xe6\x80\xbb\xe8\x80\x8c\xe8\xa8\x80\xe4\xb9\x8b', '\xe6\x81\xb0\xe6\x81\xb0\xe7\x9b\xb8\xe5\x8f\x8d', '\xe6\x82\xa8', '\xe6\x83\x9f\xe5\x85\xb6', '\xe6\x85\xa2\xe8\xaf\xb4', '\xe6\x88\x91', '\xe6\x88\x91\xe4\xbb\xac', '\xe6\x88\x96', '\xe6\x88\x96\xe5\x88\x99', '\xe6\x88\x96\xe6\x98\xaf', '\xe6\x88\x96\xe6\x9b\xb0', '\xe6\x88\x96\xe8\x80\x85', '\xe6\x88\xaa\xe8\x87\xb3', '\xe6\x89\x80', '\xe6\x89\x80\xe4\xbb\xa5', '\xe6\x89\x80\xe5\x9c\xa8', '\xe6\x89\x80\xe5\xb9\xb8', '\xe6\x89\x80\xe6\x9c\x89', '\xe6\x89\x8d', '\xe6\x89\x8d\xe8\x83\xbd', '\xe6\x89\x93', '\xe6\x89\x93\xe4\xbb\x8e', '\xe6\x8a\x8a', '\xe6\x8a\x91\xe6\x88\x96', '\xe6\x8b\xbf', '\xe6\x8c\x89', '\xe6\x8c\x89\xe7\x85\xa7', '\xe6\x8d\xa2\xe5\x8f\xa5\xe8\xaf\x9d\xe8\xaf\xb4', '\xe6\x8d\xa2\xe8\xa8\x80\xe4\xb9\x8b', '\xe6\x8d\xae', '\xe6\x8d\xae\xe6\xad\xa4', '\xe6\x8e\xa5\xe7\x9d\x80', '\xe6\x95\x85', '\xe6\x95\x85\xe6\xad\xa4', ' \xe6\x95\x85\xe8\x80\x8c', '\xe6\x97\x81\xe4\xba\xba', '\xe6\x97\xa0', '\xe6\x97\xa0\xe5\xae\x81', '\xe6\x97\xa0\xe8\xae\xba', '\xe6\x97\xa2', '\xe6\x97\xa2\xe5\xbe\x80', '\xe6\x97\xa2\xe6\x98\xaf', '\xe6\x97\xa2\xe7\x84\xb6', '\xe6\x97\xb6\xe5\x80\x99', '\xe6\x98\xaf', '\xe6\x98\xaf\xe4\xbb\xa5', '\xe6\x98\xaf\xe7\x9a\x84', '\xe6\x9b\xbe', '\xe6\x9b\xbf', '\xe6\x9b\xbf\xe4\xbb\xa3', '\xe6\x9c\x80', '\xe6\x9c\x89', '\xe6\x9c\x89\xe4\xba\x9b', '\xe6\x9c\x89\xe5\x85\xb3', '\xe6\x9c\x89\xe5\x8f\x8a', '\xe6\x9c\x89\xe6\x97\xb6', '\xe6\x9c\x89\xe7\x9a\x84', '\xe6\x9c\x9b', '\xe6\x9c\x9d', '\xe6\x9c\x9d\xe7\x9d\x80', '\xe6\x9c\xac', '\xe6\x9c\xac\xe4\xba\xba', '\xe6\x9c\xac\xe5\x9c\xb0', '\xe6\x9c\xac\xe7\x9d\x80', '\xe6\x9c\xac\xe8\xba\xab', '\xe6\x9d\xa5', '\xe6\x9d\xa5\xe7\x9d\x80', '\xe6\x9d\xa5\xe8\x87\xaa', '\xe6\x9d\xa5\xe8\xaf\xb4', '\xe6\x9e\x81\xe4\xba\x86', '\xe6\x9e\x9c\xe7\x84\xb6', '\xe6\x9e\x9c\xe7\x9c\x9f', '\xe6\x9f\x90', '\xe6\x9f\x90\xe4\xb8\xaa', '\xe6\x9f\x90\xe4\xba\x9b', '\xe6\x9f\x90\xe6\x9f\x90', '\xe6\xa0\xb9\xe6\x8d\xae', '\xe6\xac\xa4', '\xe6\xad\xa3\xe5\x80\xbc', '\xe6\xad\xa3\xe5\xa6\x82', '\xe6\xad\xa3\xe5\xb7\xa7', '\xe6\xad\xa3\xe6\x98\xaf', '\xe6\xad\xa4', '\xe6\xad\xa4\xe5\x9c\xb0', '\xe6\xad\xa4\xe5\xa4\x84', ' \xe6\xad\xa4\xe5\xa4\x96', '\xe6\xad\xa4\xe6\x97\xb6', '\xe6\xad\xa4\xe6\xac\xa1', '\xe6\xad\xa4\xe9\x97\xb4', '\xe6\xaf\x8b\xe5\xae\x81', '\xe6\xaf\x8f', '\xe6\xaf\x8f\xe5\xbd\x93', '\xe6\xaf\x94', '\xe6\xaf\x94\xe5\x8f\x8a', '\xe6\xaf\x94\xe5\xa6\x82', '\xe6\xaf\x94\xe6\x96\xb9', '\xe6\xb2\xa1\xe5\xa5\x88\xe4\xbd\x95', '\xe6\xb2\xbf', '\xe6\xb2\xbf\xe7\x9d\x80', '\xe6\xbc\xab\xe8\xaf\xb4', '\xe7\x84\x89', '\xe7\x84\xb6\xe5\x88\x99', '\xe7\x84\xb6\xe5\x90\x8e', '\xe7\x84\xb6\xe8\x80\x8c', '\xe7\x85\xa7', '\xe7\x85\xa7\xe7\x9d\x80', '\xe7\x8a\xb9\xe4\xb8\x94', '\xe7\x8a\xb9\xe8\x87\xaa', '\xe7\x94\x9a\xe4\xb8\x94', '\xe7\x94\x9a\xe4\xb9\x88', '\xe7\x94\x9a\xe6\x88\x96', '\xe7\x94\x9a\xe8\x80\x8c', '\xe7\x94\x9a\xe8\x87\xb3', '\xe7\x94\x9a\xe8\x87\xb3\xe4\xba\x8e', '\xe7\x94\xa8', '\xe7\x94\xa8\xe6\x9d\xa5', '\xe7\x94\xb1', '\xe7\x94\xb1\xe4\xba\x8e', '\xe7\x94\xb1\xe6\x98\xaf', '\xe7\x94\xb1\xe6\xad\xa4', '\xe7\x94\xb1\xe6\xad\xa4\xe5\x8f\xaf\xe8\xa7\x81', '\xe7\x9a\x84', '\xe7\x9a\x84\xe7\xa1\xae', '\xe7\x9a\x84\xe8\xaf\x9d', '\xe7\x9b\xb4\xe5\x88\xb0', '\xe7\x9b\xb8\xe5\xaf\xb9\xe8\x80\x8c\xe8\xa8\x80', '\xe7\x9c\x81\xe5\xbe\x97', '\xe7\x9c\x8b', '\xe7\x9c\xa8\xe7\x9c\xbc', '\xe7\x9d\x80', '\xe7\x9d\x80\xe5\x91\xa2', '\xe7\x9f\xa3', '\xe7\x9f\xa3\xe4\xb9\x8e', '\xe7\x9f\xa3\xe5\x93\x89', '\xe7\xa6\xbb', '\xe7\xab\x9f\xe8\x80\x8c', '\xe7\xac\xac', '\xe7\xad\x89', '\xe7\xad\x89\xe5\x88\xb0', '\xe7\xad\x89\xe7\xad\x89', '\xe7\xae\x80\xe8\xa8\x80\xe4\xb9\x8b', '\xe7\xae\xa1', '\xe7\xb1\xbb\xe5\xa6\x82', '\xe7\xb4\xa7\xe6\x8e\xa5\xe7\x9d\x80', '\xe7\xba\xb5', '\xe7\xba\xb5\xe4\xbb\xa4', '\xe7\xba\xb5\xe4\xbd\xbf', '\xe7\xba\xb5\xe7\x84\xb6', '\xe7\xbb\x8f', '\xe7\xbb\x8f\xe8\xbf\x87', '\xe7\xbb\x93\xe6\x9e\x9c', '\xe7\xbb\x99', '\xe7\xbb\xa7\xe4\xb9\x8b', '\xe7\xbb\xa7\xe5\x90\x8e', '\xe7\xbb\xa7\xe8\x80\x8c', '\xe7\xbb\xbc\xe4\xb8\x8a\xe6\x89\x80\xe8\xbf\xb0', '\xe7\xbd\xa2\xe4\xba\x86', '\xe8\x80\x85', '\xe8\x80\x8c', '\xe8\x80\x8c\xe4\xb8\x94', '\xe8\x80\x8c\xe5\x86\xb5', '\xe8\x80\x8c\xe5\x90\x8e', '\xe8\x80\x8c\xe5\xa4\x96', '\xe8\x80\x8c\xe5\xb7\xb2', '\xe8\x80\x8c\xe6\x98\xaf', '\xe8\x80\x8c\xe8\xa8\x80', '\xe8\x83\xbd', ' \xe8\x83\xbd\xe5\x90\xa6', '\xe8\x85\xbe', '\xe8\x87\xaa', '\xe8\x87\xaa\xe4\xb8\xaa\xe5\x84\xbf', '\xe8\x87\xaa\xe4\xbb\x8e', '\xe8\x87\xaa\xe5\x90\x84\xe5\x84\xbf', '\xe8\x87\xaa\xe5\x90\x8e', '\xe8\x87\xaa\xe5\xae\xb6', '\xe8\x87\xaa\xe5\xb7\xb1', '\xe8\x87\xaa\xe6\x89\x93', '\xe8\x87\xaa\xe8\xba\xab', '\xe8\x87\xb3', '\xe8\x87\xb3\xe4\xba\x8e', '\xe8\x87\xb3\xe4\xbb\x8a', '\xe8\x87\xb3\xe8\x8b\xa5', '\xe8\x87\xb4', '\xe8\x88\xac \xe7\x9a\x84', '\xe8\x8b\xa5', '\xe8\x8b\xa5\xe5\xa4\xab', '\xe8\x8b\xa5\xe6\x98\xaf', '\xe8\x8b\xa5\xe6\x9e\x9c', '\xe8\x8b\xa5\xe9\x9d\x9e', '\xe8\x8e\xab\xe4\xb8\x8d\xe7\x84\xb6', '\xe8\x8e\xab\xe5\xa6\x82', '\xe8\x8e\xab\xe8\x8b\xa5', '\xe8\x99\xbd', '\xe8\x99\xbd\xe5\x88\x99', '\xe8\x99\xbd\xe7\x84\xb6', '\xe8\x99\xbd\xe8\xaf\xb4', '\xe8\xa2\xab', '\xe8\xa6\x81', '\xe8\xa6\x81\xe4\xb8\x8d', '\xe8\xa6\x81\xe4\xb8\x8d\xe6\x98\xaf', '\xe8\xa6\x81\xe4\xb8\x8d\xe7\x84\xb6', '\xe8\xa6\x81\xe4\xb9\x88', '\xe8\xa6\x81\xe6\x98\xaf', '\xe8\xad\xac\xe5\x96\xbb', '\xe8\xad\xac\xe5\xa6\x82', '\xe8\xae\xa9', '\xe8\xae\xb8\xe5\xa4\x9a', '\xe8\xae\xba', '\xe8\xae\xbe\xe4\xbd\xbf', '\xe8\xae\xbe\xe6\x88\x96', '\xe8\xae\xbe\xe8\x8b\xa5', '\xe8\xaf\x9a\xe5\xa6\x82', '\xe8\xaf\x9a\xe7\x84\xb6', '\xe8\xaf\xa5', '\xe8\xaf\xb4\xe6\x9d\xa5', '\xe8\xaf\xb8', '\xe8\xaf\xb8\xe4\xbd\x8d', '\xe8\xaf\xb8\xe5\xa6\x82', '\xe8\xb0\x81', '\xe8\xb0\x81\xe4\xba\xba', '\xe8\xb0\x81\xe6\x96\x99', '\xe8\xb0\x81\xe7\x9f\xa5', '\xe8\xb4\xbc\xe6\xad\xbb', '\xe8\xb5\x96\xe4\xbb\xa5', '\xe8\xb5\xb6', '\xe8\xb5\xb7', '\xe8\xb5\xb7\xe8\xa7\x81', '\xe8\xb6\x81', '\xe8\xb6\x81\xe7\x9d\x80', '\xe8\xb6\x8a\xe6\x98\xaf', '\xe8\xb7\x9d', '\xe8\xb7\x9f', '\xe8\xbe\x83', '\xe8\xbe\x83\xe4\xb9\x8b', '\xe8\xbe\xb9', '\xe8\xbf\x87', '\xe8\xbf\x98', '\xe8\xbf\x98\xe6\x98\xaf', '\xe8\xbf\x98\xe6\x9c\x89', '\xe8\xbf\x98\xe8\xa6\x81', '\xe8\xbf\x99', '\xe8\xbf\x99\xe4\xb8\x80\xe6\x9d\xa5', '\xe8\xbf\x99\xe4\xb8\xaa', '\xe8\xbf\x99\xe4\xb9\x88', '\xe8\xbf\x99\xe4\xb9\x88\xe4\xba\x9b', '\xe8\xbf\x99\xe4\xb9\x88\xe6\xa0\xb7', '\xe8\xbf\x99\xe4\xb9\x88\xe7\x82\xb9\xe5\x84\xbf', '\xe8\xbf\x99\xe4\xba\x9b', '\xe8\xbf\x99\xe4\xbc\x9a\xe5\x84\xbf', '\xe8\xbf\x99\xe5\x84\xbf', '\xe8\xbf\x99\xe5\xb0\xb1\xe6\x98\xaf\xe8\xaf\xb4', '\xe8\xbf\x99\xe6\x97\xb6', '\xe8\xbf\x99\xe6\xa0\xb7', '\xe8\xbf\x99\xe6\xac\xa1', '\xe8\xbf\x99\xe8\x88\xac', '\xe8\xbf\x99\xe8\xbe\xb9', '\xe8\xbf\x99\xe9\x87\x8c', '\xe8\xbf\x9b\xe8\x80\x8c', '\xe8\xbf\x9e', '\xe8\xbf\x9e\xe5\x90\x8c', '\xe9\x80\x90\xe6\xad\xa5', '\xe9\x80\x9a\xe8\xbf\x87', '\xe9\x81\xb5\xe5\xbe\xaa', '\xe9\x81\xb5\xe7\x85\xa7', '\xe9\x82\xa3', ' \xe9\x82\xa3\xe4\xb8\xaa', '\xe9\x82\xa3\xe4\xb9\x88', '\xe9\x82\xa3\xe4\xb9\x88\xe4\xba\x9b', '\xe9\x82\xa3\xe4\xb9\x88\xe6\xa0\xb7', '\xe9\x82\xa3\xe4\xba\x9b', '\xe9\x82\xa3\xe4\xbc\x9a\xe5\x84\xbf', '\xe9\x82\xa3\xe5\x84\xbf', '\xe9\x82\xa3\xe6\x97\xb6', '\xe9\x82\xa3\xe6\xa0\xb7', '\xe9\x82\xa3\xe8\x88\xac', '\xe9\x82\xa3\xe8\xbe\xb9', '\xe9\x82\xa3\xe9\x87\x8c', '\xe9\x83\xbd', '\xe9\x84\x99\xe4\xba\xba', '\xe9\x89\xb4\xe4\xba\x8e', '\xe9\x92\x88 \xe5\xaf\xb9', '\xe9\x98\xbf', '\xe9\x99\xa4', '\xe9\x99\xa4\xe4\xba\x86', '\xe9\x99\xa4\xe5\xa4\x96', '\xe9\x99\xa4\xe5\xbc\x80', '\xe9\x99\xa4\xe6\xad\xa4\xe4\xb9\x8b\xe5\xa4\x96', '\xe9\x99\xa4\xe9\x9d\x9e', '\xe9\x9a\x8f', '\xe9\x9a\x8f\xe5\x90\x8e', '\xe9\x9a\x8f\xe6\x97\xb6', '\xe9\x9a\x8f\xe7\x9d\x80', '\xe9\x9a\xbe\xe9\x81\x93\xe8\xaf\xb4', '\xe9\x9d\x9e\xe4\xbd\x86', '\xe9\x9d\x9e\xe5\xbe\x92', '\xe9\x9d\x9e\xe7\x89\xb9', ' \xe9\x9d\x9e\xe7\x8b\xac', '\xe9\x9d\xa0', '\xe9\xa1\xba', '\xe9\xa1\xba\xe7\x9d\x80', '\xe9\xa6\x96\xe5\x85\x88', '\xef\xbc\x81', '\xef\xbc\x8c', '\xef\xbc\x9a', '\xef\xbc\x9b', '\xef\xbc\x9f']\r\n"""
core/StreamingRecorder.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport numpy as np\n\nclass StreamingRecorder():\n    def __init__(self, names):\n        """"""\n\n        Args:\n            names:  [\'prediction\', ... ]\n        """"""\n        self.__names = names\n        self.__operators = dict()\n        self.__recorder = dict()\n        for name in names:\n            self.__recorder[name] = []\n\n    def record(self, name, values, keep_dim=False):\n        """""" insert a col of multiple values\n\n        Args:\n            name:\n            values:\n\n        Returns:\n\n        """"""\n        if isinstance(values, list) or isinstance(values, np.ndarray):\n            if keep_dim is False:\n                self.__recorder[name].extend(values)\n            else:\n                self.__recorder[name].append(values)\n        else:\n            self.__recorder[name].append(values)\n\n    def record_one_row(self, values, keep_dim=False):\n        """""" insert a whole row\n\n        Args:\n            values: [col1, col2, col3, ...], each element can be either a list or a single number\n\n        Returns:\n\n        """"""\n        assert len(self.__names) == len(values)\n        for name, value in zip(self.__names, values):\n            self.record(name, value, keep_dim)\n\n    def get(self, name, operator=None):\n        """"""\n\n        Args:\n            name:\n            operator: has the same shape with names, supported operations:\n                    None or \'origin\': return the original values\n                    \'mean\': return mean of the values\n                    \'sum\': return sum of the values\n                    \'min\': return min of the values\n                    \'max\': return max of the values\n                    \'distribution\': return 0%, 10%, 20%, ..., 90%, 100% of values, from min to max\n\n        Returns:\n\n        """"""\n\n        if operator is None or operator == \'origin\':\n            return self.__recorder[name]\n        elif operator == \'mean\':\n            return np.mean(self.__recorder[name])\n        elif operator == \'sum\':\n            return np.sum(self.__recorder[name])\n        elif operator == \'min\':\n            return np.min(self.__recorder[name])\n        elif operator == \'max\':\n            return np.max(self.__recorder[name])\n        elif operator == \'distribution\':\n            data_sorted = np.sort(self.__recorder[name])\n            distribution = []\n            for i in np.linspace(0, 1, 11):\n                if i != 1:\n                    distribution.append(data_sorted[int(i * len(data_sorted))])\n                else:\n                    distribution.append(data_sorted[-1])\n            return distribution\n\n    def clear_records(self, name=None):\n        if name is None:\n            for name in self.__names:\n                self.__recorder[name] = []\n        else:\n            self.__recorder[name] = []\n\n\n\n\nif __name__ == ""__main__"":\n    streaming_recorder = StreamingRecorder([\'prediction\'])\n    streaming_recorder.record(\'prediction\', [1, 2, 3])\n    streaming_recorder.record(\'prediction\', [4, 5, 6])\n    print(streaming_recorder.get(\'prediction\', \'origin\'))\n    print(streaming_recorder.get(\'prediction\', \'distribution\'))\n\n'"
core/__init__.py,0,b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.'
dataset/get_20_newsgroups.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\nimport os\nimport shutil\nfrom os import listdir\nimport tarfile\nimport argparse\nfrom sys import version_info\nfrom sklearn.model_selection import train_test_split\nif version_info.major == 2:\n    import urllib as urldownload\nelse:\n    import urllib.request as urldownload\n\n\nclass NewsGroup(object):\n    def __init__(self, params):\n        self.params = params\n        self.url = ""http://archive.ics.uci.edu/ml/machine-learning-databases/20newsgroups-mld/20_newsgroups.tar.gz""\n        self.file_name = \'20_newsgroups.tar.gz\'\n        self.dirname = \'20_newsgroups\'\n\n    def download_or_zip(self):\n        if not os.path.exists(self.params.root_dir):\n            os.mkdir(self.params.root_dir)\n        path = os.path.join(self.params.root_dir, self.dirname)\n        if not os.path.isdir(path):\n            file_path = os.path.join(self.params.root_dir, self.file_name)\n            if not os.path.isfile(file_path):\n                print(\'DownLoading...\')\n                urldownload.urlretrieve(self.url, file_path)\n            with tarfile.open(file_path, \'r\', encoding=\'utf-8\') as fin:\n                print(\'Extracting...\')\n                fin.extractall(self.params.root_dir)\n        return path\n\n    def read_process_file(self, file_path):\n        text_lines = []\n        with open(file_path, \'rb\') as fin:\n            for single_line in fin:\n                text_lines.append(str(single_line))\n        return \'\'.join(text_lines).replace(\'\\n\', \' \').replace(\'\\t\', \' \')\n\n    def data_combination(self):\n        data_dir_path = self.download_or_zip()\n        class_name_folders = listdir(data_dir_path)\n        assert len(class_name_folders) == 20, \'The 20_newsgroups data has 20 classes and 20 sub folder accordingly, but we found %d\' % len(class_name_folders)\n        pathname_list = []\n        label_list = []\n        for sub_folder in class_name_folders:\n            sub_folder_path = os.path.join(data_dir_path, sub_folder)\n            for single_file in listdir(sub_folder_path):\n                pathname_list.append(os.path.join(sub_folder_path, single_file))\n                label_list.append(sub_folder)\n        # prepare folder and write data\n        if not os.path.exists(self.params.output_dir):\n            os.mkdir(self.params.output_dir)\n        data_all = []\n        print(\'Preprocessing...\')\n        for single_file_path, singel_label in zip(pathname_list, label_list):\n            text_line = \'%s\\t%s\\n\' % (singel_label, self.read_process_file(single_file_path))\n            data_all.append(text_line)\n\n        print(\'Write output file...\')\n        if self.params.isSplit:\n            output_train_file_path = os.path.join(self.params.output_dir, \'train.tsv\')\n            output_test_file_path = os.path.join(self.params.output_dir, \'test.tsv\')\n            train_data, test_data = train_test_split(data_all, test_size=self.params.test_size, random_state=123)\n            with open(output_train_file_path, \'w\', encoding=\'utf-8\') as fout:\n                fout.writelines(train_data)\n            with open(output_test_file_path, \'w\', encoding=\'utf-8\') as fout:\n                fout.writelines(test_data)\n        else:\n            output_file_path = os.path.join(self.output_dir, \'output.tsv\')\n            with open(output_file_path, \'w\', encoding=\'utf-8\') as fout:\n                fout.writelines(data_all)\n        try:\n            if os.path.exists(self.params.root_dir):\n                shutil.rmtree(self.params.root_dir)\n        except:\n            pass\n\n\nif __name__ == \'__main__\':\n    parse = argparse.ArgumentParser(description=\'20_newsgroups data preprocess\')\n    parse.add_argument(""--root_dir"", type=str, default=\'./data\', help=\'the folder path of saving download file and untar files\')\n    parse.add_argument(""--output_dir"", type=str, default=\'20_newsgroups\', help=\'the folder path of saving tsv format files after preprocess\')\n    parse.add_argument(""--isSplit"", type=bool, default=True, help=\'appoint split data into train dataset and test dataset or not\')\n    parse.add_argument(""--test_size"", type=float, default=0.2)\n    params, _ = parse.parse_known_args()\n    newsgroup = NewsGroup(params)\n    newsgroup.data_combination()\n'"
dataset/get_QNLI.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport os\nimport urllib.request\nimport zipfile\n\ndata_path = \'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQNLIv2.zip?alt=media&token=6fdcf570-0fc5-4631-8456-9505272d1601\'\ndata_file = \'QNLI.zip\'\nprint(""Downloading and extracting %s..."" % data_file)\nurllib.request.urlretrieve(data_path, data_file)\nwith zipfile.ZipFile(data_file) as zip_ref:\n    zip_ref.extractall()\nos.remove(data_file)\nprint(""Completed!"")\n'"
dataset/get_QQP.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport os\nimport urllib.request\nimport zipfile\n\ndata_path = \'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FQQP.zip?alt=media&token=700c6acf-160d-4d89-81d1-de4191d02cb5\'\ndata_file = \'QQP.zip\'\nprint(""Downloading and extracting %s..."" % data_file)\nurllib.request.urlretrieve(data_path, data_file)\nwith zipfile.ZipFile(data_file) as zip_ref:\n    zip_ref.extractall()\nos.remove(data_file)\nprint(""Completed!"")\n'"
dataset/get_SST-2.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport os\nimport urllib.request\nimport zipfile\n\ndata_path = \'https://firebasestorage.googleapis.com/v0/b/mtl-sentence-representations.appspot.com/o/data%2FSST-2.zip?alt=media&token=aabc5f6b-e466-44a2-b9b4-cf6337f84ac8\'\ndata_file = \'SST-2.zip\'\nprint(""Downloading and extracting %s..."" % data_file)\nurllib.request.urlretrieve(data_path, data_file)\nwith zipfile.ZipFile(data_file) as zip_ref:\n    zip_ref.extractall()\nos.remove(data_file)\nprint(""Completed!"")\n'"
dataset/get_WikiQACorpus.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport os\nimport urllib.request\nimport zipfile\n\ndata_path = \'https://download.microsoft.com/download/E/5/F/E5FCFCEE-7005-4814-853D-DAA7C66507E0/WikiQACorpus.zip\'\ndata_file = \'WikiQACorpus.zip\'\nprint(""Downloading and extracting %s..."" % data_file)\nurllib.request.urlretrieve(data_path, data_file)\nwith zipfile.ZipFile(data_file) as zip_ref:\n    zip_ref.extractall()\nos.remove(data_file)\nprint(""Completed!"")\n'"
losses/BaseLossConf.py,2,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nfrom torch.nn import CrossEntropyLoss\nimport copy\nimport logging\n\nclass BaseLossConf(object):\n    @staticmethod\n    def get_conf(**kwargs):\n        # judge loss num & setting\n        kwargs[\'multiLoss\'] = True if len(kwargs[\'losses\']) > 1 else False\n        # loss = copy.deepcopy(kwargs[\'losses\'])\n        if kwargs[\'multiLoss\']:\n            if kwargs.get(\'multi_loss_op\', \'\') is None:\n                kwargs[\'multi_loss_op\'] = \'weighted_sum\'\n                logging.info(\'model has multi-loss but no multi_loss_op, we set default option {0}.\'.format(\'weighted_sum\'))\n            if kwargs.get(\'weights\', None) is None:\n                kwargs[\'weights\'] = [1] * len(kwargs[\'losses\'])\n                logging.warning(""MultiLoss have no weights, set the weights to 1."")\n            assert len(kwargs[\'weights\']) == len(kwargs[\'losses\']), ""The number of loss is inconsistent with loss weights!""\n\n\n        # IF NEEDED, TRANSFORM SOME INT OR FLOAT, OR NUMPY ARRAY TO TENSORS.\n        for single_loss in kwargs[\'losses\']:\n            if \'inputs\' not in single_loss:\n                raise Exception(""Each loss must have inputs"")\n            if not isinstance(single_loss[\'inputs\'], list):\n                raise Exception(\'The inputs of loss must be list\')\n            if len(single_loss[\'inputs\']) != 2:\n                raise Exception(\'The length of loss inputs must be 2\')\n            if \'weight\' in single_loss[\'conf\']:\n                single_loss[\'conf\'][\'weight\'] = torch.FloatTensor(single_loss[\'conf\'][\'weight\'])\n\n        return kwargs\n\n\n'"
losses/CRFLoss.py,7,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.autograd as autograd\n\n\nclass CRFLoss(nn.Module):\n    """"""CRFLoss\n       use for crf output layer for sequence tagging task.\n    """"""\n    def __init__(self):\n        super(CRFLoss, self).__init__()\n        \n    def _score_sentence(self, scores, mask, tags, transitions, crf_layer_conf):\n        """"""\n            input:\n                scores: variable (seq_len, batch, tag_size, tag_size)\n                mask: (batch, seq_len)\n                tags: tensor  (batch, seq_len)\n            output:\n                score: sum of score for gold sequences within whole batch\n        """"""\n        # Gives the score of a provided tag sequence\n        batch_size = scores.size(1)\n        seq_len = scores.size(0)\n        tag_size = scores.size(2)\n        # convert tag value into a new format, recorded label bigram information to index\n        new_tags = autograd.Variable(torch.LongTensor(batch_size, seq_len))\n        if crf_layer_conf.use_gpu:\n            new_tags = new_tags.cuda()\n        for idx in range(seq_len):\n            if idx == 0:\n                # start -> first score\n                new_tags[:, 0] = (tag_size-2)*tag_size + tags[:, 0]\n            else:\n                new_tags[:, idx] = tags[:, idx-1]*tag_size + tags[:, idx]\n\n        # transition for label to STOP_TAG\n        end_transition = transitions[:, crf_layer_conf.target_dict[crf_layer_conf.STOP_TAG]].contiguous().view(1, tag_size).expand(batch_size, tag_size)\n        # length for batch,  last word position = length - 1\n        length_mask = torch.sum(mask.long(), dim=1).view(batch_size, 1).long()\n        # index the label id of last word\n        end_ids = torch.gather(tags, 1, length_mask - 1)\n\n        # index the transition score for end_id to STOP_TAG\n        end_energy = torch.gather(end_transition, 1, end_ids)\n\n        # convert tag as (seq_len, batch_size, 1)\n        new_tags = new_tags.transpose(1, 0).contiguous().view(seq_len, batch_size, 1)\n        # need convert tags id to search from positions of scores\n        tg_energy = torch.gather(scores.view(seq_len, batch_size, -1), 2, new_tags).view(seq_len, batch_size)  # seq_len * batch_size\n        # mask transpose to (seq_len, batch_size)\n        tg_energy = tg_energy.masked_select(mask.transpose(1, 0))\n\n        # add all score together\n        gold_score = tg_energy.sum() + end_energy.sum()\n        return gold_score\n    \n    def forward(self, forward_score, scores, masks, tags, transitions, crf_layer_conf):\n        """"""\n        \n        :param forward_score: Tensor scale\n        :param scores: Tensor [seq_len, batch_size, target_size, target_size]\n        :param masks:  Tensor [batch_size, seq_len]\n        :param tags:   Tensor [batch_size, seq_len]\n        :return: goal_score - forward_score\n        """"""\n        gold_score = self._score_sentence(scores, masks, tags, transitions, crf_layer_conf)\n        return forward_score - gold_score'"
losses/FocalLoss.py,3,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass FocalLoss(nn.Module):\n    """""" Focal loss\n        reference: Lin T Y, Goyal P, Girshick R, et al. Focal loss for dense object detection[J]. arXiv preprint arXiv:1708.02002, 2017.\n    Args:\n        gamma (float): gamma >= 0.\n        alpha (float): 0 <= alpha <= 1\n        size_average (bool, optional): By default, the losses are averaged over observations for each minibatch. However, if the field size_average is set to False, the losses are instead summed for each minibatch. Default is True\n\n    """"""\n    def __init__(self, **kwargs):\n        super(FocalLoss, self).__init__()\n\n        # default parameters\n        self.gamma = 0\n        self.alpha = 0.5\n        self.size_average = True\n\n        for key in kwargs:\n            setattr(self, key, kwargs[key])\n\n        # varification\n        assert self.alpha <= 1 and self.alpha >= 0, ""The parameter alpha in Focal Loss must be in range [0, 1].""\n        if self.alpha is not None:\n            self.alpha = torch.Tensor([self.alpha, 1 - self.alpha])\n\n    def forward(self, input, target):\n        """""" Get focal loss\n\n        Args:\n            input (Variable):  the prediction with shape [batch_size, number of classes]\n            target (Variable): the answer with shape [batch_size, number of classes]\n\n        Returns:\n            Variable (float): loss\n        """"""\n        if input.dim()>2:\n            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n        target = target.view(-1, 1)\n\n        logpt = F.log_softmax(input)\n        logpt = logpt.gather(1,target)\n        logpt = logpt.view(-1)\n        pt = logpt.data.exp()\n\n        if self.alpha is not None:\n            if self.alpha.type() != input.data.type():\n                self.alpha = self.alpha.type_as(input.data)\n            at = self.alpha.gather(0, target.data.view(-1))\n            logpt = logpt * at\n\n        loss = -1 * (1-pt)**self.gamma * logpt\n\n        if self.size_average: \n            return loss.mean()\n        else:\n            return loss.sum()\n'"
losses/Loss.py,2,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport sys\nimport logging\nsys.path.append(\'../\')\nfrom settings import LossOperationType\nfrom torch.nn import CrossEntropyLoss, L1Loss, MSELoss, NLLLoss, PoissonNLLLoss, NLLLoss2d, KLDivLoss, BCELoss, BCEWithLogitsLoss, MarginRankingLoss, HingeEmbeddingLoss, MultiLabelMarginLoss, SmoothL1Loss, SoftMarginLoss, MultiLabelSoftMarginLoss, CosineEmbeddingLoss, MultiMarginLoss, TripletMarginLoss\nfrom .CRFLoss import CRFLoss\n\nclass Loss(nn.Module):\n    \'\'\'\n    For support multi_task or multi_output, the loss type changes to list.\n    Using class Loss for parsing and constructing the loss list.\n    Args:\n        loss_conf: the loss for multi_task or multi_output.\n                   multi_loss_op: the operation for multi_loss\n                   losses: list type. Each element is single loss.\n                   eg: ""loss"": {\n                            ""multi_loss_op"": ""weighted_sum"",\n                            ""losses"": [\n                              {\n                                ""type"": ""CrossEntropyLoss"",\n                                ""conf"": {\n                                    ""gamma"": 0,\n                                    ""alpha"": 0.5,\n                                    ""size_average"": true\n                                },\n                                ""inputs"": [""start_output"", ""start_label""]\n                              },\n                              {\n                                ""type"": ""CrossEntropyLoss"",\n                                ""conf"": {\n                                    ""gamma"": 0,\n                                    ""alpha"": 0.5,\n                                    ""size_average"": true\n                                },\n                                ""inputs"": [""end_output"", ""end_label""]\n                              }\n                            ],\n                            ""weights"": [0.5, 0.5]\n                        }\n    \'\'\'\n    def __init__(self, **kwargs):\n        super(Loss, self).__init__()\n\n        self.loss_fn = nn.ModuleList()\n        self.loss_input = []\n        self.weights = kwargs[\'weights\'] if \'weights\' in kwargs else None\n        support_loss_op = set(LossOperationType.__members__.keys())\n        if kwargs[\'multiLoss\']:\n            # check multi_loss_op\n            if not kwargs[\'multi_loss_op\'].lower() in support_loss_op:\n                raise Exception(""The multi_loss_op %s is not supported. Supported multi_loss_op are: %s""\n                                % (kwargs[\'multi_loss_op\'], support_loss_op))\n            self.multi_loss_op = kwargs[\'multi_loss_op\']\n        # check single loss inputs\n        for single_loss in kwargs[\'losses\']:\n            if (not single_loss[\'inputs\'][0] in kwargs[\'output_layer_id\']) or (not single_loss[\'inputs\'][1] in kwargs[\'answer_column_name\']):\n                raise Exception(""The loss inputs are excepted to be part of output_layer_id and targets!"")\n            self.loss_fn.append(eval(single_loss[\'type\'])(**single_loss[\'conf\']))\n            self.loss_input.append(single_loss[\'inputs\'])\n\n    def forward(self, model_outputs, targets):\n        \'\'\'\n        compute multi_loss according to multi_loss_op\n        :param model_outputs: the representation of model output layer\n                              :type: dict {output_layer_id: output layer data}\n        :param targets: the label of raw data\n                        :type: dict {target: data}\n        :return:\n        \'\'\'\n        all_losses = []\n        result_loss = 0.0\n        for index, single_loss_fn in enumerate(self.loss_fn):\n            all_losses.append(single_loss_fn(model_outputs[self.loss_input[index][0]], targets[self.loss_input[index][1]]))\n        if hasattr(self, \'multi_loss_op\'):\n            if LossOperationType[self.multi_loss_op.lower()] == LossOperationType.weighted_sum:\n                for index, single_loss in enumerate(all_losses):\n                    result_loss += (self.weights[index]*single_loss)\n        else:\n            result_loss = all_losses[0]\n\n        return result_loss\n\n'"
losses/__init__.py,1,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\nfrom .FocalLoss import FocalLoss\nfrom .CRFLoss import CRFLoss\nfrom .Loss import Loss\nfrom torch.nn import CrossEntropyLoss, L1Loss, MSELoss, NLLLoss, PoissonNLLLoss, NLLLoss2d, KLDivLoss, BCELoss, BCEWithLogitsLoss, MarginRankingLoss, HingeEmbeddingLoss, MultiLabelMarginLoss, SmoothL1Loss, SoftMarginLoss, MultiLabelSoftMarginLoss, CosineEmbeddingLoss, MultiMarginLoss, TripletMarginLoss'"
metrics/Evaluator.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nfrom .conlleval import countChunks, evaluate, to_conll_format\nfrom .slot_tagging_metrics import get_ner_BIOES, get_ner_BIO\nfrom settings import TaggingSchemes\nimport numpy as np\nimport re\nimport string\nfrom collections import Counter\n\n\nclass Evaluator(object):\n    def __init__(self, metrics, pos_label=1, first_metric=None, tagging_scheme=None, label_indices=None):\n        """"""\n\n        Args:\n            metrics:\n            pos_label: the positive label for auc metric\n            first_metric:\n            tagging_scheme:\n            label_indices: label to index dictionary, for auc@average or auc@some_type metric\n        """"""\n        self.__metrics = metrics\n        self.__pos_label = pos_label\n        if first_metric is None:\n            self.__first_metric = metrics[0]\n        else:\n            self.__first_metric = first_metric\n        self.__tagging_scheme = tagging_scheme\n        self.__label_indices = label_indices\n\n        self.has_auc_type_specific = False      # if True, the recorder needs to record the pred score of all types\n        supported_metrics = self.get_supported_metrics()\n        for metric in metrics:\n            if not metric in supported_metrics:\n                if metric.find(\'@\') != -1:\n                    field, target = metric.split(\'@\')\n                    if field != \'auc\' or (self.__label_indices and (not target in self.__label_indices) and target != \'average\'):\n                        raise Exception(""The metric %s is not supported. Supported metrics are: %s"" % (metric, supported_metrics))\n                    else:\n                        self.has_auc_type_specific = True\n\n    def evaluate(self, y_true, y_pred, y_pred_pos_score=None, y_pred_scores_all=None, formatting=False):\n        """""" evalution\n\n        Args:\n            y_true:\n            y_pred:\n            y_pred_pos_score:\n            formatting:\n\n        Returns:\n\n        """"""\n        result = dict()\n\n        for metric in self.__metrics:\n            if metric == \'auc\':\n                result[metric] = getattr(self, metric)(y_true, y_pred_pos_score)\n            elif metric.startswith(\'auc@\'):\n                field, target = metric.split(\'@\')\n                if target == \'average\':\n                    results = []\n                    for i in range(len(y_pred_scores_all[0])):\n                        results.append(self.auc(y_true, np.array(y_pred_scores_all)[:, i]))\n                    result[metric] = np.mean(results)\n                else:\n                    result[metric] = self.auc(y_true, np.array(y_pred_scores_all)[:, self.__label_indices[target]])\n            else:\n                result[metric] = getattr(self, metric)(y_true, y_pred)\n\n        self.__last_result = result\n        if formatting is True:\n            ret = self.format_result(result)\n        else:\n            ret = result\n        return ret\n\n    def compare(self, current_result, previous_result, metric=None):\n        """"""\n\n        Args:\n            current_result:\n            previous_result:\n            metric:\n\n        Returns:\n            current better than previous: 1\n            current worse than previous: -1\n            current equal to previous: 0\n\n        """"""\n        if previous_result is None:\n            return 1\n\n        if metric is None:\n            metric = self.__first_metric\n\n        # by default, metrics are the bigger, the better\n        small_better_metrics = set([\'MSE\', \'RMSE\'])\n\n        if not metric in small_better_metrics:\n            if current_result > previous_result:\n                return 1\n            elif current_result < previous_result:\n                return -1\n            else:\n                return 0\n        else:\n            if current_result > previous_result:\n                return -1\n            elif current_result < previous_result:\n                return 1\n            else:\n                return 0\n\n    def get_first_metric_result(self):\n        return self.__last_result[self.__first_metric]\n\n    def get_supported_metrics(self):\n        except_methods = [""evaluate"", ""format_result"", ""get_supported_metrics"", ""get_first_metric_result"", ""normalize_answer""]\n        supported_metrics = []\n        for name in dir(self):\n            if callable(getattr(self, name)) and name.startswith(""_"") is False and not name in except_methods:\n                supported_metrics.append(name)\n        return supported_metrics\n\n    def format_result(self, result):\n        return ""; "".join([""%s: %.6f"" % (metric, result[metric]) for metric in self.__metrics])\n\n    def auc(self, y_true, y_pred_pos_score):\n        assert y_pred_pos_score is not None, ""Prediction confidence of positive label should not be None for auc metric!""\n        fpr, tpr, thresholds = metrics.roc_curve(y_true, y_pred_pos_score, pos_label=self.__pos_label)\n        return metrics.auc(fpr, tpr)\n\n    def accuracy(self, y_true, y_pred):\n        return metrics.accuracy_score(y_true, y_pred)\n\n    def seq_tag_f1(self, y_ture, y_pred):\n        \'\'\'\n\n        :param y_ture:\n        :param y_pred:\n        :return:\n        \'\'\'\n        assert self.__tagging_scheme is not None, ""Please define tagging scheme!""\n        sent_num = len(y_pred)\n        golden_full = []\n        predict_full = []\n        right_full = []\n        for idx in range(0, sent_num):\n            golden_list = y_ture[idx]\n            predict_list = y_pred[idx]\n            if self.__tagging_scheme == ""BMES"" or self.__tagging_scheme == ""BIOES"":\n                gold_matrix = get_ner_BIOES(golden_list)\n                pred_matrix = get_ner_BIOES(predict_list)\n            elif self.__tagging_scheme == ""BIO"":\n                gold_matrix = get_ner_BIO(golden_list)\n                pred_matrix = get_ner_BIO(predict_list)\n            else:\n                # raise Exception(""DETECT UNKNOWN TAGGING SCHEMES! YOU CAN USE OUR SCRIPT TO CONVERT TAG SCHEME!"")\n                raise Exception(""DETECT UNKNOWN TAGGING SCHEMES!"")\n            right_ner = list(set(gold_matrix).intersection(set(pred_matrix)))\n            golden_full += gold_matrix\n            predict_full += pred_matrix\n            right_full += right_ner\n        right_num = len(right_full)\n        golden_num = len(golden_full)\n        predict_num = len(predict_full)\n        if predict_num == 0:\n            precision = -1\n        else:\n            precision = (right_num + 0.0) / predict_num\n        if golden_num == 0:\n            recall = -1\n        else:\n            recall = (right_num + 0.0) / golden_num\n        if (precision == -1) or (recall == -1) or (precision + recall) <= 0.:\n            f_measure = -1\n        else:\n            f_measure = 2 * precision * recall / (precision + recall)\n        return f_measure\n\n\n    def seq_tag_accuracy(self, y_ture, y_pred):\n        \'\'\'\n\n        :param y_ture:\n        :param y_pred:\n        :return:\n        \'\'\'\n        sent_num = len(y_pred)\n        right_tag = 0\n        all_tag = 0\n        for idx in range(0, sent_num):\n            golden_list = y_ture[idx]\n            predict_list = y_pred[idx]\n            for idy in range(len(golden_list)):\n                if golden_list[idy] == predict_list[idy]:\n                    right_tag += 1\n            all_tag += len(golden_list)\n        accuracy = (right_tag + 0.0) / all_tag\n        return accuracy\n\n\n    def macro_f1(self, y_true, y_pred):\n        """""" For classification task, calculate f1-score for each label, and find their unweighted mean. This does not take label imbalance into account.\n\n        Args:\n            y_true:\n            y_pred:\n\n        Returns:\n\n        """"""\n        return metrics.f1_score(y_true, y_pred, average=\'macro\')\n\n    def macro_precision(self, y_true, y_pred):\n        """""" Calculate precision for each label, and find their unweighted mean. This does not take label imbalance into account.\n\n        Args:\n            y_true:\n            y_pred:\n\n        Returns:\n\n        """"""\n        return metrics.precision_score(y_true, y_pred, average=\'macro\')\n\n    def macro_recall(self, y_true, y_pred):\n        """""" Calculate recall for each label, and find their unweighted mean. This does not take label imbalance into account.\n\n        Args:\n            y_true:\n            y_pred:\n\n        Returns:\n\n        """"""\n        return metrics.recall_score(y_true, y_pred, average=\'macro\')\n\n    def micro_f1(self, y_true, y_pred):\n        """""" For classification task, calculate f1-score globally by counting the total true positives, false negatives and false positives.\n\n        Args:\n            y_true:\n            y_pred:\n\n        Returns:\n\n        """"""\n        return metrics.f1_score(y_true, y_pred, average=\'micro\')\n\n    def f1(self, y_true, y_pred):\n        """""" For classification task, calculate f1-score Only report results for the class specified by pos_label. This is applicable only if targets (y_{true,pred}) are binary..\n\n        Args:\n            y_true:\n            y_pred:\n\n        Returns:\n\n        """"""\n        return metrics.f1_score(y_true, y_pred)\n\n    def micro_precision(self, y_true, y_pred):\n        """""" Calculate precision globally by counting the total true positives, false negatives and false positives.\n\n        Args:\n            y_true:\n            y_pred:\n\n        Returns:\n\n        """"""\n        return metrics.precision_score(y_true, y_pred, average=\'micro\')\n\n    def micro_recall(self, y_true, y_pred):\n        """""" Calculate recall globally by counting the total true positives, false negatives and false positives.\n\n        Args:\n            y_true:\n            y_pred:\n\n        Returns:\n\n        """"""\n        return metrics.recall_score(y_true, y_pred, average=\'micro\')\n\n    def weighted_f1(self, y_true, y_pred):\n        """""" Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label). This alters \xe2\x80\x98macro\xe2\x80\x99 to account for label imbalance; it can result in an F-score that is not between precision and recall.\n\n        Args:\n            y_true:\n            y_pred:\n\n        Returns:\n\n        """"""\n        return metrics.f1_score(y_true, y_pred, average=\'weighted\')\n\n    def weighted_precision(self, y_true, y_pred):\n        """""" Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label). This alters \xe2\x80\x98macro\xe2\x80\x99 to account for label imbalance; it can result in an F-score that is not between precision and recall.\n\n        Args:\n            y_true:\n            y_pred:\n\n        Returns:\n\n        """"""\n        return metrics.precision_score(y_true, y_pred, average=\'weighted\')\n\n    def weighted_recall(self, y_true, y_pred):\n        """""" Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label). This alters \xe2\x80\x98macro\xe2\x80\x99 to account for label imbalance; it can result in an F-score that is not between precision and recall.\n\n        Args:\n            y_true:\n            y_pred:\n\n        Returns:\n\n        """"""\n        return metrics.recall_score(y_true, y_pred, average=\'weighted\')\n\n    def MSE(self, y_true, y_pred):\n        """""" mean square error\n\n        Args:\n            y_true: true score\n            y_pred: predict score\n\n        Returns:\n\n        """"""\n        return mean_squared_error(y_true, y_pred)\n\n    def RMSE(self, y_true, y_pred):\n        """""" root mean square error\n\n        Args:\n            y_true: true score\n            y_pred: predict score\n\n        Returns:\n\n        """"""\n        return np.sqrt(mean_squared_error(y_true, y_pred))\n\n    def normalize_answer(self, s):\n        """"""Lower text and remove punctuation, articles and extra whitespace.""""""\n\n        def remove_articles(text):\n            return re.sub(r\'\\b(a|an|the)\\b\', \' \', text)\n\n        def white_space_fix(text):\n            return \' \'.join(text.split())\n\n        def remove_punc(text):\n            exclude = set(string.punctuation)\n            return \'\'.join(ch for ch in text if ch not in exclude)\n\n        def lower(text):\n            return text.lower()\n\n        return white_space_fix(remove_articles(remove_punc(lower(s))))\n\n    def mrc_f1(self, y_true, y_pred):\n        \'\'\'\n        compute mrc task metric F1\n        :param y_true: type list. ground thruth answer text\n        :param y_pred: type list. length is same as y_true, model output answer text.\n        :return: mrc task F1 score\n        \'\'\'\n        f1 = total = 0\n        for single_true, single_pred in zip(y_true, y_pred):\n            total += 1\n            prediction_tokens = self.normalize_answer(single_pred).split()\n            ground_truth_tokens = self.normalize_answer(single_true).split()\n            common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n            num_same = sum(common.values())\n            if num_same == 0:\n                continue\n            precision = 1.0 * num_same / len(prediction_tokens)\n            recall = 1.0 * num_same / len(ground_truth_tokens)\n            f1 += (2*precision*recall) / (precision+recall)\n        return 100.0 * f1 / total\n\n    def mrc_em(self, y_true, y_pred):\n        \'\'\'\n        compute mrc task metric EM\n        :param y_true:\n        :param y_pred:\n        :return: mrc task EM score\n        \'\'\'\n        em = total = 0\n        for single_true, single_pred in zip(y_true, y_pred):\n            total += 1\n            em += (self.normalize_answer(single_true) == self.normalize_answer(single_pred))\n        return 100.0 * em / total\n\n\nif __name__ == \'__main__\':\n    evaluator = Evaluator([\'auc\', \'accuracy\'])\n    print(evaluator.get_supported_metrics())\n\n'"
metrics/__init__.py,0,b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.'
metrics/conlleval.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nfrom __future__ import division, print_function, unicode_literals\nimport argparse\nimport sys\nimport re\nfrom collections import defaultdict\nimport logging\n\n\nSENT_BOUNDARY = ""-X-""  # sentence boundary\n\n\ndef to_conll_format(y_true, y_pred):\n    """""" transform prediction and answer to the conll format\n\n    Args:\n        y_true: 2d array, [number of sentence, variable_sentence_length]\n        y_pred: 2d array, [number of sentence, variable_sentence_length]\n\n    Returns:\n        2d array in conll output format. [number of tokens + number of sentence, 3]\n        e.g. [\n                [\'\', y_true, y_pred],\n                ...,\n                [SENT_BOUNDARY, \'O\', \'O\'],      # as sentence boundary\n                [\'\', y_true, y_pred],        # next sentence\n                ...\n            ]\n\n    """"""\n    import codecs\n    fout = codecs.open(""debug.txt"", \'w\', encoding=\'utf-8\')\n\n    result_conll = []\n    for target_sent, pred_sent in zip(y_true, y_pred):\n        for target, pred in zip(target_sent, pred_sent):\n            result_conll.append([\'```\', target, pred])\n            fout.write(""```` %s %s\\n"" % (target, pred))\n        result_conll.append([SENT_BOUNDARY, ""O"", ""O""])  # sentence boundary\n        fout.write(""%s %s %s\\n"" % (SENT_BOUNDARY, ""O"", ""O""))\n    fout.close()\n    return result_conll\n\n\n# sanity check\ndef parse_args():\n    argparser = argparse.ArgumentParser()\n    argparser.add_argument(\n        ""-f"", ""--file_path"",\n        help=""input file path""\n    )\n    argparser.add_argument(\n        ""-l"", ""--latex"",\n        default=False, action=""store_true"",\n        help=""generate LaTeX output""\n    )\n    argparser.add_argument(\n        ""-r"", ""--raw"",\n        default=False, action=""store_true"",\n        help=""accept raw result tags""\n    )\n    argparser.add_argument(\n        ""-d"", ""--delimiter"",\n        default=None,\n        help=""alternative delimiter tag (default: single space)""\n    )\n    argparser.add_argument(\n        ""-o"", ""--oTag"",\n        default=""O"",\n        help=""alternative delimiter tag (default: O)""\n    )\n    args = argparser.parse_args()\n    return args\n\n\n\n""""""\n\xe2\x80\xa2 IOB1: I is a token inside a chunk, O is a token outside a chunk and B is the\nbeginning of chunk immediately following another chunk of the same Named Entity.\n\xe2\x80\xa2 IOB2: It is same as IOB1, except that a B tag is given for every token, which exists at\nthe beginning of the chunk.\n\xe2\x80\xa2 IOE1: An E tag used to mark the last token of a chunk immediately preceding another\nchunk of the same named entity.\n\xe2\x80\xa2 IOE2: It is same as IOE1, except that an E tag is given for every token, which exists at\nthe end of the chunk.\n\xe2\x80\xa2 START/END: This consists of the tags B, E, I, S or O where S is used to represent a\nchunk containing a single token. Chunks of length greater than or equal to two always\nstart with the B tag and end with the E tag.\n\xe2\x80\xa2 IO: Here, only the I and O labels are used. This therefore cannot distinguish between\nadjacent chunks of the same named entity.\n""""""\n# endOfChunk: checks if a chunk ended between the previous and current word\n# arguments:  previous and current chunk tags, previous and current types\n# note:       this code is capable of handling other chunk representations\n#             than the default CoNLL-2000 ones, see EACL\'99 paper of Tjong\n#             Kim Sang and Veenstra http://xxx.lanl.gov/abs/cs.CL/9907006\ndef endOfChunk(prevTag, tag, prevType, type):\n    """"""\n    checks if a chunk ended between the previous and current word;\n    arguments:  previous and current chunk tags, previous and current types\n    """"""\n    return ((prevTag == ""B"" and tag == ""B"") or\n        (prevTag == ""B"" and tag == ""O"") or\n        (prevTag == ""I"" and tag == ""B"") or\n        (prevTag == ""I"" and tag == ""O"") or\n\n        (prevTag == ""E"" and tag == ""E"") or\n        (prevTag == ""E"" and tag == ""I"") or\n        (prevTag == ""E"" and tag == ""O"") or\n        (prevTag == ""I"" and tag == ""O"") or\n\n        (prevTag != ""O"" and prevTag != ""."" and prevType != type) or\n        (prevTag == ""]"" or prevTag == ""[""))\n        # corrected 1998-12-22: these chunks are assumed to have length 1\n\n\n# startOfChunk: checks if a chunk started between the previous and current word\n# arguments:    previous and current chunk tags, previous and current types\n# note:         this code is capable of handling other chunk representations\n#               than the default CoNLL-2000 ones, see EACL\'99 paper of Tjong\n#               Kim Sang and Veenstra http://xxx.lanl.gov/abs/cs.CL/9907006\ndef startOfChunk(prevTag, tag, prevType, type):\n    """"""\n    checks if a chunk started between the previous and current word;\n    arguments:  previous and current chunk tags, previous and current types\n    """"""\n    chunkStart = ((prevTag == ""B"" and tag == ""B"") or\n        (prevTag == ""B"" and tag == ""B"") or\n        (prevTag == ""I"" and tag == ""B"") or\n        (prevTag == ""O"" and tag == ""B"") or\n        (prevTag == ""O"" and tag == ""I"") or\n\n        (prevTag == ""E"" and tag == ""E"") or\n        (prevTag == ""E"" and tag == ""I"") or\n        (prevTag == ""O"" and tag == ""E"") or\n        (prevTag == ""O"" and tag == ""I"") or\n\n        (tag != ""O"" and tag != ""."" and prevType != type) or\n        (tag == ""]"" or tag == ""[""))\n        # corrected 1998-12-22: these chunks are assumed to have length 1\n\n    #logging.info(""startOfChunk?"", prevTag, tag, prevType, type)\n    #logging.info(chunkStart)\n    return chunkStart\n\ndef calcMetrics(TP, P, T, percent=True):\n    """"""\n    compute overall precision, recall and FB1 (default values are 0.0)\n    if percent is True, return 100 * original decimal value\n    """"""\n    precision = TP / P if P else 0\n    recall = TP / T if T else 0\n    FB1 = 2 * precision * recall / (precision + recall) if precision + recall else 0\n    if percent:\n        return 100 * precision, 100 * recall, 100 * FB1\n    else:\n        return precision, recall, FB1\n\ndef splitTag(chunkTag, oTag = ""O"", raw = False, defaultType=""NONE""):\n    """"""\n    Split chunk tag into IOB tag and chunk type;\n    return (iob_tag, chunk_type)\n    """"""\n    if chunkTag == ""O"" or chunkTag == oTag:\n        tag, type = ""O"", defaultType\n    elif raw:\n        tag, type = ""B"", chunkTag\n    else:\n        try:\n            # split on first hyphen, allowing hyphen in type\n            tag, type = chunkTag.split(\'-\', 1)\n        except ValueError:\n            tag, type  = chunkTag, defaultType\n    return tag, type\n\ndef countChunks(fileIterator, delimiter=None, raw=False, oTag=""O""):\n    """"""\n    Process input in given format and count chunks using the last two columns;\n    return correctChunk, foundGuessed, foundCorrect, correctTags, tokenCounter\n\n    Args:\n        fileIterator: either an input stream/stdin, or a list (Note that len(fileIterator[0]) >= 3.\n            list example:\n                [\n                    [\'\', y_true, y_pred],\n                    ...,\n                    [SENT_BOUNDARY, \'O\', \'O\'],      # as sentence boundary\n                    [\'\', y_true, y_pred],        # next sentence\n                    ...\n                ]\n    """"""\n    correctChunk = defaultdict(int)     # number of correctly identified chunks\n    foundCorrect = defaultdict(int)     # number of chunks in corpus per type\n    foundGuessed = defaultdict(int)     # number of identified chunks per type\n\n    tokenCounter = 0     # token counter (ignores sentence breaks)\n    correctTags = 0      # number of correct chunk tags\n\n    lastType = None # temporary storage for detecting duplicates\n    inCorrect = False # currently processed chunk is correct until now\n    lastCorrect, lastCorrectType = ""O"", None    # previous chunk tag in corpus\n    lastGuessed, lastGuessedType = ""O"", None  # previously identified chunk tag\n\n    for line in fileIterator:\n        # each non-empty line must contain >= 3 columns\n        if isinstance(line, str):\n            features = line.strip().split(delimiter)\n        else:   # support online evaluation\n            features = line\n        if not features or features[0] == SENT_BOUNDARY:        # insert an sentence boundary\n            features = [SENT_BOUNDARY, oTag, oTag]\n        elif len(features) < 3:\n             raise IOError(""conlleval: unexpected number of features in line %s\\n"" % line)\n\n        # extract tags from last 2 columns\n        guessed, guessedType = splitTag(features[-1], oTag=oTag, raw=raw)\n        correct, correctType = splitTag(features[-2], oTag=oTag, raw=raw)\n\n        # 1999-06-26 sentence breaks should always be counted as out of chunk\n        firstItem = features[0]\n        if firstItem == SENT_BOUNDARY:\n            guessed, guessedType = ""O"", None\n\n        # decide whether current chunk is correct until now\n        if inCorrect:\n            endOfGuessed = endOfChunk(lastCorrect, correct, lastCorrectType, correctType)\n            endOfCorrect = endOfChunk(lastGuessed, guessed, lastGuessedType, guessedType)\n            if (endOfGuessed and endOfCorrect and lastGuessedType == lastCorrectType):\n                inCorrect = False\n                correctChunk[lastCorrectType] += 1\n            elif ( endOfGuessed != endOfCorrect or guessedType != correctType):\n                inCorrect = False\n\n        startOfGuessed = startOfChunk(lastGuessed, guessed, lastGuessedType, guessedType)\n        startOfCorrect = startOfChunk(lastCorrect, correct, lastCorrectType, correctType)\n        if (startOfCorrect and startOfGuessed and guessedType == correctType):\n            inCorrect = True\n        if startOfCorrect:\n            foundCorrect[correctType] += 1\n        if startOfGuessed:\n            foundGuessed[guessedType] += 1\n\n        if firstItem != SENT_BOUNDARY:\n            if correct == guessed and guessedType == correctType:\n                correctTags += 1\n            tokenCounter += 1\n\n        lastGuessed, lastGuessedType = guessed, guessedType\n        lastCorrect, lastCorrectType = correct, correctType\n\n    if inCorrect:\n        correctChunk[lastCorrectType] += 1\n\n    return correctChunk, foundGuessed, foundCorrect, correctTags, tokenCounter\n\n\ndef evaluate(correctChunk, foundGuessed, foundCorrect, correctTags, tokenCounter, latex=False):\n    # sum counts\n    correctChunkSum = sum(correctChunk.values())\n    foundGuessedSum = sum(foundGuessed.values())\n    foundCorrectSum = sum(foundCorrect.values())\n\n    # sort chunk type names\n    sortedTypes = list(foundCorrect) + list(foundGuessed)\n    sortedTypes = list(set(sortedTypes))\n    sortedTypes.sort()\n\n    # print overall performance, and performance per chunk type\n    if not latex:\n        # compute overall precision, recall and FB1 (default values are 0.0)\n        overall_precision, overall_recall, overall_FB1 = calcMetrics(correctChunkSum, foundGuessedSum, foundCorrectSum)\n\n        # print overall performance\n        logging.info(""processed %i tokens with %i phrases; found: %i phrases; correct: %i."" % (tokenCounter, foundCorrectSum, foundGuessedSum, correctChunkSum))\n        if tokenCounter:\n            logging.info(""accuracy: %6.2f%%; precision: %6.2f%%; recall: %6.2f%%; FB1: %6.2f"" % (100*correctTags/tokenCounter, overall_precision, overall_recall, overall_FB1))\n\n        for i in sortedTypes:\n            precision, recall, FB1 = calcMetrics(correctChunk[i], foundGuessed[i], foundCorrect[i])\n            logging.info(""%17s: precision: %6.2f%%; recall: %6.2f%%; FB1: %6.2f  %d"" %\n                    (i, precision, recall, FB1, foundGuessed[i]))\n\n    # generate LaTeX output for tables like in\n    # http://cnts.uia.ac.be/conll2003/ner/example.tex\n    else:\n        output_str = \'\'\n        output_str += ""        & Precision &  Recall  & F\\$_{\\\\beta=1} \\\\\\\\\\\\hline""\n        for i in sortedTypes:\n            precision, recall, FB1 = calcMetrics(correctChunk[i], foundGuessed[i], foundCorrect[i])\n            output_str += ""\\n%-7s &  %6.2f\\\\%% & %6.2f\\\\%% & %6.2f \\\\\\\\"" % (i,precision,recall,FB1)\n        logging.info(output_str)\n        logging.info(""\\\\hline"")\n\n        overall_precision, overall_recall, overall_FB1 = calcMetrics(correctChunkSum, foundGuessedSum, foundCorrectSum)\n        logging.info(""Overall &  %6.2f\\\\%% & %6.2f\\\\%% & %6.2f \\\\\\\\\\\\hline"" %\n              (overall_precision, overall_recall, overall_FB1))\n    return overall_precision, overall_recall, overall_FB1\n\n\nif __name__ == ""__main__"":\n    logging.basicConfig(level=logging.INFO, format=\'%(message)s\')\n    args = parse_args()\n\n    # process input and count chunks\n    if args.file_path:\n        with open(args.file_path) as fin:\n            correctChunk, foundGuessed, foundCorrect, correctTags, tokenCounter = countChunks(fin, args.delimiter, args.raw, args.oTag)\n    else:\n        correctChunk, foundGuessed, foundCorrect, correctTags, tokenCounter = countChunks(sys.stdin, args.delimiter, args.raw, args.oTag)\n\n    # compute metrics and print\n    overall_precision, overall_recall, overall_FB1 = evaluate(correctChunk, foundGuessed, foundCorrect, correctTags, tokenCounter, latex=args.latex)\n\n    sys.exit(0)\n'"
metrics/slot_tagging_metrics.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n\ndef get_ner_BIOES(label_list):\n    list_len = len(label_list)\n    begin_label = \'B-\'\n    end_label = \'E-\'\n    single_label = \'S-\'\n    whole_tag = \'\'\n    index_tag = \'\'\n    tag_list = []\n    stand_matrix = []\n    for i in range(0, list_len):\n        current_label = label_list[i].upper()\n        if begin_label in current_label:\n            if index_tag != \'\':\n                tag_list.append(whole_tag + \',\' + str(i-1))\n            whole_tag = current_label.replace(begin_label, """", 1) + \'[\' + str(i)\n            index_tag = current_label.replace(begin_label, """", 1)\n\n        elif single_label in current_label:\n            if index_tag != \'\':\n                tag_list.append(whole_tag + \',\' + str(i-1))\n            whole_tag = current_label.replace(single_label, """", 1) + \'[\' +str(i)\n            tag_list.append(whole_tag)\n            whole_tag = """"\n            index_tag = """"\n        elif end_label in current_label:\n            if index_tag != \'\':\n                tag_list.append(whole_tag + \',\' + str(i))\n            whole_tag = \'\'\n            index_tag = \'\'\n        else:\n            continue\n    if (whole_tag != \'\')&(index_tag != \'\'):\n        tag_list.append(whole_tag)\n    tag_list_len = len(tag_list)\n\n    for i in range(0, tag_list_len):\n        if len(tag_list[i]) > 0:\n            tag_list[i] = tag_list[i]+ \']\'\n            insert_list = reverse_style(tag_list[i])\n            stand_matrix.append(insert_list)\n    return stand_matrix\n\n\ndef get_ner_BIO(label_list):\n    list_len = len(label_list)\n    begin_label = \'B-\'\n    inside_label = \'I-\'\n    whole_tag = \'\'\n    index_tag = \'\'\n    tag_list = []\n    stand_matrix = []\n    for i in range(0, list_len):\n        current_label = label_list[i].upper()\n        if begin_label in current_label:\n            if index_tag == \'\':\n                whole_tag = current_label.replace(begin_label, """", 1) + \'[\' + str(i)\n                index_tag = current_label.replace(begin_label, """", 1)\n            else:\n                tag_list.append(whole_tag + \',\' + str(i-1))\n                whole_tag = current_label.replace(begin_label, """", 1) + \'[\' + str(i)\n                index_tag = current_label.replace(begin_label, """", 1)\n\n        elif inside_label in current_label:\n            if current_label.replace(inside_label, """", 1) == index_tag:\n                whole_tag = whole_tag\n            else:\n                if (whole_tag != \'\')&(index_tag != \'\'):\n                    tag_list.append(whole_tag + \',\' + str(i-1))\n                whole_tag = \'\'\n                index_tag = \'\'\n        else:\n            if (whole_tag != \'\')&(index_tag != \'\'):\n                tag_list.append(whole_tag + \',\' + str(i-1))\n            whole_tag = \'\'\n            index_tag = \'\'\n\n    if (whole_tag != \'\')&(index_tag != \'\'):\n        tag_list.append(whole_tag)\n    tag_list_len = len(tag_list)\n\n    for i in range(0, tag_list_len):\n        if len(tag_list[i]) > 0:\n            tag_list[i] = tag_list[i]+ \']\'\n            insert_list = reverse_style(tag_list[i])\n            stand_matrix.append(insert_list)\n    return stand_matrix\n\n\ndef reverse_style(input_string):\n    target_position = input_string.index(\'[\')\n    input_len = len(input_string)\n    output_string = input_string[target_position:input_len] + input_string[0:target_position]\n    return output_string'"
model_visualizer/get_model_graph.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport json\nfrom graphviz import *\nimport argparse\n\n\ndef json2graph(json_path, graph_path):\n    with open(json_path, \'r\') as file:\n        json_str = file.read()\n    try:\n        conf_dic = json.loads(json_str)\n    except ValueError as e:\n        print(e)\n    graph_path += \'.gv\'\n\n    color = {\n        ""Input"": ""royalblue"",\n        ""Embedding"": ""orange"",\n        ""Linear"": ""tan"",\n        ""LinearAttention"": ""tan"",\n        ""BiGRU"": ""salmon"",\n        ""BiLSTM"": ""salmon"",\n        ""BiLSTMAtt"": ""salmon1"",\n        ""BiGRULast"": ""salmon"",\n        ""Conv"": ""sandybrown"",\n        ""ConvPooling"": ""sandybrown"",\n        ""Pooling"": ""skyblue"",\n        ""Dropout"": ""yellowgreen"",\n        ""Combination"": ""purple"",\n        ""EncoderDecoder"": ""lightsalmon"",\n        ""FullAttention"": ""lightsalmon"",\n        ""Seq2SeqAttention"": ""lightsalmon""\n    }\n    layer_conf = {\n        ""Linear"": [""hidden_dim"", ""activation"", ""last_hidden_activation"", ""last_hidden_softmax"", ""batch_normalization""],\n        ""LinearAttention"": [""keep_dim""],\n        ""BiGRU"": [""hidden_dim"", ""dropout""],\n        ""BiGRULast"": [""hidden_dim"", ""dropout""],\n        ""BiLSTM"": [""hidden_dim"", ""dropout"", ""num_layers""],\n        ""BiLSTMAtt"": [""hidden_dim"", ""dropout"", ""num_layers""],\n        ""Conv"": [""stride"", ""padding"", ""window_sizes"", ""input_channel_num"", ""output_channel_num"", ""activation"",\n                 ""batch_normalization""],\n        ""ConvPooling"": [""stride"", ""padding"", ""window_sizes"", ""input_channel_num"", ""output_channel_num"",\n                        ""batch_normalization"",\n                        ""activation"", ""pool_type"", ""pool_axis""],\n        ""Pooling"": [""pool_axis"", ""pool_type""],\n        ""Dropout"": [""dropout""],\n        ""Combination"": [""operations""],\n        ""EncoderDecoder"": [""encoder"", ""decoder""],\n        ""FullAttention"": [""hidden_dim"", ""activation""],\n        ""Seq2SeqAttention"": [""attention_dropout""]\n    }\n\n    model = Digraph(format=\'svg\',\n                    node_attr={""style"": ""rounded, filled"",\n                               ""shape"": ""box"",\n                               ""fontcolor"": ""white""})\n    model.attr(rankdir=""BT"")\n\n    for item in conf_dic[\'architecture\']:\n        if item[\'layer\'] == ""Embedding"":\n            for c in item[\'conf\']:\n                dim = item[\'conf\'][c][\'dim\']\n                for n in item[\'conf\'][c][\'cols\']:\n                    label_str = ""<"" \\\n                                + ""<table border=\'0.5\' align=\'center\'>"" \\\n                                + ""<tr><td align=\'text\'><i>"" + n + ""</i></td>"" + ""<td align=\'text\'><b>Embedding</b></td></tr>"" \\\n                                + ""<tr><td align=\'text\'>dim:</td>"" + ""<td align=\'text\'>"" + str(dim) + ""</td></tr>"" \\\n                                + ""</table>>""\n                    model.node(name=n, label=label_str, fillcolor=color[""Embedding""])\n            break\n\n    for inp in conf_dic[\'inputs\'][\'model_inputs\']:\n        model.node(name=inp,\n                   label=inp,\n                   fillcolor=color[\'Input\'])\n        for n in conf_dic[\'inputs\'][\'model_inputs\'][inp]:\n            model.edge(n, inp)\n\n    layer_dic = {}\n    for item in conf_dic[\'architecture\']:\n        if \'layer_id\' in item.keys() and \'layer\' in item.keys() and \'conf\' in item.keys():\n            layer_dic[item[\'layer_id\']] = [item[\'layer\'], item[\'conf\']]\n\n    for item in conf_dic[\'architecture\']:\n        if \'layer_id\' in item.keys():\n            if item[\'layer\'] in layer_dic:\n                tmp_layer = item[\'layer\']\n                item[\'conf\'] = layer_dic[tmp_layer][1]\n                item[\'layer\'] = layer_dic[tmp_layer][0]\n            label_str = ""<"" \\\n                        + ""<table border=\'0.5\' align=\'center\'>"" \\\n                        + ""<tr><td align=\'text\'><i>"" + item[\'layer_id\'] + ""</i></td>"" + ""<td align=\'text\'><b>"" + item[\n                            \'layer\'] + ""</b></td></tr>""\n            if item[\'layer\'] in layer_conf:\n                for c in layer_conf[item[\'layer\']]:\n                    if c in item[\'conf\']:\n                        label_str = label_str + ""<tr><td align=\'text\'>"" + c + ""</td>"" + ""<td align=\'text\'>"" + str(\n                            item[\'conf\'][c]) + ""</td></tr>""\n            else:\n                for c in item[\'conf\']:\n                    label_str = label_str + ""<tr><td align=\'text\'>"" + c + ""</td>"" + ""<td align=\'text\'>"" + str(\n                        item[\'conf\'][c]) + ""</td></tr>""\n\n            label_str += ""</table>>""\n\n            model.node(name=item[\'layer_id\'],\n                       label=label_str,\n                       fillcolor=color.get(item[\'layer\'], ""grey""))\n            for inp in item[\'inputs\']:\n                model.edge(inp, item[\'layer_id\'])\n    # model\n    model.render(graph_path, view=False)\n\n    return\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=\'get model graph\')\n    parser.add_argument(""--conf_path"", type=str, help=""JSON config path"")\n    parser.add_argument(""--graph_path"", type=str, default=""graph"", help=""Model graph path"")\n    args = parser.parse_args()\n    json2graph(args.conf_path, args.graph_path)\n    print(""The model graph has been successfully generated!"")\n\n'"
optimizers/__init__.py,1,b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\nfrom torch.optim import *'
preparation/__init__.py,0,b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.'
tools/calculate_auc.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport argparse\nfrom sklearn.metrics import roc_auc_score\n\ndef read_tsv(params):\n    prediction, label = [], []\n    predict_index, label_index = int(params.predict_index), int(params.label_index)\n    min_column_num = max(predict_index, label_index) + 1\n    with open(params.input_file, mode=\'r\', encoding=\'utf-8\') as f:\n        for index, line in enumerate(f):\n            if params.header and index == 0:\n                continue\n            line = line.rstrip()\n            # skip empty line\n            if not line:\n                continue\n            line = line.split(\'\\t\')\n            if len(line) < min_column_num:\n                print(""at line:%s, %s""%(predict_index, line))\n                raise Exception(""the given index of predict or label is exceed the index of the column"")\n            prediction.append(float(line[predict_index]))\n            label.append(int(line[label_index]))\n    return prediction, label\n            \ndef calculate_AUC(prediction, label):\n    return roc_auc_score(label, prediction)\n\ndef main(params):\n    prediction, label = read_tsv(params)\n    auc = calculate_AUC(prediction, label)\n    print(""AUC is "", auc)\n    return auc\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=""AUC"")\n    parser.add_argument(""--input_file"", type=str, help=""tsv file"")\n    parser.add_argument(""--predict_index"", type=str, help=""the column index of prediction of model, start from 0"")\n    parser.add_argument(""--label_index"", type=str, help=""the column index of label, start from 0"")\n    parser.add_argument(""--header"", action=\'store_true\', default=False, help=""whether contains header row or not, default is False"")\n\n    params, _ = parser.parse_known_args()\n\n    assert params.input_file, \'Please specify a input file via --input_file\'\n    assert params.predict_index, \'Please specify the column index of prediction via --predict_index\'\n    assert params.label_index, \'Please specify the column index of label via --label_index\'\n    main(params)'"
tools/tagging_schemes_converter.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport sys\n\n\ndef BIO2BIOES(input_labels_list):\n    output_labels_list = []\n    for labels in input_labels_list:\n        new_labels = []\n        sent_len = len(labels)\n        for idx in range(sent_len):\n            if ""-"" not in labels[idx]:\n                new_labels.append(labels[idx])\n            else:\n                label_type = labels[idx].split(\'-\')[-1]\n                if ""B-"" in labels[idx]:\n                    if (idx == sent_len - 1) or (""I-"" not in labels[idx + 1]):\n                        new_labels.append(""S-""+label_type)\n                    else:\n                        new_labels.append(""B-""+label_type)\n                elif ""I-"" in labels[idx]:\n                    if (idx == sent_len - 1) or (""I-"" not in labels[idx + 1]):\n                        new_labels.append(""E-""+label_type)\n                    else:\n                        new_labels.append(""I-""+label_type)\n        assert len(labels) == len(new_labels)\n        output_labels_list.append(new_labels)\n    return output_labels_list\n\n\ndef BIOES2BIO(input_labels_list):\n    output_labels_list = []\n    for labels in input_labels_list:\n        new_labels = []\n        sent_len = len(labels)\n        for idx in range(sent_len):\n            if ""-"" not in labels[idx]:\n                new_labels.append(labels[idx])\n            else:\n                label_type = labels[idx].split(\'-\')[-1]\n                if ""E-"" in labels[idx]:\n                    new_labels.append(""I-"" + label_type)\n                elif ""S-"" in labels[idx]:\n                    new_labels.append(""B-"" + label_type)\n                else:\n                    new_labels.append(labels[idx])\n        assert len(labels) == len(new_labels)\n        output_labels_list.append(new_labels)\n    return output_labels_list\n\n\ndef IOB2BIO(input_labels_list):\n    output_labels_list = []\n    for labels in input_labels_list:\n        new_labels = []\n        sent_len = len(labels)\n        for idx in range(sent_len):\n            if ""I-"" in labels[idx]:\n                label_type = labels[idx].split(\'-\')[-1]\n                if (idx == 0) or (labels[idx - 1] == ""O"") or (label_type != labels[idx - 1].split(\'-\')[-1]):\n                    new_labels.append(""B-"" + label_type)\n                else:\n                    new_labels.append(labels[idx])\n            else:\n                new_labels.append(labels[idx])\n        assert len(labels) == len(new_labels)\n        output_labels_list.append(new_labels)\n    return output_labels_list\n\n\nif __name__ == \'__main__\':\n    \'\'\'Convert NER tagging schemes among IOB/BIO/BIOES.\n        For example: if you want to convert the IOB tagging scheme to BIO, then you run as following:\n            python taggingSchemes_Converter.py IOB2BIO input_iob_file output_bio_file\n        Input data format is tsv format.\n    \'\'\'\n    input_file_name, output_file_name = sys.argv[2], sys.argv[3]\n    words_list, labels_list, new_labels_list = [], [], []\n    with open(input_file_name, \'r\') as input_file:\n        for line in input_file:\n            item = line.rstrip().split(\'\\t\')\n            assert len(item) == 2\n            words, labels = item[0].split(\' \'), item[1].split(\' \')\n            if len(words) != len(labels):\n                print(""Error line: "" + line.rstrip())\n                continue\n            words_list.append(words)\n            labels_list.append(labels)\n\n    if sys.argv[1].upper() == ""IOB2BIO"":\n        print(""Convert IOB -> BIO..."")\n        new_labels_list = IOB2BIO(labels_list)\n    elif sys.argv[1].upper() == ""BIO2BIOES"":\n        print(""Convert BIO -> BIOES..."")\n        new_labels_list = BIO2BIOES(labels_list)\n    elif sys.argv[1].upper() == ""BIOES2BIO"":\n        print(""Convert BIOES -> BIO..."")\n        new_labels_list = BIOES2BIO(labels_list)\n    elif sys.argv[1].upper() == ""IOB2BIOES"":\n        print(""Convert IOB -> BIOES..."")\n        tmp_labels_list = IOB2BIO(labels_list)\n        new_labels_list = BIO2BIOES(tmp_labels_list)\n    else:\n        print(""Argument error: sys.argv[1] should belongs to \\""IOB2BIO/BIO2BIOES/BIOES2BIO/IOB2BIOES\\"""")\n\n    with open(output_file_name, \'w\') as output_file:\n        for index in range(len(words_list)):\n            words, labels = words_list[index], new_labels_list[index]\n            line = "" "".join(words) + \'\\t\' + "" "".join(labels) + \'\\n\'\n            output_file.write(line)\n\n'"
utils/BPEEncoder.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport nltk\n\ndef get_pairs(word):\n    """""" Return set of symbol pairs in a word.\n    word is represented as tuple of symbols (symbols being variable-length strings)\n    """"""\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\nclass BPEEncoder(object):\n    """""" Byte Pair Encoding\n    """"""\n    def __init__(self, bpe_path):\n        merges = open(bpe_path, encoding=\'utf-8\').read().split(\'\\n\')[1:-1]\n        merges = [tuple(merge.split()) for merge in merges]\n        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n        self.cache = dict()\n\n    def encode(self, sentence):\n        tokens = nltk.word_tokenize(sentence)\n        bpe_tokens = []\n        for token in tokens:\n            bpe_tokens.extend(self.bpe(token))\n\n        return bpe_tokens\n\n    def bpe(self, token):\n        """"""\n\n        Args:\n            token (string): a word token\n\n        Returns:\n            list: byte pair encodings\n\n        """"""\n        word = tuple(token[:-1]) + (token[-1] + \'</w>\',)\n        if token in self.cache:\n            return self.cache[token]\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token+\'</w>\'\n\n        while True:\n            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float(\'inf\')))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n                    new_word.append(first+second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = \' \'.join(word)\n        if word == \'\\n  </w>\':\n            word = \'\\n</w>\'\n        self.cache[token] = word\n        return word.split(\' \')\n\n\nif __name__ == \'__main__\':\n    sentences = \'trip cost to beijing\'\n    import nltk\n    tokens = nltk.word_tokenize(sentences)\n    bpe_encoder = BPEEncoder(\'../dataset/bpe/vocab_40000.bpe\')\n    bpe_tokens = []\n    for token in tokens:\n        print(token)\n        bpe_tokens.extend(bpe_encoder.bpe(token))\n    print(bpe_tokens)\n'"
utils/DocInherit.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nfrom functools import wraps\n\nclass DocInherit(object):\n    """"""\n    Docstring inheriting method descriptor\n    The class itself is also used as a decorator doc_inherit decorator\n\n    """"""\n\n    def __init__(self, mthd):\n        self.mthd = mthd\n        self.name = mthd.__name__\n\n    def __get__(self, obj, cls):\n        if obj:\n            return self.get_with_inst(obj, cls)\n        else:\n            return self.get_no_inst(cls)\n\n    def get_with_inst(self, obj, cls):\n        #overridden = getattr(super(cls, obj), self.name, None)\n        for parent in cls.__mro__[1:]:\n            overridden = getattr(parent, self.name, None)\n            if overridden:\n                break\n\n        @wraps(self.mthd, assigned=(\'__name__\', \'__module__\', \'__doc__\'))\n        def f(*args, **kwargs):\n            return self.mthd(obj, *args, **kwargs)\n\n        return self.use_parent_doc(f, overridden)\n\n    def get_no_inst(self, cls):\n        for parent in cls.__mro__[1:]:\n            overridden = getattr(parent, self.name, None)\n            if overridden:\n                break\n\n        @wraps(self.mthd, assigned=(\'__name__\', \'__module__\', \'__doc__\'))\n        def f(*args, **kwargs):\n            return self.mthd(*args, **kwargs)\n\n        return self.use_parent_doc(f, overridden)\n\n    def use_parent_doc(self, func, source):\n        if source is None:\n            raise NameError(""Can\'t find \'%s\' in parents"" % self.name)\n        if func.__doc__ is None:\n            func.__doc__ = source.__doc__\n        return func\n\n'"
utils/ProcessorsScheduler.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport multiprocessing\nfrom multiprocessing import cpu_count\nimport math\n\nclass ProcessorsScheduler(object):\n    process_num = cpu_count()\n\n    def __init__(self, cpu_num_workers=None):\n        if cpu_num_workers != None and cpu_num_workers > 0:\n            self.process_num = cpu_num_workers\n\n    def run_data_parallel(self, func, func_args):\n        data, rest_args =  func_args[0], func_args[1:]\n        res = []\n        # logging.info(""multiprocess enabled, process num: %d"" % (self.process_num))\n        process_p = multiprocessing.Pool(self.process_num)\n        data_length = len(data)\n        size = math.ceil(data_length/ self.process_num)\n        \n        for i in range(self.process_num):\n            start = size * i\n            end = (i + 1) * size if (i + 1) * size < data_length else data_length\n            args = (data[start:end], ) + rest_args\n            res.append((i, process_p.apply_async(func, args=args)))\n        process_p.close()\n        process_p.join()\n        res = sorted(res, key=lambda x:x[0])\n        return res\n'"
utils/__init__.py,0,b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.'
utils/common_utils.py,2,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport logging\nimport pickle as pkl\nimport json\nimport torch\nimport torch.nn as nn\nimport os\nimport shutil\nimport time\nimport tempfile\nimport subprocess\nimport hashlib\nfrom .exceptions import ConfigurationError\n\ndef log_set(log_path, console_level=\'INFO\', console_detailed=False, disable_log_file=False):\n    """"""\n\n    Args:\n        log_path:\n        console_level: \'INFO\', \'DEBUG\'\n\n    Returns:\n\n    """"""\n    if not disable_log_file:\n        logging.basicConfig(filename=log_path, filemode=\'w\',\n            format=\'%(asctime)s %(levelname)s %(filename)s %(funcName)s %(lineno)d: %(message)s\',\n            level=logging.DEBUG)\n    console = logging.StreamHandler()\n    console.setLevel(getattr(logging, console_level.upper()))\n    if console_detailed:\n        console.setFormatter(logging.Formatter(\n            \'%(asctime)s %(levelname)s %(filename)s %(funcName)s %(lineno)d: %(message)s\'))\n    else:\n        console.setFormatter(logging.Formatter(\n            \'%(asctime)s %(levelname)s %(message)s\'))\n    logging.getLogger().addHandler(console)\n\n\ndef load_from_pkl(pkl_path, debug=True):\n    with open(pkl_path, \'rb\') as fin:\n        obj = pkl.load(fin)\n    if debug:\n        logging.debug(""%s loaded!"" % pkl_path)\n    return obj\n\n\ndef dump_to_pkl(obj, pkl_path, debug=True):\n    with open(pkl_path, \'wb\') as fout:\n        pkl.dump(obj, fout, protocol=pkl.HIGHEST_PROTOCOL)\n    if debug:\n        logging.debug(""Obj dumped to %s!"" % pkl_path)\n\ndef load_from_json(json_path, debug=True):\n    data = None\n    with open(json_path, \'r\', encoding=\'utf-8\') as f:\n        try:\n            data = json.loads(f.read())\n        except Exception as e:\n            raise ConfigurationError(""%s is not a legal JSON file, please check your JSON format!"" % json_path)\n    if debug:\n        logging.debug(""%s loaded!"" % json_path)\n    return data\n\ndef dump_to_json(obj, json_path, debug=True):\n    with open(json_path, \'w\', encoding=\'utf-8\') as f:\n        f.write(json.dumps(obj))\n    if debug:\n        logging.debug(""Obj dumped to %s!"" % json_path)\n\ndef get_trainable_param_num(model):\n    """""" get the number of trainable parameters\n\n    Args:\n        model:\n\n    Returns:\n\n    """"""\n    if isinstance(model, nn.DataParallel):\n        if isinstance(model.module.layers[\'embedding\'].embeddings, dict):\n            model_param = list(model.parameters()) + list(model.module.layers[\'embedding\'].get_parameters())\n        else:\n            model_param = list(model.parameters())\n    else:\n        if isinstance(model.layers[\'embedding\'].embeddings, dict):\n            model_param = list(model.parameters()) + list(model.layers[\'embedding\'].get_parameters())\n        else:\n            model_param = list(model.parameters())\n\n    return sum(p.numel() for p in model_param if p.requires_grad)\n\n\ndef get_param_num(model):\n    """""" get the number of parameters\n\n    Args:\n        model:\n\n    Returns:\n\n    """"""\n    return sum(p.numel() for p in model.parameters())\n\n\ndef transfer_to_gpu(cpu_element):\n    """"""\n\n    Args:\n        cpu_element: either a tensor or a module\n\n    Returns:\n\n    """"""\n    return cpu_element.to(torch.device(""cuda"" if torch.cuda.is_available() else ""cpu""))\n\n\ndef transform_params2tensors(inputs, lengths):\n    """""" Because DataParallel only splits Tensor-like parameters, we have to transform dict parameter into tensors and keeps the information for forward().\n\n    Args:\n        inputs: dict.\n                {\n                    ""string1"":{\n                        \'word\': word ids, [batch size, seq len]\n                        \'postag\': postag ids,[batch size, seq len]\n                        ...\n                    }\n                    ""string2"":{\n                        \'word\': word ids,[batch size, seq len]\n                        \'postag\': postag ids,[batch size, seq len]\n                        ...\n                    }\n                }\n        lengths: dict.\n                {\n                    ""string1"": [...]\n                    ""string2"": [...]\n                }\n\n    Returns:\n        param_list (list): records all the tensors in inputs and lengths\n        inputs_desc (dict): the key records the information of inputs, the value indicate the index of a tensor in param_list\n            e.g. {\n                ""string1_word"": index in the param_list\n                ""string1_postag"": index in the param_list\n                ...\n            }\n        lengths_desc (dict): similar to inputs_desc\n            e.g. {\n                ""string1"": index in the param_list,\n                ""string2"": index in the param_list\n            }\n\n    """"""\n    param_list = []\n    inputs_desc = {}\n    cnt = 0\n    for input in inputs:\n        for input_type in inputs[input]:\n            inputs_desc[input + \'___\' + input_type] = cnt\n            param_list.append(inputs[input][input_type])\n            cnt += 1\n\n    length_desc = {}\n    for length in lengths:\n        if isinstance(lengths[length], dict):\n            for length_type in lengths[length]:\n                length_desc[length + \'__\' + length_type] = cnt\n                param_list.append(lengths[length][length_type])\n        else:\n            length_desc[length] = cnt\n            param_list.append(lengths[length])\n        cnt += 1\n\n    return param_list, inputs_desc, length_desc\n\n\ndef transform_tensors2params(inputs_desc, length_desc, param_list):\n    """""" Inverse function of transform_params2tensors\n\n    Args:\n        param_list:\n        inputs_desc:\n        length_desc:\n\n    Returns:\n\n    """"""\n    inputs = {}\n    for key in inputs_desc:\n        input, input_type = key.split(\'___\')\n        if not input in inputs:\n            inputs[input] = dict()\n\n        inputs[input][input_type] = param_list[inputs_desc[key]]\n\n    lengths = {}\n    for key in length_desc:\n        if \'__\' in key:\n            input, input_type = key.split(\'__\')\n            if not input in lengths:\n                lengths[input] = dict()\n            lengths[input][input_type] = param_list[length_desc[key]]\n        else:\n            lengths[key] = param_list[length_desc[key]]\n\n    return inputs, lengths\n\n\ndef prepare_dir(path, is_dir, allow_overwrite=False, clear_dir_if_exist=False, extra_info=None):\n    """""" to make dir if a dir or the parent dir of a file does not exist\n\n    Args:\n        path: can be a file path or a dir path.\n\n    Returns:\n\n    """"""\n    if is_dir:\n        if clear_dir_if_exist:\n            allow_overwrite = True\n\n        if not os.path.exists(path):\n            os.makedirs(path)\n        else:\n            if not allow_overwrite:\n                overwrite_option = input(\'The directory %s already exists, input ""yes"" to allow us to overwrite the directory contents and ""no"" to exit. (default:no): \' % path) \\\n                    if not extra_info else \\\n                    input(\'The directory %s already exists, %s, \\ninput ""yes"" to allow us to operate and ""no"" to exit. (default:no): \' % (path, extra_info))\n                if overwrite_option.lower() != \'yes\':\n                    exit(0)\n            if (allow_overwrite or overwrite_option == \'yes\') and clear_dir_if_exist:\n                shutil.rmtree(path)\n                logging.info(\'Clear dir %s...\' % path)\n                while os.path.exists(path):\n                    time.sleep(0.3)\n                os.makedirs(path)\n    else:\n        dir = os.path.dirname(path)\n        if dir == \'\':       # when the path is only a file name, the dir would be empty and raise exception when making dir\n            dir = \'.\'\n        if not os.path.exists(dir):\n            os.makedirs(dir)\n        else:\n            if os.path.exists(path) and allow_overwrite is False:\n                overwrite_option = input(\'The file %s already exists, input ""yes"" to allow us to overwrite it or ""no"" to exit. (default:no): \' % path)\n                if overwrite_option.lower() != \'yes\':\n                    exit(0)\n\ndef md5(file_paths, chunk_size=1024*1024*1024):\n    """""" Calculate a md5 of lists of files. \n\n    Args:\n        file_paths:  an iterable object contains file paths. Files will be concatenated orderly if there are more than one file\n        chunk_size:  unit is byte, default value is 1GB\n    Returns:\n        md5\n\n    """"""\n    md5 = hashlib.md5()\n    for path in file_paths:\n        with open(path, \'rb\') as fin:\n            while True:\n                data = fin.read(chunk_size)\n                if not data:\n                    break\n                md5.update(data)\n    return md5.hexdigest()\n\n\ndef get_layer_class(model, layer_id):\n    """"""get the layer class use layer_id\n\n    Args:\n        model: the model architecture, maybe nn.DataParallel type or model\n        layer_id: layer id from configuration\n    """"""\n    if isinstance(model, nn.DataParallel):\n        return model.module.layers[layer_id]\n    else:\n        return model.layers[layer_id]'"
utils/corpus_utils.py,9,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nfrom __future__ import absolute_import\n\nimport string\nimport sys\nimport numpy as np\nimport logging\nimport math\nimport warnings\nwarnings.filterwarnings(action=\'ignore\', category=UserWarning, module=\'gensim\')     # remove gensim warning\nfrom gensim.models.keyedvectors import KeyedVectors\nfrom gensim.models.fasttext import FastText\nimport codecs\nimport copy\nfrom settings import ProblemTypes\nimport torch\nimport time\n\n\nif sys.version_info < (3,):\n    maketrans = string.maketrans\nelse:\n    maketrans = str.maketrans\n\n\ndef load_embedding(embedding_path, embedding_dim, format, file_type, with_head=False, word_set=None):\n    """"""\n    Args:\n        format: \'glove\', \'word2vec\', \'fasttext\'\n        file_type: \'text\' or \'binary\'\n    """"""\n    embedding_dict = dict()\n\n    if format == \'word2vec\' or format == \'fasttext\':\n        if file_type == \'text\':\n            vector_total = KeyedVectors.load_word2vec_format(embedding_path, binary=False, unicode_errors=\'ignore\')\n        else:\n            if format == \'word2vec\':\n                vector_total = KeyedVectors.load_word2vec_format(embedding_path, binary=True, unicode_errors=\'ignore\')\n            elif format == \'fasttext\':\n                vector_total = FastText.load_fasttext_format(embedding_path, encoding=\'utf8\')\n\n        assert vector_total.vector_size == embedding_dim\n        if word_set is None:\n            embedding_dict = vector_total\n        else:\n            if not (format == \'fasttext\' and file_type == \'binary\'):\n                word_total = vector_total.index2word    # actually, vector_total.index2word is the word list\n            else:\n                word_total = vector_total.wv.index2word\n            for word in word_total:\n                if word in word_set:\n                    embedding_dict[word] = vector_total[word]\n    elif format == \'glove\':\n        with codecs.open(embedding_path, \'r\', encoding=\'utf-8\') as fin:\n            if with_head == True:\n                _ = fin.readline()\n            for idx, line in enumerate(fin):\n                line = line.rstrip()\n                if idx == 0 and len(line.split()) == 2:\n                    continue\n                if len(line) > 0:\n                    word, vec = line.split("" "", 1)\n                    if (word_set and word in word_set) or (word_set is None):\n                        vector = [float(num) for num in vec.split("" "")]\n                        assert len(vector) == embedding_dim\n                        embedding_dict[word] = vector\n    else:\n        raise Exception(\'The format supported are glove, word2vec or fasttext, dost not support %s now.\' % format)\n    return embedding_dict\n\n\ndef split_array(arr, n, small_chunk_threshold=0):\n    """"""split the array, each chunk has n elements (the last chunk might be different)\n    Args:\n        arr: the list to chunk, can be python list or numpy array\n        n:   number of elements in a chunk\n        small_chunk_threshold: chunks have less than {small_chunk_threshold} elements are forbiddened.\n                if the last chunk has less than {small_chunk_threshold} elements, merge them into the former chunk.\n    """"""\n    result = [arr[i:i + n] for i in range(0, len(arr), n)] #namely, small_chunk_threshold = 0\n    if len(result[-1]) < small_chunk_threshold:\n        if isinstance(result[-2], np.ndarray) == True:\n            result[-2] = result[-2].tolist()\n\n        result[-2].extend(result[-1])   #result[-1] can be either python list or np.ndarray\n        result[-2] = np.array(result[-2])\n        logging.debug(""The last chunk of size %d is smaller than the small_chunk_threshold %d, so merge it to chunk[-2]"" % (len(result[-1]), small_chunk_threshold))\n        logging.debug(""Now the size of chunk[-2] is increase from %d to %d"" % (n, len(result[-2])))\n        del result[-1]\n    return result\n\n\ndef split_array_averagely(arr, m):\n    """""" split the array to n small chunks with nearly the same sizes.\n\n    Args:\n        arr:\n        m:\n\n    Returns:\n\n    """"""\n    n = int(math.ceil(len(arr) / float(m)))\n    return [arr[i:i + n] for i in range(0, len(arr), n)]\n\n\ndef cut_and_padding(seq, max_len, pad=0):\n    """"""\n    cut or pad the sequence to fixed size\n    Args:\n        seq:     sequence\n        max_len:    the fixed size specified\n        pad:    symbol to pad\n    Returns:\n        fixed size sequence\n    """"""\n    if len(seq) >= max_len:\n        return seq[:max_len]\n    else:\n        return seq + [pad] * (max_len - len(seq))\n\n\ndef to_categorical(y, nb_classes=None):\n    \'\'\'Convert class vector (integers from 0 to nb_classes)\n    to binary class matrix, for use with categorical_crossentropy.\n    \'\'\'\n    if not nb_classes:\n        nb_classes = np.max(y)+1\n    Y = np.zeros((len(y), nb_classes))\n    for i in range(len(y)):\n        Y[i, y[i]] = 1.\n    return Y\n\n\n\ndef base_filter():\n    f = string.punctuation\n    f = f.replace(""\'"", \'\')\n    f += \'\\t\\n\'\n    return f\n\n\ndef text_to_word_sequence(text, filters=base_filter(), lower=True, split="" ""):\n    \'\'\'prune: sequence of characters to filter out\n    \'\'\'\n    if lower:\n        text = text.lower()\n    text = text.translate(maketrans(filters, split*len(filters)))\n    seq = text.split(split)\n    return [_f for _f in seq if _f]\n\n\ndef corpus_permutation(*corpora):\n    """"""\n\n    Args:\n        *corpora:  different fields of a corpus\n\n    Returns:\n\n    """"""\n    logging.info(""Start permutation"")\n    perm = np.random.permutation(len(corpora[0]))\n\n    corpora_perm = []\n    for i in range(len(corpora)):\n        corpora_perm.append(np.array(corpora[i])[perm])\n\n    logging.info(""Permutation end!"")\n\n    return corpora_perm\n\n\ndef get_batches(problem, data, length, target, batch_size, input_types, pad_ids=None, permutate=False, transform_tensor=True, predict_mode=\'batch\'):\n    """"""\n\n    Args:\n        data:\n                {\n                \'string1\': {\n                    \'word1\': [...],\n                    \'postage_feature1\': [..]\n                    }\n                \'string2\': {\n                    \'word1\': [...],\n                    \'postage_feature1\': [..]\n                }\n        lengths:\n                {\n                \'string1\':   [...],\n                \'string2\':   [...]\n                }\n        target:  [...]\n        input_types:  {\n                  ""word"": {\n                    ""cols"": [\n                      ""word1"",\n                      ""word2""\n                    ],\n                    ""dim"": 300\n                  },\n                  ""postag"": {\n                    ""cols"": [""postag_feature1"", ""postag_feature2""],\n                    ""dim"": 20\n                  }\n        permutate: shuffle data\n        transform_tensor: if True the data returned would be Variables in CPU (except sentence length), otherwise the data would be numpy array\n\n    Returns:\n        data_batches: each element is a batch of data\n            [\n                {\n                    ""string1"":{\n                        \'word\': ndarray/Variable, shape:[batch_size, seq_len],\n                        \'postag\': ndarray/Variable, postag ids, shape: [batch_size, seq_len],\n                        ...\n                    }\n                    ""string2"":{\n                        \'word\': ndarray/Variable, shape:[batch_size, seq_len],\n                        \'postag\': ndarray/Variable, postag ids, shape: [batch_size, seq_len],\n                        ...\n                    }\n                }\n                ...\n            ]\n        length_batches: {\n            \'string1"": ndarray, [number of batches, batch size]\n            \'string2"": ndarray, [number of batches, batch size]\n        }\n        target_batches: ndarray/Variable shape: [number of batches, batch_size, targets]\n\n    """"""\n    if predict_mode == \'batch\':\n        logging.info(""Start making batches"")\n    if permutate is True:\n        #CAUTION! data and length would be revised\n        # data = copy.deepcopy(data)\n        # length = copy.deepcopy(length)\n        # if target is not None:\n        #     target = copy.deepcopy(target)\n\n        # shuffle the data\n        permutation = np.random.permutation(len(list(target.values())[0]))\n        for input_cluster in data:\n            for input in data[input_cluster]:\n                data[input_cluster][input] = np.array(data[input_cluster][input])[permutation]\n            for single_type in length[input_cluster]:\n                length[input_cluster][single_type] = np.array(length[input_cluster][single_type])[permutation]\n        if target is not None:\n            for single_target in target:\n                length[\'target\'][single_target] = np.array(length[\'target\'][single_target])[permutation]\n                target[single_target] = np.array(target[single_target])[permutation]\n    else:\n        for input_cluster in data:\n            for input in data[input_cluster]:\n                data[input_cluster][input] = np.array(data[input_cluster][input])\n            for single_type in length[input_cluster]:\n                length[input_cluster][single_type] = np.array(length[input_cluster][single_type])\n        if target is not None:\n            for single_target in target:\n                length[\'target\'][single_target] = np.array(length[\'target\'][single_target])\n                target[single_target] = np.array(target[single_target])\n\n    # set up padding symbols for inputs and target\n    if pad_ids is None:\n        pad_ids = dict()\n        for branch in input_types:\n            pad_ids[branch] = problem.input_dicts[branch].id(\'<pad>\')\n\n        if ProblemTypes[problem.problem_type] == ProblemTypes.sequence_tagging:\n            #pad_ids[\'target\'] = problem.output_dict.id(\'O\')\n            if problem.target_with_pad:\n                pad_ids[\'target\'] = problem.output_dict.id(\'<pad>\')\n            else:\n                pad_ids[\'target\'] = 0       # CAUTION\n        elif ProblemTypes[problem.problem_type] == ProblemTypes.classification:\n            if problem.target_with_pad:\n                pad_ids[\'target\'] = problem.output_dict.id(\'<pad>\')       # CAUTION\n            else:\n                pad_ids[\'target\'] = 0       # CAUTION\n        elif ProblemTypes[problem.problem_type] == ProblemTypes.regression:\n            pad_ids[\'target\'] = None\n        elif ProblemTypes[problem.problem_type] == ProblemTypes.mrc:\n            pass\n    type2cluster = dict()       # type2cluster[\'word1\'] = \'word\'\n    for input_type in input_types:\n        for col_name in input_types[input_type][\'cols\']:\n            type2cluster[col_name] = input_type\n\n    # get the corpus size\n    for input_cluster in data:\n        for input_type in data[input_cluster]:\n            corpus_size = len(data[input_cluster][input_type])\n            break\n        break\n\n    data_batches = []\n    if target is not None:\n        target_batches = []\n    else:\n        target_batches = None\n    length_batches = []\n    for stidx in range(0, corpus_size, batch_size):\n        data_batch = dict()\n        length_batch = dict()\n\n        for input_cluster in data:\n            data_batch[input_cluster] = dict()\n            length_batch[input_cluster] = dict()\n            max_sen_len_cur_batch = None\n            max_word_len_cur_batch = None\n            if transform_tensor is True:\n                # For nn.DataParallel, the length must be Variable as well, otherwise the length would not split for multiple GPUs\n                #length_batch[input_cluster] = Variable(torch.LongTensor(length[input_cluster][stidx: stidx + batch_size]))\n                for single_input_cluster in length[input_cluster]:\n                    if not isinstance(length[input_cluster][single_input_cluster][0], list):\n                        length_batch[input_cluster][single_input_cluster] = \\\n                            torch.LongTensor(np.array(length[input_cluster][single_input_cluster][stidx: stidx + batch_size]))\n                    else:\n                        length_batch[input_cluster][single_input_cluster] = []\n                        for single_iterm in length[input_cluster][single_input_cluster][stidx: stidx + batch_size]:\n                            length_batch[input_cluster][single_input_cluster].append(torch.LongTensor(np.array(single_iterm)))\n            else:\n                for single_input_cluster in length[input_cluster]:\n                    length_batch[input_cluster][single_input_cluster] = \\\n                        np.array(length[input_cluster][single_input_cluster][stidx: stidx + batch_size])\n\n            # max_len_cur_batch = np.sort(length[input_cluster][stidx: stidx + batch_size])[-1]\n            for single_input_cluster in length[input_cluster]:\n                if \'sentence\' in single_input_cluster:\n                    max_sen_len_cur_batch = np.sort(length[input_cluster][single_input_cluster][stidx: stidx + batch_size])[-1]\n                elif \'word\' in single_input_cluster:\n                    max_word_len_cur_batch = np.sort([y for x in length[input_cluster][single_input_cluster][stidx: stidx + batch_size] for y in x])[-1]\n            #logging.info(""stidx: %d, max_len: %d"" % (stidx, max_len_cur_batch))\n            for input_type in data[input_cluster]:\n                if input_type in type2cluster:\n                    batch_with_pad = []\n                    # process char data\n                    if \'char\' in input_type.lower():\n                        for seq in data[input_cluster][input_type][stidx: stidx + batch_size]:\n                            batch_char_pad = []\n                            for seq_index in range(max_sen_len_cur_batch):\n                                if seq_index < len(seq):\n                                    batch_char_pad.append(cut_and_padding(seq[seq_index], max_word_len_cur_batch, pad_ids[type2cluster[input_type]]))\n                                else:\n                                    batch_char_pad.append(cut_and_padding([pad_ids[type2cluster[input_type]]], max_word_len_cur_batch, pad_ids[type2cluster[input_type]]))\n                            batch_with_pad.append(batch_char_pad)\n                    else:\n                        for seq in data[input_cluster][input_type][stidx: stidx + batch_size]:\n                        #batch_with_pad.append(cut_and_padding(seq, max_len_cur_batch, pad_ids[input_type]))\n                            batch_with_pad.append(cut_and_padding(seq, max_sen_len_cur_batch, pad_ids[type2cluster[input_type]]))\n                    if transform_tensor is True:\n                        data_batch[input_cluster][type2cluster[input_type]] = torch.LongTensor(batch_with_pad)\n                    else:\n                        data_batch[input_cluster][type2cluster[input_type]] = np.array(batch_with_pad)\n                else:\n                    data_batch[input_cluster][input_type] = data[input_cluster][input_type][stidx: stidx + batch_size]\n            # word_length is used for padding char sequence, now only save sentence_length\n            length_batch[input_cluster] = length_batch[input_cluster][\'sentence_length\']\n\n        data_batches.append(data_batch)\n        length_batches.append(length_batch)\n\n        if target is not None:\n            target_batch = {}\n            length_batch[\'target\'] = {}\n            for single_target in target:\n                if transform_tensor is True:\n                    length_batch[\'target\'][single_target] = torch.LongTensor(np.array(length[\'target\'][single_target][stidx: stidx + batch_size]))\n                else:\n                    length_batch[\'target\'][single_target] = np.array(length[\'target\'][single_target][stidx: stidx + batch_size])\n                if not (isinstance(target[single_target][0], list) or isinstance(target[single_target][0], np.ndarray)):\n                    target_batch[single_target] = target[single_target][stidx: stidx + batch_size]\n                else:\n                    # target is also a sequence, padding needed\n                    temp_target_batch = []\n                    for seq in target[single_target][stidx: stidx + batch_size]:\n                        temp_target_batch.append(cut_and_padding(seq, max_sen_len_cur_batch, pad_ids[\'target\']))\n                    target_batch[single_target] = temp_target_batch\n                if transform_tensor is True:\n                    if ProblemTypes[problem.problem_type] == ProblemTypes.classification \\\n                            or ProblemTypes[problem.problem_type] == ProblemTypes.sequence_tagging:\n                        target_batch[single_target] = torch.LongTensor(target_batch[single_target])\n                    elif ProblemTypes[problem.problem_type] == ProblemTypes.regression:\n                        target_batch[single_target] = torch.FloatTensor(target_batch[single_target])\n                    elif ProblemTypes[problem.problem_type] == ProblemTypes.mrc:\n                        if not isinstance(target_batch[single_target][0], str):\n                            target_batch[single_target] = torch.LongTensor(target_batch[single_target])\n                else:\n                    target_batch[single_target] = np.array(target_batch[single_target])\n\n            target_batches.append(target_batch)\n\n    if predict_mode == \'batch\':\n        logging.info(""Batches got!"")\n    return data_batches, length_batches, target_batches\n\n\ndef get_seq_mask(seq_len, max_seq_len=None):\n    """"""\n\n    Args:\n        seq_len (ndarray): 1d numpy array/list\n\n    Returns:\n        ndarray : 2d array seq_len_mask. the mask symbol for a real token is 1 and for <pad> is 0.\n\n    """"""\n    if torch.is_tensor(seq_len):\n        seq_len = seq_len.cpu().numpy()\n\n    if max_seq_len is None:\n        max_seq_len = seq_len.max()\n    masks = np.array([[1]*seq_len[i] + [0] * (max_seq_len - seq_len[i]) for i in range(len(seq_len))])\n    return masks\n\n\n\nif __name__ == ""__main__"":\n    \'\'\'\n    y = [1, 0, 1, 0]\n    y_convert = to_categorical(y, 2)\n    print(y_convert)\n    \'\'\'\n\n    \'\'\'\n    load_embedding(r\'/data/t-wulin/data/embeddings/glove/glove.840B.300d.txt\', 300, \'glove\', \'text\', word_set=None)\n    print(\'glove text loaded\')\n    load_embedding(r\'/data/t-wulin/data/embeddings/GoogleNews-vectors-negative300.bin\', 300, \'word2vec\', \'binary\', word_set=None)\n    print(\'word2vec bin loaded\')\n    load_embedding(r\'/data/t-wulin/data/embeddings/fasttext.wiki.en/wiki.en.bin\', 300, \'fasttext\', \'binary\', word_set=None)\n    print(\'fasttext bin loaded\')\n    load_embedding(r\'/data/t-wulin/data/embeddings/fasttext.wiki.en/wiki.en.vec\', 300, \'fasttext\', \'text\', word_set=None)\n    print(\'fasttext text loaded\')\n    \'\'\'\n    load_embedding(r\'/data/t-wulin/data/embeddings/fasttext.wiki.en/wiki.en.bin\', 300, \'fasttext\', \'binary\', word_set=None)\n    print(\'fasttext bin loaded\')\n\n\n'"
utils/exceptions.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport sys\n\n\nclass BaseError(RuntimeError):\n    """""" Error base class\n\n    """"""\n    def __init__(self, arg, err_id=None):\n        self.arg = arg\n        self.err_id = err_id\n\n    def __str__(self):\n        if self.err_id is None:\n            return self.arg\n        else:\n            return ""error=%d, %s"" % (self.err_id, self.arg)\n\n\nclass LayerConfigUndefinedError(BaseError):\n    """""" Errors occur when the corresponding configuration class of a layer is not defined\n\n    """"""\n    pass\n\n\nclass LayerUndefinedError(BaseError):\n    """""" Errors occur when some undefined layers are used\n\n    """"""\n    pass\n\n\nclass LayerDefineError(BaseError):\n    """""" (For developers) Errors occurs when there are some problems with the defined layers\n\n    """"""\n    pass\n\n\nclass ConfigurationError(BaseError):\n    """""" Errors occur when model configuration\n\n    """"""\n    pass\n\n\nclass InputError(BaseError):\n    """""" Error occur when the input to model is wrong\n\n    """"""\n    pass\n\n\nclass PreprocessError(BaseError):\n    """""" Error occur when the input to model is wrong\n\n    """"""\n    pass\n\n'"
utils/philly_utils.py,2,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport os\nimport tempfile\nimport subprocess\nimport shutil\nimport codecs\nimport torch\nimport pickle as pkl\nimport logging\nfrom contextlib import contextmanager\nimport random\nimport string\n\ndef convert_to_tmppath(filepath):\n    tmpfolder = tempfile.gettempdir()\n    if filepath.startswith(\'/hdfs/\'):\n        filepath = filepath.replace(\'/hdfs/\', \'\', 1)\n    tmppath = os.path.join(tmpfolder, \'neuronblocks\', filepath + \'_\' + \'\'.join(random.sample(string.ascii_letters+string.digits, 16)))\n    tmpdir = os.path.dirname(tmppath)\n    if not os.path.exists(tmpdir):\n        os.makedirs(tmpdir)\n    logging.info(\'Obtain a local tmp path %s for %s\' % (tmppath, filepath))\n    return tmppath\n\n\ndef convert_to_hdfspath(filepath):\n    if not filepath or not filepath.startswith(""/hdfs/""):\n        return filepath\n    hdfs_direct_path = filepath.replace(\'/hdfs/\', \'hdfs:///\', 1)\n    logging.info(\'Convert %s to hdfs direct path %s\' % (filepath, hdfs_direct_path))\n    return hdfs_direct_path\n\n\ndef move_from_local_to_hdfs(tmpfilepath, remotefilepath):\n    \'\'\'\n    ret = subprocess.check_call(""/var/storage/shared/public/philly/sethadoop.sh && \\\n                    hdfs dfs -mkdir -p {2} && \\\n                    hdfs dfs -moveFromLocal -f {0} {1}"".format(tmpfilepath, remotefilepath, os.path.dirname(remotefilepath)), shell=True)\n    \'\'\'\n    ret = subprocess.check_call(""hdfs dfs -mkdir -p {2} && \\\n                    hdfs dfs -moveFromLocal -f {0} {1}"".format(tmpfilepath, remotefilepath, os.path.dirname(remotefilepath)), shell=True)\n    logging.info(\'With hdfs command, %s moved to hdfs path %s\' % (tmpfilepath, remotefilepath))\n\n\nclass HDFSDirectTransferer(object):\n    """""" Help to access HDFS directly, for pickle file and torch.save\n    For a hdfs path, save it to a local file firstly and then move it to the hdfs direct path\n    For a local path, do save it at local path\n\n    """"""\n    def __init__(self, path, with_hdfs_command=True):\n        """"""\n\n        Args:\n            path:\n            with_hdfs_command: if True, transfer data with hdfs command; if False, with shutil.move\n        """"""\n        self.origin_path = path\n        if path.startswith(\'/hdfs/\'):\n            self.activate = True\n        else:\n            self.activate = False\n\n        self.with_hdfs_command = with_hdfs_command\n\n    def __enter__(self):\n        if self.activate:\n            self.tmp_path = convert_to_tmppath(self.origin_path)\n        else:\n            self.tmp_path = self.origin_path\n        logging.info(\'Convert %s to temp path %s\' % (self.origin_path, self.tmp_path))\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if self.activate:\n            if self.with_hdfs_command:\n                self.hdfs_direct_path = convert_to_hdfspath(self.origin_path)\n                move_from_local_to_hdfs(self.tmp_path, self.hdfs_direct_path)\n            else:\n                shutil.move(self.tmp_path, self.origin_path)\n                logging.info(\'With shutil.move command, %s moved to hdfs path %s\' % (self.tmp_path, self.origin_path))\n\n    def pkl_dump(self, obj):\n        with open(self.tmp_path, \'wb\') as fout:\n            pkl.dump(obj, fout, protocol=pkl.HIGHEST_PROTOCOL)\n\n    def torch_save(self, model):\n        torch.save(model, self.tmp_path, pickle_protocol=pkl.HIGHEST_PROTOCOL)\n\n\n@contextmanager\ndef open_and_move(path):\n    """""" write to a file directly or indirectly\n\n    Args:\n        path: the path to write to finally\n        with_temp: if True, write to a local file firstly and move to another place,\n                    if False, write to a path directly\n    """"""\n\n    if path.startswith(\'/hdfs/\'):\n        activate = True\n    else:\n        activate = False\n\n    if activate:\n        middle_path = convert_to_tmppath(path)\n    else:\n        middle_path = path\n\n    fp = codecs.open(middle_path, \'w\', encoding=\'utf-8\')\n    yield fp\n    fp.close()\n\n    if activate:\n        hdfs_direct_path = convert_to_hdfspath(path)\n        move_from_local_to_hdfs(middle_path, hdfs_direct_path)\n        #shutil.move(middle_path, path)\n\n\nif __name__ == \'__main__\':\n    logging.basicConfig(level=logging.DEBUG, format=\'%(asctime)s %(levelname)s %(filename)s %(funcName)s %(lineno)d: %(message)s\')\n\n    \'\'\'\n    a = {1: 2, 3: 4}\n    save_path = \'/hdfs/a.pkl\'\n    with HDFSDirectTransferer(save_path, with_hdfs_command=True) as transferer:\n        transferer.pkl_dump(a)\n    \'\'\'\n\n    with open_and_move(\'/hdfs/test/a.txt\') as fout:\n        fout.write(\'hello world\\n\')\n        fout.write(\'hello world2\\n\')\n'"
autotest/tools/calculate_AUC.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport argparse\nfrom sklearn.metrics import roc_auc_score\n\ndef read_tsv(params):\n    prediction, label = [], []\n    #predict_index, label_index = int(params.predict_index), int(params.label_index)\n    predict_index, label_index = int(params[\'predict_index\']), int(params[\'label_index\'])\n    min_column_num = max(predict_index, label_index) + 1\n    with open(params[\'input_file\'], mode=\'r\', encoding=\'utf-8\') as f:\n        for index, line in enumerate(f):\n            if params[\'header\'] and index == 0:\n                continue\n            line = line.rstrip()\n            # skip empty line\n            if not line:\n                continue\n            line = line.split(\'\\t\')\n            if len(line) < min_column_num:\n                print(""at line:%s, %s""%(predict_index, line))\n                raise Exception(""the given index of predict or label is exceed the index of the column"")\n            prediction.append(float(line[predict_index]))\n            label.append(int(line[label_index]))\n    return prediction, label\n            \ndef calculate_AUC(prediction, label):\n    return roc_auc_score(label, prediction)\n\ndef main(params):\n    prediction, label = read_tsv(params)\n    auc = calculate_AUC(prediction, label)\n    #print(""AUC is "", auc)\n    return auc\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=""AUC"")\n    parser.add_argument(""--input_file"", type=str, help=""tsv file"")\n    parser.add_argument(""--predict_index"", type=str, help=""the column index of prediction of model, start from 0"")\n    parser.add_argument(""--label_index"", type=str, help=""the column index of label, start from 0"")\n    parser.add_argument(""--header"", action=\'store_true\', default=False, help=""whether contains header row or not, default is False"")\n\n    params, _ = parser.parse_known_args()\n\n    assert params.input_file, \'Please specify a input file via --input_file\'\n    assert params.predict_index, \'Please specify the column index of prediction via --predict_index\'\n    assert params.label_index, \'Please specify the column index of label via --label_index\'\n    main(params)\n'"
autotest/tools/get_results.py,0,"b""import re\nfrom calculate_AUC import main\n\nbase_dir = './autotest/models'\ntask_dir = ['/20_newsgroup_bilstm_attn', '/chinese_text_matching', '/question_pairs_bilstm_attn']\n\nresults = {'english_text_matching': [0.96655], 'chinese_text_matching': [0.70001], 'quora_question_pairs': [0.72596], 'knowledge_distillation': [0.66329]}\nfor each_dir, key in zip(task_dir, results.keys()):\n    target_dir = base_dir + each_dir\n    try:\n        with open(target_dir + '/train_autotest.log', 'r') as f_r:\n            last_line = f_r.readlines()[-1].strip()\n            score = ''.join(re.findall(r'(?<=accuracy:).*?(?=loss|;)', last_line))\n            try:\n                results[key].append(float(score))\n            except:\n                results[key].append('wrong number in train log')\n                print ('GPU test. Wrong number in %s/train_autotest.log' %target_dir)\n    except:\n        results[key].append('no train log')\n\n    try:\n        with open(target_dir + '/test_autotest.log', 'r') as f_r:\n            last_line = f_r.readlines()[-1].strip()\n            score = ''.join(re.findall(r'(?<=accuracy:).*?(?=loss|;)', last_line))\n            try:\n                results[key].append(float(score))\n            except:\n                results[key].append('wrong number in test log')\n                print ('CPU test. Wrong number in %s/test_autotest.log' %target_dir)\n    except:\n        results[key].append('no test log')\n\n# for kdtm_match_linearAttn task, we use calculate_AUC.main()\nparams = {'input_file': './autotest/models/kdtm_match_linearAttn/predict.tsv', 'predict_index': '3', 'label_index': '2', 'header': False}\ntry:\n    AUC = float(main(params))\n    results['knowledge_distillation'].append(AUC)\nexcept:\n    results['knowledge_distillation'].append('wrong')\n\nwith open('./autotest/contrast_results.txt', 'w') as f_w:\n    f_w.write('tasks' + '\\t'*5 + 'GPU/CPU' + '\\t' + 'old accuracy/AUC' + '\\t' + 'new accuracy/AUC ' + '\\n')\n    for key, value in results.items():\n        if key == 'knowledge_distillation':\n            #f_w.write(key + '\\t'*2 + 'GPU' + str(value[0]) + '\\t'*3 + str(value[1]) + '\\n')\n            f_w.write(key + '\\t'*2 + 'CPU' + '\\t'*2 + str(value[0]) + '\\t'*3 + str(value[1]) + '\\n')\n        else:\n            f_w.write(key + '\\t'*2 + 'GPU' + '\\t'*2 + str(value[0]) + '\\t'*3 + str(value[1]) + '\\n')\n            f_w.write(key + '\\t'*2 + 'CPU' + '\\t'*2 + str(value[0]) + '\\t'*3 + str(value[2]) + '\\n')\n"""
block_zoo/attentions/Attention.py,2,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport numpy as np\nimport torch.nn as nn\nimport copy\n\nfrom block_zoo.BaseLayer import BaseLayer, BaseConf\nfrom utils.DocInherit import DocInherit\n\n\nclass AttentionConf(BaseConf):\n    """"""Configuration for Attention layer\n\n    """"""\n    def __init__(self, **kwargs):\n        super(AttentionConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        pass\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 2\n        self.input_ranks = [3, 3]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = copy.deepcopy(self.input_dims[0])\n        super(AttentionConf, self).inference()  # PUT THIS LINE AT THE END OF inference()\n\n    @DocInherit\n    def verify(self):\n        super(AttentionConf, self).verify()\n\n\nclass Attention(BaseLayer):\n    """"""  Attention layer\n\n    Given sequences X and Y, match sequence Y to each element in X.\n\n    Args:\n        layer_conf (AttentionConf): configuration of a layer\n\n    """"""\n    def __init__(self, layer_conf):\n\n        super(Attention, self).__init__(layer_conf)\n        assert layer_conf.input_dims[0][-1] == layer_conf.input_dims[1][-1]\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x, x_len, y, y_len):\n        """"""\n\n        Args:\n            x (Tensor):      [batch_size, x_max_len, dim].\n            x_len (Tensor):  [batch_size], default is None.\n            y (Tensor):      [batch_size, y_max_len, dim].\n            y_len(Tensor):  [batch_size], default is None.\n\n        Returns:\n            output: has the same shape as x.\n\n        """"""\n\n        scores = x.bmm(y.transpose(2, 1))     # [batch_size, x_max_len, y_max_len]\n\n        batch_size, y_max_len, _ = y.size()\n        y_length = y_len.cpu().numpy()\n        y_mask = np.ones((batch_size, y_max_len))\n        for i, single_len in enumerate(y_length):\n            y_mask[i][:single_len] = 0\n        y_mask = torch.from_numpy(y_mask).byte().to(scores.device)\n        y_mask = y_mask.unsqueeze(1).expand(scores.size())\n        scores.data.masked_fill_(y_mask.data, float(\'-inf\'))\n\n        alpha = self.softmax(scores)    # [batch_size, x_max_len, y_len]\n        output = alpha.bmm(y)   # [batch_size, x_max_len, dim]\n\n        return output, x_len\n\n\n'"
block_zoo/attentions/BiAttFlow.py,7,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom block_zoo.BaseLayer import BaseConf, BaseLayer\nfrom utils.DocInherit import DocInherit\nimport copy\n\n\nclass BiAttFlowConf(BaseConf):\n    """"""Configuration for AttentionFlow layer\n\n    Args:\n        attention_dropout(float): dropout rate of attention matrix dropout operation\n    """"""\n    def __init__(self, **kwargs):\n        super(BiAttFlowConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        self.attention_dropout = 0\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 2\n        self.input_ranks = [3]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = copy.deepcopy(self.input_dims[0])\n        self.output_dim[-1] = 4 * self.input_dims[0][-1]\n        super(BiAttFlowConf, self).inference()\n\n    @DocInherit\n    def verify(self):\n        super(BiAttFlowConf, self).verify()\n        necessary_attrs_for_user = [\'attention_dropout\']\n        for attr in necessary_attrs_for_user:\n            self.add_attr_exist_assertion_for_user(attr)\n\n\nclass BiAttFlow(BaseLayer):\n    """"""\n    implement AttentionFlow layer for BiDAF\n    [paper]: https://arxiv.org/pdf/1611.01603.pdf\n\n    Args:\n        layer_conf(AttentionFlowConf): configuration of the AttentionFlowConf\n    """"""\n    def __init__(self, layer_conf):\n        super(BiAttFlow, self).__init__(layer_conf)\n        self.layer_conf = layer_conf\n        # self.W = nn.Linear(layer_conf.input_dims[0][-1]*3, 1)\n        self.attention_weight_content = nn.Linear(layer_conf.input_dims[0][-1], 1)\n        self.attention_weight_query = nn.Linear(layer_conf.input_dims[0][-1], 1)\n        self.attention_weight_cq = nn.Linear(layer_conf.input_dims[0][-1], 1)\n        self.attention_dropout = nn.Dropout(layer_conf.attention_dropout)\n\n    def forward(self, content, content_len, query, query_len=None):\n        """"""\n        implement the attention flow layer of BiDAF model\n\n        :param content (Tensor): [batch_size, content_seq_len, dim]\n        :param content_len: [batch_size]\n        :param query (Tensor): [batch_size, query_seq_len, dim]\n        :param query_len: [batch_size]\n        :return: the tensor has same shape as content\n        """"""\n        assert content.size()[2] == query.size()[2], \'The dimension of axis 2 of content and query must be consistent! But now, content.size() is %s and query.size() is %s\' % (content.size(), query.size())\n        batch_size = content.size()[0]\n        content_seq_len = content.size()[1]\n        query_seq_len = query.size()[1]\n        feature_dim = content.size()[2]\n\n        # content_aug = content.unsqueeze(1).expand(batch_size, query_seq_len, content_seq_len, feature_dim)   #[batch_size, string2_seq_len, string_seq_len, feature_dim]\n        content_aug = content.unsqueeze(2).expand(batch_size, content_seq_len, query_seq_len, feature_dim)  # [batch_size, string2_seq_len, string_seq_len, feature_dim]\n        query_aug = query.unsqueeze(1).expand(batch_size, content_seq_len, query_seq_len, feature_dim) #[batch_size, string_seq_len, string2_seq_len, feature_dim]\n        content_aug = content_aug.contiguous().view(batch_size*content_seq_len*query_seq_len, feature_dim)\n        query_aug = query_aug.contiguous().view(batch_size*content_seq_len*query_seq_len, feature_dim)\n\n        content_query_comb = torch.cat((query_aug, content_aug, content_aug * query_aug), 1)\n        attention = self.W(content_query_comb)\n        attention = self.attention_dropout(attention)\n        # [batch_size, string_seq_len, string2_seq_len]\n        attention = attention.view(batch_size, content_seq_len, query_seq_len)\n        attention_logits = F.softmax(attention, dim=2)\n        # [batch_size*content_seq_len*query_seq_len] * [batch_size*query_seq_len*feature_dim] --> [batch_size*content_seq_len*feature_dim]\n        content2query_att = torch.bmm(attention_logits, query)\n        # [batch_size, 1, content_seq_len]\n        b = F.softmax(torch.max(attention, dim=2)[0], dim=1).unsqueeze(1)\n        # [batch_size, 1, content_seq_len]*[batch_size, content_seq_len, feature_dim] = [batch_size, feature_dim]\n        query2content_att = torch.bmm(b, content).squeeze(1)\n        # [batch_size, content_seq_len, feature_dim]\n        query2content_att = query2content_att.unsqueeze(1).expand(-1, content_seq_len, -1)\n\n        result = torch.cat([content, content2query_att, content*content2query_att, content*query2content_att], dim=-1)\n\n        return result, content_len\n\n\n\n'"
block_zoo/attentions/BilinearAttention.py,1,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch.nn as nn\nimport copy\n\nfrom block_zoo.BaseLayer import BaseLayer, BaseConf\nfrom utils.DocInherit import DocInherit\n\nclass BilinearAttentionConf(BaseConf):\n    """"""Configuration for Bilinear attention layer\n\n    """"""\n    def __init__(self, **kwargs):\n        super(BilinearAttentionConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        pass\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 2\n        self.input_ranks = [3, 2]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = copy.deepcopy(self.input_dims[0])\n        self.output_dim[-1] = 1\n        super(BilinearAttentionConf, self).inference()  # PUT THIS LINE AT THE END OF inference()\n\n    @DocInherit\n    def verify(self):\n        super(BilinearAttentionConf, self).verify()\n\n\nclass BilinearAttention(BaseLayer):\n    """"""  BilinearAttention layer for DrQA\n    [paper]  https://arxiv.org/abs/1704.00051\n    Args:\n        layer_conf (BilinearAttentionConf): configuration of a layer\n\n    """"""\n    def __init__(self, layer_conf):\n        super(BilinearAttention, self).__init__(layer_conf)\n        self.linear = nn.Linear(layer_conf.input_dims[1][-1], layer_conf.input_dims[0][-1])\n\n    def forward(self, x, x_len, y, y_len):\n        """""" process inputs\n\n        Args:\n            x (Tensor):      [batch_size, x_len, x_dim].\n            x_len (Tensor):  [batch_size], default is None.\n            y (Tensor):      [batch_size, y_dim].\n            y_len (Tensor):  [batch_size], default is None.\n        Returns:\n            output: [batch_size, x_len, 1].\n            x_len:\n\n        """"""\n\n        Wy = self.linear(y)     # [batch_size, x_dim]\n        xWy = x.bmm(Wy.unsqueeze(2))    # [batch_size, x_len, 1]\n\n        return xWy, x_len\n\n\n'"
block_zoo/attentions/FullAttention.py,7,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.parameter import Parameter\nimport copy\nimport numpy as np\nfrom utils.DocInherit import DocInherit\n\nfrom block_zoo.BaseLayer import BaseLayer, BaseConf\nfrom utils.exceptions import ConfigurationError\n\nclass FullAttentionConf(BaseConf):\n    def __init__(self, **kwargs):\n        super(FullAttentionConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        self.hidden_dim = 128\n        self.activation = \'ReLU\'\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 4\n        self.input_ranks = [3]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = copy.deepcopy(self.input_dims[1])     # e.g. use query to represent passage, there fore the output dim depends on query\'s dim\n        super(FullAttentionConf, self).inference()  # PUT THIS LINE AT THE END OF inference()\n\n    @DocInherit\n    def verify_before_inference(self):\n        super(FullAttentionConf, self).verify_before_inference()\n        necessary_attrs_for_user = [\'hidden_dim\']\n        for attr in necessary_attrs_for_user:\n            self.add_attr_exist_assertion_for_user(attr)\n\n    def verify(self):\n        super(FullAttentionConf, self).verify()\n\n        supported_activation_pytorch = [None, \'Sigmoid\', \'Tanh\', \'ReLU\', \'PReLU\', \'ReLU6\', \'LeakyReLU\', \'LogSigmoid\',\n                                        \'ELU\',\n                                        \'SELU\', \'Threshold\', \'Hardtanh\', \'Softplus\', \'Softshrink\', \'Softsign\',\n                                        \'Tanhshrink\', \'Softmin\',\n                                        \'Softmax\', \'Softmax2d\', \'LogSoftmax\']\n        value_checks = [(\'activation\', supported_activation_pytorch)]\n        for attr, legal_values in value_checks:\n            self.add_attr_value_assertion(attr, legal_values)\n\n\nclass FullAttention(BaseLayer):\n    """""" Full-aware fusion of:\n            Via, U., With, T., & To, P. (2018). Fusion Net: Fusing Via Fully-Aware Attention with Application to Machine Comprehension, 1\xe2\x80\x9317.\n\n    """"""\n    def __init__(self, layer_conf):\n        super(FullAttention, self).__init__(layer_conf)\n        self.layer_conf.hidden_dim = layer_conf.hidden_dim\n        self.linear = nn.Linear(layer_conf.input_dims[2][-1], layer_conf.hidden_dim, bias=False)        # this requires that input_dims[0][-1] == input_dims[1][-1]\n        if layer_conf.input_dims[2][-1] == layer_conf.input_dims[3][-1]:\n            self.linear2 = self.linear\n        else:\n            self.linear2 = nn.Linear(layer_conf.input_dims[3][-1], layer_conf.hidden_dim, bias=False)\n        self.linear_final = Parameter(torch.ones(1, layer_conf.hidden_dim), requires_grad=True)\n\n        self.activation = eval(""nn."" + layer_conf.activation)()\n\n    def forward(self, string1, string1_len, string2, string2_len, string1_HoW, string1_How_len, string2_HoW, string2_HoW_len):\n        """""" To get representation of string1, we use string1 and string2 to obtain attention weights and use string2 to represent string1\n\n        Note: actually, the semantic information of string1 is not used, we only need string1\'s seq_len information\n\n        Args:\n            string1: [batch size, seq_len, input_dim1]\n            string1_len: [batch_size]\n            string2: [batch size, seq_len, input_dim2]\n            string2_len: [batch_size]\n            string1_HoW: [batch size, seq_len, att_dim1]\n            string1_HoW_len: [batch_size]\n            string2_HoW: [batch size, seq_len, att_dim2]\n            string2_HoW_len: [batch_size]\n\n        Returns:\n            string1\'s representation\n            string1_len\n\n        """"""\n        string1_key = self.activation(self.linear(string1_HoW.contiguous().view(-1, string1_HoW.size()[2])))     #[bs * seq_len, atten_dim1] -> [bs * seq_len, hidden_dim]\n        string2_key = self.activation(self.linear2(string2_HoW.contiguous().view(-1, string2_HoW.size()[2])))    #[bs * seq_len, atten_dim2] -> [bs * seq_len, hidden_dim]\n        final_v = self.linear_final.expand_as(string2_key)\n        string2_key = final_v * string2_key\n\n        string1_rep = string1_key.view(-1, string1.size(1), 1, self.layer_conf.hidden_dim).transpose(1, 2).contiguous().view(-1, string1.size(1), self.layer_conf.hidden_dim)        # get [bs, seq_len, hidden_dim]\n        string2_rep = string2_key.view(-1, string2.size(1), 1, self.layer_conf.hidden_dim).transpose(1, 2).contiguous().view(-1, string2.size(1), self.layer_conf.hidden_dim)       # get [bs, seq_len, hidden_dim]\n\n        scores = string1_rep.bmm(string2_rep.transpose(1, 2)).view(-1, 1, string1.size(1), string2.size(1)) # [bs, 1, seq_len1, seq_len2]\n\n        string2_len_np = string2_len.cpu().numpy()\n        if torch.cuda.device_count() > 1:\n            # otherwise, it will raise a Exception because the length inconsistence\n            string2_max_len = string2.shape[1]\n        else:\n            string2_max_len = string2_len_np.max()\n        string2_mask = np.array([[0] * num + [1] * (string2_max_len - num) for num in string2_len_np])\n        string2_mask = torch.from_numpy(string2_mask).unsqueeze(1).unsqueeze(2).expand_as(scores)\n        if self.is_cuda():\n            device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n            string2_mask = string2_mask.to(device)\n        scores.data.masked_fill_(string2_mask.data.byte(), -float(\'inf\'))\n\n        alpha_flat = F.softmax(scores.view(-1, string2.size(1)), dim=1)        # [bs * seq_len1, seq_len2]\n        alpha = alpha_flat.view(-1, string1.size(1), string2.size(1))   # [bs, seq_len1, seq_len2]\n\n        #size_per_level = self.layer_conf.hidden_dim // 1\n        #string1_atten_seq = alpha.bmm(string2.contiguous().view(-1, string2.size(1), 1, size_per_level).transpose(1, 2).contiguous().view(-1, string2.size(1), size_per_level))\n        string1_atten_seq = alpha.bmm(string2)\n\n        #return string1_atten_seq.view(-1, 1, string1.size(1), size_per_level).transpose(1, 2).contiguous().view(-1, string1.size(1), self.layer_conf.hidden_dim), string1_len\n        return string1_atten_seq, string1_len'"
block_zoo/attentions/Interaction.py,7,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport copy\n\nfrom block_zoo.BaseLayer import BaseLayer, BaseConf\nfrom utils.DocInherit import DocInherit\nfrom utils.common_utils import transfer_to_gpu\n\n\nclass InteractionConf(BaseConf):\n    """"""Configuration of Interaction Layer\n\n    Args:\n        hidden_dim (int): dimension of hidden state\n        matching_type (string): shoule be \'general\', \'mul\', \'plus\', \'minus\', \'dot\', \'concat\'\n\n    """"""\n    def __init__(self, **kwargs):\n        super(InteractionConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        self.matching_type = \'general\'\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 2\n        self.input_ranks = [3, 3]\n\n    @DocInherit\n    def inference(self):\n        shape1 = self.input_dims[0]\n        shape2 = self.input_dims[1]\n        if shape1[1] == -1 or shape2[1] == -1:\n            raise ConfigurationError(""For Interaction layer, the sequence length should be fixed"")\n        # print(shape1,shape2)\n        self.output_dim = None\n        if self.matching_type in [\'mul\', \'plus\', \'minus\']:        \n            self.output_dim = [shape1[0], shape1[1], shape2[1], shape1[2]] \n        elif self.matching_type in [\'dot\', \'general\']:\n            self.output_dim = [shape1[0], shape1[1], shape2[1], 1]\n        elif self.matching_type == \'concat\':\n            self.output_dim = [shape1[0], shape1[1], shape2[1], shape1[2] + shape2[2]]\n        else:\n            raise ValueError(f""Invalid `matching_type`.""\n                             f""{self.matching_type} received.""\n                             f""Must be in `mul`, `general`, `plus`, `minus` ""\n                             f""`dot` and `concat`."")\n        # print(self.output_dim)\n        super(InteractionConf, self).inference()  # PUT THIS LINE AT THE END OF inference()\n\n    @DocInherit\n    def verify(self):\n        super(InteractionConf, self).verify()\n        assert hasattr(self, \'matching_type\'), ""Please define matching_type attribute of BiGRUConf in default() or the configuration file""\n        assert self.matching_type in [\'general\', \'dot\', \'mul\', \'plus\', \'minus\', \'add\', \'concat\'], ""Invalid `matching_type`{self.matching_type} received. Must be in `mul`, `general`, `plus`, `minus`, `dot` and `concat`.""\n\n\nclass Interaction(BaseLayer):\n    """"""Bidirectional GRU\n\n    Args:\n        layer_conf (BiGRUConf): configuration of a layer\n    """"""\n    def __init__(self, layer_conf):\n        super(Interaction, self).__init__(layer_conf)\n        self.matching_type = layer_conf.matching_type\n        shape1 = layer_conf.input_dims[0]\n        shape2 = layer_conf.input_dims[1]\n        if self.matching_type == \'general\':\n            self.linear_in = nn.Linear(shape1[-1], shape2[-1], bias=False)\n\n    def forward(self, string1, string1_len, string2, string2_len):\n        """""" process inputs\n\n        Args:\n            string1 (Tensor): [batch_size, seq_len1, dim]\n            string1_len (Tensor): [batch_size]\n            string2 (Tensor): [batch_size, seq_len2, dim]\n            string2_len (Tensor): [batch_size]\n\n        Returns:\n            Tensor: [batch_size, seq_len1, seq_len2]\n\n        """"""\n        padded_seq_len1 = string1.shape[1]\n        padded_seq_len2 = string2.shape[1]\n        seq_dim1 = string1.shape[-1]\n        seq_dim2 = string2.shape[-1]\n        x1 = string1\n        x2 = string2\n        result = None\n\n\n        if self.matching_type == \'dot\' or self.matching_type == \'general\':\n            # if self._normalize:\n            #     x1 = K.l2_normalize(x1, axis=2)\n            #     x2 = K.l2_normalize(x2, axis=2)\n            if self.matching_type==\'general\':\n                x1 = x1.view(-1, seq_dim1)\n                x1 = self.linear_in(x1)\n                x1 = x1.view(-1, padded_seq_len1, seq_dim2)\n            result = torch.bmm(x1, x2.transpose(1, 2).contiguous())\n            result = torch.unsqueeze(result, -1)\n            # print(""result"", result.size())\n        else:\n            if self.matching_type == \'mul\':\n                def func(x, y):\n                    return x * y\n            elif self.matching_type == \'plus\':\n                def func(x, y):\n                    return x + y\n            elif self.matching_type == \'minus\':\n                def func(x, y):\n                    return x - y\n            elif self.matching_type == \'concat\':\n                def func(x, y):\n                    return torch.cat([x, y], dim=-1)\n            else:\n                raise ValueError(f""Invalid matching type.""\n                                 f""{self.matching_type} received.""\n                                 f""Mut be in `dot`, `general`, `mul`, `plus`, ""\n                                 f""`minus` and `concat`."")\n            x1_exp = torch.stack([x1] * padded_seq_len2, dim=2)\n            x2_exp = torch.stack([x2] * padded_seq_len1, dim=1)\n            result = func(x1_exp, x2_exp)\n\n        return result, padded_seq_len1\n\n'"
block_zoo/attentions/LinearAttention.py,6,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport copy\nimport numpy as np\n\nfrom block_zoo.BaseLayer import BaseLayer, BaseConf\nfrom utils.DocInherit import DocInherit\n\nclass LinearAttentionConf(BaseConf):\n    """"""Configuration for Linear attention layer\n\n    Args:\n        keep_dim (bool): Whether to sum up the sequence representation along the sequence axis.\n                if False, the layer would return (batch_size, dim)\n                if True, the layer would keep the same dimension as input, thus return (batch_size, sequence_length, dim)\n    """"""\n    def __init__(self, **kwargs):\n        super(LinearAttentionConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        self.keep_dim = False\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 1\n        self.input_ranks = [-1]\n\n    @DocInherit\n    def inference(self):\n        self.attention_weight_dim = self.input_dims[0][-1]\n\n        if self.keep_dim:\n            self.output_dim = copy.deepcopy(self.input_dims[0])\n        else:\n            self.output_dim = []\n            for idx, dim in enumerate(self.input_dims[0]):\n                if idx != len(self.input_dims[0]) - 2:\n                    self.output_dim.append(dim)\n\n        super(LinearAttentionConf, self).inference()  # PUT THIS LINE AT THE END OF inference()\n\n    @DocInherit\n    def verify_before_inference(self):\n        super(LinearAttentionConf, self).verify_before_inference()\n        necessary_attrs_for_user = [\'keep_dim\']\n        for attr in necessary_attrs_for_user:\n            self.add_attr_exist_assertion_for_user(attr)\n\n    @DocInherit\n    def verify(self):\n        super(LinearAttentionConf, self).verify()\n        necessary_attrs_for_user = [\'attention_weight_dim\', \'keep_dim\']\n        for attr in necessary_attrs_for_user:\n            self.add_attr_exist_assertion_for_user(attr)\n\n        type_checks = [(\'attention_weight_dim\', int),\n                       (\'keep_dim\', bool)]\n        for attr, attr_type in type_checks:\n            self.add_attr_type_assertion(attr, attr_type)\n\n        assert self.input_ranks[0] in set([2, 3]) and not (self.input_ranks[0] == 2 and self.keep_dim == False)\n\n\nclass LinearAttention(BaseLayer):\n    """"""  Linear attention.\n    Combinate the original sequence along the sequence_length dimension.\n\n    Args:\n        layer_conf (LinearAttentionConf): configuration of a layer\n\n    """"""\n    def __init__(self, layer_conf):\n        """"""\n\n        Args:\n            layer_conf (LinearAttentionConf): configuration of a layer\n        """"""\n        super(LinearAttention, self).__init__(layer_conf)\n        self.attention_weight = nn.Parameter(torch.FloatTensor(torch.randn(self.layer_conf.attention_weight_dim, 1)))\n\n    def forward(self, string, string_len=None):\n        """""" process inputs\n\n        Args:\n            string (Variable): (batch_size, sequence_length, dim)\n            string_len (ndarray or None): [batch_size]\n\n        Returns:\n            Variable:\n                if keep_dim == False:\n                    Output dimention: (batch_size, dim)\n                else:\n                    just reweight along the sequence_length dimension: (batch_size, sequence_length, dim)\n\n        """"""\n        attention_weight = torch.mm(string.contiguous().view(string.shape[0] * string.shape[1], string.shape[2]), self.attention_weight)\n        attention_weight = nn.functional.softmax(attention_weight.view(string.shape[0], string.shape[1]), dim=1)\n\n        attention_tiled = attention_weight.unsqueeze(2).expand_as(string)\n        string_reweighted = torch.mul(string, attention_tiled)\n        if self.layer_conf.keep_dim is False:\n            string_reweighted = torch.sum(string_reweighted, 1)\n\n        return string_reweighted, string_len\n\n\n'"
block_zoo/attentions/MatchAttention.py,2,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport numpy as np\nimport torch.nn as nn\nimport copy\n\nfrom block_zoo.BaseLayer import BaseLayer, BaseConf\nfrom utils.DocInherit import DocInherit\n\n\nclass MatchAttentionConf(BaseConf):\n    """"""Configuration for MatchAttention layer\n\n    """"""\n    def __init__(self, **kwargs):\n        super(MatchAttentionConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        self.activation = \'ReLU\'\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 2\n        self.input_ranks = [3, 3]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = copy.deepcopy(self.input_dims[0])\n        super(MatchAttentionConf, self).inference()  # PUT THIS LINE AT THE END OF inference()\n\n    @DocInherit\n    def verify(self):\n        super(MatchAttentionConf, self).verify()\n\n\nclass MatchAttention(BaseLayer):\n    """"""  MatchAttention layer for DrQA\n    [paper]  https://arxiv.org/abs/1704.00051\n\n    Given sequences X and Y, match sequence Y to each element in X.\n\n    Args:\n        layer_conf (MatchAttentionConf): configuration of a layer\n\n    """"""\n    def __init__(self, layer_conf):\n\n        super(MatchAttention, self).__init__(layer_conf)\n        assert layer_conf.input_dims[0][-1] == layer_conf.input_dims[1][-1]\n        self.linear = nn.Linear(layer_conf.input_dims[0][-1], layer_conf.input_dims[0][-1])\n        if layer_conf.activation:\n            self.activation = eval(""nn."" + self.layer_conf.activation)()\n        else:\n            self.activation = None\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x, x_len, y, y_len):\n        """"""\n\n        Args:\n            x:      [batch_size, x_max_len, dim].\n            x_len:  [batch_size], default is None.\n            y:      [batch_size, y_max_len, dim].\n            y_len:  [batch_size], default is None.\n\n        Returns:\n            output: has the same shape as x.\n\n        """"""\n\n        x_proj = self.linear(x)  # [batch_size, x_max_len, dim]\n        y_proj = self.linear(y)  # [batch_size, y_max_len, dim]\n        if self.activation:\n            x_proj = self.activation(x_proj)\n            y_proj = self.activation(y_proj)\n        scores = x_proj.bmm(y_proj.transpose(2, 1))     # [batch_size, x_max_len, y_max_len]\n\n        # batch_size, y_max_len, _ = y.size()\n        # y_length = y_len.cpu().numpy()\n        # y_mask = np.ones((batch_size, y_max_len))\n        # for i, single_len in enumerate(y_length):\n        #     y_mask[i][:single_len] = 0\n        # y_mask = torch.from_numpy(y_mask).byte().to(scores.device)\n        # y_mask = y_mask.unsqueeze(1).expand(scores.size())\n        # scores.data.masked_fill_(y_mask.data, float(\'-inf\'))\n\n        alpha = self.softmax(scores)    # [batch_size, x_max_len, y_len]\n        output = alpha.bmm(y)   # [batch_size, x_max_len, dim]\n\n        return output, x_len\n\n\n'"
block_zoo/attentions/Seq2SeqAttention.py,7,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import OrderedDict\nimport copy\nfrom block_zoo.BaseLayer import BaseLayer, BaseConf\nfrom utils.DocInherit import DocInherit\n\n\nclass Seq2SeqAttentionConf(BaseConf):\n    """"""Configuration for Seq2SeqAttention layer\n\n    """"""\n    def __int__(self, **kwargs):\n        super(Seq2SeqAttentionConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        #self.input_dim = 128\n        self.attention_dropout = 0\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 2\n        self.input_ranks = [3]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = copy.deepcopy(self.input_dims[0])\n        self.output_dim[-1] = 2 * self.input_dims[0][-1]    # all the inputs have the same input dim\n        super(Seq2SeqAttentionConf, self).inference()  # PUT THIS LINE AT THE END OF inference()\n\n    @DocInherit\n    def verify(self):\n        super(Seq2SeqAttentionConf, self).verify()\n\n        necessary_attrs_for_user = [\'attention_dropout\']\n        for attr in necessary_attrs_for_user:\n            self.add_attr_exist_assertion_for_user(attr)\n\n\nclass Seq2SeqAttention(BaseLayer):\n    """""" Linear layer\n\n    Args:\n        layer_conf (LinearConf): configuration of a layer\n    """"""\n    def __init__(self, layer_conf):\n\n        super(Seq2SeqAttention, self).__init__(layer_conf)\n        self.layer_conf = layer_conf\n        self.W = nn.Linear(layer_conf.input_dims[0][-1] * 3, 1)\n        self.attention_dropout = nn.Dropout(layer_conf.attention_dropout)\n\n    def forward(self, string, string_len, string2, string2_len=None):\n        """""" utilize both string2 and string itself to generate attention weights to represent string.\n            There are two steps:\n                1. get a string2 to string attention to represent string.\n                2. get a string to string attention to represent string it self.\n                3. merge the two representation above.\n\n        Args:\n            string (Variable): [batch_size, string_seq_len, dim].\n            string_len (ndarray or None): [batch_size], default is None.\n            string2 (Variable): [batch_size, string2_seq_len, dim].\n            string2_len (ndarray or None): [batch_size], default is None.\n\n        Returns:\n            Variable: has the same shape as string.\n        """"""\n        assert string.size()[2] == string2.size()[2], \'The dimension of axis 2 of string and string2 must be consistent! But now, string.size() is %s and string2.size() is %s\' % (string.size(), string2.size())\n\n        batch_size = string.size()[0]\n        string_seq_len = string.size()[1]\n        string2_seq_len = string2.size()[1]\n        feature_dim = string.size()[2]\n        string2_aug = string2.unsqueeze(1).expand(batch_size, string_seq_len, string2_seq_len, feature_dim)  # [batch_size, string2_len, dim] -> [batch_size, string_len, string2_len, dim]\n        string_aug = string.unsqueeze(1).expand(batch_size, string2_seq_len, string_seq_len, feature_dim)  # [batch_size, string_len, dim] -> [batch_size, string2_len, string_len, dim]\n\n        string2_aug = string2_aug.contiguous().view(batch_size * string_seq_len * string2.size()[1], feature_dim)\n        string_aug = string_aug.contiguous().view(batch_size * string2_seq_len * string_seq_len, feature_dim)\n\n        # string2_string_comb = torch.cat((string2_aug, string_aug, string2_aug * string2_aug), 1)  # [batch_size * string2_len * string_len, 3 * dim]\n        string2_string_comb = torch.cat((string2_aug, string_aug, string_aug * string2_aug), 1)  # [batch_size * string2_len * string_len, 3 * dim]\n\n        attention = self.W(string2_string_comb)     # [batch_size * string2_len * string_len, 1]\n        attention = self.attention_dropout(attention)\n        attention = attention.view(batch_size, string_seq_len, string2_seq_len)  # [batch_size, string_len, string2_len]\n\n        string_to_string_att_weight = torch.unsqueeze(nn.Softmax(dim=1)(torch.max(attention, 2)[0]), 2)  # [batch_size, string_len, 1]\n        string_to_string_attention = string_to_string_att_weight * string  # [batch_size, string1_seq_len, feature_dim]\n\n        string2_to_string_att_weight = nn.Softmax(dim=2)(attention)  # [batch_size, string1_seq_len, string2_seq_len]\n\n        string2_to_string_attention = torch.sum(string2.unsqueeze(dim=1) * string2_to_string_att_weight.unsqueeze(dim=3), dim=2)  # [batch_size, string1_seq_len, feature_dim]\n\n        string_out = torch.cat((string_to_string_attention, string2_to_string_attention), 2)  # [batch_size, string1_seq_len, 2 * feature_dim]\n\n        return string_out, string_len\n'"
block_zoo/attentions/__init__.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\nfrom .FullAttention import FullAttention, FullAttentionConf\nfrom .Seq2SeqAttention import Seq2SeqAttention, Seq2SeqAttentionConf\nfrom .LinearAttention import LinearAttention, LinearAttentionConf\nfrom .BiAttFlow import BiAttFlow, BiAttFlowConf\nfrom .MatchAttention import MatchAttention, MatchAttentionConf\nfrom .Attention import Attention, AttentionConf\nfrom .BilinearAttention import BilinearAttention, BilinearAttentionConf\nfrom .Interaction import Interaction, InteractionConf'"
block_zoo/embedding/CNNCharEmbedding.py,7,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport numpy as np\nfrom block_zoo.BaseLayer import BaseLayer, BaseConf\nfrom utils.DocInherit import DocInherit\n\n\nclass CNNCharEmbeddingConf(BaseConf):\n    """""" Configuration of CNNCharEmbedding\n\n    Args:\n        dim (int, optional): the dimension of character embedding after convolution. Default: 30\n        embedding_matrix_dim(int, optional): the dimension of character initialized embedding. Default: 30\n        stride(int, optional): Stride of the convolution. Default: 1\n        padding(int, optional): Zero-padding added to both sides of the input. Default: 0\n        window_size(int, optional): width of convolution kernel. Default: 3\n        activation(Str, optional): activation after convolution operation, can set null. Default: \'ReLU\'\n    """"""\n    def __init__(self, **kwargs):\n        super(CNNCharEmbeddingConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        self.dim = [30]       # cnn\'s output channel dim\n        self.embedding_matrix_dim = 30      #\n        self.stride = [1]\n        self.padding = 0\n        self.window_size = [3]\n        self.activation = \'ReLU\'\n\n    @DocInherit\n    def declare(self):\n        self.input_channel_num = 1\n        self.num_of_inputs = 1\n        self.input_ranks = [3]\n\n    def change_to_list(self, attribute):\n        for single in attribute:\n            if not isinstance(getattr(self, single), list):\n                setattr(self, single, [getattr(self, single)])\n\n    @DocInherit\n    def inference(self):\n        self.change_to_list([\'dim\', \'stride\', \'window_size\'])\n        self.output_channel_num = self.dim\n        self.output_rank = 3\n\n    @DocInherit\n    def verify(self):\n        # super(CNNCharEmbeddingConf, self).verify()\n\n        necessary_attrs_for_user = [\'dim\', \'embedding_matrix_dim\', \'stride\', \'window_size\', \'activation\', \'vocab_size\']\n        for attr in necessary_attrs_for_user:\n            self.add_attr_exist_assertion_for_user(attr)\n\n\nclass CNNCharEmbedding(BaseLayer):\n    """"""\n    This layer implements the character embedding use CNN\n    Args:\n        layer_conf (CNNCharEmbeddingConf): configuration of CNNCharEmbedding\n    """"""\n    def __init__(self, layer_conf):\n        super(CNNCharEmbedding, self).__init__(layer_conf)\n        self.layer_conf = layer_conf\n\n        assert len(layer_conf.dim) == len(layer_conf.window_size) == len(layer_conf.stride), ""The attribute dim/window_size/stride must have the same length.""\n\n        self.char_embeddings = nn.Embedding(layer_conf.vocab_size, layer_conf.embedding_matrix_dim, padding_idx=self.layer_conf.padding)\n        nn.init.uniform_(self.char_embeddings.weight, -0.001, 0.001)\n\n        self.char_cnn = nn.ModuleList()\n        for i in range(len(layer_conf.output_channel_num)):\n            self.char_cnn.append(nn.Conv2d(1, layer_conf.output_channel_num[i], (layer_conf.window_size[i], layer_conf.embedding_matrix_dim),\n                                   stride=self.layer_conf.stride[i], padding=self.layer_conf.padding))\n        if layer_conf.activation:\n            self.activation = eval(""nn."" + self.layer_conf.activation)()\n        else:\n            self.activation = None\n        # if self.is_cuda():\n        #     self.char_embeddings = self.char_embeddings.cuda()\n        #     self.char_cnn = self.char_cnn.cuda()\n        #     if self.activation and hasattr(self.activation, \'weight\'):\n        #         self.activation.weight = torch.nn.Parameter(self.activation.weight.cuda())\n\n    def forward(self, string):\n        """"""\n        Step1: [batch_size, seq_len, char num in words] -> [batch_size, seq_len * char num in words]\n        Step2: lookup embedding matrix -> [batch_size, seq_len * char num in words, embedding_dim]\n        reshape -> [batch_size * seq_len, char num in words, embedding_dim]\n        Step3: after convolution operation, got [batch_size * seq_len, char num related, output_channel_num]\n        Step4: max pooling on axis 1 and -reshape-> [batch_size * seq_len, output_channel_dim]\n        Step5: reshape -> [batch_size, seq_len, output_channel_dim]\n\n        Args:\n            string (Variable): [[char ids of word1], [char ids of word2], [...], ...], shape: [batch_size, seq_len, char num in words]\n\n        Returns:\n            Variable: [batch_size, seq_len, output_dim]\n\n        """"""\n        string_reshaped = string.view(string.size()[0], -1)     #[batch_size, seq_len * char num in words]\n\n        char_embs_lookup = self.char_embeddings(string_reshaped).float()    # [batch_size, seq_len * char num in words, embedding_dim]\n        char_embs_lookup = char_embs_lookup.view(-1, string.size()[2], self.layer_conf.embedding_matrix_dim)    #[batch_size * seq_len, char num in words, embedding_dim]\n\n        string_input = torch.unsqueeze(char_embs_lookup, 1)   # [batch_size * seq_len, input_channel_num=1, char num in words, embedding_dim]\n\n        outputs = []\n        for index, single_cnn in enumerate(self.char_cnn):\n            string_conv = single_cnn(string_input).squeeze(3)\n            if self.activation:\n                string_conv = self.activation(string_conv)\n\n            string_maxpooling = F.max_pool1d(string_conv, string_conv.size(2)).squeeze()\n            string_out = string_maxpooling.view(string.size()[0], -1, self.layer_conf.output_channel_num[index])\n\n            outputs.append(string_out)\n\n        if len(outputs) > 1:\n            string_output = torch.cat(outputs, 2)\n        else:\n            string_output = outputs[0]\n\n        return string_output\n\n\nif __name__ == \'__main__\':\n    conf = {\n        \'dim\': 30,\n        \'output_channel_num\': 30,\n        \'input_channel_num\': 1,\n        \'window_size\': 3,\n        \'activation\': \'PReLU\',\n\n        # should be infered from the corpus\n        \'vocab_size\': 10,\n        \'input_dims\': [5],\n        \'input_ranks\': [3],\n        \'use_gpu\': True\n    }\n    layer_conf = CNNCharEmbeddingConf(**conf)\n\n    # make a fake input: [bs, seq_len, char num in words]\n    # assume in this batch, the padded sentence length is 3 and the each word has 5 chars, including padding 0.\n    input_chars = np.array([\n        [[3, 1, 2, 5, 4], [1, 2, 3, 4, 0], [0, 0, 0, 0, 0]],\n        [[1, 1, 0, 0, 0], [2, 3, 1, 0, 0], [1, 2, 3, 4, 5]]\n    ])\n\n    char_emb_layer = CNNCharEmbedding(layer_conf)\n\n    input_chars = torch.LongTensor(input_chars)\n    output = char_emb_layer(input_chars)\n\n    print(output)\n\n\n\n'"
block_zoo/embedding/LSTMCharEmbedding.py,4,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport numpy as np\nfrom block_zoo.BaseLayer import BaseLayer, BaseConf\nfrom utils.DocInherit import DocInherit\n\n\nclass LSTMCharEmbeddingConf(BaseConf):\n    """""" Configuration of LSTMCharEmbedding\n\n    Args:\n        dim (int, optional): the dimension of character embedding after lstm. Default: 50\n        embedding_matrix_dim(int, optional): the dimension of character initialized embedding. Default: 30\n        padding(int, optional): Zero-padding added to both sides of the input. Default: 0\n        dropout(float, optional): dropout rate. Default: 0.2\n        bidirect_flag(Bool, optional): Using BiLSTM or not. Default: True\n    """"""\n    def __init__(self, **kwargs):\n        super(LSTMCharEmbeddingConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n\n        self.dim = 50  # lstm\'s output channel dim\n        self.embedding_matrix_dim = 30\n        self.padding = 0\n        self.dropout = 0.2\n        self.bidirect_flag = True\n\n    @DocInherit\n    def declare(self):\n        #self.input_channel_num = 1\n        self.num_of_inputs = 1\n        self.input_ranks = [3]\n\n    @DocInherit\n    def inference(self):\n        #self.output_channel_num = self.hidden_dim\n        self.output_rank = 3\n\n    @DocInherit\n    def verify(self):\n        # super(LSTMCharEmbeddingConf, self).verify()\n\n        necessary_attrs_for_user = [\'embedding_matrix_dim\', \'dim\', \'dropout\', \'bidirect_flag\', \'vocab_size\']\n        for attr in necessary_attrs_for_user:\n            self.add_attr_exist_assertion_for_user(attr)\n\n\nclass LSTMCharEmbedding(BaseLayer):\n    """"""\n    This layer implements the character embedding use LSTM\n    Args:\n        layer_conf (LSTMCharEmbeddingConf): configuration of LSTMCharEmbedding\n    """"""\n    def __init__(self, layer_conf):\n        super(LSTMCharEmbedding, self).__init__(layer_conf)\n        self.layer_conf = layer_conf\n\n        self.char_embeddings = nn.Embedding(layer_conf.vocab_size, layer_conf.embedding_matrix_dim, padding_idx=self.layer_conf.padding)\n        nn.init.uniform_(self.char_embeddings.weight, -0.001, 0.001)\n\n        if layer_conf.bidirect_flag:\n            self.dim = layer_conf.dim // 2\n        self.dropout = nn.Dropout(layer_conf.dropout)\n        self.char_lstm = nn.LSTM(layer_conf.embedding_matrix_dim, self.dim, num_layers=1, batch_first=True, bidirectional=layer_conf.bidirect_flag)\n\n        if self.is_cuda():\n            self.char_embeddings = self.char_embeddings.cuda()\n            self.dropout = self.dropout.cuda()\n            self.char_lstm = self.char_lstm.cuda()\n\n    def forward(self, string):\n        """"""\n        Step1: [batch_size, seq_len, char num in words] -> [batch_size*seq_len, char num in words]\n        Step2: lookup embedding matrix -> [batch_size*seq_len, char num in words, embedding_dim]\n        Step3: after lstm operation, got [num_layer* num_directions, batch_size * seq_len, dim]\n        Step5: reshape -> [batch_size, seq_len, dim]\n\n        Args:\n            string (Variable): [[char ids of word1], [char ids of word2], [...], ...], shape: [batch_size, seq_len, char num in words]\n\n        Returns:\n            Variable: [batch_size, seq_len, output_dim]\n\n        """"""\n        #print (\'string shape: \', string.size())\n        string_reshaped = string.view(string.size()[0]*string.size()[1],  -1)     #[batch_size, seq_len * char num in words]\n\n        char_embs_lookup = self.char_embeddings(string_reshaped).float()    # [batch_size, seq_len * char num in words, embedding_dim]\n        char_embs_drop = self.dropout(char_embs_lookup)\n        char_hidden = None\n        char_rnn_out, char_hidden = self.char_lstm(char_embs_drop, char_hidden)\n        #print(\'char_hidden shape: \', char_hidden[0].size())\n        string_out = char_hidden[0].transpose(1,0).contiguous().view(string.size()[0], string.size()[1], -1)\n        #print(\'string_out shape: \', string_out.size())\n        return string_out\n\n\nif __name__ == \'__main__\':\n    conf = {\n        \'embedding_matrix_dim\': 30,\n        \'dim\': 30,  # lstm\'s output channel dim\n        \'padding\': 0,\n        \'dropout\': 0.2,\n        \'bidirect_flag\': True,\n\n        # should be infered from the corpus\n        \'vocab_size\': 10,\n        \'input_dims\': [5],\n        \'input_ranks\': [3],\n        \'use_gpu\': True\n    }\n    layer_conf = LSTMCharEmbeddingConf(**conf)\n\n    # make a fake input: [bs, seq_len, char num in words]\n    # assume in this batch, the padded sentence length is 3 and the each word has 5 chars, including padding 0.\n    input_chars = np.array([\n        [[3, 1, 2, 5, 4], [1, 2, 3, 4, 0], [0, 0, 0, 0, 0]],\n        [[1, 1, 0, 0, 0], [2, 3, 1, 0, 0], [1, 2, 3, 4, 5]]\n    ])\n\n    char_emb_layer = LSTMCharEmbedding(layer_conf)\n\n    input_chars = torch.LongTensor(input_chars)\n    output = char_emb_layer(input_chars)\n\n    print(output)\n\n\n'"
block_zoo/embedding/__init__.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\nfrom .CNNCharEmbedding import CNNCharEmbeddingConf, CNNCharEmbedding\nfrom .LSTMCharEmbedding import LSTMCharEmbeddingConf, LSTMCharEmbedding'"
block_zoo/encoder_decoder/SLUDecoder.py,14,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport copy\nimport numpy as np\nfrom block_zoo.BaseLayer import BaseLayer, BaseConf\n#from layers.EncoderDecoder import EncoderDecoderConf\nfrom utils.DocInherit import DocInherit\nfrom utils.corpus_utils import get_seq_mask\n\nclass SLUDecoderConf(BaseConf):\n    """""" Configuration of Spoken Language Understanding Encoder\n\n    References:\n        Liu, B., & Lane, I. (2016). Attention-based recurrent neural network models for joint intent detection and slot filling. Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH, (1), 685\xe2\x80\x93689. https://doi.org/10.21437/Interspeech.2016-1352\n\n    Args:\n        hidden_dim (int): dimension of hidden state\n        dropout (float): dropout rate\n        num_layers (int): number of BiLSTM layers\n        num_decoder_output (int):\n    """"""\n    def __init__(self, **kwargs):\n        super(SLUDecoderConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        self.hidden_dim = 128\n        self.dropout = 0.0\n        self.num_layers = 1\n        self.decoder_emb_dim = 100\n\n        # number of decoder\'s outputs. E.g., for slot tagging, num_decoder_output means the number of tags;\n        # for machine translation, num_decoder_output means the number of words in the target language;\n        self.decoder_vocab_size = 10000\n\n        #input_dim and input_context_dim should be inferenced from encoder\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 1\n        self.input_ranks = [3]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = copy.deepcopy(self.input_dims[0])\n        self.output_dim[-1] = self.decoder_vocab_size\n        super(SLUDecoderConf, self).inference()  # PUT THIS LINE AT THE END OF inference()\n\n    @DocInherit\n    def verify(self):\n        super(SLUDecoderConf, self).verify()\n\n        necessary_attrs_for_user = [\'hidden_dim\', \'dropout\', \'num_layers\', \'decoder_emb_dim\', \'decoder_vocab_size\']\n        for attr in necessary_attrs_for_user:\n            self.add_attr_exist_assertion_for_user(attr)\n\n        necessary_attrs_for_dev = [\'input_dims\', \'input_context_dims\']\n        for attr in necessary_attrs_for_dev:\n            self.add_attr_exist_assertion_for_dev(attr)\n\n\nclass SLUDecoder(BaseLayer):\n    """""" Spoken Language Understanding Encoder\n\n    References:\n        Liu, B., & Lane, I. (2016). Attention-based recurrent neural network models for joint intent detection and slot filling. Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH, (1), 685\xe2\x80\x93689. https://doi.org/10.21437/Interspeech.2016-1352\n\n    Args:\n        layer_conf (SLUEncoderConf): configuration of a layer\n    """"""\n    def __init__(self, layer_conf):\n        super(SLUDecoder, self).__init__(layer_conf)\n        self.layer_conf = layer_conf\n\n        self.embedding = nn.Embedding(layer_conf.decoder_vocab_size, layer_conf.decoder_emb_dim)\n        self.embedding.weight.data.uniform_(-0.1, 0.1)  # init\n        #nn.init.uniform(self.embedding.weight, -0.1, 0.1)\n\n        #self.dropout = nn.Dropout(self.dropout_p)\n        #self.lstm = nn.LSTM(layer_conf.decoder_emb_dim + layer_conf.hidden_dim * 2, layer_conf.hidden_dim, layer_conf.num_layers, batch_first=True)\n        self.lstm = nn.LSTM(layer_conf.decoder_emb_dim + layer_conf.input_dims[0][-1] + layer_conf.input_context_dims[0][-1],\n            layer_conf.hidden_dim, layer_conf.num_layers, batch_first=True)     # CAUTION: single direction\n        self.attn = nn.Linear(layer_conf.input_context_dims[0][-1], layer_conf.hidden_dim *layer_conf.num_layers) # Attention\n        self.slot_out = nn.Linear(layer_conf.input_context_dims[0][-1] + layer_conf.hidden_dim * 1 *layer_conf.num_layers, layer_conf.decoder_vocab_size)\n\n    def Attention(self, hidden, encoder_outputs, encoder_maskings):\n        """"""\n\n        Args:\n            hidden : 1,B,D\n            encoder_outputs : B,T,D\n            encoder_maskings : B,T # ByteTensor\n        """"""\n\n        hidden = hidden.view(hidden.size()[1], -1).unsqueeze(2)\n\n        batch_size = encoder_outputs.size(0)  # B\n        max_len = encoder_outputs.size(1)  # T\n        energies = self.attn(encoder_outputs.contiguous().view(batch_size * max_len, -1))  # B*T,D -> B*T,D\n        energies = energies.view(batch_size, max_len, -1)  # B,T,D\n        attn_energies = energies.bmm(hidden).transpose(1, 2)  # B,T,D * B,D,1 --> B,1,T\n        attn_energies = attn_energies.squeeze(1).masked_fill(encoder_maskings, -1e12)  # PAD masking\n\n        alpha = F.softmax(attn_energies)  # B,T\n        alpha = alpha.unsqueeze(1)  # B,1,T\n        context = alpha.bmm(encoder_outputs)  # B,1,T * B,T,D => B,1,D\n\n        return context  # B,1,D\n\n    def forward(self, string, string_len, context, encoder_outputs):\n        """""" process inputs\n\n        Args:\n            string (Variable): word ids, [batch_size, seq_len]\n            string_len (ndarray): [batch_size]\n            context (Variable): [batch_size, 1, input_dim]\n            encoder_outputs (Variable): [batch_size, max_seq_len, input_dim]\n\n        Returns:\n            Variable : decode scores with shape [batch_size, seq_len, decoder_vocab_size]\n\n        """"""\n        batch_size = string.size(0)\n        if torch.cuda.device_count() > 1:\n            # otherwise, it will raise a Exception because the length inconsistence\n            string_mask = torch.ByteTensor(1 - get_seq_mask(string_len, max_seq_len=string.shape[1]))  # [batch_size, max_seq_len]\n        else:\n            string_mask = torch.ByteTensor(1 - get_seq_mask(string_len))  # [batch_size, max_seq_len]\n\n        decoded = torch.LongTensor([[1] * batch_size])\n        hidden_init = torch.zeros(self.layer_conf.num_layers * 1, batch_size, self.layer_conf.hidden_dim)\n        context_init = torch.zeros(self.layer_conf.num_layers*1, batch_size, self.layer_conf.hidden_dim)\n        if self.is_cuda():\n            device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n            string_mask = string_mask.to(device)\n            # Note id of ""<start>"" is 1!   decoded  is a batch of \'<start>\' at first\n            decoded = decoded.to(device)\n            hidden_init = hidden_init.to(device)\n            context_init = context_init.to(device)\n\n        decoded = decoded.transpose(1, 0)      # [batch_size, 1]\n\n        embedded = self.embedding(decoded)\n        hidden = (hidden_init, context_init)\n\n        decode = []\n        aligns = encoder_outputs.transpose(0, 1)    #[seq_len, bs, input_dim]\n        length = encoder_outputs.size(1)\n        for i in range(length):\n            aligned = aligns[i].unsqueeze(1)  # [bs, 1, input_dim]\n            self.lstm.flatten_parameters()\n            _, hidden = self.lstm(torch.cat((embedded, context, aligned), 2), hidden)\n\n            concated = torch.cat((hidden[0].view(1, batch_size, -1), context.transpose(0, 1)), 2)\n            score = self.slot_out(concated.squeeze(0))\n            softmaxed = F.log_softmax(score)        # decoder_vocab_dim\n            decode.append(softmaxed)\n            _, decoded = torch.max(softmaxed, 1)\n            embedded = self.embedding(decoded.unsqueeze(1))\n            context = self.Attention(hidden[0], encoder_outputs, string_mask)\n        slot_scores = torch.cat(decode, 1)\n        return slot_scores.view(batch_size, length, -1)\n\n\n\n\n'"
block_zoo/encoder_decoder/SLUEncoder.py,9,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport numpy as np\nfrom block_zoo.BaseLayer import BaseLayer, BaseConf\n#from layers.EncoderDecoder import EncoderDecoderConf\nfrom utils.DocInherit import DocInherit\nfrom utils.corpus_utils import get_seq_mask\nimport copy\n\nclass SLUEncoderConf(BaseConf):\n    """""" Configuration of Spoken Language Understanding Encoder\n\n    References:\n        Liu, B., & Lane, I. (2016). Attention-based recurrent neural network models for joint intent detection and slot filling. Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH, (1), 685\xe2\x80\x93689. https://doi.org/10.21437/Interspeech.2016-1352\n\n    Args:\n        hidden_dim (int): dimension of hidden state\n        dropout (float): dropout rate\n        num_layers (int): number of BiLSTM layers\n    """"""\n    def __init__(self, **kwargs):\n        super(SLUEncoderConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        self.hidden_dim = 128\n        self.dropout = 0.0\n        self.num_layers = 1\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 1\n        self.input_ranks = [3]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = copy.deepcopy(self.input_dims[0])\n        self.output_dim[-1] = 2 * self.hidden_dim   #no matter what the num_layers is\n        self.output_context_dim = copy.deepcopy(self.input_dims[0])\n        self.output_context_dim[-1] = 2 * self.hidden_dim   #no matter what the num_layers is\n\n        super(SLUEncoderConf, self).inference()  # PUT THIS LINE AT THE END OF inference()\n\n    @DocInherit\n    def verify_before_inference(self):\n        super(SLUEncoderConf, self).verify_before_inference()\n        necessary_attrs_for_user = [\'hidden_dim\']\n        for attr in necessary_attrs_for_user:\n            self.add_attr_exist_assertion_for_user(attr)\n\n    @DocInherit\n    def verify(self):\n        super(SLUEncoderConf, self).verify()\n        necessary_attrs_for_user = [\'dropout\', \'num_layers\']\n        for attr in necessary_attrs_for_user:\n            self.add_attr_exist_assertion_for_user(attr)\n\n\nclass SLUEncoder(BaseLayer):\n    """""" Spoken Language Understanding Encoder\n\n    References:\n        Liu, B., & Lane, I. (2016). Attention-based recurrent neural network models for joint intent detection and slot filling. Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH, (1), 685\xe2\x80\x93689. https://doi.org/10.21437/Interspeech.2016-1352\n\n    Args:\n        layer_conf (SLUEncoderConf): configuration of a layer\n    """"""\n    def __init__(self, layer_conf):\n        super(SLUEncoder, self).__init__(layer_conf)\n        self.layer_conf = layer_conf\n        self.lstm = nn.LSTM(layer_conf.input_dims[0][-1], layer_conf.hidden_dim, layer_conf.num_layers, batch_first=True,\n            bidirectional=True, dropout=layer_conf.dropout)\n\n    def forward(self, string, string_len):\n        """""" process inputs\n\n        Args:\n            string (Variable): [batch_size, seq_len, dim]\n            string_len (ndarray): [batch_size]\n\n        Returns:\n            Variable: output of bi-lstm with shape [batch_size, seq_len, 2 * hidden_dim]\n            ndarray: string_len with shape [batch_size]\n            Variable: context with shape [batch_size, 1, 2 * hidden_dim]\n\n        """"""\n        if torch.cuda.device_count() > 1:\n            # otherwise, it will raise a Exception because the length inconsistence\n            string_mask = torch.ByteTensor(1 - get_seq_mask(string_len, max_seq_len=string.shape[1]))  # [batch_size, max_seq_len]\n        else:\n            string_mask = torch.ByteTensor(1 - get_seq_mask(string_len))  # [batch_size, max_seq_len]\n\n        hidden_init = torch.zeros(self.layer_conf.num_layers * 2, string.size(0), self.layer_conf.hidden_dim)\n        context_init = torch.zeros(self.layer_conf.num_layers * 2, string.size(0), self.layer_conf.hidden_dim)\n        if self.is_cuda():\n            device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n            string_mask = string_mask.to(device)\n            hidden_init = hidden_init.to(device)\n            context_init = context_init.to(device)\n\n        hidden = (hidden_init, context_init)\n        self.lstm.flatten_parameters()\n        output, hidden = self.lstm(string, hidden)\n\n        assert output.shape[1] == string_mask.shape[1]\n\n        real_context = []\n        for i, o in enumerate(output):\n            real_length = string_mask[i].data.tolist().count(0)\n            real_context.append(o[real_length - 1])\n\n        return output, torch.cat(real_context).view(string.size(0), -1).unsqueeze(1)'"
block_zoo/encoder_decoder/__init__.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\nfrom .SLUEncoder import SLUEncoder, SLUEncoderConf\nfrom .SLUDecoder import SLUDecoder, SLUDecoderConf\n'"
block_zoo/math/Add2D.py,2,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport logging\n\nfrom ..BaseLayer import BaseConf\nfrom utils.DocInherit import DocInherit\nfrom utils.exceptions import ConfigurationError\nimport copy\n\nclass Add2DConf(BaseConf):\n    """""" Configuration of Add2D layer\n\n    """"""\n\n    #init the args\n    def __init__(self, **kwargs):\n        super(Add2DConf, self).__init__(**kwargs)\n\n    #set default params\n    #@DocInherit\n    #def default(self):\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 2\n        self.input_ranks = [2,2]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = copy.deepcopy(self.input_dims[0])\n        if self.input_dims[0][1] != 1:\n            self.output_dim[-1] = self.input_dims[0][1]\n        else:\n            self.output_dim[-1] = self.input_dims[1][1]\n\n        super(Add2DConf, self).inference()\n\n    @DocInherit\n    def verify(self):\n        super(Add2DConf, self).verify()\n\n        # # to check if the ranks of all the inputs are equal\n        # rank_equal_flag = True\n        # for i in range(len(self.input_ranks)):\n        #     if self.input_ranks[i] != self.input_ranks[0]:\n        #         rank_equal_flag = False\n        #         break\n        # if rank_equal_flag == False:\n        #     raise ConfigurationError(""For layer Add2D, the ranks of each inputs should be equal!"")\n\n        # to check if the dimensions of all the inputs are equal or is 1\n        dim_flag = True\n        input_dims = list(self.input_dims)\n        for i in range(len(input_dims)):\n            if input_dims[i][1] != input_dims[0][1] and input_dims[i][1] != 1 and input_dims[0][1] != 1:\n                dim_flag = False\n                break\n        if dim_flag == False:\n            raise ConfigurationError(""For layer Add2D, the dimensions of each inputs should be equal or 1"")\n\nclass Add2D(nn.Module):\n    """""" Add2D layer to get sum of two sequences(2D representation)\n\n    Args:\n        layer_conf (Add2DConf): configuration of a layer\n\n    """"""\n    def __init__(self, layer_conf):\n        super(Add2D, self).__init__()\n        self.layer_conf = layer_conf\n\n        logging.warning(""The length Add2D layer returns is the length of first input"")\n\n    def forward(self, *args):\n        """""" process input\n\n        Args:\n            *args: (Tensor): string, string_len, string2, string2_len\n                e.g. string (Tensor): [batch_size, dim], string_len (Tensor): [batch_size]\n\n        Returns:\n            Tensor: [batch_size, output_dim], [batch_size]\n        """"""\n        return torch.add(args[0],args[2]),args[1]'"
block_zoo/math/Add3D.py,2,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport logging\n\nfrom ..BaseLayer import BaseConf\nfrom utils.DocInherit import DocInherit\nfrom utils.exceptions import ConfigurationError\nimport copy\n\nclass Add3DConf(BaseConf):\n    """""" Configuration of Add3D layer\n\n    """"""\n\n    #init the args\n    def __init__(self, **kwargs):\n        super(Add3DConf, self).__init__(**kwargs)\n\n    #set default params\n    #@DocInherit\n    #def default(self):\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 2\n        self.input_ranks = [3,3]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = copy.deepcopy(self.input_dims[0])\n        if self.input_dims[0][-1] != 1:\n            self.output_dim[-1] = self.input_dims[0][-1]\n        else:\n            self.output_dim[-1] = self.input_dims[1][-1]\n            \n        super(Add3DConf, self).inference()\n\n    @DocInherit\n    def verify(self):\n        super(Add3DConf, self).verify()\n\n        # # to check if the ranks of all the inputs are equal\n        # rank_equal_flag = True\n        # for i in range(len(self.input_ranks)):\n        #     if self.input_ranks[i] != self.input_ranks[0]:\n        #         rank_equal_flag = False\n        #         break\n        # if rank_equal_flag == False:\n        #     raise ConfigurationError(""For layer Add3D, the ranks of each inputs should be equal!"")\n\nclass Add3D(nn.Module):\n    """""" Add3D layer to get sum of two sequences(3D representation)\n\n    Args:\n        layer_conf (Add3DConf): configuration of a layer\n\n    """"""\n    def __init__(self, layer_conf):\n        super(Add3D, self).__init__()\n        self.layer_conf = layer_conf\n\n        logging.warning(""The length Add3D layer returns is the length of first input"")\n\n    def forward(self, *args):\n        """""" process input\n\n        Args:\n            *args: (Tensor): string, string_len, string2, string2_len\n                e.g. string (Tensor): [batch_size, seq_len, dim], string_len (Tensor): [batch_size]\n\n        Returns:\n            Tensor: [batch_size, seq_len, output_dim], [batch_size]\n        """"""\n        dim_flag = True\n        input_dims = list(self.layer_conf.input_dims)\n        if (args[0].shape[1] * args[0].shape[2]) != (args[2].shape[1] * args[2].shape[2]):\n            if args[0].shape[1] == args[2].shape[1] and (input_dims[1][-1] == 1 or input_dims[0][-1] == 1):\n                dim_flag = True\n            else:\n                dim_flag = False\n\n        if dim_flag == False:\n            raise ConfigurationError(""For layer Add3D, the dimensions of each inputs should be equal or 1 ,or the elements number of two inputs (expect for the first dimension) should be equal"")\n\n\n        return torch.add(args[0],args[2]),args[1]'"
block_zoo/math/ElementWisedMultiply2D.py,2,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport logging\n\nfrom ..BaseLayer import BaseConf\nfrom utils.DocInherit import DocInherit\nfrom utils.exceptions import ConfigurationError\nimport copy\n\nclass ElementWisedMultiply2DConf(BaseConf):\n    """""" Configuration of ElementWisedMultiply2D layer\n\n    """"""\n\n    # init the args\n    def __init__(self,**kwargs):\n        super(ElementWisedMultiply2DConf, self).__init__(**kwargs)\n\n    #set default params\n    #@DocInherit\n    #def default(self):\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 2\n        self.input_ranks = [2,2]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = copy.deepcopy(self.input_dims[0])\n        if self.input_dims[0][1] != 1:\n            self.output_dim[-1] = self.input_dims[0][1]\n        else:\n            self.output_dim[-1] = self.input_dims[1][1]\n        \n        super(ElementWisedMultiply2DConf, self).inference()\n\n    @DocInherit\n    def verify(self):\n        super(ElementWisedMultiply2DConf, self).verify()\n\n        # # to check if the ranks of all the inputs are equal\n        # rank_equal_flag = True\n        # for i in range(len(self.input_ranks)):\n        #     if self.input_ranks[i] != self.input_ranks[0]:\n        #         rank_equal_flag = False\n        #         break\n        # if rank_equal_flag == False:\n        #     raise ConfigurationError(""For layer ElementWisedMultiply2D, the ranks of each inputs should be equal!"")\n\n        # to check if the dimensions of all the inputs are equal or is 1\n        dim_flag = True\n        input_dims = list(self.input_dims)\n        for i in range(len(input_dims)):\n            if input_dims[i][1] != input_dims[0][1] and input_dims[i] != 1 and input_dims[0][1] != 1:\n                dim_flag = False\n                break\n        if dim_flag == False:\n            raise ConfigurationError(""For layer ElementWisedMultiply2D, the dimensions of each inputs should be equal or 1"")\n\nclass ElementWisedMultiply2D(nn.Module):\n    """""" ElementWisedMultiply2D layer to do Element-Wised Multiply of two sequences(2D representation)\n\n    Args:\n        layer_conf (ElementWisedMultiply2DConf): configuration of a layer\n\n    """"""\n    def __init__(self, layer_conf):\n        super(ElementWisedMultiply2D, self).__init__()\n        self.layer_conf = layer_conf\n\n        logging.warning(""The length ElementWisedMultiply2D layer returns is the length of first input"")\n\n    def forward(self, *args):\n        """""" process input\n\n        Args:\n            *args: (Tensor): string, string_len, string2, string2_len\n                e.g. string (Tensor): [batch_size, dim], string_len (Tensor): [batch_size]\n\n\n        Returns:\n            Tensor: [batch_size, output_dim], [batch_size]\n        """"""\n        return torch.addcmul(torch.zeros(args[0].size()).to(\'cuda\'),1,args[0],args[2]),args[1]\n'"
block_zoo/math/ElementWisedMultiply3D.py,2,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport logging\n\nfrom ..BaseLayer import BaseConf\nfrom utils.DocInherit import DocInherit\nfrom utils.exceptions import ConfigurationError\nimport copy\n\nclass ElementWisedMultiply3DConf(BaseConf):\n    """""" Configuration of ElementWisedMultiply3D layer\n\n    """"""\n\n    # init the args\n    def __init__(self,**kwargs):\n        super(ElementWisedMultiply3DConf, self).__init__(**kwargs)\n\n    #set default params\n    #@DocInherit\n    #def default(self):\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 2\n        self.input_ranks = [3,3]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = copy.deepcopy(self.input_dims[0])\n        if self.input_dims[0][-1] != 1:\n            self.output_dim[-1] = self.input_dims[0][-1]\n        else:\n            self.output_dim[-1] = self.input_dims[1][-1]\n        \n        super(ElementWisedMultiply3DConf, self).inference()\n\n    @DocInherit\n    def verify(self):\n        super(ElementWisedMultiply3DConf, self).verify()\n\n        # # to check if the ranks of all the inputs are equal\n        # rank_equal_flag = True\n        # for i in range(len(self.input_ranks)):\n        #     if self.input_ranks[i] != self.input_ranks[0]:\n        #         rank_equal_flag = False\n        #         break\n        # if rank_equal_flag == False:\n        #     raise ConfigurationError(""For layer ElementWisedMultiply3D, the ranks of each inputs should be equal!"")\n\nclass ElementWisedMultiply3D(nn.Module):\n    """""" ElementWisedMultiply3D layer to do Element-Wised Multiply of two sequences(3D representation)\n\n    Args:\n        layer_conf (ElementWisedMultiply3DConf): configuration of a layer\n\n    """"""\n    def __init__(self, layer_conf):\n        super(ElementWisedMultiply3D, self).__init__()\n        self.layer_conf = layer_conf\n\n        logging.warning(""The length ElementWisedMultiply3D layer returns is the length of first input"")\n\n    def forward(self, *args):\n        """""" process input\n\n        Args:\n            *args: (Tensor): string, string_len, string2, string2_len\n                e.g. string (Tensor): [batch_size, seq_len, dim], string_len (Tensor): [batch_size]\n\n\n        Returns:\n            Tensor: [batch_size, seq_len, output_dim], [batch_size]\n        """"""\n        dim_flag = True\n        input_dims = list(self.layer_conf.input_dims)\n        if (args[0].shape[1] * args[0].shape[2]) != (args[2].shape[1] * args[2].shape[2]):\n            if args[0].shape[1] == args[2].shape[1] and (input_dims[1][-1] == 1 or input_dims[0][-1] == 1):\n                dim_flag = True\n            else:\n                dim_flag = False\n        if dim_flag == False:\n            raise ConfigurationError(""For layer ElementWisedMultiply3D, the dimensions of each inputs should be equal or 1 ,or the elements number of two inputs (expect for the first dimension) should be equal"")\n        return torch.addcmul(torch.zeros(args[0].size()).to(\'cuda\'),1,args[0],args[2]),args[1]\n'"
block_zoo/math/MatrixMultiply.py,4,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport logging\n\nfrom ..BaseLayer import BaseConf\nfrom utils.DocInherit import DocInherit\nfrom utils.exceptions import ConfigurationError\nimport copy\n\nclass MatrixMultiplyConf(BaseConf):\n    """""" Configuration of MatrixMultiply layer\n\n    Args:\n        operation(String):  a element of [\'common\', \'seq_based\', \'dim_based\'], default is \'dim_based\'\n                \'common\' means (batch_size, seq_len, dim)*(batch_size, seq_len, dim)\n                \'seq_based\' means (batch_size, dim, seq_len)*(batch_size, seq_len, dim)\n                \'dim_based\' means (batch_size, seq_len, dim)*(batch_size, dim, seq_len)\n\n    """"""\n\n    #init the args\n    def __init__(self,**kwargs):\n        super(MatrixMultiplyConf, self).__init__(**kwargs)\n\n    #set default params\n    @DocInherit\n    def default(self):\n        self.operation = \'dim_based\'\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 2\n        self.input_ranks = [3,3]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = copy.deepcopy(self.input_dims[0])\n        if self.operation == \'common\':\n            self.output_dim[-1] = self.input_dims[1][-1]\n        if self.operation == \'seq_based\':\n            self.output_dim[-1] = self.input_dims[1][-1]\n            self.output_dim[1] = self.input_dims[0][-1]\n        if self.operation == \'dim_based\':\n            self.output_dim[-1] = self.input_dims[1][1]\n\n        super(MatrixMultiplyConf, self).inference()\n\n    @DocInherit\n    def varify(self):\n        super(MatrixMultiplyConf, self).varify()\n        # # to check if the ranks of all the inputs are equal\n        # rank_equal_flag = True\n        # for i in range(len(self.input_ranks)):\n        #     if self.input_ranks[i] != self.input_ranks[0]:\n        #         rank_equal_flag = False\n        #         break\n        # if rank_equal_flag == False:\n        #     raise ConfigurationError(""For layer MatrixMultiply, the ranks of each inputs should be equal!"")\n\n        # to check if the value of operation is legal\n        if self.operation not in [\'common\', \'seq_based\', \'dim_based\']:\n            raise ConfigurationError(""the operation must be one of the \'common\', \'seq_based\' and \'dim_based\'"")\n\nclass MatrixMultiply(nn.Module):\n    """""" MatrixMultiply layer to multiply two matrix\n\n    Args:\n        layer_conf (MatrixMultiplyConf): configuration of a layer\n    """"""\n    def __init__(self, layer_conf):\n        super(MatrixMultiply, self).__init__()\n        self.layer_conf = layer_conf\n\n        logging.warning(""The length MatrixMultiply layer returns is the length of first input"")\n\n    def forward(self, *args):\n        """""" process input\n\n        Args:\n            *args: (Tensor): string, string_len, string2, string2_len\n                e.g. string (Tensor): [batch_size, seq_len, dim], string_len (Tensor): [batch_size]\n\n        Returns:\n            Tensor: [batch_size, seq_len, output_dim], [batch_size]\n\n        """"""\n        if self.layer_conf.operation == \'common\':\n            if args[0].shape[2] == args[2].shape[1]:\n                return torch.matmul(args[0],args[2]),args[1]\n            else:\n                raise Exception(""the dimensions of the two matrix for multiply is illegal"")\n        if self.layer_conf.operation == \'seq_based\':\n            if args[0].shape[1] == args[2].shape[1]:\n                string = args[0].permute(0,2,1)\n                return torch.matmul(string,args[2]),args[1]\n            else:\n                raise Exception(""the dimensions of the two matrix for multiply is illegal"")\n        if self.layer_conf.operation == \'dim_based\':\n            if args[0].shape[2] == args[2].shape[2]:\n                string = args[2].permute(0,2,1)\n                return torch.matmul(args[0],string),args[1]\n            else:\n                raise Exception(""the dimensions of the two matrix for multiply is illegal"")\n'"
block_zoo/math/Minus2D.py,2,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n\nimport torch\nimport torch.nn as nn\nimport logging\n\nfrom ..BaseLayer import BaseConf\nfrom utils.DocInherit import DocInherit\nfrom utils.exceptions import ConfigurationError\nimport copy\n\nclass Minus2DConf(BaseConf):\n    """""" Configuration of Minus2D layer\n\n    Args:\n        abs_flag: if the result of the Minus2D is abs, default is False\n\n    """"""\n\n    #init the args\n    def __init__(self,**kwargs):\n        super(Minus2DConf, self).__init__(**kwargs)\n\n    #set default params\n    @DocInherit\n    def default(self):\n        self.abs_flag = False\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 2\n        self.input_ranks = [2,2]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = copy.deepcopy(self.input_dims[0])\n        if self.input_dims[0][1] != 1:\n            self.output_dim[-1] = self.input_dims[0][1]\n        else:\n            self.output_dim[-1] = self.input_dims[1][1]\n\n        super(Minus2DConf, self).inference()\n\n    @DocInherit\n    def verify(self):\n        super(Minus2DConf, self).verify()\n\n        # # to check if the ranks of all the inputs are equal\n        # rank_equal_flag = True\n        # for i in range(len(self.input_ranks)):\n        #     if self.input_ranks[i] != self.input_ranks[0]:\n        #         rank_equal_flag = False\n        #         break\n        # if rank_equal_flag == False:\n        #     raise ConfigurationError(""For layer Minus2D, the ranks of each inputs should be equal!"")\n\n        # to check if the dimensions of all the inputs are equal or is 1\n        dim_flag = True\n        input_dims = list(self.input_dims)\n        for i in range(len(input_dims)):\n            if input_dims[i][1] != input_dims[0][1] and input_dims[i][1] != 1 and input_dims[0][1] != 1:\n                dim_flag = False\n                break\n        if dim_flag == False:\n            raise ConfigurationError(""For layer Minus2D, the dimensions of each inputs should be equal or 1"")\n\nclass Minus2D(nn.Module):\n    """"""Minus2D layer to get subtraction of two sequences(2D representation)\n\n    Args:\n        layer_conf (Minus2DConf): configuration of a layer\n\n    """"""\n    def __init__(self, layer_conf):\n        super(Minus2D, self).__init__()\n        self.layer_conf = layer_conf\n\n        logging.warning(""The length Minus2D layer returns is the length of first input"")\n\n    def forward(self, *args):\n        """""" process inputs\n\n        Args:\n            *args: (Tensor): string, string_len, string2, string2_len\n                e.g. string (Tensor): [batch_size, dim], string_len (Tensor): [batch_size]\n\n        Returns:\n            Tensor: [batch_size, output_dim], [batch_size]\n\n        """"""\n        if self.layer_conf.abs_flag == False:\n            return (args[0] - args[2]), args[1]\n        if self.layer_conf.abs_flag == True:\n            return torch.abs(args[0] - args[2]),args[1]\n\n\n'"
block_zoo/math/Minus3D.py,2,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport logging\n\nfrom ..BaseLayer import BaseConf\nfrom utils.DocInherit import DocInherit\nfrom utils.exceptions import ConfigurationError\nimport copy\n\nclass Minus3DConf(BaseConf):\n    """""" Configuration of Minus3D layer\n\n    Args:\n        abs_flag: if the result of the Minus3D is abs, default is False\n\n    """"""\n\n    # init the args\n    def __init__(self, **kwargs):\n        super(Minus3DConf, self).__init__(**kwargs)\n\n    # set default params\n    @DocInherit\n    def default(self):\n        self.abs_flag = False\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 2\n        self.input_ranks = [3,3]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = copy.deepcopy(self.input_dims[0])\n        if self.input_dims[0][-1] != 1:\n            self.output_dim[-1] = self.input_dims[0][-1]\n        else:\n            self.output_dim[-1] = self.input_dims[1][-1]\n        \n        super(Minus3DConf, self).inference()\n\n    @DocInherit\n    def verify(self):\n        super(Minus3DConf, self).verify()\n\n        # # to check if the ranks of all the inputs are equal\n        # rank_equal_flag = True\n        # for i in range(len(self.input_ranks)):\n        #     if self.input_ranks[i] != self.input_ranks[0]:\n        #         rank_equal_flag = False\n        #         break\n        # if rank_equal_flag == False:\n        #     raise ConfigurationError(""For layer Minus3D, the ranks of each inputs should be equal!"")\n\nclass Minus3D(nn.Module):\n    """""" Minus3D layer to get subtraction of two sequences(3D representation)\n\n    Args:\n        layer_conf (Minus3DConf): configuration of a layer\n\n    """"""\n    def __init__(self, layer_conf):\n        super(Minus3D, self).__init__()\n        self.layer_conf = layer_conf\n\n        logging.warning(""The length Minus3D layer returns is the length of first input"")\n\n    def forward(self, *args):\n        """""" process input\n\n        Args:\n            *args: (Tensor): string, string_len, string2, string2_len\n                e.g. string (Tensor): [batch_size, seq_len, dim], string_len (Tensor): [batch_size]\n\n        Returns:\n            Tensor: [batch_size, seq_len, output_dim], [batch_size]\n        """"""\n\n        # to check if the dimensions of all the inputs are legal for the Minus3D\n        dim_flag = True\n        input_dims = list(self.layer_conf.input_dims)\n        if (args[0].shape[1] * args[0].shape[2]) != (args[2].shape[1] * args[2].shape[2]):\n            if args[0].shape[1] == args[2].shape[1] and (input_dims[1][-1] == 1 or input_dims[0][-1] == 1):\n                dim_flag = True\n            else:\n                dim_flag = False\n        if dim_flag == False:\n            raise ConfigurationError(""For layer Minus3D, the dimensions of each inputs should be equal or 1 ,or the elements number of two inputs (expect for the first dimension) should be equal"")\n\n\n        if self.layer_conf.abs_flag == False:\n            return (args[0] - args[2]), args[1]\n        if self.layer_conf.abs_flag == True:\n            return torch.abs(args[0] - args[2]),args[1]'"
block_zoo/math/__init__.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\nfrom .Add2D import Add2D, Add2DConf\nfrom .Add3D import Add3D, Add3DConf\nfrom .ElementWisedMultiply2D import ElementWisedMultiply2D, ElementWisedMultiply2DConf\nfrom .ElementWisedMultiply3D import ElementWisedMultiply3D, ElementWisedMultiply3DConf\nfrom .Minus2D import Minus2D, Minus2DConf\nfrom .Minus3D import Minus3D, Minus3DConf\nfrom .MatrixMultiply import MatrixMultiply, MatrixMultiplyConf'"
block_zoo/normalizations/LayerNorm.py,4,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\n\nfrom block_zoo.BaseLayer import BaseLayer,BaseConf\nfrom utils.DocInherit import DocInherit\nimport copy\n\nclass LayerNormConf(BaseConf):\n    """""" Configuration of LayerNorm Layer\n\n    """"""\n    def __init__(self,**kwargs):\n        super(LayerNormConf, self).__init__(**kwargs)\n\n    # @DocInherit\n    # def default(self):\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 1\n        self.input_ranks = [3]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = copy.deepcopy(self.input_dims[0])\n        super(LayerNormConf, self).inference()\n\n    @DocInherit\n    def verify(self):\n        super(LayerNormConf, self).verify()\n\nclass LayerNorm(nn.Module):\n    """""" LayerNorm layer\n\n    Args:\n        layer_conf (LayerNormConf): configuration of a layer\n\n    """"""\n    def __init__(self,layer_conf):\n        super(LayerNorm, self).__init__()\n        self.layer_conf = layer_conf\n        self.g = nn.Parameter(torch.ones(self.layer_conf.input_dims[0][-1]))\n        self.b = nn.Parameter(torch.zeros(self.layer_conf.input_dims[0][-1]))\n        self.e = 1e-5\n\n    def forward(self, string, string_len):\n        """""" process input\n\n        Args:\n            string, string_len\n            e.g. string (Tensor): [batch_size, seq_len, dim], string_len (Tensor): [batch_size]\n\n        Returns:\n            Tensor: [batch_size, seq_len, output_dim], [batch_size]\n        """"""\n        u = string.mean(-1,keepdim=True)\n        s = (string - u).pow(2).mean(-1,keepdim=True)\n        string = (string - u)/torch.sqrt(s+self.e)\n        return self.g * string + self.b, string_len'"
block_zoo/normalizations/__init__.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\nfrom .LayerNorm import LayerNorm, LayerNormConf'"
block_zoo/op/CalculateDistance.py,5,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport logging\n\nfrom ..BaseLayer import BaseConf, BaseLayer\nfrom utils.DocInherit import DocInherit\nfrom utils.exceptions import ConfigurationError\nimport copy\n\n\nclass CalculateDistanceConf(BaseConf):\n    """""" Configuration of CalculateDistance Layer\n\n    Args:\n        operations (list):  a subset of [""cos"", ""euclidean"", ""manhattan"", ""chebyshev""].\n    """"""\n\n    # init the args\n    def __init__(self, **kwargs):\n        super(CalculateDistanceConf, self).__init__(**kwargs)\n\n    # set default params\n    @DocInherit\n    def default(self):\n        self.operations = [""cos"", ""euclidean"", ""manhattan"", ""chebyshev""]\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 2\n        self.input_ranks = [2]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = copy.deepcopy(self.input_dims[0])\n        self.output_dim[-1] = 1\n\n        super(CalculateDistanceConf, self).inference()\n\n    @DocInherit\n    def verify(self):\n        super(CalculateDistanceConf, self).verify()\n\n        assert len(self.input_dims) == 2, ""Operation requires that there should be two inputs""\n\n        # to check if the ranks of all the inputs are equal\n        rank_equal_flag = True\n        for i in range(len(self.input_ranks)):\n            if self.input_ranks[i] != self.input_ranks[0] or self.input_ranks[i] != 2:\n                rank_equal_flag = False\n                break\n        if rank_equal_flag == False:\n            raise ConfigurationError(""For layer CalculateDistance, the ranks of each inputs should be equal and 2!"")\n\n\nclass CalculateDistance(BaseLayer):\n    """""" CalculateDistance layer to calculate the distance of sequences(2D representation)\n\n    Args:\n        layer_conf (CalculateDistanceConf): configuration of a layer\n    """"""\n\n    def __init__(self, layer_conf):\n        super(CalculateDistance, self).__init__(layer_conf)\n        self.layer_conf = layer_conf\n\n\n    def forward(self, x, x_len, y, y_len):\n        """"""\n\n        Args:\n            x: [batch_size, dim]\n            x_len: [batch_size]\n            y: [batch_size, dim]\n            y_len: [batch_size]\n        Returns:\n            Tensor: [batch_size, 1], None\n\n        """"""\n\n        batch_size = x.size()[0]\n        if ""cos"" in self.layer_conf.operations:\n            result = F.cosine_similarity(x , y)\n        elif ""euclidean"" in self.layer_conf.operations:\n            result = torch.sqrt(torch.sum((x-y)**2, dim=1))\n        elif ""manhattan"" in self.layer_conf.operations:\n            result = torch.sum(torch.abs((x - y)), dim=1)\n        elif ""chebyshev"" in self.layer_conf.operations:\n            result = torch.abs((x - y)).max(dim=1)\n        else:\n            raise ConfigurationError(""This operation is not supported!"")\n\n        result = result.view(batch_size, 1)\n        return result, None\n'"
block_zoo/op/Combination.py,5,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport numpy as np\nimport logging\n\nfrom block_zoo.BaseLayer import BaseConf\nfrom utils.DocInherit import DocInherit\nfrom utils.exceptions import ConfigurationError\nimport copy\n\nclass CombinationConf(BaseConf):\n    """""" Configuration for combination layer\n\n    Args:\n        operations (list):  a subset of [""origin"", ""difference"", ""dot_multiply""].\n                ""origin"" means to keep the original representations;\\n\n                ""difference"" means abs(sequence1 - sequence2);\n                ""dot_multiply"" means element-wised product;\n\n    """"""\n    def __init__(self, **kwargs):\n        super(CombinationConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        # supported operations: ""origin"", ""difference"", ""dot_multiply""\n        self.operations = [""origin"", ""difference"", ""dot_multiply""]\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = -1\n        self.input_ranks = [-1]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = copy.deepcopy(self.input_dims[0])\n        self.output_dim[-1] = 0\n        if ""origin"" in self.operations:\n            self.output_dim[-1] += sum([input_dim[-1] for input_dim in self.input_dims])\n        if ""difference"" in self.operations:\n            self.output_dim[-1] += int(np.mean([input_dim[-1] for input_dim in self.input_dims]))     # difference operation requires dimension of all the inputs should be equal\n        if ""dot_multiply"" in self.operations:\n            self.output_dim[-1] += int(np.mean([input_dim[-1] for input_dim in self.input_dims]))     # dot_multiply operation requires dimension of all the inputs should be equal\n        super(CombinationConf, self).inference()\n\n    @DocInherit\n    def verify(self):\n        super(CombinationConf, self).verify()\n\n        # to check if the ranks of all the inputs are equal\n        rank_equal_flag = True\n        for i in range(len(self.input_ranks)):\n            if self.input_ranks[i] != self.input_ranks[0]:\n                rank_equal_flag = False\n                break\n        if rank_equal_flag == False:\n            raise ConfigurationError(""For layer Combination, the ranks of each inputs should be consistent!"")\n\n        if ""difference"" in self.operations:\n            assert len(self.input_dims) == 2, ""Difference operation requires that there should be two inputs""\n\n        if ""difference"" in self.operations or ""dot_multiply"" in self.operations:\n            input_dims = list(self.input_dims)\n            dim_equal_flag = True\n            for i in range(len(input_dims)):\n                if input_dims[i] != input_dims[0]:\n                    dim_equal_flag = False\n                    break\n            if dim_equal_flag == False:\n                raise Exception(""Difference and dot multiply require that the input dimensions should be the same"")\n\n\nclass Combination(nn.Module):\n    """""" Combination layer to merge the representation of two sequence\n\n    Args:\n        layer_conf (CombinationConf): configuration of a layer\n    """"""\n    def __init__(self, layer_conf):\n        super(Combination, self).__init__()\n        self.layer_conf = layer_conf\n\n        logging.warning(""The length Combination layer returns is the length of first input"")\n\n    def forward(self, *args):\n        """""" process inputs\n\n        Args:\n            args (list): [string, string_len, string2, string2_len, ...]\n                e.g. string (Variable): [batch_size, dim], string_len (ndarray): [batch_size]\n\n        Returns:\n            Variable: [batch_size, output_dim], None\n\n        """"""\n        result = []\n        if ""origin"" in self.layer_conf.operations:\n            for idx, input in enumerate(args):\n                if idx % 2 == 0:\n                    result.append(input)\n\n        if ""difference"" in self.layer_conf.operations:\n            result.append(torch.abs(args[0] - args[2]))\n\n        if ""dot_multiply"" in self.layer_conf.operations:\n            result_multiply = None\n            for idx, input in enumerate(args):\n                if idx % 2 == 0:\n                    if result_multiply is None:\n                        result_multiply = input\n                    else:\n                        result_multiply = result_multiply * input\n            result.append(result_multiply)\n\n        last_dim = len(args[0].size()) - 1\n        return torch.cat(result, last_dim), args[1]  #concat on the last dimension\n\n'"
block_zoo/op/Concat2D.py,2,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport logging\n\nfrom ..BaseLayer import BaseConf,BaseLayer\nfrom utils.DocInherit import DocInherit\nfrom utils.exceptions import ConfigurationError\nimport copy\n\nclass Concat2DConf(BaseConf):\n    """""" Configuration of Concat2D Layer\n\n    Args:\n        concat2D_axis(int): which axis to conduct concat2D, default is 1.\n    """"""\n\n    # init the args\n    def __init__(self,**kwargs):\n        super(Concat2DConf, self).__init__(**kwargs)\n\n    # set default params\n    @DocInherit\n    def default(self):\n        self.concat2D_axis = 1\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = -1\n        self.input_ranks = [2]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = copy.deepcopy(self.input_dims[0])\n        self.output_dim[-1] = 0\n        self.output_dim[-1] += sum([input_dim[-1] for input_dim in self.input_dims])\n        \n        super(Concat2DConf, self).inference()\n\n    @DocInherit\n    def verify(self):\n        super(Concat2DConf, self).verify()\n\n        # to check if the ranks of all the inputs are equal\n        rank_equal_flag = True\n        for i in range(len(self.input_ranks)):\n            if self.input_ranks[i] != self.input_ranks[0]:\n                rank_equal_flag = False\n                break\n        if rank_equal_flag == False:\n            raise ConfigurationError(""For layer Concat2D, the ranks of each inputs should be equal!"")\n\n        # to check if the concat2D_axis is legal\n        if self.concat2D_axis != 1:\n            raise ConfigurationError(""For layer Concat2D, the concat axis must be 1!"")\n\nclass Concat2D(nn.Module):\n    """""" Concat2D layer to merge sum of sequences(2D representation)\n\n    Args:\n        layer_conf (Concat2DConf): configuration of a layer\n    """"""\n    def __init__(self, layer_conf):\n        super(Concat2D, self).__init__()\n        self.layer_conf = layer_conf\n\n        logging.warning(""The length Concat2D layer returns is the length of first input"")\n\n    def forward(self, *args):\n        """""" process inputs\n\n        Args:\n            *args: (Tensor): string, string_len, string2, string2_len, ...\n                e.g. string (Tensor): [batch_size, dim], string_len (Tensor): [batch_size]\n\n        Returns:\n            Tensor: [batch_size, output_dim], [batch_size]\n\n        """"""\n        result = []\n        for idx, input in enumerate(args):\n            if idx % 2 == 0:\n                result.append(input)\n        return torch.cat(result,self.layer_conf.concat2D_axis), args[1]'"
block_zoo/op/Concat3D.py,2,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport logging\n\nfrom ..BaseLayer import BaseConf,BaseLayer\nfrom utils.DocInherit import DocInherit\nfrom utils.exceptions import ConfigurationError\nimport copy\n\nclass Concat3DConf(BaseConf):\n    """""" Configuration of Concat3D layer\n\n    Args:\n        concat3D_axis(1 or 2): which axis to conduct Concat3D, default is 2.\n\n    """"""\n\n    # init the args\n    def __init__(self,**kwargs):\n        super(Concat3DConf, self).__init__(**kwargs)\n\n    # set default params\n    @DocInherit\n    def default(self):\n        self.concat3D_axis = 2\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = -1\n        self.input_ranks =[3]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = copy.deepcopy(self.input_dims[0])\n        self.output_dim[-1] = 0\n        self.output_dim[1] = 0\n        if self.concat3D_axis == 1:\n            self.output_dim[-1] = self.input_dims[0][-1]\n            self.output_dim[1] = sum([input_dim[1] for input_dim in self.input_dims])\n        if self.concat3D_axis == 2:\n            self.output_dim[-1] = sum([input_dim[-1] for input_dim in self.input_dims])\n            self.output_dim[1] = self.input_dims[0][1]\n\n        super(Concat3DConf, self).inference()\n\n    @DocInherit\n    def verify(self):\n        super(Concat3DConf, self).verify()\n\n        # to check if the ranks of all the inputs are equal\n        rank_equal_flag = True\n        for i in range(len(self.input_ranks)):\n            if self.input_ranks[i] != self.input_ranks[0]:\n                rank_equal_flag = False\n                break\n        if rank_equal_flag == False:\n            raise ConfigurationError(""For layer Concat3D, the ranks of each inputs should be equal!"")\n\n        if self.concat3D_axis == 1:\n            # to check if the dimensions of all the inputs are equal\n            input_dims = list(self.input_dims)\n            dim_equal_flag = True\n            for i in range(len(input_dims)):\n                if input_dims[i][-1] != input_dims[0][-1]:\n                    dim_equal_flag = False\n                    break\n            if dim_equal_flag == False:\n                raise Exception(""Concat3D with axis = 1 require that the input dimensions should be the same!"")\n\n        # to check if the concat3D_axis is legal\n        if self.concat3D_axis not in [1, 2]:\n            raise ConfigurationError(""For layer Concat3D, the concat axis must be 1 or 2!"")\n\nclass Concat3D(nn.Module):\n    """""" Concat3D layer to merge sum of sequences(3D representation)\n\n    Args:\n        layer_conf (Concat3DConf): configuration of a layer\n    """"""\n    def __init__(self,layer_conf):\n        super(Concat3D, self).__init__()\n        self.layer_conf = layer_conf\n\n        logging.warning(""The length Concat3D layer returns is the length of first input"")\n\n    def forward(self, *args):\n        """""" process inputs\n\n        Args:\n            *args: (Tensor): string, string_len, string2, string2_len, ...\n                e.g. string (Tensor): [batch_size, seq_len, dim], string_len (Tensor): [batch_size]\n\n        Returns:\n            Tensor: [batch_size, seq_len, output_dim], [batch_size]\n\n        """"""\n        result = []\n        if self.layer_conf.concat3D_axis == 1:\n            for idx, input in enumerate(args):\n                if idx % 2 == 0:\n                    result.append(input)\n        if self.layer_conf.concat3D_axis == 2:\n            input_shape = args[0].shape[1]\n            for idx, input in enumerate(args):\n                if idx % 2 == 0:\n                    if input_shape == input.shape[1]:\n                        result.append(input)\n                    else:\n                        raise Exception(""Concat3D with axis = 2 require that the input sequences length should be the same!"")\n\n        return torch.cat(result, self.layer_conf.concat3D_axis), args[1]\n'"
block_zoo/op/Expand_plus.py,3,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n# Come from http://www.hangli-hl.com/uploads/3/1/6/8/3168008/hu-etal-nips2014.pdf [ARC-II]\n\nimport torch\nimport torch.nn as nn\nimport copy\n\nfrom block_zoo.BaseLayer import BaseLayer, BaseConf\nfrom utils.DocInherit import DocInherit\nfrom utils.exceptions import ConfigurationError\n\nclass Expand_plusConf(BaseConf):\n    """"""Configuration for Expand_plus layer\n\n    """"""\n    def __init__(self, **kwargs):\n        super(Expand_plusConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        self.operation = \'Plus\'\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 2\n        self.input_ranks = [3, 3]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = copy.deepcopy(self.input_dims[0])\n        if self.input_dims[0][1] == -1 or self.input_dims[1][1] == -1:\n            raise ConfigurationError(""For Expand_plus layer, the sequence length should be fixed"")\n        self.output_dim.insert(2, self.input_dims[1][1])   # y_len\n        super(Expand_plusConf, self).inference()  # PUT THIS LINE AT THE END OF inference()\n\n    @DocInherit\n    def verify(self):\n        super(Expand_plusConf, self).verify()\n\n\nclass Expand_plus(BaseLayer):\n    """"""  Expand_plus layer\n    Given sequences X and Y, put X and Y expand_dim, and then add.\n\n    Args:\n        layer_conf (Expand_plusConf): configuration of a layer\n\n    """"""\n    def __init__(self, layer_conf):\n\n        super(Expand_plus, self).__init__(layer_conf)\n        assert layer_conf.input_dims[0][-1] == layer_conf.input_dims[1][-1]\n\n\n    def forward(self, x, x_len, y, y_len):\n        """"""\n\n        Args:\n            x:      [batch_size, x_max_len, dim].\n            x_len:  [batch_size], default is None.\n            y:      [batch_size, y_max_len, dim].\n            y_len:  [batch_size], default is None.\n\n        Returns:\n            output: batch_size, x_max_len, y_max_len, dim].\n\n        """"""\n\n        x_new = torch.stack([x]*y.size()[1], 2) # [batch_size, x_max_len, y_max_len, dim]\n        y_new = torch.stack([y]*x.size()[1], 1) # [batch_size, x_max_len, y_max_len, dim]\n\n        return x_new + y_new, None\n\n\n'"
block_zoo/op/Flatten.py,1,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch.nn as nn\nimport logging\n\nfrom block_zoo.BaseLayer import BaseConf\nfrom utils.DocInherit import DocInherit\nfrom utils.exceptions import ConfigurationError\nimport copy\n\nclass FlattenConf(BaseConf):\n    """"""Configuration of Flatten layer\n\n    """"""\n\n    #init the args\n    def __init__(self, **kwargs):\n        super(FlattenConf, self).__init__(**kwargs)\n\n    #set default params\n    #@DocInherit\n    #def default(self):\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 1\n        self.input_ranks = [-1]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = []\n        flatted_length = 1\n        for i in range(1, len(self.input_dims[0])):\n            if self.input_dims[0][i] == -1:\n                raise ConfigurationError(""For Flatten layer, the sequence length should be fixed"")\n            else:\n                flatted_length *= self.input_dims[0][i]\n        \n        self.output_dim = [self.input_dims[0][0], flatted_length]\n            \n        super(FlattenConf, self).inference()\n\n    @DocInherit\n    def verify(self):\n        super(FlattenConf, self).verify()\n\nclass Flatten(nn.Module):\n    """"""  Flatten layer to flatten the input from [bsatch_size, seq_len, dim] to [batch_size, seq_len*dim]\n\n    Args:\n        layer_conf(FlattenConf): configuration of a layer\n    """"""\n\n    def __init__(self, layer_conf):\n        super(Flatten, self).__init__()\n        self.layer_conf = layer_conf\n\n    def forward(self, string, string_len):\n        """""" process input\n\n        Args:\n            *args: (Tensor): string,string_len\n                e.g. string (Tensor): [batch_size, seq_len, dim], string_len (Tensor): [batch_size]\n        Returns:\n            Tensor: [batch_size, seq_len*dim], [batch_size]\n        """"""\n        flattened = string.view(string.shape[0], -1)\n        string_len = flattened.size(1)\n\n        return flattened, string_len\n\n'"
block_zoo/op/Match.py,1,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch.nn as nn\nimport copy\n\nfrom block_zoo.BaseLayer import BaseLayer, BaseConf\nfrom utils.DocInherit import DocInherit\nfrom utils.exceptions import ConfigurationError\n\nclass MatchConf(BaseConf):\n    """"""Configuration for MatchAttention layer\n\n    """"""\n    def __init__(self, **kwargs):\n        super(MatchConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        self.activation = \'PReLU\'\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 2\n        self.input_ranks = [3, 3]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = copy.deepcopy(self.input_dims[0])\n        if self.input_dims[0][1] == -1 or self.input_dims[1][1] == -1:\n            raise ConfigurationError(""For Match layer, the sequence length should be fixed"")\n        self.output_dim[-1] = self.input_dims[1][1]     # y_len\n        super(MatchConf, self).inference()  # PUT THIS LINE AT THE END OF inference()\n\n    @DocInherit\n    def verify(self):\n        super(MatchConf, self).verify()\n\n\nclass Match(BaseLayer):\n    """"""  Match layer\n    Given sequences X and Y, match sequence Y to each element in X.\n\n    Args:\n        layer_conf (MatchConf): configuration of a layer\n\n    """"""\n    def __init__(self, layer_conf):\n\n        super(Match, self).__init__(layer_conf)\n        assert layer_conf.input_dims[0][-1] == layer_conf.input_dims[1][-1]\n        self.linear = nn.Linear(layer_conf.input_dims[0][-1], layer_conf.input_dims[0][-1])\n        if layer_conf.activation:\n            self.activation = eval(""nn."" + self.layer_conf.activation)()\n        else:\n            self.activation = None\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x, x_len, y, y_len):\n        """"""\n\n        Args:\n            x:      [batch_size, x_max_len, dim].\n            x_len:  [batch_size], default is None.\n            y:      [batch_size, y_max_len, dim].\n            y_len:  [batch_size], default is None.\n\n        Returns:\n            output: has the same shape as x.\n\n        """"""\n\n        x_proj = self.linear(x)  # [batch_size, x_max_len, dim]\n        y_proj = self.linear(y)  # [batch_size, y_max_len, dim]\n        if self.activation:\n            x_proj = self.activation(x_proj)\n            y_proj = self.activation(y_proj)\n        scores = x_proj.bmm(y_proj.transpose(2, 1))     # [batch_size, x_max_len, y_max_len]\n        return scores, x_len\n\n\n'"
block_zoo/op/__init__.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\nfrom .Concat2D import Concat2D, Concat2DConf\nfrom .Concat3D import Concat3D, Concat3DConf\nfrom .Combination import Combination, CombinationConf\nfrom .Match import Match, MatchConf\nfrom .Flatten import Flatten, FlattenConf\nfrom .Expand_plus import Expand_plus, Expand_plusConf\nfrom .CalculateDistance import CalculateDistance, CalculateDistanceConf'"
block_zoo/transformer/MLP.py,2,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport math\n\nfrom block_zoo.BaseLayer import BaseLayer,BaseConf\nfrom utils.DocInherit import DocInherit\nimport copy\n\nclass MLPConf(BaseConf):\n    """""" Configuration of MLP layer\n\n    Args:\n        dropout (float): the dropout of MLP layer\n\n    """"""\n    def __init__(self, **kwargs):\n        super(MLPConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        self.dropout = 0.1\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 1\n        self.input_ranks = [3]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = copy.deepcopy(self.input_dims[0])\n        super(MLPConf, self).inference()\n\n    @DocInherit\n    def verify(self):\n        super(MLPConf, self).verify()\n\nclass MLP(nn.Module):\n    """""" MLP layer\n\n    Args:\n        layer_conf (MLPConf): configuration of a layer\n\n    """"""\n    def __init__(self, layer_conf):\n        super(MLP, self).__init__()\n        self.layer_conf = layer_conf\n        self.n_state = self.layer_conf.input_dims[0][-1]\n        self.c_fc = nn.Linear(self.layer_conf.input_dims[0][-1], 4*self.n_state)\n        self.c_proj = nn.Linear(4*self.n_state, self.layer_conf.input_dims[0][-1])\n\n    def gelu(self,x):\n        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n\n    def forward(self, string, string_len):\n        """""" process input\n\n        Args:\n            string, string_len\n            e.g. string (Tensor): [batch_size, seq_len, dim], string_len (Tensor): [batch_size]\n\n        Returns:\n            Tensor: [batch_size, seq_len, output_dim], [batch_size]\n\n        """"""\n        h = self.gelu(self.c_fc(string))\n        h2 = self.c_proj(h)\n        return nn.Dropout(self.layer_conf.dropout)(h2), string_len\n'"
block_zoo/transformer/MultiHeadAttention.py,5,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport math\n\nfrom block_zoo.BaseLayer import BaseLayer,BaseConf\nfrom utils.DocInherit import DocInherit\nimport copy\n\nclass MultiHeadAttentionConf(BaseConf):\n    """""" Configuration of MultiHeadAttention Layer\n\n    Args:\n        n_head (int): the head number of attention\n        scale (bool): if need to scale\n        attn_dropout (float): the dropout of attention layer\n        resid_dropout (float): the dropout of last Linear\n    """"""\n    \n    def __init__(self,**kwargs):\n\n        super(MultiHeadAttentionConf, self).__init__(**kwargs)\n\n    @DocInherit\n    def default(self):\n        self.n_head = 12\n        self.scale = True\n        self.attn_dropout = 0.1\n        self.resid_dropout = 0.1\n\n    @DocInherit\n    def declare(self):\n        self.num_of_inputs = 1\n        self.input_ranks = [3]\n\n    @DocInherit\n    def inference(self):\n        self.output_dim = copy.deepcopy(self.input_dims[0])\n        super(MultiHeadAttentionConf, self).inference()\n\n    @DocInherit\n    def verify(self):\n        super(MultiHeadAttentionConf, self).verify()\n\nclass MultiHeadAttention(nn.Module):\n    """""" MultiHeadAttention Layer\n\n    Args:\n        layer_conf (MultiHeadAttentionConf): configuration of a layer\n\n    """"""\n    def __init__(self, layer_conf):\n        super(MultiHeadAttention, self).__init__()\n        self.layer_conf = layer_conf\n        self.split_size = self.layer_conf.input_dims[0][-1]\n        self.n_state = self.layer_conf.input_dims[0][-1]\n        self.device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n        assert self.n_state % self.layer_conf.n_head == 0\n\n        self.c_attn = nn.Linear(self.layer_conf.input_dims[0][-1],self.n_state * 3)\n        self.c_proj = nn.Linear(self.layer_conf.input_dims[0][-1],self.n_state)\n\n    def _attn(self, q, k, v):\n        w = torch.matmul(q, k).to(self.device)\n        if self.layer_conf.scale:\n            w = w / math.sqrt(v.size(-1))\n        w = w * self.b + -1e9 * (1 - self.b)\n        w = nn.Softmax(dim=-1)(w)\n        w = nn.Dropout(self.layer_conf.attn_dropout)(w)\n        return torch.matmul(w, v)\n\n    def merge_heads(self, x):\n        x = x.permute(0, 2, 1, 3).contiguous()\n        new_x_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)\n        return x.view(*new_x_shape)\n\n    def split_heads(self, x, k=False):\n        new_x_shape = x.size()[:-1] + (self.layer_conf.n_head, x.size(-1) // self.layer_conf.n_head)\n        x = x.view(*new_x_shape)\n        if k:\n            return x.permute(0, 2, 3, 1)\n        else:\n            return x.permute(0, 2, 1, 3)\n\n    def forward(self, string, string_len):\n        """""" process input\n\n        Args:\n            string, string_len\n            e.g. string (Tensor): [batch_size, seq_len, dim], string_len (Tensor): [batch_size]\n\n        Returns:\n            Tensor: [batch_size, seq_len, output_dim], [batch_size]\n        """"""\n        self.register_buffer(\'b\', torch.tril(torch.ones(string.shape[1], string.shape[1]).to(self.device)).view(1, 1, string.shape[1], string.shape[1]))\n        x = self.c_attn(string)\n        query, key, value = x.split(self.split_size, dim=2)\n        query = self.split_heads(query)\n        key = self.split_heads(key, k=True)\n        value = self.split_heads(value)\n        a = self._attn(query, key, value)\n        a = self.merge_heads(a)\n        a = self.c_proj(a)\n        a = nn.Dropout(self.layer_conf.resid_dropout)(a)\n        return a, string_len'"
block_zoo/transformer/__init__.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\nfrom .MLP import MLP,MLPConf\nfrom .MultiHeadAttention import MultiHeadAttention,MultiHeadAttentionConf'"
model_visualizer/server/main.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\n# -*- coding:utf-8 -*-\nimport web\nimport json\nfrom web import form\nfrom mv import json2graph\n\nrender = web.template.render(\'templates/\')\nurls = (\n    \'/\', \'index\',\n    \'/mv\', \'model_visualizer\'\n)\n\nmv_form = form.Form(\n    form.Textarea(""json"", description=""config_json""),\n    form.Textarea(""output"", description=""output""),\n    form.Button(""submit"", type=""submit"", description=""Submit""),\n)\n\n\nclass index:\n    def GET(self):\n        raise web.seeother(\'/mv\')\n\n\nclass model_visualizer:\n    def GET(self):\n        f = mv_form()\n        status = False\n        return render.model_visualizer(f, status)\n\n\n    def POST(self):\n        f = mv_form()\n        post_value = web.input(json=None)\n        f[\'json\'].value = post_value.json\n        json2graph(post_value.json)\n        status = True\n        return render.model_visualizer(f, status)\n\nif __name__ == ""__main__"":\n    app = web.application(urls, globals())\n    app.run()\n\n\n\n\n'"
model_visualizer/server/mv.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n# Licensed under the MIT license.\n\nimport json\nfrom graphviz import *\n\ndef json2graph(json_str):\n    try:\n        conf_dic = json.loads(json_str)\n    except ValueError as e:\n        return e, False\n        # return ""ERROR JSON!""\n\n    color = {\n        ""Input"": ""royalblue"",\n        ""Embedding"": ""orange"",\n        ""Linear"": ""tan"",\n        ""LinearAttention"": ""tan"",\n        ""BiGRU"": ""salmon"",\n        ""BiLSTM"": ""salmon"",\n        ""BiLSTMAtt"": ""salmon1"",\n        ""BiGRULast"": ""salmon"",\n        ""Conv"": ""sandybrown"",\n        ""ConvPooling"": ""sandybrown"",\n        ""Pooling"": ""skyblue"",\n        ""Dropout"": ""yellowgreen"",\n        ""Combination"": ""purple"",\n        ""EncoderDecoder"": ""lightsalmon"",\n        ""FullAttention"": ""lightsalmon"",\n        ""Seq2SeqAttention"": ""lightsalmon""\n    }\n    layer_conf = {\n        ""Linear"": [""hidden_dim"", ""activation"", ""last_hidden_activation"", ""last_hidden_softmax"", ""batch_normalization""],\n        ""LinearAttention"": [""keep_dim""],\n        ""BiGRU"": [""hidden_dim"", ""dropout""],\n        ""BiGRULast"": [""hidden_dim"", ""dropout""],\n        ""BiLSTM"": [""hidden_dim"", ""dropout"", ""num_layers""],\n        ""BiLSTMAtt"": [""hidden_dim"", ""dropout"", ""num_layers""],\n        ""Conv"": [""stride"", ""padding"", ""window_sizes"", ""input_channel_num"", ""output_channel_num"", ""activation"",\n                 ""batch_normalization""],\n        ""ConvPooling"": [""stride"", ""padding"", ""window_sizes"", ""input_channel_num"", ""output_channel_num"",\n                        ""batch_normalization"",\n                        ""activation"", ""pool_type"", ""pool_axis""],\n        ""Pooling"": [""pool_axis"", ""pool_type""],\n        ""Dropout"": [""dropout""],\n        ""Combination"": [""operations""],\n        ""EncoderDecoder"": [""encoder"", ""decoder""],\n        ""FullAttention"": [""hidden_dim"", ""activation""],\n        ""Seq2SeqAttention"": [""attention_dropout""]\n    }\n\n    model = Digraph(format=\'svg\',\n                    node_attr={""style"": ""rounded, filled"",\n                               ""shape"": ""box"",\n                               ""fontcolor"": ""white""})\n    model.attr(rankdir=""BT"")\n\n    for item in conf_dic[\'architecture\']:\n        if item[\'layer\'] == ""Embedding"":\n            for c in item[\'conf\']:\n                dim = item[\'conf\'][c][\'dim\']\n                for n in item[\'conf\'][c][\'cols\']:\n                    label_str = ""<"" \\\n                                + ""<table border=\'0.5\' align=\'center\'>"" \\\n                                + ""<tr><td align=\'text\'><i>"" + n + ""</i></td>"" + ""<td align=\'text\'><b>Embedding</b></td></tr>"" \\\n                                + ""<tr><td align=\'text\'>dim:</td>"" + ""<td align=\'text\'>"" + str(dim) + ""</td></tr>"" \\\n                                + ""</table>>""\n                    model.node(name=n, label=label_str, fillcolor=color[""Embedding""])\n            break\n\n    for inp in conf_dic[\'inputs\'][\'model_inputs\']:\n        model.node(name=inp,\n                   label=inp,\n                   fillcolor=color[\'Input\'])\n        for n in conf_dic[\'inputs\'][\'model_inputs\'][inp]:\n            model.edge(n, inp)\n\n    layer_dic = {}\n    for item in conf_dic[\'architecture\']:\n        if \'layer_id\' in item.keys() and \'layer\' in item.keys() and \'conf\' in item.keys():\n            layer_dic[item[\'layer_id\']] = [item[\'layer\'], item[\'conf\']]\n\n    for item in conf_dic[\'architecture\']:\n        if \'layer_id\' in item.keys():\n            if item[\'layer\'] in layer_dic:\n                tmp_layer = item[\'layer\']\n                item[\'conf\'] = layer_dic[tmp_layer][1]\n                item[\'layer\'] = layer_dic[tmp_layer][0]\n            label_str = ""<"" \\\n                        + ""<table border=\'0.5\' align=\'center\'>"" \\\n                        + ""<tr><td align=\'text\'><i>"" + item[\'layer_id\'] + ""</i></td>"" + ""<td align=\'text\'><b>"" + item[\n                            \'layer\'] + ""</b></td></tr>""\n            if item[\'layer\'] in layer_conf:\n                for c in layer_conf[item[\'layer\']]:\n                    if c in item[\'conf\']:\n                        label_str = label_str + ""<tr><td align=\'text\'>"" + c + ""</td>"" + ""<td align=\'text\'>"" + str(\n                            item[\'conf\'][c]) + ""</td></tr>""\n            else:\n                for c in item[\'conf\']:\n                    label_str = label_str + ""<tr><td align=\'text\'>"" + c + ""</td>"" + ""<td align=\'text\'>"" + str(\n                        item[\'conf\'][c]) + ""</td></tr>""\n\n            label_str += ""</table>>""\n\n            model.node(name=item[\'layer_id\'],\n                       label=label_str,\n                       fillcolor=color.get(item[\'layer\'], ""grey""))\n            for inp in item[\'inputs\']:\n                model.edge(inp, item[\'layer_id\'])\n    # model\n    model.render(\'static/graph.gv\', view=False)\n\n    return\n'"
